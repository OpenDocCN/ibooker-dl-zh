- en: 'Chapter 12\. Magic Wand: Training a Model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 11](ch11.xhtml#chapter_magic_wand_application), we used a 20 KB
    pretrained model to interpret raw accelerometer data, using it to identify which
    of a set of gestures was performed. In this chapter, we show you how this model
    was trained, and then we talk about how it actually works.
  prefs: []
  type: TYPE_NORMAL
- en: Our wake-word and person detection models both required large amounts of data
    to train. This is mostly due to the complexity of the problems they were trying
    to solve. There are a huge number of different ways in which a person can say
    “yes” or “no”—think of all the variations of accent, intonation, and pitch that
    make someone’s voice unique. Similarly, a person can appear in an image in an
    infinite variety of ways; you might see their face, their whole body, or a single
    hand, and they could be standing in any possible pose.
  prefs: []
  type: TYPE_NORMAL
- en: So that it can accurately classify such a diversity of valid inputs, a model
    needs to be trained on an equally diverse set of training data. This is why our
    datasets for wake-word and person detection training were so large, and why training
    takes so long.
  prefs: []
  type: TYPE_NORMAL
- en: Our magic wand gesture recognition problem is a lot simpler. In this case, rather
    than trying to classify a huge range of natural voices or human appearances and
    poses, we’re attempting to understand the differences between three specific and
    deliberately selected gestures. Although there’ll be some variation in the way
    different people perform each gesture, we’re hoping that our users will strive
    to perform the gestures as correctly and uniformly as possible.
  prefs: []
  type: TYPE_NORMAL
- en: This means that there’ll be a lot less variation in our expected valid inputs,
    which makes it a lot easier to train an accurate model without needing vast amounts
    of data. In fact, the dataset we’ll be using to train the model contains only
    around 150 examples for each gesture and is only 1.5 MB in size. It’s exciting
    to think about how a useful model can be trained on such a small dataset, because
    obtaining sufficient data is often the most difficult part of a machine learning
    project.
  prefs: []
  type: TYPE_NORMAL
- en: In the first part of this chapter, you’ll learn how to train the original model
    used in the magic wand application. In the second part, we’ll talk about how this
    model actually works. And finally, you’ll see how you can capture your own data
    and train a new model that recognizes different gestures.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To train our model, we use training scripts located in the TensorFlow repository.
    You can find them in [*magic_wand/train*](https://oreil.ly/LhZGT).
  prefs: []
  type: TYPE_NORMAL
- en: 'The scripts perform the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare raw data for training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate synthetic data.^([1](ch12.xhtml#idm46473549700952))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split the data for training, validation, and testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform data augmentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the model architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the training process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert the model into the TensorFlow Lite format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make life easy, the scripts are accompanied by a Jupyter notebook which demonstrates
    how to use them. You can run the notebook in Colaboratory (Colab) on a GPU runtime.
    With our tiny dataset, training will take only a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: To begin, let’s walk through the training process in Colab.
  prefs: []
  type: TYPE_NORMAL
- en: Training in Colab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Open the Jupyter notebook at [*magic_wand/train/train_magic_wand_model.ipynb*](https://oreil.ly/2BLtj)
    and click the “Run in Google Colab” button, as shown in [Figure 8-1](ch08.xhtml#run_in_google_colab_2).
  prefs: []
  type: TYPE_NORMAL
- en: '![The ''Run in Google Colab'' button](Images/timl_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-1\. The “Run in Google Colab” button
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As of this writing, there’s a bug in GitHub that results in intermittent error
    messages when displaying Jupyter notebooks. If you see the message “Sorry, something
    went wrong. Reload?” when trying to access the notebook, follow the instructions
    in [“Building Our Model”](ch04.xhtml#ch4_building_our_model).
  prefs: []
  type: TYPE_NORMAL
- en: 'This notebook walks through the process of training the model. It includes
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading and preparing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading TensorBoard to visualize the training process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating a C source file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable GPU Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training this model should be very quick, but it will be even faster if we use
    a GPU runtime. To enable this option, go to Colab’s Runtime menu and choose “Change
    runtime type,” as illustrated in [Figure 12-2](#change_runtime_type_2).
  prefs: []
  type: TYPE_NORMAL
- en: This opens the “Notebook settings” dialog box shown in [Figure 12-3](#notebook_settings_2).
  prefs: []
  type: TYPE_NORMAL
- en: From the “Hardware accelerator” drop-down list, select GPU, as depicted in [Figure 12-4](#hardware_accelerator_2),
    and then click SAVE.
  prefs: []
  type: TYPE_NORMAL
- en: You’re now ready to run the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '![The ''Change runtime type'' option in Colab](Images/timl_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-2\. The “Change runtime type” option in Colab
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![The ''Notebook settings'' box](Images/timl_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-3\. The “Notebook settings” dialog box
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![The ''Hardware accelerator'' dropdown](Images/timl_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-4\. The “Hardware accelerator” drop-down list
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Install dependencies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step is to install the required dependencies. In the “Install dependencies”
    section, run the cells to install the correct versions of TensorFlow and grab
    a copy of the training scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, in the “Prepare the data” section, run the cells to download the dataset
    and split it into training, validation, and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first cell downloads and extracts the dataset into the training scripts’
    directory. The dataset consists of four directories, one for each gesture (“wing,”
    “ring,” and “slope”) plus a “negative” directory for data that represents no distinct
    gesture. Each directory contains files that represent raw data resulting from
    the capture process for the gesture being performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: There are 10 files for each gesture, which we’ll walk through later on. Each
    file contains a gesture being demonstrated by a named individual, with the last
    part of the filename corresponding to their user ID. For example, the file *output_slope_dengyl.txt*
    contains data for the “slope” gesture being demonstrated by a user whose ID is
    `dengyl`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are approximately 15 individual performances of a given gesture in each
    file, one accelerometer reading per row, with each performance being prefixed
    by the row `-,-,-`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Each performance consists of a log of up to a few seconds’ worth of data, with
    25 rows per second. The gesture itself occurs at some point within that window,
    with the device being held still for the remainder of the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the way the measurements were captured, the files also contain some
    garbage characters. Our first training script, [*data_prepare.py*](https://oreil.ly/SCZe9),
    which is run in our second training cell, will clean up this dirty data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This script is designed to read the raw data files from their folders, ignore
    any garbage characters, and write them in a sanitized form to another location
    within the training scripts’ directory (*data/complete_data*). Cleaning up messy
    data sources is a common task when training machine learning models given that
    it’s very common for errors, corruption, and other issues to creep into large
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to cleaning the data, the script generates some *synthetic data*.
    This is a term for data that is generated algorithmically, rather than being captured
    from the real world. In this case, the `generate_negative_data()` function in
    *data_prepare.py* creates synthetic data that is equivalent to movement of the
    accelerometer that doesn’t correspond to any particular gesture. This data is
    used to train our “unknown” category.
  prefs: []
  type: TYPE_NORMAL
- en: Because creating synthetic data is much faster than capturing real-world data,
    it’s useful to help augment our training process. However, real-world variation
    is unpredictable, so it’s not often possible to create an entire dataset from
    synthetic data. In our case, it’s helpful for making our “unknown” category more
    robust, but it wouldn’t be helpful for classifying the known gestures.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next script to run in the second cell is [*data_split_person.py*](https://oreil.ly/1U0FW):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This script splits the data into training, validation, and test sets. Because
    our data is labeled with the person who created it, we’re able to use one set
    of people’s data for training, another set for validation, and a final set for
    test. The data is split as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We use six people’s data for training, two for validation, and two for testing.
    In addition, we mix in our negative data, which isn’t associated with a particular
    user. Our total data is split between the three sets at a ratio of roughly 60%/20%/20%,
    which is pretty standard for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: In splitting by person, we’re trying to ensure that our model will be able to
    generalize to new data. Because the model will be validated and tested on data
    from individuals who were not included in the training dataset, the model will
    need to be robust against individual variations in how each person performs each
    gesture.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also possible to split the data randomly, instead of by person. In this
    case, the training, validation, and testing datasets would each contain some samples
    of each gesture from every single individual. The resulting model will have been
    trained on data from every single person rather than just six, so it will have
    had more exposure to people’s varying gesturing styles.
  prefs: []
  type: TYPE_NORMAL
- en: However, because the validation and training sets also contain data from every
    individual, we’d have no way of testing whether the model is able to generalize
    to new gesturing styles that it has not seen before. A model developed in this
    way might report higher accuracy during validation and testing, but it would not
    be guaranteed to work as well with new data.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you’ve run both cells in the “Prepare the data” section before continuing.
  prefs: []
  type: TYPE_NORMAL
- en: Load TensorBoard
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After the data has been prepared, we can run the next cell to load TensorBoard,
    which will help us monitor the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Training logs will be written to the *logs/scalars* subdirectory of the training
    scripts’ directory, so we pass this in to TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: Begin training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After TensorBoard has loaded, it’s time to begin training. Run the following
    cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The script [*train.py*](https://oreil.ly/S3w0X) sets up the model architecture,
    loads the data using [*data_load.py*](https://oreil.ly/aCZgu), and begins the
    training process.
  prefs: []
  type: TYPE_NORMAL
- en: As the data is loaded, *load_data.py* also performs data augmentation using
    code defined in [*data_augmentation.py*](https://oreil.ly/zL6wm). The function
    `augment_data()` takes data representing a gesture and creates a number of new
    versions of it, each modified slightly from the original. The modifications include
    shifting and warping the datapoints in time, adding random noise, and increasing
    the amount of acceleration. This augmented data is used alongside the original
    data to train the model, helping make the most of our small dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'As training ramps up, you’ll see some output appearing below the cell you just
    ran. There’s a lot there, so let’s pick out the most noteworthy parts. First,
    Keras generates a nice table that shows the architecture of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It tells us all the layers that are used, along with their shapes and their
    numbers of parameters—which is another term for weights and biases. You can see
    that our model uses `Conv2D` layers, as it’s a convolutional model. Not shown
    in this table is the fact that our model’s input shape is `(None, 128, 3)`. We’ll
    look more closely at the model’s architecture later.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will also show us an estimate of the model’s size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This represents the amount of memory that will be taken up by the model’s trainable
    parameters. It doesn’t include the extra space required to store the model’s execution
    graph, so our actual model file will be slightly larger, but it gives us an idea
    of the correct order of magnitude. This will definitely qualify as a tiny model!
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll eventually see the training process itself begin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: At this point, you can take a look at TensorBoard to see the training process
    moving along.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate the results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When training is complete, we can look at the cell’s output for some useful
    information. First, we can see that the validation accuracy in our final epoch
    looks very promising at 0.9743, and the loss is nice and low, too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is great, especially as we’re using a per-person data split, meaning our
    validation data is from a completely different set of individuals. However, we
    can’t just rely on our validation accuracy to evaluate our model. Because the
    model’s hyperparameters and architecture were hand-tuned on the validation dataset,
    we might have overfit it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a better understanding of our model’s final performance, we can evaluate
    it against our test dataset by calling Keras’s `model.evaluate()` function. The
    next line of output shows the results of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Although not as amazing as the validation numbers, the model shows a good-enough
    accuracy of 0.9323, with a loss that is still low. The model will predict the
    correct class 93% of the time, which should be fine for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next few lines show the *confusion matrix* for the results, calculated
    by the [`tf.math.confusion_matrix()`](https://oreil.ly/xlIKj) function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: A confusion matrix is a helpful tool for evaluating the performance of classification
    models. It shows how well the predicted class of each input in the test dataset
    agrees with its actual value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each column of the confusion matrix corresponds to a predicted label, in order
    (“wing,” “ring,” “slope,” then “unknown”). Each row, from the top down, corresponds
    to the actual label. From our confusion matrix, we can see that the vast majority
    of predictions agree with the actual labels. We can also see the specific places
    where confusion is occurring: most significantly, a fair number of inputs were
    misclassified as “unknown,” especially those belonging to the “ring” category.'
  prefs: []
  type: TYPE_NORMAL
- en: The confusion matrix gives us an idea of where our model’s weak points are.
    In this case, it informs us that it might be beneficial to obtain more training
    data for the “ring” gesture in order to help the model better learn the differences
    between “ring” and “unknown.”
  prefs: []
  type: TYPE_NORMAL
- en: 'The final thing that *train.py* does is convert the model to TensorFlow Lite
    format, in both floating-point and quantized variations. The following output
    reveals the sizes of each variant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Our 20 KB model shrinks down to 8.8 KB after quantization. This is a *very*
    tiny model, and a great result.
  prefs: []
  type: TYPE_NORMAL
- en: Create a C array
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next cell, in the “Create a C source file” section, transforms this into
    a C source file. Run this cell to see the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We can copy and paste the contents of this file into our project so that we
    can use the newly trained model in our application. Later, you’ll learn how to
    collect new data and teach the application to understand new gestures. For now,
    let’s keep moving.
  prefs: []
  type: TYPE_NORMAL
- en: Other Ways to Run the Scripts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’d prefer not to use Colab, or you’re making changes to the model training
    scripts and would like to test them out locally, you can easily run the scripts
    from your own development machine. You can find the instructions in [*README.md*](https://oreil.ly/6-KPf).
  prefs: []
  type: TYPE_NORMAL
- en: Next up, we walk through how the model itself works.
  prefs: []
  type: TYPE_NORMAL
- en: How the Model Works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we’ve established that our model is a convolutional neural network
    (CNN) and that it transforms a sequence of 128 three-axis accelerometer readings,
    representing around five seconds of time, into an array of four probabilities:
    one for each gesture, and one for “unknown.”'
  prefs: []
  type: TYPE_NORMAL
- en: CNNs are used when the relationships between adjacent values contain important
    information. In the first part of our explanation, we’ll take a look at our data
    and learn why a CNN is well suited to making sense of it.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the Input
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our time-series accelerometer data, adjacent accelerometer readings give
    us clues about the device’s motion. For example, if acceleration on one axis changes
    rapidly from zero to positive, then back to zero, the device might have begun
    motion in that direction. [Figure 12-5](#gesture_single_axis_acceleration_graph)
    shows a hypothetical example of this.
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of accelerometer values for a single axis of a device being moved](Images/timl_1205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-5\. Accelerometer values for a single axis of a device being moved
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Any given gesture is composed of a series of motions, one after the other. For
    example, consider our “wing” gesture, shown in [Figure 12-6](#gesture_wing_gesture).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing the ''wing'' gesture](Images/timl_1206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-6\. The “wing” gesture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The device is first moved down and to the right, then up and to the right, then
    down and to the right, then up and to the right again. [Figure 12-7](#gesture_wing_gesture_graph)
    shows a sample of real data captured during the “wing” gesture, measured in milli-Gs.
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of accelerometer values during the ''wing'' gesture](Images/timl_1207.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-7\. Accelerometer values during the “wing” gesture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By looking at this graph and breaking it down into its component parts, we can
    understand which gesture is being made. From the z-axis acceleration, it’s very
    clear that the device is being moved up and down in the way we would expect given
    the “wing” gesture’s shape. More subtly, we can see how the acceleration on the
    x-axis correlates with the z-axis changes in a way that indicates the device’s
    motion across the width of the gesture. Meanwhile, we can observe that the y-axis
    remains mostly stable.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, a CNN with multiple layers is able to learn how to discern each gesture
    through its telltale component parts. For example, a network might learn to distinguish
    an up-and-down motion, and that two of them, when combined with the appropriate
    z- and y-axis movements, indicates a “wing” gesture.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, a CNN learns a series of *filters*, arranged in layers. Each filter
    learns to spot a particular type of feature in the data. When it notices this
    feature, it passes this high-level information to the next layer of the network.
    For example, one filter in the first layer of the network might learn to spot
    something simple, like a period of upward acceleration. When it identifies such
    a structure, it passes this information to the next layer of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequent layers of filters learn how the outputs of earlier, simpler filters
    are composed together to form larger structures. For example, a series of four
    alternating upward and downward accelerations might fit together to represent
    the “W” shape in our “wing” gesture.
  prefs: []
  type: TYPE_NORMAL
- en: In this process, the noisy input data is progressively transformed into a high-level,
    symbolic representation. Subsequent layers of our network can analyze this symbolic
    representation to guess which gesture was performed.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we walk through the actual model architecture and see how
    it maps onto this process.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Model Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The architecture of our model is defined in [*train.py*](https://oreil.ly/vxT1v),
    in the `build_cnn()` function. This function uses the Keras API to define a model,
    layer by layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a sequential model, meaning the output of each layer is passed directly
    into the next one. Let’s walk through the layers one by one and explore what’s
    going on. The first layer is a `Conv2D`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This is a convolutional layer; it directly receives our network’s input, which
    is a sequence of raw accelerometer data. The input’s shape is provided in the
    `input_shape` argument. It’s set to `(seq_length, 3, 1)`, where `seq_length` is
    the total number of accelerometer measurements that are passed in (128 by default).
    Each measurement is composed of three values, representing the x-, y-, and z-axes.
    The input is visualized in [Figure 12-8](#gesture_model_input_diagram).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of the model''s input](Images/timl_1208.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-8\. The model’s input
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The job of our convolutional layer is to take this raw data and extract some
    basic features that can be interpreted by subsequent layers. The arguments to
    the `Conv2D()` function determine how many features will be extracted. The arguments
    are described in the [`tf.keras.layers.Conv2D()` documentation](https://oreil.ly/hqXJF).
  prefs: []
  type: TYPE_NORMAL
- en: The first argument determines how many filters the layer will have. During training,
    each filter learns to identify a particular feature in the raw data—for example,
    one filter might learn to identify the telltale signs of an upward motion. For
    each filter, the layer outputs a *feature map* that shows where the feature it
    has learned occurs within the input.
  prefs: []
  type: TYPE_NORMAL
- en: The layer defined in our code has eight filters, meaning that it will learn
    to recognize and output eight different types of high-level features from the
    input data. You can see this reflected in the output shape, `(batch_size, 128,
    3, 8)`, which has eight *feature channels* in its final dimension, one for each
    feature. The value in each channel indicates the degree to which a feature was
    present in that location of the input.
  prefs: []
  type: TYPE_NORMAL
- en: As we learned in [Chapter 8](ch08.xhtml#chapter_training_micro_speech), convolutional
    layers slide a window across the data and decide whether a given feature is present
    in that window. The second argument to `Conv2D()` is where we provide the dimensions
    of this window. In our case, it’s `(4, 3)`. This means that the features for which
    our filters are hunting span four consecutive accelerometer measurements and all
    three axes. Because the window spans four measurements, each filter analyzes a
    small snapshot of time, meaning it can generate features that represent a change
    in acceleration over time. You can see how this works in [Figure 12-9](#gesture_model_convolutional_window_diagram).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a convolution window overlaid on the data](Images/timl_1209.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-9\. A convolution window overlaid on the data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `padding` argument determines how the window will be moved across the data.
    When `padding` is set to `"same"`, the layer’s output will have the same length
    (128) and width (3) as the input. Because every movement of the filter window
    results in a single output value, the `"same"` argument means the window must
    be moved three times across the data, and 128 times down it.
  prefs: []
  type: TYPE_NORMAL
- en: Because the window has a width of 3, this means it must start by overhanging
    the lefthand side of the data. The empty spaces, where the filter window doesn’t
    cover an actual value, are *padded* with zeros. To move a total of 128 times down
    the length of the data, the filter must also overhang the top of the data. You
    can see how this works in Figures [12-10](#gesture_model_convolutional_window_padding)
    and [12-11](#gesture_model_convolutional_window_padding_2).
  prefs: []
  type: TYPE_NORMAL
- en: 'As soon as the convolution window has moved across all the data, using each
    filter to create eight different feature maps, the output will be passed to our
    next layer, `MaxPool2D`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![A diagram of a convolution window moving across the data](Images/timl_1210.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-10\. The convolution window in its first position, necessitating padding
    on the top and left sides
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![A diagram of a convolution window moving across the data](Images/timl_1211.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-11\. The same convolution window having moved to its second position,
    requiring padding only on the top
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This `MaxPool2D` layer takes the output of the previous layer, a `(128, 3, 8)`
    tensor, and shrinks it down to a `(42, 1, 8)` tensor—a third of its original size.
    It does this by looking at a window of input data and then selecting the largest
    value in the window and propagating only that value to the output. The process
    is then repeated with the next window of data. The argument provided to the [`MaxPool2D()`](https://oreil.ly/HZo0q)
    function, `(3, 3)`, specifies that a 3 × 3 window should be used. By default,
    the window is always moved so that it contains entirely new data. [Figure 12-12](#gesture_model_max_pooling)
    shows how this process works.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of max pooling at work](Images/timl_1212.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-12\. Max pooling at work
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that although the diagram shows a single value for each element, our data
    actually has eight feature channels per element.
  prefs: []
  type: TYPE_NORMAL
- en: But why do we need to shrink our input like this? When used for classification,
    the goal of a CNN is to transform a big, complex input tensor into a small, simple
    output. The `MaxPool2D` layer helps make this happen. It boils down the output
    of our first convolutional layer into a concentrated, high-level representation
    of the relevant information that it contains.
  prefs: []
  type: TYPE_NORMAL
- en: By concentrating the information, we begin to strip out things that aren’t relevant
    to the task of identifying which gesture was contained within the input. Only
    the most significant features, which were maximally represented in the first convolutional
    layer’s output, are preserved. It’s interesting to note that even though our original
    input had three accelerometer axes for each measurement, a combination of `Conv2D`
    and `MaxPool2D` has now merged these together into a single value.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we’ve shrunk our data down, it goes through a [`Dropout` layer](https://oreil.ly/JuQtU):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `Dropout` layer randomly sets some of a tensor’s values to zero during training.
    In this case, by calling `Dropout(0.1)`, we set 10% of the values to zero, entirely
    obliterating that data. This might seem like a strange thing to do, so let’s explain.
  prefs: []
  type: TYPE_NORMAL
- en: '*Dropout* is a regularization technique. As mentioned earlier in the book,
    *regularization* is the process of improving machine learning models so that they
    are less likely to overfit their training data. Dropout is a simple but effective
    way to limit overfitting. By randomly removing some data between one layer and
    the next, we force the neural network to learn how to cope with unexpected noise
    and variation. Adding dropout between layers is a common and effective practice.'
  prefs: []
  type: TYPE_NORMAL
- en: The dropout layer is only active during training. During inference, it has no
    effect; all of the data is allowed through.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the `Dropout` layer, we again feed the data through a `MaxPool2D` layer
    and a `Dropout` layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This layer has 16 filters and a window size of `(4, 1)`. These numbers are part
    of the model’s hyperparameters, and they were chosen in an iterative process while
    the model was being developed. Designing an effective architecture is a process
    of trial and error, and these magic numbers are what was arrived at after a lot
    of experimentation. It’s unlikely that you’ll ever select the exact right values
    the first time around.
  prefs: []
  type: TYPE_NORMAL
- en: Like the first convolutional layer, this one also learns to spot patterns in
    adjacent values that contain meaningful information. Its output is an even higher-level
    representation of the content of a given input. The features it recognizes are
    compositions of the features identified by our first convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'After this convolutional layer, we do another `MaxPool2D` and `Dropout`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This continues the process of distilling the original input down to a smaller,
    more manageable representation. The output, with a shape of `(14, 1, 16)`, is
    a multidimensional tensor that symbolically represents only the most significant
    structures contained within the input data.
  prefs: []
  type: TYPE_NORMAL
- en: If we wanted to, we could continue with the process of convolution and pooling.
    The number of layers in a CNN is just another hyperparameter that we can tune
    during model development. However, during the development of this model, we found
    that two convolutional layers was sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Up until this point, we’ve been running our data through convolutional layers,
    which care only about the relationships between adjacent values—we haven’t really
    been considering the bigger picture. However, because we now have high-level representations
    of the major features contained within our input, we can “zoom out” and study
    them in aggregate. To do so, we flatten our data and feed it into a `Dense` layer
    (also known as a *fully connected layer*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The [`Flatten` layer](https://oreil.ly/TUIZc) is used to transform a multidimensional
    tensor into one with a single dimension. In this case, our `(14, 1, 16)` tensor
    is squished down into a single dimension with shape `(224)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s then fed into a [`Dense` layer](https://oreil.ly/FbpDB) with 16 neurons.
    This is one of the most basic tools in the deep learning toolbox: a layer where
    every input is connected to every neuron. By considering all of our data, all
    at once, this layer can learn the meanings of various combinations of inputs.
    The output of this `Dense` layer will be a set of 16 values representing the content
    of the original input in a highly compressed form.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our final task is to shrink these 16 values down into 4 classes. To do this,
    we first add some more dropout and then a final `Dense` layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This layer has four neurons; one representing each class of gesture. Each of
    them is connected to all 16 of the outputs from the previous layer. During training,
    each neuron will learn the combination of previous-layer activations that correspond
    to the gesture it represents.
  prefs: []
  type: TYPE_NORMAL
- en: The layer is configured with a `"softmax"` activation function, which results
    in the layer’s output being a set of probabilities that sum to 1\. This output
    is what we see in the model’s output tensor.
  prefs: []
  type: TYPE_NORMAL
- en: This type of model architecture—a combination of convolutional and fully connected
    layers—is very useful in classifying time-series sensor data like the measurements
    we obtain from our accelerometer. The model learns to identify the high-level
    features that represent the “fingerprint” of a particular class of input. It’s
    small, runs fast, and doesn’t take long to train. This architecture will be a
    valuable tool in your belt as an embedded machine learning engineer.
  prefs: []
  type: TYPE_NORMAL
- en: Training with Your Own Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll show you how to train your own, custom model that recognizes
    new gestures. We’ll walk through how to capture accelerometer data, modify the
    training scripts to incorporate it, train a new model, and integrate it into the
    embedded application.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To obtain training data, we can use a simple program to log accelerometer data
    to the serial port while gestures are being performed.
  prefs: []
  type: TYPE_NORMAL
- en: SparkFun Edge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The fastest way to get started is by modifying one of the examples in the [SparkFun
    Edge Board Support Package (BSP)](https://oreil.ly/z4eHX). First, follow SparkFun’s
    [“Using SparkFun Edge Board with Ambiq Apollo3 SDK”](https://oreil.ly/QqKPa) guide
    to set up the Ambiq SDK and SparkFun Edge BSP.
  prefs: []
  type: TYPE_NORMAL
- en: After you’ve downloaded the SDK and BSP, you’ll need to tweak the example code
    so it does what we want.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, open the file *AmbiqSuite-Rel2.2.0/boards/SparkFun_Edge_BSP/examples/example1_edge_test/src/tf_adc/tf_adc.c*
    in your text editor of choice. Find the call to `am_hal_adc_samples_read()`, on
    line 61 of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Change its second parameter to `true` so that the entire function call looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you’ll need to modify the file *AmbiqSuite-Rel2.2.0/boards/SparkFun_Edge_BSP/examples/example1_edge_test/src/main.c*.
    Find the `while` loop on line 51:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the code to add the following extra line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now find this line a little further along in the `while` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Delete the original line and replace it with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The program will now output data in the format expected by the training scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Next, follow the instructions in [SparkFun’s guide](https://oreil.ly/BPJMG)
    to build the `example1_edge_test` example application and flash it to the device.
  prefs: []
  type: TYPE_NORMAL
- en: Logging data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After you’ve built and flashed the example code, follow these instructions to
    capture some data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, open a new terminal window. Then run the following command to begin
    logging all of the terminal’s output to a file named *output.txt*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, in the same window, use `screen` to connect to the device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Measurements from the accelerometer will be shown on the screen and saved to
    *output.txt* in the same comma-delimited format expected by the training scripts.
  prefs: []
  type: TYPE_NORMAL
- en: You should aim to capture multiple performances of the same gesture in a single
    file. To start capturing a single performance of a gesture, press the button marked
    `RST`. The characters `-,-,-` will be written to the serial port; this output
    is used by the training scripts to identify the start of a gesture performance.
    After you’ve performed the gesture, press the button marked `14` to stop logging
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you’ve logged the same gesture a number of times, exit `screen` by pressing
    Ctrl- A, immediately followed by the K key, and then the Y key. After you’ve exited
    `screen`, enter the following command to stop logging data to *output.txt*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: You now have a file, *output.txt*, which contains data for one person performing
    a single gesture. To train an entirely new model, you should aim to collect a
    similar amount of data as in the original dataset, which contains around 15 performances
    of each gesture by 10 people.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t care about your model working for people other than yourself, you
    can probably get away with capturing only your own performances. That said, the
    more variation in performances you can collect, the better.
  prefs: []
  type: TYPE_NORMAL
- en: 'For compatibility with the training scripts, you should rename your captured
    data files in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, data for a hypothetical “triangle” gesture made by “Daniel” would
    have the following name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The training scripts will expect the data to be organized in directories for
    each gesture name; for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: You’ll also need to provide data for the “unknown” category, in a directory
    named *negative*. In this case, you can just reuse the data files from the original
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note that because the model architecture is designed to output probabilities
    for four classes (three gestures plus “unknown”), you should provide three gestures
    of your own. If you want to train on more or fewer gestures, you’ll need to change
    the training scripts and adjust the model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the Training Scripts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train a model with your new gestures, you need to make some changes to the
    training scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, replace all of the gesture names within the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*data_load.py*](https://oreil.ly/1Tplr)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*data_prepare.py*](https://oreil.ly/O7eym)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*data_split.py*](https://oreil.ly/w8ORq)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, replace all of the person names within the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*data_prepare.py*](https://oreil.ly/3swnY)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*data_split_person.py*](https://oreil.ly/xhVh7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that if you have a different number of person names (the original dataset
    has 10) and you want to split the data by person during training, you’ll need
    to decide on a new split. If you have data from only a few people, it won’t be
    possible to split by person during training, so don’t worry about *data_split_person.py*.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train a new model, copy your data files directories into the training scripts’
    directory and follow the process we walked through earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: If you have data from only a few people, you should split the data randomly
    rather than per person. To do this, run *data_split.py* instead of *data_split_person.py*
    when preparing for training.
  prefs: []
  type: TYPE_NORMAL
- en: Because you’re training on new gestures, it’s worth playing with the model’s
    hyperparameters to obtain the best accuracy. For example, you can see whether
    you get better results by training for more or fewer epochs, or with a different
    arrangement of layers or number of neurons, or with different convolutional hyperparameters.
    You can use TensorBoard to monitor your progress.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have a model with acceptable accuracy, you’ll need to make a few changes
    to the project to make sure it works.
  prefs: []
  type: TYPE_NORMAL
- en: Using the New Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, you’ll need to copy the new model’s data, as formatted by `xxd -i`, into
    *magic_wand_model_data.cc*. Make sure you also update the value of `g_magic_wand_model_data_len`
    to match the number output by `xxd`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, in the array `should_continuous_count`, you’ll need to update the values
    in *accelerometer_handler.cc* that specify the number of continuous predictions
    required for each gesture. The value corresponds to how long the gesture takes
    to perform. Given that the original “wing” gesture requires a continuous count
    of 15, estimate how long your new gestures will take relative to that, and update
    the values in the array. You can tune these values iteratively until you get the
    most reliable performance.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, update the code in *output_handler.cc* to print the correct names for
    your new gestures. When this is done, you can build your code and flash your device.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve taken our deepest dive yet into the architecture of a
    typical embedded machine learning model. This type of convolutional model is a
    powerful tool for classifying time-series data, and you’ll come across it often.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you hopefully have an understanding of what embedded machine learning
    applications look like, and how their application code works together with models
    to understand the world around them. As you build your own projects, you’ll begin
    to put together a toolbox of familiar models that you can use to solve different
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This book is intended to provide a gentle introduction to the possibilities
    of embedded machine learning, but it’s not a complete reference on machine learning
    itself. If you’d like to dig deeper into building your own models, there are some
    amazing and highly accessible resources that are suitable for students of all
    backgrounds and will give you a running start.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of our favorites, which will build on what you’ve learned here:'
  prefs: []
  type: TYPE_NORMAL
- en: François Chollet’s [*Deep Learning with Python*](https://oreil.ly/PFF3r) (Manning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aurélien Géron’s [*Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow*,
    2nd Edition](https://oreil.ly/M5KrN) (O’Reilly)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deeplearning.ai’s [Deep Learning Specialization](https://oreil.ly/xKQMP) and
    [TensorFlow in Practice](https://oreil.ly/4q7HY) courses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Udacity’s [Intro to TensorFlow for Deep Learning](https://oreil.ly/YJlYd) course
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What’s Next
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The remaining chapters of this book take a deeper dive into the tools and workflows
    of embedded machine learning. You’ll learn how to think about designing your own
    TinyML applications, how to optimize models and application code to run well on
    low-powered devices, how to port existing machine learning models to embedded
    devices, and how to debug embedded machine learning code. We’ll also address some
    high-level concerns, like deployment, privacy, and security.
  prefs: []
  type: TYPE_NORMAL
- en: But first, let’s learn a bit more about TensorFlow Lite, the framework that
    powers all of the examples in this book.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch12.xhtml#idm46473549700952-marker)) This is a new term, which we’ll
    talk about later.
  prefs: []
  type: TYPE_NORMAL
