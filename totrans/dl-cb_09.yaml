- en: Chapter 9\. Reusing a Pretrained Image Recognition Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。重用预训练的图像识别网络
- en: Image recognition and computer vision is one of the areas where deep learning
    has made some significant impacts. Networks with dozens of layers, sometimes more
    than a hundred, have proven to be very effective in image classification tasks,
    to the point where they outperform humans.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 图像识别和计算机视觉是深度学习取得重大影响的领域之一。拥有几十层甚至超过一百层的网络已经被证明在图像分类任务中非常有效，甚至超过了人类。
- en: Training such networks, though, is very involved, both in terms of processing
    power and the amount of training images needed. Fortunately, we often don’t have
    to start from scratch, but can reuse an existing network.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，训练这样的网络非常复杂，无论是在处理能力还是所需的训练图像数量方面。幸运的是，我们通常不必从头开始，而是可以重用现有的网络。
- en: In this chapter we’ll walk through how to load one of the five pretrained networks
    that are supported by Keras, go into the preprocessing that is needed before we
    can feed an image into a network, and finally show how we can run the network
    in inference mode, where we ask it what it thinks the image contains.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何加载Keras支持的五个预训练网络之一，讨论在将图像输入网络之前需要的预处理，最后展示如何在推理模式下运行网络，询问它认为图像包含什么。
- en: We’ll then look into what is known as *transfer learning*—taking a pretrained
    network and partly retraining it on new data for a new task. We’ll first acquire
    a set of images from Flickr containing cats and dogs. We’ll then teach our network
    to tell them apart. This will be followed by an application where we use this
    network to improve upon Flickr’s search results. Finally, we’ll download a set
    of images that contain pictures of 37 different types of pets and train a network
    that beats the average human at labeling them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨所谓的*迁移学习*——将一个预训练网络部分重新训练以适应新任务的新数据。我们首先从Flickr获取一组包含猫和狗的图像。然后教会我们的网络区分它们。接下来，我们将应用这个网络来改进Flickr的搜索结果。最后，我们将下载一组包含37种不同宠物图片的图像，并训练一个网络，使其在标记它们方面超过平均人类。
- en: 'The following notebooks contain the code referred to in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 以下笔记本包含本章中提到的代码：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 9.1 Loading a Pretrained Network
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.1 加载预训练网络
- en: Problem
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You’d like to know how to instantiate a pretrained image recognition network.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您想知道如何实例化一个预训练的图像识别网络。
- en: Solution
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use Keras to load up a pretrained network, downloading the weights if necessary.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Keras加载一个预训练网络，如果需要的话下载权重。
- en: 'Keras doesn’t only make it easier to compose networks, it also ships with references
    to a variety of pretrained networks that we can easily load:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Keras不仅使得组合网络更容易，还提供了对各种预训练网络的引用，我们可以轻松加载：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will also print a summary of the network, showing its various layers. This
    is useful when we want to use the network, since it not only shows the names of
    the layers but also their sizes and how they are connected.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这也将打印网络的摘要，显示其各个层。当我们想要使用网络时，这是有用的，因为它不仅显示层的名称，还显示它们的大小以及它们是如何连接的。
- en: Discussion
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Keras ships with access to a number of popular image recognition networks that
    can be readily downloaded. The downloads are cached in *~/.keras/models/*, so
    you’ll usually only have to wait for the download the first time.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了访问多个流行的图像识别网络的功能，可以直接下载。这些下载被缓存在*~/.keras/models/*中，所以通常您只需要在第一次下载时等待。
- en: In total we can use five different networks (VGG16, VGG19, ResNet50, Inception
    V3, and Xception). They differ in complexity and architecture, though for most
    simpler applications it probably doesn’t matter which model you pick. VGG16 has
    “only” a depth of 16 layers, which makes it easier to inspect. Inception is a
    much deeper network but has 85% fewer variables, which makes it quicker to load
    and less memory-intensive.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 总共我们可以使用五种不同的网络（VGG16、VGG19、ResNet50、Inception V3和Xception）。它们在复杂性和架构上有所不同，但对于大多数简单的应用程序来说，可能不太重要选择哪个模型。VGG16只有16层深度，更容易检查。Inception是一个更深的网络，但变量少了85%，这使得加载更快，占用内存更少。
- en: 9.2 Preprocessing Images
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.2 图像预处理
- en: Problem
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You’ve loaded a pretrained network, but now you need to know how to preprocess
    an image before feeding it into the network.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经加载了一个预训练网络，但现在需要知道如何在将图像输入网络之前对图像进行预处理。
- en: Solution
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Crop and resize the image to the right size and normalize the colors.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 裁剪和调整图像到正确的大小，并对颜色进行归一化。
- en: All of the pretrained networks included in Keras expect their inputs to be square
    and of a certain size. They also expect the color channels to be normalized. Normalizing
    the images while training makes it easier for the networks to focus on the things
    that matter and not get “distracted.”
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Keras包含的所有预训练网络都期望它们的输入是方形的，并且具有特定的大小。它们还期望颜色通道被归一化。在训练时对图像进行归一化使得网络更容易专注于重要的事物，而不会被“分心”。
- en: 'We can use PIL/Pillow to load and center-crop an image:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用PIL/Pillow加载和中心裁剪图像：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can get the desired size from the first layer of the network by querying
    the `input_shape` property. This property also contains the color depth, but depending
    on the architecture this might be the first or the last dimension. By calling
    `max` on it we’ll get the right number:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查询`input_shape`属性，我们可以从网络的第一层获取所需的大小。该属性还包含颜色深度，但根据架构的不同，这可能是第一个或最后一个维度。通过对其调用`max`，我们将得到正确的数字：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![processed image of our cat](assets/dlcb_09in01.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![我们猫的处理图像](assets/dlcb_09in01.png)'
- en: 'Finally, we need to convert the image to a format suitable for the network
    to process. This involves converting the image to an array, expanding the dimensions
    so it’s a batch, and normalizing the colors:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将图像转换为适合网络处理的格式。这涉及将图像转换为数组，扩展维度使其成为一个批次，并对颜色进行归一化：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We are now ready to classify the image!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备对图像进行分类！
- en: Discussion
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Center cropping is not the only option. In fact, Keras has a function in the
    `image` module called `load_img` that will load and resize an image, but doesn’t
    do the cropping. It is a good general-purpose strategy for converting an image
    to the size that the network expects, though.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 中心裁剪并不是唯一的选择。事实上，Keras的`image`模块中有一个名为`load_img`的函数，它可以加载和调整图像的大小，但不进行裁剪。尽管如此，这是一个将图像转换为网络期望大小的良好通用策略。
- en: Center cropping is often the best strategy, since what we want to classify typically
    sits in the middle of our image and straightforward resizing distorts the picture.
    But in some cases, special strategies might work better. For example, if we have
    very tall images on a white background, then center cropping might cut off too
    much of the actual image, while resizing leads to large distortions. In this case
    a better solution might be to pad the image with white pixels on either side to
    make it square.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 中心裁剪通常是最佳策略，因为我们想要分类的内容通常位于图像中间，直接调整大小会扭曲图片。但在某些情况下，特殊策略可能效果更好。例如，如果我们有很高的白色背景图像，那么中心裁剪可能会切掉太多实际图像，而调整大小会导致严重扭曲。在这种情况下，更好的解决方案可能是在两侧用白色像素填充图像，使其变成正方形。
- en: 9.3 Running Inference on Images
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.3 在图像上运行推断
- en: Problem
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: If you have an image, how do you find out what it shows?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一张图像，如何找出它显示的是什么？
- en: Solution
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Run inference on the image using the pretrained network.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练网络对图像进行推断。
- en: 'Once we have the image in the right format, we can call `predict` on the model:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将图像转换为正确的格式，就可以在模型上调用`predict`：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The predictions are returned as a `numpy` array shaped (1, 1,000)—a vector of
    1,000 for each image in the batch. Each entry in the vector corresponds to a label,
    while the value of the entry indicates how likely it is that the image represents
    the label.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 预测结果以`numpy`数组的形式返回（1, 1,000）—每个批次中的每个图像对应一个包含1,000个元素的向量。向量中的每个条目对应一个标签，而条目的值表示图像代表该标签的可能性有多大。
- en: 'Keras has the convenient `decode_predictions` function to find the best-scoring
    entries and return the labels and corresponding scores:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Keras有方便的`decode_predictions`函数，可以找到得分最高的条目并返回标签和相应的分数：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here are the results for the image in the previous recipe:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是上一个配方中图像的结果：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The network thinks we’re looking at a cat. The second guess of it being a radiator
    is a bit of surprise, although the background does look a bit like a radiator.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 网络认为我们在看一只猫。它猜测是暖气片有点令人惊讶，尽管背景看起来有点像暖气片。
- en: Discussion
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The last layer of this network has a softmax activation function. The softmax
    function makes sure that the sum for the activations of all the classes is equal
    to 1\. Because of how the network learns when it is training, these activations
    can be thought of as the likelihood that the image matches the class.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络的最后一层具有softmax激活函数。softmax函数确保所有类别的激活总和等于1。由于网络在训练时学习的方式，这些激活可以被视为图像匹配类别的可能性。
- en: The pretrained networks all come with a thousand classes of images they can
    recognize. The reason for this is that they are all trained for the [ImageNet
    competition](http://www.image-net.org/challenges/LSVRC/). This makes it easy to
    compare their relative performance, but unless we happen to want to detect the
    images that are part of this competition, it is not immediately useful for practical
    purposes. In the next chapter we’ll see how we can use these pretrained networks
    to classify images of our own choosing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 所有预训练网络都具有一千个可以识别的图像类别。这是因为它们都是为[ImageNet竞赛](http://www.image-net.org/challenges/LSVRC/)进行训练的。这使得比较它们的相对性能变得容易，但除非我们碰巧想要检测这个竞赛中的图像，否则对于实际目的来说并不立即有用。在下一章中，我们将看到如何使用这些预训练网络来对我们自己选择的图像进行分类。
- en: Another restriction is that these types of networks only return one answer,
    while often there are multiple objects in an image. We’ll look into resolving
    this in [Chapter 11](ch11.html#multiple_images).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个限制是这些类型的网络只返回一个答案，而图像中通常有多个对象。我们将在[第11章](ch11.html#multiple_images)中探讨如何解决这个问题。
- en: 9.4 Using the Flickr API to Collect a Set of Labeled Images
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.4 使用Flickr API收集一组带标签的图像
- en: Problem
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you quickly put together a set of labeled images for experimentation?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如何快速组合一组带标签的图像进行实验？
- en: Solution
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the `search` method of the Flickr API.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Flickr API的`search`方法。
- en: 'To use the Flickr API you need to have an application key, so head over to
    [*https://www.flickr.com/services/apps/create*](https://www.flickr.com/services/apps/create)
    to register your app. Once you have a key and a secret, you can search for images
    using the `flickrapi` library:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Flickr API，您需要一个应用程序密钥，所以请前往[*https://www.flickr.com/services/apps/create*](https://www.flickr.com/services/apps/create)注册您的应用程序。一旦您有了密钥和秘钥，您就可以使用`flickrapi`库搜索图像：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The photos returned by Flickr don’t by default contain a URL. We can compose
    the URL from the record though:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Flickr返回的照片默认不包含URL。但我们可以从记录中组合URL：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `HTML` method is the easiest way to display images inside a notebook:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`HTML`方法是在笔记本中显示图像的最简单方法：'
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This should show us a bunch of cat pictures. After we’ve confirmed that we
    have decent images, let’s download a slightly bigger test set:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该显示一堆猫的图片。确认我们有不错的图片后，让我们下载一个稍大一点的测试集：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Discussion
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Getting good training data is always a key concern when running experiments
    in deep learning. When it comes to images, it is hard to beat the Flickr API,
    giving us access to billions of images. Not only can we find images based on keywords
    and tags, but also on where they were taken. We can also filter on how we can
    use the images. For random experiments that isn’t really a factor, but if we want
    to republish the images in some way this certainly comes in handy.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习实验中运行时，获取良好的训练数据始终是一个关键问题。在图像方面，很难超越Flickr API，它为我们提供了数十亿张图像。我们不仅可以根据关键字和标签找到图像，还可以根据它们的拍摄地点进行筛选。我们还可以根据如何使用这些图像进行过滤。对于随机实验来说，这并不是一个因素，但如果我们想以某种方式重新发布这些图像，这肯定会派上用场。
- en: The Flickr API gives us access to general, user-generated images. There are
    other APIs available that, depending on your purpose, might work better. In [Chapter 10](ch10.html#image_search)
    we look at how we can acquire images directly from Wikipedia. [Getty Images](http://developers.gettyimages.com/)
    provides a good API for stock images, while [500px](https://github.com/500px/api-documentation)
    provides access to high-quality images through its API. The last two have strict
    requirements for republishing, but are great for experimentation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Flickr API让我们可以访问一般的用户生成的图像。根据您的目的，可能有其他可用的API更适合。在[Chapter 10](ch10.html#image_search)中，我们将看看如何直接从维基百科获取图像。[Getty
    Images](http://developers.gettyimages.com/)提供了一个用于库存图像的良好API，而[500px](https://github.com/500px/api-documentation)通过其API提供了高质量图像的访问。最后两个对于再发布有严格的要求，但对于实验来说非常好。
- en: 9.5 Building a Classifier That Can Tell Cats from Dogs
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.5 构建一个可以区分猫和狗的分类器
- en: Problem
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You’d like to be able to classify images into one of two categories.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望能够将图像分类为两个类别之一。
- en: Solution
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Train a support vector machine on top of the features coming out of a pretrained
    network.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练网络的特征之上训练支持向量机。
- en: 'Let’s start by fetching a training set for dogs:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从获取狗的训练集开始。
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Load the images as one vector with the cats first, followed by the dogs:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像加载为一个向量，先是猫，然后是狗：
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now load the pretrained model and construct a new model out of it with `fc2`
    as its output. `fc2` is the last fully connected layer before the network assigns
    labels. The values of this layer for an image describe the image in an abstract
    way. Another way to put this is to say that this projects the image into a high-dimensional
    semantic space:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在加载预训练模型，并构建一个以`fc2`为输出的新模型。`fc2`是网络分配标签之前的最后一个全连接层。这一层的值描述了图像的抽象方式。另一种说法是，这将图像投影到高维语义空间中：
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we’ll run the model over all our images:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将在所有图像上运行模型：
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: For every one of our 500 images, we now have a 4,096-dimensional vector characterizing
    that image. As in [Chapter 4](ch04.html#movie_recommender) we can construct a
    support vector machine to find the distinction between cats and dogs in this space.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的500张图像中的每一张，我们现在有一个描述该图像的4,096维向量。就像在[Chapter 4](ch04.html#movie_recommender)中一样，我们可以构建一个支持向量机来找到这个空间中猫和狗之间的区别。
- en: 'Let’s run the SVM and print our performance:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行支持向量机并打印我们的性能：
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Depending on which of the images we fetched, we should see precision around
    90%. We can take a look at the images for which we predicted the wrong class with
    the following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们获取的图像，我们应该看到大约90%的精度。我们可以查看以下代码中我们预测错误类别的图像：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: All in all, our network is not doing too badly. We would be confused too about
    some of these images labeled as cats or dogs!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们的网络表现还不错。对于一些被标记为猫或狗的图像，我们也会感到困惑！
- en: Discussion
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: As we saw in [Recipe 4.3](ch04.html#building-a-movie-recommender), support vector
    machines are a good choice when we need a classifier on top of high-dimensional
    spaces. Here we extract the output of an image recognition network and treat those
    vectors as image embeddings. We let the SVM find hyperplanes that separate the
    cats from the dogs. This works well for binary cases. We can use SVMs for situations
    where we have more than two classes, but things get more complicated and it might
    make more sense to add a layer to our network to do the heavy lifting. [Recipe
    9.7](#retraining-image-recognition-networks) shows how to do this.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[Recipe 4.3](ch04.html#building-a-movie-recommender)中看到的，当我们需要在高维空间上建立分类器时，支持向量机是一个不错的选择。在这里，我们提取图像识别网络的输出，并将这些向量视为图像嵌入。我们让支持向量机找到将猫和狗分开的超平面。这对于二元情况效果很好。我们可以在有多于两个类别的情况下使用支持向量机，但情况会变得更加复杂，也许更合理的做法是在我们的网络中添加一层来完成繁重的工作。[Recipe
    9.7](#retraining-image-recognition-networks)展示了如何做到这一点。
- en: A lot of the times the classifier doesn’t get the right answer, you can really
    blame the quality of the search results. In the next recipe, we’ll take a look
    at how we can improve search results using the image features we’ve extracted.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 很多时候分类器没有得到正确答案，你可以真的归咎于搜索结果的质量。在下一个配方中，我们将看看如何利用我们提取的图像特征来改进搜索结果。
- en: 9.6 Improving Search Results
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.6 改进搜索结果
- en: Problem
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you filter out the outliers from a set of images?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如何从一组图像中过滤掉异常值？
- en: Solution
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Treat the features from the highest-but-one layer of the image classifier as
    image embeddings and find the outliers in that space.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像分类器的最高但一个层的特征视为图像嵌入，并在该空间中找到异常值。
- en: As we saw in the previous recipe, one of the reasons why our network sometimes
    failed to distinguish between cats and dogs was that the images it saw weren’t
    very good. Sometimes the images weren’t pictures of cats or dogs at all and the
    network just had to guess.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一个配方中看到的，我们的网络有时无法区分猫和狗的原因之一是它看到的图像质量不太好。有时图像根本不是猫或狗的图片，网络只能猜测。
- en: The Flickr search API doesn’t return images that match the supplied text query,
    but images whose tags, descriptions, or titles match the text. Even major search
    engines have only recently started to take into account what can actually be seen
    in the images they return. (So, a search for “cat” might return a picture of a
    lion captioned “look at this big cat.”)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Flickr搜索API不会返回与提供的文本查询匹配的图像，而是返回其标签、描述或标题与文本匹配的图像。即使是主要搜索引擎最近也开始考虑返回的图像中实际可见的内容。（因此，搜索“猫”可能会返回一张标题为“看这只大猫”的狮子图片。）
- en: As long as the majority of the returned images do match the intent of the user,
    we can improve upon the search by filtering out the outliers. For a production
    system it might be worth exploring something more sophisticated; in our case,
    where we have at most a few hundred images and thousands of dimensions, we can
    get away with something simpler.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 只要返回的图像大多数符合用户的意图，我们可以通过过滤掉异常值来改进搜索。对于生产系统，值得探索更复杂的东西；在我们的情况下，我们最多有几百张图像和数千个维度，我们可以使用更简单的方法。
- en: 'Let’s start by getting some recent cat pictures. Since we sort by `recent`
    and not `relevance` here, we expect the search results to be slightly less accurate:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最近的猫图片开始。由于我们按`recent`而不是`relevance`排序，我们预计搜索结果会稍微不太准确：
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As before, we load the images as one vector:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，我们将图像加载为一个向量：
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We’ll look for outliers by first finding the average point in the “maybe cat”
    space:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先通过找到“可能是猫”的空间中的平均点来寻找异常值：
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then we calculate the distances of the cat vectors to the centroid:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算猫向量到质心的距离：
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'And now we can take a look at the things that are least like the average cat:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看看与平均猫最不相似的东西：
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Filtering out the noncats this way works reasonably well, but since the outliers
    disproportionately influence the average vector, the top of our list looks a bit
    noisy. One way to improve upon this is to repeatedly recalculate the centroid
    only on top of the results so far, like a poor man’s outlier filter:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式过滤掉非猫的效果还不错，但由于异常值对平均向量的影响较大，我们列表的前面看起来有点杂乱。改进的一种方法是反复在迄今为止的结果上重新计算质心，就像一个穷人的异常值过滤器：
- en: '[PRE25]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This results in very decent top results.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了非常不错的顶级结果。
- en: Discussion
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: In this recipe we used the same technique from [Recipe 9.5](#building-a-classifier-that-can-tell-cats-from-dogs)
    to improve upon the search results from Flickr. We can imagine the high-dimensional
    space with our images as a large “point cloud.”
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用了与[示例9.5](#building-a-classifier-that-can-tell-cats-from-dogs)相同的技术来改进从Flickr获取的搜索结果。我们可以将我们的图像看作一个大的“点云”高维空间。
- en: Rather than finding a hyperplane that separates the dogs from the cats, we try
    to find the most central cat. We then assume that the distance to this archetypical
    cat is a good measure for “catness.”
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与其找到一个将狗与猫分开的超平面，我们试图找到最中心的猫。然后我们假设到这个典型猫的距离是“猫性”的一个很好的度量。
- en: We’ve taken a simplistic approach to finding the most central cat; just average
    the coordinates, throw out the outliers, take the average again, and repeat. Ranking
    outliers in high-dimensional spaces is an active area of research and there are
    many interesting algorithms being developed.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采取了一种简单的方法来找到最中心的猫；只需平均坐标，去除异常值，再次取平均，重复。在高维空间中排名异常值是一个活跃的研究领域，正在开发许多有趣的算法。
- en: 9.7 Retraining Image Recognition Networks
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.7 重新训练图像识别网络
- en: Problem
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you train a network to recognize images in a specialized category?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如何训练一个网络来识别一个专门的类别中的图像？
- en: Solution
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Train a classifier on top of the features extracted from a pretrained network.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在从预训练网络中提取的特征之上训练一个分类器。
- en: Running an SVM on top of a pretrained network is a good solution if we have
    two categories of images, but less suitable if we have a large number of classes
    to choose from. The Oxford-IIIT Pet Dataset, for example, contains 37 different
    pet categories, each of which has around 200 pictures.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练网络的基础上运行SVM是一个很好的解决方案，如果我们有两类图像，但如果我们有大量可选择的类别，则不太适合。例如，牛津-IIIT宠物数据集包含37种不同的宠物类别，每个类别大约有200张图片。
- en: Training a network from scratch would take a lot of time and might not be super
    effective—7,000 images isn’t a lot when it comes to deep learning. What we’ll
    do instead is take a pretrained network minus the top layers and build on top
    of that. The intuition here is that the bottom layers of the pretrained layer
    recognize features in the images that the layers that we provide can use to learn
    how to distinguish these pets from each other.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始训练一个网络会花费很多时间，而且可能效果不是很好——当涉及到深度学习时，7000张图片并不多。我们将采取的做法是拿一个去掉顶层的预训练网络，然后在其基础上构建。这里的直觉是，预训练层的底层识别图像中的特征，我们提供的层可以利用这些特征学习如何区分这些宠物。
- en: 'Let’s load the Inception model, minus the top layers, and freeze the weights.
    Freezing the weights means that they are no longer changed during training:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载Inception模型，去掉顶层，并冻结权重。冻结权重意味着它们在训练过程中不再改变：
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now let’s add some trainable layers on top. With one fully connected layer
    in between, we ask the model to predict our animal pet classes:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在顶部添加一些可训练的层。在中间加入一个全连接层，我们要求模型预测我们的动物宠物类别：
- en: '[PRE27]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s load up the data from the unpacked *tar.gz* provided by the Oxford-IIIT
    Pet Dataset. The filenames are of the format *<class_name>_<idx>.jpg*, so we can
    split off the *<class_name>* while updating the `label_to_idx` and `idx_to_label`
    tables:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从牛津-IIIT宠物数据集提供的解压缩的*tar.gz*中加载数据。文件名的格式为*<class_name>_<idx>.jpg*，因此我们可以分离出*<class_name>*，同时更新`label_to_idx`和`idx_to_label`表：
- en: '[PRE28]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we convert the images into training data:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将图像转换为训练数据：
- en: '[PRE29]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'And set up the labels as one-hot encoded vectors:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 并将标签设置为独热编码向量：
- en: '[PRE30]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Training the model for 15 epochs produces decent results with over 90% precision:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对模型进行15个时期的训练可以产生不错的结果，精度超过90%：
- en: '[PRE31]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'What we’ve done so far is called *transfer learning*. We can do a bit better
    by unfreezing the top layers of the pretrained network to give it some more leeway
    to train. `mixed9` is a layer in the network about two-thirds of the way up:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止我们所做的被称为*迁移学习*。我们可以通过解冻预训练网络的顶层来使其有更多的训练余地，做得更好。`mixed9`是网络中的一层，大约在中间的三分之二处：
- en: '[PRE32]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can continue training:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续训练：
- en: '[PRE33]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: And we should see that performance improves even more, up to 98%!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到性能进一步提高，达到98%！
- en: Discussion
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Transfer learning is a key concept in deep learning. The world’s leaders in
    machine learning often publish the architectures of their top-performing networks,
    which makes for a good start if we want to reproduce their results, but we don’t
    always have easy access to the training data they used to get those results. And
    even if we do have access, training these world-class networks takes a lot of
    computing resources.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是深度学习中的一个关键概念。世界上机器学习领域的领导者经常发布他们最佳网络的架构，如果我们想要复现他们的结果，这是一个很好的起点，但我们并不总是能够轻松获得他们用来获得这些结果的训练数据。即使我们有访问权限，训练这些世界级网络需要大量的计算资源。
- en: Having access to the actual trained networks is extremely useful if we want
    to do the same things the networks were trained for, but using transfer learning
    also can help us a lot when we want to perform similar tasks. Keras ships with
    a variety of models, but if they don’t suffice, we can adapt models built for
    different frameworks.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要做与网络训练相同的事情，那么拥有实际训练过的网络是非常有用的，但是当我们想要执行类似任务时，使用迁移学习也可以帮助我们很多。Keras附带了各种模型，但如果它们不够用，我们可以调整为其他框架构建的模型。
