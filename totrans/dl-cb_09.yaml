- en: Chapter 9\. Reusing a Pretrained Image Recognition Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image recognition and computer vision is one of the areas where deep learning
    has made some significant impacts. Networks with dozens of layers, sometimes more
    than a hundred, have proven to be very effective in image classification tasks,
    to the point where they outperform humans.
  prefs: []
  type: TYPE_NORMAL
- en: Training such networks, though, is very involved, both in terms of processing
    power and the amount of training images needed. Fortunately, we often don’t have
    to start from scratch, but can reuse an existing network.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we’ll walk through how to load one of the five pretrained networks
    that are supported by Keras, go into the preprocessing that is needed before we
    can feed an image into a network, and finally show how we can run the network
    in inference mode, where we ask it what it thinks the image contains.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll then look into what is known as *transfer learning*—taking a pretrained
    network and partly retraining it on new data for a new task. We’ll first acquire
    a set of images from Flickr containing cats and dogs. We’ll then teach our network
    to tell them apart. This will be followed by an application where we use this
    network to improve upon Flickr’s search results. Finally, we’ll download a set
    of images that contain pictures of 37 different types of pets and train a network
    that beats the average human at labeling them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following notebooks contain the code referred to in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 9.1 Loading a Pretrained Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to know how to instantiate a pretrained image recognition network.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use Keras to load up a pretrained network, downloading the weights if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras doesn’t only make it easier to compose networks, it also ships with references
    to a variety of pretrained networks that we can easily load:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This will also print a summary of the network, showing its various layers. This
    is useful when we want to use the network, since it not only shows the names of
    the layers but also their sizes and how they are connected.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keras ships with access to a number of popular image recognition networks that
    can be readily downloaded. The downloads are cached in *~/.keras/models/*, so
    you’ll usually only have to wait for the download the first time.
  prefs: []
  type: TYPE_NORMAL
- en: In total we can use five different networks (VGG16, VGG19, ResNet50, Inception
    V3, and Xception). They differ in complexity and architecture, though for most
    simpler applications it probably doesn’t matter which model you pick. VGG16 has
    “only” a depth of 16 layers, which makes it easier to inspect. Inception is a
    much deeper network but has 85% fewer variables, which makes it quicker to load
    and less memory-intensive.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Preprocessing Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ve loaded a pretrained network, but now you need to know how to preprocess
    an image before feeding it into the network.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Crop and resize the image to the right size and normalize the colors.
  prefs: []
  type: TYPE_NORMAL
- en: All of the pretrained networks included in Keras expect their inputs to be square
    and of a certain size. They also expect the color channels to be normalized. Normalizing
    the images while training makes it easier for the networks to focus on the things
    that matter and not get “distracted.”
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use PIL/Pillow to load and center-crop an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can get the desired size from the first layer of the network by querying
    the `input_shape` property. This property also contains the color depth, but depending
    on the architecture this might be the first or the last dimension. By calling
    `max` on it we’ll get the right number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![processed image of our cat](assets/dlcb_09in01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we need to convert the image to a format suitable for the network
    to process. This involves converting the image to an array, expanding the dimensions
    so it’s a batch, and normalizing the colors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to classify the image!
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Center cropping is not the only option. In fact, Keras has a function in the
    `image` module called `load_img` that will load and resize an image, but doesn’t
    do the cropping. It is a good general-purpose strategy for converting an image
    to the size that the network expects, though.
  prefs: []
  type: TYPE_NORMAL
- en: Center cropping is often the best strategy, since what we want to classify typically
    sits in the middle of our image and straightforward resizing distorts the picture.
    But in some cases, special strategies might work better. For example, if we have
    very tall images on a white background, then center cropping might cut off too
    much of the actual image, while resizing leads to large distortions. In this case
    a better solution might be to pad the image with white pixels on either side to
    make it square.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Running Inference on Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have an image, how do you find out what it shows?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Run inference on the image using the pretrained network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the image in the right format, we can call `predict` on the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The predictions are returned as a `numpy` array shaped (1, 1,000)—a vector of
    1,000 for each image in the batch. Each entry in the vector corresponds to a label,
    while the value of the entry indicates how likely it is that the image represents
    the label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras has the convenient `decode_predictions` function to find the best-scoring
    entries and return the labels and corresponding scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the results for the image in the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The network thinks we’re looking at a cat. The second guess of it being a radiator
    is a bit of surprise, although the background does look a bit like a radiator.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last layer of this network has a softmax activation function. The softmax
    function makes sure that the sum for the activations of all the classes is equal
    to 1\. Because of how the network learns when it is training, these activations
    can be thought of as the likelihood that the image matches the class.
  prefs: []
  type: TYPE_NORMAL
- en: The pretrained networks all come with a thousand classes of images they can
    recognize. The reason for this is that they are all trained for the [ImageNet
    competition](http://www.image-net.org/challenges/LSVRC/). This makes it easy to
    compare their relative performance, but unless we happen to want to detect the
    images that are part of this competition, it is not immediately useful for practical
    purposes. In the next chapter we’ll see how we can use these pretrained networks
    to classify images of our own choosing.
  prefs: []
  type: TYPE_NORMAL
- en: Another restriction is that these types of networks only return one answer,
    while often there are multiple objects in an image. We’ll look into resolving
    this in [Chapter 11](ch11.html#multiple_images).
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Using the Flickr API to Collect a Set of Labeled Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you quickly put together a set of labeled images for experimentation?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the `search` method of the Flickr API.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the Flickr API you need to have an application key, so head over to
    [*https://www.flickr.com/services/apps/create*](https://www.flickr.com/services/apps/create)
    to register your app. Once you have a key and a secret, you can search for images
    using the `flickrapi` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The photos returned by Flickr don’t by default contain a URL. We can compose
    the URL from the record though:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `HTML` method is the easiest way to display images inside a notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This should show us a bunch of cat pictures. After we’ve confirmed that we
    have decent images, let’s download a slightly bigger test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Getting good training data is always a key concern when running experiments
    in deep learning. When it comes to images, it is hard to beat the Flickr API,
    giving us access to billions of images. Not only can we find images based on keywords
    and tags, but also on where they were taken. We can also filter on how we can
    use the images. For random experiments that isn’t really a factor, but if we want
    to republish the images in some way this certainly comes in handy.
  prefs: []
  type: TYPE_NORMAL
- en: The Flickr API gives us access to general, user-generated images. There are
    other APIs available that, depending on your purpose, might work better. In [Chapter 10](ch10.html#image_search)
    we look at how we can acquire images directly from Wikipedia. [Getty Images](http://developers.gettyimages.com/)
    provides a good API for stock images, while [500px](https://github.com/500px/api-documentation)
    provides access to high-quality images through its API. The last two have strict
    requirements for republishing, but are great for experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: 9.5 Building a Classifier That Can Tell Cats from Dogs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to be able to classify images into one of two categories.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Train a support vector machine on top of the features coming out of a pretrained
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by fetching a training set for dogs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the images as one vector with the cats first, followed by the dogs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now load the pretrained model and construct a new model out of it with `fc2`
    as its output. `fc2` is the last fully connected layer before the network assigns
    labels. The values of this layer for an image describe the image in an abstract
    way. Another way to put this is to say that this projects the image into a high-dimensional
    semantic space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’ll run the model over all our images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: For every one of our 500 images, we now have a 4,096-dimensional vector characterizing
    that image. As in [Chapter 4](ch04.html#movie_recommender) we can construct a
    support vector machine to find the distinction between cats and dogs in this space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run the SVM and print our performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Depending on which of the images we fetched, we should see precision around
    90%. We can take a look at the images for which we predicted the wrong class with
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: All in all, our network is not doing too badly. We would be confused too about
    some of these images labeled as cats or dogs!
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in [Recipe 4.3](ch04.html#building-a-movie-recommender), support vector
    machines are a good choice when we need a classifier on top of high-dimensional
    spaces. Here we extract the output of an image recognition network and treat those
    vectors as image embeddings. We let the SVM find hyperplanes that separate the
    cats from the dogs. This works well for binary cases. We can use SVMs for situations
    where we have more than two classes, but things get more complicated and it might
    make more sense to add a layer to our network to do the heavy lifting. [Recipe
    9.7](#retraining-image-recognition-networks) shows how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of the times the classifier doesn’t get the right answer, you can really
    blame the quality of the search results. In the next recipe, we’ll take a look
    at how we can improve search results using the image features we’ve extracted.
  prefs: []
  type: TYPE_NORMAL
- en: 9.6 Improving Search Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you filter out the outliers from a set of images?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Treat the features from the highest-but-one layer of the image classifier as
    image embeddings and find the outliers in that space.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the previous recipe, one of the reasons why our network sometimes
    failed to distinguish between cats and dogs was that the images it saw weren’t
    very good. Sometimes the images weren’t pictures of cats or dogs at all and the
    network just had to guess.
  prefs: []
  type: TYPE_NORMAL
- en: The Flickr search API doesn’t return images that match the supplied text query,
    but images whose tags, descriptions, or titles match the text. Even major search
    engines have only recently started to take into account what can actually be seen
    in the images they return. (So, a search for “cat” might return a picture of a
    lion captioned “look at this big cat.”)
  prefs: []
  type: TYPE_NORMAL
- en: As long as the majority of the returned images do match the intent of the user,
    we can improve upon the search by filtering out the outliers. For a production
    system it might be worth exploring something more sophisticated; in our case,
    where we have at most a few hundred images and thousands of dimensions, we can
    get away with something simpler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by getting some recent cat pictures. Since we sort by `recent`
    and not `relevance` here, we expect the search results to be slightly less accurate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, we load the images as one vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll look for outliers by first finding the average point in the “maybe cat”
    space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we calculate the distances of the cat vectors to the centroid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we can take a look at the things that are least like the average cat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Filtering out the noncats this way works reasonably well, but since the outliers
    disproportionately influence the average vector, the top of our list looks a bit
    noisy. One way to improve upon this is to repeatedly recalculate the centroid
    only on top of the results so far, like a poor man’s outlier filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This results in very decent top results.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe we used the same technique from [Recipe 9.5](#building-a-classifier-that-can-tell-cats-from-dogs)
    to improve upon the search results from Flickr. We can imagine the high-dimensional
    space with our images as a large “point cloud.”
  prefs: []
  type: TYPE_NORMAL
- en: Rather than finding a hyperplane that separates the dogs from the cats, we try
    to find the most central cat. We then assume that the distance to this archetypical
    cat is a good measure for “catness.”
  prefs: []
  type: TYPE_NORMAL
- en: We’ve taken a simplistic approach to finding the most central cat; just average
    the coordinates, throw out the outliers, take the average again, and repeat. Ranking
    outliers in high-dimensional spaces is an active area of research and there are
    many interesting algorithms being developed.
  prefs: []
  type: TYPE_NORMAL
- en: 9.7 Retraining Image Recognition Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you train a network to recognize images in a specialized category?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Train a classifier on top of the features extracted from a pretrained network.
  prefs: []
  type: TYPE_NORMAL
- en: Running an SVM on top of a pretrained network is a good solution if we have
    two categories of images, but less suitable if we have a large number of classes
    to choose from. The Oxford-IIIT Pet Dataset, for example, contains 37 different
    pet categories, each of which has around 200 pictures.
  prefs: []
  type: TYPE_NORMAL
- en: Training a network from scratch would take a lot of time and might not be super
    effective—7,000 images isn’t a lot when it comes to deep learning. What we’ll
    do instead is take a pretrained network minus the top layers and build on top
    of that. The intuition here is that the bottom layers of the pretrained layer
    recognize features in the images that the layers that we provide can use to learn
    how to distinguish these pets from each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s load the Inception model, minus the top layers, and freeze the weights.
    Freezing the weights means that they are no longer changed during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s add some trainable layers on top. With one fully connected layer
    in between, we ask the model to predict our animal pet classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s load up the data from the unpacked *tar.gz* provided by the Oxford-IIIT
    Pet Dataset. The filenames are of the format *<class_name>_<idx>.jpg*, so we can
    split off the *<class_name>* while updating the `label_to_idx` and `idx_to_label`
    tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we convert the images into training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'And set up the labels as one-hot encoded vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Training the model for 15 epochs produces decent results with over 90% precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'What we’ve done so far is called *transfer learning*. We can do a bit better
    by unfreezing the top layers of the pretrained network to give it some more leeway
    to train. `mixed9` is a layer in the network about two-thirds of the way up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can continue training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: And we should see that performance improves even more, up to 98%!
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transfer learning is a key concept in deep learning. The world’s leaders in
    machine learning often publish the architectures of their top-performing networks,
    which makes for a good start if we want to reproduce their results, but we don’t
    always have easy access to the training data they used to get those results. And
    even if we do have access, training these world-class networks takes a lot of
    computing resources.
  prefs: []
  type: TYPE_NORMAL
- en: Having access to the actual trained networks is extremely useful if we want
    to do the same things the networks were trained for, but using transfer learning
    also can help us a lot when we want to perform similar tasks. Keras ships with
    a variety of models, but if they don’t suffice, we can adapt models built for
    different frameworks.
  prefs: []
  type: TYPE_NORMAL
