- en: Chapter 7\. Recurrent Neural Networks for Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章\. 自然语言处理中的循环神经网络
- en: 'In [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759),
    you saw how to tokenize and sequence text, turning sentences into tensors of numbers
    that could then be fed into a neural network. You then extended that in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)
    by looking at embeddings, which constitute a way to have words with similar meanings
    cluster together to enable the calculation of sentiment. This worked really well,
    as you saw by building a sarcasm classifier. But there’s a limitation to that:
    namely, sentences aren’t just collections of words—and often, the *order* in which
    the words appear will dictate their overall meaning. Also, adjectives can add
    to or change the meaning of the nouns they appear beside. For example, the word
    *blue* might be meaningless from a sentiment perspective, as might *sky*, but
    when you put them together to get *blue sky*, it indicates a clear sentiment that’s
    usually positive. Finally, some nouns may qualify others, such as in *rain cloud*,
    *writing desk*, and *coffee mug*.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)中，你看到了如何标记和序列化文本，将句子转换为数字张量，然后可以将其输入到神经网络中。然后你在[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中扩展了这一点，通过查看嵌入，这些嵌入构成了一种让具有相似含义的词聚集在一起的方法，以便计算情感。正如你所看到的，通过构建讽刺分类器，这确实很有效。但是，这有一个局限性：即句子不仅仅是单词的集合——而且，单词出现的*顺序*往往决定了它们的整体含义。此外，形容词可以增加或改变它们旁边名词的含义。例如，单词*blue*从情感角度来看可能没有意义，*sky*也是如此，但当你将它们组合成*blue
    sky*时，它表明了一种通常积极的明确情感。最后，一些名词可能对其他名词有资格限制，例如在*rain cloud*、*writing desk*和*coffee
    mug*中。
- en: 'To take sequences like this into account, you need to take an additional approach:
    you need to factor *recurrence* into the model architecture. In this chapter,
    you’ll look at different ways of doing this. We’ll explore how sequence information
    can be learned and how you can use this information to create a type of model
    that is better able to understand text: the *recurrent neural network* (RNN).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了考虑这样的序列，你需要采取一种额外的方法：你需要将*循环*因素纳入模型架构。在本章中，你将了解不同的实现方式。我们将探讨如何学习序列信息，以及你如何使用这些信息来创建一种能够更好地理解文本的模型：*循环神经网络*（RNN）。
- en: The Basis of Recurrence
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环的基础
- en: To understand how recurrence might work, let’s first consider the limitations
    of the models used thus far in the book. Ultimately, creating a model looks a
    little bit like [Figure 7-1](#ch07_figure_1_1748549654877957). You provide data
    and labels and define a model architecture, and the model learns the rules that
    fit the data to the labels. Those rules then become available to you as an application
    programming interface (API) that will give you back predicted labels for future
    data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解循环可能的工作方式，让我们首先考虑本书迄今为止使用的模型的局限性。最终，创建一个模型看起来有点像[图7-1](#ch07_figure_1_1748549654877957)。你提供数据和标签，并定义模型架构，然后模型学习适合数据的规则，并将这些规则作为应用程序编程接口（API）提供给你，以便为未来的数据返回预测标签。
- en: '![](assets/aiml_0701.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0701.png)'
- en: Figure 7-1\. High-level view of model creation
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1\. 模型创建的高级视图
- en: But, as you can see, the data is lumped in wholesale. There’s no granularity
    involved and no effort to understand the sequence in which that data occurs. This
    means the words *blue* and *sky* have no different meaning in sentences such as,
    “Today I am blue, because the sky is gray,” and “Today I am happy, and there’s
    a beautiful blue sky.” To us, the difference in the use of these words is obvious,
    but to a model, with the architecture shown here, there really is no difference.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，正如你所看到的，数据是整体处理的。没有涉及粒度，也没有努力理解数据发生的顺序。这意味着在句子“今天我很沮丧，因为天空是灰的”和“今天我很高兴，有一个美丽的蓝天”中，“blue”和“sky”这两个词没有不同的含义。对我们来说，这些词的使用差异是明显的，但对一个模型来说，具有这里所示架构的模型，实际上并没有差异。
- en: So, how do we fix this? Let’s first explore the nature of recurrence, and from
    there, you’ll be able to see how a basic RNN can work.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们该如何解决这个问题呢？首先，让我们探索一下循环的本质，然后你将能够看到基本循环神经网络（RNN）是如何工作的。
- en: Consider the famous Fibonacci sequence of numbers. In case you aren’t familiar
    with it, I’ve put some of them into [Figure 7-2](#ch07_figure_2_1748549654878009).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑著名的斐波那契数列。如果你不熟悉它，我已经在[图7-2](#ch07_figure_2_1748549654878009)中放入了一些数字。
- en: '![](assets/aiml_0702.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0702.png)'
- en: Figure 7-2\. The first few numbers in the Fibonacci sequence
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2. 斐波那契数列的前几个数
- en: The idea behind this sequence is that every number is the sum of the two numbers
    preceding it. So if we start with 1 and 2, the next number is 1 + 2, which is
    3\. The one after that is 2 + 3, which is 5, and then there’s 3 + 5, which is
    8, and so on.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个序列背后的想法是每个数都是它前面两个数的和。所以如果我们从1和2开始，下一个数是1 + 2，即3。接下来的一个数是2 + 3，即5，然后是3 + 5，即8，以此类推。
- en: We can place this in a computational graph to get [Figure 7-3](#ch07_figure_3_1748549654878036).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个放在计算图中，得到[图7-3](#ch07_figure_3_1748549654878036)。
- en: '![](assets/aiml_0703.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0703.png)'
- en: Figure 7-3\. A computational graph representation of the Fibonacci sequence
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-3. 斐波那契数列的计算图表示
- en: Here, you can see that we feed 1 and 2 into the function and get 3 as the output.
    We then carry the second parameter (2) over to the next step and feed it into
    the function along with the output from the previous step (3). The output of this
    is 5, and it gets fed into the function with the second parameter from the previous
    step (3) to produce an output of 8\. This process continues indefinitely, with
    every operation depending on those before it. The 1 at the top left sort of “survives”
    through the process—it’s an element of the 3 that gets fed into the second operation,
    it’s an element of the 5 that gets fed into the third operation, and so on. Thus,
    some of the essence of the 1 is preserved throughout the sequence, though its
    impact on the overall value is diminished.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到我们将1和2输入到函数中，得到3作为输出。然后我们将第二个参数（2）传递到下一步，并连同前一步的输出（3）一起输入到函数中。这个输出的结果是5，然后它与前一步的第二个参数（3）一起输入到函数中，产生一个输出8。这个过程无限进行下去，每个操作都依赖于之前的操作。左上角的1在过程中“存活”下来——它是3的一个元素，被输入到第二个操作中，它是5的一个元素，被输入到第三个操作中，以此类推。因此，1的一些本质在整个序列中得以保留，尽管它对整体值的影响减弱了。
- en: This is analogous to how a recurrent neuron is architected. You can see the
    typical representation of a recurrent neuron in [Figure 7-4](#ch07_figure_4_1748549654878060).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这与循环神经元的架构类似。你可以在[图7-4](#ch07_figure_4_1748549654878060)中看到循环神经元的典型表示。
- en: '![](assets/aiml_0704.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0704.png)'
- en: Figure 7-4\. A recurrent neuron
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4. 循环神经元
- en: A value *x* is fed into the function *F* at a time step, so it’s typically labeled
    *x*[*t*]. This produces an output *y* at that time step, which is typically labeled
    *y*[*t*]. It also produces a value that is fed forward to the next step, which
    is indicated by the arrow from *F* to itself.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值*x*在时间步长被输入到函数*F*中，所以它通常被标记为*x*[*t*]。这在该时间步长产生一个输出*y*，通常被标记为*y*[*t*]。它还产生一个传递到下一步的值，这由从*F*到自身的箭头表示。
- en: This is made a little clearer if you look at how recurrent neurons work beside
    one another across time steps, which you can see in [Figure 7-5](#ch07_figure_5_1748549654878082).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看看循环神经元在时间步长中是如何相互工作的，这会使得这个过程更加清晰，你可以在[图7-5](#ch07_figure_5_1748549654878082)中看到这一点。
- en: Here, *x*[0] is operated on to get *y*[0] and a value that’s passed forward.
    The next step gets that value and *x*[1] and produces *y*[1] and a value that’s
    passed forward. The next one gets that value and *x*[2] and produces *y*[2] and
    a passed-forward value, and so on down the line. This is similar to what we saw
    with the Fibonacci sequence, and I always find it to be a handy mnemonic when
    trying to remember how an RNN works.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x*[0]被操作以得到*y*[0]和一个传递的值。下一步获取那个值和*x*[1]，产生*y*[1]和一个传递的值。接下来的一步获取那个值和*x*[2]，产生*y*[2]和一个传递的值，以此类推。这与我们看到的斐波那契数列类似，而且我在尝试记住RNN的工作方式时，总是觉得这是一个有用的记忆法。
- en: '![](assets/aiml_0705.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0705.png)'
- en: Figure 7-5\. Recurrent neurons in time steps
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-5. 时间步长中的循环神经元
- en: Extending Recurrence for Language
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展语言中的循环
- en: 'In the previous section, you saw how an RNN operating over several time steps
    can help maintain context across a sequence. Indeed, we’ll use RNNs for sequence
    modeling later in this book—but there’s a nuance when it comes to language that
    you can miss when using a simple RNN like those shown in [Figure 7-4](#ch07_figure_4_1748549654878060)
    and [Figure 7-5](#ch07_figure_5_1748549654878082). As in the Fibonacci sequence
    example mentioned earlier, the amount of context that’s carried over will diminish
    over time. The effect of the output of the neuron at step 1 is huge at step 2,
    smaller at step 3, smaller still at step 4, and so on. So, if we have a sentence
    like “Today has a beautiful blue <something>,” the word *blue* will have a strong
    impact on what the next word could be: we can guess that it’s likely to be *sky*.
    But what about context that comes from earlier in a sentence? For example, consider
    the sentence “I lived in Ireland, so in high school, I had to learn how to speak
    and write <something>.”'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，您看到了一个RNN在多个时间步长上操作如何帮助在序列中保持上下文。确实，我们将在本书的后面部分使用RNN进行序列建模——但是当使用像[图7-4](#ch07_figure_4_1748549654878060)和[图7-5](#ch07_figure_5_1748549654878082)中展示的简单RNN时，您可能会错过语言方面的细微差别。就像前面提到的斐波那契数列示例一样，携带的上下文量会随着时间的推移而减少。第1步神经元输出的影响在第2步时很大，在第3步时较小，在第4步时更小，以此类推。因此，如果我们有一个句子像“今天有一个美丽的蓝色<某物>”，那么单词*蓝色*将对下一个单词可能是什么有强烈的影响：我们可以猜测它很可能是*天空*。但是，句子开头部分的上下文怎么办呢？例如，考虑这个句子：“我在爱尔兰生活过，所以在高中时，我不得不学习如何说和写<某物>。”
- en: That <something> is *Gaelic*, but the word that really gives us that context
    is *Ireland*, which is much earlier in the sentence. Thus, for us to be able to
    recognize what <something> should be, we need a way to preserve context across
    a longer distance. The short-term memory of an RNN needs to get longer, and in
    recognition of this, an enhancement to the architecture called *long short-term
    memory* (LSTM) was invented.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 那个<某物>是*盖尔语*，但真正给我们这个上下文的单词是*爱尔兰*，它在句子中要早得多。因此，为了能够识别<某物>应该是什么，我们需要一种方法来在更长的距离上保持上下文。RNN的短期记忆需要更长，为此，人们发明了一种称为*长短期记忆*（LSTM）的架构增强。
- en: While I won’t go into detail on the underlying architecture of how LSTMs work,
    the high-level diagram shown in [Figure 7-6](#ch07_figure_6_1748549654878103)
    gets the main point across. To learn more about the internal operations of LSTM,
    check out Christopher Olah’s excellent [blog post on the subject](https://oreil.ly/6KcFA).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我不会详细介绍LSTM的工作的底层架构，但[图7-6](#ch07_figure_6_1748549654878103)中展示的高级图解清楚地说明了主要观点。要了解更多关于LSTM内部操作的信息，请查看Christopher
    Olah关于这个主题的优秀[博客文章](https://oreil.ly/6KcFA)。
- en: The LSTM architecture enhances the basic RNN by adding a “cell state” that enables
    context to be maintained not just from step to step but across the entire sequence
    of steps. Remembering that these are neurons that learn in the way neurons do,
    you can see that this enhancement ensures that the context that is important will
    be learned over time.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM架构通过添加一个“细胞状态”来增强基本的RNN，这使得上下文不仅可以从一步到下一步保持，还可以在整个步骤序列中保持。记住这些是像神经元一样学习的神经元，您可以看到这种增强确保了随着时间的推移，重要的上下文将被学习。
- en: '![](assets/aiml_0706.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0706.png)'
- en: Figure 7-6\. High-level view of LSTM architecture
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-6\. LSTM架构的高级视图
- en: An important part of an LSTM is that it can be *bidirectional*—the time steps
    can be iterated both forward and backward so that context can be learned in both
    directions. Often, context for a word can come *after* it in the sentence and
    not just before.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM的一个重要部分是它可以*双向*——时间步长可以向前和向后迭代，以便在两个方向上学习上下文。通常，一个单词的上下文可以来自句子中的*之后*，而不仅仅是之前。
- en: See [Figure 7-7](#ch07_figure_7_1748549654878124) for a high-level view of this.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 见[图7-7](#ch07_figure_7_1748549654878124)以了解其高级视图。
- en: '![](assets/aiml_0707.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0707.png)'
- en: Figure 7-7\. High-level view of LSTM bidirectional architecture
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-7\. LSTM双向架构的高级视图
- en: This is how evaluation in the direction from 0 to `number_of_steps` is done,
    and it’s also how evaluation from `number_of_steps` to 0 is done. At each step,
    the *y* result is an aggregation of the “forward” pass and the “backward” pass.
    You can see this in [Figure 7-8](#ch07_figure_8_1748549654878143).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是从0到`number_of_steps`方向的评估是如何进行的，这也是从`number_of_steps`到0的评估是如何进行的。在每一步，*y*结果都是“正向”传递和“反向”传递的聚合。您可以在[图7-8](#ch07_figure_8_1748549654878143)中看到这一点。
- en: '![](assets/aiml_0708.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0708.png)'
- en: Figure 7-8\. Bidirectional LSTM
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-8\. 双向LSTM
- en: It’s easy to confuse the bidirectional nature of the LSTM with the terms *forward*
    and *backward* when it comes to the training of the network, but they’re very
    different. When I refer to the forward and backward pass, I’m referring to the
    setting of the parameters of the neurons and their updating from the learning
    process, respectively. Don’t confuse this with the values that the LSTM is paying
    attention to as being the next or previous tokens in the sequence.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到网络的训练时，很容易将LSTM的双向性（bidirectional nature）与术语*forward*和*backward*混淆，但它们是非常不同的。当我提到正向和反向传递时，我指的是设置神经元参数及其从学习过程中更新的过程。不要将这一点与LSTM关注的下一个或前一个序列标记的值混淆。
- en: 'Also, consider each neuron at each time step to be F0, F1, F2, etc. The direction
    of the time step is shown, so the calculation at F1 in the forward direction is
    F1(->), and in the reverse direction, it’s (<-)F1\. The values of these are aggregated
    to give the *y* value for that time step. Additionally, the cell state is bidirectional,
    and this can be really useful for managing context in sentences. Again, considering
    the sentence “I lived in Ireland, so in high school, I had to learn how to speak
    and write <something>,” you can see how the <something> was qualified to be *Gaelic*
    by the context word *Ireland*. But what if it were the other way around: “I lived
    in <this country>, so in high school, I had to learn how to speak and write Gaelic”?
    You can see that by going *backward* through the sentence, we can learn about
    what <this country> should be. Thus, using bidirectional LSTMs can be very powerful
    for understanding sentiment in text. (And as you’ll see in [Chapter 8](ch08.html#ch08_using_ml_to_create_text_1748549671852453),
    they’re really powerful for generating text, too!)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，考虑每个时间步的每个神经元为F0，F1，F2等。时间步的方向是显示的，所以正向方向F1的计算是F1(->)，反向方向是(<-)F1。这些值的聚合给出了该时间步的*y*值。此外，细胞状态是双向的，这可以非常有助于管理句子中的上下文。再次考虑句子“我在爱尔兰生活过，所以在高中时，我不得不学习如何说和写<something>”，你可以看到<something>是如何通过上下文词*Ireland*被限定为*Gaelic*的。但如果情况相反呢：“我在<这个国家>生活过，所以在高中时，我不得不学习如何说和写Gaelic”？你可以看到，通过句子中的*反向*传递，我们可以了解<这个国家>应该是什么。因此，使用双向LSTM对于理解文本中的情感非常有用。（而且正如你将在[第8章](ch08.html#ch08_using_ml_to_create_text_1748549671852453)中看到的，它们对于生成文本也非常强大！）
- en: Of course, there’s a lot going on with LSTMs, in particular bidirectional ones,
    so expect training to be slow. Here’s where it’s worth investing in a GPU or at
    the very least using a hosted one in Google Colab if you can.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，LSTM有很多内容，特别是双向LSTM，所以请预期训练会慢。这就是值得投资GPU或至少在Google Colab中使用托管GPU的时候了。
- en: Creating a Text Classifier with RNNs
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RNN创建文本分类器
- en: In [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888),
    you experimented with creating a classifier for the Sarcasm dataset by using embeddings.
    In that case, you turn words into vectors before aggregating them and then feeding
    them into dense layers for classification. But when you’re using an RNN layer
    such as an LSTM, you don’t do the aggregation, and you can feed the output of
    the embedding layer directly into the recurrent layer. When it comes to the dimensionality
    of the recurrent layer, a rule of thumb you’ll often see is that it’s the same
    size as the embedding dimension. This isn’t necessary, but it can be a good starting
    point. Also note that while in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)
    I mentioned that the embedding dimension is often the fourth root of the size
    of the vocabulary, when using RNNs, you’ll often see that that rule may be ignored
    because it would make the size of the recurrent layer too small.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中，你通过使用嵌入（embeddings）来创建一个针对讽刺数据集的分类器进行了实验。在这种情况下，你将单词转换为向量，然后在聚合它们之前将它们输入到密集层进行分类。但是当你使用像LSTM这样的RNN层时，你不需要进行聚合，可以直接将嵌入层的输出输入到循环层。至于循环层的维度，一个常见的经验法则是它的大小与嵌入维度相同。这并不是必需的，但它可以是一个好的起点。此外，请注意，虽然我在[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中提到，嵌入维度通常是词汇表大小的四次方根，但在使用RNN时，你经常会看到这个规则可能被忽略，因为它会使循环层的大小变得太小。
- en: For this example, I have used the number of neurons in the hidden layer as a
    starting point, and you can experiment from there.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我使用了隐藏层中神经元的数量作为起点，你可以从这里开始实验。
- en: 'So, for example, you could update the simple model architecture for the sarcasm
    classifier you developed in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)
    to the following to use a bidirectional LSTM:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以将你在[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中开发的讽刺分类器的简单模型架构更新如下，以使用双向LSTM：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can then set the loss function and classifier to this. (Note that the LR
    is 0.001, or 1e–3.):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将损失函数和分类器设置为这个。注意LR是0.001，或者1e–3：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When you print out the model architecture summary, you’ll see something like
    the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当你打印出模型架构摘要时，你会看到如下内容：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that the vocab size is 2,000 and the embedding dimension is 7\. This gives
    14,000 parameters in the embedding layer, and the bidirectional layer will have
    48 neurons (24 out, 24 back) with a sequence length of 85 characters
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，词汇量大小为2,000，嵌入维度为7。这给嵌入层提供了14,000个参数，双向层将有48个神经元（24个输出，24个回传）和85个字符的序列长度。
- en: '[Figure 7-9](#ch07_figure_9_1748549654878164) shows the results of training
    with this over three hundred epochs.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-9](#ch07_figure_9_1748549654878164)显示了在超过300个epoch的训练结果。'
- en: This gives us a network with only 21,537 parameters. As you can see, the accuracy
    of the network on training data rapidly climbs toward 85%, but the validation
    data plateaus at around 75%. This is similar to the figures we got earlier, but
    inspecting the loss chart in [Figure 7-10](#ch07_figure_10_1748549654878185) shows
    that while the loss for the test set diverged after 15 epochs, the validation
    loss turned to increase, indicating we have overfitting.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个只有21,537个参数的网络。如图所示，网络在训练数据上的准确率迅速攀升至85%，但验证数据在约75%处停滞。这与我们之前得到的结果相似，但检查[图7-10](#ch07_figure_10_1748549654878185)中的损失图表显示，测试集的损失在15个epoch后发散，而验证损失开始增加，表明我们出现了过拟合。
- en: '![](assets/aiml_0709.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0709.png)'
- en: Figure 7-9\. Accuracy for LSTM over 30 epochs
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-9\. LSTM在30个epoch上的准确率
- en: '![](assets/aiml_0710.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0710.png)'
- en: Figure 7-10\. Loss with LSTM over 30 epochs
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-10\. LSTM在30个epoch上的损失
- en: However, this was just using a single LSTM layer with a hidden layer of 24 neurons.
    In the next section, you’ll see how to use stacked LSTMs and explore the impact
    on the accuracy of classifying this dataset.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这仅仅使用了一个具有24个神经元的隐藏层的单个LSTM层。在下一节中，你将看到如何使用堆叠LSTM，并探讨其对分类此数据集准确率的影响。
- en: Stacking LSTMs
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠LSTM
- en: In the previous section, you saw how to use an LSTM layer after the embedding
    layer to help classify the contents of the sarcasm dataset. But LSTMs can be stacked
    on top of one another, and this approach is used in many state-of-the-art NLP
    models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你看到了如何在嵌入层之后使用LSTM层来帮助分类讽刺数据集的内容。但是，LSTM可以堆叠在一起，这种方法被许多最先进的NLP模型所采用。
- en: 'Stacking LSTMs with PyTorch is pretty straightforward. You add them as extra
    layers just like you would with any other layer, but you will need to be careful
    in specifying the dimensions. So, for example, if the first LSTM has *x* number
    of hidden layers, then the next LSTM will have *x* number of inputs. If the LST
    is bidirectional, then the next will need to double the size. Here’s an example:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyTorch堆叠LSTM相当直接。你就像添加任何其他层一样添加它们，但你需要小心指定维度。例如，如果第一个LSTM有*x*个隐藏层，那么下一个LSTM将有*x*个输入。如果LSTM是双向的，那么下一个需要加倍大小。以下是一个例子：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the `input_size` for the first layer is the embedding dimension because
    it’s preceded by the embedding layer. The second LSTM then has its input size
    as (`hidden_dim * 2`) because the output from the first LSTM is that size, given
    that it’s bidirectional.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第一层的`input_size`是嵌入维度，因为它在嵌入层之前。第二个LSTM的输入大小为(`hidden_dim * 2`)，因为第一个LSTM的输出大小就是那样，考虑到它是双向的。
- en: 'The model architecture will look like this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构将看起来如下：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Adding the extra layer will give us roughly 14,000 extra parameters that need
    to be learned, which is an increase of about 75%. So, it might slow the network
    down, but the cost is relatively low if there’s a reasonable benefit.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 添加额外的层将给我们大约14,000个额外的参数需要学习，这增加了大约75%。因此，它可能会减慢网络的运行速度，但如果收益合理，成本相对较低。
- en: After training for three hundred epochs, the result looks like [Figure 7-11](#ch07_figure_11_1748549654878205).
    While the accuracy on the validation set is flat, examining the loss (shown in
    [Figure 7-12](#ch07_figure_12_1748549654878225)) tells a different story. As you
    can see in [Figure 7-12](#ch07_figure_12_1748549654878225), while the accuracy
    for both training and validation looked good, the validation loss quickly took
    off upward, which is a clear sign of overfitting.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 经过300个epoch的训练后，结果看起来像[图7-11](#ch07_figure_11_1748549654878205)。虽然验证集上的准确率保持平稳，但检查损失（如图[7-12](#ch07_figure_12_1748549654878225)所示）却讲述了一个不同的故事。如图[7-12](#ch07_figure_12_1748549654878225)所示，虽然训练和验证的准确率看起来不错，但验证损失迅速上升，这是过度拟合的明显迹象。
- en: '![](assets/aiml_0711.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0711.png)'
- en: Figure 7-11\. Accuracy for stacked LSTM architecture
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-11\. 堆叠LSTM架构的准确率
- en: This overfitting (which is indicated by the training accuracy climbing toward
    100% as the loss falls smoothly while the validation accuracy is relatively steady
    and the loss increases drastically) is a result of the model getting overspecialized
    for the training set. As with the examples in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888),
    this shows that it’s easy to be lulled into a false sense of security if you just
    look at the accuracy metrics without examining the loss.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这种过度拟合（表现为训练准确率随着损失的平滑下降而接近100%，而验证准确率相对稳定且损失急剧增加）是模型对训练集过度专业化的结果。与[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中的例子一样，这表明如果你只看准确率指标而不检查损失，很容易陷入虚假的安全感。
- en: '![](assets/aiml_0712.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0712.png)'
- en: Figure 7-12\. Loss for stacked LSTM architecture
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-12\. 堆叠LSTM架构的损失
- en: Optimizing stacked LSTMs
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化堆叠LSTMs
- en: In [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888),
    you saw that a very effective method of reducing overfitting was to reduce the
    LR. It’s worth exploring here whether that will have a positive effect on an RNN,
    too.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中，你看到了降低过度拟合的一个非常有效的方法是降低LR。在这里探索它是否对RNN也有积极影响是值得的。
- en: 'For example, the following code reduces the LR by 50%, from 0.0001 to 0.00005:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下代码将LR降低了50%，从0.0001降低到0.00005：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Figure 7-13](#ch07_figure_13_1748549654878247) demonstrates the impact of
    this on training. As you can see, there’s a small difference in the validation
    accuracy, indicating that we’re overfitting a bit less.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-13](#ch07_figure_13_1748549654878247)展示了这对其训练的影响。如图所示，验证准确率有细微差异，表明我们略微减少了过度拟合。'
- en: '![](assets/aiml_0713.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0713.png)'
- en: Figure 7-13\. Impact of reduced LR on accuracy with stacked LSTMs
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-13\. 减少LR对堆叠LSTMs准确率的影响
- en: While an initial look at [Figure 7-14](#ch07_figure_14_1748549654878268) similarly
    suggests a decent impact on loss due to the reduced LR, with the curve not moving
    up so sharply, it’s worth looking a little closer. We see that the loss on the
    training set is actually a little higher (~0.35 versus ~0.27) than the previous
    example, while the loss on the validation set is lower (~0.5 versus 0.6).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对[图7-14](#ch07_figure_14_1748549654878268)的初步观察同样表明减少LR对损失有相当的影响，曲线没有如此急剧上升，但值得仔细观察。我们看到训练集上的损失实际上略高于前一个例子（~0.35比~0.27），而验证集上的损失则较低（~0.5比0.6）。
- en: Adjusting the LR hyperparameter certainly seems worth investigation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 调整LR超参数显然值得研究。
- en: Indeed, further experimentation with the LR showed a marked improvement in getting
    training and validation curves to converge, indicating that while the network
    was less accurate after training, we could tell that it was generalizing better.
    Figures [7-15](#ch07_figure_15_1748549654878288) and [7-16](#ch07_figure_16_1748549654878310)
    show the impact of using a lower LR (.0003 rather than .0005).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，进一步实验表明，降低LR（从0.0005降低到0.0003）显著提高了训练和验证曲线的收敛，这表明虽然网络在训练后准确性有所降低，但我们能看出它泛化得更好。图[7-15](#ch07_figure_15_1748549654878288)和[7-16](#ch07_figure_16_1748549654878310)展示了使用更低LR的影响。
- en: '![](assets/aiml_0714.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0714.png)'
- en: Figure 7-14\. Impact of reduced LR on loss with stacked LSTMs
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-14\. 减少学习率（LR）对堆叠LSTMs损失的冲击
- en: '![](assets/aiml_0715.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0715.png)'
- en: Figure 7-15\. Accuracy with further-reduced LR with stacked LSTM
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-15\. 堆叠LSTM进一步降低LR后的准确率
- en: '![](assets/aiml_0716.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0716.png)'
- en: Figure 7-16\. Loss with further-reduced LR and stacked LSTM
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-16\. 进一步降低LR和堆叠LSTM的损失
- en: Indeed, reducing the LR even further, to .00001, gave potentially even better
    results, as shown in Figures [7-17](#ch07_figure_17_1748549654878331) and [7-18](#ch07_figure_18_1748549654878358).
    As with the previous diagrams, while the overall accuracy isn’t as good and the
    loss is higher, that’s an indication that we’re getting closer to a “realistic”
    result for this network architecture and not being led into having a false sense
    of security by overfitting on the training data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，将LR进一步降低到0.00001，可能给出了更好的结果，如图[7-17](#ch07_figure_17_1748549654878331)和[7-18](#ch07_figure_18_1748549654878358)所示。与之前的图表一样，虽然整体准确率不是很好，损失也更高，但这表明我们正在接近这个网络架构的“真实”结果，而不是通过在训练数据上过拟合而得到错误的安心感。
- en: In addition to changing the LR parameter, you should also consider using dropout
    in the LSTM layers. It works exactly the same as for dense layers, as discussed
    in [Chapter 3](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912),
    where random neurons are dropped to prevent a proximity bias from impacting the
    learning. That being said, you should be careful about setting it *too* low, because
    when you start tweaking with different architectures, you might freeze the ability
    of the network to learn.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了改变LR参数外，你还应该考虑在LSTM层中使用dropout。它的工作方式与密集层完全相同，如第3章中讨论的（ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912），其中随机神经元被丢弃以防止邻近偏差影响学习。尽管如此，你应该小心不要将其设置得太低，因为当你开始调整不同的架构时，你可能会冻结网络学习的能力。
- en: '![](assets/aiml_0717.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0717.png)'
- en: Figure 7-17\. Accuracy with lower LR
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-17\. 使用较低学习率的准确率
- en: '**![](assets/aiml_0718.png)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](assets/aiml_0718.png)'
- en: Figure 7-18\. Loss with lower LR**  **### Using dropout
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-18\. 使用较低学习率的损失**  **### 使用dropout
- en: In addition to changing the LR parameter, you should also consider using dropout
    in the LSTM layers. It works exactly the same as for dense layers, as discussed
    in [Chapter 3](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912),
    where random neurons are dropped to prevent a proximity bias from impacting the
    learning.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 除了改变LR参数外，你还应该考虑在LSTM层中使用dropout。它的工作方式与密集层完全相同，如第3章中讨论的（ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912），其中随机神经元被丢弃以防止邻近偏差影响学习。
- en: 'You can implement dropout by using `nn.Dropout`. Here’s an example:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用`nn.Dropout`来实现dropout。以下是一个示例：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, in your forward pass, you can apply the dropouts at the appropriate levels,
    like this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在你的前向传递中，你可以在适当的层级应用dropout，如下所示：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When I ran this with the lowest LR I had tested prior to dropout, the network
    didn’t learn. So, I moved the LR back up to 0.0003 and ran for 300 epochs using
    this dropout (note that the dropout rate is 0.2, so about 20% of neurons are dropped
    at random). The accuracy results can be seen in [Figure 7-19](#ch07_figure_19_1748549654878378).
    The curves for training and validation are still close to each other, but they’re
    hitting greater than 75% accuracy, whereas without dropout, it was hard to get
    above 70%.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当我用在dropout之前测试过的最低学习率运行时，网络没有学习。所以我将LR调整回0.0003，并使用这个dropout运行了300个epoch（注意dropout率是0.2，所以大约有20%的神经元被随机丢弃）。准确率结果可以在[图7-19](#ch07_figure_19_1748549654878378)中看到。训练和验证曲线仍然很接近，但它们的准确率已经超过75%，而没有dropout时，很难超过70%。
- en: '![](assets/aiml_0719.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0719.png)'
- en: Figure 7-19\. Accuracy of stacked LSTMs using dropout
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-19\. 使用dropout的堆叠LSTMs的准确率
- en: As you can see, using dropout can have a positive impact on the accuracy of
    the network, which is good! There’s always a worry that losing neurons will make
    your model perform worse, but as we can see here, that’s not the case. But do
    be careful when using dropout because it can lead to underfitting or overfitting
    if not used appropriately.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，使用dropout可以对网络的准确率产生积极影响，这是好事！总是担心丢失神经元会使你的模型表现更差，但正如我们在这里所看到的，情况并非如此。但使用dropout时一定要小心，因为它如果不适当使用可能会导致欠拟合或过拟合。
- en: There’s also a positive impact on loss, as you can see in [Figure 7-20](#ch07_figure_20_1748549654878397).
    While the curves are clearly diverging, they are closer than they were previously,
    and the validation set is flattening out at a loss of about 0.45, which also demonstrates
    an improvement! As this example shows, dropout is another handy technique that
    you can use to improve the performance of LSTM-based RNNs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图7-20](#ch07_figure_20_1748549654878397)所示，这也有助于降低损失。虽然曲线明显发散，但它们比之前更接近，验证集在约0.45的损失处趋于平坦，这也证明了改进！正如这个例子所示，dropout是另一种可以用来提高基于LSTM的RNN性能的实用技术。
- en: 'It’s worth exploring these techniques for avoiding overfitting in your data,
    and it’s also worth exploring the techniques for preprocessing your data that
    we covered in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888).
    But there’s one thing that we haven’t yet tried: a form of transfer learning in
    which you can use pre-learned embeddings for words instead of trying to learn
    your own. We’ll explore that next.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 值得探索这些避免数据过拟合的技术，同时也值得探索我们在[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中提到的数据预处理技术。但还有一件事我们尚未尝试：一种迁移学习的形式，其中你可以使用预学习的词嵌入而不是尝试学习自己的。我们将在下一节中探讨这一点。
- en: '![](assets/aiml_0720.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0720.png)'
- en: Figure 7-20\. Loss curves for dropout-enabled LSTMs**  **# Using Pretrained
    Embeddings with RNNs
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-20.启用dropout的LSTMs的损失曲线**  **# 使用预训练嵌入的RNN
- en: In all the previous examples, you gathered the full set of words to be used
    in the training set and then trained embeddings with them. You initially aggregated
    them before feeding them into a dense network, and in this chapter, you explored
    how to improve the results using an RNN. While doing this, you were restricted
    to the words in your dataset and how their embeddings could be learned by using
    the labels from that dataset.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有之前的例子中，你收集了用于训练集的完整单词集，并使用它们训练嵌入。你最初将它们聚合起来，然后输入到密集网络中，在本章中，你探讨了如何使用RNN来改进结果。在这个过程中，你受到数据集中单词的限制以及如何使用该数据集的标签来学习它们的嵌入。
- en: Now, think back to [Chapter 4](ch04.html#ch04_using_data_with_pytorch_1748548966496246),
    where we discussed transfer learning. What if instead of learning the embeddings
    for yourself, you could use pre-learned embeddings, where researchers have already
    done the hard work of turning words into vectors and those vectors are proven?
    One example of this, as we saw in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888),
    is the [GloVe (Global Vectors for Word Representation) model](https://oreil.ly/4ENdQ)
    developed by Jeffrey Pennington, Richard Socher, and Christopher Manning at Stanford.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回想一下[第4章](ch04.html#ch04_using_data_with_pytorch_1748548966496246)，我们讨论了迁移学习。如果你不自己学习嵌入，而是使用预学习的嵌入，其中研究人员已经完成了将单词转换为向量的艰苦工作，并且这些向量已经被证明是有效的，那会怎么样？正如我们在[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中看到的，一个例子是斯坦福大学的Jeffrey
    Pennington、Richard Socher和Christopher Manning开发的[GloVe (全局词向量)模型](https://oreil.ly/4ENdQ)。
- en: 'In this case, the researchers have shared their pretrained word vectors for
    a variety of datasets:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，研究人员已经分享了他们在各种数据集上的预训练词向量：
- en: A 6-billion-token, 400,000-word vocabulary set in 50, 100, 200, and 300 dimensions
    with words taken from Wikipedia and Gigaword
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自维基百科和Gigaword的词汇表，包含60亿个标记、40万个词汇，维度为50、100、200和300
- en: A 42-billion-token, 1.9-million-word vocabulary in 300 dimensions from a common
    crawl
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自公共爬取的42亿个标记、1900万个词汇，在300维度的词汇表
- en: An 840-billion-token, 2.2-million-word vocabulary in 300 dimensions from a common
    crawl
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自公共爬取的840亿个标记、2200万个词汇的词汇表，在300维度
- en: A 27-billion-token, 1.2-million-word vocabulary in 25, 50, 100, and 200 dimensions
    from a Twitter crawl of 2 billion tweets
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自200亿条推文的Twitter爬取的27亿个标记、120万个词汇的词汇表，维度为25、50、100和200
- en: 'Given that the vectors are already pretrained, it’s simple for you to reuse
    them in your PyTorch code, instead of learning them from scratch. First, you’ll
    have to download the GloVe data. I’ve opted to use the 6-billion-word version,
    in 50 dimensions, using this code to download and unzip it:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些向量已经预训练，你可以在PyTorch代码中简单地重用它们，而不是从头开始学习。首先，你将需要下载GloVe数据。我选择使用50维度的60亿词汇版本，使用以下代码下载并解压：
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Each entry in the file is a word, followed by the dimensional coefficients
    that were learned for it. The easiest way to use this is to create a dictionary
    where the key is the word and the values are the embeddings. You can set up this
    dictionary like this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 文件中的每个条目都是一个单词，后面跟着为它学习到的维度系数。使用它的最简单方法是创建一个字典，其中键是单词，值是嵌入。你可以这样设置这个字典：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'At this point, you’ll be able to look up the set of coefficients for any word
    simply by using it as the key. So, for example, to see the embeddings for the
    word *frog*, you could use this:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 到这个时候，你将能够通过简单地使用它作为键来查找任何单词的系数集。例如，要查看单词*青蛙*的嵌入，你可以使用这个：
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With these pretrained embeddings in hand, you can now load them into the embeddings
    layer in your neural architecture and use them as pretrained embeddings instead
    of learning them from scratch. See the following model architecture definition.
    If the `pretrained_embeddings` value is not null, then the weights for the embedding
    layer will be loaded from that. If `freeze_embeddings` is `True`, then they’ll
    be frozen; otherwise, they’ll be used as the starting point for learning (i.e.,
    you’ll fine-tune the embeddings based on your corpus):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些预训练嵌入后，你现在可以将它们加载到你的神经网络架构中的嵌入层，并使用它们作为预训练嵌入而不是从头开始学习。请参见以下模型架构定义。如果`pretrained_embeddings`值不为空，则嵌入层的权重将从该值加载。如果`freeze_embeddings`为`True`，则它们将被冻结；否则，它们将用作学习的起点（即，你将根据你的语料库微调嵌入）：
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This model shows a total of 406.817 parameters of which only 6,817 are trainable,
    so training will be fast!
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型显示了总共406.817个参数，其中只有6,817个是可训练的，因此训练将会很快！
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can now train as before, and you can see how this architecture, with the
    pretrained embeddings and stacked LSTMs, reduces overfitting really nicely! [Figure 7-21](#ch07_figure_21_1748549654878419)
    shows the Training versus Validation accuracy on the sarcasm dataset using LSTMs
    and pretrained GloVe embeddings, while [Figure 7-22](#ch07_figure_22_1748549654878440)
    shows the loss on training versus validation, where the closeness of the curves
    demonstrates that we’re not overfitting.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以像以前一样进行训练，并且你可以看到这个架构，即使用预训练嵌入和堆叠的LSTMs，如何真正地减少过拟合！[图7-21](#ch07_figure_21_1748549654878419)显示了使用LSTMs和预训练GloVe嵌入在讽刺数据集上的训练与验证准确率，而[图7-22](#ch07_figure_22_1748549654878440)显示了训练与验证的损失，曲线的接近程度表明我们没有过拟合。
- en: '![](assets/aiml_0721.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0721.png)'
- en: Figure 7-21\. Training versus validation accuracy on the sarcasm dataset with
    LSTMs and GloVe
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-21\. 使用LSTMs和GloVe在讽刺数据集上的训练与验证准确率
- en: '![](assets/aiml_0722.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0722.png)'
- en: Figure 7-22\. Training and validation loss on the sarcasm dataset with LSTMs
    and GloVe
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-22\. 使用LSTMs和GloVe在讽刺数据集上的训练和验证损失
- en: 'For further analysis, you’ll want to consider your vocab size. One of the optimizations
    you did in the previous chapter to avoid overfitting was intended to prevent the
    embeddings becoming overburdened with learning low-frequency words: you avoided
    overfitting by using a smaller vocabulary of frequently used words. In this case,
    as the word embeddings have already been learned for you with GloVe, you could
    expand the vocabulary—but by how much?'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步分析，你需要考虑你的词汇量大小。在前一章中，你为了避免过拟合所做的优化之一是防止嵌入学习低频词而变得负担过重：你通过使用常用词的较小词汇量来避免过拟合。在这种情况下，由于GloVe已经为你学习过单词嵌入，你可以扩展词汇量——但扩展多少呢？
- en: The first thing to explore is how many of the words in your corpus are actually
    in the GloVe set. It has 1.2 million words, but there’s no guarantee it has *all*
    of your words.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要探索的是，你语料库中的单词有多少实际上在GloVe集中。它有1.2百万个单词，但无法保证它有*所有*你的单词。
- en: 'When building the `word_index`, you can call `build_vocab_glove` with a *really*
    large number and it will ignore any words over the total amount. So, for example,
    say you call this:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建`word_index`时，你可以使用一个非常大的数字调用`build_vocab_glove`，它会忽略超过总数量的任何单词。例如，你可以这样做：
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With the sarcasm dataset, you’ll get a vocab_size of 22,457 returned. If you
    like, you can then explore the GloVe embeddings to see just how many of these
    words are present in GloVE. Start by creating a dictionary for the embeddings
    and reading the GloVE file into it:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用讽刺数据集，你会得到一个22,457的vocab_size返回值。如果你愿意，你可以探索GloVe嵌入，看看其中有多少单词存在于GloVE中。首先创建一个用于嵌入的字典，并将GloVE文件读入其中：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, you can compare this with your `word_index` that you created from the
    entire corpus with the preceding line:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以将其与您使用上一行创建的整个语料库的`word_index`进行比较：
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the case of sarcasm, 21,291 of the words were found in GloVE, which is the
    vast majority, so the principles you used in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)
    to choose how many you should train on (i.e., picking those with sufficient frequency
    to have a signal) will still apply!
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在讽刺的情况下，21,291个单词在GloVE中被找到，这是绝大多数，所以您在[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中使用的原则来选择您应该训练多少（即，选择那些有足够频率以产生信号的单词）仍然适用！
- en: 'Using this method, I chose to use a vocabulary size of 8,000 (instead of the
    2,000 that was previously used to avoid overfitting) to get the results you saw
    just now. I then tested it with headlines from *The Onion*, the source of the
    sarcastic headlines in the sarcasm dataset, against other sentences, as shown
    here:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，我选择使用8,000个词汇量（而不是之前为了避免过拟合所使用的2,000个词汇量）来得到您刚才看到的那些结果。然后，我使用来自*The Onion*的标题进行了测试，*The
    Onion*是讽刺数据集中讽刺性标题的来源，与其他句子进行了比较，如下所示：
- en: '[PRE16][PRE17]``                     `learn` `to` `speak` `and` `write` `in`
    `Gaelic``",` [PRE18]                     `Doors` `To` `Tally` `Household` `Sizes``"]`
    [PRE19]` [PRE20][PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE16][PRE17]``                     `learn` `to` `speak` `and` `write` `in`
    `Gaelic``",` [PRE18]                     `Doors` `To` `Tally` `Household` `Sizes``"]`
    [PRE19]` [PRE20][PRE21]'
- en: '[PRE22] [PRE23]`py  [PRE24]**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE22] [PRE23]`py  [PRE24]**'
