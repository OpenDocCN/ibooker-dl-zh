- en: Introduction to TensorFlow, PyTorch, JAX, and Keras
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow、PyTorch、JAX和Keras简介
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter03_introduction-to-ml-frameworks](https://deeplearningwithpython.io/chapters/chapter03_introduction-to-ml-frameworks)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[深度学习与Python](https://deeplearningwithpython.io/chapters/chapter03_introduction-to-ml-frameworks)'
- en: 'This chapter is meant to give you everything you need to start doing deep learning
    in practice. First, you’ll get familiar with three popular deep learning frameworks
    that can be used with Keras:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在为你提供开始实际进行深度学习所需的一切。首先，你将熟悉三个可以与Keras一起使用的流行深度学习框架：
- en: TensorFlow ([https://tensorflow.org](https://tensorflow.org))
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow ([https://tensorflow.org](https://tensorflow.org))
- en: PyTorch ([https://pytorch.org/](https://pytorch.org/))
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch ([https://pytorch.org/](https://pytorch.org/))
- en: JAX ([https://jax.readthedocs.io/](https://jax.readthedocs.io/))
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JAX ([https://jax.readthedocs.io/](https://jax.readthedocs.io/))
- en: Then, building on top of the first contact you’ve had with Keras in chapter
    2, we’ll review the core components of neural networks and how they translate
    to Keras APIs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在第二章中与Keras的第一次接触基础上，我们将回顾神经网络的核心组件以及它们如何转换为Keras API。
- en: By the end of this chapter, you’ll be ready to move on to practical, real-world
    applications — which will start with chapter 4.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将准备好进入实际、现实世界的应用——这些应用将从第4章开始。
- en: A brief history of deep learning frameworks
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习框架简史
- en: In the real world, you’re not going to be writing low-level code from scratch
    like we did at the end of chapter 2\. Instead, you’re going to use a framework.
    Besides Keras, the main deep learning frameworks today are JAX, TensorFlow, and
    PyTorch. This book will teach you about all four.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，你不会像我们在第2章末尾那样从头编写底层代码。相反，你将使用一个框架。除了Keras之外，今天主要的深度学习框架还有JAX、TensorFlow和PyTorch。这本书将教你了解所有这四个框架。
- en: If you’re just getting started with deep learning, it may seem like all these
    frameworks have been here forever. In reality, they’re all quite recent, with
    Keras being the oldest among the four (launched in March 2015). The ideas behind
    these frameworks, however, have a long history — the first paper about automatic
    differentiation was published in 1964^([[1]](#footnote-1))
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你刚开始接触深度学习，可能会觉得所有这些框架似乎一直都在这里。实际上，它们都相当新，Keras 是其中最古老的（于2015年3月推出）。然而，这些框架背后的理念有着悠久的历史——关于自动微分的第一篇论文发表于1964年^([[1]](#footnote-1))。
- en: 'All these frameworks combine three key features:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些框架都结合了三个关键特性：
- en: A way to compute gradients for arbitrary differentiable functions (automatic
    differentiation)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算任意可微分函数的梯度的一种方法（自动微分）
- en: A way to run tensor computations on CPUs and GPUs (and possibly even on other
    specialized deep learning hardware)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在CPU和GPU（甚至可能在其他专门的深度学习硬件）上运行张量计算的方法
- en: A way to distribute computation across multiple devices or multiple computers,
    such as multiple GPUs on one computer, or even multiple GPUs across multiple separate
    computers
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个设备或多台计算机上分布计算的方法，例如一台计算机上的多个GPU，甚至多台不同计算机上的多个GPU
- en: Together, these three simple features unlock all modern deep learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个简单特性共同解锁了所有现代深度学习。
- en: It took a long time for the field to develop robust solutions for all three
    problems and package those solutions in a reusable form. Since its inception in
    the 1960s and until the 2000s, autodifferentiation had no practical applications
    in machine learning — folks who worked with neural networks simply wrote their
    own gradient logic by hand, usually in a language like C++. Meanwhile, GPU programming
    was all but impossible.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域花了很长时间才为这三个问题开发出稳健的解决方案，并将这些解决方案打包成可重用的形式。自从20世纪60年代诞生以来，直到2000年代，自动微分在机器学习中没有实际应用——与神经网络打交道的人只是手动编写自己的梯度逻辑，通常是在C++这样的语言中。同时，GPU编程几乎是不可能的。
- en: Things started to slowly change in the late 2000s. First, Python and its ecosystem
    were slowly rising in popularity in the scientific community, gaining traction
    over MATLAB and C++. Second, NVIDIA released CUDA in 2006, unlocking the possibility
    of building neural networks that could run on consumer GPUs. The initial focus
    on CUDA was on physics simulation rather than machine learning, but that didn’t
    stop machine learning researchers from starting to implement CUDA-based neural
    networks from 2009 onward. They were typically one-off implementations that ran
    on a single GPU without any autodifferentiation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 事情在2000年代末开始慢慢发生变化。首先，Python及其生态系统在科学界逐渐流行起来，逐渐取代了MATLAB和C++。其次，NVIDIA在2006年发布了CUDA，解锁了在消费级GPU上构建神经网络的可能性。最初的CUDA重点是物理模拟而不是机器学习，但这并没有阻止机器学习研究人员从2009年开始实施基于CUDA的神经网络。这些通常是单次实现的，在单个GPU上运行，没有任何自动微分。
- en: The first framework to enable autodifferentiation and GPU computation to train
    deep learning models was Theano, circa 2009\. Theano is the conceptual ancestor
    of all modern deep learning tools. It started getting good traction in the machine
    learning research community in 2013–2014, after the results of the ImageNet 2012
    competition ignited the world’s interest in deep learning. Around the same time,
    a few other GPU-enabled deep learning libraries started gaining popularity in
    the computer vision world — in particular, Torch 7 (Lua-based) and Caffe (C++-based).
    Keras launched in early 2015 as a higher-level, easier-to-use deep learning library
    powered by Theano, and it quickly gained traction with the few thousands of people
    who were into deep learning at the time.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个使自动微分和GPU计算能够训练深度学习模型的框架是Theano，大约在2009年左右。Theano是所有现代深度学习工具的概念先驱。它在2013-2014年开始在机器学习研究社区中获得良好的势头，这得益于ImageNet
    2012竞赛的结果，它引发了全世界对深度学习的兴趣。大约与此同时，一些其他支持GPU的深度学习库开始在计算机视觉领域流行起来——特别是基于Lua的Torch
    7和基于C++的Caffe。Keras于2015年初作为由Theano驱动的更高级、更易于使用的深度学习库推出，并迅速获得了当时对深度学习感兴趣的几千人的青睐。
- en: Then in late 2015, Google launched TensorFlow, which took many of the key ideas
    from Theano and added support for large-scale distributed computation. The release
    of TensorFlow was a watershed moment that precipitated deep learning in the mainstream
    developer zeitgeist. Keras immediately added support for TensorFlow. By mid-2016,
    over half of all TensorFlow users were using it through Keras.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在2015年底，谷歌推出了TensorFlow，它从Theano中汲取了许多关键思想，并增加了对大规模分布式计算的支持。TensorFlow的发布是一个分水岭事件，它推动了深度学习在主流开发者中的普及。Keras立即增加了对TensorFlow的支持。到2016年中，超过一半的TensorFlow用户都是通过Keras来使用的。
- en: In response to TensorFlow, Meta (named Facebook at the time) launched PyTorch
    about one year later, taking ideas from Chainer (a niche but innovative framework
    launched in mid-2015, now long dead) and NumPy-Autograd, a CPU-only autodifferentiation
    library for NumPy released by Maclaurin et al. in 2014\. Meanwhile, Google released
    TPUs as an alternative to GPUs, alongside XLA, a high-performance compiler developed
    to enable TensorFlow to run on TPUs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对TensorFlow的回应，Meta（当时名为Facebook）大约一年后推出了PyTorch，它借鉴了Chainer（一个于2015年中推出的小众但创新的框架，现在已经不复存在）和NumPy-Autograd的思想，NumPy-Autograd是由Maclaurin等人于2014年发布的一个仅适用于CPU的自动微分库。与此同时，谷歌发布了TPU作为GPU的替代品，同时发布了XLA，这是一个高性能编译器，旨在使TensorFlow能够在TPU上运行。
- en: A few years later, at Google, Matthew Johnson — one of the developers who worked
    on NumPy-Autograd — released JAX as an alternative way to use autodifferentiation
    with XLA. JAX quickly gained traction with researchers thanks to its minimalistic
    API and high scalability. Today, Keras, TensorFlow, PyTorch, and JAX are the top
    frameworks in the deep learning world.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 几年后，在谷歌，NumPy-Autograd的开发者之一Matthew Johnson发布了JAX，作为一种使用XLA进行自动微分的新方法。JAX因其简约的API和高可扩展性而迅速受到研究人员的青睐。今天，Keras、TensorFlow、PyTorch和JAX是深度学习领域的顶级框架。
- en: Looking back on this chaotic history, we can ask, What’s next? Will a new framework
    arise tomorrow? Will we switch to a new programming language or a new hardware
    platform?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾这段混乱的历史，我们可以问，接下来会怎样？明天会出现一个新的框架吗？我们会转向新的编程语言或新的硬件平台吗？
- en: 'If you ask me, three things today are certain:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果问我，今天有三件事是确定的：
- en: Python has won. Its machine learning and data science ecosystem simply has too
    much momentum at this point. There won’t be a brand new language to replace it
    — at least not in the next 15 years.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’re in a multiframework world — all four frameworks are well established
    and are unlikely to go anywhere in the next few years. It’s a good idea for you
    to learn a little bit about each one. However, it’s highly possible that *new*
    frameworks will gain popularity in the future, in addition to them; Apple’s recently
    released MLX could be one such example. In this context, using Keras is a considerable
    advantage: you should be able to run your existing Keras models on any new up-and-coming
    framework via a new Keras backend. Keras will keep providing future-proof stability
    to machine learning developers in the future, like it has since 2015 — back when
    neither TensorFlow nor PyTorch nor JAX existed.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New chips may certainly arise in the future, alongside NVIDIA’s GPUs and Google’s
    TPUs. For instance, AMD’s GPU line likely has bright days ahead. But any new such
    chip will have to work with the existing frameworks to gain traction. New hardware
    is unlikely to disrupt your workflows.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How these frameworks relate to each other
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Keras, TensorFlow, PyTorch, and JAX don’t all have the same feature set and
    aren’t interchangeable. They have some overlap, but to a large extent, they serve
    different roles for different use cases. The biggest difference is between Keras
    and the three others. Keras is a high-level framework, while the others are lower
    level. Imagine building a house. Keras is like a prefabricated building kit: it
    provides a streamlined interface for setting up and training neural networks.
    In contrast, TensorFlow, PyTorch, and JAX are like the raw materials used in construction.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'As you saw in the previous chapters, training a neural network revolves around
    the following concepts:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '*First, low-level tensor manipulation* — The infrastructure that underlies
    all modern machine learning. This translates to low-level APIs found in TensorFlow,
    PyTorch^([[2]](#footnote-2)), and JAX:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tensors*, including special tensors that store the network’s state (*variables*)'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tensor operations* such as addition, `relu`, or `matmul`'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Backpropagation*, a way to compute the gradient of mathematical expressions'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Second, high-level deep learning concepts* — This translates to Keras APIs:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Layers*, which are combined into a *model*'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A *loss function*, which defines the feedback signal used for learning
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An *optimizer*, which determines how learning proceeds
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Metrics* to evaluate model performance, such as accuracy'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A *training loop* that performs mini-batch stochastic gradient descent
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Further, Keras is unique in that it isn’t a fully standalone framework. It needs
    a *backend engine* to run, (see figure 3.4), much like a prefabricated house-building
    kit needs to source building materials from somewhere. TensorFlow, PyTorch, and
    JAX can all be used as Keras backends. In addition, Keras can run on NumPy, but
    since NumPy does not provide an API for gradients, Keras workflows on NumPy are
    restricted to making predictions from a model — training is impossible.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Keras 的独特之处在于它不是一个完全独立的框架。它需要一个 *后端引擎* 来运行，(见图 3.4)，就像预制房屋建造套件需要从某处获取建筑材料一样。TensorFlow、PyTorch
    和 JAX 都可以用作 Keras 后端。此外，Keras 还可以在 NumPy 上运行，但由于 NumPy 不提供梯度 API，因此基于 NumPy 的
    Keras 工作流程仅限于从模型进行预测——训练是不可能的。
- en: 'Now that you have a clearer understanding of how all these frameworks came
    to be and how they relate to each other, let’s dive into what it’s like to work
    with them. We’ll cover them in chronological order: TensorFlow first, then PyTorch,
    and finally JAX.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经对所有这些框架是如何产生以及它们之间如何相互关联有了更清晰的理解，让我们深入了解与它们一起工作的感觉。我们将按时间顺序介绍它们：首先是 TensorFlow，然后是
    PyTorch，最后是 JAX。
- en: Introduction to TensorFlow
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 简介
- en: TensorFlow is a Python-based open source machine learning framework developed
    primarily by Google. Its initial release was in November 2015, followed by a v1
    release in February 2017, and a v2 release in October 2019. TensorFlow is heavily
    used in production-grade machine learning applications across the industry.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是一个由 Google 主要开发的基于 Python 的开源机器学习框架。它的首次发布是在 2015 年 11 月，随后在 2017
    年 2 月发布了 v1 版本，在 2019 年 10 月发布了 v2 版本。TensorFlow 在整个行业的生产级机器学习应用中被广泛使用。
- en: It’s important to keep in mind that TensorFlow is more than a single library.
    It’s really a platform, home to a vast ecosystem of components, some developed
    by Google, some developed by third parties. For instance, there’s TFX for industry-strength
    machine learning workflow management, TF-Serving for production deployment, the
    TF Optimization Toolkit for model quantization and pruning, and TFLite and MediaPipe
    for mobile application deployment.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，TensorFlow 不仅仅是一个库。它实际上是一个平台，拥有一个庞大的组件生态系统，其中一些由 Google 开发，一些由第三方开发。例如，有
    TFX 用于工业级机器学习工作流程管理，TF-Serving 用于生产部署，TF Optimization Toolkit 用于模型量化修剪，以及 TFLite
    和 MediaPipe 用于移动应用部署。
- en: Together, these components cover a very wide range of use cases, from cutting-edge
    research to large-scale production applications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件共同覆盖了非常广泛的使用案例，从尖端研究到大规模生产应用。
- en: First steps with TensorFlow
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow 的第一步
- en: 'Over the next paragraphs, you’ll get familiar with all the basics of TensorFlow.
    We’ll cover the following key concepts:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的段落中，您将熟悉 TensorFlow 的所有基础知识。我们将介绍以下关键概念：
- en: Tensors and variables
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量和变量
- en: Numerical operations in TensorFlow
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 中的数值运算
- en: Computing gradients with a `GradientTape`
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `GradientTape` 计算梯度
- en: Making TensorFlow functions fast by using just-in-time compilation
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过即时编译使 TensorFlow 函数快速运行
- en: 'We’ll then conclude the introduction with an end-to-end example: a pure-TensorFlow
    implementation of linear regression.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将通过一个端到端示例结束介绍：一个纯 TensorFlow 实现的线性回归。
- en: Let’s get those tensors flowing.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们让那些张量流动起来。
- en: Tensors and variables in TensorFlow
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TensorFlow 中的张量和变量
- en: To do anything in TensorFlow, we’re going to need some tensors. There are a
    few different ways you can create them.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 TensorFlow 中做任何事情，我们都需要一些张量。您可以通过几种不同的方式创建它们。
- en: Constant tensors
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 常数张量
- en: Tensors need to be created with some initial value, so common ways to create
    tensors are via `tf.ones` (equivalent to `np.ones`) and `tf.zeros` (equivalent
    to `np.zeros`). You can also create a tensor from Python or NumPy values using
    `tf.constant`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 张量需要用一些初始值来创建，因此常见的创建张量的方式是通过 `tf.ones`（相当于 `np.ones`）和 `tf.zeros`（相当于 `np.zeros`）。您还可以使用
    `tf.constant` 从 Python 或 NumPy 值创建张量。
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Listing 3.1](#listing-3-1): All-ones or all-zeros tensors'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 3.1](#listing-3-1)：全为 1 或全为 0 的张量'
- en: Random tensors
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 随机张量
- en: You can also create tensors filled with random values via one of the methods
    of the `tf.random` submodule (equivalent to the `np.random` submodule).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过 `tf.random` 子模块（相当于 `np.random` 子模块）的方法之一创建填充随机值的张量。
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Listing 3.2](#listing-3-2): Random tensors'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 3.2](#listing-3-2)：随机张量'
- en: Tensor assignment and the Variable class
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 张量赋值和 Variable 类
- en: 'A significant difference between NumPy arrays and TensorFlow tensors is that
    TensorFlow tensors aren’t assignable: they’re constant. For instance, in NumPy,
    you can do the following.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy数组与TensorFlow张量之间一个显著的区别是TensorFlow张量不可赋值：它们是常量。例如，在NumPy中，你可以这样做。
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Listing 3.3](#listing-3-3): NumPy arrays are assignable'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.3](#listing-3-3)：NumPy数组可赋值'
- en: 'Try to do the same thing in TensorFlow: you will get an error, `EagerTensor
    object does not support item assignment`.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在TensorFlow中做同样的事情：你会得到一个错误，`EagerTensor object does not support item assignment`。
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Listing 3.4](#listing-3-4): TensorFlow tensors are not assignable'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.4](#listing-3-4)：TensorFlow张量不可赋值'
- en: To train a model, we’ll need to update its state, which is a set of tensors.
    If tensors aren’t assignable, how do we do it, then? That’s where variables come
    in. `tf.Variable` is the class meant to manage modifiable state in TensorFlow.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个模型，我们需要更新其状态，这是一个张量集合。如果张量不可赋值，我们该如何操作呢？这就是变量的用武之地。`tf.Variable`是用于在TensorFlow中管理可修改状态的类。
- en: To create a variable, you need to provide some initial value, such as a random
    tensor.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个变量，你需要提供一个初始值，例如一个随机张量。
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Listing 3.5](#listing-3-5): Creating a `tf.Variable`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.5](#listing-3-5)：创建`tf.Variable`'
- en: The state of a variable can be modified via its `assign` method.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 变量的状态可以通过其`assign`方法进行修改。
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Listing 3.6](#listing-3-6): Assigning a value to a `Variable`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.6](#listing-3-6)：将值赋给`Variable`'
- en: Assignment also works for a subset of the coefficients.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 赋值也适用于系数的子集。
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Listing 3.7](#listing-3-7): Assigning a value to a subset of a `Variable`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.7](#listing-3-7)：将值赋给`Variable`的子集'
- en: Similarly, `assign_add` and `assign_sub` are efficient equivalents of `+=` and
    `-=`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，`assign_add`和`assign_sub`是`+=`和`-=`的有效等效操作。
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Listing 3.8](#listing-3-8): Using `assign_add`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.8](#listing-3-8)：使用`assign_add`'
- en: 'Tensor operations: Doing math in TensorFlow'
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 张量运算：在TensorFlow中进行数学运算
- en: Just like NumPy, TensorFlow offers a large collection of tensor operations to
    express mathematical formulas. Here are a few examples.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 就像NumPy一样，TensorFlow提供了一大批张量运算来表示数学公式。以下是一些示例。
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Listing 3.9](#listing-3-9): A few basic math operations in TensorFlow'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.9](#listing-3-9)：TensorFlow中的几个基本数学运算'
- en: 'Here’s an equivalent of the `Dense` layer we saw in chapter 2:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是第2章中看到的`Dense`层的等效形式：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Gradients in TensorFlow: A second look at the GradientTape API'
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TensorFlow中的梯度：再次审视GradientTape API
- en: 'So far, TensorFlow seems to look a lot like NumPy. But here’s something NumPy
    can’t do: retrieve the gradient of any differentiable expression with respect
    to any of its inputs. Just open a `GradientTape` scope, apply some computation
    to one or several input tensors, and retrieve the gradient of the result with
    respect to the inputs.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，TensorFlow看起来很像NumPy。但这里有一个NumPy做不到的事情：检索任何可微表达式相对于其任何输入的梯度。只需打开`GradientTape`作用域，对一或多个输入张量进行一些计算，然后检索相对于输入的结果的梯度。
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Listing 3.10](#listing-3-10): Using the `GradientTape`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.10](#listing-3-10)：使用`GradientTape`'
- en: 'This is most commonly used to retrieve the gradients of the loss of a model
    with respect to its weights: `gradients = tape.gradient(loss, weights)`.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常用于检索模型损失相对于其权重的梯度：`gradients = tape.gradient(loss, weights)`。
- en: In chapter 2, you saw how the `GradientTape` works on either a single input
    or a list of inputs and how inputs could be either scalars or high-dimensional
    tensors.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章中，你看到了`GradientTape`在单个输入或输入列表上的工作方式，以及输入可以是标量或高维张量。
- en: So far, you’ve only seen the case where the input tensors in `tape.gradient()`
    were TensorFlow variables. It’s actually possible for these inputs to be any arbitrary
    tensor. However, only *trainable variables* are being tracked by default. With
    a constant tensor, you’d have to manually mark it as being tracked, by calling
    `tape.watch()` on it.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你只看到了`tape.gradient()`中的输入张量是TensorFlow变量的情况。实际上，这些输入可以是任何任意的张量。然而，默认情况下，只有*可训练变量*被跟踪。对于常量张量，你必须手动将其标记为被跟踪，通过在它上面调用`tape.watch()`。
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[Listing 3.11](#listing-3-11): Using the `GradientTape` with constant tensor
    inputs'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.11](#listing-3-11)：使用`GradientTape`与常量张量输入'
- en: Why? Because it would be too expensive to preemptively store the information
    required to compute the gradient of anything with respect to anything. To avoid
    wasting resources, the tape needs to know what to watch. Trainable variables are
    watched by default because computing the gradient of a loss with regard to a list
    of trainable variables is the most common use case of the gradient tape.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The gradient tape is a powerful utility, even capable of computing *second-order
    gradients* — that is, the gradient of a gradient. For instance, the gradient of
    the position of an object with regard to time is the speed of that object, and
    the second-order gradient is its acceleration.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: If you measure the position of a falling apple along a vertical axis over time,
    and find that it verifies `position(time) = 4.9 * time ** 2`, what is its acceleration?
    Let’s use two nested gradient tapes to find out.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Listing 3.12](#listing-3-12): Using nested gradient tapes to compute second-order
    gradients'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Making TensorFlow functions fast using compilation
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All the TensorFlow code you’ve written so far has been executing “eagerly.”
    This means operations are executed one after the other in the Python runtime,
    much like any Python code or NumPy code. Eager execution is great for debugging,
    but it is typically quite slow. It can often be beneficial to parallelize some
    computation, or “fuse” operations — replacing two consecutive operations, like
    `matmul` followed by `relu`, with a single, more efficient operation that does
    the same thing without materializing the intermediate output.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: This can be achieved via *compilation*. The general idea of compilation is to
    take certain functions you’ve written in Python, lift them out of Python, automatically
    rewrite them into a faster and more efficient “compiled program,” and then call
    that program from the Python runtime.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'The main benefit of compilation is improved performance. There’s a drawback
    too: the code you write is no longer the code that gets executed, which can make
    the debugging experience painful. Only turn on compilation after you’ve already
    debugged your code in the Python runtime.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'You can apply compilation to any TensorFlow function by wrapping it in a `tf.function`
    decorator, like this:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: When you do this, any call to `dense()` is replaced with a call to a compiled
    program that implements a more optimized version of the function. The first call
    to the function will take a bit longer, because TensorFlow will be compiling your
    code. This only happens once — all subsequent calls to the same function will
    be fast.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow has two compilation modes:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: First, the default one, which we refer to as “graph mode.” Any function decorated
    with `@tf.function` runs in graph mode.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Second, compilation with XLA, a high-performance compiler for ML (it’s short
    for Accelerated Linear Algebra). You can turn it on by specifying `jit_compile=True`,
    like this:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It is often the case that compiling a function with XLA will make it run faster
    than graph mode — though it takes more time to execute the function the first
    time, since the compiler has more work to do.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'An end-to-end example: A linear classifier in pure TensorFlow'
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You know about tensors, variables, and tensor operations, and you know how to
    compute gradients. That’s enough to build any TensorFlow-based machine learning
    model based on gradient descent. Let’s walk through an end-to-end example to make
    sure everything is crystal clear.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'In a machine learning job interview, you may be asked to implement a linear
    classifier from scratch: a very simple task that serves as a filter between candidates
    who have some minimal machine learning background, and those who don’t. Let’s
    get you past that filter, and use your newfound knowledge of TensorFlow to implement
    such a linear classifier.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s come up with some nicely linearly separable synthetic data to
    work with: two classes of points in a 2D plane.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Listing 3.13](#listing-3-13): Generating two classes of random points in a
    2D plane'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '`negative_samples` and `positive_samples` are both arrays with shape `(1000,
    2)`. Let’s stack them into a single array with shape `(2000, 2)`.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[Listing 3.14](#listing-3-14): Stacking the two classes into an array with
    shape `(2000, 2)`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Let’s generate the corresponding target labels, an array of 0s and 1s of shape
    `(2000, 1)`, where `targets[i, 0]` is 0 if `inputs[i]` belongs to class 0 (and
    inversely).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[Listing 3.15](#listing-3-15): Generating the corresponding targets (0 and
    1)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Let’s plot our data with Matplotlib, a well-known Python data visualization
    library (it comes preinstalled in Colab, so no need for you to install it yourself),
    as shown in figure 3.1.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[Listing 3.16](#listing-3-16): Plotting the two point classes'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ff4f19ade555d51b5dc7ea520e28a8b.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: '[Figure 3.1](#figure-3-1): Our synthetic data: two classes of random points
    in the 2D plane'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s create a linear classifier that can learn to separate these two blobs.
    A linear classifier is an affine transformation (`prediction = matmul(input, W)
    + b`) trained to minimize the square of the difference between predictions and
    the targets.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: As you’ll see, it’s actually a much simpler example than the end-to-end example
    of a toy two-layer neural network from the end of chapter 2\. However, this time,
    you should be able to understand everything about the code, line by line.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create our variables `W` and `b`, initialized with random values and with
    zeros, respectively.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Listing 3.17](#listing-3-17): Creating the linear classifier variables'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Here’s our forward pass function.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[Listing 3.18](#listing-3-18): The forward pass function'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Because our linear classifier operates on 2D inputs, `W` is really just two
    scalar coefficients: `W = [[w1], [w2]]`. Meanwhile, `b` is a single scalar coefficient.
    As such, for given input point `[x, y]`, its prediction value is `prediction =
    [[w1], [w2]] • [x, y] + b = w1 * x + w2 * y + b`.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Here’s our loss function.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[Listing 3.19](#listing-3-19): The mean squared error loss function'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Now, we move to the training step, which receives some training data and updates
    the weights `W` and `b` to minimize the loss on the data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[Listing 3.20](#listing-3-20): The training-step function'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, we’ll do *batch training* instead of *mini-batch training*:
    we’ll run each training step (gradient computation and weight update) on the entire
    data, rather than iterate over the data in small batches. On one hand, this means
    that each training step will take much longer to run, since we compute the forward
    pass and the gradients for 2,000 samples at once. On the other hand, each gradient
    update will be much more effective at reducing the loss on the training data,
    since it will encompass information from all training samples instead of, say,
    only 128 random samples. As a result, we will need many fewer steps of training,
    and we should use a larger learning rate than what we would typically use for
    mini-batch training (we’ll use `learning_rate = 0.1`, as previously defined).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[Listing 3.21](#listing-3-21): The batch training loop'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'After 40 steps, the training loss seems to have stabilized around 0.025. Let’s
    plot how our linear model classifies the training data points, as shown in figure
    3.2. Because our targets are 0s and 1s, a given input point will be classified
    as “0” if its prediction value is below 0.5, and as “1” if it is above 0.5:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/874d3f2bae456f9cb13b84e03bbe8466.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: '[Figure 3.2](#figure-3-2): Our model’s predictions on the training inputs:
    pretty similar to the training targets'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the prediction value for a given point `[x, y]` is simply `prediction
    == [[w1], [w2]] • [x, y] + b == w1 * x + w2 * y + b`. Thus, class “0” is defined
    as `w1 * x + w2 * y + b < 0.5` and class “1” is defined as `w1 * x + w2 * y +
    b > 0.5`. You’ll notice that what you’re looking at is really the equation of
    a line in the 2D plane: `w1 * x + w2 * y + b = 0.5`. Class 1 is above the line;
    class 0 is below the line. You may be used to seeing line equations in the format
    `y = a * x + b`; in the same format, our line becomes `y = - w1 / w2 * x + (0.5
    - b) / w2`.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot this line, as shown in figure 3.3:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](../Images/dfe1caf49e1828628a7c2c086715ca4d.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: '[Figure 3.3](#figure-3-3): Our model, visualized as a line'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'This is really what a linear classifier is all about: finding the parameters
    of a line (or, in higher-dimensional spaces, a hyperplane) neatly separating two
    classes of data.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: What makes the TensorFlow approach unique
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You’re now familiar with all the basic APIs that underlie TensorFlow-based workflows,
    and you’re about to dive into more frameworks — in particular, PyTorch and JAX.
    What makes working with TensorFlow different from working with any other framework?
    When should you use TensorFlow, and when could you use something else?
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经熟悉了所有支撑 TensorFlow 工作流程的基本 API，你即将深入探索更多框架——特别是 PyTorch 和 JAX。与使用其他任何框架相比，使用
    TensorFlow 有何不同？你应该在什么情况下使用 TensorFlow，以及在什么情况下可以使用其他工具？
- en: 'If you ask us, here are the main benefits of TensorFlow:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你们问我们，以下是 TensorFlow 的主要优点：
- en: Thanks to graph mode and XLA compilation, it’s fast. It’s usually significantly
    faster than PyTorch and NumPy, though JAX is often even faster.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多亏了图模式和 XLA 编译，它运行得很快。它通常比 PyTorch 和 NumPy 快得多，尽管 JAX 有时甚至更快。
- en: It is extremely feature complete. Unique among all frameworks, it has support
    for string tensors as well as “ragged tensors” (tensors where different entries
    may have different dimensions — very useful for handling sequences without requiring
    to pad them to a shared length). It also has outstanding support for data preprocessing,
    via the highly performant `tf.data` API. `tf.data` is so good that even JAX recommends
    it for data preprocessing. Whatever you need to do, TensorFlow has a solution
    for it.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的功能非常全面。在所有框架中独一无二，它支持字符串张量以及“不规则张量”（不同条目可能具有不同维度的张量——在不需要将它们填充到共享长度的情况下处理序列非常有用）。它还通过高性能的
    `tf.data` API 提供出色的数据预处理支持。`tf.data` 非常出色，以至于 JAX 也推荐它用于数据预处理。无论你需要做什么，TensorFlow
    都有相应的解决方案。
- en: Its ecosystem for production deployment is the most mature among all frameworks,
    especially when it comes to deploying on mobile or in the browser.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的生产部署生态系统在所有框架中最为成熟，尤其是在移动或浏览器部署方面。
- en: 'However, TensorFlow also has some noticeable flaws:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，TensorFlow 也有一些明显的缺点：
- en: It has a sprawling API — the flipside of being very feature complete. TensorFlow
    includes thousands of different operations.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有一个庞大的 API——这是功能全面的一个副作用。TensorFlow 包含了数千种不同的操作。
- en: Its numerical API is occasionally inconsistent with the NumPy API, making it
    a bit harder to approach if you’re already familiar with NumPy.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的数值 API 有时与 NumPy API 不一致，如果你已经熟悉 NumPy，那么它可能会让你觉得有点难以接近。
- en: The popular pretrained model-sharing platform Hugging Face has less support
    for TensorFlow, which means that the latest generative AI models may not always
    be available in TensorFlow.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的预训练模型共享平台 Hugging Face 对 TensorFlow 的支持较少，这意味着最新的生成式 AI 模型可能并不总是可在 TensorFlow
    中找到。
- en: Now, let’s move on to PyTorch.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转向 PyTorch。
- en: Introduction to PyTorch
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch 简介
- en: PyTorch is a Python-based open source machine learning framework developed primarily
    by Meta (formerly Facebook) It was originally released in September 2016 (as a
    response to the release of TensorFlow), with its 1.0 version launched in 2018,
    and its 2.0 version launched in 2023. PyTorch inherits its programming style from
    the now-defunct Chainer framework, which was itself inspired by NumPy-Autograd.
    PyTorch is used extensively in the machine learning research community.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是由 Meta（前身为 Facebook）主要开发的一个基于 Python 的开源机器学习框架。它最初于 2016 年 9 月发布（作为对
    TensorFlow 发布的回应），其 1.0 版本于 2018 年推出，其 2.0 版本于 2023 年推出。PyTorch 从现在已停用的 Chainer
    框架继承了其编程风格，而 Chainer 框架本身又受到了 NumPy-Autograd 的启发。PyTorch 在机器学习研究社区中得到广泛应用。
- en: Like TensorFlow, PyTorch is at the center of a large ecosystem of related packages,
    such as `torchvision`, `torchaudio`, or the popular model-sharing platform Hugging
    Face.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 与 TensorFlow 类似，PyTorch 也处于一个庞大的相关包生态系统中心，例如 `torchvision`、`torchaudio` 或流行的模型共享平台
    Hugging Face。
- en: 'The PyTorch API is higher level than that of TensorFlow and JAX: it includes
    layers and optimizers, like Keras. These layers and optimizers are compatible
    with Keras workflows when you use Keras with the PyTorch backend.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch API 比 TensorFlow 和 JAX 的 API 更高级：它包括层和优化器，就像 Keras 一样。当你使用 PyTorch 后端与
    Keras 一起使用时，这些层和优化器与 Keras 工作流程兼容。
- en: First steps with PyTorch
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch 的第一步
- en: 'Over the next paragraphs, you’ll get familiar with all the basics of PyTorch.
    We’ll cover the following key concepts:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的段落中，你将熟悉 PyTorch 的所有基础知识。我们将涵盖以下关键概念：
- en: Tensors and parameters
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量和参数
- en: Numerical operations in PyTorch
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 中的数值操作
- en: Computing gradients with the `backward()` method
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `backward()` 方法计算梯度
- en: Packaging computation with the `Module` class
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `Module` 类包装计算
- en: Speeding up PyTorch by using compilation
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过编译加速 PyTorch
- en: We’ll conclude the introduction by reimplementing our linear regression end-to-end
    example in pure PyTorch.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Tensors and parameters in PyTorch
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A first gotcha about PyTorch is that the package isn’t named `pytorch`. It’s
    actually named `torch`. You’d install it via `pip install torch` and you’d import
    it via `import torch`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Like in NumPy and TensorFlow, the object at the heart of the framework is the
    tensor. First, let’s get our hands on some PyTorch tensors.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Constant tensors
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Here are some constant tensors.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[Listing 3.22](#listing-3-22): All-ones or all-zeros tensors'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Random tensors
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Random tensor creation is similar to NumPy and TensorFlow, but with divergent
    syntax. Consider the function `normal`: it doesn’t take a shape argument. Instead,
    the mean and standard deviation should be provided as PyTorch tensors with the
    expected output shape.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[Listing 3.23](#listing-3-23): Random tensors'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'As for creating a random uniform tensor, you’d do that via `torch.rand`. Unlike
    `np.random.uniform` or `tf.random.uniform`, the output shape should be provided
    as independent arguments for each dimension, like this:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Tensor assignment and the Parameter class
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Like NumPy arrays, but unlike TensorFlow tensors, PyTorch tensors are assignable.
    You can do operations like this:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: While you can just use a regular `torch.Tensor` to store the trainable state
    of a model, PyTorch does provide a specialized tensor subclass for that purpose,
    the `torch.nn.parameter.Parameter` class. Compared to a regular tensor, it provides
    semantic clarity — if you see a `Parameter`, you’ll know it’s a piece of trainable
    state, whereas a `Tensor` could be anything. As a result, it enables PyTorch to
    automatically track and retrieve the `Parameters` you assign to PyTorch models
    — similar to what Keras does with Keras `Variable` instances.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a `Parameter`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[Listing 3.24](#listing-3-24): Creating a PyTorch parameter'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Tensor operations: Doing math in PyTorch'
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Math in PyTorch works just the same as math in NumPy or TensorFlow, although
    much like TensorFlow, the PyTorch API often diverges in subtle ways from the NumPy
    API.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[Listing 3.25](#listing-3-25): A few basic math operations in PyTorch'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a dense layer:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Computing gradients with PyTorch
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There’s no explicit “gradient tape” in PyTorch. A similar mechanism does exist:
    when you run any computation in PyTorch, the framework creates a one-time computation
    graph (a “tape”) that records what just happened. However, that tape is hidden
    from the user. The public API for using it is at the level of tensors themselves:
    you can call `tensor.backward()` to run backpropagation through all operations
    previously executed that led to that tensor. Doing this will populate the `.grad`
    attribute of all tensors that are tracking gradients.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[Listing 3.26](#listing-3-26): Computing a gradient with `.backward()`'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'If you call `backward()` multiple times in a row, the `.grad` attribute will
    “accumulate” gradients: each new call will sum the new gradient with the preexisting
    one. For instance, in the following code, `input_var.grad` is not the gradient
    of `square(input_var)` with respect to `input_var`; rather, it is the sum of that
    gradient and the previously computed gradient — its value has doubled since our
    last code snippet:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你连续多次调用`backward()`，`.grad`属性将“累积”梯度：每次新的调用都会将新的梯度与现有的梯度相加。例如，在下面的代码中，`input_var.grad`不是`square(input_var)`相对于`input_var`的梯度；而是该梯度与之前计算的梯度的总和——它的值自从我们上一个代码片段以来已经翻倍了：
- en: '[PRE34]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To reset gradients, you can just set `.grad` to `None`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 要重置梯度，你只需将`.grad`设置为`None`：
- en: '[PRE35]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now let’s put this into practice!
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将其付诸实践！
- en: 'An end-to-end example: A linear classifier in pure PyTorch'
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个端到端的例子：纯PyTorch中的线性分类器
- en: You now know enough to rewrite our linear classifier in PyTorch. It will stay
    very similar to the TensorFlow one — the only major difference is how we compute
    the gradients.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道足够多的知识来用PyTorch重写我们的线性分类器。它将非常类似于TensorFlow版本——唯一的重大区别是我们如何计算梯度。
- en: 'Let’s start by creating our model variables. Don’t forget to pass `requires_grad=True`
    so we can compute gradients with respect to them:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先创建我们的模型变量。别忘了传递`requires_grad=True`，这样我们就可以计算相对于它们的梯度：
- en: '[PRE36]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This is our model — no difference so far. We just went from `tf.matmul` to
    `torch.matmul`:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的模型——到目前为止没有区别。我们只是从`tf.matmul`切换到`torch.matmul`：
- en: '[PRE37]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This is our loss function. We just switch from `tf.square` to `torch.square`
    and from `tf.reduce_mean` to `torch.mean`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的损失函数。我们只是从`tf.square`切换到`torch.square`，从`tf.reduce_mean`切换到`torch.mean`：
- en: '[PRE38]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now for the training step. Here’s how it works:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是训练步骤。这是它的工作方式：
- en: '`loss.backward()` runs backpropagation starting from the `loss` output node
    and populates the `tensor.grad` attribute on all tensors that were involved in
    the computation of `loss`. `tensor.grad` represents the gradient of the loss with
    regard to that tensor.'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`loss.backward()`从`loss`输出节点开始运行反向传播，并在所有参与`loss`计算的张量上填充`tensor.grad`属性。`tensor.grad`表示相对于该张量的损失梯度。'
- en: We use the `.grad` attribute to recover the gradients of the loss with regard
    to `W` and `b`.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`.grad`属性来恢复相对于`W`和`b`的损失梯度。
- en: We update `W` and `b` using those gradients. Because these updates are not intended
    to be part of the backward pass, we do them inside a `torch.no_grad()` scope,
    which skips gradient computation for everything inside it.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用这些梯度来更新`W`和`b`。因为这些更新不是反向传播的一部分，所以我们在一个`torch.no_grad()`作用域内进行，它会跳过其中所有内容的梯度计算。
- en: 'We reset the contents of the `.grad` property of our `W` and `b` parameters,
    by setting it to `None`. If we didn’t do this, gradient values would accumulate
    across multiple calls to `training_step()`, resulting in invalid values:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过将其设置为`None`来重置`W`和`b`参数的`.grad`属性的内容。如果我们不这样做，梯度值将在多次调用`training_step()`时累积，导致无效值：
- en: '[PRE39]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This could be made even simpler — let’s see how.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以变得更简单——让我们看看如何。
- en: Packaging state and computation with the Module class
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用Module类打包状态和计算
- en: 'PyTorch also has a higher-level, object-oriented API for performing backpropagation,
    which requires relying on two new classes: the `torch.nn.Module` class and an
    optimizer class from the `torch.optim` module, such as `torch.optim.SGD` (the
    equivalent of `keras.optimizers.SGD`).'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch还有一个用于执行反向传播的高级、面向对象的API，它需要依赖于两个新类：`torch.nn.Module`类和来自`torch.optim`模块的优化器类，例如`torch.optim.SGD`（与`keras.optimizers.SGD`等价）。
- en: The general idea is to define a subclass of `torch.nn.Module`, which will
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通用思路是定义`torch.nn.Module`的一个子类，它将
- en: Hold some `Parameters`, to store state variables. Those are defined in the `__init__()`
    method.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持一些`Parameters`，以存储状态变量。这些在`__init__()`方法中定义。
- en: Implement the forward pass computation in the `forward()` method.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`forward()`方法中实现前向传播计算。
- en: It should look just like the following.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该看起来就像以下这样。
- en: '[PRE40]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[Listing 3.27](#listing-3-27): Defining a `torch.nn.Module`'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.27](#listing-3-27)：定义`torch.nn.Module`'
- en: 'We can now instantiate our `LinearModel`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以实例化我们的`LinearModel`：
- en: '[PRE41]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'When using an instance of `torch.nn.Module`, rather than calling the `forward()`
    method directly, you’d use `__call__()` (i.e., directly call the model class on
    inputs), which redirects to `forward()` but adds a few framework hooks to it:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`torch.nn.Module`的一个实例时，而不是直接调用`forward()`方法，你会使用`__call__()`（即直接在输入上调用模型类），这会重定向到`forward()`但会向其中添加一些框架钩子：
- en: '[PRE42]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, let’s get our hands on a PyTorch optimizer. To instantiate it, you will
    need to provide the list of parameters that the optimizer is intended to update.
    You can retrieve it from our `Module` instance via `.parameters()`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们动手使用一个 PyTorch 优化器。为了实例化它，你需要提供优化器打算更新的参数列表。你可以通过我们的 `Module` 实例使用 `.parameters()`
    方法来获取它：
- en: '[PRE43]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Using our `Module` instance and the PyTorch `SGD` optimizer, we can run a simplified
    training step:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的 `Module` 实例和 PyTorch 的 `SGD` 优化器，我们可以运行一个简化的训练步骤：
- en: '[PRE44]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Previously, updating the model parameters looked like this:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，更新模型参数看起来是这样的：
- en: '[PRE45]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Now we can just do `optimizer.step()`.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需执行 `optimizer.step()`。
- en: Similarly, previously we needed to reset parameter gradients by hand by doing
    `tensor.grad = None` on each one. Now we can just do `model.zero_grad()`.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，之前我们需要手动通过在每个参数上执行 `tensor.grad = None` 来重置参数梯度。现在我们只需执行 `model.zero_grad()`。
- en: Overall, this may feel a bit confusing — somehow the loss tensor, the optimizer,
    and the `Module` instance all seem to be aware of each other through some hidden
    background mechanism. They’re all interacting with one another via spooky action
    at a distance. Don’t worry though — you can just treat this sequence of steps
    (`loss.backward()` - `optimizer.step()` - `model.zero_grad()`) as a magic incantation
    to be recited any time you need to write a training step function. Just make sure
    not to forget `model.zero_grad()`. That would be a major bug (and it is unfortunately
    quite common)!
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，这可能会感觉有点困惑——某种隐藏的背景机制似乎让损失张量、优化器和 `Module` 实例都相互了解。它们都是通过超距作用相互作用的。不过别担心——你只需将这个步骤序列（`loss.backward()`
    - `optimizer.step()` - `model.zero_grad()`）视为一个在任何需要编写训练步骤函数时都可以念诵的咒语。只需确保不要忘记
    `model.zero_grad()`。那将是一个严重的错误（而且遗憾的是，这种情况相当普遍）！
- en: Making PyTorch modules fast using compilation
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用编译来加速 PyTorch 模块
- en: One last thing. Similarly to how TensorFlow lets you compile functions for better
    performance, PyTorch lets you compile functions or even `Module` instances via
    the `torch.compile()` utility. This API uses PyTorch’s very own compiler, named
    Dynamo.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点。与 TensorFlow 允许你编译函数以获得更好的性能类似，PyTorch 允许你通过 `torch.compile()` 工具编译函数或甚至
    `Module` 实例。此 API 使用 PyTorch 自己的编译器，名为 Dynamo。
- en: 'Let’s try it on our linear regression `Module`:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的线性回归 `Module` 上试一试：
- en: '[PRE46]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The resulting object is intended to work identically to the original — except
    the forward and backward pass should run faster.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的对象旨在与原始对象工作方式相同——除了前向和反向传递应该运行得更快。
- en: 'You can also use `torch.compile()` as a function decorator:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以将 `torch.compile()` 用作函数装饰器：
- en: '[PRE47]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In practice, most PyTorch code out there does not use compilation and simply
    runs eagerly, as the compiler may not always work with all models and may not
    always result in a speedup when it does work. Unlike in TensorFlow and Jax where
    compilation was built in from the inception of the library, PyTorch’s compiler
    is a relatively recent addition.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，大多数 PyTorch 代码并不使用编译，而是简单地 eager 执行，因为编译器可能并不总是与所有模型兼容，并且当它工作时，可能不会总是导致速度提升。与
    TensorFlow 和 Jax 中的编译从库的诞生之初就内置不同，PyTorch 的编译器是一个相对较新的功能。
- en: What makes the PyTorch approach unique
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么，PyTorch 方法独特之处在哪里
- en: Compared to TensorFlow and JAX, which we will cover next, what makes PyTorch
    stand out? Why should you use it or not use it?
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们接下来要介绍的 TensorFlow 和 JAX 相比，什么让 PyTorch 独树一帜？为什么你应该使用它或不应使用它？
- en: 'Here are PyTorch’s two key strengths:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 PyTorch 的两个关键优势：
- en: PyTorch code executes eagerly by default, making it easy to debug. Note that
    this is also the case for TensorFlow code and JAX code, but a big difference is
    that PyTorch is generally intended to be run eagerly at all times, whereas any
    serious TensorFlow or JAX project will inevitably need compilation at some point,
    which can significantly hurt the debugging experience.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 代码默认是 eager 执行的，这使得调试变得容易。请注意，这也是 TensorFlow 代码和 JAX 代码的情况，但一个很大的区别是，PyTorch
    通常旨在始终 eager 执行，而任何严肃的 TensorFlow 或 JAX 项目最终都需要在某些时候进行编译，这可能会严重影响调试体验。
- en: The popular pretrained model-sharing platform Hugging Face has first-class support
    for PyTorch, which means that any model you’d like to use is likely available
    in PyTorch. This is the primary drive behind PyTorch adoption today.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的预训练模型共享平台 Hugging Face 对 PyTorch 提供了一级支持，这意味着你想要使用的任何模型很可能都可用在 PyTorch 中。这是
    PyTorch 当前采用的主要驱动力。
- en: 'Meanwhile, there are also some downsides to using PyTorch:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，使用 PyTorch 也有一些缺点：
- en: Like with TensorFlow, the PyTorch API is inconsistent with NumPy. Further, it’s
    also internally inconsistent. For instance, the commonly used keyword `axis` is
    occasionally named `dim` instead, depending on the function. Some pseudo-random
    number generation operations take a `seed` argument; others don’t. And so on.
    This can make PyTorch frustrating to learn, especially when coming from NumPy.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与TensorFlow一样，PyTorch API与NumPy不一致。此外，它内部也不一致。例如，常用的关键字`axis`有时会被命名为`dim`，这取决于函数。一些伪随机数生成操作需要一个`seed`参数；而另一些则不需要。等等。这可能会让PyTorch的学习变得令人沮丧，尤其是对于来自NumPy的用户。
- en: Due to its focus on eager execution, PyTorch is quite slow — it’s the slowest
    of all the major frameworks by a large margin. For most models, you may see a
    20% or 30% speedup with JAX. For some models — especially large ones — you may
    even see a 3× or a 5× speedup with JAX, even after using `torch.compile()`.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其专注于即时执行，PyTorch相当慢——它是所有主要框架中最慢的，差距很大。对于大多数模型，您可能会看到使用JAX可以提升20%或30%的速度。对于某些模型——尤其是大型模型——即使使用了`torch.compile()`，您也可能看到3倍或5倍的速度提升。
- en: While it is possible to make PyTorch code faster via `torch.compile()`, the
    PyTorch Dynamo compiler remains at this time (in 2025) quite ineffective and full
    of trapdoors. As a result, only a very small percentage of the PyTorch user base
    uses compilation. Perhaps this will be improved in future versions!
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然可以通过`torch.compile()`使PyTorch代码更快，但PyTorch Dynamo编译器在2025年仍然相当无效，充满了陷阱。因此，只有极少数的PyTorch用户使用编译。也许在未来的版本中会有所改进！
- en: Introduction to JAX
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JAX简介
- en: JAX is an open source library for differentiable computation, primarily developed
    by Google. After its release in 2018, JAX quickly gained traction in the research
    community, particularly for its ability to use Google’s TPUs at scale. Today,
    JAX is in use by most of the top players in the generative AI space — companies
    like DeepMind, Apple, Midjourney, Anthropic, Cohere, and so on.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: JAX 是一个开源的微分计算库，主要由谷歌开发。自2018年发布以来，JAX迅速在研究社区中获得认可，尤其是其能够大规模使用谷歌TPU的能力。如今，JAX被生成AI领域的多数顶级玩家所使用，例如DeepMind、苹果、Midjourney、Anthropic、Cohere
    等公司。
- en: JAX embraces a *stateless* approach to computation, meaning that functions in
    JAX do not maintain any persistent state. This contrasts with traditional imperative
    programming, where variables can hold values between function calls.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: JAX采用了一种*无状态*的计算方法，这意味着JAX中的函数不维护任何持久状态。这与传统的命令式编程形成对比，在命令式编程中，变量可以在函数调用之间保持值。
- en: The stateless nature of JAX functions has several advantages. In particular,
    it enables effective automatic parallelization and distributed computation, as
    functions can be executed independently without the need for synchronization.
    The extreme scalability of JAX is essential for handling the very large-scale
    machine learning problems faced by companies like Google and DeepMind.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: JAX函数的无状态特性有几个优点。特别是，它使得有效的自动并行化和分布式计算成为可能，因为函数可以在不需要同步的情况下独立执行。JAX的极端可扩展性对于处理像谷歌和DeepMind这样的公司面临的非常大规模的机器学习问题至关重要。
- en: First steps with JAX
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JAX的入门步骤
- en: 'We’ll go over the following key concepts:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍以下关键概念：
- en: The `array` class
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`array` 类'
- en: Random operations in JAX
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JAX中的随机操作
- en: Numerical operations in JAX
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JAX中的数值操作
- en: Computing gradients via `jax.grad` and `jax.value_and_grad`
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`jax.grad`和`jax.value_and_grad`计算梯度
- en: Making JAX functions fast by leveraging just-in-time compilation
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过即时编译来提高JAX函数的速度
- en: Let’s get started.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Tensors in JAX
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JAX中的张量
- en: One of the best features of JAX is that it doesn’t try to implement its own
    independent, similar-to-NumPy-but-slightly-divergent numerical API. Instead, it
    just implements the NumPy API, as is. It is available as the `jax.numpy` namespace,
    and you will often see it imported as `jnp` for short.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: JAX最优秀的特性之一是它不试图实现自己的独立、类似NumPy但略有差异的数值API。相反，它只是实现了NumPy API，正如其名。它作为`jax.numpy`命名空间提供，您通常会将其导入为`jnp`。
- en: Here are some JAX arrays.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些JAX数组。
- en: '[PRE48]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[Listing 3.28](#listing-3-28): All-ones or all-zeros tensors'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.28](#listing-3-28)：全1或全0的张量'
- en: 'There are, however, two minor differences between `jax.numpy` and the actual
    NumPy API: random number generation and array assignment. Let’s take a look.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`jax.numpy`和实际的NumPy API之间有两个细微的差异：随机数生成和数组赋值。让我们来看看。
- en: Random number generation in JAX
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JAX中的随机数生成
- en: The first difference between JAX and NumPy has to do with the way JAX handles
    random operations — what is known as “PRNG” (Pseudo-Random Number Generation)
    operations. We said earlier that JAX is *stateless*, which implies that JAX code
    can’t rely on any hidden global state. Consider the following NumPy code.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[Listing 3.29](#listing-3-29): Random tensors'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: How did the second call to `np.random.normal()` know to return a different value
    from the first call? That’s right — it’s a hidden piece of global state. You can
    actually retrieve that global state via `np.random.get_state()` and set it via
    `np.random.seed(seed)`.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: In a stateless framework, we can’t have any such global state. The same API
    call must always return the same value. As a result, in a stateless version of
    NumPy, you would have to rely on passing different seed arguments to your `np.random`
    calls to get different values.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it’s often the case that your PRNG calls are going to be in functions
    that get called multiple times and that are intended to use different random values
    each time. If you don’t want to rely on any global state, this requires you to
    manage your seed state outside of the target function, like this:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'It’s basically the same in JAX. However, JAX doesn’t use integer seeds. It
    uses special array structures called *keys*. You can create one from an integer
    value, like this:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'To force you to always provide a seed “key” to PRNG calls, all JAX PRNG-using
    operations take `key` (the random seed) as their first positional argument. Here’s
    how to use `random.normal()`:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Two calls to `random.normal()` that receive the same seed key will always return
    the same value.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[Listing 3.30](#listing-3-30): Using a random seed in Jax'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need a new seed key, you can simply create a new one from an existing
    one using the `jax.random.split()` function. It is deterministic, so the same
    sequence of splits will always result in the same final seed key:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This is definitely more work than `np.random`! But the benefits of statelessness
    far outweigh the costs: it makes your code *vectorizable* (i.e., the JAX compiler
    can automatically turn it into highly parallel code) while maintaining determinism
    (i.e., you can run the same code twice with the same results). That is impossible
    to achieve with a global PRNG state.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Tensor assignment
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The second difference between JAX and NumPy is tensor assignment. Like in TensorFlow,
    JAX arrays are not assignable in place. That’s because any sort of in-place modification
    would go against JAX’s stateless design. Instead, if you need to update a tensor,
    you must create a new tensor with the desired value. JAX makes this easy by providing
    the `at()`/`set()` API. These methods allow you to create a new tensor with an
    updated element at a specific index. Here’s an example of how you would update
    the first element of a JAX array to a new value.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[Listing 3.31](#listing-3-31): Modifying values in a JAX array'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Simple enough!
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'Tensor operations: Doing math in JAX'
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Doing math in JAX looks exactly the same as it does in NumPy. No need to learn
    anything new this time!
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在 JAX 中进行数学运算看起来与在 NumPy 中完全一样。这次不需要学习任何新东西！
- en: '[PRE56]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[Listing 3.32](#listing-3-32): A few basic math operations in JAX'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 3.32](#listing-3-32)：JAX 中的几个基本数学运算'
- en: 'Here’s a dense layer:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个密集层：
- en: '[PRE57]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Computing gradients with JAX
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 JAX 计算梯度
- en: 'Unlike TensorFlow and PyTorch, JAX takes a *metaprogramming* approach to gradient
    computation. Metaprogramming refers to the idea of having *functions that return
    functions* — you could call them “meta-functions.” In practice, JAX lets you *turn
    a loss-computation function into a gradient-computation function*. So computing
    gradients in JAX is a three-step process:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 与 TensorFlow 和 PyTorch 不同，JAX 采用 *元编程* 方法进行梯度计算。元编程指的是拥有 *返回函数的函数* 的想法——你可以称它们为“元函数”。在实践中，JAX
    允许你 *将损失计算函数转换为梯度计算函数*。因此，在 JAX 中计算梯度是一个三步过程：
- en: Define a loss function, `compute_loss()`.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个损失函数，`compute_loss()`。
- en: Call `grad_fn = jax.grad(compute_loss)` to retrieve a gradient-computation function.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 `grad_fn = jax.grad(compute_loss)` 来检索一个梯度计算函数。
- en: Call `grad_fn` to retrieve the gradient values.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 `grad_fn` 来检索梯度值。
- en: 'The loss function should verify the following properties:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数应满足以下属性：
- en: It should return a scalar loss value.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该返回一个标量损失值。
- en: Its first argument (which, in the following example, is also the only argument)
    should contain the state arrays we need gradients for. This argument is usually
    named `state`. For instance, this first argument could be a single array, a list
    of arrays, or a dict of arrays.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其第一个参数（在以下示例中，这也是唯一的参数）应包含我们需要计算梯度的状态数组。这个参数通常命名为 `state`。例如，这个第一个参数可以是一个数组、一个数组列表，或者一个数组字典。
- en: 'Let’s take a look at a simple example. Here’s a loss-computation function that
    takes a single scalar, `input_var` and returns a scalar loss value — just the
    square of the input:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个简单的例子。这是一个损失计算函数，它接受一个单个标量 `input_var` 并返回一个标量损失值——只是输入的平方：
- en: '[PRE58]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We can now call the JAX utility `jax.grad()` on this loss function. It returns
    a gradient-computation function — a function that takes the same arguments as
    the original loss function and returns the gradient of the loss with respect to
    `input_var`:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在这个损失函数上调用 JAX 的实用工具 `jax.grad()`。它返回一个梯度计算函数——一个接受与原始损失函数相同参数的函数，并返回相对于
    `input_var` 的损失梯度：
- en: '[PRE59]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Once you’ve obtained `grad_fn()`, you can call it with the same arguments as
    `compute_loss()`, and it will return gradients arrays corresponding to the first
    argument of `compute_loss()`. In our case, our first argument was a single array,
    so `grad_fn()` directly returns the gradient of the loss with respect to that
    one array:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你获得了 `grad_fn()`，你可以用与 `compute_loss()` 相同的参数调用它，它将返回与 `compute_loss()` 的第一个参数相对应的梯度数组。在我们的例子中，我们的第一个参数是一个单个数组，所以
    `grad_fn()` 直接返回相对于该数组的损失梯度：
- en: '[PRE60]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: JAX gradient-computation best practices
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: JAX 梯度计算最佳实践
- en: So far so good! Metaprogramming is a big word, but it turns out to be quite
    simple. Now, in real-world use cases, there are a few more things you’ll need
    to take into account. Let’s take a look.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止一切顺利！元编程是一个大词，但事实证明它相当简单。现在，在现实世界的用例中，你还需要考虑一些其他的事情。让我们看看。
- en: Returning the loss value
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 返回损失值
- en: 'It’s usually the case that you don’t just need the gradient array; you also
    need the loss value. It would be quite inefficient to recompute it independently
    outside of `grad_fn()`, so instead, you can just configure your `grad_fn()` to
    also return the loss value. This is done by using the JAX utility `jax.value_and_grad()`
    instead of `jax.grad()`. It works identically, but it returns a tuple of values,
    where the first entry is the loss value, and the second entry is the gradient(s):'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你不仅需要梯度数组；还需要损失值。在 `grad_fn()` 之外独立重新计算它将非常低效，因此，你可以配置你的 `grad_fn()` 以返回损失值。这是通过使用
    JAX 实用工具 `jax.value_and_grad()` 而不是 `jax.grad()` 来实现的。它的工作方式相同，但它返回一个值的元组，其中第一个值是损失值，第二个值是梯度（s）：
- en: '[PRE61]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Getting gradients for a complex function
  id: totrans-338
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为复杂函数获取梯度
- en: Now, what if you need gradients for more than a single variable? And what if
    your `compute_loss()` function has more than one input?
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你需要计算多个变量的梯度怎么办？如果你的 `compute_loss()` 函数有多个输入怎么办？
- en: 'Let’s say your state contains three variables, `a`, `b`, and `c`, and your
    loss function has two inputs, `x` and `y`. You would simply structure it like
    this:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的状态包含三个变量，`a`、`b` 和 `c`，而你的损失函数有两个输入，`x` 和 `y`。你只需这样构建它：
- en: '[PRE62]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Note that `state` doesn’t have to be a tuple — it could be a dict, a list, or
    any nested structure of tuples, dicts, and lists. In JAX parlance, such a nested
    structure is called a *tree*.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`state` 不一定是元组——它可以是字典、列表或任何嵌套的元组、字典和列表结构。在 JAX 术语中，这样的嵌套结构被称为 *树*。
- en: Returning auxiliary outputs
  id: totrans-343
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 返回辅助输出
- en: Finally, what if your `compute_loss()` function needs to return more than just
    the loss? Let’s say you want to return an additional value `output` that’s computed
    as a by-product of the loss computation. How to get it out?
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你的 `compute_loss()` 函数需要返回不仅仅是损失呢？比如说，你想要返回一个作为损失计算副产品的额外值 `output`。如何将其获取出来？
- en: 'You would use the `has_aux` argument:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 你会使用 `has_aux` 参数：
- en: Edit the loss function to return a tuple where the first entry is the loss,
    and the second entry is your extra output.
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑损失函数，使其返回一个元组，其中第一个元素是损失，第二个元素是你的额外输出。
- en: 'Pass the argument `has_aux=True` to `value_and_grad()`. This tells `value_and_grad()`
    to return not just the gradient but also the “auxiliary” output(s) of `compute_loss()`,
    like this:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `has_aux=True` 参数传递给 `value_and_grad()`。这告诉 `value_and_grad()` 不仅返回梯度，还返回
    `compute_loss()` 的“辅助”输出（输出），如下所示：
- en: '[PRE63]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Admittedly, things are starting to be pretty convoluted at this point. Don’t
    worry, though; this is about as hard as JAX gets! Almost everything else is simpler
    by comparison.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 诚然，到了这一点，事情开始变得相当复杂。不过，别担心；这几乎是 JAX 最难的部分！与其他大部分内容相比，这几乎简单多了。
- en: Making JAX functions fast with @jax.jit
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 @jax.jit 使 JAX 函数快速
- en: 'One more thing. As a JAX user, you will frequently use the `@jax.jit` decorator,
    which behaves identically to the `@tf.function(jit_compile=True)` decorator. It
    turns any stateless JAX function into an XLA-compiled piece of code, typically
    delivering a considerable execution speedup:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事。作为一个 JAX 用户，你将经常使用 `@jax.jit` 装饰器，它与 `@tf.function(jit_compile=True)`
    装饰器行为相同。它将任何无状态的 JAX 函数转换为 XLA 编译的代码片段，通常能带来显著的执行速度提升：
- en: '[PRE64]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Be mindful that you can only decorate a stateless function — any tensors that
    get updated by the function should be part of its return values.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你只能装饰无状态函数——任何被函数更新的张量都应该是其返回值的一部分。
- en: 'An end-to-end example: A linear classifier in pure JAX'
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个端到端示例：纯 JAX 中的线性分类器
- en: 'Now you know enough JAX to write the JAX version of our linear classifier example.
    There are two major differences from the TensorFlow and PyTorch versions you’ve
    already seen:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了足够的 JAX 知识，可以编写我们线性分类器的 JAX 版本。与 TensorFlow 和 PyTorch 版本相比，有两个主要区别：
- en: All functions we will create will be *stateless*. That means the state (the
    arrays `W` and `b`) will be provided as function arguments, and if they get modified
    by the function, their new value will be returned by the function.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将创建的所有函数都将是无状态的。这意味着状态（数组 `W` 和 `b`）将作为函数参数提供，如果它们被函数修改，函数将返回它们的新的值。
- en: Gradients are computed using the JAX `value_and_grad()` utility.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度是通过 JAX 的 `value_and_grad()` 工具计算的。
- en: 'Let’s get started. The model function and the mean squared error function should
    look familiar:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。模型函数和均方误差函数应该看起来很熟悉：
- en: '[PRE65]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'To compute gradients, we need to package loss computation in a single `compute_loss()`
    function. It returns the total loss as a scalar, and it takes `state` as its first
    argument — a tuple of all the tensors we need gradients for:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算梯度，我们需要将损失计算包装在单个 `compute_loss()` 函数中。它返回一个标量作为总损失，并接受 `state` 作为其第一个参数——我们需要计算梯度的所有张量的元组：
- en: '[PRE66]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Calling `jax.value_and_grad()` on this function gives us a new function, with
    the same argument as `compute_loss`, which returns both the loss and the gradients
    of the loss with regard to the elements of `state`:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数上调用 `jax.value_and_grad()` 给我们一个新的函数，具有与 `compute_loss` 相同的参数，它返回损失和相对于
    `state` 元素的损失梯度：
- en: '[PRE67]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Next, we can set up our training step function. It looks straightforward. Be
    mindful that, unlike its TensorFlow and PyTorch equivalents, it needs to be stateless,
    and so it must return the updated values of the `W` and `b` tensors:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以设置我们的训练步骤函数。它看起来很简单。请注意，与 TensorFlow 和 PyTorch 的等效函数不同，它必须是状态无关的，因此它必须返回
    `W` 和 `b` 张量的更新值：
- en: '[PRE68]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Because we won’t change the `learning_rate` during our example, we can consider
    it part of the function itself and not our model’s state. If we wanted to modify
    our learning rate during training, we’d need to pass it through as well.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在我们的例子中我们不会改变 `learning_rate`，所以我们可以将其视为函数本身的一部分，而不是我们模型的状态。如果我们想在训练过程中修改学习率，我们也需要将其传递过去。
- en: 'Finally, we’re ready to run the full training loop. We initialize `W` and `b`,
    and we repeatedly update them via stateless calls to `training_step()`:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备运行完整的训练循环。我们初始化`W`和`b`，然后通过无状态的`training_step()`调用反复更新它们：
- en: '[PRE69]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: That’s it! You’re now able to write a custom training loop in JAX.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些！你现在可以编写自定义的JAX训练循环了。
- en: What makes the JAX approach unique
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JAX方法独特之处在于
- en: The main thing that makes JAX unique among modern machine learning frameworks
    is its functional, stateless philosophy. While it may seem to cause friction at
    first, it is what unlocks the power of JAX — its ability to compile to extremely
    fast code and to scale to arbitrarily large models and arbitrarily many devices.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代机器学习框架中，使JAX独特的最主要因素是其函数式、无状态的哲学。虽然一开始可能会觉得这会引起一些摩擦，但它正是解锁JAX力量的关键——它能够编译成极快的代码，并且能够扩展到任意大的模型和任意多的设备。
- en: 'There’s a lot to like about JAX:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: JAX有很多优点：
- en: It’s fast. For most models, it is the fastest of all frameworks you’ve seen
    so far.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它很快。对于大多数模型来说，它是你迄今为止见过的所有框架中最快的。
- en: Its numerical API is fully consistent with NumPy, making it pleasant to learn.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的数值API与NumPy完全一致，这使得学习起来很愉快。
- en: It’s the best fit for training models on TPUs, as it was developed from the
    ground up for XLA and TPUs.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是最适合在TPU上训练模型的，因为它从头开始就是为XLA和TPU开发的。
- en: 'Using JAX can also come with some amount of developer friction:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 使用JAX也可能带来一些开发者的摩擦：
- en: Its use of metaprogramming and compilation can make it significantly harder
    to debug compared to pure eager execution.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与纯即时执行相比，它的元编程和编译使用可能会使其调试变得显著困难。
- en: Low-level training loops tend to be more verbose and more difficult to write
    than in TensorFlow or PyTorch.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与TensorFlow或PyTorch相比，低级训练循环通常更冗长，更难编写。
- en: 'At this point, you know the basics of TensorFlow, PyTorch, and JAX, and you
    can use these frameworks to implement a basic linear classifier from scratch.
    That’s a solid foundation to build upon. It’s now time to move on to a more productive
    path to deep learning: the Keras API.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经了解了TensorFlow、PyTorch和JAX的基础知识，并且可以使用这些框架从头开始实现一个基本的线性分类器。这是一个坚实的基石，现在我们可以转向更有效的深度学习路径：Keras
    API。
- en: Introduction to Keras
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras简介
- en: Keras is a deep learning API for Python that provides a convenient way to define
    and train any kind of deep learning model. It was released in March 2015, with
    its v2 in 2017 and its v3 in 2023.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是一个用于Python的深度学习API，它提供了一种方便的方式来定义和训练任何类型的深度学习模型。它于2015年3月发布，2017年发布了v2版本，2023年发布了v3版本。
- en: Keras users range from academic researchers, engineers, and data scientists
    at both startups and large companies to graduate students and hobbyists. Keras
    is used at Google, Netflix, Uber, YouTube, CERN, NASA, Yelp, Instacart, Square,
    Waymo, YouTube, and thousands of smaller organizations working on a wide range
    of problems across every industry. Your YouTube recommendations originate from
    Keras models. The Waymo self-driving cars rely on Keras models for processing
    sensor data. Keras is also a popular framework on Kaggle, the machine learning
    competition website.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的用户包括学术研究人员、初创公司和大型公司的工程师和数据科学家、研究生和爱好者。Keras被用于Google、Netflix、Uber、YouTube、CERN、NASA、Yelp、Instacart、Square、Waymo、YouTube以及成千上万的在各个行业中解决各种问题的较小组织。你的YouTube推荐来自Keras模型。Waymo的自动驾驶汽车依赖于Keras模型来处理传感器数据。Keras也是Kaggle机器学习竞赛网站上的一个流行框架。
- en: Because Keras has a diverse user base, it doesn’t force you to follow a single
    “true” way of building and training models. Rather, it enables a wide range of
    different workflows, from the very high-level to the very low-level, corresponding
    to different user profiles. For instance, you have an array of ways to build models
    and an array of ways to train them, each representing a certain tradeoff between
    usability and flexibility. In chapter 7, we’ll review in detail a good fraction
    of this spectrum of workflows.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Keras拥有多样化的用户群体，它不会强迫你遵循单一的“正确”的构建和训练模型的方式。相反，它允许广泛的不同工作流程，从非常高级到非常低级，对应不同的用户画像。例如，你有多种构建模型的方法和多种训练它们的方法，每种方法都代表了一定程度上的可用性和灵活性的权衡。在第7章中，我们将详细回顾这部分工作流程的很大一部分。
- en: First steps with Keras
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Keras的入门步骤
- en: Before we get to writing Keras code, there are a few things to consider when
    setting up the library before it’s imported.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们编写Keras代码之前，在导入库之前有一些事情需要考虑。
- en: Picking a backend framework
  id: totrans-386
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择后端框架
- en: Keras can be used together with JAX, TensorFlow, or PyTorch. They’re the “backend
    frameworks” of Keras. Through these backend frameworks, Keras can run on top of
    different types of hardware (see figure 3.4) — GPU, TPU, or plain CPU — can be
    seamlessly scaled to thousands of machines, and can be deployed to a variety of
    platforms.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/552c279c016c6f17a2d903764c737676.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
- en: '[Figure 3.4](#figure-3-4): Keras and its backends. A backend is a low-level
    tensor-computing platform; Keras is a high-level deep learning API.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: 'Backend frameworks are pluggable: you can switch to a different backend framework
    *after* you’ve written some Keras code. You aren’t locked into a single framework
    and a single ecosystem — you can move your models from JAX to TensorFlow to PyTorch
    depending on your current needs. For instance, when you develop a Keras model,
    you could debug it with PyTorch, train it on TPU with JAX for maximum efficiency,
    and finally run inference with the excellent tooling from the TensorFlow ecosystem.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'The default backend for Keras right now is TensorFlow, so if you run `import
    keras` in a fresh environment, without having configured anything, you will be
    running on top of TensorFlow. There are two ways to pick a different backend:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the environment variable `KERAS_BACKEND`. Before you start your `python`
    repl, you can run the following shell command to use JAX as your Keras backend:
    `export KERAS_BACKEND=jax`. Alternatively, you can add the following code snippet
    at the top of your Python file or notebook (note that it must imperatively go
    before the first `import keras`):'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Edit your local Keras configuration file at `~/.keras/keras.json`. If you have
    already imported Keras once, this file has already been created with default settings.
    You can use any text editor to open and modify it — it’s a human-readable JSON
    file. It should look like this:'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Now, you may ask, which backend should I be picking? It’s really your own choice:
    all Keras code examples in the rest of the book will be compatible with all three
    backends. If the need for backend-specific code arises (as in chapter 7, for instance),
    I will show you all three versions — TensorFlow, PyTorch, JAX. If you have no
    particular backend preference, my personal recommendation is JAX. It’s usually
    the most performant backend.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Once your backend is configured, you can start actually building and training
    Keras models. Let’s take a look.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: 'Layers: The building blocks of deep learning'
  id: totrans-398
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The fundamental data structure in neural networks is the *layer*, to which
    you were introduced in chapter 2\. A layer is a data processing module that takes
    as input one or more tensors and that outputs one or more tensors. Some layers
    are stateless, but more frequently layers have a state: the layer’s *weights*,
    one or several tensors learned with stochastic gradient descent, which together
    contain the network’s *knowledge*.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: Different types of layers are appropriate for different tensor formats and different
    types of data processing. For instance, simple vector data, stored in 2D tensors
    of shape `(samples, features)`, is often processed by *densely connected* layers,
    also called *fully connected* or *dense* layers (the `Dense` class in Keras).
    Sequence data, stored in 3D tensors of shape `(samples, timesteps, features)`,
    is typically processed by *recurrent* layers, such as an `LSTM` layer, or 1D convolution
    layers (`Conv1D`). Image data, stored in rank-4 tensors, is usually processed
    by 2D convolution layers (`Conv2D`).
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: You can think of layers as the LEGO bricks of deep learning, a metaphor that
    is made explicit by Keras. Building deep learning models in Keras is done by clipping
    together compatible layers to form useful data transformation pipelines.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: The base `Layer` class in Keras
  id: totrans-402
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A simple API should have a single abstraction around which everything is centered.
    In Keras, that’s the `Layer` class. Everything in Keras is either a `Layer` or
    something that closely interacts with a `Layer`.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: A `Layer` is an object that encapsulates some state (weights) and some computation
    (a forward pass). The weights are typically defined in a `build()` (although they
    could also be created in the constructor `__init__()`), and the computation is
    defined in the `call()` method.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we implemented a `NaiveDense` class that contained
    two weights `W` and `b` and applied the computation `output = activation(matmul(input,
    W) + b)`. The following is what the same layer would look like in Keras.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[Listing 3.33](#listing-3-33): A simple dense layer from scratch in Keras'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll cover in detail the purpose of these `build()` and
    `call()` methods. Don’t worry if you don’t understand everything just yet!
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: 'Once instantiated, a layer like this can be used just like a function, taking
    as input a tensor:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Now, you’re probably wondering, why did we have to implement `call()` and `build()`,
    since we ended up using our layer by plainly calling it, that is to say, by using
    its `__call__` method? It’s because we want to be able to create the state just
    in time. Let’s see how that works.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'Automatic shape inference: Building layers on the fly'
  id: totrans-412
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Just like with LEGO bricks, you can only “clip” together layers that are *compatible*.
    The notion of *layer compatibility* here refers specifically to the fact that
    every layer will only accept input tensors of a certain shape and will return
    output tensors of a certain shape. Consider the following example:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: This layer will return a tensor whose non-batch dimension is 32\. It can only
    be connected to a downstream layer that expects 32-dimensional vectors as its
    input.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 'When using Keras, you don’t have to worry about size compatibility most of
    the time because the layers you add to your models are dynamically built to match
    the shape of the incoming inputs. For instance, suppose you write the following:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The layers didn’t receive any information about the shape of their inputs. Instead,
    they automatically inferred their input shape as being the shape of the first
    inputs they see.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 层没有接收到任何关于其输入形状的信息。相反，它们自动推断它们的输入形状为它们看到的第一个输入的形状。
- en: 'In the toy version of a `Dense` layer that we’ve implemented in chapter 2,
    we had to pass the layer’s input size explicitly to the constructor in order to
    be able to create its weights. That’s not ideal, because it would lead to models
    that look like this, where each new layer needs to be made aware of the shape
    of the layer before it:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章中我们实现的`Dense`层的玩具版本中，我们必须显式地将层的输入大小传递给构造函数，以便能够创建其权重。这并不理想，因为它会导致看起来像这样的模型，其中每个新的层都需要知道它前面的层的形状：
- en: '[PRE76]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: It would be even worse when the rules used by a layer to produce its output
    shape are complex. For instance, what if our layer returned outputs of shape `(batch,
    input_size * 2 if input_size % 2 == 0 else input_size * 3)`?
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个层用来产生其输出形状的规则复杂时，情况会更糟。例如，如果我们的层返回形状为`(batch, input_size * 2 if input_size
    % 2 == 0 else input_size * 3)`的输出呢？
- en: If we were to reimplement our `NaiveDense` layer as a Keras layer capable of
    automatic shape inference, it would look like the `SimpleDense` layer, with its
    `build()` and `call()` methods.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要重新实现我们的`NaiveDense`层为一个Keras层，使其能够自动推断形状，它将看起来像`SimpleDense`层，具有它的`build()`和`call()`方法。
- en: 'In the Keras `SimpleDense`, we no longer create weights in the constructor
    like in the previous example. Instead, we create them in a dedicated state-creation
    method `build()`, which receives as argument the first input shape seen by the
    layer. The `build()` method is called automatically the first time the layer is
    called (via its `__call__()` method). In fact, that’s why we defined the computation
    in a separate `call()` method rather than in the `__call__()` method directly!
    The `__call__()` method of the base layer schematically looks like this:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras的`SimpleDense`中，我们不再像上一个例子那样在构造函数中创建权重。相反，我们在一个专门的状态创建方法`build()`中创建它们，该方法接收层看到的第一个输入形状作为参数。`build()`方法在层第一次被调用时自动调用（通过其`__call__()`方法）。事实上，这就是为什么我们定义了计算在单独的`call()`方法中而不是直接在`__call__()`方法中的原因！基类层的`__call__()`方法示意图如下：
- en: '[PRE77]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'With automatic shape inference, our previous example becomes simple and neat:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自动形状推断，我们之前的例子变得简单而整洁：
- en: '[PRE78]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Note that automatic shape inference is not the only thing that the `Layer`
    class’s `__call__()` method handles. It takes care of many more things, in particular
    routing between *eager* and *graph* execution, and input masking (which we cover
    in chapter 14). For now, just remember: when implementing your own layers, put
    the forward pass in the `call()` method.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，自动形状推断不是`Layer`类的`__call__()`方法处理的唯一事情。它还负责许多其他事情，特别是*急切*和*图*执行之间的路由，以及输入掩码（我们将在第14章中介绍）。现在，只需记住：当你实现自己的层时，将前向传递放在`call()`方法中。
- en: From layers to models
  id: totrans-428
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从层到模型
- en: A deep learning model is a graph of layers. In Keras, that’s the `Model` class.
    For now, you’ve only seen `Sequential` models (a subclass of `Model`), which are
    simple stacks of layers, mapping a single input to a single output. But as you
    move forward, you’ll be exposed to a much broader variety of network topologies.
    Some common ones are
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型是层的图。在Keras中，这是`Model`类。到目前为止，你只看到了`Sequential`模型（`Model`的子类），它们是简单的层堆叠，将单个输入映射到单个输出。但随着你的前进，你将接触到更广泛的各种网络拓扑。一些常见的是
- en: Two-branch networks
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双分支网络
- en: Multihead networks
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多头网络
- en: Residual connections
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残差连接
- en: Network topology can get quite involved. For instance, figure 3.5 shows topology
    of the graph of layers of a Transformer, a common architecture designed to process
    text data.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 网络拓扑可能相当复杂。例如，图3.5显示了Transformer层图的拓扑，这是一种常见的用于处理文本数据的架构。
- en: '![](../Images/268b0a6aaeb461be967b2cbfca279054.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/268b0a6aaeb461be967b2cbfca279054.png)'
- en: '[Figure 3.5](#figure-3-5): The Transformer architecture. There’s a lot going
    on here. Throughout the next few chapters, you’ll climb your way up to understanding
    it (in chapter 15).'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.5](#figure-3-5)：Transformer架构。这里有很多内容。在接下来的几章中，你将逐步了解它（在第15章）。'
- en: 'There are generally two ways of building such models in Keras: you can directly
    subclass the `Model` class, or you can use the Functional API, which lets you
    do more with less code. We’ll cover both approaches in chapter 7.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中通常有两种构建此类模型的方法：你可以直接子类化`Model`类，或者你可以使用功能API，这让你可以用更少的代码做更多的事情。我们将在第7章中介绍这两种方法。
- en: The topology of a model defines a *hypothesis space*. You may remember that
    in chapter 1, we described machine learning as “searching for useful representations
    of some input data, within a predefined *space of possibilities*, using guidance
    from a feedback signal.” By choosing a network topology, you constrain your space
    of possibilities (hypothesis space) to a specific series of tensor operations,
    mapping input data to output data. What you’ll then be searching for is a good
    set of values for the weight tensors involved in these tensor operations.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: To learn from data, you have to make assumptions about it. These assumptions
    define what can be learned. As such, the structure of your hypothesis space —
    the architecture of your model — is extremely important. It encodes the assumptions
    you make about your problem, the prior knowledge that the model starts with. For
    instance, if you’re working on a two-class classification problem with a model
    made of a single `Dense` layer with no activation (a pure affine transformation),
    you are assuming that your two classes are linearly separable.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: Picking the right network architecture is more an art than a science, and although
    there are some best practices and principles you can rely on, only practice can
    help you become a proper neural network architect. The next few chapters will
    both teach you explicit principles for building neural networks and help you develop
    intuition as to what works or doesn’t work for specific problems. You’ll build
    a solid intuition about what type of model architectures work for different kinds
    of problems, how to build these networks in practice, how to pick the right learning
    configuration, and how to tweak a model until it yields the results you want to
    see.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: 'The “compile” step: Configuring the learning process'
  id: totrans-440
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the model architecture is defined, you still have to choose three more
    things:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: '*Loss function (objective function)*  — The quantity that will be minimized
    during training. It represents a measure of success for the task at hand.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Optimizer*  — Determines how the network will be updated based on the loss
    function. It implements a specific variant of stochastic gradient descent (SGD).'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Metrics* — The measures of success you want to monitor during training and
    validation, such as classification accuracy. Unlike the loss, training will not
    optimize directly for these metrics. As such, metrics don’t need to be differentiable.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you’ve picked your loss, optimizer, and metrics, you can use the built-in
    `compile()` and `fit()` methods to start training your model. Alternatively, you
    can write your own custom training loops — we cover how to do this in chapter
    7\. It’s a lot more work! For now, let’s take a look at `compile()` and `fit()`.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: 'The `compile()` method configures the training process — you’ve already been
    introduced to it in your very first neural network example in chapter 2. It takes
    the arguments `optimizer`, `loss`, and `metrics` (a list):'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'In the previous call to `compile()`, we passed the optimizer, loss, and metrics
    as strings (such as `"rmsprop"`). These strings are actually shortcuts that get
    converted to Python objects. For instance, `"rmsprop"` becomes `keras.optimizers.RMSprop()`.
    Importantly, it’s also possible to specify these arguments as object instances,
    like this:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的`compile()`调用中，我们以字符串的形式传递了优化器、损失和度量标准（例如`"rmsprop"`）。实际上，这些字符串是快捷方式，会被转换为Python对象。例如，`"rmsprop"`变成了`keras.optimizers.RMSprop()`。重要的是，你也可以将这些参数指定为对象实例，如下所示：
- en: '[PRE80]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'This is useful if you want to pass your own custom losses or metrics or if
    you want to further configure the objects you’re using — for instance, by passing
    a `learning_rate` argument to the optimizer:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要传递你自己的自定义损失或度量标准，或者如果你想进一步配置你正在使用的对象——例如，通过传递`learning_rate`参数给优化器：
- en: '[PRE81]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'In chapter 7, we cover how to create custom losses and metrics. In general,
    you won’t have to create your own losses, metrics, or optimizers from scratch
    because Keras offers a wide range of built-in options that is likely to include
    what you need:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我们介绍了如何创建自定义损失和度量标准。一般来说，你不需要从头开始创建自己的损失、度量标准或优化器，因为Keras提供了一系列内置选项，很可能包含你所需要的内容：
- en: '*Optimizers*'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优化器*'
- en: '`SGD()` (with or without momentum)'
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SGD()`（带或不带动量）'
- en: '`RMSprop()`'
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RMSprop()`'
- en: '`Adam()`'
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Adam()`'
- en: Etc.
  id: totrans-457
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等。
- en: '*Losses*'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失函数*'
- en: '`CategoricalCrossentropy()`'
  id: totrans-459
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CategoricalCrossentropy()`'
- en: '`SparseCategoricalCrossentropy()`'
  id: totrans-460
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SparseCategoricalCrossentropy()`'
- en: '`BinaryCrossentropy()`'
  id: totrans-461
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BinaryCrossentropy()`'
- en: '`MeanSquaredError()`'
  id: totrans-462
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MeanSquaredError()`'
- en: '`KLDivergence()`'
  id: totrans-463
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KLDivergence()`'
- en: '`CosineSimilarity()`'
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CosineSimilarity()`'
- en: Etc.
  id: totrans-465
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等。
- en: '*Metrics*'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*度量标准*'
- en: '`CategoricalAccuracy()`'
  id: totrans-467
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CategoricalAccuracy()`'
- en: '`SparseCategoricalAccuracy()`'
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SparseCategoricalAccuracy()`'
- en: '`BinaryAccuracy()`'
  id: totrans-469
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BinaryAccuracy()`'
- en: '`AUC()`'
  id: totrans-470
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AUC()`'
- en: '`Precision()`'
  id: totrans-471
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Precision()`'
- en: '`Recall()`'
  id: totrans-472
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Recall()`'
- en: Etc.
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等。
- en: Throughout this book, you’ll see concrete applications of many of these options.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，你会看到许多这些选项的具体应用。
- en: Picking a loss function
  id: totrans-475
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择损失函数
- en: 'Choosing the right loss function for the right problem is extremely important:
    your network will take any shortcut it can to minimize the loss. So if the objective
    doesn’t fully correlate with success for the task at hand, your network will end
    up doing things you may not have wanted. Imagine a stupid, omnipotent AI trained
    via SGD, with this poorly chosen objective function: “Maximize the average well-being
    of all humans alive.” To make its job easier, this AI might choose to kill all
    humans except a few and focus on the well-being of the remaining ones because
    average well-being isn’t affected by how many humans are left. That might not
    be what you intended! Just remember that all neural networks you build will be
    just as ruthless in lowering their loss function, so choose the objective wisely,
    or you’ll have to face unintended side effects.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 为正确的问题选择正确的损失函数非常重要：你的网络将采取任何可能的捷径来最小化损失。所以如果目标与当前任务的成功不完全相关，你的网络最终会做你可能不希望的事情。想象一下，一个通过SGD训练的愚蠢、全能的AI，它选择了这个糟糕的目标函数：“最大化所有活着的人的平均幸福。”为了使它的任务更容易，这个AI可能会选择杀死除少数人之外的所有人，并专注于剩余人的幸福，因为平均幸福不受剩下多少人影响。这可能不是你想要的！记住，你构建的所有神经网络在降低损失函数方面都会同样无情，所以请明智地选择目标，否则你将不得不面对意想不到的副作用。
- en: Fortunately, when it comes to common problems such as classification, regression,
    and sequence prediction, there are simple guidelines you can follow to choose
    the correct loss. For instance, you’ll use binary crossentropy for a two-class
    classification problem, categorical crossentropy for a many-class classification
    problem, and so on. Only when you’re working on truly new research problems will
    you have to develop your own loss functions. In the next few chapters, we’ll detail
    explicitly which loss functions to choose for a wide range of common tasks.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '幸运的是，当涉及到分类、回归和序列预测等常见问题时，你可以遵循一些简单的指南来选择正确的损失函数。例如，对于二分类问题，你会使用二元交叉熵，对于多分类问题，你会使用分类交叉熵，等等。只有在你真正从事新的研究问题时，你才需要开发自己的损失函数。在接下来的几章中，我们将详细说明为各种常见任务选择哪些损失函数。 '
- en: Understanding the fit method
  id: totrans-478
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解fit方法
- en: After `compile()` comes `fit()`. The `fit` method implements the training loop
    itself. Its key arguments are
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 在`compile()`之后是`fit()`。`fit`方法实现了训练循环本身。它的关键参数包括
- en: The *data* (inputs and targets) to train on. It will typically be passed either
    in the form of NumPy arrays or a TensorFlow `Dataset` object. You’ll learn more
    about the `Dataset` API in the next chapters.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练的*数据*（输入和目标）。它通常以NumPy数组或TensorFlow `Dataset`对象的形式传递。你将在下一章中了解更多关于`Dataset`
    API的信息。
- en: 'The number of *epochs* to train for: how many times the training loop should
    iterate over the data passed.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练的*epoch*数量：训练循环应该遍历数据的次数。
- en: 'The batch size to use within each epoch of mini-batch gradient descent: the
    number of training examples considered to compute the gradients for one weight
    update step.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个epoch中用于迷你批梯度下降的批大小：用于计算一次权重更新步骤的梯度所需的训练样本数量。
- en: '[PRE82]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[Listing 3.34](#listing-3-34): Calling `fit` with NumPy data'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 3.34](#listing-3-34)：使用NumPy数据调用`fit`'
- en: 'The call to `fit` returns a `History` object. This object contains a `history`
    field, which is a dict mapping key, such as `"loss"` or specific metric names
    to the list of their per-epoch values:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit`的调用返回一个`History`对象。该对象包含一个`history`字段，它是一个字典，将键（如`"loss"`或特定的指标名称）映射到它们每个epoch的值列表：'
- en: '[PRE83]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Monitoring loss and metrics on validation data
  id: totrans-487
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控验证数据上的损失和指标
- en: The goal of machine learning is not to obtain models that perform well on the
    training data, which is easy — all you have to do is follow the gradient. The
    goal is to obtain models that perform well in general, particularly on data points
    that the model has never encountered before. Just because a model performs well
    on its training data doesn’t mean it will perform well on data it has never seen!
    For instance, it’s possible that your model could end up merely *memorizing* a
    mapping between your training samples and their targets, which would be useless
    for the task of predicting targets for data the model has never seen before. We’ll
    go over this point in much more detail in the chapter 5.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的目标不是获得在训练数据上表现良好的模型，这很容易——你只需要遵循梯度。目标是获得在一般情况下表现良好的模型，尤其是在模型之前从未遇到过的数据点上。仅仅因为模型在训练数据上表现良好，并不意味着它在从未见过的数据上也会表现良好！例如，你的模型可能最终只是*记忆*了训练样本与其目标之间的映射，这对于预测模型之前从未见过的数据的目标将是无用的。我们将在第5章中更详细地讨论这一点。
- en: 'To keep an eye on how the model does on new data, it’s standard practice to
    reserve a subset of the training data as “validation data”: you won’t be training
    the model on this data, but you will use it to compute a loss value and metrics
    value. You do this by using the `validation_data` argument in `fit()`. Like the
    training data, the validation data could be passed as NumPy arrays or as a TensorFlow
    `Dataset` object.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 为了监控模型在新数据上的表现，通常的做法是保留训练数据的一个子集作为“验证数据”：你不会在这个数据上训练模型，但你会用它来计算损失值和指标值。这是通过在`fit()`中使用`validation_data`参数来完成的。与训练数据一样，验证数据可以以NumPy数组或TensorFlow
    `Dataset`对象的形式传递。
- en: '[PRE84]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[Listing 3.35](#listing-3-35): Using the validation data argument'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 3.35](#listing-3-35)：使用验证数据参数'
- en: 'The value of the loss on the validation data is called the *validation loss*,
    to distinguish it from the *training loss*. Note that it’s essential to keep the
    training data and validation data strictly separate: the purpose of validation
    is to monitor whether what the model is learning is actually useful on new data.
    If any of the validation data has been seen by the model during training, your
    validation loss and metrics will be flawed.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 验证数据上的损失值被称为*验证损失*，以区别于*训练损失*。请注意，保持训练数据和验证数据严格分离是至关重要的：验证的目的是监控模型所学习的内容是否真正适用于新数据。如果在训练过程中模型已经看到了任何验证数据，你的验证损失和指标将会是错误的。
- en: 'If you want to compute the validation loss and metrics after training is complete,
    you can call the `evaluate` method:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在训练完成后计算验证损失和指标，可以调用`evaluate`方法：
- en: '`loss_and_metrics = model.evaluate(val_inputs, val_targets, batch_size=128)`'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss_and_metrics = model.evaluate(val_inputs, val_targets, batch_size=128)`'
- en: '`evaluate()` will iterate in batches (of size `batch_size`) over the data passed
    and return a list of scalars, where the first entry is the validation loss and
    the following entries are the validation metrics. If the model has no metrics,
    only the validation loss is returned (rather than a list).'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate()`将按批次（大小为`batch_size`）遍历传递的数据，并返回一个标量列表，其中第一个条目是验证损失，后面的条目是验证指标。如果没有指标，则只返回验证损失（而不是列表）。'
- en: 'Inference: Using a model after training'
  id: totrans-496
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理：训练后使用模型
- en: 'Once you’ve trained your model, you’re going to want to use it to make predictions
    on new data. This is called *inference*. To do this, a naive approach would simply
    be to `__call__` the model:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你训练好你的模型，你将想要用它来对新数据进行预测。这被称为*推理*。为此，一个简单的方法就是直接`__call__`模型：
- en: '[PRE85]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: However, this will process all inputs in `new_inputs` at once, which may not
    be feasible if you’re looking at a lot of data (in particular, it may require
    more memory than your GPU has).
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这会一次性处理`new_inputs`中的所有输入，如果你正在查看大量数据（特别是，可能需要比你的GPU更多的内存），这可能不可行。
- en: 'A better way to do inference is to use the `predict()` method. It will iterate
    over the data in small batches and return a NumPy array of predictions. And unlike
    `__call__`, it can also process TensorFlow `Dataset` objects:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 进行推理的更好方法是使用`predict()`方法。它将遍历数据的小批量，并返回一个包含预测的NumPy数组。而且与`__call__`不同，它还可以处理TensorFlow
    `Dataset`对象：
- en: '[PRE86]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'For instance, if we use `predict()` on some of our validation data with the
    linear model we trained earlier, we get scalar scores that correspond to the model’s
    prediction for each input sample:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们使用`predict()`对之前训练的线性模型的一些验证数据进行预测，我们得到的是与模型对每个输入样本预测相对应的标量分数：
- en: '[PRE87]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: For now, this is all you need to know about Keras models. At this point, you
    are ready to move on to solving real-world machine problems with Keras, in the
    next chapter.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，你只需要了解这么多关于Keras模型的知识。到目前为止，你已经准备好进入下一章，使用Keras解决现实世界的机器学习问题。
- en: Summary
  id: totrans-505
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: TensorFlow, PyTorch, and JAX are three popular low-level frameworks for numerical
    computation and autodifferentiation. They all have their own way of doing things
    and their own strengths and weaknesses.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow、PyTorch和JAX是三种流行的用于数值计算和自动微分的基础层框架。它们都有自己的做事方式、优势和劣势。
- en: Keras is a high-level API for building and training neural networks. It can
    be used with either TensorFlow, PyTorch, or JAX — just pick the backend you like
    best.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras是构建和训练神经网络的顶层API。它可以与TensorFlow、PyTorch或JAX一起使用——只需选择你最喜欢的后端即可。
- en: The central class of Keras is the `Layer`. A layer encapsulates some weights
    and some computation. Layers are assembled into models.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras的核心类是`Layer`。一个层封装了一些权重和一些计算。层被组装成模型。
- en: Before you start training a model, you need to pick an optimizer, a loss, and
    some metrics, which you specify via the `model.compile()` method.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开始训练模型之前，你需要选择一个优化器、一个损失函数和一些指标，你可以通过`model.compile()`方法指定它们。
- en: To train a model, you can use the `fit()` method, which runs mini-batch gradient
    descent for you. You can also use it to monitor your loss and metrics on validation
    data, a set of inputs that the model doesn’t see during training.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要训练一个模型，你可以使用`fit()`方法，它会为你运行小批量梯度下降。你还可以用它来监控验证数据上的损失和指标，这是一组模型在训练期间没有见过的输入。
- en: Once your model is trained, you can use the `model.predict()` method to generate
    predictions on new inputs.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练完成后，你可以使用`model.predict()`方法对新输入生成预测。
- en: Footnotes
  id: totrans-512
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚注
- en: R. E. Wengert, “A Simple Automatic Derivative Evaluation Program,” Communications
    of the ACM, 7 no. 8 (1964). [[↩]](#footnote-link-1)
  id: totrans-513
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: R. E. Wengert，“一个简单的自动导数评估程序”，ACM通讯，第7卷第8期（1964年）。[[↩]](#footnote-link-1)
- en: 'Note that PyTorch is a bit of an intermediate case: while it is mainly a lower-level
    framework, it also includes its own layers and its own optimizers. However, if
    you use PyTorch in conjunction with Keras, then you will only interact with low-level
    PyTorch APIs such as tensor operations. [[↩]](#footnote-link-2)'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，PyTorch是一个中间案例：虽然它主要是一个底层框架，但它也包括自己的层和自己的优化器。然而，如果你将PyTorch与Keras结合使用，那么你将只与低级的PyTorch
    API交互，例如张量操作。[[↩]](#footnote-link-2)
