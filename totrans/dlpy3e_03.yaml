- en: Introduction to TensorFlow, PyTorch, JAX, and Keras
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter03_introduction-to-ml-frameworks](https://deeplearningwithpython.io/chapters/chapter03_introduction-to-ml-frameworks)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This chapter is meant to give you everything you need to start doing deep learning
    in practice. First, you’ll get familiar with three popular deep learning frameworks
    that can be used with Keras:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow ([https://tensorflow.org](https://tensorflow.org))
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch ([https://pytorch.org/](https://pytorch.org/))
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JAX ([https://jax.readthedocs.io/](https://jax.readthedocs.io/))
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, building on top of the first contact you’ve had with Keras in chapter
    2, we’ll review the core components of neural networks and how they translate
    to Keras APIs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll be ready to move on to practical, real-world
    applications — which will start with chapter 4.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: A brief history of deep learning frameworks
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the real world, you’re not going to be writing low-level code from scratch
    like we did at the end of chapter 2\. Instead, you’re going to use a framework.
    Besides Keras, the main deep learning frameworks today are JAX, TensorFlow, and
    PyTorch. This book will teach you about all four.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: If you’re just getting started with deep learning, it may seem like all these
    frameworks have been here forever. In reality, they’re all quite recent, with
    Keras being the oldest among the four (launched in March 2015). The ideas behind
    these frameworks, however, have a long history — the first paper about automatic
    differentiation was published in 1964^([[1]](#footnote-1))
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'All these frameworks combine three key features:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: A way to compute gradients for arbitrary differentiable functions (automatic
    differentiation)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A way to run tensor computations on CPUs and GPUs (and possibly even on other
    specialized deep learning hardware)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A way to distribute computation across multiple devices or multiple computers,
    such as multiple GPUs on one computer, or even multiple GPUs across multiple separate
    computers
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, these three simple features unlock all modern deep learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: It took a long time for the field to develop robust solutions for all three
    problems and package those solutions in a reusable form. Since its inception in
    the 1960s and until the 2000s, autodifferentiation had no practical applications
    in machine learning — folks who worked with neural networks simply wrote their
    own gradient logic by hand, usually in a language like C++. Meanwhile, GPU programming
    was all but impossible.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Things started to slowly change in the late 2000s. First, Python and its ecosystem
    were slowly rising in popularity in the scientific community, gaining traction
    over MATLAB and C++. Second, NVIDIA released CUDA in 2006, unlocking the possibility
    of building neural networks that could run on consumer GPUs. The initial focus
    on CUDA was on physics simulation rather than machine learning, but that didn’t
    stop machine learning researchers from starting to implement CUDA-based neural
    networks from 2009 onward. They were typically one-off implementations that ran
    on a single GPU without any autodifferentiation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The first framework to enable autodifferentiation and GPU computation to train
    deep learning models was Theano, circa 2009\. Theano is the conceptual ancestor
    of all modern deep learning tools. It started getting good traction in the machine
    learning research community in 2013–2014, after the results of the ImageNet 2012
    competition ignited the world’s interest in deep learning. Around the same time,
    a few other GPU-enabled deep learning libraries started gaining popularity in
    the computer vision world — in particular, Torch 7 (Lua-based) and Caffe (C++-based).
    Keras launched in early 2015 as a higher-level, easier-to-use deep learning library
    powered by Theano, and it quickly gained traction with the few thousands of people
    who were into deep learning at the time.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Then in late 2015, Google launched TensorFlow, which took many of the key ideas
    from Theano and added support for large-scale distributed computation. The release
    of TensorFlow was a watershed moment that precipitated deep learning in the mainstream
    developer zeitgeist. Keras immediately added support for TensorFlow. By mid-2016,
    over half of all TensorFlow users were using it through Keras.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: In response to TensorFlow, Meta (named Facebook at the time) launched PyTorch
    about one year later, taking ideas from Chainer (a niche but innovative framework
    launched in mid-2015, now long dead) and NumPy-Autograd, a CPU-only autodifferentiation
    library for NumPy released by Maclaurin et al. in 2014\. Meanwhile, Google released
    TPUs as an alternative to GPUs, alongside XLA, a high-performance compiler developed
    to enable TensorFlow to run on TPUs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: A few years later, at Google, Matthew Johnson — one of the developers who worked
    on NumPy-Autograd — released JAX as an alternative way to use autodifferentiation
    with XLA. JAX quickly gained traction with researchers thanks to its minimalistic
    API and high scalability. Today, Keras, TensorFlow, PyTorch, and JAX are the top
    frameworks in the deep learning world.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Looking back on this chaotic history, we can ask, What’s next? Will a new framework
    arise tomorrow? Will we switch to a new programming language or a new hardware
    platform?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'If you ask me, three things today are certain:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Python has won. Its machine learning and data science ecosystem simply has too
    much momentum at this point. There won’t be a brand new language to replace it
    — at least not in the next 15 years.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’re in a multiframework world — all four frameworks are well established
    and are unlikely to go anywhere in the next few years. It’s a good idea for you
    to learn a little bit about each one. However, it’s highly possible that *new*
    frameworks will gain popularity in the future, in addition to them; Apple’s recently
    released MLX could be one such example. In this context, using Keras is a considerable
    advantage: you should be able to run your existing Keras models on any new up-and-coming
    framework via a new Keras backend. Keras will keep providing future-proof stability
    to machine learning developers in the future, like it has since 2015 — back when
    neither TensorFlow nor PyTorch nor JAX existed.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New chips may certainly arise in the future, alongside NVIDIA’s GPUs and Google’s
    TPUs. For instance, AMD’s GPU line likely has bright days ahead. But any new such
    chip will have to work with the existing frameworks to gain traction. New hardware
    is unlikely to disrupt your workflows.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How these frameworks relate to each other
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Keras, TensorFlow, PyTorch, and JAX don’t all have the same feature set and
    aren’t interchangeable. They have some overlap, but to a large extent, they serve
    different roles for different use cases. The biggest difference is between Keras
    and the three others. Keras is a high-level framework, while the others are lower
    level. Imagine building a house. Keras is like a prefabricated building kit: it
    provides a streamlined interface for setting up and training neural networks.
    In contrast, TensorFlow, PyTorch, and JAX are like the raw materials used in construction.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'As you saw in the previous chapters, training a neural network revolves around
    the following concepts:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '*First, low-level tensor manipulation* — The infrastructure that underlies
    all modern machine learning. This translates to low-level APIs found in TensorFlow,
    PyTorch^([[2]](#footnote-2)), and JAX:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tensors*, including special tensors that store the network’s state (*variables*)'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tensor operations* such as addition, `relu`, or `matmul`'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Backpropagation*, a way to compute the gradient of mathematical expressions'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Second, high-level deep learning concepts* — This translates to Keras APIs:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Layers*, which are combined into a *model*'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A *loss function*, which defines the feedback signal used for learning
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An *optimizer*, which determines how learning proceeds
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Metrics* to evaluate model performance, such as accuracy'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A *training loop* that performs mini-batch stochastic gradient descent
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Further, Keras is unique in that it isn’t a fully standalone framework. It needs
    a *backend engine* to run, (see figure 3.4), much like a prefabricated house-building
    kit needs to source building materials from somewhere. TensorFlow, PyTorch, and
    JAX can all be used as Keras backends. In addition, Keras can run on NumPy, but
    since NumPy does not provide an API for gradients, Keras workflows on NumPy are
    restricted to making predictions from a model — training is impossible.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have a clearer understanding of how all these frameworks came
    to be and how they relate to each other, let’s dive into what it’s like to work
    with them. We’ll cover them in chronological order: TensorFlow first, then PyTorch,
    and finally JAX.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to TensorFlow
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow is a Python-based open source machine learning framework developed
    primarily by Google. Its initial release was in November 2015, followed by a v1
    release in February 2017, and a v2 release in October 2019. TensorFlow is heavily
    used in production-grade machine learning applications across the industry.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to keep in mind that TensorFlow is more than a single library.
    It’s really a platform, home to a vast ecosystem of components, some developed
    by Google, some developed by third parties. For instance, there’s TFX for industry-strength
    machine learning workflow management, TF-Serving for production deployment, the
    TF Optimization Toolkit for model quantization and pruning, and TFLite and MediaPipe
    for mobile application deployment.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Together, these components cover a very wide range of use cases, from cutting-edge
    research to large-scale production applications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: First steps with TensorFlow
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Over the next paragraphs, you’ll get familiar with all the basics of TensorFlow.
    We’ll cover the following key concepts:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Tensors and variables
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numerical operations in TensorFlow
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing gradients with a `GradientTape`
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making TensorFlow functions fast by using just-in-time compilation
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ll then conclude the introduction with an end-to-end example: a pure-TensorFlow
    implementation of linear regression.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get those tensors flowing.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Tensors and variables in TensorFlow
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To do anything in TensorFlow, we’re going to need some tensors. There are a
    few different ways you can create them.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Constant tensors
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Tensors need to be created with some initial value, so common ways to create
    tensors are via `tf.ones` (equivalent to `np.ones`) and `tf.zeros` (equivalent
    to `np.zeros`). You can also create a tensor from Python or NumPy values using
    `tf.constant`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Listing 3.1](#listing-3-1): All-ones or all-zeros tensors'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Random tensors
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You can also create tensors filled with random values via one of the methods
    of the `tf.random` submodule (equivalent to the `np.random` submodule).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Listing 3.2](#listing-3-2): Random tensors'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Tensor assignment and the Variable class
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A significant difference between NumPy arrays and TensorFlow tensors is that
    TensorFlow tensors aren’t assignable: they’re constant. For instance, in NumPy,
    you can do the following.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Listing 3.3](#listing-3-3): NumPy arrays are assignable'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'Try to do the same thing in TensorFlow: you will get an error, `EagerTensor
    object does not support item assignment`.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Listing 3.4](#listing-3-4): TensorFlow tensors are not assignable'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: To train a model, we’ll need to update its state, which is a set of tensors.
    If tensors aren’t assignable, how do we do it, then? That’s where variables come
    in. `tf.Variable` is the class meant to manage modifiable state in TensorFlow.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: To create a variable, you need to provide some initial value, such as a random
    tensor.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Listing 3.5](#listing-3-5): Creating a `tf.Variable`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: The state of a variable can be modified via its `assign` method.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Listing 3.6](#listing-3-6): Assigning a value to a `Variable`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Assignment also works for a subset of the coefficients.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Listing 3.7](#listing-3-7): Assigning a value to a subset of a `Variable`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, `assign_add` and `assign_sub` are efficient equivalents of `+=` and
    `-=`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Listing 3.8](#listing-3-8): Using `assign_add`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Tensor operations: Doing math in TensorFlow'
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Just like NumPy, TensorFlow offers a large collection of tensor operations to
    express mathematical formulas. Here are a few examples.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Listing 3.9](#listing-3-9): A few basic math operations in TensorFlow'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an equivalent of the `Dense` layer we saw in chapter 2:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Gradients in TensorFlow: A second look at the GradientTape API'
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'So far, TensorFlow seems to look a lot like NumPy. But here’s something NumPy
    can’t do: retrieve the gradient of any differentiable expression with respect
    to any of its inputs. Just open a `GradientTape` scope, apply some computation
    to one or several input tensors, and retrieve the gradient of the result with
    respect to the inputs.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Listing 3.10](#listing-3-10): Using the `GradientTape`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'This is most commonly used to retrieve the gradients of the loss of a model
    with respect to its weights: `gradients = tape.gradient(loss, weights)`.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 2, you saw how the `GradientTape` works on either a single input
    or a list of inputs and how inputs could be either scalars or high-dimensional
    tensors.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: So far, you’ve only seen the case where the input tensors in `tape.gradient()`
    were TensorFlow variables. It’s actually possible for these inputs to be any arbitrary
    tensor. However, only *trainable variables* are being tracked by default. With
    a constant tensor, you’d have to manually mark it as being tracked, by calling
    `tape.watch()` on it.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[Listing 3.11](#listing-3-11): Using the `GradientTape` with constant tensor
    inputs'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Why? Because it would be too expensive to preemptively store the information
    required to compute the gradient of anything with respect to anything. To avoid
    wasting resources, the tape needs to know what to watch. Trainable variables are
    watched by default because computing the gradient of a loss with regard to a list
    of trainable variables is the most common use case of the gradient tape.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The gradient tape is a powerful utility, even capable of computing *second-order
    gradients* — that is, the gradient of a gradient. For instance, the gradient of
    the position of an object with regard to time is the speed of that object, and
    the second-order gradient is its acceleration.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: If you measure the position of a falling apple along a vertical axis over time,
    and find that it verifies `position(time) = 4.9 * time ** 2`, what is its acceleration?
    Let’s use two nested gradient tapes to find out.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Listing 3.12](#listing-3-12): Using nested gradient tapes to compute second-order
    gradients'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Making TensorFlow functions fast using compilation
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All the TensorFlow code you’ve written so far has been executing “eagerly.”
    This means operations are executed one after the other in the Python runtime,
    much like any Python code or NumPy code. Eager execution is great for debugging,
    but it is typically quite slow. It can often be beneficial to parallelize some
    computation, or “fuse” operations — replacing two consecutive operations, like
    `matmul` followed by `relu`, with a single, more efficient operation that does
    the same thing without materializing the intermediate output.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: This can be achieved via *compilation*. The general idea of compilation is to
    take certain functions you’ve written in Python, lift them out of Python, automatically
    rewrite them into a faster and more efficient “compiled program,” and then call
    that program from the Python runtime.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'The main benefit of compilation is improved performance. There’s a drawback
    too: the code you write is no longer the code that gets executed, which can make
    the debugging experience painful. Only turn on compilation after you’ve already
    debugged your code in the Python runtime.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'You can apply compilation to any TensorFlow function by wrapping it in a `tf.function`
    decorator, like this:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: When you do this, any call to `dense()` is replaced with a call to a compiled
    program that implements a more optimized version of the function. The first call
    to the function will take a bit longer, because TensorFlow will be compiling your
    code. This only happens once — all subsequent calls to the same function will
    be fast.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow has two compilation modes:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: First, the default one, which we refer to as “graph mode.” Any function decorated
    with `@tf.function` runs in graph mode.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Second, compilation with XLA, a high-performance compiler for ML (it’s short
    for Accelerated Linear Algebra). You can turn it on by specifying `jit_compile=True`,
    like this:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It is often the case that compiling a function with XLA will make it run faster
    than graph mode — though it takes more time to execute the function the first
    time, since the compiler has more work to do.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'An end-to-end example: A linear classifier in pure TensorFlow'
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You know about tensors, variables, and tensor operations, and you know how to
    compute gradients. That’s enough to build any TensorFlow-based machine learning
    model based on gradient descent. Let’s walk through an end-to-end example to make
    sure everything is crystal clear.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'In a machine learning job interview, you may be asked to implement a linear
    classifier from scratch: a very simple task that serves as a filter between candidates
    who have some minimal machine learning background, and those who don’t. Let’s
    get you past that filter, and use your newfound knowledge of TensorFlow to implement
    such a linear classifier.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s come up with some nicely linearly separable synthetic data to
    work with: two classes of points in a 2D plane.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Listing 3.13](#listing-3-13): Generating two classes of random points in a
    2D plane'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '`negative_samples` and `positive_samples` are both arrays with shape `(1000,
    2)`. Let’s stack them into a single array with shape `(2000, 2)`.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[Listing 3.14](#listing-3-14): Stacking the two classes into an array with
    shape `(2000, 2)`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Let’s generate the corresponding target labels, an array of 0s and 1s of shape
    `(2000, 1)`, where `targets[i, 0]` is 0 if `inputs[i]` belongs to class 0 (and
    inversely).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[Listing 3.15](#listing-3-15): Generating the corresponding targets (0 and
    1)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Let’s plot our data with Matplotlib, a well-known Python data visualization
    library (it comes preinstalled in Colab, so no need for you to install it yourself),
    as shown in figure 3.1.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[Listing 3.16](#listing-3-16): Plotting the two point classes'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ff4f19ade555d51b5dc7ea520e28a8b.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: '[Figure 3.1](#figure-3-1): Our synthetic data: two classes of random points
    in the 2D plane'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s create a linear classifier that can learn to separate these two blobs.
    A linear classifier is an affine transformation (`prediction = matmul(input, W)
    + b`) trained to minimize the square of the difference between predictions and
    the targets.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: As you’ll see, it’s actually a much simpler example than the end-to-end example
    of a toy two-layer neural network from the end of chapter 2\. However, this time,
    you should be able to understand everything about the code, line by line.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create our variables `W` and `b`, initialized with random values and with
    zeros, respectively.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Listing 3.17](#listing-3-17): Creating the linear classifier variables'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Here’s our forward pass function.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[Listing 3.18](#listing-3-18): The forward pass function'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Because our linear classifier operates on 2D inputs, `W` is really just two
    scalar coefficients: `W = [[w1], [w2]]`. Meanwhile, `b` is a single scalar coefficient.
    As such, for given input point `[x, y]`, its prediction value is `prediction =
    [[w1], [w2]] • [x, y] + b = w1 * x + w2 * y + b`.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Here’s our loss function.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[Listing 3.19](#listing-3-19): The mean squared error loss function'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Now, we move to the training step, which receives some training data and updates
    the weights `W` and `b` to minimize the loss on the data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[Listing 3.20](#listing-3-20): The training-step function'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, we’ll do *batch training* instead of *mini-batch training*:
    we’ll run each training step (gradient computation and weight update) on the entire
    data, rather than iterate over the data in small batches. On one hand, this means
    that each training step will take much longer to run, since we compute the forward
    pass and the gradients for 2,000 samples at once. On the other hand, each gradient
    update will be much more effective at reducing the loss on the training data,
    since it will encompass information from all training samples instead of, say,
    only 128 random samples. As a result, we will need many fewer steps of training,
    and we should use a larger learning rate than what we would typically use for
    mini-batch training (we’ll use `learning_rate = 0.1`, as previously defined).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[Listing 3.21](#listing-3-21): The batch training loop'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'After 40 steps, the training loss seems to have stabilized around 0.025. Let’s
    plot how our linear model classifies the training data points, as shown in figure
    3.2. Because our targets are 0s and 1s, a given input point will be classified
    as “0” if its prediction value is below 0.5, and as “1” if it is above 0.5:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/874d3f2bae456f9cb13b84e03bbe8466.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: '[Figure 3.2](#figure-3-2): Our model’s predictions on the training inputs:
    pretty similar to the training targets'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the prediction value for a given point `[x, y]` is simply `prediction
    == [[w1], [w2]] • [x, y] + b == w1 * x + w2 * y + b`. Thus, class “0” is defined
    as `w1 * x + w2 * y + b < 0.5` and class “1” is defined as `w1 * x + w2 * y +
    b > 0.5`. You’ll notice that what you’re looking at is really the equation of
    a line in the 2D plane: `w1 * x + w2 * y + b = 0.5`. Class 1 is above the line;
    class 0 is below the line. You may be used to seeing line equations in the format
    `y = a * x + b`; in the same format, our line becomes `y = - w1 / w2 * x + (0.5
    - b) / w2`.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot this line, as shown in figure 3.3:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](../Images/dfe1caf49e1828628a7c2c086715ca4d.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: '[Figure 3.3](#figure-3-3): Our model, visualized as a line'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'This is really what a linear classifier is all about: finding the parameters
    of a line (or, in higher-dimensional spaces, a hyperplane) neatly separating two
    classes of data.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: What makes the TensorFlow approach unique
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You’re now familiar with all the basic APIs that underlie TensorFlow-based workflows,
    and you’re about to dive into more frameworks — in particular, PyTorch and JAX.
    What makes working with TensorFlow different from working with any other framework?
    When should you use TensorFlow, and when could you use something else?
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'If you ask us, here are the main benefits of TensorFlow:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to graph mode and XLA compilation, it’s fast. It’s usually significantly
    faster than PyTorch and NumPy, though JAX is often even faster.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is extremely feature complete. Unique among all frameworks, it has support
    for string tensors as well as “ragged tensors” (tensors where different entries
    may have different dimensions — very useful for handling sequences without requiring
    to pad them to a shared length). It also has outstanding support for data preprocessing,
    via the highly performant `tf.data` API. `tf.data` is so good that even JAX recommends
    it for data preprocessing. Whatever you need to do, TensorFlow has a solution
    for it.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its ecosystem for production deployment is the most mature among all frameworks,
    especially when it comes to deploying on mobile or in the browser.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, TensorFlow also has some noticeable flaws:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: It has a sprawling API — the flipside of being very feature complete. TensorFlow
    includes thousands of different operations.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its numerical API is occasionally inconsistent with the NumPy API, making it
    a bit harder to approach if you’re already familiar with NumPy.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The popular pretrained model-sharing platform Hugging Face has less support
    for TensorFlow, which means that the latest generative AI models may not always
    be available in TensorFlow.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s move on to PyTorch.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to PyTorch
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch is a Python-based open source machine learning framework developed primarily
    by Meta (formerly Facebook) It was originally released in September 2016 (as a
    response to the release of TensorFlow), with its 1.0 version launched in 2018,
    and its 2.0 version launched in 2023. PyTorch inherits its programming style from
    the now-defunct Chainer framework, which was itself inspired by NumPy-Autograd.
    PyTorch is used extensively in the machine learning research community.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Like TensorFlow, PyTorch is at the center of a large ecosystem of related packages,
    such as `torchvision`, `torchaudio`, or the popular model-sharing platform Hugging
    Face.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'The PyTorch API is higher level than that of TensorFlow and JAX: it includes
    layers and optimizers, like Keras. These layers and optimizers are compatible
    with Keras workflows when you use Keras with the PyTorch backend.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: First steps with PyTorch
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Over the next paragraphs, you’ll get familiar with all the basics of PyTorch.
    We’ll cover the following key concepts:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Tensors and parameters
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numerical operations in PyTorch
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing gradients with the `backward()` method
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packaging computation with the `Module` class
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speeding up PyTorch by using compilation
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll conclude the introduction by reimplementing our linear regression end-to-end
    example in pure PyTorch.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Tensors and parameters in PyTorch
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A first gotcha about PyTorch is that the package isn’t named `pytorch`. It’s
    actually named `torch`. You’d install it via `pip install torch` and you’d import
    it via `import torch`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Like in NumPy and TensorFlow, the object at the heart of the framework is the
    tensor. First, let’s get our hands on some PyTorch tensors.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Constant tensors
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Here are some constant tensors.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[Listing 3.22](#listing-3-22): All-ones or all-zeros tensors'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Random tensors
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Random tensor creation is similar to NumPy and TensorFlow, but with divergent
    syntax. Consider the function `normal`: it doesn’t take a shape argument. Instead,
    the mean and standard deviation should be provided as PyTorch tensors with the
    expected output shape.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[Listing 3.23](#listing-3-23): Random tensors'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'As for creating a random uniform tensor, you’d do that via `torch.rand`. Unlike
    `np.random.uniform` or `tf.random.uniform`, the output shape should be provided
    as independent arguments for each dimension, like this:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Tensor assignment and the Parameter class
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Like NumPy arrays, but unlike TensorFlow tensors, PyTorch tensors are assignable.
    You can do operations like this:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: While you can just use a regular `torch.Tensor` to store the trainable state
    of a model, PyTorch does provide a specialized tensor subclass for that purpose,
    the `torch.nn.parameter.Parameter` class. Compared to a regular tensor, it provides
    semantic clarity — if you see a `Parameter`, you’ll know it’s a piece of trainable
    state, whereas a `Tensor` could be anything. As a result, it enables PyTorch to
    automatically track and retrieve the `Parameters` you assign to PyTorch models
    — similar to what Keras does with Keras `Variable` instances.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a `Parameter`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[Listing 3.24](#listing-3-24): Creating a PyTorch parameter'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Tensor operations: Doing math in PyTorch'
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Math in PyTorch works just the same as math in NumPy or TensorFlow, although
    much like TensorFlow, the PyTorch API often diverges in subtle ways from the NumPy
    API.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[Listing 3.25](#listing-3-25): A few basic math operations in PyTorch'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a dense layer:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Computing gradients with PyTorch
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There’s no explicit “gradient tape” in PyTorch. A similar mechanism does exist:
    when you run any computation in PyTorch, the framework creates a one-time computation
    graph (a “tape”) that records what just happened. However, that tape is hidden
    from the user. The public API for using it is at the level of tensors themselves:
    you can call `tensor.backward()` to run backpropagation through all operations
    previously executed that led to that tensor. Doing this will populate the `.grad`
    attribute of all tensors that are tracking gradients.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[Listing 3.26](#listing-3-26): Computing a gradient with `.backward()`'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'If you call `backward()` multiple times in a row, the `.grad` attribute will
    “accumulate” gradients: each new call will sum the new gradient with the preexisting
    one. For instance, in the following code, `input_var.grad` is not the gradient
    of `square(input_var)` with respect to `input_var`; rather, it is the sum of that
    gradient and the previously computed gradient — its value has doubled since our
    last code snippet:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To reset gradients, you can just set `.grad` to `None`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now let’s put this into practice!
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'An end-to-end example: A linear classifier in pure PyTorch'
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You now know enough to rewrite our linear classifier in PyTorch. It will stay
    very similar to the TensorFlow one — the only major difference is how we compute
    the gradients.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by creating our model variables. Don’t forget to pass `requires_grad=True`
    so we can compute gradients with respect to them:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This is our model — no difference so far. We just went from `tf.matmul` to
    `torch.matmul`:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This is our loss function. We just switch from `tf.square` to `torch.square`
    and from `tf.reduce_mean` to `torch.mean`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now for the training step. Here’s how it works:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '`loss.backward()` runs backpropagation starting from the `loss` output node
    and populates the `tensor.grad` attribute on all tensors that were involved in
    the computation of `loss`. `tensor.grad` represents the gradient of the loss with
    regard to that tensor.'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use the `.grad` attribute to recover the gradients of the loss with regard
    to `W` and `b`.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We update `W` and `b` using those gradients. Because these updates are not intended
    to be part of the backward pass, we do them inside a `torch.no_grad()` scope,
    which skips gradient computation for everything inside it.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We reset the contents of the `.grad` property of our `W` and `b` parameters,
    by setting it to `None`. If we didn’t do this, gradient values would accumulate
    across multiple calls to `training_step()`, resulting in invalid values:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This could be made even simpler — let’s see how.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Packaging state and computation with the Module class
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'PyTorch also has a higher-level, object-oriented API for performing backpropagation,
    which requires relying on two new classes: the `torch.nn.Module` class and an
    optimizer class from the `torch.optim` module, such as `torch.optim.SGD` (the
    equivalent of `keras.optimizers.SGD`).'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: The general idea is to define a subclass of `torch.nn.Module`, which will
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Hold some `Parameters`, to store state variables. Those are defined in the `__init__()`
    method.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the forward pass computation in the `forward()` method.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should look just like the following.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[Listing 3.27](#listing-3-27): Defining a `torch.nn.Module`'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now instantiate our `LinearModel`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'When using an instance of `torch.nn.Module`, rather than calling the `forward()`
    method directly, you’d use `__call__()` (i.e., directly call the model class on
    inputs), which redirects to `forward()` but adds a few framework hooks to it:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, let’s get our hands on a PyTorch optimizer. To instantiate it, you will
    need to provide the list of parameters that the optimizer is intended to update.
    You can retrieve it from our `Module` instance via `.parameters()`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Using our `Module` instance and the PyTorch `SGD` optimizer, we can run a simplified
    training step:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Previously, updating the model parameters looked like this:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Now we can just do `optimizer.step()`.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, previously we needed to reset parameter gradients by hand by doing
    `tensor.grad = None` on each one. Now we can just do `model.zero_grad()`.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this may feel a bit confusing — somehow the loss tensor, the optimizer,
    and the `Module` instance all seem to be aware of each other through some hidden
    background mechanism. They’re all interacting with one another via spooky action
    at a distance. Don’t worry though — you can just treat this sequence of steps
    (`loss.backward()` - `optimizer.step()` - `model.zero_grad()`) as a magic incantation
    to be recited any time you need to write a training step function. Just make sure
    not to forget `model.zero_grad()`. That would be a major bug (and it is unfortunately
    quite common)!
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Making PyTorch modules fast using compilation
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One last thing. Similarly to how TensorFlow lets you compile functions for better
    performance, PyTorch lets you compile functions or even `Module` instances via
    the `torch.compile()` utility. This API uses PyTorch’s very own compiler, named
    Dynamo.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try it on our linear regression `Module`:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The resulting object is intended to work identically to the original — except
    the forward and backward pass should run faster.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use `torch.compile()` as a function decorator:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In practice, most PyTorch code out there does not use compilation and simply
    runs eagerly, as the compiler may not always work with all models and may not
    always result in a speedup when it does work. Unlike in TensorFlow and Jax where
    compilation was built in from the inception of the library, PyTorch’s compiler
    is a relatively recent addition.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: What makes the PyTorch approach unique
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared to TensorFlow and JAX, which we will cover next, what makes PyTorch
    stand out? Why should you use it or not use it?
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are PyTorch’s two key strengths:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch code executes eagerly by default, making it easy to debug. Note that
    this is also the case for TensorFlow code and JAX code, but a big difference is
    that PyTorch is generally intended to be run eagerly at all times, whereas any
    serious TensorFlow or JAX project will inevitably need compilation at some point,
    which can significantly hurt the debugging experience.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The popular pretrained model-sharing platform Hugging Face has first-class support
    for PyTorch, which means that any model you’d like to use is likely available
    in PyTorch. This is the primary drive behind PyTorch adoption today.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meanwhile, there are also some downsides to using PyTorch:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Like with TensorFlow, the PyTorch API is inconsistent with NumPy. Further, it’s
    also internally inconsistent. For instance, the commonly used keyword `axis` is
    occasionally named `dim` instead, depending on the function. Some pseudo-random
    number generation operations take a `seed` argument; others don’t. And so on.
    This can make PyTorch frustrating to learn, especially when coming from NumPy.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to its focus on eager execution, PyTorch is quite slow — it’s the slowest
    of all the major frameworks by a large margin. For most models, you may see a
    20% or 30% speedup with JAX. For some models — especially large ones — you may
    even see a 3× or a 5× speedup with JAX, even after using `torch.compile()`.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While it is possible to make PyTorch code faster via `torch.compile()`, the
    PyTorch Dynamo compiler remains at this time (in 2025) quite ineffective and full
    of trapdoors. As a result, only a very small percentage of the PyTorch user base
    uses compilation. Perhaps this will be improved in future versions!
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to JAX
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX is an open source library for differentiable computation, primarily developed
    by Google. After its release in 2018, JAX quickly gained traction in the research
    community, particularly for its ability to use Google’s TPUs at scale. Today,
    JAX is in use by most of the top players in the generative AI space — companies
    like DeepMind, Apple, Midjourney, Anthropic, Cohere, and so on.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: JAX embraces a *stateless* approach to computation, meaning that functions in
    JAX do not maintain any persistent state. This contrasts with traditional imperative
    programming, where variables can hold values between function calls.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: The stateless nature of JAX functions has several advantages. In particular,
    it enables effective automatic parallelization and distributed computation, as
    functions can be executed independently without the need for synchronization.
    The extreme scalability of JAX is essential for handling the very large-scale
    machine learning problems faced by companies like Google and DeepMind.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: First steps with JAX
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll go over the following key concepts:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: The `array` class
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random operations in JAX
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numerical operations in JAX
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing gradients via `jax.grad` and `jax.value_and_grad`
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making JAX functions fast by leveraging just-in-time compilation
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Tensors in JAX
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the best features of JAX is that it doesn’t try to implement its own
    independent, similar-to-NumPy-but-slightly-divergent numerical API. Instead, it
    just implements the NumPy API, as is. It is available as the `jax.numpy` namespace,
    and you will often see it imported as `jnp` for short.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Here are some JAX arrays.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[Listing 3.28](#listing-3-28): All-ones or all-zeros tensors'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'There are, however, two minor differences between `jax.numpy` and the actual
    NumPy API: random number generation and array assignment. Let’s take a look.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Random number generation in JAX
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first difference between JAX and NumPy has to do with the way JAX handles
    random operations — what is known as “PRNG” (Pseudo-Random Number Generation)
    operations. We said earlier that JAX is *stateless*, which implies that JAX code
    can’t rely on any hidden global state. Consider the following NumPy code.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[Listing 3.29](#listing-3-29): Random tensors'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: How did the second call to `np.random.normal()` know to return a different value
    from the first call? That’s right — it’s a hidden piece of global state. You can
    actually retrieve that global state via `np.random.get_state()` and set it via
    `np.random.seed(seed)`.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: In a stateless framework, we can’t have any such global state. The same API
    call must always return the same value. As a result, in a stateless version of
    NumPy, you would have to rely on passing different seed arguments to your `np.random`
    calls to get different values.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it’s often the case that your PRNG calls are going to be in functions
    that get called multiple times and that are intended to use different random values
    each time. If you don’t want to rely on any global state, this requires you to
    manage your seed state outside of the target function, like this:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'It’s basically the same in JAX. However, JAX doesn’t use integer seeds. It
    uses special array structures called *keys*. You can create one from an integer
    value, like this:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'To force you to always provide a seed “key” to PRNG calls, all JAX PRNG-using
    operations take `key` (the random seed) as their first positional argument. Here’s
    how to use `random.normal()`:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Two calls to `random.normal()` that receive the same seed key will always return
    the same value.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[Listing 3.30](#listing-3-30): Using a random seed in Jax'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need a new seed key, you can simply create a new one from an existing
    one using the `jax.random.split()` function. It is deterministic, so the same
    sequence of splits will always result in the same final seed key:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This is definitely more work than `np.random`! But the benefits of statelessness
    far outweigh the costs: it makes your code *vectorizable* (i.e., the JAX compiler
    can automatically turn it into highly parallel code) while maintaining determinism
    (i.e., you can run the same code twice with the same results). That is impossible
    to achieve with a global PRNG state.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Tensor assignment
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The second difference between JAX and NumPy is tensor assignment. Like in TensorFlow,
    JAX arrays are not assignable in place. That’s because any sort of in-place modification
    would go against JAX’s stateless design. Instead, if you need to update a tensor,
    you must create a new tensor with the desired value. JAX makes this easy by providing
    the `at()`/`set()` API. These methods allow you to create a new tensor with an
    updated element at a specific index. Here’s an example of how you would update
    the first element of a JAX array to a new value.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[Listing 3.31](#listing-3-31): Modifying values in a JAX array'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Simple enough!
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'Tensor operations: Doing math in JAX'
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Doing math in JAX looks exactly the same as it does in NumPy. No need to learn
    anything new this time!
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[Listing 3.32](#listing-3-32): A few basic math operations in JAX'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a dense layer:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Computing gradients with JAX
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Unlike TensorFlow and PyTorch, JAX takes a *metaprogramming* approach to gradient
    computation. Metaprogramming refers to the idea of having *functions that return
    functions* — you could call them “meta-functions.” In practice, JAX lets you *turn
    a loss-computation function into a gradient-computation function*. So computing
    gradients in JAX is a three-step process:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Define a loss function, `compute_loss()`.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call `grad_fn = jax.grad(compute_loss)` to retrieve a gradient-computation function.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call `grad_fn` to retrieve the gradient values.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The loss function should verify the following properties:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: It should return a scalar loss value.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its first argument (which, in the following example, is also the only argument)
    should contain the state arrays we need gradients for. This argument is usually
    named `state`. For instance, this first argument could be a single array, a list
    of arrays, or a dict of arrays.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a look at a simple example. Here’s a loss-computation function that
    takes a single scalar, `input_var` and returns a scalar loss value — just the
    square of the input:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We can now call the JAX utility `jax.grad()` on this loss function. It returns
    a gradient-computation function — a function that takes the same arguments as
    the original loss function and returns the gradient of the loss with respect to
    `input_var`:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Once you’ve obtained `grad_fn()`, you can call it with the same arguments as
    `compute_loss()`, and it will return gradients arrays corresponding to the first
    argument of `compute_loss()`. In our case, our first argument was a single array,
    so `grad_fn()` directly returns the gradient of the loss with respect to that
    one array:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: JAX gradient-computation best practices
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So far so good! Metaprogramming is a big word, but it turns out to be quite
    simple. Now, in real-world use cases, there are a few more things you’ll need
    to take into account. Let’s take a look.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Returning the loss value
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'It’s usually the case that you don’t just need the gradient array; you also
    need the loss value. It would be quite inefficient to recompute it independently
    outside of `grad_fn()`, so instead, you can just configure your `grad_fn()` to
    also return the loss value. This is done by using the JAX utility `jax.value_and_grad()`
    instead of `jax.grad()`. It works identically, but it returns a tuple of values,
    where the first entry is the loss value, and the second entry is the gradient(s):'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Getting gradients for a complex function
  id: totrans-338
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now, what if you need gradients for more than a single variable? And what if
    your `compute_loss()` function has more than one input?
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say your state contains three variables, `a`, `b`, and `c`, and your
    loss function has two inputs, `x` and `y`. You would simply structure it like
    this:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Note that `state` doesn’t have to be a tuple — it could be a dict, a list, or
    any nested structure of tuples, dicts, and lists. In JAX parlance, such a nested
    structure is called a *tree*.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Returning auxiliary outputs
  id: totrans-343
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Finally, what if your `compute_loss()` function needs to return more than just
    the loss? Let’s say you want to return an additional value `output` that’s computed
    as a by-product of the loss computation. How to get it out?
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'You would use the `has_aux` argument:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Edit the loss function to return a tuple where the first entry is the loss,
    and the second entry is your extra output.
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pass the argument `has_aux=True` to `value_and_grad()`. This tells `value_and_grad()`
    to return not just the gradient but also the “auxiliary” output(s) of `compute_loss()`,
    like this:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Admittedly, things are starting to be pretty convoluted at this point. Don’t
    worry, though; this is about as hard as JAX gets! Almost everything else is simpler
    by comparison.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Making JAX functions fast with @jax.jit
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One more thing. As a JAX user, you will frequently use the `@jax.jit` decorator,
    which behaves identically to the `@tf.function(jit_compile=True)` decorator. It
    turns any stateless JAX function into an XLA-compiled piece of code, typically
    delivering a considerable execution speedup:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Be mindful that you can only decorate a stateless function — any tensors that
    get updated by the function should be part of its return values.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'An end-to-end example: A linear classifier in pure JAX'
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now you know enough JAX to write the JAX version of our linear classifier example.
    There are two major differences from the TensorFlow and PyTorch versions you’ve
    already seen:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: All functions we will create will be *stateless*. That means the state (the
    arrays `W` and `b`) will be provided as function arguments, and if they get modified
    by the function, their new value will be returned by the function.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradients are computed using the JAX `value_and_grad()` utility.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s get started. The model function and the mean squared error function should
    look familiar:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'To compute gradients, we need to package loss computation in a single `compute_loss()`
    function. It returns the total loss as a scalar, and it takes `state` as its first
    argument — a tuple of all the tensors we need gradients for:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Calling `jax.value_and_grad()` on this function gives us a new function, with
    the same argument as `compute_loss`, which returns both the loss and the gradients
    of the loss with regard to the elements of `state`:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Next, we can set up our training step function. It looks straightforward. Be
    mindful that, unlike its TensorFlow and PyTorch equivalents, it needs to be stateless,
    and so it must return the updated values of the `W` and `b` tensors:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Because we won’t change the `learning_rate` during our example, we can consider
    it part of the function itself and not our model’s state. If we wanted to modify
    our learning rate during training, we’d need to pass it through as well.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we’re ready to run the full training loop. We initialize `W` and `b`,
    and we repeatedly update them via stateless calls to `training_step()`:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: That’s it! You’re now able to write a custom training loop in JAX.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: What makes the JAX approach unique
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main thing that makes JAX unique among modern machine learning frameworks
    is its functional, stateless philosophy. While it may seem to cause friction at
    first, it is what unlocks the power of JAX — its ability to compile to extremely
    fast code and to scale to arbitrarily large models and arbitrarily many devices.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s a lot to like about JAX:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: It’s fast. For most models, it is the fastest of all frameworks you’ve seen
    so far.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its numerical API is fully consistent with NumPy, making it pleasant to learn.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s the best fit for training models on TPUs, as it was developed from the
    ground up for XLA and TPUs.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using JAX can also come with some amount of developer friction:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Its use of metaprogramming and compilation can make it significantly harder
    to debug compared to pure eager execution.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low-level training loops tend to be more verbose and more difficult to write
    than in TensorFlow or PyTorch.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At this point, you know the basics of TensorFlow, PyTorch, and JAX, and you
    can use these frameworks to implement a basic linear classifier from scratch.
    That’s a solid foundation to build upon. It’s now time to move on to a more productive
    path to deep learning: the Keras API.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Keras
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keras is a deep learning API for Python that provides a convenient way to define
    and train any kind of deep learning model. It was released in March 2015, with
    its v2 in 2017 and its v3 in 2023.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Keras users range from academic researchers, engineers, and data scientists
    at both startups and large companies to graduate students and hobbyists. Keras
    is used at Google, Netflix, Uber, YouTube, CERN, NASA, Yelp, Instacart, Square,
    Waymo, YouTube, and thousands of smaller organizations working on a wide range
    of problems across every industry. Your YouTube recommendations originate from
    Keras models. The Waymo self-driving cars rely on Keras models for processing
    sensor data. Keras is also a popular framework on Kaggle, the machine learning
    competition website.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Because Keras has a diverse user base, it doesn’t force you to follow a single
    “true” way of building and training models. Rather, it enables a wide range of
    different workflows, from the very high-level to the very low-level, corresponding
    to different user profiles. For instance, you have an array of ways to build models
    and an array of ways to train them, each representing a certain tradeoff between
    usability and flexibility. In chapter 7, we’ll review in detail a good fraction
    of this spectrum of workflows.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: First steps with Keras
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we get to writing Keras code, there are a few things to consider when
    setting up the library before it’s imported.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Picking a backend framework
  id: totrans-386
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Keras can be used together with JAX, TensorFlow, or PyTorch. They’re the “backend
    frameworks” of Keras. Through these backend frameworks, Keras can run on top of
    different types of hardware (see figure 3.4) — GPU, TPU, or plain CPU — can be
    seamlessly scaled to thousands of machines, and can be deployed to a variety of
    platforms.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/552c279c016c6f17a2d903764c737676.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
- en: '[Figure 3.4](#figure-3-4): Keras and its backends. A backend is a low-level
    tensor-computing platform; Keras is a high-level deep learning API.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: 'Backend frameworks are pluggable: you can switch to a different backend framework
    *after* you’ve written some Keras code. You aren’t locked into a single framework
    and a single ecosystem — you can move your models from JAX to TensorFlow to PyTorch
    depending on your current needs. For instance, when you develop a Keras model,
    you could debug it with PyTorch, train it on TPU with JAX for maximum efficiency,
    and finally run inference with the excellent tooling from the TensorFlow ecosystem.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'The default backend for Keras right now is TensorFlow, so if you run `import
    keras` in a fresh environment, without having configured anything, you will be
    running on top of TensorFlow. There are two ways to pick a different backend:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the environment variable `KERAS_BACKEND`. Before you start your `python`
    repl, you can run the following shell command to use JAX as your Keras backend:
    `export KERAS_BACKEND=jax`. Alternatively, you can add the following code snippet
    at the top of your Python file or notebook (note that it must imperatively go
    before the first `import keras`):'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Edit your local Keras configuration file at `~/.keras/keras.json`. If you have
    already imported Keras once, this file has already been created with default settings.
    You can use any text editor to open and modify it — it’s a human-readable JSON
    file. It should look like this:'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Now, you may ask, which backend should I be picking? It’s really your own choice:
    all Keras code examples in the rest of the book will be compatible with all three
    backends. If the need for backend-specific code arises (as in chapter 7, for instance),
    I will show you all three versions — TensorFlow, PyTorch, JAX. If you have no
    particular backend preference, my personal recommendation is JAX. It’s usually
    the most performant backend.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Once your backend is configured, you can start actually building and training
    Keras models. Let’s take a look.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: 'Layers: The building blocks of deep learning'
  id: totrans-398
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The fundamental data structure in neural networks is the *layer*, to which
    you were introduced in chapter 2\. A layer is a data processing module that takes
    as input one or more tensors and that outputs one or more tensors. Some layers
    are stateless, but more frequently layers have a state: the layer’s *weights*,
    one or several tensors learned with stochastic gradient descent, which together
    contain the network’s *knowledge*.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: Different types of layers are appropriate for different tensor formats and different
    types of data processing. For instance, simple vector data, stored in 2D tensors
    of shape `(samples, features)`, is often processed by *densely connected* layers,
    also called *fully connected* or *dense* layers (the `Dense` class in Keras).
    Sequence data, stored in 3D tensors of shape `(samples, timesteps, features)`,
    is typically processed by *recurrent* layers, such as an `LSTM` layer, or 1D convolution
    layers (`Conv1D`). Image data, stored in rank-4 tensors, is usually processed
    by 2D convolution layers (`Conv2D`).
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: You can think of layers as the LEGO bricks of deep learning, a metaphor that
    is made explicit by Keras. Building deep learning models in Keras is done by clipping
    together compatible layers to form useful data transformation pipelines.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: The base `Layer` class in Keras
  id: totrans-402
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A simple API should have a single abstraction around which everything is centered.
    In Keras, that’s the `Layer` class. Everything in Keras is either a `Layer` or
    something that closely interacts with a `Layer`.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: A `Layer` is an object that encapsulates some state (weights) and some computation
    (a forward pass). The weights are typically defined in a `build()` (although they
    could also be created in the constructor `__init__()`), and the computation is
    defined in the `call()` method.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we implemented a `NaiveDense` class that contained
    two weights `W` and `b` and applied the computation `output = activation(matmul(input,
    W) + b)`. The following is what the same layer would look like in Keras.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[Listing 3.33](#listing-3-33): A simple dense layer from scratch in Keras'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll cover in detail the purpose of these `build()` and
    `call()` methods. Don’t worry if you don’t understand everything just yet!
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: 'Once instantiated, a layer like this can be used just like a function, taking
    as input a tensor:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Now, you’re probably wondering, why did we have to implement `call()` and `build()`,
    since we ended up using our layer by plainly calling it, that is to say, by using
    its `__call__` method? It’s because we want to be able to create the state just
    in time. Let’s see how that works.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'Automatic shape inference: Building layers on the fly'
  id: totrans-412
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Just like with LEGO bricks, you can only “clip” together layers that are *compatible*.
    The notion of *layer compatibility* here refers specifically to the fact that
    every layer will only accept input tensors of a certain shape and will return
    output tensors of a certain shape. Consider the following example:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: This layer will return a tensor whose non-batch dimension is 32\. It can only
    be connected to a downstream layer that expects 32-dimensional vectors as its
    input.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 'When using Keras, you don’t have to worry about size compatibility most of
    the time because the layers you add to your models are dynamically built to match
    the shape of the incoming inputs. For instance, suppose you write the following:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The layers didn’t receive any information about the shape of their inputs. Instead,
    they automatically inferred their input shape as being the shape of the first
    inputs they see.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'In the toy version of a `Dense` layer that we’ve implemented in chapter 2,
    we had to pass the layer’s input size explicitly to the constructor in order to
    be able to create its weights. That’s not ideal, because it would lead to models
    that look like this, where each new layer needs to be made aware of the shape
    of the layer before it:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: It would be even worse when the rules used by a layer to produce its output
    shape are complex. For instance, what if our layer returned outputs of shape `(batch,
    input_size * 2 if input_size % 2 == 0 else input_size * 3)`?
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: If we were to reimplement our `NaiveDense` layer as a Keras layer capable of
    automatic shape inference, it would look like the `SimpleDense` layer, with its
    `build()` and `call()` methods.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Keras `SimpleDense`, we no longer create weights in the constructor
    like in the previous example. Instead, we create them in a dedicated state-creation
    method `build()`, which receives as argument the first input shape seen by the
    layer. The `build()` method is called automatically the first time the layer is
    called (via its `__call__()` method). In fact, that’s why we defined the computation
    in a separate `call()` method rather than in the `__call__()` method directly!
    The `__call__()` method of the base layer schematically looks like this:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'With automatic shape inference, our previous example becomes simple and neat:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Note that automatic shape inference is not the only thing that the `Layer`
    class’s `__call__()` method handles. It takes care of many more things, in particular
    routing between *eager* and *graph* execution, and input masking (which we cover
    in chapter 14). For now, just remember: when implementing your own layers, put
    the forward pass in the `call()` method.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: From layers to models
  id: totrans-428
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A deep learning model is a graph of layers. In Keras, that’s the `Model` class.
    For now, you’ve only seen `Sequential` models (a subclass of `Model`), which are
    simple stacks of layers, mapping a single input to a single output. But as you
    move forward, you’ll be exposed to a much broader variety of network topologies.
    Some common ones are
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: Two-branch networks
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multihead networks
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Residual connections
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network topology can get quite involved. For instance, figure 3.5 shows topology
    of the graph of layers of a Transformer, a common architecture designed to process
    text data.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/268b0a6aaeb461be967b2cbfca279054.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
- en: '[Figure 3.5](#figure-3-5): The Transformer architecture. There’s a lot going
    on here. Throughout the next few chapters, you’ll climb your way up to understanding
    it (in chapter 15).'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: 'There are generally two ways of building such models in Keras: you can directly
    subclass the `Model` class, or you can use the Functional API, which lets you
    do more with less code. We’ll cover both approaches in chapter 7.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: The topology of a model defines a *hypothesis space*. You may remember that
    in chapter 1, we described machine learning as “searching for useful representations
    of some input data, within a predefined *space of possibilities*, using guidance
    from a feedback signal.” By choosing a network topology, you constrain your space
    of possibilities (hypothesis space) to a specific series of tensor operations,
    mapping input data to output data. What you’ll then be searching for is a good
    set of values for the weight tensors involved in these tensor operations.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: To learn from data, you have to make assumptions about it. These assumptions
    define what can be learned. As such, the structure of your hypothesis space —
    the architecture of your model — is extremely important. It encodes the assumptions
    you make about your problem, the prior knowledge that the model starts with. For
    instance, if you’re working on a two-class classification problem with a model
    made of a single `Dense` layer with no activation (a pure affine transformation),
    you are assuming that your two classes are linearly separable.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: Picking the right network architecture is more an art than a science, and although
    there are some best practices and principles you can rely on, only practice can
    help you become a proper neural network architect. The next few chapters will
    both teach you explicit principles for building neural networks and help you develop
    intuition as to what works or doesn’t work for specific problems. You’ll build
    a solid intuition about what type of model architectures work for different kinds
    of problems, how to build these networks in practice, how to pick the right learning
    configuration, and how to tweak a model until it yields the results you want to
    see.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: 'The “compile” step: Configuring the learning process'
  id: totrans-440
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the model architecture is defined, you still have to choose three more
    things:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: '*Loss function (objective function)*  — The quantity that will be minimized
    during training. It represents a measure of success for the task at hand.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Optimizer*  — Determines how the network will be updated based on the loss
    function. It implements a specific variant of stochastic gradient descent (SGD).'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Metrics* — The measures of success you want to monitor during training and
    validation, such as classification accuracy. Unlike the loss, training will not
    optimize directly for these metrics. As such, metrics don’t need to be differentiable.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you’ve picked your loss, optimizer, and metrics, you can use the built-in
    `compile()` and `fit()` methods to start training your model. Alternatively, you
    can write your own custom training loops — we cover how to do this in chapter
    7\. It’s a lot more work! For now, let’s take a look at `compile()` and `fit()`.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: 'The `compile()` method configures the training process — you’ve already been
    introduced to it in your very first neural network example in chapter 2. It takes
    the arguments `optimizer`, `loss`, and `metrics` (a list):'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'In the previous call to `compile()`, we passed the optimizer, loss, and metrics
    as strings (such as `"rmsprop"`). These strings are actually shortcuts that get
    converted to Python objects. For instance, `"rmsprop"` becomes `keras.optimizers.RMSprop()`.
    Importantly, it’s also possible to specify these arguments as object instances,
    like this:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'This is useful if you want to pass your own custom losses or metrics or if
    you want to further configure the objects you’re using — for instance, by passing
    a `learning_rate` argument to the optimizer:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'In chapter 7, we cover how to create custom losses and metrics. In general,
    you won’t have to create your own losses, metrics, or optimizers from scratch
    because Keras offers a wide range of built-in options that is likely to include
    what you need:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '*Optimizers*'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SGD()` (with or without momentum)'
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RMSprop()`'
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Adam()`'
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Etc.
  id: totrans-457
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Losses*'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CategoricalCrossentropy()`'
  id: totrans-459
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SparseCategoricalCrossentropy()`'
  id: totrans-460
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BinaryCrossentropy()`'
  id: totrans-461
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MeanSquaredError()`'
  id: totrans-462
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KLDivergence()`'
  id: totrans-463
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CosineSimilarity()`'
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Etc.
  id: totrans-465
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Metrics*'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CategoricalAccuracy()`'
  id: totrans-467
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SparseCategoricalAccuracy()`'
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BinaryAccuracy()`'
  id: totrans-469
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AUC()`'
  id: totrans-470
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Precision()`'
  id: totrans-471
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Recall()`'
  id: totrans-472
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Etc.
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout this book, you’ll see concrete applications of many of these options.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: Picking a loss function
  id: totrans-475
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Choosing the right loss function for the right problem is extremely important:
    your network will take any shortcut it can to minimize the loss. So if the objective
    doesn’t fully correlate with success for the task at hand, your network will end
    up doing things you may not have wanted. Imagine a stupid, omnipotent AI trained
    via SGD, with this poorly chosen objective function: “Maximize the average well-being
    of all humans alive.” To make its job easier, this AI might choose to kill all
    humans except a few and focus on the well-being of the remaining ones because
    average well-being isn’t affected by how many humans are left. That might not
    be what you intended! Just remember that all neural networks you build will be
    just as ruthless in lowering their loss function, so choose the objective wisely,
    or you’ll have to face unintended side effects.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, when it comes to common problems such as classification, regression,
    and sequence prediction, there are simple guidelines you can follow to choose
    the correct loss. For instance, you’ll use binary crossentropy for a two-class
    classification problem, categorical crossentropy for a many-class classification
    problem, and so on. Only when you’re working on truly new research problems will
    you have to develop your own loss functions. In the next few chapters, we’ll detail
    explicitly which loss functions to choose for a wide range of common tasks.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the fit method
  id: totrans-478
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After `compile()` comes `fit()`. The `fit` method implements the training loop
    itself. Its key arguments are
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: The *data* (inputs and targets) to train on. It will typically be passed either
    in the form of NumPy arrays or a TensorFlow `Dataset` object. You’ll learn more
    about the `Dataset` API in the next chapters.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The number of *epochs* to train for: how many times the training loop should
    iterate over the data passed.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The batch size to use within each epoch of mini-batch gradient descent: the
    number of training examples considered to compute the gradients for one weight
    update step.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[Listing 3.34](#listing-3-34): Calling `fit` with NumPy data'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: 'The call to `fit` returns a `History` object. This object contains a `history`
    field, which is a dict mapping key, such as `"loss"` or specific metric names
    to the list of their per-epoch values:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Monitoring loss and metrics on validation data
  id: totrans-487
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of machine learning is not to obtain models that perform well on the
    training data, which is easy — all you have to do is follow the gradient. The
    goal is to obtain models that perform well in general, particularly on data points
    that the model has never encountered before. Just because a model performs well
    on its training data doesn’t mean it will perform well on data it has never seen!
    For instance, it’s possible that your model could end up merely *memorizing* a
    mapping between your training samples and their targets, which would be useless
    for the task of predicting targets for data the model has never seen before. We’ll
    go over this point in much more detail in the chapter 5.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: 'To keep an eye on how the model does on new data, it’s standard practice to
    reserve a subset of the training data as “validation data”: you won’t be training
    the model on this data, but you will use it to compute a loss value and metrics
    value. You do this by using the `validation_data` argument in `fit()`. Like the
    training data, the validation data could be passed as NumPy arrays or as a TensorFlow
    `Dataset` object.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[Listing 3.35](#listing-3-35): Using the validation data argument'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: 'The value of the loss on the validation data is called the *validation loss*,
    to distinguish it from the *training loss*. Note that it’s essential to keep the
    training data and validation data strictly separate: the purpose of validation
    is to monitor whether what the model is learning is actually useful on new data.
    If any of the validation data has been seen by the model during training, your
    validation loss and metrics will be flawed.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to compute the validation loss and metrics after training is complete,
    you can call the `evaluate` method:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: '`loss_and_metrics = model.evaluate(val_inputs, val_targets, batch_size=128)`'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: '`evaluate()` will iterate in batches (of size `batch_size`) over the data passed
    and return a list of scalars, where the first entry is the validation loss and
    the following entries are the validation metrics. If the model has no metrics,
    only the validation loss is returned (rather than a list).'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: 'Inference: Using a model after training'
  id: totrans-496
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once you’ve trained your model, you’re going to want to use it to make predictions
    on new data. This is called *inference*. To do this, a naive approach would simply
    be to `__call__` the model:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你训练好你的模型，你将想要用它来对新数据进行预测。这被称为*推理*。为此，一个简单的方法就是直接`__call__`模型：
- en: '[PRE85]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: However, this will process all inputs in `new_inputs` at once, which may not
    be feasible if you’re looking at a lot of data (in particular, it may require
    more memory than your GPU has).
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这会一次性处理`new_inputs`中的所有输入，如果你正在查看大量数据（特别是，可能需要比你的GPU更多的内存），这可能不可行。
- en: 'A better way to do inference is to use the `predict()` method. It will iterate
    over the data in small batches and return a NumPy array of predictions. And unlike
    `__call__`, it can also process TensorFlow `Dataset` objects:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 进行推理的更好方法是使用`predict()`方法。它将遍历数据的小批量，并返回一个包含预测的NumPy数组。而且与`__call__`不同，它还可以处理TensorFlow
    `Dataset`对象：
- en: '[PRE86]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'For instance, if we use `predict()` on some of our validation data with the
    linear model we trained earlier, we get scalar scores that correspond to the model’s
    prediction for each input sample:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们使用`predict()`对之前训练的线性模型的一些验证数据进行预测，我们得到的是与模型对每个输入样本预测相对应的标量分数：
- en: '[PRE87]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: For now, this is all you need to know about Keras models. At this point, you
    are ready to move on to solving real-world machine problems with Keras, in the
    next chapter.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，你只需要了解这么多关于Keras模型的知识。到目前为止，你已经准备好进入下一章，使用Keras解决现实世界的机器学习问题。
- en: Summary
  id: totrans-505
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: TensorFlow, PyTorch, and JAX are three popular low-level frameworks for numerical
    computation and autodifferentiation. They all have their own way of doing things
    and their own strengths and weaknesses.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow、PyTorch和JAX是三种流行的用于数值计算和自动微分的基础层框架。它们都有自己的做事方式、优势和劣势。
- en: Keras is a high-level API for building and training neural networks. It can
    be used with either TensorFlow, PyTorch, or JAX — just pick the backend you like
    best.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras是构建和训练神经网络的顶层API。它可以与TensorFlow、PyTorch或JAX一起使用——只需选择你最喜欢的后端即可。
- en: The central class of Keras is the `Layer`. A layer encapsulates some weights
    and some computation. Layers are assembled into models.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras的核心类是`Layer`。一个层封装了一些权重和一些计算。层被组装成模型。
- en: Before you start training a model, you need to pick an optimizer, a loss, and
    some metrics, which you specify via the `model.compile()` method.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开始训练模型之前，你需要选择一个优化器、一个损失函数和一些指标，你可以通过`model.compile()`方法指定它们。
- en: To train a model, you can use the `fit()` method, which runs mini-batch gradient
    descent for you. You can also use it to monitor your loss and metrics on validation
    data, a set of inputs that the model doesn’t see during training.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要训练一个模型，你可以使用`fit()`方法，它会为你运行小批量梯度下降。你还可以用它来监控验证数据上的损失和指标，这是一组模型在训练期间没有见过的输入。
- en: Once your model is trained, you can use the `model.predict()` method to generate
    predictions on new inputs.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练完成后，你可以使用`model.predict()`方法对新输入生成预测。
- en: Footnotes
  id: totrans-512
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚注
- en: R. E. Wengert, “A Simple Automatic Derivative Evaluation Program,” Communications
    of the ACM, 7 no. 8 (1964). [[↩]](#footnote-link-1)
  id: totrans-513
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: R. E. Wengert，“一个简单的自动导数评估程序”，ACM通讯，第7卷第8期（1964年）。[[↩]](#footnote-link-1)
- en: 'Note that PyTorch is a bit of an intermediate case: while it is mainly a lower-level
    framework, it also includes its own layers and its own optimizers. However, if
    you use PyTorch in conjunction with Keras, then you will only interact with low-level
    PyTorch APIs such as tensor operations. [[↩]](#footnote-link-2)'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，PyTorch是一个中间案例：虽然它主要是一个底层框架，但它也包括自己的层和自己的优化器。然而，如果你将PyTorch与Keras结合使用，那么你将只与低级的PyTorch
    API交互，例如张量操作。[[↩]](#footnote-link-2)
