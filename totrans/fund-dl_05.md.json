["```py\nconda install pytorch torchvision torchaudio -c pytorch\n\n```", "```py\nconda install pytorch torchvision torchaudio cpuonly -c pytorch\n\n```", "```py\nimport torch\n\n```", "```py\narr = [1,2]\ntensor = torch.tensor(arr)\nval = 2.0\ntensor = torch.tensor(val)\n\n```", "```py\nimport numpy as np\nnp_arr = np.array([1,2])\nx_t = torch.from_numpy(np_arr)\n\n```", "```py\nzeros_t = torch.zeros((2,3)) # Returns 2x3 tensor of zeros\nones_t = torch.ones((2,3)) # Returns 2x3 tensor of ones\nrand_t = torch.randn((2,3)) # Returns 2x3 tensor of random numbers\n\n```", "```py\nzeros_t.shape # Returns torch.Size([2, 3])\n\n```", "```py\nx_t = torch.tensor(2.0)\nx_t.dtype # Returns torch.float32\n\n```", "```py\narr = [1,2]\nx_t = torch.tensor(arr, dtype=torch.float32)\n\n```", "```py\nx_t.device # Returns device(type='cpu') by default\n\n```", "```py\n# PyTorch will use GPU if it's available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\narr = [1,2]\nx_t = torch.tensor(arr, dtype=torch.float32, device=device)\n\n```", "```py\nx_t = x_t.to(device, dtype=torch.int)\n\n```", "```py\nc = 10\nx_t = x_t*c\n\n```", "```py\nx1_t = torch.zeros((1,2))\nx2_t = torch.ones((1,2))\nx1_t + x2_t\n# returns tensor([[1., 1.]])\n\n```", "```py\nx1_t = torch.tensor([[1,2],[3,4]])\nx2_t = torch.tensor([[1,2,3],[4,5,6]])\ntorch.matmul(x1_t, x2_t) # Returns tensor([[9,12,15],[19,26,33]])\n\n```", "```py\ni,j,k = 0,1,1\nx3_t = torch.tensor([[[3,7,9],[2,4,5]],[[8,6,2],[3,9,1]]])\nprint(x3_t)\n# out:\n# tensor([[[3, 7, 9],\n#\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[2, 4, 5]],\n#\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[[8, 6, 2],\n#\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[3, 9, 1]]])\n\nx3_t[i,j,k]\n# out:\n# tensor(4)\n\n```", "```py\nx3_t[0] # Returns the matrix at position 0 in tensor\nx3_t[0,:,:] # Also returns the matrix at position 0 in tensor!\n# out:\n# tensor([[3, 7, 9],\n#\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[2, 4, 1]])\n\n```", "```py\nx3_t[0,1:3,:]\n# returns tensor([[2, 4, 5]])\n\n```", "```py\nx3_t[0,1,2] = 1\n\n# out:\n# tensor([[[3, 7, 9],\n#\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[2, 4, 1]],\n\n#\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[[8, 6, 2],\n#\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[3, 9, 1]]])\n\n```", "```py\nx_t = torch.randn(2,3,4)\nsub_tensor = torch.randn(2,4)\nx_t[0,1:3,:] = sub_tensor\n\n```", "```py\nx_t[0,1:3,:] = 1\nsub_tensor = torch.randn(1,4)\nx_t[0,1:3,:] = sub_tensor\n\n```", "```py\nx = torch.tensor(2.0, requires_grad=True)\ny = torch.tensor(3.0, requires_grad=True)\nz = torch.tensor(1.5, requires_grad=True)\nf = x**2+y**2+z**2\nf.backward()\nx.grad, y.grad, z.grad\n# out:\n# (tensor(4.), tensor(6.), tensor(3.))\n\n```", "```py\n import torch.nn as nn\n\n```", "```py\nin_dim, out_dim = 256, 10\nvec = torch.randn(256)\nlayer = nn.Linear(in_dim, out_dim, bias=True)\nout = layer(vec)\n\n```", "```py\nW = torch.rand(10,256)\nb = torch.zeros(10,1)\nout = torch.matmul(W, vec) + b\n\n```", "```py\nin_dim, feature_dim, out_dim = 784, 256, 10\nvec = torch.randn(784)\nlayer1 = nn.Linear(in_dim, feature_dim, bias=True)\nlayer2 = nn.Linear(feature_dim, out_dim, bias=True)\nout = layer2(layer1(vec))\n\n```", "```py\nrelu = nn.ReLU()\nout  = layer2(relu(layer1(vec)))\n\n```", "```py\nclass BaseClassifier(nn.Module):\n\u00a0\u00a0def __init__(self, in_dim, feature_dim, out_dim):\n\u00a0\u00a0\u00a0\u00a0super(BaseClassifier, self).__init__()\n\u00a0\u00a0\u00a0\u00a0self.layer1 = nn.Linear(in_dim, feature_dim, bias=True)\n\u00a0\u00a0\u00a0\u00a0self.layer2 = nn.Linear(feature_dim, out_dim, bias=True)\n\u00a0\u00a0\u00a0\u00a0self.relu = nn.ReLU()\n\n\u00a0\u00a0def forward(self, x):\n\u00a0\u00a0\u00a0\u00a0x = self.layer1(x)\n\u00a0\u00a0\u00a0\u00a0x = self.relu(x)\n\u00a0\u00a0\u00a0\u00a0out = self.layer2(x)\n\u00a0\u00a0\u00a0\u00a0return out\n\n```", "```py\nno_examples = 10\nin_dim, feature_dim, out_dim = 784, 256, 10\nx = torch.randn((no_examples, in_dim))\nclassifier = BaseClassifier(in_dim, feature_dim, out_dim)\nout = classifier(x)\n\n```", "```py\nloss = nn.CrossEntropyLoss()\ntarget = torch.tensor([0,3,2,8,2,9,3,7,1,6])\ncomputed_loss = loss(out, target)\ncomputed_loss.backward()\n\n```", "```py\nfor p in classifier.parameters():\n\u00a0\u00a0print(p.shape)\n\n# out:\n# torch.Size([256, 784])\n# torch.Size([256])\n# torch.Size([10, 256])\n# torch.Size([10])\n\n```", "```py\nfrom torch import optim\n\nlr = 1e-3\noptimizer = optim.SGD(classifier.parameters(), lr=lr) \n\n```", "```py\noptimizer.step() # Updates parameters via SGD\noptimizer.zero_grad() # Zeroes out gradients between minibatches\n\n```", "```py\nimport os\nfrom PIL import Image\nfrom torchvision import transforms\n\nclass ImageDataset(Dataset):\n\u00a0\u00a0def __init__(self, img_dir, label_file):\n\u00a0\u00a0\u00a0\u00a0super(ImageDataset, self).__init__()\n\u00a0\u00a0\u00a0\u00a0self.img_dir = img_dir\n\u00a0\u00a0\u00a0\u00a0self.labels = torch.tensor(np.load(label_file, allow_pickle=True))\n\u00a0\u00a0\u00a0\u00a0self.transforms = transforms.ToTensor()\n\n\u00a0\u00a0def __getitem__(self, idx):\n\u00a0\u00a0\u00a0\u00a0img_pth = os.path.join(self.img_dir, \"img_{}.jpg\".format(idx))\n\u00a0\u00a0\u00a0\u00a0img = Image.open(img_pth)\n\u00a0\u00a0\u00a0\u00a0img = self.transforms(img).flatten()\n\u00a0\u00a0\u00a0\u00a0label = self.labels[idx]\n\u00a0\u00a0\u00a0\u00a0return {\"data\":img, \"label\":label}\n\n\u00a0\u00a0def __len__(self):\n\u00a0\u00a0\u00a0\u00a0return len(self.labels)\n\n```", "```py\ntrain_dataset = ImageDataset(img_dir='./data/train/',\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0label_file='./data/train/labels.npy')\n\ntrain_loader = DataLoader(train_dataset,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0batch_size=4,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0shuffle=True)\n\n```", "```py\nfor minibatch in train_loader:\n\u00a0\u00a0data, labels = minibatch['data'], minibatch['label']\n\u00a0\u00a0print(data)\n\u00a0\u00a0print(labels)\n\n```", "```py\nfor minibatch in train_loader:\n\u00a0\u00a0data, labels = minibatch['data'], minibatch['label']\n\u00a0\u00a0out = classifier(data) # to be completed in the next section!\n\n```", "```py\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import optim\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\n\nclass BaseClassifier(nn.Module):\n\u00a0\u00a0def __init__(self, in_dim, feature_dim, out_dim):\n\u00a0\u00a0\u00a0\u00a0super(BaseClassifier, self).__init__()\n\u00a0\u00a0\u00a0\u00a0self.classifier = nn.Sequential(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(in_dim, feature_dim, bias=True),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.ReLU(),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(feature_dim, out_dim, bias=True)\n\u00a0\u00a0\u00a0\u00a0)\n\n\u00a0\u00a0def forward(self, x):\n\u00a0\u00a0\u00a0\u00a0return self.classifier(x)\n\n# Load in MNIST dataset from PyTorch\ntrain_dataset = MNIST(\".\", train=True,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0download=True, transform=ToTensor())\ntest_dataset = MNIST(\".\", train=False,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0download=True, transform=ToTensor())\ntrain_loader = DataLoader(train_dataset,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0batch_size=64, shuffle=False)\n\n```", "```py\n# Instantiate model, optimizer, and hyperparameter(s)\nin_dim, feature_dim, out_dim = 784, 256, 10\nlr=1e-3\nloss_fn = nn.CrossEntropyLoss()\nepochs=40\nclassifier = BaseClassifier(in_dim, feature_dim, out_dim)\noptimizer = optim.SGD(classifier.parameters(), lr=lr)\n\ndef train(classifier=classifier,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0optimizer=optimizer,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0epochs=epochs,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0loss_fn=loss_fn):\n\n\u00a0\u00a0classifier.train()\n\u00a0\u00a0loss_lt = []\n\u00a0\u00a0for epoch in range(epochs):\n\u00a0\u00a0\u00a0\u00a0running_loss = 0.0\n\u00a0\u00a0\u00a0\u00a0for minibatch in train_loader:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0data, target = minibatch\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0data = data.flatten(start_dim=1)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0out = classifier(data)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0computed_loss = loss_fn(out, target)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0computed_loss.backward()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0optimizer.step()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0optimizer.zero_grad()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Keep track of sum of loss of each minibatch\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0running_loss += computed_loss.item()\n\u00a0\u00a0\u00a0\u00a0loss_lt.append(running_loss/len(train_loader))\n\u00a0\u00a0\u00a0\u00a0print(\"Epoch: {} train loss: {}\".format(epoch+1, \n\u00a0         running_loss/len(train_loader)))\n\n\u00a0\u00a0plt.plot([i for i in range(1,epochs+1)], loss_lt)\n\u00a0\u00a0plt.xlabel(\"Epoch\")\n\u00a0\u00a0plt.ylabel(\"Training Loss\")\n\u00a0\u00a0plt.title(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"MNIST Training Loss: optimizer {}, lr {}\".format(\"SGD\", lr))\n\u00a0\u00a0plt.show()\n\n\u00a0\u00a0# Save state to file as checkpoint\n\u00a0\u00a0torch.save(classifier.state_dict(), 'mnist.pt')\n\ndef test(classifier=classifier,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0loss_fn = loss_fn):\n\u00a0\u00a0classifier.eval()\n\u00a0\u00a0accuracy = 0.0\n\u00a0\u00a0computed_loss = 0.0\n\n\u00a0\u00a0with torch.no_grad():\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0for data, target in test_loader:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0data = data.flatten(start_dim=1)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0out = classifier(data)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0_, preds = out.max(dim=1)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Get loss and accuracy\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0computed_loss += loss_fn(out, target)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0accuracy += torch.sum(preds==target)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0print(\"Test loss: {}, test accuracy: {}\".format(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0computed_loss.item()/(len(test_loader)*64), \n\u00a0         accuracy*100.0/(len(test_loader)*64)))\n\n```"]