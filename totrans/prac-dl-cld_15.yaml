- en: Intersection over Union
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In an object detection training pipeline, metrics such as Intersection over
    Union (IoU) are typically used as a threshold value to filter detections of a
    certain quality. To calculate the IoU, we use both the predicted bounding box
    and the ground truth bounding box to calculate two different numbers. The first
    is the area where both the prediction and the ground truth overlap—the intersection.
    The second is the span covered by both the prediction and the ground truth—the
    union. As the name suggests, we simply divide the total area of the intersection
    by the area of the union to get the IoU. [Figure 14-11](part0017.html#a_visual_representation_of_the_iou_ratio)
    visually demonstrates the IoU concept for two 2x2 squares that intersect in a
    single 1x1 square in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: '![A visual representation of the IoU ratio](../images/00007.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-11\. A visual representation of the IoU ratio
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the perfect scenario in which the predicted bounding box matches the ground
    truth exactly, the IoU value would be 1\. In the worst-case scenario, the prediction
    would have no overlap with the ground truth and the IoU value would be 0\. As
    we can see, the IoU value would range between 0 and 1, with higher numbers indicating
    higher-quality predictions, as illustrated in [Figure 14-12](part0017.html#iou_illustratedsemicolon_predictions_fro).
  prefs: []
  type: TYPE_NORMAL
- en: To help us filter and report results, we would set a minimum IoU threshold value.
    Setting the threshold to a really aggressive value (such as 0.9) would result
    in a loss of a lot of predictions that might turn out to be important further
    down the pipeline. Conversely, setting the threshold value too low would result
    in too many spurious bounding boxes. A minimum IoU of 0.5 is typically used for
    reporting the precision of an object detection model.
  prefs: []
  type: TYPE_NORMAL
- en: '![IoU illustrated; predictions from better models tend to have heavier overlap
    with the ground truth, resulting in a higher IoU](../images/00293.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-12\. IoU illustrated; predictions from better models tend to have
    heavier overlap with the ground truth, resulting in a higher IoU
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s worth reiterating here that the IoU value is calculated per instance of
    a category, not per image. However, to calculate the quality of the detector over
    a larger set of images, we would use IoU in conjunction with other metrics such
    as Average Precision.
  prefs: []
  type: TYPE_NORMAL
- en: Mean Average Precision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While reading research papers on object detection, we often come across numbers
    such as AP@0.5\. That term conveys the average precision at IoU=0.5\. Another
    more elaborate representation would be AP@[0.6:0.05:0.75], which is the average
    precision from IoU=0.6 to IoU=0.75 at increments of 0.05\. The Mean Average Precision
    (or mAP) is simply the average precision across all categories. For the COCO challenge,
    the MAP metric used is AP@[0.5:0.05:0.95].
  prefs: []
  type: TYPE_NORMAL
- en: Non-Maximum Suppression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Internally, object detection algorithms make a number of proposals for the potential
    locations of objects that might be in the image. For each object in the image,
    it is expected to have multiple of these bounding box proposals at varying confidence
    values. Our task is to find the one proposal that best represents the real location
    of the object. A naïve way would be to consider only the proposal with the maximum
    confidence value. This approach might work if there were only one object in the
    entire image. But this won’t work if there are multiple categories, each with
    multiple instances in a single image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-Maximum Suppression (NMS) comes to the rescue ([Figure 14-13](part0017.html#using_nms_to_find_the_bounding_box_that)).
    The key idea behind NMS is that two instances of the same object will not have
    heavily overlapping bounding boxes; for instance, the IoU of their bounding boxes
    will be less than a certain IoU threshold (say 0.5). A greedy way to achieve this
    is by repeating the following steps for each category:'
  prefs: []
  type: TYPE_NORMAL
- en: Filter out all proposals under a minimum confidence threshold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accept the proposal with the maximum confidence value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For all remaining proposals sorted in descending order of their confidence values,
    check whether the IoU of the current box with respect to one of the previously
    accepted proposals ≥ 0.5\. If so, filter it; otherwise, accept it as a proposal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Using NMS to find the bounding box that best represents the location of the
    object in an image](../images/00223.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 14-13\. Using NMS to find the bounding box that best represents the location
    of the object in an image
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Using the TensorFlow Object Detection API to Build Custom Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we run through a full end-to-end example of building an object
    detector. We look at several steps in the process, including collecting, labeling,
    and preprocessing data, training the model, and exporting it to the TensorFlow
    Lite format.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s look at some strategies for collecting data.
  prefs: []
  type: TYPE_NORMAL
- en: Data Collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We know by now that, in the world of deep learning, data reigns supreme. There
    are a few ways in which we can acquire data for the object that we want to detect:'
  prefs: []
  type: TYPE_NORMAL
- en: Using ready-to-use datasets
  prefs: []
  type: TYPE_NORMAL
- en: There are a few public datasets available to train object detection models such
    as MS COCO (80 categories), ImageNet (200 categories), Pascal VOC (20 categories),
    and the newer Open Images (600 categories). MS COCO and Pascal VOC are used in
    most object detection benchmarks, with COCO-based benchmarks being more realistic
    for real-world scenarios due to more complex imagery.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading from the web
  prefs: []
  type: TYPE_NORMAL
- en: We should already be familiar with this process from [Chapter 12](part0014.html#DB7S3-13fa565533764549a6f0ab7f11eed62b)
    in which we collected images for the “Hot Dog” and “Not Hot Dog” classes. Browser
    extensions such as Fatkun come in handy for quickly downloading a large number
    of images from search engine results.
  prefs: []
  type: TYPE_NORMAL
- en: Taking pictures manually
  prefs: []
  type: TYPE_NORMAL
- en: This is the more time consuming (but potentially fun) option. For a robust model,
    it’s important to train it with pictures taken in a variety of different settings.
    With the object to detect in hand, we should take at least 100 to 150 images of
    it against different backgrounds, with a variety of compositions (framing) and
    angles, and in many different lighting conditions. [Figure 14-14](part0017.html#photographs_of_objects_taken_in_a_variet)
    shows some examples of pictures taken to train a Coke and Pepsi object detection
    model. Considering that a model can potentially learn the spurious correlation
    that red means Coke and blue means Pepsi, it’s a good idea to mix objects with
    backgrounds that could potentially confuse it so that eventually we realize a
    more robust model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Photographs of objects taken in a variety of different settings to train
    an object detector model](../images/00212.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-14\. Photographs of objects taken in a variety of different settings
    to train an object detector model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s worth taking pictures of the object in environments that it’s unlikely
    to be used in order to diversify the dataset and improve the robustness of the
    model. It also has the added bonus of keeping an otherwise boring and tedious
    process a little entertaining. We can make it interesting by challenging ourselves
    to come up with unique and innovative photos. [Figure 14-15](part0017.html#some_creative_photographs_taken_during_t)
    shows some creative examples of photos taken during the process of building a
    currency detector.
  prefs: []
  type: TYPE_NORMAL
- en: '![Some creative photographs taken during the process of building a diverse
    currency dataset](../images/00178.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-15\. Some creative photographs taken during the process of building
    a diverse currency dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Because we want to make a cat detector, we will be reusing the cat images from
    the “Cats and Dogs” Kaggle dataset that we used in [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b).
    We randomly chose images from that set and split them into a train and test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'To maintain uniformity of input size to our network, we resize all images to
    a fixed size. We will use the same size as part of our network definition and
    while converting to a *.tflite* model. We can use the ImageMagick tool to resize
    all images at once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you have a large number of images (i.e., several tens of thousands of images)
    in the current directory, the preceding command might fail to enlist all of the
    images. A solution would be to use the `find` command to list all of the images
    and pipe the output to the `mogrify` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Labeling the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After we have our data, the next step would be to label it. Unlike classification
    where simply placing an image in the correct directory would be sufficient, we
    need to manually draw the bounding rectangles for the objects of interest. Our
    tool of choice for this task is LabelImg (available for Windows, Linux, and Mac)
    for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: You can save annotations either as XML files in PASCAL VOC format (also adopted
    by ImageNet) or the YOLO format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports single and multiclass labels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you are the curious cat like us, you’d want to know what the difference
    is between the YOLO format and the PASCAL VOC format. YOLO happens to use simple
    *.txt* files, one per image with the same name, to store information about the
    classes and the bounding boxes within the image. The following is what a *.txt*
    file in YOLO format typically looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note that the x, y, width, and height in this example are normalized using the
    full width and height of the image.
  prefs: []
  type: TYPE_NORMAL
- en: PASCAL VOC format, on the other hand, is XML based. Similar to the YOLO format,
    we use one XML file per image (ideally with the same name). You can view the sample
    format on the Git repository at *code/chapter-14/pascal-voc-sample.xml*.
  prefs: []
  type: TYPE_NORMAL
- en: First, download the application from [GitHub](https://oreil.ly/jH31E) to a directory
    on your computer. This directory will have an executable and a data directory
    containing some sample data. Because we will be using our own custom data, we
    don’t need the data within the provided data directory. You can start the application
    by double-clicking the executable file.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we should have a collection of images that is ready to be used
    during the training process. We first divide our data randomly into training and
    test sets and place them in separate directories. We can do this split by simply
    dragging and dropping the images randomly into either directory. After the train
    and test directories are created, we load the training directory by clicking Open
    Dir, as shown in [Figure 14-16](part0017.html#click_the_open_dir_button_and_then_selec).
  prefs: []
  type: TYPE_NORMAL
- en: '![Click the Open Dir button and then select the directory that contains the
    training data](../images/00133.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-16\. Click the Open Dir button and then select the directory that
    contains the training data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After LabelImg loads the training directory, we can start our labeling process.
    We must go through each image and manually draw bounding boxes for each object
    (only cats in our case), as shown in [Figure 14-17](part0017.html#select_the_create_rectbox_from_the_panel).
    After we make the bounding box, we are prompted to provide a label to accompany
    the bounding box. For this exercise, enter the name of the object, “cat”. After
    a label is entered, we need only to select the checkbox to specify that label
    again for subsequent images. For images with multiple objects, we would make multiple
    bounding boxes and add their corresponding labels. If there are different kinds
    of objects, we simply add a new label for that object class.
  prefs: []
  type: TYPE_NORMAL
- en: '![Select the Create RectBox from the panel on the left to make a bounding box
    that covers the cat](../images/00068.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-17\. Select the Create RectBox from the panel on the left to make
    a bounding box that covers the cat
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, repeat this step for all of the training and test images. We want to obtain
    tight bounding boxes for each object, such that all parts of the object are bounded
    by the box. Finally, in the train and test directories, we see *.xml* files accompanying
    each image file, as depicted in [Figure 14-18](part0017.html#each_image_is_accompanied_by_an_xml_file).
    We can open the .`xml` file in a text editor and inspect metadata such as image
    file name, bounding box coordinates, and label name.
  prefs: []
  type: TYPE_NORMAL
- en: '![Each image is accompanied by an XML file that contains the label information
    and the bounding box information](../images/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-18\. Each image is accompanied by an XML file that contains the label
    information and the bounding box information
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Preprocessing the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, we have handy XML data that gives the bounding boxes for all
    of our objects. However, for us to use TensorFlow for training on the data, we
    must preprocess it into a format that TensorFlow understands, namely TFRecords.
    Before we can convert our data into TFRecords, we must go through the intermediate
    step of consolidating the data from all the XML files into a single comma-separated
    values (CSV) file. TensorFlow provides helper scripts that assist us with these
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our environment set up, it’s time to begin doing some real
    work:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Convert our cats dataset directory to a single CSV file using the `xml_to_csv`
    tool from the [racoon_dataset](https://oreil.ly/k8QGl) repository by Dat Tran.
    We have provided a slightly edited copy of this file in our repository at *code/chapter-14/xml_to_csv.py*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Do the same for the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make the *label_map.pbtxt* file. This file contains the label and identifier
    mappings for all of our classes. We use this file to convert our text label into
    an integer identifier, which is what TFRecord expects. Because we have only one
    class, the file is rather small and looks as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Generate the TFRecord format files that will contain the data to be used for
    training and testing our model later on. This file is also from the [racoon_dataset](https://oreil.ly/VNwwE)
    repository by Dat Tran. We have provided a slightly edited copy of this file in
    our repository at *code/chapter-14/generate_tfrecord.py*. (It’s worth noting that
    the `image_dir` path in the argument should be the same as the path in the XML
    files; LabelImg uses absolute paths.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With our *train.tfrecord* and *test.tfrecord* files ready, we can now begin
    our training process.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: (This section is informational only and is not necessary for the training process.
    You can skip ahead to [“Training”](part0017.html#training-id00003) if you want
    to.)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can inspect our model using the `saved_model_cli` tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of that script looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is how we can interpret the output from the previous command:'
  prefs: []
  type: TYPE_NORMAL
- en: The shape of `inputs` being `(-1, -1, -1, 3)` indicates that the trained model
    can accept any arbitrarily sized image input that contains three channels. Due
    to this inherent flexibility in input size, the converted model would be larger
    than if the model had a fixed-size input. When we train our own object detector
    later in this chapter, we will be fixing the input size to 800 x 600 pixels to
    make the resulting model more compact.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Moving on to the outputs, the very first output is `detection_boxes (-1, 100,
    4)`. This tells us how many detection boxes are possible, and what will they look
    like. In particular, the very first number (i.e., -1) indicates that we can have
    any number of boxes based on the detections for all 100 classes (second number)
    with four coordinates (third number)—x, y, width, and height—defining each box.
    In other words, we are not limiting the number of detections for each of the 100
    classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For `detection_classes`, we have a list of two elements: the first defining
    how many objects we detected, and the second, a one-hot encoded vector with the
    detected class enabled.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`num_detections` is the number of objects detected in the image. It is a single
    floating point.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`raw_detection_boxes` defines the coordinates of each box that was detected
    for each object. All of these detections are before the application of NMS on
    all the detections.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`raw_detection_scores` is a list of two floating-point numbers, the first describing
    the total number of detections, and the second giving the total number of categories
    including the background if it is considered a separate category.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because we are using SSD MobileNetV2, we will need to create a copy of the *pipeline.config*
    file (from the TensorFlow repository) and modify it based on our configuration
    parameters. First, make a copy of the configuration file and search for the string
    `PATH_TO_BE_CONFIGURED` within the file. This parameter indicates all of the places
    that need to be updated with the correct paths (absolute paths preferably) within
    our filesystem. We will also want to edit some parameters in the configuration
    file, such as number of classes (`num_classes`), number of steps (`num_steps`),
    number of validation samples (`num_examples`), and path of label mappings for
    both the train and test/eval sections. (You’ll find our version of the *pipeline.config*
    file on the book’s GitHub website (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In our example, we’re using the SSD MobileNetV2 model to train our object detector.
    Under different conditions, you might want to choose a different model on which
    to base your training. Because each model comes with its own pipeline configuration
    file, you’d be modifying that configuration file, instead. The configuration file
    comes bundled with each model. You’d use a similar process in which you’d identify
    the paths that need to be changed and update them using a text editor accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to modifying paths, you might want to modify other parameters within
    the configuration file such as image size, number of classes, optimizer, learning
    rate, and number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we’ve been at the *models/research* directory within the TensorFlow
    repository. Let’s now move into the *object_detection* directory and run the *model_main.py*
    script from TensorFlow, which trains our model based on the configuration provided
    by the *pipeline.config* file that we just edited:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll know the training is happening correctly when we see lines of output
    that look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Depending on the number of iterations that we are training for, the training
    will take from a few minutes to a couple of hours, so plan to have a snack, do
    the laundry, clean the litter box, or better yet, read the chapter ahead. After
    the training is complete, we should see the latest checkpoint file in the training/
    directory. The following files are also created as a result of the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We convert the latest checkpoint file to the *.TFLite* format in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Sometimes, you might want to do a deeper inspection of your model for debugging,
    optimization, and/or informational purposes. Think of it like a DVD extra of a
    behind-the-scenes shot of your favorite TV show. You would run the following command
    along with arguments pointing to the checkpoint file, the configuration file,
    and the input type to view all the different parameters, the model analysis report,
    and other golden nuggets of information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from the this command looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This report helps you to learn how many parameters you have, which could in
    turn help you in finding opportunities for model optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Model Conversion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have the latest checkpoint file (the suffix should match the number
    of epochs in the *pipeline.config* file), we will feed it into the `export_tflite_ssd_graph`
    script like we did earlier in the chapter with our pretrained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If the preceding script execution was successful, we’ll see the following files
    in the *tflite_model* directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We have one last step remaining: convert the frozen graph files to the *.TFLite*
    format. We can do that using the `tflite_convert` tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In this command, notice that we’ve used a few different arguments that might
    not be intuitive at first sight:'
  prefs: []
  type: TYPE_NORMAL
- en: For `--input_arrays`, the argument simply indicates that the images that will
    be provided during predictions will be normalized `float32` tensors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The arguments supplied to `--output_arrays` indicate that there will be four
    different types of information in each prediction: the number of bounding boxes,
    detection scores, detection classes, and the coordinates of bounding boxes themselves.
    This is possible because we used the argument `--add_postprocessing_op=true` in
    the export graph script in the previous step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For `--input_shape`, we provide the same dimension values as we did in the *pipeline.config*
    file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rest are fairly trivial. We should now have a spanking new *cats.tflite*
    model file within our *tflite_model/* directory. It is now ready to be plugged
    into an Android, iOS, or edge device to do live detection of cats. This model
    is well on its way to saving Bob’s garden!
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the floating-point MobileNet model, '`normalized_image_tensor`' has values
    between `[-1,1)`. This typically means mapping each pixel (linearly) to a value
    between [–1,1]. Input image values between 0 and 255 are scaled by (1/128) and
    then a value of –1 is added to them to ensure the range is `[-1,1)`.
  prefs: []
  type: TYPE_NORMAL
- en: In the quantized MobileNet model, '`normalized_image_tensor`' has values between
    `[0, 255]`.
  prefs: []
  type: TYPE_NORMAL
- en: In general, see the `preprocess` function defined in the feature extractor class
    in the TensorFlow Models repository at *models/research/object_detection/models*
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: Image Segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Taking a step further beyond object detection, to get more precise locations
    of objects, we can perform object segmentation. As we saw earlier in this chapter,
    this involves making category predictions for each pixel in an image frame. Architectures
    such as U-Net, Mask R-CNN, and DeepLabV3+ are commonly used to perform segmentation
    tasks. Similar to object detection, there has been a growing trend of running
    segmentation networks in real time, including on resource-constrained devices
    such as smartphones. Being real time opens up many consumer app scenarios like
    face filters ([Figure 14-19](part0017.html#colorizing_hair_by_accurately_mapping_th)),
    and industrial scenarios such as detecting drivable roads for autonomous cars
    ([Figure 14-20](part0017.html#image_segmentation_performed_on_frames_f)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Colorizing hair with ModiFace app by accurately mapping the pixels belonging
    to the hair](../images/00012.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-19\. Colorizing hair with ModiFace app by accurately mapping the pixels
    belonging to the hair
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Image segmentation performed on frames from a dashcam (CamVid dataset)](../images/00300.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-20\. Image segmentation performed on frames from a dashcam (CamVid
    dataset)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you might have grown accustomed to hearing in this book by now, you can accomplish
    much of this without needing much code. Labeling tools like Supervisely, LabelBox,
    and Diffgram can help not just label the data, but also load previously annotated
    data and train further on a pretrained object segmentation model. Moreover, these
    tools provide AI-assisted labeling, which can dramatically speed up (~10 times)
    the otherwise laborious and expensive labeling process. If this sounds exciting,
    you are in luck! We have included a bonus resource guide on how to learn and build
    segmentation projects on the book’s GitHub website (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)).
  prefs: []
  type: TYPE_NORMAL
- en: Case Studies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now look at how object detection is being used to power real-world applications
    in the industry.
  prefs: []
  type: TYPE_NORMAL
- en: Smart Refrigerator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Smart fridges have been gaining popularity for the past few years because they
    have started to become more affordable. People seem to like the convenience of
    knowing what’s in their fridge even when they are not home to look inside. Microsoft
    teamed up with Swiss manufacturing giant Liebherr to use deep learning in its
    new generation SmartDeviceBox refrigerator ([Figure 14-21](part0017.html#detected_objects_along_with_their_classe)).
    The company used Fast R-CNN to perform object detection within its refrigerators
    to detect different food items, maintain an inventory, and keep the user abreast
    of the latest state.
  prefs: []
  type: TYPE_NORMAL
- en: '![Detected objects along with their classes from the SmartDeviceBox refrigerator
    (image source)](../images/00257.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-21\. Detected objects along with their classes from the SmartDeviceBox
    refrigerator ([image source](https://oreil.ly/xg0wT))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Crowd Counting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Counting people is not just something that only cats do while staring out of
    windows. It’s useful in a lot of situations including managing security and logistics
    at large sporting events, political gatherings, and other high-traffic areas.
    Crowd counting, as its name suggests, can be used to count any kind of object
    including people and animals. Wildlife conservation is an exceptionally challenging
    problem because of the lack of labeled data and intense data collection strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Wildlife conservation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Several organizations, including the University of Glasgow, University of Cape
    Town, Field Museum of Natural History, and Tanzania Wildlife Research Institute,
    came together to count the largest terrestrial animal migration on earth: 1.3
    million blue wildebeest and 250,000 zebra between the Serengeti and the Masaai
    Mara National Reserve, Kenya ([Figure 14-22](part0017.html#an_aerial_photograph_of_wildebeest_taken)).
    They employed data labeling through 2,200 volunteer citizen scientists using a
    platform called Zooniverse and used automated object detection algorithms like
    YOLO for counting the wildebeest and zebra population. They observed that the
    count from the volunteers and from the object detection algorithms were within
    1% of each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '![An aerial photograph of wildebeest taken from a small survey airplane (image
    source)](../images/00218.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-22\. An aerial photograph of wildebeest taken from a small survey
    airplane ([image source](https://oreil.ly/hTodA))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Kumbh Mela
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another example of dense crowd counting is from Prayagraj, India where approximately
    250 million people visit the city once every 12 years for the Kumbh Mela festival
    ([Figure 14-23](part0017.html#the_2013_kumbh_melacomma_as_captured_by)). Crowd
    control for such large crowds has historically been difficult. In fact, in the
    year 2013, due to poor crowd management, a stampede broke out tragically resulting
    in the loss of 42 lives.
  prefs: []
  type: TYPE_NORMAL
- en: In 2019, the local state government contracted with Larsen & Toubro to use AI
    for a variety of logistical tasks including traffic monitoring, garbage collection,
    security assessments, and also crowd monitoring. Using more than a thousand CCTV
    cameras, authorities analyzed crowd density and designed an alerting system based
    on the density of people in a fixed area. In places of higher density, the monitoring
    was more intense. It made the Guinness Book of World Records for being the largest
    crowd management system ever built in the history of humankind!
  prefs: []
  type: TYPE_NORMAL
- en: '![The 2013 Kumbh Mela, as captured by an attendee](../images/00144.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-23\. The 2013 Kumbh Mela, as captured by an attendee ([image source](https://oreil.ly/OIy7N))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Face Detection in Seeing AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Microsoft’s Seeing AI app (for the blind and low-vision community) provides
    a real-time facial detection feature with which it informs the user of the people
    in front of the phone’s camera, their relative locations, and their distance from
    the camera. A typical guidance might sound like “One face in the top-left corner,
    four feet away.” Additionally, the app uses the facial-detection guidance to perform
    facial recognition against a known list of faces. If the face is a match, it will
    announce the name of the person, as well. An example of vocal guidance would be
    “Elizabeth near top edge, three feet away,” as demonstrated in [Figure 14-24](part0017.html#face_detection_feature_on_seeing_ai).
  prefs: []
  type: TYPE_NORMAL
- en: To identify a person’s location in the camera feed, the system is running on
    a fast mobile-optimized object detector. A crop of this face is then passed for
    further processing to age, emotion, hair style, and other recognition algorithms
    on the cloud from Microsoft’s Cognitive Services. For identifying people in a
    privacy-friendly way, the app asks the user’s friends and family to take three
    selfies of their faces and generates a feature representation (embedding) of the
    face that is stored on the device (rather than storing any images). When a face
    is detected in the camera live feed in the future, an embedding is calculated
    and compared with the database of embeddings and associated names. This is based
    on the concept of one-shot learning, similar to Siamese networks that we saw in
    [Chapter 4](part0006.html#5N3C3-13fa565533764549a6f0ab7f11eed62b). Another benefit
    of not storing images is that the size of the app doesn’t increase drastically
    even with a large number of faces stored.
  prefs: []
  type: TYPE_NORMAL
- en: '![Face detection feature on Seeing AI](../images/00142.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-24\. Face detection feature on Seeing AI
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Autonomous Cars
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Several self-driving car companies including Waymo, Uber, Tesla, Cruise, NVIDIA,
    and others use object detection, segmentation, and other deep learning techniques
    for building their self-driving cars. Advanced Driver Assistance Systems (ADAS)
    such as pedestrian detection, vehicle type identification, and speed limit sign
    recognition are important components of a self-driving car.
  prefs: []
  type: TYPE_NORMAL
- en: For the decision-making involved in self-driving cars, NVIDIA uses specialized
    networks for different tasks. For example, WaitNet is a neural network used for
    doing fast, high-level detection of traffic lights, intersections, and traffic
    signs. In addition to being fast, it is also meant to withstand noise and work
    reliably in tougher conditions such as rain and snow. The bounding boxes detected
    by WaitNet are then fed to specialized networks for more detailed classification.
    For instance, if a traffic light is detected in a frame, it is then fed into LightNet—a
    network used for detecting traffic light shape (circle versus arrow) and state
    (red, yellow, green). Additionally, if a traffic sign is detected in a frame,
    it’s fed into SignNet, a network to classify among a few hundred types of traffic
    signs from the US and Europe, including stop signs, yield signs, directional information,
    speed limit information, and more. Cascading output from faster networks to specialized
    networks helps improve performance and modularize development of different networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting traffic lights and signs on a self-driving car using the NVIDIA
    Drive Platform (image source)](../images/00097.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-25\. Detecting traffic lights and signs on a self-driving car using
    the NVIDIA Drive Platform ([image source](https://oreil.ly/vcJY6))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the types of computer-vision tasks, including how
    they relate to one another and how they are different from one another. We took
    a deeper look at object detection, how the pipeline works, and how it evolved
    over time. We then collected data, labeled it, trained a model (with and without
    code), and deployed the model to mobile. We next looked at how object detection
    is being used in the real world by industry, government, and non-governmental
    organizations (NGOs). And then we took a sneak peek into the world of object segmentation.
    Object detection is a very powerful tool whose vast potential is still being discovered
    every day in numerous areas of our lives. What innovative use of object detection
    can you think of? Share it with us on Twitter [@PracticalDLBook](https://www.twitter.com/PracticalDLBook).
  prefs: []
  type: TYPE_NORMAL
