- en: Intersection over Union
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交并比
- en: In an object detection training pipeline, metrics such as Intersection over
    Union (IoU) are typically used as a threshold value to filter detections of a
    certain quality. To calculate the IoU, we use both the predicted bounding box
    and the ground truth bounding box to calculate two different numbers. The first
    is the area where both the prediction and the ground truth overlap—the intersection.
    The second is the span covered by both the prediction and the ground truth—the
    union. As the name suggests, we simply divide the total area of the intersection
    by the area of the union to get the IoU. [Figure 14-11](part0017.html#a_visual_representation_of_the_iou_ratio)
    visually demonstrates the IoU concept for two 2x2 squares that intersect in a
    single 1x1 square in the middle.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标检测训练流程中，通常使用诸如交并比（IoU）之类的指标作为过滤特定质量检测的阈值。为了计算IoU，我们使用预测的边界框和实际边界框来计算两个不同的数字。第一个是预测和实际情况重叠的区域——交集。第二个是预测和实际情况覆盖的范围——并集。正如名称所示，我们简单地将交集的总面积除以并集的面积来获得IoU。[图14-11](part0017.html#a_visual_representation_of_the_iou_ratio)直观地展示了两个2x2正方形的IoU概念，它们在中间的一个1x1正方形中相交。
- en: '![A visual representation of the IoU ratio](../images/00007.jpeg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![IoU比率的可视化表示](../images/00007.jpeg)'
- en: Figure 14-11\. A visual representation of the IoU ratio
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-11。IoU比率的可视化表示
- en: In the perfect scenario in which the predicted bounding box matches the ground
    truth exactly, the IoU value would be 1\. In the worst-case scenario, the prediction
    would have no overlap with the ground truth and the IoU value would be 0\. As
    we can see, the IoU value would range between 0 and 1, with higher numbers indicating
    higher-quality predictions, as illustrated in [Figure 14-12](part0017.html#iou_illustratedsemicolon_predictions_fro).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想情况下，预测的边界框与实际情况完全匹配，IoU值将为1。在最坏的情况下，预测与实际情况没有重叠，IoU值将为0。正如我们所看到的，IoU值将在0和1之间变化，较高的数字表示较高质量的预测，如[图14-12](part0017.html#iou_illustratedsemicolon_predictions_fro)所示。
- en: To help us filter and report results, we would set a minimum IoU threshold value.
    Setting the threshold to a really aggressive value (such as 0.9) would result
    in a loss of a lot of predictions that might turn out to be important further
    down the pipeline. Conversely, setting the threshold value too low would result
    in too many spurious bounding boxes. A minimum IoU of 0.5 is typically used for
    reporting the precision of an object detection model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们过滤和报告结果，我们会设置一个最小IoU阈值。将阈值设置为非常激进的值（如0.9）会导致丢失许多可能在后续流程中变得重要的预测。相反，将阈值设置得太低会导致太多虚假的边界框。通常用于报告目标检测模型精度的最小IoU为0.5。
- en: '![IoU illustrated; predictions from better models tend to have heavier overlap
    with the ground truth, resulting in a higher IoU](../images/00293.jpeg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![IoU的图示；更好模型的预测往往与实际情况有更重叠，导致更高的IoU](../images/00293.jpeg)'
- en: Figure 14-12\. IoU illustrated; predictions from better models tend to have
    heavier overlap with the ground truth, resulting in a higher IoU
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-12。IoU的图示；更好模型的预测往往与实际情况有更重叠，导致更高的IoU
- en: It’s worth reiterating here that the IoU value is calculated per instance of
    a category, not per image. However, to calculate the quality of the detector over
    a larger set of images, we would use IoU in conjunction with other metrics such
    as Average Precision.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 值得在这里重申的是，IoU值是针对每个类别实例计算的，而不是针对每个图像。然而，为了计算检测器在更大一组图像上的质量，我们会将IoU与其他指标（如平均精度）结合使用。
- en: Mean Average Precision
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平均精度
- en: While reading research papers on object detection, we often come across numbers
    such as AP@0.5\. That term conveys the average precision at IoU=0.5\. Another
    more elaborate representation would be AP@[0.6:0.05:0.75], which is the average
    precision from IoU=0.6 to IoU=0.75 at increments of 0.05\. The Mean Average Precision
    (or mAP) is simply the average precision across all categories. For the COCO challenge,
    the MAP metric used is AP@[0.5:0.05:0.95].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究目标检测的论文中，我们经常遇到诸如AP@0.5的数字。该术语表示IoU=0.5时的平均精度。另一个更详细的表示方法是AP@[0.6:0.05:0.75]，这是从IoU=0.6到IoU=0.75的平均精度，间隔为0.05。平均精度（或mAP）简单地是所有类别的平均精度。对于COCO挑战，使用的MAP指标是AP@[0.5:0.05:0.95]。
- en: Non-Maximum Suppression
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非极大值抑制
- en: Internally, object detection algorithms make a number of proposals for the potential
    locations of objects that might be in the image. For each object in the image,
    it is expected to have multiple of these bounding box proposals at varying confidence
    values. Our task is to find the one proposal that best represents the real location
    of the object. A naïve way would be to consider only the proposal with the maximum
    confidence value. This approach might work if there were only one object in the
    entire image. But this won’t work if there are multiple categories, each with
    multiple instances in a single image.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，目标检测算法对可能在图像中的对象的潜在位置提出了许多建议。对于图像中的每个对象，预期会有多个具有不同置信度值的边界框建议。我们的任务是找到最能代表对象真实位置的建议。一种天真的方法是只考虑置信度值最大的建议。如果整个图像中只有一个对象，这种方法可能有效。但如果图像中有多个类别，每个类别中有多个实例，这种方法就不起作用了。
- en: 'Non-Maximum Suppression (NMS) comes to the rescue ([Figure 14-13](part0017.html#using_nms_to_find_the_bounding_box_that)).
    The key idea behind NMS is that two instances of the same object will not have
    heavily overlapping bounding boxes; for instance, the IoU of their bounding boxes
    will be less than a certain IoU threshold (say 0.5). A greedy way to achieve this
    is by repeating the following steps for each category:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 非极大值抑制（NMS）来拯救我们（[图14-13](part0017.html#using_nms_to_find_the_bounding_box_that)）。NMS背后的关键思想是，同一对象的两个实例不会有严重重叠的边界框；例如，它们的边界框的IoU将小于某个IoU阈值（比如0.5）。实现这一点的一种贪婪方法是对每个类别重复以下步骤：
- en: Filter out all proposals under a minimum confidence threshold.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过滤掉所有置信度低于最小阈值的建议。
- en: Accept the proposal with the maximum confidence value.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接受置信度值最大的提议。
- en: For all remaining proposals sorted in descending order of their confidence values,
    check whether the IoU of the current box with respect to one of the previously
    accepted proposals ≥ 0.5\. If so, filter it; otherwise, accept it as a proposal.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有按其置信度值降序排序的剩余提议，检查当前框与先前接受的提议之一的IoU是否≥0.5。如果是，则过滤掉；否则，接受它作为提议。
- en: '![Using NMS to find the bounding box that best represents the location of the
    object in an image](../images/00223.jpeg)'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用NMS找到最能代表图像中对象位置的边界框](../images/00223.jpeg)'
- en: Figure 14-13\. Using NMS to find the bounding box that best represents the location
    of the object in an image
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-13。使用NMS找到最能代表图像中对象位置的边界框
- en: Using the TensorFlow Object Detection API to Build Custom Models
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow目标检测API构建自定义模型
- en: In this section, we run through a full end-to-end example of building an object
    detector. We look at several steps in the process, including collecting, labeling,
    and preprocessing data, training the model, and exporting it to the TensorFlow
    Lite format.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过一个完整的端到端示例来构建一个对象检测器。我们将看到该过程中的几个步骤，包括收集、标记和预处理数据，训练模型，并将其导出为TensorFlow
    Lite格式。
- en: First, let’s look at some strategies for collecting data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看一些收集数据的策略。
- en: Data Collection
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据收集
- en: 'We know by now that, in the world of deep learning, data reigns supreme. There
    are a few ways in which we can acquire data for the object that we want to detect:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道，在深度学习的世界中，数据至关重要。我们可以通过几种方式获取我们想要检测的对象的数据：
- en: Using ready-to-use datasets
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用现成的数据集
- en: There are a few public datasets available to train object detection models such
    as MS COCO (80 categories), ImageNet (200 categories), Pascal VOC (20 categories),
    and the newer Open Images (600 categories). MS COCO and Pascal VOC are used in
    most object detection benchmarks, with COCO-based benchmarks being more realistic
    for real-world scenarios due to more complex imagery.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些公共数据集可用于训练对象检测模型，如MS COCO（80个类别）、ImageNet（200个类别）、Pascal VOC（20个类别）和更新的Open
    Images（600个类别）。MS COCO和Pascal VOC在大多数对象检测基准测试中使用，基于COCO的基准测试对于真实世界场景更具现实性，因为图像更复杂。
- en: Downloading from the web
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从网上下载
- en: We should already be familiar with this process from [Chapter 12](part0014.html#DB7S3-13fa565533764549a6f0ab7f11eed62b)
    in which we collected images for the “Hot Dog” and “Not Hot Dog” classes. Browser
    extensions such as Fatkun come in handy for quickly downloading a large number
    of images from search engine results.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该已经熟悉这个过程，因为在[第12章](part0014.html#DB7S3-13fa565533764549a6f0ab7f11eed62b)中，我们为“热狗”和“非热狗”类别收集了图像。浏览器扩展程序如Fatkun对于快速从搜索引擎结果中下载大量图像非常方便。
- en: Taking pictures manually
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 手动拍照
- en: This is the more time consuming (but potentially fun) option. For a robust model,
    it’s important to train it with pictures taken in a variety of different settings.
    With the object to detect in hand, we should take at least 100 to 150 images of
    it against different backgrounds, with a variety of compositions (framing) and
    angles, and in many different lighting conditions. [Figure 14-14](part0017.html#photographs_of_objects_taken_in_a_variet)
    shows some examples of pictures taken to train a Coke and Pepsi object detection
    model. Considering that a model can potentially learn the spurious correlation
    that red means Coke and blue means Pepsi, it’s a good idea to mix objects with
    backgrounds that could potentially confuse it so that eventually we realize a
    more robust model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是更耗时（但潜在有趣）的选项。为了构建一个强大的模型，重要的是用在各种不同环境中拍摄的照片来训练它。拿着要检测的对象，我们应该至少拍摄100到150张不同背景下的照片，具有各种构图和角度，并在许多不同的光照条件下。[图14-14](part0017.html#photographs_of_objects_taken_in_a_variet)展示了用于训练可乐和百事对象检测模型的一些照片示例。考虑到模型可能会学习到红色代表可乐，蓝色代表百事这种虚假相关性，最好混合对象和可能会混淆它的背景，以便最终实现更强大的模型。
- en: '![Photographs of objects taken in a variety of different settings to train
    an object detector model](../images/00212.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![在各种不同环境中拍摄的对象照片，用于训练对象检测模型](../images/00212.jpeg)'
- en: Figure 14-14\. Photographs of objects taken in a variety of different settings
    to train an object detector model
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-14。在各种不同环境中拍摄的对象照片，用于训练对象检测模型
- en: It’s worth taking pictures of the object in environments that it’s unlikely
    to be used in order to diversify the dataset and improve the robustness of the
    model. It also has the added bonus of keeping an otherwise boring and tedious
    process a little entertaining. We can make it interesting by challenging ourselves
    to come up with unique and innovative photos. [Figure 14-15](part0017.html#some_creative_photographs_taken_during_t)
    shows some creative examples of photos taken during the process of building a
    currency detector.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 值得在对象不太可能使用的环境中拍摄对象的照片，以使数据集多样化并提高模型的稳健性。这也有一个额外的好处，可以使原本乏味和繁琐的过程变得有趣。我们可以通过挑战自己想出独特和创新的照片来使其有趣。[图14-15](part0017.html#some_creative_photographs_taken_during_t)展示了在构建货币检测器过程中拍摄的一些创意照片的示例。
- en: '![Some creative photographs taken during the process of building a diverse
    currency dataset](../images/00178.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![在构建多样化货币数据集过程中拍摄的一些创意照片](../images/00178.jpeg)'
- en: Figure 14-15\. Some creative photographs taken during the process of building
    a diverse currency dataset
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-15。在构建多样化货币数据集过程中拍摄的一些创意照片
- en: Because we want to make a cat detector, we will be reusing the cat images from
    the “Cats and Dogs” Kaggle dataset that we used in [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b).
    We randomly chose images from that set and split them into a train and test set.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们想要制作一个猫检测器，我们将重复使用“猫和狗”Kaggle数据集中的猫图片，该数据集在[第3章](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b)中使用过。我们随机选择了该数据集中的图片，并将它们分成训练集和测试集。
- en: 'To maintain uniformity of input size to our network, we resize all images to
    a fixed size. We will use the same size as part of our network definition and
    while converting to a *.tflite* model. We can use the ImageMagick tool to resize
    all images at once:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持输入到我们网络的大小的统一性，我们将所有图像调整为固定大小。我们将使用相同的大小作为网络定义的一部分，并在转换为*.tflite*模型时使用。我们可以使用ImageMagick工具一次调整所有图像的大小：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tip
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'If you have a large number of images (i.e., several tens of thousands of images)
    in the current directory, the preceding command might fail to enlist all of the
    images. A solution would be to use the `find` command to list all of the images
    and pipe the output to the `mogrify` command:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在当前目录中有大量图像（即数万张图像），上述命令可能无法列出所有图像。解决方法是使用`find`命令列出所有图像，并将输出传输到`mogrify`命令：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Labeling the Data
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记数据
- en: 'After we have our data, the next step would be to label it. Unlike classification
    where simply placing an image in the correct directory would be sufficient, we
    need to manually draw the bounding rectangles for the objects of interest. Our
    tool of choice for this task is LabelImg (available for Windows, Linux, and Mac)
    for the following reasons:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得数据之后，下一步是对其进行标记。与分类不同，仅将图像放入正确的目录中就足够了，我们需要手动为感兴趣的对象绘制边界矩形。我们选择此任务的工具是LabelImg（适用于Windows、Linux和Mac），原因如下：
- en: You can save annotations either as XML files in PASCAL VOC format (also adopted
    by ImageNet) or the YOLO format.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以将注释保存为PASCAL VOC格式的XML文件（也被ImageNet采用）或YOLO格式。
- en: It supports single and multiclass labels.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持单标签和多标签。
- en: Note
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you are the curious cat like us, you’d want to know what the difference
    is between the YOLO format and the PASCAL VOC format. YOLO happens to use simple
    *.txt* files, one per image with the same name, to store information about the
    classes and the bounding boxes within the image. The following is what a *.txt*
    file in YOLO format typically looks like:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您像我们一样是好奇的猫，您可能想知道YOLO格式和PASCAL VOC格式之间的区别是什么。 YOLO恰好使用简单的*.txt*文件，每个图像一个文件，文件名相同，用于存储有关图像中类别和边界框的信息。以下是YOLO格式中*.txt*文件的典型外观：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that the x, y, width, and height in this example are normalized using the
    full width and height of the image.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此示例中的x、y、宽度和高度是使用图像的完整宽度和高度进行归一化的。
- en: PASCAL VOC format, on the other hand, is XML based. Similar to the YOLO format,
    we use one XML file per image (ideally with the same name). You can view the sample
    format on the Git repository at *code/chapter-14/pascal-voc-sample.xml*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，PASCAL VOC格式是基于XML的。与YOLO格式类似，我们每个图像使用一个XML文件（最好与相同名称）。您可以在Git存储库的*code/chapter-14/pascal-voc-sample.xml*中查看示例格式。
- en: First, download the application from [GitHub](https://oreil.ly/jH31E) to a directory
    on your computer. This directory will have an executable and a data directory
    containing some sample data. Because we will be using our own custom data, we
    don’t need the data within the provided data directory. You can start the application
    by double-clicking the executable file.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从[GitHub](https://oreil.ly/jH31E)下载应用程序到计算机上的一个目录。该目录将包含一个可执行文件和一个包含一些示例数据的数据目录。因为我们将使用自己的自定义数据，所以我们不需要提供的数据目录中的数据。您可以通过双击可执行文件来启动应用程序。
- en: At this point, we should have a collection of images that is ready to be used
    during the training process. We first divide our data randomly into training and
    test sets and place them in separate directories. We can do this split by simply
    dragging and dropping the images randomly into either directory. After the train
    and test directories are created, we load the training directory by clicking Open
    Dir, as shown in [Figure 14-16](part0017.html#click_the_open_dir_button_and_then_selec).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们应该有一组准备用于训练过程的图像。我们首先将数据随机分成训练集和测试集，并将它们放在不同的目录中。我们可以通过简单地将图像随机拖放到任一目录中来进行此拆分。创建训练和测试目录后，我们通过单击“打开目录”来加载训练目录，如[图14-16](part0017.html#click_the_open_dir_button_and_then_selec)所示。
- en: '![Click the Open Dir button and then select the directory that contains the
    training data](../images/00133.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![单击“打开目录”按钮，然后选择包含训练数据的目录](../images/00133.jpeg)'
- en: Figure 14-16\. Click the Open Dir button and then select the directory that
    contains the training data
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-16\. 单击“打开目录”按钮，然后选择包含训练数据的目录
- en: After LabelImg loads the training directory, we can start our labeling process.
    We must go through each image and manually draw bounding boxes for each object
    (only cats in our case), as shown in [Figure 14-17](part0017.html#select_the_create_rectbox_from_the_panel).
    After we make the bounding box, we are prompted to provide a label to accompany
    the bounding box. For this exercise, enter the name of the object, “cat”. After
    a label is entered, we need only to select the checkbox to specify that label
    again for subsequent images. For images with multiple objects, we would make multiple
    bounding boxes and add their corresponding labels. If there are different kinds
    of objects, we simply add a new label for that object class.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: LabelImg加载训练目录后，我们可以开始标记过程。我们必须逐个图像地为每个对象（在我们的情况下仅为猫）手动绘制边界框，如[图14-17](part0017.html#select_the_create_rectbox_from_the_panel)所示。绘制边界框后，我们将提示提供标签以配合边界框。对于此练习，请输入对象的名称“cat”。输入标签后，我们只需选择复选框，以指定再次为后续图像选择该标签。对于具有多个对象的图像，我们将制作多个边界框并添加相应的标签。如果存在不同类型的对象，我们只需为该对象类别添加新标签。
- en: '![Select the Create RectBox from the panel on the left to make a bounding box
    that covers the cat](../images/00068.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![从左侧面板中选择Create RectBox以创建覆盖猫的边界框](../images/00068.jpeg)'
- en: Figure 14-17\. Select the Create RectBox from the panel on the left to make
    a bounding box that covers the cat
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-17\. 从左侧面板中选择Create RectBox以创建覆盖猫的边界框
- en: Now, repeat this step for all of the training and test images. We want to obtain
    tight bounding boxes for each object, such that all parts of the object are bounded
    by the box. Finally, in the train and test directories, we see *.xml* files accompanying
    each image file, as depicted in [Figure 14-18](part0017.html#each_image_is_accompanied_by_an_xml_file).
    We can open the .`xml` file in a text editor and inspect metadata such as image
    file name, bounding box coordinates, and label name.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对所有训练和测试图像重复这一步骤。我们希望为每个对象获取紧密的边界框，以便对象的所有部分都被框住。最后，在训练和测试目录中，我们看到每个图像文件都有一个*.xml*文件，如[图14-18](part0017.html#each_image_is_accompanied_by_an_xml_file)所示。我们可以在文本编辑器中打开`.xml`文件并检查元数据，如图像文件名、边界框坐标和标签名称。
- en: '![Each image is accompanied by an XML file that contains the label information
    and the bounding box information](../images/00049.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![每个图像都附带一个包含标签信息和边界框信息的XML文件](../images/00049.jpeg)'
- en: Figure 14-18\. Each image is accompanied by an XML file that contains the label
    information and the bounding box information
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-18\. 每个图像都附带一个包含标签信息和边界框信息的XML文件
- en: Preprocessing the Data
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: At this point, we have handy XML data that gives the bounding boxes for all
    of our objects. However, for us to use TensorFlow for training on the data, we
    must preprocess it into a format that TensorFlow understands, namely TFRecords.
    Before we can convert our data into TFRecords, we must go through the intermediate
    step of consolidating the data from all the XML files into a single comma-separated
    values (CSV) file. TensorFlow provides helper scripts that assist us with these
    operations.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有方便的XML数据，提供了所有对象的边界框。但是，为了使用TensorFlow对数据进行训练，我们必须将其预处理成TensorFlow理解的格式，即TFRecords。在将数据转换为TFRecords之前，我们必须经历一个中间步骤，将所有XML文件中的数据合并到一个单独的逗号分隔值（CSV）文件中。TensorFlow提供了帮助脚本来协助我们进行这些操作。
- en: 'Now that we have our environment set up, it’s time to begin doing some real
    work:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的环境已经设置好，是时候开始做一些真正的工作了：
- en: 'Convert our cats dataset directory to a single CSV file using the `xml_to_csv`
    tool from the [racoon_dataset](https://oreil.ly/k8QGl) repository by Dat Tran.
    We have provided a slightly edited copy of this file in our repository at *code/chapter-14/xml_to_csv.py*:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用来自Dat Tran的[racoon_dataset](https://oreil.ly/k8QGl)存储库中的`xml_to_csv`工具，将我们的cats数据集目录转换为一个单独的CSV文件。我们在我们的存储库中提供了这个文件的稍作编辑副本，位于*code/chapter-14/xml_to_csv.py*中：
- en: '[PRE3]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Do the same for the test data:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对测试数据做同样的操作：
- en: '[PRE4]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Make the *label_map.pbtxt* file. This file contains the label and identifier
    mappings for all of our classes. We use this file to convert our text label into
    an integer identifier, which is what TFRecord expects. Because we have only one
    class, the file is rather small and looks as follows:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建*label_map.pbtxt*文件。该文件包含所有类别的标签和标识符映射。我们使用这个文件将文本标签转换为整数标识符，这是TFRecord所期望的。因为我们只有一个类别，所以这个文件相对较小，如下所示：
- en: '[PRE5]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Generate the TFRecord format files that will contain the data to be used for
    training and testing our model later on. This file is also from the [racoon_dataset](https://oreil.ly/VNwwE)
    repository by Dat Tran. We have provided a slightly edited copy of this file in
    our repository at *code/chapter-14/generate_tfrecord.py*. (It’s worth noting that
    the `image_dir` path in the argument should be the same as the path in the XML
    files; LabelImg uses absolute paths.)
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成TFRecord格式的文件，其中包含以后用于训练和测试模型的数据。这个文件也来自Dat Tran的[racoon_dataset](https://oreil.ly/VNwwE)存储库。我们在我们的存储库中提供了这个文件的稍作编辑副本，位于*code/chapter-14/generate_tfrecord.py*中。（值得注意的是，参数中的`image_dir`路径应与XML文件中的路径相同；LabelImg使用绝对路径。）
- en: '[PRE6]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: With our *train.tfrecord* and *test.tfrecord* files ready, we can now begin
    our training process.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有了准备好的*train.tfrecord*和*test.tfrecord*文件，我们现在可以开始训练过程了。
- en: Inspecting the Model
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查模型
- en: (This section is informational only and is not necessary for the training process.
    You can skip ahead to [“Training”](part0017.html#training-id00003) if you want
    to.)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: （此部分仅供信息，对训练过程并非必需。如果您愿意，可以直接跳转到[“训练”](part0017.html#training-id00003)。）
- en: 'We can inspect our model using the `saved_model_cli` tool:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`saved_model_cli`工具检查我们的模型：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output of that script looks like the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本的输出如下：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following is how we can interpret the output from the previous command:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们如何解释前一个命令的输出：
- en: The shape of `inputs` being `(-1, -1, -1, 3)` indicates that the trained model
    can accept any arbitrarily sized image input that contains three channels. Due
    to this inherent flexibility in input size, the converted model would be larger
    than if the model had a fixed-size input. When we train our own object detector
    later in this chapter, we will be fixing the input size to 800 x 600 pixels to
    make the resulting model more compact.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`inputs`的形状为`(-1, -1, -1, 3)`表明训练过的模型可以接受包含三个通道的任意大小的图像输入。由于输入大小的固有灵活性，转换后的模型会比具有固定大小输入的模型更大。当我们在本章后面训练自己的目标检测器时，我们将把输入大小固定为800
    x 600像素，以使得生成的模型更紧凑。'
- en: Moving on to the outputs, the very first output is `detection_boxes (-1, 100,
    4)`. This tells us how many detection boxes are possible, and what will they look
    like. In particular, the very first number (i.e., -1) indicates that we can have
    any number of boxes based on the detections for all 100 classes (second number)
    with four coordinates (third number)—x, y, width, and height—defining each box.
    In other words, we are not limiting the number of detections for each of the 100
    classes.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在转向输出时，第一个输出是`detection_boxes (-1, 100, 4)`。这告诉我们有多少个检测框可能存在，以及它们的样子。特别是，第一个数字（即-1）表示我们可以根据所有100个类别的检测结果拥有任意数量的框（第二个数字），每个框有四个坐标（第三个数字）——x、y、宽度和高度——定义每个框。换句话说，我们不限制每个100个类别的检测数量。
- en: 'For `detection_classes`, we have a list of two elements: the first defining
    how many objects we detected, and the second, a one-hot encoded vector with the
    detected class enabled.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于`detection_classes`，我们有一个包含两个元素的列表：第一个定义了我们检测到多少个对象，第二个是一个独热编码向量，其中检测到的类别被启用。
- en: '`num_detections` is the number of objects detected in the image. It is a single
    floating point.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`num_detections`是图像中检测到的对象数量。它是一个单一的浮点数。'
- en: '`raw_detection_boxes` defines the coordinates of each box that was detected
    for each object. All of these detections are before the application of NMS on
    all the detections.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`raw_detection_boxes`定义了每个对象检测到的每个框的坐标。所有这些检测都是在对所有检测应用NMS之前进行的。'
- en: '`raw_detection_scores` is a list of two floating-point numbers, the first describing
    the total number of detections, and the second giving the total number of categories
    including the background if it is considered a separate category.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`raw_detection_scores`是两个浮点数的列表，第一个描述了总检测数，第二个描述了总类别数，包括背景（如果将其视为单独的类别）。'
- en: Training
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: Because we are using SSD MobileNetV2, we will need to create a copy of the *pipeline.config*
    file (from the TensorFlow repository) and modify it based on our configuration
    parameters. First, make a copy of the configuration file and search for the string
    `PATH_TO_BE_CONFIGURED` within the file. This parameter indicates all of the places
    that need to be updated with the correct paths (absolute paths preferably) within
    our filesystem. We will also want to edit some parameters in the configuration
    file, such as number of classes (`num_classes`), number of steps (`num_steps`),
    number of validation samples (`num_examples`), and path of label mappings for
    both the train and test/eval sections. (You’ll find our version of the *pipeline.config*
    file on the book’s GitHub website (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们使用的是SSD MobileNetV2，所以我们需要创建一个*pipeline.config*文件的副本（来自TensorFlow存储库），并根据我们的配置参数进行修改。首先，复制配置文件并在文件中搜索字符串`PATH_TO_BE_CONFIGURED`。该参数指示需要使用正确路径（最好是绝对路径）更新的所有位置。我们还需要编辑配置文件中的一些参数，例如类别数（`num_classes`）、步数（`num_steps`）、验证样本数（`num_examples`）以及训练和测试/评估部分的标签映射路径。
    （您可以在本书的GitHub网站上找到我们版本的*pipeline.config*文件（请参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)））。
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In our example, we’re using the SSD MobileNetV2 model to train our object detector.
    Under different conditions, you might want to choose a different model on which
    to base your training. Because each model comes with its own pipeline configuration
    file, you’d be modifying that configuration file, instead. The configuration file
    comes bundled with each model. You’d use a similar process in which you’d identify
    the paths that need to be changed and update them using a text editor accordingly.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们使用SSD MobileNetV2模型来训练我们的目标检测器。在不同条件下，您可能希望选择不同的模型来进行训练。因为每个模型都附带自己的管道配置文件，您将修改该配置文件。配置文件与每个模型捆绑在一起。您将使用类似的过程，识别需要更改的路径并使用文本编辑器相应地更新它们。
- en: In addition to modifying paths, you might want to modify other parameters within
    the configuration file such as image size, number of classes, optimizer, learning
    rate, and number of epochs.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了修改路径，您可能还想修改配置文件中的其他参数，例如图像大小、类别数、优化器、学习率和迭代次数。
- en: 'So far, we’ve been at the *models/research* directory within the TensorFlow
    repository. Let’s now move into the *object_detection* directory and run the *model_main.py*
    script from TensorFlow, which trains our model based on the configuration provided
    by the *pipeline.config* file that we just edited:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在TensorFlow存储库中的*models/research*目录中。现在让我们进入*object_detection*目录，并从TensorFlow运行*model_main.py*脚本，根据我们刚刚编辑的*pipeline.config*文件提供的配置来训练我们的模型：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We’ll know the training is happening correctly when we see lines of output
    that look like the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们看到以下输出行时，我们就知道训练正在正确进行：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Depending on the number of iterations that we are training for, the training
    will take from a few minutes to a couple of hours, so plan to have a snack, do
    the laundry, clean the litter box, or better yet, read the chapter ahead. After
    the training is complete, we should see the latest checkpoint file in the training/
    directory. The following files are also created as a result of the training process:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们进行训练的迭代次数，训练将需要几分钟到几个小时，所以计划一下，吃点零食，洗洗衣服，清理猫砂盆，或者更好的是，提前阅读下一章。训练完成后，我们应该在training/目录中看到最新的检查点文件。训练过程还会生成以下文件：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We convert the latest checkpoint file to the *.TFLite* format in the next section.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节将最新的检查点文件转换为*.TFLite*格式。
- en: Tip
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Sometimes, you might want to do a deeper inspection of your model for debugging,
    optimization, and/or informational purposes. Think of it like a DVD extra of a
    behind-the-scenes shot of your favorite TV show. You would run the following command
    along with arguments pointing to the checkpoint file, the configuration file,
    and the input type to view all the different parameters, the model analysis report,
    and other golden nuggets of information:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，您可能希望对模型进行更深入的检查，以进行调试、优化和/或信息目的。可以将以下命令与指向检查点文件、配置文件和输入类型的参数一起运行，以查看所有不同参数、模型分析报告和其他有用信息：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output from the this command looks like the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令的输出如下：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This report helps you to learn how many parameters you have, which could in
    turn help you in finding opportunities for model optimization.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这份报告可以帮助您了解您拥有多少参数，从而帮助您找到模型优化的机会。
- en: Model Conversion
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型转换
- en: 'Now that we have the latest checkpoint file (the suffix should match the number
    of epochs in the *pipeline.config* file), we will feed it into the `export_tflite_ssd_graph`
    script like we did earlier in the chapter with our pretrained model:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了最新的检查点文件（后缀应该与*pipeline.config*文件中的迭代次数匹配），我们将像之前在本章中使用我们的预训练模型一样，将其输入到`export_tflite_ssd_graph`脚本中：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If the preceding script execution was successful, we’ll see the following files
    in the *tflite_model* directory:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的脚本执行成功，我们将在*tflite_model*目录中看到以下文件：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We have one last step remaining: convert the frozen graph files to the *.TFLite*
    format. We can do that using the `tflite_convert` tool:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In this command, notice that we’ve used a few different arguments that might
    not be intuitive at first sight:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: For `--input_arrays`, the argument simply indicates that the images that will
    be provided during predictions will be normalized `float32` tensors.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The arguments supplied to `--output_arrays` indicate that there will be four
    different types of information in each prediction: the number of bounding boxes,
    detection scores, detection classes, and the coordinates of bounding boxes themselves.
    This is possible because we used the argument `--add_postprocessing_op=true` in
    the export graph script in the previous step.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For `--input_shape`, we provide the same dimension values as we did in the *pipeline.config*
    file.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rest are fairly trivial. We should now have a spanking new *cats.tflite*
    model file within our *tflite_model/* directory. It is now ready to be plugged
    into an Android, iOS, or edge device to do live detection of cats. This model
    is well on its way to saving Bob’s garden!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the floating-point MobileNet model, '`normalized_image_tensor`' has values
    between `[-1,1)`. This typically means mapping each pixel (linearly) to a value
    between [–1,1]. Input image values between 0 and 255 are scaled by (1/128) and
    then a value of –1 is added to them to ensure the range is `[-1,1)`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: In the quantized MobileNet model, '`normalized_image_tensor`' has values between
    `[0, 255]`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: In general, see the `preprocess` function defined in the feature extractor class
    in the TensorFlow Models repository at *models/research/object_detection/models*
    directory.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Image Segmentation
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Taking a step further beyond object detection, to get more precise locations
    of objects, we can perform object segmentation. As we saw earlier in this chapter,
    this involves making category predictions for each pixel in an image frame. Architectures
    such as U-Net, Mask R-CNN, and DeepLabV3+ are commonly used to perform segmentation
    tasks. Similar to object detection, there has been a growing trend of running
    segmentation networks in real time, including on resource-constrained devices
    such as smartphones. Being real time opens up many consumer app scenarios like
    face filters ([Figure 14-19](part0017.html#colorizing_hair_by_accurately_mapping_th)),
    and industrial scenarios such as detecting drivable roads for autonomous cars
    ([Figure 14-20](part0017.html#image_segmentation_performed_on_frames_f)).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![Colorizing hair with ModiFace app by accurately mapping the pixels belonging
    to the hair](../images/00012.jpeg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: Figure 14-19\. Colorizing hair with ModiFace app by accurately mapping the pixels
    belonging to the hair
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Image segmentation performed on frames from a dashcam (CamVid dataset)](../images/00300.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: Figure 14-20\. Image segmentation performed on frames from a dashcam (CamVid
    dataset)
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you might have grown accustomed to hearing in this book by now, you can accomplish
    much of this without needing much code. Labeling tools like Supervisely, LabelBox,
    and Diffgram can help not just label the data, but also load previously annotated
    data and train further on a pretrained object segmentation model. Moreover, these
    tools provide AI-assisted labeling, which can dramatically speed up (~10 times)
    the otherwise laborious and expensive labeling process. If this sounds exciting,
    you are in luck! We have included a bonus resource guide on how to learn and build
    segmentation projects on the book’s GitHub website (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Case Studies
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now look at how object detection is being used to power real-world applications
    in the industry.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Smart Refrigerator
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Smart fridges have been gaining popularity for the past few years because they
    have started to become more affordable. People seem to like the convenience of
    knowing what’s in their fridge even when they are not home to look inside. Microsoft
    teamed up with Swiss manufacturing giant Liebherr to use deep learning in its
    new generation SmartDeviceBox refrigerator ([Figure 14-21](part0017.html#detected_objects_along_with_their_classe)).
    The company used Fast R-CNN to perform object detection within its refrigerators
    to detect different food items, maintain an inventory, and keep the user abreast
    of the latest state.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 智能冰箱近几年来越来越受欢迎，因为它们开始变得更加实惠。人们似乎喜欢知道冰箱里有什么，即使他们不在家也能了解。微软与瑞士制造业巨头利勃海尔合作，在其新一代SmartDeviceBox冰箱中使用深度学习（图14-21）。该公司使用Fast
    R-CNN在其冰箱内执行目标检测，以检测不同的食物项目，维护库存，并让用户了解最新状态。
- en: '![Detected objects along with their classes from the SmartDeviceBox refrigerator
    (image source)](../images/00257.jpeg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![从SmartDeviceBox冰箱检测到的对象及其类别（图片来源）](../images/00257.jpeg)'
- en: Figure 14-21\. Detected objects along with their classes from the SmartDeviceBox
    refrigerator ([image source](https://oreil.ly/xg0wT))
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-21。从SmartDeviceBox冰箱检测到的对象及其类别（图片来源）
- en: Crowd Counting
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人群计数
- en: Counting people is not just something that only cats do while staring out of
    windows. It’s useful in a lot of situations including managing security and logistics
    at large sporting events, political gatherings, and other high-traffic areas.
    Crowd counting, as its name suggests, can be used to count any kind of object
    including people and animals. Wildlife conservation is an exceptionally challenging
    problem because of the lack of labeled data and intense data collection strategies.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 人群计数不仅仅是猫在窗户外凝视时才会做的事情。在许多情况下都很有用，包括管理大型体育赛事、政治集会和其他高流量区域的安全和后勤。人群计数，顾名思义，可以用于计算任何类型的对象，包括人和动物。野生动物保护是一个极具挑战性的问题，因为缺乏标记数据和强烈的数据收集策略。
- en: Wildlife conservation
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 野生动物保护
- en: 'Several organizations, including the University of Glasgow, University of Cape
    Town, Field Museum of Natural History, and Tanzania Wildlife Research Institute,
    came together to count the largest terrestrial animal migration on earth: 1.3
    million blue wildebeest and 250,000 zebra between the Serengeti and the Masaai
    Mara National Reserve, Kenya ([Figure 14-22](part0017.html#an_aerial_photograph_of_wildebeest_taken)).
    They employed data labeling through 2,200 volunteer citizen scientists using a
    platform called Zooniverse and used automated object detection algorithms like
    YOLO for counting the wildebeest and zebra population. They observed that the
    count from the volunteers and from the object detection algorithms were within
    1% of each other.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 包括格拉斯哥大学、开普敦大学、芝加哥自然历史博物馆和坦桑尼亚野生动物研究所在内的几个组织联合起来，统计了地球上最大的陆地动物迁徙：在塞伦盖蒂和肯尼亚马赛马拉国家保护区之间的1,300,000只蓝色角马和25万只斑马（图14-22）。他们通过2200名志愿公民科学家使用Zooniverse平台进行数据标记，并使用像YOLO这样的自动化目标检测算法来统计角马和斑马的数量。他们观察到志愿者和目标检测算法的计数相差不到1%。
- en: '![An aerial photograph of wildebeest taken from a small survey airplane (image
    source)](../images/00218.jpeg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![从一架小型勘测飞机拍摄的角马的航拍照片（图片来源）](../images/00218.jpeg)'
- en: Figure 14-22\. An aerial photograph of wildebeest taken from a small survey
    airplane ([image source](https://oreil.ly/hTodA))
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-22。从一架小型勘测飞机拍摄的角马的航拍照片（图片来源）
- en: Kumbh Mela
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 昆布梅拉
- en: Another example of dense crowd counting is from Prayagraj, India where approximately
    250 million people visit the city once every 12 years for the Kumbh Mela festival
    ([Figure 14-23](part0017.html#the_2013_kumbh_melacomma_as_captured_by)). Crowd
    control for such large crowds has historically been difficult. In fact, in the
    year 2013, due to poor crowd management, a stampede broke out tragically resulting
    in the loss of 42 lives.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个密集人群计数的例子来自印度的普拉亚格拉吉，大约每12年有2.5亿人参加昆布梅拉节（图14-23）。对于如此庞大的人群进行控制一直是困难的。事实上，在2013年，由于糟糕的人群管理，一场踩踏事件导致42人不幸丧生。
- en: In 2019, the local state government contracted with Larsen & Toubro to use AI
    for a variety of logistical tasks including traffic monitoring, garbage collection,
    security assessments, and also crowd monitoring. Using more than a thousand CCTV
    cameras, authorities analyzed crowd density and designed an alerting system based
    on the density of people in a fixed area. In places of higher density, the monitoring
    was more intense. It made the Guinness Book of World Records for being the largest
    crowd management system ever built in the history of humankind!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年，当地政府与拉森和图博签约，利用人工智能进行各种后勤任务，包括交通监控、垃圾收集、安全评估以及人群监测。当局使用了一千多个闭路电视摄像头，分析了人群密度，并设计了一个基于固定区域人群密度的警报系统。在人群密度较高的地方，监测更加密集。这使得它成为有史以来建立的最大人群管理系统，进入了吉尼斯世界纪录！
- en: '![The 2013 Kumbh Mela, as captured by an attendee](../images/00144.jpeg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![2013年昆布梅拉，由一名参与者拍摄](../images/00144.jpeg)'
- en: Figure 14-23\. The 2013 Kumbh Mela, as captured by an attendee ([image source](https://oreil.ly/OIy7N))
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-23。2013年昆布梅拉，由一名参与者拍摄（图片来源）
- en: Face Detection in Seeing AI
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Seeing AI中的人脸检测
- en: Microsoft’s Seeing AI app (for the blind and low-vision community) provides
    a real-time facial detection feature with which it informs the user of the people
    in front of the phone’s camera, their relative locations, and their distance from
    the camera. A typical guidance might sound like “One face in the top-left corner,
    four feet away.” Additionally, the app uses the facial-detection guidance to perform
    facial recognition against a known list of faces. If the face is a match, it will
    announce the name of the person, as well. An example of vocal guidance would be
    “Elizabeth near top edge, three feet away,” as demonstrated in [Figure 14-24](part0017.html#face_detection_feature_on_seeing_ai).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 微软的Seeing AI应用程序（为盲人和低视力社区）提供了实时人脸检测功能，通过该功能，它会告知用户手机摄像头前的人员、他们的相对位置以及与摄像头的距离。一个典型的引导可能会听起来像“左上角有一个脸，距离四英尺”。此外，该应用程序使用人脸检测引导来对已知人脸列表进行人脸识别。如果脸部匹配，它还会宣布该人的姓名。一个语音引导的例子可能是“伊丽莎白靠近顶部边缘，距离三英尺”，如[图14-24](part0017.html#face_detection_feature_on_seeing_ai)所示。
- en: To identify a person’s location in the camera feed, the system is running on
    a fast mobile-optimized object detector. A crop of this face is then passed for
    further processing to age, emotion, hair style, and other recognition algorithms
    on the cloud from Microsoft’s Cognitive Services. For identifying people in a
    privacy-friendly way, the app asks the user’s friends and family to take three
    selfies of their faces and generates a feature representation (embedding) of the
    face that is stored on the device (rather than storing any images). When a face
    is detected in the camera live feed in the future, an embedding is calculated
    and compared with the database of embeddings and associated names. This is based
    on the concept of one-shot learning, similar to Siamese networks that we saw in
    [Chapter 4](part0006.html#5N3C3-13fa565533764549a6f0ab7f11eed62b). Another benefit
    of not storing images is that the size of the app doesn’t increase drastically
    even with a large number of faces stored.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在摄像头视频中识别一个人的位置，系统正在运行一个快速的移动优化对象检测器。然后将这张脸的裁剪图传递给微软认知服务中的年龄、情绪、发型等识别算法进行进一步处理。为了以一种尊重隐私的方式识别人，该应用程序要求用户的朋友和家人拍摄他们脸部的三张自拍照，并生成脸部的特征表示（嵌入），该表示存储在设备上（而不是存储任何图像）。当未来在摄像头实时视频中检测到一张脸时，将计算一个嵌入并与嵌入数据库和相关名称进行比较。这是基于一次性学习的概念，类似于我们在[第4章](part0006.html#5N3C3-13fa565533764549a6f0ab7f11eed62b)中看到的孪生网络。不存储图像的另一个好处是，即使存储了大量的人脸，应用程序的大小也不会急剧增加。
- en: '![Face detection feature on Seeing AI](../images/00142.jpeg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![Seeing AI上的人脸检测功能](../images/00142.jpeg)'
- en: Figure 14-24\. Face detection feature on Seeing AI
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-24。Seeing AI上的人脸检测功能
- en: Autonomous Cars
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动驾驶汽车
- en: Several self-driving car companies including Waymo, Uber, Tesla, Cruise, NVIDIA,
    and others use object detection, segmentation, and other deep learning techniques
    for building their self-driving cars. Advanced Driver Assistance Systems (ADAS)
    such as pedestrian detection, vehicle type identification, and speed limit sign
    recognition are important components of a self-driving car.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 包括Waymo、Uber、特斯拉、Cruise、NVIDIA等在内的几家自动驾驶汽车公司使用目标检测、分割和其他深度学习技术来构建他们的自动驾驶汽车。行人检测、车辆类型识别和速限标志识别等高级驾驶辅助系统（ADAS）是自动驾驶汽车的重要组成部分。
- en: For the decision-making involved in self-driving cars, NVIDIA uses specialized
    networks for different tasks. For example, WaitNet is a neural network used for
    doing fast, high-level detection of traffic lights, intersections, and traffic
    signs. In addition to being fast, it is also meant to withstand noise and work
    reliably in tougher conditions such as rain and snow. The bounding boxes detected
    by WaitNet are then fed to specialized networks for more detailed classification.
    For instance, if a traffic light is detected in a frame, it is then fed into LightNet—a
    network used for detecting traffic light shape (circle versus arrow) and state
    (red, yellow, green). Additionally, if a traffic sign is detected in a frame,
    it’s fed into SignNet, a network to classify among a few hundred types of traffic
    signs from the US and Europe, including stop signs, yield signs, directional information,
    speed limit information, and more. Cascading output from faster networks to specialized
    networks helps improve performance and modularize development of different networks.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶汽车的决策中，NVIDIA使用专门的网络来执行不同的任务。例如，WaitNet是一个用于快速、高级检测交通灯、十字路口和交通标志的神经网络。除了快速外，它还能够抵抗噪音，并能够在雨雪等恶劣条件下可靠工作。WaitNet检测到的边界框然后被馈送到更详细分类的专门网络中。例如，如果在一帧中检测到交通灯，那么它将被馈送到LightNet——一个用于检测交通灯形状（圆形与箭头）和状态（红、黄、绿）的网络。此外，如果在一帧中检测到交通标志，它将被馈送到SignNet，一个用于对美国和欧洲几百种交通标志进行分类的网络，包括停车标志、让行标志、方向信息、速限信息等。从更快的网络到专门网络的级联输出有助于提高性能并模块化不同网络的开发。
- en: '![Detecting traffic lights and signs on a self-driving car using the NVIDIA
    Drive Platform (image source)](../images/00097.jpeg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![使用NVIDIA Drive平台在自动驾驶汽车上检测交通灯和标志（图片来源）](../images/00097.jpeg)'
- en: Figure 14-25\. Detecting traffic lights and signs on a self-driving car using
    the NVIDIA Drive Platform ([image source](https://oreil.ly/vcJY6))
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-25。使用NVIDIA Drive平台在自动驾驶汽车上检测交通灯和标志（[图片来源](https://oreil.ly/vcJY6)）
- en: Summary
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored the types of computer-vision tasks, including how
    they relate to one another and how they are different from one another. We took
    a deeper look at object detection, how the pipeline works, and how it evolved
    over time. We then collected data, labeled it, trained a model (with and without
    code), and deployed the model to mobile. We next looked at how object detection
    is being used in the real world by industry, government, and non-governmental
    organizations (NGOs). And then we took a sneak peek into the world of object segmentation.
    Object detection is a very powerful tool whose vast potential is still being discovered
    every day in numerous areas of our lives. What innovative use of object detection
    can you think of? Share it with us on Twitter [@PracticalDLBook](https://www.twitter.com/PracticalDLBook).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们探讨了计算机视觉任务的类型，包括它们之间的关系以及它们之间的区别。我们深入研究了目标检测，管道的工作原理以及它随时间的演变。然后我们收集数据，标记数据，训练模型（有或没有代码），并将模型部署到移动设备上。接着我们看了看目标检测在工业、政府和非政府组织（NGOs）中如何被使用。然后我们偷偷看了一眼目标分割的世界。目标检测是一个非常强大的工具，其巨大潜力仍在每天在我们生活的许多领域中被发现。你能想到哪些创新的目标检测用途？在Twitter上与我们分享[@PracticalDLBook](https://www.twitter.com/PracticalDLBook)。
