- en: 5 How do we constrain the behavior of LLMs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Constraining LLM behavior to make them more useful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The four areas where we can constrain LLM behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How fine-tuning allows us to update LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How reinforcement learning can change the output of LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying the inputs of an LLM using retrieval augmented generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may seem counterintuitive that you can make a model more useful by controlling
    the output the model is allowed to produce, but it is almost always necessary
    when working with LLMs. This control is necessitated by the fact that when presented
    with an arbitrary text prompt, an LLM will attempt to generate what it believes
    to be an appropriate response, regardless of its intended use. Consider a chatbot
    helping a customer buy a car; you do not want the LLM going off-script and talking
    to them about athletics or sports just because they asked something related to
    taking the vehicle to their kid’s soccer games.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss in more detail why you would want to limit,
    or constrain, the output an LLM produces and the nuances associated with such
    constraints. Accurately constraining an LLM is one of the hardest things to accomplish
    because of the nature of how LLMs are trained to complete input based on what
    they observe in training data. Currently, there are no perfect solutions. We will
    discuss the four potential places where an LLM’s behavior can be modified:'
  prefs: []
  type: TYPE_NORMAL
- en: Before training occurs, curating the data used to train the LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By altering how the LLM is trained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By fine-tuning the LLM on a set of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By writing special code after training is complete to control the outputs of
    the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These four cases are summarized in figure [5.1](#fig__constraint_stages). Each
    stage of developing an LLM feeds into the next. The fine-tuning stage, a second
    round of training done on a smaller data set, is the most important for how tools
    like ChatGPT function today and the most likely approach you might use in practice.
    The first, larger training stage we’ve learned about in chapters 2 to 4 is often
    referred to as *pretraining* because it occurs before fine-tuning makes the model
    useful. The model produced by the pretraining process is sometimes referred to
    as either a *base model* or a *foundation model* because it is a point from which
    to build a task-specific, or fine-tuned, model.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F01_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 One may intervene to change or constrain an LLM’s behavior in four
    places. The two stages of model training are shown in the middle of the diagram,
    where the model’s parameters are altered. On the left, one could also alter the
    training data before model training. On the right, one could intercept the model
    outputs after model training and write code to handle specific situations.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Due to the importance and effectiveness of fine-tuning, we will spend most of
    the chapter on that factor and how it may be performed.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Why do we want to constrain behavior?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are incredibly successful because they are the first technology to deliver
    on the idea of “Tell a computer what to do in plain English, and it does it.”
    By being very explicit about what you want to happen, establishing a specific
    level of detail and specifying a certain tone, you can get an LLM to be a shockingly
    effective tool. This detailed set of instructions is called a *prompt*, and the
    art of designing a good prompt has been referred to as *prompt engineering*. For
    example, we could develop a prompt for a car-selling bot as demonstrated in figure
    [5.2](#fig__car_prompt).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F02_Boozallen.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 Commercial LLMs like ChatGPT are designed to follow instructions
    (within some limits) and can perform a lot of low-cognition or pattern-matching
    tasks with very high efficacy. These tasks include stylized writing, such as pattern
    matching, and instruction following, such as roleplaying as a car salesperson.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You could give an LLM a prompt on organizing data into comma-separated values
    so that you can copy them into Excel. You could design a prompt about how to categorize
    free-form survey responses into summarized themes. In all cases, prompting is
    an exercise in limiting, or constraining, the behavior to a particular task and
    set of goals. Yet, the tokenization and training techniques we have discussed
    in the previous chapters do not enable this kind of instruction following.
  prefs: []
  type: TYPE_NORMAL
- en: Remembering that models do what they are trained to do is essential. In the
    case of a standard LLM, this task is to take a text passage and generate a continuation
    of that document that looks like a typical passage from the training corpus with
    the provided beginning. It is not trained to answer questions, think, summarize
    text, hold a conversation, or anything else. To get this desirable instruction-following
    behavior, we must perform fine-tuning, a second round of training with different
    objectives that will produce the intended behavior. You may wonder, “Why don’t
    we train LLMs for the task we want them to perform?” In most deep learning applications,
    we strongly recommend following the process we defined in chapter 4 to create
    a loss function that is specific, computable, and smooth. However, for the kinds
    of tasks that LLMs are good at, there are many reasons why this two-stage training
    process works well.
  prefs: []
  type: TYPE_NORMAL
- en: The first reason involves the breadth of knowledge required to complete specific
    goals. Think back to the task of a chatbot selling a car. If we aim to build a
    model that successfully sells cars, it would be great to construct a dataset of
    only car-relevant facts. But when the potential buyer wants to know if the car
    can fit all the needed equipment for a hockey player, how easy it will be to clean,
    whether it will be possible for their arthritic grandparent to get in and out
    of the passenger seats, or any host of other possible questions someone might
    have about how their car interacts with their life, you encounter the problem
    of enumerating every possible question you might receive about cars. There is
    no way to get all the information required to generate answers for every possible
    situation. Instead, we rely on the training processes we have discussed so far,
    which can be considered pretraining, to capture information from an extensive
    content collection containing text about sports, arthritis, etc. We hope this
    information helps the model be better prepared or generically helpful in answering
    broader questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the second and primary reason why we use a two-stage training process:
    obtaining hundreds of millions of documents that describe a specific problem to
    use as a part of the pretraining stage would be impossible. At the current state
    of the art, this massive scale is necessary to create the impressive capabilities
    seen in GPT. However, with relatively little effort, one can pretrain with hundreds
    of millions of pieces of general information, such as web pages, to impart models
    with general knowledge. Subsequently, it is often sufficient to fine-tune with
    just hundreds of documents to constrain the model to produce something usefully
    tailored to a task at hand. Obtaining a few hundred documents for a specific problem
    may be challenging but achievable.'
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, a second fine-tuning training stage can help constrain an LLM
    to some subset of useful behaviors because the original model is not incentivized
    to do what we want. In the following sections, we will present concrete examples
    of the different problems that crop up with base models that will make the reasons
    why this works evident.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Base models are not very usable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training an LLM following the process described in chapter 4 produces a model
    typically referred to as a *base model* because it can serve as a base platform
    for building applications or fine-tuned models. Unfortunately, base models are
    not very useful to most people because they don’t expose their underlying knowledge
    via a user-friendly UI, they can be challenging to keep on-topic, and sometimes
    they produce unsavory content. Base models are not even trained with the concept
    of being a chatbot like ChatGPT is.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Not all model outputs are desirable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, what a model thinks is likely to come next in a document is undesirable.
    There are several reasons for this, including
  prefs: []
  type: TYPE_NORMAL
- en: '*Memorization*—Sometimes, LLMs can generate long, exact copies of sequences
    found in their training data, which is often referred to as *memorization*, which
    refers to the idea that the text is being reproduced by memory from the training
    set. Memorization can be beneficial, such as memorizing the answers to specific
    factual questions. For example, if someone asks, “When was Abraham Lincoln born?”
    you want the model to regurgitate “February 12, 1809.” However, it can also be
    substantially detrimental if it leads a model to infringe copyright. If someone
    asks for “A copy of *Inside Deep Learning* by Edward Raff,” and the model produces
    a verbatim copy, Edward may be upset with you for copyright infringement!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bad things on the web*—Not everything found on the internet is something you
    would want to expose a user to. There is a lot of vile and hateful content on
    the internet, as well as factually incorrect info ranging from common misconceptions
    to conspiracy theories. While model developers often try to filter out this data
    before training the model, that’s not always possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Missing and new information*—Inconveniently, the world keeps evolving and
    growing more complex after we train our models. So a model trained oninformation
    up to 2018 will not know of anything that happened after, such as COVID-19 or
    the nightmare-fuel invention of necrobotics [1]. But you may want your model to
    know about these developments to remain useful, without having to pay a considerable
    cost to retrain your base model from scratch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Waiting for the legal system to catch up
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We are not your lawyers; this is not a law book! The legal problems around LLMs
    are complex, and there is a lot of nuance regarding fair use and infringement.
    Search engines can show you the content of their sources verbatim, but why? A
    combination of laws explicitly addressing these concerns, such as the Digital
    Millennium Copyright Act (DMCA), and precedents set by court rulings, such as
    Field v Google, Inc. (412 F.Supp. 2d 1106 [D. Nev. 2006]), establish acceptable
    and nonacceptable use over time. However, legislation and court cases take time
    to create, and the revolution of generative AI does not fit neatly into existing
    legal understanding.
  prefs: []
  type: TYPE_NORMAL
- en: You may want a nice, clean answer about what is and is not forbidden by law
    in the United States or your own country, and the likely answer is that such certainty
    does not yet exist for LLMs. Plus, we wouldn’t be caught dead giving such legal
    advice in print—we don’t even play lawyers on TV!
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-3.5 and 4 have been improved to avoid answering things they do not know
    (not always successfully), but we can look to some open-source base models like
    GPT-Neo to see what happens without proactive countermeasures. For example, if
    we make up the new fake drug, MELTON-24, and ask “What is MELTON-24, and can it
    help me sleep better?” we get the unhelpful response: “There is a great number
    of sleep problems that go with Melatonin, including insomnia and fatigue. This
    causes insomnia, and why it is important to avoid certain foods that can suppress
    melatonin.”'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the similarity of MELTON to melatonin and the prompt of “sleep”
    were enough for the model to catch onto the melatonin theme. Still, the answer
    is obviously nonsensical since MELTON-24 does not exist. Ideally, we want the
    model to recognize and respond, acknowledging its lack of information rather than
    producing more text like it has done here.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 Some cases require specific formatting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If a user asks for data in a specific format, such as a structured text format
    like JSON (for an example of a common format for exchanging data between computers,
    see [https://en.wikipedia.org/wiki/JSON](https://en.wikipedia.org/wiki/JSON)),
    and you do not match every opening or closing bracket or encode special characters
    properly, the output won’t satisfy their goals. It does not matter how sophisticated
    or close to correct the output may have been; formatting requirements are almost
    always strict requirements. We presented an example of this kind of problem in
    chapter 4 when we asked ChatGPT to write code in Modula-3, and it borrowed Python
    syntax that was invalid for Modula-3\. The code won’t compile if it violates syntax
    rules. An LLM’s probabilistic approach to generating text for specific desired
    outputs will not guarantee that all desired syntax rules are adhered to 100% of
    the time.
  prefs: []
  type: TYPE_NORMAL
- en: '5.2 Fine-tuning: The primary method of changing behavior'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we understand various reasons why we want to constrain and control
    the behavior of an LLM, we are better prepared to introduce new information to
    the model to address the problem we are trying to solve while avoiding the problem
    of producing harmful or legally questionable content. Remember, while there are
    four different places where we can intervene to change behavior, fine-tuning is
    far more effective than the others. Both closed source options like OpenAI [2]
    and open source tools like Hugging Face [3], among many others, have varying options
    for fine-tuning, making it the most accessible method for practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: 'Any fine-tuning method will have the same effect—producing a new variant of
    an LLM with updated parameters that control its behavior. As a result, it is possible
    to mix and match different fine-tuning strategies because the fundamental effect
    they produce is the same: a new set of parameters that can be used as is or altered
    yet again. One person’s base model could be another person’s fine-tuned model.
    This happens with many open source LLMs where an initial model (e.g., Llama) will
    be altered by another party (e.g., you can find many “Instruct Llama” models),
    which you may then further fine-tune to your data or specific use case.'
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward way to customize an LLM is by prompting and iteratively
    refining prompts until the desired behavior is obtained. However, fine-tuning
    is the next logical step if that does not work well. This step involves a moderate
    increase in effort and cost, such as collecting the data to fine-tune and acquiring
    the hardware for running a fine-tuning session.
  prefs: []
  type: TYPE_NORMAL
- en: Two fine-tuning methods you should know in particular are *supervised* fine-tuning
    (SFT) and the more intimidatingly named *reinforcement learning from human feedback*
    (RLHF). SFT is the more straightforward approach and is excellent for incorporating
    new knowledge into a model or simply giving it a boost in your preferred application
    domain. RLHF is more complex but provides a strategy for getting an LLM to follow
    harder and more abstract goals like “be a good chatbot.”
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Supervised fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most common way to influence a model’s output is SFT. SFT involves taking
    high-quality, typically human-authored, example content that captures information
    vital to your task but is not necessarily well reflected in the base model.
  prefs: []
  type: TYPE_NORMAL
- en: This often occurs because LLMs are trained on a large amount of generally available
    content, which may have minimal overlap with your specific needs. If you run a
    hospital, LLMs have seen very few doctors’ notes. If you run a law firm, an LLM
    probably has not seen too many deposition transcripts. If you run a repair shop,
    LLMs probably have not seen all the manuals you might have access to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Warning Fine-tuning is a helpful way to add new information to your model but
    can also have security ramifications. If you want to build an LLM on medical records,
    it makes sense to fine-tune the LLM on example medical records. But now there
    is a risk someone could get your LLM to reproduce sensitive information contained
    in that fine-tuning data because fundamentally, LLMs attempt to complete input
    based on the training data they have seen. The bottom line: do not train or fine-tune
    LLMs on data you want to keep private.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider again our example of the car company and its sales chatbot. A base
    model from a third-party source may generally be aware of cars but probably will
    not know everything about the company’s products. By fine-tuning a model on internal
    manuals, chat histories, emails, marketing materials, and other internal documents,
    you could ensure the model is prepared with as much information as possible about
    your cars. You could even write example documents about the merits of your vehicles
    over competitors, advantages, scripts, and more to ensure that the LLM is armed
    with the information you want it to have.
  prefs: []
  type: TYPE_NORMAL
- en: The mechanics of SFT are easy to explain. As we’ve alluded to, SFT simply needs
    more documents. They can be in any format from which text can be extracted. This
    constitutes all of the work necessary to apply SFT because SFT is just repeating
    the same training process you learned in chapter 4\. Figure [5.3](#fig__sft) shows
    that the process for SFT is the same as you saw previously. The difference is
    that the initial parameters are random and unhelpful the first time you train
    the base model. The second time you fine-tune, you start with the base model’s
    parameters that encode what the base model has learned by observing its training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F03_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 Supervised fine-tuning (SFT) is a simple approach to improving model
    results. You repeat the same process used to build the base model. Once the base
    model is trained on a large amount of general data, you continue training on the
    smaller specialized data collection.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Delightfully, you now have a good understanding of SFT. Like the original training
    process, it reuses the “predict the next token” task to ensure your model has
    information from the new documents built inside. As a direct consequence of predicting
    the next token, SFT also does not allow us to change the incentives of the LLM.
    For this reason, abstract goals like “Do not curse at the user” are difficult
    to achieve with SFT.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning pitfalls
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: By reusing the gradient descent strategy from chapter 4, all fine-tuning methods
    tend to inherit two problems around an LLM’s ability to return content on which
    it was trained. Since SFT is so simple, this is a good time for us to review the
    broader problems with fine-tuning beyond just SFT.
  prefs: []
  type: TYPE_NORMAL
- en: There are no guarantees that SFT will retain the information you provide correctly.
    This problem, known as *catastrophic forgetting* [4], occurs when you train the
    model on new data but do not continue training on older data, and the model begins
    to “forget” that older information. It is not easy to determine what will and
    will not be forgotten. Catastrophic forgetting has been a recognized problem since
    1989 [5]. In other words, fine-tuning is not purely additive; you give up something
    for it.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Reinforcement learning from human feedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At the time of writing, RLHF is the dominant paradigm for constraining models.
    As the name implies, it uses an approach from the field of *reinforcement learning*
    (RL). RL is a broad family of techniques where an algorithm must make multiple
    decisions toward maximizing a long-term goal, as shown in figure [5.4](#fig__rl),
    where four terms are used with a technical meaning:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Agent*—The entity/AI/robot with some overarching goal that it wishes toaccomplish
    that may take multiple actions to achieve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Action*—The space of all possible things the agent may be able to perform
    or engage in to advance the agent’s goals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Environment*—The place/object/space affected by an action. The environment
    may or may not change as a result of the action, actions taken by other agents,
    or the natural continuous change of the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reward*—The numeric quantification of improvement (which may be negative)
    that may or may not occur after any given number of actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F04_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 RL is about iterative interactions, where the reward for your actions
    may not materialize for a long time and requires multiple steps to achieve. For
    a chatbot like ChatGPT, the environment is the conversation with a user, and the
    actions are the infinite possible texts that ChatGPT might complete. The reward
    becomes, in some sense, the user’s satisfaction with the chatbot at the end of
    the conversation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In the example of an LLM being used as a chatbot to interact with people, the
    users are the environment. The LLM is itself the agent, and the text it can produce
    is the action. This leaves one final thing to specify: the reward. If we were
    to get a user to score a +1 for a good conversation with a chatbot (e.g., no foul
    language, no lying, provided helpful responses) and a -1 for a lousy conversation
    (e.g., it suggested destroying all humans), then we would be adding human feedback
    to our reinforcement learning.'
  prefs: []
  type: TYPE_NORMAL
- en: An astute reader might notice that a reward sounds suspiciously similar to the
    loss function discussed in chapter 4\. In fact, our example of a good and bad
    conversation falls into the very subjective and difficult-to-quantify regime that
    we stated was a bad example of a loss function. The +1/-1 reward is not smooth
    because the value points in one direction or the other, and there is no middle
    ground, another poor characteristic for a loss function.
  prefs: []
  type: TYPE_NORMAL
- en: One of the powerful things about RL is that it can work with noncontinuous and
    hard-to-quantify objectives. We use the term *reward* instead of *loss* to imply
    the difference between these two situations. Generally, the types of objectives
    that RL can learn are referred to as *nondifferentiable*. As a result, these objectives
    can’t be learned using the same mathematical techniques like gradient descent,
    which we covered when describing how neural networks learn in chapter 4\. We will
    explain how RLHF works specifically in a moment. The caveat lector of RL is that
    it can be computationally expensive and require a significant amount of data.
    RL is a notoriously challenging way to learn. It often works worse than other
    fine-tuning techniques like SFT because RL requires many more examples of the
    “right” and “wrong” way of doing things than other approaches, and since we are
    using human feedback to guide RLHF, the results are not always perfect. For example,
    in figure [5.5](#fig__dolphinchat), RLHF cannot help an LLM understand basic instructions
    outside of what it has seen explicitly during RLHF training because it does not
    add any capability to perform basic logic, such as understanding the user’s request
    to avoid displaying information about dolphins, to the underlying model.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F05_Boozallen.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 RLHF is quite good at getting LLMs to avoid known, specific problems.
    However, it does not endow the model with new tools to handle novel problems.
    The desire to talk about the Miami Dolphins as the logical thing to say next after
    asking about football in Miami violates the first request to avoid ever mentioning
    dolphins.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: LLMs do not perform reasoning in the same way that we humans think of reasoning.
    You can get very far by collecting hundreds of millions of examples of “everything,”
    but the world is weird. We have little evidence that LLMs can reliably produce
    satisfying responses when something novel occurs. However, RLHF is the best so
    far for constraining how an LLM behaves. Despite its challenges, RL presents a
    way of learning that is not available with gradient-based methods that require
    differentiable objectives. Most importantly, ChatGPT has shown that RL can work
    in many cases. So let us dive deeper into how RLHF works.
  prefs: []
  type: TYPE_NORMAL
- en: '5.2.3 Fine-tuning: The big picture'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SFT and RLHF are the two primary methods of fine-tuning an LLM. SFT can work
    with thousands of documents or samples, whereas RLHF often requires tens of thousands
    of examples. That should not stop you from investigating if you have less data,
    but if you have less data, it may be a better use of your time to develop better
    prompts.
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, SFT and RLHF are not mutually exclusive. They both modify
    the underlying parameters of the model, and you can apply one after the other
    to obtain the benefits of each approach. They are also not the only fine-tuning
    methods that currently exist. For example, new fine-tuning methods are being developed
    that remove concepts from an LLM as a way of forcing a model to ignore data it
    has learned from after it has been trained [6]. Additional techniques for model
    alteration will be developed in the coming years. All will likely require you
    to do some data collection, but they will involve less work overall than trying
    to build an LLM from scratch yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 The mechanics of RLHF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To describe how RLHF works, we will introduce an incomplete version of RLHF,
    explain why it does not work, and then explain how to fix it. In this section,
    we will not discuss the detailed math used by RLHF, as it would not give you any
    particularly great insights into RLHF from a high level. If you want to learn
    more about the nitty-gritty details, we recommend starting with "Implementing
    RLHF: Learning to Summarize with trlX" [7] after you’ve completed this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Beginning with a naive RLHF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, let’s look at the incomplete and naive version of RLHF. We have discussed
    how RL can learn with nondifferentiable objectives. So let us assume that we have
    a human who will score an LLM’s output with a *quality reward*, where +1 indicates
    a good response and -1 is an inadequate response. This quality reward is simply
    an arbitrary score we assign to the output produced by the LLM to indicate that
    one example is somehow better than others.
  prefs: []
  type: TYPE_NORMAL
- en: So if a user requests of an LLM, “Tell me a joke,” and the LLM produces a response
    of “How many ducks does it take to screw in a light bulb?” we might assign a score
    of +1 for a (reasonably) good joke. If the LLM instead produces a sentence like
    “Dogs are evil,” we will assign a score of -1 because it is not even attempting
    to make a joke. Because RL is difficult to do using simple quality rewards of
    +1 and -1, we will add additional information for the RL algorithm, such as the
    probabilities of each generated token. This way, the RL algorithm knows how probable
    each token may be. This whole process is summarized in figure [5.6](#fig__RLHF_naive).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F06_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 A naive and incomplete version of RLHF. The dashed lines represent
    text being sent from one component to another. Since text is incompatible with
    gradient descent, a more difficult RL algo-rithm must be used instead. This allows
    us to alter the weights of the LLM based on a quality score for the LLM’s outputs.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Why provide RL with probabilities?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It may seem odd that we are providing the RL algorithm with the probabilities
    of each token. There are deeper mathematical reasons why this is useful, which
    we will not get into in this chapter. But for some intuition, a good joke often
    requires misdirection or surprise. If all the probabilities of a sequence are
    high values (near 1.0), it is probably not a good joke because it’s too predictable.
  prefs: []
  type: TYPE_NORMAL
- en: Broadly, across natural language processing, producing good generated text is
    a balancing act between making something probable (i.e., likely to occur) and
    not making it too probable (i.e., repetitive).
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 The quality reward model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We described the quality reward as human-assigned scores for every prompt completion.
    Although scoring completions manually in real time would technically work, it
    would be unreasonable due to the level of effort involved. However, human feedback
    is still incorporated via the quality reward. Instead, we train a neural network
    as a *reward model*. This is accomplished by having people manually collect hundreds
    of thousands of prompt and completion pairs and scoring them as good or bad. These
    scorings become the labeled data used to train the reward model, as shown in figure
    [5.7](#fig__reward_model).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F07_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 The reward model is trained like a standard supervised classification
    algorithm. A neural network, which could be an LLM itself or another simpler network
    like a convolutional or recurrent neural network, is trained to predict how a
    human would score a prompt completion pair. Because neural networks are differentiable,
    this training works and provides a tool that stands in as the “human” in RLHF.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Collecting hundreds of thousands of scored prompts and completion pairs is expensive
    but doable (e.g., [https://huggingface.co/datasets/Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)),
    especially when using crowd-sourcing tools like Mechanical Turk ([https://www.mturk.com/](https://www.mturk.com/)).
    That is a lot of data to curate manually but orders of magnitude smaller than
    the billions of tokens used to create the initial base models. These RLHF datasets
    must be large because you must cover many scenarios, questions, and requests that
    a user might provide. As we already saw in figure [5.5](#fig__dolphinchat) with
    the dolphin example, RLHF tends to work for relatively straightforward and known
    topics. So breadth in handling different situations comes directly from breadth
    in the fine-tuning data.
  prefs: []
  type: TYPE_NORMAL
- en: Note We have been using +1/-1 as the example of providing a quality reward because
    it is the easiest to describe. Since RL does not need gradients, you can use any
    score relevant to your problem. Using a ranking score, where you compare multiple
    completions for a given prompt and rank them from best to worst, is more popular
    and more effective because you are grading multiple completions against each other
    simultaneously. Regardless, providing positive and negative feedback remains fundamentally
    the same.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 The similar-but-different RLHF objective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once you have trained a reward model, you can create and score as many prompts
    as you desire for the RLHF process. The human feedback is baked into the reward
    model and can now be distributed, parallelized, and reused. The only remaining
    problem is that the current naive version of RLHF is incentivized purely to maximize
    the quality reward, which is not the sole goal RL must focus on.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result, the model will start to degrade over time by producing gibberish
    and nonsensical outputs that are not high quality and would not be valuable to
    any reader. This degradation is related to a phenomenon called *adversarial attacks*,
    where it is surprisingly easy to trick a neural network into absurd decisions
    with relatively minor changes to the input. Adversarial machine learning (AML)
    is fast evolving and has its own rabbit hole of complexity, so we’ll defer that
    discussion to other folks [8]. But the naive implementation of RLHF we describe
    in figure [5.6](#fig__RLHF_naive) essentially performs an adversarial attack against
    an LLM because it will focus only on maximizing the quality reward, not on being
    useful to the user. Essentially, this is Goodhart’s law happening to AI/ML: “When
    a measure becomes a target, it ceases to be a good measure.”'
  prefs: []
  type: TYPE_NORMAL
- en: To address this problem, we must add a second objective to the RL algorithm.
    We will calculate a second reward for the similarity between the original base
    LLM’s output and the fine-tuned LLM’s output. Conceptually, this reward can be
    considered a reward when the fine-tuned LLM produces better output, similar to
    how the original LLM behaved. It prevents the model from going off the rails by
    getting too novel. Fundamentally, we want the generated output of the fine-tuned
    LLM to be grounded by the training data initially observed by the original LLM.
    We don’t want the fine-tuned model to get so creative that it generates nonsense.
    This reward is added to the RL algorithm to stabilize the fine-tuning. Figure
    [5.8](#fig__RLHF_full) provides the complete picture of how RLHF works.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F08_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 The full version of RLHF. The dashed lines are text and require RL
    to update the parameters. The original LLM is the base model without any alterations,
    while the LLM to fine-tune starts as the base model but is altered to improve
    the quality of its outputs. The similarity and quality reward components are provided
    with word probabilities to improve calculation. RL adjusts the parameters by combining
    the quality and similarity scores.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A model that learns to produce gibberish output would receive a high penalty
    for lack of similarity, discouraging the model from becoming too different. A
    model that produces the exact same outputs will receive a low quality score, discouraging
    a lack of change. The balance of both does an excellent job of achieving a Goldilocks
    effect that allows the model enough flexibility to change without causing it to
    lose its human-like output.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Other factors in customizing LLM behavior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-tuning is the dominant means of altering the behavior of an LLM, but fine-tuning
    is not foolproof and is not the only place where behavior changes can occur. Our
    focus on fine-tuning is based on the value of RLHF in producing LLM behaviors
    beyond simple next-token prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F09_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 In addition to fine-tuning, you can change the model’s behavior by
    altering the training data, altering the base model training process, or modifying
    the model outputs by writing code to handle specific situations.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The other three stages where LLM behavior can be modified, described in figure
    [5.9](#fig__constraint_stages2), are not easily accessible to you as a user. However,
    we will briefly review the other stages now, along with some key details you should
    know for completeness. These factors can help you understand what is challenging
    to achieve by fine-tuning and the scope of questions you might want to investigate
    in your LLM provider.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 Altering training data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The adage “garbage in, garbage out” is evergreen in all areas of ML. You may
    indeed notice that OpenAI [9] and Google [10] provide many low-level technical
    details about how they develop their LLMs but much less detail on the data used
    for building the LLM. That is because most of the “secret sauce” in building capable
    LLMs is around data curation—developing a collection of data representing diverse
    tasks, high-quality language use, and a spectrum of different situations. The
    size and quality of the data sets used to train, validate, and test LLMs matter.
  prefs: []
  type: TYPE_NORMAL
- en: The size and quality of data have become especially pertinent as LLM-generated
    content works its way into regular use and back online. For example, an estimated
    6% to 16% of academic peer reviews are using LLMs [11], and it is highly likely
    that many copyediting services will soon be using these new technologies. This
    increased use potentially creates a negative feedback cycle. As the amount of
    data generated by LLMs grows, there will be proportionally less non-LLM content
    available for LLMs to train on. This will result in an overall decrease in the
    diversity of language available and, thus, the novelty that LLMs will be able
    to capture. In turn, the quality of an LLM trained on newer data is reduced [12].
    This problem will likely be significant in keeping LLMs up to date, as curating
    a high-quality dataset will not be as simple as before.
  prefs: []
  type: TYPE_NORMAL
- en: There is also the problem that LLMs can only reflect information available at
    training and are disproportionally more likely to reflect information that is
    more prevalent in training. If you want an LLM that does not curse or use racist
    language, you must work to scrub your dataset of all cursing and racist language.
  prefs: []
  type: TYPE_NORMAL
- en: However, this problem is potentially a double-edged sword. If we want our LLM
    to know how to recognize and appropriately reject racist or foul language, it
    must know what racist and foul language are. You can imagine using prompting on
    an LLM that has never seen any racist text to “teach” the LLM to use racist words
    in a context that, without knowing `X` is racist, appears benign. But in the final
    form, we, as readers who are aware of racism, would recognize the sentence as
    objectionable. This problem, as of now, has no answer but is something to be mindful
    of.
  prefs: []
  type: TYPE_NORMAL
- en: Altering data is also important, as it is your only chance to influence how
    tokenization is performed in an LLM. As discussed in chapter 2, different approaches
    to tokenization have tradeoffs, but the choices you make are forever baked into
    the model once you start training.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2 Altering base model training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training data privacy must be a significant concern when training or fine-tuning
    LLMs. Generally, it is possible to reconstruct a model’s training data by crafting
    inputs into a model in a special way. In some cases, LLMs have been shown to generate
    the exact passages on which they were trained. This is problematic if the training
    data contains private information, such as personally identifiable information
    (PII), private health information (PHI), or some other class of sensitive data.
    A user of a model could, perhaps unwittingly, provide a prompt that reveals this
    data verbatim.
  prefs: []
  type: TYPE_NORMAL
- en: Initial training of an algorithm is an ideal place to mitigate some of these
    privacy concerns by using a technique known as *differential privacy* (DP). DP
    is complex, so if you want to learn more, we recommend the book *Programming Differential
    Privacy* [13]. In short, DP adds a carefully constructed amount of random noise
    to provide provable guarantees about data privacy in the model training process.
    DP does not handle everything, but it provides much more protection than what
    is available with most algorithms today.
  prefs: []
  type: TYPE_NORMAL
- en: So why hasn’t everyone done just that? Well, adding noise naturally tends to
    reduce the quality of the result. Large training runs are expensive, costing hundreds
    of thousands to millions of dollars each. If you had to do ![equation image](../Images/eq-chapter-5-102-1.png)
    more training runs to set your privacy parameters correctly, you would have a
    million to tens-of-millions-of-dollars problem. But with DP becoming better every
    year, we suspect it will become more prevalent over time.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.3 Altering the outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, we can examine the tokens being produced and write code to change its
    behavior based on the combinations of tokens generated by the model. After fine-tuning,
    this is the second most likely stage that a consumer of LLMs will use to modify
    their behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier in this chapter, we discussed a common need for LLMs to generate output
    that adheres to a precise format, such as XML or JSON. Implementing formatting
    requirements like these is a common problem with LLMs. Any single failed prediction
    results in a failure to generate valid output. You can see an example of this
    type of failure in figure [5.10](#fig__force_output), where we ask the LLM to
    complete some Python code; the next token should be a semicolon (;), but it erroneously
    attempts a newline (\ n) instead.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F10_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 By writing code that enforces a format specification, you can catch
    invalid output from an LLM as it is being generated. Once detected, having the
    LLM produce the next most likely token until a valid output is found is a simple
    way to improve the situation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Various tools exist (e.g., [https://github.com/noamgat/lm-format-enforcer](https://github.com/noamgat/lm-format-enforcer))
    for specifying strict formats as a part of the LLM’s decoding step. If these tools
    detect a parse error, they immediately regenerate the last token until a valid
    output is produced.
  prefs: []
  type: TYPE_NORMAL
- en: More sophisticated approaches to selecting the next token are possible. Still,
    the important lesson here is the ability to use the intermediate outputs to make
    decisions before generating the entire output. Even simple old-school “go/no-go”
    lists are valuable tools for catching bad behavior. You do not need to pass an
    output to the user in true real time; you can always introduce an artificial delay
    so that you can see more of the response before sending it to the user. This gives
    you time to chat against bad language filters or other hard-coded checks. If a
    match occurs, just like in figure [5.10](#fig__force_output), you can regenerate
    an output or abort the user’s session.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Integrating LLMs into larger workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point in the chapter, we have covered some basic approaches to manipulating
    an LLM to produce more desirable and consistent outputs. So far, we have focused
    on techniques that involve the LLM itself, whether through prompting, manipulating
    training data, or fine-tuning a base model. In this section, we will explore how
    to tailor the output produced by an LLM by integrating the inputs and outputs
    of LLMs into multistep chains of operations to achieve more tailored results.
    This space is quickly evolving, so we will briefly cover one concrete example
    of integrating an LLM into a broader information retrieval workflow and then discuss
    a general-purpose tool to show you how to customize LLM outputs using multiple
    interactions with an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.1 Customizing LLMs with retrieval augmented generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Retrieval augmented generation* (RAG) is a technique that allows us to produce
    answers from an LLM while reducing the likelihood of generating nonsensical or
    otherwise errant explanations. The “retrieval” component of the RAG moniker should
    give you a helpful hint as to how the technique operates. When a user provides
    input to a RAG system, it uses an LLM to create a query that is run against a
    search engine that contains an index of documents. Depending on the use case,
    this might be an index of general information, such as Google, or a subject-specific
    index, such as a collection of automotive marketing materials. In response to
    the query, the search engine generates a list of relevant documents. The RAG system
    then uses the LLM to extract information from those documents to generate better
    answers. To do this, the RAG system combines the contents of the retrieved documents
    with the original user query to create a comprehensive prompt for the LLM that
    will result in a better response. This method tends to work well because instead
    of asking an LLM to generate a response based on its training or fine-tuning data,
    we are now asking the LLM to generate a response to input by summarizing a set
    of documents relevant to a regular old search engine query and providing that
    set of relevant documents from which to draw its answers to the LLM. In other
    words, we’re helping the LLM focus on the data it needs to properly answer a given
    question. We describe this process in figure [5.11](#fig__rag) and compare it
    with the normal LLM use cases we have described so far.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The two most significant benefits of the RAG approach thus far are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The output of a RAG system is more accurate, factually correct, or otherwise
    useful to the user’s original question because it is based on specific sources
    contained in a document index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM can generate citations or references to the source documents used to
    produce its responses, allowing users to validate or correlate against the original
    source material.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F11_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 On the left, we show the normal use of an LLM of a user asking about
    how to write JSON. LLMs naturally have the chance of producing errant outputs,
    which we want to minimize. On the right, we show the RAG approach. By using a
    search engine, we can find documents that are relevant to a query and combine
    them into a new prompt, giving the LLM more information and context to produce
    a better answer.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The latter point regarding citations is particularly important. RAG will not
    solve all of LLMs’ problems because the LLM still generates the final output in
    a RAG system. The LLM may still produce errors or hallucinations due to content
    that it cannot find or that doesn’t exist. It is also possible that the LLM will
    not accurately capture or represent the content of any of the source documents
    it uses. As a result, the utility of the RAG approach is directly related to the
    quality of the search it performs and the documents that are returned. The bottom
    line is that if you can’t build an effective search engine for your problem, you
    can’t build an effective RAG model.
  prefs: []
  type: TYPE_NORMAL
- en: Context size
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When thinking about LLMs, it is important to consider one aspect of LLMs known
    as the *context size*. The context size of the LLM determines how many tokens
    it can computationally handle in a single request for completions. You can think
    of it as the amount of data that an LLM is able to look at when receiving input
    in the form of a prompt. For example, GPT-3 has a context size of 2,048 tokens.
    However, in chatbots, for example, the context is often used to hold a running
    transcript of the entire conversation, including any LLM outputs. If you have
    a conversation with GPT-3 that goes beyond 2,048 tokens in length, you’ll find
    that GPT-3 often loses track of some of the things discussed early on in the chat.
  prefs: []
  type: TYPE_NORMAL
- en: Context size is an enabling and limiting factor for RAG use. If a RAG system
    retrieves an entire book for your LLM to digest, your LLM will require a huge
    context size to be able to use it. Otherwise, the LLM can only consume the first
    part of a retrieved document (up to the LLM’s context size) and may miss information.
    As a result, context size is an important operational characteristic you should
    consider when choosing a model. Some models today, such as X’s Grok, can handle
    up to 128,000 tokens as their context size. While large context sizes like Grok’s
    increase the hard limit of what an LLM can consume, the effectiveness of LLMs
    when dealing with large amounts of input enabled by larger context sizes is still
    an active area of study.
  prefs: []
  type: TYPE_NORMAL
- en: You may notice in figure [5.11](#fig__rag) that we have to create a new prompt.
    We added the prefix “Answer the question:” followed by the postfix “Using the
    following information:” Hypothetically, you could obtain better results by tweaking
    this prompt. You may get thoughts about adding some instructions like “Ignore
    any of the following information if it is not relevant to the original question.”
    These ideas are starting to get into prompt engineering, the practice of tweaking
    and modifying the text going into an LLM to change its behavior, as we talked
    about earlier in chapter 4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt engineering is indeed useful and a good way to combine multiple calls
    to an LLM to improve results. For example, you could try to improve your search
    results by asking the LLM to rewrite the question. (This discussion touches on
    a classic area of information retrieval called *query expansion*, if you wish
    to learn more on the topic.) However, prompt engineering can be very brittle:
    any update to an LLM may change what prompts do or don’t work, and it would be
    a pain to have to rewrite every prompt—especially as you get into anything more
    complex, like a RAG model or something even more sophisticated.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.2 General-purpose LLM programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although still new, we are already starting to see programming libraries and
    other software tools built using LLMs as a component of custom applications. One
    we particularly like is DSPy ([https://dspy.ai](https://dspy.ai)), which can make
    it easier to build and maintain programs that attempt to alter the inputs to and
    outputs of an LLM. A good software library will hide details that get in the way
    of productivity, and DSPy does a good job of abstracting away the following tasks
    around LLM usage:'
  prefs: []
  type: TYPE_NORMAL
- en: Integrating the specific LLM being used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing common patterns of prompting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tweaking the prompts for your desired combination of data, task, and LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is not a coding book, so a full tutorial on DSPy is out of scope. But it
    is illustrative to look at the ways DSPy can be used to implement the RAG model
    we described in section [5.5.1](#sec__rag). It will require that we pick an LLM
    to use (GPT-3.5, in this case), as well as a database of information (Wikipedia
    will work well), and define the RAG algorithm. DSPy works by defining a default
    LLM and database used by all components (unless you intervene), making it easy
    to separate and replace the parts being used. This process is shown in the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1 Simplest RAG in DSPy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Uses OpenAI''s GPT-3.5, which can be swapped out with other online or local
    LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Uses the ColBERTv2 algorithm to vectorize a copy of Wikipedia'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Uses the LLM and document database we just created'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Searches for the three most relevant documents from the database'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Specifies a ``signature" string, which defines the inputs and output of
    the LLM'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Calls the functions to build a RAG model. The parameter names match the
    names in the signature.'
  prefs: []
  type: TYPE_NORMAL
- en: This code sets the aforementioned choices in LLMs and databases as the defaults,
    making it just as easy to replace OpenAI with another online LLM or a local one
    such as Llama. The `class RAG(dspy.Module):` class then defines the RAG algorithm.
    The initializer only has two parts.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need a way to search a database of strings based on vectorized documents,
    which is defined with `ColBERTv2`. It uses an older—as in just four years ago
    (wild how fast the field is moving)—but much faster language model for speed and
    efficiency. Remember, the larger language model (that is, the more expensive to
    run) just needs reasonable documents to be retrieved. While ColBERTv2 probably
    won’t do as good a job as GPT-3.5, it is more than good enough to get you the
    right documents most of the time. The `dspy.Retrieve` then uses this default database
    for searching, so there is no need to specify anything more than how many documents
    to retrieve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, we need to combine the questions and documents into a query for the
    LLM. In DSPy, the prompt is abstracted away from us. Instead, we write what DSPy
    calls a *signature*, which you can think of as the inputs and outputs of a function.
    These should be given meaningful English names so that DSPy can generate a good
    prompt for you. (Under the hood, DSPy uses a language model to optimize prompts!)
    In this case, we have two inputs (`question` and `relevant_documents`) separated
    by a comma. The `->` is used to denote the start of the outputs, of which we have
    only one: the `answer` to the question.'
  prefs: []
  type: TYPE_NORMAL
- en: Note DSPy supports some basic types in signatures. For example, you can enforce
    that the answer must be an integer by denoting `"question, relevant``_documents
    -> answer:int"` in the string. This command will apply the same technique to regenerating
    on errors that we just learned about in figure [5.10](#fig__force_output).
  prefs: []
  type: TYPE_NORMAL
- en: That is all it takes to define our RAG model! The objects are called and passed
    out in the `forward` function, but you can modify this code to add additional
    details if you want. You can convert everything to lowercase, run a spell checker,
    or use whatever kind of code you want here. This approach lets you mix and match
    programming rules with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: You can also easily modify the RAG definition to include new constraints and
    write code to have an LLM perform validation. More importantly, DSPy supports
    using a training/validation set to tune the prompts better, fine-tune local LLMs,
    and help you create an empirically tested, improved, and quantified model to achieve
    your goals without having to spend a lot of time on LLM-specific details. Adopting
    tools like this early will give you a far more robust solution that allows you
    to upgrade to newer architectures more easily.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can intervene to change a model’s behavior in four places: the datacollection/tokenization,
    training the initial base model, fine-tuning the base model, and intercepting
    the predicted tokens. All four places are important, but fine-tuning is the most
    effective place for most users to make changes that lower the cost and provide
    the optimal ability to change the model’s goals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised fine-tuning (SFT) performs the normal training process on a smaller
    bespoke data collection and is useful for refining the model’s knowledge of a
    particular domain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning from human feedback (RLHF) requires more data, but it
    allows us to specify objectives more complex than “predict the next token.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use existing tools like syntax checkers to detect incorrect LLM outputs
    in cases where the output format must be strict, such as for JSON or XML. Generation
    and syntax checking can be run in a loop until the output satisfies the necessary
    syntax constraints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval augmented generation (RAG) is a popular method of augmenting the input
    of an LLM by first finding relevant content via a search engine or database and
    then inserting it into the prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding frameworks like DSPy are beginning to emerge that separate the specific
    LLM, vectorization, and prompt definition from the logic of how inputs and outputs
    from the LLM are modified for a specific task. This method allows you to build
    more reliable and repeatable LLM solutions that can quickly adapt to new models
    and methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
