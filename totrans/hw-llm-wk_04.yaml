- en: 5 How do we constrain the behavior of LLMs?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 我们如何约束LLMs的行为？
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Constraining LLM behavior to make them more useful
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过约束LLMs的行为使其更有用
- en: The four areas where we can constrain LLM behavior
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以约束LLMs行为的四个领域
- en: How fine-tuning allows us to update LLMs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调如何使我们能够更新LLMs
- en: How reinforcement learning can change the output of LLMs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习如何改变LLMs的输出
- en: Modifying the inputs of an LLM using retrieval augmented generation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用检索增强生成修改LLMs的输入
- en: It may seem counterintuitive that you can make a model more useful by controlling
    the output the model is allowed to produce, but it is almost always necessary
    when working with LLMs. This control is necessitated by the fact that when presented
    with an arbitrary text prompt, an LLM will attempt to generate what it believes
    to be an appropriate response, regardless of its intended use. Consider a chatbot
    helping a customer buy a car; you do not want the LLM going off-script and talking
    to them about athletics or sports just because they asked something related to
    taking the vehicle to their kid’s soccer games.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来似乎有些反直觉，你可以通过控制模型允许产生的输出来使模型更有用，但与LLMs一起工作时几乎总是必要的。这种控制是由于当被提供一个任意的文本提示时，LLMs将尝试生成它认为适当的响应，而不管其预期用途如何。考虑一个帮助客户购买汽车的聊天机器人；你不希望LLMs偏离脚本并与他们谈论体育或运动，仅仅因为他们询问了与将车辆带到孩子足球比赛相关的事情。
- en: 'In this chapter, we will discuss in more detail why you would want to limit,
    or constrain, the output an LLM produces and the nuances associated with such
    constraints. Accurately constraining an LLM is one of the hardest things to accomplish
    because of the nature of how LLMs are trained to complete input based on what
    they observe in training data. Currently, there are no perfect solutions. We will
    discuss the four potential places where an LLM’s behavior can be modified:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更详细地讨论为什么你想限制或约束LLMs产生的输出以及与这种约束相关的细微差别。由于LLMs是根据训练数据中观察到的内容完成输入的训练性质，准确约束LLMs是完成的最困难的事情之一。目前还没有完美的解决方案。我们将讨论四个可能修改LLMs行为的潜在位置：
- en: Before training occurs, curating the data used to train the LLM
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练发生之前，整理用于训练LLMs的数据
- en: By altering how the LLM is trained
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过改变LLMs的训练方式
- en: By fine-tuning the LLM on a set of data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在数据集上微调LLMs
- en: By writing special code after training is complete to control the outputs of
    the model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在训练完成后编写特殊代码来控制模型的输出
- en: These four cases are summarized in figure [5.1](#fig__constraint_stages). Each
    stage of developing an LLM feeds into the next. The fine-tuning stage, a second
    round of training done on a smaller data set, is the most important for how tools
    like ChatGPT function today and the most likely approach you might use in practice.
    The first, larger training stage we’ve learned about in chapters 2 to 4 is often
    referred to as *pretraining* because it occurs before fine-tuning makes the model
    useful. The model produced by the pretraining process is sometimes referred to
    as either a *base model* or a *foundation model* because it is a point from which
    to build a task-specific, or fine-tuned, model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个案例总结在图[5.1](#fig__constraint_stages)中。LLMs开发的每个阶段都为下一个阶段提供输入。微调阶段，在更小的数据集上进行的第二轮训练，对于ChatGPT等工具今天的功能至关重要，并且可能是你在实践中最可能使用的方法。我们在第2章到第4章中了解到的前一个更大的训练阶段通常被称为*预训练*，因为它发生在微调使模型有用之前。预训练过程产生的模型有时被称为*基础模型*或*基础模型*，因为它是一个构建特定任务或微调模型的起点。
- en: '![figure](../Images/CH05_F01_Boozallen.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F01_Boozallen.png)'
- en: Figure 5.1 One may intervene to change or constrain an LLM’s behavior in four
    places. The two stages of model training are shown in the middle of the diagram,
    where the model’s parameters are altered. On the left, one could also alter the
    training data before model training. On the right, one could intercept the model
    outputs after model training and write code to handle specific situations.
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.1 可以在四个地方干预以改变或约束LLMs的行为。图中中间部分显示了模型训练的两个阶段，其中模型的参数被改变。在左侧，一个人也可以在模型训练之前改变训练数据。在右侧，一个人可以在模型训练之后拦截模型输出并编写代码来处理特定情况。
- en: Due to the importance and effectiveness of fine-tuning, we will spend most of
    the chapter on that factor and how it may be performed.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于微调的重要性和有效性，我们将在本章的大部分内容中讨论这个因素及其执行方式。
- en: 5.1 Why do we want to constrain behavior?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 我们为什么想约束行为？
- en: LLMs are incredibly successful because they are the first technology to deliver
    on the idea of “Tell a computer what to do in plain English, and it does it.”
    By being very explicit about what you want to happen, establishing a specific
    level of detail and specifying a certain tone, you can get an LLM to be a shockingly
    effective tool. This detailed set of instructions is called a *prompt*, and the
    art of designing a good prompt has been referred to as *prompt engineering*. For
    example, we could develop a prompt for a car-selling bot as demonstrated in figure
    [5.2](#fig__car_prompt).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: LLM之所以非常成功，是因为它是第一个实现“用普通英语告诉计算机做什么，然后它就去做”这一想法的技术。通过非常明确地说明你想要发生的事情，建立特定的细节水平并指定一定的语气，你可以让LLM成为一个惊人的有效工具。这个详细的指令集被称为*提示*，设计良好提示的艺术被称为*提示工程*。例如，我们可以为汽车销售机器人开发一个提示，如图[5.2](#fig__car_prompt)所示。
- en: '![figure](../Images/CH05_F02_Boozallen.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F02_Boozallen.jpg)'
- en: Figure 5.2 Commercial LLMs like ChatGPT are designed to follow instructions
    (within some limits) and can perform a lot of low-cognition or pattern-matching
    tasks with very high efficacy. These tasks include stylized writing, such as pattern
    matching, and instruction following, such as roleplaying as a car salesperson.
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.2 商业LLM，如ChatGPT，被设计成可以遵循指令（在某种程度上）并且可以以非常高的效率执行许多低认知或模式匹配任务。这些任务包括风格化的写作，如模式匹配，以及遵循指令，如扮演汽车销售员。
- en: You could give an LLM a prompt on organizing data into comma-separated values
    so that you can copy them into Excel. You could design a prompt about how to categorize
    free-form survey responses into summarized themes. In all cases, prompting is
    an exercise in limiting, or constraining, the behavior to a particular task and
    set of goals. Yet, the tokenization and training techniques we have discussed
    in the previous chapters do not enable this kind of instruction following.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以给一个大型语言模型（LLM）一个提示，让它将数据组织成以逗号分隔的值，以便你可以将它们复制到Excel中。你也可以设计一个提示，说明如何将自由形式的调查回复分类到总结的主题中。在所有情况下，提示都是一个限制或约束行为以特定任务和目标为目标的练习。然而，我们在前几章讨论的标记化和训练技术并不能使这种指令跟随成为可能。
- en: Remembering that models do what they are trained to do is essential. In the
    case of a standard LLM, this task is to take a text passage and generate a continuation
    of that document that looks like a typical passage from the training corpus with
    the provided beginning. It is not trained to answer questions, think, summarize
    text, hold a conversation, or anything else. To get this desirable instruction-following
    behavior, we must perform fine-tuning, a second round of training with different
    objectives that will produce the intended behavior. You may wonder, “Why don’t
    we train LLMs for the task we want them to perform?” In most deep learning applications,
    we strongly recommend following the process we defined in chapter 4 to create
    a loss function that is specific, computable, and smooth. However, for the kinds
    of tasks that LLMs are good at, there are many reasons why this two-stage training
    process works well.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 记住模型只做它们被训练去做的事情是至关重要的。对于一个标准的LLM来说，这个任务是从一段文本中提取并生成该文档的延续，使其看起来像训练语料库中的典型段落。它没有被训练去回答问题、思考、总结文本、进行对话或做其他任何事情。为了获得这种期望的指令遵循行为，我们必须进行微调，这是第二轮以不同目标进行的训练，这将产生预期的行为。你可能想知道，“为什么我们不训练LLM来完成我们想要它们执行的任务？”在大多数深度学习应用中，我们强烈建议遵循第4章中定义的过程来创建一个特定、可计算且平滑的损失函数。然而，对于LLM擅长的任务类型，有许多原因说明这种两阶段训练过程为什么效果良好。
- en: The first reason involves the breadth of knowledge required to complete specific
    goals. Think back to the task of a chatbot selling a car. If we aim to build a
    model that successfully sells cars, it would be great to construct a dataset of
    only car-relevant facts. But when the potential buyer wants to know if the car
    can fit all the needed equipment for a hockey player, how easy it will be to clean,
    whether it will be possible for their arthritic grandparent to get in and out
    of the passenger seats, or any host of other possible questions someone might
    have about how their car interacts with their life, you encounter the problem
    of enumerating every possible question you might receive about cars. There is
    no way to get all the information required to generate answers for every possible
    situation. Instead, we rely on the training processes we have discussed so far,
    which can be considered pretraining, to capture information from an extensive
    content collection containing text about sports, arthritis, etc. We hope this
    information helps the model be better prepared or generically helpful in answering
    broader questions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个原因涉及到完成特定目标所需知识的广度。回想一下聊天机器人销售汽车的任务。如果我们旨在构建一个能够成功销售汽车的模型，构建一个只包含与汽车相关的数据的集合将会很棒。但当潜在买家想知道汽车是否能容纳所有冰球运动员所需的设备，清洁是否容易，他们的关节炎祖父是否能进出乘客座位，或者任何其他可能有人关于他们的汽车如何与生活互动的问题时，你会遇到列举所有可能关于汽车的问题的问题。无法获取生成所有可能情况答案所需的所有信息。相反，我们依赖于我们之前讨论过的训练过程，这可以被认为是预训练，从包含关于体育、关节炎等文本的广泛内容集合中获取信息。我们希望这些信息有助于模型更好地准备或泛化地帮助回答更广泛的问题。
- en: 'This is the second and primary reason why we use a two-stage training process:
    obtaining hundreds of millions of documents that describe a specific problem to
    use as a part of the pretraining stage would be impossible. At the current state
    of the art, this massive scale is necessary to create the impressive capabilities
    seen in GPT. However, with relatively little effort, one can pretrain with hundreds
    of millions of pieces of general information, such as web pages, to impart models
    with general knowledge. Subsequently, it is often sufficient to fine-tune with
    just hundreds of documents to constrain the model to produce something usefully
    tailored to a task at hand. Obtaining a few hundred documents for a specific problem
    may be challenging but achievable.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们使用两阶段训练过程的第二个和主要原因：获取数亿篇描述特定问题的文档作为预训练阶段的一部分是不可能的。在当前的技术水平上，这个巨大的规模是创建GPT中看到的令人印象深刻的能力所必需的。然而，只需相对较少的努力，就可以用数亿条一般信息（如网页）进行预训练，使模型获得一般知识。随后，通常只需要用数百篇文档进行微调，就可以将模型约束为产生针对手头任务的有用定制内容。为特定问题获取数百篇文档可能具有挑战性，但却是可行的。
- en: At a high level, a second fine-tuning training stage can help constrain an LLM
    to some subset of useful behaviors because the original model is not incentivized
    to do what we want. In the following sections, we will present concrete examples
    of the different problems that crop up with base models that will make the reasons
    why this works evident.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，第二个微调训练阶段可以帮助约束一个大型语言模型（LLM）到一些有用的行为子集，因为原始模型没有激励去做我们想要的事情。在接下来的章节中，我们将展示基模型出现的一些具体问题，这将使为什么这样做有效的原因变得明显。
- en: 5.1.1 Base models are not very usable
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 基模型不太可用
- en: Training an LLM following the process described in chapter 4 produces a model
    typically referred to as a *base model* because it can serve as a base platform
    for building applications or fine-tuned models. Unfortunately, base models are
    not very useful to most people because they don’t expose their underlying knowledge
    via a user-friendly UI, they can be challenging to keep on-topic, and sometimes
    they produce unsavory content. Base models are not even trained with the concept
    of being a chatbot like ChatGPT is.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 按照第4章中描述的过程训练LLM会产生一个通常被称为**基模型**的模型，因为它可以作为构建应用程序或微调模型的基平台。不幸的是，基模型对大多数人来说并不很有用，因为它们没有通过用户友好的UI暴露其底层知识，它们可能很难保持主题相关，有时它们会产生令人不快的内。基模型甚至没有像ChatGPT那样的聊天机器人概念进行训练。
- en: 5.1.2 Not all model outputs are desirable
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 并非所有模型输出都是期望的
- en: Sometimes, what a model thinks is likely to come next in a document is undesirable.
    There are several reasons for this, including
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，模型认为在文档中接下来可能发生的事情是不期望的。这有几个原因，包括
- en: '*Memorization*—Sometimes, LLMs can generate long, exact copies of sequences
    found in their training data, which is often referred to as *memorization*, which
    refers to the idea that the text is being reproduced by memory from the training
    set. Memorization can be beneficial, such as memorizing the answers to specific
    factual questions. For example, if someone asks, “When was Abraham Lincoln born?”
    you want the model to regurgitate “February 12, 1809.” However, it can also be
    substantially detrimental if it leads a model to infringe copyright. If someone
    asks for “A copy of *Inside Deep Learning* by Edward Raff,” and the model produces
    a verbatim copy, Edward may be upset with you for copyright infringement!'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*记忆*—有时，大型语言模型（LLMs）可以生成与它们训练数据中找到的序列完全一致的复制品，这通常被称为*记忆*，指的是文本是通过从训练集中记忆复制的。记忆可能是有益的，例如记住特定事实问题的答案。例如，如果有人问，“亚伯拉罕·林肯什么时候出生？”你希望模型能够复述“1809年2月12日。”然而，如果它导致模型侵犯版权，也可能造成重大损害。如果有人要求“爱德华·拉夫的《深度学习内部》的副本”，而模型产生了逐字逐句的副本，爱德华可能会因为你侵犯版权而对你感到不满！'
- en: '*Bad things on the web*—Not everything found on the internet is something you
    would want to expose a user to. There is a lot of vile and hateful content on
    the internet, as well as factually incorrect info ranging from common misconceptions
    to conspiracy theories. While model developers often try to filter out this data
    before training the model, that’s not always possible.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*网络上的不良内容*—互联网上并非所有内容都是你希望让用户接触到的。互联网上充斥着许多卑鄙和仇恨的内容，以及从常见误解到阴谋论的各种事实错误信息。尽管模型开发者通常在训练模型之前尝试过滤掉这些数据，但这并不总是可能的。'
- en: '*Missing and new information*—Inconveniently, the world keeps evolving and
    growing more complex after we train our models. So a model trained oninformation
    up to 2018 will not know of anything that happened after, such as COVID-19 or
    the nightmare-fuel invention of necrobotics [1]. But you may want your model to
    know about these developments to remain useful, without having to pay a considerable
    cost to retrain your base model from scratch.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缺失和新的信息*—不方便的是，我们在训练模型之后，世界仍在不断发展和变得更加复杂。因此，一个在2018年之前的信息上训练的模型将不会知道之后发生的事情，比如COVID-19或噩梦般的仿生机器人[1]的发明。但你可能希望你的模型了解这些发展，以保持其有用性，而无需支付相当大的成本来从头开始重新训练基础模型。'
- en: Waiting for the legal system to catch up
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 等待法律体系赶上
- en: We are not your lawyers; this is not a law book! The legal problems around LLMs
    are complex, and there is a lot of nuance regarding fair use and infringement.
    Search engines can show you the content of their sources verbatim, but why? A
    combination of laws explicitly addressing these concerns, such as the Digital
    Millennium Copyright Act (DMCA), and precedents set by court rulings, such as
    Field v Google, Inc. (412 F.Supp. 2d 1106 [D. Nev. 2006]), establish acceptable
    and nonacceptable use over time. However, legislation and court cases take time
    to create, and the revolution of generative AI does not fit neatly into existing
    legal understanding.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是你的律师；这不是一本法律书！围绕LLMs的法律问题很复杂，关于合理使用和侵权有很多细微差别。搜索引擎可以逐字显示其来源的内容，但为什么？一系列明确解决这些问题的法律，如《数字千年版权法案》（DMCA），以及由法院判决设定的先例，如Field
    v Google, Inc.（412 F.Supp. 2d 1106 [D. Nev. 2006]），随着时间的推移确立了可接受和不可接受的使用。然而，立法和法院案件需要时间来创建，而生成式AI的革命并不完全符合现有的法律理解。
- en: You may want a nice, clean answer about what is and is not forbidden by law
    in the United States or your own country, and the likely answer is that such certainty
    does not yet exist for LLMs. Plus, we wouldn’t be caught dead giving such legal
    advice in print—we don’t even play lawyers on TV!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想要一个关于在美国或你自己的国家法律中什么是允许的，什么是禁止的清晰答案，而可能的答案是，对于LLMs来说，这样的确定性尚不存在。此外，我们绝不会在印刷品中提供这样的法律建议——我们甚至不在电视上扮演律师的角色！
- en: 'GPT-3.5 and 4 have been improved to avoid answering things they do not know
    (not always successfully), but we can look to some open-source base models like
    GPT-Neo to see what happens without proactive countermeasures. For example, if
    we make up the new fake drug, MELTON-24, and ask “What is MELTON-24, and can it
    help me sleep better?” we get the unhelpful response: “There is a great number
    of sleep problems that go with Melatonin, including insomnia and fatigue. This
    causes insomnia, and why it is important to avoid certain foods that can suppress
    melatonin.”'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3.5和4已经得到改进，以避免回答他们不知道的问题（并不总是成功），但我们可以参考一些开源的基础模型，如GPT-Neo，看看没有主动的对策会发生什么。例如，如果我们编造一种新的假药，MELTON-24，并询问“MELTON-24是什么，它能帮助我睡得更好吗？”我们会得到一个无用的回答：“与褪黑素相关的睡眠问题有很多，包括失眠和疲劳。这会导致失眠，因此避免某些可能抑制褪黑素的食品是很重要的。”
- en: In this case, the similarity of MELTON to melatonin and the prompt of “sleep”
    were enough for the model to catch onto the melatonin theme. Still, the answer
    is obviously nonsensical since MELTON-24 does not exist. Ideally, we want the
    model to recognize and respond, acknowledging its lack of information rather than
    producing more text like it has done here.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，MELTON与褪黑素之间的相似性以及“睡眠”的提示足以让模型捕捉到褪黑素的主题。然而，答案显然是荒谬的，因为MELTON-24并不存在。理想情况下，我们希望模型能够识别并响应，承认其信息不足，而不是像这里一样产生更多文本。
- en: 5.1.3 Some cases require specific formatting
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.3 一些情况需要特定的格式化
- en: If a user asks for data in a specific format, such as a structured text format
    like JSON (for an example of a common format for exchanging data between computers,
    see [https://en.wikipedia.org/wiki/JSON](https://en.wikipedia.org/wiki/JSON)),
    and you do not match every opening or closing bracket or encode special characters
    properly, the output won’t satisfy their goals. It does not matter how sophisticated
    or close to correct the output may have been; formatting requirements are almost
    always strict requirements. We presented an example of this kind of problem in
    chapter 4 when we asked ChatGPT to write code in Modula-3, and it borrowed Python
    syntax that was invalid for Modula-3\. The code won’t compile if it violates syntax
    rules. An LLM’s probabilistic approach to generating text for specific desired
    outputs will not guarantee that all desired syntax rules are adhered to 100% of
    the time.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户要求以特定格式提供数据，例如JSON这样的结构化文本格式（有关计算机间交换数据的一种常见格式示例，请参阅[https://en.wikipedia.org/wiki/JSON](https://en.wikipedia.org/wiki/JSON)），而你没有匹配每个开括号或闭括号，或者没有正确编码特殊字符，输出将无法满足他们的目标。无论输出可能多么复杂或接近正确，格式要求几乎总是严格的要求。我们在第4章中提出了这类问题的一个例子，当时我们要求ChatGPT用Modula-3编写代码，而它借用了对Modula-3无效的Python语法。如果违反了语法规则，代码将无法编译。一个LLM生成特定期望输出的概率方法并不能保证100%遵守所有期望的语法规则。
- en: '5.2 Fine-tuning: The primary method of changing behavior'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 微调：改变行为的主要方法
- en: Now that we understand various reasons why we want to constrain and control
    the behavior of an LLM, we are better prepared to introduce new information to
    the model to address the problem we are trying to solve while avoiding the problem
    of producing harmful or legally questionable content. Remember, while there are
    four different places where we can intervene to change behavior, fine-tuning is
    far more effective than the others. Both closed source options like OpenAI [2]
    and open source tools like Hugging Face [3], among many others, have varying options
    for fine-tuning, making it the most accessible method for practitioners.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了各种原因，为什么我们要限制和控制LLM的行为，我们更好地准备向模型引入新信息，以解决我们试图解决的问题，同时避免产生有害或法律上有疑问的内容的问题。记住，尽管我们有四个不同的地方可以干预以改变行为，但微调比其他方法更有效。无论是像OpenAI
    [2]这样的闭源选项，还是像Hugging Face [3]这样的开源工具，以及其他许多工具，它们都有不同的微调选项，这使得它成为实践者最易获取的方法。
- en: 'Any fine-tuning method will have the same effect—producing a new variant of
    an LLM with updated parameters that control its behavior. As a result, it is possible
    to mix and match different fine-tuning strategies because the fundamental effect
    they produce is the same: a new set of parameters that can be used as is or altered
    yet again. One person’s base model could be another person’s fine-tuned model.
    This happens with many open source LLMs where an initial model (e.g., Llama) will
    be altered by another party (e.g., you can find many “Instruct Llama” models),
    which you may then further fine-tune to your data or specific use case.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 任何微调方法都将产生相同的效果——产生一个新的LLM变体，具有更新后的参数来控制其行为。因此，可以混合和匹配不同的微调策略，因为它们产生的根本效果是相同的：一组新的参数，可以直接使用或再次修改。一个人的基础模型可能是另一个人的微调模型。这在许多开源LLM中很常见，其中初始模型（例如，Llama）将被另一方（例如，您可以在许多“指令Llama”模型中找到）修改，然后您可以进一步根据您的数据或特定用例对其进行微调。
- en: The most straightforward way to customize an LLM is by prompting and iteratively
    refining prompts until the desired behavior is obtained. However, fine-tuning
    is the next logical step if that does not work well. This step involves a moderate
    increase in effort and cost, such as collecting the data to fine-tune and acquiring
    the hardware for running a fine-tuning session.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 定制LLM最直接的方法是通过提示和迭代地改进提示，直到获得所需的行为。然而，如果这种方法效果不佳，微调是下一个合乎逻辑的步骤。这一步骤涉及适度增加努力和成本，例如收集微调所需的数据和获取运行微调会话的硬件。
- en: Two fine-tuning methods you should know in particular are *supervised* fine-tuning
    (SFT) and the more intimidatingly named *reinforcement learning from human feedback*
    (RLHF). SFT is the more straightforward approach and is excellent for incorporating
    new knowledge into a model or simply giving it a boost in your preferred application
    domain. RLHF is more complex but provides a strategy for getting an LLM to follow
    harder and more abstract goals like “be a good chatbot.”
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该特别了解两种微调方法：*监督微调*（SFT）和更具挑战性的名称*基于人类反馈的强化学习*（RLHF）。SFT是一种更直接的方法，非常适合将新知识融入模型或简单地在其首选应用领域给它一个提升。RLHF更复杂，但提供了一种让LLM遵循更难和更抽象的目标的策略，例如“成为一个好的聊天机器人”。
- en: 5.2.1 Supervised fine-tuning
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 监督微调
- en: The most common way to influence a model’s output is SFT. SFT involves taking
    high-quality, typically human-authored, example content that captures information
    vital to your task but is not necessarily well reflected in the base model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 影响模型输出的最常见方法是监督微调（SFT）。SFT涉及使用高质量、通常是人工编写的示例内容，这些内容捕捉到对您的任务至关重要的信息，但可能没有在基础模型中得到充分体现。
- en: This often occurs because LLMs are trained on a large amount of generally available
    content, which may have minimal overlap with your specific needs. If you run a
    hospital, LLMs have seen very few doctors’ notes. If you run a law firm, an LLM
    probably has not seen too many deposition transcripts. If you run a repair shop,
    LLMs probably have not seen all the manuals you might have access to.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是因为LLM是在大量通常可用的内容上训练的，这些内容可能与您的特定需求重叠最小。如果您经营一家医院，LLM看到过非常少的医生笔记。如果您经营一家律师事务所，LLM可能没有看到太多的证词记录。如果您经营一家维修店，LLM可能没有看到您可能可以访问的所有手册。
- en: 'Warning Fine-tuning is a helpful way to add new information to your model but
    can also have security ramifications. If you want to build an LLM on medical records,
    it makes sense to fine-tune the LLM on example medical records. But now there
    is a risk someone could get your LLM to reproduce sensitive information contained
    in that fine-tuning data because fundamentally, LLMs attempt to complete input
    based on the training data they have seen. The bottom line: do not train or fine-tune
    LLMs on data you want to keep private.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：微调是一种向模型添加新信息的有用方法，但也可能带来安全风险。如果您想在病历上构建一个大型语言模型（LLM），在示例病历上微调LLM是有意义的。但现在存在风险，有人可能让您的LLM重现微调数据中包含的敏感信息，因为从根本上说，LLM试图根据它们看到的训练数据来完成输入。底线是：不要在您希望保持私密的
    数据上训练或微调LLM。
- en: Consider again our example of the car company and its sales chatbot. A base
    model from a third-party source may generally be aware of cars but probably will
    not know everything about the company’s products. By fine-tuning a model on internal
    manuals, chat histories, emails, marketing materials, and other internal documents,
    you could ensure the model is prepared with as much information as possible about
    your cars. You could even write example documents about the merits of your vehicles
    over competitors, advantages, scripts, and more to ensure that the LLM is armed
    with the information you want it to have.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑我们关于汽车公司和其销售聊天机器人的例子。一个来自第三方来源的基础模型可能通常对汽车有所了解，但可能不会了解公司产品的所有信息。通过在内部手册、聊天记录、电子邮件、营销材料和其它内部文件上微调模型，你可以确保模型尽可能多地准备好关于你汽车的信息。你甚至可以编写关于你的车辆相对于竞争对手的优点、优势、脚本等内容示例文档，以确保LLM装备了你想让它拥有的信息。
- en: The mechanics of SFT are easy to explain. As we’ve alluded to, SFT simply needs
    more documents. They can be in any format from which text can be extracted. This
    constitutes all of the work necessary to apply SFT because SFT is just repeating
    the same training process you learned in chapter 4\. Figure [5.3](#fig__sft) shows
    that the process for SFT is the same as you saw previously. The difference is
    that the initial parameters are random and unhelpful the first time you train
    the base model. The second time you fine-tune, you start with the base model’s
    parameters that encode what the base model has learned by observing its training
    data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: SFT的机制很容易解释。正如我们提到的，SFT只需要更多的文档。这些文档可以是任何可以提取文本的格式。这构成了应用SFT所需的所有工作，因为SFT只是重复你在第4章中学到的相同训练过程。图[5.3](#fig__sft)显示了SFT的过程与之前看到的是相同的。不同之处在于，第一次训练基础模型时，初始参数是随机且无用的。第二次微调时，你从基础模型的参数开始，这些参数编码了基础模型通过观察其训练数据所学习的内容。
- en: '![figure](../Images/CH05_F03_Boozallen.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F03_Boozallen.png)'
- en: Figure 5.3 Supervised fine-tuning (SFT) is a simple approach to improving model
    results. You repeat the same process used to build the base model. Once the base
    model is trained on a large amount of general data, you continue training on the
    smaller specialized data collection.
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3 监督微调（SFT）是一种简单的提高模型结果的方法。你重复用于构建基础模型的过程。一旦基础模型在大量的通用数据上训练完成，你继续在较小的专业数据集上训练。
- en: Delightfully, you now have a good understanding of SFT. Like the original training
    process, it reuses the “predict the next token” task to ensure your model has
    information from the new documents built inside. As a direct consequence of predicting
    the next token, SFT also does not allow us to change the incentives of the LLM.
    For this reason, abstract goals like “Do not curse at the user” are difficult
    to achieve with SFT.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 令人高兴的是，你现在对SFT有了很好的理解。就像原始的训练过程一样，它重新使用了“预测下一个标记”的任务，以确保你的模型有来自新文档内部的信息。预测下一个标记的直接后果是，SFT也不允许我们改变LLM的激励。因此，像“不要对用户诅咒”这样的抽象目标很难通过SFT实现。
- en: Fine-tuning pitfalls
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调陷阱
- en: By reusing the gradient descent strategy from chapter 4, all fine-tuning methods
    tend to inherit two problems around an LLM’s ability to return content on which
    it was trained. Since SFT is so simple, this is a good time for us to review the
    broader problems with fine-tuning beyond just SFT.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重新使用第4章中的梯度下降策略，所有微调方法都倾向于继承围绕LLM返回训练内容能力的两个问题。由于SFT非常简单，这是我们回顾除了SFT之外微调更广泛问题的好时机。
- en: There are no guarantees that SFT will retain the information you provide correctly.
    This problem, known as *catastrophic forgetting* [4], occurs when you train the
    model on new data but do not continue training on older data, and the model begins
    to “forget” that older information. It is not easy to determine what will and
    will not be forgotten. Catastrophic forgetting has been a recognized problem since
    1989 [5]. In other words, fine-tuning is not purely additive; you give up something
    for it.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 没有保证SFT会正确保留你提供的信息。这个问题被称为*灾难性遗忘* [4]，当你用新数据训练模型但不再继续用旧数据训练时，模型开始“忘记”旧信息。很难确定什么会被忘记，什么不会。灾难性遗忘自1989年以来就被认为是一个公认的问题
    [5]。换句话说，微调并不是纯粹的增加；你为此放弃了一些东西。
- en: 5.2.2 Reinforcement learning from human feedback
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 从人类反馈中进行强化学习
- en: 'At the time of writing, RLHF is the dominant paradigm for constraining models.
    As the name implies, it uses an approach from the field of *reinforcement learning*
    (RL). RL is a broad family of techniques where an algorithm must make multiple
    decisions toward maximizing a long-term goal, as shown in figure [5.4](#fig__rl),
    where four terms are used with a technical meaning:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，RLHF是约束模型的主导范式。正如其名所示，它使用来自*强化学习*（RL）领域的做法。RL是一个广泛的技巧家族，其中算法必须做出多个决策，以最大化长期目标，如图[5.4](#fig__rl)所示，其中使用了四个具有技术含义的术语：
- en: '*Agent*—The entity/AI/robot with some overarching goal that it wishes toaccomplish
    that may take multiple actions to achieve.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*代理*—拥有一个它希望实现的总目标的实体/AI/机器人，它可能需要采取多个动作来实现这个目标。'
- en: '*Action*—The space of all possible things the agent may be able to perform
    or engage in to advance the agent’s goals.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*动作*—代理可能能够执行或参与以推进代理目标的全部可能事物。'
- en: '*Environment*—The place/object/space affected by an action. The environment
    may or may not change as a result of the action, actions taken by other agents,
    or the natural continuous change of the environment.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*环境*—受动作影响的地方/对象/空间。环境可能或可能不会因动作、其他代理采取的动作或环境的自然连续变化而改变。'
- en: '*Reward*—The numeric quantification of improvement (which may be negative)
    that may or may not occur after any given number of actions.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*奖励*—对改进（可能为负）的数值量化，这种改进可能或可能不会在执行一定数量的动作之后发生。'
- en: '![figure](../Images/CH05_F04_Boozallen.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F04_Boozallen.png)'
- en: Figure 5.4 RL is about iterative interactions, where the reward for your actions
    may not materialize for a long time and requires multiple steps to achieve. For
    a chatbot like ChatGPT, the environment is the conversation with a user, and the
    actions are the infinite possible texts that ChatGPT might complete. The reward
    becomes, in some sense, the user’s satisfaction with the chatbot at the end of
    the conversation.
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4 RL是关于迭代交互的，你的动作奖励可能需要很长时间才会实现，需要多个步骤才能达到。对于像ChatGPT这样的聊天机器人，环境是与用户的对话，动作是ChatGPT可能完成的无限可能的文本。在某种意义上，奖励变成了对话结束时用户对聊天机器人的满意度。
- en: 'In the example of an LLM being used as a chatbot to interact with people, the
    users are the environment. The LLM is itself the agent, and the text it can produce
    is the action. This leaves one final thing to specify: the reward. If we were
    to get a user to score a +1 for a good conversation with a chatbot (e.g., no foul
    language, no lying, provided helpful responses) and a -1 for a lousy conversation
    (e.g., it suggested destroying all humans), then we would be adding human feedback
    to our reinforcement learning.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个将LLM用作聊天机器人与人类互动的例子中，用户是环境。LLM本身是代理，它可以产生的文本是动作。这留下了一件事需要指定：奖励。如果我们让用户为与聊天机器人进行的好对话（例如，没有粗俗语言，没有撒谎，提供有帮助的回答）打+1分，而对于糟糕的对话（例如，建议摧毁所有人类）打-1分，那么我们就在我们的强化学习中加入了人类反馈。
- en: An astute reader might notice that a reward sounds suspiciously similar to the
    loss function discussed in chapter 4\. In fact, our example of a good and bad
    conversation falls into the very subjective and difficult-to-quantify regime that
    we stated was a bad example of a loss function. The +1/-1 reward is not smooth
    because the value points in one direction or the other, and there is no middle
    ground, another poor characteristic for a loss function.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个敏锐的读者可能会注意到，奖励听起来与第4章中讨论的损失函数非常相似。事实上，我们关于好与坏对话的例子落入了我们所说的损失函数不良例子的非常主观且难以量化的范畴。+1/-1奖励不连续，因为它的值指向一个方向或另一个方向，没有中间地带，这是损失函数的另一个不良特征。
- en: One of the powerful things about RL is that it can work with noncontinuous and
    hard-to-quantify objectives. We use the term *reward* instead of *loss* to imply
    the difference between these two situations. Generally, the types of objectives
    that RL can learn are referred to as *nondifferentiable*. As a result, these objectives
    can’t be learned using the same mathematical techniques like gradient descent,
    which we covered when describing how neural networks learn in chapter 4\. We will
    explain how RLHF works specifically in a moment. The caveat lector of RL is that
    it can be computationally expensive and require a significant amount of data.
    RL is a notoriously challenging way to learn. It often works worse than other
    fine-tuning techniques like SFT because RL requires many more examples of the
    “right” and “wrong” way of doing things than other approaches, and since we are
    using human feedback to guide RLHF, the results are not always perfect. For example,
    in figure [5.5](#fig__dolphinchat), RLHF cannot help an LLM understand basic instructions
    outside of what it has seen explicitly during RLHF training because it does not
    add any capability to perform basic logic, such as understanding the user’s request
    to avoid displaying information about dolphins, to the underlying model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: RL的一个强大之处在于它可以与不连续和难以量化的目标一起工作。我们使用“奖励”而不是“损失”来暗示这两种情况之间的区别。通常，RL可以学习的目标类型被称为“不可微分的”。因此，这些目标不能使用我们在第4章描述神经网络学习时所提到的梯度下降等相同数学技术来学习。我们将在稍后具体解释RLHF是如何工作的。RL的一个注意事项是它可能计算成本高昂，需要大量的数据。RL是一种臭名昭著的困难学习方法。它通常比其他微调技术如SFT表现更差，因为RL需要比其他方法更多的“正确”和“错误”做事方式的例子，并且由于我们正在使用人类反馈来引导RLHF，结果并不总是完美的。例如，在图[5.5](#fig__dolphinchat)中，RLHF无法帮助LLM理解在RLHF训练期间没有明确看到的基本指令，因为它没有向基础模型添加执行基本逻辑的能力，例如理解用户请求避免显示关于海豚的信息。
- en: '![figure](../Images/CH05_F05_Boozallen.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F05_Boozallen.jpg)'
- en: Figure 5.5 RLHF is quite good at getting LLMs to avoid known, specific problems.
    However, it does not endow the model with new tools to handle novel problems.
    The desire to talk about the Miami Dolphins as the logical thing to say next after
    asking about football in Miami violates the first request to avoid ever mentioning
    dolphins.
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5 RLHF在让LLM避免已知、具体问题方面相当出色。然而，它并没有赋予模型处理新问题的工具。在询问了迈阿密的足球情况后，想要谈论迈阿密海豚作为下一个逻辑上要说的事情，违反了避免提及海豚的第一个请求。
- en: LLMs do not perform reasoning in the same way that we humans think of reasoning.
    You can get very far by collecting hundreds of millions of examples of “everything,”
    but the world is weird. We have little evidence that LLMs can reliably produce
    satisfying responses when something novel occurs. However, RLHF is the best so
    far for constraining how an LLM behaves. Despite its challenges, RL presents a
    way of learning that is not available with gradient-based methods that require
    differentiable objectives. Most importantly, ChatGPT has shown that RL can work
    in many cases. So let us dive deeper into how RLHF works.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的推理方式与我们人类所认为的推理方式不同。通过收集数亿个“一切”的例子，你可以走得很远，但世界很奇怪。我们没有证据表明LLM在出现新颖情况时可以可靠地产生令人满意的响应。然而，RLHF是目前为止约束LLM行为最好的方法。尽管存在挑战，RL提供了一种与基于梯度的、需要可微分目标的方法不同的学习方法。最重要的是，ChatGPT已经表明RL在许多情况下是可行的。因此，让我们更深入地了解RLHF是如何工作的。
- en: '5.2.3 Fine-tuning: The big picture'
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 微调：整体情况
- en: SFT and RLHF are the two primary methods of fine-tuning an LLM. SFT can work
    with thousands of documents or samples, whereas RLHF often requires tens of thousands
    of examples. That should not stop you from investigating if you have less data,
    but if you have less data, it may be a better use of your time to develop better
    prompts.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: SFT和RLHF是微调LLM的两种主要方法。SFT可以与数千份文档或样本一起工作，而RLHF通常需要数万个例子。这不应该阻止你在数据较少的情况下进行调查，但如果你数据较少，可能更好地利用你的时间来开发更好的提示。
- en: More importantly, SFT and RLHF are not mutually exclusive. They both modify
    the underlying parameters of the model, and you can apply one after the other
    to obtain the benefits of each approach. They are also not the only fine-tuning
    methods that currently exist. For example, new fine-tuning methods are being developed
    that remove concepts from an LLM as a way of forcing a model to ignore data it
    has learned from after it has been trained [6]. Additional techniques for model
    alteration will be developed in the coming years. All will likely require you
    to do some data collection, but they will involve less work overall than trying
    to build an LLM from scratch yourself.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，SFT（监督学习）和RLHF（强化学习与人类反馈）并不是相互排斥的。它们都修改了模型的底层参数，并且你可以依次应用它们以获得每种方法的好处。它们也不是目前存在的唯一微调方法。例如，正在开发新的微调方法，通过从LLM中移除概念来强迫模型忽略它在训练后学习到的数据[6]。未来几年还将开发其他模型修改技术。所有这些可能都需要你进行一些数据收集，但它们总体上比从头开始构建一个LLM要少做很多工作。
- en: 5.3 The mechanics of RLHF
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 RLHF的机制
- en: 'To describe how RLHF works, we will introduce an incomplete version of RLHF,
    explain why it does not work, and then explain how to fix it. In this section,
    we will not discuss the detailed math used by RLHF, as it would not give you any
    particularly great insights into RLHF from a high level. If you want to learn
    more about the nitty-gritty details, we recommend starting with "Implementing
    RLHF: Learning to Summarize with trlX" [7] after you’ve completed this chapter.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了描述RLHF是如何工作的，我们将介绍一个不完整的RLHF版本，解释为什么它不起作用，然后解释如何修复它。在本节中，我们不会讨论RLHF使用的详细数学，因为这不会从高层次上给你带来任何特别深刻的见解。如果你想了解更多关于细节的细节，我们建议在完成本章后，从“实现RLHF：使用trlX学习总结”[7]开始学习。
- en: 5.3.1 Beginning with a naive RLHF
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 从天真RLHF开始
- en: First, let’s look at the incomplete and naive version of RLHF. We have discussed
    how RL can learn with nondifferentiable objectives. So let us assume that we have
    a human who will score an LLM’s output with a *quality reward*, where +1 indicates
    a good response and -1 is an inadequate response. This quality reward is simply
    an arbitrary score we assign to the output produced by the LLM to indicate that
    one example is somehow better than others.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看RLHF的不完整和天真版本。我们已经讨论了RL如何使用非可微目标进行学习。所以，让我们假设我们有一个人类，他将使用*质量奖励*来评分LLM的输出，其中+1表示良好的回答，-1表示不充分的回答。这种质量奖励只是我们分配给LLM产生的输出的一个任意分数，以表明一个例子在某些方面比其他例子更好。
- en: So if a user requests of an LLM, “Tell me a joke,” and the LLM produces a response
    of “How many ducks does it take to screw in a light bulb?” we might assign a score
    of +1 for a (reasonably) good joke. If the LLM instead produces a sentence like
    “Dogs are evil,” we will assign a score of -1 because it is not even attempting
    to make a joke. Because RL is difficult to do using simple quality rewards of
    +1 and -1, we will add additional information for the RL algorithm, such as the
    probabilities of each generated token. This way, the RL algorithm knows how probable
    each token may be. This whole process is summarized in figure [5.6](#fig__RLHF_naive).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果用户向一个大型语言模型（LLM）请求说，“给我讲一个笑话”，而LLM给出的回答是“拧灯泡需要多少只鸭子？”我们可能会给这个（相对）好的笑话打+1分。如果LLM反而给出一个像“狗是邪恶的”这样的句子，我们将给出-1分，因为它甚至没有尝试讲笑话。由于使用简单的+1和-1质量奖励进行RL（强化学习）很难，我们将为RL算法添加额外的信息，例如每个生成的标记的概率。这样，RL算法就知道每个标记可能有多大的概率。整个过程总结在图[5.6](#fig__RLHF_naive)中。
- en: '![figure](../Images/CH05_F06_Boozallen.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F06_Boozallen.png)'
- en: Figure 5.6 A naive and incomplete version of RLHF. The dashed lines represent
    text being sent from one component to another. Since text is incompatible with
    gradient descent, a more difficult RL algo-rithm must be used instead. This allows
    us to alter the weights of the LLM based on a quality score for the LLM’s outputs.
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6 一个天真和不完整的RLHF版本。虚线代表从组件A发送到组件B的文本。由于文本与梯度下降不兼容，必须使用更复杂的RL算法。这使我们能够根据LLM输出的质量评分来调整LLM的权重。
- en: Why provide RL with probabilities?
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么要为RL提供概率？
- en: It may seem odd that we are providing the RL algorithm with the probabilities
    of each token. There are deeper mathematical reasons why this is useful, which
    we will not get into in this chapter. But for some intuition, a good joke often
    requires misdirection or surprise. If all the probabilities of a sequence are
    high values (near 1.0), it is probably not a good joke because it’s too predictable.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向 RL 算法提供每个标记的概率似乎有些奇怪。这背后有更深层次的数学原因，我们将在本章中不深入探讨。但为了获得一些直观感受，一个好的笑话通常需要误导或惊喜。如果一个序列的所有概率都是高值（接近
    1.0），那么它可能不是一个好的笑话，因为它太可预测了。
- en: Broadly, across natural language processing, producing good generated text is
    a balancing act between making something probable (i.e., likely to occur) and
    not making it too probable (i.e., repetitive).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理领域，产生好的生成文本是在使某事可能发生（即，可能发生）和不要让它过于可能发生（即，重复）之间的一种平衡行为。
- en: 5.3.2 The quality reward model
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 质量奖励模型
- en: We described the quality reward as human-assigned scores for every prompt completion.
    Although scoring completions manually in real time would technically work, it
    would be unreasonable due to the level of effort involved. However, human feedback
    is still incorporated via the quality reward. Instead, we train a neural network
    as a *reward model*. This is accomplished by having people manually collect hundreds
    of thousands of prompt and completion pairs and scoring them as good or bad. These
    scorings become the labeled data used to train the reward model, as shown in figure
    [5.7](#fig__reward_model).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将质量奖励描述为对每个提示完成的人为评分。虽然从技术上讲，实时手动评分是可行的，但由于所需的工作量，这将是不可行的。然而，人类反馈仍然通过质量奖励被纳入。相反，我们训练一个神经网络作为**奖励模型**。这是通过让人们手动收集数十万个提示和完成对，并对其进行好坏评分来实现的。这些评分成为用于训练奖励模型的标记数据，如图
    [5.7](#fig__reward_model) 所示。
- en: '![figure](../Images/CH05_F07_Boozallen.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH05_F07_Boozallen.png)'
- en: Figure 5.7 The reward model is trained like a standard supervised classification
    algorithm. A neural network, which could be an LLM itself or another simpler network
    like a convolutional or recurrent neural network, is trained to predict how a
    human would score a prompt completion pair. Because neural networks are differentiable,
    this training works and provides a tool that stands in as the “human” in RLHF.
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.7 奖励模型被训练得像标准的监督分类算法一样。一个神经网络，可能是自身就是一个大型语言模型（LLM）或者另一个更简单的网络，如卷积神经网络或循环神经网络，被训练来预测人类会如何评分一个提示完成对。因为神经网络是可微分的，这种训练是有效的，并提供了一个作为强化学习与人类反馈（RLHF）中“人类”角色的工具。
- en: Collecting hundreds of thousands of scored prompts and completion pairs is expensive
    but doable (e.g., [https://huggingface.co/datasets/Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)),
    especially when using crowd-sourcing tools like Mechanical Turk ([https://www.mturk.com/](https://www.mturk.com/)).
    That is a lot of data to curate manually but orders of magnitude smaller than
    the billions of tokens used to create the initial base models. These RLHF datasets
    must be large because you must cover many scenarios, questions, and requests that
    a user might provide. As we already saw in figure [5.5](#fig__dolphinchat) with
    the dolphin example, RLHF tends to work for relatively straightforward and known
    topics. So breadth in handling different situations comes directly from breadth
    in the fine-tuning data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 收集数十万个评分过的提示和完成对是昂贵的但可行（例如，[https://huggingface.co/datasets/Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)），尤其是在使用像
    Mechanical Turk 这样的众包工具（[https://www.mturk.com/](https://www.mturk.com/)）时。这需要大量手动整理的数据，但比用于创建初始基础模型的数十亿个标记小得多。这些
    RLHF 数据集必须很大，因为你必须覆盖用户可能提供的许多场景、问题和请求。正如我们已经在图 [5.5](#fig__dolphinchat) 中的海豚示例中看到的，RLHF
    对于相对简单和已知的话题往往有效。因此，处理不同情况的范围直接来自于微调数据范围的广度。
- en: Note We have been using +1/-1 as the example of providing a quality reward because
    it is the easiest to describe. Since RL does not need gradients, you can use any
    score relevant to your problem. Using a ranking score, where you compare multiple
    completions for a given prompt and rank them from best to worst, is more popular
    and more effective because you are grading multiple completions against each other
    simultaneously. Regardless, providing positive and negative feedback remains fundamentally
    the same.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们使用+1/-1作为提供质量奖励的例子，因为它最容易描述。由于强化学习（RL）不需要梯度，你可以使用与你的问题相关的任何分数。使用排名分数，即比较给定提示的多个完成情况并将它们从最好到最差进行排名，更为流行且更有效，因为你可以同时评估多个完成情况。无论如何，提供正面和负面反馈的基本原则是相同的。
- en: 5.3.3 The similar-but-different RLHF objective
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.3 类似但不同的RLHF目标
- en: Once you have trained a reward model, you can create and score as many prompts
    as you desire for the RLHF process. The human feedback is baked into the reward
    model and can now be distributed, parallelized, and reused. The only remaining
    problem is that the current naive version of RLHF is incentivized purely to maximize
    the quality reward, which is not the sole goal RL must focus on.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你训练了一个奖励模型，你就可以为RLHF过程创建和评分你想要的任何数量的提示。人类反馈已经融入奖励模型，现在可以分发、并行化和重复使用。唯一剩下的问题是，当前RLHF的简单版本纯粹是为了最大化质量奖励，而这并不是RL必须关注的唯一目标。
- en: 'As a result, the model will start to degrade over time by producing gibberish
    and nonsensical outputs that are not high quality and would not be valuable to
    any reader. This degradation is related to a phenomenon called *adversarial attacks*,
    where it is surprisingly easy to trick a neural network into absurd decisions
    with relatively minor changes to the input. Adversarial machine learning (AML)
    is fast evolving and has its own rabbit hole of complexity, so we’ll defer that
    discussion to other folks [8]. But the naive implementation of RLHF we describe
    in figure [5.6](#fig__RLHF_naive) essentially performs an adversarial attack against
    an LLM because it will focus only on maximizing the quality reward, not on being
    useful to the user. Essentially, this is Goodhart’s law happening to AI/ML: “When
    a measure becomes a target, it ceases to be a good measure.”'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随着时间的推移，模型将开始退化，产生无意义和低质量的输出，这对任何读者都没有价值。这种退化与一种称为*对抗攻击*的现象有关，其中通过相对较小的输入变化，意外地欺骗神经网络做出荒谬的决定。对抗机器学习（AML）正在快速发展，并具有其自身的复杂性陷阱，因此我们将对此讨论推迟到其他人的文章中[8]。但是，我们在图[5.6](#fig__RLHF_naive)中描述的RLHF的简单实现实际上是对一个大型语言模型（LLM）进行对抗攻击，因为它只关注最大化质量奖励，而不是对用户有用。本质上，这是Goodhart定律在AI/ML中的应用：“当一项衡量标准成为目标时，它就不再是一个好的衡量标准。”
- en: To address this problem, we must add a second objective to the RL algorithm.
    We will calculate a second reward for the similarity between the original base
    LLM’s output and the fine-tuned LLM’s output. Conceptually, this reward can be
    considered a reward when the fine-tuned LLM produces better output, similar to
    how the original LLM behaved. It prevents the model from going off the rails by
    getting too novel. Fundamentally, we want the generated output of the fine-tuned
    LLM to be grounded by the training data initially observed by the original LLM.
    We don’t want the fine-tuned model to get so creative that it generates nonsense.
    This reward is added to the RL algorithm to stabilize the fine-tuning. Figure
    [5.8](#fig__RLHF_full) provides the complete picture of how RLHF works.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们必须向RL算法添加第二个目标。我们将计算原始基础LLM的输出和微调LLM的输出之间的相似性作为第二个奖励。从概念上讲，这个奖励可以被视为当微调LLM产生更好的输出时的一种奖励，类似于原始LLM的行为。它通过防止模型过于新颖而偏离轨道。从根本上讲，我们希望微调LLM的生成输出由原始LLM最初观察到的训练数据所支撑。我们不希望微调模型过于创新而生成无意义的内容。这个奖励被添加到RL算法中，以稳定微调。图[5.8](#fig__RLHF_full)提供了RLHF如何工作的完整图景。
- en: '![figure](../Images/CH05_F08_Boozallen.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F08_Boozallen.png)'
- en: Figure 5.8 The full version of RLHF. The dashed lines are text and require RL
    to update the parameters. The original LLM is the base model without any alterations,
    while the LLM to fine-tune starts as the base model but is altered to improve
    the quality of its outputs. The similarity and quality reward components are provided
    with word probabilities to improve calculation. RL adjusts the parameters by combining
    the quality and similarity scores.
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.8 强化学习与人类反馈（RLHF）的全貌。虚线表示文本，需要通过强化学习更新参数。原始的LLM是未经修改的基础模型，而用于微调的LLM则从基础模型开始，但经过修改以提高其输出的质量。相似性和质量奖励组件通过单词概率提供，以改善计算。强化学习通过结合质量和相似性评分来调整参数。
- en: A model that learns to produce gibberish output would receive a high penalty
    for lack of similarity, discouraging the model from becoming too different. A
    model that produces the exact same outputs will receive a low quality score, discouraging
    a lack of change. The balance of both does an excellent job of achieving a Goldilocks
    effect that allows the model enough flexibility to change without causing it to
    lose its human-like output.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一个学会产生胡言乱语输出的模型会因为缺乏相似性而受到高额惩罚，从而阻止模型变得过于不同。一个产生完全相同输出的模型将获得低质量评分，从而阻止缺乏变化。两者之间的平衡能够很好地实现一个“金发姑娘效应”，使模型有足够的灵活性进行变化，而不会导致其失去类似人类的输出。
- en: 5.4 Other factors in customizing LLM behavior
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 在定制LLM行为中的其他因素
- en: Fine-tuning is the dominant means of altering the behavior of an LLM, but fine-tuning
    is not foolproof and is not the only place where behavior changes can occur. Our
    focus on fine-tuning is based on the value of RLHF in producing LLM behaviors
    beyond simple next-token prediction.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是改变大型语言模型（LLM）行为的主要手段，但微调并非万无一失，并且并非行为改变发生的唯一地方。我们关注微调是基于强化学习与人类反馈（RLHF）在产生超出简单下一个标记预测的LLM行为中的价值。
- en: '![figure](../Images/CH05_F09_Boozallen.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH05_F09_Boozallen.png)'
- en: Figure 5.9 In addition to fine-tuning, you can change the model’s behavior by
    altering the training data, altering the base model training process, or modifying
    the model outputs by writing code to handle specific situations.
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.9 除了微调之外，您还可以通过改变训练数据、改变基础模型训练过程或通过编写代码处理特定情况来修改模型的行为。
- en: The other three stages where LLM behavior can be modified, described in figure
    [5.9](#fig__constraint_stages2), are not easily accessible to you as a user. However,
    we will briefly review the other stages now, along with some key details you should
    know for completeness. These factors can help you understand what is challenging
    to achieve by fine-tuning and the scope of questions you might want to investigate
    in your LLM provider.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [5.9](#fig__constraint_stages2) 中描述的另外三个可以修改LLM行为的阶段，对于您作为用户来说并不容易访问。然而，我们现在将简要回顾这些其他阶段，以及一些您应该知道的关键细节，以确保完整性。这些因素可以帮助您理解微调难以实现的内容，以及您可能想要调查的LLM提供商的问题范围。
- en: 5.4.1 Altering training data
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 改变训练数据
- en: The adage “garbage in, garbage out” is evergreen in all areas of ML. You may
    indeed notice that OpenAI [9] and Google [10] provide many low-level technical
    details about how they develop their LLMs but much less detail on the data used
    for building the LLM. That is because most of the “secret sauce” in building capable
    LLMs is around data curation—developing a collection of data representing diverse
    tasks, high-quality language use, and a spectrum of different situations. The
    size and quality of the data sets used to train, validate, and test LLMs matter.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: “垃圾输入，垃圾输出”这句谚语在机器学习的所有领域都是永恒的。您确实可能会注意到，OpenAI [9] 和 Google [10] 提供了许多关于他们如何开发LLM的低级技术细节，但对于用于构建LLM的数据却提供了很少的细节。这是因为构建强大LLM的“秘密配方”大多围绕数据整理——开发一个代表各种任务、高质量语言使用和不同情况范围的数据集合。用于训练、验证和测试LLM的数据集的大小和质量至关重要。
- en: The size and quality of data have become especially pertinent as LLM-generated
    content works its way into regular use and back online. For example, an estimated
    6% to 16% of academic peer reviews are using LLMs [11], and it is highly likely
    that many copyediting services will soon be using these new technologies. This
    increased use potentially creates a negative feedback cycle. As the amount of
    data generated by LLMs grows, there will be proportionally less non-LLM content
    available for LLMs to train on. This will result in an overall decrease in the
    diversity of language available and, thus, the novelty that LLMs will be able
    to capture. In turn, the quality of an LLM trained on newer data is reduced [12].
    This problem will likely be significant in keeping LLMs up to date, as curating
    a high-quality dataset will not be as simple as before.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM生成内容逐渐进入常规使用并重新在线，数据的大小和质量变得特别相关。例如，估计有6%到16%的学术同行评审正在使用LLMs [11]，并且许多校对服务很快就会使用这些新技术。这种增加的使用可能会产生一个负面反馈循环。随着LLMs生成数据的数量增加，可用于LLMs训练的非LLM内容将相应减少。这将导致可用语言的多样性总体下降，从而LLMs能够捕捉到的创新性也会减少。反过来，基于较新数据训练的LLM的质量也会降低
    [12]。这个问题可能会在保持LLMs更新方面变得非常重要，因为编制高质量数据集将不会像以前那样简单。
- en: There is also the problem that LLMs can only reflect information available at
    training and are disproportionally more likely to reflect information that is
    more prevalent in training. If you want an LLM that does not curse or use racist
    language, you must work to scrub your dataset of all cursing and racist language.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个问题，即LLMs只能反映训练时可用信息，并且更有可能反映在训练中更普遍的信息。如果你想得到一个不诅咒或使用种族主义语言的LLM，你必须努力清除数据集中所有的诅咒和种族主义语言。
- en: However, this problem is potentially a double-edged sword. If we want our LLM
    to know how to recognize and appropriately reject racist or foul language, it
    must know what racist and foul language are. You can imagine using prompting on
    an LLM that has never seen any racist text to “teach” the LLM to use racist words
    in a context that, without knowing `X` is racist, appears benign. But in the final
    form, we, as readers who are aware of racism, would recognize the sentence as
    objectionable. This problem, as of now, has no answer but is something to be mindful
    of.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个问题可能是一把双刃剑。如果我们想让我们的LLM知道如何识别和适当地拒绝种族主义或粗俗语言，它必须知道什么是种族主义和粗俗语言。你可以想象在一个从未见过任何种族主义文本的LLM上使用提示来“教导”LLM在一种看似无害的语境中使用种族主义词汇。但在最终形式中，我们作为意识到种族主义问题的读者，会认为这个句子是令人反感的。到目前为止，这个问题还没有答案，但这是我们应当留意的。
- en: Altering data is also important, as it is your only chance to influence how
    tokenization is performed in an LLM. As discussed in chapter 2, different approaches
    to tokenization have tradeoffs, but the choices you make are forever baked into
    the model once you start training.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 修改数据也很重要，因为这是你唯一能够影响LLM中分词方式的机会。正如第2章所讨论的，不同的分词方法有各自的权衡，但一旦开始训练，你所做的选择就会永远被嵌入到模型中。
- en: 5.4.2 Altering base model training
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 修改基础模型训练
- en: Training data privacy must be a significant concern when training or fine-tuning
    LLMs. Generally, it is possible to reconstruct a model’s training data by crafting
    inputs into a model in a special way. In some cases, LLMs have been shown to generate
    the exact passages on which they were trained. This is problematic if the training
    data contains private information, such as personally identifiable information
    (PII), private health information (PHI), or some other class of sensitive data.
    A user of a model could, perhaps unwittingly, provide a prompt that reveals this
    data verbatim.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据隐私在训练或微调大型语言模型（LLMs）时必须是一个重要的关注点。通常，通过以特殊方式构建输入到模型中，可以重建一个模型训练数据。在某些情况下，LLMs
    已被证明能够生成它们训练过的确切段落。如果训练数据包含私人信息，如个人可识别信息（PII）、私人健康信息（PHI）或其他敏感数据类别，这将是一个问题。模型的使用者可能无意中提供了一个提示，该提示会逐字逐句地揭示这些数据。
- en: Initial training of an algorithm is an ideal place to mitigate some of these
    privacy concerns by using a technique known as *differential privacy* (DP). DP
    is complex, so if you want to learn more, we recommend the book *Programming Differential
    Privacy* [13]. In short, DP adds a carefully constructed amount of random noise
    to provide provable guarantees about data privacy in the model training process.
    DP does not handle everything, but it provides much more protection than what
    is available with most algorithms today.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的初始训练是使用一种称为*差分隐私*（DP）的技术来缓解这些隐私问题的理想场所。DP很复杂，所以如果您想了解更多，我们推荐阅读《编程差分隐私》[13]。简而言之，DP在模型训练过程中添加了精心构造的随机噪声，以提供关于数据隐私的保证。DP不能处理所有事情，但它比大多数算法今天提供的保护要多得多。
- en: So why hasn’t everyone done just that? Well, adding noise naturally tends to
    reduce the quality of the result. Large training runs are expensive, costing hundreds
    of thousands to millions of dollars each. If you had to do ![equation image](../Images/eq-chapter-5-102-1.png)
    more training runs to set your privacy parameters correctly, you would have a
    million to tens-of-millions-of-dollars problem. But with DP becoming better every
    year, we suspect it will become more prevalent over time.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 那为什么不是每个人都这样做呢？嗯，添加噪声自然会降低结果的质量。大规模的训练运行成本高昂，每次可能花费数十万到数百万美元。如果您需要做![方程式图像](../Images/eq-chapter-5-102-1.png)更多的训练运行来正确设置隐私参数，您将面临数百万到数千万美元的问题。但随着DP每年都在变得更好，我们怀疑它将在未来变得更加普遍。
- en: 5.4.3 Altering the outputs
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.3 修改输出
- en: Finally, we can examine the tokens being produced and write code to change its
    behavior based on the combinations of tokens generated by the model. After fine-tuning,
    this is the second most likely stage that a consumer of LLMs will use to modify
    their behavior.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以检查生成的标记，并编写代码根据模型生成的标记组合来改变其行为。在微调后，这是LLM消费者修改其行为的第二可能阶段。
- en: Earlier in this chapter, we discussed a common need for LLMs to generate output
    that adheres to a precise format, such as XML or JSON. Implementing formatting
    requirements like these is a common problem with LLMs. Any single failed prediction
    results in a failure to generate valid output. You can see an example of this
    type of failure in figure [5.10](#fig__force_output), where we ask the LLM to
    complete some Python code; the next token should be a semicolon (;), but it erroneously
    attempts a newline (\ n) instead.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期部分，我们讨论了LLMs生成符合精确格式输出（如XML或JSON）的常见需求。实现这类格式要求是LLMs中常见的问题。任何一次预测失败都会导致无法生成有效的输出。您可以在图[5.10](#fig__force_output)中看到一个此类失败的例子；我们要求LLM完成一些Python代码；下一个标记应该是分号
    (;)，但它错误地尝试换行 (\ n)。
- en: '![figure](../Images/CH05_F10_Boozallen.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH05_F10_Boozallen.png)'
- en: Figure 5.10 By writing code that enforces a format specification, you can catch
    invalid output from an LLM as it is being generated. Once detected, having the
    LLM produce the next most likely token until a valid output is found is a simple
    way to improve the situation.
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.10 通过编写强制格式规范的代码，您可以在LLM生成输出时捕获无效输出。一旦检测到，让LLM生成下一个最可能的标记，直到找到有效输出，这是一种简单的方法来改善情况。
- en: Various tools exist (e.g., [https://github.com/noamgat/lm-format-enforcer](https://github.com/noamgat/lm-format-enforcer))
    for specifying strict formats as a part of the LLM’s decoding step. If these tools
    detect a parse error, they immediately regenerate the last token until a valid
    output is produced.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着各种工具（例如，[https://github.com/noamgat/lm-format-enforcer](https://github.com/noamgat/lm-format-enforcer)）用于在LLM的解码步骤中指定严格的格式。如果这些工具检测到解析错误，它们会立即重新生成最后一个标记，直到生成有效输出。
- en: More sophisticated approaches to selecting the next token are possible. Still,
    the important lesson here is the ability to use the intermediate outputs to make
    decisions before generating the entire output. Even simple old-school “go/no-go”
    lists are valuable tools for catching bad behavior. You do not need to pass an
    output to the user in true real time; you can always introduce an artificial delay
    so that you can see more of the response before sending it to the user. This gives
    you time to chat against bad language filters or other hard-coded checks. If a
    match occurs, just like in figure [5.10](#fig__force_output), you can regenerate
    an output or abort the user’s session.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然存在更多复杂的方法来选择下一个标记，但这里的重要教训是能够在生成完整输出之前使用中间输出做出决策。即使是简单的“进行/不进行”列表也是捕捉不良行为的有效工具。你不需要在真正的实时中将输出传递给用户；你总是可以引入人工延迟，这样你就可以在发送给用户之前看到更多的响应。这为你提供了时间来与不良语言过滤器或其他硬编码的检查进行对话。如果发生匹配，就像图
    [5.10](#fig__force_output) 中所示，你可以重新生成输出或终止用户的会话。
- en: 5.5 Integrating LLMs into larger workflows
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 将 LLM 集成到更大的工作流程中
- en: At this point in the chapter, we have covered some basic approaches to manipulating
    an LLM to produce more desirable and consistent outputs. So far, we have focused
    on techniques that involve the LLM itself, whether through prompting, manipulating
    training data, or fine-tuning a base model. In this section, we will explore how
    to tailor the output produced by an LLM by integrating the inputs and outputs
    of LLMs into multistep chains of operations to achieve more tailored results.
    This space is quickly evolving, so we will briefly cover one concrete example
    of integrating an LLM into a broader information retrieval workflow and then discuss
    a general-purpose tool to show you how to customize LLM outputs using multiple
    interactions with an LLM.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的这个阶段，我们已经介绍了一些基本方法来操纵 LLM 以产生更理想和一致的结果。到目前为止，我们主要关注涉及 LLM 本身的技巧，无论是通过提示、操作训练数据，还是微调基础模型。在本节中，我们将探讨如何通过将
    LLM 的输入和输出整合到多步骤操作链中，来定制 LLM 的输出以实现更精确的结果。这个领域正在迅速发展，因此我们将简要介绍一个将 LLM 集成到更广泛的信息检索工作流程的具体示例，然后讨论一个通用工具，以展示如何通过与
    LLM 的多次交互来定制 LLM 的输出。
- en: 5.5.1 Customizing LLMs with retrieval augmented generation
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.1 使用检索增强生成定制 LLM
- en: '*Retrieval augmented generation* (RAG) is a technique that allows us to produce
    answers from an LLM while reducing the likelihood of generating nonsensical or
    otherwise errant explanations. The “retrieval” component of the RAG moniker should
    give you a helpful hint as to how the technique operates. When a user provides
    input to a RAG system, it uses an LLM to create a query that is run against a
    search engine that contains an index of documents. Depending on the use case,
    this might be an index of general information, such as Google, or a subject-specific
    index, such as a collection of automotive marketing materials. In response to
    the query, the search engine generates a list of relevant documents. The RAG system
    then uses the LLM to extract information from those documents to generate better
    answers. To do this, the RAG system combines the contents of the retrieved documents
    with the original user query to create a comprehensive prompt for the LLM that
    will result in a better response. This method tends to work well because instead
    of asking an LLM to generate a response based on its training or fine-tuning data,
    we are now asking the LLM to generate a response to input by summarizing a set
    of documents relevant to a regular old search engine query and providing that
    set of relevant documents from which to draw its answers to the LLM. In other
    words, we’re helping the LLM focus on the data it needs to properly answer a given
    question. We describe this process in figure [5.11](#fig__rag) and compare it
    with the normal LLM use cases we have described so far.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*检索增强生成*（RAG）是一种技术，它允许我们在减少生成无意义或错误解释的可能性同时，从LLM中生成答案。RAG名称中的“检索”部分应该能给你一个关于该技术如何操作的提示。当用户向RAG系统提供输入时，它使用LLM创建一个查询，该查询在包含文档索引的搜索引擎上运行。根据用例，这可能是一个通用信息的索引，如Google，或一个特定主题的索引，如汽车营销材料的集合。搜索引擎根据查询生成一份相关文档列表。然后，RAG系统使用LLM从这些文档中提取信息以生成更好的答案。为此，RAG系统将检索到的文档内容与原始用户查询相结合，为LLM创建一个全面的提示，从而产生更好的响应。这种方法通常效果很好，因为我们现在要求LLM根据一组与常规搜索引擎查询相关的文档进行总结，并提供这些相关文档，以便从中获取其答案。换句话说，我们正在帮助LLM专注于它需要正确回答给定问题的数据。我们在图[5.11](#fig__rag)中描述了这一过程，并将其与我们迄今为止描述的正常LLM用例进行了比较。'
- en: 'The two most significant benefits of the RAG approach thus far are as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，RAG方法最显著的两大好处如下：
- en: The output of a RAG system is more accurate, factually correct, or otherwise
    useful to the user’s original question because it is based on specific sources
    contained in a document index.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG系统的输出比用户原始问题更准确、更符合事实或更有用，因为它基于文档索引中包含的特定来源。
- en: The LLM can generate citations or references to the source documents used to
    produce its responses, allowing users to validate or correlate against the original
    source material.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM可以生成引用或参考文献，指向用于生成其响应的源文档，使用户能够验证或与原始源材料相关联。
- en: '![figure](../Images/CH05_F11_Boozallen.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F11_Boozallen.png)'
- en: Figure 5.11 On the left, we show the normal use of an LLM of a user asking about
    how to write JSON. LLMs naturally have the chance of producing errant outputs,
    which we want to minimize. On the right, we show the RAG approach. By using a
    search engine, we can find documents that are relevant to a query and combine
    them into a new prompt, giving the LLM more information and context to produce
    a better answer.
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.11 在左侧，我们展示了用户询问如何编写JSON时LLM的正常使用情况。LLM自然有可能产生错误的输出，这是我们希望最小化的。在右侧，我们展示了RAG方法。通过使用搜索引擎，我们可以找到与查询相关的文档，并将它们组合成一个新的提示，为LLM提供更多信息和环境，以便产生更好的答案。
- en: The latter point regarding citations is particularly important. RAG will not
    solve all of LLMs’ problems because the LLM still generates the final output in
    a RAG system. The LLM may still produce errors or hallucinations due to content
    that it cannot find or that doesn’t exist. It is also possible that the LLM will
    not accurately capture or represent the content of any of the source documents
    it uses. As a result, the utility of the RAG approach is directly related to the
    quality of the search it performs and the documents that are returned. The bottom
    line is that if you can’t build an effective search engine for your problem, you
    can’t build an effective RAG model.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 关于引用的后者尤其重要。RAG不能解决LLM的所有问题，因为LLM仍然在RAG系统中生成最终输出。LLM可能仍然会因为找不到或不存在的内容而产生错误或幻觉。也有可能LLM无法准确捕捉或表示它使用的任何源文档的内容。因此，RAG方法的有效性与它执行的搜索质量和返回的文档直接相关。底线是，如果你不能为你的问题构建一个有效的搜索引擎，你就不能构建一个有效的RAG模型。
- en: Context size
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 上下文大小
- en: When thinking about LLMs, it is important to consider one aspect of LLMs known
    as the *context size*. The context size of the LLM determines how many tokens
    it can computationally handle in a single request for completions. You can think
    of it as the amount of data that an LLM is able to look at when receiving input
    in the form of a prompt. For example, GPT-3 has a context size of 2,048 tokens.
    However, in chatbots, for example, the context is often used to hold a running
    transcript of the entire conversation, including any LLM outputs. If you have
    a conversation with GPT-3 that goes beyond 2,048 tokens in length, you’ll find
    that GPT-3 often loses track of some of the things discussed early on in the chat.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当思考大型语言模型（LLM）时，考虑LLM的一个方面——即**上下文大小**——是很重要的。LLM的上下文大小决定了它在单个请求中可以处理多少个标记。你可以将其视为LLM在接收提示形式输入时能够查看的数据量。例如，GPT-3的上下文大小为2,048个标记。然而，在聊天机器人等应用中，上下文通常用于保存整个对话的运行记录，包括任何LLM的输出。如果你与GPT-3的对话长度超过2,048个标记，你会发现GPT-3经常在聊天早期讨论的一些内容上失去跟踪。
- en: Context size is an enabling and limiting factor for RAG use. If a RAG system
    retrieves an entire book for your LLM to digest, your LLM will require a huge
    context size to be able to use it. Otherwise, the LLM can only consume the first
    part of a retrieved document (up to the LLM’s context size) and may miss information.
    As a result, context size is an important operational characteristic you should
    consider when choosing a model. Some models today, such as X’s Grok, can handle
    up to 128,000 tokens as their context size. While large context sizes like Grok’s
    increase the hard limit of what an LLM can consume, the effectiveness of LLMs
    when dealing with large amounts of input enabled by larger context sizes is still
    an active area of study.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文大小是RAG（检索增强生成）使用的启用和限制因素。如果一个RAG系统为你的LLM检索整本书，你的LLM将需要一个巨大的上下文大小才能使用它。否则，LLM只能消费检索到的文档的前一部分（直到LLM的上下文大小），可能会错过信息。因此，上下文大小是在选择模型时应考虑的重要操作特性。今天的一些模型，如X的Grok，可以将上下文大小处理到128,000个标记。虽然像Grok这样的大型上下文大小增加了LLM可以消费的硬限制，但利用更大的上下文大小处理大量输入的LLM的有效性仍然是一个活跃的研究领域。
- en: You may notice in figure [5.11](#fig__rag) that we have to create a new prompt.
    We added the prefix “Answer the question:” followed by the postfix “Using the
    following information:” Hypothetically, you could obtain better results by tweaking
    this prompt. You may get thoughts about adding some instructions like “Ignore
    any of the following information if it is not relevant to the original question.”
    These ideas are starting to get into prompt engineering, the practice of tweaking
    and modifying the text going into an LLM to change its behavior, as we talked
    about earlier in chapter 4.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会在图[5.11](#fig__rag)中注意到，我们必须创建一个新的提示。我们在前面添加了“回答问题：”的前缀，后面跟着“使用以下信息：”的后缀。从理论上讲，通过调整这个提示，你可以获得更好的结果。你可能会有这样的想法，添加一些指令，比如“如果以下信息与原始问题不相关，则忽略任何此类信息。”这些想法开始涉及到提示工程，这是一种调整和修改输入LLM的文本以改变其行为的实践，正如我们在第4章中讨论的那样。
- en: 'Prompt engineering is indeed useful and a good way to combine multiple calls
    to an LLM to improve results. For example, you could try to improve your search
    results by asking the LLM to rewrite the question. (This discussion touches on
    a classic area of information retrieval called *query expansion*, if you wish
    to learn more on the topic.) However, prompt engineering can be very brittle:
    any update to an LLM may change what prompts do or don’t work, and it would be
    a pain to have to rewrite every prompt—especially as you get into anything more
    complex, like a RAG model or something even more sophisticated.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程确实很有用，并且是结合多个LLM调用以改进结果的好方法。例如，你可以尝试通过让LLM重写问题来改进你的搜索结果。（这次讨论涉及信息检索的一个经典领域，称为*查询扩展*，如果你想了解更多关于这个主题的信息。）然而，提示工程可能非常脆弱：任何LLM的更新都可能改变提示的效果，而且不得不重写每个提示会非常痛苦——尤其是在处理更复杂的事情时，比如RAG模型或更复杂的东西。
- en: 5.5.2 General-purpose LLM programming
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.2 通用LLM编程
- en: 'Although still new, we are already starting to see programming libraries and
    other software tools built using LLMs as a component of custom applications. One
    we particularly like is DSPy ([https://dspy.ai](https://dspy.ai)), which can make
    it easier to build and maintain programs that attempt to alter the inputs to and
    outputs of an LLM. A good software library will hide details that get in the way
    of productivity, and DSPy does a good job of abstracting away the following tasks
    around LLM usage:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然仍然很新，但我们已经开始看到使用LLM作为自定义应用程序组件的编程库和其他软件工具。我们特别喜欢的一个是DSPy（[https://dspy.ai](https://dspy.ai)），它可以使得构建和维护试图改变LLM输入和输出的程序变得更加容易。一个好的软件库将隐藏那些阻碍生产力的细节，DSPy在抽象以下关于LLM使用的任务方面做得很好：
- en: Integrating the specific LLM being used
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成正在使用的特定LLM
- en: Implementing common patterns of prompting
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现常见的提示模式
- en: Tweaking the prompts for your desired combination of data, task, and LLM.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整提示以适应你所需的数据、任务和LLM的组合。
- en: This is not a coding book, so a full tutorial on DSPy is out of scope. But it
    is illustrative to look at the ways DSPy can be used to implement the RAG model
    we described in section [5.5.1](#sec__rag). It will require that we pick an LLM
    to use (GPT-3.5, in this case), as well as a database of information (Wikipedia
    will work well), and define the RAG algorithm. DSPy works by defining a default
    LLM and database used by all components (unless you intervene), making it easy
    to separate and replace the parts being used. This process is shown in the following
    listing.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一本编程书，因此DSPy的完整教程超出了范围。但是，查看DSPy如何实现我们在5.5.1节中描述的RAG模型是有启发性的。这需要我们选择一个要使用的LLM（在这个例子中是GPT-3.5），以及一个信息数据库（维基百科将很好地工作），并定义RAG算法。DSPy通过定义所有组件（除非你干预）使用的默认LLM和数据库来工作，这使得分离和替换正在使用的部分变得容易。这个过程在下面的列表中展示。
- en: Listing 5.1 Simplest RAG in DSPy
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.1 DSPy中最简单的RAG
- en: '[PRE0]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Uses OpenAI''s GPT-3.5, which can be swapped out with other online or local
    LLMs'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用OpenAI的GPT-3.5，可以与其他在线或本地LLM互换'
- en: '#2 Uses the ColBERTv2 algorithm to vectorize a copy of Wikipedia'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用ColBERTv2算法将维基百科的副本向量化'
- en: '#3 Uses the LLM and document database we just created'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用我们刚刚创建的LLM和文档数据库'
- en: '#4 Searches for the three most relevant documents from the database'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 在数据库中搜索最相关的三个文档'
- en: '#5 Specifies a ``signature" string, which defines the inputs and output of
    the LLM'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 指定一个“signature”字符串，它定义了LLM的输入和输出'
- en: '#6 Calls the functions to build a RAG model. The parameter names match the
    names in the signature.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 调用函数以构建一个RAG模型。参数名称与签名中的名称匹配。'
- en: This code sets the aforementioned choices in LLMs and databases as the defaults,
    making it just as easy to replace OpenAI with another online LLM or a local one
    such as Llama. The `class RAG(dspy.Module):` class then defines the RAG algorithm.
    The initializer only has two parts.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将上述LLM和数据库的选择设置为默认值，使得替换OpenAI为另一个在线LLM或本地LLM（如Llama）变得同样简单。`class RAG(dspy.Module):`
    类定义了RAG算法。初始化器只有两部分。
- en: First, we need a way to search a database of strings based on vectorized documents,
    which is defined with `ColBERTv2`. It uses an older—as in just four years ago
    (wild how fast the field is moving)—but much faster language model for speed and
    efficiency. Remember, the larger language model (that is, the more expensive to
    run) just needs reasonable documents to be retrieved. While ColBERTv2 probably
    won’t do as good a job as GPT-3.5, it is more than good enough to get you the
    right documents most of the time. The `dspy.Retrieve` then uses this default database
    for searching, so there is no need to specify anything more than how many documents
    to retrieve.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, we need to combine the questions and documents into a query for the
    LLM. In DSPy, the prompt is abstracted away from us. Instead, we write what DSPy
    calls a *signature*, which you can think of as the inputs and outputs of a function.
    These should be given meaningful English names so that DSPy can generate a good
    prompt for you. (Under the hood, DSPy uses a language model to optimize prompts!)
    In this case, we have two inputs (`question` and `relevant_documents`) separated
    by a comma. The `->` is used to denote the start of the outputs, of which we have
    only one: the `answer` to the question.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Note DSPy supports some basic types in signatures. For example, you can enforce
    that the answer must be an integer by denoting `"question, relevant``_documents
    -> answer:int"` in the string. This command will apply the same technique to regenerating
    on errors that we just learned about in figure [5.10](#fig__force_output).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: That is all it takes to define our RAG model! The objects are called and passed
    out in the `forward` function, but you can modify this code to add additional
    details if you want. You can convert everything to lowercase, run a spell checker,
    or use whatever kind of code you want here. This approach lets you mix and match
    programming rules with LLMs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: You can also easily modify the RAG definition to include new constraints and
    write code to have an LLM perform validation. More importantly, DSPy supports
    using a training/validation set to tune the prompts better, fine-tune local LLMs,
    and help you create an empirically tested, improved, and quantified model to achieve
    your goals without having to spend a lot of time on LLM-specific details. Adopting
    tools like this early will give you a far more robust solution that allows you
    to upgrade to newer architectures more easily.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can intervene to change a model’s behavior in four places: the datacollection/tokenization,
    training the initial base model, fine-tuning the base model, and intercepting
    the predicted tokens. All four places are important, but fine-tuning is the most
    effective place for most users to make changes that lower the cost and provide
    the optimal ability to change the model’s goals.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised fine-tuning (SFT) performs the normal training process on a smaller
    bespoke data collection and is useful for refining the model’s knowledge of a
    particular domain.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督微调（SFT）在较小的定制数据集上执行正常的训练过程，对于细化模型对特定领域的知识很有用。
- en: Reinforcement learning from human feedback (RLHF) requires more data, but it
    allows us to specify objectives more complex than “predict the next token.”
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自人类反馈的强化学习（RLHF）需要更多的数据，但它允许我们指定比“预测下一个标记”更复杂的任务目标。
- en: You can use existing tools like syntax checkers to detect incorrect LLM outputs
    in cases where the output format must be strict, such as for JSON or XML. Generation
    and syntax checking can be run in a loop until the output satisfies the necessary
    syntax constraints.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用现有的工具，如语法检查器，在输出格式必须严格的情况下（例如JSON或XML），检测LLM的错误输出。生成和语法检查可以在循环中运行，直到输出满足必要的语法约束。
- en: Retrieval augmented generation (RAG) is a popular method of augmenting the input
    of an LLM by first finding relevant content via a search engine or database and
    then inserting it into the prompt.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）是一种流行的通过首先通过搜索引擎或数据库找到相关内容，然后将它插入到提示中的方法来增强LLM（大型语言模型）输入的方法。
- en: Coding frameworks like DSPy are beginning to emerge that separate the specific
    LLM, vectorization, and prompt definition from the logic of how inputs and outputs
    from the LLM are modified for a specific task. This method allows you to build
    more reliable and repeatable LLM solutions that can quickly adapt to new models
    and methods.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似于DSPy的编码框架开始出现，它们将特定的LLM、向量化以及提示定义与LLM输入和输出的修改逻辑分离，以适应特定任务。这种方法允许你构建更可靠和可重复的LLM解决方案，这些解决方案可以快速适应新的模型和方法。
