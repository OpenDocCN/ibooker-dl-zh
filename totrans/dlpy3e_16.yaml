- en: Text generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本生成
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter16_text-generation](https://deeplearningwithpython.io/chapters/chapter16_text-generation)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://deeplearningwithpython.io/chapters/chapter16_text-generation](https://deeplearningwithpython.io/chapters/chapter16_text-generation)
- en: When I first claimed that in a not-so-distant future, most of the cultural content
    we consume would be created with substantial help from AIs, I was met with utter
    disbelief, even from long-time machine learning practitioners. That was in 2014\.
    Fast-forward a decade, and that disbelief had receded at an incredible speed.
    Generative AI tools are now common additions to word processors, image editors,
    and development environments. Prestigious awards are going out to literature and
    art created with generative models — to considerable controversy and debate.^([[1]](#footnote-1))
    It no longer feels like science fiction to consider a world where AI and artistic
    endeavors are often intertwined.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当我首次声称在不那么遥远的未来，我们消费的大部分文化内容都将得到人工智能的大量帮助时，我遭到了彻底的怀疑，甚至来自长期从事机器学习实践的人。那是在2014年。快进十年，这种怀疑以惊人的速度消退。生成式人工智能工具现在已成为文字处理器、图像编辑器和开发环境中的常见补充。文学和艺术作品因生成模型而获得声望——引起了相当大的争议和辩论。^([[1]](#footnote-1))
    考虑到人工智能与艺术创作经常交织在一起的世界，已经不再像科幻小说了。
- en: In any practical sense, AI is nowhere close to rivaling human screenwriters,
    painters, or composers. But replacing humans need not, and should not, be the
    point. In many fields, but especially in creative ones, people will use AI to
    augment their capabilities — more augmented intelligence than artificial intelligence.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何实际意义上，人工智能都远远无法与人类编剧、画家或作曲家相媲美。但取代人类并不需要，也不应该是目标。在许多领域，尤其是在创意领域，人们将使用人工智能来增强他们的能力——更多的是增强智能而非人工智能。
- en: Much of artistic creation consists of pattern recognition and technical skill.
    Our perceptual modalities, language, and artwork all have statistical structure,
    and deep learning models excel at learning this structure. Machine learning models
    can learn the statistical latent spaces of images, music, and stories, and they
    can then sample from these spaces, creating new artworks with characteristics
    similar to those the model has seen in its training data. Such sampling is hardly
    an act of artistic creation in itself — it’s a mere mathematical operation. Only
    our interpretation, as human spectators, gives meaning to what the model generates.
    But in the hands of a skilled artist, algorithmic generation can be steered to
    become meaningful — and beautiful. Latent space sampling can become a brush that
    empowers the artist, augments our creative affordances, and expands the space
    of what we can imagine. It can even make artistic creation more accessible by
    eliminating the need for technical skill and practice — setting up a new medium
    of pure expression, factoring art apart from craft.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 艺术创作的大部分内容都包括模式识别和技术技能。我们的感知模式、语言和艺术品都具有统计结构，而深度学习模型擅长学习这种结构。机器学习模型可以学习图像、音乐和故事的统计潜在空间，然后可以从这些空间中进行采样，创建具有与模型在训练数据中看到的特点相似的新艺术品。这种采样本身几乎不是艺术创作的行为——它仅仅是一个数学运算。只有我们作为人类观众的解释，才能赋予模型生成的内容意义。但一个技艺高超的艺术家可以将算法生成引导到有意义的——并且美丽的一面。潜在空间采样可以成为赋予艺术家力量的画笔，增强我们的创造能力，并扩展我们能够想象的空间。它甚至可以通过消除对技术技能和实践的需求，使艺术创作更加容易接近——建立一个纯粹表达的新媒介，将艺术与工艺分开。
- en: '![](../Images/5c3c0ff3641e2655954865fcd3da353a.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5c3c0ff3641e2655954865fcd3da353a.png)'
- en: '[Figure 16.1](#figure-16-1): An image generated with the generative image software
    Midjourney. The prompt was “A hand-drawn, sci-fi landscape of residents living
    in a building shaped like a red letter K.”'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16.1](#figure-16-1)：使用生成图像软件Midjourney生成的图像。提示词是“一个手绘的科幻景观，居民居住在一个像红色字母K形状的建筑中。”'
- en: Iannis Xenakis, a visionary pioneer of electronic and algorithmic music, beautifully
    expressed this same idea in the 1960s, in the context of the application of automation
    technology to music composition:^([[2]](#footnote-2))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 伊安尼斯·泽纳基斯，电子和算法音乐的先驱性开拓者，在20世纪60年代，在将自动化技术应用于音乐创作的背景下，巧妙地表达了同样的观点：^([[2]](#footnote-2))
- en: 'Freed from tedious calculations, the composer is able to devote himself to
    the general problems that the new musical form poses and to explore the nooks
    and crannies of this form while modifying the values of the input data. For example,
    he may test all instrumental combinations from soloists to chamber orchestras,
    to large orchestras. With the aid of electronic computers the composer becomes
    a sort of pilot: he presses the buttons, introduces coordinates, and supervises
    the controls of a cosmic vessel sailing in the space of sound, across sonic constellations
    and galaxies that he could formerly glimpse only as a distant dream.'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 脱离了繁琐的计算，作曲家能够专注于新音乐形式提出的一般问题，并在修改输入数据值的同时探索这一形式的各种角落和缝隙。例如，他可能会测试从独奏者到室内乐团再到大型乐团的所有乐器组合。借助电子计算机，作曲家变成了一种飞行员：他按按钮，输入坐标，并监督在声音空间中航行的宇宙飞船的控制，穿越他以前只能作为遥远梦想一瞥的声学星座和星系。
- en: 'The potential for generative AI extends well beyond artistic endeavors. In
    many professions, people create content where pattern recognition is even more
    apparent: think of summarizing large documents, transcribing speech, editing for
    typos, or flagging common mistakes in code. These rote tasks play directly to
    the strengths of deep learning approaches. There is a lot to consider regarding
    how we choose to deploy AI in the workplace — with real societal implications.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI的潜力远远超出了艺术创作的范畴。在许多职业中，人们创造的内容中模式识别更为明显：想想总结大量文档、转录语音、校对错别字或标记代码中的常见错误。这些例行公事的任务直接针对深度学习方法的优点。关于我们在工作场所如何选择部署AI有很多要考虑的问题——这具有真实的社会影响。
- en: In the following two chapters, we will explore the potential of deep learning
    to assist with creation. We will learn to curate latent spaces in text and image
    domains and pull new content from these spaces. We will start with text, scaling
    up the idea of a language model we first worked with in the last chapter. These
    *large language models*, or *LLMs* for short, are behind digital assistants like
    ChatGPT and a quickly growing list of real-world applications.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两章中，我们将探讨深度学习在创作方面的潜力。我们将学习在文本和图像领域管理潜在空间，并从这些空间中提取新内容。我们将从文本开始，扩展我们在上一章中首次使用的语言模型的概念。这些*大型语言模型*，或简称*LLMs*，是像ChatGPT这样的数字助手以及快速增长的现实世界应用背后的技术。
- en: A brief history of sequence generation
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列生成的简史
- en: Until quite recently, the idea of generating sequences from a model was a niche
    subtopic within machine learning — generative recurrent networks only began to
    hit the mainstream in 2016\. However, these techniques have a fairly long history,
    starting with the development of the LSTM algorithm in 1997.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，从模型生成序列的想法在机器学习领域还是一个边缘的子主题——生成式循环网络直到2016年才开始进入主流。然而，这些技术有着相当长的历史，始于1997年LSTM算法的开发。
- en: In 2002, Douglas Eck applied LSTM to music generation for the first time, with
    promising results. Eck became a researcher at Google Brain, and in 2016, he started
    a new research group called Magenta, which focused on applying modern deep learning
    techniques to produce engaging music. Sometimes, good ideas take 15 years to get
    started.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 2002年，Douglas Eck首次将LSTM应用于音乐生成，并取得了有希望的结果。Eck成为了谷歌大脑的研究员，2016年，他成立了一个名为Magenta的新研究小组，专注于将现代深度学习技术应用于创作引人入胜的音乐。有时，好主意需要15年的时间才能开始实施。
- en: 'In the late 2000s and early 2010s, Alex Graves pioneered the use of recurrent
    networks for new types of sequence data generation. In particular, some see his
    2013 work on applying recurrent mixture density networks to generate human-like
    handwriting using timeseries of pen positions as a turning point. Graves left
    a commented-out remark hidden in a 2013 LaTeX file uploaded to the preprint server
    arXiv: “Generating sequential data is the closest computers get to dreaming.”
    This work and the notion of machines that dream were significant inspirations
    when I started developing Keras.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在2000年代末和2010年代初，Alex Graves开创了使用循环网络生成新型序列数据的方法。特别是，有些人认为他在2013年将循环混合密度网络应用于使用笔位时间序列生成类似人类手写的作品的工作是一个转折点。Graves在2013年上传到预印本服务器arXiv的一个LaTeX文件中留下了一条注释掉的评论：“从模型生成序列是计算机最接近做梦的时候。”当我开始开发Keras时，这项工作和机器做梦的概念是重要的灵感来源。
- en: 'In 2018, a year after the “Attention Is All You Need” paper we discussed in
    the last chapter, a group of researchers at an organization called OpenAI put
    out a new paper “Improving Language Understanding by Generative Pre-Training.”^([[3]](#footnote-3))
    They combined a few ingredients:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中我们讨论的“Attention Is All You Need”论文发布后的一年，即 2018 年，一个名为 OpenAI 的组织的研究人员发布了一篇新论文“通过生成预训练改进语言理解”。^([[3]](#footnote-3))
    他们结合了几个要素：
- en: Unsupervised pretraining of a language model — essentially training a model
    to “guess the next token” in a sequence, as we did with our Shakespeare generator
    in chapter 15
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型的非监督预训练——本质上是在序列中训练模型“猜测下一个标记”，就像我们在第 15 章中的莎士比亚生成器所做的那样
- en: The Transformer architecture
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 架构
- en: Textual data on various topics via thousands of self-published books
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过成千上万的自出版书籍中的各种主题文本数据
- en: The authors showed that such a pretrained model could be fine-tuned to achieve
    state-of-the-art performance on a wide array of text classification tasks — from
    gauging the similarity of two sentences to answering a multiple-choice question.
    They called the pretrained model *GPT*, short for Generative Pretrained Transformer.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作者展示了这样的预训练模型可以通过微调在广泛的文本分类任务上实现最先进的性能——从衡量两个句子的相似度到回答多项选择题。他们将预训练模型称为 *GPT*，即生成预训练
    Transformer 的缩写。
- en: GPT didn’t come with any modeling or training advancements. What was interesting
    about the results was that such a general training setup could beat out more involved
    techniques across a number of tasks. There was no complex text normalization,
    no need to customize the model architecture or training data per benchmark, just
    a lot of pretraining data and compute.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 并没有带来任何建模或训练上的进步。有趣的是，结果是这样的通用训练设置可以在多个任务上击败更复杂的技巧。没有复杂的文本归一化，不需要针对每个基准定制模型架构或训练数据，只需要大量的预训练数据和计算。
- en: 'In the following years, OpenAI set about scaling this idea with a single-minded
    focus. The model architecture changed only slightly. Over four years, OpenAI released
    three versions of GPT, scaling up as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几年里，OpenAI 以单一的目标集中精力扩展这一想法。模型架构仅略有变化。在四年的时间里，OpenAI 发布了三个版本的 GPT，扩展情况如下：
- en: Released in 2018, GPT-1 had 117 million parameters and was trained on 1 billion
    tokens.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2018 年发布的 GPT-1 拥有 1.17 亿个参数，并在 10 亿个标记上进行了训练。
- en: Released in 2019, GPT-2 had 1.5 billion parameters and was trained on more than
    10 billion tokens.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2019 年发布的 GPT-2 拥有 15 亿个参数，并在超过 100 亿个标记上进行了训练。
- en: Released in 2020, GPT-3 had 175 billion parameters and was trained on somewhere
    around half a trillion tokens.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2020 年发布的 GPT-3 拥有 1750 亿个参数，并在大约半万亿个标记上进行了训练。
- en: The language modeling setup enabled each of these models to generate text, and
    the developers at OpenAI noticed that with each leap in scale, the quality of
    this generative output shot up substantially.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型设置使得每个模型都能够生成文本，OpenAI 的开发者注意到，随着规模的每次跃升，这种生成输出的质量都会显著提高。
- en: With GPT-1, the model’s generative capabilities were mostly a by-product of
    its pretraining and not the primary focus. They evaluated the model by fine-tuning
    it with an extra dense layer for classification, as we did with RoBERTa in the
    last chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPT-1 中，模型的生成能力主要是其预训练的副产品，而不是主要焦点。他们通过添加一个额外的密集层进行分类微调来评估模型，就像我们在上一章中对待 RoBERTa
    一样。
- en: 'With GPT-2, the authors noticed that you could prompt the model with a few
    examples of a task and generate quality output without any fine-tuning. For instance,
    you could prompt the model with the following to receive a French translation
    of the word cheese:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPT-2 中，作者注意到，你可以用几个任务的示例来提示模型，并生成高质量的输出，而无需任何微调。例如，你可以用以下内容提示模型以获得“奶酪”的法语翻译：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This type of setup is called *few-shot learning*, where you attempt to teach
    a model a new problem with only a handful of supervised examples — too few for
    standard gradient descent.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置被称为 *少量样本学习*，即你试图用少量监督示例来教授模型一个新问题——对于标准梯度下降来说太少了。
- en: 'With GPT-3, examples weren’t always necessary. You could prompt the model with
    a simple text description of the problem and the input and often get quality results:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPT-3 中，示例并不总是必要的。你可以用对问题、输入的简单文本描述来提示模型，并经常得到高质量的结果：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: GPT-3 was still plagued by fundamental issues that have yet to be solved. LLMs
    “hallucinate” often — their output can veer from accurate to completely false
    with zero indication. They’re extremely sensitive to prompt phrasing, with seemingly
    minor prompt rewording triggering large jumps up or down in performance. And they
    cannot adapt to problems that weren’t extensively featured in their training data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3仍然受到一些尚未解决的问题的基本问题的困扰。LLMs“幻想”很常见——它们的输出可以从准确到完全错误，没有任何指示。它们对提示语句非常敏感，看似微小的提示重写可能会触发性能的大幅上升或下降。而且它们无法适应训练数据中没有广泛涉及的问题。
- en: However, the generative output from GPT-3 was good enough that the model became
    the basis for ChatGPT — the first widespread, consumer-facing generative model.
    In the months and years since, ChatGPT has sparked a deluge of investment and
    interest in building LLMs and finding new use cases for them. In the next section,
    we will make a miniature GPT model of our own to better understand how such a
    model works, what it can do, and where it fails.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GPT-3的生成输出已经足够好，以至于该模型成为了ChatGPT的基础——第一个广泛使用的面向消费者的生成模型。在过去的几个月和几年里，ChatGPT引发了大量投资和对构建LLMs以及寻找它们的新用例的兴趣。在下一节中，我们将构建一个自己的迷你GPT模型，以更好地理解这样的模型是如何工作的，它能做什么，以及它的局限性在哪里。
- en: Training a mini-GPT
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练一个迷你GPT
- en: To begin pretraining our mini-GPT, we will need a lot of text data. GPT-1 used
    a dataset called BooksCorpus, which contained a number of free, self-published
    books added to the dataset without the explicit permission of the authors. The
    dataset has since been taken down by its publishers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始预训练我们的迷你GPT，我们需要大量的文本数据。GPT-1使用了名为BooksCorpus的数据集，其中包含了一些未经作者明确许可就添加到数据集中的免费、自出版书籍。该数据集随后被其出版商撤下。
- en: We will use a more recent pretraining dataset called the “Colossal Clean Crawled
    Corpus” (C4), released by Google in 2020\. At 750 GB, it’s far bigger than we
    could reasonably train on for a book example, so we will use less than 1% of the
    overall corpus.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个更近期的预训练数据集，称为“Colossal Clean Crawled Corpus”（C4），由谷歌在2020年发布。它有750GB，比我们为书籍示例合理训练的要大得多，所以我们将使用整个语料库的不到1%。
- en: 'Let’s start by downloading and extracting our data:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先下载并提取我们的数据：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Listing 16.1](#listing-16-1): Downloading a portion of the C4 dataset'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表16.1](#listing-16-1)：下载C4数据集的一部分'
- en: 'We have 50 shards of text data, each with about 75 MB of raw text. Each line
    contains a document in the crawl with newlines escaped. Let’s look at a document
    in our first shard:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有50个文本数据分片，每个分片大约有75MB的原始文本。每一行都包含一个爬取的文档，其中换行符被转义。让我们看看我们第一个分片中的一个文档：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will need to preprocess a lot of data to run pretraining for an LLM, even
    a miniature one like the one we are training. Using a fast tokenization routine
    to preprocess our source documents into integer tokens can simplify our lives.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是像我们正在训练的这样的迷你LLM，我们也需要预处理大量数据来运行预训练。使用快速分词程序将源文档预处理为整数标记可以简化我们的生活。
- en: We will use SentencePiece, a library for subword tokenization of text data.
    The actual tokenization technique is the same as the byte-pair encoding tokenization
    we built ourselves in chapter 14, but the library is written in C++ for speed
    and adds a `detokenize()` function that will reverse integers to strings and join
    them together. We will use a premade vocabulary with 32,000 vocabulary terms stored
    in a particular format needed by the SentencePiece library.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用SentencePiece，这是一个用于文本数据子词分词的库。实际的分词技术与我们自己在第14章中构建的字节对编码分词技术相同，但这个库是用C++编写的，以提高速度，并添加了一个`detokenize()`函数，该函数可以将整数反转成字符串并将它们连接起来。我们将使用一个包含32,000个词汇项的预制作词汇表，这些词汇项存储在SentencePiece库需要的特定格式中。
- en: As in the last chapter, we can use the KerasHub library to access some extra
    functions for working with large language models. KerasHub wraps the SentencePiece
    library as a Keras layer. Let’s try it out.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上一章所述，我们可以使用KerasHub库来访问一些用于处理大型语言模型的额外函数。KerasHub将SentencePiece库包装成一个Keras层。让我们试试看。
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Listing 16.2](#listing-16-2): Downloading a SentencePiece vocabulary and instantiating
    a tokenizer'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表16.2](#listing-16-2)：下载SentencePiece词汇表并实例化分词器'
- en: 'We can use this tokenizer to map from text to int sequences bidirectionally:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个分词器双向地将文本映射到整数序列：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let’s use this layer to tokenize our input text and then use `tf.data` to window
    our input into sequences of length 256.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个层来分词我们的输入文本，然后使用`tf.data`将输入窗口化成长度为256的序列。
- en: When training GPT, the developers chose to keep things simple and make no attempt
    to keep document boundaries from occurring in the middle of a sample. Instead,
    they marked a document boundary with a special `<|endoftext|>` token. We will
    do the same here. Once again, we will use `tf.data` for the input data pipeline
    and train with any backend.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练GPT时，开发者选择保持简单，并试图避免在样本中间出现文档边界。相反，他们使用特殊的`<|endoftext|>`标记来标记文档边界。我们在这里也将这样做。再次使用`tf.data`作为输入数据管道，并使用任何后端进行训练。
- en: We will load each file shard individually and interleave the output data into
    a single dataset. This keeps our data loading fast, and we don’t need to worry
    about text lining up across sample boundaries — each is independent. With interleaving,
    each processor on our CPU can read and tokenize a separate file simultaneously.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将单独加载每个文件分片，并将输出数据交错到一个单独的数据集中。这保持了我们的数据加载速度快，我们不需要担心文本在样本边界处对齐——每个都是独立的。通过交错，我们CPU上的每个处理器可以同时读取和标记一个单独的文件。
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Listing 16.3](#listing-16-3): Preprocessing text input for Transformer pretraining'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表16.3](#listing-16-3)：为Transformer预训练预处理文本输入'
- en: As we first did in chapter 8, we will end our `tf.data` pipeline with a call
    to `prefetch()`. This will make sure we always have some batches loaded onto our
    GPU and ready for the model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第8章中首次做的那样，我们将以对`prefetch()`的调用结束我们的`tf.data`管道。这将确保我们始终有一些批次的加载到我们的GPU上，并准备好用于模型。
- en: 'We have 58,746 batches. You could count this yourself if you would like — the
    line `ds.reduce(0, lambda c, _: c + 1)` will iterate over the entire dataset and
    increment a counter. But simply tokenizing a dataset of this size will take a
    few minutes on a decently fast CPU.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '我们有58,746个批次。如果您愿意，可以自己数一下——行`ds.reduce(0, lambda c, _: c + 1)`将遍历整个数据集并增加计数器。但是，仅对这么大的数据集进行标记化就需要在性能良好的CPU上几分钟。'
- en: 'At 64 samples per batch and 256 tokens per sample, this is just under a billion
    tokens of data. Let’s split off 500 batches as a quick validation set, and we
    are ready to start pretraining:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个批次64个样本和每个样本256个标记的情况下，这接近十亿个标记的数据。让我们分出500个批次作为快速验证集，然后我们就可以开始预训练了：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Building the model
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建模型
- en: The original GPT model simplifies the sequence-to-sequence Transformer we saw
    in the last chapter. Rather than take in a source and target sequence with an
    encoder and decoder, as we did for our translation model, the GPT approach does
    away with the encoder entirely and only uses the decoder. This means that information
    can only travel from left to right in a sequence.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的GPT模型简化了我们上章看到的序列到序列Transformer。与我们的翻译模型不同，它不是通过编码器和解码器接收源和目标序列，而是完全去掉了编码器，只使用解码器。这意味着信息只能在一个序列中从左到右传递。
- en: This was an interesting bet on the part of the GPT developers. A decoder-only
    model can still handle sequence-to-sequence problems like question-answering.
    However, rather than feeding in the question and answer as separate inputs, we
    must combine both into a single sequence to feed it to our model. So, unlike the
    original Transformer, the question tokens would not be handled any differently
    than answer tokens. All tokens are embedded into the same latent space with the
    same set of parameters.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对GPT开发者的一次有趣的赌注。仅解码器模型仍然可以处理像问答这样的序列到序列问题。然而，我们不是将问题和答案作为单独的输入提供，而是将两者合并成一个序列来提供给我们的模型。因此，与原始的Transformer不同，问题标记将不会像答案标记那样被特殊处理。所有标记都嵌入到相同的潜在空间中，使用相同的参数集。
- en: The other consequence of this approach is that the information flow is no longer
    bidirectional, even for input sequences. Given an input, such as “Where is the
    capital of France?”, the learned representation of the word “Where” cannot attend
    to the words “capital” and “France” in the attention layer. This limits the expressivity
    of the model but has a massive advantage in terms of simplicity of pretraining.
    We don’t need to curate datasets with pairs of inputs and outputs; everything
    can be a single sequence. We can train on any text we can find on the internet
    at a massive scale.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的另一个后果是，即使对于输入序列，信息流也不再是双向的。给定一个输入，例如“法国的首都是哪里？”，单词“Where”在注意力层中无法关注到“capital”和“France”。这限制了模型的表达能力，但在预训练的简单性方面具有巨大的优势。我们不需要整理包含输入和输出对的集合；一切都可以是一个单独的序列。我们可以在互联网上找到的任何文本上进行大规模训练。
- en: Let’s copy the `TransformerDecoder` from chapter 15 but remove the cross-attention
    layer, which allowed the decoder to attend to the encoder sequence. We will also
    make one minor change, adding dropout after the attention and feedforward blocks.
    In chapter 15, we only used a single Transformer layer in our encoder and decoder,
    so we could get away with only using a single dropout layer at the end of our
    entire model. For our GPT model, we will stack quite a few layers, so adding dropout
    within each decoder layer is important to prevent overfitting.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第15章复制`TransformerDecoder`，但移除交叉注意力层，这使得解码器能够关注编码器序列。我们还将进行一个小的修改，在注意力和前馈块之后添加dropout。在第15章中，我们在编码器和解码器中只使用了一个Transformer层，因此我们可以在整个模型的末尾只使用一个dropout层。对于我们的GPT模型，我们将堆叠相当多的层，因此在每个解码器层中添加dropout对于防止过拟合非常重要。
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Listing 16.4](#listing-16-4): A Transformer decoder block without cross-attention'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表16.4](#listing-16-4)：没有交叉注意力的Transformer解码器块'
- en: Next, we can copy the `PositionalEmbedding` layer from chapter 15\. Recall that
    this layer gives us a simple way to learn an embedding for each position in a
    sequence and combine that with our token embeddings.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以从第15章复制`PositionalEmbedding`层。回想一下，这个层为我们提供了一种简单的方法来学习序列中每个位置嵌入，并将其与我们的标记嵌入相结合。
- en: There’s a neat trick we can employ here to save some GPU memory. The biggest
    weights in a Transformer model are the input token embeddings and output dense
    prediction layer because they deal with our vocabulary space. The token embedding
    weight has shape `(vocab_size, hidden_dim)` to embed every possible token. Our
    output projection has shape `(hidden_dim, vocab_size)` to make a floating-point
    prediction for every possible token.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个巧妙的方法可以用来节省一些GPU内存。在Transformer模型中，最大的权重是输入标记嵌入和输出密集预测层，因为它们处理我们的词汇空间。标记嵌入权重具有形状`(vocab_size,
    hidden_dim)`，用于嵌入每个可能的标记。我们的输出投影具有形状`(hidden_dim, vocab_size)`，为每个可能的标记做出浮点预测。
- en: We can actually tie these two weight matrices together. To compute our model’s
    final predictions, we will multiply our hidden states by the transpose of our
    token embedding matrix. You can very much think of our final projection as a “reverse
    embedding.” It maps from hidden space to token space, whereas an embedding maps
    from token space to hidden space. It turns out that using the same weights for
    this input and output projection is a good idea.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以将这两个权重矩阵绑定在一起。为了计算我们模型的最终预测，我们将隐藏状态乘以标记嵌入矩阵的转置。你可以非常地将最终的投影视为一个“反向嵌入”。它将隐藏空间映射到标记空间，而嵌入则是从标记空间映射到隐藏空间。结果发现，使用相同的权重进行输入和输出投影是一个好主意。
- en: Adding this to our `PositionalEmbedding` is simple; we will just add a `reverse`
    argument to the `call` method, which computes the projection by the transpose
    of the token embedding.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个添加到我们的`PositionalEmbedding`中很简单；我们只需在`call`方法中添加一个`reverse`参数，该方法通过标记嵌入矩阵的转置来计算投影。
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[Listing 16.5](#listing-16-5): A positional embedding layer that can reverse
    a text embedding'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表16.5](#listing-16-5)：一个可以反转文本嵌入的位置嵌入层'
- en: Let’s build our model. We will stack eight decoder layers into a single “mini”
    GPT model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建我们的模型。我们将把八个解码器层堆叠成一个单一的“迷你”GPT模型。
- en: We will also turn on a Keras setting called *mixed precision* to speed up training.
    This will allow Keras to run some of the model’s computations much faster by sacrificing
    some numerical fidelity. For now, this will remain a little mysterious, but a
    full explanation is waiting in chapter 18.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将开启一个名为*mixed precision*的Keras设置来加速训练。这将允许Keras通过牺牲一些数值精度来运行模型的一些计算更快。目前，这可能会有些神秘，但完整的解释将在第18章中提供。
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Listing 16.6](#listing-16-6): Creating a mini-GPT functional model'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表16.6](#listing-16-6)：创建一个迷你GPT功能模型'
- en: This model has 41 million parameters, which is large for models in this book
    but quite small compared to most LLMs today, which range from a couple of billion
    to trillions of parameters.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型有4100万个参数，对于本书中的模型来说算是大的，但与今天大多数LLM（大型语言模型）相比，它们的参数量从几十亿到万亿不等，就显得相当小了。
- en: Pretraining the model
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预训练模型
- en: Training a large Transformer is famously finicky — the model is sensitive to
    initializations of parameters and choice of optimizer. When many Transformer layers
    are stacked, it is easy to suffer from exploding gradients, where parameters update
    too quickly and our loss function does not converge. A trick that works well is
    to linearly ease into a full learning rate over a number of warmup steps, so our
    initial updates to our model parameters are small. This is easy to implement in
    Keras with a `LearningRateSchedule`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个大型Transformer模型众所周知是相当挑剔的——模型对参数的初始化和优化器的选择很敏感。当堆叠了许多Transformer层时，很容易出现梯度爆炸的问题，即参数更新得太快，我们的损失函数无法收敛。一个有效的方法是在多个预热步骤中线性地逐渐增加到完整的学习率，这样我们模型参数的初始更新就很小。在Keras中使用`LearningRateSchedule`来实现这一点很容易。
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[Listing 16.7](#listing-16-7): Defining a custom learning rate schedule'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表16.7](#listing-16-7)：定义自定义学习率计划'
- en: 'We can plot our learning rate over time to make sure it is what we expect (figure
    16.2):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将我们的学习率随时间的变化绘制出来，以确保它符合我们的预期（图16.2）：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/e3511de77b453c5e2049712d06e82826.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/e3511de77b453c5e2049712d06e82826.png)'
- en: '[Figure 16.2](#figure-16-2): Warmup makes our updates to model parameters smaller
    at the beginning of training and can help with stability.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16.2](#figure-16-2)：预热使我们在训练开始时对模型参数的更新更小，并有助于稳定性。'
- en: We will train our model using one pass over our 1 billion tokens, split across
    eight epochs so we can occasionally check our validation set loss and accuracy.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一次遍历我们的100亿个标记来训练我们的模型，这些标记分布在八个时期中，这样我们就可以偶尔检查我们的验证集损失和准确率。
- en: We are training a miniature version of GPT, using 3× fewer parameters than GPT-1
    and 100× fewer overall training steps. But despite this being two orders of magnitude
    cheaper to train than the smallest GPT model, this call to `fit()` will be the
    most computationally expensive training run in the entire book. If you are running
    the code as you read, set things off and take a breather!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在训练一个GPT的微型版本，比GPT-1少3倍参数，总体训练步骤少100倍。但尽管这比最小的GPT模型便宜两个数量级来训练，这个`fit()`调用将是整本书中最昂贵的训练运行。如果你在阅读代码时运行，请启动它并休息一下！
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Listing 16.8](#listing-16-8): Training the mini-GPT model'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表16.8](#listing-16-8)：训练迷你GPT模型'
- en: After training, our model can predict the next token in a sequence about 36%
    of the time on our validation set, though such a metric is just a crude heuristic.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们的模型在验证集上预测序列中的下一个标记大约有36%的时间，尽管这样的指标只是一个粗略的经验法则。
- en: Note that our model is undertrained. Our validation loss will continue to tick
    down after each epoch, which is unsurprising given that we used a hundred times
    fewer training steps than GPT-1\. Training for longer would be a great idea, but
    we would need both time and money to pay for compute.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们的模型训练不足。在每个时期之后，我们的验证损失将继续下降，这在使用比GPT-1少一百倍的训练步骤的情况下并不令人惊讶。更长时间的训练将是一个很好的主意，但我们需要时间和金钱来支付计算费用。
- en: Let’s play around with our mini-GPT model.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们玩一玩我们的迷你GPT模型。
- en: Generative decoding
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成式解码
- en: To sample some output from our model, we can follow the approach we used to
    generate Shakespeare or Spanish translations in chapter 15\. We feed a prompt
    of fixed tokens into the model. For each position in the input sequence, the model
    outputs a probability distribution over the entire vocabulary for the next token.
    By selecting the most likely next token at the last location, adding it to our
    sequence, and then repeating this process, we are able to generate a new sequence,
    one token at a time.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从我们的模型中采样一些输出，我们可以遵循第15章中生成莎士比亚或西班牙语翻译的方法。我们将一个固定标记的提示输入到模型中。对于输入序列中的每个位置，模型输出整个词汇表上下一个标记的概率分布。通过在最后一个位置选择最可能的下一个标记，将其添加到我们的序列中，然后重复此过程，我们能够逐个生成新的序列。
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[Listing 16.9](#listing-16-9): A simple generation function for the mini-GPT
    model'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表16.9](#listing-16-9)：迷你GPT模型的一个简单生成函数'
- en: 'Let’s try this out with a text prompt:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个文本提示来试一试：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The first thing you will notice when running this is that it takes minutes to
    complete. That’s a bit puzzling. We predicted about 200,000 tokens a second on
    our reference hardware during training. The generative loop may add time, but
    a minute delay is much too slow. What happened? The biggest reason for our slowness,
    at least on the Jax and TensorFlow backends, is that we are running an uncompiled
    computation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这个程序时，你首先会注意到它需要几分钟才能完成。这有点令人困惑。我们在训练期间预测，在参考硬件上每秒大约有200,000个标记。生成循环可能会增加时间，但一分钟延迟太慢了。发生了什么？我们速度慢的最大原因，至少在Jax和TensorFlow后端，是我们正在运行一个未编译的计算。
- en: Every time you run `fit()` or `predict()`, Keras compiles the computation that
    runs on each batch of data. All the `keras.ops` used will be lifted out of Python
    and heavily optimized by the backend framework. It’s slow for one batch but massively
    faster for each subsequent call. However, when we directly call the model as we
    did previously, the backend framework will need to run the forward pass live and
    unoptimized at each step.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你运行`fit()`或`predict()`，Keras都会编译在每批数据上运行的计算。所有使用的`keras.ops`都会从Python中提取出来，并由后端框架进行大量优化。对于单个批次来说很慢，但对于后续的每次调用来说则快得多。然而，当我们像之前那样直接调用模型时，后端框架需要在每个步骤上实时且未经优化的运行前向传递。
- en: The easy solution here is to lean on `predict()`. With `predict()`, Keras will
    handle compilation for us, but there is one important gotcha to watch out for.
    When TensorFlow or Jax compiles a function, it will do so for a specific input
    shape. With a known shape, the backend can optimize for particular hardware, knowing
    exactly how many individual processor instructions make up a tensor operation.
    But in our generation function, we call our model with a sequence that changes
    shape after each prediction. This would trigger recompilation each time we call
    `predict()`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的简单解决方案是依赖`predict()`。使用`predict()`，Keras会为我们处理编译，但有一个重要的陷阱需要注意。当TensorFlow或Jax编译一个函数时，它将针对特定的输入形状进行编译。有了已知的形状，后端可以针对特定硬件进行优化，确切地知道组成张量操作的各个处理器指令的数量。但在我们的生成函数中，我们在每次预测后调用模型时，序列的形状都会改变。这会在每次调用`predict()`时触发重新编译。
- en: Instead, we can avoid recompiling the `predict()` function if we pad our input
    so that our sequence is always the same length. Let’s try that out.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果我们通过填充输入使得我们的序列始终具有相同的长度，我们可以避免重新编译`predict()`函数。让我们试试看。
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[Listing 16.10](#listing-16-10): A compiled generation function for the mini-GPT
    model'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表16.10](#listing-16-10)：mini-GPT模型的编译生成函数'
- en: 'Let’s see how fast this new function is:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个新函数有多快：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Our generation call went from minutes to less than a second with compilation.
    That is quite an improvement.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过编译，我们的生成调用从几分钟缩短到不到一秒。这是一个相当大的改进。
- en: Sampling strategies
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 抽样策略
- en: Another obvious problem with our generative output is that our model often repeats
    itself. On our particular training run, the model repeats the group of words “get
    a sense of what you are doing” over and over.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成输出中的另一个明显问题是我们的模型经常重复自己。在我们的特定训练运行中，模型反复重复“get a sense of what you are doing”这个单词组。
- en: This isn’t so much a bug as it’s a direct consequence of our training objective.
    Our model is trying to predict the most likely next token in a sequence across
    about a billion words on many, many topics. If there’s no obvious choice for where
    a sequence of text should head next, an effective strategy is to guess common
    words or repeated patterns of words. Unsurprisingly, our model learns to do this
    during training almost immediately. If you were to stop training our model very
    early on, it would likely generate the word `"the"` incessantly, as `"the"` is
    the most common word in the English language.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这与其说是一个错误，不如说是我们训练目标的直接后果。我们的模型试图预测在约十亿个单词和许多主题上的序列中最可能出现的下一个标记。如果没有明显的选择来决定文本序列接下来应该走向何方，一个有效的策略是猜测常见的单词或单词的重复模式。不出所料，我们的模型在训练过程中几乎立即学会了这样做。如果你在训练我们的模型非常早的时候停止，它可能会不断地生成单词“the”，因为“the”是英语中最常见的单词。
- en: During our generative loop, we have always chosen the most likely predicted
    token in our model’s output. But our output is not just a single predicted toke;
    it is a probability distribution across all 32,000 tokens in our vocabulary.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的生成循环中，我们总是选择模型输出中最可能预测的标记。但我们的输出不仅仅是一个预测的标记；它是在我们的词汇表中的32,000个标记上的概率分布。
- en: Using the most likely output at each generation step is called *greedy search*.
    It’s the most straightforward approach to using model predictions, but it is hardly
    the only one. If we instead add some randomness to the process, we can explore
    the probability distribution learned by the model more broadly. This can keep
    us from getting stuck in loops of high-probability token sequences.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个生成步骤使用最可能的输出被称为*贪婪搜索*。这是使用模型预测的最直接方法，但它绝不是唯一的方法。如果我们向过程中添加一些随机性，我们可以更广泛地探索模型学习到的概率分布。这可以防止我们陷入高概率标记序列的循环中。
- en: 'Let’s try this out. We can start by refactoring our generation function so
    that we can pass a function that maps from a model’s predictions to a choice for
    the next token. We will call this our *sampling strategy*:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一下。我们可以通过重构我们的生成函数，使其能够传递一个将模型的预测映射到下一个标记选择的函数。我们将称之为我们的*采样策略*：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now we can write our greedy search as a simple function we pass to `compiled_generate()`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将我们的贪婪搜索编写为一个简单的函数，我们将其传递给`compiled_generate()`：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The Transformer outputs define a categorical distribution where each token
    has a certain probability of being output at each time step. Instead of just choosing
    the most likely token, we could sample this distribution directly. `keras.random.categorical()`
    will pass our predictions through a softmax function to get a probability distribution
    and then randomly sample it. Let’s try it out:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的输出定义了一个分类分布，其中每个标记在每个时间步都有一定的概率被输出。我们不仅可以选择最有可能的标记，还可以直接采样这个分布。`keras.random.categorical()`将我们的预测通过softmax函数，得到一个概率分布，然后随机采样。让我们试一下：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Our outputs are more diverse, and the model no longer gets stuck in loops. But
    our sampling is now exploring too much; the output jumps around wildly without
    any continuity.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输出更加多样化，模型也不再陷入循环。但我们的采样现在探索得太多；输出跳跃不定，没有任何连续性。
- en: You’ll notice we added a parameter called `temperature`. We can use this to
    sharpen or widen our probability distribution so our sampling explores our distribution
    less or more.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们添加了一个名为`temperature`的参数。我们可以使用这个参数来锐化或拓宽我们的概率分布，以便我们的采样探索分布的程度更少或更多。
- en: 'If we pass a low temperature, we will make all logits larger before the softmax
    function, which makes our most likely output even more likely. If we pass a high
    temperature, our logits will be smaller before the softmax, and our probability
    distribution will be more spread out. Let’s try this out a few times to see how
    this affects our sampling:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们传递一个低温度，我们将在softmax函数之前使所有logits更大，这使得最有可能的输出更加可能。如果我们传递一个高温度，我们的logits将在softmax之前更小，我们的概率分布将更加分散。让我们试几次，看看这对我们的采样有什么影响：
- en: '[PRE22]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: At a high temperature, our outputs no longer resemble English, settling on seemingly
    random tokens. At a low temperature, our model behavior starts to resemble greedy
    search, repeating certain patterns of text over and over.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在高温下，我们的输出不再像英语，而是停留在看似随机的标记上。在低温下，我们的模型行为开始类似于贪婪搜索，反复重复某些文本模式。
- en: Another popular technique for shaping our distribution is restricting our sampling
    to a set of the most likely tokens. This is called *top-k sampling*, where K is
    the number of candidates you should explore. Figure 16.3 shows how top-k sampling
    strikes a middle ground between greedy and random approaches.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种塑造我们分布的流行技术是将采样限制在最有可能的标记集合中。这被称为*top-k采样*，其中K是你应该探索的候选数量。图16.3展示了top-k采样如何在贪婪和随机方法之间取得平衡。
- en: '![](../Images/c16fb6f0a5973b18e2675b41d0a59b63.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/c16fb6f0a5973b18e2675b41d0a59b63.png)'
- en: '[Figure 16.3](#figure-16-3): Greedy, top-k, and random sampling strategies
    shown on the same probability distribution'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16.3](#figure-16-3)：在相同的概率分布上展示了贪婪、top-k和随机采样策略'
- en: 'Let’s try this out in code. We can use `keras.ops.top_k` to find the top K
    elements of an array:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在代码中尝试一下。我们可以使用`keras.ops.top_k`来找到数组的前K个元素：
- en: '[PRE23]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can try a few different variations of top-k to see how it affects sampling:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试几种不同的top-k变体，看看它如何影响采样：
- en: '[PRE24]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Passing a top-k cutoff is different than temperature sampling. Passing a low
    temperature makes likely tokens more likely, but it does not rule any token out.
    top-k sampling zeros out the probability of anything outside the K candidates.
    You can combine the two, for example, sampling the top five candidates with a
    temperature of 0.5:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 传递一个top-k截止值与温度采样不同。传递一个低温度会使可能的标记更有可能，但它不会排除任何标记。top-k采样将K候选之外的所有标记的概率置零。你可以将两者结合起来，例如，用0.5的温度采样前五个候选：
- en: '[PRE25]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: A sampling strategy is an important control when generating text, and there
    are many more approaches. For example, beam search is a technique that heuristically
    explores multiple chains of predicted tokens by keeping a fixed number of “beams”
    (different chains of predicted tokens) to explore at each timestep.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 样本策略是生成文本时的重要控制手段，有更多的方法。例如，beam search 是一种通过在每个时间步长保持固定数量的“光束”（不同的预测标记链）来启发式地探索多个预测标记链的技术。
- en: With top-k sampling, our model generates something closer to plausible English
    text, but there is little apparent utility to such output. This fits with the
    results of GPT-1\. For the initial GPT paper, the generated output was more of
    a curiosity, and state-of-the-art results were only achieved by fine-tuning classification
    models. Our mini-GPT is far less trained than GPT-1.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 top-k 样本采样，我们的模型生成的文本更接近合理的英语文本，但这种输出的实际效用很小。这与 GPT-1 的结果相符。对于最初的 GPT 论文，生成的输出更多的是一种好奇心，而最先进的结果是通过微调分类模型实现的。我们的
    mini-GPT 的训练程度远低于 GPT-1。
- en: To reach the scale of generative LLMs today, we’d need to increase our parameter
    count by at least 100× and our train step count by at least 1,000×. If we did,
    we would see the same leaps in quality observed by OpenAI with GPT. And we could
    do it! The training recipe we used previously is the exact blueprint used by everyone
    training LLMs today. The only missing pieces are a very large compute budget and
    some tricks for training across multiple machines that we will cover in chapter
    18.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要达到今天生成式大型语言模型（LLM）的规模，我们需要将参数数量至少增加 100 倍，训练步数至少增加 1,000 倍。如果我们这样做，我们将看到与 OpenAI
    在 GPT 中观察到的相同的质量飞跃。而且我们能够做到！我们之前使用的训练方案是今天所有训练 LLM 的人使用的确切蓝图。唯一缺少的部分是巨大的计算预算和跨多台机器训练的一些技巧，这些内容将在第
    18 章中介绍。
- en: For a more practical approach, we will transition to using a pretrained model.
    This will allow us to explore the behavior of an LLM at today’s scale.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更实用的方法，我们将过渡到使用预训练模型。这将使我们能够探索今天规模下 LLM 的行为。
- en: Using a pretrained LLM
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练的 LLM
- en: Now that we’ve trained a mini-language model from scratch, let’s try using a
    billion-parameter pretrained model and see what it can do. Given how prohibitively
    expensive pretraining a Transformer can be, most of the industry has centered
    around using pretrained models developed by a relatively short list of companies.
    This is not purely a cost concern but also an environmental one — generative model
    training is now making up a large percentage of the total data center power consumption
    of large tech companies.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从头开始训练了一个 mini 语言模型，让我们尝试使用一个十亿参数的预训练模型，看看它能做什么。鉴于预训练 Transformer 的成本过高，大多数行业都集中在使用由相对少数公司开发的预训练模型上。这不仅仅是一个成本问题，也是一个环境问题——生成模型训练现在占了大科技公司数据中心总电力消耗的很大一部分。
- en: Meta published some environmental data on Llama 2, an LLM it published in 2023\.
    It’s a good bit smaller than GPT-3, but it needed an estimated 1.3 million kilowatt
    hours of electricity to train — the daily power usage of about 45,000 American
    households. If every organization using an LLM ran pretraining themselves, the
    scale of energy use would be a noticeable percentage of global energy consumption.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Meta 在 2023 年发布了 Llama 2 的环境数据，这是它发布的一个 LLM。它比 GPT-3 小得多，但训练它需要估计 130 万千瓦时的电力——相当于大约
    45,000 美国家庭的日用电量。如果每个使用 LLM 的组织都自行进行预训练，那么能源使用的规模将占全球能源消耗的一个明显的百分比。
- en: Let’s play around with a pretrained generative model from Google called Gemma.
    We will use the third version of the Gemma model, which was released to the public
    in 2025\. To keep the examples in this book accessible, we will use the smallest
    variation of Gemma available, which clocks in at almost exactly 1 billion parameters.
    This “small” model was trained on roughly 2 trillion tokens of pretraining data
    — 2,000 times more tokens than the mini-GPT we just trained!
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 Google 提供的一个预训练生成模型 Gemma 来玩玩。我们将使用 Gemma 的第三个版本，该版本于 2025 年公开发布。为了使本书中的示例易于理解，我们将使用可用的
    Gemma 最小变体，其参数数量几乎正好是 10 亿。这个“小型”模型在约 2 万亿个预训练数据标记上进行了训练——比我们刚刚训练的 mini-GPT 多
    2,000 倍！
- en: Text generation with the Gemma model
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Gemma 模型进行文本生成
- en: To load this pretrained model, we can use KerasHub, as we have done in previous
    chapters.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载这个预训练模型，我们可以使用 KerasHub，就像我们在前面的章节中做的那样。
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[Listing 16.11](#listing-16-11): Instantiating a pretrained LLM with KerasHub'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 16.11](#listing-16-11)：使用 KerasHub 实例化预训练的 LLM'
- en: '`CausalLM` is another example of the high-level task API, much like the `ImageClassifier`
    and `ImageSegmenter` tasks we used earlier in the book. The `CausalLM` task will
    combine a tokenizer and correctly initialized architecture into a single Keras
    model. KerasHub will load the Gemma weights into a correctly initialized architecture
    and load a matching tokenizer for the pretrained weights.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`CausalLM`是高级任务API的另一个例子，就像我们在书中早期使用的`ImageClassifier`和`ImageSegmenter`任务一样。`CausalLM`任务将结合一个分词器和正确初始化的架构到一个单一的Keras模型中。KerasHub将Gemma权重加载到正确初始化的架构中，并为预训练权重加载一个匹配的分词器。'
- en: 'Let’s take a look at the Gemma model summary:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看Gemma模型的摘要：
- en: '[PRE27]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Rather than implementing a generation routine ourselves, we can simplify our
    lives by using the `generate()` function that comes as part of the `CausalLM`
    class. This `generate()` function can be compiled with different sampling strategies,
    as we explored in the previous section:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不必亲自实现生成流程，可以通过使用`CausalLM`类中提供的`generate()`函数来简化我们的生活。这个`generate()`函数可以与不同的采样策略一起编译，正如我们在上一节中探讨的那样：
- en: '[PRE28]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We can notice a few things right off the bat. First, the output is much more
    coherent than our mini-GPT model. It would be hard to distinguish this text from
    much of the training data in the C4 dataset. Second, the output is still not that
    useful. The model will generate vaguely plausible text, but what you could do
    with it is unclear.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即注意到几点。首先，输出比我们的迷你GPT模型要连贯得多。很难将这段文本与C4数据集中的大量训练数据区分开来。其次，输出仍然不太有用。模型将生成模糊合理的文本，但你可以用它做什么并不清楚。
- en: As we saw with the mini-GPT example, this is not so much a bug as a consequence
    of our pretraining objective. The Gemma model was trained with the same “guess
    the next word” objective we used for mini-GPT, which means it’s effectively a
    fancy autocomplete for the internet. It will just keep rattling off the most probable
    word in its single sequence as if your prompt was a snippet of text found in a
    random document on the web.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在迷你GPT示例中看到的那样，这与其说是一个错误，不如说是我们预训练目标的结果。Gemma模型是用与我们为迷你GPT使用的相同“猜测下一个单词”的目标进行训练的，这意味着它实际上是一个互联网上的高级自动完成功能。它将不断地连续发出最可能的单词，就像你的提示是网络上随机文档中的一段文本一样。
- en: 'One way to change our output is to prompt the model with a longer input that
    makes it obvious which type of output we are looking for. For example, if we prompt
    the Gemma model with the beginning two sentences of a brownie recipe, we get more
    helpful output:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 改变我们输出的一个方法是通过提供一个更长的输入来提示模型，使其明显表明我们正在寻找哪种类型的输出。例如，如果我们用布朗尼食谱的前两句话提示Gemma模型，我们会得到更有帮助的输出：
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Though it’s tempting when working with a model that can “talk” to imagine it
    interpreting our prompt in some sort of human, conversational way, nothing of
    the sort is going on here. We have just constructed a prompt for which an actual
    brownie recipe is a more likely continuation than mimicking someone posting on
    a forum asking for baking help.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当与能够“交谈”的模型一起工作时，想象它以某种人类对话的方式解释我们的提示是很诱人的，但这里并没有发生这样的事情。我们只是构建了一个提示，其中实际布朗尼食谱的延续性比模仿在论坛上寻求烘焙帮助的人的帖子更有可能。
- en: You can go much further in constructing prompts. You might prompt a model with
    some natural language instructions of the role it is supposed to fill, for example,
    `"You are a large language model that gives short, helpful answers to people's
    questions."` Or you might feed the model a prompt containing a long list of harmful
    topics that should not be included in any generated responses.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建提示方面，你可以走得更远。你可能会用一些自然语言指令提示模型它应该扮演的角色，例如，“你是一个大型语言模型，为人们提供简短、有用的答案。”或者你可能给模型提供一个包含长列表的有害主题的提示，这些主题不应包含在任何生成的响应中。
- en: If this all sounds a bit hand-wavy and hard to control, that’s a good assessment.
    Attempting to visit different parts of a model’s distribution through prompting
    is often useful, but predicting how a model will respond to a given prompt is
    very difficult.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这一切听起来有些模糊不清且难以控制，这是一个很好的评估。尝试通过提示来访问模型分布的不同部分通常是有用的，但预测模型对给定提示的反应是非常困难的。
- en: 'Another well-documented problem faced by LLMs is hallucinations. A model will
    always say something — there is always a most-likely next token to a given sequence.
    Finding locations in our LLM distribution that have no grounding in actual fact
    is easy:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs面临的一个另一个广泛记录的问题是幻觉。模型总是会说出一些话——对于给定的序列，总有一个最可能的下一个标记。在我们的LLM分布中找到没有实际事实依据的位置是很容易的：
- en: '[PRE30]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Of course, this is utter nonsense, but the model could not find a more likely
    way to complete this prompt.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这完全是胡说八道，但模型找不到更可能的方式来完成这个提示。
- en: Hallucinations and uncontrollable output are fundamental problems with language
    models. If there is a silver bullet, we have yet to find it. However, one approach
    that helps immensely is to further fine-tune a model with examples of the specific
    types of generative outputs you would like.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉和不可控的输出是语言模型的基本问题。如果有一个银弹，我们还没有找到。然而，一种非常有帮助的方法是使用你希望生成的特定类型的生成输出示例进一步微调模型。
- en: In the specific case of wanting to build a chatbot that can follow instructions,
    this type of training is called *instruction fine-tuning*. Let’s try some instruction
    fine-tuning with Gemma to make it a lot more useful as a conversation partner.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在想要构建一个能够遵循指令的聊天机器人的具体情况下，这种训练被称为*指令微调*。让我们尝试使用Gemma进行一些指令微调，使其作为一个对话伙伴变得更加有用。
- en: Instruction fine-tuning
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指令微调
- en: Instruction fine-tuning involves feeding the model input/output pairs — a user
    instruction followed by a model response. We combine these into a single sequence
    that becomes new training data for the model. To make it clear during training
    when an instruction or response ends, we can add special markers like `"[instruction]"`
    and `"[response]"` directly to the combined sequence. The precise markup will
    not matter much as long as it is consistent.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 指令微调涉及向模型提供输入/输出对——一个用户指令后面跟着一个模型响应。我们将这些组合成一个单一的序列，成为模型的新训练数据。为了在训练过程中清楚地标记指令或响应的结束，我们可以直接将特殊标记如`"[instruction]"`和`"[response]"`添加到组合序列中。只要保持一致，精确的标记并不重要。
- en: We can use the combined sequence as regular training data, with the same “guess
    the next word” loss we used to pretrain an LLM. By doing further training with
    examples containing desired responses, we are essentially bending the model’s
    output in the direction we want. We won’t be learning a latent space for language
    here; that’s already been done over trillions of tokens of pretraining. We are
    simply nudging the learned representation a bit to control the tone and content
    of the output.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将组合序列作为常规训练数据使用，使用与我们在预训练LLM时使用的相同的“猜测下一个单词”损失。通过使用包含所需响应的示例进行进一步训练，我们实际上是在我们想要的方向上弯曲模型的输出。我们在这里不会学习语言的一个潜在空间；这已经在数十亿个预训练的标记上完成了。我们只是在稍微推动学习到的表示，以控制输出的语气和内容。
- en: To begin, we will need a dataset of instruction-response pairs. Training chatbots
    is a hot topic, so there are many datasets made specifically for this purpose.
    We will use a dataset made public by the company Databricks. Employees contributed
    to a dataset of 15,000 instructions and handwritten responses. Let’s download
    it and join the data into a single sequence.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一个指令-响应对的数据集。训练聊天机器人是一个热门话题，因此有许多专门为此目的制作的数据集。我们将使用由Databricks公司公开的数据集。员工们贡献了一个包含15,000条指令和手写响应的数据集。让我们下载它并将数据合并成一个单一的序列。
- en: '[PRE31]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[Listing 16.12](#listing-16-12): Loading an instruction fine-tuning dataset'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表16.12](#listing-16-12)：加载指令微调数据集'
- en: Note that some examples have additional context — textual information related
    to the instruction. To keep things simple for now, we will discard those examples.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一些示例有额外的上下文——与指令相关的文本信息。为了保持简单，我们现在将丢弃这些示例。
- en: 'Let’s take a look at a single element in our dataset:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们数据集中单个元素：
- en: '[PRE32]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Our prompt template gives our examples a predictable structure. Although Gemma
    is not a sequence-to-sequence model like our English-to-Spanish translator, we
    can still use it in a sequence-to-sequence setting by training on prompts like
    this and only generating the output after the `"[response]"` marker.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的提示模板为我们提供的示例提供了一个可预测的结构。尽管Gemma不是一个像我们的英语到西班牙语翻译器那样的序列到序列模型，但我们可以通过在类似这样的提示上进行训练，并在`"[response]"`标记之后才生成输出，来在序列到序列设置中使用它。
- en: 'Let’s make a `tf.data.Dataset` and split some validation data:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个`tf.data.Dataset`并分割一些验证数据：
- en: '[PRE33]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The `CausalLM` we loaded from the KerasHub library is a high-level object for
    end-to-end causal language modeling. It wraps two objects: a `preprocessor` layer,
    which preprocesses text input, and a `backbone` model, which contains the numerics
    of the model forward pass.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从KerasHub库加载的`CausalLM`是一个用于端到端因果语言模型的高级对象。它封装了两个对象：一个`preprocessor`层，用于预处理文本输入，以及一个`backbone`模型，它包含模型前向传递的数值。
- en: 'Preprocessing is included by default in high-level Keras functions like `fit()`
    and `predict()`. But let’s run our preprocessing on a single batch so we can better
    see what it is doing:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理默认包含在高级Keras函数如`fit()`和`predict()`中。但让我们在一个单独的批次上运行我们的预处理，这样我们可以更好地看到它在做什么：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The preprocessor layer will pad all inputs to a fixed length and compute a padding
    mask to track which token ID inputs are just padded zeros. The `sample_weight`
    tensor allows us to only compute a loss value for our response tokens. We don’t
    really care about the loss for the user prompt; it is fixed, and we definitely
    don’t want to compute the loss for the zero padding we just added.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理层将所有输入填充到固定长度，并计算一个填充掩码来跟踪哪些标记ID输入只是填充的零。`sample_weight`张量使我们能够只为我们的响应标记计算损失值。我们并不关心用户提示的损失；它是固定的，我们肯定不希望计算我们刚刚添加的零填充的损失。
- en: 'If we print a snippet of our token IDs and labels, we can see that this is
    the regular language model setup, where each label is the next token value:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印出我们的标记ID和标签的片段，我们可以看到这是一个常规语言模型设置，其中每个标签是下一个标记值：
- en: '[PRE35]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Low-Rank Adaptation (LoRA)
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 低秩自适应（LoRA）
- en: If we ran `fit()` right now on a Colab GPU with 16 GB of device memory, we would
    quickly trigger an out of memory error. But we’ve already loaded the model and
    run generation, so why would we run out of memory now?
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在在一个具有16 GB设备内存的Colab GPU上运行`fit()`，我们很快就会触发内存不足错误。但我们已经加载了模型并运行了生成，那么为什么现在会内存不足呢？
- en: Our 1-billion-parameter model takes up about 3.7 GB of memory. You can see it
    in our previous model summary. The `Adam` optimizer we have been using will need
    to track three extra floating-point numbers for *each* parameter — the actual
    gradients, a velocity value, and a momentum value. All told, it comes out to 15
    GB just for the weights and optimizer state. We also need a few gigabytes of memory
    to keep track of intermediate values in the forward pass of the model, but we
    have none left to spare. Running `fit()` would crash on the first train step.
    This is a common problem when training LLMs. Because these models have large parameter
    counts, the throughput of your GPUs and CPUs is a secondary concern to fitting
    the model on accelerator memory.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的10亿参数模型大约占用3.7 GB的内存。您可以在我们之前的模型摘要中看到这一点。我们一直在使用的`Adam`优化器需要为每个参数跟踪三个额外的浮点数——实际的梯度、一个速度值和一个动量值。总的来说，仅权重和优化器状态就需要15
    GB。我们还需要几GB的内存来跟踪模型前向传递中的中间值，但我们已经没有多余的内存了。运行`fit()`会在第一次训练步骤时崩溃。这是训练LLM时常见的问题。因为这些模型具有大量的参数数量，所以GPU和CPU的吞吐量在加速器内存上拟合模型时是次要的。
- en: We’ve seen earlier in this book how we can freeze certain parts of a model during
    fine-tuning. What we did not mention is that this will save a lot of memory! We
    do not need to track any optimizer variables for frozen parameters — they will
    never update. This allows us to save a lot of space on an accelerator.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中之前已经看到，我们如何在微调过程中冻结模型的一部分。我们没有提到的是，这将节省大量的内存！我们不需要跟踪任何冻结参数的优化器变量——它们永远不会更新。这使我们能够在加速器上节省大量的空间。
- en: Researchers have experimented extensively with freezing different parameters
    in a Transformer model during fine-tuning, and it turns out, perhaps intuitively,
    that the most important weights to leave unfrozen are in the attention mechanism.
    But our attention layers still have hundreds of millions of parameters. Can we
    do even better?
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员已经广泛地实验了在微调期间冻结Transformer模型中的不同参数，结果发现，也许直观地，最重要的未冻结权重是在注意力机制中。但我们的注意力层仍然有数亿个参数。我们能否做得更好？
- en: 'In 2021, researchers at Microsoft proposed a technique called LoRA, short for
    *Low-Rank Adaptation of Large Language Models*, specifically to solve this memory
    issue^([[4]](#footnote-4)). To explain it, let’s imagine a simple linear projection
    layer:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在2021年，微软的研究人员提出了一种名为LoRA的技术，即*大型语言模型的低秩自适应*，专门用来解决这个内存问题^([[4]](#footnote-4))。为了解释它，让我们想象一个简单的线性投影层：
- en: '[PRE36]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The LoRA paper proposes freezing the `kernel` matrix and adding a new “low
    rank” decomposition of the kernel projection. This decomposition has two new projection
    matrices, `alpha` and `beta`, which project to and from an inner `rank`. Let’s
    take a look:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA论文建议冻结`kernel`矩阵，并添加一个新的“低秩”核投影分解。这个分解有两个新的投影矩阵，`alpha`和`beta`，它们将投影到和从内部`rank`。
- en: '[PRE37]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: If our `kernel` is shape 2048 × 2048, that is 4,194,304 frozen parameters. But
    if we keep the `rank` low, say, 8, we will have only 32,768 parameters for the
    low-rank decomposition. This update will not have the same expressive power as
    the original kernel; at the narrow middle point, the entire update must be represented
    as eight floats. But during LLM fine-tuning, you no longer need the expressive
    power you needed during pretraining (figure 16.4).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的`kernel`形状是2048 × 2048，那么就是4,194,304个冻结参数。但如果我们将`rank`保持得较低，比如说8，那么低秩分解将只有32,768个参数。这次更新将不具有原始核相同的表达能力；在狭窄的中间点，整个更新必须表示为八个浮点数。但在LLM微调期间，你不再需要像预训练期间那样的表达能力（图16.4）。
- en: '![](../Images/87c87a4dbb8282b1485ee08bc057c02d.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87c87a4dbb8282b1485ee08bc057c02d.png)'
- en: '[Figure 16.4](#figure-16-4): The low-rank kernel decomposition contains far
    fewer parameters than the kernel itself.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16.4](#figure-16-4)：低秩核分解包含的参数远少于核本身。'
- en: The LoRA authors suggest freezing the entire Transformer and adding LoRA weights
    to only the query and key projections in the attention layer. Let’s try that out.
    KerasHub models have a built-in method for LoRA training.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA的作者建议冻结整个Transformer，并将LoRA权重仅添加到注意力层的查询和键投影中。让我们试试这个方法。KerasHub模型内置了LoRA训练的方法。
- en: '[PRE38]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[Listing 16.13](#listing-16-13): Enabling LoRA training for a KerasHub model'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表16.13](#listing-16-13)：为KerasHub模型启用LoRA训练'
- en: 'Let’s look at our model summary again:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次查看我们的模型摘要：
- en: '[PRE39]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Although our model parameters still occupy 3.7 GB of space, our trainable parameters
    now use only 5 MB of data — a thousandfold decrease! This can take our optimizer
    state from many gigabytes to just megabytes on the GPU (figure 16.5).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的模型参数仍然占用3.7 GB的空间，但我们的可训练参数现在只使用5 MB的数据——减少了千倍！这可以将我们的优化器状态从多个GB减少到GPU上的仅几MB（图16.5）。
- en: '![](../Images/586115dfbef6c1652bf5bf8a33c8f45d.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/586115dfbef6c1652bf5bf8a33c8f45d.png)'
- en: '[Figure 16.5](#figure-16-5): LoRA greatly reduces the memory we need for gradients
    and optimizer states.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16.5](#figure-16-5)：LoRA大大减少了我们需要的梯度优化器状态内存。'
- en: With this optimization in place, we are at last ready to instruction-tune our
    Gemma model. Let’s give it a go.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个优化到位后，我们终于准备好对Gemma模型进行指令微调了。让我们试试。
- en: '[PRE40]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[Listing 16.14](#listing-16-14): Fine-tuning a pretrained LLM'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表16.14](#listing-16-14)：微调预训练的LLM'
- en: After training, we get to 55% accuracy when guessing the next word in our model’s
    response. That’s a huge jump from the 35% accuracy of our mini-GPT model. This
    shows the power of a larger model and more extensive pretraining.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们在猜测模型响应中的下一个单词时达到了55%的准确率。这比我们迷你GPT模型的35%准确率有巨大的提升。这显示了更大模型和更广泛预训练的威力。
- en: 'Did our fine-tuning make our model better at following directions? Let’s give
    it a try:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的微调是否使我们的模型在遵循指令方面变得更好？让我们试试：
- en: '[PRE41]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Much better. Our model will now respond to questions, instead of trying to simply
    carry on the thought of the prompt text.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 好得多。我们的模型现在将回答问题，而不是试图简单地延续提示文本的思想。
- en: Have we solved the hallucination problem?
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否解决了幻觉问题？
- en: '[PRE42]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Not at all. However, we could still use instruction tuning to make some inroads
    here. A common technique is to train the model on a lot of instruction/response
    pairs where the desired response is `"I don't know"` or `"As a language model,
    I cannot help you with that"`. This can train the model to avoid attempting to
    answer specific topics where it would often give poor-quality results.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 完全没有。然而，我们仍然可以使用指令微调在这里取得一些进展。一种常见的技术是在大量指令/响应对上训练模型，其中期望的响应是“我不知道”或“作为一个语言模型，我无法帮助你”。这可以训练模型避免尝试回答它通常会给出低质量结果的特定主题。
- en: Going further with LLMs
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在LLM上更进一步
- en: We have now trained a GPT model from scratch and fine-tuned a language model
    into our very own chatbot. However, we are just scratching the surface of LLM
    research today. In this section, we will cover a non-exhaustive list of extensions
    and improvements to the basic “autocomplete the internet” language modeling setup.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在从头开始训练了一个GPT模型，并将语言模型微调到了我们自己的聊天机器人中。然而，我们今天只是触及了LLM研究的表面。在本节中，我们将介绍一些对基本“自动完成互联网”语言建模设置的扩展和改进，这些改进并不全面。
- en: Reinforcement Learning with Human Feedback (RLHF)
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带有人类反馈的强化学习（RLHF）
- en: The type of instruction fine-tuning we just did is often called *supervised
    fine-tuning*. It is *supervised* because we are curating, by hand, a list of example
    prompts and responses we want from the model.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才所做的指令微调通常被称为*监督式微调*。它被称为*监督式*，因为我们通过手工整理，创建了一个我们希望从模型中获得示例提示和响应的列表。
- en: Any need to manually write text examples will almost always become a bottleneck
    — such data is slow and expensive to come by. Moreover, this approach will be
    limited by the human performance ceiling on the instruction-following task. If
    we want to do better than human performance in a chatbot-like experience, we cannot
    rely on manually written output to supervise LLM training.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 任何需要手动编写文本示例的需求几乎总会成为瓶颈——此类数据获取缓慢且成本高昂。此外，这种方法将受到人类在指令跟随任务上的性能上限的限制。如果我们想在类似聊天机器人的体验中超越人类的表现，我们不能依赖于手动编写的输出来监督大型语言模型（LLM）的训练。
- en: The real problem we are trying to optimize is our preference for certain responses
    over others. With a large enough sample of people, this preference problem is
    perfectly defined, but figuring out how to translate from “our preferences” to
    a loss function we could use to compute gradients is quite tricky. This is what
    *Reinforcement Learning with Human Feedback*, or *RLHF*, attempts to solve.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正试图优化的真正问题是我们在某些响应与其他响应之间的偏好。对于足够大的人群样本，这个偏好问题是完美定义的，但弄清楚如何将“我们的偏好”转换为我们可以用来计算梯度的损失函数是非常棘手的。这正是*带人类反馈的强化学习*（Reinforcement
    Learning with Human Feedback，简称*RLHF*）试图解决的问题。
- en: The first step in RLHF fine-tuning is exactly what we did in the last section
    — supervised fine-tuning with handwritten prompts and responses. This gets us
    to a good baseline performance; we now need to improve on this baseline. To this
    end, we will build a *reward model* that can act as a proxy for human preference.
    We can gather a large number of prompts and responses to these prompts. Some of
    these responses can be handwritten; the model can write others. Responses could
    even be written by other chatbot LLMs. We then need to get human evaluators to
    rank these responses by preference. Given a prompt and several potential responses,
    an evaluator’s task is to rank them from most helpful to least helpful. Such data
    collection is expensive and slow, but still faster than writing all the desired
    responses by hand.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF微调的第一步正是我们在上一节中所做的——使用手写的提示和响应进行监督微调。这使我们达到了一个良好的基线性能；我们现在需要在这个基线之上进行改进。为此，我们将构建一个可以作为人类偏好的代理的*奖励模型*。我们可以收集大量提示及其响应。其中一些响应可以是手写的；模型可以写其他响应。这些响应甚至可以由其他聊天机器人LLM编写。然后我们需要让人类评估者根据偏好对这些响应进行排序。给定一个提示和几个潜在的响应，评估者的任务是按从最有用到最无用的顺序对它们进行排序。这种数据收集既昂贵又缓慢，但仍然比手动编写所有所需的响应要快。
- en: We can use this ranked preference dataset to build the reward model, which takes
    in a prompt-response pair and outputs a single floating-point value. The higher
    the value, the better the response. This reward model is usually another, smaller
    Transformer. Instead of predicting the next token, it reads a whole sequence and
    outputs a single float — a rating for a given response.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个排序偏好数据集来构建奖励模型，该模型接收一个提示-响应对并输出一个单一的浮点值。值越高，响应越好。这个奖励模型通常是另一个较小的Transformer。它不是预测下一个标记，而是读取整个序列并输出一个单一的浮点数——即对给定响应的评分。
- en: We can then use this reward model to tune our model further, using a reinforcement
    learning setup. We won’t get too deep into the details of reinforcement learning
    in this book, but don’t be too intimidated by the term — it refers to any training
    setup where a deep learning model learns by making predictions (called *actions*)
    and getting feedback on that output (called *rewards*). In short, a model’s own
    predictions become its training data.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用这个奖励模型通过强化学习设置进一步调整我们的模型。在这本书中，我们不会深入探讨强化学习的细节，但不要被这个术语吓倒——它指的是任何深度学习模型通过做出预测（称为*动作*）并对其输出（称为*奖励*）获得反馈来学习的训练设置。简而言之，模型自己的预测成为其训练数据。
- en: In our case, the action is simply generating a response to an input prompt,
    like we have been doing above with the `generate()` function. The reward is simply
    applying a separate regression model to that string output. Here’s a simple example
    in pseudocode.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，动作仅仅是生成对输入提示的响应，就像我们上面使用`generate()`函数所做的那样。奖励就是将一个单独的回归模型应用到那个字符串输出上。以下是一个简单的伪代码示例。
- en: '[PRE43]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[Listing 16.15](#listing-16-15): Pseudocode for the simplest possible RLHF
    algorithm'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表16.15](#listing-16-15)：最简单可能的RLHF算法的伪代码'
- en: In this simple example, we filter our generated responses with a reward cutoff,
    and simply treat the “good” output as new training data for more supervised fine-tuning
    like we just did in the last section. In practice, you will usually not discard
    your bad responses but rather use specialized gradient update algorithms to steer
    your model’s parameters using all responses and rewards. After all, a bad response
    gives a good signal on what not to do. OpenAI originally described RLHF in a 2022
    paper^([[5]](#footnote-5)) and used this training setup to go from GPT-3’s initial
    pretrained parameters to the first version of ChatGPT.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的例子中，我们通过奖励截止值过滤我们的生成响应，并将“好的”输出作为新的训练数据，进行更多像上一节中那样进行的监督微调。在实践中，你通常不会丢弃你的不良响应，而是使用专门的梯度更新算法，利用所有响应和奖励来引导你的模型参数。毕竟，一个不良的响应给出了一个好的信号，表明不应该做什么。OpenAI最初在2022年的一篇论文中描述了RLHF^([[5]](#footnote-5))，并使用这种训练设置将GPT-3的初始预训练参数转换为ChatGPT的第一个版本。
- en: An advantage of this setup is that it can be iterative. You can take this newly
    trained model, generate new and improved responses to prompts, rank these responses
    by human preference, and train a new and improved reward model.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置的优点在于它可以迭代。你可以使用这个新训练的模型，生成新的改进后的响应，根据人类偏好对这些响应进行排序，并训练一个新的改进后的奖励模型。
- en: Using a chatbot trained with RLHF
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用经过RLHF训练的聊天机器人
- en: 'We can make this more concrete by trying a model trained with this form of
    iterative preference tuning. Since building chatbots is the “killer app” for large
    Transformer models, it is common practice for companies that release pretrained
    models like Gemma to release specialized “instruction-tuned” versions, built just
    for chat. Let’s try loading one now. This will be a 4-billion-parameter model,
    quadruple the size of the model we just loaded and the largest model we will use
    in this book:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过尝试使用这种形式的迭代偏好调整训练的模型来使这个问题更加具体。由于构建聊天机器人是大型Transformer模型的“杀手级应用”，因此发布预训练模型如Gemma的公司通常会发布专门的“指令调整”版本，专为聊天构建。现在让我们尝试加载一个。这将是一个400亿参数的模型，是我们刚刚加载的模型大小的四倍，也是本书中我们将使用的最大模型：
- en: '[PRE44]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[Listing 16.16](#listing-16-16): Loading an instruction-tuned Gemma variant'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表16.16](#listing-16-16)：加载一个指令调整过的Gemma变体'
- en: 'Like the earlier Gemma model we fine-tuned ourselves, this instruction-tuned
    checkpoint comes with a specific template for formatting its input. Again, the
    exact text does not matter, what is important is that our prompt template matches
    what was used to tune the model:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前微调的Gemma模型一样，这个指令调整过的检查点附带了一个特定的模板来格式化其输入。同样，确切文本并不重要，重要的是我们的提示模板与用于调整模型的文本相匹配：
- en: '[PRE45]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let’s try asking it a question:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试着问它一个问题：
- en: '[PRE46]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This 4-billion-parameter model was first pretrained on 14 trillion tokens of
    text and then extensively fine-tuned to make it more helpful when answering questions.
    Some of this tuning was done with supervised fine-tuning like we did in the previous
    section, some with RLHF as we covered in this section, and some with still other
    techniques — like using an even larger model as a “teacher” to guide training.
    The increase in ability to do question-answering is easily noticeable.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这个400亿参数的模型最初在140万亿个文本标记上进行了预训练，然后进行了广泛的微调，使其在回答问题时更有帮助。其中一些调整与我们在上一节中进行的监督微调类似，一些与我们在本节中介绍的RLHF类似，还有一些使用更大的模型作为“教师”来引导训练的其他技术。在问答能力上的提升很容易察觉。
- en: 'Let’s try this model on the prompt that has been giving us trouble with hallucinations:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用这个模型来解决我们之前在幻觉方面遇到麻烦的提示：
- en: '[PRE47]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This more capable model refuses to take the bait. This is not the result of
    a new modeling technique, but rather the result of extensive training on trick
    questions like this one with responses like the one we just received. In fact,
    you can see clearly here why removing hallucinations can be a bit like playing
    whack-a-mole — even though it refused to hallucinate a US president, the model
    now manages to make up today’s date.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这个更强大的模型拒绝上钩。这不是新建模技术的结果，而是对像这样一个问题以及我们刚刚收到的类似响应的复杂问题的广泛训练的结果。事实上，你可以清楚地看到为什么移除幻觉有点像玩打地鼠——尽管它拒绝幻想一个美国总统，但模型现在设法编造了今天的日期。
- en: Multimodal LLMs
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多模态LLM
- en: One obvious chatbot extension is the ability to handle new modalities of input.
    An assistant that can respond to audio input and process images would be far more
    useful than one that can only operate on text.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 一个明显的聊天机器人扩展是处理新的输入模态的能力。一个能够响应音频输入并处理图像的助手，将比只能处理文本的助手更有用。
- en: Extending a Transformer to different modalities can be done in a conceptually
    simple way. The Transformer is not a text-specific model; it’s a highly effective
    model for *learning patterns in sequence data*. If we can figure out how to coerce
    other data types into a sequence representation, we can feed this sequence into
    a Transformer and train with it.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 将Transformer扩展到不同的模态可以通过一种概念上简单的方式进行。Transformer不是一个特定于文本的模型；它是一个在序列数据中学习模式的高效模型。如果我们能想出如何将其他数据类型强制转换为序列表示，我们就可以将这个序列输入到Transformer中，并用它进行训练。
- en: In fact, the Gemma model we just loaded does just that. The model comes with
    a separate 420-million-parameter image encoder that cuts an input image into 256
    patches and encodes each patch as a vector with the same dimensionality as Gemma’s
    hidden transformer dimension. Each image will be embedded as a `(256, 2560)` sequence.
    Because 2560 is the hidden dimensionality of the Gemma Transformer model, this
    image representation can simply be spliced into our text sequence after the token
    embedding layer. You can think of it like 256 special tokens representing the
    image, where each `(1, 2560)` vector is sometimes called a “soft token” (figure
    16.6). Unlike our normal “hard tokens,” where each token ID can only take on a
    fixed number of possible vectors in our token embedding matrix, these image soft
    tokens can take on any vector value output by the vision encoder.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们刚刚加载的Gemma模型正是如此。该模型附带一个独立的4.2亿参数图像编码器，它将输入图像切割成256个补丁，并将每个补丁编码为与Gemma的隐藏Transformer维度相同的向量。每个图像将被嵌入为一个`(256,
    2560)`序列。因为2560是Gemma Transformer模型的隐藏维度，所以这种图像表示可以简单地拼接到我们的文本序列的令牌嵌入层之后。你可以将其想象为256个代表图像的特殊令牌，其中每个`(1,
    2560)`向量有时被称为“软令牌”（图16.6）。与我们的正常“硬令牌”不同，每个令牌ID在我们的令牌嵌入矩阵中只能取固定数量的可能向量，这些图像软令牌可以取视觉编码器输出的任何向量值。
- en: '![](../Images/5954c7114547a585c17d82ad8a65d987.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5954c7114547a585c17d82ad8a65d987.png)'
- en: '[Figure 16.6](#figure-16-6): Handling image input by splicing text tokens and
    soft image tokens together'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16.6](#figure-16-6)：通过拼接文本令牌和软图像令牌来处理图像输入'
- en: 'Let’s load an image to see how this works in a little more detail (figure 16.7):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载一个图像，以更详细地了解这是如何工作的（图16.7）：
- en: '[PRE48]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](../Images/5c9974d386f43a2a47131297e4b59860.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5c9974d386f43a2a47131297e4b59860.png)'
- en: '[Figure 16.7](#figure-16-7): A test image for the Gemma model'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16.7](#figure-16-7)：Gemma模型的测试图像'
- en: 'We can use Gemma to ask some questions about this image:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Gemma来对这个图像提出一些问题：
- en: '[PRE49]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Each of our input prompts contains the special token `<start_of_image>`. This
    is turned into 256 placeholder values in our input sequence, which, in turn, is
    replaced with the soft tokens representing our image.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们输入的每个提示都包含特殊令牌`<start_of_image>`。这在我们输入序列中转换成了256个占位符值，这些值反过来又用代表我们图像的软令牌替换。
- en: Training for a multimodal model like this is quite similar to regular LLM pretraining
    and fine-tuning. Usually, you would want to first pretrain your image encoder
    separately, like we first did in Chapter 8 of this book. Then you can simply do
    the same basic “guess the next word” pretraining and also feed in mixed image
    and text content combined into a single sequence. Our transformer would not be
    trained to output image soft tokens; we would simply zero the loss at these image
    token locations.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种多模态模型，训练过程与常规LLM预训练和微调非常相似。通常，你首先会单独对图像编码器进行预训练，就像我们在本书第8章中首先做的那样。然后你可以简单地执行相同的“猜测下一个单词”预训练，并将混合图像和文本内容组合成一个序列输入。我们的Transformer不会训练输出图像软令牌；我们只需在这些图像令牌位置将损失置零。
- en: It might seem almost magical that we can simply add image data to an LLM, but
    when we consider the power of the sequence model we’re working with, it’s really
    quite an expected result. We’ve taken a Transformer, recast our image input as
    sequence data, and done a lot of extra training. The model can preserve the original
    language model’s ability to ingest and produce text while learning to also embed
    images in the Transformer’s latent space.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地将图像数据添加到LLM中，这似乎几乎像魔法一样，但当我们考虑到我们正在使用的序列模型的力量时，这实际上是一个非常预期的结果。我们使用了一个Transformer，将我们的图像输入重新表示为序列数据，并进行了大量的额外训练。该模型可以在学习在Transformer的潜在空间中嵌入图像的同时，保留原始语言模型摄取和产生文本的能力。
- en: Foundation models
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基础模型
- en: As LLMs venture into different modalities, the “large language model” moniker
    can become a bit misleading. They *do* model language, but also images, audio,
    maybe even structured data. In the next chapter, we will see a distinct architecture,
    called *diffusion models*, that works quite differently in terms of underlying
    structure but has a similar feel — they too are trained on massive amounts of
    data at “internet scale” with a self-supervised loss.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM探索不同的模态，"大型语言模型"这个名称可能有点误导。它们*确实*建模语言，但也包括图像、音频，甚至可能是结构化数据。在下一章中，我们将看到一种独特的架构，称为*扩散模型*，它在底层结构方面有所不同，但感觉相似——它们也是在“互联网规模”的大量数据上通过自监督损失进行训练。
- en: An umbrella term for models like this is *foundation models*. More specifically,
    a foundation model is any model that is trained on broad data (generally using
    self-supervision at scale) that can be fine-tuned to a wide range of downstream
    tasks.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型的统称是*基础模型*。更具体地说，基础模型是任何在广泛数据上训练（通常使用大规模自监督）的模型，可以微调到广泛的下游任务。
- en: In general, you can think of a foundation model as learning to *reconstruct*
    data pulled from large swaths of the internet, given a partial representation
    of it. While LLMs are the first and best-known of these models, there are many
    others. The hallmarks of a foundation model are the self-supervised learning objective
    (a reconstruction loss) and the fact that these models are not specialized to
    a single task and can be used for a number of downstream purposes.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你可以将基础模型视为学习从互联网的大量区域中*重建*数据，给定其部分表示。虽然LLM是这些模型中第一个且最广为人知的，但还有很多其他模型。基础模型的标志是自监督学习目标（重建损失）以及这些模型不是针对单一任务进行专门化的，并且可以用于多种下游目的。
- en: This is an important and striking shift that has happened quite recently in
    the long history of machine learning. Rather than training a model from scratch
    on your individual dataset, you will often be better off using a foundation model
    to get a rich representation of your input (whether it’s images, text, or something
    else) and then specialize that model for your final downstream task. Of course,
    this comes with the downside of needing to run large models with billions of parameters,
    so it’s hardly a fit for all real-world applications of machine learning.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在机器学习漫长历史中最近发生的一个重要且引人注目的转变。与其从头开始在自己的数据集上训练模型，你通常会更好地使用基础模型来获取输入（无论是图像、文本还是其他内容）的丰富表示，然后针对最终下游任务对该模型进行微调。当然，这伴随着需要运行具有数十亿参数的大型模型的缺点，因此它几乎不适合所有机器学习的实际应用。
- en: Retrieval Augmented Generation (RAG)
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）
- en: 'Sticking extra information in the prompt is not just helpful in handling image
    data; it can be a general way to extend the capabilities of an LLM. One notable
    example is when using an LLM for search. If we naively compare an LLM to a search
    engine, it has a couple of fatal flaws:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示中添加额外信息不仅有助于处理图像数据；这可以是一种扩展LLM能力的一般方法。一个显著的例子是当使用LLM进行搜索时。如果我们天真地将LLM与搜索引擎进行比较，它有几个致命的缺陷：
- en: An LLM will occasionally make things up. It will output false “facts” that were
    not present in the training data but could be interpolated from the training data.
    This information can range from misleading to dangerous.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM有时会编造事实。它会输出训练数据中不存在的错误“事实”，但这些可以从训练数据中推断出来。这些信息可能从误导到危险不等。
- en: An LLM’s knowledge of the world has a cutoff date — at best, the date the model
    was pretrained. Training an LLM is quite expensive, and it is not feasible to
    train continuously on new data. So at some arbitrary point in time, an LLM’s knowledge
    of the world will just stop.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个大型语言模型（LLM）对世界的知识有一个截止日期——最多是模型预训练的日期。训练一个LLM相当昂贵，并且不可能在新的数据上持续训练。因此，在某个任意的时间点，LLM对世界的知识将停止增长。
- en: No one wants to use a search engine that can only tell you about things that
    happened six months ago. But if we think of an LLM as more like “conversational
    software” that can handle any sequence data in a prompt, what if we instead used
    the model as the interface to information retrieved by more traditional search?
    That’s the idea behind *retrieval-augmented generation* or *RAG*.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 没有人愿意使用只能告诉你六个月前发生的事情的搜索引擎。但如果我们把LLM看作是更类似于“对话软件”，能够处理提示中的任何序列数据，那么如果我们将模型作为检索更多传统搜索信息界面的接口，会怎样呢？这就是*检索增强生成*或*RAG*背后的想法。
- en: 'RAG works by taking an initial user question and doing some form of a query
    to pull in additional text context. This query can be to a database, a search
    engine, or anything that can give further information on the question asked by
    a user. This extra information is then added straight into the prompt. For example,
    you might construct a prompt like this:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: RAG通过获取初始用户问题并执行某种形式的查询来拉取额外的文本上下文来工作。这个查询可以是数据库、搜索引擎或任何可以提供关于用户提出的问题的额外信息的任何东西。然后，这些额外信息直接添加到提示中。例如，你可能构建这样的提示：
- en: '[PRE50]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: A common approach for looking up relevant information is to use a *vector database*.
    To build a vector database, you can use an LLM, or any model, to embed a series
    of source documents as vectors. The document text will be stored in the database,
    with the embedding vector used as a key. During retrieval, an LLM can again be
    used to embed the user query as a vector. The vector database is responsible for
    searching for key vectors close to the query vector and for surfacing the corresponding
    text. This might sound a lot like the attention mechanism itself — recall that
    the terms “query,” “key,” and “value” actually came from database systems.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 查找相关信息的一种常见方法是使用*向量数据库*。要构建向量数据库，你可以使用LLM或任何模型将一系列源文档嵌入为向量。文档文本将存储在数据库中，嵌入向量用作键。在检索过程中，LLM可以再次用于将用户查询嵌入为向量。向量数据库负责搜索与查询向量接近的键向量，并显示相应的文本。这听起来可能很像注意力机制本身——回想一下，“查询”、“键”和“值”这些术语实际上来自数据库系统。
- en: 'Surfacing information to assist with generation does a few things:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 显示信息以协助生成做了一些事情：
- en: It gives you an obvious way to work around the cutoff date of the model.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它为你提供了一个明显的方法来绕过模型的截止日期。
- en: It allows the model to access private data. Companies might want to use an LLM
    trained on public data to serve as an interface to information stored privately.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许模型访问私有数据。公司可能希望使用在公共数据上训练的LLM作为访问存储在私有的信息的接口。
- en: It can help factually ground the model. There is no silver bullet that stops
    hallucinations entirely, but an LLM is much less likely to make up facts on a
    topic if presented with correct context about the subject in a prompt.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以帮助模型在事实上站稳脚跟。没有银弹可以完全阻止幻觉，但如果在提示中提供了关于主题的正确背景信息，LLM在某个主题上编造事实的可能性就会大大降低。
- en: “Reasoning” models
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: “推理”模型
- en: 'For years since the first LLMs, researchers have struggled with the well-known
    fact that these models were abysmal at math problems and logic puzzles. A model
    might give a perfect response to a problem directly in its training data, but
    substitute a few names or numbers in the prompt, and it would become evident that
    the model had no grasp on what it was trying to solve. For many problems in natural
    language processing, LLMs gave an easy recipe for progress: increase the amount
    of training data, increase some benchmark score. Grade school math problems, however,
    defied progress.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 自从第一个大型语言模型（LLM）出现以来，研究人员一直苦于这样一个众所周知的事实：这些模型在数学问题和逻辑谜题上表现糟糕。一个模型可能直接在其训练数据中对一个问题给出完美的回答，但如果你在提示中替换几个名字或数字，就会很明显地看出模型对它试图解决的问题毫无头绪。对于自然语言处理中的许多问题，LLM提供了一种简单的进步方法：增加训练数据量，提高某些基准分数。然而，小学数学问题却挑战了进步。
- en: In 2023, researchers from Google noticed that if you prompted the model with
    a few examples of “showing your work” on a math problem — as in literally writing
    out the steps like you would on a homework assignment — the model would start
    to do the same. As the model mimicked writing out intermediate steps, it would
    actually do far better at reaching the correct solution by attending to its own
    output. They called this “chain-of-thought” prompting, and the name stuck. Another
    group of researchers noticed that you didn’t even need examples; you could simply
    prompt the model with the phrase “Let’s think step by step” and get better output.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 到2023年，谷歌的研究人员注意到，如果你用几个数学问题的“展示你的工作”示例来提示模型——就像在家庭作业上那样逐字写出步骤——模型就会开始这样做。随着模型模仿写出中间步骤，它实际上通过关注自己的输出，达到了更正确的解决方案。他们将这种方法称为“思维链”提示，这个名字也一直沿用下来。另一组研究人员注意到，你甚至不需要示例；你只需用短语“让我们一步步思考”来提示模型，就能得到更好的输出。
- en: Since these discoveries, there has been heavy interest in directly training
    LLMs to get better at chain-of-thought reasoning. Models like OpenAI’s o1 and
    DeepSeek’s r1 have made headlines by showing significant strides in math and coding
    problems by training a model to “think out loud” on difficult questions.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 自从这些发现以来，人们对于直接训练LLM以更好地进行思维链推理的兴趣浓厚。像OpenAI的o1和DeepSeek的r1这样的模型通过训练模型在困难问题上“大声思考”来展示在数学和编码问题上的显著进步而成为头条新闻。
- en: The approach for this chain-of-thought fine-tuning is very similar to RLHF.
    We will first train the model on a few supervised examples of “showing your work”
    on a math problem and arriving at a correct answer. Next, we will prompt the model
    with a new math question and check whether the model got the final answer correct.
    Finally, we use these new generated outputs to further tune the model’s weights.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这种思维链微调的方法与RLHF非常相似。我们首先将在几个“展示你的工作”的数学问题并得出正确答案的监督示例上训练模型。接下来，我们将用一个新的数学问题提示模型，并检查模型是否得到了正确的最终答案。最后，我们使用这些新生成的输出进一步调整模型的权重。
- en: 'Let’s try this out with the Gemma model. We can write out our own word problem
    and turn on random sampling so we get a somewhat random response each time:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用Gemma模型来试试。我们可以写出自己的文字问题，并打开随机采样，这样每次都能得到一个相对随机的响应：
- en: '[PRE51]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let’s try generating a couple of responses:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试生成几个响应：
- en: '[PRE52]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: In the first attempt, our model was hung up on the superfluous detail that each
    letter has two pages. In the second attempt, the model gets the problem right.
    This instruction-tuned Gemma model we are working with has already been tuned
    on math problems like this; you would not get nearly as good results from the
    “untuned” Gemma model from the last section.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次尝试中，我们的模型被每个字母有两页的冗余细节所困扰。在第二次尝试中，模型正确地解决了问题。我们正在使用的这个指令调整过的Gemma模型已经针对这类数学问题进行了调整；如果你使用上一节中的“未调整”的Gemma模型，你几乎得不到这么好的结果。
- en: 'We could extend this idea to a very simple form of chain-of-thought training:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个想法扩展到一种非常简单的思维链训练形式：
- en: Collect a bunch of basic math and reasoning problems and desired answers.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集一些基本的数学和推理问题以及期望的答案。
- en: Generate (with some randomness) a number of responses.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成（带有一些随机性）一定数量的响应。
- en: Find all the responses with a correct answer via string parsing. You can prompt
    the model to use a specific text marker for the final answer as we did previously.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过字符串解析找到所有正确答案的响应。你可以提示模型使用我们之前使用的特定文本标记作为最终答案。
- en: Run supervised fine-tuning on correct responses, including all the intermediate
    output.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在正确的响应上运行监督微调，包括所有中间输出。
- en: Repeat!
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复！
- en: The previously described process is a reinforcement learning algorithm. Our
    answer checking acts as the *environment*, and the generated outputs are the *actions*
    the model uses to learn. As with RLHF, in practice you would use a more complex
    gradient update step to use information from all responses (even the incorrect
    ones), but the basic principle is the same.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 之前描述的过程是一个强化学习算法。我们的答案检查行为作为*环境*，生成的输出是模型用来学习的*动作*。与RLHF一样，在实践中，你会使用更复杂的梯度更新步骤来使用所有响应（包括错误的）的信息，但基本原理是相同的。
- en: The same idea is being used to improve LLM performance in other domains that
    have obvious, verifiable answers to text prompts. Coding is an important one —
    you can prompt the LLM to output code and then actually run the code to test the
    quality of the response.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的想法被用来提高LLM在其他领域的性能，这些领域对文本提示有明显的、可验证的答案。编码是一个重要的领域——你可以提示LLM输出代码，然后实际运行代码来测试响应的质量。
- en: In all these domains, one trend is clear — as a model learns to solve more difficult
    questions, the model will spend more and more time “showing its work” before reaching
    a final answer. You can think of this as the model learning to *search* over its
    own output of potential solutions. We will discuss this idea further in the final
    chapter of the book.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些领域，一个明显的趋势是——随着模型学会解决更难的问题，模型在得出最终答案之前将花费越来越多的时间“展示其工作”。你可以把这看作是模型学习在其潜在解决方案的输出中进行*搜索*。我们将在本书的最后一章进一步讨论这个想法。
- en: Where are LLMs heading next?
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM下一步将走向何方？
- en: Given the trajectory of LLMs discussed at the beginning of this chapter, it
    may seem obvious where LLMs will be heading. More parameters! Even better performance!
    In a general sense, that’s probably correct, but our trajectory might not be quite
    so linear.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 根据本章开头讨论的LLM轨迹，LLM将走向何方可能看起来很明显。更多的参数！更好的性能！在一般意义上，这可能是对的，但我们的轨迹可能并不那么线性。
- en: If you have a fixed budget for pretraining, say, a million dollars, you can
    roughly think of it as buying you a fixed amount of compute or floating-point
    operations (flops). You can either spend those flops on training with more data
    or training a bigger model. Recent research has pointed out that GPT-3, at 175
    billion parameters, was way too big for its computing budget. Training a smaller
    model on more data would have led to better model performance. So recently, model
    sizes have trended flatter while data sizes have trended up.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个固定的预训练预算，比如说一百万美元，你可以大致将其视为购买了一定数量的计算或浮点运算（flops）。你可以将这些flops用于使用更多数据进行训练，或者训练一个更大的模型。最近的研究指出，GPT-3在1750亿个参数的情况下，其计算预算过大。在更多数据上训练一个较小的模型将导致更好的模型性能。因此，最近模型大小趋于平稳，而数据大小趋于上升。
- en: This doesn’t mean that scaling will stop — more computing power *does* generally
    lead to better LLM performance, and we have yet to see signs of an asymptote where
    next token prediction performance levels off. Companies are continuing to invest
    billions of dollars in scaling LLMs and seeing what new capabilities emerge.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着扩展会停止——更多的计算能力通常确实会导致更好的LLM性能，我们还没有看到下一个标记预测性能趋于平稳的渐近线迹象。公司仍在继续投资数十亿美元用于扩展LLM，并观察新的功能如何出现。
- en: Figure 16.8 shows details for some of the major LLMs released from 2018 to 2025\.
    We can note that while the total number of tokens used for pretraining has climbed
    steadily and massively, model parameter counts have varied substantially since
    GPT-3\. In part, this is because we now know GPT-3 was undertrained, but it is
    also for a more practical reason. When deploying a model, it’s often worth it
    to sacrifice performance for a smaller model that fits on cheaper hardware. A
    really good model won’t help very much if it’s prohibitively expensive to run.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.8显示了2018年至2025年间发布的一些主要LLM的详细信息。我们可以注意到，尽管用于预训练的总标记数稳步且大幅上升，但自GPT-3以来，模型参数数量变化很大。部分原因是我们现在知道GPT-3训练不足，但也有一个更实际的原因。在部署模型时，为了适应更便宜的硬件而牺牲性能通常是值得的。如果运行成本过高，那么一个非常好的模型帮助不大。
- en: '![](../Images/60c3d90caff9394e86d032bd4873cb51.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/60c3d90caff9394e86d032bd4873cb51.png)'
- en: '[Figure 16.8](#figure-16-8): LLM parameter counts (left) and pretraining dataset
    sizes (right) over time. Many recent proprietary LLMs (e.g., GPT-4 and Gemini)
    are not included because model details have not been disclosed.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16.8](#figure-16-8)：随时间变化的LLM参数数量（左）和预训练数据集大小（右）。许多最近发布的专有LLM（例如，GPT-4和Gemini）不包括在内，因为模型细节尚未公开。'
- en: 'There’s another reason we might not be able to just scale up these models thoughtlessly:
    we are starting to run out of pretraining data! Tech companies are starting to
    have trouble finding more high-quality, public, human-written content to throw
    at pretraining. Models are even starting to “eat their own tail” by training on
    a significant portion of content created by other LLMs, which runs into a whole
    other host of concerns. This is one of the reasons reinforcement learning is getting
    a lot of attention recently. If you can create a difficult, self-contained *environment*
    that generates new problems for an LLM to attempt, you will have found a way to
    continue training using the model’s own output — no need to scrounge the web for
    more morsels of quality text.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能无法无限制地扩展这些模型还有另一个原因：我们开始缺乏预训练数据！科技公司开始难以找到更多高质量、公开、人工编写的内容用于预训练。模型甚至开始“吃掉自己的尾巴”，通过训练其他LLM创建的大量内容，这引发了一系列其他问题。这也是最近强化学习受到很多关注的原因之一。如果你能创建一个难以解决、自包含的环境，为LLM生成新问题，你将找到一种使用模型自身输出继续训练的方法——无需在网络上寻找更多优质文本的碎片。
- en: None of the solutions we touched on will be a silver bullet for the issues facing
    LLMs. At the end of the day, the fundamental problem remains that LLMs are wildly
    inefficient at learning compared to humans. Model capabilities only come from
    training on many orders of magnitude more text than people will read in their
    lifetimes. As scaling LLMs continues, so too will more fundamental research in
    how to make models that can learn quickly with limited data.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到的任何解决方案都不会是LLMs面临问题的银弹。最终，基本问题仍然是LLMs在学习和人类相比效率极低。模型能力仅来自在比人们一生中阅读的文本多许多数量级的文本上进行训练。随着LLMs的扩展继续，关于如何使模型能够快速学习有限数据的更基础的研究也将继续。
- en: Still, LLMs represent the ability to build fluent natural language interfaces,
    and that alone will bring about a massive shift in what we can accomplish with
    computing devices. In this chapter, we have laid out the basic recipe that many
    LLMs use to achieve these capabilities.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，LLMs代表了构建流畅的自然语言界面的能力，而这本身将带来我们在计算设备上所能实现的事情的巨大转变。在本章中，我们概述了许多LLMs用来实现这些功能的基本配方。
- en: Summary
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large language models, or LLMs, are the combination of a few key ingredients:'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）是几个关键成分的组合：
- en: The Transformer architecture
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer架构
- en: A language modeling task (predicting the next token based on past tokens)
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言建模任务（根据过去标记预测下一个标记）
- en: A large amount of unlabeled text data
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大量的未标记文本数据
- en: 'An LLM learns a probability distribution for predicting individual tokens.
    This can be combined with a sampling strategy to generate a long string of text.
    There are many popular ways to sample text:'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个LLM学习一个概率分布来预测单个标记。这可以与采样策略结合，生成一长串文本。有许多流行的文本采样方法：
- en: '*Greedy search* takes the most likely predicted token at each generation step.'
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*贪婪搜索*在每个生成步骤中采取最可能的预测标记。'
- en: '*Random sampling* directly samples the predicted categorical distribution over
    all tokens.'
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机采样*直接从所有标记上的预测分类分布中进行采样。'
- en: '*Top-k sampling* restricts the categorical distribution to the top set of K
    candidates.'
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Top-k采样*将分类分布限制在K个候选者的顶部集合。'
- en: LLMs use billions of parameters and are trained on trillions of words of text.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs使用数十亿个参数，并在万亿个单词的文本上训练。
- en: LLM output is unreliable, and all LLMs will occasionally hallucinate factually
    incorrect information.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的输出不可靠，所有LLMs偶尔都会产生事实错误的信息。
- en: 'LLMs can be fine-tuned to follow instructions in a chat dialog. This type of
    fine-tuning is called *instruction fine-tuning*:'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs可以被微调以遵循聊天对话中的指令。这种微调类型被称为*指令微调*：
- en: The simplest form of instruction fine-tuning involves directly training the
    model on instruction and response pairs.
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最简单的指令微调形式涉及直接在指令和响应对上训练模型。
- en: More advanced forms of instruction fine-tuning involve reinforcement learning.
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高级的指令微调形式涉及强化学习。
- en: The most common resource bottleneck when working with LLMs is accelerator memory.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用LLMs时，最常见的资源瓶颈是加速器内存。
- en: LoRA is a technique to reduce memory usage by freezing most Transformer parameters
    and only updating a low-rank decomposition of attention projection weights.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoRA是一种技术，通过冻结大多数Transformer参数，仅更新注意力投影权重的低秩分解来减少内存使用。
- en: LLMs can input or output data from different modalities if you can figure out
    how to frame these inputs or outputs as sequences in a sequence prediction problem.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs可以输入或输出来自不同模态的数据，如果你能想出如何将这些输入或输出作为序列预测问题中的序列来构建。
- en: A *foundation model* is a general term for models of any modality trained using
    self-supervision for a wide range of downstream tasks.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基础模型*是一个通用术语，用于任何模态的训练模型，这些模型使用自监督在广泛的下游任务上进行训练。'
- en: Footnotes
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚注
- en: In 2022, Jason Allen used the image generation software Midjourney to win an
    award for digital artists, and in 2024, Rie Kudan won one of Japan’s most prestigious
    literary awards for a novel written with substantial help from generative software.
    [[↩]](#footnote-link-1)
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在2022年，Jason Allen使用图像生成软件Midjourney赢得了数字艺术家的奖项，而在2024年，Rie Kudan凭借一部大量借助生成软件创作的小说赢得了日本最负盛名的文学奖项之一。[[↩]](#footnote-link-1)
- en: 'Iannis Xenakis, “Musiques formelles: nouveaux principes formels de composition
    musicale,” special issue of *La Revue musicale*, nos. 253-254 (1963). [[↩]](#footnote-link-2)'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Iannis Xenakis，“Musiques formelles: nouveaux principes formels de composition
    musicale，”*La Revue musicale*特刊，第253-254期（1963年）。[[↩]](#footnote-link-2)'
- en: Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever, “Improving
    Language Understanding by Generative Pre-Training,” (2018), [https://mng.bz/GweD](https://mng.bz/GweD).
    [[↩]](#footnote-link-3)
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alec Radford, Karthik Narasimhan, Tim Salimans和Ilya Sutskever，“通过生成预训练改进语言理解”，（2018年），[https://mng.bz/GweD](https://mng.bz/GweD).
    [[↩]](#footnote-link-3)
- en: 'J. Edward Hu et al., “LoRA: Low-Rank Adaptation of Large Language Models,”
    arXiv (2021), [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685).
    [[↩]](#footnote-link-4)'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: J. Edward Hu等人，“LoRA：大型语言模型的低秩自适应”，arXiv（2021年），[https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685).
    [[↩]](#footnote-link-4)
- en: Ouyang et al., “Training Language Models to Follow Instructions with Human Feedback,”
    Proceedings of the 36th International Conference on Neural Information Processing
    Systems (2022), [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155).
    [[↩]](#footnote-link-5)
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ouyang等人，“通过人类反馈训练语言模型遵循指令”，第36届国际神经网络信息处理系统会议论文集（2022年），[https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155).
    [[↩]](#footnote-link-5)
