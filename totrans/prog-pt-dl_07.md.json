["```py\npip install tensorboard\nconda install tensorboard\n```", "```py\ntensorboard --logdir=runs\n```", "```py\nfrom torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nwriter.add_scalar('example', 3)\n```", "```py\nimport random\nvalue = 10\nwriter.add_scalar('test_loop', value, 0)\nfor i in range(1,10000):\n  value += random.random() - 0.5\n  writer.add_scalar('test_loop', value, i)\n```", "```py\nimport torch\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms,models\n\nwriter = SummaryWriter()\nmodel = models.resnet18(False)\nwriter.add_graph(model,torch.rand([1,3,224,224]))\n\ndef train(model, optimizer, loss_fn, train_data_loader, test_data_loader, epochs=20):\n    model = model.train()\n    iteration = 0\n\n    for epoch in range(epochs):\n        model.train()\n        for batch in train_loader:\n            optimizer.zero_grad()\n            input, target = batch\n            output = model(input)\n            loss = loss_fn(output, target)\n            writer.add_scalar('loss', loss, epoch)\n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n        num_correct = 0\n        num_examples = 0\n        for batch in val_loader:\n            input, target = batch\n            output = model(input)\n            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], target).view(-1)\n            num_correct += torch.sum(correct).item()\n            num_examples += correct.shape[0]\n            print(\"Epoch {}, accuracy = {:.2f}\".format(epoch,\n                   num_correct / num_examples)\n            writer.add_scalar('accuracy', num_correct / num_examples, epoch)\n        iterations += 1\n```", "```py\ndef print_hook(self, module, input, output):\n  print(f\"Shape of input is {input.shape}\")\n\nmodel = models.resnet18()\nhook_ref  = model.fc.register_forward_hook(print_hook)\nmodel(torch.rand([1,3,224,224]))\nhook_ref.remove()\nmodel(torch.rand([1,3,224,224]))\n```", "```py\ndef send_stats(i, module, input, output):\n  writer.add_scalar(f\"{i}-mean\",output.data.std())\n  writer.add_scalar(f\"{i}-stddev\",output.data.std())\n```", "```py\nfrom functools import partial\n\nfor i,m in enumerate(model.children()):\n  m.register_forward_hook(partial(send_stats, i))\n```", "```py\nclass SaveActivations():\n    activations=None\n    def __init__(self, m):\n      self.hook = m.register_forward_hook(self.hook_fn)\n    def hook_fn(self, module, input, output):\n      self.features = output.data\n    def remove(self):\n      self.hook.remove()\n```", "```py\nimport torch\nfrom torchvision import models, transforms\nfrom torch.nn import functional as F\n\ncasper = Image.open(\"casper.jpg\")\n# Imagenet mean/std\n\nnormalize = transforms.Normalize(\n   mean=[0.485, 0.456, 0.406],\n   std=[0.229, 0.224, 0.225]\n)\n\npreprocess = transforms.Compose([\n   transforms.Resize((224,224)),\n   transforms.ToTensor(),\n   normalize\n])\n\ndisplay_transform = transforms.Compose([\n   transforms.Resize((224,224))])\n\ncasper_tensor = preprocess(casper)\n\nmodel = models.resnet18(pretrained=True)\nmodel.eval()\ncasper_activations = SaveActivations(model.layer_4)\nprediction = model(casper_tensor.unsqueeze(0))\npred_probabilities = F.softmax(prediction).data.squeeze()\ncasper_activations.remove()\ntorch.topk(pred_probabilities,1)\n```", "```py\nfts = sf[0].features[idx]\n        prob = np.exp(to_np(log_prob))\n        preds = np.argmax(prob[idx])\n        fts_np = to_np(fts)\n        f2=np.dot(np.rollaxis(fts_np,0,3), prob[idx])\n        f2-=f2.min()\n        f2/=f2.max()\n        f2\nplt.imshow(dx)\nplt.imshow(scipy.misc.imresize(f2, dx.shape), alpha=0.5, cmap='jet');\n```", "```py\n65.00%     0.00%  mysqld   [kernel.kallsyms]   [k] entry_SYSCALL_64_fastpath\n             |\n             ---entry_SYSCALL_64_fastpath\n                |\n                |--18.75%-- sys_io_getevents\n                |          read_events\n                |          schedule\n                |          __schedule\n                |          finish_task_switch\n                |\n                |--10.00%-- sys_fsync\n                |          do_fsync\n                |          vfs_fsync_range\n                |          ext4_sync_file\n                |          |\n                |          |--8.75%-- jbd2_complete_transaction\n                |          |          jbd2_log_wait_commit\n                |          |          |\n                |          |          |--6.25%-- _cond_resched\n                |          |          |          preempt_schedule_common\n                |          |          |          __schedule\n```", "```py\npip install py-spy\n```", "```py\npy-spy --flame profile.svg --pid 12345\n```", "```py\nimport torch\nimport torchvision\n\ndef get_model():\n    return torchvision.models.resnet18(pretrained=True)\n\ndef get_pred(model):\n    return model(torch.rand([1,3,224,224]))\n\nmodel = get_model()\n\nfor i in range(1,10000):\n    get_pred(model)\n```", "```py\npy-spy -r 99 -d 30 --flame profile.svg -- python t.py\n```", "```py\nimport torch\nimport torchvision\nfrom torch import optim\nimport torch.nn as nn\nfrom torchvision import datasets, transforms, models\nimport torch.utils.data\nfrom PIL import Image\nimport numpy as np\n\ndevice = \"cuda:0\"\nmodel = models.resnet18(pretrained=True)\nmodel.to(device)\n\nclass BadRandom(object):\n    def __call__(self, img):\n        img_np = np.array(img)\n        random = np.random.random_sample(img_np.shape)\n        out_np = img_np + random\n        out = Image.fromarray(out_np.astype('uint8'), 'RGB')\n        return out\n\n    def __repr__(self):\n        str = f\"{self.__class__.__name__  }\"\n        return str\n\ntrain_data_path = \"catfish/train\"\nimage_transforms =\ntorchvision.transforms.Compose(\n  [transforms.Resize((224,224)),BadRandom(), transforms.ToTensor()])\n```", "```py\ntrain_data = torchvision.datasets.ImageFolder(root=train_data_path,\ntransform=image_transforms)\nbatch_size=32\ntrain_data_loader = torch.utils.data.DataLoader(train_data,\nbatch_size=batch_size)\n\noptimizer = optim.Adam(model.parameters(), lr=2e-2)\ncriterion = nn.CrossEntropyLoss()\n\ndef train(model, optimizer, loss_fn,  train_loader, val_loader,\nepochs=20, device='cuda:0'):\n    model.to(device)\n    for epoch in range(epochs):\n        print(f\"epoch {epoch}\")\n        model.train()\n        for batch in train_loader:\n            optimizer.zero_grad()\n            ww, target = batch\n            ww = ww.to(device)\n            target= target.to(device)\n            output = model(ww)\n            loss = loss_fn(output, target)\n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n        num_correct = 0\n        num_examples = 0\n        for batch in val_loader:\n            input, target = batch\n            input = input.to(device)\n            target= target.to(device)\n            output = model(input)\n            correct = torch.eq(torch.max(output, dim=1)[1], target).view(-1)\n            num_correct += torch.sum(correct).item()\n            num_examples += correct.shape[0]\n        print(\"Epoch {}, accuracy = {:.2f}\"\n        .format(epoch, num_correct / num_examples))\n\ntrain(model,optimizer,criterion,\ntrain_data_loader,train_data_loader,epochs=10)\n```", "```py\npy-spy -r 99 -d 120 --flame slowloader.svg -- python slowloader.py\n```", "```py\ncpu_t1 = torch.rand(64,3,224,224)\ncpu_t2 = torch.rand(64,3,224,224)\n%timeit cpu_t1 + cpu_t2\n>> 5.39 ms \u00b1 4.29 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\ngpu_t1 = torch.rand(64,3,224,224).to(\"cuda\")\ngpu_t2 = torch.rand(64,3,224,224).to(\"cuda\")\n%timeit gpu_t1 + gpu_t2\n>> 297 \u00b5s \u00b1 338 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n```", "```py\ndef add_noise_gpu(tensor, device):\n  random_noise = torch_rand_like(tensor).to(device)\n  return tensor.add_(random_noise)\n```", "```py\ninput = add_noise_gpu(input, device)\n```", "```py\nnvidia-smi --query-gpu=timestamp,\nmemory.used, memory.free,memory.total,utilization.gpu --format=csv -l 5\n```", "```py\nimport gc\ndel tensor_to_be_deleted\ngc.collect()\n```", "```py\nfrom torch.utils.checkpoint import checkpoint_sequential\nimport torch.nn as nn\n\nclass CheckpointedAlexNet(nn.Module):\n\n    def __init__(self, num_classes=1000, chunks=2):\n        super(CheckpointedAlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = checkpoint_sequential(self.features, chunks, x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), 256 * 6 * 6)\n        x = self.classifier(x)\n        return x\n```"]