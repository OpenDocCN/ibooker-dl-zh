- en: 11 International ACH transactions and OFAC scanning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: IAT batches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancing the project to process IAT batches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OFAC list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scanning ACH files to stay in compliance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter covers the final sprint of our program iteration. Of course, the
    business unit will get together for another PI planning session, and the process
    will start all over again. In this sprint, we are tasked with enhancing the project
    to expand beyond domestic ACH transactions and deal with batches containing international
    ACH transactions (IAT), which enable sending money electronically between accounts
    in different countries. Along with IAT processing, there is the need to ensure
    the financial institution is not sending transactions to individuals or countries
    that are currently under restrictions or sanctioned by the US government. Specifically,
    the Office of Foreign Asset Control (OFAC) provides a list of “Specially Designated
    Nationals,” also known as the SDN list, which is a register of individuals and
    companies whose assets are blocked, and dealing with them is prohibited.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 Sprint planning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this final sprint, we have a big request by the line of business, which is
    to enable IAT ACH transactions. In the original scope of our project, we were
    told that the financial institution was not going to take on the additional risk
    of processing international transactions. Therefore, the database was not designed
    to support these types of transactions. However, recently, the financial institution
    has been attempting to pursue larger business customers and has had trouble attracting
    them because such customers require the ability to receive and transfer funds
    internationally.
  prefs: []
  type: TYPE_NORMAL
- en: As it often happens when meeting customer demands, we now have to update the
    dashboard to be able to support these types of transactions, which means including
    additional tables to the database, parsing of the file, and scanning of the customers
    involved to stay in compliance. Figure 11.1 provides a timeline for the proposed
    tasks associated with this sprint. Having a timeline that we can provide to other
    stakeholders can be helpful when addressing questions about the tasks we will
    be working on during the sprint and our schedule.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a software project  Description automatically generated](../Images/CH11_F01_Kardell.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1  Timeline for IAT ACH transaction processing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: With the sprint planning in place, we can move forward with work to support
    international ACH transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 International ACH transactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until this point, we have dealt with Prearranged Payment and Deposit Entry (PPD)
    batches. As you know, PPD batches are commonly used for direct deposits of payroll
    and pension payments. We may also remember that ACH started as a way of processing
    payments for domestic transactions in the early 1970s. With the expansion of the
    ACH system, support for IAT began in 2009, and the capabilities for ACH were expanded
    beyond domestic transactions. So, what is so different about IAT batches that
    we need to dedicate an entire sprint to add support for processing them? First
    and foremost, the IAT batches must conform to the 94-character limit imposed on
    all ACH records. As we will see in this section, this affects the way data must
    be transmitted.
  prefs: []
  type: TYPE_NORMAL
- en: '11.2.1 IAT batches: An overview'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we dive into supporting IAT batches, we first need to understand how
    a batch may be laid out. Figure 11.2 shows a sample batch we will work with to
    better understand the data involved. This batch represents an individual named
    Elena Santiago from Bilbao, Spain, sending a gift of $1.00 to a friend named David
    Wiliams from her account at Iberia Global Bank to his checking account at Futuristic
    FinTech.
  prefs: []
  type: TYPE_NORMAL
- en: '![A white background with black numbers and letters  Description automatically
    generated](../Images/CH11_F02_Kardell.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2  Sample IAT batch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: That is a fair amount of information packed into the batch, so let’s take a
    moment to unpack it (figure 11.3), and then we can jump into the code.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in figure 11.4, the batch is marked as an IAT, with a Company Entry
    Description of GIFT. The FF3 is the Foreign Exchange Indicator, while the FF means
    fixed-to-fixed. The originated amount is the same as the one being received, and
    the 3 indicates that the Foreign Exchange Reference is spaces. The USDUSD represents
    the originating currency code (USD) and the destination currency code (USD), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The entry record contains the number of addenda records that were passed (figure
    11.5). We expect to see seven (0007) addenda records passed, which also happen
    to be a type 7\. Another important field is obviously the amount—the $1.00 that
    was sent in this case.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](../Images/CH11_F03_Kardell.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3  Routing number and account
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![A computer screen with text  Description automatically generated](../Images/CH11_F04_Kardell.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4  IAT batch header
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![A screen shot of a computer  Description automatically generated](../Images/CH11_F05_Kardell.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5  IAT entry record
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are between 7 and 12 addenda records per record when dealing with IAT
    entries. This is different from what we have dealt with when parsing PPD batches,
    as we had an indicator for an optional addenda record. While that addenda indicator
    is still present on an IAT entry, it will always be set to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'To keep things relatively simple, we will only consider the seven mandatory
    addenda records. They all start with a 7 to indicate an addenda record, followed
    by an addenda type code in the next two positions, producing the mandator records
    range from 710 to 716:'
  prefs: []
  type: TYPE_NORMAL
- en: '*71**0*—Foreign payment amount and receiver’s name'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*711*—Originator’s name and street'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*712*—Originator’s city, state, country, and postal code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*71**3*—ODFI name, ID, and branch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*71**4*—RDFI name, ID, and branch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*71**5*—Receiver’s ID number and street'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*71**6*—Receiver’s city, state, country and postal code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Equipped with new knowledge, let’s create some IAT batches.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3 Creating IAT batches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we can start making changes and parsing a file, we must have a file.
    We have a sample provided in figure 11.2, and that, combined with the existing
    code we have to produce ACH files, should be more than enough to build from. If
    you reference the [https://achdevguide.nacha.org/ach-file-details](https://achdevguide.nacha.org/ach-file-details)
    and [https://mng.bz/eyOv](https://mng.bz/eyOv) as guides, you can see that we
    may be dealing with a few different layouts.
  prefs: []
  type: TYPE_NORMAL
- en: When we dig into the details, we’ll find we are working with new records and
    formats. This means that when creating files specifically, we need to consider
    whether we have an IAT batch and create a file accordingly. We already have the
    code within our ach_file_creation.feature and test_create_ach_files.py to deal
    with creating batches with a specified Standard Entry Class (SEC) code such as
    the line `And` `I` `want` `to` `have` `1` `batch` `with` `ACH` `credits` `and`
    `debits` `and` `a` `standard` `entry` `class` `code` `of` `"PPD"`, which drives
    the creating of the batch. We must update our method `create_batch_header` to
    take the SEC code into consideration. So, as shown in the following listing, we
    start by defaulting (hardcoding) on some of the values. As usual, we go back when
    we need them to be dynamic and handle them at that point.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.1  Updated `create_batch_header`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 IAT indicator'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Foreign exchange indicator'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Foreign exchange reference indicator'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Foreign exchange reference'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 ISO destination country code'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Originator identification'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Standard entry class code'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Company entry description'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 ISO originating currency code'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 ISO destination currency code'
  prefs: []
  type: TYPE_NORMAL
- en: '#11 Effective entry date'
  prefs: []
  type: TYPE_NORMAL
- en: '#12 Settlement date'
  prefs: []
  type: TYPE_NORMAL
- en: '#13 Originator status code'
  prefs: []
  type: TYPE_NORMAL
- en: '#14 Originating DFI identification'
  prefs: []
  type: TYPE_NORMAL
- en: '#15 Previous creation logic of the batch header'
  prefs: []
  type: TYPE_NORMAL
- en: Since the SEC code is stored as part of our unit test information, we can reference
    it wherever necessary, and we will need it again when writing out the entries
    for an IAT batch. As described in the previous section, every entry record for
    an IAT record contains at least seven addenda records. We follow the same pattern,
    using the SEC code to create a new method that will take care of creating the
    needed entry and addenda records keeping things hardcoded for the most part and
    allowing for dynamic values. The code to create files is now over 500 lines long,
    and we should start to consider where refactoring is possible to clean it up.
    The logic to create specific types of files is a good candidate for refactoring
    since those details do not necessarily need to be part of this process.
  prefs: []
  type: TYPE_NORMAL
- en: For now, however, the code should suffice for our initial file creation. The
    feature in the following listing should look familiar to our other file-creation
    steps. This is a good thing as it means our grammar is generic enough to handle
    some variations. Of course, we have some work to do to really make things dynamic,
    but we have more than enough information to create the files.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.2  Creating an ACH IAT file
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: With an ACH file containing an IAT batch available, we can start working on
    the necessary tables and structure to support storing the file.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.1 Database changes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We need to add a minimum of nine tables to the database to support IAT processing.
    We are only considering the new batch header format, entry format, and the required
    addenda records. The layout of our tables in the database mimics the layout of
    the ACH records. Let’s take a look at some of the tables. Remember that our database
    structure relies on having both unparsed and parsed records, the idea being that
    our system will eventually expand to process the files asynchronously once they
    have been uploaded. Since no new record numbers have been introduced, we do not
    need to expand anything regarding the unparsed records as IAT will still fit nicely
    into that structure. If we want to have our parsed records stored (and we do),
    we need to add the nine tables.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at the `ach_iat_batch_headers` table in listing 11.3\. Notice
    how it still has a foreign key reference to the `ach_records_type_5` table. Also,
    the majority of the fields are stored as `VARCHAR`, because as an initial iteration,
    we are looking to take a simple approach to the structure. In future iterations,
    fields such as `service_class_code` and `effective_entry_date` could be updated
    to `NUMERIC` or `DATE`, respectively. Those constraints will help ensure the integrity
    of the record and are worth dealing with.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.3  IAT batch headers table
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The foreign key for the batch header with CASCADEs for the deletes and updates'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The rest of the fields required to support the parsing of the ACH record.
    Consider using more specific data types to ensure the record is formatted correctly
    by enforcing correct typing at the database level.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The rest of the fields required to support the parsing of the ACH record.
    Consider using more specific data types to ensure the record is formatted correctly
    by enforcing correct typing at the database level.'
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the tables are handled in much the same way, but we should be aware
    of a couple of design choices and standards we may need to consider when dealing
    with the addenda records. As an example, let’s use the addenda record that contains
    the originator’s city, state, country, and postal code. This addenda record has
    a record type code of 7 (because it is an addenda record) and an addenda type
    code of 12 (because that is what Nacha decided).
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to name our tables. So, should we use the `ach_iat_originator_address_info`
    or `ach_iat_addenda_712_records` or some variation on that? In general, it does
    not matter unless we go with a name that is extreme (i.e., too long or too short
    and cryptic). We originally went with `ach_iat_addenda_712_records` because the
    712 will be at the beginning of every line for those types of addenda records
    in the file, and we can key off that when trying to remember the name of our table.
    It also saves us from having to know what that type of addenda we are dealing
    with (i.e., whether this is the originator address or receiver address). Of course,
    there may be some complaints about the table name, such as
  prefs: []
  type: TYPE_NORMAL
- en: It contains the word addenda and a 7 which is redundant, as a type 7 record
    is always an addenda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It does not separate the 7 and 12 with an underscore even though they are two
    separate fields
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses the word records, and previously, tables using “records” contained unparsed
    records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We point these out to highlight the importance of consistency and standards.
    The closer we stay to established standards, the more consistent we are likely
    to be. Given that we wanted to try to adhere to a standard, we implemented the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Use `_details` for tables that contained the parsed records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `_records` for tables that contained the unparsed records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the prefix `ach_ppd`, `ach_iat`, and so on for tables that dealt with specific
    ACH formats, as we were not always consistent with where the name `ppd` was used
    in the table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, we ended up with names such as `ach_iat_entry_details` and `ach_iat_addenda_10_details`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s tackle the table itself. Listing 11.4 shows the `ach_iat_addenda_712_records`
    table. Some of these IAT addenda records are unique to the ACH standards because
    they contain fields that are delimited within the fixed records. Strange, huh?
  prefs: []
  type: TYPE_NORMAL
- en: Previously, other formats were strictly a fixed-length record. So, we could
    say those 15 characters are the name and will be stored in the name field. With
    some of these addenda records, we have a fixed length field, such as the 35 character
    “Originator City & State/Province,” which contains both the city and state. The
    data elements are delimited by an asterisk `*`, and the backslash `\` is the terminator
    for the last element. This results in a table that has seven fields (excluding
    the UUID), whereas the record has six fields. Note that one of the fields is reserved
    and not in use, and therefore, it is not represented in the table. Consequently,
    these parsed fields are represented in the table by individual fields. We left
    each individual field at the maximum field size, so although the originators city/state
    are both contained in one field of 35 characters, we kept both the city and state
    fields in the table at 35 bytes to avoid confusion.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.4  Table for IAT addenda type 12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 **Defaults the addenda_type_code to be a 12 as it must always be a 12 for
    this record**'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 **Leaves these records as VARCHAR(35), although we could consider enforcing
    a stricter data type**'
  prefs: []
  type: TYPE_NORMAL
- en: This code provides the pattern for how all the IAT tables were dealt with. Next,
    we look at how the records are parsed.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4 IAT record parsing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To parse the records, we use the file creation steps we built earlier to create
    a sample file and then use unit test to build the needed code to parse the various
    records and store them in the database (listing 11.5). We have our sample record
    that we are looking to parse—the expected result. We set up the necessary records
    for testing by calling `setup_iat_addenda_test`, which simply adds the needed
    headers and entry records to the database so that all the foreign keys work as
    expected. Then, we call the `_parse_iat_addenda_712` and the class `AchIat712AddendaSql`.
    It can sometimes be tempting to simply define the `expected_result` based on the
    return value when we are sure our logic is correct. We would advise against that
    and make sure the record is parsed in another way, whether that is by hand or
    with another tool so that the results are independently verified.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.5  Unit testing the parse routine for Type 12 addenda
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The fixture to ensure the database is empty; we have autouse set to True
    so there is no need to include it in our test methods as it will automatically
    be executed.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 A sample addenda record taken from our test ACH file'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The dictionary of the expected result to validate the retrieved record'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Sets up the needed database records so the constraints are satisfied'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Parses the record, which will also add it to the database'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Retrieves the record, excluding the UUID field. Since the UUID is assigned
    by the database, we cannot hardcode it in our expected results.'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Performs asserts to ensure there is only one row and that the record matches
    our expected value that was previously defined'
  prefs: []
  type: TYPE_NORMAL
- en: The previous unit test will fail until we have built the required functionality.
    We will work through the required pieces of code to establish a pattern. All the
    unit tests for this will follow a similar approach. The first missing method that
    we encounter is the `_parse_iat_addenda_712` method, as in the following listing.
    Although it is straightforward, we do have to populate the `expected_record_types`,
    which helps the parser determine when records are out of sequence.
  prefs: []
  type: TYPE_NORMAL
- en: We want to keep verification in mind when we get to updating the processing
    logic for our ACH file because we will need to determine whether we have received
    all the required records and ensure that there are no duplicate addenda records.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.6  The `_parse_iat_addenda_712` record
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The expected record types that can be passed to us'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The parsing is actually done with the AchRecordProcessor. At this point,
    we may also consider moving some of our parsing routines to new classes.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 With the parsed record, we need to insert it into the database.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create the method `parse_iat_addenda_712`. This task could also involve
    its own separate unit tests since the purpose of that class is twofold. First,
    it consolidates the actual parsing logic into a central location, which reduces
    the code in our ACH file processor and allows clearer understanding of the ACH
    processing flow. Second, it allows us to test the parsing logic in isolation without
    the need for a bunch of setup in the database.
  prefs: []
  type: TYPE_NORMAL
- en: However, the parsing is not overly complicated and will be tested by this overall
    process, so for the time being, we will not worry about it having its own unit
    test. The following listing shows the code for parsing the IAT addenda.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.7  Parsing the IAT addenda record
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates a regular expression that will parse the delimited fields within
    the record'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Ensures we have matches and extracts them for the city and state'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Ensures we have matches and extracts them for the country and postal code'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Parses the record and returns it as part of our schema'
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, we need to define the schema used for these records. As shown in
    listing 11.8, we provide a minimum layout that matches what we expect to insert
    into the database.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.8  IAT addenda 712 schema
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The final step is to create a class to handle the SQL logic for inserting and
    retrieving the record.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.9  IAT addenda type 12 SQL
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Defines a class and method to insert our record schema'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Obtains a database connection'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 SQL to insert the fields and values for our model'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Creates a dictionary of the schema to use with the INSERT statement'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 The rest of the methods, specifically getting a record by the UUID'
  prefs: []
  type: TYPE_NORMAL
- en: With that, we should be able to go back and ensure all the import statements
    are in place and then run our unit test successfully. Assuming we are parsing
    the record correctly (for both the expected and the actual record), we should
    have a passing unit test.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing IAT challenge
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This pattern needs to be repeated for all the new database records, and now
    is a great time to take a break and write some code. We have sample files. It
    is now just a matter of working through parsing the remaining records. If the
    previous code examples still seem a bit daunting, we could also start at a smaller
    scale by creating unit tests for `AchRecordProcessor`, which is solely responsible
    for parsing the ACH record with no database interaction. The simpler requirements
    for testing the `Ach­RecordParser` should mean that it takes less work to set
    up our unit tests. Once the parsing is verified at that level, we can take a step
    back to see the bigger picture and begin writing unit tests that involve the database,
    as outlined in this section.
  prefs: []
  type: TYPE_NORMAL
- en: While we should have all the parsing validated at this stage, we still need
    to update our `ach_file_processor` to handle the IAT batch and make use of all
    this wonderful code we just wrote.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5 IAT file processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, we have built the pieces of our IAT processing. We should feel
    fairly confident that we can handle the individual records and parsing them. Now,
    we need to be able to incorporate the parsing of an actual IAT batch within a
    file. We need to keep in mind that the work we did in previous sections tested
    individual pieces. For instance, we know that when we call the `_parse_iat_batch_header`
    method and pass it an IAT batch header record, it will be parsed and stored in
    the database. However, the method is not called in the current flow of loading
    an ACH file through the `POST` call. As we work on adding the functionality to
    the parser, we should keep some goals and requirements in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Parsing a PPD batch still works as expected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Addenda records are all present.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Addenda records are in the correct order.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we begin ensuring that we have unit tests before we start
    making changes to the actual code.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.1 Unit testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We hope it is obvious that after adding the IAT batches, we should still be
    able to load PPD batches. So, do not forget to test and verify that we have not
    broken anything with the addition of IAT processing. This means that we want to
    ensure we implement some regression testing—the last thing we want to do is spend
    all our time working on the new processing and not validate the previous work.
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, the first order of business is to create a test_loading_pdd_files
    file and ensure that we have the correct record count when loading a PPD batch.
    We start with simple tests to get the count of the unparsed records. We test for
    the individual record counts and the total number of records, ensuring that we
    did not write any exceptions, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.10  Testing PPD batches
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Defines a file name since it will be used in a few different places during
    the test'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Ensures we can reference the file'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sets up some initial expected values'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Sets up the test, parses the file, and returns any exceptions'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Queries the database to get record counts for each of the tables that stores
    our unparsed records. This test contains a lot of repetitive code to obtain the
    count for each type.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Adds the record counts together to get the total count, but it is somewhat
    redundant since we are testing the counts individually as well. However, how many
    times we would be asked about a total count if this was not here?'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Asserts the record counts are correct'
  prefs: []
  type: TYPE_NORMAL
- en: The previous test works well enough, but as it tends to happen with our development,
    we repurposed some other test to create the new one and ended up copying/pasting
    code to get the additional record counts and `assert` statement. In fact, Copilot
    is nice enough to fill in some of the code for us, so we did not even have to
    do much copying/pasting. There is one problem, though—we should be treating our
    test code as a first-class citizen, giving it the same attention we gave to our
    production code.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how we might rework the above code to make it more concise and
    easier to understand. The changes in the following listing shortened the code
    from 53 to 45 lines, and while that is not the only metric, we should use it for
    judging whether code is good or bad. Early on in our careers, we were told by
    one of our mentors that it felt more productive when they were removing code rather
    than writing it.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.11  Refactored unit test for unparsed records
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Updates the get_db call to use a row_factory of dict_row'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The record counts for each of our ACH tables'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Computes the total records'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 The queries to get the counts of each row'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 The queries to get the counts of each row'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Gets the single result'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Adds the exception count into the record_counts dictionary'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Compares the two dictionaries'
  prefs: []
  type: TYPE_NORMAL
- en: We have a similar query to verify that parsed records exist in the table for
    all the parsed records as well. Ideally, we would expect an exception to be in
    the table if we had any problems parsing, but in case we have not yet coded for
    that or perhaps missed a condition that would cause a parse error, it is good
    to check these tables as well.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.12  Unit test for parsed PPD records
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The selection logic can remain the same.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Note that the counts are now occurring on the parsed records and that the
    tables are specific to PPD batches.'
  prefs: []
  type: TYPE_NORMAL
- en: Our goal was to ensure that we had enough tests to validate the files processed
    successfully. We should now be confident that the file has been processed in the
    database, that their unit tests may validate those database fields, and that the
    files were parsed correctly. Remember from chapter 2 that we should test not only
    the happy path (success) but also the not-so-happy path (error handling). For
    now, we have enough to move forward with a similar test for IAT processing.
  prefs: []
  type: TYPE_NORMAL
- en: We can create a similar test for unparsed records that loads a file containing
    an IAT batch instead. We created one and named it (rather unimaginatively) iat.ach.
    The processing for an IAT file does not change when we consider only the unparsed
    records, and that is partially why we needed to test both the unparsed and parsed
    records. Of course, we still tested the unparsed records with a unit test, but
    the real work is for the IAT parsed records, as in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.13  Unit test for parsed IAT records
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 We select the desired records and sum them up to get the total count, which
    we also verified by pulling up the file (since it is just a text file) in any
    editor and reviewing the record counts.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 We need to include the IAT addenda tables. Keep in mind the batch header,
    entry details, and addenda records will be different from PPD batches.'
  prefs: []
  type: TYPE_NORMAL
- en: The previous test will fail because we have not yet updated the file parsing
    to make use of these detail tables. However, we are now confident that we have
    both PPD and IAT parsing covered by unit tests, which means we can move ahead
    to using these new IAT detail tables.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.2 Updating file processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With our passing PPD file load and the failing IAT file load, we begin working
    through the changes needed to support IAT. As we make changes, we will constantly
    be rerunning the unit tests to ensure that we are making progress with parsing
    IAT files and that we have not inadvertently broken anything. Since an ACH file
    is processed sequentially, the first record we need to address is the batch header
    record. We add a new field named `batch_type` that we will use when parsing the
    batch header. The following listing shows the simple `if`/`elif`/`else` processing
    that we have added, just calling the appropriate routine based on the SEC code
    and logging an exception when having a header we do not recognize.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.14  Calling the appropriate parsing routine
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The SEC code is in the same position, regardless of whether the batch header
    is for IAT or PPD. This line sets the batch_type flag appropriately for the SEC
    type.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Previously, we just called the _parse_batch_header method. Now, we ensure
    that we are dealing with a PPD batch.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 We would like to log an exception when processing, and we have an unrecognized
    SEC code. This could be because of an invalid formatted file or a code we do not
    support yet.'
  prefs: []
  type: TYPE_NORMAL
- en: With the proper parsing of the batch header, we should be making it a bit farther
    in our testing of IAT processing, and of course, PPD processing should not be
    broken. The next record we encounter is the type 6 record. With the introduction
    of the `batch_type` variable, it should follow the same pattern. Notice that we
    continue to write the unparsed record to the database like before. However, the
    need to parse the record and write it to the appropriate table depends on the
    `batch_type` flag. The following listing shows the required updates.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.15  Parsing entry details
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The writing of our unparsed records is independent of the batch type we
    are working with.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 When the batch_type is set to IAT, we will call the appropriate method to
    parse the entry.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The parameters for IAT and PPD parsing are exactly the same. Only the fields
    that need to be parsed are different.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Adds an exception when we encounter an unexpected batch_type'
  prefs: []
  type: TYPE_NORMAL
- en: Parsing the addenda records is only slightly more complicated than parsing the
    batch header and entry detail records. This is because we now also deal with the
    different addenda record types being in the expected order. Similar to the `expected_record_type`
    variable, we also introduce an `expected_addenda_type` variable. Let’s jump straight
    to the method where we parse a specific addenda type record, as shown in the following
    listing. Here, it is standard processing with the addition of the new `expected_addenda_type`.
    The next record we expect is another addenda record (type 7), and it should be
    of addenda type 11.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.16  The `_parse_iat_addenda_710` method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The expected_record should be another addenda record.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The expected_addenda_type should be the next sequence as the records are
    required to be in a specific order. Note that we only expect one type of record,
    so there is no need for an array as with the expected_record_type.'
  prefs: []
  type: TYPE_NORMAL
- en: All the addenda records will parse the same way, with the `expected_addenda_type`
    being set to the next record. But what happens when we get to the final addenda
    type (type 16)? The following listing shows how we reset our expected types once
    we reach that last record.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.17  Resetting the expected types
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '#1 There are some optional addenda records for IAT processing, so it is possible
    to encounter those, as well as a new entry or the end of the batch.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 There are no more mandatory addenda records, so the expected_addenda_type
    can be set to an empty string.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to parse the addenda records, it should be easy to put
    together how these individual methods would be called—as they follow a similar
    approach to how we parse the record types. The process is shown in the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.18  Parsing the addenda records
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Defines our method for parsing. This is an umbrella that will call the other
    methods to parse specific records.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Extracts the addenda type since we will be using it repeatedly'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 If the addenda type is not what we expected, we need to log an exception.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Depending on the addenda type we are working with, calls the appropriate
    method. It logs a different exception if we have an unexpected addenda type.'
  prefs: []
  type: TYPE_NORMAL
- en: With that, we should be able to parse files containing both IAT and PPD batches.
    Having these extra tables for IAT batches will have an effect on some of our APIs.
    In the next section, we will take a brief look at exactly what is affected.
  prefs: []
  type: TYPE_NORMAL
- en: 11.6 Effects on the dashboard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How does the addition of IAT processing affect the functionality of our dashboard?
    That question would likely require more discussion with the business. From an
    ACH standards perspective, it is certainly possible to have IAT batches in the
    same file as PPD batches. However, the fields and interesting information for
    an IAT batch may be different, and certainly, the introduction of new tables plays
    into how the components function. Whether it makes sense for our components to
    include all batches or just select batches may be a business decision with input
    from end-users. For now, let’s look at how we can fit the IAT batches into our
    current dashboard components.
  prefs: []
  type: TYPE_NORMAL
- en: 11.6.1 The get_batches method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `/{file_id}/batches` endpoint calls the `get_batches` method, which will
    need to be updated to incorporate the IAT batch headers. We know that the `ach_records_type_5_id`
    will exist in only one of the batch header tables. We employ that and the `COALESCE`
    command to use a field from the `ach_batch_headers` and if that is `NULL`, to
    use the `ach_iat_batch_headers` value instead. This works under the assumption
    that a field will not unexpectedly be `NULL`. The fields in question are all marked
    as `NOT NULL` in the database. Whether they stay that way indefinitely is another
    matter. For the time being, the following listing shows how we can update our
    query without being too invasive to the current API.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.19  Updated `get_batches`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '#1 We do not have access to a company name for IAT from the header record.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 If the company_identification is not available, uses the originating_dfi_identification'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Picks the appropriate batch_number from the batch'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Uses LEFT JOIN because the batch header is for one table or the other'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Sorts by the appropriate identification number'
  prefs: []
  type: TYPE_NORMAL
- en: As shown here, we can sometimes get away with updating the existing query; however,
    that is not the case in all situations. In fact, it is probably an exception.
    The next section discusses what to do when the query is too large or complicated
    to incorporate both PPD and IAT.
  prefs: []
  type: TYPE_NORMAL
- en: 11.6.2 Batch entries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gathering the batch entries from the endpoint `/{file_id}/batches/{batch_id}/entries`
    requires some work. The query to build the `AchBatchEntriesResponse` was sizeable,
    and it may make more sense to leave the current query in place and create a new
    query specific to IAT transaction entries. Therefore, we will take the `get_entries`
    method, move the existing query to a method `_get_ppd_entries`, and create a new
    `_get_iat_entries`. The results of the split are shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.20  Updated `get_entries`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Method to determine whether the specified batch in the file is an IAT batch'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Method to return the IAT entries for a given file and batch'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The previous SQL query has been moved to its own method.'
  prefs: []
  type: TYPE_NORMAL
- en: There are some slight changes when creating the `AchBatchEntriesResponse` for
    IAT records. With the original query for PPD records to get the addenda count,
    we had to gather that information ourselves. With an IAT batch, that information
    is part of the entry record; however, we still gather the count in the same fashion,
    in part to keep the queries similar, but more importantly, because we may want
    to perform verification on the field in the future, and this approach provides
    the mechanism to do that (listing 11.21).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.21  Query from `_get_iat_entries`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '#1 We get the number of addenda records, which is also available on the record
    type 6 for IAT transactions. Eventually, we could use this approach to validate
    the addenda count that was passed.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 We pass the receiving_name back as the individual_name to conform to our
    response, which needs to be retrieved from the addenda type 10 record.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Pulls the amount from the IAT entry record'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 The entry record also has the account number.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Joins the necessary IAT tables'
  prefs: []
  type: TYPE_NORMAL
- en: Whether we incorporate IAT into the current dashboard or create brand new components
    is going to be a matter or preference by the business and users. One aspect that
    is not up for negotiation is remaining compliant with government regulations.
    The next section discusses how to address some regulatory concerns.
  prefs: []
  type: TYPE_NORMAL
- en: 11.7 OFAC scanning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Financial institutions are subject to various regulations and compliance on
    a daily basis. Regulations such as the PATRIOT Act and Know Your Customer (KYC)
    hold the financial institution responsible for failing to report potential money
    laundering and terrorist-financing activities. The Office of Foreign Asset Control
    (OFAC) provides a “Specially Designated Nationals” list, or a SDN list, that includes
    individuals, companies, and assets such as ships/planes that are prohibited to
    do business with, and institutions may face penalties.
  prefs: []
  type: TYPE_NORMAL
- en: Note that there are other lists, as well as countries in general, that institutions
    are prohibited from doing business with. Doing business with any of these sanctioned
    entities can cost the bank significantly. A list of entities that violated US
    regulations and their associated fines is available at [https://mng.bz/pKN8](https://mng.bz/pKN8).
  prefs: []
  type: TYPE_NORMAL
- en: While there are third-party packages that specialize in this type of scanning,
    over the next few sections, we implement basic scanning for our dashboard to get
    a feel for the process. If you are interested in ML/AI and analytics, this is
    a great opportunity to dive in and build some comprehensive detection!
  prefs: []
  type: TYPE_NORMAL
- en: 11.7.1 Sanctioned individuals and countries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are XML files available for download from the OFAC website at [https://sanctionslist.ofac.treas.gov/Home/SdnList](https://sanctionslist.ofac.treas.gov/Home/SdnList)
    and an online search tool as well at [https://sanctionssearch.ofac.treas.gov/](https://sanctionssearch.ofac.treas.gov/).
    Either of these is a great way to start familiarizing ourselves with the information
    available when working with sanctioned individuals.
  prefs: []
  type: TYPE_NORMAL
- en: For our purposes, we create a table that contains names and aliases for made-up
    individuals to avoid any potential problems with using actual data from the list.
    We also create a list of made-up countries we should also scan for. As we saw
    from the civil penalties, many times, companies find themselves in trouble just
    by doing business with companies/individuals in other countries. Listing 11.22
    shows the `create` statements for tables to hold individual names as well as countries.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.22  Creating tables for OFAC scanning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Next, we populate the tables when the database is created, as shown in the following
    listing. Do not forget to add the `sdn_list` and `sanctioned_countries` tables
    to the `truncate_all` utilities method as you do not want this default data cleared
    during unit testing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.23  Populating the new tables
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: With our data populated, we can now move on to building the API for scanning.
  prefs: []
  type: TYPE_NORMAL
- en: 11.7.2 Scanning for individuals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we build an API, let’s define the query that will return the results
    for the OFAC scan. For our needs, we will be scanning all our loaded files whenever
    someone clicks on the OFAC Scanning link in our UI. However, this feature will
    not be practical later when we have larger lists of suspects, more complex scanning
    algorithms, and more files loaded. We are going to discuss some strategies to
    handle this problem, but for now, let’s break down the query we used to gather
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: As you know, the goal is to search for individual names on incoming ACH trans­actions
    and match them against known individuals on the lists provided by OFAC. Up until
    this point, when searching, we have used SQL keywords such as `ILIKE` and the
    `%` wildcard. Now, we get a little more advanced by introducing fuzzy matching
    to our query. By installing the `fuzzystrmatch` into our Postgres database, as
    in the following listing, we get additional search options.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.24  Installing `fuzzystrmatch`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This extension provides phonetic matching algorithms such as Soundex and Meta­phone,
    as well as the Levenshtein distance algorithm, for measuring the similarity between
    two strings. These algorithms can help us create a more robust searching algorithm
    because, rather than searching for an exact or partial match, we can now expand
    our search to names that may by misspelt or slightly altered by a nefarious individual
    so that they can still perform transactions. Even though we are introducing feature
    for IAT transactions, it can be incorporated into any transactions done at a financial
    institution. Often, this (and various other checks) are done when a customer opens
    an account, and the database is often periodically scanned for any individuals.
    Therefore, we start with creating a common table expression (CTE) to gather names
    for PPD transactions. The main problem is to ensure we are collecting the names
    from the appropriate record, with ACH PPD transactions that will be on the type
    6 record.
  prefs: []
  type: TYPE_NORMAL
- en: In the following listing, we want to only select distinct names to minimize
    some of the searching across batches if we have multiple names. We may want to
    consider additional information in case we have a false positive in regard to
    one customer but not another. We also used `REPLACE` to remove any spaces from
    the name, but could also use `REGEXP_REPLACE(aepd.individual_name,` `'[^a-zA-Z0-9]',`
    `'',` `'g')` if we wanted to deal with any punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.25  Collecting individual names from PPD transactions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Use SELECT DISTINCT because we do not want duplicate searches; one name
    will suffice.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Simple replacement to remove spaces, but nothing else. Use REGEXP_REPLACE
    for more complicated removal.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The record to find the particular transaction'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Necessary JOINs to get down to the individual name'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Necessary GROUP BY statements because of the SELECT DISTINCT statement'
  prefs: []
  type: TYPE_NORMAL
- en: A similar CTE is required for IAT names, as shown in the following listing.
    As for some of our other IAT processing, we need to retrieve the name from the
    addenda record instead of the entry record. We return the `receiving_name` as
    the individual name to maintain alignment between the CTEs.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.26  Collecting individual names from IAT transactions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '#1 At this time, we do not want duplicate searches because one name will suffice.
    We need to rename the field to individual_name or find a common name for the field.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Simple replacement to remove spaces, but nothing else. Use REGEXP_REPLACE
    for more complicated removal.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The record to find the particular transaction'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Necessary JOINs to get down to the individual name'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Necessary GROUP BY statements because of the SELECT DISTINCT statement'
  prefs: []
  type: TYPE_NORMAL
- en: The next CTE is used to collect the names of our suspects (listing 11.27). For
    display purposes, we handle concatenating the name together, keeping in mind the
    possibility of the `middle_name` being `NULL` with `CONCAT_WS`. Otherwise, using
    a standard `CONCAT` function will result in two blank spaces between the first
    and last names. Of course, there are ways to handle this situation if the RDMS
    we are using does not have a function such as `CONCAT_WS`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.27  Collecting the SDN names
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The CONCAT_WS will skip NULL fields. It works perfectly for combining the
    first, middle, and last names into something we can use for display.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Concatenates the names and removes any spaces'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 A similar approach for the alias'
  prefs: []
  type: TYPE_NORMAL
- en: With the names of our customers and suspects gathered, we can now start comparing
    them. The first method we use is the function `LEVENSHTEIN` to compute the distance,
    which considers the number of additions, deletions, and updates needed to convert
    one string into another. We convert string into a number between 0 and 100, where
    0 is no match and 100 is an exact match.
  prefs: []
  type: TYPE_NORMAL
- en: The other method is the `DAITCH_MOKOTOFF` function, used in listing 11.28\.
    This function allows phonetic matching of names, attempting to determine if names
    sound alike. There are some additional algorithms available in the `fuzzystrmatch`
    module. In addition, there are other commercial searching algorithms. In a production
    environment, we would need additional pieces of information such as the alias,
    address, and similar to reduce the number of false positives that we may create
    by simply finding a phonetic match. However, using these methods is a good starting
    point.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.28  Computing similarity scores for IAT individuals
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Converts the value returned by the LEVENSHTEIN function into a percentage
    that is easier to understand for end-users'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Returns whether the names matched using the DAITCH_MOKOTOFF algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 We use CROSS JOIN so that every name is compared.'
  prefs: []
  type: TYPE_NORMAL
- en: We select all results returned from both queries, as shown in the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.29  Combining the results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Pulls together all our computed scoring and matches'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we filter the results as shown in the following listing, adding a unique
    row number for each row being returned to act as an identifier. There are many
    ways in which to optimize this and increase the performance of the queries. However,
    the clarity of gathering the names, comparing them, combining, and then filtering
    makes the process easier to follow in this example. We will discuss some strategies
    for actually producing this report when we discuss presenting of the OFAC results.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.30  Filtering the results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Use the ROW_NUMBER function to generate a unique id for the row.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Filters the results, returning only matches of a certain threshold or the
    ones that phonetically match the name or alias'
  prefs: []
  type: TYPE_NORMAL
- en: We now have a good idea of what we will be returning based on the results we
    see when working with this query. We create a way for users to access these results
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 11.8 Application programming interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now have two essential pieces: the data that exists in the database and
    a way to produce desired results. We continue to follow the well-established pattern
    for creating an API, starting with a unit test and working on the necessary pieces
    to ensure it passes.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.8.1 Unit test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The unit test in the following listing loads one of our previously created files.
    Here we are concentrating on loading the data from a file called ofac_elemental_resources.ach
    and validating our API. Having previously created and loaded the database with
    the same file using the dashboard (or another unit test), we already know that
    our query should return three matches. So, the unit test simply needs to ensure
    the database is clean, load the file, and grab the results.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.31  Unit test for OFAC API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Ensures a host and IP address are passed with the client as our logging
    requires that'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Ensures that we are picking the correct file'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Prior to making our API call, we need to clear the database and load the
    file.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Once we have the database set up and the client ready to make a request,
    we can pull the OFAC results from the API and ensure they are correct. We are
    only doing a quick check of the number of items returned, but want to dive deeper
    into the results to ensure the expected data is returned.'
  prefs: []
  type: TYPE_NORMAL
- en: Having the unit test defined allows us to continually check that our code returns
    the desired results. Let’s make sure that happens!
  prefs: []
  type: TYPE_NORMAL
- en: 11.8.2 Creating the endpoint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We first want to create the endpoint for the test (listing 11.32). We want to
    define the path users will be accessing. This endpoint was defined within the
    fiels.py router definition, which prefixes everything with `/api/v1/files`, so
    the path parameter only shows `/ofac`. Otherwise, we are simply returning an empty
    list, which should be enough to ensure that our unit test passes the first assert
    with a `200`-response code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.32  Barebones endpoint
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Defines the path and a log message'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Simply returns an empty list to ensure we receive a 200-response code and
    are able to pass the first assert statement in our unit test'
  prefs: []
  type: TYPE_NORMAL
- en: After we have the barebones endpoint working, we can fill in the blanks with
    a return type and call an actual method to send back the results, as shown in
    the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.33  Updated endpoint
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Our API is documented and ready for users from one of our documentation
    endpoints.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 We have defined the type of data we intend to return and a method to create
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: The endpoint will no longer work because of the newly added objects of O`facScan­Results`
    and `OfacSql`, which we will define next.
  prefs: []
  type: TYPE_NORMAL
- en: 11.8.3 Finishing the API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the API and the SQL to power it already defined, we only need to complete
    two housekeeping steps to finish the API and have a passing unit test. First,
    we want to define the `OfacScanResults`, as shown in the following listing. This
    is the natural progression from the data returned in our `SQL` query to its Pydantic
    equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.34  The `OfacScanResults` source
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '#1 With these fields, users can jump directly to the file or batch that contained
    the match for further research.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The name/alias of the suspect and the name it matched from the ACH file'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The numeric score that was computed. For now, it is the Levenshtein score
    but could potentially reflect a score computed from a more complicated algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 These Boolean values are intended to show a check mark or some other indicator
    that the record was matched by this method as well.'
  prefs: []
  type: TYPE_NORMAL
- en: With a place to store the results of our query, all that is needed is to create
    the `OfacSql` class and the associated `get_scan_results` method (listing 11.35).
    This will execute the SQL we created earlier, so this should be straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.35  The `OfacSql` class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The SQL query we designed previously'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Gets all the results from the query'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Returns the results'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the API should is functional, and we should have a passing unit
    test. We should also circle back and update our unit test to ensure that we are
    diving into the results and validating some of the fields as we were originally
    checking the size of an array. In theory, any endpoint that returned three items
    would pass that test. Next, we look at the final piece of the puzzle—presenting
    of our results.
  prefs: []
  type: TYPE_NORMAL
- en: 11.9 User interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the data and the ability to retrieve it in place, the final step is to
    present the information to the user. Staying with our general approach, we create
    a page responsible for making an API call, the outcome of which is passed down
    to a component that will display formatted results. Figure 11.6 shows the result
    of pulling a sample report. The report provides the name of the suspect, their
    alias, and the customer’s name. We also include the score (how close of a match)
    and whether the system matched the name or alias. There are also options to find
    the file or the batch that include the suspected customer.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](../Images/CH11_F06_Kardell.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6  Sample OFAC report screen
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 11.9.1 The OFAC page
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As shown in the following listing, we start by creating the main page and retrieving
    the data from the API. We could start by creating a placeholder page like in previous
    sections and ensuring that we can navigate to it before we try to populate the
    data. In this instance, since we have used similar approaches for exceptions (chapter
    9) and company information (chapter 10), we jump right to building the page. The
    IDE should complain because we do not have an `OfacResponse` or `OfacRecords`
    component, but we will tackle those next.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.36  Sample page and API call
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Uses axios to make the API request for an OFAC scan'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Passes the results to our component to display them nicely for the user'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, this has become somewhat of a boilerplate process. Depending on
    the task at hand and how many pages need to be created, we may sometimes create
    those placeholders just to get a feel for the navigation and flow. Or we may choose
    to add pages as we begin working. We should be flexible in our approach.
  prefs: []
  type: TYPE_NORMAL
- en: 11.9.2 OFAC components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We must create the `OfacResponse` to hold the API results. As we have worked
    with generative AI, we found that it can really help improve our productivity
    by generating some of this boilerplate code. We may use it to generate a Pydantic
    definition from a SQL `CREATE` `TABLE` statement. Then, we generate the TypeScript
    interface from the Pydantic class, which usually does a good job of capturing
    the needed fields. The following listing shows the `OfacResponse` interface.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.37  The `OfacResponse` interface
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We now look at creating the component itself using a MUI DataGrid like we did
    for previous components in our dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.38  The `OfacRecords` component
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The ach_files_id is used to create a link users can click to navigate to
    the file where the match was.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 To navigate to the batch that has the match, we need to pull the ach_files_id
    from the row object, as well as the params.value, although we could have also
    used the params.row.ach_batch_id field.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Displays the similarity score; we use Math.floor to remove any decimals
    just as a display preference.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Based on the Boolean flag that was passed, displays a green circle with
    a check mark or nothing'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Based on the Boolean flag that was passed, displays a green circle with
    a check mark or nothing'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 When the alias was matched, displays a green circle with a check mark'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 The columns and rows we defined are passed to the DataGrid componenet.'
  prefs: []
  type: TYPE_NORMAL
- en: With the component created, we should be able to pull the results of an OFAC
    scan and display them in our component. That should be sufficient to meet the
    requirements for the story. In the next section, we wrap up the OFAC searching
    by outlining some potential problems.
  prefs: []
  type: TYPE_NORMAL
- en: 11.9.3 OFAC results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now have a component that can present the results of an OFAC scan and have
    met the requirements for our story. This will more than likely suffice for the
    short term, but in a production environment, we will have to address some of the
    design choices made.
  prefs: []
  type: TYPE_NORMAL
- en: First, the scan is done dynamically whenever the page is loaded. While this
    works great for limited data, the page will likely slow to a crawl in a production
    environment. The SDN list contains over 15,000 records, and each will be scanned
    for every ACH transaction we have. Remember that there is a large volume of payments
    flowing through the ACH system (currently over 31 billion payments per year),
    and while we will not be required to scan them all, apparently, it may result
    in a lot of scanning. The situation will also be exacerbated when we have multiple
    users trying to view the reports, since each page requests results in another
    scan.
  prefs: []
  type: TYPE_NORMAL
- en: Second, there is no way to save or export our results. Because the scanning
    is done upon request (when a user visits the page) using the data currently stored
    in the database, we do not have a way to pull previous scan results. The SDN and
    other lists provided by OFAC are revised constantly with individuals being added,
    removed, and updated. The scans we do today may yield different results tomorrow,
    which can be a compliance nightmare in the current setup. If the SDN list changes,
    and we can no longer show that a particular individual was matched on a certain
    date or transaction, we may find ourselves facing penalties, as outlined at the
    beginning of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Third, in production environments, scanning is typically done at the time of
    the file load in an ACH environment. The database can also be periodically scanned
    to identify potential customers and transactions that may be in violation. All
    these scans produce a copy of the results that can be reviewed and archived in
    cold storage for later retrieval as needed. Scanning at the time of the load can
    help reduce constant requests that burden the system while providing real-time
    feedback on suspicious activity. However, for large files, this operation can
    still take some time, and with our current architecture, we would likely want
    to introduce a status for the file, so that some of these tasks could be done
    asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: Those are just a few of the challenges we may face with OFAC scanning, mentioning
    nothing of creating a more complex algorithm to detect suspicious activity. Although
    it presents several challenges, we can review them as opportunities to explore
    other areas of ACH, finance, compliance, and security that we need to touch on
    with this subject.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we explored a new ACH layout for international ACH transactions.
    The introduction of this new type of transaction for our dashboard also caused
    an increased risk for our organization by potentially violating US laws and regulations
    for dealing with sanctioned individuals. To remain in compliance and reduce our
    risk, we explored scanning our ACH file for customers that may be on a list of
    sanctioned individuals provided by OFAC. To accomplish all this, we enhanced the
    parsing of our ACH files, performed fuzzy matching on customer names, provided
    an API to power it all, and presented the results in a UI component.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: International ACH transactions (IAT) introduce unique regulatory requirements
    and require precise formatting, which underscores the need for careful planning
    and compliance verification in financial systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expanding database structures to accommodate IAT involves integrating new tables
    tailored for specific ACH components, reinforcing the importance of planning for
    system scalability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating IAT batches requires adapting current solutions to handle additional
    header and entry information, highlighting the complexity of international transaction
    standards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comprehensive unit testing ensures new IAT functionalities align with existing
    PPD processes, while preventing disruptions, showcasing the importance of regression
    testing in software development.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streamlining code through refactoring enhances maintainability and reduces redundancy,
    indicating the continual need for codebase optimization as projects evolve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating IAT processing in dashboards requires understanding both technical
    constraints and user needs, which emphasizes cross-functional alignment in feature
    development.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing APIs to handle IAT data ensures consistent data retrieval and processing,
    demonstrating the necessity for flexible yet robust system interfaces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OFAC compliance is crucial when processing IAT to mitigate risks associated
    with sanctioned individuals, which highlights the role of regulatory adherence
    in transaction processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing fuzzy matching algorithms aids in detecting minor variations in
    suspect names, thus increasing accuracy in identifying sanctioned transactions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective UI components for OFAC results delivery underscore the value of clear,
    actionable insights for users navigating complex compliance landscapes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persisting scan results addresses compliance demands by providing traceable
    audit trails, which stresses the need for reliable data storage and retrieval
    in financial applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The addition of IAT processing elevates organizational risk awareness, which
    underscores the critical need for stringent compliance frameworks to navigate
    international regulations effectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
