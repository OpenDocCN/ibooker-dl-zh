# 第十四章：结论

2018 年 5 月，我开始着手第一版这本书的工作。五年后，我对生成 AI 的无限可能性和潜在影响感到比以往任何时候都更加兴奋。

在这段时间里，我们看到了这个领域的惊人进步，对真实世界应用有着看似无限的潜力。我对我们迄今为止所取得的成就感到敬畏和惊叹，并迫不及待地期待着生成 AI 未来几年将对世界产生的影响。生成深度学习有能力以我们无法想象的方式塑造未来。

此外，随着我为这本书研究内容，我越来越清楚地意识到这个领域不仅仅是关于创建图像、文本或音乐。我相信生成深度学习的核心是智能本身的秘密。

本章的第一部分总结了我们在生成 AI 之旅中达到这一点的过程。我们将按时间顺序浏览自 2014 年以来的生成 AI 发展时间轴，以便您可以看到每种技术在生成 AI 历史中的位置。第二部分解释了我们目前在最先进的生成 AI 方面的位置。我们将讨论生成深度学习方法的当前趋势以及普通公众可以使用的当前现成模型。接下来，我们将探讨生成 AI 的未来以及前方的机遇和挑战。我们将考虑未来五年生成 AI 可能会是什么样子，以及它对社会和商业的潜在影响，并解决一些主要的伦理和实际问题。

# 生成 AI 时间轴

图 14-1 是我们在本书中一起探索的生成建模关键发展的时间轴。颜色代表不同的模型类型。

生成 AI 领域建立在深度学习早期发展的基础上，比如反向传播和卷积神经网络，这些技术解锁了模型在大规模数据集上学习复杂关系的可能性。在本节中，我们将研究生成 AI 的现代历史，从 2014 年开始，这一历史发展速度惊人。

为了帮助我们理解所有内容如何相互关联，我们可以大致将这段历史分为三个主要时代：

1.  2014 年至 2017 年：VAE 和 GAN 时代

1.  2018 年至 2019 年：Transformer 时代

1.  2020 年至 2022 年：大模型时代

![](img/gdl2_1401.png)

###### 图 14-1。从 2014 年到 2023 年的生成 AI 简史（注意：一些重要的发展，如 LSTM 和早期基于能量的模型[例如，玻尔兹曼机]在这个时间轴之前）

## 2014 年至 2017 年：VAE 和 GAN 时代

VAE 的发明可以说是点燃生成 AI 火药桶的火花。这篇论文展示了不仅可以生成简单的图像，如 MNIST 数字，还可以生成更复杂的图像，如面孔，而且可以在一个可以平滑遍历的潜在空间中生成。2014 年，GAN 的引入紧随其后，这是一种全新的对抗性框架，用于解决生成建模问题。

接下来的三年被逐渐更令人印象深刻的 GAN 系列扩展所主导。除了对 GAN 模型架构（DCGAN，2015）、损失函数（Wasserstein GAN，2017）和训练过程（ProGAN，2017）的基本改变外，还使用 GAN 处理了新的领域，如图像到图像的转换（pix2pix，2016，和 CycleGAN，2017）和音乐生成（MuseGAN，2017）。

在这个时代，还引入了重要的 VAE 改进，如 VAE-GAN（2015）和后来的 VQ-VAE（2017），并且在“世界模型”论文中看到了对强化学习的应用。

在这段时间内，已建立的自回归模型，如 LSTMs 和 GRUs，仍然是文本生成的主导力量。相同的自回归思想也被用于生成图像，PixelRNN（2016 年）和 PixelCNN（2016 年）被引入作为思考图像生成的新方法。还在测试其他图像生成方法，例如 RealNVP 模型（2016 年），为后来的各种归一化流模型铺平了道路。

在 2017 年 6 月，一篇开创性的论文《注意力就是一切》发表，开启了以 Transformer 为中心的生成 AI 的下一个时代。

## 2018 年至 2019 年：Transformer 时代

Transformer 的核心是注意力机制，它消除了旧的自回归模型（如 LSTMs）中存在的循环层的需求。Transformer 随着 2018 年 GPT（仅解码器 Transformer）和 BERT（仅编码器 Transformer）的推出迅速崭露头角。接下来的一年，逐渐建立了更大的语言模型，通过将它们视为纯文本到文本生成问题，擅长各种任务，其中 GPT-2（2018 年，15 亿参数）和 T5（2019 年，110 亿参数）是杰出的例子。

Transformer 也开始成功应用于音乐生成，例如 Music Transformer（2018 年）和 MuseNet（2019 年）模型的引入。

在这两年里，也发布了几个令人印象深刻的 GAN，巩固了该技术作为图像生成的最先进方法的地位。特别是，SAGAN（2018 年）和更大的 BigGAN（2018 年）将注意力机制与 GAN 框架结合起来，取得了令人难以置信的结果，而 StyleGAN（2018 年）和后来的 StyleGAN2（2019 年）展示了如何以惊人的细粒度控制生成图像的风格和内容。

另一个正在积聚动力的生成 AI 领域是基于分数的模型（NCSN，2019 年），最终为生成 AI 领域的下一个重大变革——扩散模型铺平了道路。

## 2020 年至 2022 年：大模型时代

这个时代见证了几个模型的推出，这些模型融合了不同生成建模家族的思想，并加速了现有架构。例如，VQ-GAN（2020 年）将 GAN 鉴别器引入 VQ-VAE 架构，Vision Transformer（2020 年）展示了如何训练 Transformer 在图像上运行的可能性。2022 年发布了 StyleGAN-XL，这是对 StyleGAN 架构的进一步更新，可以生成 1024×1024 像素的图像。

2020 年推出了两个模型，为所有未来大型图像生成模型奠定了基础：DDPM 和 DDIM。突然之间，扩散模型在图像生成质量方面成为 GAN 的竞争对手，正如 2021 年的论文标题“扩散模型在图像合成方面击败了 GAN”所明确说明的那样。扩散模型的图像质量令人难以置信地好，它们只需要训练一个单一的 U-Net 网络，而不是 GAN 的双网络设置，使训练过程更加稳定。

大约在同一时间，GPT-3（2020 年）发布了——这是一个庞大的 1750 亿参数的 Transformer，可以以一种几乎难以理解的方式生成几乎任何主题的文本。该模型通过一个网络应用程序和 API 发布，允许公司在其基础上构建产品和服务。ChatGPT（2022 年）是一个围绕 OpenAI 最新版本的 GPT 的网络应用程序和 API 封装器，允许用户与 AI 就任何主题进行自然对话。

在 2021 年和 2022 年，一大批其他大型语言模型相继发布，以与 GPT-3 竞争，包括微软和英伟达的 Megatron-Turing NLG（2021 年），DeepMind 的 Gopher（2021 年）和 Chinchilla（2022 年），谷歌的 LaMDA（2022 年）和 PaLM（2022 年），以及 Aleph Alpha 的 Luminous（2022 年）。还发布了一些开源模型，如 EleutherAI 的 GPT-Neo（2021 年），GPT-J（2021 年）和 GPT-NeoX（2022 年）；Meta 的 66B 参数 OPT 模型（2022 年）；谷歌的 Fine-tuned Flan-T5 模型（2022 年）；Hugging Face 的 BLOOM（2022 年）等等。这些模型都是 Transformer 的变体，训练在大量数据语料库上。

强大的 Transformer 用于文本生成和最先进的扩散模型用于图像生成的迅速崛起意味着过去两年生成 AI 发展的重点大部分集中在多模态模型上，即在超过一个领域（例如文本到图像模型）上运行的模型。

这一趋势始于 2021 年，当 OpenAI 发布了 DALL.E，这是一个基于离散 VAE（类似于 VQ-VAE）和 CLIP（一种预测图像/文本对的 Transformer 模型）的文本到图像模型。随后是 GLIDE（2021 年）和 DALL.E 2（2022 年），更新了模型的生成部分，使用扩散模型而不是离散 VAE，取得了真正令人印象深刻的结果。这一时代还见证了谷歌发布的三个文本到图像模型：Imagen（2022 年，使用 Transformer 和扩散模型），Parti（2022 年，使用 Transformers 和 ViT-VQGAN 模型），以及后来的 MUSE（2023 年，使用 Transformers 和 VQ-GANs）。DeepMind 也发布了 Flamingo（2022 年），这是一个视觉语言模型，建立在他们的大型语言模型 Chinchilla 的基础上，允许图像作为提示数据的一部分。

2021 年引入的另一个重要扩散进展是潜在扩散，其中扩散模型在自动编码器的潜在空间内进行训练。这一技术推动了 Stable Diffusion 模型的诞生，该模型由 Stability AI、CompVis 和 Runway 在 2022 年联合合作发布。与 DALL.E 2、Imagen 和 Flamingo 不同，Stable Diffusion 的代码和模型权重是开源的，这意味着任何人都可以在自己的硬件上运行该模型。

# 生成 AI 的当前状态

当我们结束对生成 AI 历史的探索时，现在重要的是反思我们在当前最先进应用和模型方面的立足点。让我们花一点时间评估我们在这一领域迄今取得的进展和关键成就。

## 大型语言模型

现在，文本生成的生成 AI 几乎完全集中在构建大型语言模型（LLMs）上，它们的唯一目的是直接从大量文本语料库中建模语言，即它们被训练来预测下一个词，以解码器 Transformer 的风格。

大型语言模型方法被广泛采用，因为它具有灵活性和在各种任务上表现出色的能力。同一模型可以用于问答、文本摘要、内容创作等多种示例，因为最终每个用例都可以被构建为一个文本到文本问题，其中特定任务指令（*提示*）作为模型输入的一部分给出。

让我们以[GPT-3](https://oreil.ly/Pga1w)为例。图 14-2 展示了同一模型如何用于文本摘要和内容创作。

![](img/gdl2_1402.png)

###### 图 14-2。来自 GPT-3 的输出——未突出显示的文本是提示，绿色突出显示的文本是 GPT-3 的输出

请注意，在这两种情况下，提示包含相关的指令。GPT-3 的任务只是逐个标记地继续提示。它没有一个可以查找信息的事实数据库，也没有可以复制到答案中的文本片段。它只被要求预测接下来最有可能跟随现有标记的标记，然后将这个预测附加到提示中以生成下一个标记，依此类推。

令人难以置信的是，这种简单的设计足以使语言模型在各种任务中表现出色，如图 14-2 所示。此外，它赋予了语言模型令人难以置信的灵活性，可以根据任何提示生成逼真的文本作为回应——想象力通常是限制因素！

图 14-3 显示自 2018 年原始 GPT 模型发布以来，大型语言模型的规模如何增长。参数数量呈指数增长，直到 2021 年底，Megatron-Turing NLG 达到 5300 亿参数。最近，更多的重点放在构建更高效的语言模型上，这些模型使用更少的参数，因为更大的模型在生产环境中更昂贵且速度较慢。

![](img/gdl2_1403.png)

###### 图 14-3\. 大型语言模型（橙色）和多模型（粉色）的参数数量随时间变化

许多人仍认为 OpenAI 的 GPT 系列（GPT-3、GPT-3.5、GPT-4 等）是目前个人和商业使用中最强大的最新语言模型套件。它们可以通过[网络应用](https://platform.openai.com/playground)和[API](https://openai.com/api)使用。

大型语言模型家族的另一个最新成员是 Meta 推出的*大型语言模型 Meta AI*（LLaMA），¹，这是一套从 7B 到 65B 参数大小的模型系列，纯粹基于公开可用的数据集进行训练。

今天存在的一些最强大的 LLM 的摘要显示在表 14-1 中。有些模型，如 LLaMA，是不同规模模型的系列—在这种情况下，最大模型的规模显示在这里。一些模型的预训练权重是完全开源的，这意味着任何人都可以免费使用和构建。

表 14-1\. 大型语言模型

| 模型 | 日期 | 开发者 | # 参数 | 开源 |
| --- | --- | --- | --- | --- |
| GPT-3 | 2020 年 5 月 | OpenAI | 1750 亿 | 否 |
| GPT-Neo | 2021 年 3 月 | EleutherAI | 27 亿 | 是 |
| GPT-J | 2021 年 6 月 | EleutherAI | 60 亿 | 是 |
| Megatron-Turing NLG | 2021 年 10 月 | 微软和英伟达 | 5300 亿 | 否 |
| Gopher | 2021 年 12 月 | DeepMind | 2800 亿 | 否 |
| LaMDA | 2022 年 1 月 | 谷歌 | 1370 亿 | 否 |
| GPT-NeoX | 2022 年 2 月 | EleutherAI | 200 亿 | 是 |
| Chinchilla | 2022 年 3 月 | DeepMind | 700 亿 | 否 |
| PaLM | 2022 年 4 月 | 谷歌 | 5400 亿 | 否 |
| Luminous | 2022 年 4 月 | Aleph Alpha | 700 亿 | 否 |
| OPT | 2022 年 5 月 | Meta | 1750 亿 | 是（660 亿） |
| BLOOM | 2022 年 7 月 | Hugging Face 合作 | 1750 亿 | 是 |
| Flan-T5 | 2022 年 10 月 | 谷歌 | 110 亿 | 是 |
| GPT-3.5 | 2022 年 11 月 | OpenAI | 未知 | 否 |
| LLaMA | 2023 年 2 月 | Meta | 650 亿 | 否 |
| GPT-4 | 2023 年 3 月 | OpenAI | 未知 | 否 |

尽管大型语言模型有令人印象深刻的应用，但仍然存在重大挑战需要克服。最值得注意的是，它们容易虚构事实，无法可靠地应用逻辑思维过程，如图 14-4 所示。

![](img/gdl2_1404.png)

###### 图 14-4\. 虽然大型语言模型在某些任务上表现出色，但也容易出现与事实或逻辑推理相关的错误（显示了 GPT-3 的输出）

重要的是要记住，LLMs 只是被训练来预测下一个单词。它们与现实没有其他联系，无法可靠地识别事实或逻辑谬误。因此，在生产中使用这些强大的文本预测模型时，我们必须非常谨慎——它们尚不能可靠地用于需要精确推理的任何事情。

## 文本到代码模型

大型语言模型的另一个应用是代码生成。2021 年 7 月，OpenAI 推出了一个名为 Codex 的模型，这是一个在 GitHub 上的代码上进行了微调的 GPT 语言模型。该模型能够成功地为一系列问题编写新颖的编码解决方案，只需根据要解决的问题的评论或函数名称进行提示。这项技术如今驱动着 GitHub Copilot，这是一个可以在您输入时实时建议代码的 AI 对编程师。Copilot 是一个基于订阅的付费服务，提供免费试用期。

图 14-5 显示了两个自动生成的完成示例。第一个示例是一个从给定用户那里获取推文的函数，使用 Twitter API。给定函数名称和参数，Copilot 能够自动完成函数定义的其余部分。第二个示例要求 Copilot 解析一组费用，还包括在 docstring 中包含一个自由文本描述，解释输入参数的格式以及与任务相关的具体说明。Copilot 能够仅通过描述自动完成整个函数。

这项引人注目的技术已经开始改变程序员处理特定任务的方式。程序员通常会花费相当大的时间搜索现有解决方案的示例，阅读社区问答论坛，如 Stack Overflow，并查阅包文档中的语法。这意味着离开交互式开发环境（IDE），切换到 Web 浏览器，并从 Web 上复制和粘贴代码片段，以查看它们是否解决了您的特定问题。在许多情况下，Copilot 消除了这样做的必要性，因为您只需在 IDE 中写下您希望实现的简要描述后，就可以通过 AI 生成的潜在解决方案进行选项卡切换。

![](img/gdl2_1405.png)

###### 图 14-5. GitHub Copilot 功能的两个示例（来源：GitHub Copilot）

## 文本到图像模型

目前，最先进的图像生成主要由将给定文本提示转换为图像的大型多模态模型主导。文本到图像模型非常有用，因为它们允许用户通过自然语言轻松地操纵生成的图像。这与诸如 StyleGAN 之类的模型形成对比，后者虽然非常令人印象深刻，但没有通过您可以描述要生成的图像的文本界面。

目前可供商业和个人使用的三个重要的文本到图像生成模型是 DALL.E 2、Midjourney 和 Stable Diffusion。

OpenAI 的 DALL.E 2 是一项按需付费服务，可通过 Web 应用程序和 API 获得。Midjourney 通过其 Discord 频道提供基于订阅的文本到图像服务。DALL.E 2 和 Midjourney 都为那些加入平台进行早期实验的用户提供免费积分。

# Midjourney

Midjourney 是用于本书第 II 部分故事插图的服务！

Stable Diffusion 不同，因为它是完全开源的。用于训练模型的模型权重和代码都可以在 GitHub 上找到，因此任何人都可以在自己的硬件上运行该模型。用于训练 Stable Diffusion 的数据集也是开源的。这个名为 LAION-5B 的数据集包含了 58.5 亿个图像文本对，目前是世界上最大的公开可访问的图像文本数据集。

这种方法的一个重要推论是，基线稳定扩散模型可以被构建并适应不同的用例。ControlNet 就是这一点的一个很好的演示，它是一种神经网络结构，允许通过添加额外条件对稳定扩散的输出进行细粒度控制。例如，输出图像可以根据给定输入图像的[Canny 边缘图](https://oreil.ly/8v9Ym)进行条件化，如图 14-6 所示。

![](img/gdl2_1406.png)

###### 图 14-6。使用 Canny 边缘图和 ControlNet 对稳定扩散输出进行条件化（来源：[Lvmin Zhang, ControlNet](https://github.com/lllyasviel/ControlNet)）

ControlNet 包含一个可训练的稳定扩散编码器副本，以及一个完整的稳定扩散模型的锁定副本。这个可训练的编码器的任务是学习如何处理输入条件（例如，Canny 边缘图），而锁定副本保留了原始模型的功能。这样，稳定扩散可以仅使用少量图像对进行微调。零卷积简单地是所有权重和偏置都为零的 1×1 卷积，因此在训练之前，ControlNet 没有任何效果。

![](img/gdl2_1407.png)

###### 图 14-7。ControlNet 架构，可训练的稳定扩散编码器块用蓝色突出显示（来源：[Lvmin Zhang, ControlNet](https://github.com/lllyasviel/ControlNet)）

稳定扩散的另一个优点是，它能够在仅具有 8 GB VRAM 的单个中等大小 GPU 上运行，这使得它可以在边缘设备上运行，而不是通过调用云服务。随着文本到图像服务包含在下游产品中，生成速度变得越来越重要。这也是为什么多模态模型的大小通常趋向于减小的原因之一（参见图 14-3）。

三种模型的示例输出可以在图 14-8 中看到。所有这些模型都非常出色，能够捕捉给定描述的内容和风格。

![](img/gdl2_1408.png)

###### 图 14-8。稳定扩散 v2.1、Midjourney 和 DALL.E 2 对相同提示的输出

今天存在的一些最强大的文本到图像模型的摘要显示在表 14-2 中。

表 14-2。文本到图像模型

| 模型 | 日期 | 开发者 | # 参数 | 开源 |
| --- | --- | --- | --- | --- |
| DALL.E 2 | 2022 年 4 月 | OpenAI | 35 亿 | 否 |
| Imagen | 2022 年 5 月 | 谷歌 | 46 亿 | 否 |
| Parti | 2022 年 6 月 | 谷歌 | 200 亿 | 否 |
| 稳定扩散 | 2022 年 8 月 | Stability AI、CompVis 和 Runway | 8.9 亿 | 是 |
| MUSE | 2023 年 1 月 | 谷歌 | 30 亿 | 否 |

使用文本到图像模型的技巧之一是创建一个提示，既描述您想要生成的图像的内容，又使用鼓励模型生成特定风格或类型图像的关键词。例如，诸如*令人惊叹*或*获奖*之类的形容词通常可以用来提高生成的质量。然而，并不总是同一个提示在不同模型上都能很好地工作——这取决于用于训练模型的特定文本-图像数据集的内容。发现适合特定模型的提示的艺术被称为*提示工程*。

## 其他应用

生成式人工智能正在迅速在各种新领域中找到应用，从强化学习到其他种类的*文本到 X*多模态模型。

例如，2022 年 11 月，Meta 发表了一篇关于 CICERO 的论文，这是一个训练有素的 AI 代理人，用于玩《外交》这个棋盘游戏。在这个游戏中，玩家代表第一次世界大战前欧洲的不同国家，必须与彼此进行谈判和欺骗，以控制整个大陆。对于 AI 代理人来说，这是一个非常复杂的游戏，因为其中有一个沟通元素，玩家必须与其他玩家讨论他们的计划，以获得盟友、协调行动并提出战略目标。为了实现这一点，CICERO 包含一个能够发起对话并回应其他玩家消息的语言模型。至关重要的是，对话与代理人的战略计划一致，这些计划由模型的另一部分生成，以适应不断变化的情景。这包括代理人在与其他玩家交谈时虚张声势，即说服另一个玩家与代理人合作，然后在后续回合中对该玩家采取激进的行动。值得注意的是，在一个匿名的外交联盟中，涉及 40 场比赛，CICERO 的得分超过了人类玩家的平均水平的两倍以上，并且在参与多场比赛的参与者中排名前 10%。这是一个很好的例子，展示了生成式 AI 如何成功地与强化学习相结合。

体现大型语言模型的发展是一个令人兴奋的研究领域，谷歌的 PaLM-E 模型进一步证明了这一点。该模型将强大的语言模型 PaLM 与 Vision Transformer 相结合，将视觉和传感器数据转换为可以与文本指令交错的标记，使机器人能够根据文本提示和来自其他感官模式的持续反馈执行任务。PaLM-E 网站展示了该模型的能力，包括控制机器人根据文本描述排列方块和取物品。

文本到视频模型涉及从文本输入创建视频。这个领域建立在文本到图像建模的概念基础上，还有一个额外的挑战，即融入时间维度。例如，2022 年 9 月，Meta 发布了 Make-A-Video，这是一个生成模型，可以仅通过文本提示作为输入创建一个短视频。该模型还能在两个静态图像之间添加动作，并生成给定输入视频的变体。有趣的是，它仅在配对的文本-图像数据和无监督视频素材上进行训练，而不是直接在文本-视频对上进行训练。无监督的视频数据足以让模型学习世界如何移动；然后它使用文本-图像对学习如何映射文本图像模态，然后将其动画化。Dreamix 模型能够进行视频编辑，根据给定的文本提示转换输入视频，同时保留原始视频的摄像机角度、背景和照明元素。

同样，*文本到 3D*模型将传统的文本到图像方法扩展到第三维。2022 年 9 月，Google 发布了[*DreamFusion*](https://dreamfusion3d.github.io)，这是一个扩散模型，根据输入的文本提示生成 3D 资产。关键是，该模型不需要标记的 3D 资产进行训练。作者使用一个预先训练的 2D 文本到图像模型（Imagen）作为先验，然后训练一个 3D 神经辐射场（NeRF），使其能够在随机角度渲染时产生良好的图像。另一个例子是 OpenAI 的[*Point-E*](https://openai.com/research/point-e)，于 2022 年 12 月发布。Point-E 是一个纯扩散系统，能够根据给定的文本提示生成一个 3D 点云。虽然其输出质量不如 DreamFusion，但这种方法的优势在于比基于 NeRF 的方法快得多——它可以在单个 GPU 上在一到两分钟内产生输出，而不需要多个 GPU 小时。

鉴于文本和音乐之间的相似性，不足为奇的是也有人尝试创建*文本到音乐*模型。Google 于 2023 年 1 月发布的[*MusicLM*](https://oreil.ly/qb7II)是一种语言模型，能够将音乐片段的文本描述（例如“一段由失真吉他伴奏的平静小提琴旋律”）转换为准确反映描述的音频，时长数分钟。它建立在早期工作[*AudioLM*](https://oreil.ly/0EDRY)的基础上，通过添加模型能够由文本提示引导的功能；您可以在 Google 研究网站上找到可听的示例。

# 生成 AI 的未来

在这最后一部分中，我们将探讨强大的生成 AI 系统可能对我们生活的世界产生的潜在影响——在我们的日常生活中、工作场所以及教育领域。我们还将阐明生成 AI 将面临的关键实际和伦理挑战，如果它要成为一个使社会获得显著净正面贡献的无处不在的工具。

## 生成 AI 在日常生活中的应用

毫无疑问，未来生成 AI 将在人们的日常生活中扮演越来越重要的角色，特别是大型语言模型。通过 OpenAI 的[ChatGPT](https://chat.openai.com/chat)，已经可以使用生成 AI 为求职申请生成完美的求职信，为同事生成专业的电子邮件回复，或者在特定主题上生成有趣的社交媒体帖子。这项技术真正是互动的：它能够包含您请求的具体细节，回应反馈，并在某些地方不清楚时提出自己的问题。这种*个人助手*AI 的风格应该是科幻小说的内容，但它并不是——它已经出现了，任何选择使用它的人都可以使用。

这种应用成为主流的后果是什么？最直接的影响可能是书面沟通质量的提高。使用具有用户友好界面的大型语言模型将使人们能够在几秒钟内将一个想法的草图转化为连贯、高质量的段落。电子邮件写作、社交媒体帖子，甚至短格式即时通讯都将因此技术而发生变革。它不仅消除了与拼写、语法和可读性相关的常见障碍，而且直接将我们的思维过程与可用输出联系起来，通常无需参与构建句子的过程。

生成良好文本只是大型语言模型的一个用途。人们将开始使用这些模型进行创意生成、建议和信息检索。我相信我们可以将这视为作为一个物种获取、分享、检索和综合信息能力的第四阶段。我们开始通过获取周围人的信息或亲自前往新地点来获取信息。印刷术的发明使书籍成为传播思想的主要载体。最后，互联网的诞生使我们能够在触摸按钮时即时搜索和检索信息。生成 AI 开启了一个新的信息综合时代，我相信它将取代今天搜索引擎的许多当前用途。

例如，OpenAI 的 GPT 系列模型可以提供定制的假日目的地推荐，如图 14-9 所示，或者如何应对困难情况的建议，或者对一个晦涩概念的详细解释。使用这项技术更像是向朋友询问，而不是在搜索引擎中输入查询，因此，人们迅速涌向这项技术。ChatGPT 是发展最快的技术平台；在推出后的 5 天内获得了 100 万用户。为了对比，Instagram 花了 2.5 个月才达到相同数量的用户，Facebook 花了 10 个月。

![](img/gdl2_1409.png)

###### 图 14-9。来自 GPT-3 的输出，提供定制的假日推荐

## 工作场所中的生成 AI

除了一般用途外，生成 AI 还将在需要创造力的特定工作中找到应用。以下是一些可能受益的职业的非尽头列表：

广告

生成 AI 可以用来创建针对特定人群的个性化广告活动，基于他们的浏览和购买历史。

音乐制作

生成 AI 可以用来创作和制作原创音乐曲目，为无限的可能性提供可能。

建筑学

生成 AI 可以用来设计建筑和结构，考虑因素如风格和布局约束。

时尚设计

生成 AI 可以用来创建独特多样的服装设计，考虑到潮流和穿着者的喜好。

汽车设计

生成 AI 可以用来设计和开发新的车型，并自动找到特定设计的有趣变化。

电影和视频制作

生成 AI 可以用来创建特效和动画，以及为整个场景或故事情节生成对话。

制药研究

生成 AI 可以用来生成新的药物化合物，有助于开发新的治疗方法。

创意写作

生成 AI 可以用来生成书面内容，如小说故事、诗歌、新闻文章等。

游戏设计

生成 AI 可以用来设计和开发新的游戏关卡和内容，创造无限种游戏体验。

数字设计

生成 AI 可以用来创建原创数字艺术和动画，以及设计和开发新的用户界面和网页设计。

人们经常说 AI 对这些领域的工作构成存在威胁，但我并不认为事实就是如此。对我来说，AI 只是这些创意角色工具箱中的另一个工具（尽管是一个非常强大的工具），而不是角色本身的替代品。选择拥抱这项新技术的人会发现他们能够更快地探索新想法，并以以前不可能的方式迭代概念。

## 教育中的生成 AI

我相信最终将受到显著影响的另一个日常生活领域是教育。生成式人工智能挑战了教育的基本公理，这是我们自互联网诞生以来从未见过的。互联网使学生能够即时和明确地检索信息，使纯粹测试记忆和回忆的考试显得过时和无关紧要。这促使了一种以测试学生能够以新颖方式综合思想为重点的方法转变，而不仅仅是测试事实知识。

我相信生成式人工智能将在教育领域引起另一场变革性转变，需要重新评估和调整当前的教学方法和评估标准。如果每个学生现在都可以在口袋里拥有一个可以对问题生成新颖回答的论文写作机器，那么基于论文的课程的目的是什么？

许多人呼吁禁止使用这种人工智能工具，就像禁止剽窃一样。然而，情况并不那么简单，因为检测人工智能生成的文本比检测剽窃要困难得多，甚至更难以无疑地证明。此外，学生可以使用人工智能工具为论文生成一个骨架草稿，然后根据需要添加额外细节或更新事实不正确的信息。在这种情况下，是学生的原创作品，还是人工智能的？

显然，这些是需要解决的重大问题，以便教育和认证保持其完整性。在我看来，抵制人工智能工具在教育中的传播是毫无意义的-任何这样的方法注定会失败，因为它们将在日常生活中变得如此普遍，以至于试图限制它们的使用将是徒劳的。相反，我们需要找到方法来拥抱这项技术，并询问如何设计*开放式人工智能*课程，就像我们允许*开卷考试*课程一样，并鼓励学生使用互联网和人工智能工具公开研究材料。

生成式人工智能在辅助学习过程本身方面的潜力也是巨大且深刻的。一个由人工智能驱动的导师可以帮助学生学习新主题（如图 14-10 所示），克服误解，或生成完全个性化的学习计划。从生成的虚构中过滤真相的挑战与我们目前在互联网上可用信息所面临的挑战并无二致，这是一个需要跨学科进一步关注的生活技能。

![](img/gdl2_1410.png)

###### 图 14-10。GPT-3 的输出-展示了大型语言模型如何用于学习的示例

生成式人工智能可以是一个非常强大的工具，可以在那些有机会接触优秀教师和最佳学习材料的人与那些没有这种机会的人之间拉平竞争场。我对这一领域的进展感到兴奋，因为我相信它可以释放全球范围内的巨大潜力。

## 生成式人工智能的伦理和挑战

尽管在生成式人工智能领域取得了令人难以置信的进展，但仍然有许多挑战需要克服。其中一些挑战是实际的，另一些是伦理的。

例如，大型语言模型的一个主要批评是，当询问一个陌生或矛盾的主题时，它们很容易生成错误信息，如图 14-4 所示。这种危险在于很难知道生成的回应中包含的信息是否真实准确。即使您要求 LLM 解释其推理或引用来源，它可能会编造参考文献或说出一系列逻辑上不相连的陈述。这不是一个容易解决的问题，因为 LLM 只是一组权重，准确捕捉给定一组输入标记时最可能的下一个词-它没有可以用作参考的*真实*信息库。

解决这个问题的一个潜在方案是为大型语言模型提供调用结构化工具的能力，如计算器、代码编译器和在线信息源，用于需要精确执行或事实的任务。例如，图 14-11 展示了 Meta 于 2023 年 2 月发布的名为*Toolformer*的模型的输出。⁴

![](img/gdl2_1411.png)

###### 图 14-11\. Toolformer 能够自主调用不同的 API 以在必要时获取精确信息的示例（来源：[Schick 等人，2023](https://arxiv.org/pdf/2302.04761.pdf)）

Toolformer 能够明确调用 API 以获取信息，作为其生成式响应的一部分。例如，它可能使用维基百科 API 来检索有关特定人物的信息，而不是依赖于这些信息被嵌入在其模型权重中。这种方法特别适用于精确的数学运算，其中 Toolformer 可以说明它想要输入计算器 API 的哪些操作，而不是试图以有用的方式自动生成答案。

生成式 AI 的另一个突出的伦理关注点在于，大公司使用从网络上抓取的大量数据来训练他们的模型，而原始创作者并没有明确同意这样做。通常这些数据甚至没有公开发布，因此无法知道您的数据是否被用来训练大型语言模型或多模态文本到图像模型。显然，这是一个合理的担忧，特别是对于艺术家来说，他们可能会认为这是对他们的艺术作品的使用，而他们并没有得到任何版税或佣金。此外，艺术家的名字可能被用作提示，以生成更多风格类似于原作的艺术作品，从而降低内容的独特性并将风格商品化。

这个问题的一个解决方案是由 Stability AI 开创的，他们的多模态模型 Stable Diffusion 是在开源 LAION-5B 数据集的一个子集上进行训练的。他们还推出了网站[*Have I Been Trained?*](https://haveibeentrained.com)，任何人都可以在训练数据集中搜索特定的图像或文本段落，并选择退出未来的模型训练过程。这将控制权交还给原始创作者，并确保用于创建强大工具如此的数据具有透明度。然而，这种做法并不普遍，许多商业可用的生成式 AI 模型并不公开其数据集或模型权重，也不提供任何选择退出训练过程的选项。

总之，虽然生成式 AI 是一个强大的工具，可用于日常生活、工作场所和教育领域的沟通、生产力和学习，但其广泛使用既有优势也有劣势。重要的是要意识到使用生成式 AI 模型的输出的潜在风险，并始终确保负责任地使用它。尽管如此，我对生成式 AI 的未来充满乐观，并迫不及待地想看到企业和人们如何适应这项新的令人兴奋的技术。

# 最后思考

在本书中，我们通过过去十年的生成建模研究之旅，从 VAEs、GANs、自回归模型、正规化流模型、基于能量的模型和扩散模型的基本思想开始，建立在这些基础上，了解 VQ-GAN、Transformers、世界模型和多模态模型等最新技术如何推动生成模型在各种任务中所能实现的边界。

我相信，在未来，生成建模可能是一种更深层次的人工智能的关键，超越任何特定任务，使机器能够有机地制定自己的奖励、策略，甚至在环境中产生意识。我的信念与 Karl Friston 最初开创的“主动推理”原则密切相关。主动推理背后的理论可以轻松填满另一本完整的书籍——并且确实填满了，就像 Thomas Parr 等人在《主动推理：心智、大脑和行为中的自由能量原则》（麻省理工学院出版社）中所做的那样，我强烈推荐——所以我只会在这里尝试简短解释。

作为婴儿，我们不断地探索周围环境，建立起可能未来的心智模型，看似没有明显目的，只是为了更深入地理解世界。我们接收到的数据没有标签——从出生那一刻起就不断轰击我们感官的光和声波似乎是随机的。即使有人指着一个苹果说“苹果”，我们年幼的大脑也没有理由将这两个输入联系起来，学习到光线进入眼睛的方式与声波进入耳朵的方式有某种关联。没有声音和图像的训练集，没有气味和味道的训练集，也没有行为和奖励的训练集；只有一个无休止的极其嘈杂的数据流。

然而，此刻你正在阅读这句话，也许正在享受嘈杂咖啡馆里一杯咖啡的味道。你专注于将视网膜上的微小部分的光缺失转化为一系列抽象概念，这些概念单独来看几乎没有意义，但结合起来，会在你的脑海中引发一波平行的表征——图像、情感、想法、信念和潜在行动都涌入你的意识，等待你的认知。对于你的婴儿大脑来说基本无意义的同样嘈杂的数据流现在不再那么嘈杂。一切对你来说都是有意义的。你在任何地方都看到结构。你对日常生活的物理现象从不感到惊讶。世界是因为你的大脑决定它应该是这样。在这个意义上，你的大脑是一个极其复杂的生成模型，具有关注输入数据特定部分、在神经通路的潜在空间内形成概念表征、并随时间处理序列数据的能力。

主动推理是一个基于这一思想的框架，用来解释大脑如何处理和整合感官信息以做出决策和行动。它指出，一个生物体对其所处世界有一个生成模型，并利用这个模型对未来事件进行预测。为了减少模型与现实之间的差异所带来的惊讶，生物体相应地调整其行动和信念。Friston 的关键思想是，行动和感知优化可以被看作是同一个硬币的两面，两者都旨在最小化一个称为“自由能量”的量。

这个框架的核心是一个环境的生成模型（在大脑中捕获），它不断地与现实进行比较。关键是，大脑不是事件的被动观察者。在人类中，它连接着一条脖子和一套腿，可以将其核心输入传感器相对于输入数据源放置在多种位置。因此，可能未来的生成序列不仅取决于其对环境物理的理解，还取决于其对*自身*及其行为方式的理解。行动和感知的这种反馈循环对我来说非常有趣，我相信我们只是触及了具有行动推理原则的具体环境中能够采取行动的具体生成模型的潜力表面。

这是我认为将在未来十年继续推动生成建模走向聚光灯下的核心理念之一，作为解锁人工通用智能的关键之一。

在这个基础上，我鼓励您继续从在线和其他书籍中提供的优质材料中学习更多关于生成模型的知识。感谢您抽出时间阅读本书至此，希望您和我一样享受阅读的乐趣！

¹ Hugo Touvron 等人，“LLaMA: 开放高效的基础语言模型”，2023 年 2 月 27 日，[*https://arxiv.org/abs/2302.13971*](https://arxiv.org/abs/2302.13971)。

² Mark Chen 等人，“评估在代码上训练的大型语言模型”，2021 年 7 月 7 日，[*https://arxiv.org/abs/2107.03374*](https://arxiv.org/abs/2107.03374)。

³ 张旅民和 Maneesh Agrawala，“向文本到图像扩散模型添加条件控制”，2023 年 2 月 10 日，[*https://arxiv.org/abs/2302.05543*](https://arxiv.org/abs/2302.05543)。

⁴ Timo Schick 等人，“Toolformer: 语言模型可以自学使用工具”，2023 年 2 月 9 日，[*https://arxiv.org/abs/2302.04761*](https://arxiv.org/abs/2302.04761)。
