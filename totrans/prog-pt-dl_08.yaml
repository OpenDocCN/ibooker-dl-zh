- en: Chapter 8\. PyTorch in Production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章\. PyTorch在生产中
- en: Now that you’ve learned how to use PyTorch to classify images, text, and sound,
    the next step is to look at how to deploy PyTorch applications in production.
    In this chapter, we create applications that run inference on PyTorch models over
    HTTP and gRPC. We then package those applications into Docker containers and deploy
    them to a Kubernetes cluster running on Google Cloud.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经学会了如何使用PyTorch对图像、文本和声音进行分类，下一步是看看如何将PyTorch应用程序部署到生产环境中。在本章中，我们创建应用程序，通过HTTP和gRPC在PyTorch模型上运行推断。然后，我们将这些应用程序打包到Docker容器中，并将它们部署到在Google
    Cloud上运行的Kubernetes集群中。
- en: In the second half, we look at TorchScript, a new technology introduced in PyTorch
    1.0 that allows us to use just-in-time (JIT) tracing to produce optimized models
    that can be run from C++. We also have a brief look at how to compress models
    with quantization. First up, let’s look at model serving.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在下半部分，我们将看一下TorchScript，这是PyTorch 1.0中引入的一项新技术，它允许我们使用即时（JIT）跟踪来生成优化的模型，这些模型可以从C++中运行。我们还简要介绍了如何使用量化压缩模型。首先，让我们看一下模型服务。
- en: Model Serving
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型服务
- en: We’ve spent the last six chapters building models in PyTorch, but building a
    model is only part of building a deep learning application. After all, a model
    may have amazing accuracy (or other relevant metric), but if it never makes any
    predictions, is it worth anything? What we want is an easy way to package our
    models so they can respond to requests (either over the web or other means, as
    we’ll see) and can be run in production with the minimum of effort.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的六章中，我们一直在PyTorch中构建模型，但构建模型只是构建深度学习应用程序的一部分。毕竟，一个模型可能具有惊人的准确性（或其他相关指标），但如果它从不进行任何预测，那么它是否有价值呢？我们希望有一种简单的方法来打包我们的模型，以便它们可以响应请求（无论是通过网络还是其他方式，我们将看到），并且可以在生产环境中运行，而不需要太多的努力。
- en: Thankfully, Python allows us to get a web service up and running quickly with
    the Flask framework. In this section, we build a simple service that loads our
    ResNet-based *cat or fish* model, accepts requests that include an image URL,
    and returns a JSON response that indicates whether the image contains a cat or
    a fish.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Python允许我们使用Flask框架快速启动Web服务。在本节中，我们构建一个简单的服务，加载我们基于ResNet的*猫或鱼*模型，接受包含图像URL的请求，并返回一个JSON响应，指示图像是否包含猫或鱼。
- en: Note
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: What happens if we send the model a picture of a dog? The model will tell you
    that it is either a fish or a cat. It has no concept of anything but the available
    choices and will always pick one. Some deep learning practitioners add an extra
    class, `Unknown`, during training and throw in labeled examples that aren’t any
    of the required classes. This works to a certain extent, but it essentially tries
    to make the neural net learn *everything that isn’t a cat or fish*, which is difficult
    for you and me to express, let alone a series of matrix calculations! Another
    option is to look at the probability output generated by the final `softmax`.
    If the model is producing a prediction that is roughly 50/50 cat/fish or spread
    out across your classes, then maybe suggest `Unknown`.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们向模型发送一张狗的图片会发生什么？模型会告诉您它是鱼还是猫。它没有其他选择之外的概念，总是会选择一个。一些深度学习从业者在训练过程中添加一个额外的类别`Unknown`，并加入一些不属于所需类别的标记示例。这在一定程度上有效，但实质上是试图让神经网络学习*不是猫或鱼的所有内容*，这对您和我来说都很难表达，更不用说一系列矩阵计算了！另一个选择是查看最终`softmax`生成的概率输出。如果模型产生的预测大致是50/50的猫/鱼或分布在您的类别中，那么可能建议`Unknown`。
- en: Building a Flask Service
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建Flask服务
- en: 'Let’s get a web service-enabled version of our model up and running. *Flask*
    is a popular framework for creating web services with Python, and we’ll be using
    it as a base throughout this chapter. Install the Flask library with either `pip`
    or `conda`:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启动一个启用Web服务的模型版本。*Flask*是一个用Python创建Web服务的流行框架，我们将在本章中一直使用它作为基础。使用`pip`或`conda`安装Flask库：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create a new directory called *catfish* and copy your model definition inside
    as *model.py*:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为*catfish*的新目录，并将您的模型定义复制到其中作为*model.py*：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Note that we do not specify a pretrained model here, because we will be loading
    our saved weights in the Flask server startup process. Then create another Python
    script, *catfish_server.py*, where we will start our web service:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在这里没有指定预训练模型，因为我们将在Flask服务器启动过程中加载我们保存的权重。然后创建另一个Python脚本*catfish_server.py*，在其中我们将启动我们的Web服务：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can start up a web server on the command line by setting the `CATFISH_HOST`
    and `CATFISH_PORT` environment variables:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过设置`CATFISH_HOST`和`CATFISH_PORT`环境变量在命令行上启动Web服务器：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you point your web browser at [*http://127.0.0.1:8080*](http://127.0.0.1:8080),
    you should get a `status: "ok"` JSON response as shown in [Figure 8-1](#ok-response-from-service).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '如果您将您的Web浏览器指向[*http://127.0.0.1:8080*](http://127.0.0.1:8080)，您应该会得到一个`status:
    "ok"`的JSON响应，如[图8-1](#ok-response-from-service)所示。'
- en: '![OK Response from CATFISH](assets/ppdl_0801.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![CATFISH的OK响应](assets/ppdl_0801.png)'
- en: Figure 8-1\. OK response from CATFISH
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. CATFISH的OK响应
- en: Caution
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We discuss this in more detail later in this chapter, but don’t deploy a Flask
    service directly to production because the built-in server is not adequate for
    production usage.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面更详细地讨论这一点，但不要直接将Flask服务部署到生产环境，因为内置服务器不适合生产使用。
- en: To make a prediction, find an image URL and send it as a `GET` request with
    the `image_url` parameter to the `/predict` path. You should see a JSON response
    showing the URL and the predicted class, as shown in [Figure 8-2](#prediction-from-catfish).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行预测，找到一个图像URL，并将其作为`GET`请求发送到`/predict`路径，其中包括`image_url`参数。您应该看到一个JSON响应，显示URL和预测的类别，如[图8-2](#prediction-from-catfish)所示。
- en: '![Prediction from CATFISH](assets/ppdl_0802.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![CATFISH的预测](assets/ppdl_0802.png)'
- en: Figure 8-2\. Prediction from CATFISH
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2\. CATFISH的预测
- en: The magic in Flask is in the `@app.route()` annotations. These allow us to attach
    normal Python functions that will be run when a user hits a particular endpoint.
    In our `predict()` method, we pull out the `img_url` parameter from either a `GET`
    or `POST` HTTP request, open that URL as a PIL image, and push it through a simple
    `torchvision` transform pipeline to resize it and turn the image into a tensor.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Flask 中的魔法在于`@app.route()`注解。这使我们能够附加普通的 Python 函数，当用户访问特定端点时将运行这些函数。在我们的`predict()`方法中，我们从`GET`或`POST`HTTP请求中提取`img_url`参数，将该
    URL 打开为一个 PIL 图像，并通过一个简单的`torchvision`转换管道将其调整大小并将图像转换为张量。
- en: This gives us a tensor of shape `[3,224,224]`, but because of the way our model
    works, we need to turn it into a batch of size 1—that is, `[1,3,224,224]`. So
    we use `unsqueeze()` again to expand our tensor by inserting a new empty axis
    in front of the existing dimensions. We can then pass it through the model as
    usual, which gives us our prediction tensor. As we have done previously, we use
    `torch.argmax()` to find the element of the tensor with the highest value and
    use that to index into the `CatfishClasses` array. Finally, we return a JSON response
    with the name of the class and the image URL we performed the prediction on.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个形状为`[3,224,224]`的张量，但由于我们模型的工作方式，我们需要将其转换为大小为 1 的批次，即`[1,3,224,224]`。因此，我们再次使用`unsqueeze()`来通过在现有维度前插入一个新的空轴来扩展我们的张量。然后我们可以像往常一样将其传递给模型，这会给我们预测张量。与以前一样，我们使用`torch.argmax()`来找到张量中具有最高值的元素，并用它来索引`CatfishClasses`数组。最后，我们返回一个
    JSON 响应，其中包含类的名称和我们执行预测的图像 URL。
- en: If you experiment with the server at this point, you might be a little disappointed
    with the classification performance. Didn’t we spend a lot of time training it?
    Yes, we did, but in re-creating the model, we have simply created a set of layers
    with the standard PyTorch initialization! So no wonder it’s not good. Let’s flesh
    out `load_model()` to load in our parameters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这一点上尝试服务器，你可能会对分类性能感到有些失望。我们不是花了很多时间训练它吗？是的，我们是，但是在重新创建模型时，我们只是创建了一组具有标准
    PyTorch 初始化的层！所以难怪它不好。让我们完善`load_model()`以加载我们的参数。
- en: Note
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We’re returning only the predicted class here, not the complete set of predictions
    across all classes. You could certainly return the prediction tensor as well,
    though be aware that the complete tensor output makes it a little easier for attackers
    to build up a replica of your model through more *information leakage*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们只返回预测的类，而不是所有类别的完整预测集。当然你也可以返回预测张量，但要注意完整的张量输出会使攻击者更容易通过更多的*信息泄漏*来构建模型的副本。
- en: Setting Up the Model Parameters
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置模型参数
- en: In [Chapter 2](ch02.html#image-classification-with-pytorch), we talked about
    the two ways to save a model after training, either by writing the entire model
    to disk with `torch.save()` or by saving the `state_dict()` of all the weights
    and biases of the model (but not the structure). For our production-based service,
    we need to load in an already-trained model, so what should we use?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 2 章](ch02.html#image-classification-with-pytorch)中，我们讨论了训练后保存模型的两种方法，一种是使用`torch.save()`将整个模型写入磁盘，另一种是保存模型的所有权重和偏置的`state_dict()`（但不包括结构）。对于我们基于生产的服务，我们需要加载一个已经训练好的模型，那么我们应该使用什么呢？
- en: In my opinion, you should go for the `state_dict` approach. Saving the entire
    model is an attractive option, but you will become incredibly sensitive to any
    changes in the model structure or even the directory structure of the training
    setup. That’s likely to cause a problem with loading it up in a separate service
    that runs elsewhere. If we’re making a migration to a slightly different layout,
    we’d like to not have to rework everything.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，你应该选择`state_dict`方法。保存整个模型是一个吸引人的选择，但是你将变得对模型结构的任何更改甚至训练设置的目录结构变得非常敏感。这很可能会导致在其他地方运行的单独服务中加载它时出现问题。如果我们要进行稍微不同布局的迁移，我们希望不必重新制作所有内容。
- en: 'We’d also be better off not hardcoding the filename of the saved `state_dicts()`
    so we can decouple model updates from our service. This means we can restart the
    service with a new model or revert to an earlier model with ease. We pass in the
    filename as a parameter—but where should it point? For the moment, assume that
    we can set an environment variable called `CATFISH_MODEL_LOCATION`, and use that
    in `load_model()`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最好不要将保存的`state_dicts()`的文件名硬编码，这样我们可以将模型更新与服务解耦。这意味着我们可以轻松地使用新模型重新启动服务，或者回滚到早期的模型。我们将文件名作为参数传递，但应该指向哪里呢？暂时假设我们可以设置一个名为`CATFISH_MODEL_LOCATION`的环境变量，并在`load_model()`中使用它：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, copy in one of the model weight files you saved in [Chapter 4](ch04.html#transfer-learning-and-other-tricks)
    into the directory and set `CATFISH_MODEL_LOCATION` to point to that file:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将你在[第 4 章](ch04.html#transfer-learning-and-other-tricks)中保存的模型权重文件之一复制到目录中，并将`CATFISH_MODEL_LOCATION`设置为指向该文件：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Restart the server, and you should see that the service is a lot more accurate!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 重新启动服务器，你应该看到服务的准确性有了很大提升！
- en: We now have a working minimal web service (you’d probably want a little more
    error handling, but I’m leaving that as an exercise for you!). But how do we get
    that running on a server in, say, AWS or Google Cloud? Or just on somebody else’s
    laptop? After all, we have installed a bunch of libraries to get this working.
    We can use Docker to package everything up into one *container* that can be installed
    in any Linux (or Windows, with the new Windows Subsystem for Linux!) environment
    in seconds.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个工作的最小 Web 服务（你可能希望有更多的错误处理，但我把这留给你来练习！）。但是我们如何在服务器上运行它，比如在 AWS 或 Google
    Cloud 上？或者只是在别人的笔记本电脑上？毕竟，我们安装了一堆库来使其工作。我们可以使用 Docker 将所有内容打包到一个*容器*中，该容器可以在任何
    Linux（或 Windows，使用新的 Windows Subsystem for Linux！）环境中安装，只需几秒钟。
- en: Building the Docker Container
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建 Docker 容器
- en: Docker has become one of the de facto standards for application packaging in
    the past few years. Cutting-edge cluster environments such as Kubernetes have
    Docker at their core for deploying applications (as you’ll see later in the chapter),
    and it’s even made large inroads in enterprises as well.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，Docker已成为应用程序打包的事实标准之一。尖端的集群环境，如Kubernetes，将Docker作为部署应用程序的核心（您将在本章后面看到），它甚至在企业中也取得了很大的进展。
- en: 'If you haven’t come across Docker before, here’s a quick explanation: it’s
    modeled on the idea of shipping containers. You specify a bundle of files (typically,
    using a Dockerfile) that Docker uses to build an *image*, and Docker then runs
    that image in a *container*, which is an isolated process on your system that
    can see only the files you’ve specified and the programs you’ve told it to run.
    You can then share the Dockerfile so people can build their own images, but a
    more common approach is to push the created image to a *registry*, which is a
    list of Docker images that can be downloaded by anybody with access. These registries
    can be public or private; the Docker corporation runs [Docker Hub](https://hub.docker.com),
    which is a public registry that contains over 100,000 Docker images, but many
    companies run private registries for internal use.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您之前没有接触过Docker，这里有一个简单的解释：它的模型是基于集装箱的概念。您指定一组文件（通常使用Dockerfile），Docker用这些文件构建一个*镜像*，然后在*容器*中运行该镜像，容器是您系统上的一个隔离进程，只能看到您指定的文件和您告诉它运行的程序。然后您可以共享Dockerfile，以便其他人构建自己的镜像，但更常见的方法是将创建的镜像推送到*注册表*，这是一个包含可以被任何有访问权限的人下载的Docker镜像列表。这些注册表可以是公共的或私有的；Docker公司运行[Docker
    Hub](https://hub.docker.com)，这是一个包含超过100,000个Docker镜像的公共注册表，但许多公司也运行私有注册表供内部使用。
- en: 'What we need to do is write our own Dockerfile. This might sound a little overwhelming.
    What do we have to tell Docker to install? Our code? PyTorch? Conda? Python? Linux
    itself? Thankfully, Dockerfiles can inherit from other images, so we could, for
    example, inherit from the standard Ubuntu image and install Python, PyTorch, and
    everything else from there. But we can do better! A selection of Conda images
    is available to choose from that will give us a base Linux, Python, and Anaconda
    installation to build on. Here’s an example Dockerfile that can be used to build
    a container image for our service:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要编写自己的Dockerfile。这可能听起来有点令人不知所措。我们需要告诉Docker安装什么？我们的代码？PyTorch？Conda？Python？Linux本身？幸运的是，Dockerfile可以继承自其他镜像，因此我们可以，例如，继承标准Ubuntu镜像，并从那里安装Python、PyTorch和其他所有内容。但我们可以做得更好！可以选择一些Conda镜像，这些镜像将为我们提供一个基本的Linux、Python和Anaconda安装基础。以下是一个示例Dockerfile，可用于构建我们服务的容器镜像：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: A few things are happening here, so let’s take a look. The first line in almost
    all Dockerfiles will be `FROM`, which lists the Docker image that this file inherits
    from. In this case, it’s `continuumio/miniconda3:latest`. The first part of this
    string is the image name. Images are also versioned, so everything after the colon
    is a *tag* indicating which version of the image we want to download. There’s
    also a magic tag `latest`, which we use here to download the latest version of
    the image we’re after. You may want to pin your service to a particular version
    so you aren’t surprised by possible later changes in the base image causing issues
    in yours.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了一些事情，让我们来看看。几乎所有Dockerfile中的第一行都是`FROM`，列出了此文件继承的Docker镜像。在这种情况下，它是`continuumio/miniconda3:latest`。这个字符串的第一部分是镜像名称。镜像也有版本，所以冒号后面的所有内容都是一个*标签*，指示我们想要下载哪个版本的镜像。还有一个魔术标签`latest`，我们在这里使用它来下载我们想要的镜像的最新版本。您可能希望将服务固定在特定版本上，以免基础镜像可能导致您的问题后续更改。
- en: '`ARG` and `ENV` deal with variables. `ARG` specifies a variable that is supplied
    to Docker when we’re building the image, and then the variable can be used later
    in the Dockerfile. `ENV` allows you to specify environment variables that will
    be injected into the container at runtime. In our container, we use `ARG` to specify,
    for example, that port is a configurable option, and then use `ENV` to ensure
    that the configuration is available to our script at startup.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`ARG`和`ENV`处理变量。`ARG`指定在构建镜像时由Docker提供的变量，然后该变量可以在Dockerfile中稍后使用。`ENV`允许您指定在运行时将注入容器的环境变量。在我们的容器中，我们使用`ARG`来指定端口是可配置选项，然后使用`ENV`确保配置在启动时对我们的脚本可用。'
- en: Having done that, `RUN` and `COPY` allow us to manipulate the image we’ve inherited
    from. `RUN` runs actual commands within the image, and any changes are saved as
    a new *layer* of the image on top of the base layer. `COPY` takes something from
    the Docker build context (typically, any files from the directory that the build
    command has issued or any subdirectories) and inserts it into a location on the
    image’s filesystem. Having created `/app` by using `RUN`, we then use `COPY` to
    move our code and model parameters into the image.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了这些操作后，`RUN`和`COPY`允许我们操作继承的镜像。`RUN`在镜像内部运行实际命令，任何更改都会保存为镜像的新*层*，叠加在基础层之上。`COPY`从Docker构建上下文中获取内容（通常是构建命令发出的目录中的任何文件或任何子目录），并将其插入到镜像文件系统的某个位置。通过使用`RUN`创建了`/app`后，我们使用`COPY`将代码和模型参数移动到镜像中。
- en: '`EXPOSE` indicates to Docker which port should be mapped to the outside world.
    By default, no ports are opened, so we add one here, taken from the `ARG` command
    earlier in the file. Finally, `ENTRYPOINT` is the default command that is run
    when a container is created. Here we’ve specified a script, but we haven’t made
    it yet! Let’s do that before we build our Docker image:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`EXPOSE`指示Docker应将哪个端口映射到外部世界。默认情况下，没有打开任何端口，所以我们在这里添加一个，从文件中之前的`ARG`命令中获取。最后，`ENTRYPOINT`是创建容器时运行的默认命令。在这里，我们指定了一个脚本，但我们还没有创建它！在构建Docker镜像之前，让我们先做这个：'
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Wait, what’s happening here? Where did `waitress` come from? The issue is that
    when we were running our Flask-based server before it used a simple web server
    that is meant only for debugging purposes. If we want to put this into production,
    we need a production-grade web server. Waitress fulfills that requirement. We
    don’t need to go into much detail about it, but you can check out the [Waitress
    documentation](https://oreil.ly/x96Ir) if you want to learn more.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，这里发生了什么？`waitress`是从哪里来的？问题在于，在之前运行基于Flask的服务器时，它使用了一个仅用于调试目的的简单Web服务器。如果我们想将其投入生产，我们需要一个适用于生产的Web服务器。Waitress满足了这一要求。我们不需要详细讨论它，但如果您想了解更多信息，可以查看[Waitress文档](https://oreil.ly/x96Ir)。
- en: 'With all that set up, we can finally create our image by using `docker build`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 设置好这一切后，我们最终可以使用`docker build`来创建我们的镜像：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can make sure that the image is available on our system by using `docker
    images`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`docker images`来确保镜像在我们的系统上可用：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Running our model prediction service can then be done using `docker run`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以使用`docker run`来运行我们的模型预测服务：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We also use the `-p` argument to map the container’s port 5000 to our computer’s
    port 5000\. You should be able to go back to *http://localhost:5000/predict* just
    as before.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用`-p`参数将容器的端口5000映射到我们计算机的端口5000。您应该能够像以前一样返回到*http://localhost:5000/predict*。
- en: One thing you might notice when running `docker images` locally is that our
    Docker image is over 4GB in size! That’s quite big, considering we didn’t write
    much code. Let’s look at ways to make that image smaller and make our image more
    practical for deployment at the same time.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当在本地运行`docker images`时，您可能会注意到我们的Docker镜像超过4GB！考虑到我们没有写太多代码，这相当大。让我们看看如何使镜像更小，同时使我们的镜像更适合部署。
- en: Local Versus Cloud Storage
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地与云存储
- en: 'Obviously, the easiest answer to where to store our saved model parameters
    is on the local filesystem, whether that’s on our computer or the filesystem within
    a Docker container. But there are a couple of problems with this. First, the model
    is hardcoded into the image. Also, it’s quite possible that after the image is
    built and put into production, we need to update the model. With our current Dockerfile,
    we have to completely rebuild the image, even if the model’s structure hasn’t
    changed! Second, most of the size of our images comes from the size of the parameter
    file. You may not have noticed that they tend to be quite large! Try this out
    for size:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，存储我们保存的模型参数的最简单方法是在本地文件系统上，无论是在我们的计算机上还是在Docker容器内的文件系统上。但是这样做有几个问题。首先，模型被硬编码到镜像中。此外，很可能在构建镜像并投入生产后，我们需要更新模型。使用我们当前的Dockerfile，即使模型的结构没有改变，我们也必须完全重建镜像！其次，我们镜像的大部分大小来自参数文件的大小。您可能没有注意到它们往往相当大！试试看：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If we add these models to the filesystem on every build, our Docker images will
    likely be quite large, which makes pushing and pulling slower. What I suggest
    is local filesystems or Docker volume-mapped containers if you’re running on-premises,
    but if you’re doing a cloud deployment, which we are leading up to, it makes sense
    to take advantage of the cloud. Model parameter files can be uploaded to Azure
    Blob Storage, Amazon Simple Storage Service (Amazon S3), or Google Cloud Storage
    and be pulled in at startup.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在每次构建时将这些模型添加到文件系统中，我们的Docker镜像可能会相当大，这会使推送和拉取变慢。我建议的是如果您在本地运行，可以使用本地文件系统或Docker卷映射容器，但如果您正在进行云部署，那么可以利用云的优势。模型参数文件可以上传到Azure
    Blob Storage、Amazon Simple Storage Service（Amazon S3）或Google Cloud Storage，并在启动时拉取。
- en: 'We can rewrite our `load_model()` function to download the parameter file at
    startup:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重写我们的`load_model()`函数在启动时下载参数文件：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'There are, of course, many ways of downloading files with Python; Flask even
    comes with the `requests` module that would easily download the file. A potential
    issue, though, is that many approaches download the entire file into memory before
    writing it to disk. Most of the time, that makes sense, but when downloading model
    parameter files, they could get into the gigabyte range. So in this new version
    of `load_model()`, we use `urlopen()` and `copyfileobj()` to carry out the copying,
    and `NamedTemporaryFile()` to give us a destination that can be deleted at the
    end of the block, as by that point, we’ve already loaded the parameters in, and
    thus no longer need the file! This allows us to simplify our Dockerfile:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，有许多种使用Python下载文件的方法；Flask甚至带有`requests`模块，可以轻松下载文件。然而，一个潜在的问题是，许多方法在将文件写入磁盘之前会将整个文件下载到内存中。大多数情况下，这是有道理的，但是当下载模型参数文件时，它们可能会达到几十GB。因此，在这个新版本的`load_model()`中，我们使用`urlopen()`和`copyfileobj()`来执行复制操作，并使用`NamedTemporaryFile()`来给我们一个在块结束时可以删除的目标，因为在那时，我们已经加载了参数，因此不再需要文件！这使我们能够简化我们的Dockerfile：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'When we run this with `docker run`, we pass in the environment variable:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用`docker run`运行时，我们传入环境变量：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The service now pulls the parameters from the URL, and the Docker image is probably
    around 600MB–700MB smaller than the original one.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 该服务现在从URL中提取参数，并且Docker镜像可能比原始镜像小约600MB-700MB。
- en: Note
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In this example, we assume that the model parameter file is located at a publicly
    accessible location. If you are deploying a model service, you likely won’t be
    in that situation and will instead be pulling from a cloud storage layer like
    Amazon S3, Google Cloud Storage, or Azure Blob Storage. You’ll have to use the
    respective provider’s APIs to download the file and obtain credentials to gain
    access to it, both of which we don’t discuss here.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们假设模型参数文件位于一个公开可访问的位置。如果您部署一个模型服务，您可能不会处于这种情况，而是会从云存储层（如Amazon S3、Google
    Cloud Storage或Azure Blob Storage）中拉取。您将需要使用各自提供商的API来下载文件并获取凭据以访问它，这两点我们在这里不讨论。
- en: We now have a model service that’s capable of talking over HTTP with JSON. Now
    we need to make sure that we can monitor it while it makes predictions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个能够通过HTTP与JSON进行通信的模型服务。现在我们需要确保在它进行预测时能够监控它。
- en: Logging and Telemetry
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志和遥测
- en: One thing that we don’t have in our current service is any concept of logging.
    And although the service is incredibly simple and perhaps doesn’t need copious
    logging (except in the case of catching our error states), it would be useful,
    if not essential, for us to keep track of what’s actually being predicted. At
    some point, we’re going to want to evaluate the model; how can we do that without
    production data?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的服务中没有的一件事是任何日志记录的概念。虽然服务非常简单，也许不需要大量的日志记录（除非在捕获错误状态时），但对于我们来说，跟踪实际预测的内容是有用的，如果不是必不可少的。在某个时候，我们将想要评估模型；如果没有生产数据，我们该如何做呢？
- en: 'Let’s assume that we have a method `send_to_log()` that takes a Python `dict`
    and sends it elsewhere (perhaps, say, into an Apache Kafka cluster that backs
    up onto cloud storage). We could send appropriate information through this method
    every time we make a prediction:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个名为`send_to_log()`的方法，它接受一个Python `dict`并将其发送到其他地方（也许是一个备份到云存储的Apache
    Kafka集群）。每次进行预测时，我们可以通过这种方法发送适当的信息：
- en: '[PRE15]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With a few additions to calculate how long a prediction takes, on every request,
    this method now sends off a message to a logger or an external resource, providing
    important details such as the image URL, the predicted class, the actual prediction
    tensor, and even the complete image tensor just in case the supplied URL is transient.
    We also include a generated universally unique identifier (UUID), so that this
    prediction can always be uniquely referenced at a later time, perhaps if its predicted
    class needs to be corrected. In an actual deployment, you’d include things like
    `user_id`s and such so that downstream systems can provide a facility for users
    to indicate whether the prediction was correct or incorrect, sneakily generating
    more training data for further training iterations of the model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对每个请求计算预测所需时间的几个补充，该方法现在会向记录器或外部资源发送消息，提供重要细节，如图像URL、预测类别、实际预测张量，甚至完整的图像张量，以防所提供的URL是瞬态的。我们还包括一个生成的通用唯一标识符（UUID），以便以后始终可以唯一引用此预测，也许如果其预测类别需要更正。在实际部署中，您将包括`user_id`等内容，以便下游系统可以提供一个设施，让用户指示预测是正确还是错误，偷偷地生成更多用于模型进一步训练迭代的训练数据。
- en: And with that, we’re ready to deploy our container into the cloud. Let’s take
    a quick look at using Kubernetes to host and scale our service.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们就准备好将我们的容器部署到云端了。让我们快速看一下如何使用Kubernetes来托管和扩展我们的服务。
- en: Deploying on Kubernetes
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes上部署
- en: It’s beyond the scope of this book to go too deeply into Kubernetes, so we’ll
    stick to the basics, including how to get a service quickly up and running.^([1](ch08.html#idm45762352397768))
    *Kubernetes* (also known as *k8s*) is rapidly becoming the major cluster framework
    in the cloud. Born from Google’s original cluster management software, Borg, it
    contains all the parts and glue to form a resilient and reliable way of running
    services, including things like load balancers, resource quotas, scaling policies,
    traffic management, sharing secrets, and more.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的范围不包括深入讨论Kubernetes，因此我们将坚持基础知识，包括如何快速启动和运行服务。Kubernetes（也称为k8s）正在迅速成为云中的主要集群框架。它源自谷歌最初的集群管理软件Borg，包含所有部件和粘合剂，形成了一种弹性和可靠的运行服务的方式，包括负载均衡器、资源配额、扩展策略、流量管理、共享密钥等。
- en: You can download and set up Kubernetes on your local machine or in your cloud
    account, but the recommended way is to use a hosted service where management of
    Kubernetes itself is handled by the cloud provider and you’re just left with scheduling
    your services. We use the Google Kubernetes Engine (GKE) service for our deployment,
    but you could also deploy on Amazon, Azure, or DigitalOcean.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本地计算机或云账户上下载和设置Kubernetes，但推荐的方式是使用托管服务，其中Kubernetes本身的管理由云提供商处理，您只需安排您的服务。我们使用谷歌Kubernetes引擎（GKE）服务进行部署，但您也可以在亚马逊、Azure或DigitalOcean上进行部署。
- en: Setting Up on Google Kubernetes Engine
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在谷歌Kubernetes引擎上设置
- en: To use GKE, you need a [Google Cloud account](https://cloud.google.com). In
    addition, running services on GKE isn’t free. On the bright side, if you’re new
    to Google Cloud, you’ll get $300 in free credit, and we’re probably not going
    to burn more than a dollar or two.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用GKE，您需要一个[谷歌云账户](https://cloud.google.com)。此外，在GKE上运行服务并不是免费的。好消息是，如果您是谷歌云的新用户，您将获得价值300美元的免费信用额度，我们可能不会花费超过一两美元。
- en: 'Once you have an account, download the [`gcloud` SDK](https://cloud.google.com/sdk)
    for your system. Once that’s installed, we can use it to install `kubectl`, the
    application that we’ll use to interact with the Kubernetes cluster we’ll be creating:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您拥有账户，请为您的系统下载[`gcloud` SDK](https://cloud.google.com/sdk)。安装完成后，我们可以使用它来安装`kubectl`，这是我们将用来与我们将要创建的Kubernetes集群进行交互的应用程序：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We then need to create a new *project*, which is how Google Cloud organizes
    compute resources in your account:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要创建一个新的*项目*，这是谷歌云在您的账户中组织计算资源的方式：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we rebuild our Docker image and tag it so it can be pushed up to the
    internal registry that Google provides (we need to use `gcloud` to authenticate),
    and then we can use `docker push` to send our container image up to the cloud.
    Note that we’re also tagging our service with a `v1` version tag, which we weren’t
    doing before:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们重建我们的Docker镜像并对其进行标记，以便将其推送到谷歌提供的内部注册表（我们需要使用`gcloud`进行身份验证），然后我们可以使用`docker
    push`将我们的容器镜像发送到云端。请注意，我们还使用`v1`版本标记标记了我们的服务，这是之前没有做的：
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Creating a k8s Cluster
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个k8s集群
- en: Now we can create our Kubernetes cluster. In the following command, we’re creating
    one with two n1-standard-1 nodes, Google’s cheapest and lowest-powered instances.
    If you’re really saving pennies, you can create the cluster with just one node.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建我们的Kubernetes集群。在以下命令中，我们创建了一个具有两个n1-standard-1节点的集群，这是谷歌最便宜和最低功率的实例。如果您真的要省钱，可以只创建一个节点的集群。
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This may take a couple of minutes to fully initialize the new cluster. Once
    it’s ready, we can use `kubectl` to deploy our application!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要几分钟来完全初始化新的集群。一旦准备就绪，我们就可以使用`kubectl`部署我们的应用程序！
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Note that we’re passing the location of the model parameter file as an environment
    parameter here, just as we did with the `docker run` command on our local machine.
    Use `kubectl get pods` to see what pods are running on the cluster. A *pod* is
    a group of one or more containers combined with a specification on how to run
    and manage those containers. For our purposes, we run our model in one container
    in one pod. Here’s what you should see:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在这里将模型参数文件的位置作为环境参数传递，就像我们在本地机器上使用`docker run`命令一样。使用`kubectl get pods`查看集群上正在运行的pod。*pod*是一个包含一个或多个容器的组合，并附有如何运行和管理这些容器的规范。对于我们的目的，我们在一个pod中的一个容器中运行我们的模型。这是您应该看到的内容：
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Right, so now we can see that our application is running, but how do we actually
    talk to it? To do that, we need to deploy a *service*, in this case a load balancer
    that maps an external IP address to our internal cluster:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们可以看到我们的应用正在运行，但我们如何与它进行交流呢？为了做到这一点，我们需要部署一个*服务*，在这种情况下是一个负载均衡器，将外部IP地址映射到我们的内部集群：
- en: '[PRE22]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You can then look at the running services by using `kubectl get services` to
    get the external IP:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后您可以使用`kubectl get services`查看正在运行的服务以获取外部IP地址：
- en: '[PRE23]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You should now be able to hit *http://`external-ip`/predict* just as you could
    on your local machine. Success! We can also check in on our pod’s logs without
    logging into it:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该能够像在本地机器上一样访问*http://`external-ip`/predict*。成功！我们还可以在不登录的情况下查看我们的pod日志：
- en: '[PRE24]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We now have a deployment running in a Kubernetes cluster. Let’s explore some
    of the power that it provides.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在Kubernetes集群中运行一个部署。让我们探索一些它提供的功能。
- en: Scaling Services
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展服务
- en: 'Say we decide that one pod isn’t enough to handle all the traffic coming into
    our prediction service. In a traditional deployment, we’d have to bring up new
    servers, add them into load balancers, and work out what to do if one of the servers
    fails. But with Kubernetes, we can do all this easily. Let’s make sure that three
    copies of the service are running:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们决定一个pod无法处理进入我们预测服务的所有流量。在传统部署中，我们必须启动新服务器，将它们添加到负载均衡器中，并解决如果其中一个服务器失败该怎么办的问题。但是使用Kubernetes，我们可以轻松完成所有这些。让我们确保运行三个服务的副本：
- en: '[PRE25]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If you keep looking at `kubectl get pods`, you’ll soon see that Kubernetes
    is bringing up two more pods from your Docker image and wiring them into the load
    balancer. Even better, let’s see what happens if we delete one of the pods:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您继续查看`kubectl get pods`，您很快会看到Kubernetes正在从您的Docker镜像中启动另外两个pod，并将它们连接到负载均衡器。更好的是，让我们看看如果我们删除其中一个pod会发生什么：
- en: '[PRE26]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: You’ll see that the pod we’ve specified has been deleted. But—you should also
    see that a new pod is being spun up to replace it! We’ve told Kubernetes that
    we should be running three copies of the image, and because we deleted one, the
    cluster starts up a new pod to ensure that the replica count is what we requested.
    This also carries over to updating our application, so let’s look at that too.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 您会看到我们指定的pod已被删除。但是—您还应该看到正在启动一个新的pod来替换它！我们告诉Kubernetes我们应该运行三个镜像的副本，因为我们删除了一个，集群会启动一个新的pod来确保副本计数是我们请求的。这也适用于更新我们的应用程序，所以让我们也看看这个。
- en: Updates and Cleaning Up
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新和清理
- en: 'When it comes to pushing an update to our service code, we create a new version
    of the container with a `v2` tag:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及推送更新到我们的服务代码时，我们创建一个带有`v2`标签的容器的新版本：
- en: '[PRE27]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then we tell the cluster to use the new image for the deployment:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们告诉集群使用新镜像进行部署：
- en: '[PRE28]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Keep monitoring via `kubectl get pods` and you’ll see that new pods with the
    new image are being rolled out, and the pods with the old image are being deleted.
    Kubernetes automatically takes care of draining connections and removing the old
    pods from the load balancer.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`kubectl get pods`持续监控，您会看到正在部署具有新镜像的新pod，并且正在删除具有旧镜像的pod。Kubernetes会自动处理连接的排空和从负载均衡器中删除旧pod。
- en: 'Finally, if you’re finished playing around with the cluster, you should clean
    up so you don’t get any further surprise charges:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您已经玩够了集群，应该清理一下，以免出现任何意外费用：
- en: '[PRE29]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: That wraps up our mini-tour of Kubernetes; you now know just enough to be dangerous,
    but definitely check out [the Kubernetes website](https://kubernetes.io) as a
    starting point for further information about the system (and trust me, there’s
    a lot of it!)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对Kubernetes的迷你之旅；您现在已经知道足够多，可以做出危险的决定，但是一定要查看[Kubernetes网站](https://kubernetes.io)作为进一步了解该系统的起点（相信我，这方面有很多信息！）
- en: We’ve covered how to deploy our Python-based code, but perhaps surprisingly,
    PyTorch isn’t limited to just Python. In the next section, you’ll see how TorchScript
    brings in the wider world of C++, as well as some optimizations to our normal
    Python models.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了如何部署基于Python的代码，但也许令人惊讶的是，PyTorch并不仅限于Python。在下一节中，您将看到TorchScript如何引入更广泛的C++世界，以及对我们正常的Python模型的一些优化。
- en: TorchScript
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TorchScript
- en: 'If you can remember as far back as the introduction (I know!), you know that
    the main difference between PyTorch and TensorFlow is that TensorfFlow has a graph-based
    representation of a model, whereas PyTorch has an eager execution with tape-based
    differentiation. The eager method allows you to do all sorts of dynamic approaches
    to specifying and training models that makes PyTorch appealing for research purposes.
    On the other hand, the graph-based representation may be static, but it gains
    power from that stability; optimizations may be applied to the graph representation,
    safe in the knowledge that nothing is going to change. And just as TensorFlow
    has moved to support eager execution in version 2.0, version 1.0 of PyTorch introduced
    TorchScript, which is a way of bringing the advantages of graph-based systems
    without completely giving up the flexibility of PyTorch. This is done in two ways
    that can be mixed and matched: tracing and using TorchScript directly.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还记得介绍（我知道！）的话，您会知道PyTorch和TensorFlow之间的主要区别在于TensorfFlow具有模型的基于图形的表示，而PyTorch具有基于执行的即时执行和基于磁带的微分。即时方法允许您执行各种动态方法来指定和训练模型，使PyTorch对研究目的具有吸引力。另一方面，基于图形的表示可能是静态的，但它从稳定性中获得力量；可以应用优化到图形表示中，确保不会发生任何变化。正如TensorFlow已经在2.0版本中转向支持即时执行一样，PyTorch的1.0版本引入了TorchScript，这是一种在不完全放弃PyTorch灵活性的情况下带来图形系统优势的方法。这通过两种可以混合和匹配的方式来实现：跟踪和直接使用TorchScript。
- en: Tracing
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪
- en: PyTorch 1.0 comes with a JIT tracing engine that will turn an existing PyTorch
    module or function into a TorchScript one. It does this by passing an example
    tensor through the module and returning a `ScriptModule` result that contains
    the TorchScript representation of the original code.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 1.0带有一个JIT跟踪引擎，它将现有的PyTorch模块或函数转换为TorchScript。它通过将一个示例张量传递到模块中，并返回一个包含原始代码的TorchScript表示的`ScriptModule`结果来实现这一点。
- en: 'Let’s have a look at tracing AlexNet:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看跟踪AlexNet：
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, this will *work*, but you’ll get a message like this from the Python interpreter
    that will make you pause:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这将*起作用*，但您将从Python解释器收到这样的消息，这会让您停下来思考：
- en: '[PRE31]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: What’s going on here? When we create AlexNet (or other models), the model is
    instantiated in *training* mode. During training in many models such as AlexNet,
    we use a `Dropout` layer that randomly kills activations as a tensor goes through
    a network. What the JIT has done is send the random tensor we’ve generated through
    the model twice, compared them, and noted that the `Dropout` layers don’t match.
    This reveals an important caveat with the tracing facility; it cannot cope with
    nondeterminism or control flow. If your model uses these features, you’ll have
    to use TorchScript directly for at least part of your conversion.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么？当我们创建AlexNet（或其他模型）时，模型是在*训练*模式下实例化的。在许多模型（如AlexNet）的训练过程中，我们使用`Dropout`层，它会在张量通过网络时随机关闭激活。JIT所做的是将我们生成的随机张量通过模型两次，进行比较，并注意到`Dropout`层不匹配。这揭示了跟踪设施的一个重要注意事项；它无法处理不确定性或控制流。如果您的模型使用这些特性，您将不得不至少部分使用TorchScript进行转换。
- en: 'In AlexNet’s case, though, the fix is simple: we’ll switch the model to evaluation
    mode by using `model.eval()`. If you run the tracing line again, you’ll find that
    it completes without any complaining. We can also `print()` the traced model to
    see what it is composed of:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在AlexNet的情况下，修复很简单：我们将通过使用`model.eval()`将模型切换到评估模式。如果再次运行跟踪行，您会发现它完成而没有任何抱怨。我们还可以`print()`跟踪的模型以查看其组成部分：
- en: '[PRE32]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can also see the code that the JIT engine has created if we call `print(traced_model.code)`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果调用`print(traced_model.code)`，我们还可以看到JIT引擎创建的代码。
- en: '[PRE33]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The model (code and parameters) can then be saved with `torch.jit.save`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以使用`torch.jit.save`保存模型（代码和参数）：
- en: '[PRE34]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: That covers how tracing works. Let’s see how to use TorchScript.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了跟踪的工作原理。让我们看看如何使用TorchScript。
- en: Scripting
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 脚本化
- en: 'You might wonder why we just can’t trace everything. Although the tracer is
    good at what it does, it has limitations. For example, a simple function like
    the following is not possible to trace with a single pass:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道为什么我们不能跟踪一切。尽管跟踪器在其所做的事情上很擅长，但它也有局限性。例如，像以下简单函数这样的函数不可能通过单次传递进行跟踪：
- en: '[PRE35]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'A single trace through the function will take us down one pathway and not the
    other, meaning that the function will not be converted correctly. In these cases,
    we can use TorchScript, which is a limited subset of Python, and produce our compiled
    code. We use an *annotation* to tell PyTorch that we are using TorchScript, so
    the TorchScript implementation would look like this:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 通过函数的单个跟踪将使我们沿着一条路径而不是另一条路径，这意味着函数将无法正确转换。在这些情况下，我们可以使用TorchScript，这是Python的一个有限子集，并生成我们的编译代码。我们使用*注释*告诉PyTorch我们正在使用TorchScript，因此TorchScript实现将如下所示：
- en: '[PRE36]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Happily, we’re not using any constructs in our function that aren’t in TorchScript
    or referencing any global state, so this will just work. If we were creating a
    new architecture, we’d need to inherit from `torch.jit.ScriptModule` instead of
    `nn.Module`. You might wonder how we can use other modules (say, CNN-based layers)
    if all modules have to inherit from this different class. Is everything slightly
    different? The fix is that we can mix and match both by using explicit TorchScript
    and traced objects at will.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们的函数中没有使用TorchScript中没有的构造或引用任何全局状态，因此这将正常工作。如果我们正在创建一个新的架构，我们需要继承自`torch.jit.ScriptModule`而不是`nn.Module`。您可能想知道如果所有模块都必须继承自这个不同的类，我们如何可以使用其他模块（比如基于CNN的层）。一切都稍微不同吗？解决方法是我们可以随意使用显式TorchScript和跟踪对象来混合和匹配。
- en: 'Let’s go back to our CNNNet/AlexNet structure from [Chapter 3](ch03.html#convolutional-neural-networks)
    and see how it can be converted into TorchScript using a combination of these
    methods. For the sake of brevity, we’ll implement only the `features` component:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到[第3章](ch03.html#convolutional-neural-networks)中的CNNNet/AlexNet结构，看看如何使用这些方法的组合将其转换为TorchScript。为简洁起见，我们只实现`features`组件：
- en: '[PRE37]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: There are two things to note here. First, inside classes, we need to annotate
    using `@torch.jit.script_method`. Second, although we could have traced each separate
    layer individually, we took advantage of the `nn.Sequential` wrapper layer to
    fire the trace through just that instead. You could implement the `classifier`
    block yourself to get a feel for how this mixing works. Remember that you’ll need
    to switch the `Dropout` layers into `eval()` mode instead of training, and your
    input trace tensor will need to be of shape `[1, 256, 6, 6]` because of the downsampling
    that the `features` block carries out. And yes, you can save this network by using
    `torch.jit.save` just as we did for the traced module. Let’s have a look at what
    TorchScript allows and forbids.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两件事需要注意。首先，在类内部，我们需要使用 `@torch.jit.script_method` 进行注释。其次，尽管我们可以单独跟踪每个单独的层，但我们利用了
    `nn.Sequential` 包装层，只需通过它来触发跟踪。你可以自己实现 `classifier` 块，以了解这种混合工作的感觉。请记住，你需要将 `Dropout`
    层切换到 `eval()` 模式而不是训练模式，并且由于 `features` 块进行的下采样，你的输入跟踪张量的形状需要是 `[1, 256, 6, 6]`。是的，你可以像我们为跟踪模块所做的那样使用
    `torch.jit.save` 来保存这个网络。让我们看看TorchScript允许和禁止什么。
- en: TorchScript Limitations
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TorchScript的限制
- en: The biggest restriction in TorchScript compared to Python, at least in my mind,
    is the reduced number of types available. [Table 8-1](#table_available_python_types_in_torchscript)
    lists what’s available and what’s not.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 与Python相比，至少在我看来，TorchScript的最大限制是可用类型数量减少了。[表8-1](#table_available_python_types_in_torchscript)列出了可用和不可用的类型。
- en: Table 8-1\. Available Python types in TorchScript
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表8-1\. TorchScript中可用的Python类型
- en: '| Type | Description |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 描述 |'
- en: '| --- | --- |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `tensor` | A PyTorch tensor of any dtype, dimension, or backend |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| `tensor` | 一个PyTorch张量，可以是任何数据类型、维度或后端 |'
- en: '| `tuple`[T0, T1,…] | A tuple containing subtypes T0, T1, etc. (e.g., `tuple[tensor,
    tensor]`) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| `tuple`[T0, T1,…] | 包含子类型 T0、T1 等的元组（例如，`tuple[tensor, tensor]`） |'
- en: '| `boolean` | Boolean |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| `boolean` | 布尔值 |'
- en: '| `str` | String |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| `str` | 字符串 |'
- en: '| `int` | Int |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| `int` | 整数 |'
- en: '| `float` | Float |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| `float` | 浮点数 |'
- en: '| `list` | List of type `T` |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| `list` | 类型为 `T` 的列表 |'
- en: '| `optional[T]` | Either *None* or type `T` |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| `optional[T]` | 要么为 *None*，要么为类型 `T` |'
- en: '| `dict[K, V]` | `dict` with keys of type `K` and values of type `V`; `K` can
    be only `str`, `int`, or `float` |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| `dict[K, V]` | 键为类型 `K`，值为类型 `V` 的字典；`K` 只能是 `str`、`int` 或 `float` |'
- en: 'Another thing you can’t do that you can do in standard Python is have a function
    that mixes return types. The following is illegal in TorchScript:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准Python中可以做但在TorchScript中不能做的另一件事是具有混合返回类型的函数。以下在TorchScript中是非法的：
- en: '[PRE38]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Of course, it’s not really a good idea in Python, either, but the language’s
    dynamic typing will allow it. TorchScript is statically typed (which helps with
    applying optimizations), so you simply can’t do this in TorchScript annotated
    code. Also, TorchScript assumes that every parameter passed into a function is
    a tensor, which can result in some weirdness if you’re not aware of what’s going
    on:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在Python中也不是一个好主意，但语言的动态类型允许这样做。TorchScript是静态类型的（有助于应用优化），因此在TorchScript注释代码中你根本无法这样做。此外，TorchScript假定传递给函数的每个参数都是张量，如果你不知道发生了什么，可能会导致一些奇怪的情况：
- en: '[PRE39]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'To force different types, we need to use Python 3’s type decorators:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了强制不同的类型，我们需要使用Python 3的类型装饰器：
- en: '[PRE40]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'As you’ve already seen, classes are supported, but there are a few twists.
    All methods on a class have to be valid TorchScript, but although this code looks
    valid, it will fail:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你已经看到的，类是受支持的，但有一些细微差别。类中的所有方法都必须是有效的TorchScript，但尽管这段代码看起来有效，它将失败：
- en: '[PRE41]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This is, again, a consequence of TorchScript’s static typing. All instance variables
    have to be declared during the `__init__` and cannot be introduced elsewhere.
    Oh, and don’t get any ideas about including any expressions inside a class that
    aren’t in a method—these are explicitly banned by TorchScript.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这又是TorchScript静态类型的一个结果。所有实例变量都必须在 `__init__` 中声明，不能在其他地方引入。哦，不要想着在类中包含任何不在方法中的表达式——这些都被TorchScript明确禁止了。
- en: A useful feature of TorchScript being a subset of Python is that translation
    can be approached in a piecemeal approach, and the intermediate code is still
    valid and executable Python. TorchScript-compliant code can call out to noncompliant
    code, and while you won’t be able to execute `torch.jit.save()` until all the
    noncompliant code is converted, you can still run everything under Python.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: TorchScript作为Python的一个子集的一个有用特性是，翻译可以以逐步的方式进行，中间代码仍然是有效的可执行Python。符合TorchScript的代码可以调用不符合规范的代码，虽然在所有不符合规范的代码转换之前你无法执行
    `torch.jit.save()`，但你仍然可以在Python下运行所有内容。
- en: These are what I consider the major nuances of TorchScript. You can read about
    more [in the PyTorch documentation](https://oreil.ly/sS0o7), which goes into depth
    about things like scoping (mostly standard Pythonic rules), but the outline presented
    here is enough to convert all the models you’ve seen so far in this book. Instead
    of regurgitating all of the reference, let’s look at using one of our TorchScript-enabled
    models in C++.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我认为TorchScript的主要细微差别。你可以阅读更多关于[PyTorch文档](https://oreil.ly/sS0o7)中的内容，其中深入探讨了诸如作用域（主要是标准Python规则）之类的内容，但这里提供的概述足以将你在本书中迄今看到的所有模型转换过来。让我们看看如何在C++中使用我们的一个TorchScript启用的模型。
- en: Working with libTorch
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用libTorch
- en: In addition to TorchScript, PyTorch 1.0 introduced `libTorch`, a C++ library
    for interacting with PyTorch. Various levels of C++ interaction are available.
    The lowest levels are `ATen` and `autograd`, the C++ implementations of the tensor
    and automatic differentiation that PyTorch itself is built on. On top of those
    are a C++ frontend, which duplicates the Pythonic PyTorch API in C++, an interface
    to TorchScript, and finally an extension interface that allows new custom C++/CUDA
    operators to be defined and exposed to PyTorch’s Python implementation. We’re
    concerned with only the C++ frontend and the interface to TorchScript in this
    book, but more information on the other parts is available [in the PyTorch documentation](https://oreil.ly/y6NP5).
    Let’s start by getting `libTorch`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 除了TorchScript之外，PyTorch 1.0还引入了`libTorch`，这是一个用于与PyTorch交互的C++库。有各种不同级别的C++交互可用。最低级别是`ATen`和`autograd`，这是PyTorch本身构建在其上的张量和自动微分的C++实现。在这些之上是一个C++前端，它在C++中复制了Pythonic
    PyTorch API，一个与TorchScript的接口，最后是一个扩展接口，允许定义和暴露新的自定义C++/CUDA运算符给PyTorch的Python实现。在本书中，我们只关注C++前端和与TorchScript的接口，但有关其他部分的更多信息可在[PyTorch文档](https://oreil.ly/y6NP5)中找到。让我们从获取`libTorch`开始。
- en: Obtaining libTorch and Hello World
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取libTorch和Hello World
- en: 'Before we can do anything, we need a C++ compiler and a way of building C++
    programs on our machine. This is one of the few parts of the book where something
    like Google Colab isn’t appropriate, so you may have to create a VM in Google
    Cloud, AWS, or Azure if you don’t have easy access to a terminal window. (Everybody
    who ignored my advice not to build a dedicated machine is feeling smug right now,
    I bet!) The requirements for `libTorch` are a C++ compiler and *CMake*, so let’s
    get them installed. With a Debian-based system, use this command:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们做任何事情之前，我们需要一个C++编译器和一种在我们的机器上构建C++程序的方法。这是本书中少数几个部分之一，类似Google Colab之类的东西不适用，因此如果您没有轻松访问终端窗口，可能需要在Google
    Cloud、AWS或Azure中创建一个VM。 （所有忽略了我的建议不要构建专用机器的人现在都感到自鸣得意，我打赌！）`libTorch`的要求是一个C++编译器和*CMake*，所以让我们安装它们。对于基于Debian的系统，请使用以下命令：
- en: '[PRE42]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'If you’re using a Red Hat–based system, use this:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是基于Red Hat的系统，请使用以下命令：
- en: '[PRE43]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, we need to download `libTorch` itself. To make things a little easier,
    for what follows, we’ll use the CPU-based distribution of `libTorch`, rather than
    dealing with the additional CUDA dependencies that the GPU-enabled distribution
    brings. Create a directory called *torchscript_export* and grab the distribution:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要下载`libTorch`本身。为了让接下来的事情变得更容易，我们将使用基于CPU的`libTorch`分发，而不是处理启用GPU的分发带来的额外CUDA依赖。创建一个名为*torchscript_export*的目录并获取分发：
- en: '[PRE44]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Use `unzip` to expand the ZIP file (it should create a new *libtorch* directory)
    and create a directory called *helloworld*. In this directory, we’re going to
    add a minimal *CMakeLists.txt*, which *CMake* will use to build our executable:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`unzip`来展开ZIP文件（它应该创建一个新的*libtorch*目录），并创建一个名为*helloworld*的目录。在这个目录中，我们将添加一个最小的*CMakeLists.txt*，*CMake*将用它来构建我们的可执行文件：
- en: '[PRE45]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'And then *helloworld.cpp* is as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后*helloworld.cpp*如下：
- en: '[PRE46]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Create a *build* directory and run **`cmake`**, making sure that we provide
    an *absolute* path to the `libtorch` distribution:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个*build*目录并运行**`cmake`**，确保我们提供`libtorch`分发的*绝对*路径：
- en: '[PRE47]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can now run plain and simple `make` to create our executable:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以运行简单的`make`来创建我们的可执行文件：
- en: '[PRE48]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Congratulations on building your first C++ program with `libTorch`! Now, let’s
    expand on this and see how to use the library to load in a model we’ve previously
    saved with `torch.jit.save()`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 祝贺您使用`libTorch`构建了您的第一个C++程序！现在，让我们扩展一下，看看如何使用该库加载我们之前用`torch.jit.save()`保存的模型。
- en: Importing a TorchScript Model
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入TorchScript模型
- en: 'We’re going to export our full CNNNet model from [Chapter 3](ch03.html#convolutional-neural-networks)
    and load it into C++. In Python, create an instance of the CNNNet, switch it to
    `eval()` mode to ignore `Dropout`, trace, and save to disk:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从[第3章](ch03.html#convolutional-neural-networks)中导出我们的完整CNNNet模型，并将其加载到C++中。在Python中，创建CNNNet的实例，将其切换到`eval()`模式以忽略`Dropout`，跟踪并保存到磁盘：
- en: '[PRE49]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Over in the C++ world, create a new directory called *load-cnn* and add in
    this new *CMakeLists.txt* file:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在C++世界中，创建一个名为*load-cnn*的新目录，并添加这个新的*CMakeLists.txt*文件：
- en: '[PRE50]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Let’s create our C++ program, `load-cnn.cpp`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建我们的C++程序`load-cnn.cpp`：
- en: '[PRE51]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'A few new things are in this small program, though most of it should remind
    you of the Python PyTorch API. Our first act is to load in our TorchScript model
    with `torch::jit::load` (versus `torch.jit.load` in Python). We do a null pointer
    check to make sure that the model has loaded correctly, and then we move on to
    testing the model with a random tensor. Although we can do that fairly easily
    with `torch::rand`, when interacting with a TorchScript model, we have to create
    a vector of `torch::jit::IValue` inputs rather than just a normal tensor because
    of the way TorchScript is implemented in C++. Once that is done, we can push the
    tensor through our loaded model and then finally write the result back to standard
    output. We compile this in the same way that we compiled our earlier program:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小程序中有一些新东西，尽管其中大部分应该让您想起Python PyTorch API。我们的第一步是使用`torch::jit::load`加载我们的TorchScript模型（与Python中的`torch.jit.load`不同）。我们进行空指针检查以确保模型已正确加载，然后我们继续使用随机张量测试模型。虽然我们可以很容易地使用`torch::rand`来做到这一点，但在与TorchScript模型交互时，我们必须创建一个`torch::jit::IValue`输入向量，而不仅仅是一个普通张量，因为TorchScript在C++中的实现方式。完成后，我们可以将张量传递给我们加载的模型，最后将结果写回标准输出。我们以与之前编译我们的程序相同的方式编译它：
- en: '[PRE52]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: And voila! A C++ program that executes a custom model with little effort on
    our part. Be aware that the C++ interface is still at the time of writing in beta
    phase, so it’s possible that some of the details here may change. Make sure to
    have a look at the documentation before you use it in anger!
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 看吧！一个C++程序，可以轻松地执行自定义模型。请注意，C++接口在撰写本文时仍处于测试阶段，因此这里的一些细节可能会发生变化。在愤怒使用之前，请务必查看文档！
- en: Conclusion
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Hopefully you now understand how to take your trained (and debugged!) model
    and turn it into a Dockerized web service that can be deployed via Kubernetes.
    You’ve also seen how to use the JIT and TorchScript features to optimize our models
    and how to load TorchScript models in C++, giving us the promise of low-level
    integration of neural networks as well as in Python.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您现在了解如何将经过训练（和调试！）的模型转换为可以通过Kubernetes部署的Docker化Web服务。您还看到了如何使用JIT和TorchScript功能优化我们的模型，以及如何在C++中加载TorchScript模型，为我们提供了神经网络的低级集成承诺，以及在Python中。
- en: Obviously, with just one chapter, we can’t cover everything about production
    usage of model serving. We got to the point of deploying our service, but that’s
    not the end of the story; there’s the constant monitoring of the service to make
    sure that it is maintaining accuracy, retraining and testing against baselines,
    and more complicated versioning schemes than the ones I’ve introduced here for
    both the service and the model parameters. I recommend that you log as much detail
    as you possibly can and take advantage of that logging information for retraining
    as well as monitoring purposes.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，仅凭一章，我们无法涵盖有关模型服务的生产使用的所有内容。我们已经部署了我们的服务，但这并不是故事的结束；需要不断监控服务，确保其保持准确性，重新训练并针对基线进行测试，以及比我在这里介绍的更复杂的服务和模型参数版本控制方案。我建议您尽可能记录详细信息，并利用该日志信息进行重新训练以及监控目的。
- en: As for TorchScript, it’s still early days, but a few bindings for other languages
    (e.g., Go and Rust) are starting to appear; by 2020 it should be easy to wire
    a PyTorch model into any popular language.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 至于TorchScript，现在还处于早期阶段，但其他语言的一些绑定（例如Go和Rust）开始出现；到2020年，将很容易将PyTorch模型与任何流行语言连接起来。
- en: I’ve intentionally left out a few bits and pieces that don’t quite line up with
    the book’s scope. Back in the introduction, I promised that you could do everything
    in the book with one GPU, so we haven’t talked about PyTorch’s support for distributed
    training and inference. Also, if you read about PyTorch model exports, you’re
    almost certainly going to come across a lot of references to the Open Neural Network
    Exchange (ONNX). This standard, jointly authored by Microsoft and Facebook, was
    the main method of exporting models before the advent of TorchScript. Models can
    be exported via a similar tracing method to TorchScript and then imported in other
    frameworks such as Caffe2, Microsoft Cognitive Toolkit, and MXNet. ONNX is still
    supported and actively worked in PyTorch v1.*x*, but it appears that TorchScript
    is the preferred way for model exporting. See the “Further Reading” section for
    more details on ONNX if you’re interested.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我故意省略了一些与本书范围不太符合的细节。在介绍中，我承诺您可以使用一块GPU完成本书中的所有操作，因此我们没有讨论PyTorch对分布式训练和推断的支持。此外，如果您阅读有关PyTorch模型导出的信息，几乎肯定会遇到许多关于Open
    Neural Network Exchange（ONNX）的引用。这个标准由微软和Facebook联合撰写，在TorchScript出现之前是导出模型的主要方法。模型可以通过类似于TorchScript的跟踪方法导出，然后在其他框架（如Caffe2、Microsoft
    Cognitive Toolkit和MXNet）中导入。ONNX仍然受到PyTorch v1.*x*的支持和积极开发，但似乎TorchScript是模型导出的首选方式。如果您感兴趣，可以查看“进一步阅读”部分，了解更多关于ONNX的详细信息。
- en: Having successfully created, debugged, and deployed our models, we’ll spend
    the final chapter looking at what some companies have been doing with PyTorch.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 成功创建、调试和部署了我们的模型后，我们将在最后一章中看看一些公司如何使用PyTorch。
- en: Further Reading
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[Flask documentation](http://flask.pocoo.org)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Flask文档](http://flask.pocoo.org)'
- en: '[Waitress documentation](https://oreil.ly/bnelI)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Waitress文档](https://oreil.ly/bnelI)'
- en: '[Docker documentationd](https://docs.docker.com)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Docker文档](https://docs.docker.com)'
- en: '[Kubernetes (k8s) documentation](https://oreil.ly/jMVcN)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kubernetes（k8s）文档](https://oreil.ly/jMVcN)'
- en: '[TorchScript documentation](https://oreil.ly/sS0o7)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TorchScript文档](https://oreil.ly/sS0o7)'
- en: '[Open Neural Network Exchange](https://onnx.ai)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Open Neural Network Exchange](https://onnx.ai)'
- en: '[Using ONNX with PyTorch](https://oreil.ly/UXz5S)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用ONNX与PyTorch](https://oreil.ly/UXz5S)'
- en: '[Distributed training with PyTorch](https://oreil.ly/Q-Jao)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch进行分布式训练
- en: ^([1](ch08.html#idm45762352397768-marker)) [*Cloud Native DevOps with Kubernetes*](https://oreil.ly/2BaE1iq)
    by John Arundel and Justin Domingus (O’Reilly) is a great deep dive into this
    framework.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch08.html#idm45762352397768-marker)) [*使用Kubernetes的云原生DevOps*](https://oreil.ly/2BaE1iq)
    由John Arundel和Justin Domingus（O'Reilly）深入探讨了这一框架。
