- en: Chapter 8\. PyTorch in Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you’ve learned how to use PyTorch to classify images, text, and sound,
    the next step is to look at how to deploy PyTorch applications in production.
    In this chapter, we create applications that run inference on PyTorch models over
    HTTP and gRPC. We then package those applications into Docker containers and deploy
    them to a Kubernetes cluster running on Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: In the second half, we look at TorchScript, a new technology introduced in PyTorch
    1.0 that allows us to use just-in-time (JIT) tracing to produce optimized models
    that can be run from C++. We also have a brief look at how to compress models
    with quantization. First up, let’s look at model serving.
  prefs: []
  type: TYPE_NORMAL
- en: Model Serving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve spent the last six chapters building models in PyTorch, but building a
    model is only part of building a deep learning application. After all, a model
    may have amazing accuracy (or other relevant metric), but if it never makes any
    predictions, is it worth anything? What we want is an easy way to package our
    models so they can respond to requests (either over the web or other means, as
    we’ll see) and can be run in production with the minimum of effort.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, Python allows us to get a web service up and running quickly with
    the Flask framework. In this section, we build a simple service that loads our
    ResNet-based *cat or fish* model, accepts requests that include an image URL,
    and returns a JSON response that indicates whether the image contains a cat or
    a fish.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What happens if we send the model a picture of a dog? The model will tell you
    that it is either a fish or a cat. It has no concept of anything but the available
    choices and will always pick one. Some deep learning practitioners add an extra
    class, `Unknown`, during training and throw in labeled examples that aren’t any
    of the required classes. This works to a certain extent, but it essentially tries
    to make the neural net learn *everything that isn’t a cat or fish*, which is difficult
    for you and me to express, let alone a series of matrix calculations! Another
    option is to look at the probability output generated by the final `softmax`.
    If the model is producing a prediction that is roughly 50/50 cat/fish or spread
    out across your classes, then maybe suggest `Unknown`.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Flask Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s get a web service-enabled version of our model up and running. *Flask*
    is a popular framework for creating web services with Python, and we’ll be using
    it as a base throughout this chapter. Install the Flask library with either `pip`
    or `conda`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new directory called *catfish* and copy your model definition inside
    as *model.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we do not specify a pretrained model here, because we will be loading
    our saved weights in the Flask server startup process. Then create another Python
    script, *catfish_server.py*, where we will start our web service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can start up a web server on the command line by setting the `CATFISH_HOST`
    and `CATFISH_PORT` environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you point your web browser at [*http://127.0.0.1:8080*](http://127.0.0.1:8080),
    you should get a `status: "ok"` JSON response as shown in [Figure 8-1](#ok-response-from-service).'
  prefs: []
  type: TYPE_NORMAL
- en: '![OK Response from CATFISH](assets/ppdl_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. OK response from CATFISH
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Caution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We discuss this in more detail later in this chapter, but don’t deploy a Flask
    service directly to production because the built-in server is not adequate for
    production usage.
  prefs: []
  type: TYPE_NORMAL
- en: To make a prediction, find an image URL and send it as a `GET` request with
    the `image_url` parameter to the `/predict` path. You should see a JSON response
    showing the URL and the predicted class, as shown in [Figure 8-2](#prediction-from-catfish).
  prefs: []
  type: TYPE_NORMAL
- en: '![Prediction from CATFISH](assets/ppdl_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Prediction from CATFISH
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The magic in Flask is in the `@app.route()` annotations. These allow us to attach
    normal Python functions that will be run when a user hits a particular endpoint.
    In our `predict()` method, we pull out the `img_url` parameter from either a `GET`
    or `POST` HTTP request, open that URL as a PIL image, and push it through a simple
    `torchvision` transform pipeline to resize it and turn the image into a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: This gives us a tensor of shape `[3,224,224]`, but because of the way our model
    works, we need to turn it into a batch of size 1—that is, `[1,3,224,224]`. So
    we use `unsqueeze()` again to expand our tensor by inserting a new empty axis
    in front of the existing dimensions. We can then pass it through the model as
    usual, which gives us our prediction tensor. As we have done previously, we use
    `torch.argmax()` to find the element of the tensor with the highest value and
    use that to index into the `CatfishClasses` array. Finally, we return a JSON response
    with the name of the class and the image URL we performed the prediction on.
  prefs: []
  type: TYPE_NORMAL
- en: If you experiment with the server at this point, you might be a little disappointed
    with the classification performance. Didn’t we spend a lot of time training it?
    Yes, we did, but in re-creating the model, we have simply created a set of layers
    with the standard PyTorch initialization! So no wonder it’s not good. Let’s flesh
    out `load_model()` to load in our parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We’re returning only the predicted class here, not the complete set of predictions
    across all classes. You could certainly return the prediction tensor as well,
    though be aware that the complete tensor output makes it a little easier for attackers
    to build up a replica of your model through more *information leakage*.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up the Model Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html#image-classification-with-pytorch), we talked about
    the two ways to save a model after training, either by writing the entire model
    to disk with `torch.save()` or by saving the `state_dict()` of all the weights
    and biases of the model (but not the structure). For our production-based service,
    we need to load in an already-trained model, so what should we use?
  prefs: []
  type: TYPE_NORMAL
- en: In my opinion, you should go for the `state_dict` approach. Saving the entire
    model is an attractive option, but you will become incredibly sensitive to any
    changes in the model structure or even the directory structure of the training
    setup. That’s likely to cause a problem with loading it up in a separate service
    that runs elsewhere. If we’re making a migration to a slightly different layout,
    we’d like to not have to rework everything.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’d also be better off not hardcoding the filename of the saved `state_dicts()`
    so we can decouple model updates from our service. This means we can restart the
    service with a new model or revert to an earlier model with ease. We pass in the
    filename as a parameter—but where should it point? For the moment, assume that
    we can set an environment variable called `CATFISH_MODEL_LOCATION`, and use that
    in `load_model()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, copy in one of the model weight files you saved in [Chapter 4](ch04.html#transfer-learning-and-other-tricks)
    into the directory and set `CATFISH_MODEL_LOCATION` to point to that file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Restart the server, and you should see that the service is a lot more accurate!
  prefs: []
  type: TYPE_NORMAL
- en: We now have a working minimal web service (you’d probably want a little more
    error handling, but I’m leaving that as an exercise for you!). But how do we get
    that running on a server in, say, AWS or Google Cloud? Or just on somebody else’s
    laptop? After all, we have installed a bunch of libraries to get this working.
    We can use Docker to package everything up into one *container* that can be installed
    in any Linux (or Windows, with the new Windows Subsystem for Linux!) environment
    in seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Docker Container
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker has become one of the de facto standards for application packaging in
    the past few years. Cutting-edge cluster environments such as Kubernetes have
    Docker at their core for deploying applications (as you’ll see later in the chapter),
    and it’s even made large inroads in enterprises as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you haven’t come across Docker before, here’s a quick explanation: it’s
    modeled on the idea of shipping containers. You specify a bundle of files (typically,
    using a Dockerfile) that Docker uses to build an *image*, and Docker then runs
    that image in a *container*, which is an isolated process on your system that
    can see only the files you’ve specified and the programs you’ve told it to run.
    You can then share the Dockerfile so people can build their own images, but a
    more common approach is to push the created image to a *registry*, which is a
    list of Docker images that can be downloaded by anybody with access. These registries
    can be public or private; the Docker corporation runs [Docker Hub](https://hub.docker.com),
    which is a public registry that contains over 100,000 Docker images, but many
    companies run private registries for internal use.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What we need to do is write our own Dockerfile. This might sound a little overwhelming.
    What do we have to tell Docker to install? Our code? PyTorch? Conda? Python? Linux
    itself? Thankfully, Dockerfiles can inherit from other images, so we could, for
    example, inherit from the standard Ubuntu image and install Python, PyTorch, and
    everything else from there. But we can do better! A selection of Conda images
    is available to choose from that will give us a base Linux, Python, and Anaconda
    installation to build on. Here’s an example Dockerfile that can be used to build
    a container image for our service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: A few things are happening here, so let’s take a look. The first line in almost
    all Dockerfiles will be `FROM`, which lists the Docker image that this file inherits
    from. In this case, it’s `continuumio/miniconda3:latest`. The first part of this
    string is the image name. Images are also versioned, so everything after the colon
    is a *tag* indicating which version of the image we want to download. There’s
    also a magic tag `latest`, which we use here to download the latest version of
    the image we’re after. You may want to pin your service to a particular version
    so you aren’t surprised by possible later changes in the base image causing issues
    in yours.
  prefs: []
  type: TYPE_NORMAL
- en: '`ARG` and `ENV` deal with variables. `ARG` specifies a variable that is supplied
    to Docker when we’re building the image, and then the variable can be used later
    in the Dockerfile. `ENV` allows you to specify environment variables that will
    be injected into the container at runtime. In our container, we use `ARG` to specify,
    for example, that port is a configurable option, and then use `ENV` to ensure
    that the configuration is available to our script at startup.'
  prefs: []
  type: TYPE_NORMAL
- en: Having done that, `RUN` and `COPY` allow us to manipulate the image we’ve inherited
    from. `RUN` runs actual commands within the image, and any changes are saved as
    a new *layer* of the image on top of the base layer. `COPY` takes something from
    the Docker build context (typically, any files from the directory that the build
    command has issued or any subdirectories) and inserts it into a location on the
    image’s filesystem. Having created `/app` by using `RUN`, we then use `COPY` to
    move our code and model parameters into the image.
  prefs: []
  type: TYPE_NORMAL
- en: '`EXPOSE` indicates to Docker which port should be mapped to the outside world.
    By default, no ports are opened, so we add one here, taken from the `ARG` command
    earlier in the file. Finally, `ENTRYPOINT` is the default command that is run
    when a container is created. Here we’ve specified a script, but we haven’t made
    it yet! Let’s do that before we build our Docker image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Wait, what’s happening here? Where did `waitress` come from? The issue is that
    when we were running our Flask-based server before it used a simple web server
    that is meant only for debugging purposes. If we want to put this into production,
    we need a production-grade web server. Waitress fulfills that requirement. We
    don’t need to go into much detail about it, but you can check out the [Waitress
    documentation](https://oreil.ly/x96Ir) if you want to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: 'With all that set up, we can finally create our image by using `docker build`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can make sure that the image is available on our system by using `docker
    images`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Running our model prediction service can then be done using `docker run`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We also use the `-p` argument to map the container’s port 5000 to our computer’s
    port 5000\. You should be able to go back to *http://localhost:5000/predict* just
    as before.
  prefs: []
  type: TYPE_NORMAL
- en: One thing you might notice when running `docker images` locally is that our
    Docker image is over 4GB in size! That’s quite big, considering we didn’t write
    much code. Let’s look at ways to make that image smaller and make our image more
    practical for deployment at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Local Versus Cloud Storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Obviously, the easiest answer to where to store our saved model parameters
    is on the local filesystem, whether that’s on our computer or the filesystem within
    a Docker container. But there are a couple of problems with this. First, the model
    is hardcoded into the image. Also, it’s quite possible that after the image is
    built and put into production, we need to update the model. With our current Dockerfile,
    we have to completely rebuild the image, even if the model’s structure hasn’t
    changed! Second, most of the size of our images comes from the size of the parameter
    file. You may not have noticed that they tend to be quite large! Try this out
    for size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If we add these models to the filesystem on every build, our Docker images will
    likely be quite large, which makes pushing and pulling slower. What I suggest
    is local filesystems or Docker volume-mapped containers if you’re running on-premises,
    but if you’re doing a cloud deployment, which we are leading up to, it makes sense
    to take advantage of the cloud. Model parameter files can be uploaded to Azure
    Blob Storage, Amazon Simple Storage Service (Amazon S3), or Google Cloud Storage
    and be pulled in at startup.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rewrite our `load_model()` function to download the parameter file at
    startup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'There are, of course, many ways of downloading files with Python; Flask even
    comes with the `requests` module that would easily download the file. A potential
    issue, though, is that many approaches download the entire file into memory before
    writing it to disk. Most of the time, that makes sense, but when downloading model
    parameter files, they could get into the gigabyte range. So in this new version
    of `load_model()`, we use `urlopen()` and `copyfileobj()` to carry out the copying,
    and `NamedTemporaryFile()` to give us a destination that can be deleted at the
    end of the block, as by that point, we’ve already loaded the parameters in, and
    thus no longer need the file! This allows us to simplify our Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this with `docker run`, we pass in the environment variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The service now pulls the parameters from the URL, and the Docker image is probably
    around 600MB–700MB smaller than the original one.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this example, we assume that the model parameter file is located at a publicly
    accessible location. If you are deploying a model service, you likely won’t be
    in that situation and will instead be pulling from a cloud storage layer like
    Amazon S3, Google Cloud Storage, or Azure Blob Storage. You’ll have to use the
    respective provider’s APIs to download the file and obtain credentials to gain
    access to it, both of which we don’t discuss here.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a model service that’s capable of talking over HTTP with JSON. Now
    we need to make sure that we can monitor it while it makes predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Logging and Telemetry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One thing that we don’t have in our current service is any concept of logging.
    And although the service is incredibly simple and perhaps doesn’t need copious
    logging (except in the case of catching our error states), it would be useful,
    if not essential, for us to keep track of what’s actually being predicted. At
    some point, we’re going to want to evaluate the model; how can we do that without
    production data?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume that we have a method `send_to_log()` that takes a Python `dict`
    and sends it elsewhere (perhaps, say, into an Apache Kafka cluster that backs
    up onto cloud storage). We could send appropriate information through this method
    every time we make a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: With a few additions to calculate how long a prediction takes, on every request,
    this method now sends off a message to a logger or an external resource, providing
    important details such as the image URL, the predicted class, the actual prediction
    tensor, and even the complete image tensor just in case the supplied URL is transient.
    We also include a generated universally unique identifier (UUID), so that this
    prediction can always be uniquely referenced at a later time, perhaps if its predicted
    class needs to be corrected. In an actual deployment, you’d include things like
    `user_id`s and such so that downstream systems can provide a facility for users
    to indicate whether the prediction was correct or incorrect, sneakily generating
    more training data for further training iterations of the model.
  prefs: []
  type: TYPE_NORMAL
- en: And with that, we’re ready to deploy our container into the cloud. Let’s take
    a quick look at using Kubernetes to host and scale our service.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s beyond the scope of this book to go too deeply into Kubernetes, so we’ll
    stick to the basics, including how to get a service quickly up and running.^([1](ch08.html#idm45762352397768))
    *Kubernetes* (also known as *k8s*) is rapidly becoming the major cluster framework
    in the cloud. Born from Google’s original cluster management software, Borg, it
    contains all the parts and glue to form a resilient and reliable way of running
    services, including things like load balancers, resource quotas, scaling policies,
    traffic management, sharing secrets, and more.
  prefs: []
  type: TYPE_NORMAL
- en: You can download and set up Kubernetes on your local machine or in your cloud
    account, but the recommended way is to use a hosted service where management of
    Kubernetes itself is handled by the cloud provider and you’re just left with scheduling
    your services. We use the Google Kubernetes Engine (GKE) service for our deployment,
    but you could also deploy on Amazon, Azure, or DigitalOcean.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up on Google Kubernetes Engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use GKE, you need a [Google Cloud account](https://cloud.google.com). In
    addition, running services on GKE isn’t free. On the bright side, if you’re new
    to Google Cloud, you’ll get $300 in free credit, and we’re probably not going
    to burn more than a dollar or two.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have an account, download the [`gcloud` SDK](https://cloud.google.com/sdk)
    for your system. Once that’s installed, we can use it to install `kubectl`, the
    application that we’ll use to interact with the Kubernetes cluster we’ll be creating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We then need to create a new *project*, which is how Google Cloud organizes
    compute resources in your account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we rebuild our Docker image and tag it so it can be pushed up to the
    internal registry that Google provides (we need to use `gcloud` to authenticate),
    and then we can use `docker push` to send our container image up to the cloud.
    Note that we’re also tagging our service with a `v1` version tag, which we weren’t
    doing before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Creating a k8s Cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we can create our Kubernetes cluster. In the following command, we’re creating
    one with two n1-standard-1 nodes, Google’s cheapest and lowest-powered instances.
    If you’re really saving pennies, you can create the cluster with just one node.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This may take a couple of minutes to fully initialize the new cluster. Once
    it’s ready, we can use `kubectl` to deploy our application!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we’re passing the location of the model parameter file as an environment
    parameter here, just as we did with the `docker run` command on our local machine.
    Use `kubectl get pods` to see what pods are running on the cluster. A *pod* is
    a group of one or more containers combined with a specification on how to run
    and manage those containers. For our purposes, we run our model in one container
    in one pod. Here’s what you should see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Right, so now we can see that our application is running, but how do we actually
    talk to it? To do that, we need to deploy a *service*, in this case a load balancer
    that maps an external IP address to our internal cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then look at the running services by using `kubectl get services` to
    get the external IP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You should now be able to hit *http://`external-ip`/predict* just as you could
    on your local machine. Success! We can also check in on our pod’s logs without
    logging into it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We now have a deployment running in a Kubernetes cluster. Let’s explore some
    of the power that it provides.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Say we decide that one pod isn’t enough to handle all the traffic coming into
    our prediction service. In a traditional deployment, we’d have to bring up new
    servers, add them into load balancers, and work out what to do if one of the servers
    fails. But with Kubernetes, we can do all this easily. Let’s make sure that three
    copies of the service are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'If you keep looking at `kubectl get pods`, you’ll soon see that Kubernetes
    is bringing up two more pods from your Docker image and wiring them into the load
    balancer. Even better, let’s see what happens if we delete one of the pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: You’ll see that the pod we’ve specified has been deleted. But—you should also
    see that a new pod is being spun up to replace it! We’ve told Kubernetes that
    we should be running three copies of the image, and because we deleted one, the
    cluster starts up a new pod to ensure that the replica count is what we requested.
    This also carries over to updating our application, so let’s look at that too.
  prefs: []
  type: TYPE_NORMAL
- en: Updates and Cleaning Up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When it comes to pushing an update to our service code, we create a new version
    of the container with a `v2` tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we tell the cluster to use the new image for the deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Keep monitoring via `kubectl get pods` and you’ll see that new pods with the
    new image are being rolled out, and the pods with the old image are being deleted.
    Kubernetes automatically takes care of draining connections and removing the old
    pods from the load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, if you’re finished playing around with the cluster, you should clean
    up so you don’t get any further surprise charges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: That wraps up our mini-tour of Kubernetes; you now know just enough to be dangerous,
    but definitely check out [the Kubernetes website](https://kubernetes.io) as a
    starting point for further information about the system (and trust me, there’s
    a lot of it!)
  prefs: []
  type: TYPE_NORMAL
- en: We’ve covered how to deploy our Python-based code, but perhaps surprisingly,
    PyTorch isn’t limited to just Python. In the next section, you’ll see how TorchScript
    brings in the wider world of C++, as well as some optimizations to our normal
    Python models.
  prefs: []
  type: TYPE_NORMAL
- en: TorchScript
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you can remember as far back as the introduction (I know!), you know that
    the main difference between PyTorch and TensorFlow is that TensorfFlow has a graph-based
    representation of a model, whereas PyTorch has an eager execution with tape-based
    differentiation. The eager method allows you to do all sorts of dynamic approaches
    to specifying and training models that makes PyTorch appealing for research purposes.
    On the other hand, the graph-based representation may be static, but it gains
    power from that stability; optimizations may be applied to the graph representation,
    safe in the knowledge that nothing is going to change. And just as TensorFlow
    has moved to support eager execution in version 2.0, version 1.0 of PyTorch introduced
    TorchScript, which is a way of bringing the advantages of graph-based systems
    without completely giving up the flexibility of PyTorch. This is done in two ways
    that can be mixed and matched: tracing and using TorchScript directly.'
  prefs: []
  type: TYPE_NORMAL
- en: Tracing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch 1.0 comes with a JIT tracing engine that will turn an existing PyTorch
    module or function into a TorchScript one. It does this by passing an example
    tensor through the module and returning a `ScriptModule` result that contains
    the TorchScript representation of the original code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at tracing AlexNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, this will *work*, but you’ll get a message like this from the Python interpreter
    that will make you pause:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: What’s going on here? When we create AlexNet (or other models), the model is
    instantiated in *training* mode. During training in many models such as AlexNet,
    we use a `Dropout` layer that randomly kills activations as a tensor goes through
    a network. What the JIT has done is send the random tensor we’ve generated through
    the model twice, compared them, and noted that the `Dropout` layers don’t match.
    This reveals an important caveat with the tracing facility; it cannot cope with
    nondeterminism or control flow. If your model uses these features, you’ll have
    to use TorchScript directly for at least part of your conversion.
  prefs: []
  type: TYPE_NORMAL
- en: 'In AlexNet’s case, though, the fix is simple: we’ll switch the model to evaluation
    mode by using `model.eval()`. If you run the tracing line again, you’ll find that
    it completes without any complaining. We can also `print()` the traced model to
    see what it is composed of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also see the code that the JIT engine has created if we call `print(traced_model.code)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The model (code and parameters) can then be saved with `torch.jit.save`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: That covers how tracing works. Let’s see how to use TorchScript.
  prefs: []
  type: TYPE_NORMAL
- en: Scripting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You might wonder why we just can’t trace everything. Although the tracer is
    good at what it does, it has limitations. For example, a simple function like
    the following is not possible to trace with a single pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'A single trace through the function will take us down one pathway and not the
    other, meaning that the function will not be converted correctly. In these cases,
    we can use TorchScript, which is a limited subset of Python, and produce our compiled
    code. We use an *annotation* to tell PyTorch that we are using TorchScript, so
    the TorchScript implementation would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Happily, we’re not using any constructs in our function that aren’t in TorchScript
    or referencing any global state, so this will just work. If we were creating a
    new architecture, we’d need to inherit from `torch.jit.ScriptModule` instead of
    `nn.Module`. You might wonder how we can use other modules (say, CNN-based layers)
    if all modules have to inherit from this different class. Is everything slightly
    different? The fix is that we can mix and match both by using explicit TorchScript
    and traced objects at will.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go back to our CNNNet/AlexNet structure from [Chapter 3](ch03.html#convolutional-neural-networks)
    and see how it can be converted into TorchScript using a combination of these
    methods. For the sake of brevity, we’ll implement only the `features` component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: There are two things to note here. First, inside classes, we need to annotate
    using `@torch.jit.script_method`. Second, although we could have traced each separate
    layer individually, we took advantage of the `nn.Sequential` wrapper layer to
    fire the trace through just that instead. You could implement the `classifier`
    block yourself to get a feel for how this mixing works. Remember that you’ll need
    to switch the `Dropout` layers into `eval()` mode instead of training, and your
    input trace tensor will need to be of shape `[1, 256, 6, 6]` because of the downsampling
    that the `features` block carries out. And yes, you can save this network by using
    `torch.jit.save` just as we did for the traced module. Let’s have a look at what
    TorchScript allows and forbids.
  prefs: []
  type: TYPE_NORMAL
- en: TorchScript Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The biggest restriction in TorchScript compared to Python, at least in my mind,
    is the reduced number of types available. [Table 8-1](#table_available_python_types_in_torchscript)
    lists what’s available and what’s not.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-1\. Available Python types in TorchScript
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `tensor` | A PyTorch tensor of any dtype, dimension, or backend |'
  prefs: []
  type: TYPE_TB
- en: '| `tuple`[T0, T1,…] | A tuple containing subtypes T0, T1, etc. (e.g., `tuple[tensor,
    tensor]`) |'
  prefs: []
  type: TYPE_TB
- en: '| `boolean` | Boolean |'
  prefs: []
  type: TYPE_TB
- en: '| `str` | String |'
  prefs: []
  type: TYPE_TB
- en: '| `int` | Int |'
  prefs: []
  type: TYPE_TB
- en: '| `float` | Float |'
  prefs: []
  type: TYPE_TB
- en: '| `list` | List of type `T` |'
  prefs: []
  type: TYPE_TB
- en: '| `optional[T]` | Either *None* or type `T` |'
  prefs: []
  type: TYPE_TB
- en: '| `dict[K, V]` | `dict` with keys of type `K` and values of type `V`; `K` can
    be only `str`, `int`, or `float` |'
  prefs: []
  type: TYPE_TB
- en: 'Another thing you can’t do that you can do in standard Python is have a function
    that mixes return types. The following is illegal in TorchScript:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, it’s not really a good idea in Python, either, but the language’s
    dynamic typing will allow it. TorchScript is statically typed (which helps with
    applying optimizations), so you simply can’t do this in TorchScript annotated
    code. Also, TorchScript assumes that every parameter passed into a function is
    a tensor, which can result in some weirdness if you’re not aware of what’s going
    on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'To force different types, we need to use Python 3’s type decorators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'As you’ve already seen, classes are supported, but there are a few twists.
    All methods on a class have to be valid TorchScript, but although this code looks
    valid, it will fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This is, again, a consequence of TorchScript’s static typing. All instance variables
    have to be declared during the `__init__` and cannot be introduced elsewhere.
    Oh, and don’t get any ideas about including any expressions inside a class that
    aren’t in a method—these are explicitly banned by TorchScript.
  prefs: []
  type: TYPE_NORMAL
- en: A useful feature of TorchScript being a subset of Python is that translation
    can be approached in a piecemeal approach, and the intermediate code is still
    valid and executable Python. TorchScript-compliant code can call out to noncompliant
    code, and while you won’t be able to execute `torch.jit.save()` until all the
    noncompliant code is converted, you can still run everything under Python.
  prefs: []
  type: TYPE_NORMAL
- en: These are what I consider the major nuances of TorchScript. You can read about
    more [in the PyTorch documentation](https://oreil.ly/sS0o7), which goes into depth
    about things like scoping (mostly standard Pythonic rules), but the outline presented
    here is enough to convert all the models you’ve seen so far in this book. Instead
    of regurgitating all of the reference, let’s look at using one of our TorchScript-enabled
    models in C++.
  prefs: []
  type: TYPE_NORMAL
- en: Working with libTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to TorchScript, PyTorch 1.0 introduced `libTorch`, a C++ library
    for interacting with PyTorch. Various levels of C++ interaction are available.
    The lowest levels are `ATen` and `autograd`, the C++ implementations of the tensor
    and automatic differentiation that PyTorch itself is built on. On top of those
    are a C++ frontend, which duplicates the Pythonic PyTorch API in C++, an interface
    to TorchScript, and finally an extension interface that allows new custom C++/CUDA
    operators to be defined and exposed to PyTorch’s Python implementation. We’re
    concerned with only the C++ frontend and the interface to TorchScript in this
    book, but more information on the other parts is available [in the PyTorch documentation](https://oreil.ly/y6NP5).
    Let’s start by getting `libTorch`.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining libTorch and Hello World
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we can do anything, we need a C++ compiler and a way of building C++
    programs on our machine. This is one of the few parts of the book where something
    like Google Colab isn’t appropriate, so you may have to create a VM in Google
    Cloud, AWS, or Azure if you don’t have easy access to a terminal window. (Everybody
    who ignored my advice not to build a dedicated machine is feeling smug right now,
    I bet!) The requirements for `libTorch` are a C++ compiler and *CMake*, so let’s
    get them installed. With a Debian-based system, use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'If you’re using a Red Hat–based system, use this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to download `libTorch` itself. To make things a little easier,
    for what follows, we’ll use the CPU-based distribution of `libTorch`, rather than
    dealing with the additional CUDA dependencies that the GPU-enabled distribution
    brings. Create a directory called *torchscript_export* and grab the distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `unzip` to expand the ZIP file (it should create a new *libtorch* directory)
    and create a directory called *helloworld*. In this directory, we’re going to
    add a minimal *CMakeLists.txt*, which *CMake* will use to build our executable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'And then *helloworld.cpp* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a *build* directory and run **`cmake`**, making sure that we provide
    an *absolute* path to the `libtorch` distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now run plain and simple `make` to create our executable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations on building your first C++ program with `libTorch`! Now, let’s
    expand on this and see how to use the library to load in a model we’ve previously
    saved with `torch.jit.save()`.
  prefs: []
  type: TYPE_NORMAL
- en: Importing a TorchScript Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’re going to export our full CNNNet model from [Chapter 3](ch03.html#convolutional-neural-networks)
    and load it into C++. In Python, create an instance of the CNNNet, switch it to
    `eval()` mode to ignore `Dropout`, trace, and save to disk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Over in the C++ world, create a new directory called *load-cnn* and add in
    this new *CMakeLists.txt* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s create our C++ program, `load-cnn.cpp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'A few new things are in this small program, though most of it should remind
    you of the Python PyTorch API. Our first act is to load in our TorchScript model
    with `torch::jit::load` (versus `torch.jit.load` in Python). We do a null pointer
    check to make sure that the model has loaded correctly, and then we move on to
    testing the model with a random tensor. Although we can do that fairly easily
    with `torch::rand`, when interacting with a TorchScript model, we have to create
    a vector of `torch::jit::IValue` inputs rather than just a normal tensor because
    of the way TorchScript is implemented in C++. Once that is done, we can push the
    tensor through our loaded model and then finally write the result back to standard
    output. We compile this in the same way that we compiled our earlier program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: And voila! A C++ program that executes a custom model with little effort on
    our part. Be aware that the C++ interface is still at the time of writing in beta
    phase, so it’s possible that some of the details here may change. Make sure to
    have a look at the documentation before you use it in anger!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully you now understand how to take your trained (and debugged!) model
    and turn it into a Dockerized web service that can be deployed via Kubernetes.
    You’ve also seen how to use the JIT and TorchScript features to optimize our models
    and how to load TorchScript models in C++, giving us the promise of low-level
    integration of neural networks as well as in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, with just one chapter, we can’t cover everything about production
    usage of model serving. We got to the point of deploying our service, but that’s
    not the end of the story; there’s the constant monitoring of the service to make
    sure that it is maintaining accuracy, retraining and testing against baselines,
    and more complicated versioning schemes than the ones I’ve introduced here for
    both the service and the model parameters. I recommend that you log as much detail
    as you possibly can and take advantage of that logging information for retraining
    as well as monitoring purposes.
  prefs: []
  type: TYPE_NORMAL
- en: As for TorchScript, it’s still early days, but a few bindings for other languages
    (e.g., Go and Rust) are starting to appear; by 2020 it should be easy to wire
    a PyTorch model into any popular language.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve intentionally left out a few bits and pieces that don’t quite line up with
    the book’s scope. Back in the introduction, I promised that you could do everything
    in the book with one GPU, so we haven’t talked about PyTorch’s support for distributed
    training and inference. Also, if you read about PyTorch model exports, you’re
    almost certainly going to come across a lot of references to the Open Neural Network
    Exchange (ONNX). This standard, jointly authored by Microsoft and Facebook, was
    the main method of exporting models before the advent of TorchScript. Models can
    be exported via a similar tracing method to TorchScript and then imported in other
    frameworks such as Caffe2, Microsoft Cognitive Toolkit, and MXNet. ONNX is still
    supported and actively worked in PyTorch v1.*x*, but it appears that TorchScript
    is the preferred way for model exporting. See the “Further Reading” section for
    more details on ONNX if you’re interested.
  prefs: []
  type: TYPE_NORMAL
- en: Having successfully created, debugged, and deployed our models, we’ll spend
    the final chapter looking at what some companies have been doing with PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Flask documentation](http://flask.pocoo.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Waitress documentation](https://oreil.ly/bnelI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Docker documentationd](https://docs.docker.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kubernetes (k8s) documentation](https://oreil.ly/jMVcN)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TorchScript documentation](https://oreil.ly/sS0o7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Open Neural Network Exchange](https://onnx.ai)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using ONNX with PyTorch](https://oreil.ly/UXz5S)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Distributed training with PyTorch](https://oreil.ly/Q-Jao)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^([1](ch08.html#idm45762352397768-marker)) [*Cloud Native DevOps with Kubernetes*](https://oreil.ly/2BaE1iq)
    by John Arundel and Justin Domingus (O’Reilly) is a great deep dive into this
    framework.
  prefs: []
  type: TYPE_NORMAL
