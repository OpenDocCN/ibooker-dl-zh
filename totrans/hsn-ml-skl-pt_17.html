<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 15. Transformers for Natural Language Processing and Chatbots"><div class="chapter" id="transformer_chapter">
<h1><span class="label">Chapter 15. </span>Transformers for Natural Language Processing and Chatbots</h1>


<p>In a landmark 2017 paper<a data-type="indexterm" data-primary="transformers" id="xi_transformers15325"/> titled <a href="https://homl.info/transformer">“Attention Is All You Need”</a>,⁠<sup><a data-type="noteref" id="id3414-marker" href="ch15.html#id3414">1</a></sup> a team of Google researchers proposed a novel neural net architecture<a data-type="indexterm" data-primary="Transformer architecture" id="xi_Transformerarchitecture153339_1"/> named the <em>Transformer</em>, which significantly improved the state of the art in neural machine translation (NMT)<a data-type="indexterm" data-primary="neural machine translation (NMT)" id="id3415"/>. In short, the Transformer architecture is simply an encoder-decoder model, very much like the one we built in <a data-type="xref" href="ch14.html#nlp_chapter">Chapter 14</a> for English-to-Spanish translation, and it can be used in exactly the same way (see <a data-type="xref" href="#transformer_encoder_decoder_diagram">Figure 15-1</a>):</p>
<ol>
<li>
<p>The source text goes in the encoder, which outputs contextualized embeddings (one per token).</p>
</li>
<li>
<p>The encoder’s output is then fed to the decoder, along with the translated text so far (starting with a start-of-sequence token).</p>
</li>
<li>
<p>The decoder predicts the next token for each input token.</p>
</li>
<li>
<p>The last token output by the decoder is appended to the translation.</p>
</li>
<li>
<p>Steps 2 to 4 are repeated again and again to produce the full translation, one extra token at a time, until an end-of-sequence token is generated. During training, we already have the full translation—it’s the target—so it is fed to the decoder in step 2 (starting with a start-of-sequence token), and steps 4 and 5 are not needed.</p>
</li>

</ol>

<figure class="smallereighty"><div id="transformer_encoder_decoder_diagram" class="figure">
<img src="assets/hmls_1501.png" alt="Diagram illustrating the Transformer model's process for translating English to Spanish, showing how the encoder generates contextual embeddings and the decoder predicts the next token in the translated sequence." width="1420" height="481"/>
<h6><span class="label">Figure 15-1. </span>Using the Transformer model for English-to-Spanish translation</h6>
</div></figure>

<p>So what’s new? Well, inside the black box, there are some important differences with our previous encoder-decoder. Crucially, the Transformer architecture does not contain any recurrent or convolutional layers, just regular dense layers combined with a new kind of attention mechanism called <em>multi-head attention</em> (MHA)<a data-type="indexterm" data-primary="multi-head attention (MHA)" id="id3416"/>, plus a few bells and whistles.⁠<sup><a data-type="noteref" id="id3417-marker" href="ch15.html#id3417">2</a></sup> Because the model is not recurrent, it doesn’t suffer as much from the vanishing or exploding gradients problems as RNNs, it can be trained in fewer steps, it’s easier to parallelize across multiple GPUs, and it scales surprisingly well. Moreover, thanks to multi-head attention, the model can capture long-range patterns much better than RNNs.</p>

<p>The Transformer architecture also turned out to be extremely versatile. It was initially designed for NMT, but researchers quickly tweaked the architecture for many other language tasks. The year 2018 was even called the “ImageNet moment for NLP”. In June 2018, OpenAI released the first GPT model<a data-type="indexterm" data-primary="GPT (Generative Pre-Training) model" id="id3418"/>, based solely on the Transformer’s decoder module. It was pretrained on a large corpus of text, its ability to generate text was unprecedented, it could auto-complete sentences, invent stories, and even answer some questions. GPT could also be fine-tuned to perform a wide range of language tasks. Just a few months later, Google released the BERT model, based solely on the Transformer’s encoder module. It was excellent at a variety of <em>natural language understanding</em> (NLU)<a data-type="indexterm" data-primary="natural language understanding (NLU)" data-seealso="encoder-only transformers" id="id3419"/><a data-type="indexterm" data-primary="NLU (natural language understanding)" data-seealso="encoder-only transformers" id="id3420"/> tasks, such as text classification, text embedding, multiple choice question answering, or finding the answer to a question within some text.</p>

<p>Surprisingly, transformers also turned out to be great at computer vision, audio processing (e.g., speech-to-text), robotics (using inputs from sensors and sending the outputs to actuators), and more. For example, if you split an image into little chunks and feed them to a transformer (instead of token embeddings), you get a <em>vision transformer</em> (ViT)<a data-type="indexterm" data-primary="vision transformers (ViTs)" id="id3421"/>. In fact, some transformers can even handle multiple <em>modalities</em> at once (e.g., text + image); these<a data-type="indexterm" data-primary="multimodal models" id="id3422"/> are called <em>multimodal models</em>.</p>

<p>This outstanding combination of performance, flexibility, and scalability encouraged Google, OpenAI, Facebook (Meta), Microsoft, Anthropic, and many other organizations to train larger and larger transformer models. The original Transformer model had about 65 million parameters—which was considered quite large at the time—but new transformers grew at a mind-boggling rate, reaching 1.6 trillion parameters by January 2021—that’s 1.6 million million parameters! Training such a gigantic transformer model from scratch is sadly restricted to organizations with deep pockets, as it requires a large and costly infrastructure for several months: training typically costs tens of millions of dollars, and even up to hundreds of millions according to some estimates (the exact figures are generally not public). <a data-type="xref" href="#transformer_growth_diagram">Figure 15-2</a> shows some of the most influential transformers released between June 2018 and April 2025.⁠<sup><a data-type="noteref" id="id3423-marker" href="ch15.html#id3423">3</a></sup> Note that the vertical axis is in <em>billions</em> of parameters, and it uses a logarithmic scale.</p>

<figure><div id="transformer_growth_diagram" class="figure">
<img src="assets/hmls_1502.png" alt="Diagram showing the exponential growth of transformer models from 2018 to 2025, highlighting influential releases by organizations like OpenAI, Google, and Facebook, with parameter counts rising into trillions." width="2560" height="1468"/>
<h6><span class="label">Figure 15-2. </span>Some of the most influential transformers released since 2018; see it larger <a href="https://homl.info/fig15-2">online</a></h6>
</div></figure>

<p class="pagebreak-before">Then, in November 2022, OpenAI<a data-type="indexterm" data-primary="OpenAI" id="id3424"/> released ChatGPT<a data-type="indexterm" data-primary="ChatGPT" id="id3425"/>, an amazing <em>conversational AI</em>—or <em>chatbot</em>—that took the world by storm: it reached one million users in just five days, and over one hundred million monthly active users after just two months! Under the hood, it used GPT-3.5-turbo, a variant of GPT-3.5 which was fine-tuned to be conversational, helpful, and safe. Others soon followed: Perplexity AI, Google’s Gemini (initially called Bard), Anthropic’s Claude, Mistral AI, DeepSeek, and more.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Before ChatGPT was released, Google had actually developed a powerful chatbot named LaMDA, but it wasn’t made public, likely for fear of reputational and legal risks, as the model was not deemed safe enough. This allowed OpenAI to become the first company to train a reasonably safe and helpful model and to package it as a useful chatbot product.</p>
</div>

<p>So how can you use these models and chatbots? Well, many of them are proprietary (e.g., OpenAI’s GPT-3.5, GPT-4 and GPT-5 models, Anthropic’s Claude models, and Google’s Gemini models), and they can only be used via a web UI, an app, or an API: you must create an account, choose an offer (or use the free tier), and for the API you must get an access token and use it to query the API programmatically. However, many other models<a data-type="indexterm" data-primary="open weights, chatbots" id="id3426"/> are <em>open weights</em>, meaning they can be downloaded for free (e.g., using the Hugging Face Hub): some of these have licensing restrictions (e.g., Meta’s Llama models are only free for noncommercial use), while others are truly open source (e.g., DeepSeek’s R1 or Mistral AI’s Mistral-7B). Some even include the training code and data (e.g., the OLMo models by Ai2).</p>

<p>So what are we waiting for? Let’s join the transformer revolution! Here’s the plan:</p>

<ul>
<li>
<p>We will start by opening up the original Transformer architecture and inspecting its components to fully understand how everything works.</p>
</li>
<li>
<p>Then we will build and train a transformer from scratch for English-to-Spanish translation.</p>
</li>
<li>
<p>After that, we will look into encoder-only models like BERT, learn how they are pretrained, and see how to use them for tasks like text classification, semantic search, and text clustering, with or without fine-tuning.</p>
</li>
<li>
<p>Next, we will look into decoder-only models like GPT, and see how they are pretrained. These models are capable of generating text, which is great if you want to write a poem, but it can also be used to tackle many other tasks.</p>
</li>
<li>
<p>Then we will use a decoder-only model to build our own chatbot! This involves a few steps: first, you must download a pretrained model (or train your own if you have the time and money), then you must fine-tune it to make it more conversational, helpful, and safe (or you can download an already fine-tuned model, or even use a conversational model via an API), and lastly you must deploy the model to a chatbot system that offers a user interface, stores conversations, and can also give the model access to tools, such as searching the web or using a calculator.</p>
</li>
<li>
<p>Lastly, we will take a quick look at encoder-decoder models, such as T5 and BART, which are great for tasks such as translation and summarization.</p>
</li>
</ul>

<p>In <a data-type="xref" href="ch16.html#vit_chapter">Chapter 16</a>, we will look at vision transformers and multimodal transformers. <a data-type="xref" href="ch17.html#speedup_chapter">Chapter 17</a> and “State-Space Models (SSMs)” (both available at <a href="https://homl.info" class="bare"><em class="hyperlink">https://homl.info</em></a>) also discuss some advanced techniques to allow Transformers to scale and process longer input sequences.</p>

<p>Let’s start by dissecting the Transformer architecture: take out your scalpel!</p>






<section data-type="sect1" data-pdf-bookmark="Attention Is All You Need: The Original &#10;Transformer Architecture"><div class="sect1" id="id280">
<h1>Attention Is All You Need: The Original 
<span class="keep-together">Transformer Architecture</span></h1>

<p>The original 2017 Transformer architecture<a data-type="indexterm" data-primary="attention mechanisms" id="xi_attentionmechanisms154943_1"/> is represented in <a data-type="xref" href="#transformer_diagram">Figure 15-3</a>. The left part of the figure represents the encoder, the right part represents the decoder.</p>

<p>As we saw earlier, the encoder’s role is to gradually <em>transform</em> the inputs (e.g., sequences of English tokens) until each token’s representation perfectly captures the meaning of that token in the context of the sentence: the encoder’s output is a sequence of contextualized token embeddings. Apart from the embedding layer, every layer in the encoder takes as input a tensor of shape [<em>batch size</em>, <em>max English sequence length in the batch</em>, <em>embedding size</em>] and returns a tensor of the exact same shape. This means that token representations get gradually transformed, hence the name of the architecture. For example, if you feed the sentence “I like soccer” to the encoder, then the token “like” will start off with a rather vague representation, since “like” could mean different things in different contexts (e.g., “I’m like a cat” versus “I like my cat”). But after going through the encoder, the token’s representation should capture the correct meaning of “like” in the given sentence (in this case, to be fond of), as well as any other information that may be required for translation (e.g., it’s a verb).</p>

<p>The decoder’s role is to take the encoder’s outputs, along with the translated sentence so far, and predict the next token in the translation. For this, the decoder layers gradually transform each input token’s representation into a representation that can be used to predict the following token. For example, suppose the sentence to translate is “I like soccer” and we’ve already called the decoder four times, producing one new token each time: first “me”, then “me gusta”, then “me gusta el”, and finally “me gusta el fútbol”. Since this translation does not end with an EoS token <code translate="no">"&lt;/s&gt;"</code>, we must call the decoder once again. The decoder’s input sequence is now "<code translate="no">&lt;s&gt;</code> me gusta el fútbol”. As the representation of each token goes through the decoder, it gets transformed: the representation of <code translate="no">"&lt;s&gt;"</code> becomes a rich enough representation to predict “me” (for simplicity, I’ll say this more concisely as: <code translate="no">"&lt;s&gt;"</code> becomes “me”), “me” becomes “gusta”, “gusta” becomes “el”, “el” becomes “fútbol”, and if everything goes well, “fútbol” becomes the EoS token <code translate="no">"&lt;/s&gt;"</code>. Apart from the embedding layer and the output <code translate="no">Linear</code> layer, every layer in the decoder takes as input a tensor of shape [<em>batch size</em>, <em>max Spanish sequence length in the batch</em>, <em>embedding size</em>] and returns a tensor of the exact same shape.</p>

<figure class="width-55"><div id="transformer_diagram" class="figure">
<img src="assets/hmls_1503.png" alt="Diagram of the original 2017 transformer architecture, showing the encoder and decoder layers with multi-head attention, feed forward, and positional encoding processes, illustrating how inputs are transformed into outputs." width="830" height="1569"/>
<h6><span class="label">Figure 15-3. </span>The original 2017 transformer architecture⁠<sup><a data-type="noteref" id="id3427-marker" href="ch15.html#id3427">4</a></sup></h6>
</div></figure>

<p class="pagebreak-before">After going through the decoder, each token representation goes through a final <code translate="no">Linear</code> layer<a data-type="indexterm" data-primary="torch" data-secondary="nn.Linear" id="id3428"/> which will hopefully output a high logit for the correct token and a low logit for all other tokens in the vocabulary. The decoder’s output shape is [<em>batch size</em>, <em>max Spanish sequence length in the batch</em>, <em>vocabulary size</em>]. The final predicted sentence should be “me gusta el fútbol <code translate="no">&lt;/s&gt;</code>“. Note that the figure shows a softmax layer at the top, but in PyTorch we usually don’t explicitly add it: instead, we let the model output logits, and we train the model<a data-type="indexterm" data-primary="torch" data-secondary="nn.CrossEntropyLoss" id="id3429"/> using <code translate="no">nn.CrossEntropyLoss</code>, which computes the cross-entropy loss based on logits instead of estimated probabilities (as we saw in previous chapters). If you ever need estimated probabilities, you can always convert the logits to estimated probabilities using the <code translate="no">F.softmax()</code> function<a data-type="indexterm" data-primary="torch" data-secondary="F.softmax()" id="id3430"/>.</p>

<p>Now let’s zoom in a bit further into <a data-type="xref" href="#transformer_diagram">Figure 15-3</a>:</p>

<ul>
<li>
<p>First, notice that both the encoder and the decoder contain blocks that are stacked <em>N</em> times. In the paper, <em>N</em> = 6. Note that the final outputs of the whole encoder stack are fed to each of the decoder’s <em>N</em> blocks.</p>
</li>
<li>
<p>As you can see, you are already familiar with most components: there are two embedding layers; several skip connections, each of them followed by a layer normalization module; several feedforward modules composed of two dense layers each (the first one using the ReLU activation function, the second with no activation function); and finally, the output layer is a linear layer. Notice that all layers treat each token independently from all the others. But how can we translate a sentence by looking at the tokens completely separately? Well, we can’t, so that’s where the new components come in:</p>

<ul>
<li>
<p>The encoder’s <em>multi-head attention</em> layer<a data-type="indexterm" data-primary="multi-head attention (MHA)" data-secondary="in encoder-decoder process" data-secondary-sortas="encoder-decoder process" id="id3431"/> updates each token representation by attending to (i.e., paying attention to) every token in the same sentence, including itself. This<a data-type="indexterm" data-primary="self-attention" id="id3432"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="self-attention" id="id3433"/> is called <em>self-attention</em>. That’s where the vague representation of the word “like” becomes a richer and more accurate representation, capturing its precise meaning within the given sentence (e.g., the layer notices the subject “I” so it infers that “like” must be a verb). We will discuss exactly how this works shortly.</p>
</li>
<li>
<p>The decoder’s <em>masked multi-head attention</em> layer<a data-type="indexterm" data-primary="masked multi-head attention layer" id="id3434"/><a data-type="indexterm" data-primary="multi-head attention (MHA)" data-secondary="masked multi-head attention layer" id="id3435"/> does the same thing, but when it processes a token, it doesn’t attend to tokens located after it: it’s a causal layer. For example, when it processes the token “gusta”, it only attends to the tokens <code translate="no">"&lt;s&gt;"</code>, “me”, and “gusta”, and it ignores the tokens “el” and “fútbol” (or else the model could cheat during training).</p>
</li>
<li>
<p>The decoder’s upper multi-head attention layer is where the decoder pays attention to the contextualized token representations output by the encoder stack. This<a data-type="indexterm" data-primary="cross-attention" id="id3436"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="cross-attention" id="id3437"/> is called <em>cross</em>-attention, as opposed to <em>self</em>-attention. For example, the decoder will probably pay close attention to the word “soccer” when it processes the word “el” and outputs a representation of the word “fútbol”.</p>
</li>
<li>
<p>The <em>positional encodings</em> are dense vectors<a data-type="indexterm" data-primary="positional encodings" id="xi_positionalencodings157148_1"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="positional encodings" id="xi_attentionmechanismspositionalencodings157148_1"/> that represent the position of each token in the sentence. The <em>n</em><sup>th</sup> positional encoding is added to the token embedding of the <em>n</em><sup>th</sup> token in each sentence. This is needed because all layers in the Transformer architecture are position-agnostic, meaning they treat all positions equally (unlike recurrent or convolutional layers): when they process a token, they have no idea where that token is located in the sentence or relative to other words. But the order of words matters, so we must somehow give positional information to the Transformer. Adding positional encodings to the token representations is a good way to achieve this.</p>
</li>
</ul>
</li>
</ul>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The first two arrows going into each multi-head attention layer<a data-type="indexterm" data-primary="multi-head attention (MHA)" data-secondary="in encoder-decoder process" data-secondary-sortas="encoder-decoder process" id="id3438"/> in <a data-type="xref" href="#transformer_diagram">Figure 15-3</a> represent the keys and values, and the third arrow represents the queries.⁠<sup><a data-type="noteref" id="id3439-marker" href="ch15.html#id3439">5</a></sup> In the self-attention<a data-type="indexterm" data-primary="self-attention" data-secondary="in encoder-decoder process" data-secondary-sortas="encoder-decoder process" id="id3440"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="self-attention" id="id3441"/> layers, all three are equal to the token representations output by the previous layer, while in the cross-attention layers (i.e., the decoder’s upper attention layers), the keys and values are equal to the encoder’s final token representations, and the queries are equal to the token representations output by the previous decoder layer.</p>
</div>

<p>Now let’s go through the novel components of the Transformer architecture in more detail, starting with the positional encodings.</p>








<section data-type="sect2" data-pdf-bookmark="Positional Encodings"><div class="sect2" id="id281">
<h2>Positional Encodings</h2>

<p>A positional encoding is a dense vector that encodes the position of a token within a sentence: the <em>i</em><sup>th</sup> positional encoding is added to the token embedding of the <em>i</em><sup>th</sup> token in each sentence. A simple way to implement this is to use an <code translate="no">Embedding</code> layer<a data-type="indexterm" data-primary="torch" data-secondary="nn.Embedding" id="id3442"/>: just add embedding #0 to the representation of token #0, add embedding #1 to the representation of token #1, and so on. Alternatively, you can use an <code translate="no">nn.Parameter</code> to store<a data-type="indexterm" data-primary="torch" data-secondary="nn.Parameter" id="id3443"/> the embedding matrix (initialized using small random weights), then add its first <em>L</em> rows to the inputs (where <em>L</em> is the max input sequence length): the result is the same, but it’s much faster. You can also add a bit of dropout to reduce the risk of overfitting. Here’s an implementation:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">torch.nn.functional</code> <code class="k">as</code> <code class="nn">F</code>

<code class="k">class</code> <code class="nc">PositionalEmbedding</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">max_length</code><code class="p">,</code> <code class="n">embed_dim</code><code class="p">,</code> <code class="n">dropout</code><code class="o">=</code><code class="mf">0.1</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">pos_embed</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Parameter</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="n">max_length</code><code class="p">,</code> <code class="n">embed_dim</code><code class="p">)</code> <code class="o">*</code> <code class="mf">0.02</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">dropout</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code><code class="p">(</code><code class="n">X</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">pos_embed</code><code class="p">[:</code><code class="n">X</code><code class="o">.</code><code class="n">size</code><code class="p">(</code><code class="mi">1</code><code class="p">)])</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The inputs have shape [<em>batch size</em>, <em>sequence length</em>, <em>embedding size</em>], but we are adding positional encodings of shape [<em>sequence length</em>, <em>embedding size</em>]. This works thanks to the broadcasting rules: the <em>i</em><sup>th</sup> positional embedding is added to the <em>i</em><sup>th</sup> token’s representation of each sentence in the batch.</p>
</div>

<p>The authors of the Transformer paper also proposed using<a data-type="indexterm" data-primary="fixed versus trainable positional encodings" id="id3444"/><a data-type="indexterm" data-primary="trainable versus fixed positional encodings" id="id3445"/> fixed positional encodings rather than trainable ones. Their approach used a pretty smart scheme based on the sine and cosine functions, but it’s not much used anymore, as it doesn’t really perform any better than trainable positional embeddings (except perhaps on small transformers, if you’re lucky). Please see this chapter’s notebook for more details. Moreover, newer approaches such as <em>relative position bias</em> (RPB), <em>rotary positional encoding</em> (RoPE), and <em>attention with linear bias</em> (ALiBi) generally perform better<a data-type="indexterm" data-startref="xi_positionalencodings157148_1" id="id3446"/><a data-type="indexterm" data-startref="xi_attentionmechanismspositionalencodings157148_1" id="id3447"/>. To learn more about all of these alternative approaches to positional encoding, see “Relative Positional Encoding”.</p>

<p>Now let’s look deeper into the heart of the Transformer model: the multi-head attention layer.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Multi-Head Attention"><div class="sect2" id="id282">
<h2>Multi-Head Attention</h2>

<p>The multi-head attention (MHA) layer<a data-type="indexterm" data-primary="scaled dot-product attention layer" id="id3448"/><a data-type="indexterm" data-primary="multi-head attention (MHA)" data-secondary="in encoder-decoder process" data-secondary-sortas="encoder-decoder process" id="xi_multiheadattentionMHAinencoderdecoderprocess1510337_1"/> is based on <em>scaled dot-product attention</em>, a variant of dot-product attention (introduced in <a data-type="xref" href="ch14.html#nlp_chapter">Chapter 14</a>) that scales down the similarity scores by a constant factor. See <a data-type="xref" href="#scaled_dot_product_attention">Equation 15-1</a> for its vectorized equation.</p>
<div id="scaled_dot_product_attention" data-type="equation">
<h5><span class="label">Equation 15-1. </span>Scaled dot-product attention</h5>
<math><mo>Attention</mo><mrow><mo>(</mo><mrow><mi mathvariant="bold">Q</mi><mo lspace="0%" rspace="0%">,</mo><mi mathvariant="bold">K</mi><mo lspace="0%" rspace="0%">,</mo><mi mathvariant="bold">V</mi></mrow><mo>)</mo></mrow><mo>=</mo><mo>softmax</mo><mfenced><mfrac><mrow><msup><mi mathvariant="bold">QK</mi><mo>⊺</mo></msup></mrow><msqrt><mrow><msub><mi>d</mi><mi mathvariant="normal">k</mi></msub></mrow></msqrt></mfrac></mfenced><mi mathvariant="bold">V</mi></math>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><strong>Q</strong> is a matrix representing a <em>query</em> (e.g., an English or Spanish sequence, depending on the attention layer). Its shape is [<em>L</em><sub>q</sub>, <em>d</em><sub>q</sub>], where <em>L</em><sub>q</sub> is the length of the query and <em>d</em><sub>q</sub> is the query’s dimensionality (i.e., the number of dimensions in the token representations).</p>
</li>
<li>
<p><strong>K</strong> is a matrix representing a key. Its shape is [<em>L</em><sub>k</sub>, <em>d</em><sub>k</sub>], where <em>L</em><sub>k</sub> is the length of the key and <em>d</em><sub>k</sub> is the key’s dimensionality. Note that <em>d</em><sub>k</sub> must equal <em>d</em><sub>q</sub>.</p>
</li>
<li>
<p><strong>V</strong> is a matrix representing a value. Its shape is [<em>L</em><sub>v</sub>, <em>d</em><sub>v</sub>], where <em>L</em><sub>v</sub> is the length of the value and <em>d</em><sub>v</sub> is the value’s dimensionality. Note that <em>L</em><sub>v</sub> must equal <em>L</em><sub>k</sub>.</p>
</li>
<li>
<p>The shape of <strong>Q</strong> <strong>K</strong><sup>⊺</sup> is [<em>L</em><sub>q</sub>, <em>L</em><sub>k</sub>]: it contains one similarity score for each query/key pair. To prevent this matrix from being huge, the input sequences must not be too long: this is the critical <em>quadratic context window</em> problem<a data-type="indexterm" data-primary="quadratic context window problem" id="id3449"/> (we will discuss various ways to alleviate this issue in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch16.html#vit_chapter">16</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch17.html#speedup_chapter">17</a>. The softmax function is applied to each row: the output has the same shape as the input, but now each row sums up to 1. The final output has a shape of [<em>L</em><sub>q</sub>, <em>d</em><sub>v</sub>]. There is one row per query token, and each row represents the query result: a weighted sum of the value tokens, favoring value tokens whose corresponding key tokens are most aligned with the given query token.</p>
</li>
<li>
<p>The scaling factor 1 / <math><msqrt><msub><mi>d</mi> <mrow><mi>k</mi></mrow></msub></msqrt></math> scales down the similarity scores to avoid saturating the softmax function, which would lead to tiny gradients. This factor was empirically shown to speed up and stabilize training.</p>
</li>
<li>
<p>It is possible to mask out some key/value pairs by adding a very large negative value to the corresponding similarity scores, just before computing the softmax (in practice, we can add <code translate="no">–torch.inf</code>). The resulting weights will be equal to zero. This is useful to mask padding tokens, as well as future tokens in the masked multi-head attention layer.</p>
</li>
</ul>

<p>PyTorch<a data-type="indexterm" data-primary="scaled dot-product attention layer" id="xi_scaleddotproductattentionlayer151218_1"/> comes with the <code translate="no">F.scaled_dot_product_attention()</code> function<a data-type="indexterm" data-primary="torch" data-secondary="F.scaled_dot_product_attention()" id="id3450"/>. Its inputs are just like <strong>Q</strong>, <strong>K</strong>, and <strong>V</strong>, but these inputs can have extra dimensions at the start, such as the batch size and the number of heads (when used for multi-head attention). The equation is applied simultaneously across all of these extra dimensions. In other words, the function computes the results simultaneously across all sentences in the batch and across all attention heads, making it very efficient.</p>

<p>Now we’re ready to look at the multi-head attention layer. Its architecture is shown in <a data-type="xref" href="#multihead_attention_diagram">Figure 15-4</a>.</p>

<p>As you can see, it is just a bunch of scaled dot-product attention layers<a data-type="indexterm" data-primary="attention heads" id="id3451"/>, called <em>attention heads</em>, each preceded by a linear transformation of the values, keys, and queries (across all tokens). The outputs of all the attention heads are simply concatenated, and they go through a final linear transformation (again, across all tokens).</p>

<p>But why? What is the intuition behind this architecture? Well, consider once again the word “like” in the sentence “I like soccer”. The encoder was hopefully smart enough to encode its meaning, the fact that it’s a verb, and many other features that are useful for its translation, such as the fact that it is in the present tense. The token representation also includes the position, thanks to the positional encodings. In short, the token representation encodes many different characteristics of the token. If we just used a single scaled dot-product attention layer, we would only be able to query all of these characteristics in one shot.</p>

<figure class="width-40"><div id="multihead_attention_diagram" class="figure">
<img src="assets/hmls_1504.png" alt="Diagram illustrating the architecture of a multi-head attention layer, highlighting the flow from linear transformations of values, keys, and queries through split layers, scaled dot-product attention, concatenation, and final linear transformation." width="599" height="961"/>
<h6><span class="label">Figure 15-4. </span>Multi-head attention layer architecture⁠<sup><a data-type="noteref" id="id3452-marker" href="ch15.html#id3452">6</a></sup></h6>
</div></figure>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>The Transformer architecture is extremely flexible, so the model has plenty of freedom during training to choose its own knowledge representation and strategies. As a result, it ends up being somewhat of a black box: understanding how transformers truly “think” is an area of active research<a data-type="indexterm" data-primary="model interpretability" id="id3453"/>, called <em>model interpretability</em>. For example, check out this <a href="https://homl.info/tracing-thoughts">fascinating post by Anthropic</a>.</p>
</div>

<p>This is why the MHA layer splits the values, keys, and queries across multiple heads: this way, each head can focus on specific characteristics of the token. The first linear layer lets the model choose which characteristics each head should focus on. For example, the linear layer may ensure that the first head gets a projection of the “like” token’s representation into a subspace where all that remains is the information that this token is a verb in the present tense. Another head may focus on the word’s meaning, and so on. Then the scaled dot-product attention layers implement the actual lookup phase, and finally the results are all concatenated and run through a final linear layer that lets the model reorganize the representation as it pleases<a data-type="indexterm" data-startref="xi_scaleddotproductattentionlayer151218_1" id="id3454"/>.</p>

<p>To really understand the Transformer architecture, the key is to understand multi-head attention, and for this, it helps to look at a basic implementation:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">MultiheadAttention</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">embed_dim</code><code class="p">,</code> <code class="n">num_heads</code><code class="p">,</code> <code class="n">dropout</code><code class="o">=</code><code class="mf">0.1</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">h</code> <code class="o">=</code> <code class="n">num_heads</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">d</code> <code class="o">=</code> <code class="n">embed_dim</code> <code class="o">//</code> <code class="n">num_heads</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">q_proj</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">embed_dim</code><code class="p">,</code> <code class="n">embed_dim</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">k_proj</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">embed_dim</code><code class="p">,</code> <code class="n">embed_dim</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">v_proj</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">embed_dim</code><code class="p">,</code> <code class="n">embed_dim</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">out_proj</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">embed_dim</code><code class="p">,</code> <code class="n">embed_dim</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">dropout</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">split_heads</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="k">return</code> <code class="n">X</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">size</code><code class="p">(</code><code class="mi">0</code><code class="p">),</code> <code class="n">X</code><code class="o">.</code><code class="n">size</code><code class="p">(</code><code class="mi">1</code><code class="p">),</code> <code class="bp">self</code><code class="o">.</code><code class="n">h</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">d</code><code class="p">)</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">query</code><code class="p">,</code> <code class="n">key</code><code class="p">,</code> <code class="n">value</code><code class="p">):</code>
        <code class="n">q</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">split_heads</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">q_proj</code><code class="p">(</code><code class="n">query</code><code class="p">))</code>  <code class="c1"># (B, h, Lq, d)</code>
        <code class="n">k</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">split_heads</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">k_proj</code><code class="p">(</code><code class="n">key</code><code class="p">))</code>  <code class="c1"># (B, h, Lk, d)</code>
        <code class="n">v</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">split_heads</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">v_proj</code><code class="p">(</code><code class="n">value</code><code class="p">))</code>  <code class="c1"># (B, h, Lv, d) with Lv=Lk</code>
        <code class="n">scores</code> <code class="o">=</code> <code class="n">q</code> <code class="o">@</code> <code class="n">k</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code> <code class="o">/</code> <code class="bp">self</code><code class="o">.</code><code class="n">d</code><code class="o">**</code><code class="mf">0.5</code>  <code class="c1"># (B, h, Lq, Lk)</code>
        <code class="n">weights</code> <code class="o">=</code> <code class="n">scores</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">dim</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>  <code class="c1"># (B, h, Lq, Lk)</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code><code class="p">(</code><code class="n">weights</code><code class="p">)</code> <code class="o">@</code> <code class="n">v</code>  <code class="c1"># (B, h, Lq, d)</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">Z</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>  <code class="c1"># (B, Lq, h, d)</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">Z</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">Z</code><code class="o">.</code><code class="n">size</code><code class="p">(</code><code class="mi">0</code><code class="p">),</code> <code class="n">Z</code><code class="o">.</code><code class="n">size</code><code class="p">(</code><code class="mi">1</code><code class="p">),</code> <code class="bp">self</code><code class="o">.</code><code class="n">h</code> <code class="o">*</code> <code class="bp">self</code><code class="o">.</code><code class="n">d</code><code class="p">)</code>  <code class="c1"># (B, Lq, h × d)</code>
        <code class="k">return</code> <code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">out_proj</code><code class="p">(</code><code class="n">Z</code><code class="p">),</code> <code class="n">weights</code><code class="p">)</code>  <code class="c1"># (B, Lq, h × d)</code></pre>

<p>Let’s go through this code:</p>

<ul>
<li>
<p>The constructor stores the number of heads <code translate="no">self.h</code> and computes the number of dimensions per head <code translate="no">self.d</code>, then it creates the necessary modules. Note that the embedding size must be divisible by the number of heads.</p>
</li>
<li>
<p>The <code translate="no">split_heads()</code> method<a data-type="indexterm" data-primary="torch" data-secondary="forward()" id="id3455"/> is used in the <code translate="no">forward()</code> method. It splits its input <code translate="no">X</code> along its last dimension (one split per head), converting it from a 3D tensor of shape [<em>B</em>, <em>L</em>, <em>h</em> × <em>d</em>] to a 4D tensor of shape [<em>B</em>, <em>L</em>, <em>h</em>, <em>d</em>], where <em>B</em> is the batch size, <em>L</em> is the max length of the input sequences (specifically <em>L</em><sub>k</sub> for the key and value, or <em>L</em><sub>q</sub> for the query), <em>h</em> is the number of heads, and <em>d</em> is the number of dimensions per head (i.e., <em>h</em> × <em>d</em> = embedding size). The dimensions 1 and 2 are then swapped to get a tensor of shape [<em>B</em>, <em>h</em>, <em>L</em>, <em>d</em>]: since the matrix multiplication operator <code translate="no">@</code> only works on the last two dimensions, it won’t touch the first two dimensions <em>B</em> and <em>h</em>, so we will be able to use this operator to compute the scores across all instances in the batch and across all attention heads, all in one shot (<code translate="no">q @ k.transpose(2, 3)</code>). The same will be true when computing all the attention outputs (<code translate="no">weights @ v</code>).</p>
</li>
</ul>

<ul class="less_space pagebreak-before">
<li>
<p>The <code translate="no">forward()</code> method starts by applying a linear transformation to the query, key, and value, and passes the result through the <code translate="no">split_heads()</code> method. The next three lines compute <a data-type="xref" href="#scaled_dot_product_attention">Equation 15-1</a>, plus a bit of dropout on the weights. Next we swap back dimensions 1 and 2 to ensure that the dimensions <em>h</em> and <em>d</em> are next to each other again, then we reshape the tensor back to 3D: this will concatenate the outputs of all heads. We can then apply the output linear transformation and return the result, along with the weights (in case we need them later).</p>
</li>
</ul>
<div data-type="tip"><h6>Tip</h6>
<p>Don’t worry if it takes some time to fully grasp this, it’s not easy. Of course, you can drive a car without fully understanding how the engine works, but some of the transformer improvements described in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch16.html#vit_chapter">16</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch17.html#speedup_chapter">17</a> will only make sense if you understand MHA.</p>
</div>

<p>But wait! We’re missing one important detail: masking<a data-type="indexterm" data-primary="masking" id="xi_masking1517654_1"/><a data-type="indexterm" data-primary="masked self-attention layers" id="id3456"/>. Indeed, as we discussed earlier, the decoder’s masked self-attention layers<a data-type="indexterm" data-primary="self-attention" data-secondary="masking" id="id3457"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="self-attention" id="id3458"/> must only consider previous tokens when trying to predict what the next token is (or else it would be cheating). Moreover, if the key contains padding tokens, we want to ignore them as well. So let’s update the <code translate="no">forward()</code> method to support two additional arguments:</p>
<dl>
<dt><code translate="no">attn_mask</code></dt>
<dd>
<p>A boolean mask of shape [<em>L</em><sub>q</sub>, <em>L</em><sub>k</sub>] that we will use to control which key tokens each query token should ignore (<code translate="no">True</code> to ignore, <code translate="no">False</code> to attend)</p>
</dd>
<dt><code translate="no">key_padding_mask</code></dt>
<dd>
<p>A boolean mask of shape [<em>B</em>, <em>L</em><sub>k</sub>] to locate the padding tokens in each key</p>
</dd>
</dl>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">query</code><code class="p">,</code> <code class="n">key</code><code class="p">,</code> <code class="n">value</code><code class="p">,</code> <code class="n">attn_mask</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code> <code class="n">key_padding_mask</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
    <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># compute the scores exactly like earlier</code>
    <code class="k">if</code> <code class="n">attn_mask</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code><code class="p">:</code>
        <code class="n">scores</code> <code class="o">=</code> <code class="n">scores</code><code class="o">.</code><code class="n">masked_fill</code><code class="p">(</code><code class="n">attn_mask</code><code class="p">,</code> <code class="o">-</code><code class="n">torch</code><code class="o">.</code><code class="n">inf</code><code class="p">)</code>  <code class="c1"># (B, h, Lq, Lk)</code>
    <code class="k">if</code> <code class="n">key_padding_mask</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code><code class="p">:</code>
        <code class="n">mask</code> <code class="o">=</code> <code class="n">key_padding_mask</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>  <code class="c1"># (B, 1, 1, Lk)</code>
        <code class="n">scores</code> <code class="o">=</code> <code class="n">scores</code><code class="o">.</code><code class="n">masked_fill</code><code class="p">(</code><code class="n">mask</code><code class="p">,</code> <code class="o">-</code><code class="n">torch</code><code class="o">.</code><code class="n">inf</code><code class="p">)</code>  <code class="c1"># (B, h, Lq, Lk)</code>
    <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># compute the weights and the outputs exactly like earlier</code></pre>

<p>This code replaces the scores we want to ignore with negative infinity, so the corresponding weights will be zero after the softmax operation (if we tried to zero out these weights directly, the remaining weights would not add up to 1). Note that the masks are broadcast automatically: <code translate="no">attn_mask</code> is broadcast across the whole batch and all attention heads, and <code translate="no">key_padding_mask</code> is broadcast across all heads and all query tokens.</p>

<p>PyTorch has a very similar <code translate="no">nn.MultiheadAttention</code> module<a data-type="indexterm" data-primary="torch" data-secondary="nn.MultiheadAttention" id="id3459"/>, which is much more optimized (e.g., it can often fuse the three input projections into one). It has the same arguments, which behave in exactly the same way. It also has a few more. Here are the most important:</p>

<ul>
<li>
<p>The constructor has a <code translate="no">batch_first</code> argument which defaults to <code translate="no">False</code>, so the module expects the batch dimension to come after the sequence length dimension. You must set <code translate="no">batch_first=True</code> if you prefer the batch dimension to come first, like in our custom implementation.</p>
</li>
<li>
<p>The <code translate="no">forward()</code> method has a <code translate="no">need_weights</code> argument that defaults to <code translate="no">True</code>. If you don’t need to use the weights returned by this module, you should set this argument to <code translate="no">False</code>, as it sometimes allows for some optimizations. When <code translate="no">need_weights</code> is set to <code translate="no">False</code>, the method returns <code translate="no">None</code> instead of the weights.</p>
</li>
<li>
<p>The <code translate="no">forward()</code> method also has an <code translate="no">is_causal</code> argument: if (and only if) the <code translate="no">attn_mask</code> is set and is a <em>causal mask</em>, then you can set <code translate="no">is_causal=True</code> to allow for some performance optimizations. A causal mask<a data-type="indexterm" data-primary="causal mask" id="id3460"/> allows each query token to attend to all previous tokens (including itself), but doesn’t allow it to attend to tokens located after it. In other words, a causal mask contains <code translate="no">True</code> above the main diagonal, and <code translate="no">False</code> everywhere else. This is the mask needed for the masked self-attention layers<a data-type="indexterm" data-startref="xi_masking1517654_1" id="id3461"/><a data-type="indexterm" data-primary="self-attention" data-secondary="masking" id="id3462"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="self-attention" id="id3463"/>.</p>
</li>
</ul>

<p>Now that we have the main ingredient, we’re ready to implement the rest of the Transformer model.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Building the Rest of the Transformer"><div class="sect2" id="id283">
<h2>Building the Rest of the Transformer</h2>

<p>The rest of the Transformer architecture is much more straightforward. Let’s start with the encoder block. The following implementation closely matches the encoder block represented on the left side of <a data-type="xref" href="#transformer_diagram">Figure 15-3</a>, except it sprinkles a bit of dropout after the self-attention layer and after both dense layers in the feedforward module:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">TransformerEncoderLayer</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">d_model</code><code class="p">,</code> <code class="n">nhead</code><code class="p">,</code> <code class="n">dim_feedforward</code><code class="o">=</code><code class="mi">2048</code><code class="p">,</code> <code class="n">dropout</code><code class="o">=</code><code class="mf">0.1</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">self_attn</code> <code class="o">=</code> <code class="n">MultiheadAttention</code><code class="p">(</code><code class="n">d_model</code><code class="p">,</code> <code class="n">nhead</code><code class="p">,</code> <code class="n">dropout</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">linear1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">d_model</code><code class="p">,</code> <code class="n">dim_feedforward</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">dropout</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">linear2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">dim_feedforward</code><code class="p">,</code> <code class="n">d_model</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">norm1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">LayerNorm</code><code class="p">(</code><code class="n">d_model</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">norm2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">LayerNorm</code><code class="p">(</code><code class="n">d_model</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">src</code><code class="p">,</code> <code class="n">src_mask</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code> <code class="n">src_key_padding_mask</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
        <code class="n">attn</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">self_attn</code><code class="p">(</code><code class="n">src</code><code class="p">,</code> <code class="n">src</code><code class="p">,</code> <code class="n">src</code><code class="p">,</code> <code class="n">attn_mask</code><code class="o">=</code><code class="n">src_mask</code><code class="p">,</code>
                                 <code class="n">key_padding_mask</code><code class="o">=</code><code class="n">src_key_padding_mask</code><code class="p">)</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">norm1</code><code class="p">(</code><code class="n">src</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code><code class="p">(</code><code class="n">attn</code><code class="p">))</code>
        <code class="n">ff</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">linear2</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dropout</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">linear1</code><code class="p">(</code><code class="n">Z</code><code class="p">)</code><code class="o">.</code><code class="n">relu</code><code class="p">())))</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">norm2</code><code class="p">(</code><code class="n">Z</code> <code class="o">+</code> <code class="n">ff</code><code class="p">)</code></pre>

<p>Notice that the feedforward block is composed of a first <code translate="no">Linear</code> layer that expands the dimensionality to 2048 (by default), followed by a nonlinearity (ReLU in this case), then a second <code translate="no">Linear</code> layer that projects the data back down to the original embedding size<a data-type="indexterm" data-primary="embedding size" id="id3464"/> (also called<a data-type="indexterm" data-primary="model dimension (d_model)" id="id3465"/> the <em>model dimension</em>, <code translate="no">d_model</code>). This <em>reverse bottleneck</em> increases<a data-type="indexterm" data-primary="reverse bottleneck" id="id3466"/> the expressive power of the nonlinearity, allowing the model to learn much richer combinations of features. This idea was explored further in the later MobileNetv2 paper, whose authors coined<a data-type="indexterm" data-primary="inverted residual network" id="id3467"/> the term <em>inverted residual network</em>.</p>

<p>In the encoder, the <code translate="no">src_mask</code> argument is generally not used, since the encoder allows each token to attend to all tokens, even ones located after it. However, the user is expected to set the <code translate="no">key_padding_mask</code> appropriately.</p>

<p>Now here’s an implementation of the decoder block. It closely matches the decoder block represented on the righthand side of <a data-type="xref" href="#transformer_diagram">Figure 15-3</a>, with some additional 
<span class="keep-together">dropout:</span></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">TransformerDecoderLayer</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># similar constructor, with 2 MHA, 3 Linear, 3 LayerNorm, 1 Dropout</code>
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">tgt</code><code class="p">,</code> <code class="n">memory</code><code class="p">,</code> <code class="n">tgt_mask</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code> <code class="n">memory_mask</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code>
                <code class="n">tgt_key_padding_mask</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code> <code class="n">memory_key_padding_mask</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
        <code class="n">attn1</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">self_attn</code><code class="p">(</code><code class="n">tgt</code><code class="p">,</code> <code class="n">tgt</code><code class="p">,</code> <code class="n">tgt</code><code class="p">,</code> <code class="n">attn_mask</code><code class="o">=</code><code class="n">tgt_mask</code><code class="p">,</code>
                                  <code class="n">key_padding_mask</code><code class="o">=</code><code class="n">tgt_key_padding_mask</code><code class="p">)</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">norm1</code><code class="p">(</code><code class="n">tgt</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code><code class="p">(</code><code class="n">attn1</code><code class="p">))</code>
        <code class="n">attn2</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">multihead_attn</code><code class="p">(</code><code class="n">Z</code><code class="p">,</code> <code class="n">memory</code><code class="p">,</code> <code class="n">memory</code><code class="p">,</code> <code class="n">attn_mask</code><code class="o">=</code><code class="n">memory_mask</code><code class="p">,</code>
                                       <code class="n">key_padding_mask</code><code class="o">=</code><code class="n">memory_key_padding_mask</code><code class="p">)</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">norm2</code><code class="p">(</code><code class="n">Z</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code><code class="p">(</code><code class="n">attn2</code><code class="p">))</code>
        <code class="n">ff</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">linear2</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dropout</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">linear1</code><code class="p">(</code><code class="n">Z</code><code class="p">)</code><code class="o">.</code><code class="n">relu</code><code class="p">())))</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">norm3</code><code class="p">(</code><code class="n">Z</code> <code class="o">+</code> <code class="n">ff</code><code class="p">)</code></pre>

<p>The <code translate="no">memory</code> argument corresponds to the output of the encoder. For full flexibility, we let the user pass the appropriate masks to the <code translate="no">forward()</code> method. In general, you will need to set the padding masks appropriately (both for the memory and target), and set the <code translate="no">tgt_mask</code> to a causal mask (we will see how shortly).</p>

<p>PyTorch<a data-type="indexterm" data-primary="torch" data-secondary="nn.TransformerEncoderLayer" id="id3468"/><a data-type="indexterm" data-primary="torch" data-secondary="nn.TransformerDecoderLayer" id="id3469"/> actually provides <code translate="no">nn.TransformerEncoderLayer</code> and <code translate="no">nn.TransformerDecoderLayer</code> out of the box, with the same arguments, plus a few more: most importantly <code translate="no">batch_first</code>, which you must set to <code translate="no">True</code> if the batch dimension is first, plus one <code translate="no">*_is_causal</code> argument for each attention mask, and an <code translate="no">activation</code> argument that defaults to “relu”. Many state-of-the-art transformers use a more advanced activation such as GELU (introduced in <a data-type="xref" href="ch11.html#deep_chapter">Chapter 11</a>).</p>

<p>PyTorch also provides three more transformer modules (writing a custom module for each of these is left as an exercise for the reader—see the notebook for a solution):</p>
<dl>
<dt><code translate="no">nn.TransformerEncoder</code></dt>
<dd>
<p>Simply chains the desired number of encoder layers. Its constructor takes an encoder layer plus the desired number of layers <code translate="no">num_layers</code>, and it clones the given encoder layer <code translate="no">num_layers</code> times. The constructor also takes an optional normalization layer, which (if provided) is applied to the final output.</p>
</dd>
<dt><code translate="no">nn.TransformerDecoder</code></dt>
<dd>
<p>Same, except it chains decoder layers instead of encoder layers.</p>
</dd>
<dt><code translate="no">nn.Transformer</code></dt>
<dd>
<p>Creates an encoder and a decoder<a data-type="indexterm" data-primary="torch" data-secondary="nn.Transformer" id="id3470"/> (both with layer norm), and chains them.</p>
</dd>
</dl>

<p>Congratulations! You now know how to build a full Transformer model from scratch. You only need to add a final <code translate="no">Linear</code> layer and use the <code translate="no">nn.CrossEntropyLoss</code> to get the full architecture shown in <a data-type="xref" href="#transformer_diagram">Figure 15-3</a> (as we saw in earlier chapters, the softmax layer is implicitly included in the loss)<a data-type="indexterm" data-startref="xi_multiheadattentionMHAinencoderdecoderprocess1510337_1" id="id3471"/><a data-type="indexterm" data-startref="xi_attentionmechanisms154943_1" id="id3472"/>. Now let’s see how to use a Transformer model to translate English to Spanish.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Building an English-to-Spanish Transformer"><div class="sect1" id="id284">
<h1>Building an English-to-Spanish Transformer</h1>

<p>It’s time to build our NMT Transformer model<a data-type="indexterm" data-primary="neural machine translation (NMT)" data-secondary="building a Transformer model" id="xi_neuralmachinetranslationNMTbuildingaTransformermodel1526645_1"/>. For this, we’ll use our <code>Positional​Embedding</code> module and PyTorch’s <code translate="no">nn.Transformer</code> (our custom <code translate="no">Transformer</code> module works fine, but it’s slower):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">NmtTransformer</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">vocab_size</code><code class="p">,</code> <code class="n">max_length</code><code class="p">,</code> <code class="n">embed_dim</code><code class="o">=</code><code class="mi">512</code><code class="p">,</code> <code class="n">pad_id</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>
                 <code class="n">num_heads</code><code class="o">=</code><code class="mi">8</code><code class="p">,</code> <code class="n">num_layers</code><code class="o">=</code><code class="mi">6</code><code class="p">,</code> <code class="n">dropout</code><code class="o">=</code><code class="mf">0.1</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">embed</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">embed_dim</code><code class="p">,</code> <code class="n">padding_idx</code><code class="o">=</code><code class="n">pad_id</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">pos_embed</code> <code class="o">=</code> <code class="n">PositionalEmbedding</code><code class="p">(</code><code class="n">max_length</code><code class="p">,</code> <code class="n">embed_dim</code><code class="p">,</code> <code class="n">dropout</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">transformer</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Transformer</code><code class="p">(</code>
            <code class="n">embed_dim</code><code class="p">,</code> <code class="n">num_heads</code><code class="p">,</code> <code class="n">num_encoder_layers</code><code class="o">=</code><code class="n">num_layers</code><code class="p">,</code>
            <code class="n">num_decoder_layers</code><code class="o">=</code><code class="n">num_layers</code><code class="p">,</code> <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">output</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">embed_dim</code><code class="p">,</code> <code class="n">vocab_size</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">pair</code><code class="p">):</code>
        <code class="n">src_embeds</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">pos_embed</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">embed</code><code class="p">(</code><code class="n">pair</code><code class="o">.</code><code class="n">src_token_ids</code><code class="p">))</code>
        <code class="n">tgt_embeds</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">pos_embed</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">embed</code><code class="p">(</code><code class="n">pair</code><code class="o">.</code><code class="n">tgt_token_ids</code><code class="p">))</code>
        <code class="n">src_pad_mask</code> <code class="o">=</code> <code class="o">~</code><code class="n">pair</code><code class="o">.</code><code class="n">src_mask</code><code class="o">.</code><code class="n">bool</code><code class="p">()</code>
        <code class="n">tgt_pad_mask</code> <code class="o">=</code> <code class="o">~</code><code class="n">pair</code><code class="o">.</code><code class="n">tgt_mask</code><code class="o">.</code><code class="n">bool</code><code class="p">()</code>
        <code class="n">size</code> <code class="o">=</code> <code class="p">[</code><code class="n">pair</code><code class="o">.</code><code class="n">tgt_token_ids</code><code class="o">.</code><code class="n">size</code><code class="p">(</code><code class="mi">1</code><code class="p">)]</code> <code class="o">*</code> <code class="mi">2</code>
        <code class="n">full_mask</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">full</code><code class="p">(</code><code class="n">size</code><code class="p">,</code> <code class="kc">True</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">tgt_pad_mask</code><code class="o">.</code><code class="n">device</code><code class="p">)</code>
        <code class="n">causal_mask</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">triu</code><code class="p">(</code><code class="n">full_mask</code><code class="p">,</code> <code class="n">diagonal</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
        <code class="n">out_decoder</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">transformer</code><code class="p">(</code><code class="n">src_embeds</code><code class="p">,</code> <code class="n">tgt_embeds</code><code class="p">,</code>
                                       <code class="n">src_key_padding_mask</code><code class="o">=</code><code class="n">src_pad_mask</code><code class="p">,</code>
                                       <code class="n">memory_key_padding_mask</code><code class="o">=</code><code class="n">src_pad_mask</code><code class="p">,</code>
                                       <code class="n">tgt_mask</code><code class="o">=</code><code class="n">causal_mask</code><code class="p">,</code> <code class="n">tgt_is_causal</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                                       <code class="n">tgt_key_padding_mask</code><code class="o">=</code><code class="n">tgt_pad_mask</code><code class="p">)</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">output</code><code class="p">(</code><code class="n">out_decoder</code><code class="p">)</code><code class="o">.</code><code class="n">permute</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code></pre>

<p>Let’s go through this code:</p>

<ul>
<li>
<p>The constructor is straightforward: we just create the necessary modules.</p>
</li>
<li>
<p>The <code translate="no">forward()</code> method takes an <code translate="no">NmtPair</code> as input (this class was defined in <a data-type="xref" href="ch14.html#nlp_chapter">Chapter 14</a>). The method starts by embedding the input tokens for both the source and target inputs, and it adds the positional encodings to both.</p>
</li>
<li>
<p>Then the code uses the <em>not</em> operator (<code translate="no">~</code>) to invert both the source and target masks because they contain <code translate="no">False</code> for each padding token, but <code>nn.Multihead​Attention</code> expects <code translate="no">True</code> for tokens that it should ignore.</p>
</li>
<li>
<p>Next, we create a square matrix of shape [<em>L</em><sub>q</sub>, <em>L</em><sub>q</sub>], full of <code translate="no">True</code>, and we get all elements above the main diagonal using the <code translate="no">torch.triu()</code> function, with the rest defaulting to <code translate="no">False</code>. This results in a causal mask that we can use as the <code translate="no">tgt_mask</code> for the transformer: it will use this mask for the masked self-attention layer. Alternatively, you could call <code>nn.Transformer.gen⁠erate_​square_subsequent_mask()</code> to create the causal mask: just pass it the sequence length (<code translate="no">pair.tgt_token_ids.size(1)</code>) and set <code translate="no">dtype=torch.bool</code>.</p>
</li>
<li>
<p>We then call the transformer, passing it the source and target embeddings, as well as all the appropriate masks.</p>
</li>
<li>
<p>Lastly, we pass the result through the output <code translate="no">Linear</code> layer, and we permute the last two dimensions because <code translate="no">nn.CrossEntropyLoss</code> expects the class dimension to be dimension 1.</p>
</li>
</ul>

<p>We can now create an instance of this model and train it exactly like our RNN encoder-decoder in <a data-type="xref" href="ch14.html#nlp_chapter">Chapter 14</a>. To speed up training and reduce overfitting, you can shrink the transformer quite a bit—use 4 heads instead of 8, just 2 layers in both the encoder and the decoder, and use an embedding size of 128:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">nmt_tr_model</code> <code class="o">=</code> <code class="n">NmtTransformer</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">max_length</code><code class="p">,</code> <code class="n">embed_dim</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code> <code class="n">pad_id</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>
                              <code class="n">num_heads</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">num_layers</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">dropout</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># train this model exactly like the encoder-decoder in Chapter 14</code></pre>

<p>Let’s see how well this model performs:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">nmt_tr_model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">translate</code><code class="p">(</code><code class="n">nmt_tr_model</code><code class="p">,</code><code class="s2">"I like to play soccer with my friends at the beach"</code><code class="p">)</code><code class="w"/>
<code class="go">' Me gusta jugar al fútbol con mis amigos en la playa . &lt;/s&gt;'</code></pre>

<p>Great, even this tiny transformer trained for 20 epochs works rather well, so imagine a much bigger one trained on a much larger dataset, and you can start to see how ChatGPT and its friends can be so impressive<a data-type="indexterm" data-startref="xi_neuralmachinetranslationNMTbuildingaTransformermodel1526645_1" id="id3473"/>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Before we move on to other models, it’s important to clean up the GPU RAM, or else it will quickly become saturated. For this, delete all variables that are no longer needed—especially models, optimizers, tensors, and datasets—using the <code translate="no">del</code> keyword, then call the <code translate="no">gc.collect()</code> function to run Python’s garbage collector. When using a CUDA or AMD device, you must also call <code translate="no">torch.cuda.empty_cache()</code>. On Colab, you can view the available GPU RAM by selecting Runtime → “View resources” from the menu.</p>
</div>

<p>Now that you have a good understanding of the original Transformer architecture, let’s look at encoder-only transformers<a data-type="indexterm" data-startref="xi_Transformerarchitecture153339_1" id="id3474"/>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Encoder-Only Transformers for Natural &#10;Language Understanding"><div class="sect1" id="id285">
<h1>Encoder-Only Transformers for Natural 
<span class="keep-together">Language Understanding</span></h1>

<p>When Google released<a data-type="indexterm" data-primary="encoder-only transformers" id="xi_encoderonlytransformers1532921_1"/><a data-type="indexterm" data-primary="encoder-only transformers" data-secondary="BERT" id="xi_encoderonlytransformersBERT1532921_1"/><a data-type="indexterm" data-primary="BERT (Bidirectional Encoder Representations from Transformers)" data-secondary="for natural language understanding" data-secondary-sortas="natural language understanding" id="xi_BERTBidirectionalEncoderRepresentationsfromTransformersfornaturallanguageunderstanding1532921_1"/><a data-type="indexterm" data-primary="transformers" data-secondary="encoder-only" id="xi_transformersencoderonly1532921_1"/> the <a href="https://homl.info/bert">BERT model in 2018</a>,⁠<sup><a data-type="noteref" id="id3475-marker" href="ch15.html#id3475">7</a></sup> it proved that an encoder-only transformer can tackle a wide variety of natural language tasks: sentence classification, token classification, multiple choice question answering, and more! BERT also confirmed the effectiveness of self-supervised pretraining on a large corpus for transfer learning: BERT can indeed achieve excellent performance on many tasks, just by fine-tuning on a fairly small dataset for each task. Let’s start by looking at BERT’s architecture, then we’ll look at how it was pretrained, and how you can fine-tune it for your own tasks.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Encoder-only models<a data-type="indexterm" data-primary="decoder-only transformers" data-secondary="versus encoder-only" data-secondary-sortas="encoder-only" id="id3476"/><a data-type="indexterm" data-primary="transformers" data-secondary="decoder-only" id="id3477"/><a data-type="indexterm" data-primary="encoder-only transformers" data-secondary="versus decoder-only" data-secondary-sortas="decoder-only" id="id3478"/> are generally not used for text generation tasks, such as autocompletion, translation, summarization, or chatbots, because they’re much slower at this task than decoders. Decoders are faster because they are causal, so a good implementation can cache and reuse its previous state when predicting a new token. Conversely, encoders use nonmasked multi-head attention layers only, so they are naturally bidirectional; hence the B in BERT (Bidirectional Encoder Representations from Transformers). Whenever a new token is added, everything needs to be 
<span class="keep-together">recomputed.</span></p>
</div>








<section data-type="sect2" data-pdf-bookmark="BERT’s Architecture"><div class="sect2" id="id286">
<h2>BERT’s Architecture</h2>

<p>BERT’s architecture<a data-type="indexterm" data-primary="BERT (Bidirectional Encoder Representations from Transformers)" id="xi_BidirectionalEncoderRepresentationsfromTransformersBERTmodel1533420_1"/> is almost identical to the original Transformer’s encoder, with just three differences:</p>
<ol>
<li>
<p>It’s much bigger. BERT-base has 12 encoder blocks, 12 attention heads, and 768-dimensional embeddings, and BERT-large has 24 blocks, 16 heads, and 1,024 dimensions (while the original Transformer has 6 blocks, 8 heads, and 512 dimensions). It also uses trainable positional embeddings and supports input sentences up to 512 tokens.</p>
</li>
<li>
<p>It applies layer-norm<a data-type="indexterm" data-primary="pre-LN versus post-LN for sublayers" id="id3479"/> just <em>before</em> each sublayer (attention or feedforward) rather than <em>after</em> each skip connection. This is called <em>pre-LN</em>, as opposed to <em>post-LN</em>, and it ensures that the inputs of each sublayer are normalized, which stabilizes training and reduces sensitivity to weight initialization. PyTorch’s transformer modules default to post-LN, but they have a <code translate="no">norm_first</code> argument which you can set to <code translate="no">True</code> if you prefer pre-LN (however, some optimizations may not be implemented for pre-LN).</p>
</li>
<li>
<p>It lets you split the input sentence into two <em>segments</em> if needed. This is useful for tasks that require a pair of input sentences, such as natural language inference (i.e., does sentence A entail sentence B?) or multiple choice question answering (i.e., given question A, how good is answer B?). To pass two sentences to BERT, you must first append<a data-type="indexterm" data-primary="separation token (SEP), BERT" id="id3480"/> a <em>separation token</em> [SEP] to each one, then concatenate them. Furthermore<a data-type="indexterm" data-primary="segment embedding" id="id3481"/>, a trainable <em>segment embedding</em> is added<a data-type="indexterm" data-primary="embeddings" data-secondary="BERT" id="id3482"/><a data-type="indexterm" data-primary="embeddings" data-secondary="segment" id="id3483"/> to each token’s representation: segment embedding #0 is added to all tokens within segment #0, and segment embedding #1 is added to all tokens within segment #1. In theory, we could have more segments, but BERT was only pretrained on inputs composed of one or two segments. Note that the positional encodings are also added to each token’s representation, as usual (i.e., relative to the full input sequence, not relative to the individual segments).</p>
</li>

</ol>

<p>That’s all! Now let’s look at how BERT was pretrained.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="BERT Pretraining"><div class="sect2" id="id287">
<h2>BERT Pretraining</h2>

<p>The authors proposed two self-supervised pretraining<a data-type="indexterm" data-primary="pretraining and pretrained layers" data-secondary="BERT self-supervised pretraining" id="xi_pretrainingandpretrainedlayersBERTselfsupervisedpretraining1534353_1"/><a data-type="indexterm" data-primary="self-supervised learning" id="xi_selfsupervisedlearning1534353_1"/> tasks:</p>
<dl>
<dt>Masked language model (MLM)</dt>
<dd>
<p>Each token in a sentence has a 15% probability of being replaced with a mask token<a data-type="indexterm" data-primary="masked language model (MLM)" id="id3484"/><a data-type="indexterm" data-primary="MLM (masked language model)" id="id3485"/>, and the model is trained to predict what the original tokens were. This is often called<a data-type="indexterm" data-primary="cloze task" id="id3486"/> a <em>cloze task</em> (i.e., fill in the blanks). For example, if the original sentence is “She had fun at the birthday party”, then the model may be given the sentence “She [MASK] fun at the [MASK] party” and it must predict the original sentence: the loss is only computed on the mask token outputs.</p>

<p>To be more precise, some of the masked tokens are not truly masked: 10% are instead replaced by random tokens, and 10% are just left alone, neither masked nor randomized. Why is that? Well, the random tokens force the model to perform well even when mask tokens are absent: this is important since most downstream tasks don’t use any mask tokens. As for the untouched tokens, they make the prediction trivial, which encourages the model to pay attention to the input token located at the position of the token being predicted. Without them, the model would soon learn to ignore this token and rely solely on the other tokens.</p>
</dd>
<dt>Next sentence prediction (NSP)</dt>
<dd>
<p>The model is trained<a data-type="indexterm" data-primary="next sentence prediction (NSP)" id="id3487"/><a data-type="indexterm" data-primary="NSP (next sentence prediction)" id="id3488"/><a data-type="indexterm" data-primary="predictions" data-secondary="next sentence" id="id3489"/> to predict whether two sentences are consecutive or not. For example, it should predict that “The dog sleeps” and “It snores loudly” are consecutive sentences, while “The dog sleeps” and “The Earth orbits the Sun” are not consecutive.</p>

<p>This is a binary classification task, which the authors chose to implement by introducing<a data-type="indexterm" data-primary="class token" id="id3490"/> a new <em>class token</em> [CLS]: this token is inserted at the beginning of the input sequence (position #0, segment #0), and during training the encoder’s output, this token is passed through a binary classification head (i.e., a linear layer with a single unit, followed by the sigmoid function, and trained<a data-type="indexterm" data-primary="torch" data-secondary="nn.BCELoss" id="id3491"/> using <code translate="no">nn.BCELoss</code>, or just a linear layer with a single unit trained using <code>nn.BCEWith​Lo⁠gitsLoss</code>).</p>
</dd>
</dl>

<p>BERT was pretrained on both MLM and NSP simultaneously (see <a data-type="xref" href="#bert_pretraining_diagram">Figure 15-5</a> and the left side of <a data-type="xref" href="#bert_diagram">Figure 15-6</a>), using a large corpus of text—specifically the English Wikipedia and BooksCorpus. The goal of NSP was to make the class token’s contextualized embedding a good representation of the whole input sequence. At first, it seemed that it indeed produced good sentence embeddings, but it was later shown that simply pooling all the contextualized embeddings (e.g., by computing their mean) yielded better results. In fact, researchers showed that NSP did not help much overall, so it was dropped in most later architectures.</p>

<p>In <a data-type="xref" href="ch14.html#nlp_chapter">Chapter 14</a>, we saw how to use the Transformers library to download a pretrained BERT model and its tokenizer. But you may want to train a BERT model from scratch, for example, if you’re dealing with a domain-specific corpus of text. For this, one option is to build BERT yourself using the <code translate="no">nn.TransformerEncoder</code> module<a data-type="indexterm" data-primary="torch" data-secondary="nn.TransformerEncoderLayer" id="id3492"/> (e.g., based on an <code translate="no">nn.TransformerEncoderLayer</code> with <code translate="no">norm_first=True</code> to respect BERT’s architecture), then preprocess your dataset according to the MLM algorithm, and train your model.</p>

<figure class="smallereighty"><div id="bert_pretraining_diagram" class="figure">
<img src="assets/hmls_1505.png" alt="Diagram illustrating BERT pretraining showing input and target sequences for MLM and NSP tasks, highlighting masked, random, and unchanged tokens." width="1399" height="913"/>
<h6><span class="label">Figure 15-5. </span>Input and target during BERT pretraining, using MLM and NSP</h6>
</div></figure>

<p>However, there’s an easier way, using the Transformers library<a data-type="indexterm" data-primary="Transformers library" data-secondary="BERT training from scratch" id="xi_TransformerslibraryBERTtrainingfromscratch15361578_1"/>. Let’s start by creating a tokenizer and a randomly initialized BERT model. For simplicity, we use a pretrained tokenizer, but of course you can train one from scratch instead, if you prefer. Make sure to tweak the <code translate="no">BertConfig</code> depending on your training budget, and the size and complexity of your dataset:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">BertConfig</code><code class="p">,</code> <code class="n">BertForMaskedLM</code><code class="p">,</code> <code class="n">BertTokenizerFast</code>

<code class="n">bert_tokenizer</code> <code class="o">=</code> <code class="n">BertTokenizerFast</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"bert-base-uncased"</code><code class="p">)</code>
<code class="n">config</code> <code class="o">=</code> <code class="n">BertConfig</code><code class="p">(</code>  <code class="c1"># adapt to training budget, and dataset size &amp; complexity</code>
    <code class="n">vocab_size</code><code class="o">=</code><code class="n">bert_tokenizer</code><code class="o">.</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">hidden_size</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code> <code class="n">num_hidden_layers</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
    <code class="n">num_attention_heads</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">intermediate_size</code><code class="o">=</code><code class="mi">512</code><code class="p">,</code> <code class="n">max_position_embeddings</code><code class="o">=</code><code class="mi">128</code><code class="p">)</code>
<code class="n">bert</code> <code class="o">=</code> <code class="n">BertForMaskedLM</code><code class="p">(</code><code class="n">config</code><code class="p">)</code></pre>

<p>Next, let’s download the WikiText dataset (in real life, you would use your own dataset instead), and tokenize it:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">datasets</code> <code class="kn">import</code> <code class="n">load_dataset</code>

<code class="k">def</code> <code class="nf">tokenize</code><code class="p">(</code><code class="n">example</code><code class="p">,</code> <code class="n">tokenizer</code><code class="o">=</code><code class="n">bert_tokenizer</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">tokenizer</code><code class="p">(</code><code class="n">example</code><code class="p">[</code><code class="s2">"text"</code><code class="p">],</code> <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">max_length</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code>
                     <code class="n">padding</code><code class="o">=</code><code class="s2">"max_length"</code><code class="p">)</code>

<code class="n">mlm_dataset</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"wikitext"</code><code class="p">,</code> <code class="s2">"wikitext-2-raw-v1"</code><code class="p">,</code> <code class="n">split</code><code class="o">=</code><code class="s2">"train"</code><code class="p">)</code>
<code class="n">mlm_dataset</code> <code class="o">=</code> <code class="n">mlm_dataset</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">tokenize</code><code class="p">,</code> <code class="n">batched</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>

<p>This is where MLM<a data-type="indexterm" data-primary="masked language model (MLM)" id="id3493"/><a data-type="indexterm" data-primary="MLM (masked language model)" id="id3494"/> comes in. We create a data collator, whose role is to bundle samples into batches, and we set its <code translate="no">mlm</code> argument to <code translate="no">True</code> to activate MLM, and also set <code translate="no">mlm_probability=0.15</code>: each token has a 15% probability of being masked (or possibly randomized or left alone, as we just discussed). We also pass the tokenizer to the collator: it will not be used to tokenize the text—we’ve already done that—but it lets the data collator know the masking and padding token IDs, as well as the vocabulary size (which is needed to sample random token IDs). With that, we just need to specify the <code translate="no">TrainingArguments</code>, pass everything to the <code translate="no">Trainer</code>, and call its <code translate="no">train()</code> method:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">Trainer</code><code class="p">,</code> <code class="n">TrainingArguments</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">DataCollatorForLanguageModeling</code>

<code class="n">args</code> <code class="o">=</code> <code class="n">TrainingArguments</code><code class="p">(</code><code class="n">output_dir</code><code class="o">=</code><code class="s2">"./my_bert"</code><code class="p">,</code> <code class="n">num_train_epochs</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code>
                         <code class="n">per_device_train_batch_size</code><code class="o">=</code><code class="mi">16</code><code class="p">)</code>
<code class="n">mlm_collator</code> <code class="o">=</code> <code class="n">DataCollatorForLanguageModeling</code><code class="p">(</code><code class="n">bert_tokenizer</code><code class="p">,</code> <code class="n">mlm</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                                               <code class="n">mlm_probability</code><code class="o">=</code><code class="mf">0.15</code><code class="p">)</code>
<code class="n">trainer</code> <code class="o">=</code> <code class="n">Trainer</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="n">bert</code><code class="p">,</code> <code class="n">args</code><code class="o">=</code><code class="n">args</code><code class="p">,</code> <code class="n">train_dataset</code><code class="o">=</code><code class="n">mlm_dataset</code><code class="p">,</code>
                  <code class="n">data_collator</code><code class="o">=</code><code class="n">mlm_collator</code><code class="p">)</code>
<code class="n">trainer_output</code> <code class="o">=</code> <code class="n">trainer</code><code class="o">.</code><code class="n">train</code><code class="p">()</code></pre>

<p>Once your model is pretrained, you can try it out using the pipelines API:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">pipeline</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">fill_mask</code> <code class="o">=</code> <code class="n">pipeline</code><code class="p">(</code><code class="s2">"fill-mask"</code><code class="p">,</code> <code class="n">model</code><code class="o">=</code><code class="n">bert</code><code class="p">,</code> <code class="n">tokenizer</code><code class="o">=</code><code class="n">bert_tokenizer</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">top_predictions</code> <code class="o">=</code> <code class="n">fill_mask</code><code class="p">(</code><code class="s2">"The capital of [MASK] is Rome."</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">top_predictions</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="w"/>
<code class="go">{'score': 0.04916289076209068,</code>
<code class="go"> 'token': 1010,</code>
<code class="go"> 'token_str': ',',</code>
<code class="go"> 'sequence': 'the capital of, is rome.'}</code></pre>

<p>What? Rome is not the capital of a comma! The model is actually terrible because we only trained it for a single epoch here, just to confirm that everything works and the loss goes down. To get better results, we would need to train it for a <em>very</em> long time. The BERT authors trained it for about 4 days using 16 TPU devices on a much larger dataset. This is why most people avoid starting from scratch unless they really have to; you’re generally better off downloading a model that was pretrained on a text corpus as close as possible to yours, then fine-tuning it on your own dataset. This can be done using MLM, like we just did, but starting from a pretrained model instead<a data-type="indexterm" data-startref="xi_pretrainingandpretrainedlayersBERTselfsupervisedpretraining1534353_1" id="id3495"/><a data-type="indexterm" data-startref="xi_selfsupervisedlearning1534353_1" id="id3496"/><a data-type="indexterm" data-startref="xi_TransformerslibraryBERTtrainingfromscratch15361578_1" id="id3497"/>. Once you’re happy with your pretrained model, you can fine-tune it on your target task. Let’s see how.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="BERT Fine-Tuning"><div class="sect2" id="id288">
<h2>BERT Fine-Tuning</h2>

<p>BERT can be fine-tuned for many different tasks, changing very little for each task (see the righthand side of <a data-type="xref" href="#bert_diagram">Figure 15-6</a>).</p>

<figure class="smallereighty"><div id="bert_diagram" class="figure">
<img src="assets/hmls_1506.png" alt="Diagram illustrating the BERT model's pre-training on the left and fine-tuning on the right, highlighting different tasks achieved by modifying the classification head." width="1444" height="974"/>
<h6><span class="label">Figure 15-6. </span>BERT pre-training (left) and fine-tuning process (right)⁠<sup><a data-type="noteref" id="id3498-marker" href="ch15.html#id3498">8</a></sup></h6>
</div></figure>

<p>For sentence classification<a data-type="indexterm" data-primary="sentence classification, BERT fine-tuning" id="xi_sentenceclassificationBERTfinetuning1542928_1"/> tasks such as sentiment analysis, all output tokens are ignored except for the first one, which corresponds to the class token, and a new classification head replaces the NSP binary classification head (see the lefthand side of <a data-type="xref" href="#bert_fine_tuning_diagram">Figure 15-7</a>). You can then fine-tune the whole model using the cross-entropy loss, optionally setting a lower learning rate for the lower layers, or freezing BERT altogether during the first few epochs (i.e., training only the new classification head). Using the exact same approach, you can tackle other sentence classification tasks. For example, the authors demonstrated that fine-tuning BERT yields excellent results on the CoLA dataset, which asks whether a sentence is grammatically correct. Try it out on your own sentence classification tasks: it’s likely to perform well even if your dataset is quite small, thanks to the magic of transfer learning.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The BERT authors found that adding the MLM loss to the fine-tuning loss (scaled by a hyperparameter) helps stabilize training and reduces overfitting.</p>
</div>

<p>For token classification<a data-type="indexterm" data-primary="token classification, BERT fine-tuning" id="id3499"/>, the classification head is applied to every token (see the righthand side of <a data-type="xref" href="#bert_fine_tuning_diagram">Figure 15-7</a>). For example, BERT can be fine-tuned<a data-type="indexterm" data-primary="named entity recognition (NER)" id="id3500"/><a data-type="indexterm" data-primary="NER (named entity recognition)" id="id3501"/> for <em>named entity recognition</em> (NER), where the model tags the parts of the text that correspond to names, dates, places, organizations, or other <em>entities</em>. This is often used in legal, financial, or medical applications. The same approach can be used for other token classification tasks, such as tagging grammatical errors; analyzing sentiment at the token level; locating subjects, nouns, and verbs (this<a data-type="indexterm" data-primary="part-of-speech tagging" id="id3502"/> is <em>part-of-speech tagging</em>); or locating questions, statements, and greetings<a data-type="indexterm" data-primary="dialogue act tagging" id="id3503"/> (this is <em>dialogue act tagging</em>); and more.</p>

<figure class="smallereighty"><div id="bert_fine_tuning_diagram" class="figure">
<img src="assets/hmls_1507.png" alt="Diagram showing BERT fine-tuning: left side illustrates sentence classification for sentiment analysis, right side shows token classification for named entity recognition (NER)." width="1414" height="612"/>
<h6><span class="label">Figure 15-7. </span>Fine-tuning BERT for sentence classification such as sentiment analysis (left) or for token classification such as NER (right)</h6>
</div></figure>

<p>BERT can also be used to classify pairs of sentences. It works exactly like sentence classification, except that you pass in two sentences instead of one. For example, this can be used<a data-type="indexterm" data-primary="natural language inference (NLI)" id="id3504"/><a data-type="indexterm" data-primary="NLI (natural language inference)" id="id3505"/> for <em>natural language inference</em> (NLI) where the model must determine whether sentence A entails sentence B, or contradicts it, or neither<a data-type="indexterm" data-primary="multi-genre NLI dataset (MNLI)" id="id3506"/><a data-type="indexterm" data-primary="MNLI (multi-genre NLI dataset)" id="id3507"/> (e.g., the <em>multi-genre NLI</em> dataset, or MNLI). It can also be used to detect whether two sentences have the same meaning, are just paraphrasing each other (e.g., the QQP or MRPC datasets), or to determine whether the answer to question A is present in sentence B (e.g., QNLI dataset)<a data-type="indexterm" data-startref="xi_sentenceclassificationBERTfinetuning1542928_1" id="id3508"/>.</p>

<p>For <em>multiple choice question answering</em> (MCQA)<a data-type="indexterm" data-primary="multiple-choice question answering (MCQA)" id="id3509"/><a data-type="indexterm" data-primary="MCQA (multiple-choice question answering)" id="id3510"/>, BERT is called once for each possible answer, placing the question in segment #0 and the possible answer in segment #1. For each answer, the class token output is passed through a linear layer with a single unit, producing a score. Once we have all the answer scores, we can convert them to probabilities using a softmax layer (see <a data-type="xref" href="#multiple_choice_diagram">Figure 15-8</a>), and we can use the cross-entropy loss for fine-tuning (or better, drop the softmax layer and use the <code translate="no">nn.CrossEntropyLoss</code> directly on the answer scores).</p>

<figure class="width-75"><div id="multiple_choice_diagram" class="figure">
<img src="assets/hmls_1508.png" alt="Diagram illustrating the use of an encoder-only model, like BERT, to answer multiple-choice questions by generating scores for each option and applying a softmax layer to convert them to probabilities." width="1034" height="816"/>
<h6><span class="label">Figure 15-8. </span>Using an encoder-only model to answer multiple-choice questions</h6>
</div></figure>

<p>BERT is also great<a data-type="indexterm" data-primary="extractive question answering" id="id3511"/> for <em>extractive question answering</em>: you ask it a question (in segment #0) about some text called the <em>context</em> (in segment #1), and BERT must locate the answer within the context. For this, you can add a linear layer with two units on top of BERT to output two scores per token: a start score and an end score. During fine-tuning, you can treat them as logits for two separate binary classification tasks: the first determines whether a token is the first token in the answer, and the second determines whether it is the last. Of course most tokens are neither, and it’s possible for one token to be both if the answer is a single token. At inference time, we select the pair of indices <em>i</em> and <em>j</em> that maximizes the sum of the start score of token <em>i</em> and the end score of token <em>j</em>, subject to <em>i</em> ≤ <em>j</em> and <em>j</em> – <em>i</em> + 1 ≤ maximum acceptable answer length. This approach allowed BERT to beat the state of the art on the SQuAD dataset, a popular question answering dataset.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The Transformers library provides convenient classes and checkpoints for each of these use cases, such as <code translate="no">BertForSequenceClassification</code> or <code translate="no">BertForQuestionAnswering</code> (see <a data-type="xref" href="ch14.html#nlp_chapter">Chapter 14</a>).</p>
</div>

<p>The BERT authors also showed that BERT could be fine-tuned to measure <em>semantic textual similarity</em> (STS)<a data-type="indexterm" data-primary="semantic textual similarity (STS)" id="id3512"/><a data-type="indexterm" data-primary="STS (semantic textual similarity)" id="id3513"/>; for example, on the <em>STS benchmark</em> dataset (STS-B), you feed the model two sentences and it outputs a score that indicates how semantically similar the sentences are. That said, if you want to find the most similar pair of sentences in a dataset containing <em>N</em> sentences, you will need to run BERT on <em>O</em>(<em>N</em><sup>2</sup>) pairs: this could take hours if the dataset is large. Instead, it’s preferable to use a model<a data-type="indexterm" data-primary="Sentence-BERT (SBERT)" id="xi_SentenceBERTSBERT15453515_1"/><a data-type="indexterm" data-primary="SBERT (Sentence-BERT)" id="id3514"/> such as <a href="https://homl.info/sbert">Sentence-BERT (SBERT)</a>⁠<sup><a data-type="noteref" id="id3515-marker" href="ch15.html#id3515">9</a></sup> which is a variant of BERT that was fine-tuned to produce good sentence embeddings<a data-type="indexterm" data-primary="sentence embeddings" id="xi_sentenceembeddings15453803_1"/><a data-type="indexterm" data-primary="embeddings" data-secondary="BERT" id="xi_embeddingsBERT15453803_1"/><a data-type="indexterm" data-primary="embeddings" data-secondary="sentence" id="xi_embeddingssentence15453803_1"/>. Start by running each sentence through SBERT to get its sentence embedding, then measure the similarity between each pair of sentence embeddings using a similarity measure<a data-type="indexterm" data-primary="cosine similarity" id="id3516"/><a data-type="indexterm" data-primary="torch" data-secondary="F.cosine_similarity()" id="id3517"/> such as the <em>cosine similarity</em> (e.g., using PyTorch’s <code translate="no">F.cosine_similarity()</code> function). This is the cosine of the angle between two vectors, so its value ranges from –1 (completely opposite) to +1 (perfectly aligned). Since measuring the cosine similarity is much faster than running BERT, and since the model processes much shorter inputs (i.e., sentences rather than sentence pairs), the whole process will take seconds rather than hours.</p>

<p>Sentence embedding can also be extremely useful in many other applications:</p>
<dl>
<dt>Text clustering, to organize and better understand your data</dt>
<dd>
<p>You can process a large number of documents through SBERT to obtain their sentence embeddings, then apply a clustering algorithm such as <em>k</em>-means or HDBSCAN (see <a data-type="xref" href="ch08.html#unsupervised_learning_chapter">Chapter 8</a>) on the embeddings to group your documents based on semantic similarity. It often helps to reduce dimensionality before running the clustering algorithm, for example, using PCA or UMAP (see <a data-type="xref" href="ch07.html#dimensionality_chapter">Chapter 7</a>).</p>
</dd>
<dt>Semantic search</dt>
<dd>
<p>The goal is to let the user find documents based on the query’s meaning rather than just keyword matching. First, encode your documents (or chunks of documents) using SBERT and store the sentence embeddings. When a user submits a search query, encode it using SBERT and find the documents whose embeddings are most similar to the query’s embedding, for example, based on cosine 
<span class="keep-together">similarity.</span></p>
</dd>
<dt>Reranking search results</dt>
<dd>
<p>If you have an existing search system that you don’t want to replace, you can often improve it significantly by reranking the search results based on semantic similarity with the query.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>Vector databases<a data-type="indexterm" data-primary="databases, support for sentence embedding" id="id3518"/>, such as Pinecone, Weaviate, ChromaDB, Qdrant, or Milvus, are designed for storing and searching for documents based on their embeddings. More traditional databases, such as PostgreSQL or MongoDB, also have growing support for embeddings, although it’s not as optimized yet.</p>
</div>

<p>Over the years, many variants of SBERT have been released. One of the easiest ways to download and use them is via the <a href="https://sbert.net">Sentence Transformers library</a> created<a data-type="indexterm" data-primary="Sentence Transformers library" id="id3519"/> by UKPLab and maintained by Hugging Face (it’s preinstalled on Colab). For example, the following code downloads the all-MiniLM-L6-v2 model, which is very fast and lightweight but still produces high-quality sentence embeddings. The code uses it to encode three sentences, then it measures the similarity between each pair of sentences:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sentence_transformers</code> <code class="kn">import</code> <code class="n">SentenceTransformer</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">SentenceTransformer</code><code class="p">(</code><code class="s2">"all-MiniLM-L6-v2"</code><code class="p">)</code>
<code class="n">sentences</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"She's shopping"</code><code class="p">,</code> <code class="s2">"She bought some shoes"</code><code class="p">,</code> <code class="s2">"She's working"</code><code class="p">]</code>
<code class="n">embeddings</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="n">sentences</code><code class="p">,</code> <code class="n">convert_to_tensor</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">similarities</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">similarity</code><code class="p">(</code><code class="n">embeddings</code><code class="p">,</code> <code class="n">embeddings</code><code class="p">)</code></pre>

<p>Let’s look at the similarity matrix:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">similarities</code><code class="w"/>
<code class="go">tensor([[1.0000, 0.6328, 0.5841],</code>
<code class="go">        [0.6328, 1.0000, 0.3831],</code>
<code class="go">        [0.5841, 0.3831, 1.0000]], device='cuda:0')</code></pre>

<p>We see that there are 1s in the main diagonal, confirming that each sentence is perfectly similar to itself, and we also see that “She’s shopping” is more similar to “She bought some shoes” (the cosine similarity is 0.6328) than to “She’s working” (0.5841)<a data-type="indexterm" data-startref="xi_encoderonlytransformersBERT1532921_1" id="id3520"/><a data-type="indexterm" data-startref="xi_BidirectionalEncoderRepresentationsfromTransformersBERTmodel1533420_1" id="id3521"/><a data-type="indexterm" data-startref="xi_BERTBidirectionalEncoderRepresentationsfromTransformersfornaturallanguageunderstanding1532921_1" id="id3522"/><a data-type="indexterm" data-startref="xi_SentenceBERTSBERT15453515_1" id="id3523"/><a data-type="indexterm" data-startref="xi_sentenceembeddings15453803_1" id="id3524"/><a data-type="indexterm" data-startref="xi_embeddingsBERT15453803_1" id="id3525"/><a data-type="indexterm" data-startref="xi_embeddingssentence15453803_1" id="id3526"/>.</p>

<p>Now that we’ve examined BERT in detail, let’s look at some of its offspring.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Other Encoder-Only Models"><div class="sect2" id="id738">
<h2>Other Encoder-Only Models</h2>

<p>Following Google’s footsteps, many organizations released their own encoder-only models. Let’s look at the most popular ones and discuss their main innovations.</p>










<section data-type="sect3" data-pdf-bookmark="RoBERTa by Facebook AI, July 2019 (125M to 355M parameters)"><div class="sect3" id="id289">
<h3>RoBERTa by Facebook AI, July 2019 (125M to 355M parameters)</h3>

<p>This model<a data-type="indexterm" data-primary="RoBERTa model" id="id3527"/><a data-type="indexterm" data-primary="encoder-only transformers" data-secondary="RoBERTa model" id="id3528"/> is similar to BERT but its performance is better across the board in large part because it was pretrained for longer and on a larger dataset. MLM was used for pretraining, but NSP was dropped. Importantly, the authors<a data-type="indexterm" data-primary="dynamic masking" id="id3529"/> used <em>dynamic masking</em>, meaning that the tokens to mask were masked on the fly <em>during</em> training rather than just once before training (as BERT did), so the same piece of text is masked differently across different epochs. This provides the model with more data diversity, reducing overfitting and leading to better generalization.</p>
<div data-type="note" epub:type="note" class="less_space pagebreak-before"><h6>Note</h6>
<p>When we fine-tuned BERT earlier in this chapter, we actually used dynamic masking and we dropped NSP, so we were following RoBERTa’s pretraining approach.</p>
</div>
</div></section>










<section data-type="sect3" data-pdf-bookmark="DistilBERT by Hugging Face, October 2019 (66M)"><div class="sect3" id="id290">
<h3>DistilBERT by Hugging Face, October 2019 (66M)</h3>

<p>This model is a scaled-down version of BERT<a data-type="indexterm" data-primary="Hugging Face" id="xi_HuggingFace1550144_1"/><a data-type="indexterm" data-primary="DistilBERT" id="xi_DistilBERT1550144_1"/><a data-type="indexterm" data-primary="encoder-only transformers" data-secondary="DistilBERT model" id="xi_encoderonlytransformersDistilBERTmodel1550144_1"/>: it’s 40% smaller and 60% faster, yet it manages to reach about 97% of BERT’s performance on most tasks, making it a great choice for low-resource environments (e.g., mobile devices), for low-latency applications, or for quick fine-tuning.</p>

<p>As its name suggests, DistilBERT was trained using a technique<a data-type="indexterm" data-primary="distillation, model" data-secondary="DistilBERT" id="id3530"/> called <em>model distillation</em>, first introduced by Geoffrey Hinton et al. <a href="https://homl.info/distillation">in 2015</a>.⁠<sup><a data-type="noteref" id="id3531-marker" href="ch15.html#id3531">10</a></sup> The idea of distillation is to train<a data-type="indexterm" data-primary="student model, DistilBERT" id="id3532"/><a data-type="indexterm" data-primary="teacher model, DistilBERT" id="id3533"/> a small <em>student model</em> (e.g., DistilBERT) using the estimated probabilities from a larger <em>teacher model</em> (e.g., BERT) as the targets (see <a data-type="xref" href="#distilbert_diagram">Figure 15-9</a>). These are <em>soft targets</em> rather than the usual one-hot vectors: it makes training much faster and more data efficient, as it allows the student to directly aim for the correct distribution, rather than having to learn it over many samples, bouncing between one extreme and the other and slowly settling somewhere in between. As a result, distillation often works better than training the student from scratch on the same dataset as the teacher!</p>

<figure class="smallereighty"><div id="distilbert_diagram" class="figure">
<img src="assets/hmls_1509.png" alt="Diagram illustrating DistilBERT pretraining using distillation losses from a teacher model (BERT) and a masked language modeling (MLM) loss." width="1440" height="646"/>
<h6><span class="label">Figure 15-9. </span>DistilBERT pretraining using a weighted sum of two distillation losses and the MLM loss</h6>
</div></figure>

<p class="pagebreak-before">Note that the estimated probabilities for both the teacher and the student are smoothed a bit—during training only—by dividing the final logits by a temperature greater than 1 (typically 2). This provides the student with a more nuanced signal covering all possible options, rather than just focusing on the correct answer. Hinton dubbed<a data-type="indexterm" data-primary="dark knowledge, DistilBERT pretraining" id="id3534"/> this <em>dark knowledge</em>. For example, if the input is “It’s sunny and I feel [MASK]”, the teacher might normally estimate that the masked word has a 72% probability of being “great”, and 27% of being “good”, and just 0.5% of being “bad”. But if we apply a temperature of 2, then these probabilities get smoothed out to about 60%, 36%, and 5%, respectively. It’s helpful to know that “bad” is a plausible option here, even if it’s unlikely.</p>

<p>DistilBERT’s training loss also had two more components: the standard MLM loss, as well<a data-type="indexterm" data-primary="cosine embedding loss, DistilBERT pretraining" id="id3535"/> as a <em>cosine embedding loss</em> which minimizes the cosine similarity between the student’s and teacher’s final hidden states (i.e., the output embeddings just before the classification head). This encourages the student to “think” like the teacher, not just make the same predictions, and it leads to faster convergence and better performance. Later models, such as TinyBERT<a data-type="indexterm" data-primary="TinyBERT" id="id3536"/>, pushed this idea further by aligning other internal states, such as the attention weights. DistilBERT’s final loss is a weighted sum of the three losses (the authors used weights α=5, β=2, γ=1)<a data-type="indexterm" data-startref="xi_HuggingFace1550144_1" id="id3537"/><a data-type="indexterm" data-startref="xi_DistilBERT1550144_1" id="id3538"/><a data-type="indexterm" data-startref="xi_encoderonlytransformersDistilBERTmodel1550144_1" id="id3539"/>.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="ALBERT by Google Research, December 2019 (12M–235M)"><div class="sect3" id="id291">
<h3>ALBERT by Google Research, December 2019 (12M–235M)</h3>

<p>All encoder layers<a data-type="indexterm" data-primary="ALBERT model" id="id3540"/><a data-type="indexterm" data-primary="encoder-only transformers" data-secondary="ALBERT model" id="id3541"/> in this model share the same weights, making it much smaller than BERT, but not faster. This makes it great for use cases where memory size is limited. In particular, it’s a good model to use if you want to train an encoder-only model from scratch on a GPU with little VRAM.</p>

<p>ALBERT<a data-type="indexterm" data-primary="factorized embeddings, ALBERT" id="id3542"/><a data-type="indexterm" data-primary="embeddings" data-secondary="ALBERT" id="id3543"/><a data-type="indexterm" data-primary="embeddings" data-secondary="factorized" id="id3544"/> also introduced <em>factorized embeddings</em> to reduce the size of the embedding layer: in BERT-large, the vocabulary size is about 30,000, and the embedding size is 1,024, which means that the embedding matrix has over 30 million parameters! ALBERT replaces this huge matrix with the product of two much smaller matrices (see <a data-type="xref" href="#matrix_factorization_diagram">Figure 15-10</a>). In practice, this can be implemented by reducing the embedding size—ALBERT uses 128—then adding a linear layer immediately after the embedding layer to project the embeddings to the higher dimensional space, such as 1,024 dimensions for ALBERT-large. The embedding layer ends up with roughly 3.8M parameters (~30,000 × 128), and the linear layer has about 0.13M parameters (128 × 1,024), so the total is less than 4M parameters, down from over 30M: nice!</p>

<figure class="width-65"><div id="matrix_factorization_diagram" class="figure">
<img src="assets/hmls_1510.png" alt="Diagram illustrating the reduction of a large embedding matrix into smaller embeddings combined with a linear layer to achieve dimensional projection." width="939" height="496"/>
<h6><span class="label">Figure 15-10. </span>An excessively large embedding matrix can be replaced with the product of two smaller matrices. This can be implemented using smaller embeddings and projecting them to higher dimensions using a linear layer.</h6>
</div></figure>

<p>ALBERT also replaced NSP with <em>sentence order prediction</em> (SOP)<a data-type="indexterm" data-primary="sentence order prediction (SOP)" id="id3545"/><a data-type="indexterm" data-primary="SOP (sentence order prediction)" id="id3546"/><a data-type="indexterm" data-primary="predictions" data-secondary="sentence order" id="id3547"/>: given two consecutive sentences, the goal is to predict which one comes first. This is a much harder task than NSP, and it led to significantly better sentence embeddings<a data-type="indexterm" data-primary="sentence embeddings" id="id3548"/><a data-type="indexterm" data-primary="embeddings" data-secondary="sentence" id="id3549"/>.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="ELECTRA by Google Research, March 2020 (14M–335M)"><div class="sect3" id="id292">
<h3>ELECTRA by Google Research, March 2020 (14M–335M)</h3>

<p>This model<a data-type="indexterm" data-primary="ELECTRA model" id="id3550"/><a data-type="indexterm" data-primary="encoder-only transformers" data-secondary="ELECTRA model" id="id3551"/> introduced a new pretraining technique called <em>replaced token detection</em> (RTD)<a data-type="indexterm" data-primary="replaced token detection (RTD)" id="id3552"/><a data-type="indexterm" data-primary="RTD (replaced token detection)" id="id3553"/>: they trained two models jointly—a small generator model and a larger discriminator model, both encoder only. The generator is only used during pretraining, while the discriminator is the final model we’re after. The generator is simply trained using regular MLM with dynamic masking. For each mask token, a replacement token is sampled from its top predictions. The resulting text is fed to the discriminator model, which must predict whether each token is the original or not.</p>

<p>For example (see <a data-type="xref" href="#rtd_diagram">Figure 15-11</a>), if the original text is “She likes him” and it is masked as “She [MASK] him”, the generator’s top predictions might include “likes”, “loves”, “hears”, “pushes”, and one of these is chosen randomly, say “pushes”, so the sentence becomes “She pushes him”. The discriminator must then try to predict [1, 0, 1], since the first and third tokens are the same as in the original text, but not the second token. As the generator improves, the replaced tokens gradually become less and less obviously wrong, forcing the discriminator to become smarter and smarter. After training, we can throw away the generator and drop the binary classification head from the discriminator to get the final model.</p>

<p>This technique is more sample-efficient than MLM<a data-type="indexterm" data-primary="MLM (masked language model)" id="id3554"/><a data-type="indexterm" data-primary="masked language model (MLM)" id="id3555"/> since the discriminator learns from more tokens per example, thus it converges faster, generally achieving the same performance as larger BERT models. That said, the benefits are not always worth the additional complexity.</p>

<figure class="smallereighty"><div id="rtd_diagram" class="figure">
<img src="assets/hmls_1511.png" alt="Diagram illustrating the replaced token detection (RTD) process, showing how the generator suggests possible replacements for a masked word, and the discriminator evaluates the sentence for accuracy." width="850" height="804"/>
<h6><span class="label">Figure 15-11. </span>Replaced token detection (RTD)</h6>
</div></figure>
</div></section>










<section data-type="sect3" data-pdf-bookmark="DeBERTa by Microsoft, January 2021 (139M–1.5B)"><div class="sect3" id="id293">
<h3>DeBERTa by Microsoft, January 2021 (139M–1.5B)</h3>

<p>DeBERTa<a data-type="indexterm" data-primary="DeBERTa model" id="id3556"/><a data-type="indexterm" data-primary="encoder-only transformers" data-secondary="DeBERTa model" id="id3557"/> is a fairly large model that beat the state of the art on many NLU tasks. It removes the usual positional embedding layer, and instead<a data-type="indexterm" data-primary="relative positional embeddings, DeBERTa" id="id3558"/><a data-type="indexterm" data-primary="embeddings" data-secondary="relative position" id="id3559"/> uses <em>relative positional embeddings</em> when computing the attention scores inside every multi-head attention layer: when deciding how much the <em>i</em><sup>th</sup> query token should attend to the <em>j</em><sup>th</sup> key token, the model has access to a learned embedding for the relative position <em>i</em> – <em>j</em>. DeBERTa wasn’t the first model to do this, as we will see later in this chapter, but it introduced a variant<a data-type="indexterm" data-primary="disentangled attention, DeBERTa" id="id3560"/><a data-type="indexterm" data-primary="attention mechanisms" data-secondary="disentangled attention for DeBERTa" id="id3561"/> of this technique—named <em>disentangled attention</em>—which gives the model more flexibility in how it can combine semantic and positional information.</p>

<p>DeBERTaV3, released in July 2021, combined the ideas from DeBERTa with ELECTRA-style RTD, and it reached even higher performance. It remains a popular model for NLU tasks to this day. However, disentangled attention adds some complexity and compute cost, so subsequent models have opted for simpler approaches, as we will see.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="More encoder-only models on Hugging Face Hub"><div class="sect3" id="id294">
<h3>More encoder-only models on Hugging Face Hub</h3>

<p>If you explore the encoder-only models on the Hugging Face Hub<a data-type="indexterm" data-primary="Hugging Face Hub" id="id3562"/>, you will find many variants of the standard models we discussed so far:</p>

<ul>
<li>
<p>With various sizes (e.g., BERT-base versus BERT-large)</p>
</li>
<li>
<p>Pretrained on a non-English language (e.g., CamemBERT for French) or even on multiple languages (e.g., IndicBERT for 12 major Indian languages)</p>
</li>
<li>
<p>Pretrained on cased or uncased text</p>
</li>
<li>
<p>Tweaked for specific tasks (e.g., BERT for question answering)</p>
</li>
</ul>

<p>You will also find many domain-specific models, such as:</p>

<ul>
<li>
<p>ClinicalBERT for clinical applications</p>
</li>
<li>
<p>SciBERT for scientific applications</p>
</li>
<li>
<p>PubMedBERT for biomedicine</p>
</li>
<li>
<p>FinBERT for finance</p>
</li>
<li>
<p>GraphCodeBERT for coding applications</p>
</li>
<li>
<p>Twitter-RoBERTa-base for social media applications</p>
</li>
<li>
<p>PatentBERT for patent applications</p>
</li>
<li>
<p>LexLM for legal applications</p>
</li>
</ul>

<p>Most of these are simply fine-tuned versions of standard encoder-only models such as BERT or RoBERTa, but some were pretrained entirely from scratch. A few also introduced new ideas; for example, GraphCodeBERT is a BERT model pretrained on code using not only MLM, but also two structure-aware tasks: it has to find where in the code each variable was defined and used, and it also has to predict the data flow (e.g., in <code translate="no">z = x + y</code>, variable <code translate="no">z</code> comes from variables <code translate="no">x</code> and <code translate="no">y</code>).</p>

<p>The Hugging Face Hub also contains many compressed variants of standard models. They are small and usually fast, and were trained using distillation, weight-sharing, and/or other techniques such as quantization (see <a data-type="xref" href="app02.html#precision_appendix">Appendix B</a>). Popular examples include: DistilBERT, TinyBERT, MobileBERT, MiniLM (available for various base models), DistilRoBERTa, and MiniDeBERTa-v2. As we saw with DistilBERT, these models are great for low-resource environments, low latency, and quick fine-tuning.</p>

<p>Speaking of quick fine-tuning, you will also find<a data-type="indexterm" data-primary="adapter models" id="id3563"/><a data-type="indexterm" data-primary="Hugging Face Hub" id="id3564"/> many <em>adapter models</em> on the Hugging Face Hub. An adapter model is based on a frozen standard model such as BERT, plus some small trainable components called <em>adapters</em>: when you fine-tune the adapter model, the base model doesn’t change, only the adapters. As a result, fine-tuning is much faster and less computationally expensive, and you can get great performance on your task using fairly little training data. For example, AdapterHub/bert-base-uncased-pf-sst2 is an adapter model based on the bert-base-uncased model and fine-tuned for sentiment analysis on the SST 2 dataset. <a data-type="xref" href="ch17.html#speedup_chapter">Chapter 17</a> shows how to build and fine-tune your own adapter models.</p>

<p>OK, time to step back. We’ve learned all about the Transformer architecture, and we even built a translation transformer from scratch, and now we’ve looked into encoder-only models like BERT and how they can be used for many different NLU tasks. Lastly, we examined the key innovations powering some of the most popular encoder-only models, and the main categories of pretrained encoder-only models you can find on the Hugging Face Hub (i.e., standard, multilingual, task-specific, domain-specific, compressed, and adapter models—these categories are not exclusive)<a data-type="indexterm" data-startref="xi_encoderonlytransformers1532921_1" id="id3565"/><a data-type="indexterm" data-startref="xi_transformersencoderonly1532921_1" id="id3566"/>. It’s now time to look at decoder-only models such as GPT.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Over the last few years, large organizations have shifted their focus toward decoders, but encoder-only models are still alive and kicking. Their relatively small size makes them fast and accessible to all, easy to fine-tune, and immensely useful for a wide range of applications.</p>
</div>
</div></section>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Decoder-Only Transformers"><div class="sect1" id="id295">
<h1>Decoder-Only Transformers</h1>

<p>While Google was working on the first encoder-only model (i.e., BERT), Alec Radford and other OpenAI researchers were taking a different route: they built<a data-type="indexterm" data-primary="decoder-only transformers" data-secondary="GPT" id="xi_decoderonlytransformersGPT15575155_1"/><a data-type="indexterm" data-primary="GPT (Generative Pre-Training) model" id="xi_GPTGenerativePreTrainingmodel15575155_1"/> the first decoder-only model<a data-type="indexterm" data-primary="transformers" data-secondary="decoder-only" id="xi_transformersdecoderonly15575184_1"/><a data-type="indexterm" data-primary="decoder-only transformers" id="xi_decoderonlytransformers15575184_1"/>, named GPT.⁠<sup><a data-type="noteref" id="id3567-marker" href="ch15.html#id3567">11</a></sup> This model paved the way for today’s most impressive models, including most of the ones used in famous chatbots like ChatGPT or Claude.</p>

<p>The GPT model<a data-type="indexterm" data-primary="GPT-1" id="xi_GPT11557714_1"/> (now known as GPT-1) was released in June 2018. GPT stands for <em>Generative Pre-Training</em>: it was pretrained on a dataset of about 7,000 books and learned to predict the next token, so it can be used to generate text one token at a time, just like the original Transformer’s decoder. For example, if you feed it “Happy birthday”, it will predict “birthday to”, so you can append “to” to the input and repeat the process (see <a data-type="xref" href="#gpt_generation_diagram">Figure 15-12</a>).</p>

<figure class="smallereighty"><div id="gpt_generation_diagram" class="figure">
<img src="assets/hmls_1512.png" alt="Diagram illustrating how a decoder-only model, like GPT, generates text one token at a time by predicting the next word in the sequence, such as continuing &quot;Happy birthday&quot; with &quot;to you!&quot;." width="786" height="300"/>
<h6><span class="label">Figure 15-12. </span>Generating text one token at a time using a decoder-only model like GPT</h6>
</div></figure>

<p>Decoder-only models are great<a data-type="indexterm" data-primary="text generation (GPT)" id="id3568"/> at <em>text generation</em> tasks, such as auto-completion, code generation, question answering (including free text answers), math and logical reasoning (to some extent), and chatbots. They can also be used for summarization or translation, but encoder-decoder models<a data-type="indexterm" data-primary="encoder-decoder models" data-secondary="for summarization tasks" data-secondary-sortas="summarization tasks" id="id3569"/><a data-type="indexterm" data-primary="encoder-decoder models" data-secondary="for translation tasks" data-secondary-sortas="translation tasks" id="id3570"/><a data-type="indexterm" data-primary="summarization tasks" id="id3571"/><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="summarization tasks" id="id3572"/> are still popular choices for these tasks, as they often have a better understanding of the source text, thanks to the encoder. Decoder-only models can also perform text classification<a data-type="indexterm" data-primary="text classification" id="id3573"/> quite well, but encoder-only models shine in this area, as they are faster and often provide a similar performance with a smaller model.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>At inference time, encoder-only models only need to look at their inputs once to make their predictions, while decoder-only models require one run per generated token (just like the decoder in encoder-decoder models). That’s because decoders are autoregressive, so the generation process is sequential. That said, decoders can hugely benefit from caching, as I mentioned earlier.</p>
</div>

<p>In this section, we will look at the architecture of GPT-1 and its successor GPT-2, and we will see how decoder-only models like these can be used for various tasks. We will also see that these models can perform tasks that they were never explicitly trained on (zero-shot learning) or for which they only saw a few examples (few-shot learning). Lastly, we will then use the Transformers library to download a small decoder-only model (GPT-2) then a large one (Mistral-7B) and use them to generate text and answer questions.</p>








<section data-type="sect2" data-pdf-bookmark="GPT-1 Architecture and Generative Pretraining"><div class="sect2" id="id296">
<h2>GPT-1 Architecture and Generative Pretraining</h2>

<p>During pretraining<a data-type="indexterm" data-primary="pretraining and pretrained layers" data-secondary="GPT-1 versus BERT" id="id3574"/><a data-type="indexterm" data-primary="BERT (Bidirectional Encoder Representations from Transformers)" data-secondary="versus GPT-1 in pretraining" data-secondary-sortas="GPT-1 in pretraining" id="id3575"/>, GPT-1 was fed batches of 64 sequences randomly sampled from the book corpus, and it was trained to predict the next token for every single input token. Each sequence was exactly 512 tokens long, so GPT-1 did not need any padding token. In fact, it didn’t use special tokens at all during pretraining, not even start-of-sequence or end-of-sequence tokens. Compared to BERT, it’s a much simpler pretraining process. It also provides the same amount of data for every token position, whereas BERT sees less data for the last positions than for the first, due to padding.</p>

<p>GPT-1’s architecture has two important differences compared to the original Transformer’s decoder:</p>

<ul>
<li>
<p>There’s no cross-attention block since there’s no encoder output to attend to: each decoder block only contains a masked multi-head attention layer and a two-layer feedforward network (each with its own skip connection and layer norm).</p>
</li>
<li>
<p>It’s much bigger: it has 12 decoder layers instead of 6, the embedding size is 768 instead of 512, and it has 12 attention heads instead of 8. That’s a total of 117 million parameters.</p>
</li>
</ul>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Counterintuitively, you cannot use PyTorch’s <code translate="no">nn.TransformerDecoder</code> module<a data-type="indexterm" data-primary="PyTorch" data-secondary="decoder-only models with" id="id3576"/> to build a decoder-only model. That’s because it contains cross-attention layers that cannot be easily removed. Instead, you can use the <code translate="no">nn.TransformerEncoder</code> module, and always call it with a causal mask.</p>
</div>

<p>Out-of-the-box, GPT-1 was very impressive at text generation. For example, its authors asked it to tell the story of a scientist discovering a herd of English-speaking unicorns in an unexplored valley, and the story it generated seemed like it had been written by a human (you can read it at <a href="https://homl.info/unicorns" class="bare"><em class="hyperlink">https://homl.info/unicorns</em></a>). It’s not quite as impressive today, but back then it was truly mind-blowing.</p>

<p>The authors also fine-tuned GPT-1 on various tasks, including textual entailment, semantic similarity, reading comprehension, or common sense reasoning, and it beat the state of the art on many of them, confirming the power of pretraining for NLP. For each task, the authors only made minor changes to the architecture:</p>

<ul>
<li>
<p>For text classification<a data-type="indexterm" data-primary="text classification" data-seealso="sentiment analysis" id="id3577"/> tasks, a classification head is added on top of the last token’s output embedding. See the righthand side of <a data-type="xref" href="#gpt_training_diagram">Figure 15-13</a>.</p>
</li>
<li>
<p>For entailment<a data-type="indexterm" data-primary="entailment, GPT-1 fine-tuning" id="id3578"/> and other classification tasks requiring two input sentences, the model is fed both sentences separated by a delimiter token (just a regular $ sign), and again a classification head is added on top of the last token’s output embedding.</p>
</li>
<li>
<p>For semantic similarity<a data-type="indexterm" data-primary="semantic similarity, GPT-1 fine-tuning" id="id3579"/>, since the order of the two sentences shouldn’t matter, the model gets called twice: once with sentence 1 $ sentence 2, and once with sentence 2 $ sentence 1. The last token’s output embeddings for both cases are added itemwise and the result is fed to a regression head.</p>
</li>
<li>
<p>For multiple choice question answering<a data-type="indexterm" data-primary="multiple-choice question answering (MCQA)" id="id3580"/><a data-type="indexterm" data-primary="MCQA (multiple-choice question answering)" id="id3581"/>, the approach is very similar to BERT’s: the model is called once per possible answer, with both the context (including the question) and the possible answer as input, separated by a $ sign, then the last token’s output embedding is passed through a linear layer to get a score. All the answer scores are then passed through a softmax layer<a data-type="indexterm" data-primary="next token prediction (NTP)" id="id3582"/><a data-type="indexterm" data-primary="NTP (next token prediction)" id="id3583"/><a data-type="indexterm" data-primary="predictions" data-secondary="next token" id="id3584"/>.</p>
</li>
<li>
<p>In all cases they added a start-of-sequence token and an end-of-sequence token<a data-type="indexterm" data-startref="xi_GPT11557714_1" id="id3585"/>.</p>
</li>
</ul>

<figure class="smallereighty"><div id="gpt_training_diagram" class="figure">
<img src="assets/hmls_1513.png" alt="Diagram illustrating GPT-1 pretraining with next token prediction using softmax and fine-tuning for classification with a sigmoid function." width="1395" height="621"/>
<h6><span class="label">Figure 15-13. </span>Pretraining GPT-1 using next token prediction (NTP, left) and fine-tuning it for classification (right)</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="GPT-2 and Zero-Shot Learning"><div class="sect2" id="id297">
<h2>GPT-2 and Zero-Shot Learning</h2>

<p>Just a few months later<a data-type="indexterm" data-primary="GPT-2" id="xi_GPT21561624_1"/><a data-type="indexterm" data-primary="zero-shot learning (ZSL)" id="xi_zeroshotlearningZSL1561624_1"/>, in February 2019, Alec Radford, Jeffrey Wu, and other OpenAI researchers published the GPT-2 paper,⁠<sup><a data-type="noteref" id="id3586-marker" href="ch15.html#id3586">12</a></sup> which proposed a very similar architecture to GPT-1,⁠<sup><a data-type="noteref" id="id3587-marker" href="ch15.html#id3587">13</a></sup> but larger still. It came in four sizes, and the largest model had 48 decoder layers, 20 attention heads, an embedding size of 1,600, and a context window of 1,024 tokens, for a total of over 1.5 billion parameters!</p>

<p>For such a large model, the authors needed a gigantic dataset, so they initially tried using Common Crawl which contains over two billion web pages. However, many of these pages are just gibberish (e.g., long tables of data). So the authors built a higher-quality dataset<a data-type="indexterm" data-primary="WebText dataset" id="id3588"/> named <em>WebText</em>, composed of about eight million pages linked from highly ranked Reddit pages.</p>

<p>Most importantly, GPT-2 performed incredibly well on many tasks without any fine-tuning: this is called <em>zero-shot learning</em> (ZSL). For example:</p>

<ul>
<li>
<p>For question answering<a data-type="indexterm" data-primary="question-answering modules" id="id3589"/>, you can simply append “A:” to the question (e.g., “What is the capital of New-Zealand? A:”) then feed this prompt to GPT-2. It will complete it with the answer (e.g., “Wellington”).</p>
</li>
<li>
<p>For summarization<a data-type="indexterm" data-primary="summarization tasks" id="id3590"/><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="summarization tasks" id="id3591"/>, you can append “TL;DR:” to the document you want to summarize, and GPT-2 will often produce a decent summary.</p>
</li>
<li>
<p>For translation, you can create a prompt containing a few examples to guide the model, such as “Bonjour papa = Hello dad” and “Le chien dort = The dog is sleeping”, then append the text you want to translate, for example “Elle aime le chocolat =”, and GPT-2 will hopefully complete the prompt with the correct English translation: “She loves chocolate”.</p>
</li>
</ul>

<p>Importantly, the authors showed that ZSL performance seemed to increase regularly with the model size: doubling the model size offered a roughly constant improvement (that’s a log-linear relationship). Maybe creating a superhuman AI was just a matter of training a large enough transformer?</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>GPT-2’s performance was so impressive that OpenAI<a data-type="indexterm" data-primary="OpenAI" id="id3592"/> initially chose not to release the largest model. Officially, this was for the public’s safety, citing risks like automated disinformation and spam. But skeptics argued that it was both a publicity stunt and a shift toward closed-source AI, and perhaps even a move to influence future regulation. The full GPT-2 model was eventually released months later, but it was the last open one from OpenAI until August 2025, when a couple of open-weight models were released (GPT-OSS)<a data-type="indexterm" data-startref="xi_GPT21561624_1" id="id3593"/><a data-type="indexterm" data-startref="xi_zeroshotlearningZSL1561624_1" id="id3594"/>.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="GPT-3, In-Context Learning, One-Shot Learning, &#10;and Few-Shot Learning"><div class="sect2" id="id298">
<h2>GPT-3, In-Context Learning, One-Shot Learning, 
<span class="keep-together">and Few-Shot Learning</span></h2>

<p>Following their bigger-is-better philosophy, OpenAI<a data-type="indexterm" data-primary="GPT-3" id="xi_GPT31563152_1"/> created <a href="https://homl.info/gpt3">GPT-3</a> in 2020.⁠<sup><a data-type="noteref" id="id3595-marker" href="ch15.html#id3595">14</a></sup> It had roughly 40 billion parameters, and was trained on a monstrously large dataset of about 570 gigabytes (including WebCrawl this time).</p>

<p>This model indeed was far better across the board than GPT-2. In particular, it was much better at zero-shot tasks. But most importantly, the authors showed that GPT-3 was incredibly good at generalizing from just a few examples. This is called <em>few-shot learning</em> (FSL)<a data-type="indexterm" data-primary="few-shot learning (FSL)" id="id3596"/><a data-type="indexterm" data-primary="one-shot learning (OSL)" id="id3597"/><a data-type="indexterm" data-primary="FSL (few-shot learning)" id="id3598"/><a data-type="indexterm" data-primary="OSL (one-shot learning)" id="id3599"/>, or <em>one-shot learning</em> (OSL) if there’s a single example. To tackle FSL or OSL tasks, the authors simply inserted the example(s) in the prompt: they dubbed this <em>in-context learning</em> (ICL). For example, if you feed the following prompt to GPT-3, can you guess what it will output?</p>

<pre translate="no" data-type="programlisting">Alice was friends with Bob. Alice went to visit her friend ___. → Bob
George bought some baseball equipment, a ball, a glove, and a ___. →</pre>

<p>That’s right, it will output the missing word, “bat”. The idea of feeding the model some examples in the prompt itself was already present in the GPT-2 paper (remember the translation example?), but it wasn’t really formalized, and the GPT-3 paper explored it in much more depth.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In-context learning<a data-type="indexterm" data-primary="in-context learning (ICL)" id="id3600"/><a data-type="indexterm" data-primary="ICL (in-context learning)" id="id3601"/> is an increasingly popular approach to one-shot learning and few-shot learning, but there are many others. ICL is new, but OSL and FSL are old (like ZSL)<a data-type="indexterm" data-startref="xi_GPT31563152_1" id="id3602"/>.</p>
</div>

<p>Let’s download GPT-2 and generate some text with it (we will play with GPT-3 via the API later in this chapter).</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Using GPT-2 to Generate Text"><div class="sect2" id="id299">
<h2>Using GPT-2 to Generate Text</h2>

<p>As you might expect<a data-type="indexterm" data-primary="text generation (GPT)" id="xi_textgenerationGPT1564820_1"/>, we can use the Transformers library to download GPT-2<a data-type="indexterm" data-primary="GPT-2" id="xi_GPT21564875_1"/>. By default, we get the small version (124M parameters):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">AutoTokenizer</code><code class="p">,</code> <code class="n">AutoModelForCausalLM</code>

<code class="n">model_id</code> <code class="o">=</code> <code class="s2">"gpt2"</code>
<code class="n">gpt2_tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_id</code><code class="p">)</code>
<code class="n">gpt2</code> <code class="o">=</code> <code class="n">AutoModelForCausalLM</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
    <code class="n">model_id</code><code class="p">,</code> <code class="n">device_map</code><code class="o">=</code><code class="s2">"auto"</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="s2">"auto"</code><code class="p">)</code></pre>

<p>Let’s go through this code:</p>

<ul>
<li>
<p>After the imports, we load GPT-2’s pretrained tokenizer and the model itself.</p>
</li>
<li>
<p>To load the model, we use <code translate="no">AutoModelForCausalLM.from_pretrained()</code>, which returns an instance of the appropriate class based on the checkpoint we ask for (in this case it returns a <code translate="no">GPT2LMHeadModel</code>). Since it’s a causal language model, it’s capable of generating text, as we will see shortly.</p>
</li>
<li>
<p>The <code translate="no">device_map="auto"</code> option tells the function to automatically place the model on the best available device, typically the GPU. If you have multiple GPUs and the model is too large for one, it may even be sharded across GPUs.</p>
</li>
<li>
<p>The <code translate="no">dtype="auto"</code> option asks the function to choose the most appropriate data type for the model weights, based on what’s available in the model checkpoint and your hardware. Typically, it loads the model using 16-bit floats if your hardware supports it (e.g., a modern GPU with mixed-precision support), or it falls back to 32-bit floats. Using half precision (16-bit) uses half the memory, which lets you load larger models, and it also gives the model a substantial speed boost because modern GPUs have hardware accelerations for this, and half precision reduces the amount of data that needs to be transferred between the CPU and GPU.</p>
</li>
</ul>

<p>Now let’s write a little wrapper function around the model’s <code translate="no">generate()</code> method to make it very easy to generate text:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">generate</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">tokenizer</code><code class="p">,</code> <code class="n">prompt</code><code class="p">,</code> <code class="n">max_new_tokens</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="o">**</code><code class="n">generate_kwargs</code><code class="p">):</code>
    <code class="n">inputs</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="n">prompt</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">device</code><code class="p">)</code>
    <code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">generate</code><code class="p">(</code><code class="o">**</code><code class="n">inputs</code><code class="p">,</code> <code class="n">max_new_tokens</code><code class="o">=</code><code class="n">max_new_tokens</code><code class="p">,</code>
                             <code class="n">pad_token_id</code><code class="o">=</code><code class="n">tokenizer</code><code class="o">.</code><code class="n">eos_token_id</code><code class="p">,</code>
                             <code class="o">**</code><code class="n">generate_kwargs</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">tokenizer</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="n">outputs</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">skip_special_tokens</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>

<p>Our <code translate="no">generate()</code> function tokenizes the given prompt, transfers the resulting token IDs to the GPU, calls the given model’s <code translate="no">generate()</code> method to extend the prompt, adding up to 50 new tokens (by default) or less if it runs into an end-of-sequence token, and lastly it decodes the resulting token IDs to return a nice string containing the extended text. Since GPT-2 was pretrained without padding, we must specify which token we want to use for padding when calling the model’s <code translate="no">generate()</code> method: it’s common to use the end-of-sequence token for this. This function processes a single prompt so there will be no padding anyway, but specifying the padding token avoids a pesky warning. Our function also accepts optional extra keyword arguments (<code translate="no">**generate_kwargs</code>) and passes them on to the model’s <code translate="no">generate()</code> method. This will come handy very soon.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Decoder-only models often pad on the left side, for more efficient generation, since new tokens are added on the right.</p>
</div>

<p>Now let’s try generating some text about a talking unicorn:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">prompt</code> <code class="o">=</code> <code class="s2">"Scientists found a talking unicorn today. Here's the full story:"</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">generate</code><code class="p">(</code><code class="n">gpt2</code><code class="p">,</code> <code class="n">gpt2_tokenizer</code><code class="p">,</code> <code class="n">prompt</code><code class="p">)</code><code class="w"/>
<code class="go">"Scientists found a talking unicorn today. Here's the full story:\n\nThe unicorn</code>
<code class="go">was found in a field in the northern part of the state of New Mexico.\n\nThe</code>
<code class="go">unicorn was found in a field in the northern part of the state of New Mexico.</code>
<code class="go">\n\nThe unicorn was found in a field in"</code></pre>

<p>Hmm, it starts out pretty well, but then it just repeats itself—what’s happening? Well, by default the <code translate="no">generate()</code> method simply picks the most likely token at each step, which is fine when you expect very structured output, or for tasks such as question answering, but for creative writing it often gets the model stuck in a loop, producing repetitive and uninteresting text. To fix this, we can set <code translate="no">do_sample=True</code> to make the <code translate="no">generate()</code> method randomly sample each token based on the model’s estimated probabilities for the possible tokens, like we did with our Shakespeare model in <a data-type="xref" href="ch14.html#nlp_chapter">Chapter 14</a>. Let’s see if this works:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">generate</code><code class="p">(</code><code class="n">gpt2</code><code class="p">,</code> <code class="n">gpt2_tokenizer</code><code class="p">,</code> <code class="n">prompt</code><code class="p">,</code> <code class="n">do_sample</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code><code class="w"/>
<code class="go">"Scientists found a talking unicorn today. Here's the full story:\n\nThere</code>
<code class="go">aren't lots of other unicorns and they have been making their way across the</code>
<code class="go">United States since at least the 1800s, but this year there weren't a solitary</code>
<code class="go">unicorn on the land. Today, there are around 1,000."</code></pre>

<p>Well, that’s certainly less repetitive! To get better results, you can play with the <code translate="no">generate()</code> method’s many arguments, such as:</p>
<dl>
<dt><code translate="no">temperature</code></dt>
<dd>
<p>Defaults to 1; decrease for more predictable outputs, or increase for more diverse outputs (as we saw in <a data-type="xref" href="ch14.html#nlp_chapter">Chapter 14</a>)</p>
</dd>
<dt><code translate="no">top_k</code></dt>
<dd>
<p>Only sample from the top <em>k</em> most probable tokens</p>
</dd>
<dt><code translate="no">top_p</code></dt>
<dd>
<p>Restrict sampling to the smallest set of most probable tokens whose total probability is a least <code translate="no">top_p</code></p>
</dd>
<dt><code translate="no">num_beams</code></dt>
<dd>
<p>The beam width for beam search (introduced in <a data-type="xref" href="ch14.html#nlp_chapter">Chapter 14</a>); defaults to 1 (i.e., no beam search)</p>
</dd>
</dl>

<p>Top-<em>p</em> sampling<a data-type="indexterm" data-primary="top-p sampling" id="id3603"/> (a.k.a., nucleus sampling) is often preferred over top-<em>k</em> sampling<a data-type="indexterm" data-primary="top-k sampling" id="id3604"/>, as it adapts to the probability distribution; for example, “The capital city of France is” has only one likely next token (i.e., “Paris”), and top-<em>p</em> sampling will always select it, while top-<em>k</em> sampling might occasionally pick an incorrect token. Conversely, “My favorite city is” has many likely next tokens, and top-<em>p</em> sampling will pick any one of them (favoring the most likely cities), but top-<em>k</em> sampling will only sample from the few most likely ones, ignoring many great cities<a data-type="indexterm" data-startref="xi_textgenerationGPT1564820_1" id="id3605"/>.</p>

<p>So let’s see if top-<em>p</em> sampling helps:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">generate</code><code class="p">(</code><code class="n">gpt2</code><code class="p">,</code> <code class="n">gpt2_tokenizer</code><code class="p">,</code> <code class="n">prompt</code><code class="p">,</code> <code class="n">do_sample</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">top_p</code><code class="o">=</code><code class="mf">0.6</code><code class="p">)</code><code class="w"/>
<code class="go">"Scientists found a talking unicorn today. Here's the full story:\n\nThe first</code>
<code class="go">known unicorn sighting occurred in 1885, when a group of 18-year-old boys and</code>
<code class="go">girls in the northern French village of Villeminne, about 20 miles northeast of</code>
<code class="go">Paris, spotted a strange looking creature. The unicorn"</code></pre>

<p>That’s much better! Now let’s see how to use GPT-2 for question answering.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Using GPT-2 for Question Answering"><div class="sect2" id="id300">
<h2>Using GPT-2 for Question Answering</h2>

<p>Let’s write a little function<a data-type="indexterm" data-primary="question-answering modules" id="id3606"/> that takes a country name and asks GPT-2 to return its capital city:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">DEFAULT_TEMPLATE</code> <code class="o">=</code> <code class="s2">"Capital city of France = Paris</code><code class="se">\n</code><code class="s2">Capital city of </code><code class="si">{country}</code><code class="s2"> ="</code>

<code class="k">def</code> <code class="nf">get_capital_city</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">tokenizer</code><code class="p">,</code> <code class="n">country</code><code class="p">,</code> <code class="n">template</code><code class="o">=</code><code class="n">DEFAULT_TEMPLATE</code><code class="p">):</code>
    <code class="n">prompt</code> <code class="o">=</code> <code class="n">template</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">country</code><code class="o">=</code><code class="n">country</code><code class="p">)</code>
    <code class="n">extended_text</code> <code class="o">=</code> <code class="n">generate</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">tokenizer</code><code class="p">,</code> <code class="n">prompt</code><code class="p">,</code> <code class="n">max_new_tokens</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>
    <code class="n">answer</code> <code class="o">=</code> <code class="n">extended_text</code><code class="p">[</code><code class="nb">len</code><code class="p">(</code><code class="n">prompt</code><code class="p">):]</code>
    <code class="k">return</code> <code class="n">answer</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code><code class="o">.</code><code class="n">splitlines</code><code class="p">()[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code></pre>

<p>The function starts by creating a prompt from<a data-type="indexterm" data-primary="prompt template, GPT-2" id="id3607"/> a <em>prompt template</em>: it replaces the <code translate="no">{country}</code> placeholder with the given country name. Note that the prompt template includes one example of the task to help GPT-2 understand what to do and what format we expect: that’s in-context learning. The function then calls our <code translate="no">generate()</code> function to add 10 tokens to the prompt: this is more than we need to write the capital city’s name. Lastly, we do a bit of post-processing by removing the initial prompt as well as anything after the first line, and we strip away any extra spaces at the end. Let’s try it out!</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">get_capital_city</code><code class="p">(</code><code class="n">gpt2</code><code class="p">,</code> <code class="n">gpt2_tokenizer</code><code class="p">,</code> <code class="s2">"United Kingdom"</code><code class="p">)</code><code class="w"/>
<code class="go">'London'</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">get_capital_city</code><code class="p">(</code><code class="n">gpt2</code><code class="p">,</code> <code class="n">gpt2_tokenizer</code><code class="p">,</code> <code class="s2">"Mexico"</code><code class="p">)</code><code class="w"/>
<code class="go">'Mexico City'</code></pre>

<p>It works beautifully! Moreover, it’s quite flexible with its input; for example, if you ask it for the capital of “UK”, “The UK”, “England”, “Great Britain”, or even “Big Britane”, it will still return “London”. That said, it’s far from perfect:</p>

<ul>
<li>
<p>It makes many common mistakes (e.g., for Canada, it answers Toronto instead of Ottawa). Sadly, since GPT-2 was trained on many pages from the web, it picked up people’s misconceptions and biases.</p>
</li>
<li>
<p>When it’s not sure, it just repeats the country’s name, roughly 30% of the time. This might be because several countries have a capital city of the same name (e.g., Djibouti, Luxembourg, Singapore) or close (e.g., Guatemala City, Kuwait City).</p>
</li>
<li>
<p>When the input is not a country, the model often answers “Paris”, since that’s the only example it had in its prompt.</p>
</li>
</ul>

<p>One way to fix these issues is to simply use a much bigger and smarter model. For example, try using “gpt2-xl” (1.5B parameters) instead of “gpt2” when loading the model, then run the code again<a data-type="indexterm" data-startref="xi_decoderonlytransformersGPT15575155_1" id="id3608"/><a data-type="indexterm" data-startref="xi_GPTGenerativePreTrainingmodel15575155_1" id="id3609"/><a data-type="indexterm" data-startref="xi_GPT21564875_1" id="id3610"/>. It still won’t be perfect, but you should notice a clear improvement. So let’s see if an much larger model can do even better!</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Downloading and Running an Even Larger Model: Mistral-7B"><div class="sect2" id="id301">
<h2>Downloading and Running an Even Larger Model: Mistral-7B</h2>

<p>Mistral-7B is a decoder-only model<a data-type="indexterm" data-primary="decoder-only transformers" data-secondary="Mistral-7B" id="xi_decoderonlytransformersMistral7B1576735_1"/><a data-type="indexterm" data-primary="Mistral-7B" id="xi_Mistral7B1576735_1"/> released by a French startup named Mistral AI in May 2024. As its name suggests, it has seven billion parameters, and it implements several advanced Transformer techniques, such as grouped-query attention and sliding-window attention (see <a data-type="xref" href="ch17.html#speedup_chapter">Chapter 17</a>), which increase its speed and 
<span class="keep-together">performance.</span></p>

<p>The good news is that it’s released under the permissive Apache 2.0 license, and it’s not too big to run on Colab GPUs. However, the model is <em>gated</em> on the Hugging Face Hub, meaning that the platform requires you to log in and agree to some terms: in this case, sharing your identity with the model authors. This is common for high-demand or sensitive models to allow model authors to monitor downloads for usage analytics, reduce abuse, and contact users for potential future research collaboration. Let’s go through all the steps needed to run this model on Colab (or on your own machine if your GPU has enough VRAM):</p>

<ul>
<li>
<p>Go to <a href="https://huggingface.co" class="bare"><em class="hyperlink">https://huggingface.co</em></a> and log in if you already have an account. If not, click on Sign Up and follow the instructions.</p>
</li>
<li>
<p>Once you have logged in to your account, go to <a href="https://huggingface.co/mistralai/Mistral-7B-v0.3" class="bare"><em class="hyperlink">https://huggingface.co/mistralai/Mistral-7B-v0.3</em></a> (or use the Hub’s search feature to find this page). You should see the <em>model card</em> containing useful information about this model, including code snippets and more. For this particular model, you should also see the message asking you to agree to share your contact information (see <a data-type="xref" href="#agreement_screenshot">Figure 15-14</a>). If you agree, click “Agree and access repository”. Accepting the terms is only needed once, and you won’t see this message again for this model.</p>
</li>
</ul>

<figure class="width-90"><div id="agreement_screenshot" class="figure">
<img src="assets/hmls_1514.png" alt="Screenshot of the Hugging Face model page for Mistral-7B-v0.3 showing a notification requiring users to agree to share contact information to access the model." width="1616" height="940"/>
<h6><span class="label">Figure 15-14. </span>The Mistral-7B-v0.3 model on the Hugging Face Hub requires agreeing to share your identity with the model authors</h6>
</div></figure>

<p>Next, you need an access token, which we will use to log in to the Hub from our code:</p>

<ul>
<li>
<p>In the top righthand corner of the website, click on your profile icon, then select Access Tokens from the drop-down menu (or go to <a href="https://huggingface.co/settings/tokens" class="bare"><em class="hyperlink">https://huggingface.co/settings/tokens</em></a>). The website may ask you to confirm your identity at this point.</p>
</li>
<li>
<p>Enter a name for your token, for example, <code translate="no">hf-read-mistral</code>.</p>
</li>
<li>
<p>You must now select the “Token type”: it can be Fine-grained, Read, or Write.</p>

<ul>
<li>
<p>In production, it’s important to use an access token<a data-type="indexterm" data-primary="access tokens, Hugging Face Hub" id="id3611"/> with very limited authorizations in case the token gets compromised. You would select the Fine-grained option (see <a data-type="xref" href="#access_token_screenshot">Figure 15-15</a>), then scroll down to the “Repositories permissions” section, search for mistralai/Mistral-7B-v0.3 in the search box and select the model, then check “Read access to contents of selected repos”. For more flexibility, you could instead go to the Repositories section near the top and check the box labeled “Read access to contents of all public gated repos you can access”.</p>
</li>
<li>
<p>During development, using excessively restrictive access tokens can often slow you down, so you may prefer to select the Read token type, which gives full read access to your account, or even the Write token type, which gives full read/write access.</p>
</li>
</ul>
</li>
<li>
<p>Click the Create Token button, and copy the access token. Save it carefully, as it will never be shown again.</p>
</li>
</ul>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Keep your access tokens safe (e.g., using a password manager such as 1Password or Bitwarden), delete them when you no longer need them, and refresh them if you think they might have been compromised: this invalidates the old token and replaces it with a new one, keeping just the token name. These measures are especially important for access tokens with broad authorizations.</p>
</div>

<figure class="smallereighty"><div id="access_token_screenshot" class="figure">
<img src="assets/hmls_1515.png" alt="Interface showing the steps to create a Hugging Face access token with options for setting fine-grained permissions for repositories." width="1654" height="989"/>
<h6><span class="label">Figure 15-15. </span>Creating a Hugging Face access token</h6>
</div></figure>

<p>OK, let’s go back to Colab now. The last step before downloading the model is to get your notebook to log in to the Hugging Face Hub<a data-type="indexterm" data-primary="Hugging Face Hub" id="id3612"/> using the access token that you just created. However, hardcoding access tokens directly in your code is highly insecure: if anyone can read your notebook, they will know your secret. Luckily, Colab has a convenient feature to save your secrets safely and make them available to any notebooks you like without any hardcoding:</p>

<ul>
<li>
<p>Click on the key icon located in the vertical bar on the lefthand side of Colab’s interface (see <a data-type="xref" href="#colab_secrets_screenshot">Figure 15-16</a>).</p>
</li>
<li>
<p>Click “Add new secret”, then enter your secret’s name (e.g., <code translate="no">token-hf-read-mistral</code>) and the secret value (i.e., your access token). The secret will be stored safely on Google’s servers.</p>
</li>
<li>
<p>Click the button located in the “Notebook access” column of your secret to give the current notebook access to your secret. This button is always deactivated by default, so you will need to activate it in any other notebook that needs to know this secret.</p>
</li>
</ul>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>If you run a Colab notebook written by someone else, then make sure you trust the author or verify the code before activating notebook access for any of your secrets.</p>
</div>

<figure class="width-75"><div id="colab_secrets_screenshot" class="figure">
<img src="assets/hmls_1516.png" alt="Screenshot of a Colab interface showing a secrets manager, which includes options for configuring private environment variables, with fields for adding secret keys and managing their access." width="936" height="517"/>
<h6><span class="label">Figure 15-16. </span>Storing the access token using Colab’s secrets manager</h6>
</div></figure>

<p>Now you can run the following code to retrieve the secret access token:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">google.colab</code> <code class="kn">import</code> <code class="n">userdata</code>

<code class="n">access_token</code> <code class="o">=</code> <code class="n">userdata</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'token-hf-read-mistral'</code><code class="p">)</code></pre>

<p>Great! You now have your access token ready, so let’s use it to log in to the Hugging Face Hub:⁠<sup><a data-type="noteref" id="id3613-marker" href="ch15.html#id3613">15</a></sup></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">huggingface_hub</code> <code class="kn">import</code> <code class="n">login</code>

<code class="n">login</code><code class="p">(</code><code class="n">access_token</code><code class="p">)</code></pre>

<p>Finally, you can load Mistral-7B, exactly like you loaded GPT-2:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">model_id</code> <code class="o">=</code> <code class="s2">"mistralai/Mistral-7B-v0.3"</code>
<code class="n">mistral7b_tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_id</code><code class="p">)</code>
<code class="n">mistral7b</code> <code class="o">=</code> <code class="n">AutoModelForCausalLM</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
    <code class="n">model_id</code><code class="p">,</code> <code class="n">device_map</code><code class="o">=</code><code class="s2">"auto"</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="s2">"auto"</code><code class="p">)</code></pre>

<p>Now you can play around with this model, make it write stories about talking unicorns, or use it to answer all sorts of questions<a data-type="indexterm" data-startref="xi_transformersdecoderonly15575184_1" id="id3614"/><a data-type="indexterm" data-startref="xi_decoderonlytransformers15575184_1" id="id3615"/>. If you use it to find capital cities, as we did earlier, you will see that it finds the correct answer for almost all countries in the world<a data-type="indexterm" data-startref="xi_decoderonlytransformersMistral7B1576735_1" id="id3616"/><a data-type="indexterm" data-startref="xi_Mistral7B1576735_1" id="id3617"/>. Moreover, the very few mistakes it makes are actually quite reasonable.⁠<sup><a data-type="noteref" id="id3618-marker" href="ch15.html#id3618">16</a></sup></p>

<p>But what if we want to chat with this model?</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Turning a Large Language Model into a Chatbot"><div class="sect1" id="id302">
<h1>Turning a Large Language Model into a Chatbot</h1>

<p>To build a chatbot<a data-type="indexterm" data-primary="transformers" data-secondary="chatbot from LLM" id="xi_transformerschatbotfromLLM1584119_1"/><a data-type="indexterm" data-primary="chatbot or personal assistant" id="xi_chatbotorpersonalassistant1584119_1"/>, you need more than a base model. For example, let’s try asking Mistral-7B for something:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">prompt</code> <code class="o">=</code> <code class="s2">"List some places I should visit in Paris."</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">generate</code><code class="p">(</code><code class="n">mistral7b</code><code class="p">,</code> <code class="n">mistral7b_tokenizer</code><code class="p">,</code> <code class="n">prompt</code><code class="p">)</code><code class="w"/>
<code class="go">'List some places I should visit in Paris.\n\nI’m going to Paris in a few weeks</code>
<code class="go">and I’m looking for some places to visit. I’m not looking for the typical</code>
<code class="go">touristy places, but rather some places that are off the beaten path.\n\nI’'</code></pre>

<p>That’s not helpful at all; the model doesn’t answer the question, it just completes it! How can we get this model to be more conversational? Well, one approach is to do a bit of <em>prompt engineering</em>: this is the art of tweaking a prompt<a data-type="indexterm" data-primary="prompt engineering, chatbot training" id="xi_promptengineeringchatbottraining15852237_1"/> until the model reliably behaves as you want it to. For example, we can try adding an introduction that should make the model much more likely to act as a helpful chatbot:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">bob_introduction</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">Bob is an amazing chatbot. It knows everything and it's incredibly helpful.</code>
<code class="s2">"""</code></pre>

<p>To build the full prompt, we just concatenate this introduction and the prompt, adding “Me:” and “Bob:” to clearly indicate who is talking. These<a data-type="indexterm" data-primary="role tags, chatbot conversations" id="id3619"/> are called <em>role tags</em>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">full_prompt</code> <code class="o">=</code> <code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">bob_introduction</code><code class="si">}</code><code class="s2">Me: </code><code class="si">{</code><code class="n">prompt</code><code class="si">}</code><code class="se">\n</code><code class="s2">Bob:"</code></pre>

<p>Now let’s see how the model completes this new prompt:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">extended_text</code> <code class="o">=</code> <code class="n">generate</code><code class="p">(</code><code class="n">mistral7b</code><code class="p">,</code> <code class="n">mistral7b_tokenizer</code><code class="p">,</code> <code class="n">full_prompt</code><code class="p">,</code><code class="w"/>
<code class="gp">... </code>                         <code class="n">max_new_tokens</code><code class="o">=</code><code class="mi">100</code><code class="p">)</code><code class="w"/>
<code class="gp">...</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">answer</code> <code class="o">=</code> <code class="n">extended_text</code><code class="p">[</code><code class="nb">len</code><code class="p">(</code><code class="n">full_prompt</code><code class="p">):]</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="nb">print</code><code class="p">(</code><code class="n">answer</code><code class="p">)</code><code class="w"/>
<code class="go">The Eiffel Tower, the Louvre, and the Arc de Triomphe are all must-see</code>
<code class="go">attractions in Paris.</code>
<code class="go">Me: What's the best way to get around Paris?</code>
<code class="go">Bob: The metro is the most efficient way to get around Paris.</code>
<code class="go">Me: What's the best time of year to visit Paris?</code>
<code class="go">[...]</code></pre>

<p>Now we’re getting somewhere! Bob started with a good answer, but then it generated the rest of the conversation. That’s not too hard to fix; we can simply drop anything after Bob’s first answer, when the conversation goes back to “Me”:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">answer</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">Me: "</code><code class="p">)[</code><code class="mi">0</code><code class="p">]</code><code class="w"/>
<code class="go">'The Eiffel Tower, the Louvre, and the Arc de Triomphe are all must-see</code>
<code class="go">attractions in Paris.'</code></pre>

<p>There we go, good answer! Now suppose we’d like to ask Bob to tell us more about the first place it suggested. If we start a new conversation, Bob will not know what “first place” refers to; instead, we want to continue the same conversation. To do this, we can take the current context (i.e., the full conversation so far) and append “Me:”, followed by our new prompt, then “Bob:”, and feed this extended context to the model. It should generate Bob’s response for this second prompt. We can then repeat this process for any subsequent question. Let’s implement this idea in a small chatbot class that will keep track of the conversation so far and generate an answer for each new prompt:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">BobTheChatbot</code><code class="p">:</code>  <code class="c1"># or ChatBob if you prefer</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">tokenizer</code><code class="p">,</code> <code class="n">introduction</code><code class="o">=</code><code class="n">bob_introduction</code><code class="p">,</code>
                 <code class="n">max_answer_length</code><code class="o">=</code><code class="mi">10_000</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">model</code> <code class="o">=</code> <code class="n">model</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">tokenizer</code> <code class="o">=</code> <code class="n">tokenizer</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">context</code> <code class="o">=</code> <code class="n">introduction</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">max_answer_length</code> <code class="o">=</code> <code class="n">max_answer_length</code>

    <code class="k">def</code> <code class="nf">chat</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">prompt</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">context</code> <code class="o">+=</code> <code class="s2">"</code><code class="se">\n</code><code class="s2">Me: "</code> <code class="o">+</code> <code class="n">prompt</code> <code class="o">+</code> <code class="s2">"</code><code class="se">\n</code><code class="s2">Bob:"</code>
        <code class="n">context</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">context</code>
        <code class="n">start_index</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">context</code><code class="p">)</code>
        <code class="k">while</code> <code class="kc">True</code><code class="p">:</code>
            <code class="n">extended</code> <code class="o">=</code> <code class="n">generate</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">tokenizer</code><code class="p">,</code> <code class="n">context</code><code class="p">,</code>
                                <code class="n">max_new_tokens</code><code class="o">=</code><code class="mi">100</code><code class="p">)</code>
            <code class="n">answer</code> <code class="o">=</code> <code class="n">extended</code><code class="p">[</code><code class="n">start_index</code><code class="p">:]</code>
            <code class="k">if</code> <code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">Me: "</code> <code class="ow">in</code> <code class="n">answer</code> <code class="ow">or</code> <code class="n">extended</code> <code class="o">==</code> <code class="n">context</code> <code class="ow">or</code>
                <code class="nb">len</code><code class="p">(</code><code class="n">answer</code><code class="p">)</code> <code class="o">&gt;=</code> <code class="bp">self</code><code class="o">.</code><code class="n">max_answer_length</code><code class="p">):</code> <code class="k">break</code>
            <code class="n">context</code> <code class="o">=</code> <code class="n">extended</code>
        <code class="n">answer</code> <code class="o">=</code> <code class="n">answer</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">Me: "</code><code class="p">)[</code><code class="mi">0</code><code class="p">]</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">context</code> <code class="o">+=</code> <code class="n">answer</code>
        <code class="k">return</code> <code class="n">answer</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code></pre>

<p>Each instance of this class holds a full conversation in its <code translate="no">context</code> attribute (starting with “Bob is an amazing chatbot […​]”). Every time you call the <code translate="no">chat()</code> method with a new user prompt, this prompt gets appended to the context, then the model is used to extend the context with Bob’s answer, then this answer is extracted and appended to the context as well, and lastly the method returns the answer. The <code translate="no">while</code> loop is used to allow for long answers by calling the model multiple times: it stops whenever the conversation goes back to “Me:”, or when the answer is empty or becomes way too long. OK, time to chat with Bob:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">bob</code> <code class="o">=</code> <code class="n">BobTheChatbot</code><code class="p">(</code><code class="n">mistral7b</code><code class="p">,</code> <code class="n">mistral7b_tokenizer</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bob</code><code class="o">.</code><code class="n">chat</code><code class="p">(</code><code class="s2">"List some places I should visit in Paris."</code><code class="p">)</code><code class="w"/>
<code class="go">'The Eiffel Tower, the Louvre, and the Arc de Triomphe are all must-see</code>
<code class="go">attractions in Paris.'</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bob</code><code class="o">.</code><code class="n">chat</code><code class="p">(</code><code class="s2">"Tell me more about the first place."</code><code class="p">)</code><code class="w"/>
<code class="go">'The Eiffel Tower is a wrought iron lattice tower on the Champ de Mars in Paris,</code>
<code class="go">France. It is named after the engineer Gustave Eiffel, whose company designed</code>
<code class="go">and built the tower.'</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bob</code><code class="o">.</code><code class="n">chat</code><code class="p">(</code><code class="s2">"And Rome?"</code><code class="p">)</code><code class="w"/>
<code class="go">'Rome is the capital city of Italy and is known for its ancient ruins, art, and</code>
<code class="go">architecture. Some of the most popular attractions in Rome include the</code>
<code class="go">Colosseum, the Pantheon, and the Trevi Fountain.'</code></pre>

<p>Cool, we’ve built a working chatbot, based on Mistral-7B, in about 20 lines of code! Try chatting with Bob for a few minutes; it’s quite fun. However, after a while, you may notice some issues:</p>

<ul>
<li>
<p>Bob can fall into loops. For example, if you ask it “Tell me 5 jokes”, it will repeat the same joke five times: “What do you call a cow with no legs? Ground beef”.</p>
</li>
<li>
<p>Its answers are not always very helpful, and its tone is not very conversational. For example, if you ask it “How can I make cookies?”, it will answer: “You can make cookies by mixing flour, sugar, butter, and eggs together.” It’s a start, but good luck actually making cookies with these instructions.</p>
</li>
<li>
<p>Bob can also be a bad boy: if you ask it how to prepare a bank robbery, it will happily answer that you should wear a mask and carry a gun.</p>
</li>
</ul>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id3620">
<h1>Prompt Engineering</h1>
<p>You can get wildly different answers<a data-type="indexterm" data-primary="chatbot or personal assistant" data-secondary="prompt engineering" id="xi_chatbotorpersonalassistantpromptengineering1595037_1"/> depending on how you phrase a question; this is true for humans, but even more so for LLMs. So spending some time experimenting with various prompts is well worth the effort. Following are just a few of the most popular techniques.</p>

<p>You can try different choices of words, add some context, give a few examples, suggest a character to imitate (e.g., “You are a friendly real-estate expert”), specify the output format and style, list pitfalls to avoid, and so on. You can even write a program to try out many possible combinations of prompt variants and evaluate them to find the best prompt for your task. There<a data-type="indexterm" data-primary="automatic prompt optimization (APO) techniques" id="id3621"/> are even <em>automatic prompt optimization</em> (APO) techniques<a data-type="indexterm" data-primary="prompt tuning" id="id3622"/>, such as <a href="https://homl.info/ptuning"><em>prompt tuning</em></a>,⁠<sup><a data-type="noteref" id="id3623-marker" href="ch15.html#id3623">17</a></sup> where a few learned embeddings are prepended to the inputs.</p>

<p>You can also break down complex tasks into multiple subtasks and prompt the LLM once for each subtask. In fact, the output of one prompt can feed into the next one. This<a data-type="indexterm" data-primary="prompt chaining" id="id3624"/> is called <em>prompt chaining</em>. For example, instead of asking the LLM to “write a one-hour lesson on fluid dynamics”, you can first ask it to write the outline, then ask it to double-check this outline and add any missing topic, and lastly ask it to generate the lesson based on the final outline. This multistep approach often improves the quality of the result significantly, and it can be fully automated (e.g., to generate lessons on any other topic).</p>

<p>A related technique<a data-type="indexterm" data-primary="chain-of-thought prompting (CoT)" id="id3625"/><a data-type="indexterm" data-primary="CoT (chain-of-thought prompting)" id="id3626"/> called <a href="https://homl.info/cot"><em>chain-of-thought (CoT) prompting</em></a>⁠<sup><a data-type="noteref" id="id3627-marker" href="ch15.html#id3627">18</a></sup> encourages the LLM to think step by step, rather than jumping to the first answer that comes to mind. We can ask the LLM explicitly (e.g., “proceed step by step”), or we can give a few examples of the type of step-by-step answer we expect (or both). CoT prompting increases the reliability of the output, especially for reasoning tasks. We can even run this process several times and pick the answer that comes out most often<a data-type="indexterm" data-primary="CoT with self-consistency (CoT-SC)" id="id3628"/>: this is called <a href="https://homl.info/cotsc"><em>CoT with self-consistency (CoT-SC)</em></a>.⁠<sup><a data-type="noteref" id="id3629-marker" href="ch15.html#id3629">19</a></sup></p>

<p>Pushing further in this direction, we can make the LLM explore multiple reasoning branches: this approach<a data-type="indexterm" data-primary="tree-of-thoughts (ToT) prompting" id="id3630"/> is called <a href="https://homl.info/tot"><em>tree-of-thoughts (ToT)</em></a>.⁠<sup><a data-type="noteref" id="id3631-marker" href="ch15.html#id3631">20</a></sup> It works by asking the LLM to reason step by step, and at each step we make it generate multiple options (called <em>thoughts</em>), either by calling it several times (with random sampling to get some diversity) or by directly asking it to suggest several options. Then we ask the LLM to evaluate each option, and we explore the most promising one, continuing recursively until the answer is found. If we reach a dead-end, we can backtrack and explore another branch.</p>

<p>ToT achieves excellent results in reasoning tasks, but it’s quite costly, as the LLM needs to be run many times. An alternative approach is to organize a debate between multiple LLMs<a data-type="indexterm" data-primary="multi-agent debate (MAD)" id="id3632"/><a data-type="indexterm" data-primary="MAD (multi-agent debate)" id="id3633"/>: this is called <a href="https://homl.info/mad"><em>multi-agent debate</em> (MAD)</a>.⁠<sup><a data-type="noteref" id="id3634-marker" href="ch15.html#id3634">21</a></sup> It’s still costly, though. A more lightweight option is to ask a single LLM to <a href="https://homl.info/selfrefine">critize its own answer and refine it</a>.⁠<sup><a data-type="noteref" id="id3635-marker" href="ch15.html#id3635">22</a></sup></p>

<p>LLMs also have a strong tendency to make up convincing but factually false statements<a data-type="indexterm" data-primary="hallucinations, LLMs" id="id3636"/>: these are called <em>hallucinations</em>. To avoid this, one of the most important techniques is to add relevant context to the prompt, on the fly. For example, if a medical chatbot is asked about Aspirin, the chatbot system can retrieve information about this drug from a reliable source—such as a medical database or knowledge graph—and include it in the LLM prompt. This<a data-type="indexterm" data-primary="retrieval augmented generation (RAG)" id="id3637"/><a data-type="indexterm" data-primary="RAG (retrieval augmented generation)" id="id3638"/> is called <a href="https://homl.info/rag"><em>retrieval augmented generation</em> (RAG)</a>,⁠<sup><a data-type="noteref" id="id3639-marker" href="ch15.html#id3639">23</a></sup> and it dramatically improves the LLM’s reliability and reduces hallucinations (we will get back to this later in this chapter).</p>

<p>Lastly, we can get an LLM to generate a prompt for another transformer. For example, if the user asks for a “picture of a cute kitten”, we can ask an LLM to improve this prompt: it may generate “adorable fluffy kitten sitting in a teacup, pastel colors, bokeh background, high detail”<a data-type="indexterm" data-startref="xi_chatbotorpersonalassistantpromptengineering1595037_1" id="id3640"/><a data-type="indexterm" data-startref="xi_promptengineeringchatbottraining15852237_1" id="id3641"/>. Then we can feed this improved prompt to an image-generation transformer (see <a data-type="xref" href="ch16.html#vit_chapter">Chapter 16</a>).</p>
</div></aside>

<p>We can improve Bob with some more prompt engineering (e.g., by tweaking the introduction and describing Bob as a <em>very</em> helpful, friendly, polite, and safe chatbot), but it would probably not be enough to make Bob reliably helpful and safe. In particular, a user could easily <em>jailbreak</em> the chatbot<a data-type="indexterm" data-primary="jailbreaking a chatbot" id="id3642"/>, meaning that they could trick Bob into ignoring its directives and generate unsafe content or reveal the directives. The user could also perform a targeted data extraction attack to get an individual’s personal information, assuming some of it was leaked online and ended up in the base model’s training data (e.g., address, email, or credit card info). Luckily, we can make Bob even more helpful and safe by fine-tuning the base model.</p>








<section data-type="sect2" data-pdf-bookmark="Fine-Tuning a Model for Chatting and Following &#10;Instructions Using SFT and RLHF"><div class="sect2" id="id303">
<h2>Fine-Tuning a Model for Chatting and Following 
<span class="keep-together">Instructions Using SFT and RLHF</span></h2>

<p><a data-type="xref" href="#chatbot_diagram">Figure 15-17</a> summarizes the steps required to build a full chatbot system<a data-type="indexterm" data-primary="chatbot or personal assistant" data-secondary="fine-tuning a model using SFT and RLHF" id="xi_chatbotorpersonalassistantfinetuningamodelusingSFTandRLHF1597081_1"/>. You already know the first step: a transformer model—usually decoder-only—is pretrained on a huge corpus of text, typically using next token prediction (NTP)<a data-type="indexterm" data-primary="next token prediction (NTP)" id="id3643"/><a data-type="indexterm" data-primary="NTP (next token prediction)" id="id3644"/><a data-type="indexterm" data-primary="predictions" data-secondary="next token" id="id3645"/>. This is the most costly step, and it produces the base model, such as Mistral-7B or GPT-3.</p>

<p>This base model can then be fine-tuned for many applications. For example, it can be fine-tuned to have a nicer tone and to be more conversational, thereby turning it into<a data-type="indexterm" data-primary="conversational model, chatbot" id="id3646"/><a data-type="indexterm" data-primary="dialogue model, chatbot" id="id3647"/> a <em>conversational model</em> (or <em>dialogue model</em>). It can also be fine-tuned to better follow instructions<a data-type="indexterm" data-primary="instruct models, chatbot" id="id3648"/>, which turns it into a so-called <em>instruct model</em>. A <em>chatbot model</em> is usually fine-tuned for both. For example, Mistral-7B-Instruct was fine-tuned (starting from Mistral-7B) to be both conversational and to follow instructions.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>A note on terminology: a <em>base model</em> is a model<a data-type="indexterm" data-primary="base model, GPT" id="id3649"/> that was only pretrained (e.g., using NTP), but not fine-tuned yet. A <em>foundation model</em> is any model<a data-type="indexterm" data-primary="foundation models" id="id3650"/> that can be adapted to a wide range of tasks (e.g., via prompting or fine-tuning). It’s often a base model, but it can also be a model that was already partially fine-tuned (such as a conversational model). However, these terms are often used interchangeably.</p>
</div>

<figure class="smallersixtyfive"><div id="chatbot_diagram" class="figure">
<img src="assets/hmls_1517.png" alt="Diagram illustrating the process of building a chatbot, detailing stages from pretraining an untrained transformer, to fine-tuning a base model, and finally deploying the chatbot model with additional modules." width="1445" height="782"/>
<h6><span class="label">Figure 15-17. </span>How to build a chatbot: pretraining, two-step fine-tuning, and deployment</h6>
</div></figure>

<p>To fine-tune a model for a chatbot, the fine-tuning process is typically performed in two steps:</p>
<ol>
<li>
<p><em>Supervised Fine-Tuning</em> (SFT)<a data-type="indexterm" data-primary="Supervised Fine-Tuning (SFT), chatbot model" id="id3651"/><a data-type="indexterm" data-primary="SFT (Supervised Fine-Tuning), chatbot model" id="id3652"/>: the model is fine-tuned on a curated dataset which typically contains conversations, question/answer pairs, code generation examples, math problems with solutions, role-playing (e.g., “You are a gourmet chef. How do I make perfect risotto”?), safety-aligned responses (e.g., “How do I rob a bank”? → “Sorry, that’s illegal”.), and more. The training process is just regular supervised learning using next token prediction. However, it’s common to compute the loss only on the answer tokens: this is called <em>loss masking</em>, and it helps focus the model on improving its answers rather than mimicking the user prompts.</p>
</li>
<li>
<p>Fine-tuning with human feedback: in this step, human evaluators rank the model’s responses, then the model is fine-tuned to output higher-ranking responses. This is typically done<a data-type="indexterm" data-primary="Reinforcement Learning from Human Feedback (RLHF)" id="id3653"/><a data-type="indexterm" data-primary="RLHF (Reinforcement Learning from Human Feedback)" id="id3654"/> using either <em>Reinforcement Learning from Human Feedback</em> (RLHF) or <em>Direct Preference Optimization</em> (DPO).</p>
</li>

</ol>

<p>This two-step approach was first introduced by OpenAI in January 2022 when <a href="https://homl.info/instructgpt">InstructGPT</a> was released (via an API), a model based on GPT-3 and fine-tuned using SFT + RLHF. SFT is just straightforward supervised fine-tuning, and RLHF had been introduced several years earlier, in a <a href="https://homl.info/rlhf">2017 paper</a>⁠<sup><a data-type="noteref" id="id3655-marker" href="ch15.html#id3655">24</a></sup> by a group of OpenAI and DeepMind researchers, but the combination worked great.</p>

<p>RLHF is based on a reinforcement learning (RL) technique<a data-type="indexterm" data-primary="proximal policy optimization (PPO)" id="id3656"/><a data-type="indexterm" data-primary="PPO (proximal policy optimization)" id="id3657"/><a data-type="indexterm" data-primary="policy and policy gradients" data-secondary="PPO" id="id3658"/> named <em>proximal policy optimization</em> (PPO, not to be confused with DPO), which we will discuss in <a data-type="xref" href="ch19.html#rl_chapter">Chapter 19</a>. RLHF involves training a reward model to predict human preferences, then fine-tuning the LLM using PPO to favor answers that the reward model scores higher. During this process, the algorithm prevents the LLM from drifting too far from the original model: without this constraint, the model could overfit the human preferences dataset while forgetting useful behavior it had learned during pretraining<a data-type="indexterm" data-startref="xi_chatbotorpersonalassistantfinetuningamodelusingSFTandRLHF1597081_1" id="id3659"/>. This<a data-type="indexterm" data-primary="reward hacking, chatbot" id="id3660"/> is called <em>reward hacking</em>.</p>

<p>RLHF works rather well, and it’s still widely used today, but like many RL techniques, training can be unstable and tricky to get right. Therefore, researchers looked for simpler and more reliable techniques, and this is how DPO came to be.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Direct Preference Optimization (DPO)"><div class="sect2" id="id304">
<h2>Direct Preference Optimization (DPO)</h2>

<p><a href="https://homl.info/dpo">DPO</a> was proposed<a data-type="indexterm" data-primary="chatbot or personal assistant" data-secondary="fine-tuning a model using SFT and DPO" id="xi_chatbotorpersonalassistantfinetuningamodelwithTRLlibraryusingSFTandDPO15110771_1"/><a data-type="indexterm" data-primary="DPO (direct preference optimization)" id="xi_DPOdirectpreferenceoptimization1599340_1"/><a data-type="indexterm" data-primary="direct preference optimization (DPO)" id="xi_directpreferenceoptimizationDPO1599340_1"/> in May 2023 by a team of Stanford University researchers.⁠<sup><a data-type="noteref" id="id3661-marker" href="ch15.html#id3661">25</a></sup> It often works just as well as RLHF or better, and it’s simpler, more stable, and more data efficient, so it is quickly gaining popularity.</p>

<p>Just like RLHF, DPO works with a dataset of human preferences. Each sample in the dataset has three elements: a prompt and two possible answers, where one is preferred by human raters. The goal is to make the model more likely to output the chosen answer than the rejected one, while not drifting too far away from a frozen reference model—usually the model we started with (just after SFT). This is an instance<a data-type="indexterm" data-primary="contrastive learning" id="id3662"/> of <em>contrastive learning</em>, where a model learns by comparing positive and negative examples. To do this, the researchers showed that we can just minimize the loss defined in <a data-type="xref" href="#dpo_loss_equation">Equation 15-2</a>. They proved that this is roughly equivalent to RLHF, but it removes the need for a reward model, and it doesn’t require using complex reinforcement learning algorithms.</p>
<div id="dpo_loss_equation" data-type="equation">
<h5><span class="label">Equation 15-2. </span>Direct preference optimization (DPO) loss</h5>
<math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mi>J</mi>
          <mo>(</mo>
          <mi mathvariant="bold">θ</mi>
          <mo>)</mo>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mo>-</mo>
          <mo form="prefix">log</mo>
          <mi>σ</mi>
          <mfenced separators="" open="[" close="]">
            <mi>β</mi>
            <mfenced separators="" open="(" close=")">
              <mi>δ</mi>
              <mrow>
                <mo>(</mo>
                <msub><mi>𝐲</mi> <mtext>c</mtext> </msub>
                <mo>)</mo>
              </mrow>
              <mo>-</mo>
              <mi>δ</mi>
              <mrow>
                <mo>(</mo>
                <msub><mi mathvariant="bold">y</mi> <mtext>r</mtext> </msub>
                <mo>)</mo>
              </mrow>
            </mfenced>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mtext>with</mtext>
          <mspace width="4.pt"/>
          <mi>δ</mi>
          <mo>(</mo>
          <mi mathvariant="bold">y</mi>
          <mo>)</mo>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mo form="prefix">log</mo>
          <msub><mi>p</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">y</mi>
            <mo>∣</mo>
            <mi mathvariant="bold">x</mi>
            <mo>)</mo>
          </mrow>
          <mo>-</mo>
          <mo form="prefix">log</mo>
          <msub><mi>p</mi> <mtext>ref</mtext> </msub>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">y</mi>
            <mo>∣</mo>
            <mi mathvariant="bold">x</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><em>J</em>(<strong>θ</strong>) is the DPO loss for an instance (<strong>x</strong>, <strong>y</strong><sub>c</sub>, <strong>y</strong><sub>r</sub>), given the current model parameters <strong>θ</strong> and a frozen reference model.</p>
</li>
<li>
<p><strong>x</strong> is the prompt, <strong>y</strong><sub>c</sub> is the chosen answer, and <strong>y</strong><sub>r</sub> is the rejected answer.</p>
</li>
<li>
<p><em>σ</em>(·) is the usual sigmoid function: <math alttext="sigma left-parenthesis x right-parenthesis equals StartFraction 1 Over 1 plus exp left-parenthesis negative x right-parenthesis EndFraction">
  <mrow>
    <mi>σ</mi>
    <mrow>
      <mo>(</mo>
      <mi>x</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><mo form="prefix">exp</mo><mo>(</mo><mo>-</mo><mi>x</mi><mo>)</mo></mrow></mfrac>
  </mrow>
</math>.</p>
</li>
<li>
<p>log <em>p</em><sub><strong>θ</strong></sub>(<strong>y</strong> | <strong>x</strong>) is our model’s estimated log probability for answer <strong>y</strong> (either <strong>y</strong><sub>c</sub> or <strong>y</strong><sub>r</sub>), given the prompt <strong>x</strong>.</p>
</li>
<li>
<p>log <em>p</em><sub>ref</sub>(<strong>y</strong> | <strong>x</strong>) is the reference model’s estimated log probability for answer <strong>y</strong> given <strong>x</strong>.</p>
</li>
<li>
<p><em>β</em> is a temperature-like hyperparameter that controls how steep the sigmoid function is, which impacts how much the loss will focus on sticking to the reference model (high <em>β</em>), versus following human preferences (low <em>β</em>). It’s typically between 0.1 and 0.5.</p>
</li>
</ul>
<div data-type="tip"><h6>Tip</h6>
<p>When computing log(<em>σ</em>(·)) it’s best to use the <code translate="no">F.logsigmoid()</code> function<a data-type="indexterm" data-primary="torch" data-secondary="F.logsigmoid()" id="id3663"/>, which is faster and more numerically stable than computing <code translate="no">torch.log(torch.sigmoid(·))</code>.</p>
</div>

<p>To compute log <em>p</em>(<strong>y</strong> | <strong>x</strong>), where <em>p</em> is either <em>p</em><sub><strong>θ</strong></sub> or <em>p</em><sub>ref</sub>, and <strong>y</strong> is either <strong>y</strong><sub>c</sub> or <strong>y</strong><sub>r</sub>, we start by concatenating <strong>x</strong> and <strong>y</strong>, then we tokenize the result and run it through the model to get the output logits. We typically do this simultaneously for both the correct and rejected answers, for example:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">prompt</code> <code class="o">=</code> <code class="s2">"The capital of Argentina is "</code>
<code class="n">full_input</code> <code class="o">=</code> <code class="p">[</code><code class="n">prompt</code> <code class="o">+</code> <code class="s2">"Buenos Aires"</code><code class="p">,</code> <code class="n">prompt</code> <code class="o">+</code> <code class="s2">"Madrid"</code><code class="p">]</code>
<code class="n">mistral7b_tokenizer</code><code class="o">.</code><code class="n">pad_token</code> <code class="o">=</code> <code class="n">mistral7b_tokenizer</code><code class="o">.</code><code class="n">eos_token</code>
<code class="n">encodings</code> <code class="o">=</code> <code class="n">mistral7b_tokenizer</code><code class="p">(</code><code class="n">full_input</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">encodings</code> <code class="o">=</code> <code class="n">encodings</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
<code class="n">logits</code> <code class="o">=</code> <code class="n">mistral7b</code><code class="p">(</code><code class="o">**</code><code class="n">encodings</code><code class="p">)</code><code class="o">.</code><code class="n">logits</code>  <code class="c1"># shape [2, 8, 32768]</code></pre>

<p>Next we can call the <code translate="no">F.log_softmax()</code> function<a data-type="indexterm" data-primary="torch" data-secondary="F.log_softmax()" id="id3664"/> to turn these logits into estimated log probabilities. Remember that for each input token, we get one estimated log probability for every possible next token (all 32,768 of them). But we’re only interested in the log probability of the actual next token. For example, for the input token “Buenos”, we only want the estimated log probability for the token “Aires”, not for “días” or “noches” or any other token. We can use the <code translate="no">torch.gather()</code> function<a data-type="indexterm" data-primary="torch" data-secondary="gather()" id="id3665"/> to extract only the log probability of the next token (given its token ID) for each input token except the last one (since it doesn’t have a next token):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">next_token_ids</code> <code class="o">=</code> <code class="n">encodings</code><code class="o">.</code><code class="n">input_ids</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">:]</code>  <code class="c1"># shape [2, 7]</code>
<code class="n">log_probas</code> <code class="o">=</code> <code class="n">F</code><code class="o">.</code><code class="n">log_softmax</code><code class="p">(</code><code class="n">logits</code><code class="p">,</code> <code class="n">dim</code><code class="o">=-</code><code class="mi">1</code><code class="p">)[:,</code> <code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code>  <code class="c1"># shape [2, 7, 32768]</code>
<code class="n">next_token_log_probas</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">gather</code><code class="p">(</code>  <code class="c1"># shape [2, 7, 1]</code>
    <code class="n">log_probas</code><code class="p">,</code> <code class="n">dim</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">next_token_ids</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">2</code><code class="p">))</code></pre>

<p>The <code translate="no">torch.gather()</code> function expects the <code translate="no">index</code> argument to have the same shape as the input (or at least able to be broadcast), which is why we must add a dimension #2 to the index using <code translate="no">unsqueeze(2)</code>.</p>

<p>There’s actually a little shortcut that some people prefer—if we pass the logits to the <code translate="no">F.cross_entropy()</code> function<a data-type="indexterm" data-primary="torch" data-secondary="F.cross_entropy()" id="id3666"/>, and specify the next token IDs as the targets, then we get the desired log probabilities directly, in one step instead of two:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">next_token_log_probas</code> <code class="o">=</code> <code class="o">-</code><code class="n">F</code><code class="o">.</code><code class="n">cross_entropy</code><code class="p">(</code>  <code class="c1"># shape [2, 7]</code>
    <code class="n">logits</code><code class="p">[:,</code> <code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">permute</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code> <code class="n">next_token_ids</code><code class="p">,</code> <code class="n">reduction</code><code class="o">=</code><code class="s2">"none"</code><code class="p">)</code></pre>

<p>Note that we must set <code translate="no">reduction="none"</code> to prevent the function from computing the mean of all the log probabilities (as it does by default). We must also flip the result’s sign, since <code translate="no">F.cross_entropy()</code> returns the <em>negative</em> log likelihood. Lastly, we must swap the last two dimensions of the input tensor, since <code translate="no">F.cross_entropy()</code> expects the class dimension to be dimension 1.</p>

<p>Now let’s inspect each token’s estimated probability by computing the exponential of the log probabilities:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="p">[</code><code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">p</code><code class="o">.</code><code class="n">item</code><code class="p">()</code><code class="si">:</code><code class="s2">.2%</code><code class="si">}</code><code class="s2">"</code> <code class="k">for</code> <code class="n">p</code> <code class="ow">in</code> <code class="n">torch</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">next_token_log_probas</code><code class="p">[</code><code class="mi">0</code><code class="p">])]</code><code class="w"/>
<code class="go">['3.27%', '0.02%', '51.95%', '0.40%', '33.98%', '11.38%', '99.61%']</code>
<code class="gp">&gt;&gt;&gt; </code><code class="p">[</code><code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">p</code><code class="o">.</code><code class="n">item</code><code class="p">()</code><code class="si">:</code><code class="s2">.2%</code><code class="si">}</code><code class="s2">"</code> <code class="k">for</code> <code class="n">p</code> <code class="ow">in</code> <code class="n">torch</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">next_token_log_probas</code><code class="p">[</code><code class="mi">1</code><code class="p">])]</code><code class="w"/>
<code class="go">['0.14%', '3.27%', '0.02%', '51.95%', '0.37%', '32.03%', '0.00%']</code></pre>

<p>The first estimated probability is for the token “The” (3.27%), then “capital” (0.02%), and so on. The second sequence starts with a padding token, so you can ignore the first probability (0.14%). The estimated probabilities are the same in both sequences for the prompt tokens,⁠<sup><a data-type="noteref" id="id3667-marker" href="ch15.html#id3667">26</a></sup> but they differ for the answer tokens: 11.38% for “Buenos”, versus 0.00% for “Madrid”. The model seems to know a bit of geography! You may have expected a higher probability for “Buenos”, but tokens like “a”, “one”, and “the” were also quite likely after “is”. However, once the model saw “Buenos”, it was almost certain that the next token was going to be “Aires” (99.61%), and of course it was correct.</p>

<p>Now if we add up the log probabilities of all answer tokens (e.g., for “Buenos” and “Aires”), we get the estimated log probability for the whole answer given the previous tokens, which is precisely what we were looking for (i.e., log <em>p</em>(<strong>y</strong> | <strong>x</strong>)). In this example, it corresponds to an estimated probability of 11.38%:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">answer_log_proba</code> <code class="o">=</code> <code class="n">next_token_log_probas</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="o">-</code><code class="mi">2</code><code class="p">:]</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>  <code class="c1"># Buenos + Aires</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">torch</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">answer_log_proba</code><code class="p">)</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>  <code class="c1"># proba of "Buenos Aires" given the rest</code><code class="w"/>
<code class="go">0.11376953125</code></pre>

<p>However, having to find the exact location of the answer is cumbersome, especially when dealing with padded batches. Luckily, we can actually compute the DPO loss using the log probability of the full input <strong>xy</strong> (including both the prompt <strong>x</strong> and the answer <strong>y</strong>), rather than the log probability of the answer <strong>y</strong> given the prompt <strong>x</strong>. In other words, we can replace every log <em>p</em>(<strong>y</strong> | <strong>x</strong>) with log <em>p</em>(<strong>xy</strong>) in <a data-type="xref" href="#dpo_loss_equation">Equation 15-2</a> (for both <em>p</em><sub><strong>θ</strong></sub> and <em>p</em><sub>ref</sub>, and for both <strong>y</strong><sub>c</sub> and <strong>y</strong><sub>r</sub>). This is because log <em>p</em>(<strong>xy</strong>) = log <em>p</em>(<strong>x</strong>) + log <em>p</em>(<strong>y</strong> | <strong>x</strong>), and the extra <em>p</em>(<strong>x</strong>) for the chosen answer cancels out exactly with the extra <em>p</em>(<strong>x</strong>) for the rejected answer. We only need to mask the padding tokens—we can use the attention mask for that—then simply add up all the log probabilities for each sequence:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">padding_mask</code> <code class="o">=</code> <code class="n">encodings</code><code class="o">.</code><code class="n">attention_mask</code><code class="p">[:,</code> <code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_probas_sum</code> <code class="o">=</code> <code class="p">(</code><code class="n">next_token_log_probas</code> <code class="o">*</code> <code class="n">padding_mask</code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_probas_sum</code><code class="w"/>
<code class="go">tensor([-21.2500, -30.2500], device='cuda:0', dtype=torch.bfloat16)</code></pre>

<p>The first sequence, which contains the prompt and the chosen answer, has a higher log probability than the second sequence, which contains the prompt and the rejected answer, just as we expect. Now if you write a little <code translate="no">sum_of_log_probas()</code> function that wraps everything we just did to compute the sum of log probabilities for every sequence in a batch, then you’re ready to write a function that computes the DPO loss:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">dpo_loss</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">ref_model</code><code class="p">,</code> <code class="n">tokenizer</code><code class="p">,</code> <code class="n">full_input_c</code><code class="p">,</code> <code class="n">full_input_r</code><code class="p">,</code> <code class="n">beta</code><code class="o">=</code><code class="mf">0.1</code><code class="p">):</code>
    <code class="n">p_c</code> <code class="o">=</code> <code class="n">sum_of_log_probas</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">tokenizer</code><code class="p">,</code> <code class="n">full_input_c</code><code class="p">)</code>
    <code class="n">p_r</code> <code class="o">=</code> <code class="n">sum_of_log_probas</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">tokenizer</code><code class="p">,</code> <code class="n">full_input_r</code><code class="p">)</code>
    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>  <code class="c1"># reference model is frozen</code>
        <code class="n">p_ref_c</code> <code class="o">=</code> <code class="n">sum_of_log_probas</code><code class="p">(</code><code class="n">ref_model</code><code class="p">,</code> <code class="n">tokenizer</code><code class="p">,</code> <code class="n">full_input_c</code><code class="p">)</code>
        <code class="n">p_ref_r</code> <code class="o">=</code> <code class="n">sum_of_log_probas</code><code class="p">(</code><code class="n">ref_model</code><code class="p">,</code> <code class="n">tokenizer</code><code class="p">,</code> <code class="n">full_input_r</code><code class="p">)</code>
    <code class="k">return</code> <code class="o">-</code><code class="n">F</code><code class="o">.</code><code class="n">logsigmoid</code><code class="p">(</code><code class="n">beta</code><code class="o">*</code><code class="p">((</code><code class="n">p_c</code> <code class="o">-</code> <code class="n">p_ref_c</code><code class="p">)</code> <code class="o">-</code> <code class="p">(</code><code class="n">p_r</code> <code class="o">-</code> <code class="n">p_ref_r</code><code class="p">)))</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code></pre>

<p>You can then use this loss to fine-tune your model with human preferences (don’t forget to put your model in training mode, and the reference model in eval mode)<a data-type="indexterm" data-startref="xi_DPOdirectpreferenceoptimization1599340_1" id="id3668"/><a data-type="indexterm" data-startref="xi_directpreferenceoptimizationDPO1599340_1" id="id3669"/>. If you prefer, you can use a library to simplify the fine-tuning process: for example, the Hugging Face <em>transformer reinforcement learning</em> (TRL) library implements SFT, RLHF, DPO, and more, so let’s check it out.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Fine-Tuning a Model Using the TRL Library"><div class="sect2" id="id305">
<h2>Fine-Tuning a Model Using the TRL Library</h2>

<p>Let’s use the TRL library to fine-tune a base model using SFT then DPO<a data-type="indexterm" data-primary="transformer reinforcement learning (TRL) library" id="xi_transformerreinforcementlearningTRLlibrary15110771_1"/><a data-type="indexterm" data-primary="TRL (transformer reinforcement learning) library" id="xi_TRLtransformerreinforcementlearninglibrary15110771_1"/>. For SFT<a data-type="indexterm" data-primary="SFT (Supervised Fine-Tuning), chatbot model" id="xi_SFTSupervisedFineTuningchatbotmodel15110780_1"/>, we need a conversational dataset. In this example, we will use the Alpaca dataset<a data-type="indexterm" data-primary="Alpaca dataset" id="id3670"/>, composed of about 52,000 instructions and demonstrations generated by OpenAI’s text-davinci-003 model. Let’s load the dataset and look at an example:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">sft_dataset</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"tatsu-lab/alpaca"</code><code class="p">,</code> <code class="n">split</code><code class="o">=</code><code class="s2">"train"</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="nb">print</code><code class="p">(</code><code class="n">sft_dataset</code><code class="p">[</code><code class="mi">1</code><code class="p">][</code><code class="s2">"text"</code><code class="p">])</code><code class="w"/>
<code class="go">Below is an instruction that describes a task. Write a response that</code>
<code class="go">appropriately completes the request.</code>

<code class="go">### Instruction:</code>
<code class="go">What are the three primary colors?</code>

<code class="go">### Response:</code>
<code class="go">The three primary colors are red, blue, and yellow.</code></pre>

<p>As you can see, the goal of this dataset is to train the model to follow a single instruction and generate a coherent and helpful response. It’s a good start, but after that you will probably want to continue fine-tuning the model using a multiturn dataset (e.g., OpenAssistant/oasst1) to develop the model’s ability to hold a long conversation. This will also teach the model to output role tags, making it clear who is talking (much like “Me:” and “Bob:” in Bob the chatbot). There is no standard for this yet, but many models use the tags “User:” and “Assistant:”. OpenAI defined the ChatML format, which uses “&lt;|user|&gt;”, “&lt;|assistant|&gt;”, or “&lt;|system|&gt;” for system messages (e.g., for text similar to our Bob introduction). Each section ends with “&lt;|end|&gt;”. Lastly, Anthropic uses “Human:” and “Assistant:”.</p>

<p>Let’s preprocess the dataset to use Anthropic-style role tags. Each example in the Alpaca dataset provides the complete prompt in a “text” field, as well as its components in separate fields: “instruction”, “output”, and optionally, “input”. The “text” field will be used for training, so let’s use the individual components to compose a new “text” field and replace the existing one:</p>

<pre translate="no" data-type="programlisting" data-code-language="python" class="less_space pagebreak-before"><code class="k">def</code> <code class="nf">preprocess</code><code class="p">(</code><code class="n">example</code><code class="p">):</code>
    <code class="n">text</code> <code class="o">=</code> <code class="sa">f</code><code class="s2">"Human: </code><code class="si">{</code><code class="n">example</code><code class="p">[</code><code class="s1">'instruction'</code><code class="p">]</code><code class="si">}</code><code class="se">\n</code><code class="s2">"</code>
    <code class="k">if</code> <code class="n">example</code><code class="p">[</code><code class="s1">'input'</code><code class="p">]</code> <code class="o">!=</code> <code class="s2">""</code><code class="p">:</code>
        <code class="n">text</code> <code class="o">+=</code> <code class="sa">f</code><code class="s2">"-&gt; </code><code class="si">{</code><code class="n">example</code><code class="p">[</code><code class="s1">'input'</code><code class="p">]</code><code class="si">}</code><code class="se">\n</code><code class="s2">"</code>
    <code class="n">text</code> <code class="o">+=</code> <code class="sa">f</code><code class="s2">"</code><code class="se">\n</code><code class="s2">Assistant: </code><code class="si">{</code><code class="n">example</code><code class="p">[</code><code class="s1">'output'</code><code class="p">]</code><code class="si">}</code><code class="s2">"</code>
    <code class="k">return</code> <code class="p">{</code><code class="s2">"text"</code><code class="p">:</code> <code class="n">text</code><code class="p">}</code>

<code class="n">sft_dataset</code> <code class="o">=</code> <code class="n">sft_dataset</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">preprocess</code><code class="p">)</code></pre>

<p>Now our previous example looks like this:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="nb">print</code><code class="p">(</code><code class="n">sft_dataset</code><code class="p">[</code><code class="mi">1</code><code class="p">][</code><code class="s2">"text"</code><code class="p">])</code><code class="w"/>
<code class="go">Human: What are the three primary colors?</code>

<code class="go">Assistant: The three primary colors are red, blue, and yellow.</code></pre>

<p>The training set is ready, so we can run SFT. For simplicity, we’ll fine-tune a base GPT-2 model: it’s way too small to learn much, but you can replace it with a larger model if you’re ready to train for a long time. The TRL library’s training API is pretty similar to the one from the Transformers library. The code is self-explanatory:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">trl</code> <code class="kn">import</code> <code class="n">SFTTrainer</code><code class="p">,</code> <code class="n">SFTConfig</code>

<code class="n">sft_model_dir</code> <code class="o">=</code> <code class="s2">"./my_gpt2_sft_alpaca"</code>
<code class="n">training_args</code> <code class="o">=</code> <code class="n">SFTConfig</code><code class="p">(</code>
    <code class="n">output_dir</code><code class="o">=</code><code class="n">sft_model_dir</code><code class="p">,</code> <code class="n">max_length</code><code class="o">=</code><code class="mi">512</code><code class="p">,</code>
    <code class="n">per_device_train_batch_size</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">num_train_epochs</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">save_steps</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code>
    <code class="n">logging_steps</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">learning_rate</code><code class="o">=</code><code class="mf">5e-5</code><code class="p">)</code>
<code class="n">sft_trainer</code> <code class="o">=</code> <code class="n">SFTTrainer</code><code class="p">(</code><code class="s2">"gpt2"</code><code class="p">,</code> <code class="n">train_dataset</code><code class="o">=</code><code class="n">sft_dataset</code><code class="p">,</code> <code class="n">args</code><code class="o">=</code><code class="n">training_args</code><code class="p">)</code>
<code class="n">sft_train_output</code> <code class="o">=</code> <code class="n">sft_trainer</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>
<code class="n">sft_trainer</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">save_pretrained</code><code class="p">(</code><code class="n">sft_model_dir</code><code class="p">)</code></pre>

<p>Now on to the DPO phase<a data-type="indexterm" data-startref="xi_SFTSupervisedFineTuningchatbotmodel15110780_1" id="id3671"/><a data-type="indexterm" data-primary="DPO (direct preference optimization)" id="xi_DPOdirectpreferenceoptimization15116524_1"/>. We’ll need a human-preference dataset. We can use Anthropic’s Anthropic/hh-rlhf dataset, which is designed to train helpful and harmless chatbots. Let’s load it and look at an example:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">pref_dataset</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"Anthropic/hh-rlhf"</code><code class="p">,</code> <code class="n">split</code><code class="o">=</code><code class="s2">"train"</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">pref_dataset</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code><code class="o">.</code><code class="n">keys</code><code class="p">()</code><code class="w"/>
<code class="go">dict_keys(['chosen', 'rejected'])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="nb">print</code><code class="p">(</code><code class="n">pref_dataset</code><code class="p">[</code><code class="mi">2</code><code class="p">][</code><code class="s2">"chosen"</code><code class="p">]</code><code class="o">.</code><code class="n">strip</code><code class="p">())</code><code class="w"/>
<code class="go">Human: If you were going to steal from a convenience store, do you think it</code>
<code class="go">would be better in the morning or at night?</code>

<code class="go">Assistant: I really couldn’t say, I’m not familiar with stealing convenience</code>
<code class="go">store items.</code>
<code class="gp">&gt;&gt;&gt; </code><code class="nb">print</code><code class="p">(</code><code class="n">pref_dataset</code><code class="p">[</code><code class="mi">2</code><code class="p">][</code><code class="s2">"rejected"</code><code class="p">]</code><code class="o">.</code><code class="n">strip</code><code class="p">())</code><code class="w"/>
<code class="go">Human: If you were going to steal from a convenience store, do you think it</code>
<code class="go">would be better in the morning or at night?</code>

<code class="go">Assistant: It is good to consider the difference in human traffic at night, and</code>
<code class="go">also the security cameras.  Night time would probably be better [...]</code></pre>

<p>In this dataset, the prompt is already included (prepended) in both the chosen answer and the rejected answer. In other datasets, like OpenAssistant/oasst1 or Dahoas/full-hh-rlhf, it’s provided in a separate “prompt” field. The TRL library knows how to handle both cases, so we can go right ahead with the second phase of fine-tuning, using DPO:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">trl</code> <code class="kn">import</code> <code class="n">DPOConfig</code><code class="p">,</code> <code class="n">DPOTrainer</code>

<code class="n">dpo_model_dir</code> <code class="o">=</code> <code class="s2">"./my_gpt2_sft_alpaca_dpo_hh_rlhf"</code>
<code class="n">training_args</code> <code class="o">=</code> <code class="n">DPOConfig</code><code class="p">(</code>
    <code class="n">output_dir</code><code class="o">=</code><code class="n">dpo_model_dir</code><code class="p">,</code> <code class="n">max_length</code><code class="o">=</code><code class="mi">512</code><code class="p">,</code> <code class="n">per_device_train_batch_size</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code>
    <code class="n">num_train_epochs</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">save_steps</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">logging_steps</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">learning_rate</code><code class="o">=</code><code class="mf">2e-5</code><code class="p">)</code>
<code class="n">gpt2_tokenizer</code><code class="o">.</code><code class="n">pad_token</code> <code class="o">=</code> <code class="n">gpt2_tokenizer</code><code class="o">.</code><code class="n">eos_token</code>
<code class="n">dpo_trainer</code> <code class="o">=</code> <code class="n">DPOTrainer</code><code class="p">(</code>
    <code class="n">sft_model_dir</code><code class="p">,</code> <code class="n">args</code><code class="o">=</code><code class="n">training_args</code><code class="p">,</code> <code class="n">train_dataset</code><code class="o">=</code><code class="n">pref_dataset</code><code class="p">,</code>
    <code class="n">processing_class</code><code class="o">=</code><code class="n">gpt2_tokenizer</code><code class="p">)</code>
<code class="n">dpo_train_output</code> <code class="o">=</code> <code class="n">dpo_trainer</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>
<code class="n">dpo_trainer</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">save_pretrained</code><code class="p">(</code><code class="n">dpo_model_dir</code><code class="p">)</code></pre>

<p>Let’s take a second to appreciate the fact that you now know how to build a large transformer from scratch, pretrain it using NTP (if you have enough time and money), then fine-tune it using SFT and DPO to turn it into a chatbot model<a data-type="indexterm" data-startref="xi_DPOdirectpreferenceoptimization15116524_1" id="id3672"/>. Bravo!</p>

<p>Alternatively, you can simply download a chatbot model directly, already pretrained and fine-tuned. For example, you can download the Mistral-7B-Instruct-v0.3 model<a data-type="indexterm" data-primary="Mistral-7B-Instruct-v0.3 model" id="id3673"/><a data-type="indexterm" data-primary="instruct models, chatbot" id="id3674"/>, and use it with our <code translate="no">BobTheChatbot</code> class: you will see that it’s a significantly more pleasant and helpful model than Mistral-7B-v0.3. When you ask it to tell you five jokes, it comes up with five <em>different</em> jokes, and it adds “I hope you enjoyed these jokes! If you have any other requests, feel free to ask”. Its cookie recipe is clear and detailed. And if you ask it how to rob a bank, it answers: “I’m sorry, but I can’t assist with that. It’s illegal and unethical to provide advice on criminal activities”<a data-type="indexterm" data-startref="xi_chatbotorpersonalassistantfinetuningamodelwithTRLlibraryusingSFTandDPO15110771_1" id="id3675"/><a data-type="indexterm" data-startref="xi_transformerreinforcementlearningTRLlibrary15110771_1" id="id3676"/><a data-type="indexterm" data-startref="xi_TRLtransformerreinforcementlearninglibrary15110771_1" id="id3677"/>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Mistral-7B-Instruct-v0.3 is also gated, so before you can download it, you will need to visit the model page on the Hugging Face Hub, and accept to share your contact information, just like you did earlier with the base model. Also make sure your access token is configured to authorize read access to this model, or else you will get an error when you try to download the model.</p>
</div>

<p>Now that we have a good chat model, how can we get people to use it?</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="From a Chatbot Model to a Full Chatbot System"><div class="sect2" id="id306">
<h2>From a Chatbot Model to a Full Chatbot System</h2>

<p>The last step in building a chatbot is deploying the model<a data-type="indexterm" data-primary="chatbot or personal assistant" data-secondary="deploying the model to create a system" id="xi_chatbotorpersonalassistantdeployingthemodeltocreateasystem15121359_1"/> inside a complete chatbot system (see <a data-type="xref" href="#chatbot_diagram">Figure 15-17</a>). This system usually includes a web interface and an app for the end user, and it may also have an API endpoint so the model can be queried programmatically. Moreover, to handle complex queries and deliver truly helpful responses, chatbots increasingly rely on a system of integrated tools. For this, the chatbot typically has a component named<a data-type="indexterm" data-primary="orchestrator, chatbot" id="id3678"/> the <em>orchestrator</em> whose role is to coordinate multiple tools to process the user prompt and compose the chatbot’s answer. Here are some of the most important tools:</p>
<dl>
<dt>Calculator</dt>
<dd>
<p>If the user<a data-type="indexterm" data-primary="calculator tool, chatbot" id="id3679"/> asks “What’s 525.6 * 315 / 3942?”, the orchestrator may detect the presence of a math expression. Instead of sending this prompt directly to the chatbot model—which would generate a wrong or approximate answer—the orchestrator can extract the expression, evaluate it using a calculator tool, and add the result to the prompt before sending it to the model. The augmented prompt might look like this: “User: What’s 525.6 * 315 / 3942?\nSystem: Calculator result = 42.\nAssistant:”. All the model needs to do is to generate a nice response such as “The result of 525.6 * 315 / 3942 is 42”. No math needed.</p>

<p>Alternatively, the chatbot model itself can be fine-tuned to invoke tools, such as a calculator. This<a data-type="indexterm" data-primary="tool augmentation, chatbot" id="id3680"/><a data-type="indexterm" data-primary="function calling, chatbot" id="id3681"/> is called <em>tool augmentation</em>, or <em>function calling</em>. For example, the model might be fined-tuned to generate a special output when it encounters a math expression, like this: “Assistant: The result of 525.6 * 315 / 3942 is [calculator_tool] 525.6 * 315 / 3942 [/calculator_tool]”. The orchestrator detects this tool invocation in the model’s output, evaluates the expression using a calculator tool, and replaces the [calculator_tool] section in the result, so the user only sees “The result of 525.6 * 315 / 3942 is 42”. Or the orchestrator can add the result to the prompt and call the model again to get the final response. It’s more costly, but the advantage is that the model can see the result, so it may highlight anything noteworthy, for example: “The result of 525.6 * 315 / 3942 is 42. It’s interesting that the result is an integer”.</p>
</dd>
<dt>Web search</dt>
<dd>
<p>If the user asks about a URL<a data-type="indexterm" data-primary="web search tool, chatbot" id="id3682"/>, the orchestrator can fetch the corresponding web page and inject its text into the prompt. If the page is too long, the orchestrator may run the text through a summarization model first, then add only the summary to the prompt. Or it may chop the text into chunks (e.g., a section each, or a few paragraphs each), find the most relevant chunks, and only inject these chunks into the prompt. To find the most relevant chunks, the system can use a text similarity model to compare each chunk’s embedding with the prompt embedding.</p>

<p>Just like with the calculator tool, the chatbot model itself can be fine-tuned to ask the orchestrator to run a web search. For example, if the user asks “What’s the population of the capital of Canada?”, the model may first output “[search_tool] What is the population of Ottawa? [/search_tool]”. The orchestrator detects this search section in the model’s output and uses a web search engine to run the query. The top results are then fetched and summarized (or the system identifies the relevant chunks), and feeds the result to the chatbot model, along with information about the sources. The model can then produce a reliable and up-to-date response, and even provide its sources, for example: “As of 2025, the estimated population of Ottawa is approximately 1,089,319. Source: <a href="https://worldpopulationreview.com" class="bare"><em class="hyperlink">https://worldpopulationreview.com</em></a>“.</p>
</dd>
<dt>Retrieval Augmented Generation (RAG)</dt>
<dd>
<p>The web search idea can be generalized to all sorts of data sources<a data-type="indexterm" data-primary="retrieval augmented generation (RAG)" id="id3683"/><a data-type="indexterm" data-primary="RAG (retrieval augmented generation)" id="id3684"/>, including private and structured sources of data, like a company’s SQL database, or PDF documents, knowledge bases, and so on. For example, imagine that a user contacts a hotline chatbot and complains that their brand new fridge is making a loud humming sound. The chatbot’s orchestrator could run the user’s prompt through a search engine in the company’s internal knowledge base to gather the most relevant chunks of information (e.g., using a vector database), then feed these chunks to the chatbot model, along with the user’s prompt. These can be injected into the prompt, allowing the chatbot model to produce a reliable, up-to-date, and sourced response. And just like with the previous tools, the chatbot model itself can be fine-tuned to invoke the appropriate search query.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>This approach can also be used to detect whether the query concerns unsafe topics (e.g., robbing a bank or making a bomb) to ensure that the chatbot politely declines.</p>
</div>
<dl>
<dt>Memory (a.k.a., persistent context)</dt>
<dd>
<p>This tool stores user-specific facts and preferences across conversations<a data-type="indexterm" data-primary="memory, chatbot" id="id3685"/><a data-type="indexterm" data-primary="persistent context, chatbot" id="id3686"/>. For example, if the user tells the chatbot that they would like to be called Alice, the model will invoke the memory tool by outputting a command such as “[memory_tool] User is named Alice [/memory_tool]”. The orchestrator will detect this request and store this information in a database. Every time the user starts a new conversation with this chatbot, the orchestrator will inject “User is named Alice” at the beginning of the context, along with any other facts stored in the database for this user (e.g., “User is a doctor”, “User lives in Zimbabwe”, “User prefers concise answers”, etc.). Alternatively, whenever the user prompts the chatbot, the orchestrator can do a similarity search to find any relevant facts and inject them into the prompt. This allows the memory to grow without crowding the context window.</p>
</dd>
<dt>Agentic behavior</dt>
<dd>
<p>The chatbot model may be fine-tuned to be more autonomous<a data-type="indexterm" data-primary="agentic model (agent), chatbot" id="id3687"/> and execute a multistep task with planning and tools. This turns it into an <em>agentic model</em>, or simply an <em>agent</em>. For example, if the user asks the chatbot to perform a <em>deep search</em> on a given topic, the model may start by asking the user for a few 
<span class="keep-together">clarifications,</span> then it will plan the main steps of its task and go ahead and execute each step; for example, invoking a few web searches (with the help of the orchestrator) or tools, analyzing the results, planning more steps, running more tools, and repeating the process until it has gathered all the information it needs to write a nice document about the topic. Note that a model may just be fine-tuned to reason, without calling tools or functions: this is called<a data-type="indexterm" data-primary="reasoning model, chatbot" id="id3688"/> a <em>reasoning model</em>.</p>
</dd>
<dt>Other tools</dt>
<dd>
<p>Just about any tool you can think of can be added to a chatbot system. Here are just a few examples<a data-type="indexterm" data-startref="xi_chatbotorpersonalassistantdeployingthemodeltocreateasystem15121359_1" id="id3689"/>:</p>

<ul>
<li>
<p>A unit or currency conversion tool.</p>
</li>
<li>
<p>A weather tool.</p>
</li>
<li>
<p>A tool to upload a document.</p>
</li>
<li>
<p>A code interpreter: for data analysis, plotting, or running simulations.</p>
</li>
<li>
<p>An integration with an external system, for example, Wolfram Alpha for symbolic math, plots, and scientific knowledge.</p>
</li>
<li>
<p>A nuclear missile tool…​or not! In 1983, a Soviet lieutenant colonel named Stanislav Petrov arguably saved the world from a nuclear war by correctly judging a missile alert as a false alarm. LLMs are often unreliable, so let’s keep humans in the loop for important matters, shall we?</p>
</li>
</ul>
</dd>
</dl>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Model Context Protocol"><div class="sect2" id="id307">
<h2>Model Context Protocol</h2>

<p>If you’re interested in tool-augmented chatbots and agents<a data-type="indexterm" data-primary="chatbot or personal assistant" data-secondary="MCP standard" id="xi_chatbotorpersonalassistantMCPstandard15124759_1"/><a data-type="indexterm" data-primary="MCP (Model Context Protocol) standard" id="xi_MCPModelContextProtocolstandard15124759_1"/><a data-type="indexterm" data-primary="Model Context Protocol (MCP) standard" id="xi_ModelContextProtocolMCPstandard15124759_1"/>, you should definitely check out the <a href="https://homl.info/mcp"><em>Model Context Protocol</em> (MCP)</a>, an open standard proposed by Anthropic that specifies how your AI system can communicate with <em>MCP servers</em> to get access to all sorts of tools and resources, such as the ones listed in <a href="https://github.com/modelcontextprotocol/servers">Anthropic’s MCP server repository</a>. This includes filesystem access, email, calendar, weather, navigation, and just about any other service you can imagine.</p>

<p>MCP does not specify anything about the LLM itself: it’s your LLM orchestrator’s responsibility to interact with the LLM and detect when it wants to access a given tool or resource. For example, you might include instructions in the LLM’s system prompt telling it that it can output a custom JSON message such as <code translate="no">{"tool": "weather", "location": "Paris"}</code> whenever it needs to know the weather in some location (e.g. Paris): when the LLM outputs such a message, your LLM orchestrator can detect it. That’s when MCP comes in: your orchestrator sends an MCP request (i.e., an MCP-compliant JSON message) to a weather MCP server, and once it gets the response (i.e., another MCP-compliant JSON message), it can feed the response to the LLM, which can use it to compose a good answer for the user (e.g., “It will be sunny today in Paris, with a high of 23°C”.).</p>
<div data-type="tip"><h6>Tip</h6>
<p>The LLM can be instructed to output MCP requests directly rather than custom JSON messages. This way, the orchestrator can just validate the JSON request and determine which MCP server to forward it to (e.g., the weather server).</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id3690">
<h1>Structured Generation</h1>
<p>When your model needs to generate structured<a data-type="indexterm" data-primary="structured generation, chatbot model" id="id3691"/><a data-type="indexterm" data-primary="Transformers library" data-secondary="and structured generation" data-secondary-sortas="structured generation" id="id3692"/> output such as JSON, it may sometimes make syntax errors (e.g., adding an extra bracket), or schema errors (e.g., forgetting a required field or passing a string instead of an integer). To deal with these errors, one option is to run the output through a post-processing function that will attempt to detect and fix common issues, but this may not be reliable enough. Another option is to constrain the generation process to only sample valid tokens from the model. For example, after <code translate="no">{"name":</code>, the model must output a space or double quotes. Anything else would be a syntax error or a schema error, since we expect the name to be a string. So we should pick the legal token that the model prefers (i.e., with the highest estimated probability), ignoring all other possible tokens (even if the model prefers them). This is called <em>structured generation</em>.</p>

<p>The Transformers library offers a way to tweak the logits just before the <code translate="no">generate()</code> method samples each token: this involves creating a custom subclass of the <code translate="no">transformers.LogitsProcessor</code> class<a data-type="indexterm" data-primary="Transformers library" data-secondary="LogitsProcessor" id="id3693"/>, then passing it to the <code translate="no">generate()</code> method using the <code translate="no">logits_processor</code> argument. For example, your custom logits processor could determine the list of valid next tokens given the current context, and set the logits of all invalid tokens to negative infinity, thereby forcing the model to choose a valid token. However, this is a low-level approach, so you may prefer to use a library such as <a href="https://homl.info/outlines">Outlines</a> or <a href="https://homl.info/guidance">Guidance</a>, which simplify structured generation.</p>
</div></aside>

<p>But why use MCP rather than a more common protocol such as REST or gRPC? Aren’t we’re just querying an API? Well, it’s more than that:</p>

<ul>
<li>
<p>Firstly, the connections between the LLM orchestrator and the MCP servers are long-lived, allowing fast, stateful, and bidirectional communication. In the MCP architecture, the client-side components that manage these connections are called <em>MCP clients</em>. The software that hosts them—typically your LLM orchestrator—is referred to as the <em>MCP host</em>.</p>
</li>
<li>
<p>Secondly, MCP includes an AI-friendly <em>discovery mechanism</em> which lets the MCP client ask the MCP server for a rich, textual description of what the service does, and how exactly to use it, including the list of available functions and their parameters. In other words, it’s a self-documented API for AIs. In fact, the MCP server can also ask the MCP client for its capabilities, for example whether it supports displaying images to the user or handling streaming output: this lets the server adapt its responses accordingly.</p>
</li>
</ul>

<p>The real power of MCP comes when you tell the LLM which services are available and instruct it on how to access the discovery mechanism: your LLM can then figure out on its own what each available service does, and how to use it. Connecting your LLM to a new MCP server then becomes little more than adding the server to the orchestrator’s configuration and telling the LLM about it<a data-type="indexterm" data-startref="xi_chatbotorpersonalassistantMCPstandard15124759_1" id="id3694"/><a data-type="indexterm" data-startref="xi_MCPModelContextProtocolstandard15124759_1" id="id3695"/><a data-type="indexterm" data-startref="xi_ModelContextProtocolMCPstandard15124759_1" id="id3696"/>.</p>

<p>That said, building a chatbot from scratch can be complex, and fortunately many libraries and tools are available to simplify the process. Let’s look at some of the most popular ones.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Libraries and Tools"><div class="sect2" id="id308">
<h2>Libraries and Tools</h2>

<p>Various open source Python libraries<a data-type="indexterm" data-primary="chatbot or personal assistant" data-secondary="libraries and tools for" id="xi_chatbotorpersonalassistantlibrariesandtoolsfor15127037_1"/> are available to implement your own chatbot system, including:</p>
<dl>
<dt><a href="https://www.langchain.com">LangChain</a></dt>
<dd>
<p>A library designed<a data-type="indexterm" data-primary="LangChain" id="id3697"/> to help you build applications powered by LLMs, by chaining together components such as prompt templates, models, memory, and other tools. It simplifies the orchestration of complex workflows.</p>
</dd>
<dt><a href="https://www.langchain.com/langgraph">LangGraph</a></dt>
<dd>
<p>This built on LangChain<a data-type="indexterm" data-primary="LangGraph" id="id3698"/> and is more specifically designed to build long-running stateful agentic workflows.</p>
</dd>
<dt><a href="https://homl.info/smolagents">Smolagents</a></dt>
<dd>
<p>This is a Hugging Face library<a data-type="indexterm" data-primary="Smolagents" id="id3699"/> designed to build agentic systems. It is a standalone successor to the Transformers Agents library.</p>
</dd>
<dt><a href="https://haystack.deepset.ai">Haystack</a></dt>
<dd>
<p>Haystack<a data-type="indexterm" data-primary="Haystack" id="id3700"/> lets you build systems that can understand complex questions, retrieve relevant information,
and provide accurate answers, typically using RAG.</p>
</dd>
<dt><a href="https://www.llamaindex.ai">LlamaIndex</a></dt>
<dd>
<p>LlamaIndex<a data-type="indexterm" data-primary="LlamaIndex" id="id3701"/> lets you ingest, index, and query your data (e.g., PDFs, databases, APIs).</p>
</dd>
</dl>

<p class="pagebreak-before">There are also several popular open source user interfaces to chat with LLMs locally:</p>
<dl>
<dt><a href="https://lmstudio.ai">LM Studio</a></dt>
<dd>
<p>This is a nice GUI app<a data-type="indexterm" data-primary="LM Studio" id="id3702"/> which lets you easily download and chat with various models. It supports chat history, prompt formatting, and a few other features.</p>
</dd>
<dt><a href="https://ollama.com">Ollama</a></dt>
<dd>
<p>This is a simple command-line tool<a data-type="indexterm" data-primary="Ollama" id="id3703"/> that lets you download various LLMs and chat with them locally, right in your terminal (e.g., <code translate="no">ollama run mistral:7b</code>). Ollama can also act as an API server, which can be queried by other systems (e.g., LangChain). The <code translate="no">ollama</code> Python library lets you query this API easily. Ollama also has support for tools such as a calculator, web search, and more.</p>
</dd>
<dt><a href="https://homl.info/tgw">text-generation-webui</a> (TGWUI)</dt>
<dd>
<p>This is a web interface for chatting with local LLMs<a data-type="indexterm" data-primary="text-generation-webui (TGWUI)" id="id3704"/>. It’s one of the most feature-rich and flexible tools available for local LLM use. It has a plug-in system that lets you add a calculator, a document loader, a search tool, and more. It also includes a REST API for integration with other systems like LangChain.</p>
</dd>
</dl>

<p>Under the hood, these tools require a backend library to actually run the LLMs. LM Studio and Ollama are based on a highly optimized C++ library named <a href="https://github.com/ggml-org/llama.cpp">llama.cpp</a>, while TGWUI supports multiple backends, including llama.cpp, the Transformers library, ExLlama, AutoGPTQ, and more, so you can pick the backend that runs best on your hardware.</p>

<p>With that, you should have everything you need. For example, you could use LangChain to orchestrate a workflow that uses Ollama to run a local LLM, and Haystack to retrieve relevant information from a vector database<a data-type="indexterm" data-startref="xi_transformerschatbotfromLLM1584119_1" id="id3705"/><a data-type="indexterm" data-startref="xi_chatbotorpersonalassistant1584119_1" id="id3706"/><a data-type="indexterm" data-startref="xi_chatbotorpersonalassistantlibrariesandtoolsfor15127037_1" id="id3707"/>.
Before we close this chapter, let’s take a brief look at some of the most influential encoder-decoder transformers.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Encoder-Decoder Models"><div class="sect1" id="id309">
<h1>Encoder-Decoder Models</h1>

<p>In this chapter, other than the original Transformer architecture<a data-type="indexterm" data-primary="encoder-decoder models" data-secondary="transformers" id="xi_encoderdecodermodelstransformers15129966_1"/><a data-type="indexterm" data-primary="encoder-decoder models" id="xi_encoderdecodermodels15129966_1"/>, we have focused solely on encoder-only and decoder-only models. This might have given you the impression that encoder-decoder models are over, but for some problems, they are still very relevant, especially for tasks like translation or summarization. Indeed, since the encoder is bidirectional, it can encode the source text and output excellent contextual embeddings, which the decoder can then use to produce a better output than a decoder-only model would (at least for models of a similar size).</p>

<p>The <a href="https://homl.info/t5">T5 model</a>⁠<sup><a data-type="noteref" id="id3708-marker" href="ch15.html#id3708">27</a></sup> released by Google in 2019 is a particularly influential encoder-decoder model: it was the first to frame all NLP tasks as text to text<a data-type="indexterm" data-primary="T5 model encoder-decoder model" id="id3709"/>. For example, to translate “I like soccer” to Spanish, you can just call the model with the input sentence “translate English to Spanish: I like soccer”, and it outputs “me gusta el fútbol”. To summarize a paragraph, you enter “summarize:” followed by the paragraph, and it outputs the summary. For classification, you only need to change the prefix to “classify:”, and the model outputs the class name as text. For zero-shot classification, the possible classes can be listed in the prompt. This text-to-text approach makes the model very easy to pretrain on a variety of language tasks and just as easy to use. T5 was pretrained using a <em>masked span corruption</em> objective, similar<a data-type="indexterm" data-primary="masked span corruption, T5 model training" id="id3710"/> to MLM, but masking one or more contiguous sections.</p>

<p>Google also released several variants of T5:</p>
<dl>
<dt>mT5 (2020)</dt>
<dd>
<p>This is a multilingual T5<a data-type="indexterm" data-primary="mT5 model" id="id3711"/> supporting over 100 languages. It’s great for translation and cross-lingual tasks (e.g., asking a question in English about a Spanish text).</p>
</dd>
<dt>ByT5 (2021)</dt>
<dd>
<p>This is a byte-level<a data-type="indexterm" data-primary="ByT5 model" id="id3712"/> variant of T5 that removes the need for tokenization entirely (not even BPE). However, this approach has not caught on as it’s more efficient to use tokenizers.</p>
</dd>
<dt>FLAN-T5 (2022)</dt>
<dd>
<p>This is an instruction-tuned T5<a data-type="indexterm" data-primary="FLAN-T5 model" id="id3713"/>, with excellent ZSL and FSL capability.</p>
</dd>
<dt>UL2 (2022)</dt>
<dd>
<p>This is pretrained using several objectives<a data-type="indexterm" data-primary="UL2 model" id="id3714"/>, including masked span denoising like T5, but also standard next token prediction, and masked token prediction.</p>
</dd>
<dt>FLAN-UL2 (2023)</dt>
<dd>
<p>This improved on UL2<a data-type="indexterm" data-primary="Flan-UL2 model" id="id3715"/> using instruction tuning.</p>
</dd>
</dl>

<p>Meta also released some encoder-decoder models, starting with BART<a data-type="indexterm" data-primary="BART model" id="id3716"/> in 2020. This model was pretrained using a denoising objective: the model gets a corrupted text (e.g., masked, modified, deleted, or inserted tokens, shuffled sentences, etc.) and it must clean it up. It’s particularly effective for text generation and summarization. There’s also a multilingual variant named mBART.</p>

<p>Last but not least, the encoder-decoder architecture is quite common for vision models, typically when there are multiple outputs such as in object detection and image segmentation. They’re also common for multimodal models. This leads us to the next chapter, where we will discuss vision transformers and multimodal transformers<a data-type="indexterm" data-startref="xi_encoderdecodermodelstransformers15129966_1" id="id3717"/><a data-type="indexterm" data-startref="xi_encoderdecodermodels15129966_1" id="id3718"/>. It’s time for transformers to open their eyes<a data-type="indexterm" data-startref="xi_transformers15325" id="id3719"/>!</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="id739">
<h1>Exercises</h1>
<ol>
<li>
<p>What is the most important layer in the Transformer architecture? What is its purpose?</p>
</li>
<li>
<p>Why does the Transformer architecture need positional encodings?</p>
</li>
<li>
<p>What tasks are encoder-only models best at? How about decoder-only models? And encoder-decoder models?</p>
</li>
<li>
<p>What is the most important technique used to pretrain BERT?</p>
</li>
<li>
<p>Can you name four BERT variants and explain their main benefits?</p>
</li>
<li>
<p>What is the main task used to pretrain GPT and its successors?</p>
</li>
<li>
<p>The <code translate="no">generate()</code> method has many arguments, including <code translate="no">do_sample</code>, <code translate="no">top_k</code>, <code translate="no">top_p</code>, <code translate="no">temperature</code>, and <code translate="no">num_beams</code>. What do these five arguments do?</p>
</li>
<li>
<p>What is prompt engineering? Can you describe five prompt engineering techniques?</p>
</li>
<li>
<p>What are the main steps to build a chatbot, starting from a pretrained decoder-only model?</p>
</li>
<li>
<p>How can a chatbot use tools like a calculator or web search?</p>
</li>
<li>
<p>What is MCP used for?</p>
</li>
<li>
<p>Fine-tune BERT for sentiment analysis on the IMDb dataset.</p>
</li>
<li>
<p>Fine-tune GPT-2 on the Shakespeare dataset (from <a data-type="xref" href="ch14.html#nlp_chapter">Chapter 14</a>), then generate Shakespeare-like text.</p>
</li>
<li>
<p>Download the <a href="https://homl.info/movieplots">Wikipedia Movie Plots dataset</a>, and use SBERT to embed every movie description. Then write a function that takes a search query, embeds it, finds the most similar embeddings, and lists the corresponding movies.</p>
</li>
<li>
<p>Use an instruction-tuned model such as Qwen-7B-Instruct to build a little chatbot which acts like a movie expert. Then try adding some RAG functionality, for example by automatically injecting the most relevant movie plot into the prompt (see the previous exercise).</p>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter’s notebook, at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id3414"><sup><a href="ch15.html#id3414-marker">1</a></sup> Ashish Vaswani et al., “Attention Is All You Need”, <em>Proceedings of the 31st International Conference on Neural Information Processing Systems</em> (2017): 6000–6010.</p><p data-type="footnote" id="id3417"><sup><a href="ch15.html#id3417-marker">2</a></sup> When applying a linear layer to a sequence, all tokens are treated independently, using the same parameters. This is equivalent to using a <code translate="no">Conv1d</code> layer with <code translate="no">kernel_size=1</code>. This is why you will sometimes see Transformer diagrams showing convolutional layers instead of linear layers.</p><p data-type="footnote" id="id3423"><sup><a href="ch15.html#id3423-marker">3</a></sup> The number of parameters is not public for some models (e.g., Gemini models), so I used some rough estimates. Also, many models have smaller variants, not shown here. Lastly, several other organizations released influential models, such as the Allen Institute for AI (Ai2), Amazon, Baidu, Beijing Academy of AI, Cohere, Huawei, LAION, LMSYS, Nvidia, Stanford University, Talent International Institute (TII), Tsinghua University, xAI, Zhipu AI, and others.</p><p data-type="footnote" id="id3427"><sup><a href="ch15.html#id3427-marker">4</a></sup> This is adapted from Figure 1 from “Attention Is All You Need”, with the kind permission of the authors.</p><p data-type="footnote" id="id3439"><sup><a href="ch15.html#id3439-marker">5</a></sup> Queries, keys, and values were introduced in <a data-type="xref" href="ch14.html#nlp_chapter">Chapter 14</a> when we discussed dot-product attention.</p><p data-type="footnote" id="id3452"><sup><a href="ch15.html#id3452-marker">6</a></sup> This is adapted from the righthand part of Figure 2 from “Attention Is All You Need”, with the kind authorization of the authors.</p><p data-type="footnote" id="id3475"><sup><a href="ch15.html#id3475-marker">7</a></sup> Jacob Devlin et al., “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”, <em>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em> 1 (2019).</p><p data-type="footnote" id="id3498"><sup><a href="ch15.html#id3498-marker">8</a></sup> This is adapted from Figure 1 from the BERT paper, with the kind authorization of the authors.</p><p data-type="footnote" id="id3515"><sup><a href="ch15.html#id3515-marker">9</a></sup> Nils Reimers, Iryna Gurevych, “Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks”, arXiv preprint arXiv:1908.10084 (2019).</p><p data-type="footnote" id="id3531"><sup><a href="ch15.html#id3531-marker">10</a></sup> Geoffrey Hinton et al., “Distilling the Knowledge in a Neural Network”, arXiv preprint arXiv:1503.02531 (2015).</p><p data-type="footnote" id="id3567"><sup><a href="ch15.html#id3567-marker">11</a></sup> Alec Radford et al., <a href="https://homl.info/gpt">“Improving Language Understanding by Generative Pre-Training”</a> (2018).</p><p data-type="footnote" id="id3586"><sup><a href="ch15.html#id3586-marker">12</a></sup> Alec Radford et al., <a href="https://homl.info/gpt2">“Language Models Are Unsupervised Multitask Learners”</a> (2019).</p><p data-type="footnote" id="id3587"><sup><a href="ch15.html#id3587-marker">13</a></sup> There were a few minor tweaks, such as using pre-LN rather than post-LN, downscaling the weights depending on the number of residual layers, and tweaks to the BPE tokenizer. Please see the paper for more details.</p><p data-type="footnote" id="id3595"><sup><a href="ch15.html#id3595-marker">14</a></sup> Tom B. Brown et al., “Language Models are Few-Shot Learners”, arXiv preprint arXiv:2005.14165 (2020).</p><p data-type="footnote" id="id3613"><sup><a href="ch15.html#id3613-marker">15</a></sup> If you are not running the notebook on Colab, you can save the access token in a file and load its content in your code to avoid hardcoding it. There are many other ways to manage secrets, such as environment variables, OS keyrings, or secret management services.</p><p data-type="footnote" id="id3618"><sup><a href="ch15.html#id3618-marker">16</a></sup> For the Vatican, it answers Rome, which contains Vatican City. For Monaco, it answers Monte Carlo, which is the largest district in the city. For Burundi, it answers Bujumbura, which was the capital city until 2019. And for countries that have two or more capital cities, it gives one of them.</p><p data-type="footnote" id="id3623"><sup><a href="ch15.html#id3623-marker">17</a></sup> Brian Lester et al., “The Power of Scale for Parameter-Efficient Prompt Tuning”, arXiv preprint arXiv:2104.08691 (2021).</p><p data-type="footnote" id="id3627"><sup><a href="ch15.html#id3627-marker">18</a></sup> Jason Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models”, arXiv preprint arxiv 2201.11903 (2022).</p><p data-type="footnote" id="id3629"><sup><a href="ch15.html#id3629-marker">19</a></sup> Xuezhi Wang et al., “Self-Consistency Improves Chain of Thought Reasoning in Language Models”, arXiv preprint arXiv:2203.11171 (2022).</p><p data-type="footnote" id="id3631"><sup><a href="ch15.html#id3631-marker">20</a></sup> Shunyu Yao et al., “Tree of Thoughts: Deliberate Problem Solving with Large Language Models”, arXiv preprint arXiv:2305.10601 (2023).</p><p data-type="footnote" id="id3634"><sup><a href="ch15.html#id3634-marker">21</a></sup> Yilun Du et al., “Improving Factuality and Reasoning in Language Models through Multiagent Debate”, arXiv preprint arXiv:2305.14325 (2023).</p><p data-type="footnote" id="id3635"><sup><a href="ch15.html#id3635-marker">22</a></sup> Aman Madaan et al., “Self-Refine: Iterative Refinement with Self-Feedback”, arXiv preprint arXiv:2303.17651 (2023).</p><p data-type="footnote" id="id3639"><sup><a href="ch15.html#id3639-marker">23</a></sup> Patrick Lewis et al., “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks”, arXiv preprint arXiv:2005.11401 (2020).</p><p data-type="footnote" id="id3655"><sup><a href="ch15.html#id3655-marker">24</a></sup> Paul Christiano et al., “Deep reinforcement learning from human preferences”, arXiv preprint arXiv:1706.03741 (2017).</p><p data-type="footnote" id="id3661"><sup><a href="ch15.html#id3661-marker">25</a></sup> Rafael Rafailov et al., “Direct Preference Optimization: Your Language Model is Secretly a Reward Model”, arXiv preprint arXiv:2305.18290 (2023).</p><p data-type="footnote" id="id3667"><sup><a href="ch15.html#id3667-marker">26</a></sup> There’s a slight difference for the tokens “Argentina” and “is”, which I assume is due to the accumulation of floating-point errors in this large model.</p><p data-type="footnote" id="id3708"><sup><a href="ch15.html#id3708-marker">27</a></sup> Colin Raffel et al., “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”, arXiv preprint arXiv:1910.10683 (2019).</p></div></div></section></div></div></body></html>