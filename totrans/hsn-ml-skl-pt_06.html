<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Decision Trees"><div class="chapter" id="trees_chapter">
<h1><span class="label">Chapter 5. </span>Decision Trees</h1>


<p><em>Decision trees</em><a data-type="indexterm" data-primary="decision trees" id="xi_decisiontrees5317_1"/> are versatile machine learning algorithms that can perform both classification and regression tasks, and even multioutput tasks. They are powerful algorithms, capable of fitting complex datasets. For example, in <a data-type="xref" href="ch02.html#project_chapter">Chapter 2</a> you trained a <code translate="no">DecisionTreeRegressor</code> model on the California housing dataset, fitting it perfectly (actually, overfitting it).</p>

<p>Decision trees are also the fundamental components of random forests (see <a data-type="xref" href="ch06.html#ensembles_chapter">Chapter 6</a>), which are among the most powerful machine learning algorithms available today.</p>

<p>In this chapter we will start by discussing how to train, visualize, and make predictions with decision trees. Then we will go through the CART training algorithm used by Scikit-Learn, and we will explore how to regularize trees and use them for regression tasks. Finally, we will discuss some of the limitations of decision trees.</p>






<section data-type="sect1" data-pdf-bookmark="Training and Visualizing a Decision Tree"><div class="sect1" id="id91">
<h1>Training and Visualizing a Decision Tree</h1>

<p>To<a data-type="indexterm" data-primary="decision trees" data-secondary="training and visualizing" id="xi_decisiontreestrainingandvisualizing5103_1"/><a data-type="indexterm" data-primary="visualization of data" data-secondary="decision trees" id="xi_visualizationofdatadecisiontrees5103_1"/> understand decision trees, let’s build one and take a look at how it makes predictions. The following code trains a <code translate="no">DecisionTreeClassifier</code> on the iris dataset (see <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter 4</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_iris</code>
<code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeClassifier</code>

<code class="n">iris</code> <code class="o">=</code> <code class="n">load_iris</code><code class="p">(</code><code class="n">as_frame</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">X_iris</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code><code class="p">[[</code><code class="s2">"petal length (cm)"</code><code class="p">,</code> <code class="s2">"petal width (cm)"</code><code class="p">]]</code><code class="o">.</code><code class="n">values</code>
<code class="n">y_iris</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="n">tree_clf</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">tree_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_iris</code><code class="p">,</code> <code class="n">y_iris</code><code class="p">)</code></pre>

<p>You can visualize the trained decision tree by first using the <code translate="no">export_graphviz()</code> function to output a graph definition file called <em>iris_tree.dot</em>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">export_graphviz</code>

<code class="n">export_graphviz</code><code class="p">(</code>
        <code class="n">tree_clf</code><code class="p">,</code>
        <code class="n">out_file</code><code class="o">=</code><code class="s2">"iris_tree.dot"</code><code class="p">,</code>
        <code class="n">feature_names</code><code class="o">=</code><code class="p">[</code><code class="s2">"petal length (cm)"</code><code class="p">,</code> <code class="s2">"petal width (cm)"</code><code class="p">],</code>
        <code class="n">class_names</code><code class="o">=</code><code class="n">iris</code><code class="o">.</code><code class="n">target_names</code><code class="p">,</code>
        <code class="n">rounded</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
        <code class="n">filled</code><code class="o">=</code><code class="kc">True</code>
    <code class="p">)</code></pre>

<p>Then you can use <code translate="no">graphviz.Source.from_file()</code> to load and display the file in a Jupyter notebook:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">graphviz</code> <code class="kn">import</code> <code class="n">Source</code>

<code class="n">Source</code><code class="o">.</code><code class="n">from_file</code><code class="p">(</code><code class="s2">"iris_tree.dot"</code><code class="p">)</code></pre>

<p><a href="https://graphviz.org">Graphviz</a><a data-type="indexterm" data-primary="Graphviz" id="id1644"/> is an open source graph visualization software package. It also includes a <code translate="no">dot</code> command-line tool to convert <em>.dot</em> files to a variety of formats, such as PDF or PNG.</p>

<p>Your first decision tree looks like <a data-type="xref" href="#iris_tree">Figure 5-1</a>.<a data-type="indexterm" data-startref="xi_decisiontreestrainingandvisualizing5103_1" id="id1645"/><a data-type="indexterm" data-startref="xi_visualizationofdatadecisiontrees5103_1" id="id1646"/></p>

<figure class="width-90"><div id="iris_tree" class="figure">
<img src="assets/hmls_0501.png" alt="A diagram of a decision tree for classifying iris species based on petal length and width, showing split nodes and leaf nodes with classification results for setosa, versicolor, and virginica." width="1440" height="1032"/>
<h6><span class="label">Figure 5-1. </span>Iris decision tree</h6>
</div></figure>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Making Predictions"><div class="sect1" id="id92">
<h1>Making Predictions</h1>

<p>Let’s<a data-type="indexterm" data-primary="decision trees" data-secondary="predictions" id="xi_decisiontreespredictions5626_1"/><a data-type="indexterm" data-primary="predictions" data-secondary="decision trees" id="xi_predictionsdecisiontrees5626_1"/> see how the tree represented in <a data-type="xref" href="#iris_tree">Figure 5-1</a> makes predictions. Suppose you find an iris flower and you want to classify it based on its petals. You start at the <em>root node</em><a data-type="indexterm" data-primary="root node, decision tree" id="id1647"/> (depth 0, at the top): this node asks whether the flower’s petal length is smaller than 2.45 cm. If it is, then you move down to the root’s left child node (depth 1, left). In this case, it is a <em>leaf node</em><a data-type="indexterm" data-primary="leaf node" data-secondary="decision tree" id="id1648"/> (i.e., it does not have any child nodes), so it does not ask any questions: simply look at the predicted class for that node, and the decision tree predicts that your flower is an <em>Iris setosa</em> (<code translate="no">class=setosa</code>).</p>

<p>Now suppose you find another flower, and this time the petal length is greater than 2.45 cm. You again start at the root but now move down to its right child node (depth 1, right). This is not a leaf node, it’s a <em>split node</em>,<a data-type="indexterm" data-primary="split node, decision tree" id="id1649"/> so it asks another question: is the petal width smaller than 1.75 cm? If it is, then your flower is most likely an <em>Iris versicolor</em> (depth 2, left). If not, it is likely an <em>Iris virginica</em> (depth 2, right). It’s really that simple.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>One of the many qualities of decision trees is that they require very little data preparation. In fact, they don’t require feature scaling or centering at all.</p>
</div>

<p>A node’s <code translate="no">samples</code> attribute counts how many training instances it applies to. For example, 100 training instances have a petal length greater than 2.45 cm (depth 1, right), and of those 100, 54 have a petal width smaller than 1.75 cm (depth 2, left). A node’s <code translate="no">value</code> attribute tells you how many training instances of each class this node applies to: for example, the bottom-right node applies to 0 <em>Iris setosa</em>, 1 <em>Iris versicolor</em>, and 45 <em>Iris virginica</em>. Finally, a node’s <code translate="no">gini</code> attribute measures its <em>Gini impurity</em>:<a data-type="indexterm" data-primary="Gini impurity" id="id1650"/><a data-type="indexterm" data-primary="impurity measures" id="id1651"/> a node is “pure” (<code translate="no">gini=0</code>) if all training instances it applies to belong to the same class. For example, since the depth-1 left node applies only to <em>Iris setosa</em> training instances, its Gini impurity is 0. Conversely, the other nodes all apply to instances of multiple classes, so they are “impure”. <a data-type="xref" href="#gini_impurity">Equation 5-1</a> shows how the training algorithm computes the Gini impurity <em>G</em><sub><em>i</em></sub> of the <em>i</em><sup>th</sup> node. The more classes and the more mixed they are, the larger the impurity. For example, the depth-2 left node has a Gini impurity equal to 1 – (0/54)<sup>2</sup> – (49/54)<sup>2</sup> – (5/54)<sup>2</sup> ≈ 0.168.</p>
<div id="gini_impurity" data-type="equation" class="fifty-percent">
<h5><span class="label">Equation 5-1. </span>Gini impurity</h5>
<math alttext="upper G Subscript i Baseline equals 1 minus sigma-summation Underscript k equals 1 Overscript n Endscripts p Subscript i comma k Baseline Superscript 2" display="block">
  <mrow>
    <msub><mi>G</mi> <mi>i</mi> </msub>
    <mo>=</mo>
    <mn>1</mn>
    <mo>-</mo>
    <munderover><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </munderover>
    <msup><mrow><msub><mi>p</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow> </msub></mrow> <mn>2</mn> </msup>
  </mrow>
</math>
</div>

<p class="pagebreak-before">In this equation:</p>

<ul>
<li>
<p><em>G</em><sub><em>i</em></sub> is the Gini impurity of the <em>i</em><sup>th</sup> node.</p>
</li>
<li>
<p><em>p</em><sub><em>i</em>,<em>k</em></sub> is the ratio of class <em>k</em> instances among the training instances in the <em>i</em><sup>th</sup> node.</p>
</li>
</ul>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Scikit-Learn uses the CART algorithm<a data-type="indexterm" data-primary="CART (Classification and Regression Tree) algorithm" id="id1652"/><a data-type="indexterm" data-primary="Classification and Regression Tree (CART) algorithm" id="id1653"/> (discussed shortly), which produces only <em>binary trees</em>,<a data-type="indexterm" data-primary="binary trees" id="id1654"/> meaning trees where split nodes always have exactly two children (i.e., questions only have yes/no answers). However, other algorithms, such as ID3, can produce decision trees with nodes that have more than two <span class="keep-together">children</span>.</p>
</div>

<p><a data-type="xref" href="#decision_tree_decision_boundaries_plot">Figure 5-2</a> shows this decision tree’s decision boundaries.<a data-type="indexterm" data-primary="decision boundaries" id="id1655"/><a data-type="indexterm" data-primary="decision trees" data-secondary="decision boundaries" id="id1656"/> The thick vertical line represents the decision boundary of the root node<a data-type="indexterm" data-primary="root node, decision tree" id="id1657"/> (depth 0): petal length = 2.45 cm. Since the lefthand area is pure (only <em>Iris setosa</em>), it cannot be split any further. However, the righthand area is impure, so the depth-1 right node splits it at petal width = 1.75 cm (represented by the dashed line). Since <code translate="no">max_depth</code> was set to 2, the decision tree stops right there. If you set <code translate="no">max_depth</code> to 3, then the two depth-2 nodes would each add another decision boundary (represented by the two vertical dotted lines).<a data-type="indexterm" data-startref="xi_decisiontreespredictions5626_1" id="id1658"/><a data-type="indexterm" data-startref="xi_predictionsdecisiontrees5626_1" id="id1659"/></p>

<figure class="smallerninety"><div id="decision_tree_decision_boundaries_plot" class="figure">
<img src="assets/hmls_0502.png" alt="Diagram illustrating decision tree decision boundaries with depth levels and data points for different Iris flower species." width="2277" height="1071"/>
<h6><span class="label">Figure 5-2. </span>Decision tree decision boundaries</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>The tree structure, including all the information shown in <a data-type="xref" href="#iris_tree">Figure 5-1</a>, is available via the classifier’s <code translate="no">tree_</code> attribute. Type <strong><code translate="no">help(tree_clf.tree_)</code></strong> for details, and see <a href="https://homl.info/colab-p">this chapter’s notebook</a> for an example.<a data-type="indexterm" data-primary="interpretable ML" id="id1660"/></p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1661">
<h1>Model Interpretation: White Box Versus Black Box</h1>
<p>Decision trees are intuitive, and their decisions are easy to interpret. Such models are often called <em>white box models</em>. In contrast, as you will see, random forests and neural networks are generally considered <em>black box models</em>. They make great predictions, and you can easily check the calculations that they performed to make these predictions; nevertheless, it is usually hard to explain in simple terms why the predictions were made. For example, if a neural network says that a particular person appears in a picture, it is hard to know what contributed to this prediction: Did the model recognize that person’s eyes? Their mouth? Their nose? Their shoes? Or even the couch that they were sitting on? Conversely, decision trees provide nice, simple classification rules that can even be applied manually if need be (e.g., for flower classification). The field of <em>interpretable ML</em> aims at creating ML systems that can explain their decisions in a way humans can understand. This is important in many domains, for example in healthcare, to let a doctor review the diagnosis; in finance, to let analysts understand the risks; in a judicial system, to let a human make the final call; or in human resources, to ensure decisions aren’t biased.</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Estimating Class Probabilities"><div class="sect1" id="id93">
<h1>Estimating Class Probabilities</h1>

<p>A<a data-type="indexterm" data-primary="black box models" id="id1662"/><a data-type="indexterm" data-primary="decision trees" data-secondary="class probability estimates" id="id1663"/><a data-type="indexterm" data-primary="probabilities, estimating" id="id1664"/><a data-type="indexterm" data-primary="white box models" id="id1665"/> decision tree can also estimate the probability that an instance belongs to a particular class <em>k</em>. First it traverses the tree to find the leaf node for this instance, and then it returns the proportion of instances of class <em>k</em> among the training instances that would also reach this leaf node.<a data-type="indexterm" data-primary="leaf node" data-secondary="decision tree" id="id1666"/> For example, suppose you have found a flower whose petals are 5 cm long and 1.5 cm wide. The <span class="keep-together">corresponding</span> leaf node is the depth-2 left node, so the decision tree outputs the following probabilities: 0% for <em>Iris setosa</em> (0/54), 90.7% for <em>Iris versicolor</em> (49/54), and 9.3% for <em>Iris virginica</em> (5/54). And if you ask it to predict the class, it outputs <em>Iris versicolor</em> (class 1) because it has the highest probability. Let’s check this:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">tree_clf</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">([[</code><code class="mi">5</code><code class="p">,</code> <code class="mf">1.5</code><code class="p">]])</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code><code class="w"/>
<code class="go">array([[0.   , 0.907, 0.093]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">tree_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">([[</code><code class="mi">5</code><code class="p">,</code> <code class="mf">1.5</code><code class="p">]])</code><code class="w"/>
<code class="go">array([1])</code></pre>

<p>Perfect! Notice that the estimated probabilities would be identical anywhere else in the bottom-right rectangle of <a data-type="xref" href="#decision_tree_decision_boundaries_plot">Figure 5-2</a>—for example, if the petals were 6 cm long and 1.5 cm wide (even though it seems obvious that it would most likely be an <em>Iris virginica</em> in this case).</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="The CART Training Algorithm"><div class="sect1" id="id94">
<h1>The CART Training Algorithm</h1>

<p>Scikit-Learn<a data-type="indexterm" data-primary="CART (Classification and Regression Tree) algorithm" id="xi_CARTClassificationandRegressionTreealgorithm511913_1"/><a data-type="indexterm" data-primary="Classification and Regression Tree (CART) algorithm" id="xi_ClassificationandRegressionTreeCARTalgorithm511913_1"/><a data-type="indexterm" data-primary="decision trees" data-secondary="CART training algorithm" id="xi_decisiontreesCARTtrainingalgorithm511913_1"/><a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="CART algorithm" id="xi_ScikitLearnCARTalgorithm511913_1"/> uses the <em>Classification and Regression Tree</em> (CART) algorithm to train decision trees (also called “growing” trees). The algorithm works by first splitting the training set into two subsets using a single feature <em>k</em> and a threshold <em>t</em><sub><em>k</em></sub> (e.g., “petal length ≤ 2.45 cm”). How does it choose <em>k</em> and <em>t</em><sub><em>k</em></sub>? It searches for the pair (<em>k</em>, <em>t</em><sub><em>k</em></sub>) that produces the purest subsets, weighted by their size. <a data-type="xref" href="#classification_cart_cost_function">Equation 5-2</a> gives the cost function<a data-type="indexterm" data-primary="cost function" data-secondary="CART training algorithm" id="id1667"/> that the algorithm tries to minimize.</p>
<div id="classification_cart_cost_function" data-type="equation">
<h5><span class="label">Equation 5-2. </span>CART cost function for classification</h5>
<math alttext="StartLayout 1st Row 1st Column upper J left-parenthesis k comma t Subscript k Baseline right-parenthesis 2nd Column equals StartFraction m Subscript left Baseline Over m EndFraction upper G Subscript left Baseline plus StartFraction m Subscript right Baseline Over m EndFraction upper G Subscript right Baseline 2nd Row 1st Column where 2nd Column StartLayout Enlarged left-brace 1st Row  upper G Subscript left slash right Baseline measures the impurity of the left slash right subset 2nd Row  m Subscript left slash right Baseline is the number of instances in the left slash right subset 3rd Row  m equals m Subscript left Baseline plus m Subscript right EndLayout EndLayout" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mi>J</mi>
          <mo>(</mo>
          <mi>k</mi>
          <mo lspace="0%" rspace="0%">,</mo>
          <msub><mi>t</mi> <mi>k</mi> </msub>
          <mo>)</mo>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mfrac><msub><mi>m</mi> <mtext>left</mtext> </msub> <mi>m</mi></mfrac>
          <mspace width="0.166667em"/>
          <msub><mtext>G</mtext> <mtext>left</mtext> </msub>
          <mo>+</mo>
          <mfrac><msub><mi>m</mi> <mtext>right</mtext> </msub> <mi>m</mi></mfrac>
          <mspace width="0.166667em"/>
          <msub><mtext>G</mtext> <mtext>right</mtext> </msub>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mtext>where</mtext>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mfenced separators="" open="{" close="">
          <mtable>
            <mtr>
              <mtd columnalign="left">
                <mrow>
                  <msub><mtext>G</mtext> <mtext>left/right</mtext> </msub>
                  <mspace width="4.pt"/>
                  <mtext>measures</mtext>
                  <mspace width="4.pt"/>
                  <mtext>the</mtext>
                  <mspace width="4.pt"/>
                  <mtext>impurity</mtext>
                  <mspace width="4.pt"/>
                  <mtext>of</mtext>
                  <mspace width="4.pt"/>
                  <mtext>the</mtext>
                  <mspace width="4.pt"/>
                  <mtext>left/right</mtext>
                  <mspace width="4.pt"/>
                  <mtext>subset</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd columnalign="left">
                <mrow>
                  <msub><mi>m</mi> <mtext>left/right</mtext> </msub>
                  <mspace width="4.pt"/>
                  <mtext>is</mtext>
                  <mspace width="4.pt"/>
                  <mtext>the</mtext>
                  <mspace width="4.pt"/>
                  <mtext>number</mtext>
                  <mspace width="4.pt"/>
                  <mtext>of</mtext>
                  <mspace width="4.pt"/>
                  <mtext>instances</mtext>
                  <mspace width="4.pt"/>
                  <mtext>in</mtext>
                  <mspace width="4.pt"/>
                  <mtext>the</mtext>
                  <mspace width="4.pt"/>
                  <mtext>left/right</mtext>
                  <mspace width="4.pt"/>
                  <mtext>subset</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd columnalign="left">
                <mrow>
                  <mi>m</mi>
                  <mo>=</mo>
                  <msub><mi>m</mi> <mtext>left</mtext> </msub>
                  <mo>+</mo>
                  <msub><mi>m</mi> <mtext>right</mtext> </msub>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </mfenced>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>Once the CART algorithm has successfully split the training set in two, it splits the subsets using the same logic, then the sub-subsets, and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the <code translate="no">max_depth</code> hyperparameter),<a data-type="indexterm" data-primary="hyperparameters" data-secondary="CART algorithm" id="id1668"/> or if it cannot find a split that will reduce impurity. A few other hyperparameters (described in a moment) control additional stopping conditions: <code translate="no">min_samples_split</code>, <code translate="no">min_samples_leaf</code>, <code translate="no">max_leaf_nodes</code>, and more.<a data-type="indexterm" data-primary="big O notation" id="id1669"/><a data-type="indexterm" data-primary="greedy algorithm, CART as" id="id1670"/><a data-type="indexterm" data-primary="NP-Complete problem, CART as" id="id1671"/><a data-type="indexterm" data-primary="polynomial time" id="id1672"/></p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>As you can see, the CART algorithm is a <em>greedy algorithm</em>: it greedily searches for an optimum split at the top level, then repeats the process at each subsequent level. It does not check whether the split will lead to the lowest possible impurity several levels down. A greedy algorithm often produces a solution that’s reasonably good but not guaranteed to be optimal.</p>

<p>Unfortunately, finding the optimal tree is known to be an <em>NP-complete</em> problem.⁠<sup><a data-type="noteref" id="id1673-marker" href="ch05.html#id1673">1</a></sup> It requires <em>O</em>(exp(<em>m</em>)) time,⁠<sup><a data-type="noteref" id="id1674-marker" href="ch05.html#id1674">2</a></sup> making the problem intractable even for small training sets. This is why we must settle for a “reasonably good” solution when training decision trees.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Computational Complexity"><div class="sect1" id="id95">
<h1>Computational Complexity</h1>

<p>Making<a data-type="indexterm" data-startref="xi_CARTClassificationandRegressionTreealgorithm511913_1" id="id1675"/><a data-type="indexterm" data-startref="xi_ClassificationandRegressionTreeCARTalgorithm511913_1" id="id1676"/><a data-type="indexterm" data-primary="computational complexity" data-secondary="decision trees" id="id1677"/><a data-type="indexterm" data-startref="xi_decisiontreesCARTtrainingalgorithm511913_1" id="id1678"/><a data-type="indexterm" data-startref="xi_ScikitLearnCARTalgorithm511913_1" id="id1679"/> predictions requires traversing the decision tree from the root to a leaf. Decision trees<a data-type="indexterm" data-primary="decision trees" data-secondary="computational complexity" id="id1680"/> are generally approximately balanced, so traversing the decision tree requires going through roughly <em>O</em>(log<sub>2</sub>(<em>m</em>)) nodes, where <em>m</em> is the number of training instances, and log<sub>2</sub>(<em>m</em>) is the <em>binary logarithm</em><a data-type="indexterm" data-primary="binary logarithm" id="id1681"/> of <em>m</em>, equal to log(<em>m</em>) / log(2). Since each node only requires checking the value of one feature, the overall prediction complexity is <em>O</em>(log<sub>2</sub>(<em>m</em>)), independent of the number of features. So predictions are very fast, even when dealing with large training sets.</p>

<p>By default, the training algorithm compares all features on all samples at each node, which results in a training complexity of <em>O</em>(<em>n</em> × <em>m</em> log<sub>2</sub>(<em>m</em>)).</p>

<p>It’s possible to set a maximum tree depth using the <code translate="no">max_depth</code><a data-type="indexterm" data-primary="hyperparameters" data-secondary="max_depth" id="id1682"/><a data-type="indexterm" data-primary="max_depth hyperparameter" id="id1683"/> hyperparameter, and/or set a maximum number of features to consider at each node (the features are then chosen randomly). Doing so will help speed up training considerably, and it can also reduce the risk of overfitting (but as always, going too far would result in underfitting).</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Gini Impurity or Entropy?"><div class="sect1" id="id96">
<h1>Gini Impurity or Entropy?</h1>

<p>By default<a data-type="indexterm" data-primary="decision trees" data-secondary="Gini impurity or entropy measures" id="id1684"/>, the <code translate="no">DecisionTreeClassifier</code><a data-type="indexterm" data-primary="sklearn" data-secondary="tree.DecisionTreeClassifier" id="id1685"/> class<a data-type="indexterm" data-primary="DecisionTreeClassifier" id="id1686"/> uses the Gini impurity measure<a data-type="indexterm" data-primary="Gini impurity" id="id1687"/>, but you can select the <em>entropy</em><a data-type="indexterm" data-primary="entropy impurity measure" id="id1688"/> impurity measure<a data-type="indexterm" data-primary="impurity measures" id="id1689"/> instead by setting the <code translate="no">criterion</code> hyperparameter to <code translate="no">"entropy"</code>. The concept of entropy originated in thermodynamics as a measure of molecular disorder: entropy approaches zero when molecules are still and well ordered. Entropy later spread to a wide variety of domains, including in Shannon’s information theory<a data-type="indexterm" data-primary="information theory" id="id1690"/>, where it measures the average information content of a message, as we saw in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter 4</a>. Entropy is zero when all messages are identical. In machine learning, entropy is frequently used as an <span class="keep-together">impurity</span> measure: a set’s entropy is zero when it contains instances of only one class. <a data-type="xref" href="#entropy_function">Equation 5-3</a> shows the definition of the entropy of the <em>i</em><sup>th</sup> node. For example, the depth-2 left node in <a data-type="xref" href="#iris_tree">Figure 5-1</a> has an entropy equal to –(49/54) log<sub>2</sub> (49/54) – (5/54) log<sub>2</sub> (5/54) ≈ 0.445.</p>
<div data-type="equation" id="entropy_function">
<h5><span class="label">Equation 5-3. </span>Entropy</h5>
<math display="block">
  <mrow>
    <msub><mi>H</mi> <mi>i</mi> </msub>
    <mo>=</mo>
    <mo>-</mo>
    <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mrow><msub><mi>p</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow> </msub><mo>≠</mo><mn>0</mn></mrow></mfrac> <mi>n</mi> </munderover>
    <mrow>
      <msub><mi>p</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow> </msub>
      <msub><mo form="prefix">log</mo> <mn>2</mn> </msub>
      <mrow>
        <mo>(</mo>
        <msub><mi>p</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow> </msub>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
</math>
</div>

<p>So, should you use Gini impurity or entropy? The truth is, most of the time it does not make a big difference: they lead to similar trees. Gini impurity is slightly faster to compute, so it is a good default. However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees.⁠<sup><a data-type="noteref" id="id1691-marker" href="ch05.html#id1691">3</a></sup></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Regularization Hyperparameters"><div class="sect1" id="id97">
<h1>Regularization Hyperparameters</h1>

<p>Decision<a data-type="indexterm" data-primary="decision trees" data-secondary="regularization hyperparameters" id="xi_decisiontreesregularizationhyperparameters51869_1"/><a data-type="indexterm" data-primary="regularization" data-secondary="hyperparameters" id="xi_regularizationhyperparameters51869_1"/> trees make very few assumptions about the training data (as opposed to linear models, which assume that the data is linear, for example). If left unconstrained, the tree structure will adapt itself to the training data, fitting it very closely—indeed, most likely overfitting it. Such a model is often called a <em>nonparametric model</em>,<a data-type="indexterm" data-primary="nonparametric models" id="id1692"/> not because it does not have any parameters (it often has a lot) but because the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data. In contrast, a <em>parametric model</em>,<a data-type="indexterm" data-primary="parametric models" id="id1693"/> such as a linear model, has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of underfitting).</p>

<p>To avoid overfitting<a data-type="indexterm" data-primary="overfitting of data" data-secondary="and decision trees" data-secondary-sortas="decision" id="id1694"/> the training data, you need to restrict the decision tree’s freedom during training. As you know by now, this is called regularization. The regularization hyperparameters depend on the algorithm used, but generally you can at least restrict the maximum depth of the decision tree. In Scikit-Learn, this is controlled by the <code translate="no">max_depth</code> hyperparameter. The default value is <code translate="no">None</code>, which means unlimited. Reducing <code translate="no">max_depth</code> will regularize the model and thus reduce the risk of overfitting.</p>

<p>The <code translate="no">DecisionTreeClassifier</code><a data-type="indexterm" data-primary="sklearn" data-secondary="tree.DecisionTreeClassifier" id="id1695"/> class<a data-type="indexterm" data-primary="DecisionTreeClassifier" id="id1696"/> has a few other parameters that similarly restrict the shape of the decision tree:</p>
<dl>
<dt><code translate="no">max_features</code></dt>
<dd>
<p>Maximum number of features that are evaluated for splitting at each node</p>
</dd>
<dt><code translate="no">max_leaf_nodes</code></dt>
<dd>
<p>Maximum<a data-type="indexterm" data-primary="leaf node" data-secondary="decision tree" id="id1697"/> number of leaf nodes</p>
</dd>
<dt><code translate="no">min_samples_split</code></dt>
<dd>
<p>Minimum number of samples a node must have before it can be split</p>
</dd>
<dt><code translate="no">min_samples_leaf</code></dt>
<dd>
<p>Minimum number of samples a leaf node must have to be created</p>
</dd>
<dt><code translate="no">min_weight_fraction_leaf</code></dt>
<dd>
<p>Same as <code translate="no">min_samples_leaf</code> but expressed as a fraction of the total number of weighted instances</p>
</dd>
<dt><code translate="no">min_impurity_decrease</code></dt>
<dd>
<p>Only split a node if this split results in at least this reduction in impurity</p>
</dd>
<dt><code translate="no">ccp_alpha</code></dt>
<dd>
<p>Controls <em>minimal cost-complexity pruning</em> (MCCP)<a data-type="indexterm" data-primary="MCCP (minimal cost-complexity pruning)" id="id1698"/><a data-type="indexterm" data-primary="minimal cost-complexity pruning (MCCP)" id="id1699"/>, i.e., pruning subtrees that don’t reduce impurity enough compared to their number of leaves; a larger <code translate="no">ccp_alpha</code><a data-type="indexterm" data-primary="ccp_alpha value" id="id1700"/> value leads to more pruning, resulting in a smaller tree (the default is 0—no pruning)</p>
</dd>
</dl>

<p>To limit the model’s complexity and thereby regularize the model, you can increase <code translate="no">min_*</code> hyperparameters<a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="min_* and max_* hyperparameters" id="id1701"/> or <code translate="no">ccp_alpha</code>, or decrease <code translate="no">max_*</code> hyperparameters. Tuning <code translate="no">max_depth</code> is usually a good default: it provides effective regularization, and it keeps the tree small and easy to interpret. Setting <code translate="no">min_samples_leaf</code> is also a good idea, especially for small datasets. And <code translate="no">max_features</code> is great when working with high-dimensional datasets.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Other algorithms work by first training the decision tree without restrictions, then <em>pruning</em><a data-type="indexterm" data-primary="pruning of decision tree nodes" id="id1702"/> (deleting) unnecessary nodes. A node whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically significant<a data-type="indexterm" data-primary="statistical significance" id="id1703"/>. Standard statistical tests, such as the <em>χ</em><sup>2</sup> <em>test</em> (chi-squared test)<a data-type="indexterm" data-primary="chi-squared (χ2) test" id="id1704"/><a data-type="indexterm" data-primary="χ2 (chi-squared) test" id="id1705"/>, are used to estimate the probability that the improvement is purely the result of chance (which is called the <em>null hypothesis</em>)<a data-type="indexterm" data-primary="null hypothesis" id="id1706"/>. If this probability, called the <em>p-value</em>,<a data-type="indexterm" data-primary="p-value" id="id1707"/> is higher than a given threshold (typically 5%, controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted. The pruning continues until all unnecessary nodes have been pruned.</p>
</div>

<p>Let’s test regularization<a data-type="indexterm" data-primary="regularization" data-secondary="decision trees" id="id1708"/> on the moons dataset: this is a toy dataset for binary classification in which the data points are shaped as two interleaving crescent moons (see <a data-type="xref" href="#min_samples_leaf_plot">Figure 5-3</a>). You can generate this dataset using the <code translate="no">make_moons()</code> function.</p>

<p>We’ll train one decision tree without regularization, and another with <code translate="no">min_samples_leaf=5</code>. Here’s the code; <a data-type="xref" href="#min_samples_leaf_plot">Figure 5-3</a> shows the decision boundaries of each tree:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_moons</code>

<code class="n">X_moons</code><code class="p">,</code> <code class="n">y_moons</code> <code class="o">=</code> <code class="n">make_moons</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">150</code><code class="p">,</code> <code class="n">noise</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>

<code class="n">tree_clf1</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">tree_clf2</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">min_samples_leaf</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">tree_clf1</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_moons</code><code class="p">,</code> <code class="n">y_moons</code><code class="p">)</code>
<code class="n">tree_clf2</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_moons</code><code class="p">,</code> <code class="n">y_moons</code><code class="p">)</code></pre>

<figure><div id="min_samples_leaf_plot" class="figure">
<img src="assets/hmls_0503.png" alt="Comparison of decision boundaries: the left diagram shows an unregularized decision tree's complex boundaries, while the right depicts a regularized tree with smoother boundaries indicating potentially better generalization." width="2875" height="1069"/>
<h6><span class="label">Figure 5-3. </span>Decision boundaries of an unregularized tree (left) and a regularized tree (right)</h6>
</div></figure>

<p>The unregularized model on the left is clearly overfitting, and the regularized model on the right will probably generalize better. We can verify this by evaluating both trees on a test set generated using a different random seed:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_moons_test</code><code class="p">,</code> <code class="n">y_moons_test</code> <code class="o">=</code> <code class="n">make_moons</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">noise</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code><code class="w"/>
<code class="gp">... </code>                                        <code class="n">random_state</code><code class="o">=</code><code class="mi">43</code><code class="p">)</code><code class="w"/>
<code class="gp">...</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">tree_clf1</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_moons_test</code><code class="p">,</code> <code class="n">y_moons_test</code><code class="p">)</code><code class="w"/>
<code class="go">0.898</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">tree_clf2</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_moons_test</code><code class="p">,</code> <code class="n">y_moons_test</code><code class="p">)</code><code class="w"/>
<code class="go">0.92</code></pre>

<p>Indeed, the second tree has a better accuracy on the test set.<a data-type="indexterm" data-startref="xi_decisiontreesregularizationhyperparameters51869_1" id="id1709"/><a data-type="indexterm" data-startref="xi_regularizationhyperparameters51869_1" id="id1710"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Regression"><div class="sect1" id="id98">
<h1>Regression</h1>

<p>Decision<a data-type="indexterm" data-primary="decision trees" data-secondary="regression tasks" id="xi_decisiontreesregressiontasks52489_1"/><a data-type="indexterm" data-primary="regression models" data-secondary="decision tree tasks" id="xi_regressionmodelsdecisiontreetasks52489_1"/> trees are also capable of performing regression tasks. While linear regression only works well with linear data, decision trees can fit all sorts of complex datasets. Let’s build a regression tree using Scikit-Learn’s <code translate="no">DecisionTreeRegressor</code> class<a data-type="indexterm" data-primary="DecisionTreeRegressor" id="id1711"/><a data-type="indexterm" data-primary="sklearn" data-secondary="tree.DecisionTreeRegressor" id="id1712"/>, training it on a noisy quadratic dataset with <code translate="no">max_depth=2</code>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeRegressor</code>

<code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">default_rng</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">X_quad</code> <code class="o">=</code> <code class="n">rng</code><code class="o">.</code><code class="n">random</code><code class="p">((</code><code class="mi">200</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code> <code class="o">-</code> <code class="mf">0.5</code>  <code class="c1"># a single random input feature</code>
<code class="n">y_quad</code> <code class="o">=</code> <code class="n">X_quad</code> <code class="o">**</code> <code class="mi">2</code> <code class="o">+</code> <code class="mf">0.025</code> <code class="o">*</code> <code class="n">rng</code><code class="o">.</code><code class="n">standard_normal</code><code class="p">((</code><code class="mi">200</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>

<code class="n">tree_reg</code> <code class="o">=</code> <code class="n">DecisionTreeRegressor</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">tree_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_quad</code><code class="p">,</code> <code class="n">y_quad</code><code class="p">)</code></pre>

<p>The resulting tree is represented in <a data-type="xref" href="#regression_tree">Figure 5-4</a>.</p>

<figure><div id="regression_tree" class="figure">
<img src="assets/hmls_0504.png" alt="A decision tree diagram for regression, showing how input variables are split at different nodes with squared error, sample size, and predicted value at each node." width="1442" height="587"/>
<h6><span class="label">Figure 5-4. </span>A decision tree for regression</h6>
</div></figure>

<p>This tree looks very similar to the classification tree you built earlier. The main difference is that instead of predicting a class in each node, it predicts a value. For example, suppose you want to make a prediction for a new instance with <em>x</em><sub>1</sub> = 0.2. The root node asks whether <em>x</em><sub>1</sub> ≤ 0.343. Since it is, the algorithm goes to the left child node, which asks whether <em>x</em><sub>1</sub> ≤ –0.302. Since it is not, the algorithm goes to the right child node. This is a leaf node, and it predicts <code translate="no">value=0.038</code>. This prediction is the average target value of the 133 training instances associated with this leaf node, and it results in a mean squared error equal to 0.002 over these 133 instances.</p>

<p>This model’s predictions are represented on the left in <a data-type="xref" href="#tree_regression_plot">Figure 5-5</a>. If you set <code translate="no">max_depth=3</code>, you get the predictions represented on the right. Notice how the predicted value for each region is always the average target value of the instances in that region. The algorithm splits each region in a way that makes most training instances as close as possible to that predicted value.</p>

<figure><div id="tree_regression_plot" class="figure">
<img src="assets/hmls_0505.png" alt="Two plots show decision tree regression predictions: the left with max depth 2 having simpler splits, and the right with max depth 3 showing more detailed splits." width="2859" height="1069"/>
<h6><span class="label">Figure 5-5. </span>Predictions of two decision tree regression models</h6>
</div></figure>

<p>The CART algorithm<a data-type="indexterm" data-primary="CART (Classification and Regression Tree) algorithm" id="id1713"/><a data-type="indexterm" data-primary="Classification and Regression Tree (CART) algorithm" id="id1714"/><a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="CART algorithm" id="id1715"/> works as described earlier, except that instead of trying to split the training set in a way that minimizes impurity, it now tries to split the training set in a way that minimizes the MSE. <a data-type="xref" href="#regression_cart_cost_function">Equation 5-4</a> shows the cost function<a data-type="indexterm" data-primary="cost function" data-secondary="CART training algorithm" id="id1716"/> that the algorithm tries to minimize.</p>
<div id="regression_cart_cost_function" data-type="equation">
<h5><span class="label">Equation 5-4. </span>CART cost function for regression</h5>
<math alttext="upper J left-parenthesis k comma t Subscript k Baseline right-parenthesis equals StartFraction m Subscript left Baseline Over m EndFraction MSE Subscript left Baseline plus StartFraction m Subscript right Baseline Over m EndFraction MSE Subscript right Baseline where StartLayout Enlarged left-brace 1st Row  MSE Subscript node Baseline equals StartFraction sigma-summation Underscript i element-of node Endscripts left-parenthesis ModifyingAbove y With caret Subscript node Baseline minus y Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis squared Over m Subscript node Baseline EndFraction 2nd Row  ModifyingAbove y With caret Subscript node Baseline equals StartFraction sigma-summation Underscript i element-of node Endscripts y Superscript left-parenthesis i right-parenthesis Baseline Over m Subscript node Baseline EndFraction EndLayout" display="block">
  <mrow>
    <mi>J</mi>
    <mrow>
      <mo>(</mo>
      <mi>k</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <msub><mi>t</mi> <mi>k</mi> </msub>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mfrac><msub><mi>m</mi> <mtext>left</mtext> </msub> <mi>m</mi></mfrac>
    <msub><mtext>MSE</mtext> <mtext>left</mtext> </msub>
    <mo>+</mo>
    <mfrac><msub><mi>m</mi> <mtext>right</mtext> </msub> <mi>m</mi></mfrac>
    <msub><mtext>MSE</mtext> <mtext>right</mtext> </msub>
    <mspace width="1.em"/>
    <mtext>where</mtext>
    <mspace width="0.222222em"/>
    <mfenced separators="" open="{" close="">
      <mtable>
        <mtr>
          <mtd columnalign="left">
            <mrow>
              <msub><mtext>MSE</mtext> <mtext>node</mtext> </msub>
              <mo>=</mo>
              <mfrac><mrow><msub><mo>∑</mo> <mrow><mi>i</mi><mo>∈</mo><mtext>node</mtext></mrow> </msub><msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mtext>node</mtext> </msub><mo>-</mo><msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>)</mo></mrow> <mn>2</mn> </msup></mrow> <msub><mi>m</mi> <mtext>node</mtext> </msub></mfrac>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd columnalign="left">
            <mrow>
              <msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mtext>node</mtext> </msub>
              <mo>=</mo>
              <mfrac><mrow><msub><mo>∑</mo> <mrow><mi>i</mi><mo>∈</mo><mtext>node</mtext></mrow> </msub><msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup></mrow> <msub><mi>m</mi> <mtext>node</mtext> </msub></mfrac>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mfenced>
  </mrow>
</math>
</div>

<p>Just like for classification tasks, decision trees are prone to overfitting when dealing with regression tasks. Without any regularization (i.e., using the default hyperparameters), you get the predictions on the left in <a data-type="xref" href="#tree_regression_regularization_plot">Figure 5-6</a>. These predictions are obviously overfitting the training set very badly. Just setting <code translate="no">min_samples_leaf=10</code> results in a much more reasonable model, represented on the right in <a data-type="xref" href="#tree_regression_regularization_plot">Figure 5-6</a>.<a data-type="indexterm" data-startref="xi_decisiontreesregressiontasks52489_1" id="id1717"/><a data-type="indexterm" data-startref="xi_regressionmodelsdecisiontreetasks52489_1" id="id1718"/></p>

<figure class="width-90"><div id="tree_regression_regularization_plot" class="figure">
<img src="assets/hmls_0506.png" alt="Comparison of regression tree predictions showing overfitting with no restrictions and improved fit with `min_samples_leaf=10`." width="2875" height="1069"/>
<h6><span class="label">Figure 5-6. </span>Predictions of an unregularized regression tree (left) and a regularized tree (right)</h6>
</div></figure>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Sensitivity to Axis Orientation"><div class="sect1" id="id99">
<h1>Sensitivity to Axis Orientation</h1>

<p>Hopefully<a data-type="indexterm" data-primary="decision trees" data-secondary="sensitivity to axis orientation" id="xi_decisiontreessensitivitytoaxisorientation529910_1"/><a data-type="indexterm" data-primary="overfitting of data" data-secondary="and decision trees" data-secondary-sortas="decision" id="xi_overfittingofdataanddecisiontrees529910_1"/> by now you are convinced that decision trees have a lot going for them: they are relatively easy to understand and interpret, simple to use, versatile, and powerful. However, they do have a few limitations. First, as you may have noticed, decision trees<a data-type="indexterm" data-primary="decision boundaries" id="id1719"/> love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to the data’s orientation. For example, <a data-type="xref" href="#sensitivity_to_rotation_plot">Figure 5-7</a> shows a simple linearly separable dataset: on the left, a decision tree can split it easily, while on the right, after the dataset is rotated by 45°, the decision boundary looks unnecessarily convoluted. Although both decision trees fit the training set perfectly, it is very likely that the model on the right will not generalize well.</p>

<figure><div id="sensitivity_to_rotation_plot" class="figure">
<img src="assets/hmls_0507.png" alt="Diagram showing decision trees' sensitivity to data orientation, with a clear boundary before rotation and a complex boundary after rotation by 45 degrees." width="2875" height="1071"/>
<h6><span class="label">Figure 5-7. </span>Sensitivity to training set rotation</h6>
</div></figure>

<p>One way to limit this problem is to scale the data, then apply a principal component analysis (PCA) transformation<a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="for scaling data in decision trees" data-secondary-sortas="scaling" id="id1720"/>. We will look at PCA in detail in <a data-type="xref" href="ch07.html#dimensionality_chapter">Chapter 7</a>, but for now you only need to know that it rotates the data in a way that reduces the correlation between the features, which often (not always) makes things easier for trees.</p>

<p>Let’s create a small pipeline that scales the data and rotates it using PCA, then train a <code translate="no">DecisionTreeClassifier</code><a data-type="indexterm" data-primary="DecisionTreeClassifier" id="id1721"/><a data-type="indexterm" data-primary="sklearn" data-secondary="tree.DecisionTreeClassifier" id="id1722"/> on that data. <a data-type="xref" href="#pca_preprocessing_plot">Figure 5-8</a> shows the decision boundaries of that tree: as you can see, the rotation makes it possible to fit the dataset pretty well using only one feature, <em>z</em><sub>1</sub>, which is a linear function of the original petal length and width. Here’s the code:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">PCA</code>
<code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">make_pipeline</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>

<code class="n">pca_pipeline</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">StandardScaler</code><code class="p">(),</code> <code class="n">PCA</code><code class="p">())</code>
<code class="n">X_iris_rotated</code> <code class="o">=</code> <code class="n">pca_pipeline</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_iris</code><code class="p">)</code>
<code class="n">tree_clf_pca</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">tree_clf_pca</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_iris_rotated</code><code class="p">,</code> <code class="n">y_iris</code><code class="p">)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>The <code translate="no">DecisionTreeClassifier</code> and <code translate="no">DecisionTreeRegressor</code> classes both support missing values natively, no need for an imputer.<a data-type="indexterm" data-startref="xi_decisiontreessensitivitytoaxisorientation529910_1" id="id1723"/><a data-type="indexterm" data-startref="xi_overfittingofdataanddecisiontrees529910_1" id="id1724"/></p>
</div>

<figure><div id="pca_preprocessing_plot" class="figure">
<img src="assets/hmls_0508.png" alt="Diagram illustrating decision boundaries of a decision tree on the scaled and PCA-rotated iris dataset, with separate regions for Iris setosa, versicolor, and virginica." width="2275" height="1071"/>
<h6><span class="label">Figure 5-8. </span>A tree’s decision boundaries on the scaled and PCA-rotated iris dataset</h6>
</div></figure>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Decision Trees Have a High Variance"><div class="sect1" id="id100">
<h1>Decision Trees Have a High Variance</h1>

<p>More<a data-type="indexterm" data-primary="decision trees" data-secondary="high variance with" id="xi_decisiontreeshighvariancewith53325_1"/><a data-type="indexterm" data-primary="high variance, with decision trees" id="xi_highvariancewithdecisiontrees53325_1"/><a data-type="indexterm" data-primary="variance" data-secondary="high variance with decision trees" id="xi_variancehighvariancewithdecisiontrees53325_1"/> generally, the main issue with decision trees is that they have quite a high variance: small changes to the hyperparameters or to the data may produce very different models. In fact, since the training algorithm used by Scikit-Learn is stochastic—it randomly selects the set of features to evaluate at each node—even retraining the same decision tree on the exact same data may produce a very different model, such as the one represented in <a data-type="xref" href="#decision_tree_high_variance_plot">Figure 5-9</a> (unless you set the <code translate="no">random_state</code> hyperparameter). As you can see, it looks very different from the previous decision tree (<a data-type="xref" href="#decision_tree_decision_boundaries_plot">Figure 5-2</a>).</p>

<figure><div id="decision_tree_high_variance_plot" class="figure">
<img src="assets/hmls_0509.png" alt="Diagram showing decision boundaries for petal width and length in a decision tree, illustrating high variance due to changes in depth and classification regions." width="2277" height="1071"/>
<h6><span class="label">Figure 5-9. </span>Retraining the same model on the same data may produce a very different model</h6>
</div></figure>

<p>Luckily, by averaging predictions over many trees, it’s possible to reduce variance significantly. Such an <em>ensemble</em> of trees is called a <em>random forest</em>, and it’s one of the most powerful types of models available today, as you will see in the next chapter.<a data-type="indexterm" data-startref="xi_decisiontrees5317_1" id="id1725"/><a data-type="indexterm" data-startref="xi_decisiontreeshighvariancewith53325_1" id="id1726"/><a data-type="indexterm" data-startref="xi_highvariancewithdecisiontrees53325_1" id="id1727"/><a data-type="indexterm" data-startref="xi_variancehighvariancewithdecisiontrees53325_1" id="id1728"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="id729">
<h1>Exercises</h1>
<ol>
<li>
<p>What is the approximate depth of a decision tree trained (without restrictions) on a training set with one million instances?</p>
</li>
<li>
<p>Is a node’s Gini impurity generally lower or higher than its parent’s? Is it <em>generally</em> lower/higher, or <em>always</em> lower/higher?</p>
</li>
<li>
<p>If a decision tree is overfitting the training set, is it a good idea to try decreasing <code translate="no">max_depth</code>?</p>
</li>
<li>
<p>If a decision tree is underfitting the training set, is it a good idea to try scaling the input features?</p>
</li>
<li>
<p>If it takes one hour to train a decision tree on a training set containing one million instances, roughly how much time will it take to train another decision tree on a training set containing ten million instances? Hint: consider the CART algorithm’s computational complexity.</p>
</li>
<li>
<p>If it takes one hour to train a decision tree on a given training set, roughly how much time will it take if you double the number of features?</p>
</li>
<li>
<p>Train and fine-tune a decision tree for the moons dataset by following these steps:</p>
<ol>
<li>
<p>Use <code translate="no">make_moons(n_samples=10000, noise=0.4)</code> to generate a moons dataset.</p>
</li>
<li>
<p>Use <code translate="no">train_test_split()</code> to split the dataset into a training set and a test set.</p>
</li>
<li>
<p>Use grid search with cross-validation (with the help of the <code translate="no">GridSearchCV</code> class) to find good hyperparameter values for a <code translate="no">DecisionTreeClassifier</code>. Hint: try various values for <code translate="no">max_leaf_nodes</code>.</p>
</li>
<li>
<p>Train it on the full training set using these hyperparameters, and measure your model’s performance on the test set. You should get roughly 85% to 87% accuracy.</p>
</li>

</ol>
</li>
<li>
<p>Grow a forest by following these steps:</p>
<ol>
<li>
<p>Continuing the previous exercise, generate 1,000 subsets of the training set, each containing 100 instances selected randomly. Hint: you can use Scikit-Learn’s <code translate="no">ShuffleSplit</code> class for this.</p>
</li>
<li>
<p>Train one decision tree on each subset, using the best hyperparameter values found in the previous exercise. Evaluate these 1,000 decision trees on the test set. Since they were trained on smaller sets, these decision trees will likely perform worse than the first decision tree, achieving only about 80% <span class="keep-together">accuracy</span>.</p>
</li>
<li>
<p>Now comes the magic. For each test set instance, generate the predictions of the 1,000 decision trees, and keep only the most frequent prediction (you can use SciPy’s <code translate="no">mode()</code> function for this). This approach gives you <em>majority-vote predictions</em> over the test set.</p>
</li>
<li>
<p>Evaluate these predictions on the test set: you should obtain a slightly higher accuracy than your first model (about 0.5 to 1.5% higher). Congratulations, you have trained a random forest classifier!</p>
</li>

</ol>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter’s notebook, at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id1673"><sup><a href="ch05.html#id1673-marker">1</a></sup> P is the set of problems that can be solved in <em>polynomial time</em> (i.e., a polynomial of the dataset size). NP is the set of problems whose solutions can be verified in polynomial time. An NP-hard problem is a problem that can be reduced to a known NP-hard problem in polynomial time. An NP-complete problem is both NP and NP-hard. A major open mathematical question is whether P = NP. If P ≠ NP (which seems likely), then no polynomial algorithm will ever be found for any NP-complete problem (except perhaps one day on a quantum computer).</p><p data-type="footnote" id="id1674"><sup><a href="ch05.html#id1674-marker">2</a></sup> This <em>big O notation</em> means that as <em>m</em> (i.e., the number of training instances) gets larger, the computation time becomes proportional to the exponential of <em>m</em> (it’s actually an upper bound, but we make it as small as we can). This tells us how “fast” the computation grows with <em>m</em>, and <em>O</em>(exp(<em>m</em>)) is very fast.</p><p data-type="footnote" id="id1691"><sup><a href="ch05.html#id1691-marker">3</a></sup> See Sebastian Raschka’s <a href="https://homl.info/19">interesting analysis</a> for more details.</p></div></div></section></div></div></body></html>