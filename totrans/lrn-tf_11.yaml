- en: Appendix A. Tips on Model Construction and Using TensorFlow Serving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model Structuring and Customization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this short section we will focus on two topics that continue from and extend
    the previous chapters—how to construct a proper model, and how to customize the
    model’s entities. We start by describing how we can effectively reframe our code
    by using encapsulations and allow its variables to be shared and reused. In the
    second part of this section we will talk about how to customize our own loss functions
    and operations and use them for optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Model Structuring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ultimately, we would like to design our TensorFlow code efficiently, so that
    it can be reused for multiple tasks and is easy to follow and pass around. One
    way to make things cleaner is to use one of the available TensorFlow extension
    libraries, which were discussed in [Chapter 7](ch07.html#tensorflow_abstractions_and_simplifications).
    However, while they are great to use for typical networks, models with new components
    that we wish to implement may sometimes require the full flexibility of lower-level
    TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take another look at the optimization code from the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The entire code here is simply stacked line by line. This is OK for simple and
    focused examples. However, this way of coding has its limits—it’s neither reusable
    nor very readable when the code gets more complex.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s zoom out and think about what characteristics our infrastructure should
    have. First, we would like to encapsulate the model so it can be used for various
    tasks like training, evaluation, and forming predictions. Furthermore, it can
    also be more efficient to construct the model in a modular fashion, giving us
    specific control over its subcomponents and increasing readability. This will
    be the focus of the next few sections.
  prefs: []
  type: TYPE_NORMAL
- en: Modular design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A good start is to split the code into functions that capture different elements
    in the learning model. We can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now we can reuse the code with different inputs, and this division makes it
    easier to read, especially when it gets more complex.
  prefs: []
  type: TYPE_NORMAL
- en: In this example we called the main function twice with the same inputs and printed
    the variables that were created. Note that each call created a different set of
    variables, resulting in the creation of four variables. Let’s assume, for example,
    a scenario where we wish to build a model with multiple inputs, such as two different
    images. Say we wish to apply the same convolutional filters to both input images. New
    variables will be created. To avoid this, we “share” the filter variables, using
    the same variables on both images.
  prefs: []
  type: TYPE_NORMAL
- en: Variable sharing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It’s possible to reuse the same variables by creating them with `tf.get_variable()`
    instead of `tf.Variable()`. We use this very similarly to `tf.Variable()`, except
    that we need to pass an initializer as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here we used `tf.zeros_initializer()`. This initializer is very similar to `tf.zeros()`,
    except that it doesn’t get the shape as an argument, but rather arranges the values
    according to the shape specified by `tf.get_variable()`.
  prefs: []
  type: TYPE_NORMAL
- en: In this example the variable `w` will be initialized as `[0,0,0]`, as specified
    by the given shape, `[1,3]`.
  prefs: []
  type: TYPE_NORMAL
- en: With `get_variable()` we can reuse variables that have the same name (including
    the scope prefix, which can be set by `tf.variable_scope()`). But first we need
    to indicate this intention by either using `tf.variable_scope.reuse_variable()`
    or setting the `reuse` flag (`tf.variable.scope(reuse=True)`). An example of how
    to share variables is shown in the code that follows.
  prefs: []
  type: TYPE_NORMAL
- en: Heads-up for flag misuse
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever a variable has the exact same name as another, an exception will be
    thrown when the `reuse` flag is not set. The same goes for the opposite scenario—variables
    with mismatching names that are expected to be reused (when `reuse = True`) will
    cause an exception as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using these methods, and setting the scope prefix to `Regression`, by printing
    their names we can see that the same variables are reused:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`tf.get_variables()` is a neat, lightweight way to share variables. Another
    approach is to encapsulate our model as a class and manage the variables there.
    This approach has many other benefits, as described in the following section'
  prefs: []
  type: TYPE_NORMAL
- en: Class encapsulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with any other program, when things get more complex and the number of code
    lines grows, it becomes very convenient to have our TensorFlow code reside within
    a class, giving us quick access to methods and attributes that belong to the same
    model. Class encapsulation allows us to maintain the state of our variables and
    then perform various post-training tasks like forming predictions, model evaluation,
    further training, saving and restoring our weights, and whatever else is related
    to the specific problem our model solves.
  prefs: []
  type: TYPE_NORMAL
- en: In the next batch of code we see an example of a simple class wrapper. The model
    is created when the instance is instantiated, and the training process is performed
    by calling the `fit()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '@property and Python decorators'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This code uses a `@property` decorator. A *decorator* is simply a function that
    takes another function as input, does something with it (like adding some functionality),
    and returns it. In Python, a decorator is defined with the `@` symbol.
  prefs: []
  type: TYPE_NORMAL
- en: '`@property` is a decorator used to handle access to class attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our class wrapper is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'And we get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Splitting the code into functions is somewhat redundant in the sense that the
    same lines of code are recomputed with every call. One simple solution is to add
    a condition at the beginning of each function. In the next code iteration we will
    see an even nicer workaround.
  prefs: []
  type: TYPE_NORMAL
- en: In this setting there is no need to use variable sharing since the variables
    are kept as attributes of the model object. Also, after calling the training method
    `model.fit()` twice, we see that the variables have maintained their current state.
  prefs: []
  type: TYPE_NORMAL
- en: In our last batch of code for this section we add another enhancement, creating
    a custom decorator that automatically checks whether the function was already
    called.
  prefs: []
  type: TYPE_NORMAL
- en: Another improvement we can make is having all of our variables kept in a dictionary. This
    will allow us to keep track of our variables after each operation, as we saw in
    [Chapter 10](ch10.html#exporting_and_serving_models_with_tensorflow) when we looked
    at saving weights and models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, additional functions for getting the values of the loss function and
    our weights are added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The custom decorator checks whether an attribute exists, and if not, it sets
    it according to the input function. Otherwise, it returns the attribute. `functools.wrap()`
    is used so we can reference the name of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This was a fairly basic example of how we can improve the overall code for our
    model. This kind of optimization might be overkill for our simple linear regression
    example, but it will definitely be worth the effort for complicated models with
    plenty of layers, variables, and features.
  prefs: []
  type: TYPE_NORMAL
- en: Customization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far we’ve used two loss functions. In the classification example in [Chapter 2](ch02.html#go_with_the_flow)
    we used the cross-entropy loss, defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In contrast, in the regression example in the previous section we used the
    square error loss, defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: These are the most commonly used loss functions in machine learning and deep
    learning right now. The purpose of this section is twofold. First, we want to
    point out the more general capabilities of TensorFlow in utilizing custom loss
    functions. Second, we will discuss regularization as a form of extension of any
    loss function in order to achieve a specific goal, irrespective of the basic loss
    function used.
  prefs: []
  type: TYPE_NORMAL
- en: Homemade loss functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This book (and presumably our readers) takes a specific view of TensorFlow with
    the aspect of deep learning in mind. However, TensorFlow is more general in scope,
    and most machine learning problems can be formulated in a way that TensorFlow
    can be used to solve. Furthermore, any computation that can be formulated in the
    computation graph framework is a good candidate to benefit from TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: The predominant special case is the class of unconstrained optimization problems.
    These are extremely common throughout scientific (and algorithmic) computing,
    and for these, TensorFlow is especially helpful. The reason these problems stand
    out is that TensorFlow provides an automatic mechanism for computing gradients,
    which affords a tremendous speedup in development time for such problems.
  prefs: []
  type: TYPE_NORMAL
- en: In general, optimization with respect to an arbitrary loss function will be
    in the form
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: where any optimizer could be used in place of the `GradientDescentOptimizer`.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regularizationis the restriction of an optimization problem by imposing a penalty
    on the complexity of the solution (see the note in [Chapter 4](ch04.html#convolutional_neural_networks)
    for more details). In this section we take a look at specific instances where
    the penalty is directly added to the basic loss function in an additive form.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, building on the softmax example from [Chapter 2](ch02.html#go_with_the_flow),
    we have this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The difference between this and the original in [Chapter 2](ch02.html#go_with_the_flow)
    is that we added `LAMBDA * tf.nn.l2_loss(W)` to the loss we are optimizing with
    respect to. In this case, using a small value of the trade-off parameter `LAMBDA`
    will have very little effect on the resulting accuracy (a large value will be
    detrimental). In large networks, where overfitting is a serious issue, this sort
    of regularization can often be a lifesaver.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization of this sort can be done with respect to the weights of the model,
    as shown in the previous example (also called *weight decay*, since it will cause
    the weights to have smaller values), as well as to the activations of a specific
    layer, or indeed all layers.
  prefs: []
  type: TYPE_NORMAL
- en: Another factor is what function we use—we could have used `l1` instead of the
    `l2` regularization, or a combination of the two. All combinations of these regularizers
    are valid and used in various contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the abstraction layers make the application of regularization as easy
    as specifying the number of filters, or the activation function. In Keras (a very
    popular extension reviewed in [Chapter 7](ch07.html#tensorflow_abstractions_and_simplifications)),
    for instance, we are provided with the regularizers listed in [Table A-1](#tableA1),
    applicable to all the standard layers.
  prefs: []
  type: TYPE_NORMAL
- en: Table A-1\. Regularization with Keras
  prefs: []
  type: TYPE_NORMAL
- en: '| Regularizer | What it does | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `l1` | `l1` regularization of weights |'
  prefs: []
  type: TYPE_TB
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `l2` | `l2` regularization of weights  |'
  prefs: []
  type: TYPE_TB
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `l1l2` | Combined `l1 + l2` regularization of weights |'
  prefs: []
  type: TYPE_TB
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `activity_l1` | `l1` regularization of activations |'
  prefs: []
  type: TYPE_TB
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `activity_l2` | `l2` regularization of activations |'
  prefs: []
  type: TYPE_TB
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `activity_l1l2` | Combined `l1 + l2` regularization of activations |'
  prefs: []
  type: TYPE_TB
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Using these shortcuts makes it easy to test different regularization schemes
    when a model is overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Writing your very own op
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TensorFlow comes ready packed with a large number of native ops, ranging from
    standard arithmetic and logical operations to matrix operations, deep learning–specific
    functions, and more. When these are not enough, it is possible to extend the system
    by creating a new op. This is done in one of two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Writing a “from scratch” C++ version of the operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing Python code that combines existing ops and Python code to create the
    new one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will spend the remainder of this section discussing the second option.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main reason to construct a Python op is to utilize NumPy functionality
    in the context of a TensorFlow computational graph. For the sake of illustration,
    we will construct the regularization example from the previous section by using
    the NumPy multiplication function rather than the TensorFlow op:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note that this is done for the sake of illustration, and there is no special
    reason why anybody would want to use this instead of the native TensorFlow op.
    We use this oversimplified example in order to shift the focus to the details
    of the mechanism rather than the computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use our new creation from within TensorFlow, we use the `py_func()`
    functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, this means we compute the total loss as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Doing this, however, will not be enough. Recall that TensorFlow keeps track
    of the gradients of each of the ops in order to perform gradient-based training
    of our overall model. In order for this to work with the new Python-based op,
    we have to specify the gradient manually. This is done in two steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create and register the gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, when using the function, we point to this function as the gradient of
    the op. This is done using the string registered in the previous step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting it all together, the code for the softmax model with regularization
    through our new Python-based op is now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This can now be trained using the same code as in [Chapter 2](ch02.html#go_with_the_flow),
    when this model was first introduced.
  prefs: []
  type: TYPE_NORMAL
- en: Using the inputs in the computation of gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the simple example we just showed, the gradient depends only on the gradient
    with respect to the input, and not on the input itself. In the general case, we
    will need access to the input as well. This is done easily, using the `op.input`s
    field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Other inputs (if they exist) are accessed in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: Required and Recommended Components for TensorFlow Serving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we add details on some of the material covered in [Chapter 10](ch10.html#exporting_and_serving_models_with_tensorflow)
    and review in more depth some of the technical components used behind the scenes
    in TensorFlow Serving.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 10](ch10.html#exporting_and_serving_models_with_tensorflow), we
    used Docker to run TensorFlow Serving. Those who prefer to avoid using a Docker
    container need to have the following installed:'
  prefs: []
  type: TYPE_NORMAL
- en: Bazel
  prefs: []
  type: TYPE_NORMAL
- en: 'Bazel is Google’s own build tool, which recently became publicly available.
    When we use the term *build*, we are referring to using a bunch of rules to create
    output software from source code in a very efficient and reliable manner. The
    build process can also be used to reference external dependencies that are required
    to build the outputs. Among other languages, Bazel can be used to build C++ applications,
    and we exploit this to build the C++-written TensorFlow Serving’s programs. The
    source code Bazel builds upon is organized in a workspace directory inside nested
    hierarchies of packages, where each package groups related source files together.
    Every package consists of three types of files: human-written source files called
    *targets*, *generated files* created from the source files, and *rules* specifying
    the steps for deriving the outputs from the inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: Each package has a *BUILD* file, specifying the output to be built from the
    files inside that package. We use basic Bazel commands like `bazel build` to build
    generated files from targets, and `bazel run` to execute a build rule. We use
    the `-bin` flag when we want to specify the directories to contain the build outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Downloads and installation instructions can be found on [the Bazel website](https://bazel.build/versions/master/docs/install.html).
  prefs: []
  type: TYPE_NORMAL
- en: gRPC
  prefs: []
  type: TYPE_NORMAL
- en: Remote procedure call (RPC) is a form of client (caller)–server (executer) interaction;
    a program can request a procedure (for example, a method) that is executed on
    another computer (commonly in a shared network). gRPC is an open source framework
    developed by Google. Like any other RPC framework, gRPC lets you directly call
    methods on other machines, making it easier to distribute the computations of
    an application. The greatness of gRPC lies in how it handles the serialization, using
    the fast and efficient protocol buffers instead of XML or other methods.
  prefs: []
  type: TYPE_NORMAL
- en: Downloads and installation instructions can be found [on GitHub](https://github.com/grpc/grpc/tree/master/src/python/grpcio).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you need to make sure that the necessary dependencies for Serving are
    installed with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'And lastly, clone Serving:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As illustrated in [Chapter 10](ch10.html#exporting_and_serving_models_with_tensorflow), another
    option is to use a Docker container, allowing a simple and clean installation.
  prefs: []
  type: TYPE_NORMAL
- en: What Is a Docker Container and Why Do We Use It?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker is essentially solving the same problem as Vagrant with VirtualBox, and
    that is making sure our code will run smoothly on other machines. Different machines
    might have different operating systems as well as different tool sets (installed
    software, configurations, permissions, etc.). By replicating the same environment—maybe
    for production purposes, maybe just to share with others—we guarantee that our
    code will run exactly the same way elsewhere as on our original development machine.
  prefs: []
  type: TYPE_NORMAL
- en: What’s unique about Docker is that, unlike other similarly purposed tools, it
    doesn’t create a fully operational virtual machine on which the environment will
    be built, but rather creates a *container* on top of an existing system (Ubuntu,
    for example), acting as a virtual machine in a sense and using our existing OS
    resources. These containers are created from a local Docker *image*, which is
    built from a *dockerfile* and encapsulates everything we need (dependency installations,
    project code, etc.). From that image we can create as many containers as we want
    (until we run out of memory, of course). This makes Docker a very cool tool with
    which we can easily create complete multiple environment replicas that contain
    our code and run them anywhere (very useful for cluster computing).
  prefs: []
  type: TYPE_NORMAL
- en: Some Basic Docker Commands
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get you a bit more comfortable with using Docker, here’s a quick look at
    some useful commands, written in their most simplified form. Given that we have
    a dockerfile ready, we can build an image by using `docker build <*dockerfile*>`.
    From that image we can then create a new container by using the `docker run <*image*>`
    command. This command will also automatically run the container and open a terminal
    (type `exit` to close the terminal). To run, stop, and delete existing containers,
    we use the `docker start <*container id*>`, `docker stop <*container id*>`, and
    `docker rm <*container id*>` commands, respectively.  To see the list of all of
    our instances, both running and idle, we write `docker ps -a`.
  prefs: []
  type: TYPE_NORMAL
- en: When we run an instance, we can add the `-p` flag followed by a port for Docker
    to expose, and the `-v` flag followed by a home directory to be mounted, which
    will enable us to work locally (the home directory is addressed via the `/mnt/home`
    path in the container).
  prefs: []
  type: TYPE_NORMAL
