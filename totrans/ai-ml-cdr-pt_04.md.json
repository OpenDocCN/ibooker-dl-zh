["```py\nnew_val = (–1 * 0) + (0 * 64) + (–2 * 128) + \n     (.5 * 48) + (4.5 * 192) + (–1.5 * 144) + \n     (1.5 * 142) + (2 * 226) + (–3 * 168)\n```", "```py\n# Define the model\nclass FashionMNISTModel(nn.Module):\n    def __init__(self):\n        super(FashionMNISTModel, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10),\n            nn.LogSoftmax(dim=1)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = FashionMNISTModel()\n\n# Define the loss function and optimizer\nloss_function = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters())\n\n# Train the model\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  \n                    [{current:>5d}/{size:>5d}]\")\n\n# Training process\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_loader, model, loss_function, optimizer)\nprint(\"Done!\")\n```", "```py\nnn.Conv2d(1, 64, kernel_size=3, padding=1)\n```", "```py\nnn.MaxPool2d(kernel_size=2, stride=2)\n```", "```py\n# Define the CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(2))  # Output: 64 x 6 x 6\n\n        self.fc1 = nn.Linear(64 * 6 * 6, 128)\n        self.fc2 = nn.Linear(128, 10)  # 10 classes\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(out.size(0), –1)  # Flatten the output\n        out = self.fc1(out)\n        out = self.fc2(out)\n        return out\n```", "```py\nTrain Epoch: 44 -- Loss: 0.091689\nTrain Epoch: 45 -- Loss: 0.066864\nTrain Epoch: 46 -- Loss: 0.061322\nTrain Epoch: 47 -- Loss: 0.056557\nTrain Epoch: 48 -- Loss: 0.039695\nTrain Epoch: 49 -- Loss: 0.056213\nAccuracy of the network on the 10000 test images: 91.31%\n```", "```py\nfrom torchsummary import summary\nmodel = FashionCNN().to(device) \nsummary(model, input_size=(1, 28, 28))  # (Channels, Height, Width)\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [–1, 64, 28, 28]             640\n              ReLU-2           [–1, 64, 28, 28]               0\n         MaxPool2d-3           [–1, 64, 14, 14]               0\n            Conv2d-4           [–1, 64, 12, 12]          36,928\n              ReLU-5           [–1, 64, 12, 12]               0\n         MaxPool2d-6             [–1, 64, 6, 6]               0\n            Linear-7                  [–1, 128]         295,040\n            Linear-8                   [–1, 10]           1,290\n================================================================\nTotal params: 333,898\nTrainable params: 333,898\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 1.02\nParams size (MB): 1.27\nEstimated Total Size (MB): 2.30\n```", "```py\nimport urllib.request\nimport zipfile\n\nurl = \"https://storage.googleapis.com/learning-datasets/\n                                            horse-or-human.zip\"\nfile_name = \"horse-or-human.zip\"\ntraining_dir = 'horse-or-human/training/'\nurllib.request.urlretrieve(url, file_name)\n\nzip_ref = zipfile.ZipFile(file_name, 'r')\nzip_ref.extractall(training_dir)\nzip_ref.close()\n```", "```py\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.Resize((150, 150)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Load the datasets\ntrain_dataset = datasets.ImageFolder(root=training_dir, \n                                     transform=transform)\nval_dataset = datasets.ImageFolder(root=validation_dir, \n                                   transform=transform)\n\n# Data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n```", "```py\nclass HorsesHumansCNN(nn.Module):\n    def __init__(self):\n        super(HorsesHumansCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(64 * 18 * 18, 512)\n        self.drop = nn.Dropout(0.25)\n        self.fc2 = nn.Linear(512, 1)  \n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = x.view(–1, 64 * 18 * 18)\n        x = F.relu(self.fc1(x))\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = torch.sigmoid(x)  # Use sigmoid to output probabilities\n        return x\n```", "```py\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [–1, 16, 150, 150]             448\n         MaxPool2d-2           [–1, 16, 75, 75]               0\n            Conv2d-3           [–1, 32, 75, 75]           4,640\n         MaxPool2d-4           [–1, 32, 37, 37]               0\n            Conv2d-5           [–1, 64, 37, 37]          18,496\n         MaxPool2d-6           [–1, 64, 18, 18]               0\n            Linear-7                  [–1, 512]      10,617,344\n           Dropout-8                  [–1, 512]               0\n            Linear-9                    [–1, 1]             513\n================================================================\nTotal params: 10,641,441\nTrainable params: 10,641,441\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.26\nForward/backward pass size (MB): 5.98\nParams size (MB): 40.59\nEstimated Total Size (MB): 46.83\n----------------------------------------------------------------\n```", "```py\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n```", "```py\ndef train_model(num_epochs):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device).float()  \n            optimizer.zero_grad()\n            outputs = model(images).view(–1)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n```", "```py\ndef train_model(num_epochs):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device).float()  \n            optimizer.zero_grad()\n            outputs = model(images).view(–1)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f'Epoch {epoch + 1}, Loss: {running_loss / \n                len(train_loader)}')\n\n    # Evaluate on training set\n        model.eval()\n        with torch.no_grad():\n            correct = 0\n            total = 0\n            for images, labels in train_loader:\n                images, labels = images.to(device), \n                                 labels.to(device).float()\n                outputs = model(images).view(–1)\n                predicted = outputs > 0.5  # Threshold predictions\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n            print(f'Test Set Accuracy: {100 * correct / total}%')\n\n        # Evaluate on validation set\n        model.eval()\n        with torch.no_grad():\n            correct = 0\n            total = 0\n            for images, labels in val_loader:\n                images, labels = images.to(device), \n                                 labels.to(device).float()\n                outputs = model(images).view(–1)\n                predicted = outputs > 0.5  # Threshold predictions\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n            print(f'Validation Set Accuracy: {100 * correct / total}%')    \n\ntrain_model(50)\n```", "```py\nEpoch 7, Loss: 0.0016404045829699512\nTraining Set Accuracy: 100.0%\nValidation Set Accuracy: 88.28125%\nEpoch 8, Loss: 0.0010613293736610378\nTraining Set Accuracy: 100.0%\nValidation Set Accuracy: 89.0625%\nEpoch 9, Loss: 0.0008372313717332979\nTraining Set Accuracy: 100.0%\nValidation Set Accuracy: 86.328125%\nEpoch 10, Loss: 0.0006578459407812646\nTraining Set Accuracy: 100.0%\nValidation Set Accuracy: 87.5%\n```", "```py\ndef load_image(image_path, transform):\n    # Load image\n    image = Image.open(image_path).convert('RGB')  # Convert to RGB\n    # Apply transformations\n    image = transform(image)\n    # Add batch dimension, as the model expects batches\n    image = image.unsqueeze(0)\n    return image\n```", "```py\nwith torch.no_grad():\n    output = model(image)\n```", "```py\nclass_name = \"Human\" if prediction.item() == 1 else \"Horse\"\n```", "```py\n# Define transformations\ntransform = transforms.Compose([\n    transforms.Resize((150, 150)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n```", "```py\n# Transforms for the training data\ntrain_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.RandomResizedCrop(150),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  \n])\n\n# Transforms for the validation data\nval_transforms = transforms.Compose([\n    transforms.Resize(150),\n    transforms.CenterCrop(150),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n```", "```py\ntransforms.RandomAffine(\n    degrees=0,  # No rotation\n    translate=(0.2, 0.2),  # Translate up to 20% vert and horizontally\n    scale=(0.8, 1.2),  # Zoom in or out by 20%\n    shear=20,  # Shear by up to 20 degrees\n),\n```", "```py\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torch.optim import RMSprop\n\n# Load the pretrained Inception V3 model\npre_trained_model = models.inception_v3(pretrained=True, aux_logits=True)\n```", "```py\ndef print_model_summary(model):\n    for name, module in model.named_modules():\n        print(f\"{name} : {module.__class__.__name__}\")\n\n# Example of how to use the function with your pretrained model\nprint_model_summary(pre_trained_model)\n```", "```py\n# Freeze all layers up to and including the 'Mixed_7c'\nfor name, parameter in pre_trained_model.named_parameters():\n    parameter.requires_grad = False\n    if 'Mixed_7c' in name:\n        break\n```", "```py\n# Modify the existing fully connected layer\nnum_ftrs = pre_trained_model.fc.in_features\npre_trained_model.fc = nn.Sequential(\n    nn.Linear(num_ftrs, 1024),  # New fully connected layer \n    nn.ReLU(),                # Activation layer\n    nn.Linear(1024, 2)         # Final layer for binary classification\n)\n```", "```py\n     def load_image(image_path, transform):\n    # Load image\n    image = Image.open(image_path).convert('RGB')  # Convert to RGB \n    # Apply transformations\n    image = transform(image)\n    # Add batch dimension, as the model expects batches\n    image = image.unsqueeze(0)\n    return image\n\n    # Prediction function\ndef predict(image_path, model, device, transform):\n    model.eval()\n    image = load_image(image_path, transform)\n    image = image.to(device)\n    with torch.no_grad():\n        output = model(image)\n        print(output)\n        prediction = torch.max(output, 1)\n        print(prediction)\n```", "```py\n!wget --no-check-certificate \\\n https://storage.googleapis.com/learning-datasets/rps.zip \\\n -O /tmp/rps.zip\nlocal_zip = '/tmp/rps.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('/tmp/')\nzip_ref.close()\ntraining_dir = \"/tmp/rps/\"\n\ntrain_dataset = ImageFolder(root=training_dir, transform=transform)\n```", "```py\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n```", "```py\n# Modify the existing fully connected layer\nnum_ftrs = pre_trained_model.fc.in_features\npre_trained_model.fc = nn.Sequential(\n    nn.Linear(num_ftrs, 1024),  # New fully connected layer \n    nn.ReLU(),                  # Activation layer\n    nn.Linear(1024, 3)         # Final layer for binary classification\n)\n```", "```py\n# Only optimize parameters that are set to be trainable\noptimizer = RMSprop(filter(lambda p: p.requires_grad, \n                    pre_trained_model.parameters()), lr=0.001)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Train the model\ntrain_model(pre_trained_model, criterion, optimizer, train_loader, num_epochs=3)\n```", "```py\ndef load_image(image_path, transform):\n    # Load image\n    image = Image.open(image_path).convert('RGB')  # Convert to RGB \n    # Apply transformations\n    image = transform(image)\n    # Add batch dimension, as the model expects batches\n    image = image.unsqueeze(0)\n    return image\n\n    # Prediction function\ndef predict(image_path, model, device, transform):\n    model.eval()\n    image = load_image(image_path, transform)\n    image = image.to(device)\n    with torch.no_grad():\n        output = model(image)\n        print(output)\n        prediction = torch.max(output, 1)\n        print(prediction)\n```", "```py\nnn.Dropout(0.5)\n```", "```py\nnum_ftrs = pre_trained_model.fc.in_features\npre_trained_model.fc = nn.Sequential(\n    nn.Linear(num_ftrs, 1024),  # New fully connected layer \n    nn.ReLU(),                # Activation layer\n    nn.Linear(1024, 3)         # Final layer for RPS\n)\n```", "```py\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Sequential(\n    nn.Dropout(0.5),  # Adding dropout before the final FC layer\n    nn.Linear(num_ftrs, 1024),  # Reduce dimensionality to 1024\n    nn.ReLU(),\n    nn.Dropout(0.5),  # Adding another dropout layer after ReLU activation\n    nn.Linear(1024, 3)  # Final layer for RPS\n)\n```"]