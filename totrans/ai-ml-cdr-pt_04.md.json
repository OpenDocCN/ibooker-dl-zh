["```py\nnew_val = (–1 * 0) + (0 * 64) + (–2 * 128) + \n     (.5 * 48) + (4.5 * 192) + (–1.5 * 144) + \n     (1.5 * 142) + (2 * 226) + (–3 * 168)\n```", "```py\n# Define the model\nclass FashionMNISTModel(nn.Module):\n    def __init__(self):\n        super(FashionMNISTModel, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10),\n            nn.LogSoftmax(dim=1)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = FashionMNISTModel()\n\n# Define the loss function and optimizer\nloss_function = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters())\n\n# Train the model\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  \n                    `[{``current``:``>``5``d``}``/``{``size``:``>``5``d``}]``\")` ```", "```py\n```", "```py`` ```", "```py nn.Conv2d(1, 64, kernel_size=3, padding=1) ```", "```py nn.MaxPool2d(kernel_size=2, stride=2) ```", "```py # Define the CNN model class FashionCNN(nn.Module):     def __init__(self):         super(FashionCNN, self).__init__()         self.layer1 = nn.Sequential(             nn.Conv2d(1, 64, kernel_size=3, padding=1),             nn.ReLU(),             nn.MaxPool2d(kernel_size=2, stride=2))           self.layer2 = nn.Sequential(             nn.Conv2d(64, 64, kernel_size=3),             nn.ReLU(),             nn.MaxPool2d(2))  # Output: 64 x 6 x 6           self.fc1 = nn.Linear(64 * 6 * 6, 128)         self.fc2 = nn.Linear(128, 10)  # 10 classes       def forward(self, x):         out = self.layer1(x)         out = self.layer2(out)         out = out.view(out.size(0), –1)  # Flatten the output         out = self.fc1(out)         out = self.fc2(out)         return out ```", "```py Train Epoch: 44 -- Loss: 0.091689 Train Epoch: 45 -- Loss: 0.066864 Train Epoch: 46 -- Loss: 0.061322 Train Epoch: 47 -- Loss: 0.056557 Train Epoch: 48 -- Loss: 0.039695 Train Epoch: 49 -- Loss: 0.056213 Accuracy of the network on the 10000 test images: 91.31% ```", "```py` ```", "```py```", "```py```", "``` from torchsummary import summary model = FashionCNN().to(device)  summary(model, input_size=(1, 28, 28))  # (Channels, Height, Width) ----------------------------------------------------------------         Layer (type)               Output Shape         Param # ================================================================             Conv2d-1           [–1, 64, 28, 28]             640               ReLU-2           [–1, 64, 28, 28]               0          MaxPool2d-3           [–1, 64, 14, 14]               0             Conv2d-4           [–1, 64, 12, 12]          36,928               ReLU-5           [–1, 64, 12, 12]               0          MaxPool2d-6             [–1, 64, 6, 6]               0             Linear-7                  [–1, 128]         295,040             Linear-8                   [–1, 10]           1,290 ================================================================ Total params: 333,898 Trainable params: 333,898 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 1.02 Params size (MB): 1.27 Estimated Total Size (MB): 2.30 ```", "``` import urllib.request import zipfile   url = \"https://storage.googleapis.com/learning-datasets/ `horse``-``or``-``human``.``zip``\"` ```", "``` ```", "````` ```py`![](assets/aiml_0308.png)  ###### Figure 3-8\\. Ensuring that images are in named subdirectories    This code simply downloads the ZIP of the training data and unzips it into a directory at *horse-or-human/training*. (We’ll deal with downloading the validation data shortly.) This is the parent directory that will contain subdirectories for the image types.    Now, to use the `DataLoader`, we simply use the following code:    ``` from torchvision import datasets, transforms from torch.utils.data import DataLoader   # Define transformations transform = transforms.Compose([     transforms.Resize((150, 150)),     transforms.ToTensor(),     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) ])   # Load the datasets train_dataset = datasets.ImageFolder(root=training_dir,                                       transform=transform) val_dataset = datasets.ImageFolder(root=validation_dir,                                     transform=transform)   # Data loaders train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False) ```py    First, we create an instance of a `transforms` object that we’ll call `transform`. This will determine the rules for how we modify the images. It resizes the image to 150 × 150 and then normalizes it into a tensor. Note that the raw images are actually 300 × 300, but to make training quicker for the purposes of learning, I’ve resized them to 150 × 150.    Then, we specify the `dataset` objects to be `datasets.ImageFolder` types and point them to the required directory, and that will generate images for the training process by flowing them from that directory while applying the transform. The directory for training is `training_dir`, and the directory for validation is `validation_dir`, as specified earlier.```` ```py``  `````", "```py`## CNN Architecture for “Horses or Humans”    There are several major differences between this dataset and the Fashion MNIST one, and you have to take them into account when designing an architecture for classifying the images. First, the images are much larger—150 × 150 pixels—so more layers may be needed. Second, the images are in full color, not grayscale, so each image will have three channels instead of one. Third, there are only two image types, so we can actually classify them with only *one* output neuron. To do this, we’ll drive the value of that neuron toward 0 for one of the labels and toward 1 for the other. The `sigmoid` function is ideal for this process of driving the value to one of these extremes. You can see this at the bottom of the `forward` function:    ```", "```py    There are a number of things to note here. First of all, take a look at the very first layer. We’re defining 16 filters, each of which has a `kernel_size` of 3, but the input shape is 3\\. Remember that this is because our input image is in color: there are three channels, instead of just one for the monochrome Fashion MNIST dataset we were using earlier.    At the other end, notice that there’s only one neuron in the output layer. This is because we’re using a binary classifier, and we can get a binary classification with just a single neuron if we activate it with a sigmoid function. The purpose of the sigmoid function is to drive one set of values toward 0 and the other toward 1, which is perfect for binary classification.    Next, notice how we stack several more convolutional layers. We do this because our image source is quite large and we want, over time, to have many smaller images, each with features highlighted. If we take a look at the results of a `summary`, we’ll see this in action:    ```", "```py    Note that by the time the data has gone through all the convolutional and pooling layers, it ends up as 18 × 18 items. The theory is that these will be activated feature maps that are relatively simple because they will contain just 324 pixels. We can then pass these feature maps to the dense neural network to match them to the appropriate labels.    This, of course, leads this network to have many more parameters than the previous network, so it will be slower to train. With this architecture, we’re going to learn over 10 million parameters.    ###### Tip    The code in this section, as well as in many other places in this book, may require you to import Python libraries. To find the correct imports, you can check out [the book’s repository](https://github.com/lmoroney/PyTorch-Book-FIles).    To train the network, we’ll have to compile it with a loss function and an optimizer. In this case, the loss function can be the `BCELoss,` where `BCE` stands for *binary cross entropy*. As the name suggests, because there are only two classes in this scenario, this is a loss function that is designed for it. For the `optimizer`, we can continue using the same `Adam` that we used earlier. Here’s the code:    ```", "```py    We then train in the usual way:    ```", "```py    One thing to note is that the labels are converted to `floats` because of the binary cross entropy, where the value of the final output node will be a float value.    Over just 15 epochs, this architecture gives us a very impressive 95%+ accuracy on the training set. Of course, this is just with the training data, and this performance isn’t an indication of the network’s potential performance on data that it hasn’t previously seen.    Next, we’ll look at adding the validation set and measuring its performance to give us a good indication of how this model might perform in real life.    ## Adding Validation to the “Horses or Humans” Dataset    To add validation, you’ll need a validation dataset that’s separate from the training one. In some cases, you’ll get a master dataset that you have to split yourself, but in the case of “Horses or Humans,” there’s a separate validation set that you can download. In the preceding code snippet, you’ve already downloaded the training and validation datasets, put them in directories, and set up data loaders for each of them. However, for training, you only used one of these datasets—the one that was set up to load the training data. So next, we’ll switch the model into evaluation mode and explore how well it did with the validation data.    ###### Note    You may be wondering why we’re talking about a validation dataset here, rather than a test dataset, and whether the two are the same thing. For simple models like the ones developed in the previous chapters, it’s often sufficient to split the dataset into two parts: one for training and one for testing. But for more complex models like the one we’re building here, you’ll want to create separate validation and test sets.    What’s the difference? *Training data* is the data that is used to teach the network how the data and labels fit together, while *validation data* is used to see how the network is doing with previously unseen data *while* you are training (i.e., it isn’t used to fit data to labels but to inspect how well the fitting is going). Also, *test data* is used after training to evaluate how the network does with data it has never previously seen. Some datasets come with a three-way split, and in other cases, you’ll want to separate the test set into two parts for validation and testing. Here, you’ll download some additional images for testing the model.    To download the validation set and unzip it into a different directory, you can use code that’s very similar to that used for the training images.    Then, to perform the validation, you simply update your `train_model` method to perform a validation at the end of each training loop (or epoch) and report on the results. For example, you can do this:    ```", "```py    I added code here to do *both* the training and the validation and report on accuracy. Note that this is really just for learning purposes, so you can compare. In a real-world scenario, checking the accuracy of training data is a waste of processing time!    After training for 10 epochs, you should see that your model is 99%+ accurate on the training set but only about 88% on the validation set:    ```", "```py    This is an indication that the model is overfitting, which is something we also saw in the previous chapter.It’s easy to be lulled into a false sense of security by the 100% accuracy, but the other figure is more representative of how your model will behave in the real world.    Still, the performance isn’t bad, considering how few images it was trained on and how diverse those images were. You’re beginning to hit a wall caused by lack of data, but there are some techniques that you can use to improve your model’s performance. We’ll explore them later in this chapter, but before that, let’s take a look at how to *use* this model.    ## Testing “Horses or Humans” Images    It’s all very well and good to be able to build a model, but of course, you want to try it out. A major frustration of mine when I was starting my AI journey was that I could find lots of code that showed me how to build models and charts of how those models were performing, but very rarely was there code to help me kick the tires of the model myself to try it out. I’ll try to help you avoid that problem in this book!    Testing the model is perhaps easiest using Colab. I’ve provided a “Horses or Humans” notebook on GitHub that you can open directly in [Colab](http://bit.ly/horsehuman).    Once you’ve trained the model, you’ll see a section called “Running the Model.” Before running it, you should find a few pictures of horses or humans online and download them to your computer. I recommend you go to [Pixabay.com](http://pixabay.com), which is a really good site to check out for royalty-free images. It’s also a good idea to get your test images together first, because the node can time out while you’re searching.    [Figure 3-9](#ch03_figure_9_1748570891060144) shows a few pictures of horses and humans that I downloaded from Pixabay to test the model.  ![](assets/aiml_0309.png)  ###### Figure 3-9\\. Test images    When they were uploaded, as you can see in [Figure 3-10](#ch03_figure_10_1748570891060159), the model correctly classified one image as a human and another as a horse—but despite the fact that the third image was obviously of a human, the model incorrectly classified it as a horse!    You can also upload multiple images simultaneously and have the model make predictions for all of them. You may also notice that it tends to overfit toward horses. If the human isn’t fully posed (i.e., if you can’t see their full body), the model can skew toward horses. That’s what happened in this case. The first human model is fully posed, and the image resembles many of the poses in the dataset, so the model was able to classify her correctly. On the other hand, the second human model is facing the camera, but only her upper half is in the image. There was no training data that looked like that, so the model couldn’t correctly identify her.  ![](assets/aiml_0310.png)  ###### Figure 3-10\\. Executing the model    Let’s now explore the code to see what it’s doing. Perhaps the most important part is this chunk:    ```", "```py    Here, we are loading the image from the path that Colab wrote it to. Note that we specify a `transform` to apply to the image. The images being uploaded can be any shape, but if we are going to feed them into the model, they *must* be the same size that the model was trained on. So, if we use the same `transform` that we defined when performing the training, we’ll know it’s in the same dimensions.    At the end is this strange command: `image = image.unsqueeze(0)`.    When you look back at how the model was trained, the DataLoader objects batched the images going into it. If you think of an image as a 2D array of pixels, then the batch is an array of 2D arrays, which of course is then a 3D array.    But when we’re using this code with one image at a time, there’s no batch, so to make this a 3D array (which is technically a batch with one item in it), we can just unsqueeze the image along axis 0 to simulate this.    With our image in the right format, it’s easy to do the classification:    ```", "```py    The model then returns an array containing the classifications for the batch. Because there’s only one classification in this case, it’s effectively an array containing an array. You can see this back in [Figure 3-10](#ch03_figure_10_1748570891060159), where for the first human model, the array looks like `tensor([[2.1368e-05]], device='cuda:0').`    So now, it’s simply a matter of inspecting the value of the first element in that array. If it’s greater than 0.5, we’re looking at a human:    ```", "```py    There are a few important points to consider here. First, even though the network was trained on synthetic, computer-generated imagery, it performs quite well at spotting horses and humans and differentiating them in real photographs. This is a potential boon in that you may not need thousands of photographs to train a model, and you can do it relatively cheaply with CGI.    But this dataset also demonstrates a fundamental issue you will face. Your training set cannot hope to represent *every* possible scenario your model might face in the wild, and thus, the model will always have some level of overspecialization toward the training set. We saw a clear and simple example of this earlier in this section, when the model mischaracterized the human in the center of [Figure 3-9](#ch03_figure_9_1748570891060144). The training set didn’t include a human in that pose, and thus, the model didn’t “learn” that a human could look like that. As a result, there was every chance it might see the figure as a horse, and in this case, it did.    What’s the solution? The obvious one is to add more training data, with humans in that particular pose and others that weren’t initially represented. That isn’t always possible, though. Fortunately, there’s a neat trick in PyTorch that you can use to virtually extend your dataset—it’s called *image augmentation*, and we’ll explore that next.```", "```py``  ```", "```py`# Image Augmentation    In the previous section, you built a horse-or-human classifier model that was trained on a relatively small dataset. As a result, you soon began to hit problems classifying some previously unseen images, such as the miscategorization of a woman as a horse because the training set didn’t include any images of people in that pose.    One way to deal with such problems is with *image augmentation*. The idea behind this technique is that as PyTorch is loading your data, it can create additional new data by amending what it has using a number of transforms. For example, take a look at [Figure 3-11](#fig-3-11). While there is nothing in the dataset that looks like the woman on the right, the image on the left is somewhat similar.  ![](assets/aiml_0311.png)  ###### Figure 3-11\\. Dataset similarities    So, if you could, for example, zoom into the image on the left as you are training, as shown in [Figure 3-12](#fig-3-12), you would increase the chances of the model being able to correctly classify the image on the right as a person.  ![](assets/aiml_0312.png)  ###### Figure 3-12\\. Zooming in on the training set data    In a similar way, you can broaden the training set with a variety of other transformations, including the following:    *   Rotation (turning the image)           *   Shifting horizontally (moving the pixels horizontally with wrapping)           *   Shifting vertically (moving the pixels vertically with wrapping)           *   Shearing (moving the pixels either horizontally or vertically but offsetting so that the image would look like parallelogram)           *   Zooming (magnifying a particular region)           *   Flipping (vertically or horizontally)              Because you’ve been using the `datasets.ImageFolder` and a `DataLoader` to load the images, you’ve seen the model do a transform already—when it normalized the images like this:    ```", "```py    Many other transforms are easily available within the torchvision.transforms library, so, for example, you could do something like this:    ```", "```py    Here, in addition to rescaling the image to normalize it, you’re doing the following:    *   Randomly flipping horizontally           *   Randomly rotating up to 20 degrees left or right           *   Randomly cropping a 150 × 150 window instead of resizing              In addition, the transforms.RandomAffine library gives you the facility to do all of these things, as well as adding stuff like scaling the image (zooming in or out), shearing the image, etc. Here’s an example:    ```", "```py    When you retrain with these parameters, one of the first things you’ll notice is that training takes longer because of all the image processing. Also, your model’s accuracy may not be as high as it was, because previously it was overfitting to a largely uniform set of data.    In my case, when I was training with these augmentations, my accuracy went down from 99% to 94% after 15 epochs, with validation much lower at 64%. This likely indicates overfitting in the model, but it warrants investigation by training with more epochs! One other thing to note is that random cropping might also be an issue—the CGI images generally center the subject, so random cropping will give partial subjects.    But what about the image from [Figure 3-9](#ch03_figure_9_1748570891060144) that the model misclassified earlier? This time, the model gets it right. Thanks to the image augmentations, the training set now has sufficient coverage for the model to understand that this particular image is a human too (see [Figure 3-13](#ch03_figure_11_1748570891060175)). This is just a single data point, and it may not be representative of the results for real data, but it’s a small step in the right direction.  ![](assets/aiml_0313.png)  ###### Figure 3-13\\. The woman is now correctly classified    As you can see, even with a relatively small dataset like “Horses or Humans,” you can start to build a pretty decent classifier. With larger datasets, you could take this further. Another way you can improve the model is by using features that the model has already learned elsewhere. Many researchers with massive resources (millions of images) and huge models that have been trained on thousands of classes have shared their models, and by using a concept called *transfer learning*, you can use the features those models learned and apply them to your data. We’ll explore that next!    # Transfer Learning    As we’ve already seen in this chapter, the use of convolutions to extract features can be a powerful tool for identifying the contents of an image. If we use this tool, we can then feed the resulting feature maps into the dense layers of a neural network to match them to the labels and give us a more accurate way of determining the contents of an image. Using this approach with a simple fast-to-train neural network and some image augmentation techniques, we built a model that was 80–90% accurate at distinguishing between a horse and a human when it was trained on a very small dataset.    However, we can improve our model even further by using a method called *transfer learning*. The idea behind it is simple: instead of having our model learn a set of filters from scratch for our dataset, why not have it use a set of filters that were learned on a much larger dataset, with many more features than we can “afford” to build from scratch? We can place these filters in our network and then train a model with our data using the pre-learned filters. For example, while our “Horses or Humans” dataset has only two classes, we can use an existing model that has been pretrained for one thousand classes—but at some point, we’ll have to throw away some of the preexisting network and add the layers that will let us have a classifier for two classes.    [Figure 3-14](#ch03_figure_12_1748570891060190) shows what a CNN architecture for a classification task like ours might look like. We have a series of convolutional layers that lead to a dense layer, which in turn leads to an output layer.  ![](assets/aiml_0314.png)  ###### Figure 3-14\\. A CNN architecture    We’ve seen that we can build a pretty good classifier using this architecture. But what if we could use transfer learning to take the pre-learned layers from another model, freeze or lock them so that they aren’t trainable, and then put them on top of our model, like in [Figure 3-15](#ch03_figure_13_1748570891060207)?  ![](assets/aiml_0315.png)  ###### Figure 3-15\\. Taking and locking layers from another architecture via transfer learning    When we consider that once they’ve been trained, all these layers are just a set of numbers indicating the filter values, weights, and biases along with a known architecture (the number of filters per layer, the size of the filter, etc.), the idea of reusing them is pretty straightforward.    Let’s look at how this would appear in code. There are several pretrained models already available from a variety of sources, so we’ll use version 3 of the popular Inception model from Google, which is trained on more than a million images from a database called ImageNet. Inception has dozens of layers, and it can classify images into one thousand categories.    The torchvision.models library contains a number of models, including Inception V3, so we can easily get access to the pretrained model:    ```", "```py    Now, we have a full Inception model that’s pretrained. If you want to inspect its architecture, you can do so with this code:    ```", "```py    Be warned—this model is huge! Still, you should take a look through it to see the layers and their names. I like to use the one called `Mixed7_c` because its output is nice and small—it consists of 8 × 8 images—but you should feel free to experiment with others.    Next, we’ll freeze the entire network from retraining and then set a variable to point to `mixed7`’s output as where we want to crop the network. We can do that with this code:    ```", "```py    You’ll notice that we’re printing the output shape of the last layer, and you’ll also see that we’re getting 8 × 8 images at this point. This indicates that by the time the images have been fed through to `Mixed_7c`, the output images from the filters are 8 × 8 in size, so they’re pretty easy to manage. Again, you don’t have to choose that specific layer; you’re welcome to experiment with others.    Now, let’s see how to modify the model for transfer learning. It’s pretty straightforward—if you go back to the output from the custom `print_model_summary` from a moment ago, you’ll see that the *last* layer in the model is called `fc`. As you might expect, *fc* stands for *fully connected*, which is effectively a Linear layer with our densely connected neurons.    So now, it becomes as simple as replacing that layer with a new layer called `fc`. We don’t need to *know* the input shape for it ahead of time—we can inspect its `in_features` property to find that. So now, to create a new layer of 1,024 neurons that outputs to another layer of two neurons and replace the `fc` from Inception, all we have to do is this:    ```", "```py    It’s as simple as creating a new set of Linear layers from the last output, because we’ll be feeding the results into a dense layer. So, we then add a Linear layer of 1,024 neurons and a dense layer with two neurons for our output. Also, you’ve probably noticed that in the previous model, we did it with one neuron and used sigmoid activation for the two classes—so you’re probably wondering why we’re going to two neurons in the output layer now. This was primarily a stylistic choice. Inception was designed for *n* neurons to output for *n* classes, and I wanted to keep that approach.    Training the model on this architecture over only three epochs gave us an accuracy of 99%+, with a validation accuracy of 95%+. Clearly, that’s a vast improvement. Also, remember that Inception learned a massive set of features that it could use to classify the many classes it was trained on. It turns out that that feature set is also incredibly useful for learning how to classify any other images—not least, those from “Horses or Humans.”    The results we got from this model are much better than those we got from our previous model, but you can continue to tweak and improve it. You can also explore how the model will work with a much larger dataset, like the famous “[Dogs vs. Cats”](https://oreil.ly/UhWMk) from Kaggle. It’s an extremely varied dataset consisting of 25,000 images of cats and dogs, often with the subjects somewhat obscured—for example, if they are held by a human.    Using the same algorithm and model design as before, you can train a “Dogs vs. Cats” classifier on Colab, using a GPU at about 3 minutes per epoch.    When I tested with very complex pictures like those in [Figure 3-16](#ch03_figure_14_1748570891060222), this classifier got them all correct. I chose one picture of a dog with catlike ears and one with its back turned. Both pictures of cats were nontypical.  ![](assets/aiml_0316.png)  ###### Figure 3-16\\. Unusual dogs and cats that the model classified correctly    To parse the results, you can use code like this:    ```", "```py    Note the lines where I’m printing the output of the image, calculating the prediction from that, and printing that.    When you upload some images to Colab, you can see how they predict in [Figure 3-17](#ch03_figure_15_1748570891060236).  ![](assets/aiml_0317.png)  ###### Figure 3-17\\. Classifying the cat washing its paw    The first image uploaded was “labrador,” which, as its name suggests, is of a dog. The tensor returned from the model contained [–14.9642, 18.3943], meaning a very low number for the first label and a very high one for the second. Given that we used an image directory when training, the labels ended up being in alphabetical order, so it was low for cat and high for dog. Then, when we called `torch.max`, it gave us [1]. That indicates that neuron 1 is the one for this classification—thus, the image is a dog.    The second image had [5.3486, –4.8260], with the first neuron being higher. Thus, it detected a cat. The size of these numbers indicates the strength of the prediction. For example, it was much surer that the first image is a dog than it was that the second image is a cat.    You can find the complete code for the “Horses or Humans” and “Dogs vs. Cats” classifiers in the book’s [GitHub repository](https://github.com/lmoroney/tfbook).    # Multiclass Classification    In all of the examples so far, you’ve been building *binary* classifiers—ones that choose between two options (horses or humans, cats or dogs). On the other hand, when you’re building *multiclass classifiers*, the models are almost the same but there are a few important differences. Instead of a single neuron that is sigmoid activated or two neurons that are binary activated, your output layer will now require *n* neurons, where *n* is the number of classes you want to classify. You’ll also have to change your loss function to an appropriate one for multiple categories.    A neat feature of the `nn.CrossEntropyLoss` loss function in PyTorch is that it can handle multiple categories, so the “Cats vs. Dogs” and “Horses or Humans” transfer learning classifiers you’ve built thus far in this chapter can use it without modification. But the “Horses or Humans” classifier that you built at the beginning with a *single* output neuron will not be able to because it can’t handle more than two classes. This is always something to look out for, and it’s a common bug when you start writing code for classification.    To go beyond two-class classification, consider, for example, the game Rock, Paper, Scissors. If you wanted to train a dataset to recognize the different hand gestures used in this game, you’d need to handle three categories. Fortunately, there’s a [simple dataset](https://oreil.ly/VHhmS) you can use for this.    There are two downloads: a training set of many diverse hands, with different sizes, shapes, colors, and details such as nail polish; and a testing set of equally diverse hands, none of which are in the training set. You can see some examples in [Figure 3-18](#ch03_figure_16_1748570891060251).  ![Examples of Rock/Paper/Scissors gestures](assets/aiml_0318.png)  ###### Figure 3-18\\. Examples of Rock, Paper, Scissors gestures    Using the dataset is simple. You can download and unzip it—the sorted subdirectories are already present in the ZIP file—and then use it to initialize an `ImageFolder`:    ```", "```py    Be sure to use a `transform` that fits the input shape of your model. In the last few examples, we were using Inception, and it’s 299 × 299.    You can use the ImageFolder for your DataLoader in the usual way:    ```", "```py    Earlier, when we tweaked the Inception model for “Horses or Humans” or “Cats vs. Dogs,” there were only *two* classes and thus *two* output neurons. Given that this data has *three* classes, we need to be sure that we change the new fully connected layer at the bottom accordingly:    ```", "```py    Now, training the model works as before: you specify the loss function and optimizer, and you call the `train_model()` function. For good repetition, this function is the same as the one used in the “Horses or Humans” and “Cats vs. Dogs” examples:    ```", "```py    Your code for testing predictions will also need to change somewhat. There are now three output neurons, and they will output a high value for the predicted class and lower values for the other classes.    Note also that when you’re using the `ImageFolder`, the classes are loaded in alphabetical order—so while you might expect the output neurons to be in the order of the name of the game, the order will in fact be Paper, Rock, Scissors.    Code that you can use to try out predictions in a Colab notebook will look like the following. It’s very similar to what you saw earlier:    ```", "```py    Note that it doesn’t parse the output; it just prints the classes. [Figure 3-19](#ch03_figure_17_1748570891060264) shows what it looks like in actual use.  ![](assets/aiml_0319.png)  ###### Figure 3-19\\. Testing the Rock, Paper, Scissors classifier    You can see from the filenames what the images were.    If you explore this a little deeper, you can see that the file named *scissors4.png* had an output of –2.5582, –1.7362, 3.8465]. The largest number is the third one, and if you think alphabetically, you can see that the third neuron represents scissors, so it was classified correctly. Similar results were achieved for the other files.    Some images that you can use to test the dataset [are available to download](https://oreil.ly/dEUpx). Alternatively, of course, you can try your own. Note that the training images are all done against a plain white background, though, so there may be some confusion if there is a lot of detail in the background of the photos you take.    # Dropout Regularization    Earlier in this chapter, we discussed overfitting, in which a network may become too specialized in a particular type of input data and thus fare poorly on others. One technique to help overcome this is use of *dropout regularization*.    When a neural network is being trained, each individual neuron will have an effect on neurons in subsequent layers. Over time, particularly in larger networks, some neurons can become overspecialized—and that feeds downstream, potentially causing the network as a whole to become overspecialized and thus leading to overfitting. Additionally, neighboring neurons can end up with similar weights and biases, and if not monitored, this condition can lead the overall model to become overspecialized on the features activated by those neurons.    For example, consider the neural network in [Figure 3-20](#ch03_figure_18_1748570891060278), in which there are layers of 2, 6, 6, and 2 neurons. The neurons in the middle layers might end up with very similar weights and biases.  ![](assets/aiml_0320.png)  ###### Figure 3-20\\. A simple neural network    While training, if you remove a random number of neurons and ignore them, then their contribution to the neurons in the next layer is temporarily blocked (see [Figure 3-21](#ch03_figure_19_1748570891060292)). They are effectively dropped out, leading to the term *dropout regularization*.  ![](assets/aiml_0321.png)  ###### Figure 3-21\\. A neural network with dropouts    This reduces the chances of the neurons becoming overspecialized. The network will still learn the same number of parameters, but it should be better at generalization—that is, it should be more resilient to different inputs.    ###### Note    The concept of dropouts was proposed by Nitish Srivastava et al. in their 2014 paper “[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://oreil.ly/673CJ).”    To implement dropouts in PyTorch, you can just use a simple layer like this:    ```", "```py    This will drop out, at random, the specified percentage of neurons (here, 50%) in the specified layer. Note that it may take some experimentation to find the correct percentage for your network.    For a simple example that demonstrates this, consider the new fully connected layers we added to the bottom of Inception with the transfer learning example in this chapter.    Here it is for Rock, Paper, Scissors with three output neurons:    ```", "```py    With dropout added, it would look like this:    ```", "```py    The examples that we used in this chapter for transfer learning are already learning really well without the use of dropouts. However, I’d recommend that you always consider dropouts when building your models because they can greatly reduce waste in the ML process—letting your model learn just as well but much faster!    Additionally, as you design your neural networks, keep in mind that getting great results on your training set is not always a good thing because it could be a sign of overfitting. Introducing dropouts can help you remove that problem so that you can optimize your network in other areas without that false sense of security.    # Summary    This chapter introduced you to a more advanced way of achieving computer vision by using convolutional neural networks. You saw how to use convolutions to apply filters that can extract features from images, and you designed your first neural networks to deal with more complex vision scenarios than those you encountered with the MNIST and Fashion MNIST datasets. You also explored techniques to improve your network’s accuracy and avoid overfitting, such as the use of image augmentation and dropouts.    Before we explore further scenarios, in [Chapter 4](ch04.html#ch04_using_data_with_pytorch_1748548966496246), you’ll get an introduction to PyTorch data, which is a technology that makes it much easier for you to get access to data for training and testing your networks. In this chapter, you downloaded ZIP files and extracted images, but that’s not always going to be possible. With PyTorch datasets, you’ll be able to access lots of datasets with a standard API.```", "```py`` ```", "```py ```"]