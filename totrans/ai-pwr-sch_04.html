<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">4</span> </span> <span class="chapter-title-text">Crowdsourced relevance</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Harnessing your users’ collective insights to improve the relevance of your search platform</li>
<li class="readable-text" id="p3">Collecting and working with user behavioral signals</li>
<li class="readable-text" id="p4">Using reflected intelligence to create self-tuning models</li>
<li class="readable-text" id="p5">Building an end-to-end signals boosting model</li>
</ul>
</div>
<div class="readable-text" id="p6">
<p>In chapter 1, we introduced the dimensions of user intent as content understanding, user understanding, and domain understanding. To create an optimal AI-powered search platform, we need to be able to combine each of these contexts to understand our users’ query intent. The question, though, is how do we derive these understandings?</p>
</div>
<div class="readable-text intended-text" id="p7">
<p>We can learn from many sources of information: documents, databases, internal knowledge graphs, user behavior, domain experts, and so on. Some organizations have teams that manually tag documents with topics or categories, and some even outsource these tasks using tools like Amazon Mechanical Turk, which allows them to crowdsource answers from people all around the world. For identifying malicious behavior or errors on websites, companies often allow their users to report problems and even suggest corrections. All of these are examples of crowdsourcing—relying upon input from many people to learn new information.</p>
</div>
<div class="readable-text intended-text" id="p8">
<p>When it comes to search relevance, crowdsourcing can play a vital role, though it is usually important not to annoy your valued customers by constantly asking them for help. Fortunately, it is often possible to learn implicitly from your users, based on their behaviors. For example, to discover the most relevant documents for a query, we can examine logs to determine the documents most clicked on by other users when running that same search. Those clicks provide signals of which results are the most relevant for the query.</p>
</div>
<div class="readable-text intended-text" id="p9">
<p>In this chapter, we’ll explore how we can collect, analyze, and generate insights from these signals to crowdsource relevance. We’ll also cover the reflected intelligence process, introducing three key types of models for popularized relevance (signals boosting), for personalized relevance (collaborative filtering), and for generalized relevance (learning to rank). You’ll also index an e-commerce dataset and build your very first reflected intelligence model.</p>
</div>
<div class="readable-text" id="p10">
<h2 class="readable-text-h2" id="sigil_toc_id_53"><span class="num-string">4.1</span> Working with user signals</h2>
</div>
<div class="readable-text" id="p11">
<p>Every time a customer takes an action—such as issuing a query or purchasing a product—this provides a signal of that user’s intent. We can log and process these signals to learn insights about each user, different groups of users, or our entire user base. </p>
</div>
<div class="readable-text intended-text" id="p12">
<p>This section introduces the power of using user signals with a sample e-commerce dataset we’ll use throughout the book, and it walks you through the mechanics of collecting, storing, and processing these signals.</p>
</div>
<div class="readable-text" id="p13">
<h3 class="readable-text-h3" id="sigil_toc_id_54"><span class="num-string">4.1.1</span> Content vs. signals vs. models</h3>
</div>
<div class="readable-text" id="p14">
<p>When building search engines, two high-level sources of data affect search relevance: content and signals. Most content is in the form of documents, which can represent web pages, product listings, computer files, images, videos, facts, or any other type of searchable information. Content documents usually contain text or embedding fields that are used to search, along with other fields representing attributes related to the content (author, size, color, dates, and so on). The defining characteristic of the content documents is that they contain information users search on and ideally the answers to their queries. </p>
</div>
<div class="readable-text intended-text" id="p15">
<p>When a user sees content in response to a query, they may click on a result, add it to their shopping cart, or take some other actions. These actions are signals, and they’re key to providing insights into how users engage with the content. These signals can later be aggregated and used to build models to improve the relevance of your matching and ranking algorithms. The defining characteristic of signals is that they are user-supplied insights for demonstrating how users want to interact with content.</p>
</div>
<div class="readable-text intended-text" id="p16">
<p>Sometimes it can also be useful to rely on external data sources—or <em>models</em>—as part of the search experience. This can include querying a knowledge graph, referencing a list of entities, or invoking a large language model (LLM) or other foundation model that has been trained on external data sources. These external models can be used to better interpret user queries, reason about and understand the content, or even summarize or generate new content to return. While we can consider models as a third source of data for our search engine, they are trained on content and/or signals and thus serve as a derivative and refined representation of those two original data sources. </p>
</div>
<div class="readable-text intended-text" id="p17">
<p>In summary, we use three main sources of information to improve search: the attributes of the items (content), the observed user interactions with content (signals), and external models (which are derived from content and/or signals).</p>
</div>
<div class="readable-text intended-text" id="p18">
<p>For many tasks we undertake when building AI-powered search, we can derive similar outcomes using either content or signals, but they give us two different perspectives of relevance. In ideal cases, we can apply both perspectives to build an even smarter system, but it is useful to understand their strengths and weaknesses to best employ them.</p>
</div>
<div class="readable-text intended-text" id="p19">
<p>For example, when trying to find a synonym for the word “driver”, we can look through the text content for words that commonly appear in the same documents. We may, in this case, find words (in priority order by the percentage of documents they appear within) like “taxi” (40%), “car” (35%), “golf” (15%), “club” (12%), “printer” (3%), “linux” (3%), and “windows” (1%). Similarly, we can look at the signals from users who searched for “driver” and aggregate common keywords from their other searches in priority order like “screwdriver” (50%), “printer” (30%), “windows” (25%), “mac” (15%), “golf” (2%), and “club” (2%). The lists derived from signals versus content might be similar, or they could look very different. The content-based approach tells us the most-represented meanings within our documents, whereas the signals-based approach tells us the most-represented meanings being looked for by our users.</p>
</div>
<div class="readable-text intended-text" id="p20">
<p>Since our end goal is to present users with what they are looking for, it’s often more effective to rely on the signals-derived meanings than the content-derived meanings. But what if we don’t have good content that maps to the signals-derived meaning? Do we use the content-derived meaning, or do we try to suggest other related searches based on the signals data? What if we don’t have enough signals or if the signals data is not very clean? Can we somehow clean up the signals-derived data using the content-derived data?</p>
</div>
<div class="readable-text intended-text" id="p21">
<p>We run into similar issues with recommendations. Content-based recommendations use attributes in documents but don’t understand users, whereas signals-based recommendations don’t understand content attributes and won’t work without sufficient interactions. Content-based recommendations may be based on features that are unimportant to users, whereas signals-based recommendations can create self-reinforcing loops where users only interact with items they are recommended, but those items are only recommended because users are interacting with them.</p>
</div>
<div class="readable-text intended-text" id="p22">
<p>Ideally, we want to create a balanced system that can use the best of both content-derived and signals-derived intelligence. While this chapter focuses primarily on signals-derived, crowdsourced intelligence, a major goal of this book is to show you how to balance and combine both approaches to yield a more optimal AI-powered search experience. </p>
</div>
<div class="readable-text" id="p23">
<h3 class="readable-text-h3" id="sigil_toc_id_55"><span class="num-string">4.1.2</span> Setting up our product and signals datasets (RetroTech)</h3>
</div>
<div class="readable-text" id="p24">
<p>We’ll use various datasets throughout this book as we explore different use cases, but it is also valuable to have a consistent example that we can build on as we progress. We’ll benefit by having a robust search use case with lots of data and user interactions, and we’ll set that up in the section. </p>
</div>
<div class="readable-text intended-text" id="p25">
<p>It’s worth noting that most techniques in this book apply across almost all search cases. The deciding factor for when to use a particular technique typically depends more on the volume and variety of content and signals than on the specific use case.</p>
</div>
<div class="readable-text intended-text" id="p26">
<p>E-commerce search provides one of the most concrete use cases for the value of AI-powered search techniques, and it is also one of the most well-understood problems among likely readers, so we’ve created an e-commerce dataset to help us explore this domain: the RetroTech dataset.</p>
</div>
<div class="readable-text" id="p27">
<h4 class="readable-text-h4 sigil_not_in_toc">The RetroTech use case</h4>
</div>
<div class="readable-text" id="p28">
<p>With aggressive competition among retailers selling cutting-edge electronics, multimedia, and tech products, it is hard for a small online business to compete. However, a niche but emerging segment of the population chooses to avoid the latest and greatest products and instead falls back to the familiar technology of decades past. The RetroTech company was launched to meet the needs of this unique group of consumers, offering vintage hardware, software, and multimedia products that may be hard to find on today’s shelves. </p>
</div>
<div class="readable-text intended-text" id="p29">
<p>Let’s load the dataset for the RetroTech company so we can get started learning about the relationships between documents and user signals, and about how crowdsourced intelligence can improve our search relevance. </p>
</div>
<div class="readable-text" id="p30">
<h4 class="readable-text-h4 sigil_not_in_toc">Loading the product catalog</h4>
</div>
<div class="readable-text" id="p31">
<p>The RetroTech website has around 50,000 products available for sale, which we need to load into our search engine. If you built the AI-Powered Search codebase to run the chapter 3 examples, then your search engine is already up and running. Otherwise, the instructions for building and running all the book’s examples can be found in appendix A. </p>
</div>
<div class="readable-text intended-text" id="p32">
<p>With your search engine up, the next thing you need to do is download the RetroTech dataset that accompanies this book. The dataset includes two CSV files, one containing all of RetroTech’s products, and another containing one year of signals data from RetroTech’s users. The following listing shows a few rows of the product catalog dataset to get you familiar with the format.</p>
</div>
<div class="browsable-container listing-container" id="p33">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.1</span> Exploring the RetroTech product catalog</h5>
<div class="code-area-container">
<pre class="code-area">"upc","name","manufacturer","short_description","long_description"
"096009010836","Fists of Bruce Lee - Dolby - DVD", , ,
"043396061965","The Professional - Widescreen Uncut - DVD", , ,
"085391862024","Pokemon the Movie: 2000 - DVD", , ,
"067003016025","Summerbreeze - CD","Nettwerk", ,
"731454813822","Back for the First Time [PA] - CD","Def Jam South", ,
"024543008200","Big Momma's House - Widescreen - DVD", , ,
"031398751823","Kids - DVD", , ,
"037628413929","20 Grandes Exitos - CD","Sony Discos Inc.", ,
"060768972223","Power Of Trinity (Box) - CD","Sanctuary Records", ,</pre>
</div>
</div>
<div class="readable-text" id="p34">
<p>You can see that the products are identified by a UPC (Universal Product Code) and also have a name, a manufacturer, and both a short description (used as a teaser in search results) and a long description (the full description used on the product details pages). </p>
</div>
<div class="readable-text intended-text" id="p35">
<p>Since we’re trying to search for products, our next step is to send them to the search engine to be indexed. To enable search on our RetroTech product catalog, let’s run the document indexing code in the following listing to send the product documents to the search engine.</p>
</div>
<div class="browsable-container listing-container" id="p36">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.2</span> Send product documents to the search engine</h5>
<div class="code-area-container">
<pre class="code-area">products_collection = engine.create_collection("products")
products_dataframe = load_dataframe("data/retrotech/products.csv")
products_collection.write(products_dataframe)</pre>
</div>
</div>
<div class="readable-text" id="p37">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p38">
<div class="code-area-container">
<pre class="code-area">Wiping "products" collection
Creating "products" collection
Status: Success
Loading Products
Schema:
root
 |-- upc: long (nullable = true)
 |-- name: string (nullable = true)
 |-- manufacturer: string (nullable = true)
 |-- short_description: string (nullable = true)
 |-- long_description: string (nullable = true)

Successfully written 48194 documents</pre>
</div>
</div>
<div class="readable-text" id="p39">
<p>Finally, to verify that the documents are now indexed and searchable, let’s run an example keyword search. The following listing shows an example search for <code>ipod</code>, a true classic device!</p>
</div>
<div class="browsable-container listing-container" id="p40">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.3</span> Running a search on the product catalog</h5>
<div class="code-area-container">
<pre class="code-area">def product_search_request(query, param_overrides={}):
  request = {"query": query,
             "query_fields": ["name", "manufacturer", "long_description"],
             "return_fields": ["upc", "name", "manufacturer",
                               "short_description", "score"],
             "limit": 5,
             "order_by": [("score", "desc"), ("upc", "asc")]}
  return request | param_overrides

query = "ipod"
request = product_search_request(query)
response = products_collection.search(**request)
display_product_search(query, response["docs"])</pre>
</div>
</div>
<div class="readable-text" id="p41">
<p>The results of the preceding <code>ipod</code> search are shown in figure 4.1, demonstrating that our products are now indexed and searchable. Unfortunately, the relevance of the results is quite poor.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p42">
<img alt="figure" src="../Images/CH04_F01_Grainger.png" width="60%"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.1</span> Product search results. We can see that the product catalog has been indexed and a query for <code>ipod</code> now returns search results.</h5>
</div>
<div class="readable-text" id="p43">
<p>While the quality of the search results ranking is not yet very good, we have an out-of-the-box “keyword matching” search engine that we can begin improving. We’ll use this as our base and start introducing more intelligent AI-powered search features throughout the rest of the book. Our next step is to introduce our signals data. </p>
</div>
<div class="readable-text" id="p44">
<h4 class="readable-text-h4 sigil_not_in_toc">Loading the signals data</h4>
</div>
<div class="readable-text" id="p45">
<p>Because RetroTech is running on your computer, no live users are searching, clicking, or otherwise generating signals. Instead, we’ve generated a dataset that approximates the kind of signal activity you’d expect in similar real-world datasets. </p>
</div>
<div class="readable-text intended-text" id="p46">
<p>For simplicity, we’ll store our signals in the search engine so they can be accessed both in real-time search scenarios and for external processing. Running the following listing will simulate and index some sample signals that we can use throughout the rest of the chapter.</p>
</div>
<div class="browsable-container listing-container" id="p47">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.4</span> Indexing the user signals dataset</h5>
<div class="code-area-container">
<pre class="code-area">signals_collection = engine.create_collection("signals")
signals_collection.write(from_csv("data/retrotech/signals.csv"))</pre>
</div>
</div>
<div class="readable-text" id="p48">
<p>With our RetroTech product and signals data all loaded, we’ll soon begin exploring ways we can use the signals data to enhance search relevance. Let’s first familiarize ourselves with the signals data so we can understand how signals are structured, used, and collected in real-world systems. </p>
</div>
<div class="readable-text" id="p49">
<h3 class="readable-text-h3" id="sigil_toc_id_56"><span class="num-string">4.1.3</span> Exploring the signals data</h3>
</div>
<div class="readable-text" id="p50">
<p>Different types of signals have different attributes that need to be recorded. For a “query” signal, we want to record the user’s keywords. For a “click” signal, we want to record which document was clicked upon, as well as which query resulted in the click. For later analysis, we’d also want to record which documents were returned to and possibly viewed by a user after a query. </p>
</div>
<div class="readable-text intended-text" id="p51">
<p>To make the examples more extensible and avoid custom code for every new signal type, we’ve adopted a generic format for representing signals in this book. This format may differ from how you currently log signals, but as long as you can ultimately map your signals into this format, all the code in this book should work without requiring use-case-specific modifications.</p>
</div>
<div class="readable-text intended-text" id="p52">
<p>The signals format we use in this book is as follows:</p>
</div>
<ul>
<li class="readable-text" id="p53"> <code>query_id</code>—A unique ID for the query signal that originated this signal </li>
<li class="readable-text" id="p54"> <code>user</code>—An identifier representing a specific user of the search engine </li>
<li class="readable-text" id="p55"> <code>type</code>—What kind of signal (“query”, “click”, “purchase”, and so on) </li>
<li class="readable-text" id="p56"> <code>target</code>—The content to which the signal at this <code>signal_time</code> applies </li>
<li class="readable-text" id="p57"> <code>signal_time</code>—The date and time the signal occurred </li>
</ul>
<div class="readable-text" id="p58">
<p>As an example, assume a user performed the following sequence of actions:</p>
</div>
<ol>
<li class="readable-text" id="p59"> Issued a query for <code>ipad</code> and had three documents (doc1, doc2, and doc3) returned. </li>
<li class="readable-text" id="p60"> Clicked on doc1. </li>
<li class="readable-text" id="p61"> Went back and clicked on doc3. </li>
<li class="readable-text" id="p62"> Added doc3 to the shopping cart. </li>
<li class="readable-text" id="p63"> Went back and searched for <code>ipad</code> <code>cover</code> and had two documents returned (doc4 and doc5). </li>
<li class="readable-text" id="p64"> Clicked on doc4. </li>
<li class="readable-text" id="p65"> Added doc4 to the shopping cart. </li>
<li class="readable-text" id="p66"> Purchased the items in the shopping cart (doc3 and doc4). </li>
</ol>
<div class="readable-text" id="p67">
<p>These interactions would result in the signals shown in Table 4.1.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p68">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 4.1</span> Example signals format</h5>
<table>
<thead>
<tr>
<th>
<div>
         query_id 
       </div></th>
<th>
<div>
         user 
       </div></th>
<th>
<div>
         type 
       </div></th>
<th>
<div>
         target 
       </div></th>
<th>
<div>
         signal_time 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  1 <br/></td>
<td>  u123 <br/></td>
<td>  query <br/></td>
<td>  ipad <br/></td>
<td>  2024-05-01-09:00:00 <br/></td>
</tr>
<tr>
<td>  1 <br/></td>
<td>  u123 <br/></td>
<td>  results <br/></td>
<td>  doc1,doc2,doc3 <br/></td>
<td>  2024-05-01-09:00:00 <br/></td>
</tr>
<tr>
<td>  1 <br/></td>
<td>  u123 <br/></td>
<td>  click <br/></td>
<td>  doc1 <br/></td>
<td>  2024-05-01-09:00:10 <br/></td>
</tr>
<tr>
<td>  1 <br/></td>
<td>  u123 <br/></td>
<td>  click <br/></td>
<td>  doc3 <br/></td>
<td>  2024-05-01-09:00:29 <br/></td>
</tr>
<tr>
<td>  1 <br/></td>
<td>  u123 <br/></td>
<td>  add-to-cart <br/></td>
<td>  doc3 <br/></td>
<td>  2024-05-01-09:03:40 <br/></td>
</tr>
<tr>
<td>  2 <br/></td>
<td>  u123 <br/></td>
<td>  query <br/></td>
<td>  ipad cover <br/></td>
<td>  2024-05-01-09:04:00 <br/></td>
</tr>
<tr>
<td>  2 <br/></td>
<td>  u123 <br/></td>
<td>  results <br/></td>
<td>  doc4,doc5 <br/></td>
<td>  2024-05-01-09:04:00 <br/></td>
</tr>
<tr>
<td>  2 <br/></td>
<td>  u123 <br/></td>
<td>  click <br/></td>
<td>  doc4 <br/></td>
<td>  2024-05-01-09:04:40 <br/></td>
</tr>
<tr>
<td>  2 <br/></td>
<td>  u123 <br/></td>
<td>  add-to-cart <br/></td>
<td>  doc4 <br/></td>
<td>  2024-05-01-09:05:50 <br/></td>
</tr>
<tr>
<td>  1 <br/></td>
<td>  u123 <br/></td>
<td>  purchase <br/></td>
<td>  doc3 <br/></td>
<td>  2024-05-01-09:07:15 <br/></td>
</tr>
<tr>
<td>  2 <br/></td>
<td>  u123 <br/></td>
<td>  purchase <br/></td>
<td>  doc4 <br/></td>
<td>  2024-05-01-09:07:15 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p69">
<p>There are a few things to note about the format of the signals:</p>
</div>
<ul>
<li class="readable-text" id="p70"> <em>The “query” type and the “results” type are broken into separate signals.</em> This isn’t necessary, as they occur at the same time, but this allows us to keep the table structure consistent and not have to add an extra results column that would only apply to the query signal. Also, if the user clicks the Next Page link or scrolls down the page and sees additional results, this structure allows us to create a new signal without having to go back and modify the original signal. </li>
<li class="readable-text" id="p71"> <em>Every signal ties back to the </em><code>query_id</code><em> of the original “query” signal that started the series of content interactions.</em> The <code>query_id</code> is not merely a reference to the keywords entered by the user but is instead a reference to the specific “query” signal identifying a time-stamped instance of the user’s query keywords. Because results for the same query keywords can change over time, this enables us to do more sophisticated processing of how users reacted to the specific set of results they were shown for a query. </li>
<li class="readable-text" id="p72"> <em>Most signal types contain one item in the </em><code>target</code><em>, but the “results” signal type contains an ordered list of documents.</em> The order of results will matter for some algorithms we introduce later in the book, to measure relevance. It’s thus important to preserve the exact ordering of the search results. The <code>target</code> is an ordered list of documents in this case, instead of a single document. </li>
<li class="readable-text" id="p73"> <em>The checkout resulted in a separate “purchase” signal for each item instead of just one “checkout” signal.</em> This is done so we can track whether each purchase originated from separate queries. A “checkout” signal type could additionally be added to track the transaction, and possibly could list the two purchases as the <code>target</code>, but this is superfluous for our needs in this book. </li>
</ul>
<div class="readable-text" id="p74">
<p>With these raw signals available as our building blocks, we can now start thinking about how we could link the signals together to begin learning about our users and their interests. In the next section, we’ll discuss ways to model users, sessions, and requests within our search platform. </p>
</div>
<div class="readable-text" id="p75">
<h3 class="readable-text-h3" id="sigil_toc_id_57"><span class="num-string">4.1.4</span> Modeling users, sessions, and requests</h3>
</div>
<div class="readable-text" id="p76">
<p>In the last section, we looked at the structure of user signals as a list of independent interactions tied back to an original query. We assumed that a “user” was present with a unique ID, but how does one identify and track a unique user? Furthermore, once you have identified how to track unique users, what is the best way to break their interactions up into sessions to understand when their context may have changed?</p>
</div>
<div class="readable-text intended-text" id="p77">
<p>The concept of a user in web search can be quite fluid. If your search engine has authenticated (logged-in) users, then you already have an internal user ID to track them. If your search engine supports unauthenticated access or is publicly available, however, you’ll have many users running searches with no formal user ID. That doesn’t mean you can’t track them, however; it just requires a more fluid interpretation of what a “user” means. A unified tracking identifier allows us to relate multiple signals from the same user to learn their interaction patterns.</p>
</div>
<div class="readable-text intended-text" id="p78">
<p>If we think of the trackable information as a hierarchy from the most durable representation of a user to the least, it will look something like this:</p>
</div>
<ul>
<li class="readable-text" id="p79"> <em>User ID</em><em> </em>—A unique user ID that persists across devices (authenticated) </li>
<li class="readable-text" id="p80"> <em>Device ID</em><em> </em>—A unique ID that persists across sessions on the same device (such as a device ID or an IP address plus a device fingerprint) </li>
<li class="readable-text" id="p81"> <em>Browser ID</em><em> </em>—A unique ID that persists across sessions in the same application or browser only (a persistent cookie ID) </li>
<li class="readable-text" id="p82"> <em>Session ID</em><em> </em>—A unique ID that persists across a single session (such as a cookie in a browser’s incognito mode) </li>
<li class="readable-text" id="p83"> <em>Request ID</em><em> </em>—A unique ID that only persists for a single request (a browser with cookies turned off) </li>
</ul>
<div class="readable-text" id="p84">
<p>In most modern search applications, and certainly in most e-commerce applications, we typically need to deal with all of these. As a rule of thumb, you want to tie a user to the most durable identifier—the one as high up the list as possible. Both the links between request IDs and session IDs, as well as the links between session IDs and browser IDs, are through the user’s cookie, so ultimately the browser ID (persistent unique ID stored in the cookie) is the common denominator for each of these.</p>
</div>
<div class="readable-text intended-text" id="p85">
<p>Specifically,</p>
</div>
<ul>
<li class="readable-text" id="p86"> If a user has persistent cookies enabled, one browser ID can have many session IDs, which can have many request IDs. </li>
<li class="readable-text" id="p87"> If a user clears cookies after each session (such as by using incognito mode), each browser ID has only one session ID, which can have many request IDs. </li>
<li class="readable-text" id="p88"> If a user turns off cookies, then each request ID has a new session ID and a new browser ID. </li>
</ul>
<div class="readable-text" id="p89">
<p>When building search platforms, most organizations do not properly plan for and design their signals-tracking mechanisms. If they’re not able to correlate visitors’ queries with subsequent actions, it becomes difficult to maximize the capabilities of their AI-powered search platform. In some cases, it’s possible to derive missing signals-tracking information after the fact (such as by modeling signals into likely sessions using timestamps), but it’s usually best to design the system upfront to better handle user tracking to prevent potential information loss. In the next section, we’ll discuss how these rich signals can be used to improve relevance through a process known as “reflected intelligence”. </p>
</div>
<div class="readable-text" id="p90">
<h2 class="readable-text-h2" id="sigil_toc_id_58"><span class="num-string">4.2</span> Introducing reflected intelligence</h2>
</div>
<div class="readable-text" id="p91">
<p>In the last section, we covered how to capture signals from users as they interact with our search engine. While the signals themselves are useful to help us understand how our search engine is being used, they also serve as the primary inputs for building models that can continually learn from user interactions and enable our search engine to self-tune its relevance model. In this section, we’ll introduce how these self-tuning models work through the concept of reflected intelligence. </p>
</div>
<div class="readable-text" id="p92">
<h3 class="readable-text-h3" id="sigil_toc_id_59"><span class="num-string">4.2.1</span> What is reflected intelligence?</h3>
</div>
<div class="readable-text" id="p93">
<p>Imagine you are an employee at a hardware store. Someone asks you where they can find a hammer, and you tell them “aisle two”. A few minutes later you see the same person walk from aisle two to aisle five without a hammer, and then they walk out of aisle five holding a hammer. The next day someone else asks for a hammer, you again tell them “aisle two”, and you observe a nearly identical pattern of behavior. You would be a lousy employee if you didn’t spot this pattern and adjust your advice going forward to provide a better experience for your customers. </p>
</div>
<div class="readable-text intended-text" id="p94">
<p>Unfortunately, most search engines function in this manner by default—they have a largely static set of documents that are returned for each query, regardless of who each user is or how prior users have reacted to the list of documents shown. By applying machine learning to the collected signals, however, we can learn about users’ intent and reflect that knowledge to improve future search results. This process is called <em>reflected intelligence</em>.</p>
</div>
<div class="readable-text intended-text" id="p95">
<p>Reflected intelligence is all about creating feedback loops that constantly learn and improve based on evolving user interactions. Figure 4.2 demonstrates a high-level overview of a reflected intelligence process.</p>
</div>
<div class="browsable-container figure-container" id="p96">
<img alt="figure" height="574" src="../Images/CH04_F02_Grainger.png" width="980"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.2</span> The reflected intelligence process. A user issues a query, sees results, and takes a set of actions. Those actions (signals) are then processed to create learned relevance models that improve future searches.</h5>
</div>
<div class="readable-text" id="p97">
<p>In figure 4.2., a user (Alonzo) runs a search, entering the query <code>ipad</code> in the search box. A query signal is logged, containing the list of all search results displayed to Alonzo. Alonzo then sees the list of search results and takes two actions: clicking on a document (doc22) and then purchasing the product that document represents. Those two additional actions are logged as additional signals. All Alonzo’s signals, along with the signals from every other user, can then be aggregated and processed by various machine learning algorithms to create learned relevance models.</p>
</div>
<div class="readable-text intended-text" id="p98">
<p>These learned relevance models may boost the most popular results for specific queries, personalize results for each user, or even learn which document attributes are generally most important across all users. The models could also learn how to better interpret user queries, such as identifying common misspellings, phrases, synonyms, or other linguistic patterns and domain-specific terminology.</p>
</div>
<div class="readable-text intended-text" id="p99">
<p>Once these learned relevance models are generated, they can then be deployed back into the production search engine and immediately applied to enhance the outcomes of future queries. The process then begins again, with the next user running a search, seeing (now hopefully improved) search results, and interacting with those results. This process creates a self-learning system that improves with every additional user interaction, getting continually smarter and more relevant over time and automatically adjusting as user interests and content evolve.</p>
</div>
<div class="readable-text intended-text" id="p100">
<p>In the following sections, we’ll explore a few categories of reflected intelligence models, including signals boosting (popularized relevance), collaborative filtering (personalized relevance), and learning to rank (generalized relevance). We’ll start with one of the simplest and most effective: signals-boosting models. </p>
</div>
<div class="readable-text" id="p101">
<h3 class="readable-text-h3" id="sigil_toc_id_60"><span class="num-string">4.2.2</span> Popularized relevance through signals boosting</h3>
</div>
<div class="readable-text" id="p102">
<p>The most popular queries sent to your search engine tend to also be the most important ones to optimize from a relevance standpoint. Thankfully, since more popular queries generate more signals, we can often aggregate and boost the relevance of documents with the highest number of signals for each query. This kind of <em>popularized relevance</em> is known as <em>signals boosting</em>. It is one of the simplest forms of reflected intelligence and also one of the most effective for improving the relevance of your most popular, highest-volume queries. The following listing demonstrates an out-of-the-box search with the query <code>ipad</code> in our RetroTech search engine, prior to any signals boosting being applied. </p>
</div>
<div class="browsable-container listing-container" id="p103">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.5</span> Executing a keyword search for products matching <code>ipad</code></h5>
<div class="code-area-container">
<pre class="code-area">query = "ipad"
request = product_search_request(query)
response = products_collection.search(**request)
display_product_search(query, response["docs"])</pre>
</div>
</div>
<div class="readable-text" id="p104">
<p>As expected, this query returns many documents containing the keyword <code>ipad</code>, with the documents containing <code>ipad</code> typically ranking highest. Figure 4.3 shows the results for this query.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p105">
<img alt="figure" height="1068" src="../Images/CH04_F03_Grainger.png" width="1050"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.3</span> Results of a keyword search for the query <code>ipad</code>. Results are returned primarily based on the number of occurrences of the keyword, so accessories mentioning the keyword multiple times rank higher than the actual product the user intended to see.</h5>
</div>
<div class="readable-text" id="p106">
<p>While these results all contain the word “ipad” in their content multiple times, most users would be disappointed with these results, since they are secondary accessories as opposed to the main product type that was the focus of the search. This is a significant limitation when ranking just by document text. For very popular queries, however, it’s likely that many customers will run the same queries repeatedly and fight through the frustrating search results to find the actual products they are seeking. We can tie these repeated searches into a feedback loop that continuously updates a signals-boosting model based on new signals, as shown in figure 4.4.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p107">
<img alt="figure" height="527" src="../Images/CH04_F04_Grainger.png" width="1015"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.4</span> Signals-boosting feedback loop. A user’s search is logged, and the current signals-boosting model is applied to return the boosted results. After users take action on those results, all user interaction signals with documents are aggregated by the originating query to generate an updated model to further improve future searches.</h5>
</div>
<div class="readable-text" id="p108">
<p>Once your products are indexed and you’ve started collecting signals for your users’ queries and document interactions, the only additional steps necessary for implementing signals boosting are to aggregate your signals and then add your aggregated signals as boosts to either your queries or your documents. Listing 4.6 demonstrates a simple model for aggregating signals into a sidecar collection.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p109">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Sidecar collections</h5>
</div>
<div class="readable-text" id="p110">
<p>Sidecar collections are additional collections that sit in your search engine alongside a primary collection and that contain other useful data to improve your search application. In our e-commerce example, our primary collection is <code>products</code>. Our <code>signals</code> collection, which has already been added, can be considered a sidecar collection. We’ll add another sidecar collection, <code>signals_boosting</code>, which we’ll use at query time to enhance our queries. Throughout the book, we’ll introduce many other sidecar collections to store the inputs for and outputs of our generated models. </p>
</div>
</div>
<div class="browsable-container listing-container" id="p112">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.6</span> Creating a signals-boosting model by aggregating signals</h5>
<div class="code-area-container">
<pre class="code-area">signals_collection = engine.get_collection("signals")
create_view_from_collection(signals_collection, "signals") <span class="aframe-location"/> #1

signals_aggregation_query = """
SELECT q.target AS query, c.target AS doc,<span class="aframe-location"/> #2
COUNT(c.target) AS boost  #2
FROM signals c LEFT JOIN signals q ON c.query_id = q.query_id
WHERE c.type = 'click' AND q.type = 'query'
GROUP BY q.target, doc #2
ORDER BY boost DESC"""

dataframe = spark.sql(signals_aggregation_query) <span class="aframe-location"/> #3
signals_boosting_collection = \ <span class="aframe-location"/> #4
  engine.create_collection("signals_boosting") #4
signals_boosting_collection.write(dataframe)  #4</pre>
<div class="code-annotations-overlay-container">
     #1 Creates a view to make the signals collection queryable with SQL
     <br/>#2 Counts total clicks for each document for each keyword
     <br/>#3 Executes the signals aggregation SQL query
     <br/>#4 Writes the results to a new signals_boosting collection
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p113">
<p>The most important part of listing 4.6 is the <code>signals_aggregation_query</code>, defined as a SQL query for readability. For every query, we will get the list of documents that users have clicked on in the search results for that query, along with a count of how many times the document has been clicked on. By ordering the documents by the click count for each query, we get an ordered list of the documents ranked by popularity. </p>
</div>
<div class="readable-text intended-text" id="p114">
<p>The idea here is that users tend to choose the products they believe are the most relevant, so if we were to boost these documents, we would expect our top search results to become more relevant. We’ll test this theory in the next listing by using these aggregated counts as signals boosts. Let’s revisit our previous query for <code>ipad</code>.</p>
</div>
<div class="browsable-container listing-container" id="p115">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.7</span> Searching with signals boosting to improve relevance</h5>
<div class="code-area-container">
<pre class="code-area">def search_for_boosts(query, collection, query_field="query"):
  boosts_request = {"query": query,
                    "query_fields": [query_field],
                    "return_fields": ["query", "doc", "boost"],
                    "limit": 10,
                    "order_by": [("boost", "desc")]}
  response = collection.search(**boosts_request)
  return response["docs"]

def create_boosts_query(boost_documents):
  print(f"Boost Documents: \n{boost_documents}")
  boosts = " ".join([f'"{b["doc"]}"^{b["boost"]}'
                     for b in boost_documents])
  print(f"\nBoost Query: \n{boosts}\n")
  return boosts

query = "ipad"
boost_docs = search_for_boosts(query, signals_boosting_collection)
boosts_query = create_boosts_query(boost_docs)
request = product_search_request(query)
request["query_boosts"] = boosts_query

response = products_collection.search(**request)
display_product_search(query, response["docs"])</pre>
</div>
</div>
<div class="readable-text" id="p116">
<p>Boost documents:</p>
</div>
<div class="browsable-container listing-container" id="p117">
<div class="code-area-container">
<pre class="code-area">[{"query": "ipad", "doc": "885909457588", "boost": 966},
 {"query": "ipad", "doc": "885909457595", "boost": 205},
 {"query": "ipad", "doc": "885909471812", "boost": 202},
 {"query": "ipad", "doc": "886111287055", "boost": 109},
 {"query": "ipad", "doc": "843404073153", "boost": 73},
 {"query": "ipad", "doc": "635753493559", "boost": 62},
 {"query": "ipad", "doc": "885909457601", "boost": 62},
 {"query": "ipad", "doc": "885909472376", "boost": 61},
 {"query": "ipad", "doc": "610839379408", "boost": 29},
 {"query": "ipad", "doc": "884962753071", "boost": 28}]</pre>
</div>
</div>
<div class="readable-text" id="p118">
<p>Boost query:</p>
</div>
<div class="browsable-container listing-container" id="p119">
<div class="code-area-container">
<pre class="code-area">"885909457588"^966 "885909457595"^205 "885909471812"^202 "886111287055"^109
"843404073153"^73 "635753493559"^62 "885909457601"^62 "885909472376"^61
"610839379408"^29 "884962753071"^28</pre>
</div>
</div>
<div class="readable-text" id="p120">
<p>The query in listing 4.7 does two noteworthy things:</p>
</div>
<ul>
<li class="readable-text" id="p121"> It queries the <code>signals_boosting</code> sidecar collection for ranked documents by boost and transforms those signals boosts into another query. </li>
<li class="readable-text" id="p122"> It then passes that boosting query to the search engine as a query-time boost in the <code>query_boosts</code> parameter. In the case of Solr (our default search engine), this translates internally to a <code>boost</code> parameter of <code>sum(1,query($boost_query))</code> being added to the search request to multiply the relevance score by <code>1</code> (so it always increases) plus the calculated relevance score of the <code>boost_query</code>. (See section 3.2 if you want a refresher on influencing ranking like this through functions and multiplicative boosts.)  </li>
</ul>
<div class="readable-text" id="p123">
<p>If you remember from figure 4.3, our original keyword search for <code>ipad</code> returned mostly iPad accessories, as opposed to actual iPad devices. Figure 4.5 demonstrates the improved results after signals boosting is applied to that original query.</p>
</div>
<div class="browsable-container figure-container" id="p124">
<img alt="figure" src="../Images/CH04_F05_Grainger.png" width="60%"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.5</span> Search results with signals boosting enabled. Instead of iPad accessories showing up as before, we now see actual iPads, because we have crowdsourced the answers based on the documents that users choose to interact with.</h5>
</div>
<div class="readable-text intended-text" id="p125">
<p>The new results are significantly better than the keyword-only results. We now see products that the user was more likely looking for—iPads! You can expect to see similar improvements for most other popular queries in your search engine. Of course, as we move further down the list of popular products, the relevance improvements from signals boosting will start to decline, and with insufficient signals we may even reduce relevance. Thankfully, we’ll introduce many other techniques to improve relevance for queries with inadequate signals volume.<span class="aframe-location"/></p>
</div>
<div class="readable-text intended-text" id="p126">
<p>The goal of this section was to walk you through an initial, concrete example of implementing an end-to-end reflected intelligence model. The signals aggregation used in this implementation was very simple, though the results speak for themselves. There are many considerations and nuances to consider when implementing a signals-boosting model—whether to boost at query time or index time, how to increase the weight of newer signals versus older signals, how to avoid malicious users trying to boost particular products in the search results by generating bogus signals, how to introduce and blend signals from different sources, and so on. We’ll cover each of these topics in detail in chapter 8.</p>
</div>
<div class="readable-text intended-text" id="p127">
<p>Let’s move on from popularized relevance and signals boosting for now and discuss a few other types of reflected intelligence models. </p>
</div>
<div class="readable-text" id="p128">
<h3 class="readable-text-h3" id="sigil_toc_id_61"><span class="num-string">4.2.3</span> Personalized relevance through collaborative filtering</h3>
</div>
<div class="readable-text" id="p129">
<p>Let’s now look at a reflected intelligence approach called collaborative filtering, which we’ll categorize as <em>personalized relevance</em>. Whereas popularized relevance determines which results are usually the most popular across many users, personalized relevance focuses on determining which items are most likely to be relevant for a specific user. </p>
</div>
<div class="readable-text intended-text" id="p130">
<p><em>Collaborative filtering</em> is the process of using observations about the preferences of some users to predict the preferences of other users. It is the most popular type of algorithm used by recommendation engines, and it is the source of the common “users who liked this item also liked these items” recommendation lists that appear on many websites. Figure 4.6 demonstrates how collaborative filtering follows this same reflected intelligence feedback loop that we saw for signals-boosting models.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p131">
<img alt="figure" height="584" src="../Images/CH04_F06_Grainger.png" width="1012"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.6</span> Collaborative filtering for user-to-item recommendations. Based upon his past behavior, our user (Alonzo) receives recommendations based on items other users liked, where those users also interacted with some of the same items as Alonzo.</h5>
</div>
<div class="readable-text" id="p132">
<p>Like signals boosting, collaborative filtering involves a continuous feedback loop. Signals are collected, models are built by those signals, recommendations are generated by those models, and interactions against those recommendations are then logged again as additional signals. Collaborative filtering approaches typically generate a user-item interaction matrix, mapping each user to each item (document), with the relationship strength between each user and item being based on the strength of the positive interactions (clicks, purchases, ratings, and so on).</p>
</div>
<div class="readable-text intended-text" id="p133">
<p>If the interaction matrix is sufficiently populated, it’s possible to infer recommendations from it for any user or item with interaction data. This is done by directly looking up the other users who interacted with the same item and then boosting other items (similar to signals boosting) that those users also interacted with. If the user-item interaction matrix is too sparse, however, a matrix factorization approach will typically need to be applied. </p>
</div>
<div class="readable-text intended-text" id="p134">
<p><em>Matrix factorization</em> is the process of breaking the user-item interaction matrix into two matrices: one mapping users to latent features (or <em>factors</em>), and another mapping those latent factors to items. This is similar to the dimensionality reduction approach we described in chapter 3, where we switched from representing food items with many exact keywords (a vector including a feature for every word in the inverted index) to using a much smaller number of meaningful dimensions (a vector with eight features describing the food items) that we compressed the data into. This matrix factorization makes it possible to derive user preferences for items, as well as the similarity between items, by distilling limited signals data into a smaller number of more meaningful dimensions that better generalize the similarity between items.</p>
</div>
<div class="readable-text intended-text" id="p135">
<p>In the context of matrix factorization for collaborative filtering, the latent factors represent attributes of our documents that are learned to be important indicators of shared interests across users. By matching other documents based on these factors, we are using crowdsourcing to find other similar documents matching the same shared interests.</p>
</div>
<div class="readable-text intended-text" id="p136">
<p>As powerful as collaborative filtering can be for learning user interests and tastes based entirely on crowdsourced relevance, it suffers from a major flaw known as the <em>cold start problem</em>. This is a scenario where the returning of results is dependent upon the existence of signals, but where new documents that have never generated signals will not be returned. This creates a catch-22 situation where new content is unlikely to be shown to users (a prerequisite for generating signals) because it has not yet generated any signals (which are required for the content to be shown). To some degree, signals-boosting models demonstrate a similar problem, where documents that are already popular tend to receive a higher boost, resulting in them getting even more signals, while unseen documents continue to not receive signals boosting. This process creates a self-reinforcing cycle that can lead to a lack of diversity in search results. This problem is called <em>presentation bias</em>, and we’ll show how to overcome it in chapter 12. </p>
</div>
<div class="readable-text intended-text" id="p137">
<p>You can also generate recommendations in other ways, such as through content-based recommendations, which we’ll explore in the next chapter (section 5.4.6). Collaborative filtering is unique, however, in that it can learn the preferences and tastes of users for other documents without having to know anything about the content of the documents. This is because all decisions are made entirely by observing the interactions of users with content and determining the strength of the similarity based on those observations. We’ll take a deeper dive into collaborative filtering when implementing personalized search in chapter 9.</p>
</div>
<div class="readable-text intended-text" id="p138">
<p>Instead of only utilizing popularized and personalized relevance models (which function best when documents already have signals), a search engine can also benefit from a more generalized relevance model that can apply to all searches and documents. This goes a long way toward solving the cold-start problem. We’ll next explore how crowdsourced relevance can be generalized through a technique called “learning to rank”. </p>
</div>
<div class="readable-text" id="p139">
<h3 class="readable-text-h3" id="sigil_toc_id_62"><span class="num-string">4.2.4</span> Generalized relevance through learning to rank</h3>
</div>
<div class="readable-text" id="p140">
<p>Because signals boosting (popularized relevance, section 4.2.2) and collaborative filtering (personalized relevance, section 4.2.3) only work on documents that already have signals, a substantial proportion of queries won’t benefit until documents receive traffic. This is where learning to rank proves valuable as a form of generalized relevance. </p>
</div>
<div class="readable-text intended-text" id="p141">
<p><em>Learning to rank</em> (LTR), also known as <em>machine-learned ranking</em>, is the process of building and using a ranking classifier that can score how well any document matches any arbitrary query. You can think of the ranking classifier as a trained relevance model. Instead of manually tuning search boosts and other parameters, the LTR process trains a machine learning model that can understand the important features of your documents and then score search results appropriately. Figure 4.7 shows the general flow for rolling out LTR.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p142">
<img alt="figure" height="548" src="../Images/CH04_F07_Grainger.png" width="1014"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.7</span> Learning to rank (generalized relevance). A ranking classifier is built from user judgments about the known relevance of documents for each query (training set). That ranking classifier model is then used to rerank search results so that the top-ranked documents are more relevant.</h5>
</div>
<div class="readable-text" id="p143">
<p>In an LTR system, the same high-level reflected intelligence process applies as in signals boosting and collaborative filtering (refer to figure 4.2). The difference is that LTR can use relevance judgment lists (maps of queries to their ideal ranked set of documents) to automatically train a relevance model that can then be applied generally to all queries. You’ll see that the output of the “Build ranking classifier” step in figure 4.7 is a model of relevance features (<code>title_match_any_terms</code>, <code>is_known_category</code>, <code>popularity</code>, and <code>content_age</code>), and that model is deployed into the production search engine periodically to enhance search results ranking. The features in a very simple machine-learned ranking model might be readable like this, but there is no requirement that a ranking classifier be interpretable or explainable, and many advanced, deep-learning-based ranking classifiers are not.</p>
</div>
<div class="readable-text intended-text" id="p144">
<p>In figure 4.7, notice that the live user flow starts with a query for <code>ipad</code>. The initial search results are then run through the deployed learning-to-rank classifier, which returns the final reranked set of search results. Since the ranking classifier is typically much more intelligent and uses more complicated ranking parameters than a traditional keyword-ranking relevance model, it is usually way too slow to use the ranking classifier to score all the matching documents in the search engine. Instead, LTR will often use an initial, faster ranking function (such as BM25) to find the top-N documents (usually hundreds or thousands of documents) and then only run that subset of documents through the ranking classifier. It is possible to use the ranking classifier as the main relevance function instead of applying this reranking technique, but it’s more common to see a reranking approach, as it’s typically much faster while still yielding approximately the same results.</p>
</div>
<div class="readable-text intended-text" id="p145">
<p>LTR can use either explicit relevance judgments (created manually by experts) or implicit judgments (derived from user signals), or some combination of the two. We’ll cover examples of implementing LTR from both explicit and implicit judgment lists in chapters 10–12. </p>
</div>
<div class="readable-text" id="p146">
<h3 class="readable-text-h3" id="sigil_toc_id_63"><span class="num-string">4.2.5</span> Other reflected intelligence models</h3>
</div>
<div class="readable-text" id="p147">
<p>In addition to diving deeper into signals boosting (chapter 8), collaborative filtering (chapter 9), and learning to rank (chapter 10), we’ll explore many other kinds of reflected intelligence throughout this book. In chapter 6, we’ll explore mining user queries to automatically learn domain-specific phrases, common misspellings, synonyms, and related terms, and in chapters 11–12 we’ll explore automated ways of learning relevance judgments from users’ interactions so we can automatically generate training data for interesting machine learning approaches. </p>
</div>
<div class="readable-text intended-text" id="p148">
<p>In general, every interaction between a user and content creates a connection—an edge in a graph—that we can use to understand emerging relationships and derive deeper insights. Figure 4.8 demonstrates some of the various relationships we can learn by exploring this interaction graph. The same incoming signals data can be processed differently, through various signals aggregation and machine learning approaches, to learn</p>
</div>
<ul>
<li class="readable-text" id="p149"> The similarity between users and items (user-item recommendations) </li>
<li class="readable-text" id="p150"> The similarity between items and other items (item-item recommendations) </li>
<li class="readable-text" id="p151"> Specific attribute-based preferences that can generate a profile of a user’s interests </li>
<li class="readable-text" id="p152"> The similarity between queries and items<span class="aframe-location"/> </li>
</ul>
<div class="browsable-container figure-container" id="p153">
<img alt="figure" height="551" src="../Images/CH04_F08_Grainger.png" width="1014"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.8</span> Many reflected intelligence models. The leftmost box represents user-to-item similarity for recommendations, the next shows learning specific attribute-based preferences for a user’s profile, the third shows learning item-to-item–based similarity for recommendations, and the rightmost shows learning query-to-item recommendations.</h5>
</div>
<div class="readable-text" id="p154">
<p>We’ll continue to explore these techniques in the chapters to come, but it’s good to keep in mind that signals data contains a treasure trove of potential insights and often provides just as much benefit as the content of the documents receiving the user interactions. Reflected intelligence and crowdsourcing are not limited to only the signals boosting, collaborative filtering, and learning-to-rank techniques we’ve described. They can also be derived from content instead of signals, as we’ll discuss in the next section. </p>
</div>
<div class="readable-text" id="p155">
<h3 class="readable-text-h3" id="sigil_toc_id_64"><span class="num-string">4.2.6</span> Crowdsourcing from content</h3>
</div>
<div class="readable-text" id="p156">
<p>While we typically think of crowdsourcing as asking users to provide input, we’ve seen in this chapter that implicit feedback can often provide as much or even more value in aggregate across many user signals. While this chapter has been entirely focused on using user signals to do this crowdsourcing, it’s also important to point out that content itself can be used as crowdsourced intelligence for your AI-powered search platform. </p>
</div>
<div class="readable-text intended-text" id="p157">
<p>For example, if you’re trying to figure out the general quality of your documents, you may be able to look at customer reviews to generate a product rating or to see if the product has been reported as abusive or spam. If the customer has left comments, you may be able to run a <em>sentiment analysis</em> algorithm on the text, denoting if the comments are positive, neutral, or negative. Based on the detected sentiment, you can boost or penalize the source documents accordingly. This process is essentially deriving signals from user-submitted content, so it’s still a form of crowdsourcing, albeit from other content provided by the users. </p>
</div>
<div class="readable-text intended-text" id="p158">
<p>We mentioned that in chapter 6 we’ll walk through how we can mine user signals to automatically learn domain-specific terminology (phrases, misspellings, synonyms, and so on). Just as you can take user queries and interactions to learn this terminology, you should also realize that documents are typically written by people and that very similar kinds of relationships between terminology are therefore reflected in the written content. We’ll explore these content-based relationships further in the next chapter.</p>
</div>
<div class="readable-text intended-text" id="p159">
<p>One of the best-known search algorithms in existence is the <em>Page Rank</em> algorithm—the breakthrough algorithm that initially enabled Google to rise to prominence as the most relevant web search engine. Page Rank goes beyond the text of any given web page and looks at the implied behavior of all other web page creators to see how they have linked to other web pages. By measuring the incoming and outgoing links, it’s possible to measure a web page’s “quality” on the assumption that websites are more likely to link to higher-quality, more authoritative sources and that those higher-quality sources are less likely to link to lower-quality sources. This idea of going beyond the content within a single document and relating it to other documents—whether by direct links between them, user comments or feedback, any other user interactions, or even just the use of terminology in different, nuanced ways across documents—is incredibly powerful. The art and science of using all the available information about your content and from your users is key to building a highly relevant AI-powered search engine. In chapter 5, we’ll look into the concept of knowledge graphs and how we can use some of the relationships embedded within the implied links between documents to automatically further domain understanding. </p>
</div>
<div class="readable-text" id="p160">
<h2 class="readable-text-h2" id="sigil_toc_id_65">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p161"> Content, signals, and models (which are derived from content and signals) are the three main sources of “fuel” to power an AI-powered search engine, with signals being the primary source for crowdsourced relevance. </li>
<li class="readable-text" id="p162"> Reflected intelligence is the process of creating learning feedback loops that improve from each user interaction and reflect that learned intelligence back, to continuously increase the relevance of future results. </li>
<li class="readable-text" id="p163"> Signals boosting is a form of “popularized relevance”, which usually has the biggest effect on your highest-volume, most popular queries. </li>
<li class="readable-text" id="p164"> Collaborative filtering is a form of “personalized relevance”, which can use patterns of user interaction with items to learn user preferences or the strength of relationships between items and to then recommend similar items based upon those learned relationships. </li>
<li class="readable-text" id="p165"> Learning to rank (LTR) is a form of “generalized relevance” and is the process of training a ranking classifier based on relevance judgment lists (queries mapping to correctly ranked documents). LTR can be applied to rank all documents and avoid the cold-start problem. </li>
<li class="readable-text" id="p166"> Other kinds of reflected intelligence exist, including techniques for using content (instead of just signals) for crowdsourced relevance. </li>
</ul>
</div></body></html>