["```py\nimport psutil \n\ndef get_cpu_memory_usage(): \nprocess = psutil.Process(os.getpid()) \nreturn process.memory_info().rss\n```", "```py\nimport pynvml \n\npynvml.nvmlInit() \ndef get_gpu_memory_usage(): \n   handle = pynvml.nvmlDeviceGetHandleByIndex(0) \n  info = pynvml.nvmlDeviceGetMemoryInfo(handle) \n  return info.used\n```", "```py\nfrom thop import profile   #1\n\ninput = torch.randn(1, 3, 224, 224) \nmacs, params = profile(model, inputs=(input, )) \nprint(f\"FLOPs: {macs}\")\n```", "```py\ntorch_geometric.transforms import ToSparseTensor\n\ndataset = YourDataset(transform=ToSparseTensor())\n```", "```py\nfrom torch_sparse import SparseTensor\n\ndense_adj = ...    #1\nsparse_adj = SparseTensor.from_dense(dense_adj)\n```", "```py\nfrom torch_geometric.loader import NeighborLoader\n```", "```py\nbatch_size = 128    #1\nnum_neighbors = 2    #2\n```", "```py\nloader = NeighborLoader(data, input_nodes = train_mask, batch_size=batch_size\\\n   num_neighbors=*num_neighbors)\n```", "```py\nfor batch_size, n_id, adjs in sampler:    #1\n  x = data.x[n_id].to(device)        #2\n  y = data.y[n_id].squeeze(1).to(device)   #3\n  adjs = [adj.to(device) for adj in adjs]  #4\n\n  optimizer.zero_grad()              #5\n  out = model(x, adjs)             #6\n  loss = F.nll_loss(out, y)         #7\n  loss.backward()               #8\n. optimizer.step()     #9\n```", "```py\nimport torch \nimport torch.distributed as dist \nimport torch.multiprocessing as mp \nimport torch.nn as nn \nfrom torch.nn.parallel import DistributedDataParallel    #1\nfrom torch.utils.data import DataLoader    #2\nfrom torchvision import datasets, transforms \n\nclass Flatten(nn.Module): \n  def forward(self, input): \n    return input.view(input.size(0), -1) \n\ndef train(model, trainloader, \n                 criterion, \n                 optimizer,\n                 device):   #3\n    model.train() \n    for batch_idx, (data, target) in enumerate(trainloader): \n      print(f'Process {device}, Batch {batch_idx}') \n       data, target = data.to(device), target.to(device) \n       optimizer.zero_grad() \n       output = model(data) \n       loss = criterion(output, target) \n       loss.backward() \n       optimizer.step() \n\ndef main(rank, world_size):    #4\n    filepath = '~/.pytorch/MNIST_data/'\n    dist.init_process_group(   #5\n    backend='nccl', \n    init_method='tcp://localhost:23456', \n    rank=rank,\n    world_size=world_size    #6\n    )\n\n    torch.manual_seed(0)   #7\n    device = torch.device(f'cuda:{rank}')    #8\n\n    transform = transforms.Compose(\n                        [transforms.ToTensor(),\n                        transforms.Normalize((0.5,),\n                        (0.5,))]\n                        )\n\n    trainset = datasets.MNIST(filepath ,                #9\n                                download=True,          #9\n                                train=True,             #9\n                                transform=transform)    #9\n    train_loader = DataLoader(trainset,           #10\n                                batch_size=64,    #10\n                                shuffle=True,     #10\n                                num_workers=2)    #10\n\n    model = nn.Sequential(Flatten(), nn.Linear(784, 10)).to(device) \n    model = DistributedDataParallel(model, device_ids=[rank])   #11\n\n    criterion = nn.CrossEntropyLoss() \n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n\n    train(model, train_loader, criterion, optimizer, device)    #12\n```", "```py\nloader = NodeLoader(\n    data=(feature_store, graph_store),\n    node_sampler=node_sampler,\n    batch_size=20,\n    input_nodes='paper',\n)\n\nfor batch in loader:\n    <training loop>\n```", "```py\nimport torch\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import graclus, max_pool\nfrom torch_geometric.utils import to_undirected\nfrom torch_geometric.datasets import KarateClub\n\ndataset = KarateClub()\ndata = dataset[0]  # Get the first graph\n\nedge_index = to_undirected(data.edge_index)  #1\n\nbatch = torch.zeros(data.num_nodes, dtype=torch.long)   #2\n\ncluster = graclus(edge_index)   #3\n\ndata_coarse = max_pool(cluster, data)   #4\n```"]