- en: Chapter 8\. Training Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。训练模型
- en: “Ask not for a lighter burden, but for broader shoulders.”
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “不要求负担更轻，而要求更宽广的肩膀。”
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Jewish proverb
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —犹太谚语
- en: While the supply of impressive models and data will continue to grow and overflow,
    it’s reasonable that you’ll want to do more than just consume TensorFlow.js models.
    You’ll come up with the idea that’s never been done before, and there won’t be
    an off-the-shelf option that day. It’s time for you to train your own model.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管令人印象深刻的模型和数据的供应将继续增长并溢出，但你可能希望做的不仅仅是消费TensorFlow.js模型。你将想出以前从未做过的想法，那天不会有现成的选择。现在是时候训练你自己的模型了。
- en: Yes, this is the task where the best minds in the world compete. While libraries
    could be written about the math, strategy, and methodology of training models,
    a core understanding will be vital. It’s crucial that you become familiar with
    the basic concepts and benefits of training a model with TensorFlow.js to take
    full advantage of the framework.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这是世界上最优秀的头脑竞争的任务。虽然关于训练模型的数学、策略和方法论可以写成一本书，但核心理解将至关重要。你必须熟悉使用TensorFlow.js训练模型的基本概念和好处，以充分利用这个框架。
- en: 'We will:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将：
- en: Train your first model in JavaScript code
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用JavaScript代码训练你的第一个模型
- en: Advance your understanding of model architecture
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升对模型架构的理解
- en: Review how to keep track of status during training
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾如何在训练过程中跟踪状态
- en: Cover some fundamental concepts of training
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 涵盖一些训练的基本概念
- en: When you finish this chapter, you’ll be armed with a few ways of training a
    model and a better understanding of the process of using data to make a machine
    learning solution.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当你完成这一章时，你将掌握几种训练模型的方法，并更好地理解使用数据制定机器学习解决方案的过程。
- en: Training 101
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练101
- en: It’s time to peel back the magic and train a model with JavaScript. While Teachable
    Machine is a great tool, it’s limited. To really empower machine learning, you’re
    going to have to identify the problem you want to solve and then teach a machine
    to find the patterns for a solution. To do this, we’ll view a problem through
    the eyes of data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候揭开魔法，用JavaScript训练一个模型了。虽然Teachable Machine是一个很好的工具，但它有限。要真正赋予机器学习力量，你需要确定你想要解决的问题，然后教会机器找到解决方案的模式。为了做到这一点，我们将通过数据的眼睛看问题。
- en: 'Take a look at this example of information, and before writing a line of code,
    see if you can identify the correlation between these numbers. You have a function
    `f` that takes a single number and returns a single number. Here’s the data:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在写任何代码之前，看看这个信息的例子，看看你能否确定这些数字之间的相关性。你有一个函数`f`，它接受一个数字并返回一个数字。以下是数据：
- en: Given –1, the result is –4.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定-1，结果为-4。
- en: Given 0, the result is –2.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定0，结果为-2。
- en: Given 1, the result is 0.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定1，结果为0。
- en: Given 2, the result is 2.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定2，结果为2。
- en: Given 3, the result is 4.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定3，结果为4。
- en: Given 4, the result is 6.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定4，结果为6。
- en: 'Can you identify what the answer for 5 would be? Can you extrapolate the solution
    for 10? Take a moment to evaluate the data before moving on. Some of you might
    have found the solution: Answer = 2x – 2.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你能确定5的答案是什么吗？你能推断出10的解决方案吗？在继续之前花点时间评估数据。你们中的一些人可能已经找到了解决方案：答案 = 2x - 2。
- en: The function `f` is a simple line, as shown in [Figure 8-1](#basic_linear).
    Knowing that, you can quickly solve for an input of 10 and find it would yield
    18.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`f`是一条简单的线，如[图8-1](#basic_linear)所示。知道这一点后，你可以快速解出输入为10时的结果为18。
- en: '![Showing the line equation](assets/ltjs_0801.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![显示线性方程](assets/ltjs_0801.png)'
- en: Figure 8-1\. X = 10 means Y = 18
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1。X = 10 意味着 Y = 18
- en: Solving this problem from the given data is exactly what machine learning can
    do. Let’s prep and train a TensorFlow.js model to solve this simple problem.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从给定数据解决这个问题正是机器学习可以做到的。让我们准备并训练一个TensorFlow.js模型来解决这个简单的问题。
- en: 'To apply supervised learning, you’ll need to do the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用监督学习，你需要做以下事情：
- en: Gather your data (both input and desired solution).
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集你的数据（输入和期望解决方案）。
- en: Create and design a model architecture.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建和设计模型架构。
- en: Identify how a model should learn and measure error.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定模型应该如何学习和衡量错误。
- en: Task the model with training and for how long.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型并确定训练时间。
- en: Data Prep
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: To prep a machine, you’ll write the code to supply the input tensors, aka the
    values `[-1, 0, 1, 2, 3, 4]` and their corresponding answers `[-4, -2, 0, 2, 4,
    6]`. The index of the question has to match the index of the expected answer,
    which makes sense when you think about it. Since we are giving the model all the
    answers to the values, that is what makes this a supervised learning problem.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备一台机器，你将编写代码来提供输入张量，即值`[-1, 0, 1, 2, 3, 4]`及其对应的答案`[-4, -2, 0, 2, 4, 6]`。问题的索引必须与预期答案的索引匹配，这在思考时是有意义的。因为我们给模型所有值的答案，这就是为什么这是一个监督学习问题。
- en: In this situation, the training set is six examples. Rarely would machine learning
    be used on such a small amount of data, but the problem is relatively small and
    straightforward. As you can see, none of the training data has been reserved for
    testing the model. Fortunately, you can try the model because you know the formula
    that was used to create the data in the first place. If you’re unfamiliar with
    the definitions of training and testing datasets, please review [“Common AI/ML
    Terminology”](ch01.html#common_AI_ML_terminology) in [Chapter 1](ch01.html#the_chapter_1).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，训练集有六个例子。机器学习很少会用在这么少的数据上，但问题相对较小且简单。正如你所看到的，没有任何训练数据被保留用于测试模型。幸运的是，你可以尝试这个模型，因为你知道最初用来创建数据的公式。如果你对训练和测试数据集的定义不熟悉，请查看第1章中的“常见AI/ML术语”。
- en: Design a Model
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计模型
- en: The idea of designing a model might sound tedious, but the honest answer is
    that it’s a mix of theory, trial, and error. Models can be trained for hours or
    even weeks before the designers of that model understand the performance of the
    architecture. An entire field of study could be dedicated to model design. The
    Layers models you’ll be creating for this book will give you an excellent foundation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 设计模型的想法可能听起来很繁琐，但诚实的答案是，这是理论、试验和错误的混合。在设计师了解架构的性能之前，模型可能需要经过数小时甚至数周的训练。整个研究领域可能致力于模型设计。您将为本书创建的Layers模型将为您提供良好的基础。
- en: The easiest way to design a model is to use the TensorFlow.js Layers API, which
    is a high-level API that allows you to define each layer in sequential order.
    In fact, to start your model, you’ll begin with the code `tf.sequential();`. You
    might hear this called the “Keras API” due to the origin of this style of model
    definition.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 设计模型的最简单方法是使用TensorFlow.js的Layers API，这是一个高级API，允许您按顺序定义每个层。实际上，要启动您的模型，您将从代码`tf.sequential();`开始。您可能会听到这被称为“Keras
    API”，因为这种模型定义风格的起源。
- en: The model you’ll create to solve the simple problem you are trying to tackle
    will have only a single layer and a single neuron. This makes sense when you think
    about the formula for a line; it’s not a very complex equation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您将创建的模型来解决您正在尝试解决的简单问题将只有一个层和一个神经元。当您考虑到一条线的公式时，这是有道理的；这不是一个非常复杂的方程。
- en: Note
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When you’re familiar with the basic equations of a dense network, it becomes
    amazingly apparent why a single neuron would work in this case, because the formula
    for a line is y = mx + b and the formula for an artificial neuron is y = Wx +
    b.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当你熟悉密集网络的基本方程时，就会惊讶地发现为什么在这种情况下单个神经元会起作用，因为一条线的公式是y = mx + b，而人工神经元的公式是y = Wx
    + b。
- en: To add a layer to the model, you will use `model.add` and then define your layer.
    With the Layers API, each layer that gets added defines itself and automatically
    gets connected depending on the order of `model.add` calls, just like pushing
    to an array. You’ll define the expected input for your model in the first layer,
    and the final layer that you add will define the output of your model (see [Example 8-1](#layers_model_example)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要向模型添加一层，您将使用`model.add`，然后定义您的层。使用Layers API，每个添加的层都会自行定义并根据`model.add`调用的顺序自动连接，就像推送到数组一样。您将在第一层中定义模型的预期输入，并且您添加的最后一层将定义模型的输出（参见[示例8-1](#layers_model_example)）。
- en: Example 8-1\. Building a hypothetical model
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-1。构建一个假设模型
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The model in [Example 8-1](#layers_model_example) would have three layers. `ALayer`
    would be tasked with identifying the expected model input and itself. `BLayer`
    doesn’t need to identify its input because it is inferred that the input would
    be `ALayer`. Therefore, `BLayer` only needs to define itself. `CLayer` would identify
    itself, and because it is last, this identifies the model’s output.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例8-1](#layers_model_example)中的模型将有三层。`ALayer`将负责识别预期的模型输入和自身。`BLayer`不需要识别其输入，因为可以推断输入将是`ALayer`。因此，`BLayer`只需要定义自身。`CLayer`将识别自身，并且因为它是最后一个，这将确定模型的输出。'
- en: Let’s get back to the model you’re trying to code. The architected model goal
    for the current problem has only one layer with one neuron. When you code that
    single layer, you’ll be defining your input and your output.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到您试图编码的模型。当前问题的架构模型目标只有一个具有一个神经元的层。当您编写该单个层时，您将定义您的输入和输出。
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The result is a straightforward neural network. When graphed, the network has
    two nodes (see [Figure 8-2](#simplest_nn)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个简单的神经网络。在绘制图时，网络有两个节点（参见[图8-2](#simplest_nn)）。
- en: '![2 nodes 1 edge nn](assets/ltjs_0802.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![2 nodes 1 edge nn](assets/ltjs_0802.png)'
- en: Figure 8-2\. One input and one output
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2。一个输入和一个输出
- en: Generally, layers have more artificial neurons (graph nodes) but also are more
    complicated and have other properties to configure.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，层具有更多的人工神经元（图节点），但也更复杂，并具有其他要配置的属性。
- en: Identify Learning Metrics
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 识别学习指标
- en: Next, you’ll need to tell your model how to identify progress and how it can
    be better. These concepts aren’t foreign; they just seem strange in software.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要告诉您的模型如何识别进展以及如何变得更好。这些概念并不陌生；它们在软件中只是看起来有点奇怪。
- en: Every time I try to aim a laser pointer at something, I generally miss it. However,
    I can see I’m a bit off to the left or right, and I adjust. Machine learning does
    the same thing. It might start randomly, but the algorithm corrects itself, and
    it needs to know how you want it to do that. A method that most fits my laser
    pointer example would be *gradient descent*. The smoothest iterative way to optimize
    the laser pointer is a method called *stochastic gradient descent*. That’s what
    we’ll use in this case because it works well, and it sounds pretty cool for your
    next dinner party.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我试图将激光指示器对准某物时，我通常会错过。但是，我可以看到我稍微偏左或偏右，然后进行调整。机器学习也是如此。它可能会随机开始，但算法会自我纠正，并且需要知道您希望它如何做到这一点。最符合我的激光指示器示例的方法将是*梯度下降*。优化激光指示器的最平滑迭代方法称为*随机梯度下降*。这就是我们将在这种情况下使用的方法，因为它效果很好，并且对于您下次晚宴听起来相当酷。
- en: 'As for measuring error, you might think a simple “right” and “wrong” would
    work, but there’s a significant difference between being a couple of decimals
    off versus being wrong by thousands. For this reason, you generally rely on a
    loss function to help you identify how wrong an AI is with a predicted guess.
    There are lots of ways to measure error, but in this case, mean squared error
    (MSE) is a great measurement. For those who need to know the mathematics, MSE
    is the average squared difference between the estimated values (y) and the actual
    value (y with a little hat). Feel free to ignore this next bit, as the framework
    is calculating it for you, but if you’re familiar with common mathematical notation,
    this can be represented like so:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><mo form="prefix">MSE</mo> <mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup>
    <msup><mrow><mo>(</mo><msub><mi>Y</mi> <mi>i</mi></msub> <mo>-</mo><mover accent="true"><msub><mi>Y</mi>
    <mi>i</mi></msub> <mo>^</mo></mover><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Why would you like this formula over something simple like distance from the
    original answer? There are some mathematical benefits baked into MSE that help
    incorporate variance and bias as positive error scores. Without getting too deep
    into statistics, it’s one of the most common loss functions for solving for lines
    that fit data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Stochastic gradient descent and mean squared error reek of mathematical origins
    that do little to tell a pragmatic developer their purpose. In situations like
    these, it’s best to absorb these terms for what they are, and if you’re feeling
    adventurous, you can watch tons of videos that will explain them in greater detail.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'When you are ready to tell a model to use specific learning metrics and you’re
    done adding layers to a model, this is all wrapped up in a `.compile` call. TensorFlow.js
    is cool enough to know all about gradient descent and mean squared error. Rather
    than coding these functions, you can identify them with their approved string
    equivalents:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: One of the great benefits to using a framework is that as the machine learning
    world invents new optimizers like “Adagrad” and “Adamax,” they can be tried and
    invoked by simply changing a string^([1](ch08.html#idm45049242996728)) in your
    model architecture. Switching “sgd” to [“adamax”](https://arxiv.org/abs/1412.6980)
    takes relatively no time for you as a developer, and it might significantly improve
    your model training time without you reading the published paper on stochastic
    optimization.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Identifying functions without understanding the specifics of a function provides
    a bittersweet benefit similar to changing file types without having to understand
    the full structure of each type. A little bit of knowledge of the pros and cons
    for each goes a long way, but you don’t need to memorize the specification. It’s
    worth taking a little time when you’re architecting to read up on what’s available.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry. You will see the same names being used over and over, so it’s easy
    to get the hang of them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the model is created. It will fail if you ask it to predict anything
    because it has done zero training. The weights in the architecture are entirely
    random, but you can review the layers by calling `model.summary()`. The output
    goes directly to the console and looks somewhat like [Example 8-2](#basic_summary_output).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-2\. Calling `model.summary()` on a Layers model prints the layers
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The layer `dense_Dense6` is an automatic ID to reference this layer in the TensorFlow.js
    backend. Your ID can vary. This model has two trainable parameters, which makes
    sense since a line is y = mx + b, right? A fun way to think of this visually is
    to look back to [Figure 8-2](#simplest_nn) and count the lines and nodes. One
    line and one node means two trainable params. All parameters for the layer are
    trainable. We’ll cover non-trainable params later. This one-layer model is ready
    to go.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Task the Model with Training
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final step for training a model is to combine the inputs into the architecture
    and assign how long it should train. As mentioned earlier, this is often measured
    in epochs, which is how many times the model will review the flashcards with the
    right answers, and then when it’s complete, it stops training. The number of epochs
    you should use depends on the magnitude of the problem, the model, and how correct
    is “good enough.” In some models, getting another half of a percent is worth hours
    of training, and in our case, the model is accurate enough to be correct within
    seconds.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: The training set is a 1D tensor with six values. If the epochs were set to 1,000,
    the model would effectively train 6,000 iterations, which would take any modern
    computer a few seconds at most. The trivial problem of fitting a line to points
    is quite simple for a computer.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Put It All Together
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you’re familiar with the high-level concepts, you’re probably eager
    to solve this problem with code. Here’s the code to train a model on the data
    and then immediately ask the model for an answer for the value `10`, as discussed.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_training_models_CO1-1)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: The data is prepared in tensors with inputs and expected outputs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_training_models_CO1-2)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: A sequential model is started.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_training_models_CO1-3)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Add the only layer with one input and one output, as discussed.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_training_models_CO1-4)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Finish the sequential model with a given optimizer and loss function.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_training_models_CO1-5)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: The model is told to train with `fit` for 300 epochs. This is a trivial amount
    of time, and when the `fit` is complete, the promise it returns is resolved.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_training_models_CO1-6)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Ask the trained model to provide an answer for the input tensor `10`. You’ll
    need to round the answer to force an integer result.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_training_models_CO1-7)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Dispose of everything once you’ve got your answer.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You’ve trained a model from scratch in code. The problem you’ve
    just solved is called a *linear regression* problem. It has all kinds of uses,
    and it’s a common tool for predicting things such as housing prices, consumer
    behavior, sales forecasts, and plenty more. Generally, points don’t perfectly
    land on a line in the real world, but now you have the ability to turn scattered
    linear data into a predictive model. So when your data looks like [Figure 8-3](#points_graph),
    you can solve as shown in [Figure 8-4](#points_graph_solved).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![scattered linear data on a graph](assets/ltjs_0803.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Scattered linear data
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![graph with predicted line of best fit](assets/ltjs_0804.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. Predicted line of best fit using TensorFlow.js
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that you’re familiar with the basics of training, you can expand your process
    to understanding what it takes to solve more complex models. Training a model
    is significantly dependent on the architecture and the quality and quantity of
    the data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Nonlinear Training 101
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If every problem were based on lines, there would be no need for machine learning.
    Statisticians have been solving linear regression since the early 1800s. Unfortunately,
    this fails as soon as your data is nonlinear. What would happen if you asked the
    AI to solve for Y = X²?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![squared function](assets/ltjs_0805.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. Simple Y = X²
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: More complex problems require more complex model architecture. In this section,
    you’ll learn new properties and features of a layers-based model, as well as tackle
    a nonlinear grouping of data.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的问题需要更复杂的模型架构。在本节中，您将学习基于层的模型的新属性和特性，以及处理数据的非线性分组。
- en: You can add far more nodes to a neural network, but they would all still exist
    in a grouping of linear functions. To break linearity, it’s time to add *activation
    functions*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以向神经网络添加更多节点，但它们仍然存在于线性函数的分组中。为了打破线性，现在是时候添加*激活函数*了。
- en: Activation functions work similarly to neurons in the brain. Yes, this analogy
    again. When a neuron electrochemically receives a signal, it doesn’t always activate.
    There’s a threshold needed before a neuron fires its action potential. Similarly,
    neural networks have a degree of bias and similar on/off action potentials that
    occur when they reach their threshold due to incoming signals (similar to depolarizing
    current). Succinctly put, activation functions make neural networks capable of
    nonlinear predictions.^([2](ch08.html#idm45049242627032))
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数类似于大脑中的神经元。是的，这个比喻又来了。当一个神经元在电化学上接收到信号时，并不总是激活。在神经元发射动作电位之前，需要一个阈值。同样地，神经网络具有一定程度的偏见和类似于开关的动作电位，当它们达到由于传入信号而引起的阈值时发生（类似于去极化电流）。简而言之，激活函数使神经网络能够进行非线性预测。^([2](ch08.html#idm45049242627032))
- en: Note
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There are smarter ways to solve quadratic functions if you know that you want
    your solution to be quadratic. The way you will solve for X² in this section is
    orchestrated explicitly for learning more about TensorFlow.js, rather than solving
    for simple mathematical functions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您知道您的解决方案需要是二次的，那么有更聪明的方法来解决二次函数。在本节中，您将为X²解决问题，这是专门为了更多地了解TensorFlow.js而编排的，而不是为了解决简单的数学函数。
- en: Yes, this exercise could be solved easily without using AI, but what fun would
    that be?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这个练习可以很容易地在不使用AI的情况下解决，但那样有什么乐趣呢？
- en: Gathering the Data
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集数据
- en: Exponential functions can return some pretty large numbers, and one of the tricks
    to speed up model training is to keep numbers and their distance between each
    other small. You’ll see this time and time again. For our purposes, the training
    data for the model will be numbers between 0 and 10.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 指数函数可能返回一些非常大的数字，加快模型训练速度的一个技巧是保持数字及其之间的距离较小。您会一次又一次地看到这一点。对于我们的目的，模型的训练数据将是0到10之间的数字。
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This code prepares two tensors. The `xs` tensor is a grouping of 10,000 values,
    and `ys` is the square of these.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码准备了两个张量。`xs`张量是10,000个值的分组，`ys`是这些值的平方。
- en: Adding Activations to Neurons
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向神经元添加激活
- en: Choosing your activation functions for the neurons in a given layer, and your
    model size, is a science in itself. It depends on your goals, your data, and your
    knowledge. Just like with code, you can come up with several solutions that all
    work nearly as well as another. It’s experience and practice that help you find
    solutions that fit.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为给定层中的神经元选择激活函数以及您的模型大小本身就是一门科学。这取决于您的目标、您的数据和您的知识。就像编码一样，您可以提出几种几乎同样有效的解决方案。经验和实践将帮助您找到适合的解决方案。
- en: When adding activation, it’s important to note there are quite a few activation
    functions built into TensorFlow.js. One of the most popular activation functions
    is called ReLU, which stands for Rectified Linear Unit. As you might have gathered
    from the name, it comes from the heart of scientific terminology rather than witty
    NPM package names. There is all kinds of literature on the benefits of using ReLU
    over various other activation functions for some models. You must know ReLU is
    a popular choice for an activation function, and you’re likely to be just fine
    starting with it. You should feel free to experiment with other activation functions
    as you learn more about model architecture. ReLU helps models train faster, compared
    to many alternatives.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加激活时，重要的是要注意TensorFlow.js中内置了许多激活函数。其中最流行的激活函数之一被称为ReLU，代表修正线性单元。正如您可能从名称中推断出的那样，它来自科学术语的核心，而不是机智的NPM软件包名称。有各种文献讨论了在某些模型中使用ReLU相对于其他激活函数的好处。您必须知道ReLU是激活函数的一个流行选择，开始使用它应该没问题。随着您对模型架构的了解越来越多，您应该随意尝试其他激活函数。与许多其他选择相比，ReLU有助于模型更快地训练。
- en: In the last model, you had only a single node and an output. Now it’s important
    to grow the size of the network. There’s no formula for what size to use, so the
    first phase of each problem usually takes a bit of experimenting. For our purposes,
    we’ll increase the model with one dense layer of 20 neurons. A dense layer means
    that every node in that layer is connected to each node in the layers before and
    after it. The resulting model looks like [Figure 8-6](#nn_x_quad1).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个模型中，您只有一个节点和一个输出。现在增加网络的大小变得重要。没有一个固定的大小公式可供使用，因此每个问题的第一阶段通常需要一些试验。为了我们的目的，我们将增加一个包含20个神经元的密集层。密集层意味着该层中的每个节点都连接到其之前和之后的每个节点。生成的模型看起来像[图8-6](#nn_x_quad1)。
- en: '![Curent Neural Network shape and layers](assets/ltjs_0806.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![当前神经网络形状和层](assets/ltjs_0806.png)'
- en: Figure 8-6\. Neural network architecture (20 neurons)
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-6. 神经网络架构（20个神经元）
- en: To tour the architecture displayed in [Figure 8-6](#nn_x_quad1) from left to
    right, one number enters the network, the 20-neuron layer is called a *hidden*
    layer, and the resulting value is output in the final layer. Hidden layers are
    the layers between the input and output. These hidden layers add trainable neurons
    and make the model able to process more complex patterns.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从左到右浏览[图8-6](#nn_x_quad1)中显示的架构，一个数字进入网络，20个神经元的层被称为*隐藏*层，最终值输出在最后一层。隐藏层是输入和输出之间的层。这些隐藏层添加了可训练的神经元，并使模型能够处理更复杂的模式。
- en: 'To add this layer and provide it with an activation function, you’ll specify
    a new dense layer in the sequence:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 要添加这一层并为其提供激活函数，您将在序列中指定一个新的密集层：
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_training_models_CO2-1)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_training_models_CO2-1)'
- en: The first layer defines the input tensor as a single number.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层将输入张量定义为一个单一数字。
- en: '[![2](assets/2.png)](#co_training_models_CO2-2)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_training_models_CO2-2)'
- en: Specify the layer should be 20 nodes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 指定层应该有20个节点。
- en: '[![3](assets/3.png)](#co_training_models_CO2-3)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_training_models_CO2-3)'
- en: Specify a fancy activation function for your layer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为您的层指定一个花哨的激活函数。
- en: '[![4](assets/4.png)](#co_training_models_CO2-4)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_training_models_CO2-4)'
- en: Add the final single-unit layer for the output value.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 添加最终的单单元层以获取输出值。
- en: If you compile the model and print the summary, you’ll see output similar to
    [Example 8-3](#exp_summary_output).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您编译模型并打印摘要，您将看到类似于[示例8-3](#exp_summary_output)的输出。
- en: Example 8-3\. Calling `model.summary()` for the current structure
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-3。调用`model.summary()`以获取当前结构
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This model architecture has two layers that match the previous layer-creation
    code. The `null` sections represent the batch size, and since that can be any
    number, it is left blank. For example, the first layer is represented as `[null,20]`,
    so a batch of four values would give the model the input of `[4, 20]`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型架构有两个与先前层创建代码匹配的层。`null`部分代表批量大小，由于它可以是任何数字，因此留空。例如，第一层表示为`[null,20]`，因此四个值的批次将为模型提供输入`[4,
    20]`。
- en: You’ll notice the model has a total of 61 tunable parameters. If you review
    the diagram in [Figure 8-6](#nn_x_quad1), you can do the lines and nodes to get
    the parameters. The first layer has 20 nodes and 20 lines to them, which is why
    it has 40 parameters. The second layer has 20 lines all going to a single node,
    which is why that has only 21 parameters. Your model is ready to train, but it’s
    significantly bigger this time.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到模型共有61个可调参数。如果您查看[图8-6](#nn_x_quad1)中的图表，您可以绘制线条和节点以获取参数。第一层有20个节点和20条线连接到它们，这就是为什么它有40个参数。第二层有20条线都连接到一个单个节点，这就是为什么只有21个参数。您的模型已准备好训练，但这次要大得多。
- en: If you make these changes and kick off training, you’ll likely hear your CPU/GPU
    fan spin up and see a bunch of nothing. It sounds like the computer might be training,
    but it sure would be nice to see some kind of progress.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您进行这些更改并开始训练，您可能会听到您的CPU/GPU风扇启动并看到一堆无用的东西。听起来计算机可能正在训练，但肯定很好看到某种进展。
- en: Watching Training
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 观看训练
- en: TensorFlow.js has all kinds of amazing tools for helping you identify progress
    on training. Most particularly, there is a property of the `fit` configuration
    called `callbacks`. Inside the `callbacks` object, you can tie in with certain
    life cycles of the training model and run whatever code you’d like.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow.js拥有各种令人惊奇的工具，可帮助您识别训练进度。特别是，`fit`配置的一个属性称为`callbacks`。在`callbacks`对象内部，您可以连接到训练模型的某些生命周期，并运行任何您想要的代码。
- en: As you’re already familiar with an epoch (one full run through the training
    data), that’s the moment you’ll use in this example. Here’s a terse but effective
    method for getting some kind of console messaging.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您已经熟悉一个epoch（对训练数据的完整运行），这是您在本例中将使用的时刻。这是一个简洁但有效的获取某种控制台消息的方法。
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_training_models_CO3-1)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_training_models_CO3-1)'
- en: Create the callback object that contains all the life cycle methods you’d like
    to tie into.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 创建包含您想要连接的所有生命周期方法的回调对象。
- en: '[![2](assets/2.png)](#co_training_models_CO3-2)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_training_models_CO3-2)'
- en: '`onEpochEnd` is one of the many identified life cycle callbacks that training
    supports. The others are enumerated in the [`fit` section of the framework](https://oreil.ly/NoVqS)
    documentation.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`onEpochEnd`是训练支持的许多已识别的生命周期回调之一。其他枚举在框架的[`fit`部分](https://oreil.ly/NoVqS)文档中。'
- en: '[![3](assets/3.png)](#co_training_models_CO3-3)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_training_models_CO3-3)'
- en: Print the values for review. Normally, you would do something more involved
    with this information.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 打印审查的值。通常，您会对这些信息进行更深入的处理。
- en: Note
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: An epoch can be redefined by setting the `stepsPerEpoch` number in the `fit`
    config. Using this variable, an epoch can become any number of training data.
    By default, this is set to `null`, and therefore an epoch is set to the quantity
    of unique samples in your training set divided by the batch size.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在`fit`配置中设置`stepsPerEpoch`数字，可以重新定义一个epoch。使用此变量，一个epoch可以成为任何数量的训练数据。默认情况下，这设置为`null`，因此一个epoch设置为您的训练集中唯一样本的数量除以批量大小。
- en: All that’s left to do is pass your object to the model’s `fit` configuration
    alongside your epochs, and you should see logs while your model is training.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下要做的就是将您的对象传递给模型的`fit`配置，同时传递您的epochs，您应该在模型训练时看到日志。
- en: '[PRE9]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `onEpochEnd` callbacks print to your console, showing that the training
    is working. In [Figure 8-7](#sgd_loss), you can see your epoch and your log object.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`onEpochEnd`回调会打印到您的控制台，显示训练正在进行。在[图8-7](#sgd_loss)中，您可以看到您的epoch和日志对象。'
- en: '![Log of training progress](assets/ltjs_0807.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![训练进度日志](assets/ltjs_0807.png)'
- en: Figure 8-7\. The `onEpochEnd` log for epochs 19 through 26
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-7。epochs 19到26的`onEpochEnd`日志
- en: It’s a breath of fresh air to be able to see the model is actually training
    and to even tell what epoch it’s on. But, what’s going on with the log values?
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 能够看到模型实际上正在训练，甚至能够知道它所处的epoch，这真是一种清新的感觉。但是，日志值是怎么回事？
- en: Model logs
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型日志
- en: A model is told how to define loss with a loss function. What you want to see
    in each epoch is that the loss goes down. The loss is not only “is this right
    or wrong?” It’s about how wrong the model was so that it can learn. After each
    epoch, the model is reporting the loss, and in a good model architecture, this
    number goes down quickly.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 模型通过损失函数告知如何定义损失。您希望在每个epoch中看到的是损失下降。损失不仅仅是“对还是错？”它是关于模型有多错，以便它可以学习。每个epoch之后，模型都会报告损失，在一个良好的模型架构中，这个数字会迅速下降。
- en: You’re probably interested in seeing the accuracy. Most of the time, accuracy
    is a great metric, and we could enable accuracy in the logs. However, for a model
    like this, accuracy isn’t a very good fit as a metric. For instance, if you asked
    the model what the predicted output for `[7]` should be and the model answers
    `49.0676842` instead of `49`, its accuracy is zero because it was incorrect. While
    the close result would have a low loss and would be accurate *after* rounding,
    it’s technically wrong, and the accuracy score of the model would be poor. Let’s
    enable accuracy later when it works more effectively.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能对查看准确性感兴趣。大多数情况下，准确性是一个很好的指标，我们可以在日志中启用准确性。但是，对于这样的模型，准确性并不是一个很好的指标。例如，如果您问模型预测`[7]`的输出应该是多少，而模型回答`49.0676842`而不是`49`，那么它的准确性为零，因为它是错误的。虽然接近的结果在四舍五入后会有较低的损失并且准确，但从技术上讲，它是错误的，模型的准确性评分会很差。让我们在它更有效时再启用准确性。
- en: Improving Training
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进训练
- en: The loss values are pretty high. What is a high loss value? Concretely, it depends
    on the problem. However, when you see 800+ in error values, it’s generally safe
    to say it’s not done training.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 损失值相当高。什么是高损失值？具体而言，这取决于问题。但是，当您看到错误值为800+时，通常可以说训练尚未完成。
- en: Adam optimizer
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Adam优化器
- en: Fortunately, you don’t have to leave your computer training for weeks. Currently,
    the optimizer is set to the defaults of stochastic gradient descent (`sgd`). You
    can modify the `sgd` presets or even choose a different optimizer. One of the
    most popular optimizers is called Adam. If you’re interested in trying Adam, you
    don’t have to read [the paper on Adam published in 2015](https://arxiv.org/pdf/1412.6980.pdf);
    you simply need to change the value of `sgd` to `adam`, and now you’re good to
    go. This is where you can enjoy the benefit of a framework. Simply by changing
    a small string, your entire model architecture has changed. Adam has significant
    benefits for solving certain types of problems.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，您不必让计算机训练几周。目前，优化器设置为随机梯度下降（`sgd`）的默认值。您可以修改`sgd`预设，甚至选择不同的优化器。最受欢迎的优化器之一称为Adam。如果您有兴趣尝试Adam，您不必阅读[2015年发表的Adam论文](https://arxiv.org/pdf/1412.6980.pdf)；您只需将`sgd`的值更改为`adam`，然后您就可以开始了。这是您可以享受框架优势的地方。只需更改一个小字符串，整个模型架构就已更改。Adam对解决某些类型的问题具有显著的好处。
- en: 'The updated compile code looks like this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的编译代码如下：
- en: '[PRE10]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: With the new optimizer, the loss drops below 800 within a few epochs, and it
    even drops below one, as you can see in [Figure 8-8](#adam_loss).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用新的优化器，损失在几个时期内降至800以下，甚至降至1以下，如您在[图8-8](#adam_loss)中所见。
- en: '![Log of training progress](assets/ltjs_0808.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![训练进度日志](assets/ltjs_0808.png)'
- en: Figure 8-8\. The `onEpochEnd` log for epochs 19 through 26
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-8。时期19到26的`onEpochEnd`日志
- en: After 100 epochs, the model was still making progress for me but stopped at
    a loss value of `0.03833026438951492`. This varies on each run, but as long as
    the loss is small, the model will work.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 经过100个时期，模型对我来说仍在取得进展，但在损失值为`0.03833026438951492`时停止。每次运行都会有所不同，但只要损失很小，模型就会正常工作。
- en: Tip
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The practice of modifying and adjusting the model architecture to train or converge
    faster for a particular problem is a mix of experience and experiments.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 修改和调整模型架构以便为特定问题更快地训练或收敛是经验和实验的结合。
- en: 'Things are looking good, but there’s one more feature we should add that sometimes
    cuts training time down significantly. On a pretty decent machine, these 100 epochs
    take around 100 seconds to run. You can speed up your training by batching data
    with a single line. When you assign the `batchSize` property to the `fit` configuration,
    training gets substantially faster. Try adding a batch size to your fit call:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 情况看起来不错，但还有一个功能我们应该添加，有时可以显著缩短训练时间。在一台相当不错的机器上，这100个时期大约需要100秒才能运行。您可以通过一行批处理数据来加快训练速度。当您将`batchSize`属性分配给`fit`配置时，训练速度会大大加快。尝试在fit调用中添加批处理大小：
- en: '[PRE11]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_training_models_CO4-1)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_training_models_CO4-1)'
- en: This `batchSize` of 64 cut training from 100 seconds to 50 for my machine.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我的机器，64的`batchSize`将训练时间从100秒减少到50秒。
- en: Note
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Batch sizes are trade-offs of efficiency for memory. If the batch is too large,
    this will limit which machines are capable of running the training.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理大小是效率和内存之间的权衡。如果批处理太大，这将限制能够运行训练的机器。
- en: You have a model that trains within a reasonable time for next to no cost on
    size. However, increasing the batch size is an option you can and should review.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个在合理时间内训练的模型，几乎没有额外的成本。但是，增加批处理大小是一个您可以并且应该审查的选项。
- en: More nodes and layers
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多节点和层
- en: 'This whole time the model has been the same shape and size: one “hidden” layer
    of 20 nodes. Don’t forget, you can always add more layers. As an experiment, add
    another layer of 20 nodes, so your model architecture looks like [Figure 8-9](#nn_x_quad2).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 整个时间内，模型的形状和大小一直是相同的：一个包含20个节点的“隐藏”层。不要忘记，您可以随时添加更多层。作为一个实验，添加另一个包含20个节点的层，这样您的模型架构看起来像[图8-9](#nn_x_quad2)。
- en: '![Curent Neural Network shape and layers](assets/ltjs_0809.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![当前神经网络形状和层](assets/ltjs_0809.png)'
- en: Figure 8-9\. Neural network architecture (20 × 20 hidden nodes)
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-9。神经网络架构（20×20隐藏节点）
- en: 'With the Layers model architecture, you can build this model by adding a new
    layer. See the following code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Layers模型架构，您可以通过添加一个新层来构建这个模型。请参阅以下代码：
- en: '[PRE12]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The resulting model trains slower, which makes sense, but also converges faster,
    which also makes sense. This bigger model generates the correct value for the
    input `[7]` with only 30 epochs in a training time of 20 seconds.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 结果模型训练速度较慢，这是有道理的，但也收敛速度更快，这也是有道理的。这个更大的模型在20秒的训练时间内只需30个时期就为输入`[7]`生成了正确的值。
- en: 'Putting it all together, your resulting code does the following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容放在一起，您的结果代码执行以下操作：
- en: Creates a significant dataset
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个重要的数据集
- en: Creates several deeply connected layers that utilize ReLU activation
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建几个深度连接的层，使用ReLU激活
- en: Sets the model to use advanced Adam optimization
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型设置为使用先进的Adam优化
- en: Trains the model using the data in 64-chunk batches and prints progress along
    the way
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用64块数据训练模型，并在途中打印进度
- en: 'The entire source code from start to finish looks like this:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 从头到尾的整个源代码如下：
- en: '[PRE13]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The printed result tensor is exceptionally close to `49`. The training works.
    While this has been a bit of a strange adventure, it highlighted part of the model
    creation and validation process. Building models is one of the skills you’ll acquire
    over time as you experiment with various data and its associated solutions.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 打印的结果张量与`49`非常接近。训练成功。虽然这是一次有点奇怪的冒险，但它突出了模型创建和验证过程的一部分。构建模型是你随着时间实验各种数据及其相关解决方案而获得的技能之一。
- en: In subsequent chapters, you’ll solve more elaborate but rewarding problems,
    like classification. Everything you’ve learned here will be a tool in your workbench.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将解决更复杂但有益的问题，比如分类。你在这里学到的一切将成为你工作台上的一种工具。
- en: Chapter Review
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 章节回顾
- en: You’ve entered the world of training models. The layer model structure is not
    only an understandable visual, but now it’s something you can comprehend and build
    on demand. Machine learning is very different from normal software development,
    but you’re on your way to comprehending the differences and benefits afforded
    by TensorFlow.js.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经进入了训练模型的世界。层模型结构不仅是一个可理解的视觉，现在它是你可以理解和按需构建的东西。机器学习与普通软件开发非常不同，但你正在逐渐理解TensorFlow.js所提供的差异和好处。
- en: 'Chapter Challenge: The Model Architect'
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 章节挑战：模型架构师
- en: Now it’s your turn to build a Layers model via specification. What does this
    model do? No one knows! It’s not going to be trained with any data at all. In
    this challenge, you’ll be tasked to build a model with all kinds of properties
    you might not understand, but you should be familiar enough to at least set up
    the model. This model will be the biggest you’ve created yet. Your model will
    have five inputs and four outputs with two layers between them. It will look like
    [Figure 8-10](#chap_challenge_nn).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在轮到你通过规范构建一个Layers模型了。这个模型做什么？没人知道！它不会用任何数据进行训练。在这个挑战中，你将被要求构建一个具有各种你可能不理解的属性的模型，但你应该足够熟悉以至少设置好模型。这个模型将是你迄今为止创建的最大模型。你的模型将有五个输入和四个输出，它们之间有两层。它看起来像[图8-10](#chap_challenge_nn)。
- en: '![Your chapter challenge](assets/ltjs_0810.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![你的章节挑战](assets/ltjs_0810.png)'
- en: Figure 8-10\. The Chapter Challenge model
  id: totrans-191
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-10\. 章节挑战模型
- en: 'Do the following in your model:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的模型中做以下操作：
- en: The input layer should have 5 units.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层应该有5个单元。
- en: The next layer should have 10 units and use *sigmoid* for activation.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一层应该有10个单元并使用*sigmoid*激活。
- en: The next layer should have 7 units and use ReLU activation.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一层应该有7个单元并使用ReLU激活。
- en: The final layer should have 4 units and use *softmax* for activation.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一层应该有4个单元并使用*softmax*激活。
- en: The model should use Adam optimization.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型应该使用Adam优化。
- en: The model should use the loss function `categoricalCrossentropy`.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型应该使用损失函数`categoricalCrossentropy`。
- en: Before building this model and looking at the summary, can you calculate how
    many trainable parameters the final model will have? That’s the total number of
    lines and circles from [Figure 8-10](#chap_challenge_nn), not counting the input.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建这个模型并查看摘要之前，你能计算出最终模型将有多少可训练参数吗？这是从[图8-10](#chap_challenge_nn)中的总行数和圆圈数，不包括输入。
- en: You can find the answer to this challenge in [Appendix B](app02.html#appendix_b).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[附录B](app02.html#appendix_b)中找到这个挑战的答案。
- en: Review Questions
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回顾问题
- en: 'Let’s review the lessons you’ve learned from the code you’ve written in this
    chapter. Take a moment to answer the following questions:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下你在本章编写的代码中学到的经验。花点时间回答以下问题：
- en: Why would the Chapter Challenge model *not* work with the training data from
    this chapter?
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么章节挑战模型*不*适用于本章的训练数据？
- en: What method can you call on a model to log and review its structure?
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以调用哪个方法来记录和审查模型的结构？
- en: Why do you add activation functions to layers?
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么要向层添加激活函数？
- en: How do you specify the input shape for the Layers model?
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何为Layers模型指定输入形状？
- en: What does `sgd` stand for?
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sgd`代表什么？'
- en: What is an epoch?
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是一个epoch？
- en: If a model has one input, then a layer of two nodes, and an output of two nodes,
    how many hidden layers are present?
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个模型有一个输入，然后是两个节点的一层，和两个节点的输出，那么有多少隐藏层？
- en: Solutions to these exercises are available in [Appendix A](app01.html#book_appendix).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可以在[附录A](app01.html#book_appendix)中找到。
- en: ^([1](ch08.html#idm45049242996728-marker)) Supported optimizers are listed in
    tfjs-core’s [optimizers folder](https://oreil.ly/vnmcI).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch08.html#idm45049242996728-marker)) 支持的优化器列在tfjs-core的[optimizers文件夹](https://oreil.ly/vnmcI)中。
- en: ^([2](ch08.html#idm45049242627032-marker)) Learn more about [activation functions](https://youtu.be/Xvg00QnyaIY)
    from Andrew Ng.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch08.html#idm45049242627032-marker)) 从Andrew Ng了解更多关于[激活函数](https://youtu.be/Xvg00QnyaIY)的知识。
