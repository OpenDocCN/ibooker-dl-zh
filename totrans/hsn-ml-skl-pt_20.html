<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 18. Autoencoders, GANs, and Diffusion Models"><div class="chapter" id="autoencoders_chapter">
<h1><span class="label">Chapter 18. </span>Autoencoders, GANs, and Diffusion Models</h1>


<p>Autoencoders<a data-type="indexterm" data-primary="autoencoders" id="id3994"/> are artificial neural networks capable of learning dense representations of the input data<a data-type="indexterm" data-primary="latent representation of inputs" id="id3995"/>, called <em>latent representations</em> or <em>codings</em>, without any supervision (i.e., the training set is unlabeled). These codings<a data-type="indexterm" data-primary="codings, autoencoder’s unsupervised ability with" id="id3996"/> typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see <a data-type="xref" href="ch07.html#dimensionality_chapter">Chapter 7</a>), especially for visualization purposes. Autoencoders also act as feature detectors, and they can be used for unsupervised pretraining of deep neural networks (as we discussed in <a data-type="xref" href="ch11.html#deep_chapter">Chapter 11</a>). They are also commonly used for anomaly detection, as we will see. Lastly, some autoencoders are <em>generative models</em>: they are capable of randomly generating new data that looks very similar to the training data. For example, you could train an autoencoder on pictures of faces, and it would then be able to generate new faces.</p>

<p><em>Generative adversarial networks</em> (GANs)<a data-type="indexterm" data-primary="generative adversarial networks (GANs)" id="id3997"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="GANs" id="id3998"/><a data-type="indexterm" data-primary="deep neural networks (DNNs)" data-secondary="GANs" id="id3999"/><a data-type="indexterm" data-primary="GANs (generative adversarial networks)" id="id4000"/> are also neural nets capable of generating data. In fact, they can generate pictures of faces so convincing that it is hard to believe the people they represent do not exist. You can judge for yourself by visiting <a href="https://thispersondoesnotexist.com" class="bare"><em class="hyperlink">https://thispersondoesnotexist.com</em></a>, a website that shows faces generated by a GAN architecture<a data-type="indexterm" data-primary="StyleGANs" id="id4001"/><a data-type="indexterm" data-primary="generative models" id="id4002"/> called <em>StyleGAN</em>. GANs have been widely used for super resolution (increasing the resolution of an image), <a href="https://github.com/jantic/DeOldify">colorization</a>, powerful image editing (e.g., replacing photo bombers with realistic background), turning simple sketches into photorealistic images, predicting the next frames in a video, augmenting a dataset (to train other models), generating other types of data (such as text, audio, and time series), identifying the weaknesses in other models to strengthen them, and more.</p>

<p>However, since the early 2020s, GANs have been largely replaced by <em>diffusion models</em>, which can generate more diverse and higher-quality images than GANs, while also being much easier to train. However, diffusion models are much slower to run, so GANs are still useful when you need very fast generation.</p>

<p>Autoencoders, GANs, and diffusion models are all unsupervised, learn latent representations, can be used as generative models<a data-type="indexterm" data-primary="generative models" id="id4003"/>, and have many similar applications. However, they work very differently:</p>
<dl>
<dt>Autoencoders</dt>
<dd>
<p>Autoencoders<a data-type="indexterm" data-primary="autoencoders" id="id4004"/> simply learn to copy their inputs to their outputs. This may sound like a trivial task, but as you will see, constraining the network in various ways can make the task arbitrarily difficult. For example, you can limit the size of the latent representations, or you can add noise to the inputs and train the network to recover the original inputs. These constraints prevent the autoencoder from trivially copying the inputs directly to the outputs, which forces it to learn efficient ways of representing the data. In short, the codings are byproducts of the autoencoder learning the identity function under some constraints.</p>
</dd>
<dt>GANs</dt>
<dd>
<p>GANs<a data-type="indexterm" data-primary="generative adversarial networks (GANs)" id="id4005"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="GANs" id="id4006"/><a data-type="indexterm" data-primary="deep neural networks (DNNs)" data-secondary="GANs" id="id4007"/><a data-type="indexterm" data-primary="GANs (generative adversarial networks)" id="id4008"/> are composed of two neural networks: a <em>generator</em> that tries to generate<a data-type="indexterm" data-primary="generator, GAN" id="id4009"/> data that looks similar to the training data, and a <em>discriminator</em> that tries to tell real data<a data-type="indexterm" data-primary="discriminator, GAN" id="id4010"/> from fake data. This architecture is very original in deep learning in that the generator and the discriminator compete against each other during training; this<a data-type="indexterm" data-primary="adversarial learning" id="id4011"/> is called <em>adversarial training</em>. The generator is often compared to a criminal trying to make realistic counterfeit money, while the discriminator is like the police investigator trying to tell real money from fake.</p>
</dd>
<dt>Diffusion models</dt>
<dd>
<p>A diffusion model is trained to gradually remove noise from an image. If you then take an image entirely full of random noise and repeatedly run the diffusion model<a data-type="indexterm" data-primary="diffusion models" id="id4012"/> on that image, a high-quality image will gradually emerge, similar to the training images (but not identical).</p>
</dd>
</dl>

<p>In this chapter we will start by exploring in more depth how autoencoders work and how to use them for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. This will naturally lead us to GANs. We will build a simple GAN to generate fake images, but we will see that training is often quite difficult. We will discuss the main difficulties you will encounter with adversarial training, as well as some of the main techniques to work around these difficulties. And lastly, we will build and train a diffusion model—specifically<a data-type="indexterm" data-primary="DDPM (Denoising Diffusion Probabilistic Model)" id="id4013"/><a data-type="indexterm" data-primary="denoising diffusion probabilistic model (DDPM)" id="id4014"/> a <em>denoising diffusion probabilistic model</em> (DDPM)—and use it to generate images. Let’s start with 
<span class="keep-together">autoencoders!</span></p>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Efficient Data Representations"><div class="sect1" id="id334">
<h1>Efficient Data Representations</h1>

<p>Which of the following number sequences<a data-type="indexterm" data-primary="autoencoders" data-secondary="efficient data representations" id="xi_autoencodersefficientdatarepresentations182640_1"/><a data-type="indexterm" data-primary="efficient data representations, autoencoders" id="xi_efficientdatarepresentationsautoencoders182640_1"/><a data-type="indexterm" data-primary="data" data-secondary="efficient data representations" id="xi_dataefficientdatarepresentations182640_1"/><a data-type="indexterm" data-primary="autoencoders" id="xi_autoencoders182640_1"/> do you find the easiest to memorize?</p>

<ul>
<li>
<p>40, 27, 25, 36, 81, 57, 10, 73, 19, 68</p>
</li>
<li>
<p>50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14</p>
</li>
</ul>

<p>At first glance, it would seem that the first sequence should be easier, since it is much shorter. However, if you look carefully at the second sequence, you will notice that it is just the list of even numbers from 50 down to 14. Once you notice this pattern, the second sequence becomes much easier to memorize than the first because you only need to remember the pattern (i.e., decreasing even numbers) and the starting and ending numbers (i.e., 50 and 14). Note that if you could quickly and easily memorize very long sequences, you would not care much about the existence of a pattern in the second sequence. You would just learn every number by heart, and that would be that. The fact that it is hard to memorize long sequences is what makes it useful to recognize patterns, and hopefully this clarifies why constraining an autoencoder during training pushes it to discover and exploit patterns in the data.</p>

<p>The relationship among memory, perception, and pattern matching was famously studied by <a href="https://homl.info/111">William Chase and Herbert Simon</a>⁠<sup><a data-type="noteref" id="id4015-marker" href="ch18.html#id4015">1</a></sup> in the early 1970s. They observed that expert chess players were able to memorize the positions of all the pieces in a game by looking at the board for just five seconds, a task that most people would find impossible. However, this was only the case when the pieces were placed in realistic positions (from actual games), not when the pieces were placed randomly. Chess experts don’t have a much better memory than you and I; they just see chess patterns more easily, thanks to their experience with the game. Noticing patterns helps them store information efficiently.</p>

<p>Just like the chess players in this memory experiment, an autoencoder looks at the inputs, converts them to an efficient latent representation<a data-type="indexterm" data-primary="latent representation of inputs" id="id4016"/>, and is then capable of reconstructing something that (hopefully) looks very close to the inputs. An autoencoder is always composed of two parts<a data-type="indexterm" data-primary="encoder" data-seealso="autoencoders" id="id4017"/>: an <em>encoder</em> (or <em>recognition network</em>)<a data-type="indexterm" data-primary="recognition network" data-seealso="autoencoders" id="id4018"/> that converts the inputs to a latent representation, followed<a data-type="indexterm" data-primary="decoder" id="id4019"/> by a <em>decoder</em> (or <em>generative network</em>) that converts the internal representation to the outputs.</p>

<p>In the example shown in <a data-type="xref" href="#encoder_decoder_diagram">Figure 18-1</a>, the autoencoder is a regular<a data-type="indexterm" data-primary="multilayer perceptrons (MLPs)" data-secondary="and autoencoders" data-secondary-sortas="autoencoders" id="id4020"/> multilayer perceptron (MLP; see <a data-type="xref" href="ch09.html#ann_chapter">Chapter 9</a>). Since it must reconstruct its inputs, the number of neurons in the output layer must be equal to the number of inputs (i.e., three in this example). The lower part of the network is the encoder (in this case it’s a single layer with two neurons), and the upper part is the decoder. The outputs are often called the <em>reconstructions</em> because the autoencoder tries to reconstruct the inputs. The cost function always contains<a data-type="indexterm" data-primary="reconstruction loss" id="id4021"/> a <em>reconstruction loss</em> that penalizes the model when the reconstructions are different from the inputs.</p>

<figure class="smallerseventy"><div id="encoder_decoder_diagram" class="figure">
<img src="assets/hmls_1801.png" alt="Illustration showing a chess memory experiment with a chessboard transitioning to a latent representation, and a diagram of an autoencoder with an encoder layer reducing inputs to two neurons and a decoder layer reconstructing three outputs." width="1252" height="1077"/>
<h6><span class="label">Figure 18-1. </span>The chess memory experiment (left) and a simple autoencoder (right)</h6>
</div></figure>

<p>Because the internal representation has a lower dimensionality<a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="autoencoders" id="id4022"/> than the input data (in this example, it is 2D instead of 3D), the autoencoder is said to be <em>undercomplete</em>. An undercomplete<a data-type="indexterm" data-primary="autoencoders" data-secondary="undercomplete" id="id4023"/> autoencoder cannot trivially copy its inputs to the codings, yet it must find a way to output a copy of its inputs. It is forced to compress the data, thereby learning the most important features in the input data (and dropping the unimportant ones)<a data-type="indexterm" data-startref="xi_autoencodersefficientdatarepresentations182640_1" id="id4024"/><a data-type="indexterm" data-startref="xi_efficientdatarepresentationsautoencoders182640_1" id="id4025"/><a data-type="indexterm" data-startref="xi_dataefficientdatarepresentations182640_1" id="id4026"/>.</p>

<p>Let’s see how to implement a very simple undercomplete autoencoder for dimensionality reduction.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Performing PCA with an Undercomplete &#10;Linear Autoencoder"><div class="sect1" id="id335">
<h1>Performing PCA with an Undercomplete 
<span class="keep-together">Linear Autoencoder</span></h1>

<p>If the autoencoder<a data-type="indexterm" data-primary="autoencoders" data-secondary="PCA with undercomplete linear autoencoder" id="xi_autoencodersPCAwithundercompletelinearautoencoder184919_1"/><a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="with undercomplete linear autoencoder" data-secondary-sortas="undercomplete linear autoencoder" id="xi_principalcomponentanalysisPCAwithundercompletelinearautoencoder184919_1"/><a data-type="indexterm" data-primary="undercomplete, autoencoder as" id="xi_undercompleteautoencoderas184919_1"/><a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="undercomplete autoencoder" id="xi_dimensionalityreductionundercompleteautoencoder184919_1"/> uses only linear activations and the cost function is the mean squared error (MSE), then it ends up performing principal component analysis (PCA; see <a data-type="xref" href="ch07.html#dimensionality_chapter">Chapter 7</a>).</p>

<p>The following code builds a simple linear autoencoder that takes a 3D input, projects it down to 2D, then projects it back up to 3D. Since we will train the model using targets equal to the inputs, gradient descent will have to find the 2D plane that lies closest to the training data, just like PCA would.</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>

<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">encoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
<code class="n">decoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
<code class="n">autoencoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">encoder</code><code class="p">,</code> <code class="n">decoder</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>This code is really not very different from all the MLPs we built in past chapters, but there are a few things to note:</p>

<ul>
<li>
<p>We organized the autoencoder into two subcomponents: the encoder and the decoder, each composed of a single <code translate="no">Linear</code> layer in this example, and the autoencoder is a <code translate="no">Sequential</code> model containing the encoder followed by the decoder.</p>
</li>
<li>
<p>The autoencoder’s number of outputs is equal to the number of inputs (i.e., 3).</p>
</li>
<li>
<p>To perform PCA, we do not use any activation function (i.e., all neurons are linear), and the cost function is the MSE. That’s because PCA is a linear transformation. We will see more complex and nonlinear autoencoders shortly.</p>
</li>
</ul>

<p>Now let’s train the model on the same simple generated 3D dataset we used in <a data-type="xref" href="ch07.html#dimensionality_chapter">Chapter 7</a> and use it to encode that dataset (i.e., project it to 2D):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code><code class="p">,</code> <code class="n">TensorDataset</code>

<code class="n">X_train</code> <code class="o">=</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># generate a 3D dataset, like in Chapter 7</code>
<code class="n">train_set</code> <code class="o">=</code> <code class="n">TensorDataset</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">X_train</code><code class="p">)</code>  <code class="c1"># the inputs are also the targets</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>

<p class="pagebreak-before">Note that <code translate="no">X_train</code> is used as both the inputs and the targets. Next, let’s train the autoencoder, using the same <code translate="no">train()</code> function as in <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter 10</a> (the notebook uses a slightly fancier function that prints some info and evaluates the model at each epoch):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torchmetrics</code>

<code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">NAdam</code><code class="p">(</code><code class="n">autoencoder</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.2</code><code class="p">)</code>
<code class="n">mse</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">MSELoss</code><code class="p">()</code>
<code class="n">rmse</code> <code class="o">=</code> <code class="n">torchmetrics</code><code class="o">.</code><code class="n">MeanSquaredError</code><code class="p">(</code><code class="n">squared</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
<code class="n">train</code><code class="p">(</code><code class="n">autoencoder</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">mse</code><code class="p">,</code> <code class="n">train_loader</code><code class="p">,</code> <code class="n">n_epochs</code><code class="o">=</code><code class="mi">20</code><code class="p">)</code></pre>

<p>Now that the autoencoder is trained, we can use its encoder to compress 3D inputs to 2D. For example, let’s compress the entire training set:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">codings</code> <code class="o">=</code> <code class="n">encoder</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">))</code></pre>

<p><a data-type="xref" href="#linear_autoencoder_pca_diagram">Figure 18-2</a> shows the original 3D dataset (on the left) and the output of the autoencoder’s hidden layer (i.e., the coding layer, on the right). As you can see, the autoencoder found the best 2D plane to project the data onto, preserving as much variance in the data as it could (just like PCA).</p>

<figure><div id="linear_autoencoder_pca_diagram" class="figure">
<img src="assets/hmls_1802.png" alt="Diagram comparing a 3D dataset and its 2D projection through an autoencoder, illustrating the principle of dimensionality reduction similar to PCA." width="1589" height="662"/>
<h6><span class="label">Figure 18-2. </span>Approximate PCA performed by an undercomplete linear autoencoder</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You can think of an autoencoder as performing a form of self-supervised learning, since it is based on a supervised learning technique with automatically generated labels (in this case, simply equal to the inputs)<a data-type="indexterm" data-startref="xi_autoencodersPCAwithundercompletelinearautoencoder184919_1" id="id4027"/><a data-type="indexterm" data-startref="xi_principalcomponentanalysisPCAwithundercompletelinearautoencoder184919_1" id="id4028"/><a data-type="indexterm" data-startref="xi_undercompleteautoencoderas184919_1" id="id4029"/><a data-type="indexterm" data-startref="xi_dimensionalityreductionundercompleteautoencoder184919_1" id="id4030"/>.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Stacked Autoencoders"><div class="sect1" id="id336">
<h1>Stacked Autoencoders</h1>

<p>Just like other neural networks<a data-type="indexterm" data-primary="autoencoders" data-secondary="stacked" id="xi_autoencodersstacked1810932_1"/><a data-type="indexterm" data-primary="stacked autoencoders" id="xi_stackedautoencoders1810932_1"/> we have discussed, autoencoders can have multiple hidden layers<a data-type="indexterm" data-primary="hidden layers" data-secondary="stacked autoencoders" id="xi_hiddenlayersstackedautoencoders1810996_1"/>. In this case they are called <em>stacked autoencoders</em> (or <em>deep autoencoders</em>). Adding more layers helps the autoencoder learn more complex codings. That said, one must be careful not to make the autoencoder too powerful. Imagine an encoder so powerful that it just learns to map each input to a single arbitrary number (and the decoder learns the reverse mapping). Obviously such an autoencoder will reconstruct the training data perfectly, but it will not have learned any useful data representation in the process, and is unlikely to generalize well to new instances.</p>

<p>The architecture of a stacked autoencoder is typically symmetrical with regard to the central hidden layer (the coding layer). To put it simply, it looks like a sandwich. For example, an autoencoder for Fashion MNIST (introduced in <a data-type="xref" href="ch09.html#ann_chapter">Chapter 9</a>) may have 784 inputs, followed by a hidden layer with 128 neurons, then a central hidden layer of 32 neurons, then another hidden layer with 128 neurons, and an output layer with 784 neurons. This stacked autoencoder is represented in <a data-type="xref" href="#stacked_autoencoder_diagram">Figure 18-3</a>. Note that all hidden layers must have an activation function, such as ReLU.</p>

<figure class="width-60"><div id="stacked_autoencoder_diagram" class="figure">
<img src="assets/hmls_1803.png" alt="Diagram of a stacked autoencoder architecture with an input layer of 784 units, three hidden layers (128, 32, and 100 units), and an output layer of 784 units, illustrating the symmetrical structure." width="743" height="598"/>
<h6><span class="label">Figure 18-3. </span>Stacked autoencoder</h6>
</div></figure>








<section data-type="sect2" data-pdf-bookmark="Implementing a Stacked Autoencoder Using PyTorch"><div class="sect2" id="id337">
<h2>Implementing a Stacked Autoencoder Using PyTorch</h2>

<p>You can implement<a data-type="indexterm" data-primary="stacked autoencoders" data-secondary="implementing" id="id4031"/> a stacked autoencoder very much like a regular deep MLP<a data-type="indexterm" data-primary="PyTorch" data-secondary="stacked encoder implementation" id="id4032"/>. For example, here is an autoencoder you can use to process Fashion MNIST images:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">stacked_encoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">128</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">32</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
<code class="p">)</code>
<code class="n">stacked_decoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="mi">128</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Unflatten</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">unflattened_size</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">))</code>
<code class="p">)</code>
<code class="n">stacked_ae</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">stacked_encoder</code><code class="p">,</code> <code class="n">stacked_decoder</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>Let’s go through this code:</p>

<ul>
<li>
<p>Just like earlier, we split the autoencoder model into two submodels: the encoder and the decoder.</p>
</li>
<li>
<p>The encoder takes 28 × 28 pixel grayscale images (i.e., with a single channel), flattens them so that each image is represented as a vector of size 784, then processes these vectors through 2 <code translate="no">Linear</code> layers of diminishing sizes (128 units, then 32 units), each followed by the ReLU activation function. For each input image, the encoder outputs a vector of size 32.</p>
</li>
<li>
<p>The decoder takes codings of size 32 (output by the encoder) and processes them through 2 <code translate="no">Linear</code> layers of increasing sizes (128 units, then 784 units), and reshapes the final vectors into 1 × 28 × 28 arrays so the decoder’s outputs have the same shape as the encoder’s inputs. Note that we use the sigmoid<a data-type="indexterm" data-primary="sigmoid activation function" data-secondary="stacked encoder implementation" id="id4033"/> function for the output layer instead of ReLU to ensure that the output pixel values range between 0 and 1<a data-type="indexterm" data-startref="xi_hiddenlayersstackedautoencoders1810996_1" id="id4034"/>.</p>
</li>
</ul>

<p>We can now load the Fashion MNIST dataset using the TorchVision library and split it into <code translate="no">train_data</code>, <code translate="no">valid_data</code>, and <code translate="no">test_data</code> (just like we did in <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter 10</a>), then train the autoencoder exactly like the previous autoencoder, using the inputs as the targets and minimizing the MSE loss. Give it a try, it’s a good exercise! Don’t forget to change the targets so they match the inputs—we’re training an autoencoder, not a classifier.⁠<sup><a data-type="noteref" id="id4035-marker" href="ch18.html#id4035">2</a></sup> If you get stuck, please check out the implementation in this chapter’s notebook.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Visualizing the Reconstructions"><div class="sect2" id="id338">
<h2>Visualizing the Reconstructions</h2>

<p>Once you have trained the stacked autoencoder, how do you know if it’s any good? One way to check<a data-type="indexterm" data-primary="stacked autoencoders" data-secondary="reconstruction visualization" id="id4036"/> that an autoencoder<a data-type="indexterm" data-primary="visualization of data" data-secondary="stacked autoencoders" id="xi_visualizationofdatastackedautoencoders1814538_1"/> is properly trained is to compare the inputs and the outputs: the differences should not be too significant. Let’s plot a few images from the validation set, as well as their reconstructions:</p>

<pre translate="no" data-type="programlisting" data-code-language="python" class="widows15"><code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>

<code class="k">def</code> <code class="nf">plot_image</code><code class="p">(</code><code class="n">image</code><code class="p">):</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image</code><code class="o">.</code><code class="n">permute</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">cpu</code><code class="p">(),</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"binary"</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">plot_reconstructions</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">images</code><code class="p">,</code> <code class="n">n_images</code><code class="o">=</code><code class="mi">5</code><code class="p">):</code>
    <code class="n">images</code> <code class="o">=</code> <code class="n">images</code><code class="p">[:</code><code class="n">n_images</code><code class="p">]</code>
    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
        <code class="n">y_pred</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">images</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">))</code>

    <code class="n">fig</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">images</code><code class="p">)</code> <code class="o">*</code> <code class="mf">1.5</code><code class="p">,</code> <code class="mi">3</code><code class="p">))</code>
    <code class="k">for</code> <code class="n">idx</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">images</code><code class="p">)):</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">subplot</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">images</code><code class="p">),</code> <code class="mi">1</code> <code class="o">+</code> <code class="n">idx</code><code class="p">)</code>
        <code class="n">plot_image</code><code class="p">(</code><code class="n">images</code><code class="p">[</code><code class="n">idx</code><code class="p">])</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">subplot</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">images</code><code class="p">),</code> <code class="mi">1</code> <code class="o">+</code> <code class="nb">len</code><code class="p">(</code><code class="n">images</code><code class="p">)</code> <code class="o">+</code> <code class="n">idx</code><code class="p">)</code>
        <code class="n">plot_image</code><code class="p">(</code><code class="n">y_pred</code><code class="p">[</code><code class="n">idx</code><code class="p">])</code>

<code class="n">X_valid</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">stack</code><code class="p">([</code><code class="n">x</code> <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">_</code> <code class="ow">in</code> <code class="n">valid_data</code><code class="p">])</code>
<code class="n">plot_reconstructions</code><code class="p">(</code><code class="n">stacked_ae</code><code class="p">,</code> <code class="n">X_valid</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure class="smallerseventy"><div id="reconstruction_plot" class="figure">
<img src="assets/hmls_1804.png" alt="Original images of clothing and footwear are displayed on top, with their slightly blurred reconstructions below, illustrating the results of a stacked autoencoder." width="1410" height="518"/>
<h6><span class="label">Figure 18-4. </span>Original images (top) and their reconstructions (bottom)</h6>
</div></figure>

<p><a data-type="xref" href="#reconstruction_plot">Figure 18-4</a> shows the resulting images. The reconstructions are recognizable, but a bit too lossy. We may need to train the model for longer, or make the encoder and decoder more powerful, or make the codings larger. For now, let’s go with this model and see how we can use it<a data-type="indexterm" data-startref="xi_visualizationofdatastackedautoencoders1814538_1" id="id4037"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Anomaly Detection Using Autoencoders"><div class="sect2" id="id339">
<h2>Anomaly Detection Using Autoencoders</h2>

<p>One common use case for autoencoders is anomaly detection<a data-type="indexterm" data-primary="stacked autoencoders" data-secondary="anomaly detection" id="id4038"/><a data-type="indexterm" data-primary="anomaly detection" data-secondary="autoencoders" id="id4039"/>. Indeed, if an autoencoder is given an image that doesn’t look like the images it was trained on (the image is said to be <em>out of distribution</em>)<a data-type="indexterm" data-primary="out of distribution, image as" id="id4040"/>, then the reconstruction will be terrible. For example, <a data-type="xref" href="#bad_reconstructions_plot">Figure 18-5</a> shows some MNIST digits and their reconstructions using the model we just trained on Fashion MNIST. As you can see, these reconstructions are very different from the inputs. If you compute the reconstruction loss<a data-type="indexterm" data-primary="reconstruction loss" id="id4041"/> (i.e., the MSE between the input and the output), it will be very high. To use the model for anomaly detection, you simply need to define a threshold, then any image whose reconstruction loss is greater than that threshold can be considered an anomaly.</p>

<figure class="smallerseventy"><div id="bad_reconstructions_plot" class="figure">
<img src="assets/hmls_1805.png" alt="Example showing poor reconstructions of MNIST digit images using a model trained on Fashion MNIST, highlighting how out-of-distribution inputs result in high reconstruction loss." width="1365" height="484"/>
<h6><span class="label">Figure 18-5. </span>Out-of-distribution images are poorly reconstructed</h6>
</div></figure>

<p>That’s all there is to it! Now let’s look at another use case for autoencoders.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Visualizing the Fashion MNIST Dataset"><div class="sect2" id="id340">
<h2>Visualizing the Fashion MNIST Dataset</h2>

<p>As we saw earlier in this chapter, undercomplete autoencoders<a data-type="indexterm" data-primary="stacked autoencoders" data-secondary="Fashion MNIST Dataset visualization" id="xi_stackedautoencodersFashionMNISTDatasetvisualization1819162_1"/><a data-type="indexterm" data-primary="Fashion MNIST dataset" data-secondary="visualizing" id="xi_FashionMNISTdatasetvisualizing1819162_1"/><a data-type="indexterm" data-primary="autoencoders" data-secondary="undercomplete" id="id4042"/> can be used for dimensionality<a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="autoencoders" id="xi_dimensionalityreductionautoencoders1819193_1"/> reduction. However, for most datasets they will not do a very good job at reducing the dimensionality down to two or three dimensions; they need enough dimensions to be able to properly reconstruct the inputs. As a result, they are generally not used directly for visualization. However, they are great at handling huge datasets, so one strategy is to use an autoencoder to reduce the dimensionality down to a reasonable level, then use another dimensionality <span class="keep-together">reduction</span> algorithm for visualization, such as those we discussed in <a data-type="xref" href="ch07.html#dimensionality_chapter">Chapter 7</a>.</p>

<p>Let’s use this strategy to visualize Fashion MNIST. First we’ll use the encoder from our stacked autoencoder to reduce the dimensionality down to 32, then we’ll use Scikit-Learn’s implementation of the t-SNE<a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="t-distributed stochastic neighbor embedding" id="id4043"/><a data-type="indexterm" data-primary="t-distributed stochastic neighbor embedding (t-SNE)" data-primary-sortas="tdistributed stochastic neighbor embedding (t-SNE)" id="id4044"/><a data-type="indexterm" data-primary="t-SNE (t-distributed stochastic neighbor embedding)" data-primary-sortas="tSNE (t-distributed stochastic neighbor embedding)" id="id4045"/><a data-type="indexterm" data-primary="embeddings" data-secondary="t-SNE" id="id4046"/> algorithm to reduce the dimensionality down to 2 for visualization:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="kn">import</code> <code class="n">TSNE</code>

<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">X_valid_compressed</code> <code class="o">=</code> <code class="n">stacked_encoder</code><code class="p">(</code><code class="n">X_valid</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">))</code>

<code class="n">tsne</code> <code class="o">=</code> <code class="n">TSNE</code><code class="p">(</code><code class="n">init</code><code class="o">=</code><code class="s2">"pca"</code><code class="p">,</code> <code class="n">learning_rate</code><code class="o">=</code><code class="s2">"auto"</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">X_valid_2D</code> <code class="o">=</code> <code class="n">tsne</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_valid_compressed</code><code class="o">.</code><code class="n">cpu</code><code class="p">())</code></pre>

<p>Now we can plot the dataset:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X_valid_2D</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_valid_2D</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">y_valid</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"tab10"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p><a data-type="xref" href="#fashion_mnist_visualization_plot">Figure 18-6</a> shows the resulting scatterplot, beautified a bit by displaying some of the images. The t-SNE algorithm identified several clusters that match the classes reasonably well (each class is represented by a different color). Note that t-SNE’s output can vary greatly if you run it with a different random seed, slightly different data, or on a different platform, so your plot may look different.</p>

<figure><div id="fashion_mnist_visualization_plot" class="figure">
<img src="assets/hmls_1806.png" alt="A scatterplot visualizing Fashion MNIST data via an autoencoder and t-SNE, showing clusters corresponding to different clothing items marked by images." width="1794" height="1450"/>
<h6><span class="label">Figure 18-6. </span>Fashion MNIST visualization using an autoencoder, followed by t-SNE</h6>
</div></figure>

<p>Next let’s look at how we can use autoencoders for unsupervised pretraining<a data-type="indexterm" data-startref="xi_dimensionalityreductionautoencoders1819193_1" id="id4047"/><a data-type="indexterm" data-startref="xi_stackedautoencodersFashionMNISTDatasetvisualization1819162_1" id="id4048"/><a data-type="indexterm" data-startref="xi_FashionMNISTdatasetvisualizing1819162_1" id="id4049"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Unsupervised Pretraining Using Stacked Autoencoders"><div class="sect2" id="id341">
<h2>Unsupervised Pretraining Using Stacked Autoencoders</h2>

<p>As we discussed in <a data-type="xref" href="ch11.html#deep_chapter">Chapter 11</a>, if you are tackling a complex supervised<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="pretraining" id="xi_unsupervisedlearningpretraining1822378_1"/><a data-type="indexterm" data-primary="pretraining and pretrained layers" data-secondary="in unsupervised learning" data-secondary-sortas="unsupervised learning" id="xi_pretrainingandpretrainedlayersinunsupervisedlearning1822378_1"/> task<a data-type="indexterm" data-primary="stacked autoencoders" data-secondary="unsupervised pretraining with" id="xi_stackedautoencodersunsupervisedpretrainingwith1822383_1"/><a data-type="indexterm" data-primary="pretraining and pretrained layers" data-secondary="stacked autoencoders" id="xi_pretrainingandpretrainedlayersstackedautoencoders1822383_1"/> but you do not have a lot of labeled training data, one solution is to find a neural network that performs a similar task and reuse its lower layers. This makes it possible to train a high-performance model using little training data because your neural network won’t have to learn all the low-level features; it will just reuse the feature detectors learned by the existing network.</p>

<p>Similarly, if you have a large dataset but most of it is unlabeled, you can first train a stacked autoencoder using all the data, then reuse the lower layers to create a neural network for your actual task and train it using the labeled data. For example, <a data-type="xref" href="#unsupervised_pretraining_autoencoders_diagram">Figure 18-7</a> shows how to use a stacked autoencoder to perform unsupervised pretraining for a classification neural network. When training the classifier, if you really don’t have much labeled training data, you may want to freeze the pretrained layers (at least the lower ones).</p>

<figure class="width-65"><div id="unsupervised_pretraining_autoencoders_diagram" class="figure">
<img src="assets/hmls_1807.png" alt="Diagram illustrating the process of using a stacked autoencoder for unsupervised pretraining, showing the transfer of trained parameters from a multi-layer setup in Phase 1 to a classifier in Phase 2." width="810" height="739"/>
<h6><span class="label">Figure 18-7. </span>Unsupervised pretraining using autoencoders</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Having plenty of unlabeled data<a data-type="indexterm" data-primary="labels" data-secondary="unlabeled data issue" id="id4050"/> and little labeled data is common. Building a large unlabeled dataset is often cheap (e.g., a simple script can download millions of images off the internet), but labeling those images (e.g., classifying them as cute or not) can usually be done reliably only by humans. Labeling instances is time-consuming and costly, so it’s normal to have only a few thousand human-labeled instances, or even less. That said, there is a growing trend toward using advanced AIs to label datasets.</p>
</div>

<p>There is nothing special about the implementation: just train an autoencoder using all the training data (labeled plus unlabeled), then reuse its encoder layers to create a new neural network and train it on the labeled instances (see the exercises at the end of this chapter for an example)<a data-type="indexterm" data-startref="xi_unsupervisedlearningpretraining1822378_1" id="id4051"/><a data-type="indexterm" data-startref="xi_pretrainingandpretrainedlayersinunsupervisedlearning1822378_1" id="id4052"/>.</p>

<p>Let’s now look at a few techniques for training stacked autoencoders<a data-type="indexterm" data-startref="xi_stackedautoencodersunsupervisedpretrainingwith1822383_1" id="id4053"/><a data-type="indexterm" data-startref="xi_pretrainingandpretrainedlayersstackedautoencoders1822383_1" id="id4054"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Tying Weights"><div class="sect2" id="id342">
<h2>Tying Weights</h2>

<p>When an autoencoder is neatly symmetrical<a data-type="indexterm" data-primary="weight-tying" id="xi_weighttying1823842_1"/><a data-type="indexterm" data-primary="stacked autoencoders" data-secondary="tying weights" id="xi_stackedautoencoderstyingweights1823842_1"/><a data-type="indexterm" data-primary="tying weights of layers" id="xi_tyingweightsoflayers1823842_1"/>, like the one we just built, a common technique is to <em>tie</em> the weights of the decoder layers to the weights of the encoder layers. This halves the number of weights in the model, speeding up training and limiting the risk of overfitting. Specifically, if the autoencoder has a total of <em>N</em> layers (not counting the input layer), and <strong>W</strong><sub><em>L</em></sub> represents the connection weights of the <em>L</em><sup>th</sup> layer (e.g., layer 1 is the first hidden layer, layer <em>N</em>/2 is the coding layer, and layer <em>N</em> is the output layer), then the decoder layer weights can be defined as <strong>W</strong><sub><em>L</em></sub> = <strong>W</strong><sub><em>N</em>–<em>L</em>+1</sub><sup>⊺</sup> (with <em>L</em> = <em>N</em> / 2 + 1, …​, <em>N</em>).</p>

<p>For example, here is the same autoencoder as the previous one, except the decoder weights are tied to the encoder weights:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch.nn.functional</code> <code class="k">as</code> <code class="nn">F</code>

<code class="k">class</code> <code class="nc">TiedAutoencoder</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">enc1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">128</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">enc2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">32</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">dec1_bias</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Parameter</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="mi">128</code><code class="p">))</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">dec2_bias</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Parameter</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">))</code>

    <code class="k">def</code> <code class="nf">encode</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">)</code>  <code class="c1"># flatten</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">F</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">enc1</code><code class="p">(</code><code class="n">Z</code><code class="p">))</code>
        <code class="k">return</code> <code class="n">F</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">enc2</code><code class="p">(</code><code class="n">Z</code><code class="p">))</code>

    <code class="k">def</code> <code class="nf">decode</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">F</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">F</code><code class="o">.</code><code class="n">linear</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">enc2</code><code class="o">.</code><code class="n">weight</code><code class="o">.</code><code class="n">t</code><code class="p">(),</code> <code class="bp">self</code><code class="o">.</code><code class="n">dec1_bias</code><code class="p">))</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">F</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">F</code><code class="o">.</code><code class="n">linear</code><code class="p">(</code><code class="n">Z</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">enc1</code><code class="o">.</code><code class="n">weight</code><code class="o">.</code><code class="n">t</code><code class="p">(),</code> <code class="bp">self</code><code class="o">.</code><code class="n">dec2_bias</code><code class="p">))</code>
        <code class="k">return</code> <code class="n">Z</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">)</code>  <code class="c1"># unflatten</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="n">X</code><code class="p">))</code></pre>

<p>This model achieves a smaller reconstruction error<a data-type="indexterm" data-primary="reconstruction error" id="id4055"/> than the previous model, using about half the number of parameters<a data-type="indexterm" data-startref="xi_weighttying1823842_1" id="id4056"/><a data-type="indexterm" data-startref="xi_stackedautoencoderstyingweights1823842_1" id="id4057"/><a data-type="indexterm" data-startref="xi_tyingweightsoflayers1823842_1" id="id4058"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Training One Autoencoder at a Time"><div class="sect2" id="id343">
<h2>Training One Autoencoder at a Time</h2>

<p>Rather than training<a data-type="indexterm" data-primary="autoencoders" data-secondary="training one at a time" id="xi_autoencoderstrainingoneatatime1827121_1"/> the whole stacked autoencoder<a data-type="indexterm" data-primary="pretraining and pretrained layers" data-secondary="greedy layer-wise pretraining" id="id4059"/><a data-type="indexterm" data-primary="stacked autoencoders" data-secondary="training one autoencoder at a time" id="id4060"/><a data-type="indexterm" data-primary="greedy layer-wise pretraining" id="id4061"/> in one go like we just did, it is possible to train one shallow autoencoder at a time, then stack all of them into a single stacked autoencoder (hence the name), as shown in <a data-type="xref" href="#stacking_autoencoders_diagram">Figure 18-8</a>. This technique is called <em>greedy layerwise training</em>.</p>

<p>During the first phase of training, the first autoencoder learns to reconstruct the inputs. Then we encode the whole training set using this first autoencoder, and this gives us a new (compressed) training set. We then train a second autoencoder on this new dataset. This is the second phase of training. Finally, we build a big sandwich using all these autoencoders, as shown in <a data-type="xref" href="#stacking_autoencoders_diagram">Figure 18-8</a> (i.e., we first stack the encoder layers of each autoencoder, then the decoder layers in reverse order). This gives us the final stacked autoencoder. We could easily train more autoencoders this way, building a very deep stacked autoencoder.</p>

<figure class="width-90"><div id="stacking_autoencoders_diagram" class="figure">
<img src="assets/hmls_1808.png" alt="Diagram illustrating the three-phase process of training stacked autoencoders one layer at a time, showing the flow from input through hidden layers to output." width="1182" height="727"/>
<h6><span class="label">Figure 18-8. </span>Training one autoencoder at a time</h6>
</div></figure>

<p>As I mentioned in <a data-type="xref" href="ch11.html#deep_chapter">Chapter 11</a>, one of the triggers of the deep learning tsunami was the discovery in 2006 by <a href="https://homl.info/136">Geoffrey Hinton et al.</a> that deep neural networks can be pretrained in an unsupervised fashion using this greedy layer-wise approach. They used restricted Boltzmann machines (RBMs; see <a href="https://homl.info/extra-anns" class="bare"><em class="hyperlink">https://homl.info/extra-anns</em></a>) for this purpose, but in 2007 <a href="https://homl.info/112">Yoshua Bengio et al.</a>⁠<sup><a data-type="noteref" id="id4062-marker" href="ch18.html#id4062">3</a></sup> showed that autoencoders worked just as well. For several years this was the only efficient way to train deep nets, until many of the techniques introduced in <a data-type="xref" href="ch11.html#deep_chapter">Chapter 11</a> made it possible to just train a deep net in one shot<a data-type="indexterm" data-startref="xi_autoencodersstacked1810932_1" id="id4063"/><a data-type="indexterm" data-startref="xi_stackedautoencoders1810932_1" id="id4064"/>.</p>

<p>Autoencoders are not limited to dense networks: you can also build convolutional autoencoders<a data-type="indexterm" data-startref="xi_autoencoderstrainingoneatatime1827121_1" id="id4065"/>. Let’s look at these now.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Convolutional Autoencoders"><div class="sect1" id="id344">
<h1>Convolutional Autoencoders</h1>

<p>If you are dealing with images, then the autoencoders<a data-type="indexterm" data-primary="autoencoders" data-secondary="convolutional" id="xi_autoencodersconvolutional1828454_1"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="autoencoders" id="xi_convolutionalneuralnetworksCNNsautoencoders1828454_1"/> we have seen so far will not work well (unless the images are very small). As you saw in <a data-type="xref" href="ch12.html#cnn_chapter">Chapter 12</a>, convolutional neural networks are far better suited than dense networks to working with images. So if you want to build an autoencoder for images (e.g., for unsupervised pretraining or dimensionality reduction), you will need to build a <a href="https://homl.info/convae"><em>convolutional autoencoder</em></a>.⁠<sup><a data-type="noteref" id="id4066-marker" href="ch18.html#id4066">4</a></sup> The encoder is a regular CNN composed of convolutional layers and pooling layers. It typically reduces the spatial dimensionality<a data-type="indexterm" data-primary="dimensionality reduction" id="id4067"/> of the inputs (i.e., height and width) while increasing the depth (i.e., the number of feature maps). The decoder must do the reverse (upscale the image and reduce its depth back to the original dimensions), and for this you can use transpose convolutional layers (alternatively, you could combine upsampling layers with convolutional layers). Here is a basic convolutional autoencoder for Fashion MNIST:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">conv_encoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">16</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">MaxPool2d</code><code class="p">(</code><code class="n">kernel_size</code><code class="o">=</code><code class="mi">2</code><code class="p">),</code>  <code class="c1"># output: 16 × 14 × 14</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">32</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">MaxPool2d</code><code class="p">(</code><code class="n">kernel_size</code><code class="o">=</code><code class="mi">2</code><code class="p">),</code>  <code class="c1"># output: 32 × 7 × 7</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="mi">64</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">MaxPool2d</code><code class="p">(</code><code class="n">kernel_size</code><code class="o">=</code><code class="mi">2</code><code class="p">),</code>  <code class="c1"># output: 64 × 3 × 3</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code><code class="mi">64</code><code class="p">,</code> <code class="mi">32</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">AdaptiveAvgPool2d</code><code class="p">((</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">())</code>  <code class="c1"># output: 32</code>

<code class="n">conv_decoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="mi">16</code> <code class="o">*</code> <code class="mi">3</code> <code class="o">*</code> <code class="mi">3</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Unflatten</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">unflattened_size</code><code class="o">=</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">)),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">ConvTranspose2d</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">32</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="mi">2</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">ConvTranspose2d</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="mi">16</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
                       <code class="n">output_padding</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">ConvTranspose2d</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
                       <code class="n">output_padding</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">())</code>

<code class="n">conv_ae</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">conv_encoder</code><code class="p">,</code> <code class="n">conv_decoder</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>It’s also possible to create autoencoders with other architecture types, such as RNNs (see the notebook for an example).</p>

<p>OK, let’s step back for a second. So far we have looked at various kinds of autoencoders (basic, stacked, and convolutional) and how to train them (either in one shot or layer by layer). We also looked at a few applications: dimensionality reduction (e.g., for data visualization), anomaly detection, and unsupervised pretraining.</p>

<p>Up to now, in order to force the autoencoder to learn interesting features, we have limited the size of the coding layer, making it undercomplete<a data-type="indexterm" data-primary="autoencoders" data-secondary="undercomplete" id="id4068"/>. There are actually many other kinds of constraints that can be used, including ones that allow the coding layer to be just as large as the inputs, or even larger, resulting<a data-type="indexterm" data-primary="autoencoders" data-secondary="overcomplete" id="id4069"/><a data-type="indexterm" data-primary="overcomplete autoencoder" id="id4070"/> in an <em>overcomplete autoencoder</em>. So in the following sections we’ll look at a few more kinds of autoencoders: denoising autoencoders, sparse autoencoders, and variational autoencoders<a data-type="indexterm" data-startref="xi_autoencodersconvolutional1828454_1" id="id4071"/><a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNsautoencoders1828454_1" id="id4072"/>.</p>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Denoising Autoencoders"><div class="sect1" id="id345">
<h1>Denoising Autoencoders</h1>

<p>A simple way to force the autoencoder to learn useful features is to add noise<a data-type="indexterm" data-primary="autoencoders" data-secondary="denoising" id="xi_autoencodersdenoising1831779_1"/><a data-type="indexterm" data-primary="denoising autoencoders, stacked" id="xi_denoisingautoencodersstacked1831779_1"/> to its inputs, training it to recover the original, noise-free inputs. This idea has been around since the 1980s (e.g., it is mentioned in Yann LeCun’s 1987 master’s thesis). In a <a href="https://homl.info/113">2008 paper</a>,⁠<sup><a data-type="noteref" id="id4073-marker" href="ch18.html#id4073">5</a></sup> Pascal Vincent et al. showed that autoencoders could also be used for feature extraction. In a <a href="https://homl.info/114">2010 paper</a>,⁠<sup><a data-type="noteref" id="id4074-marker" href="ch18.html#id4074">6</a></sup> Vincent et al. introduced <em>stacked denoising autoencoders</em>.</p>

<p>The noise can be pure Gaussian noise added to the inputs, or it can be randomly switched-off inputs, just like in dropout (introduced in <a data-type="xref" href="ch11.html#deep_chapter">Chapter 11</a>). <a data-type="xref" href="#denoising_autoencoders_diagram">Figure 18-9</a> shows both options.</p>

<figure class="width-60"><div id="denoising_autoencoders_diagram" class="figure">
<img src="assets/hmls_1809.png" alt="Diagram illustrating denoising autoencoders with Gaussian noise added to inputs on the left and dropout applied on the right." width="774" height="692"/>
<h6><span class="label">Figure 18-9. </span>Denoising autoencoders, with Gaussian noise (left) or dropout (right)</h6>
</div></figure>

<p>The dropout implementation of the denoising autoencoder is straightforward: it is a regular stacked autoencoder with an additional <code translate="no">Dropout</code> layer applied to the encoder’s inputs (recall that the <code translate="no">Dropout</code> layer<a data-type="indexterm" data-primary="Dropout layer" id="id4075"/> is only active during training). Note that the coding layer does not need to compress the data as much since the noise already makes the reconstruction task nontrivial:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">dropout_encoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.5</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">128</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">128</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
<code class="p">)</code>
<code class="n">dropout_decoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">128</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Unflatten</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">unflattened_size</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">))</code>
<code class="p">)</code>
<code class="n">dropout_ae</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">dropout_encoder</code><code class="p">,</code> <code class="n">dropout_decoder</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>This may remind you of BERT’s MLM pretraining task (see <a data-type="xref" href="ch15.html#transformer_chapter">Chapter 15</a>): reconstructing masked inputs (except BERT isn’t split into an encoder and a decoder).</p>
</div>

<p><a data-type="xref" href="#dropout_denoising_plot">Figure 18-10</a> shows a few noisy images (with half of the pixels turned off), and the images reconstructed by the dropout-based denoising autoencoder, after training. Notice how the autoencoder guesses details that are actually not in the input, such as the top of the rightmost shoe<a data-type="indexterm" data-startref="xi_autoencodersdenoising1831779_1" id="id4076"/><a data-type="indexterm" data-startref="xi_denoisingautoencodersstacked1831779_1" id="id4077"/>. As you can see, not only can denoising autoencoders be used for data visualization or unsupervised pretraining, like the other autoencoders we’ve discussed so far, but they can also be used quite simply and efficiently to remove noise from images.</p>

<figure class="smallerseventy"><div id="dropout_denoising_plot" class="figure">
<img src="assets/hmls_1810.png" alt="Noisy images at the top and their denoised reconstructions at the bottom, demonstrating the effectiveness of a dropout-based denoising autoencoder." width="1410" height="518"/>
<h6><span class="label">Figure 18-10. </span>Noisy images (top) and their reconstructions (bottom)</h6>
</div></figure>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Sparse Autoencoders"><div class="sect1" id="id346">
<h1>Sparse Autoencoders</h1>

<p>Another kind of constraint<a data-type="indexterm" data-primary="autoencoders" data-secondary="sparse" id="xi_autoencoderssparse1835427_1"/><a data-type="indexterm" data-primary="sparse autoencoders" id="xi_sparseautoencoders1835427_1"/> that often leads to good feature extraction is <em>sparsity</em>: by adding an appropriate term to the cost function<a data-type="indexterm" data-primary="cost function" data-secondary="autoencoders" id="id4078"/>, the autoencoder is pushed to reduce the number of active neurons in the coding layer. This forces the autoencoder to represent each input as a combination of a small number of activations. As a result, each neuron in the coding layer typically ends up representing a useful feature (if you could speak only a few words per month, you would probably try to make them worth listening to).</p>

<p>A basic approach is to use the sigmoid activation function<a data-type="indexterm" data-primary="sigmoid activation function" data-secondary="sparse encoder implementation" id="id4079"/> in the coding layer (to constrain the codings to values between 0 and 1), use a large coding layer (e.g., with 256 units), and add some ℓ<sub>1</sub> regularization to the coding layer’s activations. This means adding the ℓ<sub>1</sub> norm of the codings (i.e., the sum of their absolute values) to the loss, weighted by a sparsity hyperparameter<a data-type="indexterm" data-primary="sparsity loss" id="id4080"/>. This <em>sparsity loss</em> will encourage the neural network to produce codings close to 0. However, the total loss will still include the reconstruction loss, so the model will be forced to output at least a few nonzero values to reconstruct the inputs correctly. Using the ℓ<sub>1</sub> norm rather than the ℓ<sub>2</sub> norm will push the neural network to preserve the most important codings while eliminating the ones that are not needed for the input image (rather than just reducing all codings).</p>

<p>Another approach—which often yields better results—is to measure the mean sparsity of each neuron in the coding layer, across each training batch, and penalize the model when the mean sparsity differs from the target sparsity (e.g., 10%). The batch size must not be too small, or the mean will not be accurate. For example, if we measure that a neuron has an average activation of 0.3, but the target sparsity is 0.1, then this neuron must be penalized to activate less. One approach could be simply adding the squared error (0.3 – 0.1)<sup>2</sup> to the loss function, but in practice it’s better to use the <a data-type="indexterm" data-primary="Kullback-Leibler (KL) divergence" id="id4081"/><a data-type="indexterm" data-primary="KL (Kullback-Leibler) divergence" id="id4082"/>Kullback–Leibler (KL) divergence (briefly discussed in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter 4</a>), since it has much stronger gradients than the mean squared error, as you can see in <a data-type="xref" href="#sparsity_loss_plot">Figure 18-11</a>.</p>

<figure class="width-80"><div id="sparsity_loss_plot" class="figure">
<img src="assets/hmls_1811.png" alt="Diagram comparing the cost functions of KL divergence, MAE, and MSE at different actual sparsity levels, with a target sparsity of 0.1." width="1667" height="914"/>
<h6><span class="label">Figure 18-11. </span>Sparsity loss with target sparsity <em>p</em> = 0.1</h6>
</div></figure>

<p>Given two discrete probability distributions <em>P</em> and <em>Q</em>, the KL divergence between these distributions, noted <em>D</em><sub>KL</sub>(<em>P</em> ∥ <em>Q</em>), can be computed using <a data-type="xref" href="#kl_divergence_equation">Equation 18-1</a>.</p>
<div id="kl_divergence_equation" data-type="equation" class="less_space pagebreak-before">
<h5><span class="label">Equation 18-1. </span>Kullback–Leibler divergence</h5>
<math alttext="upper D Subscript KL Baseline left-parenthesis upper P parallel-to upper Q right-parenthesis equals sigma-summation Underscript i Endscripts upper P left-parenthesis i right-parenthesis log StartFraction upper P left-parenthesis i right-parenthesis Over upper Q left-parenthesis i right-parenthesis EndFraction" display="block">
  <mrow>
    <msub><mi>D</mi> <mtext>KL</mtext> </msub>
    <mrow>
      <mo>(</mo>
      <mi>P</mi>
      <mo>∥</mo>
      <mi>Q</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <munder><mo>∑</mo> <mi>i</mi> </munder>
    <mi>P</mi>
    <mrow>
      <mo>(</mo>
      <mi>i</mi>
      <mo>)</mo>
    </mrow>
    <mo form="prefix">log</mo>
    <mfrac><mrow><mi>P</mi><mo>(</mo><mi>i</mi><mo>)</mo></mrow> <mrow><mi>Q</mi><mo>(</mo><mi>i</mi><mo>)</mo></mrow></mfrac>
  </mrow>
</math>
</div>

<p>In our case, we want to measure the divergence between the target probability <em>p</em> that a neuron in the coding layer will activate, and the actual probability <em>q</em>, estimated by measuring the mean activation over the training batch. So, the KL divergence simplifies to <a data-type="xref" href="#kl_divergence_equation_simplified">Equation 18-2</a>.</p>
<div id="kl_divergence_equation_simplified" data-type="equation">
<h5><span class="label">Equation 18-2. </span>KL divergence between the target sparsity <em>p</em> and the actual sparsity <em>q</em></h5>
<math alttext="upper D Subscript KL Baseline left-parenthesis p parallel-to q right-parenthesis equals p log StartFraction p Over q EndFraction plus left-parenthesis 1 minus p right-parenthesis log StartFraction 1 minus p Over 1 minus q EndFraction" display="block">
  <mrow>
    <msub><mi>D</mi> <mtext>KL</mtext> </msub>
    <mrow>
      <mo>(</mo>
      <mi>p</mi>
      <mo>∥</mo>
      <mi>q</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mi>p</mi>
    <mspace width="0.166667em"/>
    <mo form="prefix">log</mo>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac><mi>p</mi> <mi>q</mi></mfrac>
    </mstyle>
    <mo>+</mo>
    <mrow>
      <mo>(</mo>
      <mn>1</mn>
      <mo>-</mo>
      <mi>p</mi>
      <mo>)</mo>
    </mrow>
    <mspace width="0.166667em"/>
    <mo form="prefix">log</mo>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac><mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow> <mrow><mn>1</mn><mo>-</mo><mi>q</mi></mrow></mfrac>
    </mstyle>
  </mrow>
</math>
</div>

<p>To implement this approach in PyTorch, we must first ensure that the autoencoder outputs both the reconstructions and the codings, since they are both needed to compute the loss. In this code, the autoencoder’s <code translate="no">forward()</code> method returns a <code translate="no">namedtuple</code> containing two fields—<code translate="no">output</code> (i.e., the reconstructions) and <code translate="no">codings</code>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">namedtuple</code>

<code class="n">AEOutput</code> <code class="o">=</code> <code class="n">namedtuple</code><code class="p">(</code><code class="s2">"AEOutput"</code><code class="p">,</code> <code class="p">[</code><code class="s2">"output"</code><code class="p">,</code> <code class="s2">"codings"</code><code class="p">])</code>

<code class="k">class</code> <code class="nc">SparseAutoencoder</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">encoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">128</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">256</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">())</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">decoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">256</code><code class="p">,</code> <code class="mi">128</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">(),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Unflatten</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">unflattened_size</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">)))</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">codings</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">encoder</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">output</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">decoder</code><code class="p">(</code><code class="n">codings</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">AEOutput</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">codings</code><code class="p">)</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You may need to tweak your training and evaluation functions to support these <code translate="no">namedtuple</code> predictions. For example, you can add <code translate="no">y_pred = y_pred.output</code> in the <code translate="no">evaluate_tm()</code> function, just after calling the model.</p>
</div>

<p class="pagebreak-before">Next, we can define<a data-type="indexterm" data-primary="loss functions" data-secondary="sparse autoencoders" id="id4083"/> the loss function:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">mse_plus_sparsity_loss</code><code class="p">(</code><code class="n">y_pred</code><code class="p">,</code> <code class="n">y_target</code><code class="p">,</code> <code class="n">target_sparsity</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code>
                           <code class="n">kl_weight</code><code class="o">=</code><code class="mf">1e-3</code><code class="p">,</code> <code class="n">eps</code><code class="o">=</code><code class="mf">1e-8</code><code class="p">):</code>
    <code class="n">p</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="n">target_sparsity</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">y_pred</code><code class="o">.</code><code class="n">codings</code><code class="o">.</code><code class="n">device</code><code class="p">)</code>
    <code class="n">q</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">clamp</code><code class="p">(</code><code class="n">y_pred</code><code class="o">.</code><code class="n">codings</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">0</code><code class="p">),</code> <code class="n">eps</code><code class="p">,</code> <code class="mi">1</code> <code class="o">-</code> <code class="n">eps</code><code class="p">)</code>  <code class="c1"># actual sparsity</code>
    <code class="n">kl_div</code> <code class="o">=</code> <code class="n">p</code> <code class="o">*</code> <code class="n">torch</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">p</code> <code class="o">/</code> <code class="n">q</code><code class="p">)</code> <code class="o">+</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">p</code><code class="p">)</code> <code class="o">*</code> <code class="n">torch</code><code class="o">.</code><code class="n">log</code><code class="p">((</code><code class="mi">1</code> <code class="o">-</code> <code class="n">p</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">q</code><code class="p">))</code>
    <code class="k">return</code> <code class="n">mse</code><code class="p">(</code><code class="n">y_pred</code><code class="o">.</code><code class="n">output</code><code class="p">,</code> <code class="n">y_target</code><code class="p">)</code> <code class="o">+</code> <code class="n">kl_weight</code> <code class="o">*</code> <code class="n">kl_div</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code></pre>

<p>This function returns the reconstruction loss (MSE) plus a weighted sparsity loss. The sparsity loss is the KL divergence between the target sparsity and the mean sparsity across the batch. The <code translate="no">kl_weight</code> is a hyperparameter<a data-type="indexterm" data-primary="kl_weight hyperparameter" id="id4084"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="kl_weight and sparsity loss/sparse encoders" id="id4085"/> you can tune to control how much to encourage sparsity: if this hyperparameter is too high, the model will stick closely to the target sparsity, but it may not reconstruct the inputs properly, making the model useless. Conversely, if it is too low, the model will mostly ignore the sparsity objective and will not learn any interesting features. The <code translate="no">eps</code> argument is a smoothing term to avoid division by zero when computing the KL divergence.</p>

<p>Now we’re ready to create the model and train it (using the same <code translate="no">train()</code> function as earlier, from <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter 10</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">sparse_ae</code> <code class="o">=</code> <code class="n">SparseAutoencoder</code><code class="p">()</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">NAdam</code><code class="p">(</code><code class="n">sparse_ae</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.002</code><code class="p">)</code>
<code class="n">train</code><code class="p">(</code><code class="n">sparse_ae</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">mse_plus_sparsity_loss</code><code class="p">,</code> <code class="n">train_loader</code><code class="p">,</code> <code class="n">n_epochs</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code></pre>

<p>After training this sparse autoencoder on Fashion MNIST, the coding layer will have roughly 10% sparsity. Success!</p>
<div data-type="tip"><h6>Tip</h6>
<p>Sparse autoencoders often produce fairly interpretable codings, where each component corresponds to an identifiable feature in the image. For example, you can plot all the images whose <em>n</em><sup>th</sup> coding is larger than usual (e.g., above the 90<sup>th</sup> percentile): you will often notice that all the images have something in common (e.g., they are all shoes)<a data-type="indexterm" data-startref="xi_autoencoderssparse1835427_1" id="id4086"/><a data-type="indexterm" data-startref="xi_sparseautoencoders1835427_1" id="id4087"/>.</p>
</div>

<p>Now let’s move on to variational autoencoders!</p>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Variational Autoencoders"><div class="sect1" id="id347">
<h1>Variational Autoencoders</h1>

<p>An important category of autoencoders<a data-type="indexterm" data-primary="autoencoders" data-secondary="variational" id="xi_autoencodersvariational1844638_1"/><a data-type="indexterm" data-primary="variational autoencoders (VAEs)" id="xi_variationalautoencodersVAEs1844638_1"/><a data-type="indexterm" data-primary="generative models" data-secondary="VAEs" id="xi_variationalautoencodersVAEs1844638_2"/><a data-type="indexterm" data-primary="VAEs (variational autoencoders)" id="xi_VAEsvariationalautoencoders1844638_1"/> was introduced in 2013 by <a href="https://homl.info/115">Diederik Kingma and Max Welling</a>⁠<sup><a data-type="noteref" id="id4088-marker" href="ch18.html#id4088">7</a></sup> and quickly became one of the most popular variants: <em>variational autoencoders</em> (VAEs).</p>

<p>VAEs are quite different from all the autoencoders we have discussed so far, in these particular ways:</p>

<ul>
<li>
<p>They are <em>probabilistic autoencoders</em>, meaning<a data-type="indexterm" data-primary="probabilistic autoencoders" id="id4089"/> that their outputs are partly determined by chance, even after training (as opposed to denoising autoencoders, which use randomness only during training).</p>
</li>
<li>
<p>Most importantly, they<a data-type="indexterm" data-primary="autoencoders" data-secondary="generative" id="id4090"/> are <em>generative autoencoders</em>, meaning that they can generate new instances that look like they were sampled from the training set.⁠<sup><a data-type="noteref" id="id4091-marker" href="ch18.html#id4091">8</a></sup></p>
</li>
</ul>

<p>Let’s take a look at how VAEs work. <a data-type="xref" href="#variational_autoencoders_diagram">Figure 18-12</a> (left) shows a variational autoencoder. You can recognize the basic sandwich-like structure of most autoencoders, with an encoder followed by a decoder (in this example, they both have two hidden layers), but there is a twist: instead of directly producing a coding for a given input, the encoder<a data-type="indexterm" data-primary="mean coding, VAEs" id="id4092"/> produces a <em>mean coding</em> <strong>μ</strong> and a standard deviation <strong>σ</strong>. The actual coding is then sampled randomly from a Gaussian distribution<a data-type="indexterm" data-primary="Gaussian distribution" id="xi_Gaussiandistribution18453486_1"/> with mean <strong>μ</strong> and standard deviation <strong>σ</strong>. After that, the decoder decodes the sampled coding normally. The right part of the diagram shows a training instance going through this autoencoder. First, the encoder produces <strong>μ</strong> and <strong>σ</strong>, then a coding is sampled randomly (notice that it is not exactly located at <strong>μ</strong>), and finally this coding is decoded. The final output resembles the training instance.</p>

<p>As you can see in <a data-type="xref" href="#variational_autoencoders_diagram">Figure 18-12</a>, although the inputs may have a very convoluted distribution, a variational autoencoder tends to produce codings that look as though they were sampled from a simple Gaussian distribution. During training, the cost function (discussed next) pushes the codings to gradually migrate within the coding space (also called<a data-type="indexterm" data-primary="latent space" id="id4093"/> the <em>latent space</em>) to end up looking like a cloud of multidimensional Gaussian points. One great consequence is that after training a variational autoencoder, you can very easily generate a new instance: just sample a random coding from the Gaussian distribution, decode it, and voilà!</p>

<figure class="smallereighty"><div id="variational_autoencoders_diagram" class="figure">
<img src="assets/hmls_1812.png" alt="Diagram of a variational autoencoder with an encoder and decoder, illustrating Gaussian noise sampling and coding space transformation." width="1060" height="1026"/>
<h6><span class="label">Figure 18-12. </span>A variational autoencoder (left) and an instance going through it (right)</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Sampling from a random distribution is not a differentiable operation, it will block backpropagation, so how can we hope to train the encoder? Well, using a <em>reparameterization trick</em>: sample <strong>ε</strong> from 𝒩(0, 1) and compute <strong>μ</strong> + <strong>σ</strong> ⊗ <strong>ε</strong> (element-wise multiplication). This is equivalent to sampling from 𝒩(<strong>μ</strong>, <strong>σ</strong><sup>2</sup>) but it separates<a data-type="indexterm" data-primary="reparameterization trick" id="id4094"/> the deterministic and stochastic parts of the process, allowing the gradients to flow back into the encoder through <strong>μ</strong> and <strong>σ</strong>. The resulting encoder gradients are stochastic (due to <strong>ε</strong>), but they are unbiased estimates, and the randomness averages out during training.</p>
</div>

<p>The cost function<a data-type="indexterm" data-primary="cost function" data-secondary="variational autoencoders" id="id4095"/> is composed of two parts. The first is the usual reconstruction loss that pushes the autoencoder to reproduce its inputs. We can use the MSE<a data-type="indexterm" data-primary="mean squared error (MSE)" id="id4096"/><a data-type="indexterm" data-primary="MSE (mean squared error)" id="id4097"/> for this, as we did earlier. The second<a data-type="indexterm" data-primary="latent loss" id="xi_latentloss18464222_1"/> is the <em>latent loss</em> that pushes the autoencoder to have codings that look as though they were sampled from a simple Gaussian distribution: it is the KL divergence between the actual distribution of the codings and the desired latent distribution (i.e., the Gaussian distribution). The math is a bit more complex than with the sparse autoencoder, in particular because of the Gaussian noise, which limits the amount of information that can be transmitted to the coding layer<a data-type="indexterm" data-startref="xi_Gaussiandistribution18453486_1" id="id4098"/>. Luckily, the equations simplify, so the latent loss can be computed using <a data-type="xref" href="#var_ae_latent_loss_equation">Equation 18-3</a> (for the full mathematical details, check out the original paper on variational autoencoders, or Carl Doersch’s <a href="https://homl.info/vaetuto">great 2016 tutorial</a>.)</p>
<div id="var_ae_latent_loss_equation" data-type="equation">
<h5><span class="label">Equation 18-3. </span>Variational autoencoder’s latent loss</h5>
<math alttext="script upper L equals minus one-half sigma-summation Underscript i equals 1 Overscript n Endscripts left-bracket 1 plus log left-parenthesis sigma Subscript i Superscript 2 Baseline right-parenthesis minus sigma Subscript i Superscript 2 Baseline minus mu Subscript i Superscript 2 Baseline right-bracket" display="block">
  <mrow>
    <mi>ℒ</mi>
    <mo>=</mo>
    <mo>-</mo>
    <mfrac><mn>1</mn> <mn>2</mn></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </munderover>
    <mrow>
      <mo>[</mo>
      <mn>1</mn>
      <mo>+</mo>
      <mo form="prefix">log</mo>
      <mrow>
        <mo>(</mo>
        <msubsup><mi>σ</mi> <mi>i</mi> <mn>2</mn> </msubsup>
        <mo>)</mo>
      </mrow>
      <mo>-</mo>
      <msubsup><mi>σ</mi> <mi>i</mi> <mn>2</mn> </msubsup>
      <mo>-</mo>
      <msubsup><mi>μ</mi> <mi>i</mi> <mn>2</mn> </msubsup>
      <mo>]</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>In this equation, ℒ is the latent loss, <em>n</em> is the codings’ dimensionality, and <em>μ</em><sub>i</sub> and <em>σ</em><sub>i</sub> are the mean and standard deviation of the <em>i</em><sup>th</sup> component of the codings. The vectors <strong>μ</strong> and <strong>σ</strong> (which contain all the <em>μ</em><sub>i</sub> and <em>σ</em><sub>i</sub>) are output by the encoder, as shown in <a data-type="xref" href="#variational_autoencoders_diagram">Figure 18-12</a> (left).</p>

<p>A common tweak to the variational autoencoder’s architecture is to make the encoder output <strong>γ</strong> = log(<strong>σ</strong><sup>2</sup>) rather than <strong>σ</strong>. The latent loss can then be computed as shown in <a data-type="xref" href="#var_ae_latent_loss_equation_2">Equation 18-4</a>. This approach is more numerically stable and speeds up training.</p>
<div id="var_ae_latent_loss_equation_2" data-type="equation">
<h5><span class="label">Equation 18-4. </span>Variational autoencoder’s latent loss, rewritten using <strong>γ</strong> = log(<strong>σ</strong>²)</h5>
<math alttext="script upper L equals minus one-half sigma-summation Underscript i equals 1 Overscript n Endscripts left-bracket 1 plus gamma Subscript i Baseline minus exp left-parenthesis gamma Subscript i Baseline right-parenthesis minus mu Subscript i Superscript 2 Baseline right-bracket" display="block">
  <mrow>
    <mi>ℒ</mi>
    <mo>=</mo>
    <mo>-</mo>
    <mfrac><mn>1</mn> <mn>2</mn></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </munderover>
    <mrow>
      <mo>[</mo>
      <mn>1</mn>
      <mo>+</mo>
      <msub><mi>γ</mi> <mi>i</mi> </msub>
      <mo>-</mo>
      <mtext>exp</mtext>
      <mrow>
        <mo>(</mo>
        <msub><mi>γ</mi> <mi>i</mi> </msub>
        <mo>)</mo>
      </mrow>
      <mo>-</mo>
      <msubsup><mi>μ</mi> <mi>i</mi> <mn>2</mn> </msubsup>
      <mo>]</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>Let’s build a variational autoencoder for Fashion MNIST, using the architecture shown in <a data-type="xref" href="#variational_autoencoders_diagram">Figure 18-12</a>, except using the <strong>γ</strong> tweak:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">VAEOutput</code> <code class="o">=</code> <code class="n">namedtuple</code><code class="p">(</code><code class="s2">"VAEOutput"</code><code class="p">,</code>
                       <code class="p">[</code><code class="s2">"output"</code><code class="p">,</code> <code class="s2">"codings_mean"</code><code class="p">,</code> <code class="s2">"codings_logvar"</code><code class="p">])</code>

<code class="k">class</code> <code class="nc">VAE</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">codings_dim</code><code class="o">=</code><code class="mi">32</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">VAE</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">codings_dim</code> <code class="o">=</code> <code class="n">codings_dim</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">encoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">128</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">codings_dim</code><code class="p">))</code>  <code class="c1"># output both the mean and logvar</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">decoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">codings_dim</code><code class="p">,</code> <code class="mi">128</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">(),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Unflatten</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">unflattened_size</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">)))</code>

    <code class="k">def</code> <code class="nf">encode</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">encoder</code><code class="p">(</code><code class="n">X</code><code class="p">)</code><code class="o">.</code><code class="n">chunk</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="n">dim</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>  <code class="c1"># returns (mean, logvar)</code>

    <code class="k">def</code> <code class="nf">sample_codings</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">codings_mean</code><code class="p">,</code> <code class="n">codings_logvar</code><code class="p">):</code>
        <code class="n">codings_std</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="mf">0.5</code> <code class="o">*</code> <code class="n">codings_logvar</code><code class="p">)</code>
        <code class="n">noise</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randn_like</code><code class="p">(</code><code class="n">codings_std</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">codings_mean</code> <code class="o">+</code> <code class="n">noise</code> <code class="o">*</code> <code class="n">codings_std</code>

    <code class="k">def</code> <code class="nf">decode</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">Z</code><code class="p">):</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">decoder</code><code class="p">(</code><code class="n">Z</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">codings_mean</code><code class="p">,</code> <code class="n">codings_logvar</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">codings</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sample_codings</code><code class="p">(</code><code class="n">codings_mean</code><code class="p">,</code> <code class="n">codings_logvar</code><code class="p">)</code>
        <code class="n">output</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="n">codings</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">VAEOutput</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">codings_mean</code><code class="p">,</code> <code class="n">codings_logvar</code><code class="p">)</code></pre>

<p>Let’s go through this code:</p>

<ul>
<li>
<p>First, we define <code translate="no">VAEOutput</code>. This allows the model to output a <code translate="no">namedtuple</code> containing the reconstructions (<code translate="no">output</code>) as well as <strong>μ</strong> (<code translate="no">codings_mean</code>) and <strong>γ</strong> (<code translate="no">codings_logvar</code>).</p>
</li>
<li>
<p>The encoder and decoder architectures strongly resemble the previous autoencoders, but notice that the encoder’s output is twice the size of the codings. This is because the encoder does not directly output the codings; instead, it outputs the parameters of the Gaussian distribution from which the codings will be sampled: the mean (<strong>μ</strong>) and the logarithm of the variance (<strong>γ</strong>).</p>
</li>
<li>
<p>The <code translate="no">encode()</code> method calls the <code translate="no">encoder</code> model and splits the output in two, using the <code translate="no">chunk()</code> method, to obtain <strong>μ</strong> and <strong>γ</strong>.</p>
</li>
<li>
<p>The <code translate="no">sample_codings()</code> method takes <strong>μ</strong> and <strong>γ</strong> and samples the actual codings. For this, it first computes <code translate="no">torch.exp(0.5 * codings_logvar)</code> to get the codings’ standard deviation <strong>σ</strong> (you can verify that this works mathematically). Then it uses the <code translate="no">torch.randn_like()</code> function to sample a random vector of the same shape as <strong>σ</strong> from the Gaussian distribution with mean 0 and standard deviation 1, on the same device and with the same data type. Lastly, it multiplies this Gaussian noise by <strong>σ</strong>, adds <strong>μ</strong>, and returns the result. This is the reparameterization trick we discussed earlier.</p>
</li>
<li>
<p>The <code translate="no">decode()</code> method simply calls the decoder model to produce the 
<span class="keep-together">reconstructions.</span></p>
</li>
<li>
<p>The <code translate="no">forward()</code> method calls the encoder to get <strong>μ</strong> and <strong>γ</strong>, then it uses these parameters to sample the codings, which it decodes, and finally it returns a <code translate="no">VAEOutput</code> containing the reconstructions and the parameters <strong>μ</strong> and <strong>γ</strong>, which are all needed to compute the VAE loss.</p>
</li>
</ul>

<p>Speaking of which, let’s now define the loss function, which is the sum of the reconstruction loss (MSE)<a data-type="indexterm" data-primary="reconstruction loss" id="id4099"/> and the latent loss (KL divergence):</p>

<pre translate="no" data-type="programlisting" data-code-language="python" class="less_space pagebreak-before"><code class="k">def</code> <code class="nf">vae_loss</code><code class="p">(</code><code class="n">y_pred</code><code class="p">,</code> <code class="n">y_target</code><code class="p">,</code> <code class="n">kl_weight</code><code class="o">=</code><code class="mf">1.0</code><code class="p">):</code>
    <code class="n">output</code><code class="p">,</code> <code class="n">mean</code><code class="p">,</code> <code class="n">logvar</code> <code class="o">=</code> <code class="n">y_pred</code>
    <code class="n">kl_div</code> <code class="o">=</code> <code class="o">-</code><code class="mf">0.5</code> <code class="o">*</code> <code class="n">torch</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="n">logvar</code> <code class="o">-</code> <code class="n">logvar</code><code class="o">.</code><code class="n">exp</code><code class="p">()</code> <code class="o">-</code> <code class="n">mean</code><code class="o">.</code><code class="n">square</code><code class="p">(),</code> <code class="n">dim</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">F</code><code class="o">.</code><code class="n">mse_loss</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">y_target</code><code class="p">)</code> <code class="o">+</code> <code class="n">kl_weight</code> <code class="o">*</code> <code class="n">kl_div</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code> <code class="o">/</code> <code class="mi">784</code></pre>

<p>The function first uses <a data-type="xref" href="#var_ae_latent_loss_equation_2">Equation 18-4</a> to compute the latent loss (<code translate="no">kl_div</code>) for each instance in the batch (by summing over the last dimension), then it computes the mean latent loss over all the instances in the batch (<code translate="no">kl_div.mean()</code>). Note that the reconstruction loss is the mean over all instances in the batch <em>and</em> all 784 pixels: this is why we divide the latent loss by 784 to ensure that the reconstruction loss and the latent loss have the same scale<a data-type="indexterm" data-startref="xi_latentloss18464222_1" id="id4100"/>.</p>

<p>Finally, we can train the model on the Fashion MNIST dataset:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">vae</code> <code class="o">=</code> <code class="n">VAE</code><code class="p">()</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">NAdam</code><code class="p">(</code><code class="n">vae</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">1e-3</code><code class="p">)</code>
<code class="n">train</code><code class="p">(</code><code class="n">vae</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">vae_loss</code><code class="p">,</code> <code class="n">train_loader</code><code class="p">,</code> <code class="n">n_epochs</code><code class="o">=</code><code class="mi">20</code><code class="p">)</code></pre>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Generating Fashion MNIST Images"><div class="sect1" id="id348">
<h1>Generating Fashion MNIST Images</h1>

<p>Now let’s use this VAE to generate images that look like fashion items<a data-type="indexterm" data-primary="Fashion MNIST dataset" data-secondary="generating images" id="xi_FashionMNISTdatasetgeneratingimages1856071_1"/>. All we need to do is sample random codings from a Gaussian distribution with mean 0 and variance 1, and decode them:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">vae</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
<code class="n">codings</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">3</code> <code class="o">*</code> <code class="mi">7</code><code class="p">,</code> <code class="n">vae</code><code class="o">.</code><code class="n">codings_dim</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">)</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">images</code> <code class="o">=</code> <code class="n">vae</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="n">codings</code><code class="p">)</code></pre>

<p><a data-type="xref" href="#vae_generated_images_plot">Figure 18-13</a> shows the 21 generated images.</p>

<figure class="width-90"><div id="vae_generated_images_plot" class="figure">
<img src="assets/hmls_1813.png" alt="Variational autoencoder-generated images of clothing items from the Fashion MNIST dataset, appearing fuzzy and lacking detail." width="1085" height="460"/>
<h6><span class="label">Figure 18-13. </span>Fashion MNIST images generated by the variational autoencoder</h6>
</div></figure>

<p>The majority of these images look fairly convincing, if a bit too fuzzy. The rest are not great, but don’t be too harsh on the autoencoder—it only had a few minutes to learn, and you would get much better results by using convolutional layers!</p>

<p>Variational autoencoders make it possible to perform <em>semantic interpolation</em>: instead of interpolating<a data-type="indexterm" data-primary="semantic interpolation" id="id4101"/> between two images at the pixel level, which would look as if the two images were just overlaid, we can interpolate at the codings level. For example, if we sample two random codings and interpolate between them, then decode all of the interpolated codings, we get a sequence of images that gradually go from one fashion item to another (see <a data-type="xref" href="#semantic_interpolation_plot">Figure 18-14</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">codings</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="n">vae</code><code class="o">.</code><code class="n">codings_dim</code><code class="p">)</code>  <code class="c1"># start and end codings</code>
<code class="n">n_images</code> <code class="o">=</code> <code class="mi">7</code>
<code class="n">weights</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">n_images</code><code class="p">)</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="n">n_images</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">codings</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">lerp</code><code class="p">(</code><code class="n">codings</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">codings</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">weights</code><code class="p">)</code>  <code class="c1"># linear interpolation</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">images</code> <code class="o">=</code> <code class="n">vae</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="n">codings</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">))</code></pre>

<figure class="smallereighty"><div id="semantic_interpolation_plot" class="figure">
<img src="assets/hmls_1814.png" alt="A series of progressively morphing silhouettes demonstrating semantic interpolation between two clothing items." width="1046" height="134"/>
<h6><span class="label">Figure 18-14. </span>Semantic interpolation</h6>
</div></figure>

<p>There are a few variants of VAEs, for example, with different distributions for the latent variables. One important variant is discrete VAEs: let’s discuss them now.</p>








<section data-type="sect2" data-pdf-bookmark="Discrete Variational Autoencoders"><div class="sect2" id="id349">
<h2>Discrete Variational Autoencoders</h2>

<p>A <em>discrete VAE</em> (dVAE)<a data-type="indexterm" data-primary="discrete variational autoencoders (dVAEs)" id="xi_discretevariationalautoencodersdVAEs1860024_1"/><a data-type="indexterm" data-primary="dVAEs (discrete variational autoencoders)" id="xi_dVAEsdiscretevariationalautoencoders1860024_1"/> is much like a VAE, except the codings are discrete rather than continuous: each coding vector<a data-type="indexterm" data-primary="latent codes, dVAEs" id="id4102"/><a data-type="indexterm" data-primary="categories, dVAEs" id="id4103"/> contains <em>latent codes</em> (also called <em>categories</em>), each of which is an integer between 0 and <em>k</em> – 1, where <em>k</em> is the number of possible latent codes. The length of the coding vector is often denoted as <em>d</em>. For example, if you choose <em>k</em> = 10 and <em>d</em> = 6, then there are one million possible coding vectors (10<sup>6</sup>), such as [3, 0, 3, 9, 1, 4]. Discrete VAEs are very useful for tokenizing continuous inputs for transformers and other models. For example, they are at the core of models like BEiT and DALL·E (see <a data-type="xref" href="ch16.html#vit_chapter">Chapter 16</a>).</p>

<p>The most natural way to make VAEs discrete is to use a categorical distribution instead of a Gaussian distribution. This implies a couple of changes:</p>

<ul>
<li>
<p>First, the encoder must output logits rather than means and variances. For each input image, it outputs a tensor of shape [<em>d</em>, <em>k</em>] containing logits, for example [[1.2, –0.8, 0.5], [–1.3, 0.4, 0.3]] if <em>d</em> = 2 and <em>k</em> = 3.</p>
</li>
<li>
<p>Second, since categorical sampling is not a differentiable operation, we must once again use a reparameterization trick, but we cannot reuse the same as for regular VAEs: we need one designed for categorical distributions. The most popular one is the Gumbel-softmax trick. Instead of directly sampling from the categorical distribution, we call the <code translate="no">F.gumble_softmax()</code> function<a data-type="indexterm" data-primary="torch" data-secondary="F.gumble_softmax()" id="id4104"/>: this implements a differentiable approximation of categorical sampling. Given the previous logits, this function might output the discrete coding vector [0, 2].</p>
</li>
</ul>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The Gumbel distribution<a data-type="indexterm" data-primary="Gumbel distribution" id="id4105"/> is used to model the maximum of a set of samples from another distribution. For example, it can be used to estimate the probability that a river will overflow within the next 10 years. If you add Gumbel noise to the logits, then take the argmax of the result, it is mathematically equivalent to categorical sampling. However, the argmax operation is not differentiable, so we replace it with the softmax during the backward pass: this gives us a differentiable approximation of categorical sampling.</p>
</div>

<p>This idea was proposed in 2016 almost simultaneously by two independent teams of researchers, one from <a href="https://homl.info/dvae1">DeepMind and Oxford University</a>,⁠<sup><a data-type="noteref" id="id4106-marker" href="ch18.html#id4106">9</a></sup> the other from <a href="https://homl.info/dvae2">Google, Cambridge University, and Stanford University</a>.⁠<sup><a data-type="noteref" id="id4107-marker" href="ch18.html#id4107">10</a></sup></p>

<p>Let’s implement a dVAE for Fashion MNIST:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">DiscreteVAEOutput</code> <code class="o">=</code> <code class="n">namedtuple</code><code class="p">(</code><code class="s2">"DiscreteVAEOutput"</code><code class="p">,</code>
                               <code class="p">[</code><code class="s2">"output"</code><code class="p">,</code> <code class="s2">"logits"</code><code class="p">,</code> <code class="s2">"codings_prob"</code><code class="p">])</code>

<code class="k">class</code> <code class="nc">DiscreteVAE</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">coding_length</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">n_codes</code><code class="o">=</code><code class="mi">16</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mf">1.0</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">coding_length</code> <code class="o">=</code> <code class="n">coding_length</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">n_codes</code> <code class="o">=</code> <code class="n">n_codes</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">temperature</code> <code class="o">=</code> <code class="n">temperature</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">encoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="o">...</code><code class="p">])</code>  <code class="c1"># outputs [coding_length, n_codes]</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">decoder</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="o">...</code><code class="p">])</code>  <code class="c1"># outputs [1, 28, 28]</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">logits</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">encoder</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">codings_prob</code> <code class="o">=</code> <code class="n">F</code><code class="o">.</code><code class="n">gumbel_softmax</code><code class="p">(</code><code class="n">logits</code><code class="p">,</code> <code class="n">tau</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">temperature</code><code class="p">,</code> <code class="n">hard</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="n">output</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">decoder</code><code class="p">(</code><code class="n">codings_prob</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">DiscreteVAEOutput</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">logits</code><code class="p">,</code> <code class="n">codings_prob</code><code class="p">)</code></pre>

<p>As you can see, this code is very similar to the VAE code. Note that we set <code translate="no">hard=True</code> when calling the <code translate="no">F.gumbel_softmax()</code> function to ensure that the forward pass uses Gumbel-argmax (to obtain one-hot vectors of the sampled codes), while the backward pass uses the Gumbel-softmax approximation. Also note that we pass a temperature (a scalar) to this function: the logits will be divided by this temperature before calling the softmax function. The lower the temperature is, the closer the output distribution will be to one-hot vectors (this only affects the backward pass). In general, we use a temperature of 1 at the beginning of training, then gradually reduce it during training, down to a small value such as 0.1.</p>

<p>The loss function<a data-type="indexterm" data-primary="loss functions" data-secondary="dVAEs" id="id4108"/> is also similar to the regular VAE loss: it’s the sum of a reconstruction loss (MSE) and a weighted latent loss (KL divergence). However, the KL divergence equation is a bit different since the latent distribution has changed. It’s now a uniform categorical distribution, where all possible codes are equally likely, so they each have a probability of 1 / <em>k</em>. Since log(1 / <em>k</em>) = –log(<em>k</em>), we can add log(<em>k</em>) instead of subtracting log(1 / <em>k</em>) in the KL divergence equation:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">d_vae_loss</code><code class="p">(</code><code class="n">y_pred</code><code class="p">,</code> <code class="n">y_target</code><code class="p">,</code> <code class="n">kl_weight</code><code class="o">=</code><code class="mf">1.0</code><code class="p">):</code>
    <code class="n">output</code><code class="p">,</code> <code class="n">logits</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">y_pred</code>
    <code class="n">codings_prob</code> <code class="o">=</code> <code class="n">F</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">logits</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">)</code>
    <code class="n">k</code> <code class="o">=</code> <code class="n">logits</code><code class="o">.</code><code class="n">new_tensor</code><code class="p">(</code><code class="n">logits</code><code class="o">.</code><code class="n">size</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">))</code>  <code class="c1"># same device and dtype as logits</code>
    <code class="n">kl_div</code> <code class="o">=</code> <code class="p">(</code><code class="n">codings_prob</code> <code class="o">*</code> <code class="p">(</code><code class="n">codings_prob</code><code class="o">.</code><code class="n">log</code><code class="p">()</code> <code class="o">+</code> <code class="n">k</code><code class="o">.</code><code class="n">log</code><code class="p">()))</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">))</code>
    <code class="k">return</code> <code class="n">F</code><code class="o">.</code><code class="n">mse_loss</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">y_target</code><code class="p">)</code> <code class="o">+</code> <code class="n">kl_weight</code> <code class="o">*</code> <code class="n">kl_div</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code> <code class="o">/</code> <code class="mi">784</code></pre>

<p>You can now train the model. Remember to update your training loop to reduce the temperature gradually during training, for example:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">model</code><code class="o">.</code><code class="n">temperature</code> <code class="o">=</code> <code class="mi">1</code> <code class="o">-</code> <code class="mf">0.9</code> <code class="o">*</code> <code class="n">epoch</code> <code class="o">/</code> <code class="n">n_epochs</code></pre>

<p>Once the model is trained, you can generate new images by sampling random codings from a uniform distribution, one-hot encoding them, then decoding the resulting one-hot distribution:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">codings</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">d_vae</code><code class="o">.</code><code class="n">n_codes</code><code class="p">,</code>  <code class="c1"># from 0 to k – 1</code>
                        <code class="p">(</code><code class="n">n_images</code><code class="p">,</code> <code class="n">d_vae</code><code class="o">.</code><code class="n">coding_length</code><code class="p">),</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">)</code>
<code class="n">codings_prob</code> <code class="o">=</code> <code class="n">F</code><code class="o">.</code><code class="n">one_hot</code><code class="p">(</code><code class="n">codings</code><code class="p">,</code> <code class="n">num_classes</code><code class="o">=</code><code class="n">d_vae</code><code class="o">.</code><code class="n">n_codes</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">images</code> <code class="o">=</code> <code class="n">d_vae</code><code class="o">.</code><code class="n">decoder</code><code class="p">(</code><code class="n">codings_prob</code><code class="p">)</code></pre>

<p>Another popular approach to discrete VAEs<a data-type="indexterm" data-primary="vector quantization (VQ-VAE)" id="id4109"/><a data-type="indexterm" data-primary="VQ-VAE (vector quantization)" id="id4110"/> is called <em>vector quantization</em> (VQ-VAE), <a href="https://homl.info/vqvae">proposed by DeepMind researchers in 2017</a>.⁠<sup><a data-type="noteref" id="id4111-marker" href="ch18.html#id4111">11</a></sup> Instead of producing logits, the encoder outputs <em>d</em> embeddings, each of dimensionality <em>e</em>. Then instead of sampling from a categorical distribution, the VQ-VAE maps each embedding to the index of the nearest embedding in a trainable embedding matrix of shape [<em>k</em>, <em>e</em>], called the <em>codebook</em>. This produces the integer codes. Finally, these codes are embedded using the embedding matrix and passed on to the decoder.</p>

<p>Since replacing an embedding with the nearest codebook embedding is not a differentiable operation, the backward pass pretends that the codebook lookup step is the identity function, so the gradients just go straight through this operation: this is why this trick is called<a data-type="indexterm" data-primary="straight-through estimator (STE)" id="id4112"/><a data-type="indexterm" data-primary="STE (straight-through estimator)" id="id4113"/> the <em>straight-through estimator</em> (STE). It’s an approximation that assumes that the gradients around the encoder embeddings are similar to the gradients around the nearest codebook embeddings.</p>
<div data-type="tip"><h6>Tip</h6>
<p>VQ-VAEs can be a bit tricky to implement correctly, but you can use a library like <a href="https://github.com/lucidrains/vector-quantize-pytorch" class="bare"><em class="hyperlink">https://github.com/lucidrains/vector-quantize-pytorch</em></a>. On the positive side, training is more stable, and the codes are a bit easier to interpret.</p>
</div>

<p>Discrete VAEs work pretty well for small images, but not so much for large images: the small-scale features may look good, but there will often be large-scale inconsistencies. To improve on this, you can use the trained dVAE to encode your whole training set (so each instance becomes a sequence of integers), then use this new training set to train a transformer: just treat the codes as tokens, and train the transformer using next-token prediction. Intuitively, the dVAE learns the vocabulary, while the transformer learns the grammar. Once the transformer is trained, you can generate a new image by first generating a sequence of codes using the transformer, then passing this sequence to the dVAE’s decoder.</p>

<p>This two-stage approach also makes it easier to control the image generation process: when training the transformer, a textual description of the image can be fed to the transformer, for example as a prefix to the sequence of codes. We say that the transformer is <em>conditioned</em> on the description, which helps it predict the correct next code. This way, after training, we can guide the image generation process by providing a description of the image we desire. The transformer will use this description to generate the appropriate sequence of codes. This is exactly how the first DALL·E system worked.</p>

<p>In practice, the encoder and decoder are usually convolutional networks, so the latent representation<a data-type="indexterm" data-primary="latent representation of inputs" id="id4114"/> is often organized as a grid (but it’s still flattened to a sequence to train the transformer). For example, the encoder may output a tensor of shape [256, 32, 32]: that’s a 32 × 32 grid containing 256-dimensional embeddings in each cell (or 256 logits in the case of Gumbel-Softmax dVAEs). After mapping these embeddings to the indices of the nearest embeddings in the codebook (or after categorical sampling), each image is represented as a 32 × 32 grid of integers (codes), with codes ranging between 0 and 255. To generate a new image, you use the transformer to predict a sequence of 1,024 codes, organize them into a 32 × 32 grid, replace each code with its codebook vector, then pass the result to the decoder to generate the final image<a data-type="indexterm" data-startref="xi_FashionMNISTdatasetgeneratingimages1856071_1" id="id4115"/>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>To improve the image quality, you can also stack two or more dVAEs, each producing a smaller grid than the previous one: this is called a <em>hierarchical VAE</em> (HVAE)<a data-type="indexterm" data-primary="hierarchical VAE (HVAE)" id="id4116"/><a data-type="indexterm" data-primary="HVAE (hierarchical VAE)" id="id4117"/>. The encoders are stacked, followed by the decoders in reverse order, and all are trained jointly. The loss is the sum of a single reconstruction loss plus multiple KL divergence losses (one per dVAE)<a data-type="indexterm" data-startref="xi_autoencodersvariational1844638_1" id="id4118"/><a data-type="indexterm" data-startref="xi_variationalautoencodersVAEs1844638_1" id="id4119"/><a data-type="indexterm" data-startref="xi_variationalautoencodersVAEs1844638_2" id="id4120"/><a data-type="indexterm" data-startref="xi_VAEsvariationalautoencoders1844638_1" id="id4121"/><a data-type="indexterm" data-startref="xi_discretevariationalautoencodersdVAEs1860024_1" id="id4122"/><a data-type="indexterm" data-startref="xi_dVAEsdiscretevariationalautoencoders1860024_1" id="id4123"/><a data-type="indexterm" data-startref="xi_autoencoders182640_1" id="id4124"/>.</p>
</div>

<p>Let’s now turn our attention to GANs. They are harder to train, but when you manage to get them to work, they produce pretty amazing images.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Generative Adversarial Networks"><div class="sect1" id="id350">
<h1>Generative Adversarial Networks</h1>

<p>Generative adversarial networks<a data-type="indexterm" data-primary="generative adversarial networks (GANs)" id="xi_generativeadversarialnetworksGANs1868332_1"/><a data-type="indexterm" data-primary="deep neural networks (DNNs)" data-secondary="GANs" id="xi_generativeadversarialnetworksGANs1868332_2"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="GANs" id="xi_unsupervisedlearningGANs1868332_1"/><a data-type="indexterm" data-primary="GANs (generative adversarial networks)" id="xi_GANsgenerativeadversarialnetworks1868332_1"/> were proposed in a <a href="https://homl.info/gan">2014 paper</a>⁠<sup><a data-type="noteref" id="id4125-marker" href="ch18.html#id4125">12</a></sup> by Ian Goodfellow et al., and although the idea got researchers excited almost instantly, it took a few years to overcome some of the difficulties of training GANs. Like many great ideas, it seems simple in hindsight: make neural networks compete against each other in the hope that this competition will push them to excel. As shown in <a data-type="xref" href="#gan_diagram">Figure 18-15</a>, a GAN is composed of two neural networks:</p>
<dl>
<dt>Generator</dt>
<dd>
<p>Takes a random coding<a data-type="indexterm" data-primary="generator, GAN" id="xi_generatorGAN1868628_1"/><a data-type="indexterm" data-primary="images, classifying and generating" data-secondary="generating with GANs" id="xi_imagesclassifyingandgeneratinggeneratingwithGANs1868628_1"/> as input (typically sampled from a Gaussian distribution) and outputs some data—typically, an image. The coding is the latent representation of the image to be generated. So, as you can see, the generator offers the same functionality as a decoder in a variational autoencoder, and it can be used in the same way to generate new images: just feed it a random vector, and it outputs a brand-new image. However, it is trained very differently, as you will soon see.</p>
</dd>
<dt>Discriminator</dt>
<dd>
<p>Takes either a fake image from the generator<a data-type="indexterm" data-primary="discriminator, GAN" id="xi_discriminatorGAN1868845_1"/> or a real image from the training set as input, and must guess whether the input image is fake or real.</p>
</dd>
</dl>

<figure class="width-60"><div id="gan_diagram" class="figure">
<img src="assets/hmls_1815.png" alt="Diagram illustrating a generative adversarial network (GAN) where the generator produces fake images from a random vector to trick the discriminator, which aims to distinguish between fake and real images." width="926" height="971"/>
<h6><span class="label">Figure 18-15. </span>A generative adversarial network</h6>
</div></figure>

<p>During training, the generator and the discriminator have opposite goals: the discriminator tries to tell fake images from real images, while the generator tries to produce images that look real enough to trick the discriminator. Because the GAN is composed of two networks with different objectives, it cannot be trained like a regular neural network. Each training iteration is divided into two phases:</p>
<dl>
<dt>First phase: train the discriminator</dt>
<dd>
<p>A batch of real images is sampled from the training set and is completed with an equal number of fake images produced by the generator. The labels are set to 0 for fake images and 1 for real images, and the discriminator is trained on this labeled batch for one step, using the binary cross-entropy loss. Importantly, backpropagation<a data-type="indexterm" data-primary="backpropagation" id="id4126"/><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="backpropagation" id="id4127"/> only optimizes the weights of the discriminator during this phase.</p>
</dd>
<dt>Second phase: train the generator</dt>
<dd>
<p>We first use the generator to produce another batch of fake images, and once again the discriminator is used to tell whether the images are fake or real. This time we do not add real images to the batch, and all the labels are set to 1 (real); in other words, we want the generator to produce images that the discriminator will (wrongly) believe to be real! Crucially, the weights of the discriminator are frozen during this step, so backpropagation only affects the weights of the generator.</p>
</dd>
</dl>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The generator never actually sees any real images, yet it gradually learns to produce convincing fake images! All it gets is the gradients flowing back through the discriminator. Fortunately, the better the discriminator gets, the more information about the real images is contained in these secondhand gradients, so the generator can make significant progress.</p>
</div>

<p>Let’s go ahead and build a simple GAN for Fashion MNIST.</p>

<p>First, we need to build the generator and the discriminator. The generator is similar to an autoencoder’s decoder—it takes a coding vector as input and outputs an 
<span class="keep-together">image—and</span> the discriminator is a regular binary classifier—it takes an image as input and ends with a dense layer<a data-type="indexterm" data-primary="dense layers" id="id4128"/> containing a single unit and using the sigmoid activation function:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">codings_dim</code> <code class="o">=</code> <code class="mi">32</code>
<code class="n">generator</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">codings_dim</code><code class="p">,</code> <code class="mi">128</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">256</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">256</code><code class="p">,</code> <code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Unflatten</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">unflattened_size</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">)))</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
<code class="n">discriminator</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">256</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">256</code><code class="p">,</code> <code class="mi">128</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">())</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>Since the training loop is unusual, we need a new training function:</p>

<pre translate="no" data-type="programlisting" data-code-language="python" class="widows13"><code class="k">def</code> <code class="nf">train_gan</code><code class="p">(</code><code class="n">generator</code><code class="p">,</code> <code class="n">discriminator</code><code class="p">,</code> <code class="n">train_loader</code><code class="p">,</code> <code class="n">codings_dim</code><code class="p">,</code> <code class="n">n_epochs</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code>
              <code class="n">g_lr</code><code class="o">=</code><code class="mf">1e-3</code><code class="p">,</code> <code class="n">d_lr</code><code class="o">=</code><code class="mf">5e-4</code><code class="p">):</code>
    <code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BCELoss</code><code class="p">()</code>
    <code class="n">generator_opt</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">NAdam</code><code class="p">(</code><code class="n">generator</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="n">g_lr</code><code class="p">)</code>
    <code class="n">discriminator_opt</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">NAdam</code><code class="p">(</code><code class="n">discriminator</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="n">d_lr</code><code class="p">)</code>
    <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_epochs</code><code class="p">):</code>
        <code class="k">for</code> <code class="n">real_images</code><code class="p">,</code> <code class="n">_</code> <code class="ow">in</code> <code class="n">train_loader</code><code class="p">:</code>
            <code class="n">real_images</code> <code class="o">=</code> <code class="n">real_images</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
            <code class="n">pred_real</code> <code class="o">=</code> <code class="n">discriminator</code><code class="p">(</code><code class="n">real_images</code><code class="p">)</code>
            <code class="n">batch_size</code> <code class="o">=</code> <code class="n">real_images</code><code class="o">.</code><code class="n">size</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
            <code class="n">ones</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">ones</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">)</code>
            <code class="n">real_loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">pred_real</code><code class="p">,</code> <code class="n">ones</code><code class="p">)</code>
            <code class="n">codings</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="n">codings_dim</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">)</code>
            <code class="n">fake_images</code> <code class="o">=</code> <code class="n">generator</code><code class="p">(</code><code class="n">codings</code><code class="p">)</code><code class="o">.</code><code class="n">detach</code><code class="p">()</code>
            <code class="n">pred_fake</code> <code class="o">=</code> <code class="n">discriminator</code><code class="p">(</code><code class="n">fake_images</code><code class="p">)</code>
            <code class="n">zeros</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">)</code>
            <code class="n">fake_loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">pred_fake</code><code class="p">,</code> <code class="n">zeros</code><code class="p">)</code>
            <code class="n">discriminator_loss</code> <code class="o">=</code> <code class="n">real_loss</code> <code class="o">+</code> <code class="n">fake_loss</code>
            <code class="n">discriminator_opt</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
            <code class="n">discriminator_loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
            <code class="n">discriminator_opt</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>

            <code class="n">codings</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="n">codings_dim</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">)</code>
            <code class="n">fake_images</code> <code class="o">=</code> <code class="n">generator</code><code class="p">(</code><code class="n">codings</code><code class="p">)</code>
            <code class="k">for</code> <code class="n">p</code> <code class="ow">in</code> <code class="n">discriminator</code><code class="o">.</code><code class="n">parameters</code><code class="p">():</code>
                <code class="n">p</code><code class="o">.</code><code class="n">requires_grad</code> <code class="o">=</code> <code class="kc">False</code>
            <code class="n">pred_fake</code> <code class="o">=</code> <code class="n">discriminator</code><code class="p">(</code><code class="n">fake_images</code><code class="p">)</code>
            <code class="n">generator_loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">pred_fake</code><code class="p">,</code> <code class="n">ones</code><code class="p">)</code>
            <code class="n">generator_opt</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
            <code class="n">generator_loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
            <code class="n">generator_opt</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
            <code class="k">for</code> <code class="n">p</code> <code class="ow">in</code> <code class="n">discriminator</code><code class="o">.</code><code class="n">parameters</code><code class="p">():</code>
                <code class="n">p</code><code class="o">.</code><code class="n">requires_grad</code> <code class="o">=</code> <code class="kc">True</code></pre>

<p>As discussed earlier, you can see the two phases at each iteration: first the discriminator makes a gradient descent step, then it’s the generator’s turn. We use a separate optimizer for each. Let’s look in more detail:</p>
<dl>
<dt>Phase one</dt>
<dd>
<p>We feed a batch of real images to the discriminator and compute the loss given targets equal to one; indeed, we want the discriminator to predict that these images are real. We then generate some random codings and feed them to the generator to produce some fake images. Note that we call <code translate="no">detach()</code> on these images because we don’t want gradient descent to affect the generator in this phase. Then we pass these fake images to the discriminator and compute the loss given targets equal to zero; we want the discriminator to predict that these images are fake. The total discriminator loss is the <code translate="no">real_loss</code> plus the <code translate="no">fake_loss</code>. Finally, we perform the gradient descent step, improving the 
<span class="keep-together">discriminator.</span></p>
</dd>
<dt>Phase two</dt>
<dd>
<p>We generate some fake images using the generator, and we pass them to the discriminator, like we just did. However, this time we don’t call <code translate="no">detach()</code> on the fake images since we want to train the generator. Moreover, we make the discriminator untrainable by setting <code translate="no">p.required_grad = False</code> for each parameter <code translate="no">p</code>. We then compute the loss using targets equal to one: indeed, we want the generator to fool the discriminator, so we want the discriminator to wrongly predict that these are real images. And finally, we perform a gradient descent step for the generator, and we make the discriminator trainable again.</p>
</dd>
</dl>

<p>That’s it! After training, you can randomly sample some codings from a Gaussian distribution and feed them to the generator to produce new images:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">generator</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
<code class="n">codings</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="n">n_images</code><code class="p">,</code> <code class="n">codings_dim</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">)</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">generated_images</code> <code class="o">=</code> <code class="n">generator</code><code class="p">(</code><code class="n">codings</code><code class="p">)</code></pre>

<p>If you display the generated images (see <a data-type="xref" href="#gan_generated_images_plot">Figure 18-16</a>), you will see that at the end of the first epoch, they already start to look like (very noisy) Fashion MNIST images.</p>

<figure><div id="gan_generated_images_plot" class="figure">
<img src="assets/hmls_1816.png" alt="Noisy, black-and-white images attempting to depict Fashion MNIST items, generated by a GAN after one epoch of training." width="1234" height="460"/>
<h6><span class="label">Figure 18-16. </span>Images generated by the GAN after one epoch of training</h6>
</div></figure>

<p>Unfortunately, the images never really get much better than that, and you may even find epochs where the GAN seems to be forgetting what it learned. Why is that? Well, it turns out that training a GAN can be challenging. Let’s see why.</p>








<section data-type="sect2" data-pdf-bookmark="The Difficulties of Training GANs"><div class="sect2" id="id351">
<h2>The Difficulties of Training GANs</h2>

<p>During training, the generator and the discriminator constantly try to outsmart each other in a zero-sum game. As training advances, the game may end up in a state that game theorists<a data-type="indexterm" data-primary="Nash equilibrium" id="id4129"/> call a <em>Nash equilibrium</em>, named after the mathematician John Nash. This occurs when no player would be better off changing their own strategy, assuming the other players do not change theirs. For example, a Nash equilibrium is reached when everyone drives on the left side of the road: no driver would be better off being the only one to switch sides. Of course, there is a second possible Nash equilibrium: when everyone drives on the <em>right</em> side of the road. Different initial states and dynamics may lead to one equilibrium or the other. In this example, there is a single optimal strategy once an equilibrium is reached (i.e., driving on the same side as everyone else), but a Nash equilibrium can involve multiple competing strategies (e.g., a predator chases its prey, the prey tries to escape, and neither would be better off changing their strategy).</p>

<p>So how does this apply to GANs? Well, the authors of the GAN paper demonstrated that a GAN can only reach a single Nash equilibrium: that’s when the generator produces perfectly realistic images, and the discriminator is forced to guess (50% real, 50% fake). This fact is very encouraging, as it would seem that you just need to train the GAN long enough and it will eventually reach this equilibrium, giving you a perfect generator. Unfortunately, it’s not that simple: nothing guarantees that the equilibrium will ever be reached.</p>

<p class="pagebreak-before">The biggest difficulty<a data-type="indexterm" data-primary="mode collapse" id="id4130"/> is called <em>mode collapse</em>: when the generator’s outputs gradually become less diverse. How can this happen? Suppose the generator gets better at producing convincing shoes than any other class. It will fool the discriminator a bit more with shoes, and this will encourage it to produce even more images of shoes. Gradually, it will forget how to produce anything else. Meanwhile, the only fake images that the discriminator will see will be shoes, so it will also forget how to discriminate fake images of other classes. Eventually, when the discriminator manages to discriminate the fake shoes from the real ones, the generator will be forced to move to another class. It may then become good at shirts, forgetting about shoes, and the discriminator will follow. The GAN may gradually cycle across a few classes, never really becoming very good at any of them (see the top row of <a data-type="xref" href="#gan_mode_collapse_diagram">Figure 18-17</a>).</p>

<figure><div id="gan_mode_collapse_diagram" class="figure">
<img src="assets/hmls_1817.png" alt="Diagram illustrating mode collapse in GAN training, showing clustered fake examples in the top row versus diverse results in the bottom row with successful training." width="1438" height="899"/>
<h6><span class="label">Figure 18-17. </span>Mode collapse while training a GAN (top row) versus successful training without mode collapse (bottom row)</h6>
</div></figure>

<p>Moreover, because the generator and the discriminator are constantly pushing against each other, their parameters may end up oscillating and becoming unstable. Training may begin properly, then suddenly diverge for no apparent reason due to these instabilities. And since many factors affect these complex dynamics, GANs are very sensitive to the hyperparameters<a data-type="indexterm" data-primary="hyperparameters" data-secondary="GAN challenges" id="id4131"/>: you may have to spend a lot of effort fine-tuning them.</p>

<p>These problems have kept researchers very busy since 2014. Many papers have been published on this topic, some proposing new cost functions⁠<sup><a data-type="noteref" id="id4132-marker" href="ch18.html#id4132">13</a></sup> (though a <a href="https://homl.info/gansequal">2018 paper</a>⁠<sup><a data-type="noteref" id="id4133-marker" href="ch18.html#id4133">14</a></sup> by Google researchers questions their efficiency) or techniques to stabilize training or to avoid the mode collapse issue. For example, a popular technique called <em>experience replay</em> consists of storing the images produced by the generator at each iteration in a replay buffer (gradually dropping older generated images) and training the discriminator using real images plus fake images drawn from this buffer (rather than just fake images produced by the current generator). This reduces the chances that the discriminator will overfit the latest generator’s outputs. Another common technique<a data-type="indexterm" data-primary="mini-batch discrimination" id="id4134"/> is called <em>mini-batch discrimination</em>: it measures how similar images are across the batch and provides this statistic to the discriminator, so it can easily reject a whole batch of fake images that lack diversity. This encourages the generator to produce a greater variety of images, reducing the chance of mode collapse (see the bottom row of <a data-type="xref" href="#gan_mode_collapse_diagram">Figure 18-17</a>).<a data-type="indexterm" data-startref="xi_generatorGAN1868628_1" id="id4135"/><a data-type="indexterm" data-startref="xi_discriminatorGAN1868845_1" id="id4136"/><a data-type="indexterm" data-startref="xi_imagesclassifyingandgeneratinggeneratingwithGANs1868628_1" id="id4137"/></p>

<p>In short, this was a very active field of research, and much progress was made until quite recently<a data-type="indexterm" data-primary="deep convolutional GANS (DCGANs)" id="id4138"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="DCGANs" id="id4139"/><a data-type="indexterm" data-primary="DCGANs (deep convolutional GANS)" id="id4140"/>: from <em>deep Convolutional GANs</em> (DCGANs) based on convolutional layers (see the notebook for an example)<a data-type="indexterm" data-primary="progressively growing GANs" id="id4141"/>, to <em>progressively growing GANs</em> that could produce high-resolution images<a data-type="indexterm" data-primary="StyleGANs" id="id4142"/>, or <em>StyleGANs</em> that gave the user fine-grained control over the image generation process, it seemed like GANs had a bright future ahead of them<a data-type="indexterm" data-startref="xi_generativeadversarialnetworksGANs1868332_1" id="id4143"/><a data-type="indexterm" data-startref="xi_generativeadversarialnetworksGANs1868332_2" id="id4144"/><a data-type="indexterm" data-startref="xi_unsupervisedlearningGANs1868332_1" id="id4145"/><a data-type="indexterm" data-startref="xi_GANsgenerativeadversarialnetworks1868332_1" id="id4146"/>. But when diffusion models started to produce amazing images as well, with a much more stable training process and more diverse images, GANs were quickly sidelined. So let’s now turn our attention to diffusion models.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Diffusion Models"><div class="sect1" id="id352">
<h1>Diffusion Models</h1>

<p>The ideas behind diffusion models<a data-type="indexterm" data-primary="diffusion models" id="xi_diffusionmodels1880134_1"/><a data-type="indexterm" data-primary="images, classifying and generating" data-secondary="diffusion models" id="xi_imagesclassifyingandgeneratingdiffusionmodels1880134_1"/> have been around for many years, but they were first formalized in their modern form in a <a href="https://homl.info/diffusion">2015 paper</a>⁠<sup><a data-type="noteref" id="id4147-marker" href="ch18.html#id4147">15</a></sup> by Jascha Sohl-Dickstein et al. from Stanford University and UC Berkeley. The authors applied tools from statistical mechanics to model a diffusion process, similar to a drop of milk diffusing in a cup of tea. The core idea is to train a model to learn the reverse process<a data-type="indexterm" data-primary="forward and reverse processes, diffusion model" id="xi_forwardandreverseprocessesdiffusionmodel18801580_1"/><a data-type="indexterm" data-primary="reverse and forward processes, diffusion model" id="xi_reverseandforwardprocessesdiffusionmodel18801580_1"/>: start from the completely mixed state and gradually “unmix” the milk from the tea. Using this idea, they obtained promising results in image generation, but since GANs produced more convincing images back then, and they did so much faster, diffusion models did not get as much attention.</p>

<p>Then, in 2020, <a href="https://homl.info/ddpm">Jonathan Ho et al.</a>, also from UC Berkeley, managed to build a diffusion model capable of generating highly realistic images, which they called a <em>denoising diffusion probabilistic model</em> (DDPM).⁠<sup><a data-type="noteref" id="id4148-marker" href="ch18.html#id4148">16</a></sup> A few months later, a <a href="https://homl.info/ddpm2">2021 paper</a>⁠<sup><a data-type="noteref" id="id4149-marker" href="ch18.html#id4149">17</a></sup> by OpenAI researchers Alex Nichol and Prafulla Dhariwal analyzed the DDPM<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="diffusion models" id="xi_unsupervisedlearningdiffusionmodels18803560_1"/> architecture and proposed several improvements that allowed DDPMs<a data-type="indexterm" data-primary="DDPM (Denoising Diffusion Probabilistic Model)" id="xi_DDPMDenoisingDiffusionProbabilisticModel18803626_1"/><a data-type="indexterm" data-primary="denoising diffusion probabilistic model (DDPM)" id="xi_denoisingdiffusionprobabilisticmodelDDPM18803626_1"/> to finally beat GANs: not only are DDPMs much easier to train than GANs, but the generated images are more diverse and of even higher quality. The main downside of DDPMs, as you will see, is that they take a very long time to generate images, compared to GANs or VAEs.</p>

<p>So how exactly does a DDPM work? Well, suppose you start with a picture of a cat (like the one in <a data-type="xref" href="#denoising_model_diagram">Figure 18-18</a>), noted <strong>x</strong><sub>0</sub>, and at each time step <em>t</em> you add a little bit of Gaussian noise to the image, with mean 0 and variance <em>β</em><sub><em>t</em></sub> (a scalar). This noise is independent for each pixel (using the same mean and variance)<a data-type="indexterm" data-primary="isotropic noise" id="id4150"/>: we call it <em>isotropic</em>. You first obtain the image <strong>x</strong><sub>1</sub>, then <strong>x</strong><sub>2</sub>, and so on, until the cat is completely hidden by the noise, impossible to see. The last time step is noted <em>T</em>. In the original DDPM paper, the authors used <em>T</em> = 1,000, and they scheduled the variance <em>β</em><sub><em>t</em></sub> in such a way that the cat signal fades linearly between time steps 0 and <em>T</em>. In the improved DDPM paper, <em>T</em> was bumped up to 4,000, and the variance schedule was tweaked to change more slowly at the beginning and at the end. In short, we’re gradually drowning the cat in noise: this is called the <em>forward process</em>.</p>

<figure><div id="denoising_model_diagram" class="figure">
<img src="assets/hmls_1818.png" alt="Diagram of the denoising diffusion probabilistic model (DDPM) showing the forward and reverse process, where a cat image gradually becomes obscured by noise until details are lost, illustrating how noise is added and then removed." width="1441" height="532"/>
<h6><span class="label">Figure 18-18. </span>The forward process <em>q</em> and reverse process <em>p</em></h6>
</div></figure>

<p>As we add more and more Gaussian noise in the forward process, the distribution of pixel values becomes more and more Gaussian<a data-type="indexterm" data-primary="Gaussian distribution" id="id4151"/>. One important detail I left out is that the pixel values get rescaled slightly at each step, by a factor of <math alttext="StartRoot 1 minus beta Subscript t Baseline EndRoot">
  <msqrt>
    <mrow>
      <mn>1</mn>
      <mo>-</mo>
      <msub><mi>β</mi> <mi>t</mi> </msub>
    </mrow>
  </msqrt>
</math>. This ensures that the mean of the pixel values gradually approaches 0, since the scaling factor is a bit smaller than 1 (imagine repeatedly multiplying a number by 0.99). It also ensures that the variance will gradually converge to 1. This is because the standard deviation of the pixel values also gets scaled by <math alttext="StartRoot 1 minus beta Subscript t Baseline EndRoot">
  <msqrt>
    <mrow>
      <mn>1</mn>
      <mo>-</mo>
      <msub><mi>β</mi> <mi>t</mi> </msub>
    </mrow>
  </msqrt>
</math>, so the variance gets scaled by 1 – <em>β</em><sub><em>t</em></sub> (i.e., the square of the scaling factor). But the variance cannot shrink to 0 since we’re adding Gaussian noise with variance <em>β</em><sub><em>t</em></sub> at each step. And since variances add up when you sum Gaussian distributions, the variance must converge to 
<span class="keep-together">1 – <em>β</em><sub><em>t</em></sub> + <em>β</em><sub><em>t</em></sub> = 1.</span></p>

<p>The forward diffusion process is summarized in <a data-type="xref" href="#forward_process_equation">Equation 18-5</a>. This equation won’t teach you anything new about the forward process, but it’s useful to understand this type of mathematical notation, as it’s often used in ML papers. This equation defines the probability distribution <em>q</em> of <strong>x</strong><sub><em>t</em></sub>, given <strong>x</strong><sub><em>t</em>–1</sub> as a Gaussian distribution with mean <strong>x</strong><sub><em>t</em>–1</sub> times the scaling factor, and with a covariance matrix equal to <em>β</em><sub><em>t</em></sub><strong>I</strong>. This is the identity matrix <strong>I</strong> multiplied by <em>β</em><sub><em>t</em></sub>, which means that the noise is isotropic with variance <em>β</em><sub><em>t</em></sub>.</p>
<div id="forward_process_equation" data-type="equation">
<h5><span class="label">Equation 18-5. </span>Probability distribution <em>q</em> of the forward diffusion process</h5>
<math alttext="q left-parenthesis bold x Subscript t Baseline vertical-bar bold x Subscript t minus 1 Baseline right-parenthesis equals script upper N left-parenthesis StartRoot 1 minus beta Subscript t Baseline EndRoot bold x Subscript t minus 1 Baseline comma beta Subscript t Baseline bold upper I right-parenthesis" display="block">
  <mrow>
    <mi>q</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>𝐱</mi> <mi>t</mi> </msub>
      <mspace width="0.166667em"/>
      <mo>|</mo>
      <mspace width="0.166667em"/>
      <msub><mi>𝐱</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> </msub>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mi>𝒩</mi>
    <mrow>
      <mo>(</mo>
      <msqrt>
        <mrow>
          <mn>1</mn>
          <mo>-</mo>
          <msub><mi>β</mi> <mi>t</mi> </msub>
        </mrow>
      </msqrt>
      <msub><mi>𝐱</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> </msub>
      <mo lspace="0%" rspace="0%">,</mo>
      <mspace width="0.166667em"/>
      <msub><mi>β</mi> <mi>t</mi> </msub>
      <mi>𝐈</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>Interestingly, there’s a shortcut for the forward process: it’s possible to sample an image <strong>x</strong><sub><em>t</em></sub> given <strong>x</strong><sub>0</sub> without having to first compute <strong>x</strong><sub>1</sub>, <strong>x</strong><sub>2</sub>, …​, <strong>x</strong><sub><em>t</em>–1</sub>. Indeed, since the sum of multiple independent Gaussian distributions is also a Gaussian distribution, all the noise can be added in just one shot. If we define <em>α</em><sub><em>t</em></sub> = 1 – <em>β</em><sub><em>t</em></sub>, and <em>α̅</em><sub><em>t</em></sub> = <em>α</em><sub><em>1</em></sub> × <em>α</em><sub><em>2</em></sub> × …​× <em>α</em><sub><em>t</em></sub> = <math alttext="alpha overbar Subscript t Baseline equals product Underscript i equals 1 Overscript t Endscripts alpha Subscript t">
  <mrow>
    <msub><mover><mi>α</mi> <mo>¯</mo></mover> <mi>t</mi> </msub>
    <mo>=</mo>
    <msubsup><mo>∏</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>t</mi> </msubsup>
    <msub><mi>α</mi> <mi>t</mi> </msub>
  </mrow>
</math>, then we can compute <strong>x</strong><sub><em>t</em></sub> using <a data-type="xref" href="#fast_forward_process_equation">Equation 18-6</a>. This is the equation we will be using, as it is much faster.</p>
<div id="fast_forward_process_equation" data-type="equation">
<h5><span class="label">Equation 18-6. </span>Shortcut for the forward diffusion process</h5>
<math alttext="q left-parenthesis bold x Subscript t Baseline vertical-bar bold x 0 right-parenthesis equals script upper N left-parenthesis StartRoot alpha overbar Subscript t Baseline EndRoot bold x 0 comma left-parenthesis 1 minus alpha overbar Subscript t Baseline right-parenthesis bold upper I right-parenthesis" display="block">
  <mrow>
    <mi>q</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>𝐱</mi> <mi>t</mi> </msub>
      <mspace width="0.166667em"/>
      <mo>|</mo>
      <mspace width="0.166667em"/>
      <msub><mi>𝐱</mi> <mn>0</mn> </msub>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mi>𝒩</mi>
    <mrow>
      <mo>(</mo>
      <msqrt>
        <msub><mover><mi>α</mi> <mo>¯</mo></mover> <mi>t</mi> </msub>
      </msqrt>
      <msub><mi>𝐱</mi> <mn>0</mn> </msub>
      <mo lspace="0%" rspace="0%">,</mo>
      <mspace width="0.166667em"/>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>-</mo>
        <msub><mover><mi>α</mi> <mo>¯</mo></mover> <mi>t</mi> </msub>
        <mo>)</mo>
      </mrow>
      <mi>𝐈</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>Our goal, of course, is not to drown cats in noise. On the contrary, we want to create many new cats! We can do so by training a model that can perform the <em>reverse process</em>: going from <strong>x</strong><sub><em>t</em></sub> to <strong>x</strong><sub><em>t</em>–1</sub>. We can then use it to remove a tiny bit of noise from an image, and repeat the operation many times until all the noise is gone. It’s not a basic noise filter that relies only on the neighboring pixels: instead, when noise is removed, it is replaced with realistic pixels, depending on the training data. For example, if we train the model on a dataset containing many cat images, then we can give it a picture entirely full of Gaussian noise, and the model will gradually make a brand new cat appear (see <a data-type="xref" href="#denoising_model_diagram">Figure 18-18</a>).</p>

<p>OK, so let’s start coding! The first thing we need to do is to code the forward process. For this, we will first need to implement the variance schedule. How can we control how fast the cat disappears? At each time step <em>t</em>, the pixel values get multiplied by <math alttext="StartRoot 1 minus beta Subscript t Baseline EndRoot">
  <msqrt>
    <mrow>
      <mn>1</mn>
      <mo>-</mo>
      <msub><mi>β</mi> <mi>t</mi> </msub>
    </mrow>
  </msqrt>
</math> and noise with mean 0 and variance <em>β</em><sub><em>t</em></sub> gets added (as explained earlier). So, the part of the image’s variance that comes from the original cat image shrinks by a factor of <em>α</em><sub><em>t</em></sub> = 1 – \beta_t at each step. After <em>t</em> time steps, it will have shrunk by a factor of <em>α̅</em><sub><em>t</em></sub> = <em>α</em><sub><em>1</em></sub> × <em>α</em><sub><em>2</em></sub> × …​ × <em>α</em><sub><em>t</em></sub>. It’s this “cat signal” factor <em>α̅</em><sub><em>t</em></sub> that we want to schedule so it shrinks down from 1 to 0 gradually between time steps 0 and <em>T</em>. In the improved DDPM paper, the authors schedule <em>α̅</em><sub><em>t</em></sub> according to <a data-type="xref" href="#variance_schedule_equation">Equation 18-7</a>. This schedule is represented in <a data-type="xref" href="#variance_schedule_plot">Figure 18-19</a>.</p>
<div id="variance_schedule_equation" data-type="equation">
<h5><span class="label">Equation 18-7. </span>Variance schedule equation for the forward diffusion process</h5>
<math alttext="beta Subscript t Baseline equals 1 minus StartFraction alpha overbar Subscript t Baseline Over alpha overbar Subscript t minus 1 Baseline EndFraction with alpha overbar Subscript t Baseline equals StartFraction f left-parenthesis t right-parenthesis Over f left-parenthesis 0 right-parenthesis EndFraction and f left-parenthesis t right-parenthesis equals cosine squared left-parenthesis StartStartFraction StartFraction t Over upper T EndFraction plus s OverOver 1 plus s EndEndFraction dot StartFraction pi Over 2 EndFraction right-parenthesis" display="block">
  <mrow>
    <msub><mi>β</mi> <mi>t</mi> </msub>
    <mo>=</mo>
    <mn>1</mn>
    <mo>-</mo>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac><msub><mover><mi>α</mi> <mo>¯</mo></mover> <mi>t</mi> </msub> <msub><mover><mi>α</mi> <mo>¯</mo></mover> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> </msub></mfrac>
    </mstyle>
    <mspace width="1.em"/>
    <mtext>with</mtext>
    <mspace width="4.pt"/>
    <msub><mover><mi>α</mi> <mo>¯</mo></mover> <mi>t</mi> </msub>
    <mo>=</mo>
    <mfrac><mrow><mi>f</mi><mo>(</mo><mi>t</mi><mo>)</mo></mrow> <mrow><mi>f</mi><mo>(</mo><mn>0</mn><mo>)</mo></mrow></mfrac>
    <mspace width="4.pt"/>
    <mtext>and</mtext>
    <mspace width="4.pt"/>
    <mi>f</mi>
    <mrow>
      <mo>(</mo>
      <mi>t</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <msup><mo form="prefix">cos</mo> <mn>2</mn> </msup>
    <mrow>
      <mo>(</mo>
      <mfrac><mrow><mfrac><mi>t</mi> <mi>T</mi></mfrac><mo>+</mo><mi>s</mi></mrow> <mrow><mn>1</mn><mo>+</mo><mi>s</mi></mrow></mfrac>
      <mo>·</mo>
      <mfrac><mi>π</mi> <mn>2</mn></mfrac>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><em>s</em> is a tiny value which prevents <em>β</em><sub><em>t</em></sub> from being too small near <em>t</em> = 0. In the paper, the authors used <em>s</em> = 0.008.</p>
</li>
<li>
<p><em>β</em><sub><em>t</em></sub> is clipped to be no larger than 0.999 to avoid instabilities near <em>t</em> = <em>T</em>.</p>
</li>
</ul>

<figure class="width-80"><div id="variance_schedule_plot" class="figure">
<img src="assets/hmls_1819.png" alt="hmls 1819" width="1668" height="758"/>
<h6><span class="label">Figure 18-19. </span>Noise variance schedule <em>β</em><sub><em>t</em></sub>, and the remaining signal variance <em>α̅</em><sub><em>t</em></sub></h6>
</div></figure>

<p>Let’s create a small function to compute <em>α</em><sub><em>t</em></sub>, <em>β</em><sub><em>t</em></sub>, and <em>α̅</em><sub><em>t</em></sub>, using <a data-type="xref" href="#variance_schedule_equation">Equation 18-7</a>, and call this function with <em>T</em> = 4,000:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">variance_schedule</code><code class="p">(</code><code class="n">T</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mf">0.008</code><code class="p">,</code> <code class="n">max_beta</code><code class="o">=</code><code class="mf">0.999</code><code class="p">):</code>
    <code class="n">t</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">T</code><code class="p">,</code> <code class="n">T</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code>
    <code class="n">f</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">cos</code><code class="p">((</code><code class="n">t</code> <code class="o">/</code> <code class="n">T</code> <code class="o">+</code> <code class="n">s</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="n">s</code><code class="p">)</code> <code class="o">*</code> <code class="n">torch</code><code class="o">.</code><code class="n">pi</code> <code class="o">/</code> <code class="mi">2</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code>
    <code class="n">alpha_bars</code> <code class="o">=</code> <code class="n">f</code> <code class="o">/</code> <code class="n">f</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
    <code class="n">betas</code> <code class="o">=</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="p">(</code><code class="n">f</code><code class="p">[</code><code class="mi">1</code><code class="p">:]</code> <code class="o">/</code> <code class="n">f</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">]))</code><code class="o">.</code><code class="n">clamp</code><code class="p">(</code><code class="nb">max</code><code class="o">=</code><code class="n">max_beta</code><code class="p">)</code>
    <code class="n">betas</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">cat</code><code class="p">([</code><code class="n">torch</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="mi">1</code><code class="p">),</code> <code class="n">betas</code><code class="p">])</code>  <code class="c1"># for easier indexing</code>
    <code class="n">alphas</code> <code class="o">=</code> <code class="mi">1</code> <code class="o">-</code> <code class="n">betas</code>
    <code class="k">return</code> <code class="n">alphas</code><code class="p">,</code> <code class="n">betas</code><code class="p">,</code> <code class="n">alpha_bars</code>

<code class="n">T</code> <code class="o">=</code> <code class="mi">4000</code>
<code class="n">alphas</code><code class="p">,</code> <code class="n">betas</code><code class="p">,</code> <code class="n">alpha_bars</code> <code class="o">=</code> <code class="n">variance_schedule</code><code class="p">(</code><code class="n">T</code><code class="p">)</code></pre>

<p>To train our model to reverse the diffusion process, we will need noisy images from different time steps of the forward process. For this, let’s create a function that will take an image <strong>x</strong><sub>0</sub> and a time step <em>t</em> using <a data-type="xref" href="#fast_forward_process_equation">Equation 18-6</a>, and return a noisy image <strong>x</strong><sub><em>t</em></sub>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">forward_diffusion</code><code class="p">(</code><code class="n">x0</code><code class="p">,</code> <code class="n">t</code><code class="p">):</code>
    <code class="n">eps</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randn_like</code><code class="p">(</code><code class="n">x0</code><code class="p">)</code>  <code class="c1"># this unscaled noise will be the target</code>
    <code class="n">xt</code> <code class="o">=</code> <code class="n">alpha_bars</code><code class="p">[</code><code class="n">t</code><code class="p">]</code><code class="o">.</code><code class="n">sqrt</code><code class="p">()</code> <code class="o">*</code> <code class="n">x0</code> <code class="o">+</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">alpha_bars</code><code class="p">[</code><code class="n">t</code><code class="p">])</code><code class="o">.</code><code class="n">sqrt</code><code class="p">()</code> <code class="o">*</code> <code class="n">eps</code>
    <code class="k">return</code> <code class="n">xt</code><code class="p">,</code> <code class="n">eps</code></pre>

<p>The model will need both the noisy image <strong>x</strong><sub><em>t</em></sub> and the time step <em>t</em>, so let’s create a small class that will hold both. We’ll give it a handy <code translate="no">to()</code> method to move both <strong>x</strong><sub><em>t</em></sub> and <em>t</em> to the GPU:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">DiffusionSample</code><code class="p">(</code><code class="n">namedtuple</code><code class="p">(</code><code class="s2">"DiffusionSampleBase"</code><code class="p">,</code> <code class="p">[</code><code class="s2">"xt"</code><code class="p">,</code> <code class="s2">"t"</code><code class="p">])):</code>
    <code class="k">def</code> <code class="nf">to</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">device</code><code class="p">):</code>
        <code class="k">return</code> <code class="n">DiffusionSample</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">xt</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">),</code> <code class="bp">self</code><code class="o">.</code><code class="n">t</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">))</code></pre>

<p>Next, let’s create a dataset wrapper class. It takes an image dataset—Fashion MNIST in our case—and preprocesses the images so their pixel values range between –1 and +1 (this is optional but usually works better), and it uses the <code translate="no">forward_diffusion()</code> function to add noise to the image. Then it wraps the resulting noisy image as well as the time step in a <code translate="no">DiffusionSample</code> object, and returns it along with the target, which is the unscaled noise <code translate="no">eps</code>, before it was scaled by <math alttext="StartRoot 1 minus alpha overbar Subscript t Baseline EndRoot">
  <msqrt>
    <mrow>
      <mn>1</mn>
      <mo>-</mo>
      <msub><mover><mi>α</mi> <mo>¯</mo></mover> <mi>t</mi> </msub>
    </mrow>
  </msqrt>
</math> and added to the image:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">DiffusionDataset</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">dataset</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code>

    <code class="k">def</code> <code class="fm">__getitem__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">i</code><code class="p">):</code>
        <code class="n">x0</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">dataset</code><code class="p">[</code><code class="n">i</code><code class="p">]</code>
        <code class="n">x0</code> <code class="o">=</code> <code class="p">(</code><code class="n">x0</code> <code class="o">*</code> <code class="mi">2</code><code class="p">)</code> <code class="o">-</code> <code class="mi">1</code>  <code class="c1"># scale from –1 to +1</code>
        <code class="n">t</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">T</code> <code class="o">+</code> <code class="mi">1</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
        <code class="n">xt</code><code class="p">,</code> <code class="n">eps</code> <code class="o">=</code> <code class="n">forward_diffusion</code><code class="p">(</code><code class="n">x0</code><code class="p">,</code> <code class="n">t</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">DiffusionSample</code><code class="p">(</code><code class="n">xt</code><code class="p">,</code> <code class="n">t</code><code class="p">),</code> <code class="n">eps</code>

    <code class="k">def</code> <code class="fm">__len__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="k">return</code> <code class="nb">len</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dataset</code><code class="p">)</code>

<code class="n">train_set</code> <code class="o">=</code> <code class="n">DiffusionDataset</code><code class="p">(</code><code class="n">train_data</code><code class="p">)</code>  <code class="c1"># wrap Fashion MNIST</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>

<p>You may be wondering why not predict the original image directly, rather than the unscaled noise? One reason is empirical: the authors tried both approaches, and they observed that predicting the noise rather than the image led to more stable training and better results. The other reason is that the noise is Gaussian, which allows for some mathematical simplifications: in particular, the KL divergence between two Gaussian distributions is proportional to the squared distance between their means, so we can use the MSE loss, which is simple, fast, and quite stable.</p>

<p>Now we’re ready to build the actual diffusion model itself. It can be any model you want, as long as it takes a <code translate="no">DiffusionSample</code> as input and outputs images of the same shape as the input images. The DDPM authors used a modified <a href="https://homl.info/unet">U-Net architecture</a>,⁠<sup><a data-type="noteref" id="id4152-marker" href="ch18.html#id4152">18</a></sup> which has many similarities with the FCN architecture we discussed in <a data-type="xref" href="ch12.html#cnn_chapter">Chapter 12</a> for semantic segmentation. U-Net<a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="U-Net" id="id4153"/><a data-type="indexterm" data-primary="U-Net" id="id4154"/> is a convolutional neural network that gradually downsamples the input images, then gradually upsamples them again, with skip connections crossing over from each level of the downsampling part to the corresponding level in the upsampling part. To take into account the time steps, they were encoded using a fixed sinusoidal encoding (i.e., the same technique as the positional encodings in the original Transformer architecture). At every level in the U-Net architecture, they passed these time encodings through <code translate="no">Linear</code> layers and fed them to the U-Net. Lastly, they also used multi-head attention layers at various levels. See this chapter’s notebook for a basic implementation (it’s too long to copy here, and the details don’t matter: many other model architectures would work just fine).</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">DiffusionModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>  <code class="c1"># see the notebook for full details</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">T</code><code class="o">=</code><code class="n">T</code><code class="p">,</code> <code class="n">embed_dim</code><code class="o">=</code><code class="mi">64</code><code class="p">):</code>
        <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># create all the required modules to build the U-Net</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">sample</code><code class="p">):</code>
        <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># process the sample and predict the noise for each image</code></pre>

<p>For training, the authors noted that using the MAE loss worked better than the MSE. You can also use the Huber loss:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">diffusion_model</code> <code class="o">=</code> <code class="n">DiffusionModel</code><code class="p">()</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
<code class="n">huber</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">HuberLoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">NAdam</code><code class="p">(</code><code class="n">diffusion_model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">3e-3</code><code class="p">)</code>
<code class="n">train</code><code class="p">(</code><code class="n">diffusion_model</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">huber</code><code class="p">,</code> <code class="n">train_loader</code><code class="p">,</code> <code class="n">n_epochs</code><code class="o">=</code><code class="mi">20</code><code class="p">)</code></pre>

<p>Once the model is trained, you can use it to generate new images by sampling <strong>x</strong><sub><em>T</em></sub> randomly from a Gaussian distribution with mean 0 and variance 1, then using <a data-type="xref" href="#reverse_diffusion_equation">Equation 18-8</a> to get <strong>x</strong><sub><em>T</em>–1</sub>. Then use this equation 3,999 more times until you get <strong>x</strong><sub>0</sub>. If all went well, <strong>x</strong><sub>0</sub> should look like a regular Fashion MNIST image!</p>
<div data-type="equation" id="reverse_diffusion_equation">
<h5><span class="label">Equation 18-8. </span>Going one step in reverse in the DDPM diffusion process</h5>
<math>
  <mrow>
    <msub>
      <mi>𝐱</mi>
      <mrow>
        <mi>t</mi>
        <mo>−</mo>
        <mn>1</mn>
      </mrow>
    </msub>
    <mo>=</mo>
    <mfrac>
      <mn>1</mn>
      <msqrt>
        <msub>
          <mi>α</mi>
          <mi>t</mi>
        </msub>
      </msqrt>
    </mfrac>
    <mrow>
      <mo fence="true" form="prefix">(</mo>
      <msub>
        <mi>𝐱</mi>
        <mi>t</mi>
      </msub>
      <mo>−</mo>
      <mfrac>
        <msub>
          <mi>β</mi>
          <mi>t</mi>
        </msub>
        <msqrt>
          <mrow>
            <mn>1</mn>
            <mo>−</mo>
            <msub>
              <menclose notation="top">
                <mi>α</mi>
              </menclose>
              <mi>t</mi>
            </msub>
          </mrow>
        </msqrt>
      </mfrac>
      <msub>
        <mi mathvariant="bold">ε</mi>
        <mi mathvariant="bold">θ</mi>
      </msub>
      <mo form="prefix" stretchy="false">(</mo>
      <msub>
        <mi>𝐱</mi>
        <mi>t</mi>
      </msub>
      <mo lspace="0%" rspace="0%" separator="true">,</mo>
      <mi>t</mi>
      <mo form="postfix" stretchy="false">)</mo>
      <mo fence="true" form="postfix">)</mo>
    </mrow>
    <mo>+</mo>
    <msqrt>
      <msub>
        <mi>β</mi>
        <mi>t</mi>
      </msub>
    </msqrt>
    <mi>𝐳</mi>
  </mrow>
</math>
</div>

<p class="pagebreak-before">In this equation, <strong>ε<sub>θ</sub></strong>(<strong>x</strong><sub><em>t</em></sub>, <em>t</em>) represents the noise predicted by the model given the input image <strong>x</strong><sub><em>t</em></sub> and the time step <em>t</em>. The <strong>θ</strong> represents the model parameters. Moreover, <strong>z</strong> is Gaussian noise with mean 0 and variance 1. This makes the reverse process stochastic: if you run it multiple times, you will get different images<a data-type="indexterm" data-startref="xi_DDPMDenoisingDiffusionProbabilisticModel18803626_1" id="id4155"/><a data-type="indexterm" data-startref="xi_denoisingdiffusionprobabilisticmodelDDPM18803626_1" id="id4156"/>.</p>

<p>This works well, but it requires 4,000 iterations to generate an image! That’s too slow. Luckily, just a few months after the DDPM<a data-type="indexterm" data-primary="denoising diffusion implicit models (DDIM)" id="xi_denoisingdiffusionimplicitmodelsDDIM181032131_1"/><a data-type="indexterm" data-primary="DDIM (denoising diffusion implicit models)" id="xi_DDIMdenoisingdiffusionimplicitmodels181032131_1"/> paper, researchers from Stanford University proposed a technique named the <a href="https://homl.info/ddim">denoising diffusion implicit model (DDIM)</a>⁠<sup><a data-type="noteref" id="id4157-marker" href="ch18.html#id4157">19</a></sup> to generate images in much fewer steps: instead of going from <em>t</em> = 4,000 down to 0 one step at a time, DDIM can go down any number of time steps at a time, using <a data-type="xref" href="#ddim_equation">Equation 18-9</a>. Moreover, the training process is exactly the same as for DDPM, so we can simply reuse our trained DDPM model.</p>
<div data-type="equation" id="ddim_equation">
<h5><span class="label">Equation 18-9. </span>Going multiple steps in reverse with DDIM</h5>
<math>
  <mtable displaystyle="true" style="width:100%;">
    <mtr>
      <mtd style="padding:0;width:50%;padding-left:0em;padding-right:0em;"/>
      <mtd style="padding-left:1em;padding-right:0em;">
        <mtable displaystyle="true" class="tml-jot">
          <mtr>
            <mtd class="tml-right" style="padding-left:0em;padding-right:0em;">
              <mrow>
                <msub>
                  <mi>𝐱</mi>
                  <mi>p</mi>
                </msub>
                <mo>=</mo>
                <msqrt>
                  <msub>
                    <menclose notation="top">
                      <mi>α</mi>
                    </menclose>
                    <mi>p</mi>
                  </msub>
                </msqrt>
                <msub>
                  <mover>
                    <mi>𝐱</mi>
                    <mo stretchy="false" class="wbk-acc" style="math-depth:0;">^</mo>
                  </mover>
                  <mn>0</mn>
                </msub>
                <mo>+</mo>
                <msqrt>
                  <mrow>
                    <mn>1</mn>
                    <mo>−</mo>
                    <msub>
                      <menclose notation="top">
                        <mi>α</mi>
                      </menclose>
                      <mi>p</mi>
                    </msub>
                    <mo>−</mo>
                    <msup>
                      <mi>σ</mi>
                      <mn>2</mn>
                    </msup>
                  </mrow>
                </msqrt>
                <mo>⋅</mo>
                <msub>
                  <mi mathvariant="bold">ε</mi>
                  <mi mathvariant="bold">θ</mi>
                </msub>
                <mo form="prefix" stretchy="false">(</mo>
                <msub>
                  <mi>𝐱</mi>
                  <mi>t</mi>
                </msub>
                <mo lspace="0%" rspace="0%" separator="true">,</mo>
                <mi>t</mi>
                <mo form="postfix" stretchy="false">)</mo>
                <mo>+</mo>
                <mi>σ</mi>
                <mi>𝐳</mi>
              </mrow>
            </mtd>
          </mtr>
          <mtr>
            <mtd style="padding-left:0em;padding-right:0em;">
              <mrow>
                <mtext>where   </mtext>
                <msub>
                  <mover>
                    <mi>𝐱</mi>
                    <mo stretchy="false" class="wbk-acc" style="math-depth:0;">^</mo>
                  </mover>
                  <mn>0</mn>
                </msub>
                <mo>=</mo>
                <mfrac>
                  <mn>1</mn>
                  <msqrt>
                    <msub>
                      <menclose notation="top">
                        <mi>α</mi>
                      </menclose>
                      <mi>t</mi>
                    </msub>
                  </msqrt>
                </mfrac>
                <mrow>
                  <mo fence="true" form="prefix">(</mo>
                  <msub>
                    <mi>𝐱</mi>
                    <mi>t</mi>
                  </msub>
                  <mo>−</mo>
                  <msqrt>
                    <mrow>
                      <mn>1</mn>
                      <mo>−</mo>
                      <msub>
                        <menclose notation="top">
                          <mi>α</mi>
                        </menclose>
                        <mi>t</mi>
                      </msub>
                    </mrow>
                  </msqrt>
                  <msub>
                    <mi mathvariant="bold">ε</mi>
                    <mi mathvariant="bold">θ</mi>
                  </msub>
                  <mo form="prefix" stretchy="false">(</mo>
                  <msub>
                    <mi>𝐱</mi>
                    <mi>t</mi>
                  </msub>
                  <mo lspace="0%" rspace="0%" separator="true">,</mo>
                  <mi>t</mi>
                  <mo form="postfix" stretchy="false">)</mo>
                  <mo fence="true" form="postfix">)</mo>
                </mrow>
              </mrow>
            </mtd>
          </mtr>
          <mtr>
            <mtd style="padding-left:0em;padding-right:0em;">
              <mrow>
                <mtext>and   </mtext>
                <msup>
                  <mi>σ</mi>
                  <mn>2</mn>
                </msup>
                <mo>=</mo>
                <mi>η</mi>
                <mrow>
                  <mo fence="true" form="prefix">(</mo>
                  <mfrac>
                    <mrow>
                      <mn>1</mn>
                      <mo>−</mo>
                      <msub>
                        <menclose notation="top">
                          <mi>α</mi>
                        </menclose>
                        <mi>p</mi>
                      </msub>
                    </mrow>
                    <mrow>
                      <mn>1</mn>
                      <mo>−</mo>
                      <msub>
                        <menclose notation="top">
                          <mi>α</mi>
                        </menclose>
                        <mi>t</mi>
                      </msub>
                    </mrow>
                  </mfrac>
                  <mo fence="true" form="postfix">)</mo>
                </mrow>
                <msub>
                  <mi>β</mi>
                  <mi>t</mi>
                </msub>
              </mrow>
            </mtd>
          </mtr>
        </mtable>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><strong>ε<sub>θ</sub></strong>(<strong>x</strong><sub><em>t</em></sub>, <em>t</em>), <strong>θ</strong>, and <strong>z</strong> have the same meanings as in <a data-type="xref" href="#reverse_diffusion_equation">Equation 18-8</a>.</p>
</li>
<li>
<p><em>p</em> represents any time step before <em>t</em>. For example, it could be <em>p</em> = <em>t</em> – 50.</p>
</li>
<li>
<p><em>η</em> is a hyperparameter that controls how much randomness should be used during generation, from 0 (no randomness, fully deterministic) to 1 (just like DDPM).</p>
</li>
</ul>

<p>Let’s write a function that implements this reverse process, and call it to generate a few images:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">generate_ddim</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">num_steps</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">eta</code><code class="o">=</code><code class="mf">0.85</code><code class="p">):</code>
    <code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
        <code class="n">xt</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randn</code><code class="p">([</code><code class="n">batch_size</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">],</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">)</code>
        <code class="n">times</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="n">T</code> <code class="o">-</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="n">steps</code><code class="o">=</code><code class="n">num_steps</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">long</code><code class="p">()</code><code class="o">.</code><code class="n">tolist</code><code class="p">()</code>
        <code class="k">for</code> <code class="n">t</code><code class="p">,</code> <code class="n">t_prev</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">times</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">],</code> <code class="n">times</code><code class="p">[</code><code class="mi">1</code><code class="p">:]):</code>
            <code class="n">t_batch</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">full</code><code class="p">((</code><code class="n">batch_size</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code> <code class="n">t</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">)</code>
            <code class="n">sample</code> <code class="o">=</code> <code class="n">DiffusionSample</code><code class="p">(</code><code class="n">xt</code><code class="p">,</code> <code class="n">t_batch</code><code class="p">)</code>
            <code class="n">eps_pred</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">sample</code><code class="p">)</code>
            <code class="n">x0</code> <code class="o">=</code> <code class="p">((</code><code class="n">xt</code> <code class="o">-</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">alpha_bars</code><code class="p">[</code><code class="n">t</code><code class="p">])</code><code class="o">.</code><code class="n">sqrt</code><code class="p">()</code> <code class="o">*</code> <code class="n">eps_pred</code><code class="p">)</code>
                  <code class="o">/</code> <code class="p">(</code><code class="n">alpha_bars</code><code class="p">[</code><code class="n">t</code><code class="p">]</code><code class="o">.</code><code class="n">sqrt</code><code class="p">()))</code>
            <code class="n">abar_t_prev</code> <code class="o">=</code> <code class="n">alpha_bars</code><code class="p">[</code><code class="n">t_prev</code><code class="p">]</code>
            <code class="n">variance</code> <code class="o">=</code> <code class="n">eta</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">abar_t_prev</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">alpha_bars</code><code class="p">[</code><code class="n">t</code><code class="p">])</code> <code class="o">*</code> <code class="n">betas</code><code class="p">[</code><code class="n">t</code><code class="p">]</code>
            <code class="n">sigma_t</code> <code class="o">=</code> <code class="n">variance</code><code class="o">.</code><code class="n">sqrt</code><code class="p">()</code>
            <code class="n">pred_dir</code> <code class="o">=</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">abar_t_prev</code> <code class="o">-</code> <code class="n">sigma_t</code><code class="o">**</code><code class="mi">2</code><code class="p">)</code><code class="o">.</code><code class="n">sqrt</code><code class="p">()</code> <code class="o">*</code> <code class="n">eps_pred</code>
            <code class="n">noise</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randn_like</code><code class="p">(</code><code class="n">xt</code><code class="p">)</code>
            <code class="n">xt</code> <code class="o">=</code> <code class="n">abar_t_prev</code><code class="o">.</code><code class="n">sqrt</code><code class="p">()</code> <code class="o">*</code> <code class="n">x0</code> <code class="o">+</code> <code class="n">pred_dir</code> <code class="o">+</code> <code class="n">sigma_t</code> <code class="o">*</code> <code class="n">noise</code>

        <code class="k">return</code> <code class="n">torch</code><code class="o">.</code><code class="n">clamp</code><code class="p">((</code><code class="n">xt</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code> <code class="o">/</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>  <code class="c1"># from [–1, 1] range to [0, 1]</code>

<code class="n">X_gen_ddim</code> <code class="o">=</code> <code class="n">generate_ddim</code><code class="p">(</code><code class="n">diffusion_model</code><code class="p">,</code> <code class="n">num_steps</code><code class="o">=</code><code class="mi">500</code><code class="p">)</code></pre>

<p>This time the generation will only take a few seconds, and it will produce images such as the ones shown in <a data-type="xref" href="#ddim_generated_images_plot">Figure 18-20</a>. Granted, they’re not very impressive, but we’ve only trained the model for a few minutes on Fashion MNIST<a data-type="indexterm" data-startref="xi_forwardandreverseprocessesdiffusionmodel18801580_1" id="id4158"/><a data-type="indexterm" data-startref="xi_reverseandforwardprocessesdiffusionmodel18801580_1" id="id4159"/>. Give it a try on a larger dataset and train it for a few hours to get more impressive results<a data-type="indexterm" data-startref="xi_unsupervisedlearningdiffusionmodels18803560_1" id="id4160"/><a data-type="indexterm" data-startref="xi_denoisingdiffusionimplicitmodelsDDIM181032131_1" id="id4161"/><a data-type="indexterm" data-startref="xi_DDIMdenoisingdiffusionimplicitmodels181032131_1" id="id4162"/>.</p>

<figure class="width-75"><div id="ddim_generated_images_plot" class="figure">
<img src="assets/hmls_1820.png" alt="A grid of low-resolution, black-and-white images of clothing items, generated using a diffusion model with limited training on the Fashion MNIST dataset." width="1534" height="760"/>
<h6><span class="label">Figure 18-20. </span>Images generated by DDIM accelerated diffusion</h6>
</div></figure>

<p>Diffusion models have made tremendous progress since 2020. In particular, a paper published in December 2021 by <a href="https://homl.info/latentdiff">Robin Rombach, et al.</a>⁠<sup><a data-type="noteref" id="id4163-marker" href="ch18.html#id4163">20</a></sup> introduced <em>latent diffusion models</em>, where<a data-type="indexterm" data-primary="latent diffusion models" id="id4164"/> the diffusion process takes place in latent space<a data-type="indexterm" data-primary="latent space" id="id4165"/>, rather than in pixel space. To achieve this, a powerful autoencoder is used to compress each training image into a much smaller latent space, where the diffusion process takes place, then the autoencoder is used to decompress the final latent representation, generating the output image. This considerably speeds up image generation, and reduces training time and cost dramatically. Importantly, the quality of the generated images is outstanding.</p>

<p>Moreover, the researchers also adapted various conditioning techniques to guide the diffusion process using text prompts, images, or any other inputs. This makes it possible to quickly produce any image you might fancy. You can also condition the image generation process using an input image. This enables many applications, such as outpainting—where an input image is extended beyond its borders—or inpainting—where holes in an image are filled in.</p>

<p>Lastly, a powerful pretrained latent diffusion model named <em>Stable Diffusion</em> (SD)<a data-type="indexterm" data-primary="Stable Diffusion (SD)" id="id4166"/> was open sourced in August 2022 by a collaboration between LMU Munich and a few companies, including StabilityAI, and Runway, with support from EleutherAI and LAION. Now anyone can generate mindblowing images in seconds, for free, even on a regular laptop. For example, you can use the Hugging Face Diffusers library<a data-type="indexterm" data-primary="Diffusers library" id="id4167"/> to load SD (e.g., the turbo variant), create an image generation pipeline for text-to-image, and generate an image of an orangutan reading a book:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">diffusers</code> <code class="kn">import</code> <code class="n">AutoPipelineForText2Image</code>

<code class="n">pipe</code> <code class="o">=</code> <code class="n">AutoPipelineForText2Image</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
    <code class="s2">"stabilityai/sd-turbo"</code><code class="p">,</code> <code class="n">variant</code><code class="o">=</code><code class="s2">"fp16"</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float16</code><code class="p">)</code>
<code class="n">pipe</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
<code class="n">prompt</code> <code class="o">=</code> <code class="s2">"A closeup photo of an orangutan reading a book"</code>
<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">26</code><code class="p">)</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">pipe</code><code class="p">(</code><code class="n">prompt</code><code class="o">=</code><code class="n">prompt</code><code class="p">,</code> <code class="n">num_inference_steps</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">guidance_scale</code><code class="o">=</code><code class="mf">0.0</code><code class="p">)</code><code class="o">.</code><code class="n">images</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code></pre>

<figure class="smallerseventy"><div id="orangutan_image" class="figure">
<img src="assets/hmls_1821.png" alt="A digitally generated image of an orangutan closely reading an open book, illustrating the creative capabilities of the Stable Diffusion model." width="512" height="512"/>
<h6><span class="label">Figure 18-21. </span>A picture generated by Stable Diffusion using the Diffusers library</h6>
</div></figure>

<p class="pagebreak-before">The possibilities are endless!</p>

<p>In the next chapter we will move on to an entirely different branch of deep learning: deep reinforcement learning<a data-type="indexterm" data-startref="xi_diffusionmodels1880134_1" id="id4168"/><a data-type="indexterm" data-startref="xi_imagesclassifyingandgeneratingdiffusionmodels1880134_1" id="id4169"/>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="id740">
<h1>Exercises</h1>
<ol>
<li>
<p>What are the main tasks that autoencoders are used for?</p>
</li>
<li>
<p>Suppose you want to train a classifier, and you have plenty of unlabeled training data but only a few thousand labeled instances. How can autoencoders help? How would you proceed?</p>
</li>
<li>
<p>If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder? How can you evaluate the performance of an autoencoder?</p>
</li>
<li>
<p>What are undercomplete and overcomplete autoencoders? What is the main risk of an excessively undercomplete autoencoder? What about the main risk of an overcomplete autoencoder?</p>
</li>
<li>
<p>How do you tie weights in a stacked autoencoder? What is the point of doing so?</p>
</li>
<li>
<p>What is a generative model? Can you name a type of generative autoencoder?</p>
</li>
<li>
<p>What is a GAN? Can you name a few tasks where GANs can shine?</p>
</li>
<li>
<p>What are the main difficulties when training GANs?</p>
</li>
<li>
<p>What are diffusion models good at? What is their main limitation?</p>
</li>
<li>
<p>Try using a denoising autoencoder to pretrain an image classifier. You can use MNIST (the simplest option), or a more complex image dataset such as <a href="https://homl.info/122">CIFAR10</a> if you want a bigger challenge. Regardless of the dataset you’re using, follow these steps:</p>
<ol>
<li>
<p>Split the dataset into a training set and a test set. Train a deep denoising autoencoder on the full training set.</p>
</li>
<li>
<p>Check that the images are fairly well reconstructed. Visualize the images that most activate each neuron in the coding layer.</p>
</li>
<li>
<p>Build a classification DNN, reusing the lower layers of the autoencoder. Train it using only 500 images from the training set. Does it perform better with or without pretraining?</p>
</li>

</ol>
</li>
<li>
<p>Train a variational autoencoder on the image dataset of your choice, and use it to generate images. Alternatively, you can try to find an unlabeled dataset that you are interested in and see if you can generate new samples.</p>
</li>
<li>
<p>Train a DCGAN to tackle the image dataset of your choice, and use it to generate images. Add experience replay and see if this helps.</p>
</li>
<li>
<p>Train a diffusion model on your preferred image dataset (e.g., <code translate="no">torchvision.datasets.Flowers102</code>), and generate nice images. Next, add the image class as an extra input to the model, and retrain it: you should now be able to control the class of the generated image.</p>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter’s notebook, at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id4015"><sup><a href="ch18.html#id4015-marker">1</a></sup> William G. Chase and Herbert A. Simon, “Perception in Chess”, <em>Cognitive Psychology</em> 4, no. 1 (1973): 55–81.</p><p data-type="footnote" id="id4035"><sup><a href="ch18.html#id4035-marker">2</a></sup> Hint: one approach is to create a custom <code translate="no">AutoencoderDataset</code> class that wraps a given dataset and replaces the targets with the inputs.</p><p data-type="footnote" id="id4062"><sup><a href="ch18.html#id4062-marker">3</a></sup> Yoshua Bengio et al., “Greedy Layer-Wise Training of Deep Networks”, <em>Proceedings of the 19th International Conference on Neural Information Processing Systems</em> (2006): 153–160.</p><p data-type="footnote" id="id4066"><sup><a href="ch18.html#id4066-marker">4</a></sup> Jonathan Masci et al., “Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction”, <em>Proceedings of the 21st International Conference on Artificial Neural Networks</em> 1 (2011): 52–59.</p><p data-type="footnote" id="id4073"><sup><a href="ch18.html#id4073-marker">5</a></sup> Pascal Vincent et al., “Extracting and Composing Robust Features with Denoising Autoencoders”, <em>Proceedings of the 25th International Conference on Machine Learning</em> (2008): 1096–1103.</p><p data-type="footnote" id="id4074"><sup><a href="ch18.html#id4074-marker">6</a></sup> Pascal Vincent et al., “Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion”, <em>Journal of Machine Learning Research</em> 11 (2010): 3371–3408.</p><p data-type="footnote" id="id4088"><sup><a href="ch18.html#id4088-marker">7</a></sup> Diederik Kingma and Max Welling, “Auto-Encoding Variational Bayes”, arXiv preprint arXiv:1312.6114 (2013).</p><p data-type="footnote" id="id4091"><sup><a href="ch18.html#id4091-marker">8</a></sup> Both these properties make VAEs rather similar to RBMs, but they are easier to train, and the sampling process is much faster (with RBMs you need to wait for the network to stabilize into a “thermal equilibrium” before you can sample a new instance).</p><p data-type="footnote" id="id4106"><sup><a href="ch18.html#id4106-marker">9</a></sup> Chris J. Maddison et al., “The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables”, arXiv preprint arXiv:1611.00712 (2016).</p><p data-type="footnote" id="id4107"><sup><a href="ch18.html#id4107-marker">10</a></sup> Eric Jang et al., “Categorical Reparameterization with Gumbel-Softmax”, arXiv preprint arXiv:1611.01144 (2016).</p><p data-type="footnote" id="id4111"><sup><a href="ch18.html#id4111-marker">11</a></sup> Aaron van den Oord et al., “Neural Discrete Representation Learning”, arXiv preprint arXiv:1711.00937 (2017).</p><p data-type="footnote" id="id4125"><sup><a href="ch18.html#id4125-marker">12</a></sup> Ian Goodfellow et al., “Generative Adversarial Nets”, <em>Proceedings of the 27th International Conference on Neural Information Processing Systems</em> 2 (2014): 2672–2680.</p><p data-type="footnote" id="id4132"><sup><a href="ch18.html#id4132-marker">13</a></sup> For a nice comparison of the main GAN losses, check out this great <a href="https://homl.info/ganloss">GitHub project by Hwalsuk Lee</a>.</p><p data-type="footnote" id="id4133"><sup><a href="ch18.html#id4133-marker">14</a></sup> Mario Lucic et al., “Are GANs Created Equal? A Large-Scale Study”, <em>Proceedings of the 32nd International Conference on Neural Information Processing Systems</em> (2018): 698–707.</p><p data-type="footnote" id="id4147"><sup><a href="ch18.html#id4147-marker">15</a></sup> Jascha Sohl-Dickstein et al., “Deep Unsupervised Learning using Nonequilibrium Thermodynamics”, arXiv preprint arXiv:1503.03585 (2015).</p><p data-type="footnote" id="id4148"><sup><a href="ch18.html#id4148-marker">16</a></sup> Jonathan Ho et al., “Denoising Diffusion Probabilistic Models” (2020).</p><p data-type="footnote" id="id4149"><sup><a href="ch18.html#id4149-marker">17</a></sup> Alex Nichol and Prafulla Dhariwal, “Improved Denoising Diffusion Probabilistic Models” (2021).</p><p data-type="footnote" id="id4152"><sup><a href="ch18.html#id4152-marker">18</a></sup> Olaf Ronneberger et al., “U-Net: Convolutional Networks for Biomedical Image Segmentation”, arXiv preprint arXiv:1505.04597 (2015).</p><p data-type="footnote" id="id4157"><sup><a href="ch18.html#id4157-marker">19</a></sup> Jiaming Song et al., “Denoising Diffusion Implicit Models”, arXiv preprint arXiv:2010.02502 (2020).</p><p data-type="footnote" id="id4163"><sup><a href="ch18.html#id4163-marker">20</a></sup> Robin Rombach, Andreas Blattmann, et al., “High-Resolution Image Synthesis with Latent Diffusion Models”, arXiv preprint arXiv:2112.10752 (2021).</p></div></div></section></div></div></body></html>