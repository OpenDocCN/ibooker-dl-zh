["```py\nGenerate a list of Disney characters.\n```", "```py\nSure, here is a list of some popular Disney characters:\n\n1\\. Mickey Mouse\n2\\. Minnie Mouse\n...\n30\\. Bagheera (The Jungle Book)\n```", "```py\nGenerate a bullet-point list of 5 male Disney characters.\nOnly include the name of the character for each line.\nNever include the film for each Disney character.\nOnly return the Disney characters, never include any commentary.\n\nBelow is an example list:\n\n* Aladdin\n* Simba\n* Beast\n* Hercules\n* Tarzan\n```", "```py\n* Woody\n* Buzz Lightyear\n* Stitch\n* Jack Sparrow\n* Prince Charming\n```", "```py\nGenerate a hierarchical and incredibly detailed article outline on:\n\nWhat are the benefits of data engineering.\n\nSee an example of the hierarchical structure below:\n\nArticle Title: What are the benefits of digital marketing?\n\n* Introduction\n    a. Explanation of digital marketing\n    b. Importance of digital marketing in today's business world\n* Increased Brand Awareness\n    a. Definition of brand awareness\n    b. How digital marketing helps in increasing brand awareness\n```", "```py\nArticle Title: What are the benefits of data engineering?\n\n* Introduction\n    a. Explanation of data engineering\n    b. Importance of data engineering in today’s data-driven world\n\n...(10 sections later)...\n\n* Conclusion\n    a. Importance of data engineering in the modern business world\n    b. Future of data engineering and its impact on the data ecosystem\n```", "```py\nimport re\n\n# openai_result = generate_article_outline(prompt)\n# Commented out to focus on a fake LLM response, see below:\n\nopenai_result = '''\n* Introduction\n a. Explanation of data engineering\n b. Importance of data engineering in today’s data-driven world\n* Efficient Data Management\n a. Definition of data management\n b. How data engineering helps in efficient data management\n* Conclusion\n a. Importance of data engineering in the modern business world\n b. Future of data engineering and its impact on the data ecosystem\n'''\n\n# Regular expression patterns\nheading_pattern = r'\\* (.+)'\nsubheading_pattern = r'\\s+[a-z]\\. (.+)'\n\n# Extract headings and subheadings\nheadings = re.findall(heading_pattern, openai_result)\nsubheadings = re.findall(subheading_pattern, openai_result)\n\n# Print results\nprint(\"Headings:\\n\")\nfor heading in headings:\n    print(f\"* {heading}\")\n\nprint(\"\\nSubheadings:\\n\")\nfor subheading in subheadings:\n    print(f\"* {subheading}\")\n```", "```py\nHeadings:\n- Introduction\n- Efficient Data Management\n- Conclusion\n\nSubheadings:\n- Explanation of data engineering\n- Importance of data engineering in today’s data-driven world\n- Definition of data management\n- How data engineering helps in efficient data management\n- Importance of data engineering in the modern business world\n- Future of data engineering and its impact on the data ecosystem\n```", "```py\nimport re\n\nopenai_result = \"\"\"\n* Introduction\n a. Explanation of data engineering\n b. Importance of data engineering in today’s data-driven world\n* Efficient Data Management\n a. Definition of data management\n b. How data engineering helps in efficient data management\n c. Why data engineering is important for data management\n* Conclusion\n a. Importance of data engineering in the modern business world\n b. Future of data engineering and its impact on the data ecosystem\n\"\"\"\n\nsection_regex = re.compile(r\"\\* (.+)\")\nsubsection_regex = re.compile(r\"\\s*([a-z]\\..+)\")\n\nresult_dict = {}\ncurrent_section = None\n\nfor line in openai_result.split(\"\\n\"):\n    section_match = section_regex.match(line)\n    subsection_match = subsection_regex.match(line)\n\n    if section_match:\n        current_section = section_match.group(1)\n        result_dict[current_section] = []\n    elif subsection_match and current_section is not None:\n        result_dict[current_section].append(subsection_match.group(1))\n\nprint(result_dict)\n```", "```py\n{\n    \"Introduction\": [\n        \"a. Explanation of data engineering\",\n        \"b. Importance of data engineering in today’s data-driven world\"\n    ],\n    \"Efficient Data Management\": [\n        \"a. Definition of data management\",\n        \"b. How data engineering helps in efficient data management\"\n    ],\n    \"Conclusion\": [\n        \"a. Importance of data engineering in the modern business world\",\n        \"b. Future of data engineering and its impact on the data ecosystem\"\n    ]\n}\n```", "```py\nCompose a very detailed article outline on \"The benefits of learning code\" with a\nJSON payload structure that highlights key points.\n\nOnly return valid JSON.\n\nHere is an example of the JSON structure:\n{\n    \"Introduction\": [\n        \"a. Explanation of data engineering\",\n        \"b. Importance of data engineering in today’s data-driven world\"],\n    ...\n    \"Conclusion\": [\n        \"a. Importance of data engineering in the modern business world\",\n        \"b. Future of data engineering and its impact on the data ecosystem\"]\n}\n```", "```py\n{\n    \"Introduction\": [\n        \"a. Overview of coding and programming languages\",\n        \"b. Importance of coding in today's technology-driven world\"],\n    ...\n    \"Conclusion\": [\n        \"a. Recap of the benefits of learning code\",\n        \"b. The ongoing importance of coding skills in the modern world\"]\n}\n```", "```py) , such as:\n\nOutput:\n\n```", "```pyjson\n{\"Name\": \"John Smith\"} # valid payload\n{\"Name\": \"John Smith\", \"some_key\":} # invalid payload\n```", "```py\n\nIdeally you would like the model to respond like so:\n\nOutput:\n\n```", "```py\n\nThis is important because with the first output, you’d have to split after `json` and then parse the exact part of the string that contained valid JSON. There are several points that are worth adding to your prompts to improve JSON parsing:\n\n```", "```py\n\nNow let’s examine how you can parse a [JSON output with Python](https://oreil.ly/MoJHn):\n\n```", "```py\n\nWell done, you’ve successfully parsed some JSON.\n\nAs showcased, structuring data from an LLM response is streamlined when requesting the response in valid JSON format. Compared to the previously demonstrated regular expression parsing, this method is less cumbersome and more straightforward.\n\nSo what could go wrong?\n\n*   The language model accidentally adds extra text to the response such as `json output:` and your application logic only handles for valid JSON.\n\n*   The JSON produced isn’t valid and fails upon parsing (either due to the size or simply for not escaping certain characters).\n\nLater on you will examine strategies to gracefully handle for such edge cases.\n\n## YAML\n\n*.yml* files are a structured data format that offer different benefits over *.json*:\n\nNo need to escape characters\n\nYAML’s indentation pattern eliminates the need for braces, brackets, and commas to denote structure. This can lead to cleaner and less error-prone files, as there’s less risk of mismatched or misplaced punctuation.\n\nReadability\n\nYAML is designed to be human-readable, with a simpler syntax and structure compared to JSON. This makes it easier for you to create, read, and edit prompts, especially when dealing with complex or nested structures.\n\nComments\n\nUnlike JSON, YAML supports comments, allowing you to add annotations or explanations to the prompts directly in the file. This can be extremely helpful when working in a team or when revisiting the prompts after some time, as it allows for better understanding and collaboration.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nNotice with the preceding example how an LLM is able to infer the correct *.yml* format from the `User Query` string.\n\nAdditionally, you’ve given the LLM an opportunity to either:\n\n*   Return a valid *.yml* response\n\n*   Return a filtered *.yml* response\n\nIf after filtering, there are no *.yml* items left, then return *No Items*.\n\n# Filtering YAML Payloads\n\nYou might decide to use this same prompt for cleaning/filtering a *.yml* payload.\n\nFirst, let’s focus on a payload that contains both valid and invalid `schema` in reference to our desired `schema`. `Apple slices` fit the criteria; however, `Bananas` doesn’t exist, and you should expect for the `User Query` to be appropriately filtered.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nIn the preceding example, you’ve successfully filtered the user’s payload against a set criteria and have used the language model as a *reasoning engine*.\n\nBy providing the LLM with a set of instructions within the prompt, the response is closely related to what a human might do if they were manually cleaning the data.\n\nThe input prompt facilitates the delegation of more control flow tasks to a language learning model (LLM), tasks that would typically require coding in a programming language like Python or JavaScript.\n\n[Figure 3-1](#figure-3-1) provides a detailed overview of the logic applied when processing user queries by an LLM.\n\n![Using an LLM to determine the control flow of an application instead of directly using code.](assets/pega_0301.png)\n\n###### Figure 3-1\\. Using an LLM to determine the control flow of an application instead of code\n\n# Handling Invalid Payloads in YAML\n\nA completely invalid payload might look like this:\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nAs expected, the LLM returned `No Items` as none of the `User Query` items matched against the previously defined `schema`.\n\nLet’s create a Python script that gracefully accommodates for the various types of LLM results returned. The core parts of the script will focus on:\n\n*   Creating custom exceptions for each type of error that might occur due to the three LLM response scenarios\n\n*   Parsing the proposed schema\n\n*   Running a serious of custom checks against the response so you can be sure that the YML response can be safely passed to downstream software applications/microservices\n\nYou could define six specific errors that would handle for all of the edge cases:\n\n```", "```py\n\nThen provide the previously proposed `YML schema` as a string:\n\n```", "```py\n\nImport the `yaml` module and create a custom parser function called `validate_``response` that allows you to easily determine whether an LLM output is valid:\n\n```", "```py\n\nTo test these edge cases, following you’ll find several mocked LLM responses:\n\n```", "```py\n\nFinally, now you can:\n\n*   Use `yaml.safe_load(response)` to safely parse the *.yml* schema\n\n*   Call the `validate_response` function for each LLM response to test it against custom *.yml* validation logic\n\n```", "```py\n\n# Diverse Format Generation with ChatGPT\n\nChatGPT is not only capable of generating *.json* or *.yml* files but can also support many other data formats. For example, it can aid you in creating a mermaid diagram or generating a mock *.csv* document. Its versatility stretches to even generating code, conversations, and scripts.\n\nLet’s explore in detail how ChatGPT can produce diverse formatted content. Mermaid is a markdown-like scripting language for generating charts, diagrams, and other visualizations.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nThis output, when input to a mermaid diagram renderer, will create a precise flowchart reflecting the food ordering process ([Figure 3-2](#figure-3-2)).\n\n![A simple mermaid diagram output from an LLM.](assets/pega_0302.png)\n\n###### Figure 3-2\\. A streamlined flow diagram created using mermaid syntax via an LLM\n\n## Mock CSV Data\n\nChatGPT can also generate mock CSV data that can be utilized for testing purposes or other uses.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nThe example generated is a well-formatted CSV file, which can be loaded directly into many tools that read CSV data.\n\nRemember, GPT models rely heavily on the format you specify. For instance, specifying CSV implies the AI should use commas as separators. Be clear about the desired format for optimal results.\n\nYou’ve explored different ways of extracting structured data from language models including regular expressions, JSON, YML, and other formats. You can also now use LLMs to direct parts of your application’s control flow.\n\n# Explain It like I’m Five\n\nThe *Explain It like I’m Five* prompt focuses on explaining a given section of text *as if you were talking to a five-year-old child.*\n\nThis style of prompting is very effective and is a great way to convert technical documents into simple summaries that anyone can understand.\n\nLet’s apply this prompt to an abstract of [“Tumor Microenvironment: Recent Advances in Various Cancer Treatments”](https://oreil.ly/MDOsz).\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nThe ChatGPT output from the preceding example is much more digestible than the abstract.\n\n# Universal Translation Through LLMs\n\nAdditionally, language models can potentially act as *universal translators* due to their proficiency in understanding and generating human-like text across multiple languages.\n\nLet’s investigate this with a simple example:\n\n1.  Generate some simple text.\n\n2.  Change the simple text and make it difficult to read.\n\n3.  Then transform the same text back into an easy to read format in Spanish (instead of English).\n\n4.  Revert the simplistic text back into English.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nAs you can see, part of the meaning is lost while doing the translation, and this is to be expected. However, large parts of the original text’s intent remains, demonstrating that LLMs can act as *highly effective translators* for languages that have adequate resources on the internet. For languages that do not have a good amount of resources, it will yield bad results.\n\nThe same logic applies to coding languages. LLMs are very good at generating code for established programming languages such as Python and JavaScript but perform worse for newer coding languages and packages.\n\nThe boundaries between different forms of information are becoming *increasingly fluid*. The essence of information itself is evolving, allowing for effortless transformations of summaries into stories, poems, or other creative expressions, ultimately enriching our understanding and engagement with the content.\n\n*Diffusion models* are a unique class of generative models utilized in machine learning, specifically designed to produce new images that mimic those found in the training set.\n\nMoreover, when you combine language models with diffusion models, it enables seamless transitions between text, video, and other modalities. This makes it even simpler for you to convey complex ideas across various formats, facilitating a more accessible and comprehensive experience.\n\n# Ask for Context\n\nLLMs are not only capable of generating text but can also act as simple agents with a limited amount of *reasoning capability.* This allows you to write a prompt asking the language model to either:\n\n*   Return a valid result to a question or statement\n\n*   Ask for more context to appropriately answer the question\n\nIn this section, you’ll learn about the importance of *asking for context* when working with LLMs such as GPT-4\\. We will start with an example of a prompt that doesn’t provide enough context, resulting in a less useful response.\n\nThen, we will provide a better prompt that encourages the model to ask for additional context if needed. Finally, we will use the additional context provided to generate a more informed response.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nIn the preceding prompt, the model does not have enough context to make a meaningful recommendation. Instead, you can ask ChatGPT for a list of recommended points that would help it to make an effective decision.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nAfter prompting ChatGPT about how to make an informed decision, now you’re aware of what to include within your prompt to help the language model with deciding.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nIn this final example, the model uses the additional context provided to give a well-informed recommendation for using PostgreSQL. By asking for context when necessary, LLMs like ChatGPT and GPT-4 can deliver more valuable and accurate responses.\n\n[Figure 3-3](#figure-3-3) demonstrates how *asking for context* changes the decision-making process of LLMs. Upon receiving user input, the model first assesses whether the context given is sufficient. If not, it prompts the user to provide more detailed information, emphasizing the model’s reliance on context-rich inputs. Once adequate context is acquired, the LLM then generates an informed and relevant response.\n\n![The decision process of an LLM with asking for context.](assets/pega_0303.png)\n\n###### Figure 3-3\\. The decision process of an LLM while asking for context\n\n# Allow the LLM to Ask for More Context by Default\n\nYou can allow the LLM to ask for more context as a default by including this key phrase: *If you need more context, please specify what would help you to make a better decision.*\n\nIn this section, you’ve seen how LLMs can act as agents that use environmental context to make decisions. By iteratively refining the prompt based on the model’s recommendations, we eventually reach a point where the model has *enough context to make a well-informed decision.*\n\nThis process highlights the importance of providing sufficient context in your prompts and being prepared to ask for more information when necessary. By doing so, you can leverage the power of LLMs like GPT-4 to make more accurate and valuable recommendations.\n\nIn agent-based systems like GPT-4, the ability to ask for more context and provide a finalized answer is crucial for making well-informed decisions. [AutoGPT](https://oreil.ly/l3Ihy), a multiagent system, has a self-evaluation step that automatically checks whether the task can be completed given the current context within the prompt. This technique uses an actor–critic relationship, where the existing prompt context is being analyzed to see whether it could be further refined before being executed.\n\n# Text Style Unbundling\n\n*Text style unbundling* is a powerful technique in prompt engineering that allows you to extract and isolate specific textual features from a given document, such as tone, length, vocabulary, and structure.\n\nThis allows you to create new content that shares similar characteristics with the original document, ensuring consistency in style and tone across various forms of communication.\n\nThis consistency can be crucial for businesses and organizations that need to communicate with a unified voice across different channels and platforms. The benefits of this technique include:\n\nImproved brand consistency\n\nBy ensuring that all content follows a similar style, organizations can strengthen their brand identity and maintain a cohesive image.\n\nStreamlined content creation\n\nBy providing a clear set of guidelines, writers and content creators can more easily produce materials that align with a desired style.\n\nAdaptability\n\nText style unbundling allows for the easy adaptation of existing content to new formats or styles while preserving the core message and tone.\n\nThe process of text style unbundling involves *identifying the desired textual features* or creating a meta prompt (a prompt to create prompts) to extract these features and then using the extracted features to guide the generation of new content.\n\n# Identifying the Desired Textual Features\n\nTo successfully unbundle a text style, you must first identify the specific features you want to extract from the input document. Common textual features to consider include:\n\nTone of voice\n\nThe overall mood or attitude conveyed by the text, such as formal, casual, humorous, or authoritative\n\nLength\n\nThe desired word count or general length of the content\n\nVocabulary and phrasing\n\nThe choice of words and expressions, including industry-specific jargon, colloquialisms, and complexity\n\nAdditionally, you can even create a meta prompt that will dynamically show you these.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\n# Generating New Content with the Extracted Features\n\nAfter you’ve extracted the desired features from the input document, you can use this information to guide future content generation. Remember to ensure that the AI model incorporates the extracted features into its output by specifying the desired style in your prompt. For example:\n\n*   `Write a new blog post on [topic] using the same tone of voice, length, vocabulary, and structure as the previously analyzed text.`\n\nBy combining this technique with *reference text* (documents that act as grounding truth), you can produce credible, branded content that requires minimal revisions.\n\n# Extracting Specific Textual Features with LLMs\n\nYou can easily tailor a prompt to guide an LLM in extracting particular textual features from a document. This can be applied beyond just analyzing text for copywriting purposes. For instance, recognizing entities or discerning sentiment from the text can be achieved by crafting a precise instruction for the LLM.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\n# Summarization\n\nIn the era of information overload, the ability to condense large amounts of text into concise, digestible summaries has become an essential skill. As the volume of data we produce and consume continues to grow, the demand for effective summarization techniques has increased significantly.\n\nAI has made significant advancements in this domain, providing powerful tools for generating summaries that retain the most critical information while discarding less relevant details.\n\nLet’s explore how to summarize using a language model.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nSummarization is an invaluable application of AI, enabling users to quickly extract key insights from lengthy articles, reports, or research papers. This process can help individuals make informed decisions, save time, and prioritize their reading. AI-generated summaries can also facilitate information sharing among teams, allowing for more efficient collaboration and communication.\n\n# Summarizing Given Context Window Limitations\n\nFor documents larger than an LLM can handle in a single API request, a common approach is to chunk the document, summarize each chunk, and then combine these summaries into a final summary, as shown in [Figure 3-4](#figure-3-4).\n\n![.A summarization pipeline that uses text splitting and multiple summarization steps.](assets/pega_0304.png)\n\n###### Figure 3-4\\. A summarization pipeline that uses text splitting and multiple summarization steps\n\nAdditionally, people may require different types of summaries for various reasons, and this is where AI summarization comes in handy. As illustrated in the preceding diagram, a large PDF document could easily be processed using AI summarization to generate distinct summaries tailored to individual needs:\n\nSummary A\n\nProvides key insights, which is perfect for users seeking a quick understanding of the document’s content, enabling them to focus on the most crucial points\n\nSummary B\n\nOn the other hand, offers decision-making information, allowing users to make informed decisions based on the content’s implications and recommendations\n\nSummary C\n\nCaters to collaboration and communication, ensuring that users can efficiently share the document’s information and work together seamlessly\n\nBy customizing the summaries for different users, AI summarization contributes to increased information retrieval for all users, making the entire process more efficient and targeted.\n\nLet’s assume you’re only interested in finding and summarizing information about the advantages of digital marketing. Simply change your summarization prompt to `Provide a concise, abstractive summary of the above text. Only summarize the advantages: ...`\n\nAI-powered summarization has emerged as an essential tool for quickly distilling vast amounts of information into concise, digestible summaries that cater to various user needs. By leveraging advanced language models like GPT-4, AI summarization techniques can efficiently extract key insights and decision-making information, and also facilitate collaboration and communication.\n\nAs the volume of data continues to grow, the demand for effective and targeted summarization will only increase, making AI a crucial asset for individuals and organizations alike in navigating the Information Age.\n\n# Chunking Text\n\nLLMs continue to develop and play an increasingly crucial role in various applications, as the ability to process and manage large volumes of text becomes ever more important. An essential technique for handling large-scale text is known as *chunking.*\n\n*Chunking* refers to the process of breaking down large pieces of text into smaller, more manageable units or chunks. These chunks can be based on various criteria, such as sentence, paragraph, topic, complexity, or length. By dividing text into smaller segments, AI models can more efficiently process, analyze, and generate responses.\n\n[Figure 3-5](#figure-3-5) illustrates the process of chunking a large piece of text and subsequently extracting topics from the individual chunks.\n\n![Topic extraction process after chunking text.](assets/pega_0305.png)\n\n###### Figure 3-5\\. Topic extraction with an LLM after chunking text\n\n## Benefits of Chunking Text\n\nThere are several advantages to chunking text, which include:\n\nFitting within a given context length\n\nLLMs only have a certain amount of input and output tokens, which is called a *context length*. By reducing the input tokens you can make sure the output won’t be cut off and the initial request won’t be rejected.\n\nReducing cost\n\nChunking helps you to only retrieve the most important points from documents, which reduces your token usage and API costs.\n\nImproved performance\n\nChunking reduces the processing load on LLMs, allowing for faster response times and more efficient resource utilization.\n\nIncreased flexibility\n\nChunking allows developers to tailor AI responses based on the specific needs of a given task or application.\n\n## Scenarios for Chunking Text\n\nChunking text can be particularly beneficial in certain scenarios, while in others it may not be required. Understanding when to apply this technique can help in optimizing the performance and cost efficiency of LLMs.\n\n### When to chunk\n\nLarge documents\n\nWhen dealing with extensive documents that exceed the maximum token limit of the LLM\n\nComplex analysis\n\nIn scenarios where a detailed analysis is required and the document needs to be broken down for better comprehension and processing\n\nMultitopic documents\n\nWhen a document covers multiple topics and it’s beneficial to handle them individually\n\n### When not to chunk\n\nShort documents\n\nWhen the document is short and well within the token limits of the LLM\n\nSimple analysis\n\nIn cases where the analysis or processing required is straightforward and doesn’t benefit from chunking\n\nSingle-topic documents\n\nWhen a document is focused on a single topic and chunking doesn’t add value to the processing\n\n## Poor Chunking Example\n\nWhen text is not chunked correctly, it can lead to reduced LLM performance. Consider the following paragraph from a news article:\n\n```", "```py\n\nWhen the text is fragmented into isolated words, the resulting list lacks the original context:\n\n```", "```py\n\nThe main issues with this poor chunking example include:\n\nLoss of context\n\nBy splitting the text into individual words, the original meaning and relationships between the words are lost. This makes it difficult for AI models to understand and respond effectively.\n\nIncreased processing load\n\nProcessing individual words requires more computational resources, making it less efficient than processing larger chunks of text.\n\nAs a result of the poor chunking in this example, an LLM may face several challenges:\n\n*   Difficulty understanding the main ideas or themes of the text\n\n*   Struggling to generate accurate summaries or translations\n\n*   Inability to effectively perform tasks such as sentiment analysis or text `classification`\n\nBy understanding the pitfalls of poor chunking, you can apply prompt engineering principles to improve the process and achieve better results with AI language models.\n\nLet’s explore an improved chunking example using the same news article paragraph from the previous section; you’ll now chunk the text by sentence:\n\n```", "```py\n\n# Divide Labor and Evaluate Quality\n\nDefine the granularity at which the text should be chunked, such as by sentence, paragraph, or topic. Adjust parameters like the number of tokens or model temperature to optimize the chunking process.\n\nBy chunking the text in this manner, you could insert whole sentences into an LLM prompt with the most relevant sentences.\n\n# Chunking Strategies\n\nThere are many different chunking strategies, including:\n\nSplitting by sentence\n\nPreserves the context and structure of the original content, making it easier for LLMs to understand and process the information. Sentence-based chunking is particularly useful for tasks like summarization, translation, and sentiment analysis.\n\nSplitting by paragraph\n\nThis approach is especially effective when dealing with longer content, as it allows the LLM to focus on one cohesive unit at a time. Paragraph-based chunking is ideal for applications like document analysis, topic modeling, and information extraction.\n\nSplitting by topic or section\n\nThis method can help AI models better identify and understand the main themes and ideas within the content. Topic-based chunking is well suited for tasks like text classification, content recommendations, and clustering.\n\nSplitting by complexity\n\nFor certain applications, it might be helpful to split text based on its complexity, such as the reading level or technicality of the content. By grouping similar complexity levels together, LLMs can more effectively process and analyze the text. This approach is useful for tasks like readability analysis, content adaptation, and personalized learning.\n\nSplitting by length\n\nThis technique is particularly helpful when working with very long or complex documents, as it allows LLMs to process the content more efficiently. Length-based chunking is suitable for applications like large-scale text analysis, search engine indexing, and text preprocessing.\n\nSplitting by tokens using a tokenizer\n\nUtilizing a tokenizer is a crucial step in many natural language processing tasks, as it enables the process of splitting text into individual tokens. Tokenizers divide text into smaller units, such as words, phrases, or symbols, which can then be analyzed and processed by AI models more effectively. You’ll shortly be using a package called `tiktoken`, which is a bytes-pair encoding tokenizer (BPE) for chunking.\n\n[Table 3-1](#table-3-1) provides a high-level overview of the different chunking strategies; it’s worth considering what matters to you most when performing chunking.\n\nAre you more interested in preserving semantic context, or would naively splitting by length suffice?\n\nTable 3-1\\. Six chunking strategies highlighting their advantages and disadvantages\n\n| Splitting strategy | Advantages | Disadvantages |\n| --- | --- | --- |\n| Splitting by sentence | Preserves context, suitable for various tasks | May not be efficient for very long content |\n| Splitting by paragraph | Handles longer content, focuses on cohesive units | Less granularity, may miss subtle connections |\n| Splitting by topic | Identifies main themes, better for classification | Requires topic identification, may miss fine details |\n| Splitting by complexity | Groups similar complexity levels, adaptive | Requires complexity measurement, not suitable for all tasks |\n| Splitting by length | Manages very long content, efficient processing | Loss of context, may require more preprocessing steps |\n| Using a tokenizer: Splitting by tokens | Accurate token counts, which helps in avoiding LLM prompt token limits | Requires tokenization, may increase computational complexity |\n\nBy choosing the appropriate chunking strategy for your specific use case, you can optimize the performance and accuracy of AI language models.\n\n# Sentence Detection Using SpaCy\n\n*Sentence detection*, also known as sentence boundary disambiguation, is the process used in NLP that involves identifying the start and end of sentences within a given text. It can be particularly useful for tasks that require preserving the context and structure of the original content. By splitting the text into sentences, LLMs can better understand and process the information for tasks such as summarization, translation, and sentiment analysis.\n\nSplitting by sentence is possible using NLP libraries such as [spaCy](https://spacy.io). Ensure that you have spaCy installed in your Python environment. You can install it with `pip install spacy`. Download the `en_core_web_sm` model using the command `python -m spacy download en_core_web_sm`.\n\nIn [Example 3-3](#ex-3-3), the code demonstrates sentence detection using the spaCy library in Python.\n\n##### Example 3-3\\. [Sentence detection with spaCy](https://oreil.ly/GKDnc)\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nFirst, you’ll import the spaCy library and load the English model `(en_core_web_sm)` to initialize an `nlp` object. Define an input text with two sentences; the text is then processed with `doc = nlp(text)`, creating a `doc` object as a result. Finally, the code iterates through the detected sentences using the `doc.sents` attribute and prints each sentence.\n\n# Building a Simple Chunking Algorithm in Python\n\nAfter exploring many chunking strategies, it’s important to build your intuition by writing a simple chunking algorithm from scatch.\n\n[Example 3-4](#ex-3-4) shows how to chunk text based on the length of characters from the blog post “Hubspot - What Is Digital Marketing?” This file can be found in the Github repository at *[content/chapter_3/hubspot_blog_post.txt](https://oreil.ly/30rlQ)*.\n\nTo correctly read the *hubspot_blog_post.txt* file, make sure your current working directory is set to the [*content/chapter_3*](https://oreil.ly/OHurh) GitHub directory. This applies for both running the Python code or launching the Jupyter Notebook server.\n\n##### Example 3-4\\. [Character chunking](https://oreil.ly/n3sNy)\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nFirst, you open the text file *hubspot_blog_post.txt* with the `open` function and read its contents into the variable text. Then using a list comprehension you create a list of chunks, where each `chunk` is a 200 character substring of text.\n\nThen you use the `range` function to generate indices for each 200 character substring, and the `i:i+200` slice notation to extract the substring from text.\n\nFinally, you loop through each chunk in the `chunks` list and `print` it to the console.\n\nAs you can see, because the chunking implementation is relatively simple and only based on length, there are gaps within the sentences and even words.\n\nFor these reasons we believe that good NLP chunking has the following properties:\n\n*   Preserves entire words, ideally sentences and contextual points made by speakers\n\n*   Handles for when sentences span across several pages, for example, page 1 into page 2\n\n*   Provides an adequate token count for each `chunk` so that the total number of input tokens will appropriately fit into a given token context window for any LLM\n\n# Sliding Window Chunking\n\n*Sliding window chunking* is a technique used for dividing text data into overlapping chunks, or *windows*, based on a specified number of characters, tokens, or words.\n\nBut what exactly is a sliding window?\n\nImagine viewing a long piece of text through a small window. This window is only capable of displaying a fixed number of items at a time. As you slide this window from the beginning to the end of the text, you see *overlapping chunks of text*. This mechanism forms the essence of the sliding window approach.\n\nEach window size is defined by a *fixed number of characters, tokens, or words*, and the *step size* determines how far the window moves with each slide.\n\nIn [Figure 3-6](#figure-3-6), with a window size of 4 words and a step size of 1, the first chunk would contain the first 4 words of the text. The window then slides 1 word to the right to create the second chunk, which contains words 2 through 5.\n\nThis process repeats until the end of the text is reached, ensuring each chunk overlaps with the previous and next ones to retain some shared context.\n\n![pega 0306](assets/pega_0306.png)\n\n###### Figure 3-6\\. A sliding window, with a window size of 4 and a step size of 1\n\nDue to the step size being 1, there is a lot of duplicate information between chunks, and at the same time the risk of losing information between chunks is dramatically reduced.\n\nThis is in stark contrast to [Figure 3-7](#figure-3-7), which has a window size of 4 words and a step size of 2\\. You’ll notice that because of the 100% increase in step size, the amount of information shared between the chunks is greatly reduced.\n\n![pega 0307](assets/pega_0307.png)\n\n###### Figure 3-7\\. A sliding window, with a window size of 4 and a step size of 2\n\nYou will likely need a larger overlap if accuracy and preserving semanatic context are more important than minimizing token inputs or the number of requests made to an LLM.\n\n[Example 3-5](#ex-3-5) shows how you can implement a sliding window using Python’s `len()` function. The `len()` function provides us with the total number of characters rather than words in a given text string, which subsequently aids in defining the parameters of our sliding windows.\n\n##### Example 3-5\\. [Sliding window](https://oreil.ly/aCkDo)\n\n```", "```py\n\nThis code outputs:\n\n```", "```py\n\nIn the context of prompt engineering, the sliding window approach offers several benefits over fixed chunking methods. It allows LLMs to retain a higher degree of context, as there is an overlap between the chunks and offers an alternative approach to preserving context compared to sentence detection.\n\n# Text Chunking Packages\n\nWhen working with LLMs such as GPT-4, always remain wary of the maximum context length:\n\n*   `maximum_context_length = input_tokens + output_tokens`\n\nThere are various tokenizers available to break your text down into manageable units, the most popular ones being NLTK, spaCy, and tiktoken.\n\nBoth [NLTK](https://oreil.ly/wTmI7) and [spaCy](https://oreil.ly/c4MvQ) provide comprehensive support for text processing, but you’ll be focusing on tiktoken.\n\n# Text Chunking with Tiktoken\n\n[Tiktoken](https://oreil.ly/oSpVe) is a fast *byte pair encoding (BPE)* tokenizer that breaks down text into subword units and is designed for use with OpenAI’s models. Tiktoken offers faster performance than comparable open source tokenizers.\n\nAs a developer working with GPT-4 applications, using tiktoken offers you several key advantages:\n\nAccurate token breakdown\n\nIt’s crucial to divide text into tokens because GPT models interpret text as individual tokens. Identifying the number of tokens in your text helps you figure out whether the text is too lengthy for a model to process.\n\nEffective resource utilization\n\nHaving the correct token count enables you to manage resources efficiently, particularly when using the OpenAI API. Being aware of the exact number of tokens helps you regulate and optimize API usage, maintaining a balance between costs and resource usage.\n\n# Encodings\n\nEncodings define the method of converting text into tokens, with different models utilizing different encodings. Tiktoken supports three encodings commonly used by OpenAI models:\n\n| Encoding name | OpenAI models |\n| --- | --- |\n| cl100k_base | GPT-4, GPT-3.5-turbo, text-embedding-ada-002 |\n| p50k_base | Codex models, text-davinci-002, text-davinci-003 |\n| r50k_base (or gpt2) | GPT-3 models like davinci |\n\n## Understanding the Tokenization of Strings\n\nIn English, tokens can vary in length, ranging from a single character like *t*, to an entire word such as *great*. This is due to the adaptable nature of tokenization, which can accommodate even tokens shorter than a character in complex script languages or tokens longer than a word in languages without spaces or where phrases function as single units.\n\nIt is not uncommon for spaces to be included within tokens, such as `\"is\"` rather than `\"is \"` or `\" \"+\"is\"`. This practice helps maintain the original text formatting and can capture specific linguistic characteristics.\n\n###### Note\n\nTo easily examine the tokenization of a string, you can use [OpenAI Tokenizer](https://oreil.ly/K6ZQK).\n\nYou can install [tiktoken from PyPI](https://oreil.ly/HA2QD) with `pip install` `tiktoken`. In the following example, you’ll see how to easily encode text into tokens and decode tokens into text:\n\n```", "```py\n\nAdditionally let’s write a function that will tokenize the text and then count the number of tokens given a `text_string` and `encoding_name`.\n\n```", "```py\n\nThis code outputs `8`.\n\n# Estimating Token Usage for Chat API Calls\n\nChatGPT models, such as GPT-3.5-turbo and GPT-4, utilize tokens similarly to previous completion models. However, the message-based structure makes token counting for conversations more challenging:\n\n```", "```py\n\n[Example 3-6](#ex-3-6) highlights the specific structure required to make a request against any of the chat models, which are currently GPT-3x and GPT-4.\n\nNormally, chat history is structured with a `system` message first, and then succeeded by alternating exchanges between the `user` and the `assistant`.\n\n##### Example 3-6\\. A payload for the Chat Completions API on OpenAI\n\n```", "```py\n\n`\"role\": \"system\"` describes a system message that’s useful for *providing prompt instructions*. It offers a means to tweak the assistant’s character or provide explicit directives regarding its interactive approach. It’s crucial to understand, though, that the system command isn’t a prerequisite, and the model’s default demeanor without a system command could closely resemble the behavior of “You are a helpful assistant.”\n\nThe roles that you can have are `[\"system\", \"user\", \"assistant\"]`.\n\n`\"content\": \"Some content\"` is where you place the prompt or responses from a language model, depending upon the message’s role. It can be either `\"assistant\"`, `\"system\"`, or `\"user\"`.\n\n# Sentiment Analysis\n\n*Sentiment analysis* is a widely used NLP technique that helps in identifying, extracting, and understanding the emotions, opinions, or sentiments expressed in a piece of text. By leveraging the power of LLMs like GPT-4, sentiment analysis has become an essential tool for businesses, researchers, and developers across various industries.\n\nThe primary goal of sentiment analysis is to determine the attitude or emotional tone conveyed in a text, whether it’s positive, negative, or neutral. This information can provide valuable insights into consumer opinions about products or services, help monitor brand reputation, and even assist in predicting market trends.\n\nThe following are several prompt engineering techniques for creating effective sentiment analysis prompts:\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nAlthough GPT-4 identifies a “mixed tone,” the outcome is a result of several shortcomings in the prompt:\n\nLack of clarity\n\nThe prompt does not clearly define the desired output format.\n\nInsufficient examples\n\nThe prompt does not include any examples of positive, negative, or neutral sentiments, which could help guide the LLM in understanding the distinctions between them.\n\nNo guidance on handling mixed sentiments\n\nThe prompt does not specify how to handle cases where the text contains a mix of positive and negative sentiments.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nThis prompt is much better because it:\n\nProvides clear instructions\n\nThe prompt clearly states the task, which is to classify the sentiment of the given text into one of three categories: positive, negative, or neutral.\n\nOffers examples\n\nThe prompt provides examples for each of the sentiment categories, which helps in understanding the context and desired output.\n\nDefines the output format\n\nThe prompt specifies that the output should be a single word, ensuring that the response is concise and easy to understand.\n\n## Techniques for Improving Sentiment Analysis\n\nTo enhance sentiment analysis accuracy, preprocessing the input text is a vital step. This involves the following:\n\nSpecial characters removal\n\nExceptional characters such as emojis, hashtags, and punctuation may skew the rule-based sentiment algorithm’s judgment. Besides, these characters might not be recognized by machine learning and deep learning models, resulting in misclassification.\n\nLowercase conversion\n\nConverting all the characters to lowercase aids in creating uniformity. For instance, words like *Happy* and *happy* are treated as different words by models, which can cause duplication and inaccuracies.\n\nSpelling correction\n\nSpelling errors can cause misinterpretation and misclassification. Creating a spell-check pipeline can significantly reduce such errors and improve results.\n\nFor industry- or domain-specific text, embedding domain-specific content in the prompt helps in navigating the LLM’s sense of the text’s framework and sentiment. It enhances accuracy in the classification and provides a heightened understanding of particular jargon and expressions.\n\n## Limitations and Challenges in Sentiment Analysis\n\nDespite the advancements in LLMs and the application of prompt engineering techniques, sentiment analysis still faces some limitations and challenges:\n\nHandling sarcasm and irony\n\nDetecting sarcasm and irony in text can be difficult for LLMs, as it often requires understanding the context and subtle cues that humans can easily recognize. Misinterpreting sarcastic or ironic statements may lead to inaccurate sentiment classification.\n\nIdentifying context-specific sentiment\n\nSentiment analysis can be challenging when dealing with context-specific sentiments, such as those related to domain-specific jargon or cultural expressions. LLMs may struggle to accurately classify sentiments in these cases without proper guidance or domain-specific examples.\n\n# Least to Most\n\nThe *least to most* technique in prompt engineering is a powerful method for sequentially generating or extracting increasingly detailed knowledge on a given topic. This method is particularly effective when dealing with complex subjects or when a high level of detail is necessary.\n\nLeast to most uses a *chain* of prompts where each new prompt is based on the last answer. This step-by-step approach helps gather more detailed information each time, making it easier to dive deeper into any topic.\n\nThis technique can also be applied to code generation, as demonstrated in a Flask `Hello World` app example.\n\n## Planning the Architecture\n\nBefore diving into the architecture, let’s briefly understand what Flask is. [Flask](https://oreil.ly/7N-bs) is a lightweight web application framework in Python, widely used for creating web applications quickly and with minimal code. (Flask is only used for demonstration purposes here and isn’t included within the [*requirements.txt* file](https://oreil.ly/TRK0i) for the book.\n\nNow, let’s ask an LLM to outline the basic architecture for a simple Flask “Hello World” application.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\n## Coding Individual Functions\n\nBefore coding, let’s clarify what a Flask route is: it’s a function linked to a URL pattern that determines what users see on a specific web page. Next, we’ll provide the code snippet for a simple “Hello World” route in Flask.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nIn this code, we import the Flask module and create a Flask application instance named `app`. We then define a route using the `@app.route` decorator, specifying the root URL path `'/'`. The `hello_world` function is associated with this route, and it returns the string `'Hello, World!'` as the response.\n\nThe Flask application will start by calling `app.run()` when your Python script is executed directly.\n\n## Adding Tests\n\nFinally, let’s ask it to generate a test case for the “Hello World” route in the Flask application.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nBy using the least to most prompt technique, you can gradually build up the knowledge and understanding required for the task at hand.\n\n## Benefits of the Least to Most Technique\n\nThis method is particularly useful for complex tasks, as it allows an LLM to generate relevant knowledge that will subsequently be used as context for future tasks.\n\nLet’s dive deeper into the benefits of using this approach in various other scenarios:\n\nProgressive exploration\n\nBreaking a complex problem into smaller tasks allows an LLM to provide more detailed and accurate information at each step. This approach is especially helpful when working with a new subject matter or a multifaceted problem.\n\nFlexibility\n\nThe least to most technique offers flexibility in addressing different aspects of a problem. It enables you to pivot, explore alternative solutions, or dive deeper into specific areas as needed.\n\nImproved comprehension\n\nBy breaking down a task into smaller steps, an LLM can deliver information in a more digestible format, making it easier for you to understand and follow.\n\nCollaborative learning\n\nThis technique promotes collaboration between you and an LLM, as it encourages an iterative process of refining the output and adjusting your responses to achieve the desired outcome.\n\n## Challenges with the Least to Most Technique\n\nOverreliance on previously generated knowledge\n\nUsing previous chat history to store the state may lead to larger tasks forgetting their initial tasks/outputs due to limitations in context length.\n\nDependence on prior prompts\n\nSince each prompt is built upon preceding outputs, it is imperative to ensure that the quality and responses of previous prompts provide ample information for the next step.\n\n# Evaluate Quality\n\nIn the process of designing prompts, make sure to evaluate the quality of each prior LLM response. The performance of the next task depends upon the quality of information provided by the preceding one.\n\n# Role Prompting\n\n*Role prompting* is a technique in which the AI is given a specific role or character to assume while generating a response. This helps guide the AI’s response style and content to better align with the user’s needs or desired outcome.\n\n###### Note\n\n[Awesome ChatGPT prompts](https://oreil.ly/8pf40) provides an extensive list of role prompts that you can use.\n\nLet’s ask ChatGPT to take on the role of a tech reviewer for MongoDB.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nThe prompt is an excellent example of role prompting, as it clearly defines the role the AI should assume (a tech reviewer) and sets expectations for the type of response desired (an in-depth review of MongoDB).\n\n# Give Direction\n\nWhen crafting prompts, consider assigning a specific role to the AI. This sets the proper context for the response, creating a more focused and relevant output.\n\n# Benefits of Role Prompting\n\nRole prompting helps narrow down the AI’s responses, ensuring more focused, contextually appropriate, and tailored results. It can also enhance creativity by pushing the AI to think and respond from unique perspectives.\n\n# Challenges of Role Prompting\n\nRole prompting can pose certain challenges. There might be potential risks for bias or stereotyping based on the role assigned. Assigning stereotyped roles can lead to generating biased responses, which could harm usability or offend individuals. Additionally, maintaining consistency in the role throughout an extended interaction can be difficult. The model might drift off-topic or respond with information irrelevant to the assigned role.\n\n# Evaluate Quality\n\nConsistently check the quality of the LLM’s responses, especially when role prompting is in play. Monitor if the AI is sticking to the role assigned or if it is veering off-topic.\n\n# When to Use Role Prompting\n\nRole prompting is particularly useful when you want to:\n\nElicit specific expertise\n\nIf you need a response that requires domain knowledge or specialized expertise, role prompting can help guide the LLM to generate more informed and accurate responses.\n\nTailor response style\n\nAssigning a role can help an LLM generate responses that match a specific tone, style, or perspective, such as a formal, casual, or humorous response.\n\nEncourage creative responses\n\nRole prompting can be used to create fictional scenarios or generate imaginative answers by assigning roles like a storyteller, a character from a novel, or a historical figure.\n\n*   *Explore diverse perspectives*: If you want to explore different viewpoints on a topic, role prompting can help by asking the AI to assume various roles or personas, allowing for a more comprehensive understanding of the subject.\n\n*   *Enhance user engagement*: Role prompting can make interactions more engaging and entertaining by enabling an LLM to take on characters or personas that resonate with the user.\n\nIf you’re using OpenAI, then the best place to add a role is within the `System Message` for chat models.\n\n# GPT Prompting Tactics\n\nSo far you’ve already covered several prompting tactics, including asking for context, text style bundling, least to most, and role prompting.\n\nLet’s cover several more tactics, from managing potential hallucinations with appropriate reference text, to providing an LLM with critical *thinking time*, to understanding the concept of *task decomposition*—we have plenty for you to explore.\n\nThese methodologies have been designed to significantly boost the precision of your AI’s output and are recommended by [OpenAI](https://oreil.ly/QZE8n). Also, each tactic utilizes one or more of the prompt engineering principles discussed in [Chapter 1](ch01.html#five_principles_01).\n\n## Avoiding Hallucinations with Reference\n\nThe first method for avoiding text-based hallucinations is to instruct the model to *only answer using reference text.*\n\nBy supplying an AI model with accurate and relevant information about a given query, the model can be directed to use this information to generate its response.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nIf you were to ask the same reference text this question:\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\n# Give Direction and Specify Format\n\nThe preceding prompt is excellent as it both instructs the model on how to find answers and also sets a specific response format for any unanswerable questions.\n\nConsidering the constrained context windows of GPTs, a method for dynamically retrieving information relevant to the asked query might be necessary to utilize this strategy.\n\nAnother approach is to direct the model to *incorporate references* from a given text in its response. When the input is enhanced with relevant information, the model can be guided to include citations in its responses by referring to sections of the supplied documents. This approach has the added benefit that citations in the output can be *authenticated automatically by matching strings* within the given documents.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\n## Give GPTs “Thinking Time”\n\nOften, by explicitly guiding an LLM to *derive solutions from first principles* before reaching a verdict, you can garner more accurate responses. Providing an LLM with *thinking time* can often lead to better results.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nIn some cases, when using GPT-4 or other generative AI models, you may not want the model’s reasoning process to be visible to the user.\n\nYou can achieve this by asking an LLM to *generate an inner monologue*. This is particularly useful in tutoring applications, where revealing the model’s reasoning might give away the solution prematurely.\n\n## The Inner Monologue Tactic\n\nThe *inner monologue tactic* instructs the model to structure parts of the output that should be hidden from the user in a specific format. This makes it easy to remove these parts before presenting the final output to the user.\n\nHere’s how you can utilize this tactic to answer user queries effectively.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\n## Self-Eval LLM Responses\n\nAnother tactic you can use is to *critque a generated LLM output* and ask whether the LLM missed any information or important facts. You’re essentially asking an LLM to *evaluate itself* based on its previous output.\n\nFirst, create a prompt to generate a `\"Hello World\"` function.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nThe LLM returns a simple Python function called `print_hello_world()` that prints the traditional “Hello, World!” greeting.\n\nThen make a second LLM request with the previous chat history, asking for the initial output to be improved.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nConsequently GPT-4 returns an upgraded version of the function, now furnished with Python type hints and a default argument. This enriches the function with greater flexibility, allowing it to print not just `\"Hello, World!\"` but any user-specified message.\n\nThese prompt-response exchanges illustrate how you can easily refine generated LLM outputs until you’re satisfied with the final output.\n\n###### Note\n\nIt’s possible to critique an LLM’s response multiple times, until no further refinement is provided by the LLM.\n\n# Classification with LLMs\n\nClassifying, in the context of AI, refers to the process of predicting the class or category of a given data point or sample. It’s a common task in machine learning where models are trained to assign predefined labels to unlabeled data based on learned patterns.\n\nLLMs are powerful assets when it comes to classification, even with zero or only a small number of examples provided within a prompt. Why? That’s because LLMs, like GPT-4, have been previously trained on an extensive dataset and now possess a degree of reasoning.\n\nThere are two overarching strategies in solving classification problems with LLMs: *zero-shot learning* and *few-shot learning*.\n\nZero-shot learning\n\nIn this process, the LLM classifies data with exceptional accuracy, without the aid of any prior specific examples. It’s akin to acing a project without any preparation—impressive, right?\n\nFew-shot learning\n\nHere, you provide your LLM with a small number of examples. This strategy can significantly influence the structure of your output format and enhance the overall classification accuracy.\n\nWhy is this groundbreaking for you?\n\nLeveraging LLMs lets you sidestep lengthy processes that traditional machine learning processes demand. Therefore, you can quickly prototype a classification model, determine a base level accuracy, and create immediate business value.\n\n###### Warning\n\nAlthough an LLM can perform classification, depending upon your problem and training data you might find that using a traditional machine learning process could yield better results.\n\n# Building a Classification Model\n\nLet’s explore a few-shot learning example to determine the sentiment of text into either `'Compliment'`, `'Complaint'`, or `'Neutral'`.\n\n```", "```py\n\n```", "```py\n\nSeveral good use cases for LLM classification include:\n\nCustomer reviews\n\nClassify user reviews into categories like “Positive,” “Negative,” or “Neutral.” Dive deeper by further identifying subthemes such as “Usability,” “Customer Support,” or “Price.”\n\nEmail filtering\n\nDetect the intent or purpose of emails and classify them as “Inquiry,” “Complaint,” “Feedback,” or “Spam.” This can help businesses prioritize responses and manage communications efficiently.\n\nSocial media sentiment analysis\n\nMonitor brand mentions and sentiment across social media platforms. Classify posts or comments as “Praise,” “Critic,” “Query,” or “Neutral.” Gain insights into public perception and adapt marketing or PR strategies accordingly.\n\nNews article categorization\n\nGiven the vast amount of news generated daily, LLMs can classify articles by themes or topics such as “Politics,” “Technology,” “Environment,” or “Entertainment.”\n\nRésumé screening\n\nFor HR departments inundated with résumés, classify them based on predefined criteria like “Qualified,” “Overqualified,” “Underqualified,” or categorize by expertise areas such as “Software Development,” “Marketing,” or “Sales.”\n\n###### Warning\n\nBe aware that exposing emails, résumés, or sensitive data does run the risk of data being leaked into OpenAI’s future models as training data.\n\n# Majority Vote for Classification\n\nUtilizing multiple LLM requests can help in reducing the variance of your classification labels. This process, known as *majority vote*, is somewhat like choosing the most common fruit out of a bunch. For instance, if you have 10 pieces of fruit and 6 out of them are apples, then apples are the majority. The same principle goes for choosing the majority vote in classification labels.\n\nBy soliciting several classifications and taking the *most frequent classification*, you’re able to reduce the impact of potential outliers or unusual interpretations from a single model inference. However, do bear in mind that there can be significant downsides to this approach, including the increased time required and cost for multiple API calls.\n\nLet’s classify the same piece of text three times, and then take the majority vote:\n\n```", "```py\n\nCalling the `most_frequent_classification(responses)` function should pinpoint `'Neutral'` as the dominant sentiment. You’ve now learned how to use the OpenAI package for majority vote classification.\n\n# Criteria Evaluation\n\nIn [Chapter 1](ch01.html#five_principles_01), a human-based evaluation system was used with a simple thumbs-up/thumbs-down rating system to identify how often a response met our expectations. Rating manually can be expensive and tedious, requiring a qualified human to judge quality or identify errors. While this work can be outsourced to low-cost raters on services such as [Mechanical Turk](https://www.mturk.com), designing such a task in a way that gets valid results can itself be time-consuming and error prone. One increasingly common approach is to use a more sophisticated LLM to evaluate the responses of a smaller model.\n\nThe evidence is mixed on whether LLMs can act as effective evaluators, with some studies [claiming LLMs are human-level evaluators](https://oreil.ly/nfc3f) and others [identifying inconsistencies in how LLMs evaluate](https://oreil.ly/ykkzY). In our experience, GPT-4 is a useful evaluator with consistent results across a diverse set of tasks. In particular, GPT-4 is effective and reliable in evaluating the responses from smaller, less sophisticated models like GPT-3.5-turbo. In the example that follows, we generate concise and verbose examples of answers to a question using GPT-3.5-turbo, ready for rating with GPT-4.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nThis script is a Python program that interacts with the OpenAI API to generate and evaluate responses based on their conciseness. Here’s a step-by-step explanation:\n\n1.  `responses = []` creates an empty list named `responses` to store the responses generated by the OpenAI API.\n\n2.  The `for` loop runs 10 times, generating a response for each iteration.\n\n3.  Inside the loop, `style` is determined based on the current iteration number (`i`). It alternates between “concise” and “verbose” for even and odd iterations, respectively.\n\n4.  Depending on the `style`, a `prompt` string is formatted to ask, “What is the meaning of life?” in either a concise or verbose manner.\n\n5.  `response = client.chat.completions.create(...)` makes a request to the OpenAI API to generate a response based on the `prompt`. The model used here is specified as “gpt-3.5-turbo.”\n\n6.  The generated response is then stripped of any leading or trailing whitespace and added to the `responses` list.\n\n7.  `system_prompt = \"\"\"You are assessing...\"\"\"` sets up a prompt used for evaluating the conciseness of the generated responses.\n\n8.  `ratings = []` initializes an empty list to store the conciseness ratings.\n\n9.  Another `for` loop iterates over each response in `responses`.\n\n10.  For each response, the script sends it along with the `system_prompt` to the OpenAI API, requesting a conciseness evaluation. This time, the model used is “gpt-4.”\n\n11.  The evaluation rating (either 1 for concise or 0 for not concise) is then stripped of whitespace and added to the `ratings` list.\n\n12.  The final `for` loop iterates over the `ratings` list. For each rating, it prints the `style` of the response (either “concise” or “verbose”) and its corresponding conciseness `rating`.\n\nFor simple ratings like conciseness, GPT-4 performs with near 100% accuracy; however, for more complex ratings, it’s important to spend some time evaluating the evaluator. For example, by setting test cases that contain an issue, as well as test cases that do not contain an issue, you can identify the accuracy of your evaluation metric. An evaluator can itself be evaluated by counting the number of false positives (when the LLM hallucinates an issue in a test case that is known not to contain an issue), as well as the number of false negatives (when the LLM misses an issue in a test case that is known to contain an issue). In our example we generated the concise and verbose examples, so we can easily check the rating accuracy, but in more complex examples you may need human evaluators to validate the ratings.\n\n# Evaluate Quality\n\nUsing GPT-4 to evaluate the responses of less sophisticated models is an emerging standard practice, but care must be taken that the results are reliable and consistent.\n\nCompared to human-based evaluation, LLM-based or synthetic evaluation typically costs an order of magnitude less and completes in a few minutes rather than taking days or weeks. Even in important or sensitive cases where a final manual review by a human is necessary, rapid iteration and A/B testing of the prompt through synthetic reviews can save significant time and improve results considerably. However, the cost of running many tests at scale can add up, and the latency or rate limits of GPT-4 can be a blocker. If at all possible, a prompt engineer should first test using programmatic techniques that don’t require a call to an LLM, such as simply measuring the length of the response, which runs near instantly for close to zero cost.\n\n# Meta Prompting\n\n*Meta prompting* is a technique that involves the creation of text prompts that, in turn, generate other text prompts. These text prompts are then used to generate new assets in many mediums such as images, videos, and more text.\n\nTo better understand meta prompting, let’s take the example of authoring a children’s book with the assistance of GPT-4\\. First, you direct the LLM to generate the text for your children’s book. Afterward, you invoke meta prompting by instructing GPT-4 to produce prompts that are suitable for image-generation models. This could mean creating situational descriptions or specific scenes based on the storyline of your book, which then can be given to AI models like Midjourney or Stable Diffusion. These image-generation models can, therefore, deliver images in harmony with your AI-crafted children’s story.\n\n[Figure 3-8](#figure-3-8) visually describes the process of meta prompting in the context of crafting a children’s book.\n\n![Creating image prompts from an LLM that will later be used by MidJourney for image creation.](assets/pega_0308.png)\n\n###### Figure 3-8\\. Utilizing an LLM to generate image prompts for MidJourney’s image creation in the process of crafting a children’s book\n\nMeta prompts offer a multitude of benefits for a variety of applications:\n\nImage generation from product descriptions\n\nMeta prompts can be employed to derive an image generation prompt for image models like [Midjourney](https://www.midjourney.com), effectively creating a visual representation of product descriptions.\n\nGenerating style/feature prompts\n\nLet’s consider you are a copywriter needing to develop a unique style guide prompt from a couple of blog posts. Given each client has a distinctive tone and style, it’s beneficial to utilize a *meta prompt* that encapsulates all the varied features, rather than producing a single prompt output.\n\nOptimizing prompts to achieve specific goals\n\nA common approach is to ask ChatGPT or another language model to refine or improve `Prompt A` in order to attain `Goal 1`, given `Prompt A` and `Goal 1`. This method aids in identifying any missed input features, that by adding could assist the language model in generating a more detailed and desirable response.\n\nLet’s experiment with creating a meta prompt for the copywriting example.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nAs previously mentioned, it can be beneficial to *critique the model’s output* and to ask for a refined prompt either via the interface or API.\n\nChatGPT can perform web searches and can make use of the content of web pages, along with the previously generated copywriting style guide. Let’s provide two URLs that ChatGPT will read to enhance the output.\n\nInput:\n\n```", "```py\n\n[Figure 3-9](#figure-3-9) shows the output.\n\n![pega 0309](assets/pega_0309.png)\n\n###### Figure 3-9\\. ChatGPT refining a meta prompt by two URL web pages\n\n```"]