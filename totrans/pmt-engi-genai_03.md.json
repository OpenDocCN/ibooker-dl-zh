["```py\nGenerate a list of Disney characters.\n```", "```py\nSure, here is a list of some popular Disney characters:\n\n1\\. Mickey Mouse\n2\\. Minnie Mouse\n...\n30\\. Bagheera (The Jungle Book)\n```", "```py\nGenerate a bullet-point list of 5 male Disney characters.\nOnly include the name of the character for each line.\nNever include the film for each Disney character.\nOnly return the Disney characters, never include any commentary.\n\nBelow is an example list:\n\n* Aladdin\n* Simba\n* Beast\n* Hercules\n* Tarzan\n```", "```py\n* Woody\n* Buzz Lightyear\n* Stitch\n* Jack Sparrow\n* Prince Charming\n```", "```py\nGenerate a hierarchical and incredibly detailed article outline on:\n\nWhat are the benefits of data engineering.\n\nSee an example of the hierarchical structure below:\n\nArticle Title: What are the benefits of digital marketing?\n\n* Introduction\n    a. Explanation of digital marketing\n    b. Importance of digital marketing in today's business world\n* Increased Brand Awareness\n    a. Definition of brand awareness\n    b. How digital marketing helps in increasing brand awareness\n```", "```py\nArticle Title: What are the benefits of data engineering?\n\n* Introduction\n    a. Explanation of data engineering\n    b. Importance of data engineering in today’s data-driven world\n\n...(10 sections later)...\n\n* Conclusion\n    a. Importance of data engineering in the modern business world\n    b. Future of data engineering and its impact on the data ecosystem\n```", "```py\nimport re\n\n# openai_result = generate_article_outline(prompt)\n# Commented out to focus on a fake LLM response, see below:\n\nopenai_result = '''\n* Introduction\n a. Explanation of data engineering\n b. Importance of data engineering in today’s data-driven world\n* Efficient Data Management\n a. Definition of data management\n b. How data engineering helps in efficient data management\n* Conclusion\n a. Importance of data engineering in the modern business world\n b. Future of data engineering and its impact on the data ecosystem\n'''\n\n# Regular expression patterns\nheading_pattern = r'\\* (.+)'\nsubheading_pattern = r'\\s+[a-z]\\. (.+)'\n\n# Extract headings and subheadings\nheadings = re.findall(heading_pattern, openai_result)\nsubheadings = re.findall(subheading_pattern, openai_result)\n\n# Print results\nprint(\"Headings:\\n\")\nfor heading in headings:\n    print(f\"* {heading}\")\n\nprint(\"\\nSubheadings:\\n\")\nfor subheading in subheadings:\n    print(f\"* {subheading}\")\n```", "```py\nHeadings:\n- Introduction\n- Efficient Data Management\n- Conclusion\n\nSubheadings:\n- Explanation of data engineering\n- Importance of data engineering in today’s data-driven world\n- Definition of data management\n- How data engineering helps in efficient data management\n- Importance of data engineering in the modern business world\n- Future of data engineering and its impact on the data ecosystem\n```", "```py\nimport re\n\nopenai_result = \"\"\"\n* Introduction\n a. Explanation of data engineering\n b. Importance of data engineering in today’s data-driven world\n* Efficient Data Management\n a. Definition of data management\n b. How data engineering helps in efficient data management\n c. Why data engineering is important for data management\n* Conclusion\n a. Importance of data engineering in the modern business world\n b. Future of data engineering and its impact on the data ecosystem\n\"\"\"\n\nsection_regex = re.compile(r\"\\* (.+)\")\nsubsection_regex = re.compile(r\"\\s*([a-z]\\..+)\")\n\nresult_dict = {}\ncurrent_section = None\n\nfor line in openai_result.split(\"\\n\"):\n    section_match = section_regex.match(line)\n    subsection_match = subsection_regex.match(line)\n\n    if section_match:\n        current_section = section_match.group(1)\n        result_dict[current_section] = []\n    elif subsection_match and current_section is not None:\n        result_dict[current_section].append(subsection_match.group(1))\n\nprint(result_dict)\n```", "```py\n{\n    \"Introduction\": [\n        \"a. Explanation of data engineering\",\n        \"b. Importance of data engineering in today’s data-driven world\"\n    ],\n    \"Efficient Data Management\": [\n        \"a. Definition of data management\",\n        \"b. How data engineering helps in efficient data management\"\n    ],\n    \"Conclusion\": [\n        \"a. Importance of data engineering in the modern business world\",\n        \"b. Future of data engineering and its impact on the data ecosystem\"\n    ]\n}\n```", "```py\nCompose a very detailed article outline on \"The benefits of learning code\" with a\nJSON payload structure that highlights key points.\n\nOnly return valid JSON.\n\nHere is an example of the JSON structure:\n{\n    \"Introduction\": [\n        \"a. Explanation of data engineering\",\n        \"b. Importance of data engineering in today’s data-driven world\"],\n    ...\n    \"Conclusion\": [\n        \"a. Importance of data engineering in the modern business world\",\n        \"b. Future of data engineering and its impact on the data ecosystem\"]\n}\n```", "```py\n{\n    \"Introduction\": [\n        \"a. Overview of coding and programming languages\",\n        \"b. Importance of coding in today's technology-driven world\"],\n    ...\n    \"Conclusion\": [\n        \"a. Recap of the benefits of learning code\",\n        \"b. The ongoing importance of coding skills in the modern world\"]\n}\n```", "```py) , such as:\n\nOutput:\n\n```", "```pyjson\n{\"Name\": \"John Smith\"} # valid payload\n{\"Name\": \"John Smith\", \"some_key\":} # invalid payload\n```", "```py\n\nIdeally you would like the model to respond like so:\n\nOutput:\n\n```", "```py\n\nThis is important because with the first output, you’d have to split after `json` and then parse the exact part of the string that contained valid JSON. There are several points that are worth adding to your prompts to improve JSON parsing:\n\n```", "```py\n\nNow let’s examine how you can parse a [JSON output with Python](https://oreil.ly/MoJHn):\n\n```", "```py\n\nWell done, you’ve successfully parsed some JSON.\n\nAs showcased, structuring data from an LLM response is streamlined when requesting the response in valid JSON format. Compared to the previously demonstrated regular expression parsing, this method is less cumbersome and more straightforward.\n\nSo what could go wrong?\n\n*   The language model accidentally adds extra text to the response such as `json output:` and your application logic only handles for valid JSON.\n\n*   The JSON produced isn’t valid and fails upon parsing (either due to the size or simply for not escaping certain characters).\n\nLater on you will examine strategies to gracefully handle for such edge cases.\n\n## YAML\n\n*.yml* files are a structured data format that offer different benefits over *.json*:\n\nNo need to escape characters\n\nYAML’s indentation pattern eliminates the need for braces, brackets, and commas to denote structure. This can lead to cleaner and less error-prone files, as there’s less risk of mismatched or misplaced punctuation.\n\nReadability\n\nYAML is designed to be human-readable, with a simpler syntax and structure compared to JSON. This makes it easier for you to create, read, and edit prompts, especially when dealing with complex or nested structures.\n\nComments\n\nUnlike JSON, YAML supports comments, allowing you to add annotations or explanations to the prompts directly in the file. This can be extremely helpful when working in a team or when revisiting the prompts after some time, as it allows for better understanding and collaboration.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nNotice with the preceding example how an LLM is able to infer the correct *.yml* format from the `User Query` string.\n\nAdditionally, you’ve given the LLM an opportunity to either:\n\n*   Return a valid *.yml* response\n\n*   Return a filtered *.yml* response\n\nIf after filtering, there are no *.yml* items left, then return *No Items*.\n\n# Filtering YAML Payloads\n\nYou might decide to use this same prompt for cleaning/filtering a *.yml* payload.\n\nFirst, let’s focus on a payload that contains both valid and invalid `schema` in reference to our desired `schema`. `Apple slices` fit the criteria; however, `Bananas` doesn’t exist, and you should expect for the `User Query` to be appropriately filtered.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nIn the preceding example, you’ve successfully filtered the user’s payload against a set criteria and have used the language model as a *reasoning engine*.\n\nBy providing the LLM with a set of instructions within the prompt, the response is closely related to what a human might do if they were manually cleaning the data.\n\nThe input prompt facilitates the delegation of more control flow tasks to a language learning model (LLM), tasks that would typically require coding in a programming language like Python or JavaScript.\n\n[Figure 3-1](#figure-3-1) provides a detailed overview of the logic applied when processing user queries by an LLM.\n\n![Using an LLM to determine the control flow of an application instead of directly using code.](assets/pega_0301.png)\n\n###### Figure 3-1\\. Using an LLM to determine the control flow of an application instead of code\n\n# Handling Invalid Payloads in YAML\n\nA completely invalid payload might look like this:\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nAs expected, the LLM returned `No Items` as none of the `User Query` items matched against the previously defined `schema`.\n\nLet’s create a Python script that gracefully accommodates for the various types of LLM results returned. The core parts of the script will focus on:\n\n*   Creating custom exceptions for each type of error that might occur due to the three LLM response scenarios\n\n*   Parsing the proposed schema\n\n*   Running a serious of custom checks against the response so you can be sure that the YML response can be safely passed to downstream software applications/microservices\n\nYou could define six specific errors that would handle for all of the edge cases:\n\n```", "```py\n\nThen provide the previously proposed `YML schema` as a string:\n\n```", "```py\n\nImport the `yaml` module and create a custom parser function called `validate_``response` that allows you to easily determine whether an LLM output is valid:\n\n```", "```py\n\nTo test these edge cases, following you’ll find several mocked LLM responses:\n\n```", "```py\n\nFinally, now you can:\n\n*   Use `yaml.safe_load(response)` to safely parse the *.yml* schema\n\n*   Call the `validate_response` function for each LLM response to test it against custom *.yml* validation logic\n\n```", "```py\n\n# Diverse Format Generation with ChatGPT\n\nChatGPT is not only capable of generating *.json* or *.yml* files but can also support many other data formats. For example, it can aid you in creating a mermaid diagram or generating a mock *.csv* document. Its versatility stretches to even generating code, conversations, and scripts.\n\nLet’s explore in detail how ChatGPT can produce diverse formatted content. Mermaid is a markdown-like scripting language for generating charts, diagrams, and other visualizations.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nThis output, when input to a mermaid diagram renderer, will create a precise flowchart reflecting the food ordering process ([Figure 3-2](#figure-3-2)).\n\n![A simple mermaid diagram output from an LLM.](assets/pega_0302.png)\n\n###### Figure 3-2\\. A streamlined flow diagram created using mermaid syntax via an LLM\n\n## Mock CSV Data\n\nChatGPT can also generate mock CSV data that can be utilized for testing purposes or other uses.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nThe example generated is a well-formatted CSV file, which can be loaded directly into many tools that read CSV data.\n\nRemember, GPT models rely heavily on the format you specify. For instance, specifying CSV implies the AI should use commas as separators. Be clear about the desired format for optimal results.\n\nYou’ve explored different ways of extracting structured data from language models including regular expressions, JSON, YML, and other formats. You can also now use LLMs to direct parts of your application’s control flow.\n\n# Explain It like I’m Five\n\nThe *Explain It like I’m Five* prompt focuses on explaining a given section of text *as if you were talking to a five-year-old child.*\n\nThis style of prompting is very effective and is a great way to convert technical documents into simple summaries that anyone can understand.\n\nLet’s apply this prompt to an abstract of [“Tumor Microenvironment: Recent Advances in Various Cancer Treatments”](https://oreil.ly/MDOsz).\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nThe ChatGPT output from the preceding example is much more digestible than the abstract.\n\n# Universal Translation Through LLMs\n\nAdditionally, language models can potentially act as *universal translators* due to their proficiency in understanding and generating human-like text across multiple languages.\n\nLet’s investigate this with a simple example:\n\n1.  Generate some simple text.\n\n2.  Change the simple text and make it difficult to read.\n\n3.  Then transform the same text back into an easy to read format in Spanish (instead of English).\n\n4.  Revert the simplistic text back into English.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nAs you can see, part of the meaning is lost while doing the translation, and this is to be expected. However, large parts of the original text’s intent remains, demonstrating that LLMs can act as *highly effective translators* for languages that have adequate resources on the internet. For languages that do not have a good amount of resources, it will yield bad results.\n\nThe same logic applies to coding languages. LLMs are very good at generating code for established programming languages such as Python and JavaScript but perform worse for newer coding languages and packages.\n\nThe boundaries between different forms of information are becoming *increasingly fluid*. The essence of information itself is evolving, allowing for effortless transformations of summaries into stories, poems, or other creative expressions, ultimately enriching our understanding and engagement with the content.\n\n*Diffusion models* are a unique class of generative models utilized in machine learning, specifically designed to produce new images that mimic those found in the training set.\n\nMoreover, when you combine language models with diffusion models, it enables seamless transitions between text, video, and other modalities. This makes it even simpler for you to convey complex ideas across various formats, facilitating a more accessible and comprehensive experience.\n\n# Ask for Context\n\nLLMs are not only capable of generating text but can also act as simple agents with a limited amount of *reasoning capability.* This allows you to write a prompt asking the language model to either:\n\n*   Return a valid result to a question or statement\n\n*   Ask for more context to appropriately answer the question\n\nIn this section, you’ll learn about the importance of *asking for context* when working with LLMs such as GPT-4\\. We will start with an example of a prompt that doesn’t provide enough context, resulting in a less useful response.\n\nThen, we will provide a better prompt that encourages the model to ask for additional context if needed. Finally, we will use the additional context provided to generate a more informed response.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nIn the preceding prompt, the model does not have enough context to make a meaningful recommendation. Instead, you can ask ChatGPT for a list of recommended points that would help it to make an effective decision.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nAfter prompting ChatGPT about how to make an informed decision, now you’re aware of what to include within your prompt to help the language model with deciding.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nIn this final example, the model uses the additional context provided to give a well-informed recommendation for using PostgreSQL. By asking for context when necessary, LLMs like ChatGPT and GPT-4 can deliver more valuable and accurate responses.\n\n[Figure 3-3](#figure-3-3) demonstrates how *asking for context* changes the decision-making process of LLMs. Upon receiving user input, the model first assesses whether the context given is sufficient. If not, it prompts the user to provide more detailed information, emphasizing the model’s reliance on context-rich inputs. Once adequate context is acquired, the LLM then generates an informed and relevant response.\n\n![The decision process of an LLM with asking for context.](assets/pega_0303.png)\n\n###### Figure 3-3\\. The decision process of an LLM while asking for context\n\n# Allow the LLM to Ask for More Context by Default\n\nYou can allow the LLM to ask for more context as a default by including this key phrase: *If you need more context, please specify what would help you to make a better decision.*\n\nIn this section, you’ve seen how LLMs can act as agents that use environmental context to make decisions. By iteratively refining the prompt based on the model’s recommendations, we eventually reach a point where the model has *enough context to make a well-informed decision.*\n\nThis process highlights the importance of providing sufficient context in your prompts and being prepared to ask for more information when necessary. By doing so, you can leverage the power of LLMs like GPT-4 to make more accurate and valuable recommendations.\n\nIn agent-based systems like GPT-4, the ability to ask for more context and provide a finalized answer is crucial for making well-informed decisions. [AutoGPT](https://oreil.ly/l3Ihy), a multiagent system, has a self-evaluation step that automatically checks whether the task can be completed given the current context within the prompt. This technique uses an actor–critic relationship, where the existing prompt context is being analyzed to see whether it could be further refined before being executed.\n\n# Text Style Unbundling\n\n*Text style unbundling* is a powerful technique in prompt engineering that allows you to extract and isolate specific textual features from a given document, such as tone, length, vocabulary, and structure.\n\nThis allows you to create new content that shares similar characteristics with the original document, ensuring consistency in style and tone across various forms of communication.\n\nThis consistency can be crucial for businesses and organizations that need to communicate with a unified voice across different channels and platforms. The benefits of this technique include:\n\nImproved brand consistency\n\nBy ensuring that all content follows a similar style, organizations can strengthen their brand identity and maintain a cohesive image.\n\nStreamlined content creation\n\nBy providing a clear set of guidelines, writers and content creators can more easily produce materials that align with a desired style.\n\nAdaptability\n\nText style unbundling allows for the easy adaptation of existing content to new formats or styles while preserving the core message and tone.\n\nThe process of text style unbundling involves *identifying the desired textual features* or creating a meta prompt (a prompt to create prompts) to extract these features and then using the extracted features to guide the generation of new content.\n\n# Identifying the Desired Textual Features\n\nTo successfully unbundle a text style, you must first identify the specific features you want to extract from the input document. Common textual features to consider include:\n\nTone of voice\n\nThe overall mood or attitude conveyed by the text, such as formal, casual, humorous, or authoritative\n\nLength\n\nThe desired word count or general length of the content\n\nVocabulary and phrasing\n\nThe choice of words and expressions, including industry-specific jargon, colloquialisms, and complexity\n\nAdditionally, you can even create a meta prompt that will dynamically show you these.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\n# Generating New Content with the Extracted Features\n\nAfter you’ve extracted the desired features from the input document, you can use this information to guide future content generation. Remember to ensure that the AI model incorporates the extracted features into its output by specifying the desired style in your prompt. For example:\n\n*   `Write a new blog post on [topic] using the same tone of voice, length, vocabulary, and structure as the previously analyzed text.`\n\nBy combining this technique with *reference text* (documents that act as grounding truth), you can produce credible, branded content that requires minimal revisions.\n\n# Extracting Specific Textual Features with LLMs\n\nYou can easily tailor a prompt to guide an LLM in extracting particular textual features from a document. This can be applied beyond just analyzing text for copywriting purposes. For instance, recognizing entities or discerning sentiment from the text can be achieved by crafting a precise instruction for the LLM.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\n# Summarization\n\nIn the era of information overload, the ability to condense large amounts of text into concise, digestible summaries has become an essential skill. As the volume of data we produce and consume continues to grow, the demand for effective summarization techniques has increased significantly.\n\nAI has made significant advancements in this domain, providing powerful tools for generating summaries that retain the most critical information while discarding less relevant details.\n\nLet’s explore how to summarize using a language model.\n\nInput:\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nSummarization is an invaluable application of AI, enabling users to quickly extract key insights from lengthy articles, reports, or research papers. This process can help individuals make informed decisions, save time, and prioritize their reading. AI-generated summaries can also facilitate information sharing among teams, allowing for more efficient collaboration and communication.\n\n# Summarizing Given Context Window Limitations\n\nFor documents larger than an LLM can handle in a single API request, a common approach is to chunk the document, summarize each chunk, and then combine these summaries into a final summary, as shown in [Figure 3-4](#figure-3-4).\n\n![.A summarization pipeline that uses text splitting and multiple summarization steps.](assets/pega_0304.png)\n\n###### Figure 3-4\\. A summarization pipeline that uses text splitting and multiple summarization steps\n\nAdditionally, people may require different types of summaries for various reasons, and this is where AI summarization comes in handy. As illustrated in the preceding diagram, a large PDF document could easily be processed using AI summarization to generate distinct summaries tailored to individual needs:\n\nSummary A\n\nProvides key insights, which is perfect for users seeking a quick understanding of the document’s content, enabling them to focus on the most crucial points\n\nSummary B\n\nOn the other hand, offers decision-making information, allowing users to make informed decisions based on the content’s implications and recommendations\n\nSummary C\n\nCaters to collaboration and communication, ensuring that users can efficiently share the document’s information and work together seamlessly\n\nBy customizing the summaries for different users, AI summarization contributes to increased information retrieval for all users, making the entire process more efficient and targeted.\n\nLet’s assume you’re only interested in finding and summarizing information about the advantages of digital marketing. Simply change your summarization prompt to `Provide a concise, abstractive summary of the above text. Only summarize the advantages: ...`\n\nAI-powered summarization has emerged as an essential tool for quickly distilling vast amounts of information into concise, digestible summaries that cater to various user needs. By leveraging advanced language models like GPT-4, AI summarization techniques can efficiently extract key insights and decision-making information, and also facilitate collaboration and communication.\n\nAs the volume of data continues to grow, the demand for effective and targeted summarization will only increase, making AI a crucial asset for individuals and organizations alike in navigating the Information Age.\n\n# Chunking Text\n\nLLMs continue to develop and play an increasingly crucial role in various applications, as the ability to process and manage large volumes of text becomes ever more important. An essential technique for handling large-scale text is known as *chunking.*\n\n*Chunking* refers to the process of breaking down large pieces of text into smaller, more manageable units or chunks. These chunks can be based on various criteria, such as sentence, paragraph, topic, complexity, or length. By dividing text into smaller segments, AI models can more efficiently process, analyze, and generate responses.\n\n[Figure 3-5](#figure-3-5) illustrates the process of chunking a large piece of text and subsequently extracting topics from the individual chunks.\n\n![Topic extraction process after chunking text.](assets/pega_0305.png)\n\n###### Figure 3-5\\. Topic extraction with an LLM after chunking text\n\n## Benefits of Chunking Text\n\nThere are several advantages to chunking text, which include:\n\nFitting within a given context length\n\nLLMs only have a certain amount of input and output tokens, which is called a *context length*. By reducing the input tokens you can make sure the output won’t be cut off and the initial request won’t be rejected.\n\nReducing cost\n\nChunking helps you to only retrieve the most important points from documents, which reduces your token usage and API costs.\n\nImproved performance\n\nChunking reduces the processing load on LLMs, allowing for faster response times and more efficient resource utilization.\n\nIncreased flexibility\n\nChunking allows developers to tailor AI responses based on the specific needs of a given task or application.\n\n## Scenarios for Chunking Text\n\nChunking text can be particularly beneficial in certain scenarios, while in others it may not be required. Understanding when to apply this technique can help in optimizing the performance and cost efficiency of LLMs.\n\n### When to chunk\n\nLarge documents\n\nWhen dealing with extensive documents that exceed the maximum token limit of the LLM\n\nComplex analysis\n\nIn scenarios where a detailed analysis is required and the document needs to be broken down for better comprehension and processing\n\nMultitopic documents\n\nWhen a document covers multiple topics and it’s beneficial to handle them individually\n\n### When not to chunk\n\nShort documents\n\nWhen the document is short and well within the token limits of the LLM\n\nSimple analysis\n\nIn cases where the analysis or processing required is straightforward and doesn’t benefit from chunking\n\nSingle-topic documents\n\nWhen a document is focused on a single topic and chunking doesn’t add value to the processing\n\n## Poor Chunking Example\n\nWhen text is not chunked correctly, it can lead to reduced LLM performance. Consider the following paragraph from a news article:\n\n```", "```py\n\nWhen the text is fragmented into isolated words, the resulting list lacks the original context:\n\n```", "```py\n\nThe main issues with this poor chunking example include:\n\nLoss of context\n\nBy splitting the text into individual words, the original meaning and relationships between the words are lost. This makes it difficult for AI models to understand and respond effectively.\n\nIncreased processing load\n\nProcessing individual words requires more computational resources, making it less efficient than processing larger chunks of text.\n\nAs a result of the poor chunking in this example, an LLM may face several challenges:\n\n*   Difficulty understanding the main ideas or themes of the text\n\n*   Struggling to generate accurate summaries or translations\n\n*   Inability to effectively perform tasks such as sentiment analysis or text `classification`\n\nBy understanding the pitfalls of poor chunking, you can apply prompt engineering principles to improve the process and achieve better results with AI language models.\n\nLet’s explore an improved chunking example using the same news article paragraph from the previous section; you’ll now chunk the text by sentence:\n\n```", "```py\n\n# Divide Labor and Evaluate Quality\n\nDefine the granularity at which the text should be chunked, such as by sentence, paragraph, or topic. Adjust parameters like the number of tokens or model temperature to optimize the chunking process.\n\nBy chunking the text in this manner, you could insert whole sentences into an LLM prompt with the most relevant sentences.\n\n# Chunking Strategies\n\nThere are many different chunking strategies, including:\n\nSplitting by sentence\n\nPreserves the context and structure of the original content, making it easier for LLMs to understand and process the information. Sentence-based chunking is particularly useful for tasks like summarization, translation, and sentiment analysis.\n\nSplitting by paragraph\n\nThis approach is especially effective when dealing with longer content, as it allows the LLM to focus on one cohesive unit at a time. Paragraph-based chunking is ideal for applications like document analysis, topic modeling, and information extraction.\n\nSplitting by topic or section\n\nThis method can help AI models better identify and understand the main themes and ideas within the content. Topic-based chunking is well suited for tasks like text classification, content recommendations, and clustering.\n\nSplitting by complexity\n\nFor certain applications, it might be helpful to split text based on its complexity, such as the reading level or technicality of the content. By grouping similar complexity levels together, LLMs can more effectively process and analyze the text. This approach is useful for tasks like readability analysis, content adaptation, and personalized learning.\n\nSplitting by length\n\nThis technique is particularly helpful when working with very long or complex documents, as it allows LLMs to process the content more efficiently. Length-based chunking is suitable for applications like large-scale text analysis, search engine indexing, and text preprocessing.\n\nSplitting by tokens using a tokenizer\n\nUtilizing a tokenizer is a crucial step in many natural language processing tasks, as it enables the process of splitting text into individual tokens. Tokenizers divide text into smaller units, such as words, phrases, or symbols, which can then be analyzed and processed by AI models more effectively. You’ll shortly be using a package called `tiktoken`, which is a bytes-pair encoding tokenizer (BPE) for chunking.\n\n[Table 3-1](#table-3-1) provides a high-level overview of the different chunking strategies; it’s worth considering what matters to you most when performing chunking.\n\nAre you more interested in preserving semantic context, or would naively splitting by length suffice?\n\nTable 3-1\\. Six chunking strategies highlighting their advantages and disadvantages\n\n| Splitting strategy | Advantages | Disadvantages |\n| --- | --- | --- |\n| Splitting by sentence | Preserves context, suitable for various tasks | May not be efficient for very long content |\n| Splitting by paragraph | Handles longer content, focuses on cohesive units | Less granularity, may miss subtle connections |\n| Splitting by topic | Identifies main themes, better for classification | Requires topic identification, may miss fine details |\n| Splitting by complexity | Groups similar complexity levels, adaptive | Requires complexity measurement, not suitable for all tasks |\n| Splitting by length | Manages very long content, efficient processing | Loss of context, may require more preprocessing steps |\n| Using a tokenizer: Splitting by tokens | Accurate token counts, which helps in avoiding LLM prompt token limits | Requires tokenization, may increase computational complexity |\n\nBy choosing the appropriate chunking strategy for your specific use case, you can optimize the performance and accuracy of AI language models.\n\n# Sentence Detection Using SpaCy\n\n*Sentence detection*, also known as sentence boundary disambiguation, is the process used in NLP that involves identifying the start and end of sentences within a given text. It can be particularly useful for tasks that require preserving the context and structure of the original content. By splitting the text into sentences, LLMs can better understand and process the information for tasks such as summarization, translation, and sentiment analysis.\n\nSplitting by sentence is possible using NLP libraries such as [spaCy](https://spacy.io). Ensure that you have spaCy installed in your Python environment. You can install it with `pip install spacy`. Download the `en_core_web_sm` model using the command `python -m spacy download en_core_web_sm`.\n\nIn [Example 3-3](#ex-3-3), the code demonstrates sentence detection using the spaCy library in Python.\n\n##### Example 3-3\\. [Sentence detection with spaCy](https://oreil.ly/GKDnc)\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nFirst, you’ll import the spaCy library and load the English model `(en_core_web_sm)` to initialize an `nlp` object. Define an input text with two sentences; the text is then processed with `doc = nlp(text)`, creating a `doc` object as a result. Finally, the code iterates through the detected sentences using the `doc.sents` attribute and prints each sentence.\n\n# Building a Simple Chunking Algorithm in Python\n\nAfter exploring many chunking strategies, it’s important to build your intuition by writing a simple chunking algorithm from scatch.\n\n[Example 3-4](#ex-3-4) shows how to chunk text based on the length of characters from the blog post “Hubspot - What Is Digital Marketing?” This file can be found in the Github repository at *[content/chapter_3/hubspot_blog_post.txt](https://oreil.ly/30rlQ)*.\n\nTo correctly read the *hubspot_blog_post.txt* file, make sure your current working directory is set to the [*content/chapter_3*](https://oreil.ly/OHurh) GitHub directory. This applies for both running the Python code or launching the Jupyter Notebook server.\n\n##### Example 3-4\\. [Character chunking](https://oreil.ly/n3sNy)\n\n```", "```py\n\nOutput:\n\n```", "```py\n\nFirst, you open the text file *hubspot_blog_post.txt* with the `open` function and read its contents into the variable text. Then using a list comprehension you create a list of chunks, where each `chunk` is a 200 character substring of text.\n\nThen you use the `range` function to generate indices for each 200 character substring, and the `i:i+200` slice notation to extract the substring from text.\n\nFinally, you loop through each chunk in the `chunks` list and `print` it to the console.\n\nAs you can see, because the chunking implementation is relatively simple and only based on length, there are gaps within the sentences and even words.\n\nFor these reasons we believe that good NLP chunking has the following properties:\n\n*   Preserves entire words, ideally sentences and contextual points made by speakers\n\n*   Handles for when sentences span across several pages, for example, page 1 into page 2\n\n*   Provides an adequate token count for each `chunk` so that the total number of input tokens will appropriately fit into a given token context window for any LLM\n\n# Sliding Window Chunking\n\n*Sliding window chunking* is a technique used for dividing text data into overlapping chunks, or *windows*, based on a specified number of characters, tokens, or words.\n\nBut what exactly is a sliding window?\n\nImagine viewing a long piece of text through a small window. This window is only capable of displaying a fixed number of items at a time. As you slide this window from the beginning to the end of the text, you see *overlapping chunks of text*. This mechanism forms the essence of the sliding window approach.\n\nEach window size is defined by a *fixed number of characters, tokens, or words*, and the *step size* determines how far the window moves with each slide.\n\nIn [Figure 3-6](#figure-3-6), with a window size of 4 words and a step size of 1, the first chunk would contain the first 4 words of the text. The window then slides 1 word to the right to create the second chunk, which contains words 2 through 5.\n\nThis process repeats until the end of the text is reached, ensuring each chunk overlaps with the previous and next ones to retain some shared context.\n\n![pega 0306](assets/pega_0306.png)\n\n###### Figure 3-6\\. A sliding window, with a window size of 4 and a step size of 1\n\nDue to the step size being 1, there is a lot of duplicate information between chunks, and at the same time the risk of losing information between chunks is dramatically reduced.\n\nThis is in stark contrast to [Figure 3-7](#figure-3-7), which has a window size of 4 words and a step size of 2\\. You’ll notice that because of the 100% increase in step size, the amount of information shared between the chunks is greatly reduced.\n\n![pega 0307](assets/pega_0307.png)\n\n###### Figure 3-7\\. A sliding window, with a window size of 4 and a step size of 2\n\nYou will likely need a larger overlap if accuracy and preserving semanatic context are more important than minimizing token inputs or the number of requests made to an LLM.\n\n[Example 3-5](#ex-3-5) shows how you can implement a sliding window using Python’s `len()` function. The `len()` function provides us with the total number of characters rather than words in a given text string, which subsequently aids in defining the parameters of our sliding windows.\n\n##### Example 3-5\\. [Sliding window](https://oreil.ly/aCkDo)\n\n```", "```py\n\nThis code outputs:\n\n```", "```py\n\nIn the context of prompt engineering, the sliding window approach offers several benefits over fixed chunking methods. It allows LLMs to retain a higher degree of context, as there is an overlap between the chunks and offers an alternative approach to preserving context compared to sentence detection.\n\n# Text Chunking Packages\n\nWhen working with LLMs such as GPT-4, always remain wary of the maximum context length:\n\n*   `maximum_context_length = input_tokens + output_tokens`\n\nThere are various tokenizers available to break your text down into manageable units, the most popular ones being NLTK, spaCy, and tiktoken.\n\nBoth [NLTK](https://oreil.ly/wTmI7) and [spaCy](https://oreil.ly/c4MvQ) provide comprehensive support for text processing, but you’ll be focusing on tiktoken.\n\n# Text Chunking with Tiktoken\n\n[Tiktoken](https://oreil.ly/oSpVe) is a fast *byte pair encoding (BPE)* tokenizer that breaks down text into subword units and is designed for use with OpenAI’s models. Tiktoken offers faster performance than comparable open source tokenizers.\n\nAs a developer working with GPT-4 applications, using tiktoken offers you several key advantages:\n\nAccurate token breakdown\n\nIt’s crucial to divide text into tokens because GPT models interpret text as individual tokens. Identifying the number of tokens in your text helps you figure out whether the text is too lengthy for a model to process.\n\nEffective resource utilization\n\nHaving the correct token count enables you to manage resources efficiently, particularly when using the OpenAI API. Being aware of the exact number of tokens helps you regulate and optimize API usage, maintaining a balance between costs and resource usage.\n\n# Encodings\n\nEncodings define the method of converting text into tokens, with different models utilizing different encodings. Tiktoken supports three encodings commonly used by OpenAI models:\n\n| Encoding name | OpenAI models |\n| --- | --- |\n| cl100k_base | GPT-4, GPT-3.5-turbo, text-embedding-ada-002 |\n| p50k_base | Codex models, text-davinci-002, text-davinci-003 |\n| r50k_base (or gpt2) | GPT-3 models like davinci |\n\n## Understanding the Tokenization of Strings\n\nIn English, tokens can vary in length, ranging from a single character like *t*, to an entire word such as *great*. This is due to the adaptable nature of tokenization, which can accommodate even tokens shorter than a character in complex script languages or tokens longer than a word in languages without spaces or where phrases function as single units.\n\nIt is not uncommon for spaces to be included within tokens, such as `\"is\"` rather than `\"is \"` or `\" \"+\"is\"`. This practice helps maintain the original text formatting and can capture specific linguistic characteristics.\n\n###### Note\n\nTo easily examine the tokenization of a string, you can use [OpenAI Tokenizer](https://oreil.ly/K6ZQK).\n\nYou can install [tiktoken from PyPI](https://oreil.ly/HA2QD) with `pip install` `tiktoken`. In the following example, you’ll see how to easily encode text into tokens and decode tokens into text:\n\n```", "```py\n\nAdditionally let’s write a function that will tokenize the text and then count the number of tokens given a `text_string` and `encoding_name`.\n\n```", "```py\n\nThis code outputs `8`.\n\n# Estimating Token Usage for Chat API Calls\n\nChatGPT models, such as GPT-3.5-turbo and GPT-4, utilize tokens similarly to previous completion models. However, the message-based structure makes token counting for conversations more challenging:\n\n```", "```py\n\n[Example 3-6](#ex-3-6) highlights the specific structure required to make a request against any of the chat models, which are currently GPT-3x and GPT-4.\n\nNormally, chat history is structured with a `system` message first, and then succeeded by alternating exchanges between the `user` and the `assistant`.\n\n##### Example 3-6\\. A payload for the Chat Completions API on OpenAI\n\n```", "```py\n\n `` `\"role\": \"system\"` describes a system message that’s useful for *providing prompt instructions*. It offers a means to tweak the assistant’s character or provide explicit directives regarding its interactive approach. It’s crucial to understand, though, that the system command isn’t a prerequisite, and the model’s default demeanor without a system command could closely resemble the behavior of “You are a helpful assistant.”    The roles that you can have are `[\"system\", \"user\", \"assistant\"]`.    `\"content\": \"Some content\"` is where you place the prompt or responses from a language model, depending upon the message’s role. It can be either `\"assistant\"`, `\"system\"`, or `\"user\"`. ``  ```", "```py Is this text positive or negative?  I absolutely love the design of this phone, but the battery life is quite disappointing. ```", "```py The text has a mixed tone, as it contains both positive and negative aspects. The positive part is \"I absolutely love the design of this phone,\" while the negative part is \"the battery life is quite disappointing.\" ```", "```py Using the following examples as a guide: positive: 'I absolutely love the design of this phone!' negative: 'The battery life is quite disappointing.' neutral: 'I liked the product, but it has short battery life.'  Only return either a single word of: - positive - negative - neutral  Please classify the sentiment of the following text as positive, negative, or neutral: I absolutely love the design of this phone, but the battery life is quite disappointing. ```", "```py neutral ```", "```py Outline the basic architecture for a simple Flask \"Hello World\" application. ```", "```py Sure! Here's an outline for a basic Flask \"Hello World\" application:  Import Flask module: Import the Flask module to use its (((\"Flask module code generation\")))functionality.  Create an instance of the Flask application: Create an instance of the Flask class to represent your web application. Define a route and view function: ... Implement the view function: ... Run the application: Start the Flask development server to run the application. ```", "```py Provide a code snippet for a simple \"Hello World\" route in Flask. ```", "```py Certainly! Here's a simple code snippet for a \"Hello World\" route using Flask:  from flask import Flask  app = Flask(__name__)  @app.route('/') def hello_world():     return 'Hello, World!'  if __name__ == '__main__':     app.run() ```", "```py Provide an example of a test case for the \"Hello World\" route in a Flask application. ```", "```py Sure! Here's an example of a test case for the \"Hello World\" route in a Flask application:  import unittest from flask import Flask from flask.testing import FlaskClient  # Create the Flask application app = Flask(__name__)  # Create the route @app.route('/') def hello_world():     return 'Hello, World!'  class FlaskAppTestCase(unittest.TestCase):     def setUp(self):         # Create a test client         self.app = app.test_client()      def test_hello_world_route(self):         # Send a GET request to the route         response = self.app.get('/')          # Check the response status code         self.assertEqual(response.status_code, 200)          # Check the response data         self.assertEqual(response.data, b'Hello, World!')  if __name__ == '__main__':     unittest.main() ```", "```py I want you to act as a tech reviewer. I will give you the name of a piece of technology and you will provide me with an in-depth review - including pros, cons, features, and comparisons to other technologies on the market. My first suggestion request is: MongoDB. ```", "```py MongoDB is a popular NoSQL, document-oriented database management system designed to handle high-volume data storage and retrieval.  Features:  1\\. Document-Oriented Storage: MongoDB stores data in a flexible, JSON-like format called BSON, making it easy to store complex data structures like arrays and nested objects. ...  Pros: 1\\. High Performance: MongoDB's indexing and caching capabilities deliver fast read and write operations. ...  Cons: 1\\. Limited Support for Transactions: While MongoDB supports multidocument ACID transactions, it may not be the best choice for applications that require complex transactional operations. ...   Comparisons to other technologies: 1\\. Couchbase, another popular NoSQL database, offers similar features to MongoDB, including flexible schema and JSON-based storage. However, Couchbase is more focused on distributed caching and may be more suitable for applications requiring low-latency data access.  Conclusion:  MongoDB is a powerful and versatile NoSQL database solution suitable for various applications, particularly those requiring high performance and scalability. Its flexible data model, rich query language, and strong consistency make it a popular choice among developers. ```", "```py Refer to the articles enclosed within triple quotes to respond to queries.  You must follow the following principles: - In cases where the answer isn't found within these articles, simply return \"I could not find an answer\".  \"\"\" B2B clients tend to have longer decision-making processes, and thus longer sales funnels. Relationship-building strategies work better for these clients, whereas B2C customers tend to respond better to short-term offers and messages. \"\"\"  Example responses: - I could not find an answer. - Yes, B2B clients tend to have longer decision-making processes, and thus longer sales funnels. ```", "```py Yes, B2B clients tend to have longer decision-making processes, which leads to longer sales cycles. ```", "```py ...The rest of the prompt...  Question: Are B2C sales more cost-effective? ```", "```py I could not find an answer. ```", "```py You will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \"Insufficient information.\" If an answer to the question is provided, it must be annotated with a citation. Use the following format to cite relevant passages ({\"citation\": …}).  \"\"\" A nuclear reactor is a device that initiates, controls, and maintains a nuclear chain reaction. The nuclear reactor uses the energy released by nuclear fission to generate heat, which is then often used to produce steam to drive a turbine and produce electricity.  -Excerpted from https://en.wikipedia.org/wiki/Nuclear_reactor \"\"\"  Question: What is a nuclear reactor? ```", "```py A nuclear reactor is a device that initiates, controls, and maintains a nuclear chain reaction.It uses the energy released by nuclear fission to generate heat, which is then often used to produce steam to drive a turbine and produce electricity. {\"citation\": \"A nuclear reactor is a device that initiates, controls, and maintains a nuclear chain reaction. The nuclear reactor uses the energy released by nuclear fission to generate heat, which is then often used to produce steam to drive a turbine and produce electricity.\"} ```", "```py First, try to solve the problem on your own. Then, see if your answer matches the proposed solution. Don't judge the given answer until you've tried solving it yourself.  Problem: Jane has 5 apples, then John takes 2 from Mary. Mary originally had 15 apples. John then gives his apples to Jane.  How many apples does Jane have?  Proposed solution: 1\\. Janes originally had 5 apples. 2\\. John takes 2 apples from Mary, but this doesn't affect Jane's count. 3\\. John then gives his apples to Jane.  So, Jane's final count of apples is her original 5 apples plus the 2 apples that John gave her, for a total of 7 apples. ```", "```py Let's break down the problem:  1\\. Jane originally has 5 apples. 2\\. John takes 2 apples from Mary. However, this action doesn't directly impact the number of apples Jane has at this point. 3\\. John gives his apples to Jane.  Following these steps, we need to add the 2 apples John gave to Jane to Jane's original number of apples. Therefore:  5 apples (Jane's original number) + 2 apples (given by John) = 7 apples  So, Jane now has 7 apples. My solution matches the proposed solution. ```", "```py Step 1: Solve the Problem  First, you need to work out your solution to the problem. Be sure not to rely on the user's solution, as it might be incorrect. You can encapsulate your workings within triple quotes (`\"\"\"`) to indicate that this section should be hidden.  Step 2: Compare Solutions  Next, compare your solution with the user's solution. Evaluate whether the user's solution is correct or not. Again, encapsulate this section within triple quotes (`\"\"\"`).  Step 3: Prepare a Hint  If the user made a mistake, determine an appropriate hint that can help guide them toward the correct answer, without giving it away outright. This section should also be within triple quotes (`\"\"\"`).  Step 4: Deliver the Hint  If a mistake was made, present the hint from the previous step to the user. Remember not to use triple quotes for this step. Instead of using a title like \"Step 4\", use a heading such as `Hint:`    Problem Statement: A deck of cards has 52 cards: 13 hearts, 13 diamonds, 13 clubs, and 13 spades. If two cards are drawn from the deck without replacement, what is the probability that one of the drawn cards is a heart and the other one is a club?  User's Solution: (13/52)*(13/51) = 169/2652 = 13/204 ```", "```py \"\"\" Step 1 - My Solution:  First, calculate the probability of drawing 1 heart and 1 club from a deck of 52 cards without replacement.  In the first draw, the probability of drawing a heart is 13/52.  For the second draw, after removing 1 card (assuming it's a heart), the total number of cards becomes 51\\. So, the probability of drawing a club now is 13/51.  Multiplying these two probabilities gives the probability of both events happening in sequence,which is P(Heart then Club) = (13/52)*(13/51).  Repeat the calculation assuming the first drawn card is a club and the second drawn card is a heart. Remember, order matters in this case as we're dealing with separate events. \"\"\" \"\"\" Step 2 - Comparison with the User's Solution:  Compare your solution with the user's solution. The user's solution only considers the sequence of drawing a heart first and then a club. They have not accounted for the alternate sequence  (Club then Heart), and thus, the solution is incomplete. \"\"\" \"\"\" Step 3 - Preparation of Hint:  Given that the user hasn't considered both sequences (Heart then Club and Club then Heart), a hint at the consideration of both possible sequences will be helpful. This could nudge the user in the right direction without revealing the solution outright. \"\"\" # Step 4 - Delivery of Hint: # Hint: Remember that the two cards could be drawn in any order. Consider both possible sequences: Heart then Club and Club then Heart. ```", "```py Generate a \"Hello World\" Python function. ```", "```py Sure, here's a very simple Python function that prints \"Hello, World!\" to the console:  def print_hello_world():     print(\"Hello, World!\")  print_hello_world()  When you run this function, it will print the string \"Hello, World!\" to the console. ```", "```py ...Previous Chat History...  Given the previous function, can you improve it again with anything else? Perhaps defaults or type hints? ```", "```py We can add type hints to make it clear that the function doesn't take any arguments and doesn't return anything. We can also add a docstring to describe what the function does.  Here's the improved code:  def print_hello_world() -> None:     \"\"\"     Prints \"Hello, World!\" to the standard output.     \"\"\"     print(\"Hello, World!\")  If you wanted to make the function more flexible, you could add a default argument that allows you to print a different message:  def print_message(message: str = \"Hello, World!\") -> None:     \"\"\"     Prints the given message to the standard output. If no message is given     , it defaults to \"Hello, World!\".      Args:         message (str, optional): The message to be printed.         Defaults to \"Hello, World!\".     \"\"\"     print(message) ```", "```py Given the statement, classify it as either \"Compliment\", \"Complaint\", or \"Neutral\": 1\\. \"The sun is shining.\" - Neutral 2\\. \"Your support team is fantastic!\" - Compliment 3\\. \"I had a terrible experience with your software.\" - Complaint  You must follow the following principles: - Only return the single classification word. The response should be either \"Compliment\", \"Complaint\", or \"Neutral\". - Perform the classification on the text enclosed within \"\"\" delimiters.  \"\"\"The user interface is intuitive.\"\"\"  Classification: ```", "```py Compliment ```", "```py from openai import OpenAI import os  client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))  base_template = \"\"\" Given the statement, classify it as either \"Compliment\", \"Complaint\", or \"Neutral\": 1\\. \"The sun is shining.\" - Neutral 2\\. \"Your support team is fantastic!\" - Compliment 3\\. \"I had a terrible experience with your software.\" - Complaint  You must follow the following principles: - Only return the single classification word. The response should be either \"Compliment\", \"Complaint\", or \"Neutral\". - Perform the classification on the text enclosed within ''' delimiters.  '''{content}'''  Classification: \"\"\"  responses = []  for i in range(0, 3):     response = client.chat.completions.create(         model=\"gpt-4\",         messages=[{\"role\": \"system\",             \"content\": base_template.format(content='''Outside is rainy,  but I am having a great day, I just don't understand how people  live, I'm so sad!'''),}],)     responses.append(response.choices[0].message.content.strip())  def most_frequent_classification(responses):     # Use a dictionary to count occurrences of each classification     count_dict = {}     for classification in responses:         count_dict[classification] = count_dict.get(classification, 0) + 1      # Return the classification with the maximum count     return max(count_dict, key=count_dict.get)  print(most_frequent_classification(responses))  # Expected Output: Neutral ```", "```py from openai import OpenAI import os  client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))  responses = []  for i in range(10):     # concise if even, verbose if odd     style = \"concise\" if i % 2 == 0 else \"verbose\"      if style == \"concise\":         prompt = f\"\"\"Return a {style} answer to the  following question: What is the meaning of life?\"\"\"     else:         prompt = f\"\"\"Return an answer to the following  question: What is the meaning of life?\"\"\"      response = client.chat.completions.create(         # using GPT-3.5 Turbo for this example         model=\"gpt-3.5-turbo\",         messages=[{\"role\": \"user\",             \"content\": prompt}])     responses.append(         response.choices[0].message.content.strip())  system_prompt = \"\"\"You are assessing the conciseness of a response from a chatbot. You only respond with a 1 if the response is concise, and a 0 if it is not. \"\"\"  ratings = []  for idx, response in enumerate(responses):     rating = client.chat.completions.create(         model=\"gpt-4\",         messages=[{\"role\": \"system\",             \"content\": system_prompt},             {\"role\": \"system\",             \"content\": response}])     ratings.append(         rating.choices[0].message.content.strip())  for idx, rating in enumerate(ratings):     style = \"concise\" if idx % 2 == 0 else \"verbose\"     print(f\"Style: {style}, \", f\"Rating: {rating}\") ```", "```py Style: concise,  Rating: 1 Style: verbose,  Rating: 0 Style: concise,  Rating: 1 Style: verbose,  Rating: 0 Style: concise,  Rating: 1 Style: verbose,  Rating: 0 Style: concise,  Rating: 1 Style: verbose,  Rating: 0 Style: concise,  Rating: 1 Style: verbose,  Rating: 0 ```", "```py Act as a GPT Prompt Engineer, you are solely responsible for producing highly effective large language model text prompts given a task.  You must follow the following principles: - Return only a single prompt, never return the output of a prompt. - You must follow the task properly. - You must either refine the prompt or create a new prompt depending upon the Task. - Bullet point out any important features so that another Prompt Engineer can easily copy inputs into the prompt.  Task: You must create a copywriting guide ChatGPT several blog posts. Include relevant features within the ChatGPT prompt that will allow ChatGPT to imitate the writer. You must decide what the most important features are to mimic the writer's style etc. You will only be applying this style to new blog posts.  Blog post text: You need a variety of soft skills in addition to technical skills to succeed in the technology sector. Soft skills are used by software professionals to collaborate with their peers... ```", "```py Prompt:  Based on the extensive text provided, your task is to create a series of blog posts following the same style and structure of the writer. As a guide you should follow these features:  1\\. Clear, direct language: Your writing should be in clear and direct language, breaking down complex subjects into easily understandable concepts. 2\\. Use of subheadings: To provide structure to your blog posts, break them up with relevant subheadings. This helps to guide the reader and make your text more readable. ... 9\\. Inclusion of technical terminology: When discussing technical topics, use appropriate technical language to ensure accuracy and credibility. However, be sure to provide explanations or definitions for more complex terms. ```", "```py Given the following style guide and these two blog posts, I want you to improve the copywriting style prompt so an LLM or copywriter can write effectively for my client.  ...Previously Generated Copywriting guide...  URL 1: https://understandingdata.com/posts/how-to-clean-and-process-data URL 2: https://understandingdata.com/posts/how-to-scrape-twitter-data ```", "```py Based on the style guide and the two blog posts, here's an improved copywriting style prompt:  To create a blog post for the client, adhere to the following guidelines:  1\\. Clarity and Directness: Use clear, direct language to explain complex topics. Present information in an easy-to-understand manner. Aim for your audience to grasp concepts without needing prior knowledge. ... 8\\. Use of Technical Terminology: Use the appropriate technical language when discussing complex topics. However, make sure to provide definitions or explanations for these terms to keep the text accessible.  Model your writing after these blog post examples:  \"What is an API? And How Do They Relate to Data Engineering?\": This blog post uses clear and direct language to explain the concept of an API. It provides detailed explanations and uses practical examples of commonly used APIs. It links the technical concept of APIs to business growth and productivity. The information is well-categorized using subheadings like 'Types of API’s.'  \"How To Easily Setup A Google Cloud Project With APIs\": This blog post is a practical guide on setting up a Google Cloud Project. It's broken down into numbered steps, each with a clear subheading. The post uses detailed explanations and includes practical examples such as screenshots. It also uses clear, direct language to guide the reader through the process. ```"]