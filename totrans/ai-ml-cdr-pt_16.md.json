["```py\nimport torch\nimport torch.nn as nn\n\n# Simplified example with small dimensions\nd_model = 2  # Input/output dimension\nd_ff = 4     # Hidden dimension\n\n# Create some sample input\nx = torch.tensor([[–1.0, 2.0]])\n\n# First linear layer (2 → 4)\nW1 = torch.tensor([\n    [1.0, –1.0],\n    [–1.0, 1.0],\n    [0.5, 0.5],\n    [–0.5, –0.5]\n])\nb1 = torch.tensor([0.0, 0.0, 0.0, 0.0])\nlayer1_out = torch.matmul(x, W1.t()) + b1\nprint(\"After first linear layer:\", layer1_out)\n\n# Apply ReLU\nrelu_out = torch.relu(layer1_out)\nprint(\"After ReLU:\", relu_out)\n# Notice how negative values became 0; this is the nonlinear operation!\n\n# Second linear layer (4 → 2)\nW2 = torch.tensor([\n    [1.0, –1.0, 0.5, –0.5],\n    [–1.0, 1.0, 0.5, –0.5]\n])\nb2 = torch.tensor([0.0, 0.0])\nfinal_out = torch.matmul(relu_out, W2.t()) + b2\nprint(\"Final output:\", final_out)\n\n```", "```py\nW1 = torch.tensor([\n    [1.0, –1.0],\n    [–1.0, 1.0],\n    [0.5, 0.5],\n    [–0.5, –0.5]\n])\nb1 = torch.tensor([0.0, 0.0, 0.0, 0.0])\n\n```", "```py\nAfter first layer: [–3.0, 3.0, 0.5, –0.5]\n```", "```py\nOutput: [0.0, 3.0, 0.5, 0.0]\n```", "```py\nW2 = torch.tensor([\n    [1.0, –1.0, 0.5, –0.5],\n    [–1.0, 1.0, 0.5, –0.5]\n])\nb2 = torch.tensor([0.0, 0.0])\n```", "```py\nFinal output: [–2.75, 3.25]\n```", "```py\nInput: [–1.0, 2.0] → Output: [–2.75, 3.25]\nInput: [–2.0, 4.0] → Output: [–5.5, 6.5]    # Not a simple doubling!\nInput: [1.0, –2.0] → Output: [2.75, –3.25]  # Not a simple negation!\n\n```", "```py\nimport torch\n\n# Create sample feature values\nfeatures = torch.tensor([5.0, 1.0, 0.1])\nprint(\"\\nOriginal features:\", features)\nprint(\"Original mean:\", features.mean().item())\nprint(\"Original std:\", features.std().item())\n\n# Standard normalization\nmean = features.mean()\nstd = features.std()\nnormalized = (features - mean) / std\nprint(\"\\nJust normalized:\", normalized)\nprint(\"Normalized mean:\", normalized.mean().item())\nprint(\"Normalized std:\", normalized.std().item())\n\n# With learnable scale and shift\ngamma = torch.tensor([2.0, 0.5, 1.0])  # Learned parameters\nbeta = torch.tensor([1.0, 0.0, —1.0])  # Learned parameters\nscaled_shifted = gamma * normalized + beta\nprint(\"\\nAfter scale and shift:\", scaled_shifted)\nprint(\"Final mean:\", scaled_shifted.mean().item())\nprint(\"Final std:\", scaled_shifted.std().item())\n```", "```py\nOriginal features: tensor([5.0000, 1.0000, 0.1000])\nOriginal mean: 2.0333333015441895\nOriginal std: 2.6083199977874756\n\nJust normalized: tensor([ 1.1374, —0.3962, —0.7412])\nNormalized mean: 0.0\nNormalized std: 1.0\n\nAfter scale and shift: tensor([ 3.2748, —0.1981, —1.7412])\nFinal mean: 0.44515666365623474\nFinal std: 2.5691161155700684\n```", "```py\n[“If”, “you”, “are”, “happy”, “and”, “you”, “know”, “it”]\n```", "```py\n[“If”, “you”, “are”, “happy”, “and”, “you”, “know”, “it”, “clap”]\n```", "```py\n\"it\" -> [0.2, –0.5, 0.7] (position 7)\n\"clap\" -> [–0.3, 0.4, 0.1] (position 8)\n\n```", "```py\nPosition 7 -> [sin(7), cos(7), sin(7)] = [.122, .992, .122]\nPosition 8 -> [sin(8), cos(8), sin(8)] = [.139, .990, .139]\n```", "```py\n“It” -> [0.2+.122, —0.5+.992, 0.7+.122] = [0.322, 0.492, 0.822]\n“Clap” -> [—0.3+.139, 0.4+.990, 0.1+.139] = [—0.161, 1.390, 0.239]\n```", "```py\n# For Even-numbered dimensions\nPE(pos, d) = sin(pos / 10000^(d/d_model))\n\n# For Odd-numbered dimensions\nPE(pos, d) = cos(pos / 10000^(d-1/d_model))\n```", "```py\n1 0 0\n1 1 0\n1 1 1\n```", "```py\nOriginal input (word \"cat\" embedding):\n[0.5, —0.3, 0.7, 0.1]  # Contains basic info about \"cat\"\n\nAttention output (learned changes):\n[0.2, 0.1, —0.1, 0.3]  # Contains contextual updates based on other words\n\nAfter adding (final result):\n[0.7, —0.2, 0.6, 0.4]  # Original meaning of \"cat\" PLUS contextual information\n```", "```py\npip install transformers\npip install datasets  # for working with HuggingFace datasets\npip install tokenizers  # for fast tokenization\n```", "```py\nfrom huggingface_hub import login\nlogin(token=\"your_token_here\")\n```", "```py\nexport HUGGINGFACE_TOKEN=\"your_token_here\"\n```", "```py\nfrom transformers import pipeline\n\n# Basic sentiment analysis\nclassifier = pipeline(\"sentiment-analysis\")\nresult = classifier(\"I love working with transformers!\")\n```", "```py\ngenerator = pipeline(\"text-generation\")\ntext = generator(\"The future of AI is\")\nprint(text)\n```", "```py\n# Text generation with specific parameters\ngenerator = pipeline(\n    \"text-generation\",\n    model=\"gpt2\",\n    max_length=50,\n    temperature=0.7,\n    top_k=50\n)\ntext = generator(\"The future of AI is\")\n```", "```py\n              # Let's use a sentence with some interesting words to tokenize\ntext = \"The ultramarathoner prequalified for the \n        `immunohistochemistry` `conference` `in` `neuroscience``.``\"`\n```", "```py`` ```", "```py from transformers import AutoTokenizer   # Load BERT tokenizer which uses WordPiece tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') ```", "```py # Tokenize the text tokens = tokenizer.tokenize(text) print(\"Tokens:\", tokens) ```", "```py Tokens: ['the', 'ultra', '##mara', '##th', '##one', '##r', 'pre',           '##qual', '##ified', 'for', 'the', 'im', '##mun', '##oh', '##isto',           '##chemist', '##ry', 'conference', 'in', 'neuroscience', '.'] ```", "```py # Get the token IDs token_ids = tokenizer.encode(text) print(\"\\nToken IDs:\", token_ids) ```", "```py Token IDs: [101, 1996, 14357, 2108, 2339, 3840, 2837, 13462, 2005, 1996, 19763,              2172, 3075, 7903, 5273, 13172, 1027, 2005, 23021, 1012, 102]  ```", "```py # Decode back to show special tokens decoded = tokenizer.decode(token_ids) print(\"\\nDecoded with special tokens:\", decoded) ```", "```py Decoded with special tokens: [CLS] the ultramarathoner prequalified for the                               immunohistochemistry conference in                               neuroscience. [SEP] ```", "```py` ```", "```py```", "``` ```", "````` ### Byte-pair encoding    The GPT family uses a format called *byte-pair encoding* (BPE), which is a data compression algorithm as well as a tokenization one. It starts with the vocabulary of individual characters, which progressively learns common byte or character pairs to merge into new tokens.    The algorithm initially splits the training corpus into characters, assigning tokens to each. It then iteratively merges the most frequent pairs into new tokens, adding them to the vocabulary. The process continues for a predetermined number of merges. So, for example, over time, common patterns in words become their own tokens. This tends to veer toward the beginnings of the words with common prefixes (like *inter*) or the ends of words with common suffixes (like *er*) ending up having their own token. Instead of using the `##` string to determine the beginning of a subword, BPE uses a special character (usually *Ġ*).    Here’s the code you can use to tokenize the same sentence. You’ll use the `gpt2` AutoTokenizer from OpenAI:    ```py from transformers import AutoTokenizer   # Load GPT-2 tokenizer which uses BPE tokenizer = AutoTokenizer.from_pretrained('gpt2')   # Same sentence as before text = \"The ultramarathoner prequalified for the immunohistochemistry          `conference` `in` `neuroscience``.``\"` ``` `# Tokenize the text` `tokens` `=` `tokenizer``.``tokenize``(``text``)` `print``(``\"Tokens:\"``,` `tokens``)` ```py ```   ```py`` ````The output is shown here:    ```py Tokens: ['The', 'Ġult', 'ram', 'ar', 'athon', 'er', 'Ġpre', 'qualified', 'Ġfor',           'Ġthe', 'Ġimmun', 'oh', 'ist', 'ochemistry', 'Ġconference',           'Ġin', 'Ġneuroscience', '.'] ```    Over time, you’ll see that the splits will be slightly different, which reflects the difference between the training sets. BERT was trained on Wikipedia and the Toronto BookCorpus, while GPT-2 was trained on web text.```py` `````", "```py```", "```py```", "``` from transformers import AutoTokenizer   # Load T5 tokenizer which uses SentencePiece tokenizer = AutoTokenizer.from_pretrained('t5-base')   # Same sentence as before text = \"The ultramarathoner prequalified for the immunohistochemistry          `conference` `in` `neuroscience``.``\"` ```", "``` ```", "````` ```py`This produces the following set of tokens:    ``` Tokens: ['▁The', '▁ultra', 'marathon', 'er', '▁pre', 'qualified', '▁for',           '▁the', '▁immun', 'oh', 'ist', 'ochemistry', '▁conference', '▁in',          '▁neuroscience', '.']  ```py    As mentioned, where it’s particularly powerful is with non-English languages. So, for example, consider this code:    ``` # Let's also try a multilingual example with mixed scripts text2 = \"Tokyo 東京 is beautiful! Preprocessing in 2024 costs $123.45\" tokens2 = tokenizer.tokenize(text2) print(\"\\nMultilingual example tokens:\", tokens2) ```py    It will output the following:    ``` Multilingual example tokens: ['▁Tokyo', '▁東', '京', '▁is', '▁beautiful', '!',                                '▁Pre', '-', 'processing', '▁in', '▁2024',                                '▁costs', '▁$', '123', '.', '45'] ```py    Note how the Japanese characters for Tokyo were split into multiple tokens and the numbers were kept whole (i.e., `123` and `45`).    Given that transformers were initially designed to improve machine translation, you can see that SentencePiece, which predates generative AI like GPT, was designed with internationalization in mind!```` ```py`` `````", "``````py` ``````", "``````py```", "``` `` `# Summary    In this chapter, we looked at Transformers, the architecture that underpins modern LLMs, and transformers, the library from Hugging Face that makes Transformers easy to use.    We explored how the original Transformer architecture revolutionized AI through its use of attention mechanisms, in which context vectors for words were amended based on learned details of where the sequence might be paying appropriate attention to other parts of the sequence. We also looked at encoders that excel at artificial understanding of text, decoders that can intelligently generate text, and encoder-decoders that bring the best of both for sequence-to-sequence models. We also double-clicked into their architecture to understand how the mechanisms such as attention, feedforward, normalization, and many other parts of the architecture work.    We then looked into transformers, which form the library from Hugging Face that makes downloading and instantiation of Transformer-based models (including the entire inference pipeline) very easy. There’s a whole lot more still in there, and hopefully, this gave you a good head start.    In the next chapter, you’re going to go a little further and explore how to adapt LLM models to your specific needs, taking custom data and using it to fine-tune or prompt tune models to make them work for your specific use cases. Get ready to turn theory into practice!` `` ```"]