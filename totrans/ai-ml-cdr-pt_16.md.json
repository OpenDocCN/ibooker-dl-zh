["```py\nimport torch\nimport torch.nn as nn\n\n# Simplified example with small dimensions\nd_model = 2  # Input/output dimension\nd_ff = 4     # Hidden dimension\n\n# Create some sample input\nx = torch.tensor([[–1.0, 2.0]])\n\n# First linear layer (2 → 4)\nW1 = torch.tensor([\n    [1.0, –1.0],\n    [–1.0, 1.0],\n    [0.5, 0.5],\n    [–0.5, –0.5]\n])\nb1 = torch.tensor([0.0, 0.0, 0.0, 0.0])\nlayer1_out = torch.matmul(x, W1.t()) + b1\nprint(\"After first linear layer:\", layer1_out)\n\n# Apply ReLU\nrelu_out = torch.relu(layer1_out)\nprint(\"After ReLU:\", relu_out)\n# Notice how negative values became 0; this is the nonlinear operation!\n\n# Second linear layer (4 → 2)\nW2 = torch.tensor([\n    [1.0, –1.0, 0.5, –0.5],\n    [–1.0, 1.0, 0.5, –0.5]\n])\nb2 = torch.tensor([0.0, 0.0])\nfinal_out = torch.matmul(relu_out, W2.t()) + b2\nprint(\"Final output:\", final_out)\n\n```", "```py\nW1 = torch.tensor([\n    [1.0, –1.0],\n    [–1.0, 1.0],\n    [0.5, 0.5],\n    [–0.5, –0.5]\n])\nb1 = torch.tensor([0.0, 0.0, 0.0, 0.0])\n\n```", "```py\nAfter first layer: [–3.0, 3.0, 0.5, –0.5]\n```", "```py\nOutput: [0.0, 3.0, 0.5, 0.0]\n```", "```py\nW2 = torch.tensor([\n    [1.0, –1.0, 0.5, –0.5],\n    [–1.0, 1.0, 0.5, –0.5]\n])\nb2 = torch.tensor([0.0, 0.0])\n```", "```py\nFinal output: [–2.75, 3.25]\n```", "```py\nInput: [–1.0, 2.0] → Output: [–2.75, 3.25]\nInput: [–2.0, 4.0] → Output: [–5.5, 6.5]    # Not a simple doubling!\nInput: [1.0, –2.0] → Output: [2.75, –3.25]  # Not a simple negation!\n\n```", "```py\nimport torch\n\n# Create sample feature values\nfeatures = torch.tensor([5.0, 1.0, 0.1])\nprint(\"\\nOriginal features:\", features)\nprint(\"Original mean:\", features.mean().item())\nprint(\"Original std:\", features.std().item())\n\n# Standard normalization\nmean = features.mean()\nstd = features.std()\nnormalized = (features - mean) / std\nprint(\"\\nJust normalized:\", normalized)\nprint(\"Normalized mean:\", normalized.mean().item())\nprint(\"Normalized std:\", normalized.std().item())\n\n# With learnable scale and shift\ngamma = torch.tensor([2.0, 0.5, 1.0])  # Learned parameters\nbeta = torch.tensor([1.0, 0.0, —1.0])  # Learned parameters\nscaled_shifted = gamma * normalized + beta\nprint(\"\\nAfter scale and shift:\", scaled_shifted)\nprint(\"Final mean:\", scaled_shifted.mean().item())\nprint(\"Final std:\", scaled_shifted.std().item())\n```", "```py\nOriginal features: tensor([5.0000, 1.0000, 0.1000])\nOriginal mean: 2.0333333015441895\nOriginal std: 2.6083199977874756\n\nJust normalized: tensor([ 1.1374, —0.3962, —0.7412])\nNormalized mean: 0.0\nNormalized std: 1.0\n\nAfter scale and shift: tensor([ 3.2748, —0.1981, —1.7412])\nFinal mean: 0.44515666365623474\nFinal std: 2.5691161155700684\n```", "```py\n[“If”, “you”, “are”, “happy”, “and”, “you”, “know”, “it”]\n```", "```py\n[“If”, “you”, “are”, “happy”, “and”, “you”, “know”, “it”, “clap”]\n```", "```py\n\"it\" -> [0.2, –0.5, 0.7] (position 7)\n\"clap\" -> [–0.3, 0.4, 0.1] (position 8)\n\n```", "```py\nPosition 7 -> [sin(7), cos(7), sin(7)] = [.122, .992, .122]\nPosition 8 -> [sin(8), cos(8), sin(8)] = [.139, .990, .139]\n```", "```py\n“It” -> [0.2+.122, —0.5+.992, 0.7+.122] = [0.322, 0.492, 0.822]\n“Clap” -> [—0.3+.139, 0.4+.990, 0.1+.139] = [—0.161, 1.390, 0.239]\n```", "```py\n# For Even-numbered dimensions\nPE(pos, d) = sin(pos / 10000^(d/d_model))\n\n# For Odd-numbered dimensions\nPE(pos, d) = cos(pos / 10000^(d-1/d_model))\n```", "```py\n1 0 0\n1 1 0\n1 1 1\n```", "```py\nOriginal input (word \"cat\" embedding):\n[0.5, —0.3, 0.7, 0.1]  # Contains basic info about \"cat\"\n\nAttention output (learned changes):\n[0.2, 0.1, —0.1, 0.3]  # Contains contextual updates based on other words\n\nAfter adding (final result):\n[0.7, —0.2, 0.6, 0.4]  # Original meaning of \"cat\" PLUS contextual information\n```", "```py\npip install transformers\npip install datasets  # for working with HuggingFace datasets\npip install tokenizers  # for fast tokenization\n```", "```py\nfrom huggingface_hub import login\nlogin(token=\"your_token_here\")\n```", "```py\nexport HUGGINGFACE_TOKEN=\"your_token_here\"\n```", "```py\nfrom transformers import pipeline\n\n# Basic sentiment analysis\nclassifier = pipeline(\"sentiment-analysis\")\nresult = classifier(\"I love working with transformers!\")\n```", "```py\ngenerator = pipeline(\"text-generation\")\ntext = generator(\"The future of AI is\")\nprint(text)\n```", "```py\n# Text generation with specific parameters\ngenerator = pipeline(\n    \"text-generation\",\n    model=\"gpt2\",\n    max_length=50,\n    temperature=0.7,\n    top_k=50\n)\ntext = generator(\"The future of AI is\")\n```", "```py\n              # Let's use a sentence with some interesting words to tokenize\ntext = \"The ultramarathoner prequalified for the \n        immunohistochemistry conference in neuroscience.\"\n```", "```py\nfrom transformers import AutoTokenizer\n\n# Load BERT tokenizer which uses WordPiece\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n```", "```py\n# Tokenize the text\ntokens = tokenizer.tokenize(text)\nprint(\"Tokens:\", tokens)\n```", "```py\nTokens: ['the', 'ultra', '##mara', '##th', '##one', '##r', 'pre', \n         '##qual', '##ified', 'for', 'the', 'im', '##mun', '##oh', '##isto', \n         '##chemist', '##ry', 'conference', 'in', 'neuroscience', '.']\n```", "```py\n# Get the token IDs\ntoken_ids = tokenizer.encode(text)\nprint(\"\\nToken IDs:\", token_ids)\n```", "```py\nToken IDs: [101, 1996, 14357, 2108, 2339, 3840, 2837, 13462, 2005, 1996, 19763, \n            2172, 3075, 7903, 5273, 13172, 1027, 2005, 23021, 1012, 102]\n\n```", "```py\n# Decode back to show special tokens\ndecoded = tokenizer.decode(token_ids)\nprint(\"\\nDecoded with special tokens:\", decoded)\n```", "```py\nDecoded with special tokens: [CLS] the ultramarathoner prequalified for the \n                             immunohistochemistry conference in \n                             neuroscience. [SEP]\n```", "```py\nfrom transformers import AutoTokenizer\n\n# Load GPT-2 tokenizer which uses BPE\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\n\n# Same sentence as before\ntext = \"The ultramarathoner prequalified for the immunohistochemistry \n        conference in neuroscience.\"\n\n# Tokenize the text\ntokens = tokenizer.tokenize(text)\nprint(\"Tokens:\", tokens)\n```", "```py\nTokens: ['The', 'Ġult', 'ram', 'ar', 'athon', 'er', 'Ġpre', 'qualified', 'Ġfor', \n         'Ġthe', 'Ġimmun', 'oh', 'ist', 'ochemistry', 'Ġconference', \n         'Ġin', 'Ġneuroscience', '.']\n```", "```py\nfrom transformers import AutoTokenizer\n\n# Load T5 tokenizer which uses SentencePiece\ntokenizer = AutoTokenizer.from_pretrained('t5-base')\n\n# Same sentence as before\ntext = \"The ultramarathoner prequalified for the immunohistochemistry \n        conference in neuroscience.\"\n\n# Tokenize the text\ntokens = tokenizer.tokenize(text)\nprint(\"Tokens:\", tokens)\n```", "```py\nTokens: ['▁The', '▁ultra', 'marathon', 'er', '▁pre', 'qualified', '▁for', \n         '▁the', '▁immun', 'oh', 'ist', 'ochemistry', '▁conference', '▁in',\n         '▁neuroscience', '.']\n\n```", "```py\n# Let's also try a multilingual example with mixed scripts\ntext2 = \"Tokyo 東京 is beautiful! Preprocessing in 2024 costs $123.45\"\ntokens2 = tokenizer.tokenize(text2)\nprint(\"\\nMultilingual example tokens:\", tokens2)\n```", "```py\nMultilingual example tokens: ['▁Tokyo', '▁東', '京', '▁is', '▁beautiful', '!', \n                              '▁Pre', '-', 'processing', '▁in', '▁2024', \n                              '▁costs', '▁$', '123', '.', '45']\n```"]