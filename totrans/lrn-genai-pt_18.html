<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">15 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/><a id="idTextAnchor004"/>Diffusion models and text-to-image Transformers</h1>
<p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="idIndexMarker001"/><a id="marker-341"/><a id="idIndexMarker002"/></p>
<ul class="calibre5">
<li class="co-summary-bullet">How forward diffusion and reverse diffusion work</li>
<li class="co-summary-bullet">How to build and train a denoising U-Net model</li>
<li class="co-summary-bullet">Using the trained U-Net to generate flower images</li>
<li class="co-summary-bullet">Concepts behind text-to-image Transformers</li>
<li class="co-summary-bullet">Writing a Python program to generate an image through text with DALL-E 2</li>
</ul>
<p class="body">In recent years, multimodal large language models (LLMs) have gained significant attention for their ability to handle various content formats, such as text, images, video, audio, and code. A notable example of this is text-to-image Transformers, such as OpenAI’s DALL-E 2, Google’s Imagen, and Stability AI’s Stable Diffusion. These models are capable of generating high-quality images based on textual descriptions.<a id="idIndexMarker003"/></p>
<p class="body">These text-to-image models comprise three essential components: a text encoder that compresses text into a latent representation, a method to incorporate text information into the image generation process, and a diffusion mechanism to gradually refine an image to produce realistic output. Understanding the diffusion mechanism is particularly crucial for comprehending text-to-image Transformers, as diffusion models form the foundation of all leading text-to-image Transformers. For this reason, you will start by building and training a diffusion model to generate flower images in this chapter. This will provide you with a deep understanding of the forward diffusion process, where noise is incrementally added to images until they become random noise. Subsequently, you will train a model to reverse the diffusion process by gradually removing noise from images until the model can generate a new, clean image from random noise, resembling those in the training dataset.</p>
<p class="body">Diffusion models have become the go-to choice for generating high-resolution images. The success of diffusion models lies in their ability to simulate and reverse a complex noise addition process, which mimics a deep understanding of how images are structured and how to construct them from abstract patterns. This method not only ensures high quality but also maintains a balance between diversity and accuracy in the generated images.</p>
<p class="body">After that, we’ll explain how a text-to-image Transformer works conceptually. We’ll focus on the contrastive language–image pretraining (CLIP) model developed by OpenAI, which is designed to comprehend and link visual and textual information. CLIP processes two types of inputs: images and text (typically in the form of captions or descriptions). These inputs are handled separately through two encoders in the model.<a id="idIndexMarker004"/></p>
<p class="body">The image branch of CLIP employs a Vision Transformer (ViT) to encode images into a high-dimensional vector space, extracting visual features in the process. Meanwhile, the text branch uses a Transformer-based language model to encode textual descriptions into the same vector space, capturing semantic features from the text. CLIP has been trained on many pairs of matching images and text descriptions to closely align the representations of matching pairs in the vector space.<a id="idIndexMarker005"/><a id="marker-342"/></p>
<p class="body">OpenAI’s text-to-image Transformers, such as DALL-E 2, incorporate CLIP as a core component. In this chapter, you’ll learn to obtain an OpenAI API key and write a Python program to generate images using DALL-E 2 based on text descriptions.</p>
<h2 class="fm-head" id="heading_id_3">15.1 Introduction to denoising diffusion models</h2>
<p class="body">The concept of diffusion-based models can be illustrated using the following example. Consider the goal of generating high-resolution flower images using a diffusion-based model. To do that, you first acquire a set of high-quality flower images for training. The model is then instructed to incrementally introduce small amounts of random noise into these images, a process known as forward diffusion. After many steps of adding noise, the training images eventually become random noise. The next phase involves training the model to reverse this process, starting with pure noise images and progressively reducing the noise until the images are indistinguishable from those in the original training set.<a id="idIndexMarker006"/><a id="idIndexMarker007"/></p>
<p class="body">Once trained, the model is given random noise images to work with. It systematically eliminates noise from the image over many iterations until it generates a high-resolution flower image that resembles those in the training set. This is the underlying principle of diffusion-based models.<sup class="footnotenumber" id="footnote-003-backlink"><a class="url1" href="#footnote-003">1</a></sup></p>
<p class="body">In this section, you will first explore the mathematical foundations of diffusion-based models. Then you will dive into the architecture of U-Nets, the type of model used for denoising images and producing high-resolution flower images. Specifically, the U-Net employs a scaled dot product attention (SDPA) mechanism, similar to what you have seen in Transformer models in chapters 9 to 12. Finally, you will learn the training process of diffusion-based models and the image-generation process of the trained model.</p>
<h3 class="fm-head1" id="heading_id_4">15.1.1 The forward diffusion process</h3>
<p class="body"><a id="marker-343"/>Several papers have proposed diffusion-based models with similar underlying mechanisms.<sup class="footnotenumber" id="footnote-002-backlink"><a class="url1" href="#footnote-002">2</a></sup> Let’s use the flower images as a concrete example to explain the idea behind denoising diffusion models. Figure 15.1 is a diagram of how the forward diffusion process works. <a id="idIndexMarker008"/><a id="idIndexMarker009"/><a id="idIndexMarker010"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="216" src="../../OEBPS/Images/CH15_F01_Liu.png" width="551"/></p>
<p class="figurecaption">Figure 15.1 A diagram of the forward diffusion process. We start with a clean image from the training set, <span class="times"><i class="timesitalic">x</i><sub class="fm-subscript">0</sub></span>, and add noise <span class="times">є<sub class="fm-subscript">0</sub></span> to it to form a noisy image <span class="times"><i class="timesitalic">x</i><sub class="fm-subscript">١</sub> = √(1 – <i class="fm-italics">β</i><sub class="fm-subscript">1</sub>)<i class="timesitalic">x</i><sub class="fm-subscript">0</sub> + √(<i class="fm-italics">β</i><sub class="fm-subscript">١</sub>)є<sub class="fm-subscript">0</sub></span>. We repeat this process for 1,000 time steps until the image <span class="times"><i class="timesitalic">x</i><sub class="fm-subscript">1000</sub></span> becomes random noise.</p>
</div>
<p class="body">Assume that flower images, <span class="times"><i class="timesitalic">x</i><sub class="fm-subscript">0</sub></span> (illustrated in the left image in figure 15.1), follow a distribution of <span class="times">q(x)</span>. In the forward diffusion process, we’ll add small amounts of noise to the images in each of the <span class="times">T = 1,000</span> steps. The noise tensor is normally distributed and has the same shape as the flower images: <span class="times">(3, 64, 64)</span>, meaning three color channels, with a height and width of 64 pixels.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Time steps in diffusion models</p>
<p class="fm-sidebar-text">In diffusion models, time steps refer to the discrete stages during the process of gradually adding noise to data and subsequently reversing this process to generate samples. The forward phase of a diffusion model progressively adds noise over a series of time steps, transforming data from its original, clean state into a noisy distribution. During the reverse phase, the model operates over a similar series of time steps but in a reverse order. It systematically removes noise from the data to reconstruct the original or generate new, high-fidelity samples. Each time step in this reverse process involves predicting the noise that was added in the corresponding forward step and subtracting it, thereby gradually denoising the data until reaching a clean state.</p>
</div>
<p class="body">In time step 1, we add noise <span class="times">є<sub class="fm-subscript">0</sub></span> to the image <span class="times"><i class="timesitalic">x</i><sub class="fm-subscript">0</sub></span>, so that we obtain a noisy image <span class="times"><i class="timesitalic">x</i><sub class="fm-subscript">1</sub></span>:<a id="marker-344"/></p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="90%"/>
<col class="contenttable-0-col" span="1" width="10%"/>
</colgroup>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre4" height="24" src="../../OEBPS/Images/CH15_F01_Liu-EQ01.png" width="194"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="fm-equation-caption">(15.1)</p>
</td>
</tr>
</tbody>
</table>
<p class="body">That is, <span class="times"><i class="timesitalic">x</i><sub class="fm-subscript">1</sub></span> is a weighted sum of <span class="times"><i class="timesitalic">x</i><sub class="fm-subscript">0</sub></span> and <span class="times">є<sub class="fm-subscript">0</sub></span>, where <span class="times"><i class="fm-italics">β</i><sub class="fm-subscript">1</sub></span> measures the weight placed on the noise. The value of <i class="timesitalic">β</i> changes in different <a id="idTextAnchor005"/>time steps—hence the subscript in <span class="times"><i class="fm-italics">β</i><sub class="fm-subscript">1</sub></span>. If we assume <span class="times"><i class="timesitalic">x</i><sub class="fm-subscript">0</sub></span> and <span class="times">є<sub class="fm-subscript">0</sub></span> are independent of each other and follow a standard normal distribution (i.e., with mean 0 and variance 1), the noisy image <span class="times"><i class="timesitalic">x</i><sub class="fm-subscript">1</sub></span> will also follow a standard normal distribution. This is easy to prove since</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="24" src="../../OEBPS/Images/CH15_F01_Liu-EQ02.png" width="403"/></p>
</div>
<p class="body">and</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="62" src="../../OEBPS/Images/CH15_F01_Liu-EQ03.png" width="437"/></p>
</div>
<p class="body">We can keep adding noise to the image for <a id="idTextAnchor006"/>the next T–1 time steps so that</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="90%"/>
<col class="contenttable-0-col" span="1" width="10%"/>
</colgroup>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre4" height="24" src="../../OEBPS/Images/CH15_F01_Liu-EQ04.png" width="224"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="fm-equation-caption">(15.2)</p>
</td>
</tr>
</tbody>
</table>
<p class="body">We can use a reparameterization trick and define <span class="times"><i class="fm-italics">α<sub class="fm-subscript">t</sub></i> = 1 − <i class="fm-italics">β<sub class="fm-subscript">t</sub></i></span> and</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="53" src="../../OEBPS/Images/CH15_F01_Liu-EQ05.png" width="84"/></p>
</div>
<p class="body">to allow us to sample <i class="timesitalic">x<sub class="fm-subscript">t</sub></i> at any arbitrary time step <span class="times">t</span>, where <span class="times">t</span> can take any value in <span class="times">[1, 2, . . ., T−1, T]</span>. Then we have<a id="idTextAnchor007"/></p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="90%"/>
<col class="contenttable-0-col" span="1" width="10%"/>
</colgroup>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre4" height="24" src="../../OEBPS/Images/CH15_F01_Liu-EQ06.png" width="180"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="fm-equation-caption">(15.3)</p>
</td>
</tr>
</tbody>
</table>
<p class="body">Where <span class="times">є</span> is a combination of <span class="times">є<sub class="fm-subscript">0</sub></span>, <span class="times">є<sub class="fm-subscript">1</sub></span>, . . ., and <span class="times">є<i class="fm-italics"><sub class="fm-subscript">t</sub></i><sub class="fm-subscript">–1</sub></span>, using the fact that we can add two normal distributions to obtain a new normal distribution. See, for example, the blog of Lilian Weng at <a class="url" href="https://mng.bz/Aalg">https://mng.bz/Aalg</a> for proof.</p>
<p class="body">The farther left of figure 15.1 shows a clean flower, <span class="times"><i class="timesitalic">x</i><sub class="fm-subscript">0</sub></span>, from the training set. In the first time step, we inject noise <span class="times">є<sub class="fm-subscript">0</sub></span> to it to form a noisy image <span class="times"><i class="timesitalic">x</i><sub class="fm-subscript">1</sub></span> (second image in figure 15.1). We repeat this process for 1,000 time steps, until the image becomes random noise (the rightmost image).</p>
<h3 class="fm-head1" id="heading_id_5">15.1.2 Using the U-Net model to denoise images</h3>
<p class="body">Now that you understand the forward diffusion process, let’s discuss the reverse diffusion process (i.e., the denoising process). If we can train a model to reverse the forward diffusion process, we can feed the model with random noise and ask it to produce a noisy flower image. We can then feed the noisy image to the trained model again and produce a clearer, though still noisy, image. We can iteratively repeat the process for many time steps until we obtain a clean image, indistinguishable from images from the training set. The use of multiple inference steps in the reverse diffusion process, rather than just a single step, is crucial for gradually reconstructing high-quality data from a noisy distribution. It allows for a more controlled, stable, and high-quality generation of data.<a id="idIndexMarker011"/><a id="idIndexMarker012"/><a id="idIndexMarker013"/><a id="marker-345"/><a id="idIndexMarker014"/></p>
<p class="body">To that end, we’ll create a denoising U-Net model. The U-Net architecture, which was originally designed for biomedical image segmentation, is characterized by its symmetric shape, with a contracting path (encoder) and an expansive path (decoder), connected by a bottleneck layer. In the context of denoising, U-Net models are adapted to remove noise from images while preserving important details. U-Nets outperform simple convolutional networks in denoising tasks due to their efficient capturing of local and global features in images.</p>
<p class="body">Figure 15.2 is a diagram of the structure of the denoising U-Net we use in this chapter.</p>
<p class="body">The model takes a noisy image and the time step the noisy image is in (<i class="timesitalic">x<sub class="fm-subscript">t</sub></i> and <span class="times">t</span> in equation 15.3) as input and predicts the noise in the image (i.e., <span class="times">є</span>). Since the noisy image is a weighted sum of the original clean image and noise (see equation 15.3), knowing the noise allows us to deduce and reconstruct the original image.</p>
<p class="body">The contracting path (i.e., the encoder; left side of figure 15.2) consists of multiple convolutional layers and pooling layers. It progressively downsamples the image, extracting and encoding features at different levels of abstraction. This part of the network learns to recognize patterns and features that are relevant for denoising.</p>
<p class="body">The bottleneck layer (bottom of figure 15.2) connects the encoder and decoder paths. It consists of convolutional layers and is responsible for capturing the most abstract representations of the image.</p>
<p class="body">The expansive path (i.e., the decoder; right side of figure 15.2) consists of upsampling layers and convolutional layers. It progressively upsamples the feature maps, reconstructing the image while incorporating features from the encoder through skip connections. Skip connections (denoted by dashed lines in figure 15.2) are crucial in U-Net models, as they allow the model to retain fine-grained details from the input image by combining low-level and high-level features. Next, I briefly explain how skip connections work.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="554" src="../../OEBPS/Images/CH15_F02_Liu.png" width="606"/></p>
<p class="figurecaption">Figure 15.2 The architecture of the denoising U-Net model. The U-Net architecture is characterized by its symmetric shape, with a contracting path (encoder) and an expansive path (decoder), connected by a bottleneck layer. The model is designed to remove noise from images while preserving important details. The input to the model is a noisy image, along with which time step the image is in, and the output is the predicted noise in the image.</p>
</div>
<p class="body"><a id="marker-346"/>In a U-Net model, skip connections are implemented by concatenating feature maps from the encoder path with corresponding feature maps in the decoder path. These feature maps are typically of the same spatial dimensions but may have been processed differently due to the separate paths they have traversed. During the encoding process, the input image is progressively downsampled, and some spatial information (such as edges and textures) may be lost. Skip connections help preserve this information by directly passing feature maps from the encoder to the decoder, bypassing the information bottleneck.</p>
<p class="body">For example, the dashed line at the top of figure 15.2 indicates that the model concatenates the <i class="fm-italics">output</i> from the Conv2D layer in the encoder, which has a shape of (128, 64, 64), with the <i class="fm-italics">input</i> to the Conv2D layer in the decoder, which also has a shape of (128, 64, 64). As a result, the final input to the Conv2D layer in the decoder has a shape of (256, 64, 64).</p>
<p class="body">By combining high-level, abstract features from the decoder with low-level, detailed features from the encoder, skip connections enable the model to better reconstruct fine details in the denoised image. This is particularly important in denoising tasks, where retaining subtle image details is crucial.</p>
<p class="body">The scaled dot product attention (SDPA) mechanism is implemented in both the final block of the contracting path and the final block of the expansive path in our denoising U-Net model, accompanied by layer normalization and residual connections (as shown in figure 15.2 with the label Attn/Norm/Add). This SDPA mechanism is essentially the same as the one we developed in chapter 9; the key difference is its application to image pixels rather than text tokens.<a id="idIndexMarker015"/></p>
<p class="body">The use of skip connections and the model’s size lead to redundant feature extractions in our denoising U-Net, ensuring that no important feature is lost during the denoising process. However, the large size of the model also complicates the identification of relevant features, akin to searching for a needle in a haystack. The attention mechanism empowers the model to emphasize significant features while disregarding irrelevant ones, thereby enhancing the effectiveness of the learning process.<a id="idIndexMarker016"/><a id="idIndexMarker017"/><a id="idIndexMarker018"/><a id="marker-347"/><a id="idIndexMarker019"/></p>
<h3 class="fm-head1" id="heading_id_6">15.1.3 A blueprint to train the denoising U-Net model</h3>
<p class="body">The output of the denoising U-Net is the noise injected into the noisy image. The model is trained to minimize the difference between the output (predicted noise) and the ground truth (actual noise). <a id="idIndexMarker020"/><a id="idIndexMarker021"/><a id="idIndexMarker022"/></p>
<p class="body">The denoising U-Net model uses the U-Net architecture’s ability to capture both local and global context, making it effective for removing noise while preserving important details such as edges and textures. These models are widely used in various applications, including medical image denoising, photographic image restoration, and more. Figure 15.3 is a diagram of the training process of our denoising U-Net model.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="305" src="../../OEBPS/Images/CH15_F03_Liu.png" width="491"/></p>
<p class="figurecaption">Figure 15.3 The training process of the denoising U-Net model. We first obtain clean flower images as our training set. We add noise to clean flower images and present them to the U-Net model. The model predicts the noise in the noisy images. We compare the predicted noise with the actual noise injected into the flower images and tweak the model weights to minimize the mean absolute error.</p>
</div>
<p class="body">The first step is to gather a dataset of flower images. We’ll use the Oxford 102 Flower dataset as our training set. We’ll resize all images to a fixed resolution of <span class="times">64 <span class="cambria">×</span> 64</span> pixels and normalize pixel values to the range <span class="times">[–1, 1]</span>. For denoising, we need pairs of clean and noisy images. We’ll synthetically add noise to the clean flower images to create noisy counterparts (step 2 in figure 15.3) based on the formula specified in equation 15.3.</p>
<p class="body">We’ll then build a denoising U-Net model with a structure as outlined in figure 15.2. During each epoch of training, we iterate over the dataset in batches. We add noise to the flower images and present the noisy images to the U-Net model (step 3), along with the time steps the noisy images are in, t. The U-Net model predicts the noise in the noisy images (step 4) based on current parameters in the model.</p>
<p class="body">We compare the predicted noise with the actual noise and calculate the L1 loss (i.e., mean absolute error) at the pixel level (step 5). L1 loss is usually preferred in such situations because it’s less sensitive to outliers compared to the L2 loss (mean squared error). We then tweak the model parameters to minimize the L1 loss (step 6) so that in the next iteration, the model makes better predictions. We repeat this process for many iterations until the model parameters converge.<a id="marker-348"/><a id="idIndexMarker023"/><a id="idIndexMarker024"/></p>
<h2 class="fm-head" id="heading_id_7">15.2 Preparing the training data</h2>
<p class="body">We’ll use the Oxford 102 Flower dataset, which is freely available on Hugging Face, as our training data. The dataset contains about 8,000 flower images and can be downloaded directly by using the <i class="fm-italics">datasets</i> library you installed earlier. <a id="idIndexMarker025"/><a id="idIndexMarker026"/><a id="idIndexMarker027"/></p>
<p class="body">To save space, we’ll place most helper functions and classes in two local modules, ch15util.py and unet_util.py. Download these two files from the book’s GitHub repository (<a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a>) and place them in the /utils/ folder on your computer. The Python programs in this chapter are adapted from Hugging Face’s GitHub repository (<a class="url" href="https://github.com/huggingface/diffusers">https://github.com/huggingface/diffusers</a>) and Filip Basara’s GitHub repository (<a class="url" href="https://github.com/filipbasara0/simple-diffusion">https://github.com/filipbasara0/simple-diffusion</a>).</p>
<p class="body">You’ll use Python to download the dataset to your computer. After that, we’ll demonstrate the forward diffusion process by gradually adding noise to clean images in the training dataset until they become random noise. Finally, you’ll place the training data in batches so that we can use them to train the denoising U-Net model later in the chapter.</p>
<p class="body">You’ll use the following Python libraries in this chapter: datasets, einops, diffusers, and openai. To install these libraries, execute the following line of code in a new cell in your Jupyter Notebook application on your computer:</p>
<pre class="programlisting">!pip install datasets einops diffusers openai</pre>
<p class="body">Follow the on-screen instructions to finish the installation.</p>
<h3 class="fm-head1" id="heading_id_8">15.2.1 Flower images as the training data</h3>
<p class="body">The <code class="fm-code-in-text">load_dataset()</code> method from the <i class="fm-italics">datasets</i> library you installed earlier allows you to directly download the Oxford 102 Flower dataset from Hugging Face. We’ll then use the <i class="fm-italics">matplotlib</i> library to show some flower images in the dataset so that we have an idea of what the images in the training dataset look like.<a id="idIndexMarker028"/><a id="idIndexMarker029"/><a id="idIndexMarker030"/><a id="idIndexMarker031"/><a id="idIndexMarker032"/></p>
<p class="body">Run the lines of code shown in the following listing in a cell in Jupyter Notebook.</p>
<p class="fm-code-listing-caption">Listing 15.1 Downloading and visualizing flower images</p>
<pre class="programlisting">from datasets import load_dataset
from utils.ch15util import transforms
  
dataset = load_dataset("huggan/flowers-102-categories",
    split="train",)                                      <span class="fm-combinumeral">①</span>
dataset.set_transform(transforms)
  
import matplotlib.pyplot as plt
from torchvision.utils import make_grid
  
# Plot all the images of the 1st batch in grid
grid = make_grid(dataset[:16]["input"], 8, 2)            <span class="fm-combinumeral">②</span>
plt.figure(figsize=(8,2),dpi=300)
plt.imshow(grid.numpy().transpose((1,2,0)))
plt.axis("off")
plt.show()</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Downloads the images from Hugging Face</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Plots the first 16 images</p>
<p class="body">After running the preceding code listing, you’ll see the first 16 flower images in the dataset, as displayed in figure 15.4. These are high-resolution color images of various types of flowers. We have standardized the size of each image to (3, 64, 64).<a id="marker-349"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="214" src="../../OEBPS/Images/CH15_F04_Liu.png" width="773"/></p>
<p class="figurecaption">Figure 15.4 The first 16 Images from the Oxford 102 Flower dataset.</p>
</div>
<p class="body">We place the dataset in batches of 4 so that we can use them to train the denoising U-Net model later. We choose a batch size of 4 to keep the memory size small enough to fit on a GPU during training. Adjust the batch size to 2 or even 1 if your GPU memory is small:</p>
<pre class="programlisting">import torch
resolution=64
batch_size=4
train_dataloader=torch.utils.data.DataLoader(
    dataset, batch_size=batch_size, shuffle=True)</pre>
<p class="body">Next, we’ll code in and visualize the forward diffusion process.</p>
<h3 class="fm-head1" id="heading_id_9">15.2.2 Visualizing the forward diffusion process</h3>
<p class="body">We have defined a class <code class="fm-code-in-text">DDIMScheduler()</code> in the local module ch15util.py you just downloaded. Take a look at the class in the file; we’ll use it to add noise to images. We’ll also use the class to produce clean images later, along with the trained denoising U-Net model. The <code class="fm-code-in-text">DDIMScheduler()</code> class manages the step sizes and sequence of denoising steps, enabling deterministic inference that can produce high-quality samples through the denoising process. <a id="idIndexMarker033"/><a id="idIndexMarker034"/><a id="idIndexMarker035"/><a id="marker-350"/><a id="idIndexMarker036"/></p>
<p class="body">We first select four clean images from the training set and generate noise tensors that have the same shape as these images:</p>
<pre class="programlisting">clean_images=next(iter(train_dataloader))["input"]*2-1    <span class="fm-combinumeral">①</span>
print(clean_images.shape)
nums=clean_images.shape[0]
noise=torch.randn(clean_images.shape)                     <span class="fm-combinumeral">②</span>
print(noise.shape)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Obtains four clean images</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Generates a tensor, noise, which has the same shape as the clean images; each value in the noise follows an independent standard normal distribution.</p>
<p class="body">The output from the preceding code block is</p>
<pre class="programlisting">torch.Size([4, 3, 64, 64])
torch.Size([4, 3, 64, 64])</pre>
<p class="body">Both the images and the noise tensors have a shape of (4, 3, 64, 64), meaning 4 images in the batch and 3 color channels per image, and the height and width of the images are 64 pixels.</p>
<p class="body">During the forward diffusion process, there are 999 transitional noisy images between the clean images (<i class="timesitalic">x</i><sub class="fm-subscript">0</sub> as we explained in the first section) and random noise (<i class="fm-italics">x<sub class="fm-subscript">T</sub></i>). The transitional noisy images are a weighted sum of the clean image and the noise. As <i class="fm-italics">t</i> goes from 0 to 1,000, the weight on the clean image gradually decreases, and the weight on the noise gradually increases, as specified in equation 15.3.</p>
<p class="body">Next, we generate and visualize some transitional noisy images.</p>
<p class="fm-code-listing-caption">Listing 15.2 Visualizing the forward diffusion process</p>
<pre class="programlisting">from utils.ch15util import DDIMScheduler
  
noise_scheduler=DDIMScheduler(num_train_timesteps=1000)    <span class="fm-combinumeral">①</span>
allimgs=clean_images
for step in range(200,1001,200):                           <span class="fm-combinumeral">②</span>
    timesteps=torch.tensor([step-1]*4).long()
    noisy_images=noise_scheduler.add_noise(clean_images,
                 noise, timesteps)                         <span class="fm-combinumeral">③</span>
    allimgs=torch.cat((allimgs,noisy_images))              <span class="fm-combinumeral">④</span>
  
import torchvision
imgs=torchvision.utils.make_grid(allimgs,4,6)
fig = plt.figure(dpi=300)
plt.imshow((imgs.permute(2,1,0)+1)/2)                      <span class="fm-combinumeral">⑤</span>
plt.axis("off")
plt.show()</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Instantiates the DDIMScheduler() class with 1,000 time steps</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Looks at time steps 200, 400, 600, 800, and 1,000</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates noisy images at these time steps</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Concatenates noisy images with clean images</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Displays all images</p>
<p class="body">The <code class="fm-code-in-text">add_noise()</code> method in the <code class="fm-code-in-text">DDIMScheduler()</code> class takes three arguments: <code class="fm-code-in-text">clean_images</code>, <code class="fm-code-in-text">noise</code>, and <code class="fm-code-in-text">timesteps</code>. It produces a weighted sum of the clean image and the noise, which is a noisy image. Further, the weight is a function of the time step, t. As the time step, t, moves from 0 to 1,000, the weight on the clean image decreases and that on the noise increases. If you run the previous code listing, you’ll see an image similar to figure 15.5.<a id="idIndexMarker037"/><a id="idIndexMarker038"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="410" src="../../OEBPS/Images/CH15_F05_Liu.png" width="600"/></p>
<p class="figurecaption">Figure 15.5 The forward diffusion process. The four images in the first column are clean images from the training dataset. We then gradually add noise to these images from time step 1 to time step 1,000. As the time step increases, more and more noise is injected into the images. The four images in the second column are images after 200 time steps. The third column contains images after 400 time steps, and they have more noise than those in the second column. The last column contains images after 1,000 time steps, and they are 100% random noise.</p>
</div>
<p class="body"><a id="marker-351"/>The first column contains the four clean images without noise. As we move to the right, we gradually add more and more noise to the images. The very last column contains pure random noise. <a id="idIndexMarker039"/><a id="idIndexMarker040"/><a id="idIndexMarker041"/><a id="idIndexMarker042"/></p>
<h2 class="fm-head" id="heading_id_10">15.3 Building a denoising U-Net model</h2>
<p class="body">Earlier in this chapter, we discussed the architecture of the denoising U-Net model. In this section, I will guide you through implementing it using Python and PyTorch.<a id="idIndexMarker043"/><a id="idIndexMarker044"/></p>
<p class="body">The U-Net model we are going to construct is quite large, containing over 133 million parameters, reflecting the complexity of its intended task. It is engineered to capture both local and global features within an image through a process of downsampling and upsampling the input. The model uses multiple convolutional layers interconnected by skip connections, which combine features from various levels of the network. This architecture helps maintain spatial information, facilitating more effective learning.</p>
<p class="body">Given the substantial size of the denoising U-Net model and its redundant feature extraction, the SDPA attention mechanism is employed to enable the model to concentrate on the most relevant aspects of the input for the task at hand. To compute SDPA attention, we will flatten the image and treat its pixels as a sequence. We will then use SDPA to learn the dependencies among different pixels in the image in a manner akin to how we learned dependencies among different tokens in text in chapter 9.</p>
<h3 class="fm-head1" id="heading_id_11">15.3.1 The attention mechanism in the denoising U-Net model</h3>
<p class="body"><a id="marker-352"/>To implement the attention mechanism, we have defined an <code class="fm-code-in-text">Attention()</code> class in the local module ch15util.py, as shown in the following code listing.</p>
<p class="fm-code-listing-caption">Listing 15.3 The attention mechanism in the denoising U-Net model</p>
<pre class="programlisting">import torch
from torch import nn, einsum
from einops import rearrange
  
class Attention(nn.Module):
    def __init__(self, dim, heads=4, dim_head=32):
        super().__init__()
        self.scale = dim_head**-0.5
        self.heads = heads
        hidden_dim = dim_head * heads
        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)
        self.to_out = nn.Conv2d(hidden_dim, dim, 1)
    def forward(self, x):
        b, c, h, w = x.shape
        qkv = self.to_qkv(x).chunk(3, dim=1)                      <span class="fm-combinumeral">①</span>
        q, k, v = map(
        lambda t: rearrange(t, 'b (h c) x y -&gt; b h c (x y)', h=self.heads),
        qkv)                                                      <span class="fm-combinumeral">②</span>
        q = q * self.scale    
        sim = einsum('b h d i, b h d j -&gt; b h i j', q, k)
        attn = sim.softmax(dim=-1)                                <span class="fm-combinumeral">③</span>
        out = einsum('b h i j, b h d j -&gt; b h i d', attn, v)      <span class="fm-combinumeral">④</span>
        out = rearrange(out, 'b h (x y) d -&gt; b (h d) x y', x=h, y=w)
        return self.to_out(out)                                   <span class="fm-combinumeral">⑤</span>
attn=Attention(128)
x=torch.rand(1,128,64,64)
out=attn(x)
print(out.shape)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Passes the input through three linear layers to obtain query, key, and value</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Splits query, key, and value into four heads</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Calculates attention weights</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Calculates the attention vector in each head</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Concatenates the four attention vectors into one</p>
<p class="body">The output after running the preceding code listing is</p>
<pre class="programlisting">torch.Size([1, 128, 64, 64])</pre>
<p class="body">The attention mechanism used here, SDPA, is the same as the one we utilized in chapter 9, where we applied SDPA to a sequence of indices representing tokens in text. Here, we apply it to pixels in an image. We treat the flattened pixels of an image as a sequence and use SDPA to extract dependencies among different areas of the input image, enhancing the efficiency of the denoising process.</p>
<p class="body">Listing 15.3 demonstrates how SDPA operates in our context. To give you a concrete example, we have created a hypothetical image, <span class="times">x</span>, with dimensions <span class="times">(1, 128, 64, 64)</span>, indicating one image in the batch, 128 feature channels, and a size of <span class="times">64 <span class="cambria">×</span> 64 pixels</span> in each channel. The input <span class="times">x</span> is then processed through the attention layer. Specifically, each feature channel in the image is flattened into a sequence of <span class="times">64 <span class="cambria">×</span> 64 = 4,096 pixels</span>. This sequence is then passed through three distinct neural network layers to produce the query <span class="times">Q</span>, key <span class="times">K</span>, and value <span class="times">V</span>, which are subsequently split into four heads. The attention vector in each head is calculated as follows:</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="56" src="../../OEBPS/Images/CH15_F05_Liu-EQ07.png" width="364"/></p>
</div>
<p class="body">where <i class="timesitalic">d<sub class="fm-subscript">k</sub></i> represents the dimension of the key vector <span class="times">K</span>. The attention vectors from the four heads are concatenated back into a single attention vector.</p>
<h3 class="fm-head1" id="heading_id_12">15.3.2 The denoising U-Net model</h3>
<p class="body"><a id="marker-353"/>In the local module unet_util.py you just downloaded, we have defined a <code class="fm-code-in-text">UNet()</code> class to represent the denoising U-Net model. Take a look at the definition in the file, and I’ll provide a brief explanation of how it works later. The following code listing presents a portion of the <code class="fm-code-in-text">UNet()</code> class.<a id="idIndexMarker045"/><a id="idIndexMarker046"/></p>
<p class="fm-code-listing-caption">Listing 15.4 Defining the <code class="fm-code-in-text">UNet()</code> class</p>
<pre class="programlisting">class UNet(nn.Module):
… 
    def forward(self, sample, timesteps):                         <span class="fm-combinumeral">①</span>
        if not torch.is_tensor(timesteps):
            timesteps = torch.tensor([timesteps],
                                     dtype=torch.long,
                                     device=sample.device)
        timesteps = torch.flatten(timesteps)
        timesteps = timesteps.broadcast_to(sample.shape[0])
        t_emb = sinusoidal_embedding(timesteps, self.hidden_dims[0])
        t_emb = self.time_embedding(t_emb)                        <span class="fm-combinumeral">②</span>
        x = self.init_conv(sample)
        r = x.clone()
        skips = []
        for block1, block2, attn, downsample in self.down_blocks: <span class="fm-combinumeral">③</span>
            x = block1(x, t_emb)
            skips.append(x)
            x = block2(x, t_emb)
            x = attn(x)
            skips.append(x)
            x = downsample(x)
        x = self.mid_block1(x, t_emb)
        x = self.mid_attn(x)
        x = self.mid_block2(x, t_emb)                             <span class="fm-combinumeral">④</span>
        for block1, block2, attn, upsample in self.up_blocks:    
            x = torch.cat((x, skips.pop()), dim=1)                <span class="fm-combinumeral">⑤</span>
            x = block1(x, t_emb)
            x = torch.cat((x, skips.pop()), dim=1)
            x = block2(x, t_emb)
            x = attn(x)
            x = upsample(x)
        x = self.out_block(torch.cat((x, r), dim=1), t_emb)
        out = self.conv_out(x)
        return {"sample": out}                                    <span class="fm-combinumeral">⑥</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The model takes a batch of noisy images and the time steps as input.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The embedded time steps are added to the images as inputs in various stages.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Passes the input through the contracting path</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Passes the input through the bottleneck path</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Passes the input through the expansive path, with skip connections</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> The output is the predicted noise in the input images.</p>
<p class="body">The job of the denoising U-Net is to predict the noise in the input images based on the time steps these images are in. As described in equation 15.3, a noisy image at any time step <span class="times">t</span>, <i class="timesitalic">x<sub class="fm-subscript">t</sub></i>, can be represented as a weighted sum of the clean image, <span class="times"><i class="timesitalic">x</i><sub class="fm-subscript">o</sub></span>, and standard normally distributed random noise, <span class="times">є</span>. The weight assigned to the clean image decreases, and the weight assigned to the random noise increases as the time step <span class="times">t</span> progresses from <span class="times">0</span> to <span class="times">T</span>. Therefore, to deduce the noise in noisy images, the denoising U-Net needs to know which time step a noisy image is in.</p>
<p class="body">Time steps are embedded using sine and cosine functions in a manner akin to positional encoding in Transformers (discussed in chapters 9 and 10), resulting in a 128-value vector. These embeddings are then expanded to match the dimensions of the image features at various layers within the model. For instance, in the first down block, the time embeddings are broadcasted to a shape of (128, 64, 64) before being added to the image features, which also have dimensions of (128, 64, 64).</p>
<p class="body"><a id="marker-354"/>Next, we create a denoising U-Net model by instantiating the <code class="fm-code-in-text">UNet()</code> class in the local module: <a id="idIndexMarker047"/></p>
<pre class="programlisting">from utils.unet_util import UNet
  
device="cuda" if torch.cuda.is_available() else "cpu"
resolution=64
model=UNet(3,hidden_dims=[128,256,512,1024],
           image_size=resolution).to(device)
num=sum(p.numel() for p in model.parameters())
print("number of parameters: %.2fM" % (num/1e6,))
print(model) </pre>
<p class="body">The output is</p>
<pre class="programlisting">number of parameters: 133.42M</pre>
<p class="body">The model has more than 133 million parameters, as you can see from the previous output. Given the large number of parameters, the training process in this chapter will be time-consuming, requiring approximately 3 to 4 hours of GPU training. However, for those who do not have access to GPU training, the trained weights are also available on my website. The link to these weights will be provided in the following section. <a id="idIndexMarker048"/><a id="idIndexMarker049"/></p>
<h2 class="fm-head" id="heading_id_13">15.4 Training and using the denoising U-Net model</h2>
<p class="body">Now that we have both the training data and the denoising U-Net model, we’re ready to train the model using the training data.<a id="idIndexMarker050"/><a id="idIndexMarker051"/></p>
<p class="body">During each training epoch, we’ll cycle through all the batches in the training data. For each image, we’ll randomly select a time step and add noise to the clean images in the training data based on this time step value, resulting in a noisy image. These noisy images and their corresponding time step values are then fed into the denoising U-Net model to predict the noise in each image. We compare the predicted noise to the ground truth (the actual noise added to the image) and adjust the model parameters to minimize the mean absolute error between the predicted and actual noise.</p>
<p class="body">After training, we’ll use the trained model to generate flower images. We’ll perform this generation in 50 inference steps (i.e., we’ll set time step values to 980, 960, . . ., 20, and 0). Starting with random noise, we’ll input it into the trained model to obtain a noisy image. This noisy image is then fed back into the trained model to denoise it. We repeat this process for 50 inference steps, resulting in an image that is indistinguishable from the flowers in the training set.</p>
<h3 class="fm-head1" id="heading_id_14">15.4.1 Training the denoising U-Net model</h3>
<p class="body">Next, we’ll first define the optimizer and the learning rate scheduler for the training process.<a id="idIndexMarker052"/><a id="idIndexMarker053"/><a id="marker-355"/></p>
<p class="body">We’ll use the AdamW optimizer, a variant of the Adam optimizer that we have been using throughout this book. The AdamW optimizer, first proposed by Ilya Loshchilov and Frank Hutter, decouples weight decay (a form of regularization) from the optimization steps.<sup class="footnotenumber" id="footnote-001-backlink"><a class="url1" href="#footnote-001">3</a></sup> Instead of applying weight decay directly to the gradients, AdamW applies weight decay directly to the parameters (weights) after the optimization step. This modification helps achieve better generalization performance by preventing the decay rate from being adapted along with the learning rates. Interested readers can learn more about the AdamW optimizer in the original paper by Loshchilov and Hutter.<a id="idIndexMarker054"/></p>
<p class="body">We will also use a learning rate scheduler from the diffusers library to adjust the learning rate during the training process. Initially using a higher learning rate can help the model escape local minima, while gradually lowering the learning rate in later stages of training can help the model converge more steadily and accurately towards a global minimum. The learning rate scheduler is defined as shown in the following listing.</p>
<p class="fm-code-listing-caption">Listing 15.5 Choosing the optimizer and learning rate in training</p>
<pre class="programlisting">from diffusers.optimization import get_scheduler
  
num_epochs=100                                             <span class="fm-combinumeral">①</span>
optimizer=torch.optim.AdamW(model.parameters(),lr=0.0001,
    betas=(0.95,0.999),weight_decay=0.00001,eps=1e-8)      <span class="fm-combinumeral">②</span>
lr_scheduler=get_scheduler(                                <span class="fm-combinumeral">③</span>
    "cosine",
    optimizer=optimizer,
    num_warmup_steps=300,
    num_training_steps=(len(train_dataloader) * num_epochs))</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Will train the model for 100 epochs</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Uses the AdamW optimizer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Uses the learning rate scheduler in the diffusers library to control the learning rate</p>
<p class="body">The exact definition of the <code class="fm-code-in-text">get_scheduler()</code> function is defined on GitHub by Hugging Face: <a class="url" href="https://mng.bz/ZVo5">https://mng.bz/ZVo5</a>. In the first 300 training steps (warmup steps), the learning rate increases linearly from 0 to 0.0001 (the learning rate we set in the AdamW optimizer). After 300 steps, the learning rate decreases following the values of the cosine function between 0.0001 and 0. We train the model for 100 epochs in the following listing.<a id="marker-356"/><a id="idIndexMarker055"/></p>
<p class="fm-code-listing-caption">Listing 15.6 Training the denoising U-Net model</p>
<pre class="programlisting">for epoch in range(num_epochs):
    model.train()
    tloss = 0
    print(f"start epoch {epoch}")
    for step, batch in enumerate(train_dataloader):
        clean_images = batch["input"].to(device)*2-1
        nums = clean_images.shape[0]
        noise = torch.randn(clean_images.shape).to(device)
        timesteps = torch.randint(0,
                noise_scheduler.num_train_timesteps,
                (nums, ),
                device=device).long()
        noisy_images = noise_scheduler.add_noise(clean_images,
                     noise, timesteps)                         <span class="fm-combinumeral">①</span>
  
        
noise_pred = model(noisy_images, 
                       timesteps)["sample"]                    <span class="fm-combinumeral">②</span>
        loss=torch.nn.functional.l1_loss(noise_pred, noise)    <span class="fm-combinumeral">③</span>
        loss.backward()
        optimizer.step()                                       <span class="fm-combinumeral">④</span>
        lr_scheduler.step()
        optimizer.zero_grad()
        tloss += loss.detach().item()
        if step%100==0:
            print(f"step {step}, average loss {tloss/(step+1)}")
torch.save(model.state_dict(),'files/diffusion.pth')</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Adds noise to clean images in the training set</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Uses the denoising U-Net to predict noise in noisy images</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Compares the predicted noise with the actual noise to calculate the loss</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Tweaks model parameters to minimize the mean absolute error</p>
<p class="body">During each epoch, we cycle through all batches of clean flower images in the training set. We introduce noise to these clean images and feed them to the denoising U-Net to predict the noise in these images. We then compare the predicted noise to the actual noise and adjust the model parameters to minimize the mean absolute error (pixel-wise) between the two.</p>
<p class="body">The training process described here takes several hours with GPU training. After training, the trained model weights are saved on your computer. Alternatively, you can download the trained weights from my website at <a class="url" href="https://mng.bz/RNlD">https://mng.bz/RNlD</a>. Unzip the file after downloading. <a id="idIndexMarker056"/><a id="idIndexMarker057"/></p>
<h3 class="fm-head1" id="heading_id_15">15.4.2 Using the trained model to generate flower images</h3>
<p class="body"><a id="marker-357"/>To generate flower images, we’ll use 50 inference steps. This means we’ll look at 50 equally spaced time steps between <span class="times">t = 0</span> and <span class="times">t = T</span>, with <span class="times">T = 1,000</span> in our case. Therefore, the 50 inference time steps are <span class="times">t = 980, 960, 940, . . . , 20</span>, and <span class="times">0</span>. We’ll start with pure random noise, which corresponds to the image at <span class="times">t = 1000</span>. We use the trained denoising U-Net model to denoise it and create a noisy image at <span class="times">t = 980</span>. We then present the noisy image at <span class="times">t = 980</span> to the trained model to denoise it and obtain the noisy image at <span class="times">t = 960</span>. We repeat the process for many iterations until we obtain an image at <span class="times">t = 0</span>, which is a clean image. This process is implemented through the <code class="fm-code-in-text">generate()</code> method in the <code class="fm-code-in-text">DDIMScheduler()</code> class within the local module ch15util.py.<a id="idIndexMarker058"/><a id="idIndexMarker059"/><a id="idIndexMarker060"/><a id="idIndexMarker061"/></p>
<p class="fm-code-listing-caption">Listing 15.7 Defining a <code class="fm-code-in-text">generate()</code> method in the <code class="fm-code-in-text">DDIMScheduler()</code> class</p>
<pre class="programlisting">    @torch.no_grad()
    def generate(self,model,device,batch_size=1,generator=None,
         eta=1.0,use_clipped_model_output=True,num_inference_steps=50):
        imgs=[]
        image=torch.randn((batch_size,model.in_channels,model.sample_size,
            
model.sample_size),
            generator=generator).to(device)              <span class="fm-combinumeral">①</span>
  
        self.set_timesteps(num_inference_steps)
        for t in tqdm(self.timesteps):                   <span class="fm-combinumeral">②</span>
            model_output = model(image, t)["sample"]     <span class="fm-combinumeral">③</span>
            image = self.step(model_output,t,image,eta,
                  use_clipped_model_output=\
                  use_clipped_model_output)              <span class="fm-combinumeral">④</span>
            img = unnormalize_to_zero_to_one(image)
            img = img.cpu().permute(0, 2, 3, 1).numpy()
            imgs.append(img)                             <span class="fm-combinumeral">⑤</span>
        image = unnormalize_to_zero_to_one(image)
        image = image.cpu().permute(0, 2, 3, 1).numpy()
        return {"sample": image}, imgs </pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Uses random noise as the starting point (i.e., image at t = 1,000)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Uses 50 inference time steps (t = 980, 960, 940, . . , 20, 0)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Uses the trained denoising U-Net model to predict noise</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Creates an image based on predicted noise</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Saves intermediate images in a list, imgs</p>
<p class="body">In this <code class="fm-code-in-text">generate()</code> method, we have also created a list, imgs, to store all intermediate images at time steps <span class="times">t = 980, 960,. . . , 20</span>, and <span class="times">0</span>. We’ll use them to visualize the denoising process later. The <code class="fm-code-in-text">generate()</code> method returns a dictionary with the generated images and the list, imgs.<a id="idIndexMarker062"/><a id="idIndexMarker063"/></p>
<p class="body">Next, we’ll use the previous <code class="fm-code-in-text">generate()</code> method to create 10 clean images.</p>
<p class="fm-code-listing-caption">Listing 15.8 Image generation with the trained denoising U-Net model</p>
<pre class="programlisting">sd=torch.load('files/diffusion.pth',map_location=device)
model.load_state_dict(sd)
with torch.no_grad():
    generator = torch.manual_seed(1)                     <span class="fm-combinumeral">①</span>
    generated_images,imgs = noise_scheduler.generate(
        model,device,
        num_inference_steps=50,
        generator=generator,
        eta=1.0,
        use_clipped_model_output=True,
        batch_size=10)                                   <span class="fm-combinumeral">②</span>
imgnp=generated_images["sample"]    
import matplotlib.pyplot as plt
plt.figure(figsize=(10,4),dpi=300)
for i in range(10):                                      <span class="fm-combinumeral">③</span>
    ax = plt.subplot(2,5, i + 1)
    plt.imshow(imgnp[i])
    plt.xticks([])
    plt.yticks([])
    plt.tight_layout()
plt.show()  </pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Sets the random seed to 1 so results are reproducible</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Uses the defined generate() method to create 10 clean images</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Plots the generated images</p>
<p class="body">We set the random seed to 1. As a result, if you use the trained model from my website, you’ll get identical results as shown in figure 15.6. We use the <code class="fm-code-in-text">generate()</code> method defined earlier to create 10 clean images, using 50 inference steps. We then plot the 10 images in a <span class="times">2 <span class="cambria">×</span> 5</span> grid, as shown in figure 15.6.<a id="marker-358"/><a id="idIndexMarker064"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="239" src="../../OEBPS/Images/CH15_F06_Liu.png" width="600"/></p>
<p class="figurecaption">Figure 15.6 Flower images created by the trained denoising U-Net model.</p>
</div>
<p class="body">As you can see from figure 15.6, the generated flower images look real and resemble those in the training dataset.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 15.1</p>
<p class="fm-sidebar-text">Modify code listing 15.8 and change the random seed to 2. Keep the rest of the code the same. Rerun the code listing and see what the generated images look like.</p>
</div>
<p class="body">The <code class="fm-code-in-text">generate()</code> method also returns a list, imgs, which contains all the images in the 50 intermediate steps. We’ll use them to visualize the denoising process.<a id="idIndexMarker065"/></p>
<p class="fm-code-listing-caption">Listing 15.9 Visualizing the denoising process</p>
<pre class="programlisting">steps=imgs[9::10]                                <span class="fm-combinumeral">①</span>
imgs20=[]
for j in [1,3,6,9]:
    for i in range(5):
        imgs20.append(steps[i][j])               <span class="fm-combinumeral">②</span>
plt.figure(figsize=(10,8),dpi=300)
for i in range(20):                              <span class="fm-combinumeral">③</span>
    k=i%5
    ax = plt.subplot(4,5, i + 1)
    plt.imshow(imgs20[i])
    plt.xticks([])
    plt.yticks([])
    plt.tight_layout()
    plt.title(f't={800-200*k}',fontsize=15,c="r")
plt.show()</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Keeps time steps 800, 600, 400, 200, and 0</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Selects 4 sets of flowers out of 10</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Plots the 20 images in a 4 <span class="cambria">×</span> 5 grid</p>
<p class="body">The list, imgs, contains 10 sets of images in all 50 inference steps, <span class="times">t = 980, 960, . . . , 20, 0</span>. So there are a total of 500 images in the list. We select five time steps <span class="times">(t = 800, 600, 400, 200, and 0)</span> for four different flowers (the <span class="times">2<sup class="fm-superscript">nd</sup>, 4<sup class="fm-superscript">th</sup>, 7<sup class="fm-superscript">th</sup>, and 10<sup class="fm-superscript">th</sup></span> images in figure 15.6). We then plot the 20 images in a <span class="times">4 <span class="cambria">×</span> 5</span> grid, as shown in figure 15.7.<a id="marker-359"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="652" src="../../OEBPS/Images/CH15_F07_Liu.png" width="773"/></p>
<p class="figurecaption">Figure 15.7 How the trained denoising U-Net model gradually converts random noise into clean flower images. We feed random noise to the trained model to obtain the image at time step 980. We then feed the noisy image at <span class="times">t = 980</span> to the model to obtain the image at <span class="times">t = 960</span>. We repeat this process 50 inference steps until we obtain the image at <span class="times">t = 0</span>. The first column in this figure shows the four flowers at <span class="times">t = 800</span>; the second column shows the same four flowers at <span class="times">t = 600 . . . ;</span> the last column shows the four flowers at <span class="times">t = 0</span> (i.e., clean flower images).</p>
</div>
<p class="body">The first column in figure 15.7 shows the four flower images at <span class="times">t = 800</span>. They are close to random noise. The second column shows the flowers at <span class="times">t = 600</span>, and they start to look like flowers. As we move to the right, the images become clearer and clearer. The rightmost column shows the four clean flower images at <span class="times">t = 0</span>.</p>
<p class="body">Now that you understand how diffusion models work, we’ll discuss text-to-image generation. The image generation process of text-to-image Transformers such as DALL-E 2, Imagen, and Stable Diffusion is very much like the reverse diffusion process we discussed earlier in the chapter, except that the model takes the text embedding as a conditioning signal when generating an image. <a id="idIndexMarker066"/><a id="idIndexMarker067"/><a id="idIndexMarker068"/><a id="idIndexMarker069"/></p>
<h2 class="fm-head" id="heading_id_16">15.5 Text-to-image Transformers</h2>
<p class="body"><a id="marker-360"/>Text-to-image Transformers such as OpenAI’s DALL-E 2, Google’s Imagen, and Stability AI’s Stable Diffusion use diffusion models to generate images from textual descriptions. An important component of these text-to-image Transformers is a diffusion model. The process of text-to-image generation involves encoding the text input into a latent representation, which is then used as a conditioning signal for the diffusion model. These Transformers learn to generate lifelike images that correspond to the textual description by iteratively denoising a random noise vector, guided by the encoded text.<a id="idIndexMarker070"/><a id="idIndexMarker071"/></p>
<p class="body">The key to all these text-to-image Transformers is a model to understand content in different modalities. In this case, the model must understand the text descriptions and link them to images and vice versa.</p>
<p class="body">In this section, we’ll use OpenAI’s CLIP model as an example. CLIP is a key component in DALL-E 2. We’ll discuss how CLIP was trained to understand the connection between text descriptions and images. We then use a short Python program to generate an image from a text prompt by using OpenAI’s DALL-E 2.</p>
<h3 class="fm-head1" id="heading_id_17">15.5.1 CLIP: A multimodal Transformer</h3>
<p class="body">In recent years, the intersection of computer vision and natural language processing (NLP) has witnessed significant advancements, one of which is the creation of the CLIP model by OpenAI. This innovative model is designed to understand and interpret images in the context of natural language, a capability that holds immense potential for various applications such as image generation and image classification. <a id="idIndexMarker072"/><a id="idIndexMarker073"/><a id="idIndexMarker074"/></p>
<p class="body">The CLIP model is a multimodal Transformer that bridges the gap between visual and textual data. It is trained to understand images by associating them with corresponding textual descriptions. Unlike traditional models that require explicit labeling of images, CLIP uses a vast dataset of images and their natural language descriptions to learn a more generalizable representation of visual concepts.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="401" src="../../OEBPS/Images/CH15_F08_Liu.png" width="626"/></p>
<p class="figurecaption">Figure 15.8 How OpenAI’s CLIP model is trained. A large-scale training dataset of text–image pairs is collected. The text encoder of the model compresses the text description into a D-value text embedding. The image encoder converts the corresponding image into an image embedding also with D values. During training, a batch of N text–image pairs are converted to N text embeddings and N image embeddings. CLIP uses a contrastive learning approach to maximize the similarity between paired embeddings (the sum of diagonal values in the figure) while minimizing the similarity between embeddings from nonmatching text–image pairs (the sum of off-diagonal values in the figure).</p>
</div>
<p class="body"><a id="marker-361"/>The training of the CLIP model, which is illustrated in figure 15.8, begins with the collection of a large-scale dataset comprising images and their associated textual descriptions. OpenAI utilizes a diverse set of sources, including publicly available datasets and web-crawled data, to ensure a wide variety of visual and textual content. The dataset is then preprocessed to standardize the images so they all have the same shape and to tokenize the text, preparing them for input into the model.</p>
<p class="body">CLIP employs a dual-encoder architecture, consisting of an image encoder and a text encoder. The image encoder processes the input images while the text encoder processes the corresponding textual descriptions. These encoders project the images and text into a shared embedding space where they can be compared and aligned.</p>
<p class="body">The core of CLIP’s training lies in its contrastive learning approach. For each batch of N image–text pairs in the dataset, the model aims to maximize the similarity between paired embeddings (measured by the sum of diagonal values in figure 15.8) while minimizing the similarity between embeddings from nonmatching text-image pairs (the sum of off-diagonal values). Figure 15.9 is a diagram of how text-to-image Transformers such as DALL-E 2 generate realistic images based on text prompts.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="403" src="../../OEBPS/Images/CH15_F09_Liu.png" width="636"/></p>
<p class="figurecaption">Figure 15.9 How text-to-image Transformers such as DALL-E 2 create images based on text prompts. The text encoder in the trained text-to-image Transformer first converts the text description in the prompt into text embedding. The text embedding is fed to the CLIP model to obtain a prior vector that represents the image in the latent space. The text embedding and the prior are concatenated into a conditioning vector. To generate an image, the U-Net denoiser first takes a random noise vector as input to generate a noisy image using the conditioning vector. It then takes the noisy image and the conditioning vector as input and generates another image, which is less noisy. The process is repeated for many iterations until the final output, a clean image, is generated.</p>
</div>
<p class="body">The image generation process of text-to-image Transformers is similar to the reverse diffusion process we discussed earlier in the chapter. Let’s take DALL-E 2, for example, which was proposed by OpenAI researchers in 2022.<sup class="footnotenumber" id="footnote-000-backlink"><a class="url1" href="#footnote-000">4</a></sup> The text encoder in the model first converts the text description in the prompt into a text embedding. The text embedding is fed to the CLIP model to obtain a prior vector that represents the image in the latent space. The text embedding and the prior are concatenated into a conditioning vector. In the first iteration, we feed a random noise vector to the U-Net denoiser in the model and ask it to generate a noisy image based on the conditioning vector. In the second iteration, we feed the noisy image from the previous iteration to the U-Net denoiser and ask it to generate another noisy image based on the conditioning vector. We repeat this process for many iterations, and the final output is a clean image.<a id="marker-362"/></p>
<h3 class="fm-head1" id="heading_id_18">15.5.2 Text-to-image generation with DALL-E 2</h3>
<p class="body">Now that you understand how text-to-image Transformers work, let’s write a Python program to interact with DALL-E 2 to create an image based on a text prompt. <a id="idIndexMarker075"/><a id="idIndexMarker076"/><a id="idIndexMarker077"/></p>
<p class="body">First, you need to apply for an OpenAI API key. OpenAI offers various pricing tiers that vary based on the number of tokens processed and the type of models used. Go to <a class="url" href="https://chat.openai.com/auth/login">https://chat.openai.com/auth/login</a> and click on the Sign up button to create an account. After that, log in to your account, and go to <a class="url" href="https://platform.openai.com/api-keys">https://platform.openai.com/api-keys</a> to view your API key. Save it in a secure place for later use. We can generate an image by using OpenAI’s DALL-E 2.</p>
<p class="fm-code-listing-caption">Listing 15.10 Image generation with DALL-E 2</p>
<pre class="programlisting">from openai import OpenAI
  
openai_api_key=your actual OpenAI API key here, in quotes   <span class="fm-combinumeral">①</span>
client=OpenAI(api_key=openai_api_key)                       <span class="fm-combinumeral">②</span>
  
response = client.images.generate(
  model="dall-e-2",
  prompt="an astronaut in a space suit riding a unicorn",
  size="512x512",
  quality="standard",
  n=1,
)                                                           <span class="fm-combinumeral">③</span>
image_url = response.data[0].url
print(image_url)                                            <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Makes sure you provide your actual OpenAI API key here, in quotes</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Instantiates the OpenAI() class to create an agent</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Uses the images.generate() method to generate image based on the text prompt</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Prints out the image URL</p>
<p class="body"><a id="marker-363"/>You should place the OpenAI API key you obtained earlier in listing 15.10. We create an agent by instantiating the <code class="fm-code-in-text">OpenAI()</code> class. To generate an image, we need to specify the model, a text prompt, and the size of the image. We have used “an astronaut in a space suit riding a unicorn” as the prompt, and the code listing provides a URL for us to visualize and download the image. The URL expires in an hour, and the resulting image is shown in figure 15.10. <a id="idIndexMarker078"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="450" src="../../OEBPS/Images/CH15_F10_Liu.png" width="450"/></p>
<p class="figurecaption">Figure 15.10 An image generated by DALL-E 2 with the text prompt “an astronaut in a space suit riding a unicorn”</p>
</div>
<p class="body">Run listing 15.10 yourself and see what image DALLE-2 generates for you. Note that your result will be different since the output from DALLE-2 (and all LLMs) is stochastic rather than deterministic.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 15.2</p>
<p class="fm-sidebar-text">Apply for an OpenAI API key. Then modify code listing 15.10 to generate an image using the text prompt “a cat in a suit working on a computer.”</p>
</div>
<p class="body">In this chapter, you learned the inner workings of diffusion-based models and their significance in text-to-image Transformers, such as OpenAI’s CLIP model. You also discovered how to obtain your OpenAI API key and used a brief Python script to generate images from text descriptions with DALL-E 2, which incorporates CLIP.</p>
<p class="body">In the next chapter, you will continue to use the OpenAI API key obtained earlier to use pretrained LLMs for generating diverse content, including text, audio, and images. Additionally, you will integrate the LangChain Python library with other APIs, enabling you to create a know-it-all personal assistant. <a id="idIndexMarker079"/><a id="marker-364"/><a id="idIndexMarker080"/></p>
<h2 class="fm-head" id="heading_id_19">Summary</h2>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">In forward diffusion, we gradually add small amounts of random noise to clean images until they transform into pure noise. Conversely, in reverse diffusion, we begin with random noise and employ a denoising model to progressively eliminate noise from the images, transforming the noise back into a clean image.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The U-Net architecture, originally designed for biomedical image segmentation, has a symmetric shape with a contracting encoder path and an expansive decoder path, connected by a bottleneck layer. In denoising, U-Nets are adapted to remove noise while preserving details. Skip connections link encoder and decoder feature maps of the same spatial dimensions, helping to preserve spatial information like edges and textures that may be lost during downsampling in the encoding process.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Incorporating an attention mechanism into a denoising U-Net model enables it to concentrate on important features and disregard irrelevant ones. By treating image pixels as a sequence, the attention mechanism learns pixel dependencies, similar to how it learns token dependencies in NLP. This enhances the model’s ability to identify relevant features effectively.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Text-to-image Transformers like OpenAI’s DALL-E 2, Google’s Imagen, and Stability AI’s Stable Diffusion use diffusion models to create images from textual descriptions. They encode the text into a latent representation that conditions the diffusion model, which then iteratively denoises a random noise vector, guided by the encoded text, to generate lifelike images matching the textual description.</p>
</li>
</ul>
<hr class="calibre6"/>
<p class="fm-footnote"><a id="footnote-003"/><sup class="footnotenumber1"><a class="url1" href="#footnote-003-backlink">1</a></sup>  Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli, 2015, “Deep Unsupervised Learning Using Nonequilibrium Thermodynamics.” International Conference on Machine Learning, <a class="url" href="http://arxiv.org/abs/1503.03585">http://arxiv.org/abs/1503.03585</a>.</p>
<p class="fm-footnote"><a id="footnote-002"/><sup class="footnotenumber1"><a class="url1" href="#footnote-002-backlink">2</a></sup>  Sohl-Dickstein et al., 2015, “Deep Unsupervised Learning Using Nonequilibrium Thermodynamics,” h<a class="url" href="https://arxiv.org/abs/1503.03585">ttps://arxiv.org/abs/1503.03585</a>. Yang Song and Stefano Ermon, 2019, “Generative Modeling by Estimating Gradients of the Data Distribution.” <a class="url" href="https://arxiv.org/abs/1907.05600">https://arxiv.org/abs/1907.05600</a>. Jonathan Ho, Ajay Jain, and Pieter Abbeel, 2020, “Denoising Diffusion Probabilistic Models,” <a class="url" href="https://arxiv.org/abs/2006.11239">https://arxiv.org/abs/2006.11239</a>.</p>
<p class="fm-footnote"><a id="footnote-001"/><sup class="footnotenumber1"><a class="url1" href="#footnote-001-backlink">3</a></sup>  Ilya Loshchilov and Frank Hutter, 2017, “Decoupled Weight Decay Regularization.” <a class="url" href="https://arxiv.org/abs/1711.05101">https://arxiv.org/abs/1711.05101</a>.</p>
<p class="fm-footnote"><a id="footnote-000"/><sup class="footnotenumber1"><a class="url1" href="#footnote-000-backlink">4</a></sup>  Aditya Rames, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen, 2022, “Hierarchical Text-Conditional Image Generation with CLIP Latents.” <a class="url" href="https://arxiv.org/abs/2204.06125">https://arxiv.org/abs/2204.06125</a>.</p>
</div></body></html>