- en: '3 Generative adversarial networks: Shape and number generation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Building generator and discriminator networks in generative adversarial networks
    from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using GANs to generate data points to form shapes (e.g., exponential growth
    curve)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating integer sequences that are all multiples of 5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training, saving, loading, and using GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating GAN performance and determining training stop points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Close to half of the generative models in this book belong to a category called
    generative adversarial networks (GANs). The method was first proposed by Ian Goodfellow
    and his coauthors in 2014.^([1](#footnote-001)) GANs, celebrated for their ease
    of implementation and versatility, empower individuals with even rudimentary knowledge
    of deep learning to construct their models from the ground up. The word “adversarial”
    in GAN refers to the fact that the two neural networks compete against each other
    in a zero-sum game framework. The generative network tries to create data instances
    indistinguishable from real samples. In contrast, the discriminative network tries
    to identify the generated samples from real ones. These versatile models can generate
    various content formats, from geometric shapes and sequences of numbers to high-resolution
    color images and even realistic-sounding musical compositions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll briefly review the theory behind GANs. Then, I’ll show
    you how to implement that knowledge in PyTorch. You’ll learn to build your first
    GAN from scratch so that all the details are demystified. To make the example
    relatable, imagine you put $1 in a savings account that pays 8% a year. You want
    to find out the balance in your account based on the number of years you have
    invested. The true relation is an exponential growth curve. You’ll learn to use
    GANs to generate data samples—pairs of values (x, y) that form such an exponential
    growth curve, with a mathematical relation y = 1.08^x. Armed with this skill,
    you’ll be able to generate data to mimic any shape: sine, cosine, quadratic, and
    so on.'
  prefs: []
  type: TYPE_NORMAL
- en: In the second project in this chapter, you’ll learn how to use GANs to generate
    a sequence of numbers that are all multiples of 5\. But you can change the pattern
    to multiples of 2, 3, 7, or other patterns. Along the way, you’ll learn how to
    create a generator network and a discriminator network from scratch. You’ll learn
    how to train, save, and use GANs. Further, you’ll also learn to assess the performance
    of GANs either by visualizing samples generated by the generator network or by
    measuring the divergence between the generated sample distribution and the real
    data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that you need data to train a machine learning (ML) model to predict
    the relation between pairs of values (x, y). However, the training dataset is
    costly and time-consuming for human beings to prepare by hand. GANs can be well-suited
    to generate data in such cases: while the generated values of x and y generally
    conform to a mathematical relation, there is also noise in the generated data.
    The noise can be useful for preventing overfitting when the generated data is
    used to train the ML model.'
  prefs: []
  type: TYPE_NORMAL
- en: The primary goal of this chapter is not necessarily to generate novel content
    with the most practical use. Instead, my objective is to teach you how to train
    and use GANs to create various formats of content from scratch. Along the way,
    you will gain a solid understanding of the inner workings of GANs. This foundation
    will allow us to concentrate on other, more advanced, aspects of GANs in later
    chapters when generating other content such as high-resolution images or realistic-sounding
    music (e.g., convolutional neural networks or how to represent a piece of music
    as a multidimensional object).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Steps involved in training GANs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In chapter 1, you gained a high-level overview of the theories behind GANs.
    In this section, I’ll provide a summary of the steps involved in training GANs
    in general and in creating data points to form an exponential growth curve in
    particular.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s return to our previous example: you plan to invest in a savings account
    that pays 8% annual interest. You put $1 in the account today and want to know
    how much money you’ll have in the account in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: The amount in your account in the future, y, depends on how long you invest
    in the savings account. Let’s denote the number of years you invest by x, which
    can be a number, say, between 0 and 50\. For example, if you invest for 1 year,
    the balance is $1.08; if you invest for 2 years, the balance is 1.08² = $1.17.
    To generalize, the relationship between x and y is y = 1.08^x. The function depicts
    an exponential growth curve. Note here that x can be a whole number such as 1
    or 2, as well as a decimal number such as 1.14 or 2.35 and the formula still works.
  prefs: []
  type: TYPE_NORMAL
- en: Training GANs to generate data points that conform to a specific mathematical
    relation, like the preceding example, is a multistep process. In your case, you
    want to generate data points (x, y) such that y = 1.08^x. Figure 3.1 provides
    a diagram of the architecture of GANs and the steps involved in generating an
    exponential growth curve. When you generate other content such as a sequence of
    integers, images, or music, you follow similar steps, as you’ll see in the second
    project in this chapter, as well as in other GAN models later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH03_F01_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 The steps involved in training GANs to generate an exponential growth
    curve and the dual-network architecture in GANs. The generator obtains a random
    noise vector Z from the latent space (top left) to create a fake sample and presents
    it to the discriminator (middle). The discriminator classifies a sample as real
    (from the training set) or fake (created by the generator). The predictions are
    compared to the ground truth and both the discriminator and the generator learn
    from the predictions. After many iterations of training, the generator learns
    to create shapes that are indistinguishable from real samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start, we need to obtain a training dataset to train GANs. In our
    running example, we’ll generate a dataset of (x, y) pairs using the mathematical
    relation y = 1.08^x. We use the savings account example so that the numbers are
    relatable. The techniques you learn in this chapter can be applied to other shapes:
    sine, cosine, U-shape, and so on. You can choose a range of x values (say, 0 to
    50) and calculate the corresponding y values. Since we usually train models in
    batches of data in deep learning, the number of observations in your training
    dataset is usually set to a multiple of the batch size. A real sample is located
    at the top of figure 3.1, which has an exponential growth curve shape.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have the training set ready, you need to create two networks in GANs:
    a generator and a discriminator. The generator, located at the bottom left of
    figure 3.1, takes a random noise vector Z as the input and generates data points
    (step 1 of our training loop). The random noise vector Z used by the generator
    is obtained from the latent space, which represents the range of possible outputs
    the GAN can produce and is central to the GAN’s ability to generate diverse data
    samples. In chapter 5, we’ll explore the latent space to select the attributes
    of the content created by the generator. The discriminator, located at the center
    of figure 3.1, evaluates whether a given data point (x, y) is real (from the training
    dataset) or fake (created by the generator); this is step 2 of our training loop.'
  prefs: []
  type: TYPE_NORMAL
- en: The meaning of the latent space
  prefs: []
  type: TYPE_NORMAL
- en: The latent space in a GAN is a conceptual space where each point can be transformed
    into a realistic data instance by the generator. This space represents the range
    of possible outputs the GAN can produce and is central to the GAN’s ability to
    generate varied and complex data. The latent space acquires its significance exclusively
    when it is employed in conjunction with the generative model. Within this context,
    one can interpolate between points in the latent space to affect the attributes
    of output, which we’ll discuss in chapter 5.
  prefs: []
  type: TYPE_NORMAL
- en: To know how to adjust model parameters, we must choose the right loss functions.
    We need to define the loss functions for both the generator and discriminator.
    The loss function encourages the generator to generate data points that resemble
    data points from the training dataset, making the discriminator classify them
    as real. The loss function encourages the discriminator to correctly classify
    real and generated data points.
  prefs: []
  type: TYPE_NORMAL
- en: In each iteration of the training loop, we alternate between training the discriminator
    and the generator. During each training iteration, we sample a batch of real (x,
    y) data points from the training dataset and a batch of fake data points generated
    by the generator. When training the discriminator, we compare the predictions
    by the discriminative model, which is a probability that the sample is from the
    training set, with the ground truth, which is 1 if the sample is real and 0 if
    the sample is fake (shown at the right of figure 3.1); this constitutes half of
    step 3 in the training loop. We adjust the weights in the discriminator network
    slightly so that in the next iteration, the predicted probability moves closer
    to the ground truth (half of step 4 in our training loop).
  prefs: []
  type: TYPE_NORMAL
- en: When training the generator, we feed fake samples to the discriminative model
    and obtain a probability that the sample is real (the other half of step 3). We
    then adjust the weights in the generator network slightly so that in the next
    iteration, the predicted probability moves closer to 1 (since the generator wants
    to create samples to fool the discriminator into thinking they are real); this
    constitutes the other half of step 4\. We repeat this process for many iterations,
    making the generator network create more realistic data points.
  prefs: []
  type: TYPE_NORMAL
- en: A natural question is when to stop training the GANs. For that, you evaluate
    the GAN’s performance by generating a set of synthetic data points and comparing
    them to the real data points from the training dataset. In most cases, we use
    visualization techniques to assess how well the generated data conforms to the
    desired relation. However, in our running example, since we know the distribution
    of the training data, we can calculate the mean squared error (MSE) between the
    generated data and the true data distribution. We stop training GANs when the
    generated samples stop improving their qualities after a fixed number of rounds
    of training.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the model is considered trained. We then discard the discriminator
    and keep the generator. To create an exponential growth curve, we feed a random
    noise vector Z to the trained generator and obtain pairs of (x, y) to form the
    desired shape.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Preparing training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you’ll create the training dataset so that you can use it to
    train the GAN model later in this chapter. Specifically, you’ll create pairs of
    data points (x, y) that conform to the exponential growth shape. You’ll place
    them in batches so that they are ready to be fed to deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE The code for this chapter, as well as other chapters in this book, is
    available at the book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 A training dataset that forms an exponential growth curve
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll create a dataset that contains many observations of data pairs, (x, y),
    where x is uniformly distributed in the interval [0, 50] and y is related to x
    based on the formula y = 1.08^x, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.1 Creating training data to form an exponential growth shape
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ① Fixes the random state so results are reproducible
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a tensor with 2,048 rows and 2 columns
  prefs: []
  type: TYPE_NORMAL
- en: ③ Generates values of x between 0 and 50
  prefs: []
  type: TYPE_NORMAL
- en: ④ Generates values of y based on the relation y = 1.08^x
  prefs: []
  type: TYPE_NORMAL
- en: First, we create 2,048 values of x between 0 and 50 using the `torch.rand()`
    method. We use the `manual_seed()` method in PyTorch to fix the random state so
    that all results are reproducible. We first create a PyTorch tensor, `train_data`,
    with 2,048 rows and 2 columns. The values of x are placed in the first column
    in the tensor `train_data`. The `rand()` method in PyTorch generates random values
    between 0.0 and 1.0\. By multiplying the value by 50, the resulting values of
    x are between 0.0 and 50.0\. We then fill the second column of `train_data` with
    values of y = 1.08^x.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3.1
  prefs: []
  type: TYPE_NORMAL
- en: 'Modify listing 3.1 so that the relation between x and y is y = sin(x) by using
    the `torch.sin()` function. Set the value of x between –5 and 5 by using this
    line of code: `train_data[:,0]=10*(torch.rand(observations)-0.5)`.'
  prefs: []
  type: TYPE_NORMAL
- en: We plot the relation between x and y by using the Matplotlib library.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.2 Visualizing the relation between x and y
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ① Plots the relation between x and y
  prefs: []
  type: TYPE_NORMAL
- en: ② Labels y-axis
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates a title for the plot
  prefs: []
  type: TYPE_NORMAL
- en: You will see an exponential growth curve shape after running listing 3.2, which
    is similar to the top graph in figure 3.1.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3.2
  prefs: []
  type: TYPE_NORMAL
- en: Modify listing 3.2 to plot the relation between x and y = sin(x) based on your
    changes in exercise 3.1\. Make sure you change the y-axis label and the title
    in the plot to reflect the changes you made.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Preparing the training dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll place the data samples you just created into batches so that we can feed
    them to the discriminator network. We use the `DataLoader()` class in PyTorch
    to wrap an iterable around the training dataset so that we can easily access the
    samples during training, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Make sure you select the total number of observations and the batch size so
    that all batches have the same number of samples in them. We chose 2,048 observations
    with a batch size of 128\. As a result, we have 2,048/128 = 16 batches. The `shuffle=True`
    argument in `DataLoader()` shuffles the observations randomly before dividing
    them into batches.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Shuffling makes sure that the data samples are evenly distributed and observations
    within a batch are not correlated, which, in turn, stabilizes training. In this
    specific example, shuffling ensures that values of x fall randomly between 0 and
    50, instead of clustering in a certain range, say, between 0 and 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can access a batch of data by using the `next()` and `iter()` methods,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You will see 128 pairs of numbers (x, y), where the value of x falls randomly
    between 0 and 50\. Further, the values of x and y in each pair conform to the
    relation y = 1.08^x.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Creating GANs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that the training dataset is ready, we’ll create a discriminator network
    and a generator network. The discriminator network is a binary classifier, which
    is very similar to the binary classifier for clothing items we have created and
    trained in chapter 2\. Here, the discriminator’s job is to classify the samples
    into either real or fake. The generator network, on the other hand, tries to create
    data points (x, y) that are indistinguishable from those in the training set so
    that the discriminator will classify them as real.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 The discriminator network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use PyTorch to create a discriminator neural network. We’ll use fully connected
    (dense) layers with `ReLU` activations. We’ll also use dropout layers to prevent
    overfitting. We create a sequential deep neural network in PyTorch to represent
    the discriminator, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.3 Creating a discriminator network
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ① Automatically checks if CUDA-enabled GPU is available.
  prefs: []
  type: TYPE_NORMAL
- en: ② The number of input features in the first layer is 2, matching the number
    of elements in each data instance, which has two values, x and y .
  prefs: []
  type: TYPE_NORMAL
- en: ③ The dropout layer prevents overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: ④ The number of output features in the last layer is 1 so that we can squeeze
    it into a value between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that in the first layer, the input shape is 2 because, in our sample,
    each data instance has two values in it: x and y . The number of inputs in the
    first layer should always match with the size of the input data. Also, make sure
    that the number of output features is 1 in the last layer: the output of the discriminator
    network is a single value. We use the sigmoid activation function to squeeze the
    output to the range [0, 1] so that it can be interpreted as the probability, p,
    that the sample is real. With the complementary probability, 1 – p, the sample
    is fake. This is very similar to what we have done in chapter 2 when a binary
    classifier attempts to identify a piece of clothing item as either an ankle boot
    or a t-shirt.'
  prefs: []
  type: TYPE_NORMAL
- en: The hidden layers have 256, 128, and 64 neurons in them, respectively. There
    is nothing magical about these numbers, and you can easily change them and have
    similar results as long as they are in a reasonable range. If the number of neurons
    in hidden layers is too large, it may lead to overfitting of the model; if the
    number is too small, it may lead to underfitting. The number of neurons can be
    optimized separately using a validation set through hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout layers randomly deactivate (or “drop out”) a certain percentage of neurons
    in the layer to which they are applied. This means that these neurons do not participate
    in forward or backward passes during training. Overfitting occurs when a model
    learns not only the underlying patterns in the training data but also the noise
    and random fluctuations, leading to poor performance on unseen data. Dropout layers
    are an effective way to prevent overfitting.^([2](#footnote-000))
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 The generator network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The generator’s job is to create a pair of numbers (x, y) so that it can pass
    the screening of the discriminator. That is, the generator is trying to create
    a pair of numbers to maximize the probability that the discriminator thinks that
    the numbers are from the training dataset (i.e., they conform to the relation
    y = 1.08^x). We create the neural network in the following listing to represent
    the generator.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.4 Creating a generator network
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ① The number of input features in the first layer is 2, the same as the dimension
    of the random noise vector from the latent space.
  prefs: []
  type: TYPE_NORMAL
- en: ② The number of output features in the last layer is 2, the same as the dimension
    of the data sample, which contains two values (x, y).
  prefs: []
  type: TYPE_NORMAL
- en: We feed a random noise vector from a 2D latent space, (z[1], z[2]), to the generator.
    The generator then generates a pair of values (x, y), based on the input from
    the latent space. Here we use a 2D latent space, but changing the dimension to
    other numbers such as 5 or 10 wouldn’t affect our results.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3 Loss functions, optimizers, and early stopping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since the discriminator network is essentially performing a binary classification
    task (identifying a data sample as real or fake), we use binary cross-entropy
    loss, the preferred loss function in binary classifications, for the discriminator
    network. The discriminator is trying to maximize the accuracy of the binary classification:
    identify a real sample as real and a fake sample as fake. The weights in the discriminator
    network are updated based on the gradient of the loss function with respect to
    the weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The generator is trying to minimize the probability that the fake sample is
    being identified as fake. Therefore, we’ll also use binary cross-entropy loss
    for the generator network: the generator updates its network weights so that the
    generated samples will be classified as real by the discriminator in a binary
    classification problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have done in chapter 2, we use the Adam optimizer as the gradient descent
    algorithm. We set the learning rate to 0.0005\. Let’s code those steps in by using
    PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'One question remains before we get to the actual training: How many epochs
    should we train the GANs? How do we know the model is well trained so that the
    generator is ready to create samples that can mimic the exponential growth curve
    shape? If you recall, in chapter 2, we split the training set further into a train
    set and a validation set. We then used the loss in the validation set to determine
    whether the parameters had converged so that we could stop training. However,
    GANs are trained using a different approach compared to traditional supervised
    learning models (such as the classification models you have seen in chapter 2).
    Since the quality of the generated samples improves throughout training, the discriminator’s
    task becomes more and more difficult (in a way, the discriminator in GANs is making
    predictions on a moving target). The loss from the discriminator network is not
    a good indicator of the quality of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One common method to measure the performance of GANs is through visual inspection.
    Humans can assess the quality and realism of generated data instances by simply
    looking at them. This is a qualitative approach but can be very informative. But
    in our simple case, since we know the exact distribution of the training dataset,
    we’ll look at the MSE of the generated samples relative to samples in the training
    set and use it as a measure of the performance of the generator. Let’s code that
    in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ① Uses MSE as the criterion to measure performance
  prefs: []
  type: TYPE_NORMAL
- en: ② Finds out the true distribution
  prefs: []
  type: TYPE_NORMAL
- en: ③ Compares the generated distribution with the true distribution and calculates
    MSE
  prefs: []
  type: TYPE_NORMAL
- en: We’ll stop training the model if the performance of the generator doesn’t improve
    in, say, 1,000 epochs. Therefore, we define an early stopping class, as we did
    in chapter 2, to decide when to stop training the model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.5 An early stopping class to decide when to stop training
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: ① Sets the default value of patience to 1000
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines the stop() method
  prefs: []
  type: TYPE_NORMAL
- en: ③ If a new minimum difference between the generated distribution and true distribution
    is reached, updates the value of min_gdif.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Stops training if the model stops improving for 1,000 epochs
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have all the components we need to train our GANs, which we’ll
    do in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Training and using GANs for shape generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the training data and two networks, we’ll train the model.
    After that, we’ll discard the discriminator and use the generator to generate
    data points to form an exponential growth curve shape.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 The training of GANs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We first create labels for real samples and fake samples, respectively. Specifically,
    we’ll label all real samples as 1s and all fake samples as 0s. During the training
    process, the discriminator compares its own predictions with the labels to receive
    feedback so that it can adjust model parameters to make better predictions in
    the next iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we define two tensors, `real_labels` and `fake_labels`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The tensor `real_labels` is 2D with a shape of `(batch_size, 1)`—that is, 128
    rows and 1 column. We use 128 rows because we’ll feed a batch of 128 real samples
    to the discriminator network to obtain 128 predictions. Similarly, the tensor
    `fake_labels` is 2D with a shape of `(batch_size, 1)`. We’ll feed a batch of 128
    fake samples to the discriminator network to obtain 128 predictions and compare
    them with the ground truth: 128 labels of 0s. We move the two tensors to the GPU
    for fast training if your computer has a CUDA-enabled GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: To train the GANs, we define a few functions so that the training loop looks
    organized. The first function, `train_D_on_real()`, trains the discriminator network
    with a batch of real samples.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.6 Defining a `train_D_on_real()` function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ① Makes predictions on real samples
  prefs: []
  type: TYPE_NORMAL
- en: ② Calculates loss
  prefs: []
  type: TYPE_NORMAL
- en: ③ Backpropagation (i.e., updates model weights in the discriminator network
    so predictions are more accurate in the next iteration)
  prefs: []
  type: TYPE_NORMAL
- en: The function `train_D_on_real()` first moves the real samples to GPU if the
    computer has a CUDA-enabled GPU. The discriminator network, D, makes predictions
    on the batch of samples. The model then compares the discriminator’s predictions,
    `out_D`, with the ground truth, `real_labels`, and calculates the loss of the
    predictions accordingly. The `backward()` method calculates the gradients of the
    loss function with respect to model parameters. The `step()` method adjusts the
    model parameters (that is, backpropagation). The `zero_grad()` method means that
    we explicitly set the gradients to 0 before backpropagation. Otherwise, the accumulated
    gradients instead of the incremental gradients are used on every `backward()`
    call.
  prefs: []
  type: TYPE_NORMAL
- en: TIP We call the `zero_grad()` method before updating model weights when training
    each batch of data. We explicitly set the gradients to 0 before backpropagation
    to use incremental gradients instead of the accumulated gradients on every `backward()`
    call.
  prefs: []
  type: TYPE_NORMAL
- en: The second function, `train_D_on_fake()`, trains the discriminator network with
    a batch of fake samples.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.7 Defining the `train_D_on_fake()` function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ① Generates a batch of fake samples
  prefs: []
  type: TYPE_NORMAL
- en: ② Makes predictions on the fake samples
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calculates loss
  prefs: []
  type: TYPE_NORMAL
- en: ④ Backpropagation
  prefs: []
  type: TYPE_NORMAL
- en: The function `train_D_on_fake()` first feeds a batch of random noise vectors
    from the latent space to the generator to obtain a batch of fake samples. The
    function then presents the fake samples to the discriminator to obtain predictions.
    The function compares the discriminator’s predictions, `out_D`, with the ground
    truth, `fake_labels`, and calculates the loss of the predictions accordingly.
    Finally, it adjusts the model parameters based on the gradients of the loss function
    with respect to model weights.
  prefs: []
  type: TYPE_NORMAL
- en: Note We use the terms *weights* and *parameters* interchangeably. Strictly speaking,
    model parameters also include bias terms, but we use the term *model weights*
    loosely to include model biases. Similarly, we use the terms *adjusting weights*,
    *adjusting parameters*, and *backpropagation* interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: The third function, `train_G()`, trains the generator network with a batch of
    fake samples.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.8 Defining the `train_G()` function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a batch of fake samples
  prefs: []
  type: TYPE_NORMAL
- en: ② Presents the fake samples to the discriminator to obtain predictions
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calculates the loss based on whether G has succeeded
  prefs: []
  type: TYPE_NORMAL
- en: ④ Backpropagation (i.e., updates weights in the generator network so the generated
    samples are more realistic in the next iteration)
  prefs: []
  type: TYPE_NORMAL
- en: To train the generator, we first feed a batch of random noise vectors from the
    latent space to the generator to obtain a batch of fake samples. We then present
    the fake samples to the discriminator network to obtain a batch of predictions.
    We compare the discriminator’s predictions with `real_labels`, a tensor of 1s,
    and calculate the loss. It’s important that we use a tensor of 1s, not a tensor
    of 0s, as the labels, because the objective of the generator is to fool the discriminator
    into thinking that fake samples are real. Finally, we adjust the model parameters
    based on the gradients of the loss function with respect to model weights so that
    in the next iteration, the generator can create more realistic samples.
  prefs: []
  type: TYPE_NORMAL
- en: Note We use the tensor `real_labels` (a tensor of 1s) instead of `fake_labels`
    (a tensor of 0s) when calculating loss and assessing the generator network because
    the generator wants the discriminator to predict fake samples as real.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we define a function, `test_epoch()`, which prints out the losses for
    the discriminator and the generator periodically. Further, it plots the data points
    generated by the generator and compares them to those in the training set. The
    function `test_epoch()` is shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.9 Defining the `test_epoch()` function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a folder to hold files
  prefs: []
  type: TYPE_NORMAL
- en: ② Periodically prints out losses
  prefs: []
  type: TYPE_NORMAL
- en: ③ Plots the generated points as asterisks (*)
  prefs: []
  type: TYPE_NORMAL
- en: ④ Plots training data as dots (.)
  prefs: []
  type: TYPE_NORMAL
- en: After every 25 epochs, the function prints out the average losses for the generator
    and the discriminator in the epoch. Further, it plots a batch of fake data points
    generated by the generator (in asterisks) and compares them to the data points
    in the training set (in dots). The plot is saved as an image in your local folder
    /files/.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to train the model. We iterate through all batches in the training
    dataset. For each batch of data, we first train the discriminator using the real
    samples. After that, the generator creates a batch of fake samples, and we use
    them to train the discriminator again. Finally, we let the generator create a
    batch of fake samples again, but this time, we use them to train the generator
    instead. We train the model until the early stopping condition is satisfied, as
    shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.10 Training GANs to generate an exponential growth curve
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: ① Starts training loops
  prefs: []
  type: TYPE_NORMAL
- en: ② Iterates through all batches in the training dataset
  prefs: []
  type: TYPE_NORMAL
- en: ③ Shows generated samples periodically
  prefs: []
  type: TYPE_NORMAL
- en: ④ Determines if training should stop
  prefs: []
  type: TYPE_NORMAL
- en: 'The training takes a few minutes if you are using GPU training. Otherwise,
    it may take 20 to 30 minutes, depending on the hardware configuration on your
    computer. Alternatively, you can download the trained model from the book’s GitHub
    repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After 25 epochs of training, the generated data are scattered around the point
    (0,0) and don’t form any meaningful shape (an epoch is when all training data
    is used for training once). After 200 epochs of training, the data points start
    to form an exponential growth curve shape, even though many points are far away
    from the dotted curve, which is formed by points from the training set. After
    1,025 epochs, the generated points fit closely with the exponential growth curve.
    Figure 3.2 provides subplots of the output from six different epochs. Our GANs
    work really well: the generator is able to generate data points to form the desired
    shape.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH03_F02_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 Subplots of the comparison of the generated shape (asterisks in the
    graph) with the true exponential growth curve shape (dots in the graph) at different
    stages of the training process. At epoch 25, the generated samples don’t form
    any meaningful shape. At epoch 200, the samples start to look like an exponential
    growth curve shape. At epoch 1025, the generated samples align closely with the
    exponential growth curve.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Saving and using the trained generator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that the GANs are trained, we’ll discard the discriminator network, as
    we always do in GANs, and save the trained generator network in the local folder,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `torch.jit.script()` method scripts a function or a `nn.Module` class as
    TorchScript code using the TorchScript compiler. We use the method to script our
    trained generator network and save it as a file, `exponential.pt`, on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the generator, we don’t even need to define the model. We simply load
    up the saved file and use it to generate data points as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The trained generator is now loaded to your device, which is either `CPU` or
    `CUDA` depending on if you have a CUDA-enabled GPU on your computer. The `map_location=device`
    argument in `torch.jit.load()` specifies where to load the generator. We can now
    use the trained generator to generate a batch of data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we first obtain a batch of random noise vectors from the latent space.
    We then feed them to the generator to produce the fake data. We can plot the generated
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: ① Plots the generated data samples as asterisks
  prefs: []
  type: TYPE_NORMAL
- en: ② Plots the training data as dots
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see a plot similar to the last subplot in figure 3.2: the generated
    data samples closely resemble an exponential growth curve.'
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have created and trained your very first GANs. Armed with
    this skill, you can easily change the code so that the generated data matches
    other shapes such as sine, cosine, U-shape, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3.3
  prefs: []
  type: TYPE_NORMAL
- en: Modify the programs in the first project so that the generator generates data
    samples to form a sine shape between x = –5 and x = 5. When you plot the data
    samples, set the value of y between –1.2 and 1.2.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Generating numbers with patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this second project, you’ll build and train GANs to generate a sequence of
    10 integers between 0 and 99, all of them multiples of 5\. The main steps involved
    are similar to those to generate an exponential growth curve, with the exception
    that the training set is not data points with two values (x, y). Instead, the
    training dataset is a sequence of integers that are all multiples of 5 between
    0 and 99.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, you’ll first learn to convert the training data into a format
    that neural networks understand: one-hot variables. Further, you’ll convert one-hot
    variables back to an integer between 0 and 99, so it’s easy for human beings to
    understand. Hence you are essentially translating data between human-readable
    and model-ready formats. After that, you’ll create a discriminator and a generator
    and train the GANs. You’ll also use early stopping to determine when the training
    is finished. You then discard the discriminator and use the trained generator
    to create a sequence of integers with the pattern you want.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1 What are one-hot variables?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One-hot encoding is a technique used in ML and data preprocessing to represent
    categorical data as binary vectors. Categorical data consists of categories or
    labels, such as colors, types of animals, or cities, which are not inherently
    numeric. ML algorithms typically work with numerical data, so converting categorical
    data into a numerical format is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you are working with a categorical feature—for example, the color of
    a house that can take values “red,” “green,” and “blue.” With one-hot encoding,
    each category is represented as a binary vector. You’ll create three binary columns,
    one for each category. The color “red” is one-hot encoded as [1, 0, 0], “green”
    as [0, 1, 0], and “blue” as [0, 0, 1]. Doing so preserves the categorical information
    without introducing any ordinal relationship between the categories. Each category
    is treated as independent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we define a `onehot_encoder()` function to convert an integer to a one-hot
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The function takes two arguments: the first argument, `position`, is the index
    at which the value is turned on as 1, and the second argument, `depth`, is the
    length of the one-hot variable. For example, if we print out the value of `onehot_encoder(1,5)`,
    it will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The result is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The result shows a five-value tensor with the second place (the index value
    of which is 1) turned on as 1 and the rest turned off as 0s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you understand how one-hot encoding works, you can convert any integer
    between 0 and 99 to a one-hot variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s use the function to convert the number 75 to a 100-value tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The result is a 100-value tensor with the 76^(th) place (the index value of
    which is 75) turned on as 1 and all other positions turned off as 0s.
  prefs: []
  type: TYPE_NORMAL
- en: To function `int_to_onehot()` converts an integer into a one-hot variable. In
    a way, the function is translating human-readable language into model-ready language.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we want to translate model-ready language back to human-readable language.
    Suppose we have a one-hot variable: How can we convert it into an integer that
    humans understand? The following function `onehot_to_int()` accomplishes that
    goal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The function `onehot_to_int()` takes the argument `onehot` and converts it into
    an integer based on which position has the highest value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s test the function to see what happens if we use the tensor `onehot75`
    we just created as the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The result shows that the function converts the one-hot variable to an integer
    75, which is the right answer. So we know that the functions are defined properly.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll build and train GANs to generate multiples of 5.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2 GANs to generate numbers with patterns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our goal is to build and train a model so that the generator can generate a
    sequence of 10 integers, all multiples of 5\. We first prepare the training data
    and then convert them to model-ready numbers in batches. Finally, we use the trained
    generator to generate the patterns we want.
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, we’ll generate a sequence of 10 integers between 0 and 99\.
    We’ll then convert the sequence into 10 model-ready numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function generates a sequence of 10 integers, all multiples of
    5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We first use the `randint()` method in PyTorch to generate 10 numbers between
    0 and 19\. We then multiply them by 5 and convert them to PyTorch tensors. This
    creates 10 integers that are all multiples of 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to generate a sequence of training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The values in the preceding output are all multiples of 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we convert each number to a one-hot variable so that we can feed them
    to the neural network later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a sequence of 10 numbers, all multiples of 5
  prefs: []
  type: TYPE_NORMAL
- en: ② Converts each integer to a 100-value one-hot variable
  prefs: []
  type: TYPE_NORMAL
- en: The preceding function `gen_batch()` creates a batch of data so that we can
    feed them to the neural network for training purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also define a function `data_to_num()` to convert one-hot variables to a
    sequence of integers so that humans can understand the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: ① Converts vectors to integers based on the largest values in a 100-value vector
  prefs: []
  type: TYPE_NORMAL
- en: ② Applies the function on an example
  prefs: []
  type: TYPE_NORMAL
- en: 'The `dim=-1` argument in the `torch.argmax()` function means we are trying
    to find the position (i.e., index) of the largest value in the last dimension:
    that is, among the 100-value one-hot vector, which position has the highest value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll create two neural networks: one for the discriminator D and one
    for the generator G. We’ll build GANs to generate the desired pattern of numbers.
    Similar to what we did earlier in this chapter, we create a discriminator network,
    which is a binary classifier that distinguishes fake samples from real samples.
    We also create a generator network to generate a sequence of 10 numbers. Here
    is the discriminator neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Since we’ll convert integers into 100-value one-hot variables, we use 100 as
    the input size in the first `Linear` layer in the model. The last `Linear` layer
    has just one output feature in it, and we use the sigmoid activation function
    to squeeze the output to the range [0, 1] so it can be interpreted as the probability,
    p, that the sample is real. With the complementary probability 1 – p, the sample
    is fake.
  prefs: []
  type: TYPE_NORMAL
- en: The generator’s job is to create a sequence of numbers so that they can pass
    as real in front of the discriminator D. That is, G is trying to create a sequence
    of numbers to maximize the probability that D thinks that the numbers are from
    the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We create the following neural network to represent the generator G:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We’ll feed random noise vectors from a 100-dimensional latent space to the generator.
    The generator then creates a tensor of 100 values based on the input. Note here
    that we use the `ReLU` activation function at the last layer so that the output
    is nonnegative. Since we are trying to generate 100 values of 0 or 1, nonnegative
    values are appropriate here.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the first project, we use the Adam optimizer for both the discriminator
    and the generator, with a learning rate of 0.0005:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the training data and two networks, we’ll train the model.
    After that, we’ll discard the discriminator and use the generator to generate
    a sequence of 10 integers.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.3 Training the GANs to generate numbers with patterns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training process for this project is very similar to that in our first project
    in which you generated an exponential growth shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define a function `train_D_G()`, which is a combination of the three functions
    `train_D_on_real()`, `train_D_on_fake()`, and `train_G()` that we have defined
    for the first project. The function `train_D_G()` is in the Jupyter Notebook for
    this chapter in the book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).
    Take a look at the function `train_D_G()` so you can see what minor changes we
    have made compared to the three functions we defined for the first project.'
  prefs: []
  type: TYPE_NORMAL
- en: We use the same early stopping class that we defined for the first project so
    we know when to stop training. However, we have modified the `patience` argument
    to 800 when we instantiate the class, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.11 Training GANs to generate multiples of 5
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates an instance of the early stopping class
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines a distance() function to calculate the loss in the generated numbers
  prefs: []
  type: TYPE_NORMAL
- en: ③ Trains the GANs for one epoch
  prefs: []
  type: TYPE_NORMAL
- en: ④ Prints out the generated sequence of integers after every 50 epochs
  prefs: []
  type: TYPE_NORMAL
- en: 'We have also defined a `distance()` function to measure the difference between
    the training set and the generated data samples: it calculates the MSE of the
    remainder of each generated number when divided by 5\. The measure is 0 when all
    generated numbers are multiples of 5\.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run the preceding code cell, you’ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: In each iteration, we generate a batch of 10 numbers. We first train the discriminator
    D using real samples. After that, the generator creates a batch of fake samples,
    and we use them to train the discriminator D again. Finally, we let the generator
    create a batch of fake samples again, but we use them to train the generator G
    instead. We stop training if the generator network stops improving after 800 epochs
    since the last time the minimum loss was achieved. After every 50 epochs, we print
    out the sequence of 10 numbers created by the generator so you can tell if they
    are indeed all multiples of 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output during the training process is as shown previously. In the first
    few hundred epochs, the generator still produces numbers that are not multiples
    of 5\. But after 900 epochs, all the numbers generated are multiples of 5\. The
    training process takes just a minute or so with GPU training. It takes less than
    10 minutes if you use CPU training. Alternatively, you can download the trained
    model from the book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.4 Saving and using the trained model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll discard the discriminator and save the trained generator in the local
    folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now saved the generator to the local folder. To use the generator,
    we simply load up the model and use it to generate a sequence of integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: ① Loads the saved generator
  prefs: []
  type: TYPE_NORMAL
- en: ② Obtains random noise vectors
  prefs: []
  type: TYPE_NORMAL
- en: ③ Feeds the random noise vectors to the trained model to generate a sequence
    of integers
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The generated numbers are all multiples of 5.
  prefs: []
  type: TYPE_NORMAL
- en: You can easily change the code to generate other patterns, such as odd numbers,
    even numbers, multiples of 3, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3.4
  prefs: []
  type: TYPE_NORMAL
- en: Modify the programs in the second project so that the generator generates a
    sequence of ten integers that are all multiples of 3.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how GANs work, you’ll be able to extend the idea behind GANs
    to other formats in later chapters, including high-resolution images and realistic-sounding
    music.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GANs consist of two networks: a discriminator to distinguish fake samples from
    real samples and a generator to create samples that are indistinguishable from
    those in the training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The steps involved in GANs are preparing training data, creating a discriminator
    and a generator, training the model and deciding when to stop training, and finally,
    discarding the discriminator and using the trained generator to create new samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The content generated by GANs depends on the training data. When the training
    dataset contains data pairs (x, y) that form an exponential growth curve, the
    generated samples are also data pairs that mimic such a shape. When the training
    dataset has sequences of numbers that are all multiples of 5, the generated samples
    are also sequences of numbers, with multiples of 5 in them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs are versatile and capable of generating many different formats of content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](#footnote-001-backlink))  Goodfellow et al, 2014, “Generative Adversarial
    Nets.” [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661).
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](#footnote-000-backlink))  Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
    Ilya Sutskever, and Ruslan Salakhutdinov, 2014, “Dropout: A Simple Way to Prevent
    Neural Networks from Overfitting.” *Journal of Machine Learning Research* 15 (56):
    1929−1958.'
  prefs: []
  type: TYPE_NORMAL
