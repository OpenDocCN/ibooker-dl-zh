<html><head></head><body>
  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">6</span> </span> <span class="chapter-title-text">Dimensionality reduction</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">t-distributed stochastic neighbor embedding </li> 
    <li class="readable-text" id="p3">Multidimensional scaling</li> 
    <li class="readable-text" id="p4">Uniform manifold approximation and projection </li> 
    <li class="readable-text" id="p5">Python implementations of the algorithms </li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p6"> 
   <blockquote>
    <div>
     Life is really simple, but we insist on making it complicated. 
     <div class=" quote-cite">
       —Confucius 
     </div>
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>Simplicity is a virtue—both in life and in data science. We have discussed a lot of algorithms so far. A few of them are simple enough, and some of them are a bit complicated. In part 1 of the book, we studied simpler clustering algorithms, and in the last chapter, we examined advanced clustering algorithms. Similarly, we studied a few dimensionality algorithms like principal component analysis (PCA) in chapter 3. Continuing on the same note, we will study three advanced dimensionality reduction techniques in this chapter. </p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>The advanced topics we cover in this and the next part of the book are meant to prepare you for complex problems. While you can apply these advanced solutions, it is always advisable to start with the classical solutions like PCA for dimensionality reduction. And if that solution doesn’t appropriately address the problem, then you can try the advanced solutions. </p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>Dimensionality reduction is one of the most sought-after solutions, particularly when we have a large number of variables. Recall the “curse of dimensionality” we discussed in chapter 3. You are advised to refresh your memory on chapter 3 before moving forward if needed. We will cover t-distributed stochastic neighbor embedding (t-SNE), multidimensional scaling (MDS), and uniform manifold approximation and projection (UMAP) in this chapter. This chapter will cover some mathematical concepts that create the foundation of the advanced techniques we are going to discuss. As always, the concept discussion will be followed by a Python solution. This chapter also has a short case study. We will also develop a solution using an images dataset.</p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>There may be a dilemma in your mind: What is the level of mathematics required, and is an in-depth statistical knowledge a prerequisite? The answer is both yes and no. While having a mathematical understanding will allow you to understand the algorithms and appreciate the process in greater depth; at the same time, for real-world business implementation, sometimes one might want to skip the mathematics and directly move to the examples in Python. We suggest having at least more than a basic understanding of the mathematics to fully grasp the concept. In this book, we provide that level of mathematical support without going into too much depth, presenting instead an optimal mix of practical world and mathematical concepts. </p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>Welcome to the sixth chapter, and all the very best!</p> 
  </div> 
  <div class="readable-text" id="p12"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.1</span> Technical toolkit</h2> 
  </div> 
  <div class="readable-text" id="p13"> 
   <p>We will continue to use the same version of Python and Jupyter Notebook as we have used so far. The codes and datasets used in this chapter have been checked in at <a href="https://mng.bz/XxOv">https://mng.bz/XxOv</a>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p14"> 
   <p>You will need to install <code>Keras</code> as an additional Python library in this chapter. Along with this, you will need the regular modules: <code>numpy</code>, <code>pandas</code>, <code>matplotlib</code>, <code>seaborn</code>, and <code>sklearn</code>. </p> 
  </div> 
  <div class="readable-text" id="p15"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.2</span> Multidimensional scaling</h2> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>As you know, maps prove to be quite handy while traveling. Now imagine you are given a task. You receive distances between some cities around the world—for example, between London and New York, London and Paris, Paris and New Delhi, and so forth. Then you are asked to re-create the map from which these distances have been derived. If we have to re-create that 2D map, that will be through trial and error; we will make some assumptions and move ahead with the process. It will surely be a tiring exercise prone to error and quite time-consuming indeed. MDS can do this task easily for us. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p17"> 
   <p><span class="print-book-callout-head">NOTE </span> While thinking of the preceding example, ignore the fact that the earth is not flat, and assume that the distance measurement metric is constant—for example, there is no confusion in miles or kilometers. </p> 
  </div> 
  <div class="readable-text" id="p18"> 
   <p>As an illustration, consider figure 6.1. Formally put, if we have <em>x</em> data points, MDS can help us convert the information of the pairwise distance between these <em>x</em> points to a configuration of points in a Cartesian space. Or, simply put, MDS transforms a large dimensional dataset into a lower dimensional one and, in the process, keeps the distance or the similarity between the points the same. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p19">  
   <img alt="figure" src="../Images/CH06_F01_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.1</span> Illustration of distance between the cities and if they are represented on a map. The figure is only to help develop an understanding and does not represent the actual results.</h5>
  </div> 
  <div class="readable-text" id="p20"> 
   <p>To simplify, consider figure 6.2. Here we have three points: A, B, and C. We are representing these points in a 3D space. Then we represent the three points in a 2D space, and finally they are represented in a 1D space. The distance between the points is not up to scale in the diagrams in the figure. The example represents the meaning of lowering the number of dimensions. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p21">  
   <img alt="figure" src="../Images/CH06_F02_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.2</span> Representation of three points </h5>
  </div> 
  <div class="readable-text" id="p22"> 
   <p>Hence, in MDS, multidimensional data is reduced to a lower number of dimensions. </p> 
  </div> 
  <div class="readable-text intended-text" id="p23"> 
   <p>There are three types of MDS algorithms:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p24"> Classical MDS </li> 
   <li class="readable-text" id="p25"> Metric multidimensional scaling </li> 
   <li class="readable-text" id="p26"> Nonmetric multidimensional scaling </li> 
  </ul> 
  <div class="readable-text" id="p27"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.2.1</span> Classic MDS</h3> 
  </div> 
  <div class="readable-text" id="p28"> 
   <p>We will examine the metric MDS process in detail in the book, while we will cover the classical and nonmetric briefly. Imagine we have two points: <em>i</em> and <em>j</em>. Let us assume that the original distance between two points is <em>d</em><sub><em>i</em></sub><sub><em>j</em></sub> and the corresponding distance in the lower dimensional space is <em class="obliqued">d</em><sub><em>i</em></sub><sub><em>j</em></sub>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p29"> 
   <p>In classical MDS, the distances between the points are treated as Euclidean distances, and the original and fitted distances are represented in the same metric. It means that if the original distances in a higher dimensional space are calculated using the Euclidean method, the fitted distances in the lower dimensional space are also calculated using Euclidean distance. We already know how to calculate Euclidean distances. For example, we have to find the distance between points <em>i</em> and <em>j</em>, and let’s say the distance is <em>d</em><sub><em>i</em></sub><sub><em>j</em></sub>. The distance can be given by the Euclidean distance formula given by equation 6.1 in a 2D space:</p> 
  </div> 
  <div class="browsable-container figure-container" id="p30">  
   <img alt="figure" src="../Images/verdhan-ch6-eqs-0x.png"/> 
   <h5 class=" figure-container-h5">(6.1)</h5>
  </div> 
  <div class="readable-text" id="p31"> 
   <p>Recall in chapter 2, we discussed other distance functions like Manhattan distance, Euclidean distance, etc. You are advised to refresh your memory on chapter 2.</p> 
  </div> 
  <div class="readable-text" id="p32"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.2.2</span> Nonmetric MDS</h3> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>We just now noted that Euclidean distance can be used to calculate the distance between two points. Sometimes it is not possible to take the actual values of the distances, like when <em>d</em><sub><em>i</em></sub><sub><em>j</em></sub> is the result of an experiment where subjective assessments were made or, in other words, where a rank was allocated to the various data parameters. For example, if the distance between points 2 and 5 was at rank 4 in the original data, in such a scenario, it will not be wise to use absolute values of <em>d</em><sub><em>i</em></sub><sub><em>j</em></sub>, and hence relative values or <em>rank values</em> have to be used. Here, distance can mean a kind of ranking—for example, who came first in a race. This is the process in nonmetric MDS. For example, imagine we have four points: A, B, C, and D. We wish to rank the respective distances between these four points. The respective combinations of points can be A and B, A and C, A and D, B and C, B and D, and C and D. Their distances can be ranked as shown in table 6.1.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p34"> 
   <h5 class=" browsable-container-h5"><span class="num-string">Table 6.1</span> The respective distance between four points and the ranks of the distances</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Pair of points 
       </div> </th> 
      <th> 
       <div>
         Distance 
       </div> </th> 
      <th> 
       <div>
         Ranks of the respective distances 
       </div> </th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  A and B <br/> </td> 
      <td>  100 <br/> </td> 
      <td>  3 <br/> </td> 
     </tr> 
     <tr> 
      <td>  A and C <br/> </td> 
      <td>  105 <br/> </td> 
      <td>  4 <br/> </td> 
     </tr> 
     <tr> 
      <td>  A and D <br/> </td> 
      <td>  95 <br/> </td> 
      <td>  2 <br/> </td> 
     </tr> 
     <tr> 
      <td>  B and C <br/> </td> 
      <td>  205 <br/> </td> 
      <td>  6 <br/> </td> 
     </tr> 
     <tr> 
      <td>  B and D <br/> </td> 
      <td>  150 <br/> </td> 
      <td>  5 <br/> </td> 
     </tr> 
     <tr> 
      <td>  C and D <br/> </td> 
      <td>  55 <br/> </td> 
      <td>  1 <br/> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>So, in the nonmetric MDS method, instead of using the actual distances, we use the respective ranks of the distance. We next move on to the metric MDS method.</p> 
  </div> 
  <div class="readable-text intended-text" id="p36"> 
   <p>We know that in classical MDS, the original and fitted distances are represented in the same metric. In metric MDS, it is assumed that the values of <em>d</em><sub><em>i</em></sub><sub><em>j</em></sub> can be transformed into Euclidean distances by employing some parametric transformation on the datasets. In some articles, you might find classical and metric MDS used interchangeably. </p> 
  </div> 
  <div class="readable-text intended-text" id="p37"> 
   <p>In MDS, as a first step, the respective distances between the points are calculated. Once the respective distances have been calculated, then MDS will try to represent the higher dimensional data point in a lower dimensional space. To perform this, an optimization process has to be carried out so that the optimum number of resultant dimensions can be chosen. Hence, a loss function or cost function has to be optimized. </p> 
  </div> 
  <div class="readable-text" id="p38"> 
   <h4 class=" readable-text-h4">Cost function</h4> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>We use algorithms to predict the values of a variable. For example, we might use some algorithm to predict the expected demand of a product next year. We would want the algorithm to predict as accurately as possible. Cost functions are a simple method to check the performance of the algorithms. </p> 
  </div> 
  <div class="readable-text intended-text" id="p40"> 
   <p>Cost function is a simple technique to measure the effectiveness of our algorithms. It is the most common method used to gauge the performance of a predictive model. It compares the original values and the predicted values by the algorithm and calculates how wrong the model is in its prediction.</p> 
  </div> 
  <div class="readable-text intended-text" id="p41"> 
   <p>As you would imagine, in an ideal solution, we would want the predicted values to be the same as the actual values, which is very difficult to achieve. If the predicted values differ a lot from the actual values, the output of a cost function is higher. If the predicted values are closer to the actual values, then the value of a cost function is lower. A robust solution is one that has the lowest value of the cost function. Hence, the objective to optimize any algorithm will be to minimize the value of the cost function. Cost function is also referred to as loss function; these two terms can be used interchangeably. </p> 
  </div> 
  <div class="readable-text intended-text" id="p42"> 
   <p>In metric MDS, we can also call the cost function<em> stress. </em>It is just another name for cost function. The formula for stress is given in equation 6.2:</p> 
  </div> 
  <div class="browsable-container figure-container" id="p43">  
   <img alt="figure" src="../Images/verdhan-ch6-eqs-1x.png"/> 
   <h5 class=" figure-container-h5">(6.2)</h5>
  </div> 
  <div class="readable-text" id="p44"> 
   <p>In the equation,</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p45"> Term Stress<sub><em>D</em></sub> is the value the MDS function has to minimize. </li> 
   <li class="readable-text" id="p46"> The data points with the new set of coordinates in a lower dimensional space are represented by <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, <em>x</em><sub>3</sub>…. <em>x</em><sub><em>N</em></sub>. </li> 
   <li class="readable-text" id="p47"> The term ||<em>x</em><sub><em>i</em></sub><sub> </sub>– <em>x</em><sub><em>j</em></sub>|| is the distance between two points in their lower dimensional space. </li> 
   <li class="readable-text" id="p48"> The term <em>d</em><sub><em>i</em></sub><sub><em>j</em></sub> is the original distance between the two points in the original multidimensional space. </li> 
  </ul> 
  <div class="readable-text" id="p49"> 
   <p>By looking at the equation, we can see that if the values of ||<em>x</em><sub><em>i</em></sub><sub> </sub>– <em>x</em><sub><em>j</em></sub>|| and <em>d</em><sub><em>i</em></sub><sub><em>j</em></sub> are close to each other, the value of the resultant stress will be small. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p50"> 
   <p><span class="print-book-callout-head">NOTE </span> Minimizing the value of stress is the objective of the loss function. </p> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>To optimize this loss function, we can use multiple approaches. One of the most famous methods is using a gradient descent that was originally proposed by Kruskal and Wish in 1978. The gradient descent method is very simple to understand and can be explained using a simple analogy. </p> 
  </div> 
  <div class="readable-text intended-text" id="p52"> 
   <p>Imagine you are standing on top of a mountain and you want to get down. You want to choose the fastest path because you want to get down as fast as possible (no, you cannot jump!). So, to take the first step, you look around and, whichever is the steepest path, you take a step in that direction and reach a new point. Then again, you take a step in the steepest direction. This process is shown in the first diagram in figure 6.3. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p53">  
   <img alt="figure" src="../Images/CH06_F03_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.3</span> A person standing on top of a mountain and trying to get down. The process of gradient descent follows this method (left). The actual process of optimization of a cost function in gradient descent process. Note that at the point of convergence, the value of the cost function is minimal (right). </h5>
  </div> 
  <div class="readable-text" id="p54"> 
   <p>Now say an algorithm has to achieve a similar feat; the process is represented in the right diagram in figure 6.3, wherein a loss function starts at a point and finally reaches the point of convergence. At this point of convergence, the cost function is minimal.</p> 
  </div> 
  <div class="readable-text intended-text" id="p55"> 
   <p>MDS differs from the other dimensionality reduction techniques. As compared to techniques like PCA, MDS does not make any assumptions about the dataset and hence can be used for a larger number of datasets. Moreover, MDS allows the use of any distance measurement metric. Unlike PCA, MDS is not an eigenvalue-eigenvector technique. Recall in PCA, the first axis captures the maximum amount of variance, the second axis has the next best variance, and so on. In MDS, there is no such condition. The axes in MDS can be inverted or rotated as needed. Also, in most of the other dimensional reduction methods used, the algorithms do calculate a lot of axes, but they cannot be viewed. In MDS, a smaller number of dimensions are explicitly chosen at the start. Hence there is less ambiguity in the solution. Further, in other algorithms, generally, there is only one unique solution, whereas MDS tries to iteratively find the most acceptable solution. It means that in MDS there can be multiple solutions for the same dataset. </p> 
  </div> 
  <div class="readable-text intended-text" id="p56"> 
   <p>But at the same time, the computation time required for MDS is greater for bigger datasets—and there is a catch in the gradient descent method used for optimization (see figure 6.4). Let’s refer to the mountain example we covered earlier. Imagine that while you are coming down from the top of the mountain, the starting point is A, and the bottom of the mountain is point C. While you are coming down, you reach point B. As you can see in the left diagram in the figure, there is a slight elevation around point B. At this point B, you might incorrectly conclude that you have reached the bottom of the mountain. In other words, you will think that you have finished your task. This is the problem of the local minima. <span class="aframe-location"/> </p> 
  </div> 
  <div class="browsable-container figure-container" id="p57">  
   <img alt="figure" src="../Images/CH06_F04_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.4</span> While the first figure is the point of convergence and represents the gradient descent method, note that in the second figure the global minima is somewhere else, while the algorithm can be stuck at a local minima. The algorithm might check that it has optimized the cost function and reached the point of global minima, whereas it has only reached the local minima. In a local minima, there is no direction that is ascending; all the directions descend. The algorithm, if purely local, has no information about other deeper minima existing beyond a potentially small hill.</h5>
  </div> 
  <div class="readable-text" id="p58"> 
   <p>It is a possibility that instead of a global minimum, the loss function might be stuck in a local minima. The algorithm might think that it has reached the point of convergence, while the complete convergence might not have been achieved, and we are at a local minimum.</p> 
  </div> 
  <div class="readable-text intended-text" id="p59"> 
   <p>There is still a question to be answered about the efficacy of the MDS solution. How can we measure the effectiveness of the solution? In the original paper, Kruskal recommended the stress values to measure the goodness-of-fit of the solution, which are shown in table 6.2. The recommendations are mostly based on the empirical experience of Kruskal. These stress values are based on Kruskal’s experience.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p60"> 
   <h5 class=" browsable-container-h5"><span class="num-string">Table 6.2</span> Stress values and their goodness of fit</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Stress values 
       </div> </th> 
      <th> 
       <div>
         Goodness of fit 
       </div> </th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  0.200 <br/> </td> 
      <td>  Poor <br/> </td> 
     </tr> 
     <tr> 
      <td>  0.100 <br/> </td> 
      <td>  Fair <br/> </td> 
     </tr> 
     <tr> 
      <td>  0.050 <br/> </td> 
      <td>  Good <br/> </td> 
     </tr> 
     <tr> 
      <td>  0.025 <br/> </td> 
      <td>  Excellent <br/> </td> 
     </tr> 
     <tr> 
      <td>  0.000 <br/> </td> 
      <td>  Perfect <br/> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p61"> 
   <p>The next logical question is: How many final dimensions should we choose? A scree plot provides the answer, as shown in figure 6.5. Recall in chapter 2 we used a similar elbow method to choose the optimal number of clusters in k-means clustering. For MDS too, we can use the elbow method to determine the optimal number of components to represent the data.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p62">  
   <img alt="figure" src="../Images/CH06_F05_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.5</span> Scree plot to find the optimal number of components. It is similar to the k-means solution; we have to look for the elbow in the plot.</h5>
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p63"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 6.1</h5> 
   </div> 
   <div class="readable-text" id="p64"> 
    <p>Answer these questions to check your understanding:</p> 
   </div> 
   <ol> 
    <li class="readable-text" id="p65"> What is the difference between metric and nonmetric MDS algorithms? </li> 
    <li class="readable-text" id="p66"> Gradient descent is used to maximize the cost. True or False? </li> 
    <li class="readable-text" id="p67"> Explain the gradient descent method using a simple example. </li> 
   </ol> 
  </div> 
  <div class="readable-text" id="p68"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.3</span> Python implementation of MDS</h2> 
  </div> 
  <div class="readable-text" id="p69"> 
   <p>For the Python implementation of the MDS method we will use the famous Iris dataset, which we have used previously. Using the algorithm is quite simple, thanks to the libraries available in the <code>scikit learn</code> package. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p70"> 
   <p><span class="print-book-callout-head">NOTE </span> The implementation is generally simple as the heavy lifting is done by the libraries.</p> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>The steps are as follows:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p72"> Load the libraries. The usual suspects are <code>sklearn</code>, <code>matplotlib</code>, and <code>numpy</code>, and we also load <code>MDS</code> from <code>sklearn</code>: </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p73"> 
   <div class="code-area-container"> 
    <pre class="code-area">import numpy as np
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from sklearn.manifold import MDS
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import warnings
warnings.filterwarnings("ignore")</pre>  
   </div> 
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p74"><span class="faux-ol-li-counter">2. </span> Load the dataset. The Iris dataset is available in the <code>sklearn</code> library, so we need not import Excel or .csv files here:  </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p75"> 
   <div class="code-area-container"> 
    <pre class="code-area">raw_data = load_iris()
dataset = raw_data.data</pre>  
   </div> 
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p76"><span class="faux-ol-li-counter">3. </span> A requirement for MDS is that the dataset should be scaled before the actual visualization is done. We use the <code>MixMaxScalar()</code> function to achieve this. MinMax scaling simply scales the data using the formula in equation 6.3: </li> 
  </ol> 
  <div class="browsable-container figure-container" id="p77">  
   <img alt="figure" src="../Images/verdhan-ch6-eqs-2x.png"/> 
   <h5 class=" figure-container-h5">(6.3)</h5>
  </div> 
  <div class="browsable-container listing-container" id="p78"> 
   <div class="code-area-container"> 
    <pre class="code-area">d_scaler = MinMaxScaler()
dataset_scaled = d_scaler.fit_transform(dataset)</pre>  
   </div> 
  </div> 
  <div class="readable-text list-body-item" id="p79"> 
   <p>As an output of this step, the data is scaled and ready for the next step of modeling.</p> 
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p80"><span class="faux-ol-li-counter">4. </span> Invoke the MDS method from the <code>sklearn</code> library. The <code>random_state</code> value allows us to reproduce the results. We have chosen the number of components as 3 for the example: </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p81"> 
   <div class="code-area-container"> 
    <pre class="code-area">mds_output = MDS(3,random_state=5)</pre>  
   </div> 
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p82"><span class="faux-ol-li-counter">5. </span> Fit the scaled data created earlier using the MDS model: </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p83"> 
   <div class="code-area-container"> 
    <pre class="code-area">data_3d = mds_output.fit_transform(dataset_scaled)</pre>  
   </div> 
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p84"><span class="faux-ol-li-counter">6. </span> Declare the colors we wish to use for visualization. Next, the data points are visualized in a scatter plot: </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p85"> 
   <div class="code-area-container"> 
    <pre class="code-area">mds_colors = ['purple','blue', 'yellow']
for i in np.unique(raw_data.target):
  d_subset = data_3d[raw_data.target == i]

  x = [row[0] for row in d_subset]
  y = [row[1] for row in d_subset]
  plt.scatter(x,y,c=mds_colors[i],label=raw_data.target_names[i])
plt.legend()
plt.show()</pre>  
   </div> 
  </div> 
  <div class="readable-text list-body-item" id="p86"> 
   <p>The output of the preceding code is shown in figure 6.6.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p87">  
   <img alt="figure" src="../Images/CH06_F06_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.6</span> Output for the Iris data</h5>
  </div> 
  <div class="readable-text" id="p88"> 
   <p>This example of Python implementation is a visualization of the Iris data. It is quite a simple example, as it does not involve stress and optimization for the number of components. In other words, we need a more complex dataset to really optimize MDS. We will now work on a curated dataset to implement MDS (see figure 6.7). <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p89">  
   <img alt="figure" src="../Images/CH06_F07_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.7</span> Various cities and their respective distances between each other</h5>
  </div> 
  <div class="readable-text" id="p90"> 
   <p>Let us assume we have five cities and the respective distance between them is given in figure 6.7. The steps are as follows:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p91"> We have already imported the libraries in the last code: </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p92"> 
   <div class="code-area-container"> 
    <pre class="code-area">import numpy as np
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from sklearn.manifold import MDS
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import warnings
warnings.filterwarnings("ignore")</pre>  
   </div> 
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p93"><span class="faux-ol-li-counter">2. </span> Create the dataset. Although we create a dataset here, in real business scenarios, it will be in the form of distances only (see figure 6.8): </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p94"> 
   <div class="code-area-container"> 
    <pre class="code-area">data_dummy_cities = {'A':[0,40,50,30,40],
          'B':[40,0,40,50,20],
          'C':[50,40,0,20,50],
          'D':[30,50,20,0,20],
          'E':[40,20,50,20,0],
          }
cities_dataframe = pd.DataFrame(data_dummy_cities, index 
=['A','B','C','D','E'])
cities_dataframe<span class="aframe-location"/></pre>  
   </div> 
  </div> 
  <div class="browsable-container figure-container" id="p95">  
   <img alt="figure" src="../Images/CH06_F08_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.8</span> Creating the dataset</h5>
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p96"><span class="faux-ol-li-counter">3. </span> Use the <code>MinMaxScalar()</code> function to scale the dataset as we did in the last coding exercise: </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p97"> 
   <div class="code-area-container"> 
    <pre class="code-area">scaler = MinMaxScaler()
df_scaled = scaler.fit_transform(cities_dataframe)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p98"> 
   <p>Now we work toward finding the most optimal number of components. We will iterate for different values of the number of components. For each of the values of the number of components, we will get the value of stress. The point at which a kink is observed is the optimal number of components. </p> 
  </div> 
  <div class="readable-text intended-text" id="p99"> 
   <p>As a first step, we will declare an empty dataframe, which can be used to store the values of the number of components and corresponding stress values. Then we iterate from 1 to 10 in a <code>for</code> loop. Finally, for each of the values of components (1 to 10), we get the respective values of stress: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p100"> 
   <div class="code-area-container"> 
    <pre class="code-area">MDS_stress = []
for i in range(1, 10):
    mds = MDS(n_components=i)
    pts = mds.fit_transform(df_scaled)
    MDS_stress.append(mds.stress_)</pre>  
   </div> 
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p101"><span class="faux-ol-li-counter">4. </span> Now that we have the values of stress, we will plot these values in a graph. The respective labels for each of the axes are also given. Look at the kink at values 2 and 3 in figure 6.9. These can be the optimal values of the number of components: </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p102"> 
   <div class="code-area-container"> 
    <pre class="code-area">plt.plot(range(1, 10), MDS_stress)
plt.xticks(range(1, 5, 2))
plt.title('Plot of stress')
plt.xlabel('Number of components')
plt.ylabel('Stress values')
plt.show()<span class="aframe-location"/></pre>  
   </div> 
  </div> 
  <div class="browsable-container figure-container" id="p103">  
   <img alt="figure" src="../Images/CH06_F09_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.9</span> Scree plot to select the optimized number of components </h5>
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p104"><span class="faux-ol-li-counter">5. </span> Run the solution for the number of components = 3. If we look at the values of stress, number of components = 3, it generates the minimum value of stress as 0.00665 (see figure 6:10): </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p105"> 
   <div class="code-area-container"> 
    <pre class="code-area">mds = MDS(n_components=3)
x = mds.fit_transform(df_scaled)
cities = ['A','B','C','D','E']

plt.figure(figsize=(5,5))
plt.scatter(x[:,0],x[:,1])
plt.title('MDS with Sklearn')
for label, x, y in zip(cities, x[:, 0], x[:, 1]):
    plt.annotate(
        label,
        xy = (x, y), 
        xytext = (-10, 10),
        textcoords = 'offset points'
    )
plt.show()
print(mds.stress_)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>This concludes our discussion on the MDS algorithm. We discussed the foundation and concepts, pros and cons, algorithm assessment, and Python implementation of MDS. As one of the nonlinear dimensionality reduction methods, it is a great solution for visualization and dimensionality reductions. </p> 
  </div> 
  <div class="browsable-container figure-container" id="p107">  
   <img alt="figure" src="../Images/CH06_F10_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.10</span> Output for the MDS dataset: representation of the five cities in a plot</h5>
  </div> 
  <div class="readable-text" id="p108"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.4</span> t-distributed stochastic neighbor embedding</h2> 
  </div> 
  <div class="readable-text" id="p109"> 
   <p>If a dataset is really high dimensional, the analysis becomes cumbersome. The visualization is even more confusing. We have covered that in great detail in the curse of dimensionality section in chapter 3. You are advised to revisit the concept before proceeding if you need a refresher. </p> 
  </div> 
  <div class="readable-text intended-text" id="p110"> 
   <p>One such really high-dimensional dataset can be image data. We find it difficult to comprehend such data due to anything beyond 3 dimensions being increasingly difficult for us to intuit. </p> 
  </div> 
  <div class="readable-text intended-text" id="p111"> 
   <p>You may have used facial recognition software on your smartphone. For such solutions, facial images have to be analyzed, and machine learning models have to be trained. Look at the pictures in figure 6.11: we have a human face, a bike, a vacuum cleaner, and a screen capture of a phone. </p> 
  </div> 
  <div class="readable-text intended-text" id="p112"> 
   <p>Image is a complex data type. Each image is made up of pixels, and each pixel can be made up of RGB (red, green, blue) values. Values for each of the RGB can range from 0 to 255. The resulting dataset will be a very high-dimensional dataset. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p113">  
   <img alt="figure" src="../Images/CH06_F11_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.11</span> Images are quite complex to decipher by an algorithm. Images can be of any form and can be of a person, a piece of equipment, or even a phone screen.</h5>
  </div> 
  <div class="readable-text" id="p114"> 
   <p>Now recall PCA, which we studied in chapter 3. PCA is a linear algorithm. Thus, its capability to resolve nonlinear and complex polynomial functions is limited. Moreover, when a high-dimensional dataset has to be represented in a low-dimensional space, the algorithm should keep similar data points close to each other, which can be a challenge in linear algorithms. PCA, as a linear dimension reduction technique, tries to separate the different data points as far away from each other as possible, and tries to maximize the variance captured in the data. The resulting analysis is not robust and might not be best suited for further use and visualization. Hence, we have nonlinear algorithms like t-SNE to help.</p> 
  </div> 
  <div class="readable-text intended-text" id="p115"> 
   <p>t-SNE is a nonlinear dimensionality reduction technique that is quite handy for high-dimensional data. It is based on stochastic neighbor embedding, which was developed by Sam Roweis and Geoffrey Hinton. The t-distributed variant was proposed by Lauren van der Maaten. Thus, t-SNE is an improvement of the SNE algorithm. </p> 
  </div> 
  <div class="readable-text intended-text" id="p116"> 
   <p>At a high level, SNE measures the similarity between instance pairs in a high-dimensional space and in a low-dimensional space. A good solution is where the difference between these similarity measures is the least, and SNE then optimizes these similarity measures using a cost function similar to what we have discussed for MDS. </p> 
  </div> 
  <div class="readable-text intended-text" id="p117"> 
   <p>We examine the step-by-step process of t-SNE next. The process described is a little heavy on mathematics:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p118"> Consider a high-dimensional space and some points in it. </li> 
   <li class="readable-text" id="p119"> Measure the similarities between the various points in the high-dimensional space mentioned in the last point. For a point <em>x</em><sub><em>i</em></sub>, we will then create a Gaussian distribution centered at that point. We have already studied Gaussian or normal distribution in chapter 2. The Gaussian distribution is shown in figure 6.12.<span class="aframe-location"/> </li> 
  </ol> 
  <div class="browsable-container figure-container" id="p120">  
   <img alt="figure" src="../Images/CH06_F12_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.12</span> Gaussian or normal distribution.</h5>
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p121"><span class="faux-ol-li-counter">3. </span> Measure the density of points (let’s say <em>x</em><sub><em>j</em></sub>) that fall under that Gaussian distribution and then renormalize them to get the respective conditional probabilities (<em>p</em><sub><em>j</em></sub><sub>|</sub><sub><em>i</em></sub>). For the points that are nearby and hence similar, this conditional probability will be high, and for the points that are far and dissimilar, the value of conditional probabilities (<em>p</em><sub><em>j</em></sub><sub>|</sub><sub><em>i</em></sub>) will be very small. These values of probabilities are those in the high-dimensional space. For curious readers, the mathematical formula for this conditional probability is presented as equation 6.4 </li> 
  </ol> 
  <div class="browsable-container figure-container" id="p122">  
   <img alt="figure" src="../Images/verdhan-ch6-eqs-3x.png"/> 
   <h5 class=" figure-container-h5">(6.4)</h5>
  </div> 
  <div class="readable-text list-body-item" id="p123"> 
   <p>where <em class="obliqued">σ</em> is the variance of the Gaussian distribution centered at <em>x</em><sub><em>i</em></sub>. The mathematical proof is beyond the scope of this book.</p> 
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p124"><span class="faux-ol-li-counter">4. </span> Measure one more set of probabilities in the low-dimensional space. For this set of measurements, we use the Cauchy distribution, described next. We use Kullback-Liebler (KL) divergence for measuring the difference between two probability distributions.  </li> 
  </ol> 
  <div class="readable-text" id="p125"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.4.1</span> Cauchy distribution</h3> 
  </div> 
  <div class="readable-text" id="p126"> 
   <p>The Cauchy distribution belongs to the family of continuous probability distributions. Though there is a resemblance with the normal distribution, as we have represented in figure 6.13, the Cauchy distribution has a narrower peak and spreads out more<span class="aframe-location"/> slowly. It means that, compared to a normal distribution, the probability of obtaining values far from the peaks is higher. Sometimes, the Cauchy distribution is known as the <em>Lorentz distribution.</em> It is interesting to note that Cauchy does not have a well-defined mean, but the median is the center of symmetry.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p127">  
   <img alt="figure" src="../Images/CH06_F13_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.13</span> Comparison of Gaussian distribution vs. Cauchy distribution. (Image source: Quora)</h5>
  </div> 
  <ol> 
   <li class="readable-text" id="p128"> Consider we get <em>y</em><sub><em>i</em></sub> and <em>y</em><sub><em>j</em></sub> as the low-dimensional counterparts for the high-dimensional data points <em>x</em><sub><em>i</em></sub> and <em>x</em><sub><em>j</em></sub>. So we can calculate the probability score like we did in the last step. Using the Cauchy distribution, we can get a second set of probabilities <em>q</em><sub><em>j</em></sub><sub>|</sub><sub><em>i</em></sub> too. The mathematical formula is shown in equation 6.5: </li> 
  </ol> 
  <div class="browsable-container figure-container" id="p129">  
   <img alt="figure" src="../Images/verdhan-ch6-eqs-4x.png"/> 
   <h5 class=" figure-container-h5">(6.5)</h5>
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p130"><span class="faux-ol-li-counter">2. </span> So far, we have calculated two set of probabilities (<em>p</em><sub><em>j</em></sub><sub>|</sub><sub><em>i</em></sub>) and (<em>q</em><sub><em>j</em></sub><sub>|</sub><sub><em>i</em></sub>). In this step, we compare the two distributions and measure the difference between the two. In other words, while calculating (<em>p</em><sub><em>j</em></sub><sub>|</sub><sub><em>i</em></sub>) we measured the probability of similarity in a high-dimensional space whereas for (<em>q</em><sub><em>j</em></sub><sub>|</sub><sub><em>i</em></sub>) we did the same in a low-dimensional space. Ideally, the mapping of the two spaces is similar, and for that, there should not be any difference between (<em>p</em><sub><em>j</em></sub><sub>|</sub><sub><em>i</em></sub>) and (<em>q</em><sub><em>j</em></sub><sub>|</sub><sub><em>i</em></sub>). So the SNE algorithm tries to minimize the difference in the conditional probabilities (<em>p</em><sub><em>j</em></sub><sub>|</sub><sub><em>i</em></sub>) and (<em>q</em><sub><em>j</em></sub><sub>|</sub><sub><em>i</em></sub>), similar to what we have done with MDS for the distance in high- and low-dimensional spaces. </li> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p131"><span class="faux-ol-li-counter">3. </span> The difference between the two probability distributions is done using KL divergence. </li> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p132"><span class="faux-ol-li-counter">4. </span> To minimize the KL cost function, we use the gradient descent approach. We have already discussed the gradient descent approach in section 6.2 where we discussed the MDS algorithm. </li> 
  </ol> 
  <div class="readable-text print-book-callout" id="p133"> 
   <p><span class="print-book-callout-head">DEFINITION </span> KL divergence or relative entropy is used to measure the difference between two probability distributions. Usually, one probability distribution is the data or the measured scores, and the second probability distribution is an approximation or the prediction of the original probability distribution—for example, if the original probability distribution is <em>X</em> and the approximated one is <em>Y</em>. KL divergence can be used to measure the difference between <em>X</em> and <em>Y</em> probability distributions. In absolute terms, if the value is 0, then it means that the two distributions are identical. The KL divergence is applicable for neurosciences, statistics, and fluid mechanics, among others.</p> 
  </div> 
  <div class="readable-text" id="p134"> 
   <p>There is one more important factor we should be aware of while we work on t-SNE, and that is <em>perplexity</em>. Perplexity is a hyperparameter that allows us to control and optimize the number of close neighbors each of the data points has.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p135"> 
   <p><span class="print-book-callout-head">NOTE </span> As per the official paper, a typical value for perplexity lies between 5 and 50. </p> 
  </div> 
  <div class="readable-text" id="p136"> 
   <p>There can be one additional nuance: the output of a t-SNE algorithm might never be the same on successive runs. We have to optimize the values of the hyperparameters to receive the best output. </p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p137"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 6.2</h5> 
   </div> 
   <div class="readable-text" id="p138"> 
    <p>Answer these questions to check your understanding:</p> 
   </div> 
   <ol> 
    <li class="readable-text" id="p139"> Explain Cauchy distribution in your own words. </li> 
    <li class="readable-text" id="p140"> PCA is a nonlinear algorithm. True or False? </li> 
    <li class="readable-text" id="p141"> KL divergence is used to measure the difference between two probability distributions. True or False? </li> 
   </ol> 
  </div> 
  <div class="readable-text" id="p142"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.4.2</span> Python implementation of t-SNE</h3> 
  </div> 
  <div class="readable-text" id="p143"> 
   <p>We will use two datasets in this example. The first one is the Iris dataset, which we have already used more than once in this book. The second dataset is quite an interesting one: the MNIST dataset is a database of handwritten digits. It is one of the most famous datasets used to train image processing solutions and generally is considered the “Hello World” program for image detection solutions. An image representation is shown figure 6.14. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p144">  
   <img alt="figure" src="../Images/CH06_F14_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.14</span> MNIST dataset</h5>
  </div> 
  <div class="readable-text" id="p145"> 
   <p>The steps for the Iris dataset are as follows:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p146"> Import the necessary libraries. Note that we have imported the MNIST dataset from the <code>keras</code> library.  </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p147"> 
   <div class="code-area-container"> 
    <pre class="code-area">rom sklearn.manifold import TSNE
from keras.datasets import mnist
from sklearn.datasets import load_iris
from numpy import reshape
import seaborn as sns
import pandas as pd</pre>  
   </div> 
  </div> 
  <div class="readable-text print-book-callout" id="p148"> 
   <p><span class="print-book-callout-head">TIP </span> If you are not able to install modules in your Python code, refer to the appendix where we provide a solution.</p> 
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p149"><span class="faux-ol-li-counter">2. </span> Load the Iris dataset. The dataset comprises two parts: one is the “data” and the second is the respective label or “target” for it. It means that “data” is the description of the data and “target” is the type of iris. We print the features and the labels using code: </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p150"> 
   <div class="code-area-container"> 
    <pre class="code-area">iris = load_iris()
iris_data = iris.data
iris_target = iris.target
iris.feature_names
iris.target_names</pre>  
   </div> 
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p151"><span class="faux-ol-li-counter">3. </span> Invoke the t-SNE algorithm. We are using the <code>n_components=2</code>, <code>verbose=1</code>, and <code>random_state=5</code> to reproduce the results. Then the algorithm is used to fit the data (see figure 6.15): </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p152"> 
   <div class="code-area-container"> 
    <pre class="code-area">tsne = TSNE(n_components=2, verbose=1, random_state=5)
fitted_data = tsne.fit_transform(iris_data)<span class="aframe-location"/></pre>  
   </div> 
  </div> 
  <div class="browsable-container figure-container" id="p153">  
   <img alt="figure" src="../Images/CH06_F15_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.15</span> Output of the code when we are fitting the algorithm</h5>
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p154"><span class="faux-ol-li-counter">4. </span> Plot the data. This step allows us to visualize the data fitted by the algorithm in the last step. </li> 
  </ol> 
  <div class="readable-text" id="p155"> 
   <p>First, we will initiate an empty dataframe. We will add three columns, one at a time. We start with <code>iris_targ</code><span class="code-char">et</span>, followed by <code>tSNE_first_component</code> and <code>tSNE_second_ component</code>. <code>tSNE_first_component</code> is the first column of the <code>fitted_data</code> dataframe, and therefore the index is <code>0</code>. <code>tSNE_second_component</code> is the second column of the <code>fitted_data</code> dataframe and hence the index is <code>1</code>. Finally, we represent the data in a scatterplot in figure 6.16:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p156"> 
   <div class="code-area-container"> 
    <pre class="code-area">iris_df = pd.DataFrame()
iris_df["iris_target"] = iris_target
iris_df["tSNE_first_component"] = fitted_data[:,0]
iris_df["tSNE_second_component"] = fitted_data[:,1]

sns.scatterplot(x="tSNE_first_component", y="tSNE_second_component", 
hue=iris_df.iris_target.tolist(),
                palette=sns.color_palette("hls", 3),
                data=iris_df).set(title="Iris data tSNE projection")<span class="aframe-location"/></pre>  
   </div> 
  </div> 
  <div class="browsable-container figure-container" id="p157">  
   <img alt="figure" src="../Images/CH06_F16_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.16</span> t-SNE projection of the Iris dataset. Note how we are getting three separate clusters for the three classes we have in the dataset.</h5>
  </div> 
  <div class="readable-text" id="p158"> 
   <p>To implement the algorithm for the MNIST dataset, load the libraries and dataset. The libraries were already loaded in the last code example. Now load the dataset. The dataset requires <code>reshape</code>, which is done here (see figure 6.17):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p159"> 
   <div class="code-area-container"> 
    <pre class="code-area">(digit, digit_label), (_ , _) = mnist.load_data()
digit = reshape(digit, [digit.shape[0], digit.shape[1]*digit.shape[2]])
Step 2: the subsequent steps are exactly same to the last example we used. 
tsne_MNIST = TSNE(n_components=2, verbose=1, random_state=5)
fitted_data = tsne_MNIST.fit_transform(digit)

mnist_df = pd.DataFrame()
mnist_df["digit_label"] = digit_label
mnist_df["tSNE_first_component"] = fitted_data[:,0]
mnist_df["tSNE_second_component"] = fitted_data[:,1]

sns.scatterplot(x="tSNE_first_component", y="tSNE_second_component", hue=mnist_df.digit_label.tolist(),
                palette=sns.color_palette("hls", 10),
                data=mnist_df).set(title="MNIST data T-SNE projection")</pre>  
   </div> 
  </div> 
  <div class="browsable-container figure-container" id="p160">  
   <img alt="figure" src="../Images/CH06_F17_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.17</span> Output of t-SNE for the 10 classes of digits represented in different shades of gray</h5>
  </div> 
  <div class="readable-text" id="p161"> 
   <p>There are a few important points to keep in mind while running t-SNE:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p162"> Run the algorithm with different values of hyperparameters before finalizing a solution. </li> 
   <li class="readable-text" id="p163"> Ideally, perplexity should be between 5 and 50, and for an optimized solution, the value of perplexity should be less than the number of points. </li> 
   <li class="readable-text" id="p164"> T-SNE guesses the number of close neighbors for each of the points. For this reason, a dataset that is denser will require a much higher perplexity value. </li> 
   <li class="readable-text" id="p165"> Note that perplexity is the hyperparameter that balances the attention given to both the local and the global aspects of the data. </li> 
  </ul> 
  <div class="readable-text" id="p166"> 
   <p>t-SNE is a widely popular algorithm. It can be used for studying the topology of an area, but a single t-SNE cannot be used for making a final assessment. Instead, multiple t-SNE plots should be created to make any final recommendation. Sometimes there are complaints that t-SNE is a black-box algorithm. This might be true to a certain extent. What makes the adoption of t-SNE harder is that it does not generate the same results in successive iterations. Hence, you might find t-SNE recommended only for exploratory analysis.</p> 
  </div> 
  <div class="readable-text" id="p167"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.5</span> Uniform manifold approximation projection</h2> 
  </div> 
  <div class="readable-text" id="p168"> 
   <p>UMAP is a powerful and popular dimensionality reduction technique. It is designed to preserve both the local and global structures of the dataset while reducing the complexity and dimensions of the high-dimensional dataset to a low-dimensional dataset. </p> 
  </div> 
  <div class="readable-text intended-text" id="p169"> 
   <p>UMAP was introduced in 2018 by Lealand McInnes, John Healy, and James Melville. UMAP makes the data more suitable for visualizations and data analysis. This relates to the concepts of topology and manifold theory. UMAP assumes that the high-dimensional dataset often lies on a manifold, which means a low-dimensional structure is embedded in a higher-dimensional space. Hence, it attempts to project this manifold into a lower dimensional space, preserving both the nearest neighbor relationships, which is nothing but the local structure, and the larger relationships, which is the global structure.</p> 
  </div> 
  <div class="readable-text" id="p170"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.5.1</span> Working with UMAP</h3> 
  </div> 
  <div class="readable-text" id="p171"> 
   <p>UMAP methodology uses the concept of fuzzy simplicity sets. These sets represent the probability distribution of distances between various data points and capture the underlying manifold structures.</p> 
  </div> 
  <div class="readable-text intended-text" id="p172"> 
   <p>The first step in UMAP is to construct a weighted graph where each of the data points is connected to its nearest neighbor based on a distance metric. Generally, the Euclidean distance is used as the distance metric. This graph construction is an abstract representation of the data structure in high dimensions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p173"> 
   <p>The next step is to optimize the graph. The graph is optimized in a lower dimension space by minimizing cross-entropy loss between the original high-dimensional relationships and the newly created low-dimensional relationships. This uses the stochastic gradient descent, producing the UMAP embeddings. We will study stochastic gradient descent in chapter 9.</p> 
  </div> 
  <div class="readable-text intended-text" id="p174"> 
   <p>There are two key parameters for UMAP:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p175"> <code>n_neighbours</code>—The number of nearest neighbors to consider for each point. Using this parameter, we balance the preservation of the local data structure as compared to the global data structure. </li> 
   <li class="readable-text" id="p176"> <code>min_dist</code>—This is used to control how tightly the points are clustered together. Smaller values of minimum distance keep the points much closer and hence create deeper clusters. The larger value for minimum distance will create lighter clusters, which are spread out. </li> 
  </ul> 
  <div class="readable-text" id="p177"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.5.2</span> Using UMAP</h3> 
  </div> 
  <div class="readable-text" id="p178"> 
   <p>The various uses of UMAP are as follows:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p179"> One of the most popular uses of UMAP is the visualization of high-dimensional datasets in the bioinformatics field. Gene datasets are quite complex and multidimensional, where each data point might be represented by hundreds or thousands of attributes. Using UMAP, researchers can virtually inspect the clusters and the underlying relationships in the dataset. The solution helps them identify cell types, developmental stages, and gene expression patterns. </li> 
   <li class="readable-text" id="p180"> UMAP is also applied to the natural language processing field by reducing the dimensionality of embeddings. It helps in the visualization of relationships between words or sentences or documents, making it easier to understand the similarities. </li> 
   <li class="readable-text" id="p181"> UMAP can also be applied to images. It helps in the visualization of the plastering of images based on various similarities, hence it is quite useful for competitive vision tasks to understand how similar images can be clustered together. </li> 
   <li class="readable-text" id="p182"> UMAP can be used with other clustering algorithms like k-means or DBSCAN. It can uncover the hidden patterns in large datasets and since it preserves both local and global structures, the clusters found in lower dimensional representations often provide more important groupings as compared to the original high-dimensional dataset. </li> 
  </ul> 
  <div class="readable-text" id="p183"> 
   <p>In addition to helping with visualizations, UMAP can also be used as a preprocessing step to reduce the dimensions of data. It can be used as an alternative to PCA or other solutions. By reducing the number of dimensions in a dataset, the model’s performance might be improved and the computation time is reduced.</p> 
  </div> 
  <div class="readable-text intended-text" id="p184"> 
   <p>The use of UMAP in Python is straightforward. The library <code>umap-learn</code> allows us to use the power of UMAP. </p> 
  </div> 
  <div class="readable-text" id="p185"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.5.3</span> Key points of UMAP</h3> 
  </div> 
  <div class="readable-text" id="p186"> 
   <p>Let’s now cover the key points of UMAP and compare it to other algorithms:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p187"> Since UMAP is a nonlinear solution, it can capture more complex datasets and patterns as compared to PCA. Recall that PCA is a linear dimensionality tradition technique, so when the data is not on a simple linear manifold, UMAP proves to be more accurate. </li> 
   <li class="readable-text" id="p188"> The goal of PCA is to explain the maximum variance in the entire dataset. On the other hand, UMAP balances both local and global structures and hence is more versatile for tasks like anomaly detection. </li> 
   <li class="readable-text" id="p189"> As compared to PCA, UMAP can be used for larger datasets. </li> 
   <li class="readable-text" id="p190"> UMAP is faster than the other nonlinear solution, t-SNE. t-SNE can preserve the local structure of the data, but it struggles with preserving the global structure, and it can lead to a misleading interpretation of the clusters. UMAP does a better job as it preserves both local and global structures. </li> 
   <li class="readable-text" id="p191"> UMAP results are much more stable and consistent across multiple iterations. For other algorithms, the results can be unstable and might change with different values of random seeds. </li> 
  </ul> 
  <div class="readable-text" id="p192"> 
   <p>UMAP has gained a lot of popularity recently and has become a go-to tool for machine learning and AI solutions. It is fast and can preserve both local and global data structures. Hence it is a strong option compared to other dimensionality reduction solutions like PCA, t-SNE, and autoencoders.</p> 
  </div> 
  <div class="readable-text" id="p193"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.6</span> Case study</h2> 
  </div> 
  <div class="readable-text" id="p194"> 
   <p>In chapter 3, we explored a case study for the telecom industry reducing dimensionality. In this chapter, we will examine a small case study wherein t-SNE or MDS can be utilized for dimensionality reduction. </p> 
  </div> 
  <div class="readable-text intended-text" id="p195"> 
   <p>Have you heard about hyperspectral images? As you know, we humans see the colors of visible light in mostly three bands: long, medium, and short wavelengths. The long wavelengths are perceived as red, medium as green, and short as blue. All the other colors human beings perceive are simply mixtures of these three, and that is what allows screens and printers to work with only three colors. Spectral imaging, on the other hand, divides the spectrum into many more bands, and this technique can be extended beyond the visible and hence is used across biology, physics, geoscience, astronomy, agriculture, and many more avenues. Hyperspectral imaging collects and processes information from across the electromagnetic spectrum. It obtains the spectrum for each of the pixels in the image. See figure 6.18.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p196">  
   <img alt="figure" src="../Images/CH06_F18_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.18</span> Hyperspectral image of “sugar end” potato strips shows invisible defects (Source: SortingExpert, CC BY-SA 3.0)</h5>
  </div> 
  <div class="readable-text intended-text" id="p197"> 
   <p>One such dataset is the Pavia University dataset. The dataset is curated by the ROSIS sensor in Pavia, northern Italy. The details of the dataset are given next, and the dataset can be downloaded from <a href="https://mng.bz/nRVa">https://mng.bz/nRVa</a>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p198"> 
   <p>There are 103 spectral bands in this dataset. The HIS size is 610 * 340 pixels, and it contains nine classes. Such a type of data can be used for crop analysis, mineral examination and exploration, etc. Since this data also contains information about geological patterns, it is quite useful for scientific purposes. Before developing any image recognition solution, we have to reduce the number of dimensions for this dataset. The computation cost will be much higher if we have a large number of dimensions. Hence, we want a lower representative number of dimensions. Figure 6.19 shows a few example bands. You are advised to download the dataset (which is also pushed at the GitHub repository) and use the various dimensionality reduction techniques on the dataset to reduce the number of dimensions. There can be many other image datasets and complex business problems where t-SNE and MDS can be of pragmatic use. <span class="aframe-location"/> </p> 
  </div> 
  <div class="browsable-container figure-container" id="p199">  
   <img alt="figure" src="../Images/CH06_F19_Verdhan.png"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.19</span> Example of bands in the dataset. These are only random examples. </h5>
  </div> 
  <div class="readable-text" id="p200"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.7</span> Concluding thoughts</h2> 
  </div> 
  <div class="readable-text" id="p201"> 
   <p>Dimensionality reduction is quite an interesting and useful field. It makes machine learning less expensive and less time-consuming. Imagine that you have a dataset with thousands of attributes or features. You do not know the data very well, the business understanding is limited, and, at the same time, you have to find the patterns in the dataset. You are not even sure if the variables are all relevant or just random noise. At such a moment, when we want to make the dataset less complex to crack and reduce the computational time, dimensionality reduction is the solution. </p> 
  </div> 
  <div class="readable-text intended-text" id="p202"> 
   <p>We covered dimensionality reduction techniques earlier in the book. This chapter covers three advanced techniques: t-SNE, MDS, and UMAP. All three techniques should not be considered a substitute for the other, easier techniques we discussed. Rather, they can be useful if we are not getting meaningful results with basic algorithms. It is always advised to use PCA first and then try the advanced techniques.</p> 
  </div> 
  <div class="readable-text intended-text" id="p203"> 
   <p>The complexity of the book is increasing. This chapter started with images—but we have only wet our toes. In the next chapter, we deal with text data. Perhaps you will find it very interesting and useful.</p> 
  </div> 
  <div class="readable-text" id="p204"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.8</span> Practical next steps and suggested readings</h2> 
  </div> 
  <div class="readable-text" id="p205"> 
   <p>The following provides suggestions for what to do next and offers some helpful reading:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p206"> Use the vehicles dataset used in chapter 2 for clustering and implement MDS on it. Compare the performance on clustering before and after implementing MDS. </li> 
   <li class="readable-text" id="p207"> Get the datasets used in chapter 2 for Python examples and use them for implementing MDS. </li> 
   <li class="readable-text buletless-item" id="p208"> For MDS, refer to the following research papers: 
    <ul> 
     <li> “Dimensionality Reduction: A Comparative Review,” by Lauren van der Maaten, Eric Postma, and H. Japp Van Den Herik: <a href="https://mng.bz/eyxQ">https://mng.bz/eyxQ</a> </li> 
     <li> “Multidimensional Scaling-Based Data Dimension Reduction Method for Application in Short-Term Traffic Flow Prediction for Urban Road Network,” by Satish V. Ukkusuri and Jian Lu: <a href="https://mng.bz/pKmz">https://mng.bz/pKmz</a> </li> 
    </ul> </li> 
   <li class="readable-text buletless-item" id="p209"> Get t-SNE research papers from the following links and study them: 
    <ul> 
     <li> “Visualizing Data Using t-SNE,” by Laurens van der Maaten and Geoffrey Hinton: <a href="https://mng.bz/OBaE">https://mng.bz/OBaE</a> </li> 
     <li> “The Art of Using t-SNE for Single Cell Transcriptomics”: <a href="https://mng.bz/YD9A">https://mng.bz/YD9A</a> </li> 
    </ul> </li> 
   <li class="readable-text" id="p210"> See the paper “Performance Evaluation of t-SNE and MDS Dimensionality Reduction Techniques with KNN, SNN, and SVM Classifiers”: <a href="https://arxiv.org/pdf/2007.13487.pdf">https://arxiv.org/pdf/2007.13487.pdf</a> </li> 
  </ul> 
  <div class="readable-text" id="p211"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p212"> MDS is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while preserving distances. </li> 
   <li class="readable-text" id="p213"> There are three types of MDS: classical, metric, and nonmetric. </li> 
   <li class="readable-text" id="p214"> Classical MDS uses Euclidean distances, aligning original and fitted distances. </li> 
   <li class="readable-text" id="p215"> Nonmetric MDS ranks distances rather than using absolute values. </li> 
   <li class="readable-text" id="p216"> Metric MDS transforms distances to fit a lower dimensional space. </li> 
   <li class="readable-text" id="p217"> MDS involves calculating distances and optimizing a stress cost function with gradient descent, though it can be computationally intensive and is prone to local minima problems. </li> 
   <li class="readable-text" id="p218"> MDS works iteratively and does not make assumptions about data distribution, making it versatile for choosing distance metrics compared to PCA. </li> 
   <li class="readable-text" id="p219"> t-SNE is a nonlinear dimensionality reduction technique and is particularly effective for high-dimensional and complex datasets like images. </li> 
   <li class="readable-text" id="p220"> t-SNE optimizes similarity between data points in both high- and low-dimensional spaces using the Cauchy distribution and KL divergence. </li> 
   <li class="readable-text" id="p221"> t-SNE has an edge over PCA due to its nonlinear nature, though it involves hyperparameters like perplexity. </li> 
   <li class="readable-text" id="p222"> UMAP is another dimensionality reduction method that efficiently preserves both local and global data structures and is faster and more stable than t-SNE. </li> 
   <li class="readable-text" id="p223"> Python implementations are available for both MDS and t-SNE. </li> 
   <li class="readable-text" id="p224"> MDS is one of the advanced dimensionality reduction techniques, requiring optimization of a loss function or cost function. </li> 
  </ul>
 </body></html>