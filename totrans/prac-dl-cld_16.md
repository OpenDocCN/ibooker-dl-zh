# 第十五章。成为创客：探索边缘嵌入式 AI

由客座作者 Sam Sterckval 撰写

您知道如何构建出色的 AI 应用程序，但您想要更多。您不想仅仅在某台计算机上运行 AI 软件，您想将其带到现实世界中。您想要构建设备，使事物更具互动性，使生活更轻松，为人类服务，或者仅仅是为了好玩。也许您想要构建一个互动绘画，当您看着它时会微笑。门口的摄像头在未经授权的人试图偷取包裹时发出响亮的警报。也许是一个可以分类回收物和垃圾的机械臂。在树林中防止盗猎的设备，也许？或者一架可以自主巡视大面积并在洪水期间识别处于困境中的人的无人机。甚至可能是一辆可以自行驾驶的轮椅。您需要的是一个智能电子设备，但您将如何构建它，它将花费多少，它将有多强大？在本章中，我们开始探讨这些问题。

我们将看看如何在嵌入式设备上实现 AI——这是您可能在“创客”项目中使用的设备。创客是具有 DIY 精神的人，他们利用自己的创造力构建新事物。创客通常从业余爱好者开始，他们是热爱解决问题的人、机器人专家、创新者，有时也是企业家。

本章的目的是激发您选择适当设备的能力（这意味着不要试图在微小 CPU 上运行重型 GAN，或者获取一台千核 GPU 来运行“不是热狗”分类器），并尽快轻松地为测试设置它。我们通过探索一些更为人熟知的设备，看看我们如何使用它们来执行我们模型的推理。最后，我们将看看全球的创客们如何利用 AI 来构建机器人项目。

让我们迈出第一步，看看嵌入式 AI 设备的当前情况。

# 探索嵌入式 AI 设备的现状

在本节中，我们探索一些知名的嵌入式 AI 设备，列在表 15-1 中。在我们进行测试之前，我们将讨论它们的内部工作原理和它们之间的区别。

表 15-1\. 设备列表

| **Raspberry Pi 4** | **截至目前为止最著名的单板计算机** |
| --- | --- |
| **Intel Movidius NCS2** | 使用 16 核视觉处理单元（VPU）的 USB 加速器 |
| **Google Coral USB** | 使用自定义的谷歌应用特定集成电路（ASIC）的 USB 加速器 |
| **NVIDIA Jetson Nano** | 使用 CPU 和 128 核 CUDA GPU 组合的单板计算机 |
| **PYNQ-Z2** | 使用 CPU 和 50k CLB 现场可编程门阵列（FPGA）组合的单板计算机 |

我们不想说哪个更好，我们想学习如何为特定项目选择设置。我们通常在早晨上班途中看不到法拉利。更小、更紧凑的汽车更常见，它们同样有效，甚至更好。同样，对于电池供电的无人机来说，使用功能强大的 1000 美元以上的 NVIDIA 2080 Ti GPU 可能过于夸张。以下是我们应该问自己的一些问题，以了解哪种边缘设备最适合我们的需求：

1.  设备有多大（例如，与硬币相比）？

1.  设备的成本是多少？考虑一下您是否预算有限。

1.  设备有多快？它会处理单个 FPS 还是 100 FPS？

1.  设备通常需要多少功率（瓦特）？对于电池供电项目，这可能至关重要。

在考虑这些问题的同时，让我们逐个探索图 15-1 中的一些设备。

![嵌入式 AI 设备的家庭照片；从顶部开始，顺时针方向：PYNQ-Z2，Arduino UNO R3，英特尔 Movidius NCS2，树莓派 4，谷歌珊瑚 USB 加速器，NVIDIA Jetson Nano，以及一个欧元硬币用于参考在中间](img/00057.jpeg)

###### 图 15-1. 嵌入式 AI 设备的家庭照片；从顶部开始，顺时针方向：PYNQ-Z2，Arduino UNO R3，英特尔 Movidius NCS2，树莓派 4，谷歌珊瑚 USB 加速器，NVIDIA Jetson Nano，以及一个欧元硬币用于参考在中间

## 树莓派

因为我们将讨论面向制造商的嵌入式设备，让我们从与电子项目普遍同义的一个开始：树莓派（图 15-2）。它便宜，易于构建，并且有一个庞大的社区。

![树莓派 4](img/00017.jpeg)

###### 图 15-2. 树莓派 4

| **尺寸** | 85.6 毫米 x 56.5 毫米 |
| --- | --- |
| **价格** | 起价$35 |
| **处理单元** | ARM Cortex-A72 CPU |
| **功率评级** | 15W |

首先，什么是树莓派？它是一台“单板计算机”，这意味着所有计算都可以在一块印刷电路板（PCB）上完成。这款单板计算机的第四版搭载了一个 Broadcom SoC（片上系统），包含（最重要的）一个 ARMv8-A72 四核 CPU，一个 VideoCore VI 3D 单元，以及一些视频解码器和编码器。它配备了不同大小的 RAM，最高可达 4GB。

这个版本在性能上比树莓派 3 有了很大的提升，但效率稍微降低了一些。这是因为树莓派 3 拥有一个 ARMv8-A53 核心，这是高效版本，而 ARMv8-A72 核心（用于第 4 版）是高性能版本。我们可以看到这反映在电源供应建议中，树莓派 3 为 2.5 安培。对于树莓派 4，这变成了 3 安培。还要注意的是，树莓派 4 具有 USB 3 端口，而树莓派 3 只有 USB 2 端口。这将在未来证明是重要信息。

现在让我们来谈谈一些机器学习的内容。无论树莓派 4 变得多么强大，它仍然主要由四个顺序 CPU 核心组成（现在支持乱序执行）。它有 VideoCore VI，但截至目前还没有适用于这种架构的 TensorFlow 版本。Idein Inc.的中村浩一（Koichi Nakamura）构建了一个名为[py-videocore](https://oreil.ly/AuEzr)的 Python 库，用于访问树莓派 SoC 的四核处理单元（QPUs；类似 GPU 的核心）。他以前用它加速了神经网络，但目前还不能简单加速 TensorFlow。也有用于访问这些核心的 C++库。但正如你可能怀疑的那样，这些核心并不那么强大，所以即使用于加速神经网络，也可能无法产生期望的结果。为了简单起见，我们不会深入研究这些库，因为深入算法已超出本书的范围。而且，正如我们将在接下来看到的，这可能根本不是必要的。

最后，树莓派已被证明是一个非常有用的硬件设备，可用于许多任务。它经常用于教育目的，以及由制造商使用。您会惊讶地发现有多少行业在工业环境中使用树莓派（有一种叫做 netPi 的东西，它只是一个带有坚固外壳的树莓派）。

## 英特尔 Movidius 神经计算棒

现在让我们直接深入探讨树莓派成为许多项目首选基础的原因之一：英特尔 Movidius 神经计算棒 2（图 15-3），这是英特尔推出的第二代 USB 加速器。

![英特尔神经计算棒 2](img/00306.jpeg)

###### 图 15-3. 英特尔神经计算棒 2

| **尺寸** | 72.5 毫米 x 27 毫米 |
| --- | --- |
| **价格** | $87.99 |
| **处理单元** | Myriad X VPU |
| **功率评级** | 1W |

它的功能非常简单：您提供一个神经网络和一些数据，它会执行推理所需的所有计算。它只通过 USB 连接，因此您可以将其连接到树莓派上，运行 USB 加速器上的推理，并释放树莓派的 CPU 来执行您想要执行的所有其他酷炫操作。

它基于 Myriad VPU。第一代使用了 Myriad 2 VPU，而这一代使用了 Myriad X VPU。VPU 包含 16 个 SHAVE（流式混合架构向量引擎）核心，类似于 GPU 核心，但不太适合图形相关操作。它具有片上 RAM，这在进行神经网络推理时特别有用，因为这些网络在计算时会产生大量数据，这些数据可以直接存储在核心旁边，从而大大降低访问时间。

## Google Coral USB 加速器

Google Coral（图 15-4），包含 Google Edge TPU，是我们将讨论的第二个 USB 加速器。首先，解释一下为什么我们要看两种不同的 USB 加速器。如前所述，英特尔棒具有多个 SHAVE 核心，具有多个可用指令，类似于 GPU 核心，因此充当处理单元。另一方面，Google Edge TPU 是一种 ASIC（特定应用集成电路），它也进行一些处理，但只服务于一个目的（因此有“特定”关键字）。ASIC 具有一些硬件固有的属性，其中一些非常好，另一些则不太好：

速度

由于 TPU 内部的所有电子电路只服务于一个目的，因此在解码操作方面没有额外的开销。您输入输入数据和权重，它会几乎立即给出结果。

效率

所有 ASIC 只服务于一个目的，因此不需要额外的能量。ASIC 的性能/瓦特指标通常是业内最高的。

灵活性

ASIC 只能执行其设计用途；在这种情况下，那将是加速 TensorFlow Lite 神经网络。您需要使用 Google Edge TPU 编译器和 8 位*.tflite*模型。

复杂性

Google 本质上是一家软件公司，它知道如何使事物易于使用。这正是它在这里所做的。Google Coral 非常容易上手。

那么 Google Edge TPU 是如何工作的呢？关于 Edge TPU 本身的信息尚未公开，但有关 Cloud TPU 的信息已经公开，因此可以假设 Edge TPU 的工作方式与 Cloud TPU 大致相同。它具有专用硬件来执行乘法-加法、激活和池化操作。芯片上的所有晶体管都已连接在一起，以便接收权重和输入数据，并以高度并行的方式计算输出。芯片上最大的部分（除了片上内存外）是一个部分，确切地说就是“矩阵乘法单元”。它使用一种相当聪明但并非全新的原则，称为[*系统执行*](https://oreil.ly/R5VpP)。这种执行原则通过将中间结果存储在处理元素中而不是存储在内存中来降低内存带宽。

![Google Coral USB 加速器](img/00265.jpeg)

###### 图 15-4。Google Coral USB 加速器

| **尺寸** | 65 毫米 x 30 毫米 |
| --- | --- |
| **价格** | $74.99 |
| **处理单元** | Google Edge TPU |
| **功率评级** | 2.5W |

###### 注意

我们讨论的两种 USB 加速器之间的主要区别是什么？Google Coral 更强大，但比英特尔 Movidius NCS2（稍微）不太灵活。也就是说，Coral 设置和使用起来要容易得多，尤其是在您训练和转换模型之后。

## NVIDIA Jetson Nano

关于另一种硬件：NVIDIA Jetson Nano（图 15-5），一款单板 AI 计算机。有点像树莓派，但配备了一个 128 核 CUDA 启用的 Maxwell GPU。GPU 的添加使其有点类似于带有 Intel NCS2 的树莓派，但 NCS2 有 16 个核心，而这个 Jetson 有 128 个。它还包含什么？一个四核 A57 ARMv8 CPU，是树莓派 4 中 ARMv8 A72 的前身（它比树莓派 4 早几个月），4GB 的低功耗双数据速率 4（LPDDR4）RAM 内存，非常方便地与 CPU 和 GPU 共享，这使您可以在 GPU 上处理数据，而无需复制它。

![NVIDIA Jetson Nano](img/00120.jpeg)

###### 图 15-5\. NVIDIA Jetson Nano

| **尺寸** | 100 毫米 x 79 毫米 |
| --- | --- |
| **价格** | $99.00 |
| **处理单元** | ARM A57 + 128 核 Maxwell GPU |
| **功率评级** | 10W（在高负载下可能会更高） |

这款单板计算机的特点是，正如之前所说，这些 GPU 核心是 CUDA 启用的，因此是的，它们可以运行 TensorFlow-GPU 或任何其他框架，这将与树莓派相比产生很大的差异，特别是当您计划训练网络时。而且，这对于 GPU 来说价格便宜。目前，Jetson Nano 售价为$99，其中包括一款性能相当高的 ARM CPU 和一个 128 核 GPU。相比之下，带有 4GB 内存的树莓派 4 售价约为$55，Coral USB 加速器约为$75，Movidius NCS2 约为$90。后两者不是独立的，至少需要额外的树莓派才能真正发挥作用，而树莓派没有可以轻松加速深度学习应用的 GPU。

关于 Jetson Nano 的另一个说明：它可以加速默认的 TensorFlow 32 位浮点运算，但如果使用 16 位浮点运算，它将变得更加高效，如果使用其自己的 TensorRT 框架，效率将更高。幸运的是，该公司有一个名为 TensorFlow-TensorRT（TF-TRT）的小型开源库，它将自动加速可用的操作，并允许 TensorFlow 执行其余操作。与 TensorFlow-GPU 相比，这个库提供了大约四倍的加速。考虑到所有这些，这使得 Jetson Nano 成为最灵活的设备。

## FPGA + PYNQ

现在是系好安全带的时候了，因为我们即将深入探讨电子领域的黑暗世界。基于 Xilinx Zynq 芯片系列的 PYNQ 平台（图 15-6），在很大程度上与本主题中讨论的其他设备完全不同。如果您稍微研究一下，您会发现它配备了一款双核 ARM-A9 CPU，时钟频率高达 667 MHz。您会想到的第一件事是“你是认真的吗？与树莓派的 1.5 GHz 四核 A72 相比，这太荒谬了！”您是对的，这款设备中的 CPU 在很大程度上是毫无价值的。但它还有另一项功能——一个 FPGA。

![Xilinx PYNQ-Z2](img/00081.jpeg)

###### 图 15-6\. Xilinx PYNQ-Z2

| **尺寸** | 140 毫米 x 87 毫米 |
| --- | --- |
| **价格** | $119.00 |
| **处理单元** | Xilinx Zynq-7020 |
| **功率评级** | 13.8W |

### FPGAs

要理解 FPGA 是什么，我们首先需要看一些熟悉的概念。让我们从 CPU 开始：一个知道一系列操作的设备，称为指令集架构（ISA）。这是一个定义 CPU 可以执行的所有操作的集合，通常包含操作，例如“加载字”（字通常是等于 CPU 数据通路大小的位数的值，通常为 32 位或 64 位），它将某个值加载到 CPU 的内部寄存器中，“加法”，可以将两个内部寄存器相加，并将结果存储在第三个寄存器中，例如。

CPU 之所以能够做到这一点，是因为它包含了一大堆晶体管（如果你愿意，可以将其视为电气开关），这些晶体管被硬连线在一起，以便 CPU 自动翻译操作并执行操作的预期目的。软件程序只是这些操作的一个非常长的列表，按照精心考虑的顺序。单核 CPU 将逐个接收操作并执行它们，速度相当惊人。

让我们来看看并行性。正如你所知，神经网络主要由卷积或线性节点组成，这些节点都可以转换为矩阵乘法。如果我们看一下这些操作背后的数学，我们会发现每个层的每个输出点可以独立于其他输出点计算，如图 15-7 所示。

![矩阵乘法](img/00038.jpeg)

###### 图 15-7。矩阵乘法

我们现在可以看到所有这些操作实际上都可以以并行方式完成。我们的单核 CPU 无法做到这一点，因为它一次只能执行一个操作，但这就是多核 CPU 的用武之地。四核 CPU 可以同时执行四个操作，这意味着理论上的加速四倍。英特尔 Movidius NCS2 中的 SHAVE 核和 Jetson Nano 中的 CUDA 核可能不像 CPU 核心那样复杂，但它们足够好用于这些乘法和加法，而 NCS2 有 16 个，Jetson Nano 有 128 个，而像 RTX 2080 Ti 这样的大型 GPU 甚至有 4352 个 CUDA 核。现在很容易看出为什么 GPU 在执行深度学习任务时比 CPU 更好。

让我们回到 FPGAs。而 CPU 和 GPU 是巨大的晶体管集合，硬连线执行一组指令，你可以把 FPGA 想象成同样巨大的晶体管集合，但没有连线。你可以选择它们如何连线，并且随时重新连线，使它们可重构。你可以把它们连线成为 CPU。你也可以找到人们将它们连线成为 GPU 的原理图和项目。但最有趣的是，它们甚至可以被连线成为与你的深度学习神经网络完全相同的架构，这实际上使它们成为你网络的物理实现。这里故意使用了“连线”一词。通常，你会发现人们用“程序”这个词来谈论这种配置，但这可能会让人困惑。你所做的是重新配置实际的硬件，不像 CPU 或 GPU 那样下载一个硬件可以运行的程序。

### PYNQ 平台

我们通常将 FPGA 的“连线”文件称为*比特流*或*位图*，因为你刷入芯片的内容基本上只是应该建立的连接的地图。正如你可以想象的那样，制作这些比特流比运行 Python 脚本要复杂得多。这就是 PYNQ 的用武之地。它的标语是“Zynq 的 Python 生产力”。该公司安装了 PYNQ 图像，自动在芯片内部的 ARM 上运行 Jupyter Notebook，并附带一些基本的比特流；然而，未来可能会有更多的比特流可用。在 PYNQ 世界中，这些比特流被称为*叠加*。

如果你寻找示例，你很快会找到一个名为“BNN-PYNQ”的示例，其中有一个简单的 VGG 风格、六层 CNN，可以在 PYNQ-Z1 和 Z2 上以大约 3000 FPS 运行，在 ZCU104 上接近 10000 FPS，该芯片板载了 Zynq 芯片的 Ultrascale+版本。这些数字看起来相当疯狂，但要考虑到它们是在 32x32 像素图像上运行，而不是通常的 224x224 像素图像，并且这个网络是“二值化”的，这意味着它的权重和激活只有一位，而不是 TensorFlow 的 32 位。为了更好地比较性能，我尝试在 Keras 中重新创建一个类似的网络。

我构建了 FP32 模型并在 CIFAR-10 数据集上对其进行了训练。 CIFAR-10 数据集可以在 Keras 中轻松使用`keras.datasets.cifar10`获得。 FP32 模型达到了大约 17％的错误率，令人惊讶的是，仅比二进制模型好了 2％。在 Intel i9 八核 CPU 上，推理速度约为 132 FPS。据我所知，在 CPU 上没有一种简单有效地使用二进制网络的方法——您需要深入研究一些特定的 Python 软件包或一些 C 代码，以充分利用硬件。您可能会实现三到五倍的加速。然而，这仍然远远不及低端 FPGA，并且 CPU 通常会在这样做时消耗更多电力。当然，一切都有两面性，对于 FPGA 来说，这必须是设计的复杂性。存在开源框架，即由 Xilinx 支持的[FINN](https://oreil.ly/CijXF)框架。其他制造商提供额外的框架，但没有一个能与 TensorFlow 等易于使用的软件包和框架相提并论。为 FPGA 设计神经网络还涉及大量的电子知识，因此超出了本书的范围。

## Arduino

Arduino（图 15-8）在通过传感器和执行器与现实世界进行交互时可以让您的生活变得更加轻松。

微控制器单元（MCUs）Arduino 是围绕 8 位高级虚拟 RISC（AVR）ATMega328p 微控制器构建的微控制器开发板，运行速度为 16 MHz（所以是的，我们不要试图在上面运行一个体面的神经网络）。微控制器是每个人都接触过的东西——您几乎可以在稍微电子化的所有东西中找到微控制器，从每个屏幕/电视到您的桌面键盘，甚至自行车灯甚至可能包含微控制器以启用闪烁模式。

这些板子有各种形状，大小和性能，尽管 Arduino UNO 中的 MCU 性能相对较低，但更强大的 MCU 可用。 ARM Cortex-M 系列可能是最知名的。最近，TensorFlow 和 uTensor（一种针对 MCU 的极轻量级机器学习推理框架）联手，使得在这些 MCU 上运行 TensorFlow 模型成为可能。

![Arduino UNO R3](img/00327.jpeg)

###### 图 15-8\. Arduino UNO R3

| **尺寸** | 68.6 毫米 x 53.3 毫米 |
| --- | --- |
| **价格** | 少于 10.00 美元 |
| **处理单元** | AVR ATMega328p |
| **功率评级** | 0.3 瓦特 |

尽管 Arduino UNO 实际上不适用于执行大量计算，但它具有许多其他优点，使其成为制作者工作台上不可或缺的一部分。它便宜，简单，可靠，并且周围有一个庞大的社区。大多数传感器和执行器都有某种 Arduino 库和盾牌，这使得它成为一个非常容易与之交互的平台。AVR 架构是一种有点过时的 8 位修改的哈佛架构，但学习起来非常容易，尤其是在其顶部的 Arduino 框架上。围绕 Arduino 的庞大社区显然带来了许多教程，因此只需查找您选择的传感器的任何 Arduino 教程，您肯定会找到您项目所需的内容。

###### 提示

使用 Python 的串行库，您可以通过 USB 电缆使用通用异步收发器（UART）轻松地与 Arduino 进行接口，以便来回发送命令或数据。

# 嵌入式 AI 设备的定性比较

表 15-2 总结了我们谈论过的平台，图 15-9 绘制了每个设备的性能与复杂性之间的关系。

表 15-2\. 测试平台的优缺点

| **嵌入式设备** | **优点** | **缺点** |
| --- | --- | --- |
| **树莓派 4** |

+   简单

+   庞大的社区

+   易得

+   便宜

|

+   缺乏计算能力

|

| **Intel Movidius NCS2** |
| --- |

+   简单优化

+   支持大多数平台

|

+   加速而言相对昂贵

+   不那么强大

|

| **Google Coral USB** |
| --- |

+   易于入门

+   巨大的加速

+   价格与加速比

|

+   仅支持*.tflite*

+   需要八位量化

|

| **NVIDIA Jetson Nano** |
| --- |

+   一体化

+   训练是可能的

+   便宜

+   实现良好性能真的很容易

+   CUDA GPU

|

+   性能可能仍然不足

+   高级优化可能会很复杂

|

| **PYNQ-Z2** |
| --- |

+   潜在的巨大功率效率

+   实际硬件设计

+   潜在的巨大性能

+   比 Jetson Nano 更加多功能

|

+   非常复杂

+   设计流程长

|

![性能与使用复杂性之间的关系（圆圈的大小代表价格）](img/00286.jpeg)

###### 图 15-9。性能与使用复杂性之间的关系（圆圈的大小代表价格）

在本节中，我们尝试通过使用 MobileNetV2 对相同图像进行 250 次分类来测试一些平台，顶部平台经过 ImageNet 数据集训练。我们每次使用相同的图像，因为这将使我们能够消除在系统中可能发生的数据瓶颈的一部分，这些系统没有足够的 RAM 来存储所有权重和许多不同的图像。

好的，让我们先找一张图片。谁不喜欢猫呢？所以，让我们有一张美丽猫的图片（图 15-10），它的尺寸正好是 224 x 224 像素，这样我们就不需要调整图像。

![我们将用于实验的猫的图像](img/00246.jpeg)

###### 图 15-10。我们将用于实验的猫的图像

拥有基准测试总是不错的，所以让我们先在 PC 上运行模型。第二章解释了如何做到这一点，所以让我们继续编写一个脚本，该脚本将调用预测 250 次，并测量这需要多长时间。您可以在本书的 GitHub 网站上找到完整的脚本（请参阅[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）位于*code/chapter-15*：

```py
$ python3 benchmark.py
Using TensorFlow backend.
tf version : 1.14.0
keras version : 2.2.4
input tensor shape : (1, 224, 224, 3)
warmup prediction
[('n02124075', 'Egyptian_cat', 0.6629321)]
starting now...
Time[s] : 16.704
FPS     : 14.968
Model saved.
```

既然我们已经建立了基准测试，现在是时候开始研究如何在嵌入式设备上运行这个模型，看看我们是否以及如何将性能提升到一个有用的水平。

# 与树莓派一起动手

我们从先前讨论的设备中最知名和最基本的设备开始：树莓派（简称 RPi）。

我们首先安装 Raspbian，这是专为 Raspberry Pi 制作的 Linux 变体。为了简洁起见，让我们假设我们有一个已安装、更新、连接并准备就绪的 Raspberry Pi。我们可以直接在 Pi 上安装 TensorFlow，这应该可以使用`pip`安装程序完成：

```py
$ pip3 install tensorflow
```

###### 注意

由于最近切换到 Raspbian Buster，我们遇到了一些问题，通过使用以下`piwheel`解决了这些问题：

```py
$ pip3 install
https://www.piwheels.org/simple/tensorflow/tensorflow
-1.13.1-cp37-none-linux_armv7l.whl
```

使用`pip`安装 Keras 很简单，但不要忘记安装`libhdf5-dev`，如果您想要将权重加载到神经网络中，则会需要它：

```py
$ pip3 install keras
$ apt install libhdf5-dev
```

在 RPi 上安装 OpenCV（在代码中称为 cv2）可能是一种负担（特别是在最近切换操作系统后），我们可以使用 PIL 加载图像而不是 OpenCV。这意味着用 PIL 替换 cv2 的导入，并更改代码以使用此库加载图像：

```py
#input_image = cv2.imread(input_image_path)
#input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)
input_image = PIL.Image.open(input_image_path)
input_image = np.asarray(input_image)
```

###### 提示

与 OpenCV 不同，PIL 以 RGB 格式加载图像，因此不再需要从 BGR 转换为 RGB。

就是这样！现在应该可以在您的 Raspberry Pi 上运行完全相同的脚本：

```py
$ python3 benchmark.py
Using TensorFlow backend.
tf version : 1.14.0
keras version : 2.2.4
input tensor shape : (1, 224, 224, 3)
warmup prediction
[('n02124075', 'Egyptian_cat', 0.6629321)]
starting now...
Time[s] : 91.041
FPS     : 2.747
```

正如您所看到的，它运行速度慢得多，下降到不到三帧每秒。是时候考虑如何加快速度了。

我们可以做的第一件事是看看 TensorFlow Lite。TensorFlow 内置了一个转换器，让我们可以轻松地将任何模型转换为 TensorFlow Lite（*.tflite*）模型。

我们继续编写一个小脚本，与原始基准测试脚本非常相似，它将设置好一切以使用 TensorFlow Lite 模型并运行 250 次预测。当然，这个脚本也可以在本书的 GitHub 网站上找到（参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）。

让我们继续运行这个脚本，以便评估我们加快速度的努力。

```py
$ python3 benchmark_tflite.py
Using TensorFlow backend.
input tensor shape : (1, 224, 224, 3)
conversion to tflite is done
INFO: Initialized TensorFlow Lite runtime.
[[('n02124075', 'Egyptian_cat', 0.68769807)]]
starting now (tflite)...
Time[ms] : 32.152
FPS      : 7.775
```

###### 注意

[Google Coral 网站](https://coral.withgoogle.com)提供了可使用 TensorFlow Lite 运行的 8 位量化 CPU 模型。

对于树莓派 4，我们观察到速度增加了三倍（参见 Table 15-3）。对于一个快速尝试来说，这还不错，但我们仍然只达到了 7.8 FPS，对于许多应用来说已经足够好了。但是如果我们最初的计划是在实时视频上做一些事情呢？那么这还不够。对模型进行量化可以实现更高的性能，但鉴于树莓派的 CPU 并未针对 8 位整数运算进行优化，这只会带来一点点性能提升。

表 15-3。树莓派基准测试

| **设置** | **FPS** |
| --- | --- |
| Raspberry Pi 4 (tflite, 8-bit) | 8.6 |
| Raspberry Pi 4 (tflite) | 7.8 |
| Raspberry Pi 3B+ (tflite, 8-bit) | 4.2 |
| Raspberry Pi 4 | 2.7 |
| Raspberry Pi 3B+ | 2.1 |

那么我们如何进一步加快速度，使其对自动赛车或自动无人机等应用变得更有用呢？我们之前提到过：USB 加速器。

# 使用 Google Coral USB 加速器加速

没错，这些加速器现在非常方便。我们的硬件设置可以保持完全不变，只需要稍微添加一点内容。那么让我们看看：如何让 Google Coral USB 加速器在树莓派上运行，并且是否可以加快速度？

首先要做的事情。假设我们手头有一个 USB 加速器，我们应该做的第一件事是从供应商那里找到一个“入门指南”。查看[*https://coral.withgoogle.com/*](https://coral.withgoogle.com/)，转到*Docs>USB Accelerator>Get Started*，您将在几分钟内开始运行。

截至目前，Coral USB 加速器尚未完全兼容树莓派 4，但在调整*install.sh*脚本后，很容易让它运行起来。您只需要简单地在安装脚本中添加一部分，以便它识别 Pi 4，如图 15-11 所示。

![Google Coral install script changes](img/00206.jpeg)

###### 图 15-11。Google Coral 安装脚本更改

现在*install.sh*脚本将正确运行。然后，我们需要重命名一个*.so*文件，以便它也可以在 Python 3.7 中运行。使用 Unix 的复制命令来完成这一步：

```py
$ sudo cp /usr/local/lib/python3.7/dist-
packages/edgetpu/swig/_edgetpu_cpp_wrapper.cpython-
35m-arm-linux-gnueabihf.so
/usr/local/lib/python3.7/dist-
packages/edgetpu/swig/_edgetpu_cpp_wrapper.cpython-
37m-arm-linux-gnueabihf.so
```

完成这些步骤后，一切应该正常工作，Google 的演示应该可以运行。

现在我们已经让 Coral 运行起来了，让我们再次尝试对同一个 MobileNetV2 模型进行基准测试。但这次，必须使用它的量化版本，因为 Google 的 Edge TPU 仅支持 8 位整数运算。Google 提供了经过量化和转换的 MobileNetV2 模型，可以直接在 Edge TPU 上使用。我们可以从 Coral 网站下载它，路径为*Resources > See pre-compiled models > MobileNetV2(ImageNet) > Edge TPU Model*。

###### 注意

如果您想为 Edge TPU 创建、训练、量化和转换自己的模型，这需要更多工作，不能使用 Keras 完成。您可以在[Google Coral 网站](https://oreil.ly/ydoSy)上找到如何操作的信息。

我们有一个可用的 USB 加速器和一个可以在其上运行的模型。现在，让我们制作一个新的脚本来测试其性能。我们即将制作的文件与之前的文件非常相似，并且很多内容直接来自 Google 提供的 Coral 示例。而且，像往常一样，这个文件可以在 GitHub 网站上下载（请参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）：

```py
$ python3 benchmark_edgetpu.py
INFO: Initialized TensorFlow Lite runtime.
warmup prediction
Egyptian cat
0.59375
starting now (Edge TPU)...
Time[s] : 1.042
FPS     : 240.380
```

是的，没错。它已经完成了 250 次分类！对于带有 USB3 的 Raspberry Pi 4，只需要 1.04 秒，这相当于 240.38 FPS！现在这才是加速。正如您可以期待的那样，使用这些速度，实时视频将毫无问题。

有很多预编译的模型可用，用于各种不同的目的，所以请查看它们。您可能会找到一个适合您需求的模型，这样您就不需要经历创建、训练、量化和转换自己的流程（或挣扎）。

Raspberry Pi 3 没有 USB3 端口，只有 USB2。我们可以在表 15-4 中清楚地看到这为 Coral 创建了一个数据瓶颈。

表 15-4. Google Coral 基准测试

| **设置** | **FPS** |
| --- | --- |
| i7-7700K + Coral（tflite，8 位） | 352.1 |
| Raspberry Pi 4 + Coral（tflite，8 位） | 240.4 |
| Jetson Nano + Coral（tflite，8 位） | 223.2 |
| RPi3 + Coral（tflite，8 位） | 75.5 |

# 移植到 NVIDIA Jetson Nano

那个 USB 加速器很好，但是如果您的项目需要一个真正的特定模型，该模型在一些疯狂的、自制的数据集上进行了训练呢？正如我们之前所说，为 Coral Edge TPU 创建一个新模型要比创建一个 Keras 模型要复杂得多。那么，在边缘上运行具有良好性能的自定义模型是否有简单的方法？NVIDIA 来拯救！通过其 Jetson Nano，该公司已经为 Raspberry Pi 创建了一个替代品，它具有启用 CUDA 的、相当高效的 GPU，让您不仅可以加速 TensorFlow，还可以加速任何您喜欢的东西。

同样，我们必须首先安装所有所需的软件包。首先，您需要下载 NVIDIA 的 JetPack 副本，这是其针对 Jetson 平台的类似 Debian 的操作系统版本。然后，为了安装 TensorFlow 和所需的软件包，NVIDIA 指导我们如何在[这里](https://oreil.ly/d11bQ)进行操作。

###### 注意

请注意，`pip3`存在一个已知问题：“`无法导入名称‘main’`”。

这是解决方案：

```py
$ sudo apt install nano
$ sudo nano /usr/bin/pip3

Replace : main => __main__
Replace : main() => __main__._main()
```

###### 提示

更新所有`pip`软件包可能需要很长时间；如果您想确保 Jetson 没有冻结，您可能首先想要安装`htop`：

```py
$ sudo apt install htop
$ htop
```

这让您可以监视 CPU 利用率。只要这个工作正常，您的 Jetson 也是如此。

###### 注意

Jetson Nano 在编译时往往会变得很热，因此我们建议使用某种风扇。开发板有一个连接器，但请记住这会向风扇提供 5V 电压，而大多数风扇的额定电压为 12V。

一些 12V 风扇可能可以直接解决问题，有些可能需要一点推动才能启动。您将需要一个 40 毫米 x 40 毫米的风扇。也有 5V 版本可用。

不幸的是，NVIDIA 的操作指南并不像 Google 的那么简单。您可能会经历一些挣扎，偶尔会有翻桌的时刻。

但请坚持下去；您最终会成功的。

###### 提示

在 Jetson 上安装 Keras 需要 scipy，它需要`libatlas-base-dev`和`gfortran`，因此首先安装后者，然后再前进：

```py
$ sudo apt install libatlas-base-dev gfortran
$ sudo pip3 install scipy
$ sudo pip3 install keras
```

一切都完成后，我们可以选择直接运行*benchmark.py*文件，由于 GPU 的存在，速度比在 Raspberry Pi 上快得多：

```py
$ python3 benchmark.py
Using TensorFlow backend.
tf version : 1.14.0
keras version : 2.2.4
input tensor shape : (1, 224, 224, 3)
warmup prediction
[('n02124075', 'Egyptian_cat', 0.6629321)]
starting now...
Time[s] : 20.520
FPS     : 12.177
```

这立即展示了 Jetson Nano 的强大之处：它几乎与带有 GPU 的任何其他 PC 一样运行。但是，12 FPS 仍然不算很高，因此让我们看看如何优化这一点。

###### 提示

您还可以将 Google Coral 连接到 Jetson Nano 上，这样可以同时在 Coral 上运行一个模型，在 GPU 上运行另一个模型的可能性就打开了。

Jetson 的 GPU 实际上是专门用于 16 位浮点运算的，因此，这将是精度和性能之间的最佳折衷。如前所述，NVIDIA 有一个名为 TF-TRT 的软件包，可以促进优化和转换。然而，这仍然比 Coral 示例复杂一些（请记住，谷歌为我们提供了 Coral 的预编译模型文件）。您需要冻结 Keras 模型，然后创建一个 TF-TRT 推理图。如果您需要在 GitHub 上找到所有内容，这可能需要一段时间，因此它已捆绑在本书的 GitHub 网站上（请参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）。

您可以使用*tftrt_helper.py*文件来优化自己的模型，以便在 Jetson Nano 上进行推理。它实际上只是冻结 Keras 模型，删除训练节点，然后使用 NVIDIA 的 TF-TRT Python 软件包（包含在 TensorFlow Contrib 中）来优化模型。

```py
$ python3 benchmark_jetson.py
FrozenGraph build.
TF-TRT model ready to rumble!
input tensor shape : (1, 224, 224, 3)
[[('n02124075', 'Egyptian_cat', 0.66293204)]]
starting now (Jetson Nano)...
Time[s] : 5.124
FPS     : 48.834
```

48 FPS，仅几行代码就实现了四倍的加速。令人兴奋，不是吗？您可以在自己定制的模型上实现类似的加速，以满足项目特定需求；这些性能基准可以在表 15-5 中找到。

表 15-5. NVIDIA Jetson Nano 基准测试

| **设置** | **FPS** |
| --- | --- |
| Jetson Nano + Coral（tflite，8 位） | 223.2 |
| Jetson Nano（TF-TRT，16 位） | 48.8 |
| Jetson Nano 128CUDA | 12.2 |
| Jetson Nano（tflite，8 位） | 11.3 |

# 比较边缘设备的性能

表 15-6 显示了运行 MobileNetV2 模型的几个边缘设备的定量比较。

表 15-6. 完整的基准测试结果

| **设置** | **FPS** |
| --- | --- |
| i7-7700K + Coral（tflite，8 位） | 352.1 |
| i7-7700K + GTX1080 2560CUDA | 304.9 |
| 树莓派 4 + Coral | 240.4 |
| Jetson Nano + Coral（tflite，8 位） | 223.2 |
| RPi3 + Coral（tflite，8 位） | 75.5 |
| Jetson Nano（TF-TRT，16 位） | 48.8 |
| i7-7700K（tflite，8 位） | 32.4 |
| i9-9880HQ（2019 款 MacBook Pro） | 15.0 |
| Jetson Nano 128CUDA | 12.2 |
| Jetson Nano（tflite，8 位） | 11.3 |
| i7-4870HQ（2014 款 MacBook Pro） | 11.1 |
| Jetson Nano（tflite，8 位） | 10.9 |
| 树莓派 4（tflite，8 位） | 8.6 |
| 树莓派 4（tflite） | 7.8 |
| RPi3B+（tflite，8 位） | 4.2 |
| 树莓派 4 | 2.7 |
| RPi3B+ | 2.1 |

我们可以总结这些实验的要点如下：

1.  良好的优化会产生很大的影响。计算优化领域仍在迅速发展，我们将在未来几年看到一些重大变化。

1.  在谈论 AI 的计算单元时，更多总是更好的。

1.  在性能/瓦特方面，ASIC > FPGA > GPU > CPU。

1.  TensorFlow Lite 非常棒，尤其适用于小型 CPU。请记住，它专门针对小型 CPU，对于 x64 机器使用将不那么有趣。

# 案例研究

这一章是关于制造者的。没有展示他们制造的东西，这一章就是不完整的。让我们看一下通过在边缘运行 AI 所创造的一些东西。

## JetBot

NVIDIA 一直处于深度学习革命的前沿，为研究人员和开发人员提供强大的硬件和软件。2019 年，他们迈出了下一步，也为制造者们提供了 Jetson Nano。我们已经了解了这款硬件的强大之处，现在是时候用它来构建一些东西了，比如 DIY 微型汽车。幸运的是，NVIDIA 为我们提供了 JetBot（图 15-12），这是一个开源机器人，可以由 Jetson Nano 控制。

[JetBot 维基](https://oreil.ly/3nExK)提供了适合初学者的逐步构建说明。以下是一个高层次的概述：

1.  购买材料清单中指定的零件，如电机、转向器、摄像头和 WiFi 天线。除了 99 美元的 Jetson Nano 外，还有大约 30 个零件，总成本约为 150 美元。

1.  现在是时候发挥我们内心的麦克盖尔，拿起一把钳子和一个十字螺丝刀，组装这些零件。最终结果应该类似于图 15-12。

1.  接下来，我们将设置软件。这涉及将 JetBot 镜像刷入 SD 卡，启动 Jetson Nano，将其连接到 WiFi，最后通过 Web 浏览器连接到我们的 JetBot。

1.  最后，我们运行提供的示例笔记本。提供的笔记本使我们能够通过少量代码不仅可以从 Web 浏览器控制机器人，还可以使用其摄像头从流视频中收集训练数据，在设备上训练深度学习模型（毕竟 Jetson Nano 是一个迷你 GPU），并将其用于避开障碍物和跟随对象（如人或球）等任务。

![NVIDIA JetBot](img/00165.jpeg)

###### 图 15-12. NVIDIA JetBot

制造商已经在这个基础框架上扩展了 JetBot 的功能，例如将激光雷达传感器连接到 JetBot 上，以更好地理解环境，将 JetBot 带到不同的物理形式，如坦克、Roomba 吸尘器（JetRoomba）、跟随物体的蜘蛛（JetSpider）等。因为所有道路最终都通向赛车，Jetson Nano 团队最终发布了一个类似的开源配方书，用于名为 JetRacer 的赛车。它基于更快的底盘、更高的摄像头帧率，并针对推理进行了优化（使用 TensorRT）以处理速度。最终结果：JetRacer 已经在 DIY Robocars 聚会等活动中进行比赛。

这里最慢的步骤是...等待硬件到货。

## 蹲下换取地铁票

当健康不能激励人们锻炼时，还有什么？答案是...免费的火车票！为了提高公众对其不断增长的肥胖危机的认识，墨西哥城在地铁站安装了带摄像头的售票机，提供免费车票，但只有在你锻炼时才能获得。十个深蹲可以获得一张免费车票（图 15-13）。莫斯科也在其地铁站实施了类似的系统，但显然，那里的官员对每个人要求 30 个深蹲。

受到这些启发，我们可以预见如何构建我们自己的“深蹲跟踪器”。我们可以通过多种方式实现这一目标，最简单的方法是用“深蹲”和“不深蹲”这两个类别训练我们自己的分类器。这显然需要构建一个包含成千上万张人们蹲下或不蹲下的图片的数据集。

更好的方法是运行 PoseNet（跟踪身体关节，正如我们在第十章中看到的）在 Google Coral USB 加速器上。借助其 10 多 FPS，我们可以跟踪髋部点下降到足够低的次数，并更准确地计数。我们所需要的只是一个树莓派、一个 Pi 摄像头、一个 Coral USB 加速器和一个公共交通运营商提供一个车票打印机和无尽的免费车票，我们就可以开始让我们的城市变得更健康。

![蹲下换取地铁票](img/00126.jpeg)

###### 图 15-13. 蹲下换取车票

## 黄瓜分类器

日本黄瓜农场主的儿子小池真琴观察到，他的父母在收获后花费了大量时间分类黄瓜。这种分类需要根据非常小的特征（如微小的纹理差异、微小的划痕和刺，以及更大的属性，如大小、厚度和曲率）将黄瓜分成九类。这个系统很复杂，雇佣兼职工人几乎是不可能的，因为培训他们需要花费很长时间。他不能袖手旁观，看着父母每天超过八个小时地进行这种劳动密集型的任务。

我们还没有提到的一个细节是，小池先生曾是丰田的嵌入式系统设计师。他意识到很多视觉检查可以利用深度学习的力量自动化。他拍摄了超过 7000 张他母亲在三个月内手动分类的不同黄瓜的照片。然后，他训练了一个分类器，可以查看黄瓜的照片，并以高度准确性预测它属于哪个类别。但一个分类器现在单独做不了什么，对吧？

利用他在嵌入式系统方面的经验，他将一个分类器部署到一个连接了摄像头、传送带系统和分类手臂的树莓派上，根据树莓派的预测将每个黄瓜推入多个箱子中的一个（图 15-14）。树莓派首先运行一个小黄瓜/非黄瓜分类器。对于被分类为黄瓜的图像，树莓派会将图像发送到一个运行更复杂的多类别黄瓜分类器的服务器。请记住，这是在 2015 年，就在 TensorFlow 发布不久之后（远在 TensorFlow Lite、MobileNet 以及当前树莓派 4 甚至 3 的强大功能之前）。有了这样一台机器，他的父母可以花更多时间在实际的农业上，而不是整天在已经收获的黄瓜中进行分类。

![小池真的黄瓜分类机（图片来源）](img/00086.jpeg)

###### 图 15-14\. 小池真的黄瓜分类机（[图片来源](https://oreil.ly/0cSOp)）

尽管关于人工智能接管工作的辩论每天都出现在新闻中，但小池先生的故事真正突出了人工智能提供的真正价值：使人类更加高效，增强我们的能力，同时让他的父母感到骄傲。

# 进一步探索

成为一个创客时要关注的两个同等重要的方面是硬件和软件。一个缺一不可。有更多的 DIY 项目可以探索，一个人一辈子也无法完成。但关键是找到一些你热衷的项目，从头到尾构建它们，当它们开始真正运作时感受到兴奋。逐渐地，你会积累知识，下次阅读一个项目时，你可以猜测如何自己构建它。

开始的最佳方式是动手并构建更多项目。一些优秀的灵感来源包括*Hackster.io*、*Hackaday.io*和*[Instructables.com](http://Instructables.com)*，它们涵盖了各种项目主题、平台和技能水平（从初学者到高级），通常还包括逐步说明。

为了减少初学者组装硬件的开销，市场上有几种套件可用。例如，树莓派的 AI 套件包括 AIY Vision Kit（谷歌）、AIY Speech Kit（谷歌）、GoPiGo（Dexter Industries）和 DuckieTown 平台（麻省理工学院），等等。它们降低了开始电子硬件项目的门槛，使这个新领域更易接近，因此对每个人都可用，从高中生到快速原型制作者。

在软件方面，让低功耗设备上的人工智能运行更快意味着使模型更加高效。像量化和修剪这样的模型压缩技术（如第六章中提到的）可以帮助我们实现这一目标。为了更快地运行（牺牲一些准确性），BNNs 代表模型使用一位而不是正常的 32 位，是一个潜在的解决方案。像 XNOR.ai 和微软的嵌入式学习库这样的公司让我们能够制作这样的模型。对于在内存少于 100 KB 的微控制器等更低功耗设备上运行人工智能，Pete Warden 的书籍[*TinyML*](http://shop.oreilly.com/product/0636920254508.do)是一个优秀的学习资源。

# 总结

在本章中，我们介绍了一些知名的嵌入式人工智能设备，并看了看如何在其中一些设备上运行 Keras 模型。我们从高层次的视角介绍了它们的内部运作方式，以及它们如何实现运行神经网络所需的计算能力。最终，我们对它们进行基准测试，以便根据项目的大小、成本、延迟和功耗需求来开发直觉。从平台到机器人项目，我们看了看全球创客们如何利用它们来构建电子项目的一些方法。本章讨论的设备显然只是众多可用设备中的一小部分。随着边缘成为人工智能更受欢迎的地方，新设备将不断涌现。

借助边缘人工智能的力量，创客们可以想象并实现他们的梦想项目，这些项目在几年前可能被认为是科幻小说。在这个 DIY 人工智能世界中的旅程中，请记住，你是一个与许多开放和愿意分享的修补者紧密联系在一起的社区中的一员。论坛很活跃，所以如果你遇到问题，总会有乐意帮助的人可以求助。希望通过你的项目，你可以激励其他人成为创客。
