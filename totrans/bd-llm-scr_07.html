<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">appendix A</span></span> <span class="chapter-title-text">Introduction to PyTorch</span></h1> 
  </div> 
  <div class="readable-text" id="p2"> 
   <p>This appendix is designed to equip you with the necessary skills and knowledge to put deep learning into practice and implement large language models (LLMs) from scratch. PyTorch, a popular Python-based deep learning library, will be our primary tool for this book. I will guide you through setting up a deep learning workspace armed with PyTorch and GPU support. </p> 
  </div> 
  <div class="readable-text intended-text" id="p3"> 
   <p>Then you’ll learn about the essential concept of tensors and their usage in PyTorch. We will also delve into PyTorch’s automatic differentiation engine, a feature that enables us to conveniently and efficiently use backpropagation, which is a crucial aspect of neural network training.</p> 
  </div> 
  <div class="readable-text intended-text" id="p4"> 
   <p>This appendix is meant as a primer for those new to deep learning in PyTorch. While it explains PyTorch from the ground up, it’s not meant to be an exhaustive coverage of the PyTorch library. Instead, we’ll focus on the PyTorch fundamentals we will use to implement LLMs. If you are already familiar with deep learning, you may skip this appendix and directly move on to chapter 2.</p> 
  </div> 
  <div class="readable-text" id="p5"> 
   <h2 class=" readable-text-h2"><span class="num-string">A.1</span> What is PyTorch?</h2> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>PyTorch (<a href="https://pytorch.org/">https://pytorch.org/</a>) is an open source Python-based deep learning library. According to <em>Papers With Code </em>(<a href="https://paperswithcode.com/trends">https://paperswithcode.com/trends</a>), a platform that tracks and analyzes research papers, PyTorch has been the most widely used deep learning library for research since 2019 by a wide margin. And, according to the <em>Kaggle Data Science and Machine Learning Survey 2022</em> (<a href="https://www.kaggle.com/c/kaggle-survey-2022">https://www.kaggle.com/c/kaggle-survey-2022</a>), the number of respondents using PyTorch is approximately 40%, which grows every year.</p> 
  </div> 
  <div class="readable-text intended-text" id="p7"> 
   <p>One of the reasons PyTorch is so popular is its user-friendly interface and efficiency. Despite its accessibility, it doesn’t compromise on flexibility, allowing advanced users to tweak lower-level aspects of their models for customization and optimization. In short, for many practitioners and researchers, PyTorch offers just the right balance between usability and features. </p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <h3 class=" readable-text-h3"><span class="num-string">A.1.1</span> The three core components of PyTorch</h3> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p>PyTorch is a relatively comprehensive library, and one way to approach it is to focus on its three broad components, summarized in figure A.1. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p10">  
   <img alt="figure" src="../Images/A-1.png" width="657" height="309"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure A.1</span> PyTorch’s three main components include a tensor library as a fundamental building block for computing, automatic differentiation for model optimization, and deep learning utility functions, making it easier to implement and train deep neural network models.</h5>
  </div> 
  <div class="readable-text" id="p11"> 
   <p>First, PyTorch is a <em>tensor library</em> that extends the concept of the array-oriented programming library NumPy with the additional feature that accelerates computation on GPUs, thus providing a seamless switch between CPUs and GPUs. Second, PyTorch is an <em>automatic differentiation engine</em>, also known as autograd, that enables the automatic computation of gradients for tensor operations, simplifying backpropagation and model optimization. Finally, PyTorch is a <em>deep learning library</em>. It offers modular, flexible, and efficient building blocks, including pretrained models, loss functions, and optimizers, for designing and training a wide range of deep learning models, catering to both researchers and developers.</p> 
  </div> 
  <div class="readable-text" id="p12"> 
   <h3 class=" readable-text-h3"><span class="num-string">A.1.2</span> Defining deep learning</h3> 
  </div> 
  <div class="readable-text" id="p13"> 
   <p>In the news, LLMs are often referred to as AI models. However, LLMs are also a type of deep neural network, and PyTorch is a deep learning library. Sound confusing? Let’s take a brief moment and summarize the relationship between these terms before we proceed. </p> 
  </div> 
  <div class="readable-text intended-text" id="p14"> 
   <p><em>AI</em> is fundamentally about creating computer systems capable of performing tasks that usually require human intelligence. These tasks include understanding natural language, recognizing patterns, and making decisions. (Despite significant progress, AI is still far from achieving this level of general intelligence.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p15"> 
   <p><em>Machine learning</em> represents a subfield of AI, as illustrated in figure A.2, that focuses on developing and improving learning algorithms. The key idea behind machine learning is to enable computers to learn from data and make predictions or decisions without being explicitly programmed to perform the task. This involves developing algorithms that can identify patterns, learn from historical data, and improve their performance over time with more data and feedback.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p16">  
   <img alt="figure" src="../Images/A-2.png" width="537" height="304"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure A.2</span> Deep learning is a subcategory of machine learning focused on implementing deep neural networks. Machine learning is a subcategory of AI that is concerned with algorithms that learn from data. AI is the broader concept of machines being able to perform tasks that typically require human intelligence.</h5>
  </div> 
  <div class="readable-text" id="p17"> 
   <p>Machine learning has been integral in the evolution of AI, powering many of the advancements we see today, including LLMs. Machine learning is also behind technologies like recommendation systems used by online retailers and streaming services, email spam filtering, voice recognition in virtual assistants, and even self-driving cars. The introduction and advancement of machine learning have significantly enhanced AI’s capabilities, enabling it to move beyond strict rule-based systems and adapt to new inputs or changing environments.</p> 
  </div> 
  <div class="readable-text intended-text" id="p18"> 
   <p><em>Deep learning</em> is a subcategory of machine learning that focuses on the training and application of deep neural networks. These deep neural networks were originally inspired by how the human brain works, particularly the interconnection between many neurons. The “deep” in deep learning refers to the multiple hidden layers of artificial neurons or nodes that allow them to model complex, nonlinear relationships in the data. Unlike traditional machine learning techniques that excel at simple pattern recognition, deep learning is particularly good at handling unstructured data like images, audio, or text, so it is particularly well suited for LLMs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p19"> 
   <p>The typical predictive modeling workflow (also referred to as <em>supervised learning</em>) in machine learning and deep learning is summarized in figure A.3. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p20">  
   <img alt="figure" src="../Images/A-3.png" width="1088" height="817"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure A.3</span> The supervised learning workflow for predictive modeling consists of a training stage where a model is trained on labeled examples in a training dataset. The trained model can then be used to predict the labels of new observations.</h5>
  </div> 
  <div class="readable-text intended-text" id="p21"> 
   <p>Using a learning algorithm, a model is trained on a training dataset consisting of examples and corresponding labels. In the case of an email spam classifier, for example, the training dataset consists of emails and their “spam” and “not spam” labels that a human identified. Then the trained model can be used on new observations (i.e., new emails) to predict their unknown label (“spam” or “not spam”). Of course, we also want to add a model evaluation between the training and inference stages to ensure that the model satisfies our performance criteria before using it in a real-world application.</p> 
  </div> 
  <div class="readable-text intended-text" id="p22"> 
   <p>If we train LLMs to classify texts, the workflow for training and using LLMs is similar to that depicted in figure A.3. If we are interested in training LLMs to generate texts, which is our main focus, figure A.3 still applies. In this case, the labels during pretraining can be derived from the text itself (the next-word prediction task introduced in chapter 1). The LLM will generate entirely new text (instead of predicting labels), given an input prompt during inference.</p> 
  </div> 
  <div class="readable-text" id="p23"> 
   <h3 class=" readable-text-h3"><span class="num-string">A.1.3</span> Installing PyTorch</h3> 
  </div> 
  <div class="readable-text" id="p24"> 
   <p>PyTorch can be installed just like any other Python library or package. However, since PyTorch is a comprehensive library featuring CPU- and GPU-compatible codes, the installation may require additional explanation.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p25"> 
    <h5 class=" callout-container-h5 readable-text-h5">Python version </h5> 
   </div> 
   <div class="readable-text" id="p26"> 
    <p>Many scientific computing libraries do not immediately support the newest version of Python. Therefore, when installing PyTorch, it’s advisable to use a version of Python that is one or two releases older. For instance, if the latest version of Python is 3.13, using Python 3.11 or 3.12 is recommended.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p27"> 
   <p>For instance, there are two versions of PyTorch: a leaner version that only supports CPU computing and a full version that supports both CPU and GPU computing. If your machine has a CUDA-compatible GPU that can be used for deep learning (ideally, an NVIDIA T4, RTX 2080 Ti, or newer), I recommend installing the GPU version. Regardless, the default command for installing PyTorch in a code terminal is:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p28"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install torch</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p29"> 
   <p>Suppose your computer supports a CUDA-compatible GPU. In that case, it will automatically install the PyTorch version that supports GPU acceleration via CUDA, assuming the Python environment you’re working on has the necessary dependencies (like <code>pip</code>) installed.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p30"> 
   <p><span class="print-book-callout-head">NOTE</span>  As of this writing, PyTorch has also added experimental support for AMD GPUs via ROCm. See <a href="https://pytorch.org">https://pytorch.org</a> for additional instructions. </p> 
  </div> 
  <div class="readable-text" id="p31"> 
   <p>To explicitly install the CUDA-compatible version of PyTorch, it’s often better to specify the CUDA you want PyTorch to be compatible with. PyTorch’s official website (<a href="https://pytorch.org">https://pytorch.org</a>) provides the commands to install PyTorch with CUDA support for different operating systems. Figure A.4 shows a command that will also install PyTorch, as well as the <code>torchvision</code> and <code>torchaudio</code> libraries, which are optional for this book.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p32">  
   <img alt="figure" src="../Images/A-4.png" width="1012" height="484"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure A.4</span> Access the PyTorch installation recommendation on <a href="https://pytorch.org">https://pytorch.org</a> to customize and select the installation command for your system.</h5>
  </div> 
  <div class="readable-text" id="p33"> 
   <p>I use PyTorch 2.4.0 for the examples, so I recommend that you use the following command to install the exact version to guarantee compatibility with this book:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p34"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install torch==2.4.0</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>However, as mentioned earlier, given your operating system, the installation command might differ slightly from the one shown here. Thus, I recommend that you visit <a href="https://pytorch.org">https://pytorch.org</a> and use the installation menu (see figure A.4) to select the installation command for your operating system. Remember to replace <code>torch</code> with <code>torch==2.4.0</code> in the command.</p> 
  </div> 
  <div class="readable-text intended-text" id="p36"> 
   <p>To check the version of PyTorch, execute the following code in PyTorch:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p37"> 
   <div class="code-area-container"> 
    <pre class="code-area">import torch
torch.__version__</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>This prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p39"> 
   <div class="code-area-container"> 
    <pre class="code-area">'2.4.0'</pre>  
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p40"> 
    <h5 class=" callout-container-h5 readable-text-h5">PyTorch and Torch </h5> 
   </div> 
   <div class="readable-text" id="p41"> 
    <p>The Python library is named PyTorch primarily because it’s a continuation of the Torch library but adapted for Python (hence, “PyTorch”). “Torch” acknowledges the library’s roots in Torch, a scientific computing framework with wide support for machine learning algorithms, which was initially created using the Lua programming language.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p42"> 
   <p>If you are looking for additional recommendations and instructions for setting up your Python environment or installing the other libraries used in this book, visit the supplementary GitHub repository of this book at <a href="https://github.com/rasbt/LLMs-from-scratch">https://github.com/rasbt/LLMs-from-scratch</a>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p43"> 
   <p>After installing PyTorch, you can check whether your installation recognizes your built-in NVIDIA GPU by running the following code in Python:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p44"> 
   <div class="code-area-container"> 
    <pre class="code-area">import torch
torch.cuda.is_available()</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p45"> 
   <p>This returns</p> 
  </div> 
  <div class="browsable-container listing-container" id="p46"> 
   <div class="code-area-container"> 
    <pre class="code-area">True</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p47"> 
   <p>If the command returns <code>True</code>, you are all set. If the command returns <code>False</code>, your computer may not have a compatible GPU, or PyTorch does not recognize it. While GPUs are not required for the initial chapters in this book, which are focused on implementing LLMs for educational purposes, they can significantly speed up deep learning–related computations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p48"> 
   <p>If you don’t have access to a GPU, there are several cloud computing providers where users can run GPU computations against an hourly cost. A popular Jupyter notebook–like environment is Google Colab (<a href="https://colab.research.google.com">https://colab.research.google.com</a>), which provides time-limited access to GPUs as of this writing. Using the Runtime menu, it is possible to select a GPU, as shown in the screenshot in figure A.5.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p49">  
   <img alt="figure" src="../Images/A-5.png" width="1012" height="419"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure A.5</span> Select a GPU device for Google Colab under the Runtime/Change Runtime Type menu.</h5>
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p50"> 
    <h5 class=" callout-container-h5 readable-text-h5">PyTorch on Apple Silicon </h5> 
   </div> 
   <div class="readable-text" id="p51"> 
    <p>If you have an Apple Mac with an Apple Silicon chip (like the M1, M2, M3, or newer models), you can use its capabilities to accelerate PyTorch code execution. To use your Apple Silicon chip for PyTorch, you first need to install PyTorch as you normally would. Then, to check whether your Mac supports PyTorch acceleration with its Apple Silicon chip, you can run a simple code snippet in Python:</p> 
   </div> 
   <div class="browsable-container listing-container" id="p52"> 
    <div class="code-area-container"> 
     <pre class="code-area">print(torch.backends.mps.is_available())</pre>  
    </div> 
   </div> 
   <div class="readable-text" id="p53"> 
    <p>If it returns <code>True</code>, it means that your Mac has an Apple Silicon chip that can be used to accelerate PyTorch code.</p> 
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p54"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise A.1</h5> 
   </div> 
   <div class="readable-text" id="p55"> 
    <p>Install and set up PyTorch on your computer</p> 
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p56"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise A.2</h5> 
   </div> 
   <div class="readable-text" id="p57"> 
    <p>Run the supplementary code at <a href="https://mng.bz/o05v">https://mng.bz/o05v</a> that checks whether your environment is set up correctly. </p> 
   </div> 
  </div> 
  <div class="readable-text" id="p58"> 
   <h2 class=" readable-text-h2"><span class="num-string">A.2</span> Understanding tensors</h2> 
  </div> 
  <div class="readable-text" id="p59"> 
   <p>Tensors represent a mathematical concept that generalizes vectors and matrices to potentially higher dimensions. In other words, tensors are mathematical objects that can be characterized by their order (or rank), which provides the number of dimensions. For example, a scalar (just a number) is a tensor of rank 0, a vector is a tensor of rank 1, and a matrix is a tensor of rank 2, as illustrated in figure A.6.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p60">  
   <img alt="figure" src="../Images/A-6.png" width="587" height="294"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure A.6</span> Tensors with different ranks. Here 0D corresponds to rank 0, 1D to rank 1, and 2D to rank 2. A three-dimensional vector, which consists of three elements, is still a rank 1 tensor.</h5>
  </div> 
  <div class="readable-text" id="p61"> 
   <p>From a computational perspective, tensors serve as data containers. For instance, they hold multidimensional data, where each dimension represents a different feature. Tensor libraries like PyTorch can create, manipulate, and compute with these arrays efficiently. In this context, a tensor library functions as an array library. </p> 
  </div> 
  <div class="readable-text intended-text" id="p62"> 
   <p>PyTorch tensors are similar to NumPy arrays but have several additional features that are important for deep learning. For example, PyTorch adds an automatic differentiation engine, simplifying <em>computing gradients</em> (see section A.4). PyTorch tensors also support GPU computations to speed up deep neural network training (see section A.9).</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p63"> 
    <h5 class=" callout-container-h5 readable-text-h5">PyTorch with a NumPy-like API </h5> 
   </div> 
   <div class="readable-text" id="p64"> 
    <p>PyTorch adopts most of the NumPy array API and syntax for its tensor operations. If you are new to NumPy, you can get a brief overview of the most relevant concepts via my article “Scientific Computing in Python: Introduction to NumPy and Matplotlib” at <a href="https://sebastianraschka.com/blog/2020/numpy-intro.html">https://sebastianraschka.com/blog/2020/numpy-intro.html</a>. </p> 
   </div> 
  </div> 
  <div class="readable-text" id="p65"> 
   <h3 class=" readable-text-h3"><span class="num-string">A.2.1</span> Scalars, vectors, matrices, and tensors</h3> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>As mentioned earlier, PyTorch tensors are data containers for array-like structures. A scalar is a zero-dimensional tensor (for instance, just a number), a vector is a one-dimensional tensor, and a matrix is a two-dimensional tensor. There is no specific term for higher-dimensional tensors, so we typically refer to a three-dimensional tensor as just a 3D tensor, and so forth. We can create objects of PyTorch’s <code>Tensor</code> class using the <code>torch.tensor</code> function as shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p67"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing A.1</span> Creating PyTorch tensors </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import torch

tensor0d = torch.tensor(1)    <span class="aframe-location"/> #1

tensor1d = torch.tensor([1, 2, 3])   <span class="aframe-location"/> #2

tensor2d = torch.tensor([[1, 2], 
                         [3, 4]])    <span class="aframe-location"/> #3

tensor3d = torch.tensor([[[1, 2], [3, 4]], 
                         [[5, 6], [7, 8]]])   <span class="aframe-location"/> #4</pre> 
    <div class="code-annotations-overlay-container">
     #1 Creates a zero-dimensional tensor (scalar) from a Python integer
     <br/>#2 Creates a one-dimensional tensor (vector) from a Python list
     <br/>#3 Creates a two-dimensional tensor from a nested Python list
     <br/>#4 Creates a three-dimensional tensor from a nested Python list
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p68"> 
   <h3 class=" readable-text-h3"><span class="num-string">A.2.2</span> Tensor data types</h3> 
  </div> 
  <div class="readable-text" id="p69"> 
   <p>PyTorch adopts the default 64-bit integer data type from Python. We can access the data type of a tensor via the <code>.dtype</code> attribute of a tensor:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p70"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor1d = torch.tensor([1, 2, 3])
print(tensor1d.dtype)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>This prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p72"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.int64</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>If we create tensors from Python floats, PyTorch creates tensors with a 32-bit precision by default:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p74"> 
   <div class="code-area-container"> 
    <pre class="code-area">floatvec = torch.tensor([1.0, 2.0, 3.0])
print(floatvec.dtype)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p76"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.float32</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>This choice is primarily due to the balance between precision and computational efficiency. A 32-bit floating-point number offers sufficient precision for most deep learning tasks while consuming less memory and computational resources than a 64-bit floating-point number. Moreover, GPU architectures are optimized for 32-bit computations, and using this data type can significantly speed up model training and inference.</p> 
  </div> 
  <div class="readable-text intended-text" id="p78"> 
   <p>Moreover, it is possible to change the precision using a tensor’s <code>.to</code> method. The following code demonstrates this by changing a 64-bit integer tensor into a 32-bit float tensor:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p79"> 
   <div class="code-area-container"> 
    <pre class="code-area">floatvec = tensor1d.to(torch.float32)
print(floatvec.dtype)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p80"> 
   <p>This returns</p> 
  </div> 
  <div class="browsable-container listing-container" id="p81"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.float32</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>For more information about different tensor data types available in PyTorch, check the official documentation at <a href="https://pytorch.org/docs/stable/tensors.html">https://pytorch.org/docs/stable/tensors.html</a>.</p> 
  </div> 
  <div class="readable-text" id="p83"> 
   <h3 class=" readable-text-h3"><span class="num-string">A.2.3</span> Common PyTorch tensor operations</h3> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>Comprehensive coverage of all the different PyTorch tensor operations and commands is outside the scope of this book. However, I will briefly describe relevant operations as we introduce them throughout the book.</p> 
  </div> 
  <div class="readable-text intended-text" id="p85"> 
   <p>We have already introduced the <code>torch.tensor()</code> function to create new tensors: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p86"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor2d = torch.tensor([[1, 2, 3], 
                         [4, 5, 6]])
print(tensor2d)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>This prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p88"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[1, 2, 3],
        [4, 5, 6]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>In addition, the <code>.shape</code> attribute allows us to access the shape of a tensor:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p90"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(tensor2d.shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p91"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p92"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.Size([2, 3])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>As you can see, <code>.shape</code> returns <code>[2,</code> <code>3]</code>, meaning the tensor has two rows and three columns. To reshape the tensor into a 3 × 2 tensor, we can use the <code>.reshape</code> method:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p94"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(tensor2d.reshape(3, 2))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>This prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p96"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[1, 2],
        [3, 4],
        [5, 6]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>However, note that the more common command for reshaping tensors in PyTorch is <code>.view()</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p98"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(tensor2d.view(3, 2))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p99"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p100"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[1, 2],
        [3, 4],
        [5, 6]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p101"> 
   <p>Similar to <code>.reshape</code> and <code>.view</code>, in several cases, PyTorch offers multiple syntax options for executing the same computation. PyTorch initially followed the original Lua Torch syntax convention but then, by popular request, added syntax to make it similar to NumPy. (The subtle difference between <code>.view()</code> and <code>.reshape()</code> in PyTorch lies in their handling of memory layout: <code>.view()</code> requires the original data to be contiguous and will fail if it isn’t, whereas <code>.reshape()</code> will work regardless, copying the data if necessary to ensure the desired shape.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p102"> 
   <p>Next, we can use <code>.T</code> to transpose a tensor, which means flipping it across its diagonal. Note that this is not the same as reshaping a tensor, as you can see based on the following result:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p103"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(tensor2d.T)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p104"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p105"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[1, 4],
        [2, 5],
        [3, 6]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>Lastly, the common way to multiply two matrices in PyTorch is the <code>.matmul</code> method:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p107"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(tensor2d.matmul(tensor2d.T))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p108"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p109"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[14, 32],
        [32, 77]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>However, we can also adopt the <code>@</code> operator, which accomplishes the same thing more compactly:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p111"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(tensor2d @ tensor2d.T)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p112"> 
   <p>This prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p113"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[14, 32],
        [32, 77]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p114"> 
   <p>As mentioned earlier, I introduce additional operations when needed. For readers who’d like to browse through all the different tensor operations available in PyTorch (we won’t need most of these), I recommend checking out the official documentation at <a href="https://pytorch.org/docs/stable/tensors.html">https://pytorch.org/docs/stable/tensors.html</a>.</p> 
  </div> 
  <div class="readable-text" id="p115"> 
   <h2 class=" readable-text-h2"><span class="num-string">A.3</span> Seeing models as computation graphs</h2> 
  </div> 
  <div class="readable-text" id="p116"> 
   <p>Now let’s look at PyTorch’s automatic differentiation engine, also known as autograd. PyTorch’s autograd system provides functions to compute gradients in dynamic computational graphs automatically. </p> 
  </div> 
  <div class="readable-text intended-text" id="p117"> 
   <p>A computational graph is a directed graph that allows us to express and visualize mathematical expressions. In the context of deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural network—we will need this to compute the required gradients for backpropagation, the main training algorithm for neural networks.</p> 
  </div> 
  <div class="readable-text intended-text" id="p118"> 
   <p>Let’s look at a concrete example to illustrate the concept of a computation graph. The code in the following listing implements the forward pass (prediction step) of a simple logistic regression classifier, which can be seen as a single-layer neural network. It returns a score between 0 and 1, which is compared to the true class label (0 or 1) when computing the loss.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p119"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing A.2</span> A logistic regression forward pass</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import torch.nn.functional as F    <span class="aframe-location"/> #1

y = torch.tensor([1.0])         <span class="aframe-location"/> #2
x1 = torch.tensor([1.1])   <span class="aframe-location"/> #3
w1 = torch.tensor([2.2])   <span class="aframe-location"/> #4
b = torch.tensor([0.0])           <span class="aframe-location"/> #5
z = x1 * w1 + b                <span class="aframe-location"/> #6
a = torch.sigmoid(z)              <span class="aframe-location"/> #7
loss = F.binary_cross_entropy(a, y)</pre> 
    <div class="code-annotations-overlay-container">
     #1 This import statement is a common convention in PyTorch to prevent long lines of code.
     <br/>#2 True label
     <br/>#3 Input feature
     <br/>#4 Weight parameter
     <br/>#5 Bias unit
     <br/>#6 Net input
     <br/>#7 Activation and output
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p120"> 
   <p>If not all components in the preceding code make sense to you, don’t worry. The point of this example is not to implement a logistic regression classifier but rather to illustrate how we can think of a sequence of computations as a computation graph, as shown in figure A.7. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p121">  
   <img alt="figure" src="../Images/A-7.png" width="780" height="285"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure A.7</span> A logistic regression forward pass as a computation graph. The input feature x<sub>1</sub> is multiplied by a model weight w<sub>1</sub> and passed through an activation function <span class="regular-symbol">s</span> after adding the bias. The loss is computed by comparing the model output a with a given label y.</h5>
  </div> 
  <div class="readable-text" id="p122"> 
   <p>In fact, PyTorch builds such a computation graph in the background, and we can use this to calculate gradients of a loss function with respect to the model parameters (here <em>w</em><sub>1</sub> and <em>b</em>) to train the model.</p> 
  </div> 
  <div class="readable-text" id="p123"> 
   <h2 class=" readable-text-h2"><span class="num-string">A.4</span> Automatic differentiation made easy</h2> 
  </div> 
  <div class="readable-text" id="p124"> 
   <p>If we carry out computations in PyTorch, it will build a computational graph internally by default if one of its terminal nodes has the <code>requires_grad</code> attribute set to <code>True</code>. This is useful if we want to compute gradients. Gradients are required when training neural networks via the popular backpropagation algorithm, which can be considered an implementation of the <em>chain rule</em> from calculus for neural networks, illustrated in figure A.8.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p125">  
   <img alt="figure" src="../Images/A-8.png" width="737" height="499"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure A.8</span> The most common way of computing the loss gradients in a computation graph involves applying the chain rule from right to left, also called reverse-model automatic differentiation or backpropagation. We start from the output layer (or the loss itself) and work backward through the network to the input layer. We do this to compute the gradient of the loss with respect to each parameter (weights and biases) in the network, which informs how we update these parameters during training.</h5>
  </div> 
  <div class="readable-text" id="p126"> 
   <h4 class=" readable-text-h4">Partial derivatives and gradients </h4> 
  </div> 
  <div class="readable-text" id="p127"> 
   <p>Figure A.8 shows partial derivatives, which measure the rate at which a function changes with respect to one of its variables. A <em>gradient</em> is a vector containing all of the partial derivatives of a multivariate function, a function with more than one variable as input.</p> 
  </div> 
  <div class="readable-text intended-text" id="p128"> 
   <p>If you are not familiar with or don’t remember the partial derivatives, gradients, or chain rule from calculus, don’t worry. On a high level, all you need to know for this book is that the chain rule is a way to compute gradients of a loss function given the model’s parameters in a computation graph. This provides the information needed to update each parameter to minimize the loss function, which serves as a proxy for measuring the model’s performance using a method such as gradient descent. We will revisit the computational implementation of this training loop in PyTorch in section A.7.</p> 
  </div> 
  <div class="readable-text intended-text" id="p129"> 
   <p>How is this all related to the automatic differentiation (autograd) engine, the second component of the PyTorch library mentioned earlier? PyTorch’s autograd engine constructs a computational graph in the background by tracking every operation performed on tensors. Then, calling the <code>grad</code> function, we can compute the gradient of the loss concerning the model parameter <code>w1</code>, as shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p130"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing A.3</span> Computing gradients via autograd</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import torch.nn.functional as F
from torch.autograd import grad

y = torch.tensor([1.0])
x1 = torch.tensor([1.1])
w1 = torch.tensor([2.2], requires_grad=True)
b = torch.tensor([0.0], requires_grad=True)

z = x1 * w1 + b 
a = torch.sigmoid(z)

loss = F.binary_cross_entropy(a, y)

grad_L_w1 = grad(loss, w1, retain_graph=True)  <span class="aframe-location"/> #1
grad_L_b = grad(loss, b, retain_graph=True)</pre> 
    <div class="code-annotations-overlay-container">
     #1 By default, PyTorch destroys the computation graph after calculating the gradients to free memory. However, since we will reuse this computation graph shortly, we set retain_graph=True so that it stays in memory.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p131"> 
   <p>The resulting values of the loss gradients given the model’s parameters are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p132"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(grad_L_w1)
print(grad_L_b)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p133"> 
   <p>This prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p134"> 
   <div class="code-area-container"> 
    <pre class="code-area">(tensor([-0.0898]),)
(tensor([-0.0817]),)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p135"> 
   <p>Here, we have been using the grad function manually, which can be useful for experimentation, debugging, and demonstrating concepts. But, in practice, PyTorch provides even more high-level tools to automate this process. For instance, we can call <code>.backward</code> on the loss, and PyTorch will compute the gradients of all the leaf nodes in the graph, which will be stored via the tensors’ <code>.grad</code> attributes:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p136"> 
   <div class="code-area-container"> 
    <pre class="code-area">loss.backward()
print(w1.grad)
print(b.grad)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p137"> 
   <p>The outputs are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p138"> 
   <div class="code-area-container"> 
    <pre class="code-area">(tensor([-0.0898]),)
(tensor([-0.0817]),)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p139"> 
   <p>I’ve provided you with a lot of information, and you may be overwhelmed by the calculus concepts, but don’t worry. While this calculus jargon is a means to explain PyTorch’s autograd component, all you need to take away is that PyTorch takes care of the calculus for us via the <code>.backward</code> method—we won’t need to compute any derivatives or gradients by hand.</p> 
  </div> 
  <div class="readable-text" id="p140"> 
   <h2 class=" readable-text-h2"><span class="num-string">A.5</span> Implementing multilayer neural networks</h2> 
  </div> 
  <div class="readable-text" id="p141"> 
   <p>Next, we focus on PyTorch as a library for implementing deep neural networks. To provide a concrete example, let’s look at a multilayer perceptron, a fully connected neural network, as illustrated in figure A.9.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p142">  
   <img alt="figure" src="../Images/A-9.png" width="702" height="529"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure A.9</span> A multilayer perceptron with two hidden layers. Each node represents a unit in the respective layer. For illustration purposes, each layer has a very small number of nodes.</h5>
  </div> 
  <div class="readable-text" id="p143"> 
   <p>When implementing a neural network in PyTorch, we can subclass the <code>torch.nn.Module</code> class to define our own custom network architecture. This <code>Module</code> base class provides a lot of functionality, making it easier to build and train models. For instance, it allows us to encapsulate layers and operations and keep track of the model’s parameters. </p> 
  </div> 
  <div class="readable-text intended-text" id="p144"> 
   <p>Within this subclass, we define the network layers in the <code>__init__</code> constructor and specify how the layers interact in the forward method. The forward method describes how the input data passes through the network and comes together as a computation graph. In contrast, the backward method, which we typically do not need to implement ourselves, is used during training to compute gradients of the loss function given the model parameters (see section A.7). The code in the following listing implements a classic multilayer perceptron with two hidden layers to illustrate a typical usage of the <code>Module</code> class.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p145"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing A.4</span> A multilayer perceptron with two hidden layers </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">class NeuralNetwork(torch.nn.Module):
    def __init__(self, num_inputs, num_outputs):   <span class="aframe-location"/> #1
        super().__init__()

        self.layers = torch.nn.Sequential(

            # 1st hidden layer
            torch.nn.Linear(num_inputs, 30),   <span class="aframe-location"/> #2
            torch.nn.ReLU(),              <span class="aframe-location"/> #3

            # 2nd hidden layer
            torch.nn.Linear(30, 20),   <span class="aframe-location"/> #4
            torch.nn.ReLU(),

            # output layer
            torch.nn.Linear(20, num_outputs),
        )

    def forward(self, x):
        logits = self.layers(x)
        return logits          <span class="aframe-location"/> #5</pre> 
    <div class="code-annotations-overlay-container">
     #1 Coding the number of inputs and outputs as variables allows us to reuse the same code for datasets with different numbers of features and classes
     <br/>#2 The Linear layer takes the number of input and output nodes as arguments.
     <br/>#3 Nonlinear activation functions are placed between the hidden layers.
     <br/>#4 The number of output nodes of one hidden layer has to match the number of inputs of the next layer.
     <br/>#5 The outputs of the last layer are called logits.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p146"> 
   <p>We can then instantiate a new neural network object as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p147"> 
   <div class="code-area-container"> 
    <pre class="code-area">model = NeuralNetwork(50, 3)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p148"> 
   <p>Before using this new <code>model</code> object, we can call <code>print</code> on the model to see a summary of its structure:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p149"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(model)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p150"> 
   <p>This prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p151"> 
   <div class="code-area-container"> 
    <pre class="code-area">NeuralNetwork(
  (layers): Sequential(
    (0): Linear(in_features=50, out_features=30, bias=True)
    (1): ReLU()
    (2): Linear(in_features=30, out_features=20, bias=True)
    (3): ReLU()
    (4): Linear(in_features=20, out_features=3, bias=True)
  )
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p152"> 
   <p>Note that we use the <code>Sequential</code> class when we implement the <code>NeuralNetwork</code> class. <code>Sequential</code> is not required, but it can make our life easier if we have a series of layers we want to execute in a specific order, as is the case here. This way, after instantiating <code>self.layers</code> <code>=</code> <code>Sequential(...)</code> in the <code>__init__</code> constructor, we just have to call the <code>self.layers</code> instead of calling each layer individually in the <code>NeuralNetwork</code>’s <code>forward</code> method.</p> 
  </div> 
  <div class="readable-text intended-text" id="p153"> 
   <p>Next, let’s check the total number of trainable parameters of this model:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p154"> 
   <div class="code-area-container"> 
    <pre class="code-area">num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print("Total number of trainable model parameters:", num_params)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p155"> 
   <p>This prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p156"> 
   <div class="code-area-container"> 
    <pre class="code-area">Total number of trainable model parameters: 2213</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p157"> 
   <p>Each parameter for which <code>requires_grad=True</code> counts as a trainable parameter and will be updated during training (see section A.7).</p> 
  </div> 
  <div class="readable-text intended-text" id="p158"> 
   <p>In the case of our neural network model with the preceding two hidden layers, these trainable parameters are contained in the <code>torch.nn.Linear</code> layers. A <code>Linear</code> layer multiplies the inputs with a weight matrix and adds a bias vector. This is sometimes referred to as a <em>feedforward</em> or <em>fully connected</em> layer. </p> 
  </div> 
  <div class="readable-text intended-text" id="p159"> 
   <p>Based on the <code>print(model)</code> call we executed here, we can see that the first <code>Linear</code> layer is at index position 0 in the layers attribute. We can access the corresponding weight parameter matrix as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p160"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(model.layers[0].weight)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p161"> 
   <p>This prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p162"> 
   <div class="code-area-container"> 
    <pre class="code-area">Parameter containing:
tensor([[ 0.1174, -0.1350, -0.1227,  ...,  0.0275, -0.0520, -0.0192],
        [-0.0169,  0.1265,  0.0255,  ..., -0.1247,  0.1191, -0.0698],
        [-0.0973, -0.0974, -0.0739,  ..., -0.0068, -0.0892,  0.1070],
        ...,
        [-0.0681,  0.1058, -0.0315,  ..., -0.1081, -0.0290, -0.1374],
        [-0.0159,  0.0587, -0.0916,  ..., -0.1153,  0.0700,  0.0770],
        [-0.1019,  0.1345, -0.0176,  ...,  0.0114, -0.0559, -0.0088]],
       requires_grad=True)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p163"> 
   <p>Since this large matrix is not shown in its entirety, let’s use the <code>.shape</code> attribute to show its dimensions:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p164"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(model.layers[0].weight.shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p165"> 
   <p>The result is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p166"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.Size([30, 50])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p167"> 
   <p>(Similarly, you could access the bias vector via <code>model.layers[0].bias</code>.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p168"> 
   <p>The weight matrix here is a 30 × 50 matrix, and we can see that <code>requires_grad</code> is set to <code>True</code>, which means its entries are trainable—this is the default setting for weights and biases in <code>torch.nn.Linear</code>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p169"> 
   <p>If you execute the preceding code on your computer, the numbers in the weight matrix will likely differ from those shown. The model weights are initialized with small random numbers, which differ each time we instantiate the network. In deep learning, initializing model weights with small random numbers is desired to break symmetry during training. Otherwise, the nodes would be performing the same operations and updates during backpropagation, which would not allow the network to learn complex mappings from inputs to outputs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p170"> 
   <p>However, while we want to keep using small random numbers as initial values for our layer weights, we can make the random number initialization reproducible by seeding PyTorch’s random number generator via <code>manual_seed</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p171"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
model = NeuralNetwork(50, 3)
print(model.layers[0].weight)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p172"> 
   <p>The result is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p173"> 
   <div class="code-area-container"> 
    <pre class="code-area">Parameter containing:
tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],
        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],
        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],
        ...,
        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],
        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],
        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],
       requires_grad=True)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p174"> 
   <p>Now that we have spent some time inspecting the <code>NeuralNetwork</code> instance, let’s briefly see how it’s used via the forward pass:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p175"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
X = torch.rand((1, 50))
out = model(X)
print(out)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p176"> 
   <p>The result is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p177"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=&lt;AddmmBackward0&gt;)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p178"> 
   <p>In the preceding code, we generated a single random training example <code>X</code> as a toy input (note that our network expects 50-dimensional feature vectors) and fed it to the model, returning three scores. When we call <code>model(x)</code>, it will automatically execute the forward pass of the model. </p> 
  </div> 
  <div class="readable-text intended-text" id="p179"> 
   <p>The forward pass refers to calculating output tensors from input tensors. This involves passing the input data through all the neural network layers, starting from the input layer, through hidden layers, and finally to the output layer.</p> 
  </div> 
  <div class="readable-text intended-text" id="p180"> 
   <p>These three numbers returned here correspond to a score assigned to each of the three output nodes. Notice that the output tensor also includes a <code>grad_fn</code> value.</p> 
  </div> 
  <div class="readable-text intended-text" id="p181"> 
   <p>Here, <code>grad_fn=&lt;AddmmBackward0&gt;</code> represents the last-used function to compute a variable in the computational graph. In particular, <code>grad_fn=&lt;AddmmBackward0&gt;</code> means that the tensor we are inspecting was created via a matrix multiplication and addition operation. PyTorch will use this information when it computes gradients during backpropagation. The <code>&lt;AddmmBackward0&gt;</code> part of <code>grad_fn=&lt;AddmmBackward0&gt;</code> specifies the operation performed. In this case, it is an <code>Addmm</code> operation. <code>Addmm</code> stands for matrix multiplication (<code>mm</code>) followed by an addition (<code>Add</code>).</p> 
  </div> 
  <div class="readable-text intended-text" id="p182"> 
   <p>If we just want to use a network without training or backpropagation—for example, if we use it for prediction after training—constructing this computational graph for backpropagation can be wasteful as it performs unnecessary computations and consumes additional memory. So, when we use a model for inference (for instance, making predictions) rather than training, the best practice is to use the <code>torch.no_grad()</code> context manager. This tells PyTorch that it doesn’t need to keep track of the gradients, which can result in significant savings in memory and computation:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p183"> 
   <div class="code-area-container"> 
    <pre class="code-area">with torch.no_grad():
    out = model(X)
print(out)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p184"> 
   <p>The result is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p185"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[-0.1262,  0.1080, -0.1792]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p186"> 
   <p>In PyTorch, it’s common practice to code models such that they return the outputs of the last layer (logits) without passing them to a nonlinear activation function. That’s because PyTorch’s commonly used loss functions combine the <code>softmax</code> (or <code>sigmoid</code> for binary classification) operation with the negative log-likelihood loss in a single class. The reason for this is numerical efficiency and stability. So, if we want to compute class-membership probabilities for our predictions, we have to call the <code>softmax</code> function explicitly:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p187"> 
   <div class="code-area-container"> 
    <pre class="code-area">with torch.no_grad():
    out = torch.softmax(model(X), dim=1)
print(out)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p188"> 
   <p>This prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p189"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[0.3113, 0.3934, 0.2952]]))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p190"> 
   <p>The values can now be interpreted as class-membership probabilities that sum up to 1. The values are roughly equal for this random input, which is expected for a randomly initialized model without training. </p> 
  </div> 
  <div class="readable-text" id="p191"> 
   <h2 class=" readable-text-h2"><span class="num-string">A.6</span> Setting up efficient data loaders</h2> 
  </div> 
  <div class="readable-text" id="p192"> 
   <p>Before we can train our model, we have to briefly discuss creating efficient data loaders in PyTorch, which we will iterate over during training. The overall idea behind data loading in PyTorch is illustrated in figure A.10.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p193">  
   <img alt="figure" src="../Images/A-10.png" width="1009" height="392"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure A.10</span> PyTorch implements a <code>Dataset</code> and a <code>DataLoader</code> class. The <code>Dataset</code> class is used to instantiate objects that define how each data record is loaded. The <code>DataLoader</code> handles how the data is shuffled and assembled into batches.</h5>
  </div> 
  <div class="readable-text" id="p194"> 
   <p>Following figure A.10, we will implement a custom <code>Dataset</code> class, which we will use to create a training and a test dataset that we’ll then use to create the data loaders. Let’s start by creating a simple toy dataset of five training examples with two features each. Accompanying the training examples, we also create a tensor containing the corresponding class labels: three examples belong to class 0, and two examples belong to class 1. In addition, we make a test set consisting of two entries. The code to create this dataset is shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p195"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing A.5</span> Creating a small toy dataset </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">X_train = torch.tensor([
    [-1.2, 3.1],
    [-0.9, 2.9],
    [-0.5, 2.6],
    [2.3, -1.1],
    [2.7, -1.5]
])
y_train = torch.tensor([0, 0, 0, 1, 1])

X_test = torch.tensor([
    [-0.8, 2.8],
    [2.6, -1.6],
])
y_test = torch.tensor([0, 1])</pre>  
   </div> 
  </div> 
  <div class="readable-text print-book-callout" id="p196"> 
   <p><span class="print-book-callout-head">NOTE</span>  PyTorch requires that class labels start with label 0, and the largest class label value should not exceed the number of output nodes minus 1 (since Python index counting starts at zero). So, if we have class labels 0, 1, 2, 3, and 4, the neural network output layer should consist of five nodes.</p> 
  </div> 
  <div class="readable-text" id="p197"> 
   <p>Next, we create a custom dataset class, <code>ToyDataset</code>, by subclassing from PyTorch’s <code>Dataset</code> parent class, as shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p198"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing A.6</span> Defining a custom <code>Dataset</code> class </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from torch.utils.data import Dataset

class ToyDataset(Dataset):
    def __init__(self, X, y):
        self.features = X
        self.labels = y

    def __getitem__(self, index):       <span class="aframe-location"/> #1
        one_x = self.features[index]     #1
        one_y = self.labels[index]       #1
        return one_x, one_y              #1

    def __len__(self):
        return self.labels.shape[0]     <span class="aframe-location"/> #2

train_ds = ToyDataset(X_train, y_train)
test_ds = ToyDataset(X_test, y_test)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Instructions for retrieving exactly one data record and the corresponding label
     <br/>#2 Instructions for returning the total length of the dataset
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p199"> 
   <p>The purpose of this custom <code>ToyDataset</code> class is to instantiate a PyTorch <code>DataLoader</code>. But before we get to this step, let’s briefly go over the general structure of the <code>ToyDataset</code> code. </p> 
  </div> 
  <div class="readable-text intended-text" id="p200"> 
   <p>In PyTorch, the three main components of a custom <code>Dataset</code> class are the <code>__init__</code> constructor, the <code>__getitem__</code> method, and the <code>__len__</code> method (see listing A.6). In the <code>__init__</code> method, we set up attributes that we can access later in the <code>__getitem__</code> and <code>__len__</code> methods. These could be file paths, file objects, database connectors, and so on. Since we created a tensor dataset that sits in memory, we simply assign <code>X</code> and <code>y</code> to these attributes, which are placeholders for our tensor objects. </p> 
  </div> 
  <div class="readable-text intended-text" id="p201"> 
   <p>In the <code>__getitem__</code> method, we define instructions for returning exactly one item from the dataset via an <code>index</code>. This refers to the features and the class label corresponding to a single training example or test instance. (The data loader will provide this <code>index</code>, which we will cover shortly.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p202"> 
   <p>Finally, the <code>__len__</code> method contains instructions for retrieving the length of the dataset. Here, we use the <code>.shape</code> attribute of a tensor to return the number of rows in the feature array. In the case of the training dataset, we have five rows, which we can double-check:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p203"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(len(train_ds))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p204"> 
   <p>The result is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p205"> 
   <div class="code-area-container"> 
    <pre class="code-area">5</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p206"> 
   <p>Now that we’ve defined a PyTorch <code>Dataset</code> class we can use for our toy dataset, we can use PyTorch’s <code>DataLoader</code> class to sample from it, as shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p207"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing A.7</span> Instantiating data loaders </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from torch.utils.data import DataLoader


torch.manual_seed(123)

train_loader = DataLoader(
    dataset=train_ds,    <span class="aframe-location"/> #1
    batch_size=2,
    shuffle=True,         <span class="aframe-location"/> #2
    num_workers=0    <span class="aframe-location"/> #3
)

test_loader = DataLoader(
    dataset=test_ds,
    batch_size=2,
    shuffle=False,    <span class="aframe-location"/> #4
    num_workers=0
)</pre> 
    <div class="code-annotations-overlay-container">
     #1 The ToyDataset instance created earlier serves as input to the data loader.
     <br/>#2 Whether or not to shuffle the data
     <br/>#3 The number of background processes
     <br/>#4 It is not necessary to shuffle a test dataset.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p208"> 
   <p>After instantiating the training data loader, we can iterate over it. The iteration over the <code>test_loader</code> works similarly but is omitted for brevity:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p209"> 
   <div class="code-area-container"> 
    <pre class="code-area">for idx, (x, y) in enumerate(train_loader):
    print(f"Batch {idx+1}:", x, y)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p210"> 
   <p>The result is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p211"> 
   <div class="code-area-container"> 
    <pre class="code-area">Batch 1: tensor([[-1.2000,  3.1000],
                 [-0.5000,  2.6000]]) tensor([0, 0])
Batch 2: tensor([[ 2.3000, -1.1000],
                 [-0.9000,  2.9000]]) tensor([1, 0])
Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p212"> 
   <p>As we can see based on the preceding output, the <code>train_loader</code> iterates over the training dataset, visiting each training example exactly once. This is known as a training epoch. Since we seeded the random number generator using <code>torch.manual_seed(123)</code> here, you should get the exact same shuffling order of training examples. However, if you iterate over the dataset a second time, you will see that the shuffling order will change. This is desired to prevent deep neural networks from getting caught in repetitive update cycles during training.</p> 
  </div> 
  <div class="readable-text intended-text" id="p213"> 
   <p>We specified a batch size of 2 here, but the third batch only contains a single example. That’s because we have five training examples, and 5 is not evenly divisible by 2. In practice, having a substantially smaller batch as the last batch in a training epoch can disturb the convergence during training. To prevent this, set <code>drop_last=True</code>, which will drop the last batch in each epoch, as shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p214"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing A.8</span> A training loader that drops the last batch</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">train_loader = DataLoader(
    dataset=train_ds,
    batch_size=2,
    shuffle=True,
    num_workers=0,
    drop_last=True
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p215"> 
   <p>Now, iterating over the training loader, we can see that the last batch is omitted:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p216"> 
   <div class="code-area-container"> 
    <pre class="code-area">for idx, (x, y) in enumerate(train_loader):
    print(f"Batch {idx+1}:", x, y)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p217"> 
   <p>The result is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p218"> 
   <div class="code-area-container"> 
    <pre class="code-area">Batch 1: tensor([[-0.9000,  2.9000],
        [ 2.3000, -1.1000]]) tensor([0, 1])
Batch 2: tensor([[ 2.7000, -1.5000],
        [-0.5000,  2.6000]]) tensor([1, 0])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p219"> 
   <p>Lastly, let’s discuss the setting <code>num_workers=0</code> in the <code>DataLoader</code>. This parameter in PyTorch’s <code>DataLoader</code> function is crucial for parallelizing data loading and preprocessing. When <code>num_workers</code> is set to 0, the data loading will be done in the main process and not in separate worker processes. This might seem unproblematic, but it can lead to significant slowdowns during model training when we train larger networks on a GPU. Instead of focusing solely on the processing of the deep learning model, the CPU must also take time to load and preprocess the data. As a result, the GPU can sit idle while waiting for the CPU to finish these tasks. In contrast, when <code>num_workers</code> is set to a number greater than 0, multiple worker processes are launched to load data in parallel, freeing the main process to focus on training your model and better utilizing your system’s resources (figure A.11).<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p220">  
   <img alt="figure" src="../Images/A-11.png" width="1017" height="494"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure A.11</span> Loading data without multiple workers (setting <code>num_workers=0</code>) will create a data loading bottleneck where the model sits idle until the next batch is loaded (left). If multiple workers are enabled, the data loader can queue up the next batch in the background (right).</h5>
  </div> 
  <div class="readable-text intended-text" id="p221"> 
   <p>However, if we are working with very small datasets, setting <code>num_workers</code> to 1 or larger may not be necessary since the total training time takes only fractions of a second anyway. So, if you are working with tiny datasets or interactive environments such as Jupyter notebooks, increasing <code>num_workers</code> may not provide any noticeable speedup. It may, in fact, lead to some problems. One potential problem is the overhead of spinning up multiple worker processes, which could take longer than the actual data loading when your dataset is small. </p> 
  </div> 
  <div class="readable-text intended-text" id="p222"> 
   <p>Furthermore, for Jupyter notebooks, setting <code>num_workers</code> to greater than 0 can sometimes lead to problems related to the sharing of resources between different processes, resulting in errors or notebook crashes. Therefore, it’s essential to understand the tradeoff and make a calculated decision on setting the <code>num_workers</code> parameter. When used correctly, it can be a beneficial tool but should be adapted to your specific dataset size and computational environment for optimal results.</p> 
  </div> 
  <div class="readable-text intended-text" id="p223"> 
   <p>In my experience, setting <code>num_workers=4</code> usually leads to optimal performance on many real-world datasets, but optimal settings depend on your hardware and the code used for loading a training example defined in the <code>Dataset</code> class.</p> 
  </div> 
  <div class="readable-text" id="p224"> 
   <h2 class=" readable-text-h2"><span class="num-string">A.7</span> A typical training loop</h2> 
  </div> 
  <div class="readable-text" id="p225"> 
   <p>Let’s now train a neural network on the toy dataset. The following listing shows the training code.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p226"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing A.9</span> Neural network training in PyTorch </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import torch.nn.functional as F

torch.manual_seed(123)
model = NeuralNetwork(num_inputs=2, num_outputs=2)   <span class="aframe-location"/> #1
optimizer = torch.optim.SGD(
    model.parameters(), lr=0.5
)           <span class="aframe-location"/> #2

num_epochs = 3
for epoch in range(num_epochs): 

    model.train()
    for batch_idx, (features, labels) in enumerate(train_loader):
        logits = model(features)

        loss = F.cross_entropy(logits, labels)

        optimizer.zero_grad()           <span class="aframe-location"/> #3
        loss.backward()        <span class="aframe-location"/> #4
        optimizer.step()       <span class="aframe-location"/> #5

        ### LOGGING
        print(f"Epoch: {epoch+1:03d}/{num_epochs:03d}"
              f" | Batch {batch_idx:03d}/{len(train_loader):03d}"
              f" | Train Loss: {loss:.2f}")

    model.eval()
    # Insert optional model evaluation code</pre> 
    <div class="code-annotations-overlay-container">
     #1 The dataset has two features and two classes.
     <br/>#2 The optimizer needs to know which parameters to optimize.
     <br/>#3 Sets the gradients from the previous round to 0 to prevent unintended gradient accumulation
     <br/>#4 Computes the gradients of the loss given the model parameters
     <br/>#5 The optimizer uses the gradients to update the model parameters.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p227"> 
   <p>Running this code yields the following outputs:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p228"> 
   <div class="code-area-container"> 
    <pre class="code-area">Epoch: 001/003 | Batch 000/002 | Train Loss: 0.75
Epoch: 001/003 | Batch 001/002 | Train Loss: 0.65
Epoch: 002/003 | Batch 000/002 | Train Loss: 0.44
Epoch: 002/003 | Batch 001/002 | Trainl Loss: 0.13
Epoch: 003/003 | Batch 000/002 | Train Loss: 0.03
Epoch: 003/003 | Batch 001/002 | Train Loss: 0.00</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p229"> 
   <p>As we can see, the loss reaches 0 after three epochs, a sign that the model converged on the training set. Here, we initialize a model with two inputs and two outputs because our toy dataset has two input features and two class labels to predict. We used a stochastic gradient descent (<code>SGD</code>) optimizer with a learning rate (<code>lr</code>) of 0.5. The learning rate is a hyperparameter, meaning it’s a tunable setting that we must experiment with based on observing the loss. Ideally, we want to choose a learning rate such that the loss converges after a certain number of epochs—the number of epochs is another hyperparameter to choose. </p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p230"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise A.3</h5> 
   </div> 
   <div class="readable-text" id="p231"> 
    <p>How many parameters does the neural network introduced in listing A.9 have?</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p232"> 
   <p>In practice, we often use a third dataset, a so-called validation dataset, to find the optimal hyperparameter settings. A validation dataset is similar to a test set. However, while we only want to use a test set precisely once to avoid biasing the evaluation, we usually use the validation set multiple times to tweak the model settings. </p> 
  </div> 
  <div class="readable-text intended-text" id="p233"> 
   <p>We also introduced new settings called <code>model.train()</code> and <code>model.eval()</code>. As these names imply, these settings are used to put the model into a training and an evaluation mode. This is necessary for components that behave differently during training and inference, such as <em>dropout</em> or <em>batch normalization</em> layers. Since we don’t have dropout or other components in our <code>NeuralNetwork</code> class that are affected by these settings, using <code>model.train()</code> and <code>model.eval()</code> is redundant in our preceding code. However, it’s best practice to include them anyway to avoid unexpected behaviors when we change the model architecture or reuse the code to train a different model.</p> 
  </div> 
  <div class="readable-text intended-text" id="p234"> 
   <p>As discussed earlier, we pass the logits directly into the <code>cross_entropy</code> loss function, which will apply the <code>softmax</code> function internally for efficiency and numerical stability reasons. Then, calling <code>loss.backward()</code> will calculate the gradients in the computation graph that PyTorch constructed in the background. The <code>optimizer.step()</code> method will use the gradients to update the model parameters to minimize the loss. In the case of the SGD optimizer, this means multiplying the gradients with the learning rate and adding the scaled negative gradient to the parameters. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p235"> 
   <p><span class="print-book-callout-head">NOTE</span>  To prevent undesired gradient accumulation, it is important to include an <code>optimizer.zero_grad()</code> call in each update round to reset the gradients to 0. Otherwise, the gradients will accumulate, which may be undesired.</p> 
  </div> 
  <div class="readable-text" id="p236"> 
   <p>After we have trained the model, we can use it to make predictions:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p237"> 
   <div class="code-area-container"> 
    <pre class="code-area">model.eval()
with torch.no_grad():
    outputs = model(X_train)
print(outputs)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p238"> 
   <p>The results are </p> 
  </div> 
  <div class="browsable-container listing-container" id="p239"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[ 2.8569, -4.1618],
        [ 2.5382, -3.7548],
        [ 2.0944, -3.1820],
        [-1.4814,  1.4816],
        [-1.7176,  1.7342]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p240"> 
   <p>To obtain the class membership probabilities, we can then use PyTorch’s <code>softmax</code> function:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p241"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.set_printoptions(sci_mode=False)
probas = torch.softmax(outputs, dim=1)
print(probas)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p242"> 
   <p>This outputs</p> 
  </div> 
  <div class="browsable-container listing-container" id="p243"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[    0.9991,     0.0009],
        [    0.9982,     0.0018],
        [    0.9949,     0.0051],
        [    0.0491,     0.9509],
        [    0.0307,     0.9693]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p244"> 
   <p>Let’s consider the first row in the preceding code output. Here, the first value (column) means that the training example has a 99.91% probability of belonging to class 0 and a 0.09% probability of belonging to class 1. (The <code>set_printoptions</code> call is used here to make the outputs more legible.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p245"> 
   <p>We can convert these values into class label predictions using PyTorch’s <code>argmax</code> function, which returns the index position of the highest value in each row if we set <code>dim=1</code> (setting <code>dim=0</code> would return the highest value in each column instead):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p246"> 
   <div class="code-area-container"> 
    <pre class="code-area">predictions = torch.argmax(probas, dim=1)
print(predictions)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p247"> 
   <p>This prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p248"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([0, 0, 0, 1, 1])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p249"> 
   <p>Note that it is unnecessary to compute <code>softmax</code> probabilities to obtain the class labels. We could also apply the <code>argmax</code> function to the logits (outputs) directly:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p250"> 
   <div class="code-area-container"> 
    <pre class="code-area">predictions = torch.argmax(outputs, dim=1)
print(predictions)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p251"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p252"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([0, 0, 0, 1, 1])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p253"> 
   <p>Here, we computed the predicted labels for the training dataset. Since the training dataset is relatively small, we could compare it to the true training labels by eye and see that the model is 100% correct. We can double-check this using the <code>==</code> comparison operator:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p254"> 
   <div class="code-area-container"> 
    <pre class="code-area">predictions == y_train</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p255"> 
   <p>The results are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p256"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([True, True, True, True, True])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p257"> 
   <p>Using <code>torch.sum</code>, we can count the number of correct predictions:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p258"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.sum(predictions == y_train)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p259"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p260"> 
   <div class="code-area-container"> 
    <pre class="code-area">5</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p261"> 
   <p>Since the dataset consists of five training examples, we have five out of five predictions that are correct, which has 5/5 × 100% = 100% prediction accuracy.</p> 
  </div> 
  <div class="readable-text intended-text" id="p262"> 
   <p>To generalize the computation of the prediction accuracy, let’s implement a <code>compute_accuracy</code> function, as shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p263"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing A.10</span> A function to compute the prediction accuracy </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def compute_accuracy(model, dataloader):

    model = model.eval()
    correct = 0.0
    total_examples = 0

    for idx, (features, labels) in enumerate(dataloader):

        with torch.no_grad():
            logits = model(features)

        predictions = torch.argmax(logits, dim=1)
        compare = labels == predictions      <span class="aframe-location"/> #1
        correct += torch.sum(compare)     <span class="aframe-location"/> #2
        total_examples += len(compare)

    return (correct / total_examples).item()   <span class="aframe-location"/> #3</pre> 
    <div class="code-annotations-overlay-container">
     #1 Returns a tensor of True/False values depending on whether the labels match
     <br/>#2 The sum operation counts the number of True values.
     <br/>#3 The fraction of correct prediction, a value between 0 and 1. .item() returns the value of the tensor as a Python float.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p264"> 
   <p>The code iterates over a data loader to compute the number and fraction of the correct predictions. When we work with large datasets, we typically can only call the model on a small part of the dataset due to memory limitations. The <code>compute_accuracy</code> function here is a general method that scales to datasets of arbitrary size since, in each iteration, the dataset chunk that the model receives is the same size as the batch size seen during training. The internals of the <code>compute_accuracy</code> function are similar to what we used before when we converted the logits to the class labels. </p> 
  </div> 
  <div class="readable-text intended-text" id="p265"> 
   <p>We can then apply the function to the training:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p266"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(compute_accuracy(model, train_loader))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p267"> 
   <p>The result is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p268"> 
   <div class="code-area-container"> 
    <pre class="code-area">1.0</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p269"> 
   <p>Similarly, we can apply the function to the test set:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p270"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(compute_accuracy(model, test_loader))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p271"> 
   <p>This prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p272"> 
   <div class="code-area-container"> 
    <pre class="code-area">1.0</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p273"> 
   <h2 class=" readable-text-h2"><span class="num-string">A.8</span> Saving and loading models</h2> 
  </div> 
  <div class="readable-text" id="p274"> 
   <p>Now that we’ve trained our model, let’s see how to save it so we can reuse it later. Here’s the recommended way of saving and loading models in PyTorch:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p275"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.save(model.state_dict(), "model.pth")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p276"> 
   <p>The model’s <code>state_dict</code> is a Python dictionary object that maps each layer in the model to its trainable parameters (weights and biases). <code>"model.pth"</code> is an arbitrary filename for the model file saved to disk. We can give it any name and file ending we like; however, <code>.pth</code> and <code>.pt</code> are the most common conventions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p277"> 
   <p>Once we saved the model, we can restore it from disk:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p278"> 
   <div class="code-area-container"> 
    <pre class="code-area">model = NeuralNetwork(2, 2) 
model.load_state_dict(torch.load("model.pth"))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p279"> 
   <p>The <code>torch.load("model.pth")</code> function reads the file <code>"model.pth"</code> and reconstructs the Python dictionary object containing the model’s parameters while <code>model.load_state_dict()</code> applies these parameters to the model, effectively restoring its learned state from when we saved it.</p> 
  </div> 
  <div class="readable-text intended-text" id="p280"> 
   <p>The line <code>model</code> <code>=</code> <code>NeuralNetwork(2,</code> <code>2)</code> is not strictly necessary if you execute this code in the same session where you saved a model. However, I included it here to illustrate that we need an instance of the model in memory to apply the saved parameters. Here, the <code>NeuralNetwork(2,</code> <code>2)</code> architecture needs to match the original saved model exactly.</p> 
  </div> 
  <div class="readable-text" id="p281"> 
   <h2 class=" readable-text-h2"><span class="num-string">A.9</span> Optimizing training performance with GPUs</h2> 
  </div> 
  <div class="readable-text" id="p282"> 
   <p>Next, let’s examine how to utilize GPUs, which accelerate deep neural network training compared to regular CPUs. First, we’ll look at the main concepts behind GPU computing in PyTorch. Then we will train a model on a single GPU. Finally, we’ll look at distributed training using multiple GPUs.</p> 
  </div> 
  <div class="readable-text" id="p283"> 
   <h3 class=" readable-text-h3"><span class="num-string">A.9.1</span> PyTorch computations on GPU devices</h3> 
  </div> 
  <div class="readable-text" id="p284"> 
   <p>Modifying the training loop to run optionally on a GPU is relatively simple and only requires changing three lines of code (see section A.7). Before we make the modifications, it’s crucial to understand the main concept behind GPU computations within PyTorch. In PyTorch, a device is where computations occur and data resides. The CPU and the GPU are examples of devices. A PyTorch tensor resides in a device, and its operations are executed on the same device.</p> 
  </div> 
  <div class="readable-text intended-text" id="p285"> 
   <p>Let’s see how this works in action. Assuming that you installed a GPU-compatible version of PyTorch (see section A.1.3), we can double-check that our runtime indeed supports GPU computing via the following code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p286"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(torch.cuda.is_available())</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p287"> 
   <p>The result is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p288"> 
   <div class="code-area-container"> 
    <pre class="code-area">True</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p289"> 
   <p>Now, suppose we have two tensors that we can add; this computation will be carried out on the CPU by default:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p290"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor_1 = torch.tensor([1., 2., 3.])
tensor_2 = torch.tensor([4., 5., 6.])
print(tensor_1 + tensor_2)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p291"> 
   <p>This outputs</p> 
  </div> 
  <div class="browsable-container listing-container" id="p292"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([5., 7., 9.])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p293"> 
   <p>We can now use the <code>.to()</code> method. This method is the same as the one we use to change a tensor’s datatype (see 2.2.2) to transfer these tensors onto a GPU and perform the addition there: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p294"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor_1 = tensor_1.to("cuda")
tensor_2 = tensor_2.to("cuda")
print(tensor_1 + tensor_2)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p295"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p296"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([5., 7., 9.], device='cuda:0')</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p297"> 
   <p>The resulting tensor now includes the device information, <code>device='cuda:0'</code>, which means that the tensors reside on the first GPU. If your machine hosts multiple GPUs, you can specify which GPU you’d like to transfer the tensors to. You do so by indicating the device ID in the transfer command. For instance, you can use <code>.to("cuda:0")</code>, <code>.to("cuda:1")</code>, and so on.</p> 
  </div> 
  <div class="readable-text intended-text" id="p298"> 
   <p>However, all tensors must be on the same device. Otherwise, the computation will fail, where one tensor resides on the CPU and the other on the GPU:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p299"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor_1 = tensor_1.to("cpu")
print(tensor_1 + tensor_2)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p300"> 
   <p>The results are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p301"> 
   <div class="code-area-container"> 
    <pre class="code-area">RuntimeError      Traceback (most recent call last)
&lt;ipython-input-7-4ff3c4d20fc3&gt; in &lt;cell line: 2&gt;()
      1 tensor_1 = tensor_1.to("cpu")
----&gt; 2 print(tensor_1 + tensor_2)
RuntimeError: Expected all tensors to be on the same device, but found at
least two devices, cuda:0 and cpu!</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p302"> 
   <p>In sum, we only need to transfer the tensors onto the same GPU device, and PyTorch will handle the rest. </p> 
  </div> 
  <div class="readable-text" id="p303"> 
   <h3 class=" readable-text-h3"><span class="num-string">A.9.2</span> Single-GPU training</h3> 
  </div> 
  <div class="readable-text" id="p304"> 
   <p>Now that we are familiar with transferring tensors to the GPU, we can modify the training loop to run on a GPU. This step requires only changing three lines of code, as shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p305"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing A.11</span> A training loop on a GPU</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
model = NeuralNetwork(num_inputs=2, num_outputs=2)

device = torch.device("cuda")     <span class="aframe-location"/> #1
model = model.to(device)         <span class="aframe-location"/> #2

optimizer = torch.optim.SGD(model.parameters(), lr=0.5)

num_epochs = 3

for epoch in range(num_epochs):

    model.train()
    for batch_idx, (features, labels) in enumerate(train_loader):
        features, labels = features.to(device), labels.to(device)  <span class="aframe-location"/> #3
        logits = model(features)
        loss = F.cross_entropy(logits, labels) # Loss function

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        ### LOGGING
        print(f"Epoch: {epoch+1:03d}/{num_epochs:03d}"
              f" | Batch {batch_idx:03d}/{len(train_loader):03d}"
              f" | Train/Val Loss: {loss:.2f}")

    model.eval()
    # Insert optional model evaluation code</pre> 
    <div class="code-annotations-overlay-container">
     #1 Defines a device variable that defaults to a GPU
     <br/>#2 Transfers the model onto the GPU
     <br/>#3 Transfers the data onto the GPU
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p306"> 
   <p>Running the preceding code will output the following, similar to the results obtained on the CPU (section A.7):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p307"> 
   <div class="code-area-container"> 
    <pre class="code-area">Epoch: 001/003 | Batch 000/002 | Train/Val Loss: 0.75
Epoch: 001/003 | Batch 001/002 | Train/Val Loss: 0.65
Epoch: 002/003 | Batch 000/002 | Train/Val Loss: 0.44
Epoch: 002/003 | Batch 001/002 | Train/Val Loss: 0.13
Epoch: 003/003 | Batch 000/002 | Train/Val Loss: 0.03
Epoch: 003/003 | Batch 001/002 | Train/Val Loss: 0.00</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p308"> 
   <p>We can use <code>.to("cuda")</code> instead of <code>device</code> <code>=</code> <code>torch.device("cuda")</code>. Transferring a tensor to <code>"cuda"</code> instead of <code>torch.device("cuda")</code> works as well and is shorter (see section A.9.1). We can also modify the statement, which will make the same code executable on a CPU if a GPU is not available. This is considered best practice when sharing PyTorch code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p309"> 
   <div class="code-area-container"> 
    <pre class="code-area">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p310"> 
   <p>In the case of the modified training loop here, we probably won’t see a speedup due to the memory transfer cost from CPU to GPU. However, we can expect a significant speedup when training deep neural networks, especially LLMs. </p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p311"> 
    <h5 class=" callout-container-h5 readable-text-h5">PyTorch on macOS </h5> 
   </div> 
   <div class="readable-text" id="p312"> 
    <p>On an Apple Mac with an Apple Silicon chip (like the M1, M2, M3, or newer models) instead of a computer with an Nvidia GPU, you can change </p> 
   </div> 
   <div class="browsable-container listing-container" id="p313"> 
    <div class="code-area-container"> 
     <pre class="code-area">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")</pre>  
    </div> 
   </div> 
   <div class="readable-text" id="p314"> 
    <p>to</p> 
   </div> 
   <div class="browsable-container listing-container" id="p315"> 
    <div class="code-area-container"> 
     <pre class="code-area">device = torch.device(
    "mps" if torch.backends.mps.is_available() else "cpu"
)</pre>  
    </div> 
   </div> 
   <div class="readable-text" id="p316"> 
    <p>to take advantage of this chip. </p> 
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p317"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise A.4</h5> 
   </div> 
   <div class="readable-text" id="p318"> 
    <p>Compare the run time of matrix multiplication on a CPU to a GPU. At what matrix size do you begin to see the matrix multiplication on the GPU being faster than on the CPU? Hint: use the <code>%timeit</code> command in Jupyter to compare the run time. For example, given matrices <code>a</code> and <code>b</code>, run the command <code>%timeit</code> <code>a</code> <code>@</code> <code>b</code> in a new notebook cell.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p319"> 
   <h3 class=" readable-text-h3"><span class="num-string">A.9.3</span> Training with multiple GPUs</h3> 
  </div> 
  <div class="readable-text" id="p320"> 
   <p>Distributed training is the concept of dividing the model training across multiple GPUs and machines. Why do we need this? Even when it is possible to train a model on a single GPU or machine, the process could be exceedingly time-consuming. The training time can be significantly reduced by distributing the training process across multiple machines, each with potentially multiple GPUs. This is particularly crucial in the experimental stages of model development, where numerous training iterations might be necessary to fine-tune the model parameters and architecture. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p321"> 
   <p><span class="print-book-callout-head">NOTE</span>  For this book, access to or use of multiple GPUs is not required. This section is included for those interested in how multi-GPU computing works in PyTorch.</p> 
  </div> 
  <div class="readable-text" id="p322"> 
   <p>Let’s begin with the most basic case of distributed training: PyTorch’s <code>DistributedDataParallel</code> (DDP) strategy. DDP enables parallelism by splitting the input data across the available devices and processing these data subsets simultaneously.</p> 
  </div> 
  <div class="readable-text intended-text" id="p323"> 
   <p>How does this work? PyTorch launches a separate process on each GPU, and each process receives and keeps a copy of the model; these copies will be synchronized during training. To illustrate this, suppose we have two GPUs that we want to use to train a neural network, as shown in figure A.12. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p324">  
   <img alt="figure" src="../Images/A-12.png" width="1100" height="587"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure A.12</span> The model and data transfer in DDP involves two key steps. First, we create a copy of the model on each of the GPUs. Then we divide the input data into unique minibatches that we pass on to each model copy.</h5>
  </div> 
  <div class="readable-text intended-text" id="p325"> 
   <p>Each of the two GPUs will receive a copy of the model. Then, in every training iteration, each model will receive a minibatch (or just “batch”) from the data loader. We can use a <code>DistributedSampler</code> to ensure that each GPU will receive a different, non-overlapping batch when using DDP. </p> 
  </div> 
  <div class="readable-text intended-text" id="p326"> 
   <p>Since each model copy will see a different sample of the training data, the model copies will return different logits as outputs and compute different gradients during the backward pass. These gradients are then averaged and synchronized during training to update the models. This way, we ensure that the models don’t diverge, as illustrated in figure A.13.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p327">  
   <img alt="figure" src="../Images/A-13.png" width="922" height="284"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure A.13</span> The forward and backward passes in DDP are executed independently on each GPU with its corresponding data subset. Once the forward and backward passes are completed, gradients from each model replica (on each GPU) are synchronized across all GPUs. This ensures that every model replica has the same updated weights.</h5>
  </div> 
  <div class="readable-text" id="p328"> 
   <p>The benefit of using DDP is the enhanced speed it offers for processing the dataset compared to a single GPU. Barring a minor communication overhead between devices that comes with DDP use, it can theoretically process a training epoch in half the time with two GPUs compared to just one. The time efficiency scales up with the number of GPUs, allowing us to process an epoch eight times faster if we have eight GPUs, and so on.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p329"> 
   <p><span class="print-book-callout-head">NOTE</span>  DDP does not function properly within interactive Python environments like Jupyter notebooks, which don’t handle multiprocessing in the same way a standalone Python script does. Therefore, the following code should be executed as a script, not within a notebook interface like Jupyter. DDP needs to spawn multiple processes, and each process should have its own Python interpreter instance.</p> 
  </div> 
  <div class="readable-text" id="p330"> 
   <p>Let’s now see how this works in practice. For brevity, I focus on the core parts of the code that need to be adjusted for DDP training. However, readers who want to run the code on their own multi-GPU machine or a cloud instance of their choice should use the standalone script provided in this book’s GitHub repository at <a href="https://github.com/rasbt/LLMs-from-scratch">https://github.com/rasbt/LLMs-from-scratch</a>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p331"> 
   <p>First, we import a few additional submodules, classes, and functions for distributed training PyTorch, as shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p332"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing A.12</span> PyTorch utilities for distributed training</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import torch.multiprocessing as mp
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p333"> 
   <p>Before we dive deeper into the changes to make the training compatible with DDP, let’s briefly go over the rationale and usage for these newly imported utilities that we need alongside the <code>DistributedDataParallel</code> class.</p> 
  </div> 
  <div class="readable-text intended-text" id="p334"> 
   <p>PyTorch’s <code>multiprocessing</code> submodule contains functions such as <code>multiprocessing .spawn</code>, which we will use to spawn multiple processes and apply a function to multiple inputs in parallel. We will use it to spawn one training process per GPU. If we spawn multiple processes for training, we will need a way to divide the dataset among these different processes. For this, we will use the <code>DistributedSampler</code>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p335"> 
   <p><code>init_process_group</code> and <code>destroy_process_group</code> are used to initialize and quit the distributed training mods. The <code>init_process_group</code> function should be called at the beginning of the training script to initialize a process group for each process in the distributed setup, and <code>destroy_process_group</code> should be called at the end of the training script to destroy a given process group and release its resources. The code in the following listing illustrates how these new components are used to implement DDP training for the <code>NeuralNetwork</code> model we implemented earlier.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p336"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing A.13</span> Model training with the <code>DistributedDataParallel</code> strategy</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def ddp_setup(rank, world_size):
    os.environ["MASTER_ADDR"] = "localhost"   <span class="aframe-location"/> #1
    os.environ["MASTER_PORT"] = "12345"     <span class="aframe-location"/> #2
    init_process_group(
        backend="nccl",             <span class="aframe-location"/> #3
        rank=rank,                        <span class="aframe-location"/> #4
        world_size=world_size           <span class="aframe-location"/> #5
    )
    torch.cuda.set_device(rank)       <span class="aframe-location"/> #6

def prepare_dataset():
    # insert dataset preparation code 
    train_loader = DataLoader(
        dataset=train_ds,
        batch_size=2,
        shuffle=False,            <span class="aframe-location"/> #7
        pin_memory=True,          <span class="aframe-location"/> #8
        drop_last=True,
        sampler=DistributedSampler(train_ds)   <span class="aframe-location"/> #9
    )    
    return train_loader, test_loader

def main(rank, world_size, num_epochs):      <span class="aframe-location"/> #10
    ddp_setup(rank, world_size)
    train_loader, test_loader = prepare_dataset()
    model = NeuralNetwork(num_inputs=2, num_outputs=2)
    model.to(rank)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.5)
    model = DDP(model, device_ids=[rank])
    for epoch in range(num_epochs):
        train_loader.sampler.set_epoch(epoch)
        model.train()
        for features, labels in train_loader:
            features, labels = features.to(rank), labels.to(rank)     <span class="aframe-location"/> #11
            # insert model prediction and backpropagation code 
            print(f"[GPU{rank}] Epoch: {epoch+1:03d}/{num_epochs:03d}"
                  f" | Batchsize {labels.shape[0]:03d}"
                  f" | Train/Val Loss: {loss:.2f}")

    model.eval()
    train_acc = compute_accuracy(model, train_loader, device=rank)
    print(f"[GPU{rank}] Training accuracy", train_acc)
    test_acc = compute_accuracy(model, test_loader, device=rank)
    print(f"[GPU{rank}] Test accuracy", test_acc)
    destroy_process_group()                     <span class="aframe-location"/> #12

if __name__ == "__main__":
    print("Number of GPUs available:", torch.cuda.device_count())
    torch.manual_seed(123)
    num_epochs = 3
    world_size = torch.cuda.device_count()
    mp.spawn(main, args=(world_size, num_epochs), nprocs=world_size) <span class="aframe-location"/> #13</pre> 
    <div class="code-annotations-overlay-container">
     #1 Address of the main node
     <br/>#2 Any free port on the machine
     <br/>#3 nccl stands for NVIDIA Collective Communication Library.
     <br/>#4 rank refers to the index of the GPU we want to use.
     <br/>#5 world_size is the number of GPUs to use.
     <br/>#6 Sets the current GPU device on which tensors will be allocated and operations will be performed
     <br/>#7 Distributed-Sampler takes care of the shuffling now.
     <br/>#8 Enables faster memory transfer when training on GPU
     <br/>#9 Splits the dataset into distinct, non-overlapping subsets for each process (GPU)
     <br/>#10 The main function running the model training
     <br/>#11 rank is the GPU ID
     <br/>#12 Cleans up resource allocation
     <br/>#13 Launches the main function using multiple processes, where nprocs=world_size means one process per GPU.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p337"> 
   <p>Before we run this code, let’s summarize how it works in addition to the preceding annotations. We have a <code>__name__</code> <code>==</code> <code>"__main__"</code> clause at the bottom containing code executed when we run the code as a Python script instead of importing it as a module. This code first prints the number of available GPUs using <code>torch.cuda.device_count()</code>, sets a random seed for reproducibility, and then spawns new processes using PyTorch’s <code>multiprocessesing.spawn</code> function. Here, the <code>spawn</code> function launches one process per GPU setting <code>nproces=world_size</code>, where the world size is the number of available GPUs. This <code>spawn</code> function launches the code in the <code>main</code> function we define in the same script with some additional arguments provided via <code>args</code>. Note that the <code>main</code> function has a <code>rank</code> argument that we don’t include in the <code>mp.spawn()</code> call. That’s because the <code>rank</code>, which refers to the process ID we use as the GPU ID, is already passed automatically.</p> 
  </div> 
  <div class="readable-text intended-text" id="p338"> 
   <p>The <code>main</code> function sets up the distributed environment via <code>ddp_setup</code>—another function we defined—loads the training and test sets, sets up the model, and carries out the training. Compared to the single-GPU training (section A.9.2), we now transfer the model and data to the target device via .<code>to(rank)</code>, which we use to refer to the GPU device ID. Also, we wrap the model via <code>DDP</code>, which enables the synchronization of the gradients between the different GPUs during training. After the training finishes and we evaluate the models, we use <code>destroy_process_group()</code> to cleanly exit the distributed training and free up the allocated resources.</p> 
  </div> 
  <div class="readable-text intended-text" id="p339"> 
   <p>Earlier I mentioned that each GPU will receive a different subsample of the training data. To ensure this, we set <code>sampler=DistributedSampler(train_ds)</code> in the training loader.</p> 
  </div> 
  <div class="readable-text intended-text" id="p340"> 
   <p>The last function to discuss is <code>ddp_setup</code>. It sets the main node’s address and port to allow for communication between the different processes, initializes the process group with the NCCL backend (designed for GPU-to-GPU communication), and sets the <code>rank</code> (process identifier) and world size (total number of processes). Finally, it specifies the GPU device corresponding to the current model training process rank.</p> 
  </div> 
  <div class="readable-text" id="p341"> 
   <h4 class=" readable-text-h4">Selecting available GPUs on a multi-GPU machine </h4> 
  </div> 
  <div class="readable-text" id="p342"> 
   <p>If you wish to restrict the number of GPUs used for training on a multi-GPU machine, the simplest way is to use the <code>CUDA_VISIBLE_DEVICES</code> environment variable. To illustrate this, suppose your machine has multiple GPUs, and you only want to use one GPU—for example, the GPU with index 0. Instead of <code>python</code> <code>some_script.py</code>, you can run the following code from the terminal:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p343"> 
   <div class="code-area-container"> 
    <pre class="code-area">CUDA_VISIBLE_DEVICES=0 python some_script.py</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p344"> 
   <p>Or, if your machine has four GPUs and you only want to use the first and third GPU, you can use</p> 
  </div> 
  <div class="browsable-container listing-container" id="p345"> 
   <div class="code-area-container"> 
    <pre class="code-area">CUDA_VISIBLE_DEVICES=0,2 python some_script.py</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p346"> 
   <p>Setting <code>CUDA_VISIBLE_DEVICES</code> in this way is a simple and effective way to manage GPU allocation without modifying your PyTorch scripts.</p> 
  </div> 
  <div class="readable-text intended-text" id="p347"> 
   <p>Let’s now run this code and see how it works in practice by launching the code as a script from the terminal:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p348"> 
   <div class="code-area-container"> 
    <pre class="code-area">python ch02-DDP-script.py</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p349"> 
   <p>Note that it should work on both single and multi-GPU machines. If we run this code on a single GPU, we should see the following output:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p350"> 
   <div class="code-area-container"> 
    <pre class="code-area">PyTorch version: 2.2.1+cu117
CUDA available: True
Number of GPUs available: 1
[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.62
[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.32
[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.11
[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.07
[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.02
[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.03
[GPU0] Training accuracy 1.0
[GPU0] Test accuracy 1.0</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p351"> 
   <p>The code output looks similar to that using a single GPU (section A.9.2), which is a good sanity check. </p> 
  </div> 
  <div class="readable-text intended-text" id="p352"> 
   <p>Now, if we run the same command and code on a machine with two GPUs, we should see the following:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p353"> 
   <div class="code-area-container"> 
    <pre class="code-area">PyTorch version: 2.2.1+cu117
CUDA available: True
Number of GPUs available: 2
[GPU1] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.60
[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.59
[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.16
[GPU1] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.17
[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.05
[GPU1] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.05
[GPU1] Training accuracy 1.0
[GPU0] Training accuracy 1.0
[GPU1] Test accuracy 1.0
[GPU0] Test accuracy 1.0</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p354"> 
   <p>As expected, we can see that some batches are processed on the first GPU (<code>GPU0</code>) and others on the second (<code>GPU1</code>). However, we see duplicated output lines when printing the training and test accuracies. Each process (in other words, each GPU) prints the test accuracy independently. Since DDP replicates the model onto each GPU and each process runs independently, if you have a print statement inside your testing loop, each process will execute it, leading to repeated output lines. If this bothers you, you can fix it using the rank of each process to control your print statements: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p355"> 
   <div class="code-area-container"> 
    <pre class="code-area">if rank == 0:                 <span class="aframe-location"/> #1
    print("Test accuracy: ", accuracy)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Only print in the first process
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p356"> 
   <p>This is, in a nutshell, how distributed training via DDP works. If you are interested in additional details, I recommend checking the official API documentation at <a href="https://mng.bz/9dPr">https://mng.bz/9dPr</a>.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p357"> 
    <h5 class=" callout-container-h5 readable-text-h5">Alternative PyTorch APIs for multi-GPU training </h5> 
   </div> 
   <div class="readable-text" id="p358"> 
    <p>If you prefer a more straightforward way to use multiple GPUs in PyTorch, you can consider add-on APIs like the open-source Fabric library. I wrote about it in “Accelerating PyTorch Model Training: Using Mixed-Precision and Fully Sharded Data Parallelism” (<a href="https://mng.bz/jXle">https://mng.bz/jXle</a>).</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p359"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p360"> PyTorch is an open source library with three core components: a tensor library, automatic differentiation functions, and deep learning utilities. </li> 
   <li class="readable-text" id="p361"> PyTorch’s tensor library is similar to array libraries like NumPy. </li> 
   <li class="readable-text" id="p362"> In the context of PyTorch, tensors are array-like data structures representing scalars, vectors, matrices, and higher-dimensional arrays. </li> 
   <li class="readable-text" id="p363"> PyTorch tensors can be executed on the CPU, but one major advantage of PyTorch’s tensor format is its GPU support to accelerate computations. </li> 
   <li class="readable-text" id="p364"> The automatic differentiation (autograd) capabilities in PyTorch allow us to conveniently train neural networks using backpropagation without manually deriving gradients. </li> 
   <li class="readable-text" id="p365"> The deep learning utilities in PyTorch provide building blocks for creating custom deep neural networks. </li> 
   <li class="readable-text" id="p366"> PyTorch includes <code>Dataset</code> and <code>DataLoader</code> classes to set up efficient data-loading pipelines. </li> 
   <li class="readable-text" id="p367"> It’s easiest to train models on a CPU or single GPU. </li> 
   <li class="readable-text" id="p368"> Using <code>DistributedDataParallel</code> is the simplest way in PyTorch to accelerate the training if multiple GPUs are available. </li> 
  </ul>
 </div></div></body></html>