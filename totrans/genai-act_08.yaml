- en: '7 Retrieval-augmented generation: The secret weapon'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Concepts of retrieval-augmented generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benefits of the RAG architecture in conjunction with large language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the role of vector databases and indexes in implementing RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basics of vector search and understanding the distance functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges in RAG implementation and potential solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different methods of chunking text for RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we have seen, large language models (LLMs) are very powerful and help us
    achieve things that were not possible until very recently. Interestingly, LLMs
    capture the world’s knowledge and are available to anyone at the end of an API,
    anywhere in the world.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, LLMs have a knowledge constraint: their understanding and knowledge
    extend up to their last training cut-off; after that date, they do not have any
    new information. Consequently, LLMs cannot utilize the latest information. In
    addition, the training corpus of LLMs does not contain any private nonpublic knowledge.
    Therefore, LLMs cannot operate and answer specific and proprietary questions to
    enterprises.'
  prefs: []
  type: TYPE_NORMAL
- en: One practical way to solve this problem is by using a pattern called retrieval-augmented
    generation (RAG). This chapter will explore using RAG to enhance LLMs with your
    data. You will learn what RAG is, why it is useful for enterprise applications,
    and how to implement it using vector databases and indexes. Finally, the chapter
    will discuss some chunking strategies to optimize the relevance and efficiency
    of RAG.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will start by understanding RAG. In the next chapter, we
    will build on that by combining all the concepts for an end-to-end sample.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 What is RAG?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RAG is a method that combines additional data with a language model’s input
    to improve its output without altering the initial prompt. This supplemental data
    can come from an organization’s database or an external, updated source. The language
    model then processes the merged information to include factual data from the knowledge
    base in its response. This technique is particularly useful when the latest data
    and its integration into your information are required.
  prefs: []
  type: TYPE_NORMAL
- en: In technical terms, RAG merges a pretrained language model and an external knowledge
    index to enhance language generation. Facebook AI Research first introduced RAG
    in a study titled “Retrieval-Augmented Generation for Knowledge-Intensive NLP
    Tasks” [1]. It demonstrated that RAG models can achieve state-of-the-art results
    on various knowledge-intensive tasks in natural language processing (NLP), such
    as open-domain question answering, fact verification, and natural language inference.
    It also proved that RAG models can generate more precise, diverse, and factual
    language than a leading language model that doesn’t use additional data.
  prefs: []
  type: TYPE_NORMAL
- en: The RAG model combines the powers of a dense passage retriever and a sequence-to-sequence
    model to generate informative answers based on a large corpus of text. It was
    designed to improve question-answering systems, fact verification, and question-generation
    tasks by integrating information retrieval with generative language models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.1 shows an overview of the RAG pattern and the overall approach. At
    a high level, there are two components: the retriever and the generator. As the
    name suggests, the retriever is responsible for retrieving the information, and
    the generator is the LLM, used to generate the text.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F01_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 RAG architecture overview
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Foundational models, notably LLMs such as the OpenAI GPT series, possess immense
    potential but do have drawbacks. These models, while powerful, suffer from a static
    knowledge base, meaning they are unaware of events or developments posttraining,
    causing them to become outdated over time. They are also heavily influenced by
    their training data, and any bias, misinformation, or imbalance in this data can
    taint the model’s output. Furthermore, LLMs lack a genuine understanding of the
    content, often generating text based solely on patterns observed during training
    without comprehension. This can be problematic in corporate scenarios with specific
    policies and rules. Finally, these models can create plausible yet factually incorrect
    information, which can propagate misinformation without a reliable verification
    method.
  prefs: []
  type: TYPE_NORMAL
- en: RAG helps improve the quality of responses by drawing on these external sources
    of knowledge to supplement the LLM’s internal information. This is especially
    helpful in addressing the static knowledge of LLMs where they cannot provide accurate
    generations for events or facts that happened after their training cutoff dates.
  prefs: []
  type: TYPE_NORMAL
- en: RAG is an essential component of working with LLMs, along with prompt engineering.
    By accessing a broader variety of information, RAG can produce more accurate and
    informative answers. It ensures that the model relies on the most up-to-date,
    dependable facts and that users can see its sources, ensuring that its statements
    can be verified for correctness and ultimately trusted.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 RAG benefits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While RAG is still in its early stages of development, it holds the potential
    to transform the landscape of text generation models. RAG can be harnessed to
    produce more comprehensive, varied, and factual text generation models for many
    applications. This section delves into the myriad of benefits that enterprises
    can gain.
  prefs: []
  type: TYPE_NORMAL
- en: RAG’s ability to draw data from external resources in real-time is a game changer
    for sectors that require up-to-the-minute data, such as finance, healthcare, or
    news. Whether tracking market dynamics, updating healthcare records, or breaking
    news, RAG guarantees the inclusion of the latest information. This ensures that
    the output is consistently relevant and current.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to traditional ML techniques, RAG offers a cost-effective alternative
    for businesses. Traditional techniques may necessitate retraining a model each
    time new data is added. However, with RAG, businesses only need to update the
    external dataset, saving time and costs related to model training and data processing.
  prefs: []
  type: TYPE_NORMAL
- en: RAG proves particularly useful when responses need to cite data or display source
    references. It can anchor the generated data in the source material and even provide
    citations. This is of immense value in academic, legal, or professional scenarios
    where precise sourcing of information is required.
  prefs: []
  type: TYPE_NORMAL
- en: RAG’s versatility extends to the types of data it can process, accommodating
    structured and unstructured data in various formats. This adaptability allows
    RAG to be utilized in diverse applications, from analyzing intricate datasets
    to processing and generating multimedia content.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing RAG enhances customer interactions and facilitates improved decision-making.
    In customer service or chatbot applications, RAG can retrieve detailed information
    from databases or FAQs, which results in more accurate and constructive responses.
    Furthermore, RAG can combine insights from large datasets with language model
    generation in decision support systems to offer comprehensive and informed recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: RAG’s scalability and performance are exceptional, enabling businesses to utilize
    vast external datasets without overburdening the language model. This allows generating
    outputs based on a wide array of information without compromising the model’s
    performance or efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: RAG also allows customizing of external datasets based on a business domain.
    For instance, a pharmaceutical company could maintain a dataset solely for new
    drug research, allowing RAG to offer domain-specific responses. From a research
    and development perspective, sectors such as biotechnology or technology can greatly
    benefit from RAG’s ability to retrieve relevant literature or data insights, speeding
    up the innovation process.
  prefs: []
  type: TYPE_NORMAL
- en: RAG offers a dynamic, efficient, and versatile solution for integrating external
    datasets into language models. This feature results in more accurate, relevant,
    and current information in automated systems, enhancing efficiency, customer satisfaction,
    and decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: What is data grounding?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Grounding your data means connecting LLMs with external information sources.
    Grounding can be done using various methods; however, RAG is a common one. Usually,
    these external data sources are chosen based on the use case needs, enhancing
    the quality and dependability of the generated output. Grounding can make the
    generated output better by giving LLMs information that is use-case specific,
    relevant, and not included in the LLM’s training data. This way, the LLMs can
    use the data from external sources as context and generate more precise and relevant
    answers for the user.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the benefits of grounding are
  prefs: []
  type: TYPE_NORMAL
- en: It can help the LLMs produce more factual and reliable output, as it reduces
    the risk of hallucination, which is when the LLMs invent false or misleading information
    in their output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can help the LLMs produce more diverse and representative output, allowing
    them to access information from various sources and perspectives and avoid biases
    or errors in their internal knowledge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can help the LLMs produce more customized and personalized output, enabling
    them to adapt to the user’s preferences, needs, and goals and provide tailored
    solutions or suggestions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG models can utilize the vast amount of information stored in text corpora
    to enrich their outputs with relevant facts and details. They can also handle
    open-domain questions and tasks that require reasoning and inference beyond the
    scope of LLMs. Let’s explore the RAG architecture in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 RAG architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It was outlined earlier that the RAG architecture consists of two main components:
    the retriever and the LLM. The retriever extracts data from different enterprise
    systems, as illustrated in figure 7.2 [1]. These components can be adapted and
    adjusted based on the application and task at hand, and together, they give the
    RAG model a lot of flexibility and strength.'
  prefs: []
  type: TYPE_NORMAL
- en: The retriever can access information from private knowledge sources and search
    engines. This is the mechanism behind Bing Chat, which helps provide more current
    information. This retriever does more than search—it filters out only the relevant
    information, which becomes the context for the generative model.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F02_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 Overview of RAG for knowledge-intensive NLP tasks [1]
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The RAG pattern combines information retrieval and text generation to enhance
    language model outputs. The query encoder initially encodes an input question
    or statement into a vector. This vector, `q(x)`, is then utilized by a nonparametric
    retriever to sift through a precompiled document index, seeking documents relevant
    to the query.
  prefs: []
  type: TYPE_NORMAL
- en: The retriever employs maximum inner product search (MIPS), which identifies
    documents with the highest similarity to the query vector. These documents are
    pre-encoded into vectors, represented as `d(z)`, in the document index.
  prefs: []
  type: TYPE_NORMAL
- en: The generator (i.e., the LLM) utilizes the information from the retrieved documents
    to produce human-like text. This architecture component is responsible for answering
    questions, verifying facts, or generating new questions.
  prefs: []
  type: TYPE_NORMAL
- en: The final process is marginalization, where instead of relying on a single document
    to generate a response, the RAG model considers all pertinent documents. It calculates
    the overall probability of each possible answer by summing up the probabilities
    based on each retrieved document, which ensures a more comprehensive and contextual
    awareness by integrating a wide array of retrieved information into the final
    text generation.
  prefs: []
  type: TYPE_NORMAL
- en: The other key component is the LLM, which takes the context from the retrieval
    model and generates a natural language output. The generative model also provides
    feedback to the retrieval model to improve its accuracy over time. This is done
    using prompt engineering, as we saw in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Retriever system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The retriever is essentially the component that searches various knowledge sources,
    as shown in figure 7.2\. Its main purpose is to search through the corpus of information
    and find the relevant information that can be used. The retrieved information
    is then provided to the generator model, which uses it to generate its output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two main types of retriever systems are used in RAG: sparse and dense. Sparse
    retrievers are traditional retrieval systems that use traditional search techniques,
    such as term frequency-inverse document frequency (TF-IDF), to match queries to
    documents. Dense retrievers are newer retrieval systems that use machine learning
    to encode queries and documents into vectors and then match queries to documents
    based on the similarity of their vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right type of retrieval system in a RAG architecture (sparse or
    dense) is critical because it fundamentally affects the model’s performance and
    applicability. Sparse retrievers, such as those using TF-IDF, are fast and efficient,
    using inverted indexes to match queries with documents based on keyword overlap.
    This makes them suitable for large-scale, keyword-dependent search tasks with
    limited computational resources. However, they might struggle with the subtleties
    of language, such as synonyms and nuanced phrasing.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, dense retrievers utilize machine learning techniques to encode
    queries and documents into vectors, capturing deeper semantic relationships beyond
    mere keyword matching. This allows them to handle more complex queries and understand
    context better, which is particularly beneficial for queries with ambiguous or
    specialized language. While dense retrievers often yield more relevant and contextually
    appropriate documents, they are more computationally intensive and require substantial
    amounts of training data, making them resource-heavy both in the training phase
    and during inference.
  prefs: []
  type: TYPE_NORMAL
- en: The choice between sparse and dense retrievers should be guided by the task’s
    specific needs, considering the nature of the queries, domain specificity, resource
    availability, and the necessity of nuanced language understanding.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of retriever affects the balance between computational efficiency
    and depth of understanding. Despite their computational costs, dense retrievers
    are often preferred for tasks requiring a nuanced understanding of language. However,
    sparse retrievers may still be viable for applications where speed and efficiency
    are paramount, or where queries are expected to match document text closely. The
    best retriever for a given application will depend on its specific requirements
    and the resources available for implementing and maintaining the system.
  prefs: []
  type: TYPE_NORMAL
- en: What are BM25, TF-IDF, and DPR?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: BM25 is a ranking function used by search engines to estimate the relevance
    of documents to a given search query. It is one of the most widely used ranking
    functions in information retrieval. BM25 considers many factors, including the
    term frequency (TF) of the query terms in the document, the inverse document frequency
    (IDF) of the query terms, and the length of the document.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF is a statistical measure used to evaluate how important a word is to
    a document in a collection of documents. The TF-IDF value increases proportionally
    to the number of times a word appears in a document. It decreases proportionally
    to the number of documents in the collection that contain the word. TF-IDF is
    often used in information retrieval and text mining to rank documents based on
    their relevance to a given query.
  prefs: []
  type: TYPE_NORMAL
- en: DPR is a neural network model that retrieves relevant passages from a large
    text corpus. It is trained on a massive dataset of text and code and learns to
    embed passages and queries into a dense vector space. DPR can retrieve passages
    semantically, similar to the query, by calculating the cosine similarity between
    the passage and query vectors.
  prefs: []
  type: TYPE_NORMAL
- en: BM25 and TF-IDF are statistical measures of a document’s relevance to a given
    query. However, BM25 considers additional factors, such as the length of the document
    and the saturation of term frequency. DPR can be used to improve the performance
    of BM25 and TF-IDF ranking functions.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, we need to follow the process outlined in figure 7.3 to harness
    the power of LLMs on our data. The source pulled by the retriever would need to
    be split into smaller sizes. This is required to make the information more manageable
    and conform to the context windows of the LLMs. Next, we must create embeddings
    of these smaller chunks and link them to the source as metadata. Finally, these
    embeddings and associated metadata should be persisted in a data store.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F03_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 Custom data on LLMs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For RAG to be efficient and scalable, the retriever component must quickly
    fetch the most relevant documents from potentially billions of candidates. We
    need two components to help address this challenge: a vector database and an index.
    A vector database is a system that stores and provides access to structured or
    unstructured data (e.g., text or images) alongside its vector embeddings, which
    are the data’s numerical representation. A vector index is a data structure that
    enables efficient and fast lookup of nearest neighbors in the high-dimensional
    vector space.'
  prefs: []
  type: TYPE_NORMAL
- en: Without efficient vector databases and indexes, the retrieval step would become
    a bottleneck, making the entire RAG system slow and impractical. Using these tools,
    relevant documents can be retrieved in real time, allowing the generator component
    to produce answers quickly and making the system usable for applications such
    as open-domain question answering. Let’s explore both in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 Understanding vector databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vector databases enable enterprises to manage, secure, and scale embeddings
    in a production environment. For many enterprises, vector databases for semantic
    search use cases solve the performance and security requirements needed for production
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: A vector database is specifically designed to operate on embedding vectors.
    As the popularity of LLMs and generative AI has grown recently, so has the use
    of embeddings to encode unstructured data. Vector databases have emerged as an
    effective solution for enterprises to deliver and scale these use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases are specialized databases that store data as high-dimensional
    vectors and their original content. They offer the capabilities of both vector
    indexes and traditional databases, such as optimized storage, scalability, flexibility,
    and query language support. They allow users to find and retrieve similar or relevant
    data based on their semantic or contextual meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Given the vast number of documents in large corpora, brute-force comparison
    of a query vector with every document vector is computationally prohibitive. The
    solution is vector search, which comprises indexes and databases that allow efficient
    storage and near-neighbor lookups in high-dimensional spaces. Figure 7.4 shows
    a typical pipeline of incorporating a vector database when implementing a RAG
    pattern with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F04_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 Typical pipeline for vector database
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Vector databases can help RAG models quickly find the most similar documents
    or passages to a given query and use them as additional context for the LLM. Depending
    on the trade-off between speed and accuracy, vector databases can also support
    various retrieval strategies, such as exact, approximate, or hybrid methods. Having
    a vector database is a good start, but finding the most similar documents or passages
    can only happen with a vector index.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.1 What is a vector index?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A vector index is a data structure in a vector database designed to enhance
    the efficiency of processing, and it is particularly suited for the high-dimensional
    vector data encountered with LLMs. Its function is to streamline the search and
    retrieval processes within the database. By implementing a vector index, the system
    is capable of conducting quick similarity searches, identifying vectors that closely
    match or are most similar to a given input vector. Essentially, vector indexes
    are designed to enable rapid and precise similarity search, facilitating the recovery
    of vector embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: They organize the vectors using various techniques, such as hashing, clustering,
    or tree-based methods, to make finding the most similar ones easy based on their
    distance or similarity metrics. For example, FAISS (Facebook AI Similarity Search)
    is a popular vector index that efficiently handles billions of vectors.
  prefs: []
  type: TYPE_NORMAL
- en: To create vector indexes for your embeddings, there are many options, such as
    exact or approximate nearest neighbor algorithms (e.g., HNSW or IVF), different
    distance metrics (e.g., cosine or Euclidean), or various compression techniques
    (e.g., quantization or pruning). Your index method depends on balancing speed,
    accuracy, and memory consumption. We can use different mathematical methods to
    compare how similar two vector embeddings are—these are useful when searching
    and matching different embeddings. Let’s see what vector search means and how
    we can apply different mathematical functions when searching.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.2 Vector search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A vector search is a query operation that finds the vectors most similar to
    a given query vector based on a similarity metric. In a RAG pattern for LLMs,
    a vector index stores the documents’ embeddings or passages that the LLM can retrieve
    as context for generating responses. A vector search is used to find the most
    relevant documents or passages to the query based on the similarity between the
    query vector and the document vectors in the index.
  prefs: []
  type: TYPE_NORMAL
- en: Similarity measures are mathematical methods that compare two vectors and compute
    a distance value between them. This distance value indicates how dissimilar or
    similar the two vectors are in terms of their semantic meaning.
  prefs: []
  type: TYPE_NORMAL
- en: The distance can be based on multiple criteria, such as the length of the line
    segment between two points, the angle between two directions, or the number of
    mismatched elements in two arrays. Similarity measures are useful for machine
    learning tasks involving grouping or classifying data objects, especially for
    vector or semantic search.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we want to find words similar to “puppy,” we can generate a
    vector embedding for this word and look for other words with close vector embeddings,
    such as “dog” (figure 7.5).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F05_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 Vector search
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We should choose the similarity measure that best suits the data and query needs
    of the use case. We must use a similarity measure to perform a vector search,
    a mathematical method for calculating the distance between two vectors. The smaller
    the distance, the more similar the vectors are. Some popular enterprise-ready
    services, such as Azure AI Search, support several similarity measures. Some of
    the more common similarity searches are
  prefs: []
  type: TYPE_NORMAL
- en: '*Cosine similarity*—This measure calculates the cosine of the angle between
    two vectors. It ranges from –1 to 1, where 1 means identical vectors and –1 means
    opposite vectors. Cosine similarity is commonly used for normalized embedding
    spaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Squared Euclidean or L2-squared distance*—It calculates the straight-line
    distance between two vectors. It ranges from 0 to infinity [0, ∞], where 0 means
    identical vectors, and larger values mean more dissimilar vectors. Squared Euclidean
    distance is also known as the L2 norm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dot product*—This measure calculates the product of the magnitudes of two
    vectors and the cosine of the angle between them. It ranges from –infinity to
    infinity [–, ∞], where 0 means orthogonal vectors and larger values mean more
    similar vectors. The dot product is equivalent to cosine similarity for normalized
    embedding spaces but is more efficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hamming distance*—This calculates the number of differences between vectors
    at each dimension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Manhattan or L1 distance*—This measures the sum of the absolute differences
    between the coordinates of two vectors. It ranges from 0 to infinity [0, ∞], where
    0 means identical vectors and larger values mean vectors mean the opposite, that
    is, dissimilar vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 7.6 shows the different similarity measures. It is important to use the
    same metric on which the underlying foundational model has been trained. For example,
    in the case of the OpenAI GPT class of models, the distance function is cosine
    similarity.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F06_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 Different distant functions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note  OpenAI embeddings are normalized to length 1, meaning each vector’s magnitude
    equals 1\. Therefore, if we use OpenAI embeddings normalized to length 1, we can
    choose either cosine similarity or Euclidean distance as our distance function,
    and we will get the same results for vector search. However, cosine similarity
    might be slightly faster to compute because it only involves a dot product operation.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right distance measure depends on the specific use case, the nature
    of the data, and the desired outcomes. Table 7.1 gives a brief overview of when
    to use each measure.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.1 Choosing the right distance measure
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Measure | When to use | Advantage | Disadvantage |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Cosine similarity  | Ideal for text and document similarity, where the magnitude
    of the vectors is not as important as the orientation; common in NLP tasks  |
    Effective in high-dimensional spaces and for normalized vectors; ignores the magnitude
    of vectors, focusing on orientation, making it suitable for comparing documents
    of different lengths  | It is not effective if the magnitude of vectors is important.  |'
  prefs: []
  type: TYPE_TB
- en: '| Squared Euclidean (L2)  | Suitable for geometric or spatial data, like in
    image processing or when clustering multi-dimensional numerical data  | It reflects
    the actual distance between points in a Euclidean space, making it intuitive and
    suitable for spatial datasets.  | It can be sensitive to the scale of the data.
    High dimensions can lead to the curse of dimensionality.  |'
  prefs: []
  type: TYPE_TB
- en: '| Dot product  | Efficient for high-volume, high-dimensional data, such as
    user preferences in recommendation systems  | Computationally efficient, especially
    for sparse vectors. It is good for cases where the magnitude of vectors matters.  |
    Interpretation is less intuitive than cosine similarity and can be sensitive to
    vector magnitudes.  |'
  prefs: []
  type: TYPE_TB
- en: '| Hamming distance  | Best for comparing binary or categorical data, such as
    genetic sequences or error detection in data transmission  | Simple and effective
    for datasets with discrete attributes  | It only applies to strings of equal length
    and doesn’t consider the magnitude of differences.  |'
  prefs: []
  type: TYPE_TB
- en: '| Manhattan (L1) distance  | Useful in grid-like pathfinding (e.g., urban road
    layouts) and in cases where differences in individual dimensions are important  |
    It is more sensitive to differences in individual dimensions than L2 distance;
    robust to outliers.  | It may not reflect the true distance in nongrid-like spaces
    or high-dimensional data.  |'
  prefs: []
  type: TYPE_TB
- en: 7.6 RAG challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Enterprises considering implementing RAG systems face several hurdles that need
    careful consideration. First and foremost, ensuring effective scalability with
    increasing data volumes is critical. As data grows, so does the complexity and
    size of the retrieval index. Managing this growth becomes challenging, necessitating
    more powerful computational resources. Specifically, dense retrieval systems,
    which are resource-intensive in terms of computation and storage, require careful
    balancing to ensure scalability. Additionally, maintaining an efficient and fast
    retrieval index becomes crucial as the volume of documents increases. Parallelizing
    requests, managing retry mechanisms, and deploying appropriate infrastructure
    are essential for achieving scalable RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring the quality and relevance of the indexed data is another significant
    concern. The utility of the RAG system is contingent upon the quality of its data;
    outdated or irrelevant information will lead to subpar responses —the principle
    of garbage-in-garbage-out still very much holds. This underscores the need for
    meticulous curation and regular updates of the document index to align with the
    enterprise's evolving requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Once deployed, RAG systems introduce an additional layer of complexity in integrating
    existing workflows, requiring ongoing maintenance to ensure consistent performance.
    RAG systems need to be seamlessly incorporated into an enterprise's existing technical
    landscape. This process often involves navigating complex data governance problems
    and ensuring system interoperability.
  prefs: []
  type: TYPE_NORMAL
- en: RAG systems involve complex encoding and querying of dense vectors in real-time,
    which can cause delays and affect response times. For applications that need fast
    answers, such latency may not meet user expectations for promptness. In addition,
    the complicated nature of RAG models makes identifying the cause of errors difficult.
    Finding and fixing errors effectively is important, whether they happen during
    retrieval or generation. Moreover, once deployed, RAG systems introduce extra
    complexity when integrating with existing workflows. Ensuring smooth integration
    into an enterprise’s technical landscape involves dealing with data governance
    problems and system compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: From a socio-technical perspective, ensuring that RAG systems are fair and unbiased
    is imperative. The risk of perpetuating existing biases from training data is
    real and can have far-reaching implications, requiring rigorous oversight and
    mitigation strategies. In addition, privacy and security are also key, especially
    if the indexed data includes confidential information, necessitating stringent
    compliance with data protection regulations.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking is a key problem that needs to be addressed by RAG implementations.
    Chunking is splitting a long text into smaller segments that an LLM can handle
    more easily. It can help lower the model’s computational and memory demands and
    enhance the quality and relevance of the output text. Chunking can also help the
    model concentrate on the most crucial parts of the text and avoid unimportant
    or repetitive parts. The difficulties with chunking are huge; we will discuss
    them in detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprises need to be aware of these challenges and weigh them against the
    benefits that RAG systems can bring, such as improved accuracy and contextual
    relevance in natural language processing tasks. When implementing an RAG-based
    solution, they must consider the trade-offs regarding costs, resources, and potential
    risks.
  prefs: []
  type: TYPE_NORMAL
- en: 7.7 Overcoming challenges for chunking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Today, enterprises face many challenges when implementing RAG at a production
    scale. As mentioned before, chunking is the process of dividing a long sequence
    of text into smaller, more manageable pieces. This is necessary for LLMs, which
    have limited processing capacity. RAG models typically use a chunking algorithm
    to divide the input text into smaller chunks, which the LLM processes. The LLM
    generates a response for each chunk, and the responses are then concatenated to
    form the final output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chunking, however, can be challenging for RAG models for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Chunks may not be aligned with the natural boundaries of the text. This can
    lead to the LLM generating grammatically incorrect or semantically incoherent
    responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chunks may vary in length and complexity. This can make it difficult for the
    LLM to generate responses consistent in quality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chunks may contain multiple intents. This can make it difficult for the LLM
    to identify the correct intent and generate the appropriate response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We start by understanding a strategy for chunking.
  prefs: []
  type: TYPE_NORMAL
- en: 7.7.1 Chunking strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One downside of search is that we can only put so much information in the context
    window. If we use OpenAI models as a measure, depending on the model, we can only
    use a finite set of information that can be passed, as shown in table 7.2\. In
    practical terms, this length is even shorter, given that we need space for the
    generation. This is where chunking becomes key.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.2 OpenAI model context length
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Open AI model | Maximum length (token size) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 Turbo  | 4K tokens; approx. 5 pages  |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4  | 8K tokens; approx. 10 pages  |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 32K  | 32K tokens; approx. 40 pages  |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 Turbo, GPT-4o  | 128K tokens; approx. 300 pages  |'
  prefs: []
  type: TYPE_TB
- en: 'Chunking means breaking down big documents or text passages into smaller, more
    digestible parts or chunks. This is done to make the retrieval process faster
    and better, especially when working with huge collections of texts; the main reason
    is also the context window constraint of the LLMs. Chunking is useful for RAG
    for a few reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Granularity*—When querying a large corpus for relevant information, searching
    at the granularity of smaller chunks might lead to more precise retrievals than
    searching entire documents. This can enhance the overall quality of the answers
    generated by RAG.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Efficiency*—Dealing with smaller chunks can make the retrieval process more
    efficient, especially when using dense retrievers that embed each chunk into a
    high-dimensional vector space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Flexibility*—Chunking allows the system to match varying lengths of relevant
    information to a given query, offering more flexibility in what is considered
    relevant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When considering the chunking strategy, we need to consider it holistically
    and see how the resulting searches capture the essence of the user’s query. If
    a chunk is too large or small, it could lead to inaccurate results. As a simple
    rule, if a chunk makes sense to us as humans without additional information, an
    LLM could also understand it.
  prefs: []
  type: TYPE_NORMAL
- en: For conversational use cases, as the turn-by-turn back and forth happens and
    the dialogue gets longer, it is important to evaluate how much of the previous
    conversation is needed for the next turn in the ongoing context. Adding bigger
    chunks could affect relevancy, and we also will get up to the limitations of the
    context windows of the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use a customer service chatbot scenario with a service provider—a mobile
    phone provider, as an example. The conversation might start with one topic, a
    question on activating a new phone, and can turn to other topics such as details
    about plans, add-on products, coverage details, billing, payment methods, etc.
    In this example, as the conversation turns from one topic to the other, in many
    cases, we don’t need all the previous history and dialogue, and it can be either
    discarded or trimmed. Of course, in some cases, we would only want the needed
    details for the context of the ongoing conversation.
  prefs: []
  type: TYPE_NORMAL
- en: The length of the information being chunked depends on the use case and the
    user’s expected behavior. For example, if we chunk a paragraph, we get a vector
    representation that captures more meaning from the content. This differs from
    sentence-level embedding, where the vector representation reflects more of the
    sentence’s meaning. This would lead to comparisons of other sentences and be more
    limited than the previous paragraph-based approach.
  prefs: []
  type: TYPE_NORMAL
- en: 7.7.2 Factors affecting chunking strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we get into the different chunking approaches, a few additional things
    to factor in from a chunking strategy perspective will help us balance higher
    accuracy with keeping within acceptable performance and cost thresholds:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Nature of the content*—The nature of the content that’s being indexed affects
    the chunking strategy. For example, shorter content, such as tweets, might require
    different chunking than longer content like books or reports. Shorter content
    may be chunked together, while longer content, such as documents, reports, and
    similar, may need to be broken down into smaller parts for efficient processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LLM and the associated embedding model*—The LLM and the associated embedding
    model can also affect the chunking strategy. For instance, some models may be
    more efficient at processing smaller chunks or, given their architecture, a chunk
    of a certain size, while others may handle larger chunks better. Knowing which
    LLM and associated embedding model we will use is important. For example, when
    using OpenAI, we should consider the `text-embedding-ada-002` embedding with a
    size of 256 or 512 tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Query complexity*—The length and complexity of the user query can affect the
    chunking approach. More complex queries might necessitate more intricate chunking
    strategies to match the query with the relevant data. It is important to remember
    that LLMs are not search engines and should not be used as such. The query complexity
    is multidimensional, both in terms of length and complexity, and might involve
    breaking the query into smaller subqueries that target different aspects of the
    original query before bringing everything back to the answer. For example, the
    query “What is the capital of the UK?” is very specific and straightforward. In
    contrast, the query “What are the economic implications of the rise of AI in various
    industries in the United States?” is multifaceted. It requires a deeper understanding
    of the technology (AI, in this example) and the geographic and industry details
    to infer the meaning of implications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Integration into the application*—Understanding how the output (query result)
    is used within the application can also influence the chunking strategy. For example,
    the limitations of the LLM and the context window might dictate how the data should
    be chunked to achieve the best results. This also factors in other data and metadata
    that the application might need.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Preprocessing data*—Preprocessing the data will help increase the quality
    of the generation and help us determine a possible good size. Preprocessing would
    include cleaning up extra noise or using other AI techniques to extract information,
    including data cleaning, data transformation from one format to another, feature
    normalization if required, tokenization, removing common stop words (such as “is,”
    “the,” “and”), and so forth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Evaluating and comparing different chunk sizes*—It’s crucial to evaluate and
    compare the effects of different chunk sizes on both the quality and performance
    of the process. This can be especially important in enterprise settings where
    varying chunk sizes might be used based on the nature of the content, and a balance
    may need to be struck between accuracy and performance. This evaluation would
    include both quality and performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One can take a few approaches when thinking about chunking information, as outlined
    in table 7.3\. It’s worth noting that the ideal chunking strategy might vary based
    on the corpus, the nature of the queries, and the application’s specific requirements.
    Experimentation might be needed to find the most effective approach for a particular
    RAG implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.3 Chunking approaches
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Chunking approach | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Fixed length  | Divide documents into chunks of a fixed number of words or
    tokens. This is straightforward but may sometimes split information that ideally
    should be kept together.  |'
  prefs: []
  type: TYPE_TB
- en: '| Sliding window  | Use a fixed-sized sliding window with or without overlapping
    data. This can ensure that important boundaries within the text are not missed,
    but it can also lead to redundancy if there’s significant overlap.  |'
  prefs: []
  type: TYPE_TB
- en: '| Punctuation based  | Divide the text based on punctuation, such as paragraphs
    or sections. This is less arbitrary than fixed-length chunking and often preserves
    the semantic integrity of the content. However, it can result in variable chunk
    sizes.  |'
  prefs: []
  type: TYPE_TB
- en: '| Topic or section breaks  | In structured documents such as Wikipedia articles,
    natural breaks like sections or subsections can be used to define chunks. This
    method ensures that the content within a chunk is semantically coherent.  |'
  prefs: []
  type: TYPE_TB
- en: '| Adaptive  | Use algorithms or models that adaptively determine the best way
    to chunk documents based on their content. This can be more complex but might
    yield semantically cohesive chunks.  |'
  prefs: []
  type: TYPE_TB
- en: Depending on the size and structure of the text, there are different ways to
    chunk it for RAG. Some of the common methods are
  prefs: []
  type: TYPE_NORMAL
- en: '*Sentence splitting*—As the name suggests, sentence boundaries are used to
    split the text, which is useful to ensure that each chunk contains whole sentences,
    preserving the context and meaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fixed-length splitting*—Here, text into is divided into fixed-length chunks.
    This can sometimes result in sentences being cut off in the middle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Token-based splitting*—Splitting the text based on a fixed number of tokens
    (e.g., words). This is more fine-grained than sentence splitting but can still
    result in sentences being cut off.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Semantic chunking*—Using natural language processing (NLP) tools to identify
    coherent segments in the text. For instance, splitting a text based on topics
    or paragraphs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hierarchical chunking*—Dividing text into hierarchical sections, such as chapters,
    sections, and subsections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To illustrate how different chunking approaches (fixed length and a semantic
    NLP) might affect the outcomes, we use an example of the UK Constitution from
    Wikipedia [2] as our input text. We can see the outcome in figure 7.7 when we
    apply a fixed-length chunking approach. The text is broken up into chunks of a
    fixed size, and in this simple example, we see that some information is cut off
    and some context is missing.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F07_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 Fixed-length chunking approach
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The text of the UK’s constitution appears in figure 7.8, using an NLP-based
    chunking approach. Because NLP comprehends the text and context, it splits it
    at the proper level with the correct tokens to maintain the sense and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F08_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 NLP-based chunking approach
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: These chunking strategies are useful and important for any provider or LLM we
    choose. In the following sections, we will see how to apply these strategies.
    We will begin with Sentence Splitter, a text splitter that splits the text based
    on a new line.
  prefs: []
  type: TYPE_NORMAL
- en: 7.7.3 Handling unknown complexities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes, we don’t know the complexities and length of the user queries in
    advance. In such cases, RAG implementations that can deal with unknown lengths
    and complexities of user queries can be challenging. Here are several strategies
    to determine the chunking approach in such scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Adaptive chunking*—Implement an adaptive chunking mechanism that automatically
    adjusts the size of chunks based on the query length and complexity. Smaller chunks
    can be used for shorter, simpler queries, while larger chunks might be needed
    to capture the necessary context for longer, more complex queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Preprocessing heuristics*—Use heuristics to analyze the query before chunking.
    These heuristics could estimate the complexity by looking at factors like the
    number of unique words, the presence of specialized terminology, or the syntactic
    structure. Based on this estimation, the chunking mechanism can adapt the size
    of the chunks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dynamic retrieval window*—Implement a dynamic retrieval window that expands
    or contracts based on the query. If the initial retrieval results are unsatisfactory,
    the window can be adjusted to include more or fewer documents or to change the
    granularity of the chunking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Overlapping chunks*—Create overlapping chunks to ensure that no critical information
    is lost at the boundaries of chunks. This approach can help maintain context when
    queries span multiple chunks. Depending on the use case, this can also overpower
    the other information, which isn’t something one should do by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ML approaches*—Use traditional ML models to predict the optimal chunk size
    based on the query characteristics. The model can be trained on a dataset of queries
    and optimal chunk sizes determined by performance on a validation set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fallback strategies*—Have fallback strategies in place for when the initial
    chunking does not yield good results. This can involve re-querying with different
    chunk sizes or using different chunking strategies if the initial response does
    not meet certain confidence thresholds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Feedback loop*—Implement a feedback loop where user interactions can help
    adjust the chunking. If a user indicates an unsatisfactory response, the system
    could automatically try different chunking strategies to improve the response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hybrid approaches*—Combine several of the preceding strategies to handle various
    queries. For example, adaptive chunking with a fallback strategy that continuously
    employs user feedback can improve the chunking mechanism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, the optimal solution would combine these strategies for a specific
    use case, and trial and error are needed to enhance performance. Moreover, making
    the system's components flexible can enable changes and upgrades to the chunking
    mechanism as more information is collected about the kinds of queries users enter.
  prefs: []
  type: TYPE_NORMAL
- en: 7.7.4 Chunking sentences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A sentence-based splitter is a method that splits the text into chunks based
    on sentence boundaries, such as periods, question marks, or exclamation points.
    This method can preserve the meaning and coherence of the text, as each chunk
    contains one or more complete sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 7.1 shows a simple implementation: an incoming text is split into sentences
    using regular expressions. The function splits the input text at every occurrence
    of a period (.), exclamation mark (!), or question mark (?). These characters
    are typically used to denote the end of a sentence in English. The result is a
    list of strings, each being a sentence from the original text.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.1 Split sentence function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Splits the sentence at every occurrence of these characters'
  prefs: []
  type: TYPE_NORMAL
- en: Another way to implement the same thing is using a sentence-based splitter,
    such as the `textwrap` library in Python. This function, `wrap()`, can split a
    string into a list of strings based on a given width. We can pass additional parameters
    to ensure that words don’t get split mid-sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.2 Splitting sentences using `textwrap`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Sets the maximum chunk size'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Splits the text into chunks'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to point out that both the `textwrap.wrap()`and `re.split()`
    functions serve different purposes, and their efficiency, speed, and accuracy
    depend on the specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: The original purpose of the `textwrap` library is for display purposes and to
    help format and wrap strings where we want control over the maximum line length.
    It’s efficient and fast for its intended use case. However, it’s not designed
    to split text into sentences, so if you use it for that purpose, it may not be
    accurate. For example, it could split a sentence in the middle if the sentence
    is longer than the specified width.
  prefs: []
  type: TYPE_NORMAL
- en: The `split()` function in regular expressions divides a string where the pattern
    matches. It can split a text into sentences well when used with a pattern such
    as '[.!?]'. It’s also quick and effective for what it does. However, it doesn’t
    consider line length or word boundaries, so if you need to limit the length of
    each chunk, `re.split()` would not be the best option.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of speed, both functions are quite fast and should perform well for
    most typical use cases. The speed could become a problem for very large strings,
    but in most cases, the difference would not be noticeable. Regarding accuracy,
    if we need to split the text into sentences, `re.split()` would be more accurate.
    If you need to wrap text to a certain line length, `textwrap.wrap()` would be
    more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Both functions are quite efficient, as they are part of Python’s standard library
    and are implemented in C. The efficiency would also depend on the size and complexity
    of the input string.
  prefs: []
  type: TYPE_NORMAL
- en: 7.7.5 Chunking using natural language processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As outlined in the earlier example, we can use a natural language processing
    (NLP) approach to split the text into chunks; these chunks can be based on linguistic
    features, such as clauses, phrases, or entities. Compared to the sentence splitter
    methods outlined earlier, this method can capture the meaning and context of the
    text, but it may require more computational resources and domain knowledge. Let’s
    see some examples using two of the most common NLP libraries available today—the
    Natural Language Toolkit (NLTK) and spaCy.
  prefs: []
  type: TYPE_NORMAL
- en: Using the NLTK
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The NLTK is one of the most well-known libraries for natural language processing
    and text analytics. It provides easy-to-use interfaces to many corpora and lexical
    resources. Furthermore, it includes a suite of text-processing libraries for classification,
    tokenization, stemming, tagging, parsing, and more. NTLK can be installed in many
    ways; in the case of conda, we can use the following: `conda install -c anaconda
    nltk`. For pip, we can use `pip install nltk`. Before we can use NLTK, we need
    to install the NLTK data, which can be done using the NLTK’s data downloader.
    A simple way to do this is to run a Python interpreter using administrator privileges
    and run the following commands. More details can be found at [https://www.nltk.org/data.html](https://www.nltk.org/data.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The following listing shows how to implement NLTK using the `sent_tokenize()`
    function to split the text into sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.3 Chunking text using NLP
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `sent_tokenize()` function uses an instance of `PunktSentenceTokenizer`,
    an unsupervised ML-based tokenizer that comes pretrained and is ready for sentence
    splitting. If the text is very large, you might consider using a generator expression
    instead of a list comprehension for memory efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: The next listing shows how the previous function could be rewritten as a generative
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 7.4 Chunking using NLP: Generative function'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The NLTK can be quite advantageous when it comes to chunking. It can detect
    sentence boundaries and split on those lines, and it is also effective for splitting
    texts into individual sentences, which can be useful for chunking large texts,
    while ensuring that sentences are not broken in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: From an enterprise perspective, it’s worth noting that while NLTK is comprehensive
    and suitable for research and educational purposes, it might not always be the
    most efficient in terms of speed. Other libraries such as spaCy might be more
    suitable for production-level applications, especially when processing vast amounts
    of text.
  prefs: []
  type: TYPE_NORMAL
- en: Using spaCy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: spaCy is a free, open source NLP library for Python that provides a wide range
    of NLP tasks, including sentence segmentation, named entity recognition, part-of-speech
    tagging, and dependency parsing. It is also good for chunking text and grouping
    words into meaningful units, such as noun phrases, verb phrases, and prepositional
    phrases.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy is a good choice for RAG implementations, as it is efficient and fast,
    especially when processing large amounts of text in real-time. It is accurate
    and reliable and can be customized depending on the specific needs. For example,
    spaCy can be used to chunk text using different linguistic theories, such as phrase
    structure grammar and dependency grammar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can use spaCy, we need to install the packages and download the appropriate
    pretrained language model for spaCy to use. If using conda, we can install spaCy
    using `conda install -c conda-forge spacy`. If we are using pip, then we can use
    the following: `pip` `install` `spacy`. In the example, we download the small
    general-purpose English language model called `en_core_web_sm` using the following
    command: `python -m spacy download en_core_web_sm`.'
  prefs: []
  type: TYPE_NORMAL
- en: spaCy offers additional models for different purposes and languages. In addition
    to the small English model, medium and large models are available—`en_core_web_md`
    and `en_core_web_lg`, respectively, for more comprehensive word vectors. The larger
    the model, the longer it will take to process. Choosing a model involves more
    than just the size; one must factor in accuracy, languages, and domain. More details
    on the pre-trained models can be found at [https://spacy.io/usage/models/](https://spacy.io/usage/models/).
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows how we can use spaCy for chunking. In this example,
    we factor in token counts for the LLMs context windows, and we also have the option
    to overlap text between chunks to allow for context continuity.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.5 Sentence chunking using spaCy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Loads the spaCy model'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Tokenizes the text into sentences using spaCy'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Tokenizes sentences and accumulates tokens'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Sliding window adjustment'
  prefs: []
  type: TYPE_NORMAL
- en: These techniques have different benefits and computing characteristics. Let’s
    try them all and compare their performance, duration, and effect. For example,
    we use Azure OpenAI and the FIFA 2023 Women’s World Cup as data [3]. This happened
    in 2023, and at the time of this publication, the LLMs lack this knowledge, as
    it is beyond the training cut-off.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we save the Wikipedia page for the FIFA 2023 Women’s World
    Cup as a raw text field. This file is not processed, and the resulting file is
    messy enough to reflect many real-world problems enterprises would face.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, as shown in listing 7.6, we run through the four different
    chunking techniques using the same file and outline the time it takes for each
    technique to execute, the number of chunks created, and the tokens used. We also
    use GPT3 to create a summary of the text read.
  prefs: []
  type: TYPE_NORMAL
- en: We begin by loading the stored text file named women_ fifa_worldcup_2023.txt.
    We apply four different chunking techniques separately and then use the same GPT
    mode to summarize them. We first chunk using a basic sentence chunking method
    and process those. Then, we process the same file using `textwrap`, NLTK, and
    spaCy. We record some simple telemetry at each run and show all of these at the
    end, along with the summary.
  prefs: []
  type: TYPE_NORMAL
- en: Note that several helper functions, such as `get_embedding()`, `count_tokens()`,
    and so forth, have been used earlier in the book—we do not call those out again
    for brevity. The complete code samples are in the GitHub code repository accompanying
    the book ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.6 Sentence-chunking comparison
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Empty list to store the summaries'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Completion to generate a summary for the chunk'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Rate limiting'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Counts tokens in the sentence'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 File that we want to chunk'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Chunks text using textwrap'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Chunks text using NLTK'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Chunks text using spaCy'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Generates summaries for each chunk using OpenAI API'
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.4 shows the output when we run this, with the time duration in seconds.
    As expected, the time it takes to process the same input text differs greatly
    depending on the technique used.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.4 Sentence-chunking comparison
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Chunking method | Embeddings count | Tokens count | Execution time (secs)
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Simple  | 120  | 5815  | 16.96  |'
  prefs: []
  type: TYPE_TB
- en: '| Using `textwrap`  | 12  | 5933  | 1.66  |'
  prefs: []
  type: TYPE_TB
- en: '| Using `NLTK`  | 105  | 5909  | 13.31  |'
  prefs: []
  type: TYPE_TB
- en: '| Using `spaCy`  | 4  | 5876  | 5.8  |'
  prefs: []
  type: TYPE_TB
- en: 'The following is the summary generated by the LLM using the spaCy chunks; these
    summaries are concise and informative, which is what we intended:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This example shows that the `textwrap` approach is the quickest, taking 1.66
    seconds; this does not imply that the `textwrap` approach is always the most suitable
    and the one we should adopt. We have to evaluate this for each situation, depending
    on the kind of information and the use case involved. Let’s explore the decision
    factors required to select the best strategy for chunking.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right strategy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Whether to use an NLP-based chunking strategy or a fixed-length chunking approach
    depends on the specific requirements and constraints of the task at hand. Table
    7.5 outlines some of the decision factors.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.5 Chunking decision factors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Decision factor | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Task requirements  | If the task requires understanding the nuances of language,
    such as answering questions that depend on context or generating coherent text,
    NLP-based chunking is preferable.  |'
  prefs: []
  type: TYPE_TB
- en: '| Performance  | If maintaining the context isn’t critical, and there are performance
    constraints, fixed-length chunking could be the better choice.  |'
  prefs: []
  type: TYPE_TB
- en: '| Resource availability  | Fixed-length chunking is less resource-intensive
    for projects with limited computational resources and is easier to scale.  |'
  prefs: []
  type: TYPE_TB
- en: '| Data characteristics  | NLP-based chunking can use those boundaries for text
    with clear linguistic demarcations (such as well-structured documents). In contrast,
    fixed-length chunking might be more practical if the text is poorly structured
    or if the boundaries are unclear.  |'
  prefs: []
  type: TYPE_TB
- en: One might begin with a fixed-length method because it is easy and then switch
    to an NLP-based method when more complexity is required. Some advanced systems
    might even employ both, using fixed-length chunking to deal with large amounts
    of text quickly and then using NLP-based chunking for the smaller, more controllable
    chunks to improve the context and meaning. Let’s change topics and see how we
    can chunk other documents, such as PDFs.
  prefs: []
  type: TYPE_NORMAL
- en: 7.8 Chunking PDFs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At a high level, chunking PDFs is quite similar to chunking sentences. There
    are different options for PDFs that are not too complex and have basic tables
    or images. A simple method to start is to use the PyPDF2 library. PyPDF2 is an
    open source Python PDF library that can perform various operations on PDF pages,
    such as splitting, merging, cropping, and transforming. It can also extract text,
    custom data, passwords, and metadata from PDFs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 7.7 shows how to use it. We can install PyPDF2 using the following
    command in conda: `conda` `install` `-c` `conda-forge` `pypdf2`, or if using pip,
    then `pip install pypdf2`. This output is text that can be chunked and processed
    like any other text previously discussed. This library doesn’t handle images;
    if any images are in PDF, those will be ignored. Note that the following listing
    only shows the relevant section for brevity; the book’s GitHub repository has
    the complete code.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.7 Extracting text from PDF
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Handling tables and images in PDF
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While handling text in the last example seems quite straightforward, PDFs can
    add a lot of complexity. The accuracy of text extraction depends on the PDF itself,
    given that not all PDFs encode text in a manner that is easily extractable. The
    following listing shows one example of how to process images and tables for chunking
    from PDFs, but overall, this will be challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.8 Example of how to extract tables and images
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates an empty string to store the text'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Exports the image as a BMP file'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Opens the BMP file with PIL'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Converts the image to PNG and saves'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Creates an ImageWriter object to save the images'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Gets the first page from the PDF file'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Reads the tables'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Saves each table into a separate file'
  prefs: []
  type: TYPE_NORMAL
- en: Images and tables are difficult to parse and handle in PDFs, especially consistently
    and predictably. One way enterprises can solve this problem is by daisy-chaining
    other ML models instead of just trying to parse documents. Microsoft’s Azure Document
    Intelligence is a service that allows enterprises to implement this.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Document Intelligence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Azure AI Document Intelligence is a cloud service that employs advanced ML
    models to auto-extract text, key values, tables, and structures from documents,
    converting them into actionable data. It offers three types of ML models for document
    analysis: prebuilt models for common scenarios (e.g., IDs, receipts), custom models
    trained on your data, and document analysis models for structured content extraction.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike many Python PDF-handling packages, it supports multiple document formats,
    handles complex layouts, handwritten text, and objects, and allows for custom
    ML model creation. Integration is simple via a REST API to extract data from documents
    such as PDFs and use an RAG pattern for summarization or answer generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 7.9 shows how to analyze a document using a prebuilt layout model.
    We start the analysis of a sample document using the prebuilt layout model and
    iterate the result. It detects the text and tables for each page, including understanding
    rows and columns. Please note that Document Intelligence isn''t available on conda
    yet, so we''ll use pip to install it: `pip install azure-ai-formrecognizer`.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.9 Azure AI Document Intelligence prebuilt layout operations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Calls the API to analyze a PDF using a pre-build layout'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Gets the results from the analysis'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Iterates through all the pages in the PDF'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Extracts the text on each line on the page'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Examines whether there is a Selection mark'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Parses tables found in the PDF'
  prefs: []
  type: TYPE_NORMAL
- en: More details on Azure Document Intelligence can be found at [https://mng.bz/6YNA](https://mng.bz/6YNA).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs don’t have any up-to-date information past their training cut-off, and
    they don’t know private and proprietary information. Retrieval-augmented generation
    (RAG) is the technique that helps address these limitations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG is a powerful technique that provides up-to-date and grounded information,
    leading to improved LLM responses. It also helps ground data and can improve the
    generation of LLMs regarding quality, diversity, and customization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector searches powered by vector indexes and databases are pivotal in making
    the retriever component of RAG implementations efficient and scalable. They enable
    real-time, large-scale semantic search, essential for applications that require
    rapid access to vast amounts of information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG must deal with various challenges, with chunking being one of the most critical.
    Due to LLM content window limitations, we need to chunk a large corpus of data
    and employ various techniques for splitting information—fixed length, sliding
    window, punctuation based, sections based, or adaptive. Each has advantages and
    challenges and needs to be considered in the context of the use case, shape, and
    data type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding and parsing PDFs into chunks is difficult, especially if they
    contain images and tables. Daisy-chaining other ML models, such as Azure Document
    Intelligence, can help simplify this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combined with prompt engineering, RAG helps address requirements such as dynamic
    information access, cost efficiency, grounding, citation, scalability, and customization
    in various enterprise scenarios and contexts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
