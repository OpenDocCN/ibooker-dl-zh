- en: '7 Retrieval-augmented generation: The secret weapon'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 检索增强生成：秘密武器
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Concepts of retrieval-augmented generation
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）的概念
- en: Benefits of the RAG architecture in conjunction with large language models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合大型语言模型的 RAG 架构的优势
- en: Understanding the role of vector databases and indexes in implementing RAG
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解向量数据库和索引在实现 RAG 中的作用
- en: Basics of vector search and understanding the distance functions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量搜索的基础和距离函数的理解
- en: Challenges in RAG implementation and potential solutions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG 实现中的挑战和潜在解决方案
- en: Different methods of chunking text for RAG
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG 中文本分块的不同方法
- en: As we have seen, large language models (LLMs) are very powerful and help us
    achieve things that were not possible until very recently. Interestingly, LLMs
    capture the world’s knowledge and are available to anyone at the end of an API,
    anywhere in the world.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，大型语言模型（LLMs）非常强大，帮助我们实现直到最近都不可能完成的事情。有趣的是，LLMs 捕获了世界的知识，并且任何人都可以通过 API
    在世界任何地方访问。
- en: 'However, LLMs have a knowledge constraint: their understanding and knowledge
    extend up to their last training cut-off; after that date, they do not have any
    new information. Consequently, LLMs cannot utilize the latest information. In
    addition, the training corpus of LLMs does not contain any private nonpublic knowledge.
    Therefore, LLMs cannot operate and answer specific and proprietary questions to
    enterprises.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LLMs 有一个知识限制：它们的理解和知识延伸到它们最后一次训练截止日期；在那之后，它们没有新的信息。因此，LLMs 无法利用最新信息。此外，LLMs
    的训练语料库不包含任何私有或非公开知识。因此，LLMs 无法操作并回答企业特定的专有问题。
- en: One practical way to solve this problem is by using a pattern called retrieval-augmented
    generation (RAG). This chapter will explore using RAG to enhance LLMs with your
    data. You will learn what RAG is, why it is useful for enterprise applications,
    and how to implement it using vector databases and indexes. Finally, the chapter
    will discuss some chunking strategies to optimize the relevance and efficiency
    of RAG.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的实际方法之一是使用一个名为检索增强生成（RAG）的模式。本章将探讨如何使用 RAG 来增强 LLMs 并使用您的数据。您将了解 RAG 是什么，为什么它对企业管理应用有用，以及如何使用向量数据库和索引来实现它。最后，本章将讨论一些分块策略，以优化
    RAG 的相关性和效率。
- en: In this chapter, we will start by understanding RAG. In the next chapter, we
    will build on that by combining all the concepts for an end-to-end sample.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先了解 RAG。在下一章中，我们将在此基础上结合所有概念，构建一个端到端的示例。
- en: 7.1 What is RAG?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 什么是 RAG？
- en: RAG is a method that combines additional data with a language model’s input
    to improve its output without altering the initial prompt. This supplemental data
    can come from an organization’s database or an external, updated source. The language
    model then processes the merged information to include factual data from the knowledge
    base in its response. This technique is particularly useful when the latest data
    and its integration into your information are required.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 是一种方法，它将额外数据与语言模型的输入相结合，以改进其输出，而不改变初始提示。这些补充数据可以来自组织的数据库或外部更新源。然后，语言模型处理合并的信息，将其知识库中的事实数据包含在其响应中。这种技术在需要最新数据和将其整合到您的信息中时特别有用。
- en: In technical terms, RAG merges a pretrained language model and an external knowledge
    index to enhance language generation. Facebook AI Research first introduced RAG
    in a study titled “Retrieval-Augmented Generation for Knowledge-Intensive NLP
    Tasks” [1]. It demonstrated that RAG models can achieve state-of-the-art results
    on various knowledge-intensive tasks in natural language processing (NLP), such
    as open-domain question answering, fact verification, and natural language inference.
    It also proved that RAG models can generate more precise, diverse, and factual
    language than a leading language model that doesn’t use additional data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术术语上，RAG 将预训练的语言模型和外部知识索引合并，以增强语言生成。Facebook AI Research 在一篇题为“用于知识密集型 NLP
    任务的检索增强生成”的研究中首次介绍了 RAG [1]。它证明了 RAG 模型可以在自然语言处理（NLP）的各种知识密集型任务上实现最先进的成果，例如开放域问答、事实验证和自然语言推理。它还证明，与不使用额外数据的领先语言模型相比，RAG
    模型可以生成更精确、多样和事实的语言。
- en: The RAG model combines the powers of a dense passage retriever and a sequence-to-sequence
    model to generate informative answers based on a large corpus of text. It was
    designed to improve question-answering systems, fact verification, and question-generation
    tasks by integrating information retrieval with generative language models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: RAG模型结合了密集段落检索器和序列到序列模型的力量，基于大量文本生成有信息量的答案。它旨在通过将信息检索与生成语言模型相结合，改善问答系统、事实核查和问题生成任务。
- en: 'Figure 7.1 shows an overview of the RAG pattern and the overall approach. At
    a high level, there are two components: the retriever and the generator. As the
    name suggests, the retriever is responsible for retrieving the information, and
    the generator is the LLM, used to generate the text.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1展示了RAG模式的整体方法和概述。从高层次来看，有两个组件：检索器和生成器。正如其名所示，检索器负责检索信息，而生成器是LLM，用于生成文本。
- en: '![figure](../Images/CH07_F01_Bahree.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F01_Bahree.png)'
- en: Figure 7.1 RAG architecture overview
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1 RAG架构概述
- en: Foundational models, notably LLMs such as the OpenAI GPT series, possess immense
    potential but do have drawbacks. These models, while powerful, suffer from a static
    knowledge base, meaning they are unaware of events or developments posttraining,
    causing them to become outdated over time. They are also heavily influenced by
    their training data, and any bias, misinformation, or imbalance in this data can
    taint the model’s output. Furthermore, LLMs lack a genuine understanding of the
    content, often generating text based solely on patterns observed during training
    without comprehension. This can be problematic in corporate scenarios with specific
    policies and rules. Finally, these models can create plausible yet factually incorrect
    information, which can propagate misinformation without a reliable verification
    method.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型，特别是像OpenAI GPT系列这样的LLM，具有巨大的潜力，但也存在一些缺点。这些模型虽然强大，但知识库是静态的，这意味着它们在训练后不知道事件或发展，随着时间的推移会变得过时。它们也受到训练数据的影响很大，任何数据中的偏差、错误信息或不平衡都会污染模型的输出。此外，LLM缺乏对内容的真正理解，通常仅基于训练期间观察到的模式生成文本，而没有理解。这在具有特定政策和规则的企业场景中可能会出现问题。最后，这些模型可以创建看似合理但实际上错误的信息，如果没有可靠的验证方法，可能会传播错误信息。
- en: RAG helps improve the quality of responses by drawing on these external sources
    of knowledge to supplement the LLM’s internal information. This is especially
    helpful in addressing the static knowledge of LLMs where they cannot provide accurate
    generations for events or facts that happened after their training cutoff dates.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: RAG通过利用这些外部知识来源来补充LLM的内部信息，有助于提高响应质量。这在解决LLM的静态知识方面特别有帮助，因为它们无法为其训练截止日期之后发生的事件或事实提供准确的生成。
- en: RAG is an essential component of working with LLMs, along with prompt engineering.
    By accessing a broader variety of information, RAG can produce more accurate and
    informative answers. It ensures that the model relies on the most up-to-date,
    dependable facts and that users can see its sources, ensuring that its statements
    can be verified for correctness and ultimately trusted.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是与LLM一起工作的一个重要组件，与提示工程一样。通过访问更广泛的信息种类，RAG可以产生更准确和更有信息量的答案。它确保模型依赖于最新、最可靠的事实，并且用户可以看到其来源，确保其陈述可以经过验证以确保正确性，并最终值得信赖。
- en: 7.2 RAG benefits
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 RAG的优势
- en: While RAG is still in its early stages of development, it holds the potential
    to transform the landscape of text generation models. RAG can be harnessed to
    produce more comprehensive, varied, and factual text generation models for many
    applications. This section delves into the myriad of benefits that enterprises
    can gain.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RAG仍处于发展的早期阶段，但它有潜力改变文本生成模型的格局。RAG可以被利用来为许多应用产生更全面、多样和事实性的文本生成模型。本节深入探讨了企业可以获得的众多好处。
- en: RAG’s ability to draw data from external resources in real-time is a game changer
    for sectors that require up-to-the-minute data, such as finance, healthcare, or
    news. Whether tracking market dynamics, updating healthcare records, or breaking
    news, RAG guarantees the inclusion of the latest information. This ensures that
    the output is consistently relevant and current.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: RAG能够实时从外部资源中提取数据，对于需要最新数据的行业，如金融、医疗保健或新闻业，是一个变革性的突破。无论是跟踪市场动态、更新医疗记录还是报道新闻，RAG都能保证包含最新信息。这确保了输出始终相关且最新。
- en: Compared to traditional ML techniques, RAG offers a cost-effective alternative
    for businesses. Traditional techniques may necessitate retraining a model each
    time new data is added. However, with RAG, businesses only need to update the
    external dataset, saving time and costs related to model training and data processing.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的机器学习技术相比，RAG为企业提供了一种成本效益高的替代方案。传统的技术可能需要在添加新数据时重新训练模型。然而，使用RAG，企业只需更新外部数据集，从而节省了与模型训练和数据处理相关的宝贵时间和成本。
- en: RAG proves particularly useful when responses need to cite data or display source
    references. It can anchor the generated data in the source material and even provide
    citations. This is of immense value in academic, legal, or professional scenarios
    where precise sourcing of information is required.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要引用数据或显示来源引用时，RAG特别有用。它可以将生成数据锚定在原始材料中，甚至提供引用。这在学术、法律或专业场景中非常有价值，在这些场景中，精确的信息来源是必需的。
- en: RAG’s versatility extends to the types of data it can process, accommodating
    structured and unstructured data in various formats. This adaptability allows
    RAG to be utilized in diverse applications, from analyzing intricate datasets
    to processing and generating multimedia content.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: RAG的适用性扩展到它可以处理的数据类型，它能够处理各种格式的结构化和非结构化数据。这种适应性使得RAG可以在各种应用中使用，从分析复杂的数据集到处理和生成多媒体内容。
- en: Implementing RAG enhances customer interactions and facilitates improved decision-making.
    In customer service or chatbot applications, RAG can retrieve detailed information
    from databases or FAQs, which results in more accurate and constructive responses.
    Furthermore, RAG can combine insights from large datasets with language model
    generation in decision support systems to offer comprehensive and informed recommendations.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 实施RAG（检索与评估）技术可以增强客户互动并促进决策的改善。在客户服务或聊天机器人应用中，RAG可以从数据库或常见问题解答（FAQs）中检索详细信息，从而产生更准确和建设性的回应。此外，RAG可以将来自大型数据集的见解与语言模型生成相结合，在决策支持系统中提供全面和有信息量的建议。
- en: RAG’s scalability and performance are exceptional, enabling businesses to utilize
    vast external datasets without overburdening the language model. This allows generating
    outputs based on a wide array of information without compromising the model’s
    performance or efficiency.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: RAG的可扩展性和性能非常出色，使企业能够在不过度负担语言模型的情况下利用大量外部数据集。这允许基于广泛的信息生成输出，而不会损害模型的性能或效率。
- en: RAG also allows customizing of external datasets based on a business domain.
    For instance, a pharmaceutical company could maintain a dataset solely for new
    drug research, allowing RAG to offer domain-specific responses. From a research
    and development perspective, sectors such as biotechnology or technology can greatly
    benefit from RAG’s ability to retrieve relevant literature or data insights, speeding
    up the innovation process.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: RAG还允许根据业务领域定制外部数据集。例如，一家制药公司可以维护一个仅用于新药研究的独立数据集，从而使RAG能够提供特定领域的回应。从研发角度来看，生物技术或技术等行业可以从RAG检索相关文献或数据洞察的能力中大大受益，从而加快创新过程。
- en: RAG offers a dynamic, efficient, and versatile solution for integrating external
    datasets into language models. This feature results in more accurate, relevant,
    and current information in automated systems, enhancing efficiency, customer satisfaction,
    and decision-making.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: RAG为将外部数据集集成到语言模型中提供了一个动态、高效和灵活的解决方案。这一特性使得自动化系统中的信息更加准确、相关和及时，从而提高了效率、客户满意度和决策质量。
- en: What is data grounding?
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 什么是数据锚定？
- en: Grounding your data means connecting LLMs with external information sources.
    Grounding can be done using various methods; however, RAG is a common one. Usually,
    these external data sources are chosen based on the use case needs, enhancing
    the quality and dependability of the generated output. Grounding can make the
    generated output better by giving LLMs information that is use-case specific,
    relevant, and not included in the LLM’s training data. This way, the LLMs can
    use the data from external sources as context and generate more precise and relevant
    answers for the user.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据锚定意味着将大型语言模型（LLMs）与外部信息源连接起来。数据锚定可以通过各种方法完成；然而，RAG是一种常见的方法。通常，这些外部数据源是根据用例需求选择的，这提高了生成输出的质量和可靠性。通过为LLMs提供特定用例、相关且不在LLMs训练数据中的信息，数据锚定可以使生成的输出更好。这样，LLMs可以使用来自外部源的数据作为上下文，并为用户提供更精确和相关的答案。
- en: Some of the benefits of grounding are
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据锚定的某些好处包括
- en: It can help the LLMs produce more factual and reliable output, as it reduces
    the risk of hallucination, which is when the LLMs invent false or misleading information
    in their output.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以帮助LLMs产生更事实性和可靠的输出，因为它减少了幻觉的风险，即LLMs在输出中创造虚假或误导性信息的情况。
- en: It can help the LLMs produce more diverse and representative output, allowing
    them to access information from various sources and perspectives and avoid biases
    or errors in their internal knowledge.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以帮助大型语言模型（LLMs）产生更多样化和具有代表性的输出，使它们能够从各种来源和角度获取信息，并避免在内部知识中产生偏见或错误。
- en: It can help the LLMs produce more customized and personalized output, enabling
    them to adapt to the user’s preferences, needs, and goals and provide tailored
    solutions or suggestions.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以帮助LLMs产生更定制化和个性化的输出，使它们能够适应用户的偏好、需求和目标，并提供定制化的解决方案或建议。
- en: RAG models can utilize the vast amount of information stored in text corpora
    to enrich their outputs with relevant facts and details. They can also handle
    open-domain questions and tasks that require reasoning and inference beyond the
    scope of LLMs. Let’s explore the RAG architecture in more detail.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: RAG模型可以利用存储在文本语料库中的大量信息，通过相关的事实和细节来丰富其输出。它们还可以处理开放域问题以及需要推理和推断的任务，这些任务超出了LLMs的范围。让我们更详细地探讨RAG架构。
- en: 7.3 RAG architecture
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 RAG架构
- en: 'It was outlined earlier that the RAG architecture consists of two main components:
    the retriever and the LLM. The retriever extracts data from different enterprise
    systems, as illustrated in figure 7.2 [1]. These components can be adapted and
    adjusted based on the application and task at hand, and together, they give the
    RAG model a lot of flexibility and strength.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 之前概述了RAG架构由两个主要组件组成：检索器和LLM。检索器从不同的企业系统中提取数据，如图7.2[1]所示。这些组件可以根据应用和任务进行调整和适应，并且它们共同为RAG模型提供了很大的灵活性和力量。
- en: The retriever can access information from private knowledge sources and search
    engines. This is the mechanism behind Bing Chat, which helps provide more current
    information. This retriever does more than search—it filters out only the relevant
    information, which becomes the context for the generative model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器可以访问私有知识源和搜索引擎中的信息。这是Bing Chat背后的机制，有助于提供更及时的信息。这个检索器不仅仅进行搜索——它只过滤出相关信息，这些信息成为生成模型的上下文。
- en: '![figure](../Images/CH07_F02_Bahree.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F02_Bahree.png)'
- en: Figure 7.2 Overview of RAG for knowledge-intensive NLP tasks [1]
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2 RAG在知识密集型NLP任务中的概述[1]
- en: The RAG pattern combines information retrieval and text generation to enhance
    language model outputs. The query encoder initially encodes an input question
    or statement into a vector. This vector, `q(x)`, is then utilized by a nonparametric
    retriever to sift through a precompiled document index, seeking documents relevant
    to the query.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: RAG模式结合信息检索和文本生成，以增强语言模型的输出。查询编码器最初将输入问题或陈述编码成一个向量。这个向量`q(x)`随后被非参数检索器用来筛选预先编译的文档索引，寻找与查询相关的文档。
- en: The retriever employs maximum inner product search (MIPS), which identifies
    documents with the highest similarity to the query vector. These documents are
    pre-encoded into vectors, represented as `d(z)`, in the document index.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器采用最大内积搜索（MIPS），它识别与查询向量最相似的文档。这些文档被预先编码成向量，在文档索引中表示为`d(z)`。
- en: The generator (i.e., the LLM) utilizes the information from the retrieved documents
    to produce human-like text. This architecture component is responsible for answering
    questions, verifying facts, or generating new questions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器（即LLM）利用检索到的文档中的信息来生成类似人类的文本。这个架构组件负责回答问题、验证事实或生成新问题。
- en: The final process is marginalization, where instead of relying on a single document
    to generate a response, the RAG model considers all pertinent documents. It calculates
    the overall probability of each possible answer by summing up the probabilities
    based on each retrieved document, which ensures a more comprehensive and contextual
    awareness by integrating a wide array of retrieved information into the final
    text generation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最终过程是边缘化，在这个过程中，RAG模型不是依赖于单一文档来生成响应，而是考虑所有相关的文档。它通过将每个检索到的文档的概率相加来计算每个可能答案的整体概率，通过将广泛检索到的信息整合到最终文本生成中，确保了更全面和情境化的意识。
- en: The other key component is the LLM, which takes the context from the retrieval
    model and generates a natural language output. The generative model also provides
    feedback to the retrieval model to improve its accuracy over time. This is done
    using prompt engineering, as we saw in the previous chapter.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键组件是LLM（大型语言模型），它从检索模型中获取上下文并生成自然语言输出。生成模型还会向检索模型提供反馈，以随着时间的推移提高其准确性。这通过提示工程（prompt
    engineering）来实现，正如我们在上一章所看到的。
- en: 7.4 Retriever system
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 检索系统
- en: The retriever is essentially the component that searches various knowledge sources,
    as shown in figure 7.2\. Its main purpose is to search through the corpus of information
    and find the relevant information that can be used. The retrieved information
    is then provided to the generator model, which uses it to generate its output.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器本质上是在图7.2中所示的各种知识源中进行搜索的组件。其主要目的是在信息库中搜索，找到可以使用的相关信息。检索到的信息随后提供给生成模型，该模型使用这些信息来生成其输出。
- en: 'Two main types of retriever systems are used in RAG: sparse and dense. Sparse
    retrievers are traditional retrieval systems that use traditional search techniques,
    such as term frequency-inverse document frequency (TF-IDF), to match queries to
    documents. Dense retrievers are newer retrieval systems that use machine learning
    to encode queries and documents into vectors and then match queries to documents
    based on the similarity of their vectors.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG中使用了两种主要的检索系统：稀疏和密集。稀疏检索器是传统的检索系统，使用传统的搜索技术，如词频-逆文档频率（TF-IDF），将查询与文档相匹配。密集检索器是较新的检索系统，使用机器学习将查询和文档编码为向量，然后根据它们的向量相似度将查询与文档相匹配。
- en: Choosing the right type of retrieval system in a RAG architecture (sparse or
    dense) is critical because it fundamentally affects the model’s performance and
    applicability. Sparse retrievers, such as those using TF-IDF, are fast and efficient,
    using inverted indexes to match queries with documents based on keyword overlap.
    This makes them suitable for large-scale, keyword-dependent search tasks with
    limited computational resources. However, they might struggle with the subtleties
    of language, such as synonyms and nuanced phrasing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG架构（检索增强生成）中选择正确的检索系统类型（稀疏或密集）至关重要，因为它从根本上影响模型的表现力和适用性。稀疏检索器，如使用TF-IDF的检索器，速度快且效率高，使用倒排索引根据关键词重叠匹配查询与文档。这使得它们适合于计算资源有限的大规模、关键词依赖型搜索任务。然而，它们可能在语言的细微之处，如同义词和细微措辞上遇到困难。
- en: In contrast, dense retrievers utilize machine learning techniques to encode
    queries and documents into vectors, capturing deeper semantic relationships beyond
    mere keyword matching. This allows them to handle more complex queries and understand
    context better, which is particularly beneficial for queries with ambiguous or
    specialized language. While dense retrievers often yield more relevant and contextually
    appropriate documents, they are more computationally intensive and require substantial
    amounts of training data, making them resource-heavy both in the training phase
    and during inference.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，密集检索器利用机器学习技术将查询和文档编码为向量，捕捉到超越简单关键词匹配的更深层次的语义关系。这使得它们能够处理更复杂的查询并更好地理解上下文，这对于具有模糊或专业语言的查询尤其有益。虽然密集检索器通常会产生更相关和上下文适当的文档，但它们计算密集，需要大量的训练数据，这使得它们在训练阶段和推理阶段都资源密集。
- en: The choice between sparse and dense retrievers should be guided by the task’s
    specific needs, considering the nature of the queries, domain specificity, resource
    availability, and the necessity of nuanced language understanding.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择稀疏检索器或密集检索器时，应考虑任务的具体需求，包括查询的性质、领域特定性、资源可用性和对细微语言理解的需求。
- en: The choice of retriever affects the balance between computational efficiency
    and depth of understanding. Despite their computational costs, dense retrievers
    are often preferred for tasks requiring a nuanced understanding of language. However,
    sparse retrievers may still be viable for applications where speed and efficiency
    are paramount, or where queries are expected to match document text closely. The
    best retriever for a given application will depend on its specific requirements
    and the resources available for implementing and maintaining the system.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器的选择会影响计算效率和深度理解之间的平衡。尽管它们有计算成本，但密集检索器通常被用于需要细微理解语言的任务。然而，稀疏检索器在速度和效率至关重要的应用中或预期查询与文档文本紧密匹配的情况下可能仍然可行。最适合特定应用的最佳检索器将取决于其具体要求和实施和维护系统的资源。
- en: What are BM25, TF-IDF, and DPR?
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: BM25、TF-IDF 和 DPR 是什么？
- en: BM25 is a ranking function used by search engines to estimate the relevance
    of documents to a given search query. It is one of the most widely used ranking
    functions in information retrieval. BM25 considers many factors, including the
    term frequency (TF) of the query terms in the document, the inverse document frequency
    (IDF) of the query terms, and the length of the document.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: BM25 是搜索引擎用来估计文档与给定搜索查询的相关性的排名函数。它是信息检索中最广泛使用的排名函数之一。BM25 考虑了许多因素，包括文档中查询术语的词频（TF）、查询术语的逆文档频率（IDF）以及文档的长度。
- en: TF-IDF is a statistical measure used to evaluate how important a word is to
    a document in a collection of documents. The TF-IDF value increases proportionally
    to the number of times a word appears in a document. It decreases proportionally
    to the number of documents in the collection that contain the word. TF-IDF is
    often used in information retrieval and text mining to rank documents based on
    their relevance to a given query.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 是一种用于评估一个词在文档集合中对文档重要性的统计度量。TF-IDF 值与词在文档中出现的次数成正比。它与包含该词的文档集合中的文档数量成反比。TF-IDF
    通常用于信息检索和文本挖掘，根据文档与给定查询的相关性对文档进行排名。
- en: DPR is a neural network model that retrieves relevant passages from a large
    text corpus. It is trained on a massive dataset of text and code and learns to
    embed passages and queries into a dense vector space. DPR can retrieve passages
    semantically, similar to the query, by calculating the cosine similarity between
    the passage and query vectors.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: DPR 是一种神经网络模型，用于从大型文本语料库中检索相关段落。它在大量的文本和代码数据集上训练，并学习将段落和查询嵌入到密集的向量空间中。DPR 通过计算段落和查询向量之间的余弦相似度，可以语义上检索与查询相似的段落。
- en: BM25 and TF-IDF are statistical measures of a document’s relevance to a given
    query. However, BM25 considers additional factors, such as the length of the document
    and the saturation of term frequency. DPR can be used to improve the performance
    of BM25 and TF-IDF ranking functions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: BM25 和 TF-IDF 是文档与给定查询相关性的统计度量。然而，BM25 考虑了额外的因素，例如文档的长度和词频的饱和度。DPR 可以用来提高 BM25
    和 TF-IDF 排名函数的性能。
- en: At a high level, we need to follow the process outlined in figure 7.3 to harness
    the power of LLMs on our data. The source pulled by the retriever would need to
    be split into smaller sizes. This is required to make the information more manageable
    and conform to the context windows of the LLMs. Next, we must create embeddings
    of these smaller chunks and link them to the source as metadata. Finally, these
    embeddings and associated metadata should be persisted in a data store.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，我们需要遵循图 7.3 中概述的过程来利用 LLMs 在我们的数据上的力量。检索器抽取的源数据需要分成更小的块。这是为了使信息更易于管理并符合
    LLMs 的上下文窗口。接下来，我们必须创建这些小块的嵌入并将它们作为元数据链接到源数据。最后，这些嵌入和相关元数据应该持久化存储在数据存储中。
- en: '![figure](../Images/CH07_F03_Bahree.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F03_Bahree.png)'
- en: Figure 7.3 Custom data on LLMs
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.3 LLMs 的自定义数据
- en: 'For RAG to be efficient and scalable, the retriever component must quickly
    fetch the most relevant documents from potentially billions of candidates. We
    need two components to help address this challenge: a vector database and an index.
    A vector database is a system that stores and provides access to structured or
    unstructured data (e.g., text or images) alongside its vector embeddings, which
    are the data’s numerical representation. A vector index is a data structure that
    enables efficient and fast lookup of nearest neighbors in the high-dimensional
    vector space.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使RAG高效和可扩展，检索组件必须快速从数十亿个潜在候选中获取最相关的文档。我们需要两个组件来帮助解决这个挑战：一个向量数据库和一个索引。向量数据库是一个存储并提供对结构化或非结构化数据（例如文本或图像）及其向量嵌入（数据的数值表示）访问的系统。向量索引是一种数据结构，它能够在高维向量空间中实现高效和快速的近邻查找。
- en: Without efficient vector databases and indexes, the retrieval step would become
    a bottleneck, making the entire RAG system slow and impractical. Using these tools,
    relevant documents can be retrieved in real time, allowing the generator component
    to produce answers quickly and making the system usable for applications such
    as open-domain question answering. Let’s explore both in more detail.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 没有高效的向量数据库和索引，检索步骤将变成瓶颈，使整个RAG系统变得缓慢且不实用。使用这些工具，相关文档可以实时检索，使生成组件能够快速生成答案，并使系统适用于如开放域问答等应用。让我们更详细地探讨这两个方面。
- en: 7.5 Understanding vector databases
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 理解向量数据库
- en: Vector databases enable enterprises to manage, secure, and scale embeddings
    in a production environment. For many enterprises, vector databases for semantic
    search use cases solve the performance and security requirements needed for production
    systems.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库使企业能够在生产环境中管理、安全和扩展嵌入。对于许多企业来说，用于语义搜索用例的向量数据库解决了生产系统所需的性能和安全要求。
- en: A vector database is specifically designed to operate on embedding vectors.
    As the popularity of LLMs and generative AI has grown recently, so has the use
    of embeddings to encode unstructured data. Vector databases have emerged as an
    effective solution for enterprises to deliver and scale these use cases.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库专门设计用于操作嵌入向量。随着最近LLMs和生成式AI的普及，使用嵌入来编码非结构化数据的使用也日益增长。向量数据库已成为企业交付和扩展这些用例的有效解决方案。
- en: Vector databases are specialized databases that store data as high-dimensional
    vectors and their original content. They offer the capabilities of both vector
    indexes and traditional databases, such as optimized storage, scalability, flexibility,
    and query language support. They allow users to find and retrieve similar or relevant
    data based on their semantic or contextual meaning.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库是专门存储数据为高维向量和其原始内容的数据库。它们提供了向量索引和传统数据库的能力，例如优化存储、可扩展性、灵活性和查询语言支持。它们允许用户根据其语义或上下文意义找到和检索相似或相关的数据。
- en: Given the vast number of documents in large corpora, brute-force comparison
    of a query vector with every document vector is computationally prohibitive. The
    solution is vector search, which comprises indexes and databases that allow efficient
    storage and near-neighbor lookups in high-dimensional spaces. Figure 7.4 shows
    a typical pipeline of incorporating a vector database when implementing a RAG
    pattern with LLMs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到大型语料库中文档的庞大数量，将查询向量与每个文档向量进行暴力比较在计算上是不可行的。解决方案是向量搜索，它包括索引和数据库，允许在多维空间中进行高效的存储和近邻查找。图7.4展示了在实现LLMs的RAG模式时，整合向量数据库的典型流程。
- en: '![figure](../Images/CH07_F04_Bahree.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F04_Bahree.png)'
- en: Figure 7.4 Typical pipeline for vector database
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.4 向量数据库典型流程
- en: Vector databases can help RAG models quickly find the most similar documents
    or passages to a given query and use them as additional context for the LLM. Depending
    on the trade-off between speed and accuracy, vector databases can also support
    various retrieval strategies, such as exact, approximate, or hybrid methods. Having
    a vector database is a good start, but finding the most similar documents or passages
    can only happen with a vector index.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库可以帮助RAG模型快速找到与给定查询最相似的文档或段落，并将它们用作LLM的额外上下文。根据速度和准确度之间的权衡，向量数据库还可以支持各种检索策略，如精确、近似或混合方法。拥有向量数据库是一个好的开始，但找到最相似的文档或段落只能通过向量索引来实现。
- en: 7.5.1 What is a vector index?
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 什么是向量索引？
- en: A vector index is a data structure in a vector database designed to enhance
    the efficiency of processing, and it is particularly suited for the high-dimensional
    vector data encountered with LLMs. Its function is to streamline the search and
    retrieval processes within the database. By implementing a vector index, the system
    is capable of conducting quick similarity searches, identifying vectors that closely
    match or are most similar to a given input vector. Essentially, vector indexes
    are designed to enable rapid and precise similarity search, facilitating the recovery
    of vector embeddings.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 向量索引是向量数据库中的数据结构，旨在提高处理效率，尤其适用于与LLMs（大型语言模型）一起遇到的高维向量数据。其功能是简化数据库内的搜索和检索过程。通过实现向量索引，系统可以执行快速相似度搜索，识别与给定输入向量最接近或最相似的向量。本质上，向量索引的设计是为了实现快速和精确的相似度搜索，促进向量嵌入的恢复。
- en: They organize the vectors using various techniques, such as hashing, clustering,
    or tree-based methods, to make finding the most similar ones easy based on their
    distance or similarity metrics. For example, FAISS (Facebook AI Similarity Search)
    is a popular vector index that efficiently handles billions of vectors.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 他们使用各种技术，如哈希、聚类或基于树的方法，来组织向量，以便基于它们的距离或相似性度量轻松找到最相似的向量。例如，FAISS（Facebook AI
    Similarity Search）是一个流行的向量索引，能够高效地处理数十亿个向量。
- en: To create vector indexes for your embeddings, there are many options, such as
    exact or approximate nearest neighbor algorithms (e.g., HNSW or IVF), different
    distance metrics (e.g., cosine or Euclidean), or various compression techniques
    (e.g., quantization or pruning). Your index method depends on balancing speed,
    accuracy, and memory consumption. We can use different mathematical methods to
    compare how similar two vector embeddings are—these are useful when searching
    and matching different embeddings. Let’s see what vector search means and how
    we can apply different mathematical functions when searching.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要为您的嵌入创建向量索引，有许多选项，例如精确或近似最近邻算法（例如HNSW或IVF）、不同的距离度量（例如余弦或欧几里得）或各种压缩技术（例如量化或剪枝）。您的索引方法取决于平衡速度、准确性和内存消耗。我们可以使用不同的数学方法来比较两个向量嵌入的相似程度——这些在搜索和匹配不同嵌入时很有用。让我们看看向量搜索的含义以及我们如何在搜索时应用不同的数学函数。
- en: 7.5.2 Vector search
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.2 向量搜索
- en: A vector search is a query operation that finds the vectors most similar to
    a given query vector based on a similarity metric. In a RAG pattern for LLMs,
    a vector index stores the documents’ embeddings or passages that the LLM can retrieve
    as context for generating responses. A vector search is used to find the most
    relevant documents or passages to the query based on the similarity between the
    query vector and the document vectors in the index.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 向量搜索是一种查询操作，根据相似性度量找到与给定查询向量最相似的向量。在LLMs的RAG模式中，向量索引存储LLM可以检索作为生成响应上下文的文档嵌入或段落。向量搜索用于根据查询向量与索引中文档向量的相似度，找到与查询最相关的文档或段落。
- en: Similarity measures are mathematical methods that compare two vectors and compute
    a distance value between them. This distance value indicates how dissimilar or
    similar the two vectors are in terms of their semantic meaning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度度量是数学方法，用于比较两个向量并计算它们之间的距离值。这个距离值表示两个向量在语义意义上的相似程度或差异程度。
- en: The distance can be based on multiple criteria, such as the length of the line
    segment between two points, the angle between two directions, or the number of
    mismatched elements in two arrays. Similarity measures are useful for machine
    learning tasks involving grouping or classifying data objects, especially for
    vector or semantic search.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 距离可以基于多个标准，例如两点之间的线段长度、两个方向之间的角度或两个数组中不匹配元素的数量。相似度度量对于涉及分组或分类数据对象的机器学习任务很有用，特别是对于向量或语义搜索。
- en: For example, if we want to find words similar to “puppy,” we can generate a
    vector embedding for this word and look for other words with close vector embeddings,
    such as “dog” (figure 7.5).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想找到与“puppy”相似的单词，我们可以为这个单词生成一个向量嵌入，并寻找具有接近向量嵌入的其他单词，例如“dog”（图7.5）。
- en: '![figure](../Images/CH07_F05_Bahree.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F05_Bahree.png)'
- en: Figure 7.5 Vector search
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5 向量搜索
- en: We should choose the similarity measure that best suits the data and query needs
    of the use case. We must use a similarity measure to perform a vector search,
    a mathematical method for calculating the distance between two vectors. The smaller
    the distance, the more similar the vectors are. Some popular enterprise-ready
    services, such as Azure AI Search, support several similarity measures. Some of
    the more common similarity searches are
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该选择最适合用例数据需求和查询需求的相似度度量。我们必须使用相似度度量来执行向量搜索，这是一种计算两个向量之间距离的数学方法。距离越小，向量越相似。一些流行的企业级服务，如Azure
    AI Search，支持多种相似度度量。一些更常见的相似度搜索包括
- en: '*Cosine similarity*—This measure calculates the cosine of the angle between
    two vectors. It ranges from –1 to 1, where 1 means identical vectors and –1 means
    opposite vectors. Cosine similarity is commonly used for normalized embedding
    spaces.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*余弦相似度*—这个度量计算两个向量之间角度的余弦值。它的范围从 –1 到 1，其中 1 表示相同的向量，–1 表示相反的向量。余弦相似度常用于归一化嵌入空间。'
- en: '*Squared Euclidean or L2-squared distance*—It calculates the straight-line
    distance between two vectors. It ranges from 0 to infinity [0, ∞], where 0 means
    identical vectors, and larger values mean more dissimilar vectors. Squared Euclidean
    distance is also known as the L2 norm.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*平方欧几里得或L2平方距离*—它计算两个向量之间的直线距离。它的范围从 0 到 ∞ [0, ∞]，其中 0 表示相同的向量，较大的值表示更不相似的向量。平方欧几里得距离也称为L2范数。'
- en: '*Dot product*—This measure calculates the product of the magnitudes of two
    vectors and the cosine of the angle between them. It ranges from –infinity to
    infinity [–, ∞], where 0 means orthogonal vectors and larger values mean more
    similar vectors. The dot product is equivalent to cosine similarity for normalized
    embedding spaces but is more efficient.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*点积*—这个度量计算两个向量的模长乘积以及它们之间角度的余弦值。它的范围从 –∞ 到 ∞ [–, ∞]，其中 0 表示正交向量，较大的值表示更相似的向量。点积对于归一化嵌入空间等同于余弦相似度，但更高效。'
- en: '*Hamming distance*—This calculates the number of differences between vectors
    at each dimension.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*汉明距离*—这个度量计算每个维度上向量之间的差异数量。'
- en: '*Manhattan or L1 distance*—This measures the sum of the absolute differences
    between the coordinates of two vectors. It ranges from 0 to infinity [0, ∞], where
    0 means identical vectors and larger values mean vectors mean the opposite, that
    is, dissimilar vectors.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*曼哈顿或L1距离*—这个度量衡量两个向量坐标之间绝对差值的总和。它的范围从 0 到 ∞ [0, ∞]，其中 0 表示相同的向量，较大的值表示向量不同，即不相似的向量。'
- en: Figure 7.6 shows the different similarity measures. It is important to use the
    same metric on which the underlying foundational model has been trained. For example,
    in the case of the OpenAI GPT class of models, the distance function is cosine
    similarity.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6展示了不同的相似度函数。使用与基础模型训练相同的度量是很重要的。例如，在OpenAI GPT类模型的情况下，距离函数是余弦相似度。
- en: '![figure](../Images/CH07_F06_Bahree.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F06_Bahree.png)'
- en: Figure 7.6 Different distant functions
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.6 不同的相似度函数
- en: Note  OpenAI embeddings are normalized to length 1, meaning each vector’s magnitude
    equals 1\. Therefore, if we use OpenAI embeddings normalized to length 1, we can
    choose either cosine similarity or Euclidean distance as our distance function,
    and we will get the same results for vector search. However, cosine similarity
    might be slightly faster to compute because it only involves a dot product operation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：OpenAI嵌入被归一化到长度1，这意味着每个向量的模长等于1。因此，如果我们使用归一化到长度1的OpenAI嵌入，我们可以选择余弦相似度或欧几里得距离作为我们的距离函数，向量搜索将得到相同的结果。然而，余弦相似度可能计算得更快，因为它只涉及点积运算。
- en: Choosing the right distance measure depends on the specific use case, the nature
    of the data, and the desired outcomes. Table 7.1 gives a brief overview of when
    to use each measure.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的距离度量取决于具体用例、数据的性质和期望的结果。表7.1简要概述了何时使用每种度量。
- en: Table 7.1 Choosing the right distance measure
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.1 选择合适的距离度量
- en: '| Measure | When to use | Advantage | Disadvantage |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 度量 | 何时使用 | 优点 | 缺点 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Cosine similarity  | Ideal for text and document similarity, where the magnitude
    of the vectors is not as important as the orientation; common in NLP tasks  |
    Effective in high-dimensional spaces and for normalized vectors; ignores the magnitude
    of vectors, focusing on orientation, making it suitable for comparing documents
    of different lengths  | It is not effective if the magnitude of vectors is important.  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 余弦相似度 | 适用于文本和文档相似度，其中向量的幅度不如方向重要；在NLP任务中很常见 | 在高维空间和归一化向量中有效；忽略向量的幅度，专注于方向，使其适合比较不同长度的文档
    | 如果向量的幅度很重要，则效果不佳。 |'
- en: '| Squared Euclidean (L2)  | Suitable for geometric or spatial data, like in
    image processing or when clustering multi-dimensional numerical data  | It reflects
    the actual distance between points in a Euclidean space, making it intuitive and
    suitable for spatial datasets.  | It can be sensitive to the scale of the data.
    High dimensions can lead to the curse of dimensionality.  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 平方欧几里得（L2）距离 | 适用于几何或空间数据，如图像处理或聚类多维数值数据时 | 反映了欧几里得空间中点之间的实际距离，使其直观且适用于空间数据集。
    | 它可能对数据的规模敏感。高维可能导致维度诅咒。 |'
- en: '| Dot product  | Efficient for high-volume, high-dimensional data, such as
    user preferences in recommendation systems  | Computationally efficient, especially
    for sparse vectors. It is good for cases where the magnitude of vectors matters.  |
    Interpretation is less intuitive than cosine similarity and can be sensitive to
    vector magnitudes.  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 点积 | 适用于高容量、高维数据，如推荐系统中的用户偏好 | 计算效率高，特别是对于稀疏向量。对于向量幅度很重要的情况很有用。 | 其解释不如余弦相似度直观，并且可能对向量幅度敏感。
    |'
- en: '| Hamming distance  | Best for comparing binary or categorical data, such as
    genetic sequences or error detection in data transmission  | Simple and effective
    for datasets with discrete attributes  | It only applies to strings of equal length
    and doesn’t consider the magnitude of differences.  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 汉明距离 | 最佳用于比较二进制或分类数据，如遗传序列或数据传输中的错误检测 | 对于具有离散属性的集合简单而有效 | 它仅适用于长度相等的字符串，不考虑差异的幅度。
    |'
- en: '| Manhattan (L1) distance  | Useful in grid-like pathfinding (e.g., urban road
    layouts) and in cases where differences in individual dimensions are important  |
    It is more sensitive to differences in individual dimensions than L2 distance;
    robust to outliers.  | It may not reflect the true distance in nongrid-like spaces
    or high-dimensional data.  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 曼哈顿（L1）距离 | 适用于网格状路径查找（例如，城市道路布局）以及个体维度差异重要的场合 | 比L2距离更敏感于个体维度的差异；对异常值有鲁棒性。
    | 它可能无法反映非网格状空间或高维数据中的真实距离。 |'
- en: 7.6 RAG challenges
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.6 RAG挑战
- en: Enterprises considering implementing RAG systems face several hurdles that need
    careful consideration. First and foremost, ensuring effective scalability with
    increasing data volumes is critical. As data grows, so does the complexity and
    size of the retrieval index. Managing this growth becomes challenging, necessitating
    more powerful computational resources. Specifically, dense retrieval systems,
    which are resource-intensive in terms of computation and storage, require careful
    balancing to ensure scalability. Additionally, maintaining an efficient and fast
    retrieval index becomes crucial as the volume of documents increases. Parallelizing
    requests, managing retry mechanisms, and deploying appropriate infrastructure
    are essential for achieving scalable RAG systems.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑实施RAG系统的企业面临几个需要仔细考虑的障碍。首先，确保随着数据量的增加而有效扩展至关重要。随着数据增长，检索索引的复杂性和大小也随之增加。管理这种增长变得具有挑战性，需要更多的计算资源。具体来说，密集检索系统在计算和存储方面资源密集，需要仔细平衡以确保可扩展性。此外，随着文档量的增加，保持高效和快速的检索索引变得至关重要。并行化请求、管理重试机制和部署适当的基础设施对于实现可扩展的RAG系统是必不可少的。
- en: Ensuring the quality and relevance of the indexed data is another significant
    concern. The utility of the RAG system is contingent upon the quality of its data;
    outdated or irrelevant information will lead to subpar responses —the principle
    of garbage-in-garbage-out still very much holds. This underscores the need for
    meticulous curation and regular updates of the document index to align with the
    enterprise's evolving requirements.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 确保索引数据的质最和相关性是另一个重大关注点。RAG系统的效用取决于其数据的质量；过时或不相关信息将导致低质量的响应——垃圾输入垃圾输出的原则仍然非常适用。这强调了精心管理和定期更新文档索引以符合企业不断变化需求的必要性。
- en: Once deployed, RAG systems introduce an additional layer of complexity in integrating
    existing workflows, requiring ongoing maintenance to ensure consistent performance.
    RAG systems need to be seamlessly incorporated into an enterprise's existing technical
    landscape. This process often involves navigating complex data governance problems
    and ensuring system interoperability.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦部署，RAG系统在集成现有工作流程时引入了额外的复杂性，需要持续维护以确保性能一致。RAG系统需要无缝集成到企业的现有技术环境中。这个过程通常涉及解决复杂的数据治理问题并确保系统互操作性。
- en: RAG systems involve complex encoding and querying of dense vectors in real-time,
    which can cause delays and affect response times. For applications that need fast
    answers, such latency may not meet user expectations for promptness. In addition,
    the complicated nature of RAG models makes identifying the cause of errors difficult.
    Finding and fixing errors effectively is important, whether they happen during
    retrieval or generation. Moreover, once deployed, RAG systems introduce extra
    complexity when integrating with existing workflows. Ensuring smooth integration
    into an enterprise’s technical landscape involves dealing with data governance
    problems and system compatibility.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统涉及实时对密集向量进行复杂编码和查询，这可能导致延迟并影响响应时间。对于需要快速答案的应用，这种延迟可能无法满足用户对及时性的期望。此外，RAG模型的复杂性质使得确定错误原因变得困难。无论错误发生在检索还是生成过程中，找到并有效修复错误都是重要的。此外，一旦部署，RAG系统在集成现有工作流程时引入了额外的复杂性。确保顺利集成到企业的技术环境中涉及处理数据治理问题和系统兼容性。
- en: From a socio-technical perspective, ensuring that RAG systems are fair and unbiased
    is imperative. The risk of perpetuating existing biases from training data is
    real and can have far-reaching implications, requiring rigorous oversight and
    mitigation strategies. In addition, privacy and security are also key, especially
    if the indexed data includes confidential information, necessitating stringent
    compliance with data protection regulations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从社会技术角度来看，确保RAG系统公平无偏至关重要。从训练数据中延续现有偏差的风险是真实存在的，可能会产生深远的影响，需要严格的监督和缓解策略。此外，隐私和安全也是关键因素，尤其是如果索引数据包括机密信息时，需要严格遵守数据保护法规。
- en: Chunking is a key problem that needs to be addressed by RAG implementations.
    Chunking is splitting a long text into smaller segments that an LLM can handle
    more easily. It can help lower the model’s computational and memory demands and
    enhance the quality and relevance of the output text. Chunking can also help the
    model concentrate on the most crucial parts of the text and avoid unimportant
    or repetitive parts. The difficulties with chunking are huge; we will discuss
    them in detail in the following sections.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 分块是RAG实现需要解决的关键问题。分块是将长文本分割成LLM可以更容易处理的较小段。这有助于降低模型的计算和内存需求，并提高输出文本的质量和相关性。分块还可以帮助模型专注于文本中最关键的部分，避免不重要的或重复的部分。分块困难很大；我们将在以下章节中详细讨论这些问题。
- en: Enterprises need to be aware of these challenges and weigh them against the
    benefits that RAG systems can bring, such as improved accuracy and contextual
    relevance in natural language processing tasks. When implementing an RAG-based
    solution, they must consider the trade-offs regarding costs, resources, and potential
    risks.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 企业需要意识到这些挑战，并权衡它们与RAG系统可以带来的好处，例如在自然语言处理任务中提高准确性和上下文相关性。在实施基于RAG的解决方案时，他们必须考虑成本、资源和潜在风险方面的权衡。
- en: 7.7 Overcoming challenges for chunking
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.7 克服分块挑战
- en: Today, enterprises face many challenges when implementing RAG at a production
    scale. As mentioned before, chunking is the process of dividing a long sequence
    of text into smaller, more manageable pieces. This is necessary for LLMs, which
    have limited processing capacity. RAG models typically use a chunking algorithm
    to divide the input text into smaller chunks, which the LLM processes. The LLM
    generates a response for each chunk, and the responses are then concatenated to
    form the final output.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，企业在实施生产规模的RAG时面临许多挑战。如前所述，分块是将长序列文本划分为更小、更易管理的片段的过程。这对于处理能力有限的LLMs来说是必要的。RAG模型通常使用分块算法将输入文本划分为更小的分块，然后LLM进行处理。LLM为每个分块生成一个响应，然后将这些响应连接起来形成最终输出。
- en: 'Chunking, however, can be challenging for RAG models for the following reasons:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，分块对RAG模型来说可能具有挑战性，原因如下：
- en: Chunks may not be aligned with the natural boundaries of the text. This can
    lead to the LLM generating grammatically incorrect or semantically incoherent
    responses.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分块可能与文本的自然边界不一致。这可能导致LLM生成语法错误或语义不连贯的响应。
- en: Chunks may vary in length and complexity. This can make it difficult for the
    LLM to generate responses consistent in quality.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分块在长度和复杂性上可能有所不同。这可能会使LLM难以生成质量一致的响应。
- en: Chunks may contain multiple intents. This can make it difficult for the LLM
    to identify the correct intent and generate the appropriate response.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分块可能包含多个意图。这可能会使LLM难以识别正确的意图并生成适当的响应。
- en: We start by understanding a strategy for chunking.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先理解一种分块策略。
- en: 7.7.1 Chunking strategies
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7.1 分块策略
- en: One downside of search is that we can only put so much information in the context
    window. If we use OpenAI models as a measure, depending on the model, we can only
    use a finite set of information that can be passed, as shown in table 7.2\. In
    practical terms, this length is even shorter, given that we need space for the
    generation. This is where chunking becomes key.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索的一个缺点是我们只能将有限的信息放入上下文窗口。如果我们以OpenAI模型为衡量标准，根据模型的不同，我们只能使用有限的信息集进行传递，如表7.2所示。在实际情况中，这个长度甚至更短，因为我们需要为生成留出空间。这就是分块变得关键的地方。
- en: Table 7.2 OpenAI model context length
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.2 OpenAI模型上下文长度
- en: '| Open AI model | Maximum length (token size) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Open AI模型 | 最大长度（标记大小） |'
- en: '| --- | --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GPT-3.5 Turbo  | 4K tokens; approx. 5 pages  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 Turbo  | 4K个标记；约5页  |'
- en: '| GPT-4  | 8K tokens; approx. 10 pages  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4  | 8K个标记；约10页  |'
- en: '| GPT-4 32K  | 32K tokens; approx. 40 pages  |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 32K  | 32K个标记；约40页  |'
- en: '| GPT-4 Turbo, GPT-4o  | 128K tokens; approx. 300 pages  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 Turbo, GPT-4o  | 128K个标记；约300页  |'
- en: 'Chunking means breaking down big documents or text passages into smaller, more
    digestible parts or chunks. This is done to make the retrieval process faster
    and better, especially when working with huge collections of texts; the main reason
    is also the context window constraint of the LLMs. Chunking is useful for RAG
    for a few reasons:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 分块意味着将大文档或文本段落分解成更小、更易于消化的部分或分块。这样做是为了使检索过程更快、更好，尤其是在处理大量文本集合时；主要原因也是LLMs的上下文窗口约束。分块对RAG有用的几个原因包括：
- en: '*Granularity*—When querying a large corpus for relevant information, searching
    at the granularity of smaller chunks might lead to more precise retrievals than
    searching entire documents. This can enhance the overall quality of the answers
    generated by RAG.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*粒度*—在查询大型语料库以获取相关信息时，在较小分块粒度上搜索可能比在整个文档上搜索更精确的检索结果。这可以提高RAG生成的答案的整体质量。'
- en: '*Efficiency*—Dealing with smaller chunks can make the retrieval process more
    efficient, especially when using dense retrievers that embed each chunk into a
    high-dimensional vector space.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*效率*—处理较小的分块可以使检索过程更高效，尤其是在使用将每个分块嵌入到高维向量空间中的密集检索器时。'
- en: '*Flexibility*—Chunking allows the system to match varying lengths of relevant
    information to a given query, offering more flexibility in what is considered
    relevant.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*灵活性*—分块允许系统将不同长度的相关信息与给定查询相匹配，从而在考虑相关性的方面提供更大的灵活性。'
- en: When considering the chunking strategy, we need to consider it holistically
    and see how the resulting searches capture the essence of the user’s query. If
    a chunk is too large or small, it could lead to inaccurate results. As a simple
    rule, if a chunk makes sense to us as humans without additional information, an
    LLM could also understand it.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑块化策略时，我们需要全面考虑，并看看由此产生的搜索如何捕捉用户的查询本质。如果一个块太大或太小，可能会导致不准确的结果。作为一个简单的规则，如果一个块对我们人类来说有意义，无需额外信息，那么大型语言模型（LLM）也能理解它。
- en: For conversational use cases, as the turn-by-turn back and forth happens and
    the dialogue gets longer, it is important to evaluate how much of the previous
    conversation is needed for the next turn in the ongoing context. Adding bigger
    chunks could affect relevancy, and we also will get up to the limitations of the
    context windows of the LLM.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对话用例，随着对话的逐轮来回和对话的变长，评估在当前上下文中下一次轮次需要多少先前的对话是很重要的。添加更大的块可能会影响相关性，我们也会达到LLM上下文窗口的限制。
- en: Let’s use a customer service chatbot scenario with a service provider—a mobile
    phone provider, as an example. The conversation might start with one topic, a
    question on activating a new phone, and can turn to other topics such as details
    about plans, add-on products, coverage details, billing, payment methods, etc.
    In this example, as the conversation turns from one topic to the other, in many
    cases, we don’t need all the previous history and dialogue, and it can be either
    discarded or trimmed. Of course, in some cases, we would only want the needed
    details for the context of the ongoing conversation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以客户服务聊天机器人场景为例，服务提供商——一个移动通信提供商。对话可能从一个主题开始，比如激活新手机的问题，然后转向其他主题，如计划细节、附加产品、覆盖范围详情、账单、支付方式等。在这个例子中，当对话从一个主题转向另一个主题时，在许多情况下，我们不需要所有的先前历史和对话，它可以被丢弃或裁剪。当然，在某些情况下，我们可能只想获取当前对话上下文所需的详细信息。
- en: The length of the information being chunked depends on the use case and the
    user’s expected behavior. For example, if we chunk a paragraph, we get a vector
    representation that captures more meaning from the content. This differs from
    sentence-level embedding, where the vector representation reflects more of the
    sentence’s meaning. This would lead to comparisons of other sentences and be more
    limited than the previous paragraph-based approach.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 信息块的大小取决于用例和用户的预期行为。例如，如果我们对一个段落进行块化，我们得到一个向量表示，它从内容中捕获了更多的意义。这与句子级别的嵌入不同，句子级别的嵌入向量表示反映了句子更多的意义。这可能导致与其他句子的比较，并且比之前的基于段落的策略更有限。
- en: 7.7.2 Factors affecting chunking strategies
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7.2 影响块化策略的因素
- en: 'Before we get into the different chunking approaches, a few additional things
    to factor in from a chunking strategy perspective will help us balance higher
    accuracy with keeping within acceptable performance and cost thresholds:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究不同的块化方法之前，从块化策略的角度考虑一些额外因素将有助于我们在保持可接受的性能和成本阈值的同时，平衡更高的准确性：
- en: '*Nature of the content*—The nature of the content that’s being indexed affects
    the chunking strategy. For example, shorter content, such as tweets, might require
    different chunking than longer content like books or reports. Shorter content
    may be chunked together, while longer content, such as documents, reports, and
    similar, may need to be broken down into smaller parts for efficient processing.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*内容性质*——被索引的内容的性质会影响块化策略。例如，较短的文本，如推文，可能需要与较长的文本，如书籍或报告，不同的块化方式。较短的文本可能需要组合在一起，而较长的文本，如文档、报告等，可能需要分解成更小的部分以进行高效处理。'
- en: '*LLM and the associated embedding model*—The LLM and the associated embedding
    model can also affect the chunking strategy. For instance, some models may be
    more efficient at processing smaller chunks or, given their architecture, a chunk
    of a certain size, while others may handle larger chunks better. Knowing which
    LLM and associated embedding model we will use is important. For example, when
    using OpenAI, we should consider the `text-embedding-ada-002` embedding with a
    size of 256 or 512 tokens.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM及其关联的嵌入模型*——LLM及其关联的嵌入模型也会影响块化策略。例如，一些模型可能更擅长处理较小的块，或者根据它们的架构，处理特定大小的块，而其他模型可能更适合处理较大的块。了解我们将使用哪个LLM及其关联的嵌入模型很重要。例如，当使用OpenAI时，我们应该考虑256或512个标记大小的`text-embedding-ada-002`嵌入。'
- en: '*Query complexity*—The length and complexity of the user query can affect the
    chunking approach. More complex queries might necessitate more intricate chunking
    strategies to match the query with the relevant data. It is important to remember
    that LLMs are not search engines and should not be used as such. The query complexity
    is multidimensional, both in terms of length and complexity, and might involve
    breaking the query into smaller subqueries that target different aspects of the
    original query before bringing everything back to the answer. For example, the
    query “What is the capital of the UK?” is very specific and straightforward. In
    contrast, the query “What are the economic implications of the rise of AI in various
    industries in the United States?” is multifaceted. It requires a deeper understanding
    of the technology (AI, in this example) and the geographic and industry details
    to infer the meaning of implications.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*查询复杂性*—用户查询的长度和复杂性会影响分块方法。更复杂的查询可能需要更复杂的分块策略来匹配相关数据。重要的是要记住，LLM 不是搜索引擎，不应作为搜索引擎使用。查询复杂性是多维的，包括长度和复杂性，可能需要将查询分解成更小的子查询，这些子查询针对原始查询的不同方面，然后再将所有内容汇总到答案中。例如，查询“英国的首都是什么？”非常具体且直接。相比之下，查询“美国各行业中人工智能崛起的经济影响是什么？”是多方面的。它需要更深入地了解技术（例如，本例中的
    AI）以及地理和行业细节，以推断影响的含义。'
- en: '*Integration into the application*—Understanding how the output (query result)
    is used within the application can also influence the chunking strategy. For example,
    the limitations of the LLM and the context window might dictate how the data should
    be chunked to achieve the best results. This also factors in other data and metadata
    that the application might need.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集成到应用中*—理解输出（查询结果）在应用中的使用方式也可以影响分块策略。例如，LLM 的限制和上下文窗口可能决定了如何分块数据以实现最佳结果。这还包括应用可能需要的其他数据和元数据。'
- en: '*Preprocessing data*—Preprocessing the data will help increase the quality
    of the generation and help us determine a possible good size. Preprocessing would
    include cleaning up extra noise or using other AI techniques to extract information,
    including data cleaning, data transformation from one format to another, feature
    normalization if required, tokenization, removing common stop words (such as “is,”
    “the,” “and”), and so forth.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据预处理*—预处理数据有助于提高生成质量并帮助我们确定一个可能的好大小。预处理可能包括清理额外噪声或使用其他 AI 技术提取信息，包括数据清理、将数据从一种格式转换到另一种格式、如果需要则进行特征归一化、分词、移除常见停用词（如“是”、“的”、“和”）等。'
- en: '*Evaluating and comparing different chunk sizes*—It’s crucial to evaluate and
    compare the effects of different chunk sizes on both the quality and performance
    of the process. This can be especially important in enterprise settings where
    varying chunk sizes might be used based on the nature of the content, and a balance
    may need to be struck between accuracy and performance. This evaluation would
    include both quality and performance.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评估和比较不同的分块大小*—评估和比较不同分块大小对过程质量和性能的影响至关重要。在企业环境中，这可能尤为重要，因为分块大小可能会根据内容性质而变化，并且需要在准确性和性能之间取得平衡。这种评估将包括质量和性能两个方面。'
- en: One can take a few approaches when thinking about chunking information, as outlined
    in table 7.3\. It’s worth noting that the ideal chunking strategy might vary based
    on the corpus, the nature of the queries, and the application’s specific requirements.
    Experimentation might be needed to find the most effective approach for a particular
    RAG implementation.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑分块信息时，可以采取一些方法，如表 7.3 所述。值得注意的是，理想的分块策略可能因语料库、查询性质和应用的具体要求而异。可能需要进行实验，以找到特定
    RAG 实现中最有效的方案。
- en: Table 7.3 Chunking approaches
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 7.3 分块方法
- en: '| Chunking approach | Description |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 分块方法 | 描述 |'
- en: '| --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Fixed length  | Divide documents into chunks of a fixed number of words or
    tokens. This is straightforward but may sometimes split information that ideally
    should be kept together.  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 固定长度 | 将文档分成固定数量的单词或标记的分块。这很简单，但有时可能会分割本应保留在一起的信息。  |'
- en: '| Sliding window  | Use a fixed-sized sliding window with or without overlapping
    data. This can ensure that important boundaries within the text are not missed,
    but it can also lead to redundancy if there’s significant overlap.  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 滑动窗口 | 使用固定大小的滑动窗口，带有或不带重叠数据。这可以确保不会错过文本中的重要边界，但如果有显著的重叠，也可能导致冗余。 |'
- en: '| Punctuation based  | Divide the text based on punctuation, such as paragraphs
    or sections. This is less arbitrary than fixed-length chunking and often preserves
    the semantic integrity of the content. However, it can result in variable chunk
    sizes.  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 基于标点的分割 | 根据标点符号，如段落或章节来分割文本。这种方法比固定长度块分割更少随意性，并且通常可以保留内容的语义完整性。然而，它可能导致块大小不固定。
    |'
- en: '| Topic or section breaks  | In structured documents such as Wikipedia articles,
    natural breaks like sections or subsections can be used to define chunks. This
    method ensures that the content within a chunk is semantically coherent.  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 主题或部分分隔符 | 在结构化文档，如维基百科文章中，可以使用自然分隔符，如章节或子章节来定义内容块。这种方法确保了块内内容在语义上是连贯的。 |'
- en: '| Adaptive  | Use algorithms or models that adaptively determine the best way
    to chunk documents based on their content. This can be more complex but might
    yield semantically cohesive chunks.  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 自适应 | 使用算法或模型，根据文档的内容自适应地确定最佳的块分割方式。这可能更复杂，但可能产生语义上连贯的块。 |'
- en: Depending on the size and structure of the text, there are different ways to
    chunk it for RAG. Some of the common methods are
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 根据文本的大小和结构，有不同方法可以对RAG进行块分割。一些常见的方法包括
- en: '*Sentence splitting*—As the name suggests, sentence boundaries are used to
    split the text, which is useful to ensure that each chunk contains whole sentences,
    preserving the context and meaning.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*句子分割*——正如其名所示，使用句子边界来分割文本，这对于确保每个块包含完整的句子，保留上下文和意义是有用的。'
- en: '*Fixed-length splitting*—Here, text into is divided into fixed-length chunks.
    This can sometimes result in sentences being cut off in the middle.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*固定长度分割*——在这里，文本被分割成固定长度的块。这有时会导致句子在中间被截断。'
- en: '*Token-based splitting*—Splitting the text based on a fixed number of tokens
    (e.g., words). This is more fine-grained than sentence splitting but can still
    result in sentences being cut off.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于标记的分割*——根据固定数量的标记（例如，单词）来分割文本。这比句子分割更细致，但仍然可能导致句子被截断。'
- en: '*Semantic chunking*—Using natural language processing (NLP) tools to identify
    coherent segments in the text. For instance, splitting a text based on topics
    or paragraphs.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语义块分割*——使用自然语言处理（NLP）工具来识别文本中的连贯段。例如，根据主题或段落分割文本。'
- en: '*Hierarchical chunking*—Dividing text into hierarchical sections, such as chapters,
    sections, and subsections.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分层块分割*——将文本分割成分层部分，如章节、节和子节。'
- en: To illustrate how different chunking approaches (fixed length and a semantic
    NLP) might affect the outcomes, we use an example of the UK Constitution from
    Wikipedia [2] as our input text. We can see the outcome in figure 7.7 when we
    apply a fixed-length chunking approach. The text is broken up into chunks of a
    fixed size, and in this simple example, we see that some information is cut off
    and some context is missing.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明不同的块分割方法（固定长度和基于语义的NLP）可能如何影响结果，我们以维基百科中的英国宪法[2]为例作为我们的输入文本。当我们应用固定长度块分割方法时，我们可以在图7.7中看到结果。文本被分割成固定大小的块，在这个简单的例子中，我们可以看到一些信息被截断，一些上下文丢失。
- en: '![figure](../Images/CH07_F07_Bahree.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F07_Bahree.png)'
- en: Figure 7.7 Fixed-length chunking approach
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.7 固定长度块分割方法
- en: The text of the UK’s constitution appears in figure 7.8, using an NLP-based
    chunking approach. Because NLP comprehends the text and context, it splits it
    at the proper level with the correct tokens to maintain the sense and accuracy.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 英国宪法的文本出现在图7.8中，使用基于NLP的块分割方法。由于NLP理解文本和上下文，它会在适当的级别上使用正确的标记来保持意义和准确性。
- en: '![figure](../Images/CH07_F08_Bahree.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F08_Bahree.png)'
- en: Figure 7.8 NLP-based chunking approach
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.8 基于NLP的块分割方法
- en: These chunking strategies are useful and important for any provider or LLM we
    choose. In the following sections, we will see how to apply these strategies.
    We will begin with Sentence Splitter, a text splitter that splits the text based
    on a new line.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这些块分割策略对于任何我们选择的提供者或LLM都很有用和重要。在接下来的章节中，我们将看到如何应用这些策略。我们将从Sentence Splitter开始，这是一个基于新行分割文本的文本分割器。
- en: 7.7.3 Handling unknown complexities
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7.3 处理未知复杂性
- en: 'Sometimes, we don’t know the complexities and length of the user queries in
    advance. In such cases, RAG implementations that can deal with unknown lengths
    and complexities of user queries can be challenging. Here are several strategies
    to determine the chunking approach in such scenarios:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们事先不知道用户查询的复杂性和长度。在这种情况下，能够处理未知长度和复杂性的用户查询的RAG实现可能会具有挑战性。以下是一些确定这种场景下分块方法的策略：
- en: '*Adaptive chunking*—Implement an adaptive chunking mechanism that automatically
    adjusts the size of chunks based on the query length and complexity. Smaller chunks
    can be used for shorter, simpler queries, while larger chunks might be needed
    to capture the necessary context for longer, more complex queries.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自适应分块*—实现一个自适应分块机制，根据查询长度和复杂性自动调整分块的大小。对于较短、较简单的查询，可以使用较小的分块，而对于较长、较复杂的查询，可能需要较大的分块来捕捉必要上下文。'
- en: '*Preprocessing heuristics*—Use heuristics to analyze the query before chunking.
    These heuristics could estimate the complexity by looking at factors like the
    number of unique words, the presence of specialized terminology, or the syntactic
    structure. Based on this estimation, the chunking mechanism can adapt the size
    of the chunks.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预处理启发式方法*—在分块之前使用启发式方法分析查询。这些启发式方法可以通过查看诸如独特单词数量、专业术语的存在或句法结构等因素来估计复杂性。基于这种估计，分块机制可以调整分块的大小。'
- en: '*Dynamic retrieval window*—Implement a dynamic retrieval window that expands
    or contracts based on the query. If the initial retrieval results are unsatisfactory,
    the window can be adjusted to include more or fewer documents or to change the
    granularity of the chunking.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*动态检索窗口*—实现一个基于查询动态扩展或收缩的动态检索窗口。如果初始检索结果不满意，窗口可以调整以包含更多或更少的文档，或改变分块粒度。'
- en: '*Overlapping chunks*—Create overlapping chunks to ensure that no critical information
    is lost at the boundaries of chunks. This approach can help maintain context when
    queries span multiple chunks. Depending on the use case, this can also overpower
    the other information, which isn’t something one should do by default.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重叠分块*—创建重叠的分块以确保在分块边界处不会丢失任何关键信息。这种方法有助于在查询跨越多个分块时保持上下文。根据具体用例，这也可能压倒其他信息，这不是默认应该做的事情。'
- en: '*ML approaches*—Use traditional ML models to predict the optimal chunk size
    based on the query characteristics. The model can be trained on a dataset of queries
    and optimal chunk sizes determined by performance on a validation set.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习方法*—使用传统的机器学习模型根据查询特征预测最佳分块大小。该模型可以在查询和最佳分块大小的数据集上训练，并通过在验证集上的性能确定最佳分块大小。'
- en: '*Fallback strategies*—Have fallback strategies in place for when the initial
    chunking does not yield good results. This can involve re-querying with different
    chunk sizes or using different chunking strategies if the initial response does
    not meet certain confidence thresholds.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*回退策略*—当初始分块没有产生良好结果时，应制定回退策略。这可能涉及使用不同的分块大小重新查询，或者在初始响应未达到某些置信度阈值时使用不同的分块策略。'
- en: '*Feedback loop*—Implement a feedback loop where user interactions can help
    adjust the chunking. If a user indicates an unsatisfactory response, the system
    could automatically try different chunking strategies to improve the response.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*反馈循环*—实现一个反馈循环，其中用户交互可以帮助调整分块。如果用户表示不满意，系统可以自动尝试不同的分块策略以改进响应。'
- en: '*Hybrid approaches*—Combine several of the preceding strategies to handle various
    queries. For example, adaptive chunking with a fallback strategy that continuously
    employs user feedback can improve the chunking mechanism.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*混合方法*—结合前面提到的几种策略来处理各种查询。例如，结合自适应分块和回退策略，该策略持续使用用户反馈，可以改进分块机制。'
- en: In practice, the optimal solution would combine these strategies for a specific
    use case, and trial and error are needed to enhance performance. Moreover, making
    the system's components flexible can enable changes and upgrades to the chunking
    mechanism as more information is collected about the kinds of queries users enter.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，最佳解决方案将结合这些策略以适应特定用例，并且需要通过试错来提高性能。此外，使系统的组件灵活可以允许在收集更多关于用户输入查询类型的信息时对分块机制进行更改和升级。
- en: 7.7.4 Chunking sentences
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7.4 分块句子
- en: A sentence-based splitter is a method that splits the text into chunks based
    on sentence boundaries, such as periods, question marks, or exclamation points.
    This method can preserve the meaning and coherence of the text, as each chunk
    contains one or more complete sentences.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 基于句子的分割器是一种根据句子边界（如句号、问号或感叹号）将文本分割成块的方法。这种方法可以保留文本的意义和连贯性，因为每个块包含一个或多个完整的句子。
- en: 'Listing 7.1 shows a simple implementation: an incoming text is split into sentences
    using regular expressions. The function splits the input text at every occurrence
    of a period (.), exclamation mark (!), or question mark (?). These characters
    are typically used to denote the end of a sentence in English. The result is a
    list of strings, each being a sentence from the original text.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.1展示了简单实现：使用正则表达式将输入文本分割成句子。该函数在输入文本中每个句号（.）、感叹号（!）或问号（?）的出现处分割文本。这些字符通常用于表示英语中句子的结束。结果是字符串列表，每个字符串都是原始文本中的一个句子。
- en: Listing 7.1 Split sentence function
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.1 分割句子函数
- en: '[PRE0]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Splits the sentence at every occurrence of these characters'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 在这些字符的每个出现处分割句子'
- en: Another way to implement the same thing is using a sentence-based splitter,
    such as the `textwrap` library in Python. This function, `wrap()`, can split a
    string into a list of strings based on a given width. We can pass additional parameters
    to ensure that words don’t get split mid-sentence.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 实现相同功能的另一种方法是使用基于句子的分割器，例如Python中的`textwrap`库。这个函数`wrap()`可以根据给定的宽度将字符串分割成字符串列表。我们可以传递额外的参数来确保单词不会在句子中间被分割。
- en: Listing 7.2 Splitting sentences using `textwrap`
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.2 使用`textwrap`分割句子
- en: '[PRE1]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Sets the maximum chunk size'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 设置最大块大小'
- en: '#2 Splits the text into chunks'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将文本分割成块'
- en: It is important to point out that both the `textwrap.wrap()`and `re.split()`
    functions serve different purposes, and their efficiency, speed, and accuracy
    depend on the specific use case.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 需要指出的是，`textwrap.wrap()`和`re.split()`函数有不同的用途，它们的效率、速度和准确性取决于具体的使用场景。
- en: The original purpose of the `textwrap` library is for display purposes and to
    help format and wrap strings where we want control over the maximum line length.
    It’s efficient and fast for its intended use case. However, it’s not designed
    to split text into sentences, so if you use it for that purpose, it may not be
    accurate. For example, it could split a sentence in the middle if the sentence
    is longer than the specified width.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`textwrap`库的原始目的是为了显示目的，并帮助我们格式化和包装字符串，以便我们可以控制最大行长度。对于其预期用例来说，它既高效又快速。然而，它不是为分割文本成句子而设计的，所以如果您用它来达到这个目的，可能不会很准确。例如，如果句子比指定的宽度长，它可能会在句子中间进行分割。'
- en: The `split()` function in regular expressions divides a string where the pattern
    matches. It can split a text into sentences well when used with a pattern such
    as '[.!?]'. It’s also quick and effective for what it does. However, it doesn’t
    consider line length or word boundaries, so if you need to limit the length of
    each chunk, `re.split()` would not be the best option.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式中的`split()`函数在模式匹配的地方分割字符串。当与模式`[.!?]`一起使用时，它可以很好地将文本分割成句子。它对于所做的工作来说既快又有效。然而，它不考虑行长度或单词边界，所以如果您需要限制每个块的大小，`re.split()`可能不是最佳选择。
- en: In terms of speed, both functions are quite fast and should perform well for
    most typical use cases. The speed could become a problem for very large strings,
    but in most cases, the difference would not be noticeable. Regarding accuracy,
    if we need to split the text into sentences, `re.split()` would be more accurate.
    If you need to wrap text to a certain line length, `textwrap.wrap()` would be
    more accurate.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在速度方面，这两个函数都非常快，应该适用于大多数典型用例。对于非常长的字符串，速度可能会成为一个问题，但在大多数情况下，差异不会很明显。至于准确性，如果我们需要将文本分割成句子，`re.split()`将更准确。如果您需要将文本包装到特定的行长度，`textwrap.wrap()`将更准确。
- en: Both functions are quite efficient, as they are part of Python’s standard library
    and are implemented in C. The efficiency would also depend on the size and complexity
    of the input string.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数都非常高效，因为它们是Python标准库的一部分，并且是用C语言实现的。效率也会取决于输入字符串的大小和复杂性。
- en: 7.7.5 Chunking using natural language processing
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7.5 使用自然语言处理进行块分割
- en: As outlined in the earlier example, we can use a natural language processing
    (NLP) approach to split the text into chunks; these chunks can be based on linguistic
    features, such as clauses, phrases, or entities. Compared to the sentence splitter
    methods outlined earlier, this method can capture the meaning and context of the
    text, but it may require more computational resources and domain knowledge. Let’s
    see some examples using two of the most common NLP libraries available today—the
    Natural Language Toolkit (NLTK) and spaCy.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所述，我们可以使用自然语言处理（NLP）方法将文本分割成块；这些块可以基于语言特征，如子句、短语或实体。与前面概述的句子分割方法相比，这种方法可以捕捉文本的意义和上下文，但它可能需要更多的计算资源和领域知识。让我们看看使用目前最常用的两个NLP库——自然语言工具包（NLTK）和spaCy的一些示例。
- en: Using the NLTK
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用NLTK
- en: 'The NLTK is one of the most well-known libraries for natural language processing
    and text analytics. It provides easy-to-use interfaces to many corpora and lexical
    resources. Furthermore, it includes a suite of text-processing libraries for classification,
    tokenization, stemming, tagging, parsing, and more. NTLK can be installed in many
    ways; in the case of conda, we can use the following: `conda install -c anaconda
    nltk`. For pip, we can use `pip install nltk`. Before we can use NLTK, we need
    to install the NLTK data, which can be done using the NLTK’s data downloader.
    A simple way to do this is to run a Python interpreter using administrator privileges
    and run the following commands. More details can be found at [https://www.nltk.org/data.html](https://www.nltk.org/data.html):'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK是自然语言处理和文本分析中最知名的库之一。它提供了易于使用的接口，用于访问许多语料库和词汇资源。此外，它还包括一系列用于分类、分词、词干提取、标注、解析等文本处理的库。NTLK可以通过多种方式安装；在conda的情况下，我们可以使用以下命令：`conda
    install -c anaconda nltk`。对于pip，我们可以使用`pip install nltk`。在我们能够使用NLTK之前，我们需要安装NLTK数据，这可以通过NLTK的数据下载器完成。一种简单的方法是使用管理员权限运行Python解释器并运行以下命令。更多详细信息可以在[https://www.nltk.org/data.html](https://www.nltk.org/data.html)找到：
- en: '[PRE2]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The following listing shows how to implement NLTK using the `sent_tokenize()`
    function to split the text into sentences.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表展示了如何使用`sent_tokenize()`函数实现NLTK，将文本分割成句子。
- en: Listing 7.3 Chunking text using NLP
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.3 使用NLP进行文本块化
- en: '[PRE3]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `sent_tokenize()` function uses an instance of `PunktSentenceTokenizer`,
    an unsupervised ML-based tokenizer that comes pretrained and is ready for sentence
    splitting. If the text is very large, you might consider using a generator expression
    instead of a list comprehension for memory efficiency.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`sent_tokenize()`函数使用`PunktSentenceTokenizer`的一个实例，这是一个基于无监督机器学习的分词器，它预训练好并准备好进行句子分割。如果文本非常大，你可能需要考虑使用生成器表达式而不是列表推导式以提高内存效率。'
- en: The next listing shows how the previous function could be rewritten as a generative
    function.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表展示了如何将前面的函数重写为生成函数。
- en: 'Listing 7.4 Chunking using NLP: Generative function'
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.4 使用NLP进行块化：生成函数
- en: '[PRE4]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The NLTK can be quite advantageous when it comes to chunking. It can detect
    sentence boundaries and split on those lines, and it is also effective for splitting
    texts into individual sentences, which can be useful for chunking large texts,
    while ensuring that sentences are not broken in the middle.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK在块化方面具有相当的优势。它可以检测句子边界并在这些行上分割，它对于将文本分割成单个句子也非常有效，这对于块化大型文本很有用，同时确保句子不会被中间打断。
- en: From an enterprise perspective, it’s worth noting that while NLTK is comprehensive
    and suitable for research and educational purposes, it might not always be the
    most efficient in terms of speed. Other libraries such as spaCy might be more
    suitable for production-level applications, especially when processing vast amounts
    of text.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 从企业角度来看，值得注意的是，虽然NLTK功能全面，适合研究和教育目的，但在速度方面可能并不总是最有效的。其他库如spaCy可能更适合生产级应用，尤其是在处理大量文本时。
- en: Using spaCy
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用spaCy
- en: spaCy is a free, open source NLP library for Python that provides a wide range
    of NLP tasks, including sentence segmentation, named entity recognition, part-of-speech
    tagging, and dependency parsing. It is also good for chunking text and grouping
    words into meaningful units, such as noun phrases, verb phrases, and prepositional
    phrases.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy是一个免费的、开源的Python NLP库，它提供了一系列NLP任务，包括句子分割、命名实体识别、词性标注和依存句法分析。它还适用于将文本块化并将单词分组到有意义的单元中，例如名词短语、动词短语和介词短语。
- en: spaCy is a good choice for RAG implementations, as it is efficient and fast,
    especially when processing large amounts of text in real-time. It is accurate
    and reliable and can be customized depending on the specific needs. For example,
    spaCy can be used to chunk text using different linguistic theories, such as phrase
    structure grammar and dependency grammar.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can use spaCy, we need to install the packages and download the appropriate
    pretrained language model for spaCy to use. If using conda, we can install spaCy
    using `conda install -c conda-forge spacy`. If we are using pip, then we can use
    the following: `pip` `install` `spacy`. In the example, we download the small
    general-purpose English language model called `en_core_web_sm` using the following
    command: `python -m spacy download en_core_web_sm`.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: spaCy offers additional models for different purposes and languages. In addition
    to the small English model, medium and large models are available—`en_core_web_md`
    and `en_core_web_lg`, respectively, for more comprehensive word vectors. The larger
    the model, the longer it will take to process. Choosing a model involves more
    than just the size; one must factor in accuracy, languages, and domain. More details
    on the pre-trained models can be found at [https://spacy.io/usage/models/](https://spacy.io/usage/models/).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows how we can use spaCy for chunking. In this example,
    we factor in token counts for the LLMs context windows, and we also have the option
    to overlap text between chunks to allow for context continuity.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.5 Sentence chunking using spaCy
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Loads the spaCy model'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Tokenizes the text into sentences using spaCy'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Tokenizes sentences and accumulates tokens'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Sliding window adjustment'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: These techniques have different benefits and computing characteristics. Let’s
    try them all and compare their performance, duration, and effect. For example,
    we use Azure OpenAI and the FIFA 2023 Women’s World Cup as data [3]. This happened
    in 2023, and at the time of this publication, the LLMs lack this knowledge, as
    it is beyond the training cut-off.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we save the Wikipedia page for the FIFA 2023 Women’s World
    Cup as a raw text field. This file is not processed, and the resulting file is
    messy enough to reflect many real-world problems enterprises would face.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: In this example, as shown in listing 7.6, we run through the four different
    chunking techniques using the same file and outline the time it takes for each
    technique to execute, the number of chunks created, and the tokens used. We also
    use GPT3 to create a summary of the text read.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: We begin by loading the stored text file named women_ fifa_worldcup_2023.txt.
    We apply four different chunking techniques separately and then use the same GPT
    mode to summarize them. We first chunk using a basic sentence chunking method
    and process those. Then, we process the same file using `textwrap`, NLTK, and
    spaCy. We record some simple telemetry at each run and show all of these at the
    end, along with the summary.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Note that several helper functions, such as `get_embedding()`, `count_tokens()`,
    and so forth, have been used earlier in the book—we do not call those out again
    for brevity. The complete code samples are in the GitHub code repository accompanying
    the book ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.6 Sentence-chunking comparison
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Empty list to store the summaries'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Completion to generate a summary for the chunk'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Rate limiting'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Counts tokens in the sentence'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '#5 File that we want to chunk'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Chunks text using textwrap'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Chunks text using NLTK'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Chunks text using spaCy'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Generates summaries for each chunk using OpenAI API'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.4 shows the output when we run this, with the time duration in seconds.
    As expected, the time it takes to process the same input text differs greatly
    depending on the technique used.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.4 Sentence-chunking comparison
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Chunking method | Embeddings count | Tokens count | Execution time (secs)
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| Simple  | 120  | 5815  | 16.96  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| Using `textwrap`  | 12  | 5933  | 1.66  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| Using `NLTK`  | 105  | 5909  | 13.31  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| Using `spaCy`  | 4  | 5876  | 5.8  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: 'The following is the summary generated by the LLM using the spaCy chunks; these
    summaries are concise and informative, which is what we intended:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This example shows that the `textwrap` approach is the quickest, taking 1.66
    seconds; this does not imply that the `textwrap` approach is always the most suitable
    and the one we should adopt. We have to evaluate this for each situation, depending
    on the kind of information and the use case involved. Let’s explore the decision
    factors required to select the best strategy for chunking.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right strategy
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Whether to use an NLP-based chunking strategy or a fixed-length chunking approach
    depends on the specific requirements and constraints of the task at hand. Table
    7.5 outlines some of the decision factors.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.5 Chunking decision factors
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Decision factor | Description |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: '| Task requirements  | If the task requires understanding the nuances of language,
    such as answering questions that depend on context or generating coherent text,
    NLP-based chunking is preferable.  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| Performance  | If maintaining the context isn’t critical, and there are performance
    constraints, fixed-length chunking could be the better choice.  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '| Resource availability  | Fixed-length chunking is less resource-intensive
    for projects with limited computational resources and is easier to scale.  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: '| Data characteristics  | NLP-based chunking can use those boundaries for text
    with clear linguistic demarcations (such as well-structured documents). In contrast,
    fixed-length chunking might be more practical if the text is poorly structured
    or if the boundaries are unclear.  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: One might begin with a fixed-length method because it is easy and then switch
    to an NLP-based method when more complexity is required. Some advanced systems
    might even employ both, using fixed-length chunking to deal with large amounts
    of text quickly and then using NLP-based chunking for the smaller, more controllable
    chunks to improve the context and meaning. Let’s change topics and see how we
    can chunk other documents, such as PDFs.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 7.8 Chunking PDFs
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At a high level, chunking PDFs is quite similar to chunking sentences. There
    are different options for PDFs that are not too complex and have basic tables
    or images. A simple method to start is to use the PyPDF2 library. PyPDF2 is an
    open source Python PDF library that can perform various operations on PDF pages,
    such as splitting, merging, cropping, and transforming. It can also extract text,
    custom data, passwords, and metadata from PDFs.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 7.7 shows how to use it. We can install PyPDF2 using the following
    command in conda: `conda` `install` `-c` `conda-forge` `pypdf2`, or if using pip,
    then `pip install pypdf2`. This output is text that can be chunked and processed
    like any other text previously discussed. This library doesn’t handle images;
    if any images are in PDF, those will be ignored. Note that the following listing
    only shows the relevant section for brevity; the book’s GitHub repository has
    the complete code.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.7 Extracting text from PDF
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Handling tables and images in PDF
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While handling text in the last example seems quite straightforward, PDFs can
    add a lot of complexity. The accuracy of text extraction depends on the PDF itself,
    given that not all PDFs encode text in a manner that is easily extractable. The
    following listing shows one example of how to process images and tables for chunking
    from PDFs, but overall, this will be challenging.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.8 Example of how to extract tables and images
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Creates an empty string to store the text'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Exports the image as a BMP file'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Opens the BMP file with PIL'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Converts the image to PNG and saves'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Creates an ImageWriter object to save the images'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Gets the first page from the PDF file'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Reads the tables'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Saves each table into a separate file'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Images and tables are difficult to parse and handle in PDFs, especially consistently
    and predictably. One way enterprises can solve this problem is by daisy-chaining
    other ML models instead of just trying to parse documents. Microsoft’s Azure Document
    Intelligence is a service that allows enterprises to implement this.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Azure Document Intelligence
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Azure AI Document Intelligence is a cloud service that employs advanced ML
    models to auto-extract text, key values, tables, and structures from documents,
    converting them into actionable data. It offers three types of ML models for document
    analysis: prebuilt models for common scenarios (e.g., IDs, receipts), custom models
    trained on your data, and document analysis models for structured content extraction.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Unlike many Python PDF-handling packages, it supports multiple document formats,
    handles complex layouts, handwritten text, and objects, and allows for custom
    ML model creation. Integration is simple via a REST API to extract data from documents
    such as PDFs and use an RAG pattern for summarization or answer generation.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 7.9 shows how to analyze a document using a prebuilt layout model.
    We start the analysis of a sample document using the prebuilt layout model and
    iterate the result. It detects the text and tables for each page, including understanding
    rows and columns. Please note that Document Intelligence isn''t available on conda
    yet, so we''ll use pip to install it: `pip install azure-ai-formrecognizer`.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.9 Azure AI Document Intelligence prebuilt layout operations
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Calls the API to analyze a PDF using a pre-build layout'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Gets the results from the analysis'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Iterates through all the pages in the PDF'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Extracts the text on each line on the page'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Examines whether there is a Selection mark'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Parses tables found in the PDF'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: More details on Azure Document Intelligence can be found at [https://mng.bz/6YNA](https://mng.bz/6YNA).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs don’t have any up-to-date information past their training cut-off, and
    they don’t know private and proprietary information. Retrieval-augmented generation
    (RAG) is the technique that helps address these limitations.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG is a powerful technique that provides up-to-date and grounded information,
    leading to improved LLM responses. It also helps ground data and can improve the
    generation of LLMs regarding quality, diversity, and customization.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector searches powered by vector indexes and databases are pivotal in making
    the retriever component of RAG implementations efficient and scalable. They enable
    real-time, large-scale semantic search, essential for applications that require
    rapid access to vast amounts of information.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG must deal with various challenges, with chunking being one of the most critical.
    Due to LLM content window limitations, we need to chunk a large corpus of data
    and employ various techniques for splitting information—fixed length, sliding
    window, punctuation based, sections based, or adaptive. Each has advantages and
    challenges and needs to be considered in the context of the use case, shape, and
    data type.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding and parsing PDFs into chunks is difficult, especially if they
    contain images and tables. Daisy-chaining other ML models, such as Azure Document
    Intelligence, can help simplify this.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combined with prompt engineering, RAG helps address requirements such as dynamic
    information access, cost efficiency, grounding, citation, scalability, and customization
    in various enterprise scenarios and contexts.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合提示工程，RAG有助于解决各种企业场景和环境中如动态信息访问、成本效率、扎根、引用、可扩展性和定制化等方面的需求。
