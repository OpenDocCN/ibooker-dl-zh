- en: Chapter 3\. Deep Learning Development with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have your development environment running and a good understanding
    of tensors and their operations, we can start developing and deploying deep learning
    models with PyTorch. This chapter provides a quick reference to the basic NN development
    process and the PyTorch code needed to execute it.
  prefs: []
  type: TYPE_NORMAL
- en: First we’ll review the overall process, then we’ll dive into each stage and
    look at some sample PyTorch code that implements each function. We’ll build off
    what you learned in [Chapter 2](ch02.xhtml#Chapter_2) to load your data into tensors
    and apply data transforms that convert your tensors to suitable inputs for your
    model.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll build a deep learning model and train the model using a common training
    loop structure. Then, you’ll test your model’s performance and tweak hyperparameters
    to improve your results and training speed. Finally, we’ll explore ways to deploy
    your model to prototype systems or production. At each stage, I’ll provide commonly
    used PyTorch code for you to use as a reference as you develop your own deep learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Future chapters in this book will provide additional examples and cover more
    advanced topics, such as customization, optimization, acceleration, distributed
    training, and advanced deployment. For now, we’ll focus on the basic NN development
    process.
  prefs: []
  type: TYPE_NORMAL
- en: The Overall Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although everyone builds their deep learning models in a different way, the
    overall process is pretty much the same. Regardless of whether you are conducting
    supervised learning with labeled data, unsupervised learning with unlabeled data,
    or semisupervised learning with a mixture of both, a basic pipeline is used to
    train, test, and deploy your deep learning models. I will assume that you have
    some familiarity with deep learning model development, but before we get started,
    let’s review the basic deep learning training process. Then I’ll show how you
    can implement this process in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-1](#fig_basic_process) illustrates the most common tasks in deep
    learning development. The first stage is the data preparation stage, in which
    we will load data from an external source and convert it to the appropriate format
    for model training. This data could be images, videos, speech recordings, audio
    files, text, general tabular data, or any combination of these.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we load this data and convert it to numeric values in the form of tensors.
    The tensors will act as inputs during the model training stage; however, before
    they are passed in, the tensors are usually preprocessed via transforms and grouped
    into batches for better training performance. Thus, the data preparation stage
    takes generic data and converts it to batches of tensors that can be passed into
    your NN model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, in the model experimentation and development stage, we will design an
    NN model, train the model with our training data, test its performance, and optimize
    our hyperparameters to improve performance to a desired level. To do so, we will
    separate our dataset into three parts: one for training, one for validation, and
    one for testing. We’ll design an NN model and train its parameters with our training
    data. PyTorch provides elegantly designed modules and classes in the `torch.nn`
    module to help you create and train your NNs. We will define a loss function and
    optimizer from a selection of the many built-in PyTorch functions. Then we’ll
    perform backpropagation and update the model parameters in our training loop.'
  prefs: []
  type: TYPE_NORMAL
- en: '![“Basic Deep Learning Development Process”](Images/ptpr_0301.PNG)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. The basic deep learning development process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Within each epoch, we’ll also validate our model by passing in validation data,
    measuring performance, and potentially tuning hyperparameters. Finally, we’ll
    test our model by passing in test data and measuring the model’s performance against
    unseen data. In practice, validation and test loops may be optional, but we show
    them here for completeness.
  prefs: []
  type: TYPE_NORMAL
- en: The last stage of deep learning model development is the model deployment stage.
    In this stage, we have a fully trained model—so what do we do with it? If you
    are a deep learning research scientist conducting experiments, you may want to
    simply save the model to a file and load it for further research and experimentation,
    or you may want to provide access to it via a repository like PyTorch Hub. You
    may also want to deploy it to an edge device or local server to demonstrate a
    prototype or a proof of concept.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if you are a software developer or systems engineer, you
    may want to deploy your model to a product or service. In this case, you can deploy
    your model to a production environment on a cloud server or deploy it to an edge
    device or mobile phone. When deploying trained models, the model often requires
    additional postprocessing. For example, you may classify a batch of images, but
    you only want to report the most confident result. The model deployment stage
    also handles any postprocessing that is needed to go from your model’s output
    values to the final solution.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve explored the overall development process, let’s dive into each
    part and show how PyTorch can help you develop deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first stage of deep learning development starts with data preparation. In
    this stage, we acquire data to train and test our NN models and convert it to
    a tensor of numbers that our PyTorch models can process. The size of the dataset
    and the data itself are important to developing good models; however, generating
    good datasets is beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, I’ll assume that you’ve already determined the data is good,
    so I’ll focus on describing how to load the data, apply transforms, and batch
    the data using PyTorch’s built-in capabilities. First I’ll show how you can prepare
    image data with the `torchvision` package, then we’ll explore PyTorch resources
    for preparing other types of data.
  prefs: []
  type: TYPE_NORMAL
- en: Data Loading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch provides powerful built-in classes and utilities, such as the `Dataset`,
    `DataLoader`, and `Sampler` classes, for loading various types of data. The `Dataset`
    class defines how to access and preprocess data from a file or data sources. The
    `Sampler` class defines how to sample data from a dataset in order to create batches,
    while the `DataLoader` class combines a dataset with a sampler and allows you
    to iterate over a set of batches.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch libraries such as Torchvision and Torchtext also provide classes to
    support specialized data like computer vision and natural language data. The `torchvision.datasets`
    module is a good example of how to utilize built-in classes to load data. The
    `torchvision.datasets` module provides a number of subclasses to load image data
    from popular academic datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of these popular datasets is CIFAR-10\. The CIFAR-10 dataset was collected
    by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton during their research for
    the Canadian Institute for Advanced Research (CIFAR). It consists of 50,000 training
    images and 10,000 test images of 10 possible objects: airplanes, cars, birds,
    cats, deer, dogs, frogs, horses, ships, and trucks. The following code shows how
    to use CIFAR-10 to create a training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `train` parameter determines whether we load the training data or the testing
    data, and setting `download` to `True` will download the data for us if we don’t
    have it already.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore the `train_data` dataset object. We can access information about
    the dataset using its methods and attributes as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#comarker11)'
  prefs: []
  type: TYPE_NORMAL
- en: Printing the object returns its general information.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#comarker22)'
  prefs: []
  type: TYPE_NORMAL
- en: Check the number of data samples with `len()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#comarker33)'
  prefs: []
  type: TYPE_NORMAL
- en: The data is a NumPy array of 50,000 32 × 32-pixel color images.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#comarker44)'
  prefs: []
  type: TYPE_NORMAL
- en: The targets are a list of 50,000 data labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#comarker55)'
  prefs: []
  type: TYPE_NORMAL
- en: You can map numeric labels to class names using `classes`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#comarker66)'
  prefs: []
  type: TYPE_NORMAL
- en: You can map class names to index values using `class_to_idx`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a closer look at the `train_data` dataset’s data and labels. We
    can access a data sample using an index, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the code, `train_data[0]` returns a tuple with two elements—the
    data and the label. Let’s examine the data first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The data consists of a PIL image object. PIL is a common image format that uses
    the Pillow library to store image pixel values in the format of height × width
    × channels. A color image has three channels (RGB) for red, green, and blue. The
    data format is good to know because we may need to convert this format for our
    model if the model expects a different format (more on this later).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-2](#fig_pil_image) shows the PIL image. It’s a little blurry because
    the resolution is only 32 × 32, but can you tell what it is?'
  prefs: []
  type: TYPE_NORMAL
- en: '![“Sample Image”](Images/ptpr_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Sample image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s examine the label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the code, the `label` is an integer value representing the class of the image
    (e.g., airplane, dog, etc.). We can use the `classes` attribute to see that an
    index of 6 corresponds to a frog.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also load the test data into another dataset object called `test_data`.
    Changing the root folder and setting the `train` flag to `False` will do the trick,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `test_data` dataset is similar to the `train_data` dataset. However, there
    are only 10,000 images in the test dataset. Try accessing some of the methods
    from the dataset class and the attributes on the `test_data` dataset yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Data Transforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the data loading step, we pulled data from its source and created dataset
    objects that contain information about the dataset and the data itself. However,
    the data might need to be adjusted before it is passed into the NN model for training
    and testing. For example, data values may be normalized to assist training, augmented
    to create larger datasets, or converted from one type of object to a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: These adjustments are accomplished by applying *transforms*. The beauty of using
    transforms in PyTorch is that you can define a sequence of transforms and apply
    it when the data is accessed. Later, in [Chapter 5](ch05.xhtml#Chapter_5), you’ll
    see how you can even apply transforms on a CPU in parallel with your training
    on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code example, we’ll define our transforms and create our `train_data`
    dataset using these transforms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The mean and standard deviation values here were predetermined based on the
    dataset itself.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Set the `transform` parameter when creating the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We define a set of transforms using the `transforms.Compose()` class. This class
    accepts a list of transforms and applies them in sequence. Here we randomly crop
    and flip images, convert them to tensors, and normalize the tensor values to predetermined
    means and standard deviations.
  prefs: []
  type: TYPE_NORMAL
- en: The transforms are passed to the dataset class during instantiation and become
    part of the dataset object. The transforms are applied whenever the dataset object
    is accessed, returning a new result consisting of the transformed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can view the transforms by printing the dataset or its `transforms` attribute,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can access the data using indexing, as shown in the next code block. PyTorch
    automatically applies the transforms when the data is accessed, so the output
    data will be different from what we saw earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the data output is now a tensor of size 3 × 32 × 32\. It has
    also been randomly cropped, horizontally flipped, and normalized. [Figure 3-3](#fig_image_transformed)
    shows the image after applying the transforms.
  prefs: []
  type: TYPE_NORMAL
- en: '![“Image After Transforms”](Images/ptpr_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Image after transforms
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The colors may look strange because of the normalization, but this actually
    helps NN models do a better job of classifying the images.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define a different set of transforms for testing and apply them to our
    test data as well. In the case of test data, we do not want to crop or flip the
    image, but we do need to convert the image to tensors and normalize the tensor
    values, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Data Batching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have defined the transforms and created the datasets, we can access
    data samples one at a time. However, when you train your model, you will want
    to pass in small batches of data at each iteration, as we will see in [“Model
    Development”](#model-dev-section). Sending data in batches not only allows more
    efficient training but also takes advantage of the parallel nature of GPUs to
    accelerate training.
  prefs: []
  type: TYPE_NORMAL
- en: Batch processing can easily be implemented using the `torch.utils.data.DataLoader`
    class. Let’s start with an example of how Torchvision uses this class, and then
    we’ll cover it in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we create a dataloader for `train_data` that we can
    use to load a batch of samples and apply our transforms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We use a batch size of 16 samples and shuffle our dataset so that the dataloader
    retrieves a random sampling of the data.
  prefs: []
  type: TYPE_NORMAL
- en: The dataloader object combines a dataset and a sampler, and provides an iterable
    over the given dataset. In other words, your training loop can use this object
    to sample your dataset and apply transforms one batch at a time instead of applying
    them for the complete dataset at once. This considerably improves efficiency and
    speed when training and testing models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to retrieve a batch of samples from the `trainloader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We need to use `iter()` to cast the `trainloader` to an iterator and then use
    `next()` to iterate over the data one more time. This is only necessary when accessing
    one batch. As we’ll see later, our training loops will access the dataloader directly
    without the need for `iter()` and `next()`. After checking the sizes of the data
    and labels, we see they return batches of size 16.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a dataloader for our `test_data` dataset as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we set `shuffle` to `False` since there’s usually no need to shuffle the
    test data and researchers like to see repeatable test results.
  prefs: []
  type: TYPE_NORMAL
- en: General Data Preparation (torch.utils.data)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, I’ve shown you how to load, transform, and batch image data using Torchvision.
    However, you can use PyTorch to prepare other types of data as well. PyTorch libraries
    such as Torchtext and Torchaudio provide dataset and dataloader classes for text
    and audio data, and new external libraries are being developed all the time.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch also provides a submodule called `torch.utils.data` that you can use
    to create your own dataset and dataloader classes like the ones you saw in Torchvision.
    It consists of `Dataset`, `Sampler`, and `DataLoader` classes.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset classes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyTorch supports map- and iterable-style dataset classes. A *map-style dataset*
    is derived from the abstract class `torch.utils.data.Dataset`. It implements the
    `getitem()` and `len()` functions, and represents a map from (possibly nonintegral)
    indices/keys to data samples. For example, such a dataset, when accessed with
    `dataset[idx]`, could read the idx-th image and its corresponding label from a
    folder on the disk. Map-style datasets are more commonly used than iterable-style
    datasets, and all datasets that represent a map made from keys or data samples
    should use this subclass.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The simplest way to create your own dataset class is to subclass the map-style
    `torch.utils.data.Dataset` class and override the `getitem()` and `len()` functions
    with your own code.
  prefs: []
  type: TYPE_NORMAL
- en: All subclasses should overwrite `getitem()`, which fetches a data sample for
    a given key. Subclasses can also optionally overwrite `len()`, which returns the
    size of the dataset by many `Sampler` implementations and the default options
    of `DataLoader`.
  prefs: []
  type: TYPE_NORMAL
- en: An *iterable-style dataset*, on the other hand, is derived from the `torch.utils.data.IterableDataset`
    abstract class. It implements the `iter()` protocol and represents an iterable
    over data samples. This type of dataset is typically used when reading data from
    a database or a remote server, as well as data generated in real time. Iterable
    datasets are useful when random reads are expensive or uncertain, and when the
    batch size depends on fetched data.
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch’s `torch.utils.data` submodule also provides dataset operations to
    convert, combine, or split dataset objects. These operations include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TensorDataset(*tensors*)`'
  prefs: []
  type: TYPE_NORMAL
- en: Creates a dataset object from a tensor
  prefs: []
  type: TYPE_NORMAL
- en: '`ConcatDataset(*datasets*)`'
  prefs: []
  type: TYPE_NORMAL
- en: Creates a dataset from multiple datasets
  prefs: []
  type: TYPE_NORMAL
- en: '`ChainDataset(*datasets*)`'
  prefs: []
  type: TYPE_NORMAL
- en: Chains multiple `IterableDatasets`
  prefs: []
  type: TYPE_NORMAL
- en: '`Subset(*dataset*, *indices*)`'
  prefs: []
  type: TYPE_NORMAL
- en: Creates a subset of a dataset from specified indices
  prefs: []
  type: TYPE_NORMAL
- en: Sampler classes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to dataset classes PyTorch also provides sampler classes, which
    offer a way to iterate over indices of dataset samples. Sampler are derived from
    the `torch.utils.data.Sampler` base class.
  prefs: []
  type: TYPE_NORMAL
- en: Every `Sampler` subclass needs to implement an `iter()` method to provide a
    way to iterate over indices of dataset elements and a `len()` method that returns
    the length of the returned iterators. [Table 3-1](#table_dataset_samplers) provides
    a list of available samplers for your reference.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-1\. Dataset samplers (`torch.utils.data`)
  prefs: []
  type: TYPE_NORMAL
- en: '| Sampler | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `SequentialSampler(`*`data_source`*`)` | Samples data in sequence |'
  prefs: []
  type: TYPE_TB
- en: '| `RandomSampler(`*`data_source, replacement=False,`* *`num_samples=None,`*
    *`generator=None`*`)` | Samples data randomly |'
  prefs: []
  type: TYPE_TB
- en: '| `SubsetRandomSampler(`*`indices,`* *`generator=None`*`)` | Samples data randomly
    from a subset of the dataset |'
  prefs: []
  type: TYPE_TB
- en: '| `WeightedRandomSampler(`*`weights,`* *`num_samples,`* *`replacement=True,`*
    *`generator=None`*`)` | Samples randomly from a weighted distribution |'
  prefs: []
  type: TYPE_TB
- en: '| `BatchSampler(`*`sampler, batch_size, drop_last`*`)` | Returns a batch of
    samples |'
  prefs: []
  type: TYPE_TB
- en: '| `distributed.DistributedSampler(`*`dataset,`* *`num_replicas=None,`* *`rank=None,`*
    *`shuffle=True,`* *`seed=0`*`)` | Samples across distributed datasets |'
  prefs: []
  type: TYPE_TB
- en: Samplers are usually not used directly. They are often passed to dataloaders
    to define the way the dataloader samples the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: DataLoader classes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `Dataset` class returns a dataset object that includes data and information
    about the data. The `Sampler` class returns the actual data itself in a specified
    or random fashion. The `DataLoader` class combines a dataset with a sampler and
    returns an iterable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset and sampler objects are not iterables, meaning you cannot run a
    `for` loop on them. The dataloader object solves this problem. We used the `DataLoader`
    class to construct a dataloader object for our CIFAR-10 example earlier in this
    chapter. The `DataLoader` prototype is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `dataset`, `batch_size`, `shuffle`, and `sampler` parameters are the most
    commonly used. The `num_workers` parameter is often used to increase the number
    of CPU processes that generate batches in parallel. The rest of the parameters
    are used only for advanced cases.
  prefs: []
  type: TYPE_NORMAL
- en: If you write your own dataset class, all you need to do is call the built-in
    `DataLoader` to generate an iterable for your data. There is no need to create
    a dataloader class from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: This section provided a quick reference to the data preparation capabilities
    of PyTorch. Now that you understand how you can load, transform, and batch your
    data with PyTorch, you can begin to use your data to develop and train deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Model Development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most research and development is focused on developing new and innovative deep
    learning models. The model development process consists of several steps. At this
    point, I assume that you have created good datasets and have prepared them for
    processing by your model.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in the process is model design, in which you design one or more
    model architectures and initialize the model’s parameters (e.g., weights and biases.)
    It’s common practice to start with an existing design and then modify it or create
    your own. I’ll show you how to do both in this section.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is training. During training you’ll pass training data through
    your model, measure the error or loss, and adjust the parameters to improve the
    results.
  prefs: []
  type: TYPE_NORMAL
- en: During validation, you’ll measure the performance of your model against validation
    data that was not used during training. This helps to guard against *overfitting*,
    where the model performs well against training data but does not generalize to
    other input data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the model development process often concludes with testing. Testing
    is when you measure the performance of your trained model against previously unseen
    data. This section provides a quick reference on how to accomplish the steps and
    substeps of model development in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Model Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model design research has expanded significantly over the past decade, in all
    industries and fields. Thousands of papers are written every year in areas like
    computer vision, natural language processing, speech recognition, and audio processing
    to solve problems such as early cancer detection and innovate new technologies
    such as self-driving cars. As a result, there are many different types of model
    architectures to choose from, depending on the problem you’re trying to solve.
    You may even create some of your own!
  prefs: []
  type: TYPE_NORMAL
- en: Using existing and pretrained models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most users begin model development by selecting an existing model. Maybe you
    would like to start off with an existing design and make minor modifications or
    experiment with small improvements before designing your own architecture. You
    can also use models or parts of an existing model that have already been trained
    with tons of data.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch provides many resources to leverage existing model designs and pretrained
    NNs. One example resource is the PyTorch-based `torchvision` library for computer
    vision. The `torchvision.models` subpackage contains definitions of models for
    addressing different tasks, including image classification, pixelwise semantic
    segmentation, object detection, instance segmentation, person keypoint detection,
    and video classification.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we want to use the famous VGG16 model for our design. VGG16 (also
    called OxfordNet) is a convolutional NN architecture named after the Visual Geometry
    Group from Oxford, who developed it. It was submitted to the Large Scale Visual
    Recognition Challenge in 2014 and achieved 92.7% top-5 test accuracy on ImageNet,
    a very large dataset of 14 million hand-annotated images.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily create a pretrained VGG16 model as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: By default, the model will be untrained and have randomly initialized weights.
    However, in our situation we want to use a pretrained model, so we set `pretrained
    = True`. This downloads the weights that were pretrained with the ImageNet dataset
    and initializes our model’s weights with these values.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can view the sequence of layers contained in the VGG16 model by printing
    the model. The VGG16 model consists of three parts: `features`, `avgpool`, and
    `classifier`. It’s too large to print all the layers here, so we’ll just print
    the `classifier` part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`Linear`, `ReLU`, and `Dropout` are `torch.nn` modules. `torch.nn` is used
    to create NN layers, activations, loss functions, and other NN components. Don’t
    worry about it too much right now; we’ll cover it in more detail in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many famous untrained and pretrained models available, including AlexNet,
    VGG, ResNet, Inception, and MobileNet, to name a few. Refer to [the Torchvision
    model documentation](https://pytorch.tips/torchvision-models) for a complete list
    of models and details regarding their use.
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch Hub is another excellent resource for existing and pretrained PyTorch
    models. You can load models from another repository using the `torch.hub.load()`
    API. The following code shows how you would load a model from PyTorch Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here we load a model called WaveGlow that is used to generate speech from the
    NVIDIA DeepLearningExamples repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find a list of PyTorch Hub repositories at [the main PyTorch Hub site.](https://pytorch.tips/pytorch-hub)
    To explore all the available API endpoints of a particular repository you can
    use the `torch.hub.list()` function on the repository, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This lists all the models available in the *nvidia/DeepLearningExamples:torchhub*
    repo, including WaveGlow, Tacotron 2, SSD, and others. Try using `hub.list()`
    on other repositories that support PyTorch Hub to see what other preexisting models
    you can find.
  prefs: []
  type: TYPE_NORMAL
- en: Loading preexisting and pretrained models from Python libraries like Torchvision
    and from repositories through PyTorch Hub allows you to build off previous research
    for your own work. Later in this chapter, I will show you how to deploy your models
    to packages and repositories so that others can access or build off your own research
    and development.
  prefs: []
  type: TYPE_NORMAL
- en: The PyTorch NN module (torch.nn)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the most powerful features of PyTorch is its Python module `torch.nn`,
    which makes it easy to design and experiment with new models. The following code
    illustrates how you can create a simple model with `torch.nn`. In this example,
    we will create a fully connected model called SimpleNet. It consists of an input
    layer, a hidden layer, and an output layer that takes in 2,048 input values and
    returns 2 output values for classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Typically creates layers as class attributes
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Calls the base class’s `__init__()` function to initialize parameters
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Required to define how the model processes data
  prefs: []
  type: TYPE_NORMAL
- en: Creating a model in PyTorch is said to be very “Pythonic,” meaning it creates
    objects in the preferred Python fashion. We first create a new subclass called
    `SimpleNet` that inherits from the `nn.Module` class, and then we define the `__init__()`
    and `forward()` methods. The `__init__()` function initializes the model parameters
    and the `forward()` function defines how data is passed through our model.
  prefs: []
  type: TYPE_NORMAL
- en: In `__init__()`, we call the `super()` function to execute the parent `nn.Module`
    class’s `__init__()` method to initialize the class parameters. Then we define
    some layers using the `nn.Linear` module.
  prefs: []
  type: TYPE_NORMAL
- en: The `forward()` function defines how data is passed through the network. In
    the `forward()` function, we first use `view()` to reshape the input into a 2,048-element
    vector, then we process the input through each layer and apply `relu()` activation
    functions. Finally, we apply the `softmax()` function and return the output.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PyTorch uses the term *module* to describe an NN layer or block. Python uses
    this term to describe a library package that you can import. In this book, I’ll
    stick to the PyTorch usage and will use the term *Python module* to describe a
    Python library module.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve defined what layers or modules are contained in our SimpleNet
    model, how they are connected, and how the parameters are initialized (through
    `super().init()`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to create the model by instantiating the model
    object, called `simplenet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate or create the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Run data through the model (forward pass).
  prefs: []
  type: TYPE_NORMAL
- en: If we print the model, we can see how it’s structured. Executing our model is
    as simple as calling the model object as a function. We pass in the inputs, and
    the model runs the forward pass and returns the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'This simple model demonstrates the following decisions you need to make during
    model design:'
  prefs: []
  type: TYPE_NORMAL
- en: Module definition
  prefs: []
  type: TYPE_NORMAL
- en: How will you define the layers of your NN? How will you combine these layers
    into building blocks? In the example, we chose three linear or fully connected
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs: []
  type: TYPE_NORMAL
- en: Which activation functions will you use at the end of each layer or module?
    In the example, we chose to use `relu` activation for the input and hidden layers
    and `softmax` for the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Module connections
  prefs: []
  type: TYPE_NORMAL
- en: How will your modules be connected to each other? In the example, we chose to
    simply connect each linear layer in sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Output selection
  prefs: []
  type: TYPE_NORMAL
- en: What output values and formats will be returned? In this example, we return
    two values from the `softmax()` function.
  prefs: []
  type: TYPE_NORMAL
- en: The simplicity, flexibility, and Pythonic nature of this paradigm are what make
    PyTorch so popular for deep learning research. PyTorch’s `torch.nn` Python module
    includes classes for creating the building blocks, layers, and activation functions
    required for NN model design. Let’s walk through the different types of building
    blocks available in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 3-2](#table_nn_containers) provides a list of *NN containers*. You can
    use the container classes to create higher-level sets of building blocks. For
    example, you can use `Sequential` to create a sequence of layers in one block.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-2\. PyTorch NN containers
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Module` | The base class for all NN modules |'
  prefs: []
  type: TYPE_TB
- en: '| `Sequential` | A sequential container |'
  prefs: []
  type: TYPE_TB
- en: '| `ModuleList` | A container that holds submodules in a list |'
  prefs: []
  type: TYPE_TB
- en: '| `ModuleDict` | A container that holds submodules in a dictionary |'
  prefs: []
  type: TYPE_TB
- en: '| `ParameterList` | A container that holds parameters in a list |'
  prefs: []
  type: TYPE_TB
- en: '| `ParameterDict` | A container that holds parameters in a dictionary |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`nn.Module` is the base class for all NN building blocks. Your NN may consist
    of a single module or multiple modules containing other modules that may also
    contain modules, creating a hierarchy of building blocks.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 3-3](#table_nn_linear) lists a few *linear layers* supported by `torch.nn`.
    `Linear` is commonly used for fully connected layers.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-3\. PyTorch NN linear layers
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Identity` | A placeholder identity operator that is argument-insensitive
    |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Linear` | A layer that applies a linear transformation to the incoming
    data |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Bilinear` | A layer that applies a bilinear transformation to the incoming
    data |'
  prefs: []
  type: TYPE_TB
- en: '[Table 3-4](#table_nn_convolutional) lists several *convolutional layers* supported
    by `torch.nn`. Convolutional layers are used often in deep learning to apply filters
    to data at various stages. As you can see in the table, PyTorch has built-in support
    for 1D, 2D, and 3D convolutions as well as transposed and folded variations.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-4\. PyTorch NN convolutional layers
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Conv1d` | Applies a 1D convolution over an input signal composed of several
    input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Conv2d` | Applies a 2D convolution over an input signal composed of several
    input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Conv3d` | Applies a 3D convolution over an input signal composed of several
    input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.ConvTranspose1d` | Applies a 1D transposed convolution operator over
    an input image composed of several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.ConvTranspose2d` | Applies a 2D transposed convolution operator over
    an input image composed of several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.ConvTranspose3d` | Applies a 3D transposed convolution operator over
    an input image composed of several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Unfold` | Extracts sliding local blocks from a batched-input tensor |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Fold` | Combines an array of sliding local blocks into a large containing
    tensor |'
  prefs: []
  type: TYPE_TB
- en: '[Table 3-5](#table_nn_pooling) shows the *pooling layers* available in `torch.nn`.
    Pooling is often used to downsample or reduce the complexity of output layers.
    PyTorch supports 1D, 2D, and 3D pooling and max or average pooling methods, including
    their adaptive variations.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-5\. PyTorch NN pooling layers
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.MaxPool1d` | Applies a 1D max pooling over an input signal composed of
    several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.MaxPool2d` | Applies a 2D max pooling over an input signal composed of
    several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.MaxPool3d` | Applies a 3D max pooling over an input signal composed of
    several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.MaxUnpool1d` | Computes a partial inverse of `MaxPool1d` |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.MaxUnpool2d` | Computes a partial inverse of `MaxPool2d` |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.MaxUnpool3d` | Computes a partial inverse of `MaxPool3d` |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.AvgPool1d` | Applies a 1D average pooling over an input signal composed
    of several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.AvgPool2d` | Applies a 2D average pooling over an input signal composed
    of several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.AvgPool3d` | Applies a 3D average pooling over an input signal composed
    of several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.FractionalMaxPool2d` | Applies a 2D fractional max pooling over an input
    signal composed of several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.LPPool1d` | Applies a 1D power-average pooling over an input signal composed
    of several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.LPPool2d` | Applies a 2D power-average pooling over an input signal composed
    of several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.AdaptiveMaxPool1d` | Applies a 1D adaptive max pooling over an input
    signal composed of several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.AdaptiveMaxPool2d` | Applies a 2D adaptive max pooling over an input
    signal composed of several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.AdaptiveMaxPool3d` | Applies a 3D adaptive max pooling over an input
    signal composed of several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.AdaptiveAvgPool1d` | Applies a 1D adaptive average pooling over an input
    signal composed of several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.AdaptiveAvgPool2d` | Applies a 2D adaptive average pooling over an input
    signal composed of several input planes |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.AdaptiveAvgPool3d` | Applies a 3D adaptive average pooling over an input
    signal composed of several input planes |'
  prefs: []
  type: TYPE_TB
- en: '[Table 3-6](#table_nn_padding) lists the available *padding layers*. Padding
    fills in missing data when the layer outputs increase in size. PyTorch supports
    1D, 2D, and 3D padding, and can pad your data with reflections, replications,
    zeros, or constants.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-6\. PyTorch NN padding layers
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.ReflectionPad1d` | Pads the input tensor using the reflection of the
    input boundary |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.ReflectionPad2d` | Pads the input tensor using the reflection of the
    input boundary for 2D inputs |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.ReplicationPad1d` | Pads the input tensor using the replication of the
    input boundary |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.ReplicationPad2d` | Pads the input tensor using the replication of the
    input boundary for 2D inputs |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.ReplicationPad3d` | Pads the input tensor using the replication of the
    input boundary for 3D inputs |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.ZeroPad2d` | Pads the input tensor boundaries with zeros |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.ConstantPad1d` | Pads the input tensor boundaries with a constant value
    |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.ConstantPad2d` | Pads the input tensor boundaries with a constant value
    for 2D inputs |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.ConstantPad3d` | Pads the input tensor boundaries with a constant value
    for 3D inputs |'
  prefs: []
  type: TYPE_TB
- en: '[Table 3-7](#table_nn_dropout) lists the available layers for *dropout*. Dropout
    is often used to reduce complexity, speed up training, and introduce some regularization
    to prevent overfitting. PyTorch supports dropout for 1D, 2D, and 3D layers, and
    provides support for alpha dropout as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-7\. PyTorch NN dropout layers
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Dropout` | During training, randomly zeros out some of the elements of
    the input tensor with probability *p* using samples from a Bernoulli distribution
    |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Dropout2d` | Randomly zeros out entire channels for 2D inputs |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Dropout3d` | Randomly zeros out entire channels for 3D inputs |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.AlphaDropout` | Applies alpha dropout over the input |'
  prefs: []
  type: TYPE_TB
- en: '[Table 3-8](#table_nn_normalization) provides a list of classes that support
    *normalization*. Normalization is performed between some layers to prevent vanishing
    or exploding gradients by keeping intermediate layer inputs within a certain range.
    It can also help speed up the training process. PyTorch supports normalization
    for 1D, 2D, and 3D inputs and provides normalization methods such as batch, instance,
    group, and sync normalization.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-8\. PyTorch NN normalization layers
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.BatchNorm1d` | Applies batch normalization over a 2D or 3D input (a mini-batch
    of 1D inputs with an optional additional channel dimension), as described in the
    paper “Batch Normalization: Accelerating Deep Network Training by Reducing Internal
    Covariate Shift” |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.BatchNorm2d` | Applies batch normalization over a 4D input (a mini-batch
    of 2D inputs with an additional channel dimension), as described in the paper
    “Batch Normalization” |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.BatchNorm3d` | Applies batch normalization over a 5D input (a mini-batch
    of 3D inputs with an additional channel dimension), as described in the paper
    “Batch Normalization” |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.GroupNorm` | Applies group normalization over a mini-batch of inputs
    as described in the paper “Group Normalization” |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.SyncBatchNorm` | Applies batch normalization over an *n*-dimensional
    input (a mini-batch of [*n*–2]D inputs with an additional channel dimension),
    as described in the paper “Batch Normalization” |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.InstanceNorm1d` | Applies instance normalization over a 3D input (a mini-batch
    of 1D inputs with an optional additional channel dimension), as described in the
    paper “Instance Normalization: The Missing Ingredient for Fast Stylization” |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.InstanceNorm2d` | Applies instance normalization over a 4D input (a mini-batch
    of 2D inputs with an additional channel dimension), as described in the paper
    “Instance Normalization” |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.InstanceNorm3d` | Applies instance normalization over a 5D input (a mini-batch
    of 3D inputs with an additional channel dimension), as described in the paper
    “Instance Normalization” |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.LayerNorm` | Applies layer normalization over a mini-batch of inputs,
    as described in the paper “Layer Normalization” |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.LocalResponseNorm` | Applies local response normalization over an input
    signal composed of several input planes, in which channels occupy the second dimension
    |'
  prefs: []
  type: TYPE_TB
- en: '[Table 3-9](#table_nn_recurrent) shows the *recurrent layers* used for recurrent
    neural networks (RNNs). RNNs are often used to process time series or sequence-based
    data. PyTorch has built-in support for RNN, long short-term memory (LSTM), and
    gated recurrent unit (GRU) layers as well as classes for RNN, LSTM, and GRU individual
    cells.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-9\. PyTorch NN recurrent layers
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.RNNBase` | The RNN base class |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.RNN` | A layer that applies a multilayer Elman RNN with \Tanh or ReLU
    nonlinearity to an input sequence |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.LSTM` | A layer that applies a multilayer LSTM RNN to an input sequence
    |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.GRU` | A layer that applies a multilayer GRU RNN to an input sequence
    |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.RNNCell` | An Elman RNN cell with tanh or ReLU nonlinearity |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.LSTMCell` | An LSTM cell |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.GRUCell` | A GRU cell |'
  prefs: []
  type: TYPE_TB
- en: '[Table 3-10](#table_nn_transformer) lists the *transformer layers* used for
    transformer networks. Transformer networks are often considered the state of the
    art for processing sequence data. PyTorch supports the complete `Transformer`
    model class in addition to providing the `Encoder` and `Decoder` submodules in
    stack and layer formats.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-10\. PyTorch NN transformer layers
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Transformer` | A transformer model |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.TransformerEncoder` | A stack of *N* encoder layers |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.TransformerDecoder` | A stack of *N* decoder layers |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.TransformerEncoderLayer` | A layer made up of a self-attention (attn)
    and feed-forward network |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.TransformerDecoderLayer` | A layer made up of a self-attn, multihead-attn,
    and feed-forward network |'
  prefs: []
  type: TYPE_TB
- en: '[Table 3-11](#table_nn_sparse) contains a list of *sparse layers*. PyTorch
    provides built-in support for text data embeddings as well as sparse layers for
    cosine similarity and pairwise distance, often used in recommendation engine algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-11\. PyTorch NN sparse layers and distance functions
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Embedding` | Stores the embeddings of a fixed dictionary and size |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.EmbeddingBag` | Computes sums or means of “bags” of embeddings without
    instantiating the intermediate embeddings |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.CosineSimilarity` | Returns the cosine similarity between *x*[1] and
    *x*[2​] computed along a dimension |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.PairwiseDistance` | Computes the batchwise pairwise distance between
    the vectors *v*[1] and *v*[2] using the *p*-norm |'
  prefs: []
  type: TYPE_TB
- en: '[Table 3-12](#table_nn_vision) contains a list of *vision layers* to support
    computer vision. They include layers to shuffle pixels and perform several upsampling
    algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-12\. PyTorch NN vision layers
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.PixelShuffle` | Rearranges elements in a tensor of shape (∗, <math alttext="upper
    C times r squared"><mrow><mi>C</mi> <mo>×</mo> <msup><mi>r</mi> <mn>2</mn></msup></mrow></math>
    , *H*, *W*) to a tensor of shape (∗, *C*, <math alttext="upper H times r"><mrow><mi>H</mi>
    <mo>×</mo> <mi>r</mi></mrow></math> , <math alttext="upper W times r"><mrow><mi>W</mi>
    <mo>×</mo> <mi>r</mi></mrow></math> ) |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Upsample` | Upsamples the given multichannel 1D (temporal), 2D (spatial),
    or 3D (volumetric) data |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.UpsamplingNearest2d` | Applies a 2D nearest neighbor upsampling to an
    input signal composed of several input channels |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.UpsamplingBilinear2d` | Applies a 2D bilinear upsampling to an input
    signal composed of several input channels |'
  prefs: []
  type: TYPE_TB
- en: '[Table 3-13](#table_nn_activations) provides a list of all the *activations*
    available in `torch.nn`. Activation functions are often applied to layer outputs
    to introduce nonlinearities into a model. PyTorch supports traditional activations
    such as sigmoid, tanh, softmax, and ReLU as well as more recent functions such
    as leaky ReLU. More functions are being added as researchers design and apply
    new activations in their publications.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-13\. PyTorch NN nonlinear activations
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.ELU` | Applies the exponential linear unit function element-wise |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Hardshrink` | Applies the hard shrinkage function element-wise |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Hardsigmoid` | Applies the hard sigmoid function element-wise |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Hardtanh` | Applies the hardtanh function element-wise |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Hardswish` | Applies the hardswish function element-wise |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.LeakyReLU` | Applies the leaky rectified linear unit function element-wise
    |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.LogSigmoid` | Applies the logarithmic sigmoid function element-wise |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.MultiheadAttention` | Allows the model to jointly attend to information
    from different representation subspaces |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.PReLU` | Applies the parametric rectified linear unit function element-wise
    |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.ReLU` | Applies the rectified linear unit function element-wise |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.ReLU6` | Applies the rectified linear unit function with a maximum |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.RReLU` | Applies the randomized leaky rectified liner unit function element-wise
    |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.SELU` | Applies the scaled exponential linear unit function element-wise
    |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.CELU` | Applies the continuously differentiable exponential linear unit
    function element-wise |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.GELU` | Applies the Gaussian error linear unit function |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Sigmoid` | Applies the sigmoid function element-wise |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Softplus` | Applies the softplus function element-wise |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Softshrink` | Applies the soft shrinkage function element-wise |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Softsign` | Applies the softsign function element-wise |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Tanh` | Applies the hyperbolic tangent function element-wise |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Tanhshrink` | Applies the hyperbolic tangent function with shrinkage
    element-wise |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Threshold` | Establishes the threshold of each element of the input tensor
    |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Softmin` | Applies the softmin function to an *n*-dimensional input tensor
    to rescale them so the elements of the *n*-dimensional output tensor lie in the
    range [0, 1] and sum to 1 |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Softmax` | Applies the softmax function to an *n*-dimensional input tensor
    to rescale them so the elements of the *n*-dimensional output tensor lie in the
    range [0,1] and sum to 1 |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Softmax2d` | Applies the softmax function to features in each spatial
    location |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.LogSoftmax` | Applies the log(softmax(*x*)) function to an *n*-dimensional
    input tensor |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.AdaptiveLogSoftmaxWithLoss` | Gives an efficient softmax approximation,
    as described in “Efficient Softmax Approximation for GPUs” by Edouard Grave et
    al. |'
  prefs: []
  type: TYPE_TB
- en: As you can see, the PyTorch `torch.nn` module supports a robust set of NN layers
    and activation functions. You can use its classes to create everything from simple
    sequential models to complex multiple hierarchical networks, generative adversarial
    networks (GANs), transformer networks, RNNs, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to design your model, let’s explore how you can train
    and test your own NN model designs with PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During model design, you defined your NN modules, their parameters, and how
    they are connected to each other. In PyTorch, your model design is implemented
    as a model object derived from the `torch.nn.Module` class. You can call the object
    to pass data into the model and generate outputs based on the model architecture
    and the current values of its parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The next step in model development is to train your model with your training
    data. Training a model involves nothing more than estimating the model’s parameters,
    passing in data, and adjusting the parameters to achieve a more accurate representation
    of how the data is generally modeled.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, you set the parameters to some values, pass through data, and
    then compare the model’s outputs with true outputs to measure the error. The goal
    is to change the parameters and repeat the process until the error is minimized
    and the model’s outputs are the same as the true outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Fundamental training loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the key advantages of PyTorch over other machine learning frameworks
    is its flexibility, especially when creating customized training loops. In this
    chapter, we’ll explore a fundamental training loop commonly used for supervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will train the LeNet5 model with the CIFAR-10 dataset that
    we used earlier in this chapter. The LeNet5 model is a simple convolutional NN
    developed by Yann LeCun and his team at Bell Labs in the 1990s to classify hand-written
    digits. (Unbeknownst to me at the time, I actually worked for Bell Labs in the
    same building in Holmdel, NJ, while this work was being performed.)
  prefs: []
  type: TYPE_NORMAL
- en: 'A modernized version of the LeNet5 model can be created using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Define the model class.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Use a GPU if it’s available.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Create the model and move it to a GPU (if available).
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding code, our LeNet5 model uses two convolutional layers
    and three fully connected or linear layers. It has been modernized with max pooling
    and ReLU activations. We’ll also utilize a GPU for training in this example, if
    we can, to speed up training. Here, we create the model object, called `model`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to define the loss function (which is also called the *criterion*)
    and the optimizer algorithm. The loss function determines how we measure the performance
    of our model and computes the loss or error between predictions and truth. We’ll
    attempt to minimize the loss by adjusting the model parameters during training.
    The optimizer defines how we update our model’s parameters during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'To define the loss function and the optimizer, we use the `torch.optim` and
    `torch.nn` packages as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to pass in the `model.parameters()` for your model.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we use the `CrossEntropyLoss()` function and the stochastic
    gradient descent (SGD) optimizer. Cross entropy loss is frequently used for classification
    problems. The SGD algorithm is also commonly used as an optimizer function. Choosing
    a loss function and an optimizer is beyond the scope of this book; however, we’ll
    examine many built-in PyTorch loss functions and optimizers later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PyTorch optimizers require that you pass in the model parameters using the `parameters()`
    method (i.e., `model.parameters()`). It’s a common mistake to forget the `()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following PyTorch code demonstrates the fundamental training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Outer training loop; loop over 10 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Move inputs and labels to GPU if available.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Zero out gradients before each backpropagation pass, or they’ll accumulate.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO6-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Perform forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO6-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Compute loss.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO6-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Perform backpropagation; compute gradients.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO6-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Adjust parameters based on gradients.
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](Images/8.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO6-8)'
  prefs: []
  type: TYPE_NORMAL
- en: Accumulate batch loss so we can average over the epoch.
  prefs: []
  type: TYPE_NORMAL
- en: The training loop consists of two loops. In the outer loop, we will process
    the entire set of training data during every iteration or epoch. However, instead
    of waiting to process the entire dataset before updating the model’s parameters,
    we process smaller batches of data, one batch at a time. The inner loop loops
    over each batch.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By default, PyTorch accumulates the gradients during each call to `loss.backward()`
    (i.e., the backward pass). This is convenient while training some types of NNs,
    such as RNNs; however, it is not desired for convolutional neural networks (CNNs).
    In most cases, you will need to call `optimizer.zero_grad()` to zero the gradients
    before doing backpropagation so the optimizer updates the model parameters correctly.
  prefs: []
  type: TYPE_NORMAL
- en: For each batch, we pass the batch (called `inputs`) into the model. It runs
    the forward pass and returns the computed outputs. Next, we compare the model
    outputs (called `outputs`) with the true values from the training dataset (called
    `labels`) using `criterion()` to compute the error or loss.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we adjust the model parameters (i.e., the weights and biases of the NN)
    to reduce the loss. To do so, we first perform backpropagation with `loss.backward()`
    to compute the gradients and then run the optimizer with `optimizer.step()` to
    update the parameters based on the computed gradients.
  prefs: []
  type: TYPE_NORMAL
- en: This is the fundamental process used for training NN models. Implementations
    may vary, but you can use this example as a quick reference when creating your
    own training loops. When designing the training loop, you will need to decide
    how data will be processed or batched, what loss function to use, and what optimizer
    algorithm to run.
  prefs: []
  type: TYPE_NORMAL
- en: You can use one of PyTorch’s built-in loss functions and optimizer algorithms,
    or you can create your own.
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyTorch includes many built-in loss functions in the `torch.nn` Python module.
    [Table 3-14](#table_loss_functions) provides a list of available loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-14\. Loss functions
  prefs: []
  type: TYPE_NORMAL
- en: '| Loss function | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.L1Loss()` | Creates a criterion that measures the mean absolute error
    (MAE) between each element in the input *x* and target *y* |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.MSELoss()` | Creates a criterion that measures the mean squared error
    (squared L2 norm) between each element in the input *x* and target *y* |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.CrossEntropyLoss()` | Combines `nn.LogSoftmax()` and `nn.NLLLoss()` in
    a single class |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.CTCLoss()` | Calculates the connectionist temporal classification loss
    |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.NLLLoss()` | Calculates the negative log likelihood loss |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.PoissonNLLLoss()` | Calculates the negative log likelihood loss with
    a Poisson distribution of the target |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.KLDivLoss()` | Measures the Kullback–Leibler divergence loss |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.BCELoss()` | Creates a criterion that measures the binary cross entropy
    between the target and the output |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.BCEWithLogitsLoss()` | Combines a sigmoid layer and the `nn.BCELoss()`
    in a single class |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.MarginRankingLoss()` | Creates a criterion that measures the loss when
    given inputs *x*¹, *x*², two 1D mini-batch tensors, and a label 1D mini-batch
    tensor *y* (containing 1 or –1) |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.HingeEmbeddingLoss()` | Measures the loss when given an input tensor
    *x* and a label tensor *y* (containing 1 or –1) |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.MultiLabelMarginLoss()` | Creates a criterion that optimizes a multiclass
    classification hinge loss (i.e., a margin-based loss) between input *x* (a 2D
    mini-batch tensor) and output *y* (a 2D tensor of target class indices) |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.SmoothL1Loss()` | Creates a criterion that uses a squared term if the
    absolute element-wise error falls below 1 or an L1 term otherwise |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.SoftMarginLoss()` | Creates a criterion that optimizes a two-class classification
    logistic loss between input tensor *x* and target tensor *y* (containing 1 or
    –1) |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.MultiLabelSoftMarginLoss()` | Creates a criterion that optimizes a multilabel
    one-versus-all loss based on the maximum entropy |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.CosineEmbeddingLoss()` | Creates a criterion that measures the loss given
    input tensors *x*¹, *x*² and a tensor labeled *y* with values 1 or –1 |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.MultiMarginLoss()` | Creates a criterion that optimizes a multiclass
    classification hinge loss |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.TripletMarginLoss()` | Creates a criterion that measures the triplet
    loss when given input tensors *x*¹, *x*², *x*³ and a margin with a value greater
    than 0 |'
  prefs: []
  type: TYPE_TB
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `CrossEntropyLoss()` function includes the softmax calculation, which is
    usually performed in the last step of an NN classifier model. When using `CrossEntropyLoss()`,
    do not include `Softmax()` in the output layer of your model definition.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyTorch also includes many built-in optimizer algorithms in the `torch.optim`
    Python submodule. [Table 3-15](#table_optimizers) lists the available optimizer
    algorithms and their descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-15\. Optimizer algorithms
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Adadelta()` | An adaptive learning rate method |'
  prefs: []
  type: TYPE_TB
- en: '| `Adagrad()` | An adaptive gradient algorithm |'
  prefs: []
  type: TYPE_TB
- en: '| `Adam()` | A method for stochastic optimization |'
  prefs: []
  type: TYPE_TB
- en: '| `AdamW()` | An Adam variant proposed in [“Decoupled Weight Decay Regularization”](https://arxiv.org/abs/1711.05101)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `SparseAdam()` | A version of Adam suitable for sparse tensors |'
  prefs: []
  type: TYPE_TB
- en: '| `Adamax()` | A variant of Adam based on the infinity norm |'
  prefs: []
  type: TYPE_TB
- en: '| `ASGD()` | Averaged stochastic gradient descent |'
  prefs: []
  type: TYPE_TB
- en: '| `LBFGS()` | A limited-memory implementation of the BFGS algorithm, heavily
    inspired by [minFunc](https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html) |'
  prefs: []
  type: TYPE_TB
- en: '| `RMSprop()` | Root mean square propagation |'
  prefs: []
  type: TYPE_TB
- en: '| `Rprop()` | Resilient backpropagation |'
  prefs: []
  type: TYPE_TB
- en: '| `SGD()` | Stochastic gradient descent |'
  prefs: []
  type: TYPE_TB
- en: The `torch.optim` Python submodule supports most commonly used algorithms. The
    interface is general enough so new ones can also be easily integrated in the future.
    Visit [the `torch.optim` documentation](https://pytorch.org/docs/stable/optim.html)
    for more details on how to configure the algorithms and adjust their learning
    rates.
  prefs: []
  type: TYPE_NORMAL
- en: Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have trained our model and attempted to minimize the loss, how can
    we evaluate its performance? How do we know that our model will generalize and
    work with data it has never seen before?
  prefs: []
  type: TYPE_NORMAL
- en: Model development often includes validation and testing loops to ensure that
    overfitting does not occur and that the model will perform well against unseen
    data. Let’s address validation first. Here, I’ll provide you with a quick reference
    for how you can add validation to your training loops with PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, we will reserve a portion of the training data for validation. The
    validation data will not be used to train the NN; instead, we’ll use it to test
    the performance of the model at the end of each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Validation is good practice when training your models. It’s commonly performed
    when adjusting hyperparameters. For example, maybe we want to slow down the learning
    rate after five epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we perform validation, we need to split our training dataset into a
    training dataset and a validation dataset, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We use the `random_split()` function from `torch.utils.data` to reserve 10,000
    of our 50,000 training images for validation. Once we create our `train_set` and
    `val_set`, we create our dataloaders for each one.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then define our model, loss function (or criterion), and optimizer, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows the previous fundamental training example with validation
    added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Configure the model for training.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Configure the model for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Validation occurs at every epoch after the training data has been processed.
    During validation, the model is passed data that was not used in training and
    that has not yet been seen by the model. We only perform the forward pass during
    validation.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Running the `.train()` or `.eval()` method on your model object puts the model
    in training or testing mode, respectively. Calling these methods is only necessary
    if your model operates differently for training and evaluation. For example, dropout
    and batch normalization are used in training but not in validation or testing.
    It’s good practice to call `.train()` and `.eval()` in your loops.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the loss decreases for validation data, then the model is doing well. However,
    if the training loss decreases but the validation loss does not, then there’s
    a good chance the model is overfitting. Look at your results from the previous
    training loop. You should have similar results to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, our model is training well and does not seem to be overfitting,
    since both the training loss and the validation loss are decreasing. If we train
    the model for more epochs, we may get even better results.
  prefs: []
  type: TYPE_NORMAL
- en: We’re not quite finished, though. Our model may still be overfitting. We might
    have just gotten lucky with our choice of hyperparameters, leading to good validation
    results. As a further test against overfitting, we will run some test data through
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: The model has never seen the test data during training, nor has the test data
    had any influence on the hyperparameters. Let’s see how we perform against the
    test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CIFAR-10 provides its own test dataset, and we created `test_data` and a testloader
    earlier in the chapter. Let’s run the test data through our test loop, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Set the model to evaluation mode for testing.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Predict the outcomes for each batch.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO8-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Select the class index with the highest probability.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO8-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Compare the prediction to the true label and count the number of correct predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_deep_learning_development___span_class__keep_together__with_pytorch__span__CO8-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the percentage of correct predictions (accuracy).
  prefs: []
  type: TYPE_NORMAL
- en: Our initial test results after 10 epochs of training show a 63% accuracy rate
    against the test data. That’s not a bad start; see if you can improve the accuracy
    by training over more epochs.
  prefs: []
  type: TYPE_NORMAL
- en: You now know how to create training, validation, and test loops using PyTorch.
    Feel free to use this code as a reference when creating your own loops.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a fully trained model, let’s explore what you can do with
    it in the model deployment stage.
  prefs: []
  type: TYPE_NORMAL
- en: Model Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending upon your goals, there are many options for saving or deploying your
    trained models. If you are conducting deep learning research, you may want to
    save your models in such a way that you can repeat your experiments or access
    them later for presentations and publishing papers. You may also wish to publish
    your models as part of a Python package like Torchvision or release them to a
    repository like PyTorch Hub so that other researchers can access your work.
  prefs: []
  type: TYPE_NORMAL
- en: On the development side, you may want to deploy your trained NN model to a production
    environment or integrate your model with a product or service. This could be a
    prototype system, edge device, or mobile device. You may also want to deploy it
    to a local production server or a cloud server that provides an API endpoint that
    a system can use. Whatever your goal, PyTorch provides capabilities to help you
    deploy your models as you wish.
  prefs: []
  type: TYPE_NORMAL
- en: Saving Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the simplest things you can do is save your trained model for future
    use. When you want to run your model against new inputs, you can simply load it
    and call the model with the new values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code illustrates the recommended way to save and load a trained
    model. It uses the `state_dict()` method, which creates a dictionary object that
    maps each layer to its parameter tensor. In other words, we only need to save
    the model’s learned parameters. We already have the model’s design defined in
    our model class, so we don’t need to save the architecture. When we load the model,
    we use the constructor to create a “blank model,” and then we use `load_state_dict()`
    to set the parameters for each layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note that `load_state_dict()` requires a dictionary object, not a path to a
    saved `state_dict` object. You must use `torch.load()` to deserialize the saved
    *state_dict* file before passing it to `load_state_dict()`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A common PyTorch convention is to save models using either a *.pt* or *.pth*
    file extension.
  prefs: []
  type: TYPE_NORMAL
- en: You can save and load the entire model using `torch.save(`*`PATH`*`)` and `model
    = torch.load(`*`PATH`*`)` too. Although this is more intuitive, it is not recommended
    because the serialization process is bound to the exact file path and directory
    structure used to define the model class. Your code can break if you refactor
    your class code and try to load the model in other projects. Saving and loading
    the `state_dict` object instead will give you more flexibility to restore the
    model later.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to PyTorch Hub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch Hub is a pretrained model repository designed to facilitate research
    reproducibility. Earlier in this chapter, I showed you how to load a preexisting
    or pretrained model from PyTorch Hub. Now, I’ll show you how to publish your pretrained
    models, including model definitions and pretrained weights, to a GitHub repository
    by adding a simple *hubconf.py* file. The *hubconf.py* file defines the code dependencies
    and provides one or more endpoints to the PyTorch API.
  prefs: []
  type: TYPE_NORMAL
- en: 'In most cases just importing the right function will be sufficient, but you
    can define the entry point explicitly. The following code shows how you would
    load a model from PyTorch Hub using the VGG16 endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if you had created VGG16 and wanted to deploy it to PyTorch Hub, all you
    would need to do is include the following *hubconf.py* file in the root of your
    repository. The *hubconf.py* configuration file sets `torch` as a dependency.
    Any function defined in this file will act as an endpoint, so simply importing
    the VGG16 function does the job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to explicitly define the endpoint, you can write a function like
    the one in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! Researchers around the world will rejoice as they easily load
    your pretrained models from PyTorch Hub.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Saving models to files and repositories may be fine when you’re conducting research;
    however, to solve most problems, we must integrate our models into products and
    services. This is often called “deploying to production.” There are many ways
    to do this, and PyTorch has built-in capabilities to support them. Deploying to
    production is a comprehensive topic that will be discussed in depth in [Chapter 7](ch07.xhtml#deploying_pytorch_to_production).
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covered a lot of ground, exploring the deep learning development
    process and providing a quick reference to the PyTorch capabilities for implementing
    each step. The next chapter presents additional reference designs that you can
    use for projects involving transfer learning, sentiment analysis, and generative
    learning .
  prefs: []
  type: TYPE_NORMAL
