<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 4. Understanding Drug–Drug Interactions Using Graphs"><div class="chapter" id="understanding-drug-drug-interactions-using-graphs">
<h1><span class="label">Chapter 4. </span>Understanding Drug–Drug Interactions Using Graphs</h1>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="understanding using graphs" data-type="indexterm" id="ch04_graphs.html0"/><em>Graphs</em> <a contenteditable="false" data-primary="graphs" data-secondary="defined" data-type="indexterm" id="id722"/>are a fundamental structure found everywhere in the world around us. A familiar example is social networks, where <em>nodes</em> represent individuals and <em>edges</em> capture the relationship between them. In train systems, nodes could represent stations and edges the routes linking them. Less obvious examples include research collaborations linked by coauthorship, web pages interconnected by hyperlinks, and supermarket baskets, where frequently copurchased items are connected.</p>

<p>Biology, too, is filled with data that naturally lends itself to a network framework—genes interact to control cell functions, proteins physically bind to each other, and cells send signals to each other, all forming graph-like systems. Even molecules can be represented as graphs, with atoms as nodes and chemical bonds as edges, as shown in <a data-type="xref" href="#example-graphs">Figure 4-1</a>. At larger biological scales, ecological food webs capture predator–prey and other species interactions, while disease transmission networks map the spread of pathogens through populations.</p>

<figure><div id="example-graphs" class="figure"><img alt="" src="assets/dlfb_0401.png" width="600" height="234"/>
<h6><span class="label">Figure 4-1. </span>Examples of graphs from different contexts. The social network shows people as nodes connected by edges representing relationships. The rail network illustrates stations as nodes and train routes as edges. The molecule network depicts the molecular structure of caffeine, where nodes represent atoms, and edges represent chemical bonds (hydrogen atoms are not shown).</h6>
</div></figure>

<p>These types of network relationships can be modeled using <em>graph neural networks</em> (GNNs). Recently, deep learning on graphs has become increasingly popular and effective. In this chapter, we will explore a graph of <em>drug–drug interactions</em> (DDIs) to gain insights into its connectivity. <a contenteditable="false" data-primary="link prediction" data-type="indexterm" id="id723"/>Specifically, we aim to predict whether two nodes should connect, which is a task known as <em>link prediction</em>. Link prediction is valuable here because, while we have an existing DDI graph, it may be incomplete—some true connections between drugs might be missing due to limited research or untested combinations. By accurately predicting these links, one could improve drug safety by identifying potential negative interactions and even discover new combination therapies by predicting which drugs might interact positively.</p>

<div data-type="tip"><h6>Tip</h6>
	<p>As in earlier chapters, we recommend keeping this chapter’s companion Colab notebook open as you read. Running the code yourself helps reinforce the material and gives you a place to immediately experiment with new ideas.</p>
</div>

<section data-type="sect1" data-pdf-bookmark="Biology Primer"><div class="sect1" id="biology-primer_89461704">
<h1>Biology Primer</h1>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="biology primer" data-type="indexterm" id="ch04_graphs.html1"/>DDIs occur when the effects of one drug are altered by the presence of another. DDIs can amplify each drug’s effects, counteract them, or change the way a drug is processed in the body, which may result in either therapeutic benefits or adverse outcomes.</p>

<section data-type="sect2" data-pdf-bookmark="Beneficial Drug–Drug Interactions"><div class="sect2" id="beneficial-drug-drug-interactions">
<h2>Beneficial Drug–Drug Interactions</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="biology primer" data-tertiary="beneficial interactions" data-type="indexterm" id="id724"/>In some cases, DDIs can be harnessed for therapeutic advantage. In cancer treatment, for example, combination therapies pair drugs that target different pathways in cancer cells. One drug may inhibit tumor growth, while another restricts the tumor’s blood supply, weakening it further. This multitargeted approach not only improves patient outcomes but also reduces the likelihood of drug resistance.</p>

<p>Similarly, certain antibiotics work better in combination. For instance, penicillin and gentamicin are often combined to treat infections like endocarditis. Penicillin weakens the bacterial cell wall, allowing gentamicin to penetrate the cell and disrupt protein synthesis, leading to a more effective antibiotic treatment.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Harmful Drug–Drug Interactions"><div class="sect2" id="harmful-drug-drug-interactions">
<h2>Harmful Drug–Drug Interactions</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="biology primer" data-tertiary="harmful interactions" data-type="indexterm" id="id725"/>Harmful DDIs are generally much more common than beneficial ones—most drugs are not designed with other drugs in mind, which often leads to unintended side effects in patients taking multiple medications. Additionally, many drugs influence similar biological pathways, increasing the likelihood that one drug will amplify or counteract the effects of another. For example:</p>

<dl>
	<dt>Amplification example</dt>
	<dd>
	<p>Aspirin, commonly used as a pain reliever or blood thinner, can amplify the effects of other anticoagulants, such as warfarin. When taken together, both drugs thin the blood more than intended, raising the risk of excessive bleeding or bruising.</p>
	</dd>
	<dt>Counteraction example</dt>
	<dd>
	<p>Ibuprofen can reduce the effectiveness of antihypertensive drugs, such as ACE inhibitors and beta-blockers. Ibuprofen causes the body to retain sodium and fluid, which raises blood pressure and counteracts these medications.</p>
	</dd>
</dl>

<p>Most negative DDIs are actually more <em>indirect</em>. For instance, many drugs are metabolized in the liver by the cytochrome P450 enzyme system, so drugs that inhibit this system can impact a wide range of other medications. Grapefruit, though not a “drug” in the traditional sense, contains compounds that inhibit the cytochrome P450 system. One of the most serious grapefruit interactions occurs with certain statins used to control cholesterol. Grapefruit compounds inhibit an enzyme that would normally break down these statins, causing higher-than-expected drug levels to accumulate in the bloodstream. This buildup can lead to very severe side effects, including liver damage and muscle tissue breakdown.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="DrugBank"><div class="sect2" id="drugbank">
<h2>DrugBank</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="biology primer" data-tertiary="DrugBank database" data-type="indexterm" id="ch04_graphs.html2"/><a contenteditable="false" data-primary="DrugBank database" data-type="indexterm" id="ch04_graphs.html3"/>DrugBank is one of the largest databases of drug interactions, providing detailed information on drugs and their known interactions. It has been widely used in various DDI studies. For example, in <a data-type="xref" href="#drugbank-clusters">Figure 4-2</a>, an early study from 2016 clustered DrugBank DDIs (at the time, the database contained around 1,000 nodes; in this chapter, we work with a more recent version containing over 4,000 nodes) to reveal major drug clusters, including those related to cytochrome P450 interactions discussed earlier.<sup><a data-type="noteref" id="id726-marker" href="ch04.html#id726">1</a></sup></p>

<figure><div id="drugbank-clusters" class="figure"><img alt="" src="assets/dlfb_0402.png" width="600" height="503"/>
<h6><span class="label">Figure 4-2. </span>Community-based drug–drug interaction network using data from DrugBank 4.1, containing 1,141 nodes (drugs) and 11,688 edges (drug–drug interactions). Clustering was performed using the Force Atlas 2 layout algorithm, which simulates a physical system to position nodes closer together based on their interactions, with colors assigned to highlight distinct communities of interacting drugs.</h6>
</div></figure>

<p>In this chapter, we will use a processed version of DrugBank’s DDI data, available through a publicly accessible benchmark dataset from the <a href="https://oreil.ly/S_wR-">Open Graph Benchmark</a> resource by researchers from Stanford University.<sup><a data-type="noteref" id="id727-marker" href="ch04.html#id727">2</a></sup> Before diving into the dataset and its applications, let’s begin with a brief primer on machine learning on graphs<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html3" data-type="indexterm" id="id728"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html2" data-type="indexterm" id="id729"/>.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html1" data-type="indexterm" id="id730"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Machine Learning Primer"><div class="sect1" id="machine-learning-primer_63173704">
<h1>Machine Learning Primer</h1>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="machine learning primer" data-type="indexterm" id="ch04_graphs.html4"/><a contenteditable="false" data-primary="machine learning" data-secondary="for graphing drug–drug interactions" data-type="indexterm" id="ch04_graphs.html5"/>You probably already have an intuitive sense of what a graph is, but to be more precise, a graph is a structure that represents relationships between pairs of objects. It consists of two main components:</p>

<dl>
	<dt>Nodes (or vertices)</dt>
	<dd>
	<p><a contenteditable="false" data-primary="node (graph component)" data-type="indexterm" id="id731"/>These represent individual entities, like people in a social network or proteins in an interaction network.</p>
	</dd>
	<dt>Edges</dt>
	<dd>
	<p><a contenteditable="false" data-primary="edge (graph component)" data-type="indexterm" id="id732"/>These are the connections between nodes, indicating relationships or interactions. In a social network, for example, an edge might represent a friendship, while in a protein interaction network, an edge represents an observed physical interaction between two proteins.</p>
	</dd>
</dl>

<p><a contenteditable="false" data-primary="directed graphs" data-type="indexterm" id="id733"/><a contenteditable="false" data-primary="undirected graphs" data-type="indexterm" id="id734"/>Graphs can be <em>directed</em> (where edges have a direction, showing a one-way relationship) or <em>undirected</em> (indicating a two-way relationship). An example of a directed biological graph is predator–prey relationships between species in an ecosystem—an owl preys on mice; it’s usually not the other way around. An example of an undirected biological graph is gene coexpression networks, where the nodes are genes and the edges are correlations between the expression levels of each gene pair.</p>

<p>Edges can have <em>attributes</em> such as <em>weights</em>, which reflect the strength of a connection. Nodes can also have attributes that capture additional information. For example, in the predator–prey example, edge weights might represent the number of times one species predates another, and each node might contain additional information about that species such as its estimated population size. Graphs vary in connection density (sparse versus dense), may include self-loops (nodes connected to themselves), and can be dynamic (changing over time, like social networks) or static.</p>

<p>Certain graph properties have significant computational implications. For instance, <em>graph size</em> can pose a challenge, as large graphs may need to be distributed across multiple processing units to avoid memory overload. <em>Graph sparsity</em>—which is the proportion of existing edges relative to the total possible edges in the graph—affects storage and computation efficiency, with specialized techniques designed to handle sparsely connected networks. Additionally, sparse graphs allow for more efficient convolution operations, as fewer neighbors need to be considered (explained further later in this chapter). Finally, the level of <em>connectivity</em> plays a crucial role. While graphs with many small, disconnected subgraphs can often be processed in parallel, densely connected graphs are more challenging to parallelize.</p>

<section data-type="sect2" data-pdf-bookmark="Representing Graph Structures"><div class="sect2" id="representing-graph-structures">
<h2>Representing Graph Structures</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="machine learning primer" data-tertiary="representing graph structures" data-type="indexterm" id="id735"/><a contenteditable="false" data-primary="machine learning" data-secondary="for graphing drug–drug interactions" data-tertiary="representing graph structures" data-type="indexterm" id="id736"/>In <a data-type="xref" href="#graph-representations">Figure 4-3</a>, we see an undirected graph containing five nodes (N0, N1, N2, N3, N4) and five edges:</p>

<figure><div id="graph-representations" class="figure"><img alt="" src="assets/dlfb_0403.png" width="600" height="225"/>
<h6><span class="label">Figure 4-3. </span>Visual representation of an undirected graph. The same graph is represented as an adjacency matrix and as a bidirectional edge list, where each undirected edge is shown in both directions. In practice, many GNN libraries require such bidirectional edge lists. Self-edges (not shown here) are also often included by default to help preserve node identity. Each node is often associated with a feature vector (not shown), but not always (as in this chapter).</h6>
</div></figure>

<p>We can numerically represent the graph structure in two main ways:</p>

<dl>
	<dt>Adjacency matrix</dt>
	<dd>
	<p>Each node is listed along the rows and columns of a matrix, with edges indicated by values in the corresponding cells.</p>
	</dd>
	<dt>Edge list</dt>
	<dd>
	<p>Each row in this list represents an edge by specifying its start and end nodes.</p>
	</dd>
</dl>

<p>The choice of representation impacts memory usage, especially depending on graph sparsity. An adjacency matrix has fixed high memory usage, as it accounts for all possible edges that could exist, while an edge list is more compact because it only stores the edges that exist. For sparse graphs, where the number of edges is much smaller than the total possible, an edge list is typically much more memory efficient.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Graph Neural Networks"><div class="sect2" id="graph-neural-networks">
<h2>Graph Neural Networks</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="machine learning primer" data-tertiary="graph neural networks" data-type="indexterm" id="id737"/><a contenteditable="false" data-primary="graph neural networks (GNNs)" data-type="indexterm" id="id738"/><a contenteditable="false" data-primary="machine learning" data-secondary="for graphing drug–drug interactions" data-tertiary="graph neural networks" data-type="indexterm" id="id739"/>With this foundational understanding of graphs, we can explore how graph neural networks learn from them. GNNs are a class of models designed to operate directly on graph structures, capturing information from both nodes and their connections. At a high level, GNNs work by iteratively aggregating information from a node’s neighbors, producing rich representations (embeddings) that reflect both the node’s features and its position within the broader graph structure. We’ll break down this process in more detail shortly—but first, why do we need GNNs in the first place? What kinds of graph-related problems can they solve?
</p>

<p>GNNs are commonly used for these main tasks:</p>

<dl>
	<dt>Node classification</dt>
	<dd>
	<p>Predicting the type or property of a node within a graph; for example, determining the category of a drug within a DDI network (e.g., antidepressant, antihistamine, or antibiotic).</p>
	</dd>
	<dt>Edge classification</dt>
	<dd>
	<p>Predicting the type or existence of a connection between two nodes; for example, determining whether two drugs are likely to interact.</p>
	</dd>
	<dt>Edge regression</dt>
	<dd>
	<p>Estimating a continuous value for a connection between nodes. In the context of DDI networks, this could involve predicting the severity or strength of an interaction rather than just its presence or type.</p>
	</dd>
	<dt>Graph classification</dt>
	<dd>
	<p>Predicting a property of an entire graph; for example, identifying whether a drug molecule, represented as a graph of atoms and bonds, has a specific property, such as being water soluble or binding to a specific disease-associated protein.</p>
	</dd>
</dl>

<p>These tasks all rely on the GNN’s ability to extract meaningful representations from the graph structure. Whether the goal is to classify nodes, predict edges, or assess whole-graph properties, the core mechanism remains the same: learning expressive embeddings through iterative information exchange. This leads us to the central idea behind most GNN architectures: <em>message passing</em>.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Graph Embeddings and Message Passing"><div class="sect2" id="graph-embeddings-and-message-passing">
<h2>Graph Embeddings and Message Passing</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="machine learning primer" data-tertiary="graph embeddings and message passing" data-type="indexterm" id="ch04_graphs.html6"/><a contenteditable="false" data-primary="graph neural networks (GNNs)" data-secondary="graph embeddings and message passing" data-type="indexterm" id="ch04_graphs.html7"/><a contenteditable="false" data-primary="machine learning" data-secondary="for graphing drug–drug interactions" data-tertiary="graph embeddings and message passing" data-type="indexterm" id="ch04_graphs.html8"/><a contenteditable="false" data-primary="message passing" data-type="indexterm" id="ch04_graphs.html9"/>A primary goal in GNNs is to learn the structure around each node by generating a per-node embedding vector that captures information from its neighborhood. Unlike in images, where pixels have a fixed spatial arrangement, graph connections lack inherent order, making traditional convolutional approaches less applicable.</p> 

<p>To address this, modern GNNs use a framework known as <em>message passing</em>, where each node iteratively exchanges messages with its neighbors and aggregates their information to update its own representation. This idea was formalized in the Message Passing Neural Network (MPNN) framework by Gilmer <em>et al</em>., which has become the foundation for many contemporary GNN architectures.<sup><a data-type="noteref" id="id740-marker" href="ch04.html#id740">3</a></sup> Earlier forms of GNNs were introduced by Scarselli <em>et al</em>., who proposed recursive neural models for learning on graphs, though without the modular message-passing abstraction seen today.<sup><a data-type="noteref" id="id741-marker" href="ch04.html#id741">4</a></sup></p>

<p>Message passing is a flexible framework that underpins many GNN models. It often refers to the interaction between <em>sender</em> and <em>receiver</em> nodes, where the sender transmits information and the receiver aggregates it to update its own representation. <em>Graph convolution</em> is one specific implementation of message passing, where nodes aggregate information from their neighbors using functions such as summation, mean, or max. In contrast, nonconvolutional approaches such as graph attention networks (GATs) use attention mechanisms to assign different weights to neighbors based on their relative importance. The choice of aggregation function—whether sum, mean, max, or attention—affects the types of patterns the GNN can learn.</p>

<p>Increasing the number of message-passing layers (i.e., the number of hops a node can “see”) expands each node’s receptive field, enabling it to incorporate information from more distant parts of the graph. However, deeper GNNs can run into two key challenges:</p>

<dl>
<dt>Over-smoothing</dt>
<dd><p>As the number of message-passing layers increases, each node incorporates information from a broader neighborhood. While this can be beneficial up to a point, stacking too many layers causes node embeddings to become increasingly similar—eventually collapsing to near-identical representations regardless of a node’s local structure or features. This degrades the model’s ability to distinguish between nodes, especially in classification tasks where fine-grained differences can be very important.</p></dd>

<dt>Over-squashing</dt>
<dd><p>When long-range information must pass through a limited number of intermediate nodes or edges, it becomes overly compressed. This bottleneck prevents distant signals from being accurately preserved—especially in graphs with long, narrow paths, such as trees or hierarchies often seen in biology (e.g., gene regulatory networks or phylogenetic trees). As a result, important context from far-apart nodes gets “squashed” before it can meaningfully influence predictions.</p></dd>	
</dl>

<p>To mitigate these issues, common strategies include adding skip connections, incorporating attention mechanisms, or rewiring the graph to shorten path lengths. In practice, using just two or three message-passing layers—capturing information from a few hops away—often provides a good balance between expressivity and stability.</p>

<div data-type="tip"><h6>Tip</h6>
	<p>In GNNs, “layers” are better understood as message-passing iterations rather than traditional neural network layers. Unlike MLPs, where each layer applies a distinct learned transformation, each GNN layer aggregates information from a node’s immediate neighbors. Thus, a model with three layers enables each node to incorporate information from up to three hops away in the graph.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html9" data-type="indexterm" id="id742"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html8" data-type="indexterm" id="id743"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html7" data-type="indexterm" id="id744"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html6" data-type="indexterm" id="id745"/></p>
</div>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Cold-Start Problem"><div class="sect2" id="cold-start-problem">
<h2>Cold-Start Problem</h2>

<p><a contenteditable="false" data-primary="cold-start problem" data-type="indexterm" id="id746"/><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="machine learning primer" data-tertiary="cold-start problem" data-type="indexterm" id="id747"/><a contenteditable="false" data-primary="graph neural networks (GNNs)" data-secondary="cold-start problem" data-type="indexterm" id="id748"/><a contenteditable="false" data-primary="machine learning" data-secondary="for graphing drug–drug interactions" data-tertiary="cold-start problem" data-type="indexterm" id="id749"/>A significant challenge in GNNs is predicting on <em>unseen nodes</em>, often referred to as the <em>cold-start problem</em>. Many traditional graph models operate in a <em>transductive</em> setting, where training occurs on a fixed graph, limiting predictions to relationships among nodes seen during training.</p>

<p>However, real-world applications often involve dynamic graphs where new nodes are introduced. For example:</p>

<ul class="simple">
	<li>
	<p>In a social network, a new user joins, and the platform needs to predict their potential connections.</p>
	</li>
	<li>
	<p>In a recommendation system, a newly released product must be matched to relevant users based on their preferences.</p>
	</li>
	<li>
	<p>In drug discovery, a newly synthesized compound must be evaluated for interactions with existing molecules.</p>
	</li>
</ul>

<p><a contenteditable="false" data-primary="inductive learning" data-type="indexterm" id="id750"/>To address the cold-start problem, GNNs can adopt an <em>inductive learning</em> approach, enabling generalization to new, unseen nodes. This capability is essential for dynamic graphs where new nodes are frequently added, as it eliminates the need to retrain the model whenever the graph changes. It is achieved by learning patterns and relationships that are transferable across the graph. For example:</p>

<ul class="simple">
	<li>
	<p>Instead of memorizing specific connections, the model identifies structural similarities (e.g., the role of a node in its local neighborhood) or shared features (e.g., common attributes across nodes).</p>
	</li>
	<li>
	<p>When a new node is added, its feature vector and immediate connections to existing nodes provide enough context for the model to embed it within the graph and make predictions.</p>
	</li>
</ul>

<p>Notable frameworks like GraphSAGE focus on inductive learning by sampling neighborhoods and aggregating local features to generate embeddings for unseen nodes. Techniques such as feature propagation and attention mechanisms further enhance this capability, making GNNs highly adaptable to evolving, real-world graphs.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>For a more detailed (but highly accessible) introduction to GNNs, we recommend the excellent lecture <a href="https://oreil.ly/GXy3v">“Theoretical Foundations of Graph Neural Networks”</a> by Petar Veličković on YouTube. It offers clear GNN explanations from one of the leading experts in the field.</p>
</div>
</div></section>

<section data-type="sect2" data-pdf-bookmark="GraphSAGE"><div class="sect2" id="graphsage">
<h2>GraphSAGE</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="machine learning primer" data-tertiary="GraphSAGE model" data-type="indexterm" id="id751"/><a contenteditable="false" data-primary="GraphSAGE model" data-type="indexterm" id="id752"/><a contenteditable="false" data-primary="machine learning" data-secondary="for graphing drug–drug interactions" data-tertiary="GraphSAGE model" data-type="indexterm" id="id753"/>In this chapter, we implement a GraphSAGE model,<sup><a data-type="noteref" id="id754-marker" href="ch04.html#id754">5</a></sup> an inductive approach that can predict the properties of nodes it has never seen before by aggregating information from their neighbors. In the original paper, GraphSAGE was evaluated on tasks like classifying academic papers into six biology-related categories using citation graphs, assigning Reddit posts to 50 communities based on user interactions, and predicting protein functions across multiple protein–protein interaction graphs. These benchmarks demonstrated GraphSAGE’s ability to generalize to unseen nodes and outperform traditional methods, highlighting its versatility in dynamic, real-world graphs.</p>

<p>A key advantage of GraphSAGE is its scalability to massive graphs. Training on large graphs can be resource intensive because embedding updates for each node requires iterating over its neighbors. GraphSAGE addresses this challenge by using <em>subsampling</em>, where only a small, fixed number of neighbors is sampled for each node. These subgraphs are processed in mini-batches, significantly reducing memory and computation costs.</p>

<p>As illustrated in <a data-type="xref" href="#graphsage-illustration">Figure 4-4</a> (from the <a href="https://oreil.ly/wz_mG"> original paper</a>), GraphSAGE has two main components: sampling a subgraph and aggregating neighborhood information for each node. The resulting embeddings can be used for downstream tasks such as node classification or link prediction. While GraphSAGE can incorporate edge or node annotations, it does not depend on them, and for most of this chapter, we will focus solely on the graph structure.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html5" data-type="indexterm" id="id755"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html4" data-type="indexterm" id="id756"/></p>

<figure><div id="graphsage-illustration" class="figure"><img alt="" src="assets/dlfb_0404.png" width="600" height="261"/>
<h6><span class="label">Figure 4-4. </span>GraphSAGE stands for Graph SAmple and AggreGatE, representing its two main steps: (1) sampling a node’s neighbors and (2) aggregating their features to generate an embedding. These embeddings can be used for downstream tasks, such as (3) predicting node properties or relationships within the graph.</h6>
</div></figure>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Selecting a Dataset"><div class="sect1" id="selecting-a-dataset">
<h1>Selecting a Dataset</h1>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="selecting a dataset" data-type="indexterm" id="ch04_graphs.html10"/>In this chapter, we’ll work with a unique data source: the Open Graph Benchmark (OGB) dataset of processed DrugBank DDIs called <a href="https://oreil.ly/WWr52"><code>ogbl-ddi</code></a>. This dataset is particularly convenient for two reasons:</p>

<ol class="arabic simple">
	<li>
	<p>It is well studied, providing a wealth of existing research to draw inspiration from.</p>
	</li>
	<li>
	<p>It enables us to compare our model’s performance with other approaches using the <a href="https://oreil.ly/VPv1R">leaderboard</a>.</p>
	</li>
</ol>

<p>Additionally, OGB simplifies the workflow by offering built-in data loaders compatible with various deep learning frameworks and an <code>Evaluator</code> class for computing problem-specific metrics. This allows us to focus on building and refining our model rather than spending a long time on data preparation.</p>

<section data-type="sect2" data-pdf-bookmark="Describing the Dataset"><div class="sect2" id="dataset-description">
<h2>Describing the Dataset</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="selecting a dataset" data-tertiary="describing the dataset" data-type="indexterm" id="id757"/>We have already discussed DDI networks in general. The OGBL DDI dataset in particular is an unweighted, undirected graph of DDIs, where each node is an FDA-approved or experimental drug and edges represent either beneficial or harmful interactions between drugs.</p>

<p>To make the problem more challenging, the dataset is split in an interesting way—by the proteins that each drug targets. This “protein–target split” ensures that the test set contains drugs that primarily bind to different proteins than those in the training and validation sets, meaning they are more likely to operate through distinct biological mechanisms. This forces the model to learn more generalizable biology. If we created our own split—such as a random split of drugs—there would likely be greater overlap in biological mechanisms between the training and test sets, which would make the problem easier but would ultimately reduce the model’s ability to generalize to unseen drugs in real-world scenarios.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Exploring the Dataset"><div class="sect2" id="exploring-the-dataset">
<h2>Exploring the Dataset</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="selecting a dataset" data-tertiary="exploring the dataset" data-type="indexterm" id="ch04_graphs.html11"/>As always, let’s start by doing some exploratory analysis of the dataset to get a feel for what we’re dealing with. We start by loading the data:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">ogb.linkproppred</code> <code class="kn">import</code> <code class="n">LinkPropPredDataset</code>

<code class="kn">from</code> <code class="nn">dlfb.utils.context</code> <code class="kn">import</code> <code class="n">assets</code>

<code class="c1"># Quite a large graph, may take a few minutes to load.</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">LinkPropPredDataset</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s2">"ogbl-ddi"</code><code class="p">,</code> <code class="n">root</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"graphs/datasets"</code><code class="p">))</code>
</pre>

<p>This downloads the <code>ogbl-ddi</code> dataset and neatly packs it into an object ready for inspection. The full graph is accessible with <code>.graph</code>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">dataset</code><code class="o">.</code><code class="n">graph</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">
{'edge_index': array([[4039, 2424, 4039, ...,  338,  835, 3554],
        [2424, 4039,  225, ...,  708, 3554,  835]]),
 'edge_feat': None,
 'node_feat': None,
 'num_nodes': 4267}
</pre>


<p>The graph is stored in <em>edge-list</em> format under the key <code>edge_index</code>. Both <code>edge_feat</code> and <code>node_feat</code> are <code>None</code>, meaning the graph includes only the structure—without additional edge features such as interaction strengths or node features such as drug properties. Next, let’s examine the number of nodes and edges in the graph:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="nb">print</code><code class="p">(</code>
  <code class="sa">f</code><code class="s1">'The graph contains </code><code class="si">{</code><code class="n">dataset</code><code class="o">.</code><code class="n">graph</code><code class="p">[</code><code class="s2">"num_nodes"</code><code class="p">]</code><code class="si">}</code><code class="s1"> nodes and '</code>
  <code class="sa">f</code><code class="s1">'</code><code class="si">{</code><code class="n">dataset</code><code class="o">.</code><code class="n">graph</code><code class="p">[</code><code class="s2">"edge_index"</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="si">}</code><code class="s1"> edges.'</code>
<code class="p">)</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">
The graph contains 4267 nodes and 2135822 edges.
</pre>

<p>We can plot the <em>degree distribution</em>, or the distribution of the number of connections per node, to get a sense of the high-level graph structure (depicted in <a data-type="xref" href="#fig4-5">Figure 4-5</a>):</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="k">as</code> <code class="nn">sns</code>

<code class="n">degrees</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">bincount</code><code class="p">(</code><code class="n">dataset</code><code class="o">.</code><code class="n">graph</code><code class="p">[</code><code class="s2">"edge_index"</code><code class="p">]</code><code class="o">.</code><code class="n">flatten</code><code class="p">())</code>

<code class="n">sns</code><code class="o">.</code><code class="n">histplot</code><code class="p">(</code><code class="n">degrees</code><code class="p">,</code> <code class="n">kde</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Degree"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Frequency"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"Degree Distribution"</code><code class="p">);</code>
</pre>

<figure><div id="fig4-5" class="figure">
<img alt="" src="assets/dlfb_0405.png" width="600" height="472"/>
<h6><span class="label">Figure 4-5. </span>The degree distribution of nodes in the DDI network follows a power-law distribution where a few drugs interact with many others, but the majority is <span class="keep-together">more isolated</span>.</h6>
</div></figure>

<p><a contenteditable="false" data-primary="hubs, in GNNs" data-type="indexterm" id="id758"/>We observe that a few drugs act as <em>hubs</em>, exhibiting a high degree of interaction with many other drugs, while most drugs have a low degree, interacting with only a few other drugs. <a contenteditable="false" data-primary="power-law distribution" data-type="indexterm" id="id759"/>This pattern is consistent with a <a href="https://oreil.ly/pcVKt"><em>power-law distribution</em></a>, commonly seen in biological and social networks, where a small number of elements have very high connectivity (hubs) while the majority have low connectivity. However, it is important to note that this characteristic might be specific to this dataset and may not generalize to all DDI networks.</p>

<p>We can compute the <em>density</em> of the graph, or the ratio of edges to the number of possible edges, to quantify how densely interconnected our graph is:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">num_nodes</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">graph</code><code class="p">[</code><code class="s2">"num_nodes"</code><code class="p">]</code>
<code class="n">num_observed_edges</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">graph</code><code class="p">[</code><code class="s2">"edge_index"</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>

<code class="c1"># Since each edge in an undirected graph can be represented in two ways, we</code>
<code class="c1"># multiply by 2 to account for the bidirectionality.</code>
<code class="n">num_observed_edges</code> <code class="o">=</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">num_observed_edges</code>

<code class="c1"># For any graph with n nodes, the maximum number of edges (assuming no</code>
<code class="c1"># self-loops) is n * (n-1).</code>
<code class="n">num_possible_edges</code> <code class="o">=</code> <code class="n">num_nodes</code> <code class="o">*</code> <code class="p">(</code><code class="n">num_nodes</code> <code class="o">-</code> <code class="mi">1</code><code class="p">)</code>

<code class="n">density</code> <code class="o">=</code> <code class="n">num_observed_edges</code> <code class="o">/</code> <code class="n">num_possible_edges</code>

<code class="nb">print</code><code class="p">(</code>
  <code class="sa">f</code><code class="s2">"There are </code><code class="si">{</code><code class="n">num_observed_edges</code><code class="si">}</code><code class="s2"> observed edges and </code><code class="si">{</code><code class="n">num_possible_edges</code><code class="si">}</code><code class="s2"> "</code>
  <code class="sa">f</code><code class="s2">"possible edges,</code><code class="se">\n</code><code class="s2">giving a graph density of </code><code class="si">{</code><code class="nb">round</code><code class="p">(</code><code class="n">density</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code><code class="si">}</code><code class="s2">"</code>
<code class="p">)</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">
There are 4271644 observed edges and 18203022 possible edges,
giving a graph density of 0.23
</pre>

<p>This shows that while the dataset contains a seemingly large number of edges, it is not extremely dense, as 77% of possible connections are absent. With a density of 23%, the graph might be considered moderately interconnected, though this label is a bit subjective and depends on the specific context.</p>

<p>The dataset comes with its own methods to extract useful information. For example, <code>.get_edge_split</code> will list the graph’s edges across the different data splits:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">data_split</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">get_edge_split</code><code class="p">()</code>
<code class="n">data_split</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">
{'train': {'edge': array([[4039, 2424],
         [4039,  225],
         [4039, 3901],
         ...,
         [ 647,  708],
         [ 708,  338],
         [ 835, 3554]])},
 'valid': {'edge': array([[ 722,  548],
         [ 874, 3436],
         [ 838, 1587],
         ...,
         [3661, 3125],
         [3272, 3330],
         [1330,  776]]),
  'edge_neg': array([[   0,   58],
         [   0,   84],
         [   0,   90],
         ...,
         [4162, 4180],
         [4168, 4260],
         [4180, 4221]])},
 'test': {'edge': array([[2198, 1172],
         [1205,  719],
         [1818, 2866],
         ...,
         [ 326, 1109],
         [ 911, 1250],
         [4127, 2480]]),
  'edge_neg': array([[   0,    2],
         [   0,   16],
         [   0,   42],
         ...,
         [4168, 4259],
         [4208, 4245],
         [4245, 4259]])}}
</pre>

<p>We can see that the <code>valid</code> and <code>test</code> splits actually contain two types of edges:</p>

<ul>
	<li>
	<p>The <code>edge</code> key holds the positive data, representing known drug interactions. Here, <em>positive</em> refers to the fact that these interactions are known, not whether they are beneficial or harmful.</p>
	</li>
	<li>
	<p>The <code>edge_neg</code> key contains negative edges, representing drug pairs with no known interactions. However, because some interactions may simply be undiscovered, this data is considered <em>weakly labeled</em> and may include inaccuracies (false negatives).</p>
	</li>
</ul>

<p>Importantly, the training dataset does not include explicit negative edges (i.e., there’s no predefined <code>edge_neg</code> list in <code>train</code>). However, since most node pairs in a sparse graph are unconnected, negative samples can be drawn from this large set of nonedges during training. The method used to sample these negatives is an important hyperparameter, as it can significantly affect performance. Some negative edges are trivially easy to distinguish, which can lead to inflated metrics. In contrast, the validation and test datasets include a predefined <code>edge_neg</code> key that specifies which unconnected node pairs to use for evaluation.</p>

<p>Let’s now examine the relative sizes of the <code>train</code>, <code>valid</code>, and <code>test</code> splits:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="nb">print</code><code class="p">(</code>
  <code class="sa">f</code><code class="s1">'Number of edges in train set: </code><code class="si">{</code><code class="n">data_split</code><code class="p">[</code><code class="s2">"train"</code><code class="p">][</code><code class="s2">"edge"</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="si">}</code><code class="se">\n</code><code class="s1">'</code>
  <code class="sa">f</code><code class="s1">'Number of edges in valid set: </code><code class="si">{</code><code class="n">data_split</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">][</code><code class="s2">"edge"</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="si">}</code><code class="se">\n</code><code class="s1">'</code>
  <code class="sa">f</code><code class="s1">'Number of edges in test set: </code><code class="si">{</code><code class="n">data_split</code><code class="p">[</code><code class="s2">"test"</code><code class="p">][</code><code class="s2">"edge"</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="si">}</code><code class="s1">'</code>
<code class="p">)</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">
Number of edges in train set: 1067911
Number of edges in valid set: 133489
Number of edges in test set: 133489
</pre>

<p>In this dataset, the training set contains roughly 10 times more positive edges than the validation and test sets.</p>

<p>Another important consideration is whether all nodes in the validation and test sets also appear in the training set. This determines whether the model will encounter completely unseen nodes during evaluation—a key distinction between transductive and inductive learning. You can check this with the following code:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">train_nodes</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">data_split</code><code class="p">[</code><code class="s2">"train"</code><code class="p">][</code><code class="s2">"edge"</code><code class="p">])</code>
<code class="n">valid_nodes</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">data_split</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">][</code><code class="s2">"edge"</code><code class="p">])</code>
<code class="n">test_nodes</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">data_split</code><code class="p">[</code><code class="s2">"test"</code><code class="p">][</code><code class="s2">"edge"</code><code class="p">])</code>

<code class="c1"># Check if all nodes in valid and test sets are present in train set.</code>
<code class="n">valid_in_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">valid_nodes</code><code class="p">,</code> <code class="n">train_nodes</code><code class="p">)</code><code class="o">.</code><code class="n">all</code><code class="p">()</code>
<code class="n">test_in_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">test_nodes</code><code class="p">,</code> <code class="n">train_nodes</code><code class="p">)</code><code class="o">.</code><code class="n">all</code><code class="p">()</code>

<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"All validation nodes are in training nodes: </code><code class="si">{</code><code class="n">valid_in_train</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"All test nodes are in training nodes: </code><code class="si">{</code><code class="n">test_in_train</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">
All validation nodes are in training nodes: True
All test nodes are in training nodes: True
</pre>

<p>In our case, all nodes in the validation and test sets are indeed present in the training graph. This defines a transductive setting, where the model sees all nodes during training and only needs to predict whether specific edges exist between them. This setup is simpler than the inductive case, where the model must make predictions involving entirely unseen nodes.</p>
<p>Starting with transductive evaluation allows us to assess model performance in a controlled setting before tackling the more complex inductive scenario. Models like GraphSAGE are well suited to inductive tasks because they generate node embeddings based on local neighborhoods. This means that even unseen nodes can be embedded meaningfully, provided they connect to known parts of the graph.</p>
<p>For now, we’ll focus on the transductive case and ensure that the model performs well when all nodes are known.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html11" data-type="indexterm" id="id760"/></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Examining Drug Names"><div class="sect2" id="examining-drug-names">
<h2>Examining Drug Names</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="selecting a dataset" data-tertiary="examining drug names" data-type="indexterm" id="ch04_graphs.html12"/>Although not immediately available in the graph object, there is additional annotation data that comes with the <code>ogbl-ddi</code> dataset. Let’s examine this information:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>

<code class="n">ddi_descriptions</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code>
  <code class="n">assets</code><code class="p">(</code><code class="s2">"graphs/datasets/ogbl_ddi/mapping/ddi_description.csv.gz"</code><code class="p">)</code>
<code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">ddi_descriptions</code><code class="p">)</code>
</pre>

<p class="pagebreak-before">Output:</p>
<pre data-type="programlisting">
        first drug id   first drug name second drug id     second drug name  \
0             DB00001         Lepirudin        DB06605             Apixaban   
1             DB00001         Lepirudin        DB06695  Dabigatran etexi...   
2             DB00001         Lepirudin        DB01254            Dasatinib   
...               ...               ...            ...                  ...   
2669761       DB15657  Ala-geninthiocin        DB14055         (S)-Warfarin   
2669762       DB15657  Ala-geninthiocin        DB00581            Lactulose   
2669763       DB15657  Ala-geninthiocin        DB14443  Vibrio cholerae ...   

                 description  
0        Apixaban may inc...  
1        Dabigatran etexi...  
2        The risk or seve...  
...                      ...  
2669761  The risk or seve...  
2669762  The therapeutic ...  
2669763  The therapeutic ...  

[2669764 rows x 5 columns]
</pre>



<p>We can see that each row is a DDI, with each drug having an ID (an accession in the <a href="https://oreil.ly/CfISy"> DrugBank database</a>) and a description of the nature of the interaction.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>This chapter’s dataset is ultimately derived from <a href="https://oreil.ly/MYb6A">DrugBank</a>, which provides extensive information about drugs and their interactions. While some of this information is included in the benchmark dataset, much more could be added, such as chemical properties, target genes, and other drug-specific details. However, access to the full DrugBank resource is not free for nonacademic users.</p>
</div>

<p>When working with our graph, we will mostly be dealing with node indices, but we can always look up the mapping between node ID and DrugBank drug IDs:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">node_to_dbid_lookup</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code>
  <code class="n">assets</code><code class="p">(</code><code class="s2">"graphs/datasets/ogbl_ddi/mapping/nodeidx2drugid.csv.gz"</code><code class="p">)</code>
<code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">node_to_dbid_lookup</code><code class="p">)</code>
</pre>

<p class="pagebreak-before">Output:</p>
<pre data-type="programlisting">
      node idx  drug id
0            0  DB00001
1            1  DB00002
2            2  DB00004
...        ...      ...
4264      4264  DB15617
4265      4265  DB15623
4266      4266  DB15657

[4267 rows x 2 columns]
</pre>

<p>This lookup allows us to look a bit deeper at the degree distribution observation from earlier. What are the drugs that bind many other drugs? Let’s examine the drugs with the highest number of edges. Since all but 14 drug interactions are represented twice in this dataframe (once as <em>A-B</em> and once as <em>B-A</em>), we can count on the <code>first drug name</code> column to get the most frequently binding drugs:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">ddi_descriptions</code><code class="p">[</code><code class="s2">"first drug name"</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">
first drug name
Quinidine         2477
Chlorpromazine    2431
Desipramine       2345
Amitriptyline     2338
Clozapine         2324
Doxepin           2273
Clomipramine      2269
Haloperidol       2269
Carbamazepine     2267
Imipramine        2260
Name: count, dtype: int64
</pre>

<p><a data-type="xref" href="#top-interacting-drugs">Figure 4-6</a> visualizes the structure of these top interacting drugs. Interestingly, many of these drugs, such as desipramine, amitriptyline, and clomipramine, share a common three-ring (tricyclic) core structure, which may contribute to their similar interaction profiles.</p>

<figure><div id="top-interacting-drugs" class="figure"><img alt="" src="assets/dlfb_0406.png" width="600" height="259"/>
<h6><span class="label">Figure 4-6. </span>Chemical structures of the top 10 drugs with the highest number of drug–drug interactions in the dataset. Interestingly, many of these drugs, such as desipramine, amitriptyline, and clomipramine, share a common three-ring (tricyclic) core structure, which may contribute to their similar interaction profiles by promoting broad target binding and extensive metabolism via cytochrome P450 enzymes. Structures were acquired from DrugBank.</h6>
</div></figure>

<p>This list of drugs may seem a bit obscure if you’re not accustomed to memorizing drug names, but there are a few emergent patterns here:</p>

<dl>
	<dt>Affecting transporter proteins</dt>
	<dd>
	<p>The drug with the highest number of interactions (2,477) is quinidine, used to treat certain heart arrhythmias. Like other drugs on this list, such as clozapine and carbamazepine, quinidine interacts strongly with transporter proteins (with the most famous one being a protein called P-glycoprotein), which regulate the absorption and transport of many drugs across cells. This broad influence on drug levels largely explains its high interaction count in this dataset.</p>
	</dd>
	<dt>Affecting drug metabolism</dt>
	<dd>
	<p>Many of these drugs, like the antidepressants (desipramine, amitriptyline, clomipramine, imipramine), the antipsychotics (chlorpromazine, clozapine, haloperidol), and the mood stabilizer carbamazepine, are metabolized by the cytochrome P450 enzyme family in the liver. This system, introduced earlier, plays a major role in drug metabolism and is central to many drug interactions, because drugs that inhibit or activate cytochrome P450 enzymes can alter the metabolism of other drugs taken simultaneously.</p>
	</dd>
	<dt>Dosage sensitivity</dt>
	<dd>
	<p>Finally, these top interacting drugs also tend to have narrow <em>therapeutic ranges</em>, meaning even small changes in blood concentrations can lead to adverse effects. This makes interactions more likely to occur and be noticed.</p>
	</dd>
</dl>

<p>From this additional table of drug information, we can construct a lookup table of <code>node_id</code> to DrugBank <code>dbid</code> to drug names, allowing us to bring more biological context to our project as we start modeling:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">first_drug</code> <code class="o">=</code> <code class="n">ddi_descriptions</code><code class="p">[[</code><code class="s2">"first drug id"</code><code class="p">,</code> <code class="s2">"first drug name"</code><code class="p">]]</code><code class="o">.</code><code class="n">rename</code><code class="p">(</code>
  <code class="n">columns</code><code class="o">=</code><code class="p">{</code><code class="s2">"first drug id"</code><code class="p">:</code> <code class="s2">"dbid"</code><code class="p">,</code> <code class="s2">"first drug name"</code><code class="p">:</code> <code class="s2">"drug_name"</code><code class="p">}</code>
<code class="p">)</code>
<code class="n">second_drug</code> <code class="o">=</code> <code class="n">ddi_descriptions</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code>
  <code class="p">:,</code> <code class="p">[</code><code class="s2">"second drug id"</code><code class="p">,</code> <code class="s2">"second drug name"</code><code class="p">]</code>
<code class="p">]</code><code class="o">.</code><code class="n">rename</code><code class="p">(</code><code class="n">columns</code><code class="o">=</code><code class="p">{</code><code class="s2">"second drug id"</code><code class="p">:</code> <code class="s2">"dbid"</code><code class="p">,</code> <code class="s2">"second drug name"</code><code class="p">:</code> <code class="s2">"drug_name"</code><code class="p">})</code>
<code class="n">dbid_to_name_lookup</code> <code class="o">=</code> <code class="p">(</code>
  <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">first_drug</code><code class="p">,</code> <code class="n">second_drug</code><code class="p">])</code><code class="o">.</code><code class="n">drop_duplicates</code><code class="p">()</code><code class="o">.</code><code class="n">reset_index</code><code class="p">(</code><code class="n">drop</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="p">)</code>

<code class="n">drugs_lookup</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code>
  <code class="n">node_to_dbid_lookup</code><code class="o">.</code><code class="n">rename</code><code class="p">(</code>
    <code class="n">columns</code><code class="o">=</code><code class="p">{</code><code class="s2">"drug id"</code><code class="p">:</code> <code class="s2">"dbid"</code><code class="p">,</code> <code class="s2">"node idx"</code><code class="p">:</code> <code class="s2">"node_id"</code><code class="p">}</code>
  <code class="p">),</code>
  <code class="n">dbid_to_name_lookup</code><code class="p">,</code>
  <code class="n">on</code><code class="o">=</code><code class="s2">"dbid"</code><code class="p">,</code>
  <code class="n">how</code><code class="o">=</code><code class="s2">"inner"</code><code class="p">,</code>
<code class="p">)</code>

<code class="n">drugs_lookup</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="mi">935</code><code class="p">]</code>
</pre>

<p>Output:</p>
<pre data-code-language="python" data-type="programlisting">
<code class="n">node_id</code>            <code class="mi">935</code>
<code class="n">dbid</code>           <code class="n">DB01043</code>
<code class="n">drug_name</code>    <code class="n">Memantine</code>
<code class="n">Name</code><code class="p">:</code> <code class="mi">935</code><code class="p">,</code> <code class="n">dtype</code><code class="p">:</code> <code class="nb">object</code>
</pre>

<p>For example, using this lookup table, we can see that node ID 935 corresponds to the drug memantine, which has the DrugBank ID DB01043.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html12" data-type="indexterm" id="id761"/></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Visualizing Graphs"><div class="sect2" id="visualizing-graphs">
<h2>Visualizing Graphs</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="selecting a dataset" data-tertiary="visualizing graphs" data-type="indexterm" id="ch04_graphs.html13"/>Now let’s take a look at what a portion of this graph data actually looks like. The entire graph is too large to meaningfully visualize all at once, but we can sample a subgraph and visualize that. The strategy here is to select a subset of nodes from the original training graph and then subset the split dataset by these nodes:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>


<code class="k">def</code> <code class="nf">get_subgraph</code><code class="p">(</code><code class="n">edges</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">,</code> <code class="n">node_limit</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">:</code>
  <code class="sd">"""Gets a subgraph by sampling nodes and their edges."""</code>
  <code class="n">nodes</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">edges</code><code class="p">)</code>
  <code class="n">sampled_nodes</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">nodes</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="n">node_limit</code><code class="p">,</code> <code class="n">replace</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
  <code class="n">filtered_edges</code> <code class="o">=</code> <code class="n">edges</code><code class="p">[</code>
    <code class="n">np</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">edges</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">sampled_nodes</code><code class="p">)</code> <code class="o">&amp;</code> <code class="n">np</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">edges</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">sampled_nodes</code><code class="p">)</code>
  <code class="p">]</code>
  <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Subgraph has </code><code class="si">{</code><code class="n">filtered_edges</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="si">}</code><code class="s2"> edges"</code><code class="p">)</code>
  <code class="k">return</code> <code class="n">filtered_edges</code>


<code class="c1"># Sample 50 nodes from the training set.</code>
<code class="n">subgraph</code> <code class="o">=</code> <code class="n">get_subgraph</code><code class="p">(</code><code class="n">node_limit</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">edges</code><code class="o">=</code><code class="n">data_split</code><code class="p">[</code><code class="s2">"train"</code><code class="p">][</code><code class="s2">"edge"</code><code class="p">])</code>
</pre>


<p>Output:</p>
<pre data-type="programlisting">
Subgraph has 152 edges
</pre>

<p>This extracts a subgraph containing 50 nodes from the training set. We can visualize it in <a data-type="xref" href="#ddi-subgraph-plot">Figure 4-7</a> using the <code>plot_ddi_graph</code> function, which leverages the popular <code>networkx</code> library—a widely used Python tool for creating and visualizing graph structures:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">networkx</code> <code class="k">as</code> <code class="nn">nx</code>
<code class="kn">from</code> <code class="nn">adjustText</code> <code class="kn">import</code> <code class="n">adjust_text</code>


<code class="k">def</code> <code class="nf">plot_ddi_graph</code><code class="p">(</code><code class="n">graph</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">,</code> <code class="n">drugs_lookup</code><code class="p">:</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">plt</code><code class="o">.</code><code class="n">Figure</code><code class="p">:</code>
  <code class="sd">"""Plots a drug–drug interaction graph with labeled nodes."""</code>
  <code class="n">fig</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">15</code><code class="p">,</code> <code class="mi">15</code><code class="p">))</code>
  <code class="n">G</code> <code class="o">=</code> <code class="n">nx</code><code class="o">.</code><code class="n">Graph</code><code class="p">()</code>
  <code class="n">G</code><code class="o">.</code><code class="n">add_edges_from</code><code class="p">(</code><code class="n">graph</code><code class="p">)</code>
  <code class="n">pos</code> <code class="o">=</code> <code class="n">nx</code><code class="o">.</code><code class="n">spring_layout</code><code class="p">(</code><code class="n">G</code><code class="p">)</code>
  <code class="n">nx</code><code class="o">.</code><code class="n">draw</code><code class="p">(</code>
    <code class="n">G</code><code class="o">=</code><code class="n">G</code><code class="p">,</code>
    <code class="n">pos</code><code class="o">=</code><code class="n">pos</code><code class="p">,</code>
    <code class="n">with_labels</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>
    <code class="n">node_color</code><code class="o">=</code><code class="s2">"lightgray"</code><code class="p">,</code>
    <code class="n">edge_color</code><code class="o">=</code><code class="s2">"gray"</code><code class="p">,</code>
    <code class="n">node_size</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
    <code class="n">alpha</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code>
  <code class="p">)</code>
  <code class="n">names</code> <code class="o">=</code> <code class="p">(</code>
    <code class="n">drugs_lookup</code><code class="p">[</code><code class="n">drugs_lookup</code><code class="p">[</code><code class="s2">"node_id"</code><code class="p">]</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">G</code><code class="o">.</code><code class="n">nodes</code><code class="p">)]</code>
    <code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s2">"node_id"</code><code class="p">)[</code><code class="s2">"drug_name"</code><code class="p">]</code>
    <code class="o">.</code><code class="n">to_dict</code><code class="p">()</code>
  <code class="p">)</code>
  <code class="n">labels</code> <code class="o">=</code> <code class="n">nx</code><code class="o">.</code><code class="n">draw_networkx_labels</code><code class="p">(</code><code class="n">G</code><code class="o">=</code><code class="n">G</code><code class="p">,</code> <code class="n">pos</code><code class="o">=</code><code class="n">pos</code><code class="p">,</code> <code class="n">labels</code><code class="o">=</code><code class="n">names</code><code class="p">,</code> <code class="n">font_size</code><code class="o">=</code><code class="mi">20</code><code class="p">)</code>
  <code class="n">adjust_text</code><code class="p">(</code><code class="nb">list</code><code class="p">(</code><code class="n">labels</code><code class="o">.</code><code class="n">values</code><code class="p">()))</code>
  <code class="k">return</code> <code class="n">fig</code>


<code class="n">plot_ddi_graph</code><code class="p">(</code><code class="n">subgraph</code><code class="p">,</code> <code class="n">drugs_lookup</code><code class="p">);</code>
</pre>

<figure><div id="ddi-subgraph-plot" class="figure"><img alt="" src="assets/dlfb_0407.png" width="600" height="601"/>
<h6><span class="label">Figure 4-7. </span>A sampled subgraph of the DDI network, with nodes labeled by drug names. While this graph was sampled to just 50 nodes for clarity, the visualization already highlights the diversity of interactions, including densely connected drugs and more isolated drugs.</h6>
</div></figure>

<p><a data-type="xref" href="#ddi-subgraph-plot">Figure 4-7</a> highlights the diversity of interactions, including densely connected clusters (e.g., around Clomipramine, an antidepressant) and isolated or sparsely connected drugs. While this graph was sampled for clarity, it illustrates how certain drugs act as hubs, reflecting their broad interaction profiles, and others interact more selectively, potentially due to specific biological mechanisms<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html13" data-type="indexterm" id="id762"/>.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html10" data-type="indexterm" id="id763"/></p>

<p>With this initial data exploration complete, we’re ready to move on to building the dataset.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Building a Dataset"><div class="sect1" id="ch4-building-a-dataset">
<h1>Building a Dataset</h1>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="building a dataset" data-type="indexterm" id="ch04_graphs.html14"/>Having explored the dataset from <code>LinkPropPredDataset</code>, we now turn our attention to the process of preparing it for use in the JAX/Flax framework. Although the dataset isn’t out-of-the-box compatible, this offers a valuable opportunity to better understand the intricacies of graph processing. In this section, we’ll walk through the necessary adjustments to ensure that the dataset is properly formatted for our model.</p>

<p>Fortunately, we don’t have to start from scratch. The JAX ecosystem has <code>jraph</code>, a graph library that offers foundational, graph-aware classes and data structures, allowing us to build flexible graph processing models while benefiting from JAX’s speed and efficiency. If you’d like to explore <code>jraph</code> in more detail before diving into our implementation, we recommend this excellent tutorial on graph nets with <code>jraph</code> from DeepMind.<sup><a data-type="noteref" id="id764-marker" href="ch04.html#id764">6</a></sup></p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>PyTorch, particularly its extension library <code>pytorch-geometric</code>, is arguably the most comprehensive deep learning framework for working with graphs. It offers a robust toolkit that simplifies selecting graph models from a model zoo, handling efficient data loading, and working with convenient data classes. Datasets like OGBL have dedicated data loaders tailored for this framework. However, in this chapter, we are using <code>jraph</code>, as it integrates seamlessly with JAX, aligning better with our overall approach for the book.</p>
</div>

<p>Let’s get started with building a dataset we can train models on. As mentioned, there are several ways to represent a graph, such as using an adjacency matrix or an edge list. Since we’re using <code>jraph</code>, we go for the edge-list format, the default, which is much more memory efficient for sparser datasets like a DDI network.</p>

<section data-type="sect2" data-pdf-bookmark="Creating a Dataset Builder"><div class="sect2" id="creating-a-datasets-builder">
<h2>Creating a Dataset Builder</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="building a dataset" data-tertiary="creating a dataset builder" data-type="indexterm" id="id765"/>We have packaged the dataset building into a class called <code>DatasetBuilder</code>. As we go along, you’ll recognize many parts from the previous section where we explored the raw dataset. Let’s go through it step-by-step, starting with the main method, <code>build</code>:</p>

<pre data-code-language="python" data-type="programlisting">
  <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">path</code><code class="p">):</code>
    <code class="sd">"""Initializes the dataset builder with a path to the dataset."""</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">path</code> <code class="o">=</code> <code class="n">path</code>


  <code class="k">def</code> <code class="nf">build</code><code class="p">(</code>
    <code class="bp">self</code><code class="p">,</code>
    <code class="n">node_limit</code><code class="p">:</code> <code class="nb">int</code> <code class="o">|</code> <code class="kc">None</code> <code class="o">=</code> <code class="kc">None</code><code class="p">,</code>
    <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code> <code class="o">|</code> <code class="kc">None</code> <code class="o">=</code> <code class="kc">None</code><code class="p">,</code>
    <code class="n">keep_original_ids</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">False</code><code class="p">,</code>
  <code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">Dataset</code><code class="p">]:</code>
    <code class="sd">"""Builds and returns a dictionary of dataset splits."""</code>
    <code class="n">dataset_splits</code> <code class="o">=</code> <code class="p">{}</code>
    <code class="n">n_nodes</code><code class="p">,</code> <code class="n">split_pairs</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">download</code><code class="p">()</code>
    <code class="n">annotation</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">prepare_annotation</code><code class="p">()</code>

    <code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">split</code> <code class="ow">in</code> <code class="n">split_pairs</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
      <code class="n">pos_pairs</code><code class="p">,</code> <code class="n">neg_pairs</code> <code class="o">=</code> <code class="n">split</code><code class="p">[</code><code class="s2">"edge"</code><code class="p">],</code> <code class="n">split</code><code class="p">[</code><code class="s2">"edge_neg"</code><code class="p">]</code>
      <code class="n">graph</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">prepare_graph</code><code class="p">(</code><code class="n">n_nodes</code><code class="p">,</code> <code class="n">pos_pairs</code><code class="p">)</code>
      <code class="n">pairs</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">prepare_pairs</code><code class="p">(</code><code class="n">graph</code><code class="p">,</code> <code class="n">pos_pairs</code><code class="p">,</code> <code class="n">neg_pairs</code><code class="p">)</code>
      <code class="n">dataset_splits</code><code class="o">.</code><code class="n">update</code><code class="p">({</code><code class="n">name</code><code class="p">:</code> <code class="n">Dataset</code><code class="p">(</code><code class="n">n_nodes</code><code class="p">,</code> <code class="n">graph</code><code class="p">,</code> <code class="n">pairs</code><code class="p">,</code> <code class="n">annotation</code><code class="p">)})</code>

    <code class="k">if</code> <code class="n">node_limit</code> <code class="ow">and</code> <code class="p">(</code><code class="n">rng</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code><code class="p">):</code>
      <code class="n">dataset_splits</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">subset</code><code class="p">(</code>
        <code class="n">dataset_splits</code><code class="p">,</code> <code class="n">rng</code><code class="p">,</code> <code class="n">node_limit</code><code class="p">,</code> <code class="n">keep_original_ids</code>
      <code class="p">)</code>

    <code class="k">return</code> <code class="n">dataset_splits</code>
</pre>

<p>During instantiation, the builder receives a <code>path</code> to ensure that the dataset is stored in the specified location, eliminating the need to redownload it every time. The <code>build</code> method then generates a dictionary where the keys indicate data splits, each associated with a <code>Dataset</code> value. We’ll examine the <code>Dataset</code> class in a little more detail shortly, but for now, think of it as a dataset bundle with convenience methods for easier handling during training. The parameters passed to <code>build</code> help with subsetting the graph, which we will also get into a little bit later.</p>

</div></section>
<section data-type="sect2" data-pdf-bookmark="Download the Raw Dataset"><div class="sect2" id="download-the-raw-dataset">
<h2>Download the Raw Dataset</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="building a dataset" data-tertiary="downloading the raw dataset" data-type="indexterm" id="id766"/>Looking at the <code>build</code> code, we can see that the raw dataset is first downloaded, leveraging the <code>LinkPropPredDataset</code> we saw before. Since the training split does not have negative pairs, we add a <code>neg_edges</code> key to simplify later handling:</p>

<pre data-code-language="python" data-type="programlisting">
  <code class="k">def</code> <code class="nf">download</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">tuple</code><code class="p">[</code><code class="nb">int</code><code class="p">,</code> <code class="nb">dict</code><code class="p">]:</code>
    <code class="sd">"""Downloads the dataset and returns the number of nodes and edge splits."""</code>
    <code class="n">raw</code> <code class="o">=</code> <code class="n">LinkPropPredDataset</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s2">"ogbl-ddi"</code><code class="p">,</code> <code class="n">root</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">path</code><code class="p">)</code>
    <code class="n">n_nodes</code> <code class="o">=</code> <code class="n">raw</code><code class="p">[</code><code class="mi">0</code><code class="p">][</code><code class="s2">"num_nodes"</code><code class="p">]</code>
    <code class="n">split_pairs</code> <code class="o">=</code> <code class="n">raw</code><code class="o">.</code><code class="n">get_edge_split</code><code class="p">()</code>
    <code class="n">split_pairs</code><code class="p">[</code><code class="s2">"train"</code><code class="p">][</code><code class="s2">"edge_neg"</code><code class="p">]</code> <code class="o">=</code> <code class="kc">None</code>  <code class="c1"># Placeholder for negative edges.</code>
    <code class="k">return</code> <code class="n">n_nodes</code><code class="p">,</code> <code class="n">split_pairs</code>
</pre>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Prepare the Annotation"><div class="sect2" id="prepare-the-annotation">
<h2>Prepare the Annotation</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="building a dataset" data-tertiary="preparing the annotation" data-type="indexterm" id="id767"/>The dataset annotation is not directly useful when training our model in this project, but it is handy to have readily accessible to perform all sorts of sanity checks. You will recognize the implementation from before:</p>

<pre data-code-language="python" data-type="programlisting">
    <code class="k">def</code> <code class="nf">prepare_annotation</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">:</code>
      <code class="sd">"""Annotates nodes by mapping node IDs to database IDs and drug names."""</code>
      <code class="n">ddi_descriptions</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code>
        <code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="bp">self</code><code class="o">.</code><code class="n">path</code><code class="si">}</code><code class="s2">/ogbl_ddi/mapping/ddi_description.csv.gz"</code>
      <code class="p">)</code>
      <code class="n">node_to_dbid_lookup</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code>
        <code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="bp">self</code><code class="o">.</code><code class="n">path</code><code class="si">}</code><code class="s2">/ogbl_ddi/mapping/nodeidx2drugid.csv.gz"</code>
      <code class="p">)</code>
      <code class="c1"># Merge first and second drug descriptions into a single lookup.</code>
      <code class="n">first_drug</code> <code class="o">=</code> <code class="n">ddi_descriptions</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code>
        <code class="p">:,</code> <code class="p">[</code><code class="s2">"first drug id"</code><code class="p">,</code> <code class="s2">"first drug name"</code><code class="p">]</code>
      <code class="p">]</code><code class="o">.</code><code class="n">rename</code><code class="p">(</code><code class="n">columns</code><code class="o">=</code><code class="p">{</code><code class="s2">"first drug id"</code><code class="p">:</code> <code class="s2">"dbid"</code><code class="p">,</code> <code class="s2">"first drug name"</code><code class="p">:</code> <code class="s2">"drug_name"</code><code class="p">})</code>

      <code class="n">second_drug</code> <code class="o">=</code> <code class="n">ddi_descriptions</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code>
        <code class="p">:,</code> <code class="p">[</code><code class="s2">"second drug id"</code><code class="p">,</code> <code class="s2">"second drug name"</code><code class="p">]</code>
      <code class="p">]</code><code class="o">.</code><code class="n">rename</code><code class="p">(</code>
        <code class="n">columns</code><code class="o">=</code><code class="p">{</code><code class="s2">"second drug id"</code><code class="p">:</code> <code class="s2">"dbid"</code><code class="p">,</code> <code class="s2">"second drug name"</code><code class="p">:</code> <code class="s2">"drug_name"</code><code class="p">}</code>
      <code class="p">)</code>
      <code class="n">dbid_to_name_lookup</code> <code class="o">=</code> <code class="p">(</code>
        <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">first_drug</code><code class="p">,</code> <code class="n">second_drug</code><code class="p">])</code>
        <code class="o">.</code><code class="n">drop_duplicates</code><code class="p">()</code>
        <code class="o">.</code><code class="n">reset_index</code><code class="p">(</code><code class="n">drop</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
      <code class="p">)</code>

      <code class="c1"># Merge with node-to-DBID lookup.</code>
      <code class="n">annotation</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code>
        <code class="n">node_to_dbid_lookup</code><code class="o">.</code><code class="n">rename</code><code class="p">(</code>
          <code class="n">columns</code><code class="o">=</code><code class="p">{</code><code class="s2">"drug id"</code><code class="p">:</code> <code class="s2">"dbid"</code><code class="p">,</code> <code class="s2">"node idx"</code><code class="p">:</code> <code class="s2">"node_id"</code><code class="p">}</code>
        <code class="p">),</code>
        <code class="n">dbid_to_name_lookup</code><code class="p">,</code>
        <code class="n">on</code><code class="o">=</code><code class="s2">"dbid"</code><code class="p">,</code>
        <code class="n">how</code><code class="o">=</code><code class="s2">"inner"</code><code class="p">,</code>
      <code class="p">)</code>
      <code class="k">return</code> <code class="n">annotation</code>
</pre>

<p>The annotation is the same for all the dataset splits; hence, we only need to prepare it once and assign it to the <code>Dataset</code>.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Prepare the Graph"><div class="sect2" id="prepare-the-graph">
<h2>Prepare the Graph</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="building a dataset" data-tertiary="preparing the graph" data-type="indexterm" id="ch04_graphs.html15"/>Next we look at <code>prepare_graph</code>, one of the main functions of the dataset builder:</p>

<pre data-code-language="python" data-type="programlisting">
  <code class="k">def</code> <code class="nf">prepare_graph</code><code class="p">(</code>
    <code class="bp">self</code><code class="p">,</code> <code class="n">n_nodes</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">pos_pairs</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code>
  <code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jraph</code><code class="o">.</code><code class="n">GraphsTuple</code><code class="p">:</code>
    <code class="sd">"""Prepares a Jraph graph from positive edge pairs."""</code>
    <code class="n">senders</code><code class="p">,</code> <code class="n">receivers</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">make_undirected</code><code class="p">(</code><code class="n">pos_pairs</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">pos_pairs</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">])</code>
    <code class="n">graph</code> <code class="o">=</code> <code class="n">jraph</code><code class="o">.</code><code class="n">GraphsTuple</code><code class="p">(</code>
      <code class="n">nodes</code><code class="o">=</code><code class="p">{</code><code class="s2">"gid"</code><code class="p">:</code> <code class="n">jnp</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="n">n_nodes</code><code class="p">)},</code>  <code class="c1"># Optional global node ID.</code>
      <code class="n">edges</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code>
      <code class="n">senders</code><code class="o">=</code><code class="n">senders</code><code class="p">,</code>
      <code class="n">receivers</code><code class="o">=</code><code class="n">receivers</code><code class="p">,</code>
      <code class="n">n_node</code><code class="o">=</code><code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">n_nodes</code><code class="p">]),</code>
      <code class="n">n_edge</code><code class="o">=</code><code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="nb">len</code><code class="p">(</code><code class="n">senders</code><code class="p">)]),</code>
      <code class="nb">globals</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">graph</code>
</pre>

<p>The <code>make_undirected</code> method ensures that the DDI graph is undirected, meaning the relationship between drugs <em>A-B</em> is equivalent to <em>B-A</em>. Since <code>jraph</code> does not offer a toggle between directed and undirected graphs, we need to represent all edges in both directions. This process, known as <em>symmetrizing</em> the graph, makes the adjacency relationships symmetric, effectively converting a directed graph into an undirected one. This transformation is applied across all dataset splits.</p>

<p>Practically speaking, implementing this transformation is straightforward. We start with the <code>pos_pairs</code> and add a corresponding set of edges where the sender and receiver nodes are swapped:</p>

<pre data-code-language="python" data-type="programlisting">
  <code class="nd">@staticmethod</code>
  <code class="k">def</code> <code class="nf">make_undirected</code><code class="p">(</code>
    <code class="n">senders</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">receivers</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code>
  <code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">tuple</code><code class="p">[</code><code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">]:</code>
    <code class="sd">"""Makes an undirected graph by duplicating edges in both directions."""</code>
    <code class="c1"># Jraph requires undirected graphs to have both A-&gt;B and B-&gt;A edges</code>
    <code class="c1"># explicitly.</code>
    <code class="n">senders_undir</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">concatenate</code><code class="p">((</code><code class="n">senders</code><code class="p">,</code> <code class="n">receivers</code><code class="p">))</code>
    <code class="n">receivers_undir</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">concatenate</code><code class="p">((</code><code class="n">receivers</code><code class="p">,</code> <code class="n">senders</code><code class="p">))</code>
    <code class="k">return</code> <code class="n">senders_undir</code><code class="p">,</code> <code class="n">receivers_undir</code>
</pre>

<p>Next, we prepare the graph using the main parameters that <code>GraphsTuple</code> expects: <code>senders</code> and <code>receivers</code>, which define the edges by specifying the source and destination nodes. Each node or edge can be annotated (although they are not here), with node annotations stored in <code>nodes</code> and edge annotations in <code>edges</code>. In addition, <code>GraphsTuple</code> incorporates metadata such as <code>n_nodes</code> and <code>n_edges</code>, which indicate the number of nodes and edges, respectively, and <code>globals</code>, which can store graph-level information such as a unique graph identifier or aggregated features. While we won’t use <code>globals</code> here, it remains available for scenarios requiring data applicable to the entire graph.</p>

<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>You may wonder why we sometimes need to pass the number of nodes independently from the graph. Why can’t this be inferred from the edges? This is because inferring node count from edges could miss isolated nodes, which have no connections to other nodes (i.e., no interactions with other drugs).</p>
</div>

<p>In general, <code>GraphsTuple</code> is a versatile data structure that can host data in various ways. Instead of having one <code>GraphsTuple</code> per data split, we could construct a single graph containing both training and evaluation datasets, using the <code>nodes</code> attribute to specify which set each node belongs to.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html15" data-type="indexterm" id="id768"/></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Prepare the Pairs"><div class="sect2" id="prepare-the-pairs">
<h2>Prepare the Pairs</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="building a dataset" data-tertiary="preparing the pairs" data-type="indexterm" id="ch04_graphs.html16"/>With the graph in place, we use the <code>prepare_pairs</code> method to obtain the drug–drug pairs—both positive and negative—that the model will classify as either connected or not:</p>

<pre data-code-language="python" data-type="programlisting">
  <code class="k">def</code> <code class="nf">prepare_pairs</code><code class="p">(</code>
    <code class="bp">self</code><code class="p">,</code> <code class="n">graph</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">pos_pairs</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">neg_pairs</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code> <code class="o">|</code> <code class="kc">None</code> <code class="o">=</code> <code class="kc">None</code>
  <code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Pairs</code><code class="p">:</code>
    <code class="sd">"""Prepares positive and negative edge pairs."""</code>
    <code class="k">if</code> <code class="n">neg_pairs</code> <code class="ow">is</code> <code class="kc">None</code><code class="p">:</code>
      <code class="n">neg_pairs</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">infer_negative_pairs</code><code class="p">(</code><code class="n">graph</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">Pairs</code><code class="p">(</code><code class="n">pos</code><code class="o">=</code><code class="n">pos_pairs</code><code class="p">,</code> <code class="n">neg</code><code class="o">=</code><code class="n">neg_pairs</code><code class="p">)</code>
</pre>

<p>For evaluation datasets, preparing pairs is straightforward, as we can directly use the positive and negative pairs provided by the OGBL dataset.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You might wonder why we don’t use the edges from the graph we just created to generate the positive pairs. The reason is that since we made the graph undirected, each positive pair is represented twice, which could lead to redundancy and errors during <span class="keep-together">evaluation</span>.</p>
</div>

<p>For the training dataset, preparing pairs is slightly more complex because negative pairs are not provided and must be inferred using the <code>infer_negative_pairs</code> method:</p>

<pre data-code-language="python" data-type="programlisting">
  <code class="k">def</code> <code class="nf">infer_negative_pairs</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">graph</code><code class="p">:</code> <code class="n">jraph</code><code class="o">.</code><code class="n">GraphsTuple</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Infers negative edge pairs in a graph."""</code>
    <code class="c1"># Initialize a matrix where all possible edges are marked as potential</code>
    <code class="c1"># negative edges (1).</code>
    <code class="n">neg_adj_mask</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">ones</code><code class="p">((</code><code class="n">graph</code><code class="o">.</code><code class="n">n_node</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">graph</code><code class="o">.</code><code class="n">n_node</code><code class="p">[</code><code class="mi">0</code><code class="p">]),</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">uint8</code><code class="p">)</code>

    <code class="c1"># Mask out existing edges in the graph (set to 0).</code>
    <code class="n">neg_adj_mask</code><code class="p">[</code><code class="n">graph</code><code class="o">.</code><code class="n">senders</code><code class="p">,</code> <code class="n">graph</code><code class="o">.</code><code class="n">receivers</code><code class="p">]</code> <code class="o">=</code> <code class="mi">0</code>

    <code class="c1"># Use the upper triangular part of the matrix to avoid duplicate pairs and</code>
    <code class="c1"># self-loops.</code>
    <code class="n">neg_adj_mask</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">triu</code><code class="p">(</code><code class="n">neg_adj_mask</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
    <code class="n">neg_pairs</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">neg_adj_mask</code><code class="o">.</code><code class="n">nonzero</code><code class="p">())</code><code class="o">.</code><code class="n">T</code>  <code class="c1"># Extract indices.</code>
    <code class="k">return</code> <code class="n">neg_pairs</code>
</pre>

<p>The method begins by constructing an adjacency matrix and initializing it with zeros using NumPy. It then marks all existing edges with ones. To identify negative edges, the method flips the matrix values so that connections become zeros and nonconnections become ones. Finally, it retains only the upper triangle (<code>triu</code>) of the matrix (excluding the diagonal) to avoid self-loops and duplicate pairs. The remaining nonzero entries are converted into an edge list of negative node pairs.</p>

<p>The resulting negative pairs far outnumber the positive pairs due to the graph’s sparsity. This imbalance can be advantageous, as it provides more examples of negative edges to sample. However, as mentioned previously, the way negative pairs are sampled significantly affects performance, as some pairs are trivial to predict as being unconnected. We will need to carefully select a fair subset of negative pairs during training.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Using an adjacency matrix approach assumes that it can fit into memory. If this is not feasible, alternative methods for generating negative node pairs include sampling a noncomprehensive subset or using efficient implementations that rely on sparse adjacency matrices.</p>
</div>

<p>The positive and negative pairs are then encapsulated in a <code>Pairs</code> data class, which we’ll examine further during training. It is a simple data class that stores arrays of positive and negative pairs and includes utilities for subsampling pairs during learning and accessing pairs in batches.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html16" data-type="indexterm" id="id769"/></p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Subsetting the Graph"><div class="sect2" id="subsetting-the-graph">
<h2>Subsetting the Graph</h2>
<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="building a dataset" data-tertiary="subsetting the graph" data-type="indexterm" id="id770"/>To efficiently explore how a model learns from graphs, it is useful to be able to create a smaller subset of the dataset. This allows us to work with a more manageable graph size for experimentation. The subset method does exactly that:</p>

<pre data-code-language="python" data-type="programlisting">
  <code class="k">def</code> <code class="nf">subset</code><code class="p">(</code>
    <code class="bp">self</code><code class="p">,</code>
    <code class="n">dataset_splits</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">Dataset</code><code class="p">],</code>
    <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code>
    <code class="n">node_limit</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>
    <code class="n">keep_original_ids</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">False</code><code class="p">,</code>
  <code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">Dataset</code><code class="p">]:</code>
    <code class="sd">"""Creates subset of dataset splits by sampling a fixed number of nodes."""</code>
    <code class="c1"># Get a random subset of node_ids.</code>
    <code class="n">node_ids</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code>
      <code class="n">rng</code><code class="p">,</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">n_nodes</code><code class="p">,</code> <code class="p">(</code><code class="n">node_limit</code><code class="p">,),</code> <code class="n">replace</code><code class="o">=</code><code class="kc">False</code>
    <code class="p">)</code>

    <code class="c1"># Subset every dataset split by the same node_ids.</code>
    <code class="n">dataset_subset_splits</code> <code class="o">=</code> <code class="p">{}</code>
    <code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">dataset</code> <code class="ow">in</code> <code class="n">dataset_splits</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
      <code class="n">dataset_subset_splits</code><code class="p">[</code><code class="n">name</code><code class="p">]</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">subset</code><code class="p">(</code><code class="n">node_ids</code><code class="p">,</code> <code class="n">keep_original_ids</code><code class="p">)</code>

    <code class="k">return</code> <code class="n">dataset_subset_splits</code>
</pre>

<p>It selects a subset of nodes based on a specified <code>node_limit</code> and applies this subset consistently across all dataset splits (i.e., training, validation, test). By default, the subsetted graph renumbers the node IDs to create a smaller, compact graph. However, you can retain the original node IDs from the full dataset by setting the <code>keep_original_ids</code> parameter to <code>True</code>.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="The Dataset Class"><div class="sect2" id="dataset-class">
<h2>The Dataset Class</h2>

<p><a contenteditable="false" data-primary="Dataset class" data-type="indexterm" id="id771"/><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="building a dataset" data-tertiary="creating Dataset class" data-type="indexterm" id="id772"/>Finally, we have all the pieces in place to create a <code>Dataset</code> class that will enable flexible exploration of the graph along with its annotations:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="nd">@dataclass</code>
<code class="k">class</code> <code class="nc">Dataset</code><code class="p">:</code>
  <code class="sd">"""Graph dataset with nodes, pairs, and optional annotations."""</code>

  <code class="n">n_nodes</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">graph</code><code class="p">:</code> <code class="n">jraph</code><code class="o">.</code><code class="n">GraphsTuple</code>
  <code class="n">pairs</code><code class="p">:</code> <code class="n">Pairs</code>
  <code class="n">annotation</code><code class="p">:</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code> <code class="o">=</code> <code class="n">field</code><code class="p">(</code><code class="n">default_factory</code><code class="o">=</code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">)</code>
  </pre>

<p>Here, we show only the <code>Dataset</code> fields, but the class also provides several useful methods. Notably, it handles subsetting the graph and managing annotations consistently behind the scenes. While we encourage you to explore the code online to get a better sense of its functionality, understanding every detail isn’t necessary to begin training.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html14" data-type="indexterm" id="id773"/></p>	
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Building a Prototype"><div class="sect1" id="building-a-prototype">
<h1>Building a Prototype</h1>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="building a prototype" data-type="indexterm" id="ch04_graphs.html17"/>Let’s start simple. We need to build a model that predicts links between nodes. We’ll do this using only the graph’s connectivity—no node features or annotations. Surprisingly, this connectivity information alone can be quite powerful for learning which node pairs are likely to be connected.</p>
<p>While link prediction can be framed as a binary classification task (i.e., connection versus no-connection), it differs from typical classification problems in key ways. The input to the model is not a single node or feature vector, but a pair of nodes, and the prediction depends on their structural relationship in the graph.</p>
<p>Our prototype model will be made up of several key components, some defined directly within the model and others handled as part of the training process:</p>

<ul>
  <li><p>Model components:</p>
  <ul>
    <li><p>Neighborhood encoding: Generates node embeddings that reflect the local graph structure</p></li>
    <li><p>Link prediction: Uses these embeddings to score the likelihood of a connection between node pairs</p></li>
  </ul></li>
  <li class="less_space pagebreak-before"><p>Training components:</p>
  <ul>
    <li><p>Negative sampling: Selects unconnected node pairs to contrast with true edges</p></li>
    <li><p>Loss function: Computes the training signal to optimize model performance</p></li>
  </ul></li>
</ul>


<p>We begin by focusing on the most crucial part: how we encode each node’s local neighborhood.</p>


<section data-type="sect2" data-pdf-bookmark="Node Encoder"><div class="sect2" id="node-encoder">
<h2>Node Encoder</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="building a prototype" data-tertiary="node encoder" data-type="indexterm" id="ch04_graphs.html18"/>Arguably the most impactful choice for our model is how we encode the nodes’ neighborhood. For this, we use a GraphSAGE-inspired implementation:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">class</code> <code class="nc">NodeEncoder</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Encodes nodes into embeddings using a two-layer GraphSAGE model."""</code>

  <code class="n">n_nodes</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">embedding_dim</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">last_layer_self</code><code class="p">:</code> <code class="nb">bool</code>
  <code class="n">degree_norm</code><code class="p">:</code> <code class="nb">bool</code>
  <code class="n">dropout_rate</code><code class="p">:</code> <code class="nb">float</code>

  <code class="k">def</code> <code class="nf">setup</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
    <code class="sd">"""Initializes node embeddings, which cover the full graph's n_nodes."""</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">node_embeddings</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Embed</code><code class="p">(</code>
      <code class="n">num_embeddings</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">n_nodes</code><code class="p">,</code>
      <code class="n">features</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">embedding_dim</code><code class="p">,</code>
      <code class="n">embedding_init</code><code class="o">=</code><code class="n">jax</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">initializers</code><code class="o">.</code><code class="n">glorot_uniform</code><code class="p">(),</code>
    <code class="p">)</code>

  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">graph</code><code class="p">:</code> <code class="n">jraph</code><code class="o">.</code><code class="n">GraphsTuple</code><code class="p">,</code> <code class="n">is_training</code><code class="p">:</code> <code class="nb">bool</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Encodes the nodes of a graph into embeddings."""</code>
    <code class="c1"># Graph can be a subgraph and thus we use a subset of embeddings</code>
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">node_embeddings</code><code class="p">(</code><code class="n">graph</code><code class="o">.</code><code class="n">nodes</code><code class="p">[</code><code class="s2">"gid"</code><code class="p">])</code>

    <code class="c1"># First convolutional layer.</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">SAGEConv</code><code class="p">(</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">embedding_dim</code><code class="p">,</code> <code class="n">with_self</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">degree_norm</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">degree_norm</code>
    <code class="p">)(</code><code class="n">graph</code><code class="p">,</code> <code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">rate</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">dropout_rate</code><code class="p">,</code> <code class="n">deterministic</code><code class="o">=</code><code class="ow">not</code> <code class="n">is_training</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>

    <code class="c1"># Second convolutional layer.</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">SAGEConv</code><code class="p">(</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">embedding_dim</code><code class="p">,</code>
      <code class="n">with_self</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">last_layer_self</code><code class="p">,</code>
      <code class="n">degree_norm</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">degree_norm</code><code class="p">,</code>
    <code class="p">)(</code><code class="n">graph</code><code class="p">,</code> <code class="n">x</code><code class="p">)</code>

    <code class="k">return</code> <code class="n">x</code>
</pre>

<p>The main input parameters to the module are:</p>

<dl>
	<dt><code>n_nodes</code></dt>
	<dd>
	<p>Defines the total number of nodes in the original graph.</p>
	</dd>
	<dt><code>embedding_dim</code></dt>
	<dd>
	<p>Specifies the dimensionality of the node embeddings. This controls how richly the model can represent neighborhood information. Lower values (e.g., 16 or 32) may limit expressiveness, while higher values (e.g., 128 or 256) offer more capacity at the cost of increased computation. In practice, smaller graphs can support higher embedding dimensions, while larger graphs benefit from lower values here.</p>
	</dd>
	<dt><code>dropout_rate</code></dt>
	<dd>
	<p>Sets the fraction of neurons randomly deactivated during training to reduce overfitting.</p>
	</dd>
	<dt><code>last_layer_self</code> and <code>degree_norm</code></dt>
	<dd>
	<p>Configure aspects of the graph convolution behavior, which are described in more detail in the next section.</p>
	</dd>
</dl>

<p>At the heart of GraphSAGE is the <code>node_embeddings</code> matrix. In the <code>setup</code> method, this is initialized as a learnable parameter using <code>nn.Embed</code>, with shape <code>[n_nodes, embedding_dim]</code>. The embeddings are initialized using the <code>glorot_uniform</code> method to promote stable training dynamics. During training, these embeddings are updated by aggregating information from each node’s neighbors, gradually encoding higher-order structural patterns. The goal is for these embeddings to converge to representations that reflect the likelihood of connections between node pairs.</p>
<p>The main logic of the encoder is implemented in the <code>__call__</code> method. One important (though hard-coded) design choice here is the number of <code>SAGEConv</code> layers, which defines how many rounds of message passing are applied:</p>

<ul class="simple">
	<li>
	<p>With one layer, each node aggregates information from its immediate neighbors.</p>
	</li>
	<li>
	<p>With two layers, each node can access information from neighbors up to two hops away.</p>
	</li>
</ul>

<p>Thus, the number of layers controls the receptive field of each node. Between layers, ReLU activation introduces nonlinearity, and dropout is applied for regularization to prevent overfitting.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html18" data-type="indexterm" id="id774"/></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Graph Convolution"><div class="sect2" id="graph-convolution">
<h2>Graph Convolution</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="building a prototype" data-tertiary="graph convolution" data-type="indexterm" id="ch04_graphs.html19"/><a contenteditable="false" data-primary="graph convolution" data-type="indexterm" id="ch04_graphs.html20"/>We’ve now reached the core architectural component of our model: the <code>SAGEConv</code> layer. Let’s dive into it:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">class</code> <code class="nc">SAGEConv</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""GraphSAGE convolutional layer with optional self-loops."""</code>

  <code class="n">embedding_dim</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">with_self</code><code class="p">:</code> <code class="nb">bool</code>
  <code class="n">degree_norm</code><code class="p">:</code> <code class="nb">bool</code>

  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">graph</code><code class="p">:</code> <code class="n">jraph</code><code class="o">.</code><code class="n">GraphsTuple</code><code class="p">,</code> <code class="n">x</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="n">n_nodes</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">get_n_nodes</code><code class="p">(</code><code class="n">graph</code><code class="p">)</code>

    <code class="c1"># Add self-loops if enabled.</code>
    <code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">with_self</code><code class="p">:</code>
      <code class="n">senders</code><code class="p">,</code> <code class="n">receivers</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_add_self_edges</code><code class="p">(</code><code class="n">graph</code><code class="p">,</code> <code class="n">n_nodes</code><code class="p">)</code>
    <code class="k">else</code><code class="p">:</code>
      <code class="n">senders</code><code class="p">,</code> <code class="n">receivers</code> <code class="o">=</code> <code class="n">graph</code><code class="o">.</code><code class="n">senders</code><code class="p">,</code> <code class="n">graph</code><code class="o">.</code><code class="n">receivers</code>

    <code class="c1"># Aggregate node features from neighbors.</code>
    <code class="k">if</code> <code class="ow">not</code> <code class="bp">self</code><code class="o">.</code><code class="n">degree_norm</code><code class="p">:</code>
      <code class="n">x_updated</code> <code class="o">=</code> <code class="n">jraph</code><code class="o">.</code><code class="n">segment_mean</code><code class="p">(</code>
        <code class="n">x</code><code class="p">[</code><code class="n">senders</code><code class="p">],</code> <code class="n">receivers</code><code class="p">,</code> <code class="n">num_segments</code><code class="o">=</code><code class="n">n_nodes</code>
      <code class="p">)</code>
    <code class="k">else</code><code class="p">:</code>

      <code class="k">def</code> <code class="nf">get_degree</code><code class="p">(</code><code class="n">n</code><code class="p">):</code>
        <code class="k">return</code> <code class="n">jax</code><code class="o">.</code><code class="n">ops</code><code class="o">.</code><code class="n">segment_sum</code><code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">ones_like</code><code class="p">(</code><code class="n">senders</code><code class="p">),</code> <code class="n">n</code><code class="p">,</code> <code class="n">n_nodes</code><code class="p">)</code>

      <code class="n">x_updated</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">normalize_by_degree</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">get_degree</code><code class="p">(</code><code class="n">senders</code><code class="p">))</code>
      <code class="n">x_updated</code> <code class="o">=</code> <code class="n">jraph</code><code class="o">.</code><code class="n">segment_mean</code><code class="p">(</code>
        <code class="n">x_updated</code><code class="p">[</code><code class="n">senders</code><code class="p">],</code> <code class="n">receivers</code><code class="p">,</code> <code class="n">num_segments</code><code class="o">=</code><code class="n">n_nodes</code>
      <code class="p">)</code>
      <code class="n">x_updated</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">normalize_by_degree</code><code class="p">(</code><code class="n">x_updated</code><code class="p">,</code> <code class="n">get_degree</code><code class="p">(</code><code class="n">receivers</code><code class="p">))</code>

    <code class="c1"># Combine node and neighbor embeddings by concatenation.</code>
    <code class="n">combined_embeddings</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">concatenate</code><code class="p">([</code><code class="n">x</code><code class="p">,</code> <code class="n">x_updated</code><code class="p">],</code> <code class="n">axis</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>

    <code class="k">return</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">embedding_dim</code><code class="p">)(</code><code class="n">combined_embeddings</code><code class="p">)</code>

  <code class="nd">@staticmethod</code>
  <code class="k">def</code> <code class="nf">_add_self_edges</code><code class="p">(</code>
    <code class="n">graph</code><code class="p">:</code> <code class="n">jraph</code><code class="o">.</code><code class="n">GraphsTuple</code><code class="p">,</code> <code class="n">n_nodes</code><code class="p">:</code> <code class="nb">int</code>
  <code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">tuple</code><code class="p">[</code><code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">]:</code>
    <code class="sd">"""Adds self-loops to the graph."""</code>
    <code class="n">all_nodes</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="n">n_nodes</code><code class="p">)</code>
    <code class="n">senders</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">concatenate</code><code class="p">([</code><code class="n">graph</code><code class="o">.</code><code class="n">senders</code><code class="p">,</code> <code class="n">all_nodes</code><code class="p">])</code>
    <code class="n">receivers</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">concatenate</code><code class="p">([</code><code class="n">graph</code><code class="o">.</code><code class="n">receivers</code><code class="p">,</code> <code class="n">all_nodes</code><code class="p">])</code>
    <code class="k">return</code> <code class="n">senders</code><code class="p">,</code> <code class="n">receivers</code>

  <code class="nd">@staticmethod</code>
  <code class="k">def</code> <code class="nf">normalize_by_degree</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">degree</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Normalizes node features by the square root of the degree."""</code>
    <code class="c1"># We set the the degree to a minimum of 1.</code>
    <code class="k">return</code> <code class="n">x</code> <code class="o">*</code> <code class="n">jax</code><code class="o">.</code><code class="n">lax</code><code class="o">.</code><code class="n">rsqrt</code><code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">maximum</code><code class="p">(</code><code class="n">degree</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">))[:,</code> <code class="kc">None</code><code class="p">]</code>

  <code class="nd">@staticmethod</code>
  <code class="k">def</code> <code class="nf">get_n_nodes</code><code class="p">(</code><code class="n">graph</code><code class="p">):</code>
    <code class="sd">"""Returns the number of nodes in the graph in a jittable way."""</code>
    <code class="k">return</code> <code class="n">tree</code><code class="o">.</code><code class="n">tree_leaves</code><code class="p">(</code><code class="n">graph</code><code class="o">.</code><code class="n">nodes</code><code class="p">)[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
</pre>

<p>When setting up a <code>SAGEConv</code> layer, we specify the embedding dimension (using <code>embedding_dim</code>), whether to add self-loops (<code>with_self</code>), and whether to apply degree normalization (<code>degree_norm</code>). The latter two options are optional, as their impact on model performance depends on the dataset’s characteristics, such as size and connectivity patterns. Enabling or disabling these features can significantly influence model behavior.</p>

<p>Each <code>SAGEConv</code> layer performs the following key steps:</p>

<dl>
	<dt>Optionally adds self-edges</dt>
	<dd>
	<p>Allows each node to consider its own embedding during aggregation</p>
	</dd>
	<dt>Aggregates neighborhood embeddings</dt>
	<dd>
	<p>Collects and averages the embeddings of each node’s neighbors</p>
	</dd>
	<dt>Optionally normalizes by degree</dt>
	<dd>
	<p>Scales the contribution of neighbors based on their degree, reducing bias from highly connected nodes</p>
	</dd>
	<dt>Combines embeddings with neighbors</dt>
	<dd>
	<p>Merges the original node embeddings with the aggregated neighbor embeddings</p>
	</dd>
</dl>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>When reading the implementation of the <code>SAGEConv</code> convolutional layer, you might be wondering: <em>where’s the convolution?</em></p>

<p><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="GNNs versus" data-type="indexterm" id="id775"/><a contenteditable="false" data-primary="graph neural networks (GNNs)" data-secondary="CNNs versus" data-type="indexterm" id="id776"/>Unlike convolutional neural networks (CNNs) for images or sequences, where <code>nn.Conv</code> applies spatial filters over regular grids, graph neural networks define “convolution” more abstractly. In GNNs, convolution means aggregating information from a node’s local neighborhood and combining it with its own embedding. This is implemented using operations like <code>segment_mean</code>, followed by a learnable transformation—typically via <code>nn.Dense</code>. So, while you won’t see an explicit <code>nn.Conv</code> in <code>SAGEConv</code>, the dense layer at the end serves as the core trainable part of the “graph convolution.”</p>
</div>

<p>It’s worth noting that <code>SAGEConv</code> focuses solely on aggregation and linear transformation. It does not include nonlinear activations, batch normalization, or dropout—those are typically applied at the model level to allow for greater flexibility and reuse.</p>

<section data-type="sect3" data-pdf-bookmark="Adding self edges"><div class="sect3" id="adding-self-edges">
<h3>Adding self edges</h3>

<p><a contenteditable="false" data-primary="graph convolution" data-secondary="adding self edges" data-type="indexterm" id="id777"/>In the <code>NodeEncoder</code>, we use two <code>SAGEConv</code> layers, and the first layer has the <code>with_self</code> parameter set to <code>True</code>. Adding self-loops ensures that a node’s own embedding is included in the aggregation process during neighborhood updates (by including the node in its own list of senders). Without self-loops, a node’s updated embedding would reflect only information from its neighbors, potentially biasing the representation away from its original identity. Including self-loops allows each node to contribute to its own update, balancing its existing features with neighborhood context.</p>

<p>This can be accomplished by adding ones to the diagonal of the adjacency matrix, effectively connecting each node to itself. In our implementation, the <code>with_self</code> and <code>last_layer_self</code> parameters control whether self-loops are included in the first and second <code>SAGEConv</code> layers, respectively.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Even without explicit self-loops, models can preserve a node’s identity by using skip connections. For example, in our implementation, we concatenate the original and updated embeddings using <code>combined_embeddings = jnp.concatenate([x, x_updated], axis=-1)</code>. This helps retain the node’s original features. However, it differs from self-loops in that the original embedding is added after the neighborhood aggregation, not as part of it. Including self-loops ensures that the node’s identity contributes directly to the aggregation step itself.</p>
</div>

</div></section>

<section data-type="sect3" data-pdf-bookmark="Aggregating the neighborhood"><div class="sect3" id="aggregating-the-neighborhood">
<h3>Aggregating the neighborhood</h3>

<p><a contenteditable="false" data-primary="graph convolution" data-secondary="aggregating the neighborhood" data-type="indexterm" id="id778"/>We now turn to the core operation of message passing: aggregating information from each node’s neighborhood.</p>

<p>This is the first point where the graph structure is explicitly used. For each edge, the sender node’s embedding (or features) is gathered and aggregated on a per-receiver basis. In other words, each receiver node updates its embedding by combining information from all its connected senders. The result is a new representation that reflects the structure and features of its local neighborhood.</p>

<p>In our implementation, we use <code>jraph.segment_mean</code> as the aggregation function, which computes the mean of sender embeddings for each receiver. Other common choices include sum, max, or even attention-weighted aggregation, as used in GAT-style models. The optimal aggregation method often depends on the graph’s topology and the downstream task, so experimenting with different strategies can be valuable.</p>

</div></section>

<section data-type="sect3" data-pdf-bookmark="Normalizing by degree"><div class="sect3" id="normalize-by-degree">
<h3>Normalizing by degree</h3>

<p><a contenteditable="false" data-primary="degree normalization" data-type="indexterm" id="id779"/><a contenteditable="false" data-primary="graph convolution" data-secondary="normalizing by degree" data-type="indexterm" id="id780"/>Degree normalization ensures that all nodes, regardless of their connectivity, contribute more evenly during training. This can help stabilize optimization, avoiding exploding or vanishing gradients. However, excessive normalization may <em>over-smooth</em> node embeddings, potentially erasing finer details of a node’s local structure.</p>

<p><a contenteditable="false" data-primary="symmetric degree normalization" data-type="indexterm" id="id781"/>In our implementation, we apply <em>symmetric degree normalization</em>, which works as follows:</p>

<ul class="simple">
	<li>
	<p>Messages are divided by the square root of the <em>sender’s degree</em> before aggregation.</p>
	</li>
	<li>
	<p>The aggregated result is then divided by the square root of the <em>receiver’s degree</em>.</p>
	</li>
</ul>

<p>This square-root scaling—common in various flavors of GNNs—balances message influence across nodes while avoiding unintended scaling effects.</p>

<div data-type="tip"><h6>Tip</h6>
<p>Whether degree normalization improves performance depends on the dataset and its structural properties. It’s especially helpful in graphs with a high degree of variability, where a few high-degree nodes might otherwise dominate message passing. It also improves stability in deeper GNNs by keeping signal magnitudes under <span class="keep-together">control</span>.</p>
<p>However, in graphs where raw connection strength or node centrality carries important meaning—such as physical interaction networks, citation graphs, or transportation systems—normalization may suppress informative signals. If time allows, it’s often worth comparing normalized and unnormalized variants during model development.</p>
</div>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Combining embeddings with neighborhoods"><div class="sect3" id="combine-embeddings-with-neighborhood">
<h3>Combining embeddings with neighborhoods</h3>

<p><a contenteditable="false" data-primary="graph convolution" data-secondary="combining embeddings with neighborhoods" data-type="indexterm" id="id782"/>After aggregation (and optional normalization), we combine the updated node embeddings with the original embeddings—typically by concatenation. This produces a unified representation that captures both the node’s initial features and the information gathered from its local neighborhood. This enrichment step ensures that embeddings reflect both individual identity and structural context.</p>

<p>Concatenating the original and aggregated embeddings doubles the feature dimensionality. To bring this back to the intended embedding size, the combined vector is passed through a fully connected <code>Dense</code> layer. This layer serves two key purposes:</p>

<dl>
	<dt>Maintains consistent dimensionality</dt>
	<dd>
	<p>Projects the concatenated vector back to the original embedding size, ensuring compatibility with the next model layer</p>
	</dd>
	<dt>Learns better representations</dt>
	<dd>
	<p>Learns how to optimally fuse original and neighborhood features, enabling the model to refine what information to retain or emphasize</p>
	</dd>
</dl>

<p>Because the <code>Dense</code> transformation is learnable, the model adapts over training to make the most effective use of both types of input.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>During model training, the graph structure remains fixed. Only the <code>node_embeddings</code> are updated, evolving over time as the model learns from the neighborhood aggregation and feature transformation process.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html20" data-type="indexterm" id="id783"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html19" data-type="indexterm" id="id784"/></p>
</div>
</div></section>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Link Prediction"><div class="sect2" id="predicting-links">
<h2>Link Prediction</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="building a prototype" data-tertiary="link prediction" data-type="indexterm" id="ch04_graphs.html21"/>We now use the learned node embeddings to predict whether a given pair of nodes is connected. If the embeddings have effectively captured the graph’s structure, the model should be able to assign high scores to true edges and low scores to unrelated node pairs.</p>

<p>Let’s look at how the embeddings are used in the <code>LinkPredictor</code> module:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">class</code> <code class="nc">LinkPredictor</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Predicts interaction scores for pairs of node embeddings."""</code>

  <code class="n">embedding_dim</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">n_layers</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">dropout_rate</code><code class="p">:</code> <code class="nb">float</code>

  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code>
    <code class="bp">self</code><code class="p">,</code>
    <code class="n">sender_embeddings</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code>
    <code class="n">receiver_embeddings</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code>
    <code class="n">is_training</code><code class="p">:</code> <code class="nb">bool</code><code class="p">,</code>
  <code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Computes scores for node pairs."""</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">sender_embeddings</code> <code class="o">*</code> <code class="n">receiver_embeddings</code>  <code class="c1"># Element-wise multiplication.</code>

    <code class="c1"># Apply MLP layers with ReLU activation and dropout.</code>
    <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">n_layers</code><code class="p">)[:</code><code class="o">-</code><code class="mi">1</code><code class="p">]:</code>
      <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">embedding_dim</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
      <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
      <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dropout_rate</code><code class="p">,</code> <code class="n">deterministic</code><code class="o">=</code><code class="ow">not</code> <code class="n">is_training</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>

    <code class="c1"># Final output layer is a single neuron. Logit output used for binary link</code>
    <code class="c1"># classification.</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>

    <code class="k">return</code> <code class="n">jnp</code><code class="o">.</code><code class="n">squeeze</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
</pre>

<p>The <code>LinkPredictor</code> takes a pair of node embeddings—one from the sender and one from the receiver—and estimates the likelihood of an edge between them. Here’s how it works:</p>

<dl>
	<dt>Combining embeddings</dt>
	<dd>
	<p>The sender and receiver embeddings are combined using element-wise multiplication. This operation captures the interaction between corresponding dimensions of each embedding, producing a fixed-size vector that reflects their pairwise compatibility.</p>
	</dd>
	<dt>Transforming representations</dt>
	<dd>
	<p>The combined vector is passed through a multilayer perceptron (MLP), consisting of several <code>Dense</code> layers with ReLU activation and dropout. These layers are defined by the <code>n_layers</code> and <code>embedding_dim</code> parameters and serve to learn increasingly abstract representations of the node pair interaction.</p>
	</dd>
	<dt>Output layer</dt>
	<dd>
	<p>The final layer is a single-neuron <code>Dense</code> layer that outputs a logit—an unnormalized score representing the likelihood of an edge. During training, this logit is passed through a sigmoid to produce a probability between 0 and 1.</p>
	</dd>
</dl>
<p>The overall goal is for the model to learn to output high scores for true (positive) edges and low scores for negative edges, forming the basis for binary link prediction.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html21" data-type="indexterm" id="id785"/></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Drug–Drug Interaction Model"><div class="sect2" id="drug-drug-interaction-model">
<h2>Drug–Drug Interaction Model</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="building a prototype" data-tertiary="drug–drug interaction model completion" data-type="indexterm" id="ch04_graphs.html22"/>Let’s now put everything together into a <code>DdiModel</code> that we can train:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">class</code> <code class="nc">DdiModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Graph-based model for predicting drug-drug interactions (DDIs)."""</code>

  <code class="n">n_nodes</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">embedding_dim</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">dropout_rate</code><code class="p">:</code> <code class="nb">float</code>
  <code class="n">last_layer_self</code><code class="p">:</code> <code class="nb">bool</code>
  <code class="n">degree_norm</code><code class="p">:</code> <code class="nb">bool</code>
  <code class="n">n_mlp_layers</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">2</code>

  <code class="k">def</code> <code class="nf">setup</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
    <code class="sd">"""Initializes the node encoder and link predictor modules."""</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">node_encoder</code> <code class="o">=</code> <code class="n">NodeEncoder</code><code class="p">(</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">n_nodes</code><code class="p">,</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">embedding_dim</code><code class="p">,</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">last_layer_self</code><code class="p">,</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">degree_norm</code><code class="p">,</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">dropout_rate</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">link_predictor</code> <code class="o">=</code> <code class="n">LinkPredictor</code><code class="p">(</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">embedding_dim</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">n_mlp_layers</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">dropout_rate</code>
    <code class="p">)</code>

  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code>
    <code class="bp">self</code><code class="p">,</code>
    <code class="n">graph</code><code class="p">:</code> <code class="n">jraph</code><code class="o">.</code><code class="n">GraphsTuple</code><code class="p">,</code>
    <code class="n">pairs</code><code class="p">:</code> <code class="nb">dict</code><code class="p">,</code>
    <code class="n">is_training</code><code class="p">:</code> <code class="nb">bool</code><code class="p">,</code>
    <code class="n">is_pred</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">False</code><code class="p">,</code>
  <code class="p">):</code>
    <code class="sd">"""Generates interaction scores for node pairs."""</code>
    <code class="c1"># Compute node embeddings. The 'h' stands for hidden state or embedding.</code>
    <code class="n">h</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">node_encoder</code><code class="p">(</code><code class="n">graph</code><code class="p">,</code> <code class="n">is_training</code><code class="p">)</code>

    <code class="k">if</code> <code class="n">is_pred</code><code class="p">:</code>
      <code class="n">scores</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">link_predictor</code><code class="p">(</code><code class="n">h</code><code class="p">[</code><code class="n">pairs</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">]],</code> <code class="n">h</code><code class="p">[</code><code class="n">pairs</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">]],</code> <code class="kc">False</code><code class="p">)</code>

    <code class="k">else</code><code class="p">:</code>
      <code class="n">pos_senders</code><code class="p">,</code> <code class="n">pos_receivers</code> <code class="o">=</code> <code class="n">pairs</code><code class="p">[</code><code class="s2">"pos"</code><code class="p">][:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">pairs</code><code class="p">[</code><code class="s2">"pos"</code><code class="p">][:,</code> <code class="mi">1</code><code class="p">]</code>
      <code class="n">neg_senders</code><code class="p">,</code> <code class="n">neg_receivers</code> <code class="o">=</code> <code class="n">pairs</code><code class="p">[</code><code class="s2">"neg"</code><code class="p">][:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">pairs</code><code class="p">[</code><code class="s2">"neg"</code><code class="p">][:,</code> <code class="mi">1</code><code class="p">]</code>
      <code class="n">scores</code> <code class="o">=</code> <code class="p">{</code>
        <code class="s2">"pos"</code><code class="p">:</code> <code class="bp">self</code><code class="o">.</code><code class="n">link_predictor</code><code class="p">(</code>
          <code class="n">h</code><code class="p">[</code><code class="n">pos_senders</code><code class="p">],</code> <code class="n">h</code><code class="p">[</code><code class="n">pos_receivers</code><code class="p">],</code> <code class="n">is_training</code>
        <code class="p">),</code>
        <code class="s2">"neg"</code><code class="p">:</code> <code class="bp">self</code><code class="o">.</code><code class="n">link_predictor</code><code class="p">(</code>
          <code class="n">h</code><code class="p">[</code><code class="n">neg_senders</code><code class="p">],</code> <code class="n">h</code><code class="p">[</code><code class="n">neg_receivers</code><code class="p">],</code> <code class="n">is_training</code>
        <code class="p">),</code>
      <code class="p">}</code>
    <code class="k">return</code> <code class="n">scores</code>

  <code class="k">def</code> <code class="nf">create_train_state</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">dummy_input</code><code class="p">,</code> <code class="n">tx</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">TrainState</code><code class="p">:</code>
    <code class="sd">"""Initializes the training state with model parameters."""</code>
    <code class="n">rng</code><code class="p">,</code> <code class="n">rng_init</code><code class="p">,</code> <code class="n">rng_dropout</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
    <code class="n">variables</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">init</code><code class="p">(</code><code class="n">rng_init</code><code class="p">,</code> <code class="n">is_training</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="o">**</code><code class="n">dummy_input</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">TrainState</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
      <code class="n">apply_fn</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">apply</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="n">variables</code><code class="p">[</code><code class="s2">"params"</code><code class="p">],</code> <code class="n">tx</code><code class="o">=</code><code class="n">tx</code><code class="p">,</code> <code class="n">key</code><code class="o">=</code><code class="n">rng_dropout</code>
    <code class="p">)</code>

  <code class="nd">@staticmethod</code>
  <code class="k">def</code> <code class="nf">add_mean_embedding</code><code class="p">(</code><code class="n">embeddings</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Concatenates a mean embedding to the existing embeddings."""</code>
    <code class="n">mean_embeddings</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">embeddings</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">keepdims</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
    <code class="n">embeddings</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">concatenate</code><code class="p">([</code><code class="n">embeddings</code><code class="p">,</code> <code class="n">mean_embeddings</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">embeddings</code>
</pre>

<p>Let’s walk through the main components of this code:</p>

<ul>
<li><code>setup</code> method: Initializes two core submodules:
	<ul> 
 		<li><code>node_encoder</code>: Generates node embeddings from the input graph</li>
 		<li><code>link_predictor</code>: Scores node pairs based on their embeddings to predict the presence or absence of an edge</li>
 	</ul>
</li>
<li><code>__call__</code> method: Defines the forward pass of the model and supports both training and inference:
	<ul>
 		<li>The <code>node_encoder</code> computes embeddings <code>h</code> for all nodes in the graph. The use of <code>h</code> as a variable name follows a common convention, where <code>h</code> represents a “hidden state” or embedding.</li>
 		<li>If <code>is_pred=False</code> (training mode):
 			<ul>
 			<li>Positive pairs: The model passes sender and receiver embeddings through the <code>link_predictor</code> to estimate connection likelihood.</li> 
 			<li>Negative pairs: Similarly processed to estimate nonconnection scores.</li>
       <li>Returns a dictionary of predicted scores for both “pos” and “neg” pairs.</li>
       </ul>
 		</li>
        <li>And if <code>is_pred=True</code> (inference mode): Takes an arbitrary array of node pairs and returns predicted scores. This enables applying the model to new or unseen node pairs after training.</li>
      
 	</ul>
 </li>
<li><code>create_train_state</code> method: Sets up the training process:
	<ul>
 		<li>Initializes model parameters using dummy inputs and a random seed</li>
 		<li>Constructs a <code>TrainState</code> object with the model’s <code>apply_fn</code>, parameters, optimizer (<code>tx</code>), and dropout key for training</li>
 	</ul>
</li>
<li><code>add_mean_embedding</code> static method: Appends a global mean embedding to the existing embedding matrix. This can be useful in downstream tasks where a graph-level summary representation is desired.</li>
</ul>

<p>Together, these components define a full link prediction pipeline for drug–drug interaction graphs. Next, we’ll look at how to train this model end to end<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html22" data-type="indexterm" id="id786"/>.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html17" data-type="indexterm" id="id787"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Training the Model"><div class="sect1" id="training-the-model_99585222">
<h1>Training the Model</h1>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="training the model" data-type="indexterm" id="ch04_graphs.html23"/><a contenteditable="false" data-primary="training" data-secondary="drug–drug interaction graphs" data-tertiary="training the model" data-type="indexterm" id="ch04_graphs.html24"/>From the previous sections, we have established how to prepare datasets and define a model. Now we’ll proceed by creating instances of both before moving forward with training.</p>

<section data-type="sect2" data-pdf-bookmark="Create a Manageable Dataset"><div class="sect2" id="create-a-manageable-dataset">
<h2>Create a Manageable Dataset</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="training the model" data-tertiary="creating manageable dataset" data-type="indexterm" id="id788"/><a contenteditable="false" data-primary="training" data-secondary="drug–drug interaction graphs" data-tertiary="creating manageable dataset" data-type="indexterm" id="id789"/>We will create a subset containing approximately 10% of the total graph data:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">node_limit</code> <code class="o">=</code> <code class="mi">500</code>
<code class="n">rng</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">rng</code><code class="p">,</code> <code class="n">rng_dataset</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>

<code class="n">dataset_splits</code> <code class="o">=</code> <code class="n">DatasetBuilder</code><code class="p">(</code><code class="n">path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"graphs/datasets"</code><code class="p">))</code><code class="o">.</code><code class="n">build</code><code class="p">(</code>
  <code class="n">node_limit</code><code class="p">,</code> <code class="n">rng_dataset</code>
<code class="p">)</code>
</pre>


<p>By reducing the dataset size, we create a graph that is easier to handle during initial experimentation. This smaller graph allows us to test the model architecture and training setup more efficiently.</p>

<p>We now have a graph with a set of positive and negative node pairs that we can learn from. The visualization in <a data-type="xref" href="#ddi-training-subgraph">Figure 4-8</a> provides a high-level view of the training dataset’s structure, where nodes represent drugs and edges represent interactions between them. The circular layout arranges all nodes around a circle, with edges connecting related nodes.</p>

<figure><div id="ddi-training-subgraph" class="figure"><img alt="" src="assets/dlfb_0408.png" width="600" height="610"/>
<h6><span class="label">Figure 4-8. </span>Circular layout of training dataset of 500 nodes. Each node represents a drug, and edges represent interactions between them.</h6>
</div></figure>

<p>While the individual node labels and details may not be legible in this plot, it offers a broad overview of key graph properties. These include the density of connections, overall sparsity, and presence of clusters or isolated nodes. This visualization illustrates the graph’s complexity, despite being a small subsampled set.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">dlfb.graphs.inspect</code> <code class="kn">import</code> <code class="n">plot_graph</code>

<code class="n">plot_graph</code><code class="p">(</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]);</code>
</pre>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Create the Training Loop"><div class="sect2" id="the-training-loop">
<h2>Create the Training Loop</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="training the model" data-tertiary="creating training loop" data-type="indexterm" id="ch04_graphs.html25"/><a contenteditable="false" data-primary="training loop" data-secondary="for drug–drug interaction graphs" data-type="indexterm" id="ch04_graphs.html26"/>Next, let’s examine the training loop:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="nd">@restorable</code>
<code class="k">def</code> <code class="nf">train</code><code class="p">(</code>
  <code class="n">state</code><code class="p">:</code> <code class="n">TrainState</code><code class="p">,</code>
  <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">Dataset</code><code class="p">],</code>
  <code class="n">num_epochs</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>
  <code class="n">loss_fn</code><code class="p">:</code> <code class="n">Callable</code><code class="p">,</code>
  <code class="n">norm_loss</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">False</code><code class="p">,</code>
  <code class="n">eval_every</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">10</code><code class="p">,</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">tuple</code><code class="p">[</code><code class="n">TrainState</code><code class="p">,</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">list</code><code class="p">[</code><code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">float</code><code class="p">]]]]]:</code>
  <code class="sd">"""Training loop for the drug-drug interaction model."""</code>
  <code class="c1"># Initialize metrics and estimate optimal batch sizes.</code>
  <code class="n">metrics</code> <code class="o">=</code> <code class="n">MetricsLogger</code><code class="p">()</code>
  <code class="n">batch_size</code> <code class="o">=</code> <code class="n">optimal_batch_size</code><code class="p">(</code><code class="n">dataset_splits</code><code class="p">)</code>

  <code class="c1"># Epochs with progress bar.</code>
  <code class="n">epochs</code> <code class="o">=</code> <code class="n">tqdm</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">num_epochs</code><code class="p">))</code>
  <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="n">epochs</code><code class="p">:</code>
    <code class="n">epochs</code><code class="o">.</code><code class="n">set_description</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Epoch </code><code class="si">{</code><code class="n">epoch</code> <code class="o">+</code> <code class="mi">1</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
    <code class="n">rng</code><code class="p">,</code> <code class="n">rng_shuffle</code><code class="p">,</code> <code class="n">rng_sample</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>

    <code class="c1"># Training loop.</code>
    <code class="k">for</code> <code class="n">pairs_batch</code> <code class="ow">in</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">pairs</code><code class="o">.</code><code class="n">get_train_batches</code><code class="p">(</code>
      <code class="n">batch_size</code><code class="p">,</code> <code class="n">rng_shuffle</code><code class="p">,</code> <code class="n">rng_sample</code>
    <code class="p">):</code>
      <code class="n">rng</code><code class="p">,</code> <code class="n">rng_dropout</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
      <code class="n">state</code><code class="p">,</code> <code class="n">batch_metrics</code> <code class="o">=</code> <code class="n">train_step</code><code class="p">(</code>
        <code class="n">state</code><code class="p">,</code>
        <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">graph</code><code class="p">,</code>
        <code class="n">pairs_batch</code><code class="p">,</code>
        <code class="n">rng_dropout</code><code class="p">,</code>
        <code class="n">loss_fn</code><code class="p">,</code>
        <code class="n">norm_loss</code><code class="p">,</code>
      <code class="p">)</code>
      <code class="n">metrics</code><code class="o">.</code><code class="n">log_step</code><code class="p">(</code><code class="n">split</code><code class="o">=</code><code class="s2">"train"</code><code class="p">,</code> <code class="o">**</code><code class="n">batch_metrics</code><code class="p">)</code>

    <code class="c1"># Evaluation loop.</code>
    <code class="k">if</code> <code class="n">epoch</code> <code class="o">%</code> <code class="n">eval_every</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
      <code class="k">for</code> <code class="n">pairs_batch</code> <code class="ow">in</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">]</code><code class="o">.</code><code class="n">pairs</code><code class="o">.</code><code class="n">get_eval_batches</code><code class="p">(</code>
        <code class="n">batch_size</code>
      <code class="p">):</code>
        <code class="n">batch_metrics</code> <code class="o">=</code> <code class="n">eval_step</code><code class="p">(</code>
          <code class="n">state</code><code class="p">,</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">]</code><code class="o">.</code><code class="n">graph</code><code class="p">,</code> <code class="n">pairs_batch</code><code class="p">,</code> <code class="n">loss_fn</code><code class="p">,</code> <code class="n">norm_loss</code>
        <code class="p">)</code>
        <code class="n">metrics</code><code class="o">.</code><code class="n">log_step</code><code class="p">(</code><code class="n">split</code><code class="o">=</code><code class="s2">"valid"</code><code class="p">,</code> <code class="o">**</code><code class="n">batch_metrics</code><code class="p">)</code>

    <code class="n">metrics</code><code class="o">.</code><code class="n">flush</code><code class="p">(</code><code class="n">epoch</code><code class="o">=</code><code class="n">epoch</code><code class="p">)</code>
    <code class="n">epochs</code><code class="o">.</code><code class="n">set_postfix_str</code><code class="p">(</code><code class="n">metrics</code><code class="o">.</code><code class="n">latest</code><code class="p">([</code><code class="s2">"hits@20"</code><code class="p">]))</code>

  <code class="k">return</code> <code class="n">state</code><code class="p">,</code> <code class="n">metrics</code><code class="o">.</code><code class="n">export</code><code class="p">()</code>
</pre>

<p>The <code>train</code> function coordinates the entire training process, broken down into several key stages:</p>

<ol class="arabic simple">
	<li><p><em>Initialization</em></p>
	<ul class="simple">
	<li><p>Initializes a <code>MetricsLogger</code> to track training and evaluation metrics</p></li>
	<li><p>Estimates an optimal batch size using the <code>optimal_batch_size</code> function</p></li>
	<li><p>Displays training progress using a <code>tqdm</code> progress bar</p></li>
	</ul>
	</li>

	<li><p><em>Training over epochs</em></p>
		<p>For each epoch:</p>
	<ul class="simple">
	<li><p>Sets up random number generators (<code>rng</code>) for shuffling and sampling training pairs.</p></li>
	<li><p>The training set is iterated over in batches using <code>get_train_batches</code>, which samples positive and negative node pairs.</p></li>
	<li><p>Each batch is passed to <code>train_step</code>, which updates model parameters based on the current loss.</p></li>
	<li><p>Training metrics (e.g., loss and hits@20) are logged after each batch via <span class="keep-together"><code>metrics.log_step</code></span>.</p></li>
	</ul>
	</li>

	<li><p><em>Evaluation</em></p>
	<ul class="simple">
	<li><p>At intervals defined by <code>eval_every</code>, the model is evaluated on the validation set using <code>get_eval_batches</code> and <code>eval_step</code>.</p></li>
	<li><p>Evaluation metrics are logged via <code>metrics.log_step</code>, and summary statistics are printed using <code>metrics.latest()</code>.</p></li>
	</ul>
	</li>
</ol>

<p>Additional features of this training loop include:</p>

<dl>
	<dt>Support for custom loss functions</dt>
	<dd><p>A user-defined <code>loss_fn</code> can be passed in to control the optimization objective.</p></dd>
	<dt>Optional loss normalization</dt>
	<dd><p>Controlled by the <code>norm_loss</code> flag. When <code>True</code>, the loss is averaged over examples to ensure scale invariance across batch sizes.</p></dd>
	<dt>Restorable training state</dt>
	<dd><p>The function is decorated with <code>@restorable</code>, enabling checkpointing and resumption of training mid-run.</p></dd>
</dl>

<p>The dataset is processed in batches using the <code>Pairs</code> class, which handles consistent sampling of node pairs for both training and evaluation. We’ll take a closer look at how this class works next.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html26" data-type="indexterm" id="id790"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html25" data-type="indexterm" id="id791"/></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Create the Pairs Class"><div class="sect2" id="the-pairs-class">
<h2>Create the Pairs Class</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="training the model" data-tertiary="creating Pairs class" data-type="indexterm" id="ch04_graphs.html27"/><a contenteditable="false" data-primary="Pairs class" data-type="indexterm" id="ch04_graphs.html28"/><a contenteditable="false" data-primary="training" data-secondary="drug–drug interaction graphs" data-tertiary="creating Pairs class" data-type="indexterm" id="ch04_graphs.html29"/>The <code>Pairs</code> class is a utility that simplifies the handling of positive and negative node pairs during training and evaluation. You’ve already seen it in action within the training loop. Here, we’ll break down its functionality more explicitly:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="nd">@dataclass</code>
<code class="k">class</code> <code class="nc">Pairs</code><code class="p">:</code>
  <code class="sd">"""Represents positive and negative pairs of drug-drug interactions."""</code>

  <code class="n">pos</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code>
  <code class="n">neg</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code>

  <code class="k">def</code> <code class="nf">get_eval_batches</code><code class="p">(</code>
    <code class="bp">self</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">:</code> <code class="nb">int</code>
  <code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Generator</code><code class="p">[</code><code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">],</code> <code class="kc">None</code><code class="p">,</code> <code class="kc">None</code><code class="p">]:</code>
    <code class="sd">"""Generates evaluation batches of positive and negative pairs."""</code>
    <code class="n">indices</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">_n_pairs</code><code class="p">())</code>
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">_n_batches</code><code class="p">(</code><code class="n">batch_size</code><code class="p">)):</code>
      <code class="n">batch_indices</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">indices</code><code class="p">[</code><code class="n">i</code> <code class="o">*</code> <code class="n">batch_size</code> <code class="p">:</code> <code class="p">(</code><code class="n">i</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code> <code class="o">*</code> <code class="n">batch_size</code><code class="p">])</code>
      <code class="k">yield</code> <code class="n">Pairs</code><code class="p">(</code>
        <code class="n">pos</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">pos</code><code class="p">[</code><code class="n">batch_indices</code><code class="p">],</code> <code class="n">neg</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">neg</code><code class="p">[</code><code class="n">batch_indices</code><code class="p">]</code>
      <code class="p">)</code><code class="o">.</code><code class="n">to_dict</code><code class="p">()</code>

  <code class="k">def</code> <code class="nf">_n_batches</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>
    <code class="sd">"""Calculates number of batches in the dataset given a batch size."""</code>
    <code class="k">return</code> <code class="nb">int</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">floor</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">_n_pairs</code><code class="p">()</code> <code class="o">/</code> <code class="n">batch_size</code><code class="p">))</code>

  <code class="k">def</code> <code class="nf">_n_pairs</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>
    <code class="sd">"""Returns the smaller number of positive or negative pairs."""</code>
    <code class="k">return</code> <code class="nb">int</code><code class="p">(</code><code class="nb">min</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">pos</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="bp">self</code><code class="o">.</code><code class="n">neg</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]))</code>

  <code class="k">def</code> <code class="nf">get_train_batches</code><code class="p">(</code>
    <code class="bp">self</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">rng_shuffle</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">rng_sample</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code>
  <code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Generator</code><code class="p">[</code><code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">],</code> <code class="kc">None</code><code class="p">,</code> <code class="kc">None</code><code class="p">]:</code>
    <code class="sd">"""Generates shuffled training batches with sampled negative pairs."""</code>
    <code class="c1"># Shuffle indices for positive pairs.</code>
    <code class="n">indices</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">permutation</code><code class="p">(</code><code class="n">rng_shuffle</code><code class="p">,</code> <code class="n">jnp</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">_n_pairs</code><code class="p">()))</code>

    <code class="c1"># Get sample of negative pairs.</code>
    <code class="n">neg_sample</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_global_negative_sampling</code><code class="p">(</code><code class="n">rng_sample</code><code class="p">)</code>

    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">_n_batches</code><code class="p">(</code><code class="n">batch_size</code><code class="p">)):</code>
      <code class="n">batch_indices</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">indices</code><code class="p">[</code><code class="n">i</code> <code class="o">*</code> <code class="n">batch_size</code> <code class="p">:</code> <code class="p">(</code><code class="n">i</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code> <code class="o">*</code> <code class="n">batch_size</code><code class="p">])</code>
      <code class="k">yield</code> <code class="n">Pairs</code><code class="p">(</code>
        <code class="n">pos</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">pos</code><code class="p">[</code><code class="n">batch_indices</code><code class="p">],</code> <code class="n">neg</code><code class="o">=</code><code class="n">neg_sample</code><code class="p">[</code><code class="n">batch_indices</code><code class="p">]</code>
      <code class="p">)</code><code class="o">.</code><code class="n">to_dict</code><code class="p">()</code>

  <code class="k">def</code> <code class="nf">_global_negative_sampling</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">rng_sample</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Samples negative pairs from the entire set to match positive set size."""</code>
    <code class="k">return</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code>
      <code class="n">rng_sample</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">neg</code><code class="p">,</code> <code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">pos</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">],),</code> <code class="n">replace</code><code class="o">=</code><code class="kc">False</code>
    <code class="p">)</code>

  <code class="k">def</code> <code class="nf">get_dummy_input</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">]:</code>
    <code class="sd">"""Returns a small dummy subset of positive and negative pairs."""</code>
    <code class="k">return</code> <code class="n">Pairs</code><code class="p">(</code><code class="n">pos</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">pos</code><code class="p">[:</code><code class="mi">2</code><code class="p">],</code> <code class="n">neg</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">neg</code><code class="p">[:(</code><code class="mi">2</code><code class="p">)])</code><code class="o">.</code><code class="n">to_dict</code><code class="p">()</code>

  <code class="k">def</code> <code class="nf">to_dict</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">:</code>
    <code class="sd">"""Converts the Pairs object back to a dictionary."""</code>
    <code class="k">return</code> <code class="p">{</code><code class="s2">"pos"</code><code class="p">:</code> <code class="bp">self</code><code class="o">.</code><code class="n">pos</code><code class="p">,</code> <code class="s2">"neg"</code><code class="p">:</code> <code class="bp">self</code><code class="o">.</code><code class="n">neg</code><code class="p">}</code>
</pre>

<p>The class provides several key methods for batching and sampling:</p>

<dl>
	<dt><code>get_eval_batches</code></dt>
	<dd>
	<p>Returns evaluation batches of positive and negative pairs, ensuring shape alignment and balanced sizes. It slices the <code>pos</code> and <code>neg</code> arrays using the same indices, up to the size of the smaller set.</p>
	</dd>
	<dt><code>get_train_batches</code></dt>
	<dd>
	<p>Returns shuffled training batches. Positive pairs are shuffled using <code>rng_shuffle</code>, and fresh negative pairs are sampled using <code>_global_negative_sampling</code>. This introduces variation between epochs and improves generalization.</p>
	</dd>
	<dt><code>get_dummy_input</code></dt>
	<dd>
	<p>Returns a very small batch (two positive and two negative pairs). This is useful to get the correct shape of data that we can use to initialize the model parameters.</p>
	</dd>
</dl>

<p>Together, these methods enable consistent and efficient batch generation for both training and evaluation, while introducing enough variability to improve learning.</p>

<section data-type="sect3" data-pdf-bookmark="Batching by pairs"><div class="sect3" id="batching-by-pairs">
<h3>Batching by pairs</h3>

<p><a contenteditable="false" data-primary="batching" data-type="indexterm" id="ch04_graphs.html30"/><a contenteditable="false" data-primary="Pairs class" data-secondary="batching by pairs" data-type="indexterm" id="ch04_graphs.html31"/>During each epoch, we must process a large number of positive and negative node pairs. As we’ve seen in the introduction to graph convolution layers, calculating node embeddings for large networks can become computationally expensive. To address this, we use batching—processing subsets of the data one at a time. This strategy is applied to both training and evaluation data, with some key differences:</p>

<ul class="simple">
	<li>
	<p>Training batches</p>

	<ul>
		<li><p>The <code>get_train_batches</code> method provides batches of positive and negative pairs, shuffling the data at the start of every epoch to introduce diversity in the order of pairs.</p></li>
		<li><p>Negative pairs are resampled once per epoch using the <code>_global_negative_sampling</code> method. This variation helps the model learn from a more diverse set of examples.</p></li>
	</ul>
	</li>

	<li><p>Evaluation batches</p>

	<ul>
		<li><p>The <code>get_eval_batches</code> method returns batches of positive and negative pairs according to the specified <code>batch_size</code>.</p></li>
		<li><p>To ensure compatibility, the <code>_n_pairs</code> method limits the batch size to the smaller of the positive or negative sets, so the two arrays match in shape.</p></li>
		<li><p>Evaluation batches are processed in a fixed order for reproducibility and consistent metrics across runs. This deterministic behavior simplifies debugging and ensures that order-sensitive metrics like Hits@K and MRR are stable.</p></li>
	</ul>
	</li>
</ul>

<p>To maintain uniform batch sizes, the <code>Pairs</code> class drops the final incomplete batch during both training and evaluation. This avoids irregularities in computation. Over multiple epochs, shuffling ensures that all data points are eventually seen, even if some are skipped in a single run.</p>

<p>To maximize efficiency, we use the <code>optimal_batch_size</code> utility function:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">optimal_batch_size</code><code class="p">(</code>
  <code class="n">dataset_splits</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">Dataset</code><code class="p">],</code> <code class="n">remainder_tolerance</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.125</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>
  <code class="sd">"""Calculates optimal batch size for optimizing JAX compilation."""</code>
  <code class="c1"># Calculate the minimum length of positive and negative pairs for each</code>
  <code class="c1"># dataset.</code>
  <code class="n">lengths</code> <code class="o">=</code> <code class="p">[</code>
    <code class="nb">min</code><code class="p">(</code><code class="n">dataset</code><code class="o">.</code><code class="n">pairs</code><code class="o">.</code><code class="n">pos</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">dataset</code><code class="o">.</code><code class="n">pairs</code><code class="o">.</code><code class="n">neg</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
    <code class="k">for</code> <code class="n">dataset</code> <code class="ow">in</code> <code class="n">dataset_splits</code><code class="o">.</code><code class="n">values</code><code class="p">()</code>
  <code class="p">]</code>

  <code class="c1"># Determine the allowable remainders per split based on the remainder</code>
  <code class="c1"># tolerance.</code>
  <code class="n">remainder_thresholds</code> <code class="o">=</code> <code class="p">[</code>
    <code class="nb">int</code><code class="p">(</code><code class="n">length</code> <code class="o">*</code> <code class="n">remainder_tolerance</code><code class="p">)</code> <code class="k">for</code> <code class="n">length</code> <code class="ow">in</code> <code class="n">lengths</code>
  <code class="p">]</code>
  <code class="n">max_possible_batch_size</code> <code class="o">=</code> <code class="nb">min</code><code class="p">(</code><code class="n">lengths</code><code class="p">)</code>

  <code class="k">for</code> <code class="n">batch_size</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">max_possible_batch_size</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">):</code>
    <code class="n">remainders</code> <code class="o">=</code> <code class="p">[</code><code class="n">length</code> <code class="o">%</code> <code class="n">batch_size</code> <code class="k">for</code> <code class="n">length</code> <code class="ow">in</code> <code class="n">lengths</code><code class="p">]</code>
    <code class="k">if</code> <code class="nb">all</code><code class="p">(</code>
      <code class="n">remainder</code> <code class="o">&lt;=</code> <code class="n">threshold</code>
      <code class="k">for</code> <code class="n">remainder</code><code class="p">,</code> <code class="n">threshold</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">remainders</code><code class="p">,</code> <code class="n">remainder_thresholds</code><code class="p">)</code>
    <code class="p">):</code>
      <code class="k">return</code> <code class="n">batch_size</code>
  <code class="k">return</code> <code class="n">max_possible_batch_size</code>
</pre>

<p>This function computes the largest batch size that minimizes dropped data and ensures consistency across training and evaluation. It balances computational efficiency and data utilization by selecting sizes that are both large and compatible with the dataset structure.</p>

<p>Consistent batch sizes are critical for optimizing jitted functions, which rely on static input shapes. They prevent unnecessary recompilation, improve memory and compute efficiency on accelerators like GPUs and TPUs, and reduce the complexity of handling irregular input.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html31" data-type="indexterm" id="id792"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html30" data-type="indexterm" id="id793"/></p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Sampling negative pairs"><div class="sect3" id="sampling-negative-pairs">
<h3>Sampling negative pairs</h3>

<p><a contenteditable="false" data-primary="negative pairs" data-type="indexterm" id="id794"/><a contenteditable="false" data-primary="Pairs class" data-secondary="sampling negative pairs" data-type="indexterm" id="id795"/>An important aspect of training is how we sample negative pairs. Since there are many more pairs without connections than with connections, we cannot use all negative examples; doing so would create a highly imbalanced training dataset. Instead, we select a subset of negative pairs to balance the dataset. This is where <code>_global_negative_sampling</code> comes in.</p>

<p>The subset of negative samples can significantly impact training. In this implementation, we use the simplest approach: <em>global sampling</em>, where we uniformly sample from all possible negative pairs. This strategy is suitable when we are broadly interested in potential node connections across the entire graph:</p>

<pre data-code-language="python" data-type="programlisting">
  <code class="k">def</code> <code class="nf">_global_negative_sampling</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">rng_sample</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Samples negative pairs from the entire set to match positive set size."""</code>
    <code class="k">return</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code>
      <code class="n">rng_sample</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">neg</code><code class="p">,</code> <code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">pos</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">],),</code> <code class="n">replace</code><code class="o">=</code><code class="kc">False</code>
    <code class="p">)</code>
</pre>

<p>While global sampling is straightforward and effective, many alternative strategies exist that drive the model to learn different patterns. For example:</p>

<dl>
	<dt>Local sampling</dt>
	<dd>
	<p>Ensures that negative pairs share at least one sender node, focusing on pairs that are structurally similar to positive pairs. This can help the model learn more fine-grained distinctions.</p>
	</dd>
	<dt>Hard negative sampling</dt>
	<dd>
	<p>Selects negative pairs that the model struggles to classify as negatives (i.e., pairs with a high predicted likelihood of being connected, even though they are not). This approach forces the model to improve on challenging cases and can accelerate learning.</p>
	</dd>
	<dt>Adversarial negative sampling</dt>
	<dd>
	<p>Generates challenging negative pairs using an adversarial approach, where a secondary model selects negatives that maximize the main model’s loss. While computationally expensive, it can lead to robust embeddings and improved <span class="keep-together">performance</span>.</p>
	</dd>
	<dt>Ratio of positive to negative pairs</dt>
	<dd>
	<p>Balances the number of positive and negative pairs in the dataset. While a 1:1 ratio is common, some tasks may benefit from a higher ratio of negatives (e.g., 1:5). In our DDI problem, we explored varying the ratio, but it did not significantly impact performance (not shown) and introduced unnecessary complexity.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html29" data-type="indexterm" id="id796"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html28" data-type="indexterm" id="id797"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html27" data-type="indexterm" id="id798"/></p>
	</dd>
</dl>
</div></section>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Create the Train Step Function"><div class="sect2" id="the-train-step">
<h2>Create the Train Step Function</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="training the model" data-tertiary="creating train step function" data-type="indexterm" id="ch04_graphs.html32"/><a contenteditable="false" data-primary="train_step function" data-type="indexterm" id="ch04_graphs.html33"/><a contenteditable="false" data-primary="training" data-secondary="drug–drug interaction graphs" data-tertiary="creating train step function" data-type="indexterm" id="ch04_graphs.html34"/>The <code>train_step</code> function is where learning actually happens during training. It performs a forward pass, computes the loss, and applies gradients to update the model’s parameters. This function is applied to batches of node pairs throughout each epoch.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="nd">@partial</code><code class="p">(</code><code class="n">jax</code><code class="o">.</code><code class="n">jit</code><code class="p">,</code> <code class="n">static_argnames</code><code class="o">=</code><code class="p">[</code><code class="s2">"loss_fn"</code><code class="p">,</code> <code class="s2">"norm_loss"</code><code class="p">])</code>
<code class="k">def</code> <code class="nf">train_step</code><code class="p">(</code>
  <code class="n">state</code><code class="p">:</code> <code class="n">TrainState</code><code class="p">,</code>
  <code class="n">graph</code><code class="p">:</code> <code class="n">jraph</code><code class="o">.</code><code class="n">GraphsTuple</code><code class="p">,</code>
  <code class="n">pairs</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">],</code>
  <code class="n">rng_dropout</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code>
  <code class="n">loss_fn</code><code class="p">:</code> <code class="n">Callable</code> <code class="o">=</code> <code class="n">binary_log_loss</code><code class="p">,</code>
  <code class="n">norm_loss</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">False</code><code class="p">,</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">tuple</code><code class="p">[</code><code class="n">TrainState</code><code class="p">,</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">]]:</code>
  <code class="sd">"""Performs a single training step, updating model parameters."""</code>

  <code class="k">def</code> <code class="nf">calculate_loss</code><code class="p">(</code><code class="n">params</code><code class="p">):</code>
    <code class="sd">"""Computes loss and hits@20 metric for the given model parameters."""</code>
    <code class="n">scores</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_fn</code><code class="p">(</code>
      <code class="p">{</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">params</code><code class="p">},</code>
      <code class="n">graph</code><code class="p">,</code>
      <code class="n">pairs</code><code class="p">,</code>
      <code class="n">is_training</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
      <code class="n">rngs</code><code class="o">=</code><code class="p">{</code><code class="s2">"dropout"</code><code class="p">:</code> <code class="n">rng_dropout</code><code class="p">},</code>
    <code class="p">)</code>
    <code class="n">loss</code> <code class="o">=</code> <code class="n">loss_fn</code><code class="p">(</code><code class="n">scores</code><code class="p">)</code>
    <code class="n">metric</code> <code class="o">=</code> <code class="n">evaluate_hits_at_20</code><code class="p">(</code><code class="n">scores</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">loss</code><code class="p">,</code> <code class="n">metric</code>

  <code class="c1"># to additional variables (e.g., state, graph, pairs) without requiring them</code>
  <code class="c1"># to be explicitly passed, while maintaining compatibility with</code>
  <code class="c1"># jax.value_and_grad.</code>
  <code class="n">grad_fn</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">value_and_grad</code><code class="p">(</code><code class="n">calculate_loss</code><code class="p">,</code> <code class="n">has_aux</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
  <code class="p">(</code><code class="n">loss</code><code class="p">,</code> <code class="n">metric</code><code class="p">),</code> <code class="n">grads</code> <code class="o">=</code> <code class="n">grad_fn</code><code class="p">(</code><code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">)</code>
  <code class="n">state</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_gradients</code><code class="p">(</code><code class="n">grads</code><code class="o">=</code><code class="n">grads</code><code class="p">)</code>

  <code class="n">metrics</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"loss"</code><code class="p">:</code> <code class="n">loss</code><code class="p">,</code> <code class="s2">"hits@20"</code><code class="p">:</code> <code class="n">metric</code><code class="p">}</code>
  <code class="k">if</code> <code class="n">norm_loss</code><code class="p">:</code>
    <code class="n">metrics</code><code class="p">[</code><code class="s2">"loss"</code><code class="p">]</code> <code class="o">=</code> <code class="n">metrics</code><code class="p">[</code><code class="s2">"loss"</code><code class="p">]</code> <code class="o">/</code> <code class="p">(</code>
      <code class="n">pairs</code><code class="p">[</code><code class="s2">"pos"</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">+</code> <code class="n">pairs</code><code class="p">[</code><code class="s2">"neg"</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
    <code class="p">)</code>

  <code class="k">return</code> <code class="n">state</code><code class="p">,</code> <code class="n">metrics</code>
</pre>

<p>You’ll notice that the function is decorated using <code>@partial(jax.jit, static_argnames=["loss_fn", "norm_loss"])</code>. This pattern allows these arguments—the loss function (<code>loss_fn</code>) and whether to normalize the loss (<code>norm_loss</code>)—to be treated as static during JAX’s just-in-time (JIT) compilation. By marking these arguments as static, JAX avoids recompiling the function every time they change.</p>

<p>Inside the function, a nested <code>calculate_loss</code> method defines the core of the <span class="keep-together">computation</span>:</p>

<ul>
	<li><p>The model is applied to the current batch using <code>state.apply_fn(...)</code>, producing predictions (<code>scores</code>) for positive and negative node pairs.</p></li>
	<li><p>The specified <code>loss_fn</code> is used to compute the training loss.</p></li>
	<li><p>The <code>evaluate_hits_at_20</code> function computes the <code>hits@20</code> metric to track how well the model ranks correct node pairs near the top.</p></li>
</ul>

<p>JAX’s <code>value_and_grad</code> is then used to calculate both the loss and its gradient with respect to the model parameters. These gradients are applied to the training state using <code>state.apply_gradients</code>.</p>
<p>If <code>norm_loss=True</code>, the total loss is divided by the number of training pairs in the batch (positive + negative), ensuring that loss magnitudes remain comparable across different batch sizes or sampling ratios.</p>


<p>The default loss function used in training is the binary log loss function:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="nd">@jax</code><code class="o">.</code><code class="n">jit</code>
<code class="k">def</code> <code class="nf">binary_log_loss</code><code class="p">(</code><code class="n">scores</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
  <code class="sd">"""Computes the binary log loss for positive and negative drug pairs."""</code>
  <code class="c1"># Clip probabilities to avoid numerical instability.</code>
  <code class="n">probs</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">tree</code><code class="o">.</code><code class="n">map</code><code class="p">(</code>
    <code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">jnp</code><code class="o">.</code><code class="n">clip</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">x</code><code class="p">),</code> <code class="mf">1e-7</code><code class="p">,</code> <code class="mi">1</code> <code class="o">-</code> <code class="mf">1e-7</code><code class="p">),</code> <code class="n">scores</code>
  <code class="p">)</code>

  <code class="c1"># Compute positive and negative losses.</code>
  <code class="n">pos_loss</code> <code class="o">=</code> <code class="o">-</code><code class="n">jnp</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">probs</code><code class="p">[</code><code class="s2">"pos"</code><code class="p">])</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
  <code class="n">neg_loss</code> <code class="o">=</code> <code class="o">-</code><code class="n">jnp</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">probs</code><code class="p">[</code><code class="s2">"neg"</code><code class="p">])</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>

  <code class="k">return</code> <code class="n">pos_loss</code> <code class="o">+</code> <code class="n">neg_loss</code>
</pre>

<p>This loss function performs the following steps:</p>

<dl>
	<dt>Sigmoid transformation</dt>
	<dd><p>Converts the raw logits (unbounded scores) into probabilities in the range (0, 1). These probabilities represent the model’s confidence that a given node pair is a positive class (i.e., a true drug–drug interaction).</p></dd>
	<dt>Clipping</dt>
	<dd><p>Ensures numerical stability by constraining probabilities to lie slightly within (0, 1). This avoids issues such as <code>log(0)</code>, which can cause the loss to diverge or return <code>NaN</code> during training.</p></dd>
	<dt>Loss calculation</dt>
	<dd><p>Ensures that positive pairs are penalized when their predicted probabilities are far from 1, and negative pairs are penalized when their predicted probabilities are far from 0. The total loss is computed as the average of the positive and negative log losses.</p>
	</dd>
</dl>
<p>This loss provides a simple yet effective objective for training the model to distinguish between interacting and noninteracting drug pairs.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html34" data-type="indexterm" id="id799"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html33" data-type="indexterm" id="id800"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html32" data-type="indexterm" id="id801"/></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Create the Evaluation Metric"><div class="sect2" id="evaluation-metric">
<h2>Create the Evaluation Metric</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="training the model" data-tertiary="creating evaluation metric" data-type="indexterm" id="ch04_graphs.html35"/><a contenteditable="false" data-primary="training" data-secondary="drug–drug interaction graphs" data-tertiary="creating evaluation metric" data-type="indexterm" id="ch04_graphs.html36"/>Finally, we want to evaluate the model’s performance. This follows a similar approach to <code>train_step</code>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="nd">@partial</code><code class="p">(</code><code class="n">jax</code><code class="o">.</code><code class="n">jit</code><code class="p">,</code> <code class="n">static_argnames</code><code class="o">=</code><code class="p">[</code><code class="s2">"loss_fn"</code><code class="p">,</code> <code class="s2">"norm_loss"</code><code class="p">])</code>
<code class="k">def</code> <code class="nf">eval_step</code><code class="p">(</code>
  <code class="n">state</code><code class="p">:</code> <code class="n">TrainState</code><code class="p">,</code>
  <code class="n">graph</code><code class="p">:</code> <code class="n">jraph</code><code class="o">.</code><code class="n">GraphsTuple</code><code class="p">,</code>
  <code class="n">pairs</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">],</code>
  <code class="n">loss_fn</code><code class="p">:</code> <code class="n">Callable</code> <code class="o">=</code> <code class="n">binary_log_loss</code><code class="p">,</code>
  <code class="n">norm_loss</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">False</code><code class="p">,</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">]:</code>
  <code class="sd">"""Performs an evaluation step, computing loss and hits@20 metric."""</code>
  <code class="n">scores</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_fn</code><code class="p">(</code>
    <code class="p">{</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">},</code> <code class="n">graph</code><code class="p">,</code> <code class="n">pairs</code><code class="p">,</code> <code class="n">is_training</code><code class="o">=</code><code class="kc">False</code>
  <code class="p">)</code>
  <code class="n">metrics</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"loss"</code><code class="p">:</code> <code class="n">loss_fn</code><code class="p">(</code><code class="n">scores</code><code class="p">),</code> <code class="s2">"hits@20"</code><code class="p">:</code> <code class="n">evaluate_hits_at_20</code><code class="p">(</code><code class="n">scores</code><code class="p">)}</code>
  <code class="k">if</code> <code class="n">norm_loss</code><code class="p">:</code>
    <code class="n">metrics</code><code class="p">[</code><code class="s2">"loss"</code><code class="p">]</code> <code class="o">=</code> <code class="n">metrics</code><code class="p">[</code><code class="s2">"loss"</code><code class="p">]</code> <code class="o">/</code> <code class="p">(</code>
      <code class="n">pairs</code><code class="p">[</code><code class="s2">"pos"</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">+</code> <code class="n">pairs</code><code class="p">[</code><code class="s2">"neg"</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
    <code class="p">)</code>

  <code class="k">return</code> <code class="n">metrics</code>
</pre>

<p>The notable differences in <code>eval_step</code> are:</p>

<dl>
	<dt>No training mode</dt>
	<dd>
	<p>The <code>is_training</code> flag is set to <code>False</code>, which disables behaviors like dropout and ensures that the model is evaluated deterministically.</p>
	</dd>
	<dt>Evaluation metric</dt>
	<dd>
	<p>Rather than just returning the loss, the function also computes <code>hits@20</code>, a ranking-based metric commonly used in link prediction tasks.</p>
	</dd>
</dl>

<p>Hits@20 evaluates how well the model ranks positive node pairs compared to negative ones, giving an intuitive signal of ranking quality. Specifically, it identifies the 20th highest score among the negative pairs as a threshold and calculates the proportion of positive scores that exceed this threshold; then it calculates the proportion of positive scores that exceeds this threshold. A higher Hits@20 indicates that the model correctly ranks many true interactions above even one of the most confident false ones.
</p>

<p>Here’s the implementation:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="nd">@jax</code><code class="o">.</code><code class="n">jit</code>
<code class="k">def</code> <code class="nf">evaluate_hits_at_20</code><code class="p">(</code><code class="n">scores</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
  <code class="sd">"""Computes the hits@20 metric capturing positive pairs ranking."""</code>
  <code class="c1"># Implementation inspired by the OGB benchmark: https://oreil.ly/Oej2Y</code>
  <code class="c1"># Find the 20th highest score among negative edges.</code>
  <code class="n">kth_score_in_negative_edges</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">sort</code><code class="p">(</code><code class="n">scores</code><code class="p">[</code><code class="s2">"neg"</code><code class="p">])[</code><code class="o">-</code><code class="mi">20</code><code class="p">]</code>

  <code class="c1"># Compute the proportion of positive scores greater than the threshold.</code>
  <code class="k">return</code> <code class="p">(</code>
    <code class="n">jnp</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">scores</code><code class="p">[</code><code class="s2">"pos"</code><code class="p">]</code> <code class="o">&gt;</code> <code class="n">kth_score_in_negative_edges</code><code class="p">)</code>
    <code class="o">/</code> <code class="n">scores</code><code class="p">[</code><code class="s2">"pos"</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
  <code class="p">)</code>
</pre>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>We could have imported the <code>ogb.linkproppred.Evaluator</code> from the Open Graph Benchmark (OGB) library, which computes Hits@20. However, by directly implementing the metric, we make the evaluation process more transparent and tailored to our specific use case. This approach provides greater flexibility for modifications and extensions while clearly showing how the model is evaluated.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html36" data-type="indexterm" id="id802"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html35" data-type="indexterm" id="id803"/></p>
</div>
</div></section>

<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Train the Simplest Model"><div class="sect2" id="train-the-simplest-model">
<h2>Train the Simplest Model</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="training the model" data-tertiary="training the simplest model" data-type="indexterm" id="ch04_graphs.html37"/><a contenteditable="false" data-primary="training" data-secondary="drug–drug interaction graphs" data-tertiary="training the simplest model" data-type="indexterm" id="ch04_graphs.html38"/>We are finally ready to train the model. We’ll start with a relatively simple architecture and monitor its performance:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">optax</code>

<code class="n">rng</code><code class="p">,</code> <code class="n">rng_init</code><code class="p">,</code> <code class="n">rng_train</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">DdiModel</code><code class="p">(</code>
  <code class="n">n_nodes</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">n_nodes</code><code class="p">,</code>
  <code class="n">embedding_dim</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code>
  <code class="n">last_layer_self</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>
  <code class="n">degree_norm</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>
  <code class="n">dropout_rate</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code>
<code class="p">)</code>

<code class="n">state</code><code class="p">,</code> <code class="n">metrics</code> <code class="o">=</code> <code class="n">train</code><code class="p">(</code>
  <code class="n">state</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code>
    <code class="n">dummy_input</code><code class="o">=</code><code class="p">{</code>
      <code class="s2">"graph"</code><code class="p">:</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">graph</code><code class="p">,</code>
      <code class="s2">"pairs"</code><code class="p">:</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">pairs</code><code class="o">.</code><code class="n">get_dummy_input</code><code class="p">(),</code>
    <code class="p">},</code>
    <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adam</code><code class="p">(</code><code class="mf">0.001</code><code class="p">),</code>
  <code class="p">),</code>
  <code class="n">rng</code><code class="o">=</code><code class="n">rng_train</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">,</code>
  <code class="n">num_epochs</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code>
  <code class="n">eval_every</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
  <code class="n">loss_fn</code><code class="o">=</code><code class="n">binary_log_loss</code><code class="p">,</code>
  <code class="n">norm_loss</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>
  <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"graphs/models/initial_model"</code><code class="p">),</code>
<code class="p">)</code>
</pre>

<p>The learning curves in <a data-type="xref" href="#ddi-simplest-model">Figure 4-9</a> show the training process over 500 epochs. The training loss decreases steadily and the Hits@20 metric improves on both training and validation sets. However, the growing divergence between the training and validation curves—particularly visible in the later epochs—suggests moderate overfitting. This indicates that while the model is capturing meaningful patterns, its generalization to unseen data could be improved.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">dlfb.graphs.inspect</code> <code class="kn">import</code> <code class="n">plot_learning</code>

<code class="n">plot_learning</code><code class="p">(</code><code class="n">metrics</code><code class="p">);</code>
</pre>

<figure><div id="ddi-simplest-model" class="figure"><img alt="" src="assets/dlfb_0409.png" width="600" height="288"/>
<h6><span class="label">Figure 4-9. </span>Learning curves showing the training loss (left) and Hits@20 metric (right) over 500 epochs. While both metrics improve on the training set, a growing gap between training and validation suggests moderate overfitting.</h6>
</div></figure>

<p>A word of warning: two sources of variance can lead to surprisingly different metrics curves in this plot—even with the same model and training setup.</p>
<dl>
  <dt>Stochastic node sampling</dt>
  <dd><p>Graph neural networks that use neighborhood sampling can be highly sensitive to the random seed. In drug–drug interaction prediction, some nodes are connected to many easily predictable links, while others are not. Which nodes get included in your training data can have a major impact on the learned representations, loss, and evaluation metrics.</p></dd>
  <dt>Discontinuous metrics</dt>
  <dd><p>Hits@20 is a ranking-based metric that compares each positive score to the 20th highest negative score. This thresholding introduces discontinuity: a small change in any score near the threshold—especially among the top-ranked negatives—can flip the outcome for many positives from success to failure or vice versa. This makes Hits@20 unusually sensitive to minor score shifts, even when the model or loss appears unchanged. This doesn’t mean it’s a bad metric—just something to be aware of.
</p></dd>
</dl>
<p>To reduce this variability, we can increase the number of nodes we include in the dataset (as shown in <a data-type="xref" href="#training-on-a-larger-dataset">“Train on a Larger Dataset”</a>).</p>
<p>Now that we’ve trained a first working model, we’ll explore strategies to reduce overfitting and improve overall performance.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html38" data-type="indexterm" id="id804"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html37" data-type="indexterm" id="id805"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html24" data-type="indexterm" id="id806"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html23" data-type="indexterm" id="id807"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Improving the Model"><div class="sect1" id="improving-the-model"> 
<h1>Improving the Model</h1>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="improving the model" data-type="indexterm" id="ch04_graphs.html39"/>Everything is working end-to-end—we have prepared a dataset, built a model, and trained it. However, the model’s performance is suboptimal. In this section, we’ll explore some tweaks to see if we can achieve better results.</p>

<section data-type="sect2" data-pdf-bookmark="Change to AUC Loss"><div class="sect2" id="change-to-auc-loss">
<h2>Change to AUC Loss</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="improving the model" data-tertiary="change from binary log loss to AUC loss" data-type="indexterm" id="ch04_graphs.html40"/>So far, we’ve used binary log loss to train our model. However, for our task, the primary goal is to ensure that positive pairs are ranked higher than negative pairs. While probabilities can also be used to prioritize, they often saturate near 1 or 0 for confident predictions, making it harder to differentiate between highly ranked pairs. In contrast, ranking-based metrics focus on the relative ordering of scores, which better aligns with the task of identifying and prioritizing the most promising drug interactions. This is particularly valuable in DDI prediction, where the goal is often to focus on the top-scoring pairs for further investigation.</p>

<p>Inspired by a paper on pairwise learning for neural link prediction (PLNLP),<sup><a data-type="noteref" id="id808-marker" href="ch04.html#id808">7</a></sup> which outlines key stages of a link prediction pipeline, we will swap the loss function to better align with our objective. Instead of focusing on binary classifications, we adopt a ranking-based approach that encourages the model to score connected pairs higher than unconnected ones, aligning conceptually with the area under the curve (AUC) metric.</p>

<p>AUC measures the probability that a randomly chosen positive instance (connected node pair) has a higher score than a randomly chosen negative instance (nonconnected pair). While directly optimizing AUC would be ideal, it is computationally challenging because its gradients are often undefined or zero. To address this, we use a <em>surrogate loss function</em> that mimics AUC’s properties while remaining easy to optimize.</p>

<p>A simple and effective surrogate is the squared loss, which penalizes deviations from the target score difference of 1 between positive and negative pairs. This means the model is penalized both when the difference is less than 1 (underestimation) and when it is greater than 1 (overestimation). By minimizing this penalty, the model learns to consistently assign higher scores to connected pairs while maintaining an appropriate margin over unconnected ones. Here’s the implementation:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="nd">@jax</code><code class="o">.</code><code class="n">jit</code>
<code class="k">def</code> <code class="nf">auc_loss</code><code class="p">(</code><code class="n">scores</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
  <code class="sd">"""Computes AUC-based loss for positive and negative drug pairs."""</code>
  <code class="k">return</code> <code class="n">jnp</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="p">(</code><code class="n">scores</code><code class="p">[</code><code class="s2">"pos"</code><code class="p">]</code> <code class="o">-</code> <code class="n">scores</code><code class="p">[</code><code class="s2">"neg"</code><code class="p">]))</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>
</pre>

<p>This loss function encourages the model to score linked pairs higher than nonlinked pairs, improving its ranking performance. The motivation for discarding the cross-entropy loss is that the Hits@N metric, commonly used by Open Graph Benchmark (OGB) for evaluating link prediction benchmarks, does not measure the quality of predicted probabilities. Instead, it focuses solely on ensuring that true edges are ranked higher than false edges. Although the difference between these approaches is subtle, it has significant practical implications. Let’s explore the effect of the loss function switch on the model:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">rng</code><code class="p">,</code> <code class="n">rng_init</code><code class="p">,</code> <code class="n">rng_train</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">DdiModel</code><code class="p">(</code>
  <code class="n">n_nodes</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">n_nodes</code><code class="p">,</code>
  <code class="n">embedding_dim</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code>
  <code class="n">last_layer_self</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>
  <code class="n">degree_norm</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>
  <code class="n">dropout_rate</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code>
<code class="p">)</code>

<code class="n">_</code><code class="p">,</code> <code class="n">metrics</code> <code class="o">=</code> <code class="n">train</code><code class="p">(</code>
  <code class="n">state</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code>
    <code class="n">dummy_input</code><code class="o">=</code><code class="p">{</code>
      <code class="s2">"graph"</code><code class="p">:</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">graph</code><code class="p">,</code>
      <code class="s2">"pairs"</code><code class="p">:</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">pairs</code><code class="o">.</code><code class="n">get_dummy_input</code><code class="p">(),</code>
    <code class="p">},</code>
    <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adam</code><code class="p">(</code><code class="mf">0.001</code><code class="p">),</code>
  <code class="p">),</code>
  <code class="n">rng</code><code class="o">=</code><code class="n">rng_train</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">,</code>
  <code class="n">num_epochs</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code>
  <code class="n">eval_every</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
  <code class="n">loss_fn</code><code class="o">=</code><code class="n">auc_loss</code><code class="p">,</code>
  <code class="n">norm_loss</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
  <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"graphs/models/initial_model_auc"</code><code class="p">),</code>
<code class="p">)</code>
</pre>

<p>From the learning curves in <a data-type="xref" href="#ddi-simplest-auc-model">Figure 4-10</a>, we can see that changing the loss function to an AUC-based objective has led to better results. First, training is noticeably smoother: the training and validation losses fall in near-lockstep, with virtually no gap. Second, Hits@20 climbs more quickly and saturates at a higher level on the validation set than before, reflecting stronger generalization. In short, aligning the objective with our ranking metric directly translates into better and more reliable performance.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">plot_learning</code><code class="p">(</code><code class="n">metrics</code><code class="p">);</code>
</pre>

<figure><div id="ddi-simplest-auc-model" class="figure"><img alt="" src="assets/dlfb_0410.png" width="600" height="288"/>
<h6><span class="label">Figure 4-10. </span>Learning curves for loss (left) and Hits@20 (right) after replacing binary-cross-entropy with an AUC-optimizing loss. The new objective keeps train and validation losses tightly coupled and pushes the validation Hits@20 to higher, stabler values.</h6>
</div></figure>

<p>It appears that this model is already fairly strong as measured by our choice of metric, but let’s see if we can push performance even higher and reduce overfitting further by exploring hyperparameter sweeps.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html40" data-type="indexterm" id="id809"/></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Set Model Sweeping and Training Parameters"><div class="sect2" id="sweeping-model-and-training-parameters">
<h2>Set Model Sweeping and Training Parameters</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="improving the model" data-tertiary="set model sweeping/training parameters" data-type="indexterm" id="ch04_graphs.html41"/><a contenteditable="false" data-primary="training" data-secondary="drug–drug interaction graphs" data-tertiary="set model sweeping/training parameters" data-type="indexterm" id="ch04_graphs.html42"/>Our model and training loop include several hyperparameters, and it’s not immediately clear which combinations will yield the best performance. One natural starting point is the embedding dimension, which directly controls the capacity of the model to richly represent graph structure.</p>

<section data-type="sect3" data-pdf-bookmark="Varying embedding dimensions"><div class="sect3" id="varying-embedding-dimensions">
<h3>Varying embedding dimensions</h3>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="improving the model" data-tertiary="varying embedding dimensions" data-type="indexterm" id="ch04_graphs.html43"/>We will vary the <code>embedding_dim</code> parameter and train new models for each value to evaluate its impact on performance. Since previous experiments showed improved results with longer training, we will also extend the number of epochs in this sweep:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">embedding_dims</code> <code class="o">=</code> <code class="p">[</code><code class="mi">64</code><code class="p">,</code> <code class="mi">128</code><code class="p">,</code> <code class="mi">256</code><code class="p">,</code> <code class="mi">512</code><code class="p">]</code>
<code class="n">model_params</code> <code class="o">=</code> <code class="p">{</code>
  <code class="s2">"n_nodes"</code><code class="p">:</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">n_nodes</code><code class="p">,</code>
  <code class="s2">"last_layer_self"</code><code class="p">:</code> <code class="kc">False</code><code class="p">,</code>
  <code class="s2">"degree_norm"</code><code class="p">:</code> <code class="kc">False</code><code class="p">,</code>
  <code class="s2">"dropout_rate"</code><code class="p">:</code> <code class="mf">0.3</code><code class="p">,</code>
<code class="p">}</code>
<code class="n">training_params</code> <code class="o">=</code> <code class="p">{</code>
  <code class="s2">"rng"</code><code class="p">:</code> <code class="n">rng_train</code><code class="p">,</code>
  <code class="s2">"dataset_splits"</code><code class="p">:</code> <code class="n">dataset_splits</code><code class="p">,</code>
  <code class="s2">"num_epochs"</code><code class="p">:</code> <code class="mi">500</code><code class="p">,</code>
  <code class="s2">"eval_every"</code><code class="p">:</code> <code class="mi">25</code><code class="p">,</code>
  <code class="s2">"loss_fn"</code><code class="p">:</code> <code class="n">auc_loss</code><code class="p">,</code>
  <code class="s2">"norm_loss"</code><code class="p">:</code> <code class="kc">True</code><code class="p">,</code>
<code class="p">}</code>
</pre>

<p>The following loop automates training across a range of <code>embedding_dim</code> values, storing the evaluation metrics for later analysis:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">dlfb.utils.metric_plots</code> <code class="kn">import</code> <code class="n">to_df</code>

<code class="n">all_metrics</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">embedding_dim</code> <code class="ow">in</code> <code class="n">embedding_dims</code><code class="p">:</code>
  <code class="n">model</code> <code class="o">=</code> <code class="n">DdiModel</code><code class="p">(</code><code class="o">**</code><code class="p">{</code><code class="s2">"embedding_dim"</code><code class="p">:</code> <code class="n">embedding_dim</code><code class="p">,</code> <code class="o">**</code><code class="n">model_params</code><code class="p">})</code>
  <code class="n">_</code><code class="p">,</code> <code class="n">metrics</code> <code class="o">=</code> <code class="n">train</code><code class="p">(</code>
    <code class="n">state</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
      <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code>
      <code class="n">dummy_input</code><code class="o">=</code><code class="p">{</code>
        <code class="s2">"graph"</code><code class="p">:</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">graph</code><code class="p">,</code>
        <code class="s2">"pairs"</code><code class="p">:</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">pairs</code><code class="o">.</code><code class="n">get_dummy_input</code><code class="p">(),</code>
      <code class="p">},</code>
      <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adam</code><code class="p">(</code><code class="mf">0.001</code><code class="p">),</code>
    <code class="p">),</code>
    <code class="o">**</code><code class="n">training_params</code><code class="p">,</code>
    <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="sa">f</code><code class="s2">"graphs/models/sweep_embedding_dim:</code><code class="si">{</code><code class="n">embedding_dim</code><code class="si">}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">)</code>
  <code class="n">df</code> <code class="o">=</code> <code class="n">to_df</code><code class="p">(</code><code class="n">metrics</code><code class="p">)</code><code class="o">.</code><code class="n">assign</code><code class="p">(</code><code class="o">**</code><code class="p">{</code><code class="s2">"embedding_dim"</code><code class="p">:</code> <code class="n">embedding_dim</code><code class="p">})</code>
  <code class="n">all_metrics</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">df</code><code class="p">)</code>
<code class="n">all_metrics_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">(</code><code class="n">all_metrics</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
</pre>

<p>As shown in <a data-type="xref" href="#ddi-sweep-embedding-dim">Figure 4-11</a>, there’s a clear relationship between embedding size and model performance: larger embedding vectors tend to yield better validation metrics. This is likely because higher-dimensional embeddings provide greater capacity to capture complex relationships and patterns in the graph:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="k">as</code> <code class="nn">sns</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="kn">from</code> <code class="nn">dlfb.utils.metric_plots</code> <code class="kn">import</code> <code class="n">DEFAULT_SPLIT_COLORS</code>

<code class="n">data</code> <code class="o">=</code> <code class="n">all_metrics_df</code>
<code class="n">data</code> <code class="o">=</code> <code class="n">data</code><code class="p">[(</code><code class="n">data</code><code class="p">[</code><code class="s2">"metric"</code><code class="p">]</code> <code class="o">==</code> <code class="s2">"hits@20"</code><code class="p">)]</code>
<code class="n">data</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">groupby</code><code class="p">([</code><code class="s2">"metric"</code><code class="p">,</code> <code class="s2">"split"</code><code class="p">,</code> <code class="s2">"embedding_dim"</code><code class="p">],</code> <code class="n">as_index</code><code class="o">=</code><code class="kc">False</code><code class="p">)[</code>
  <code class="s2">"mean"</code>
<code class="p">]</code><code class="o">.</code><code class="n">max</code><code class="p">()</code>
<code class="n">data</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="p">[</code><code class="s2">"split"</code><code class="p">,</code> <code class="s2">"mean"</code><code class="p">])</code>

<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">7</code><code class="p">,</code> <code class="mf">3.5</code><code class="p">))</code>
<code class="n">sns</code><code class="o">.</code><code class="n">barplot</code><code class="p">(</code>
  <code class="n">data</code><code class="o">=</code><code class="n">data</code><code class="p">,</code>
  <code class="n">x</code><code class="o">=</code><code class="s2">"embedding_dim"</code><code class="p">,</code>
  <code class="n">y</code><code class="o">=</code><code class="s2">"mean"</code><code class="p">,</code>
  <code class="n">hue</code><code class="o">=</code><code class="s2">"split"</code><code class="p">,</code>
  <code class="n">palette</code><code class="o">=</code><code class="n">DEFAULT_SPLIT_COLORS</code><code class="p">,</code>
<code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">(</code><code class="mf">0.5</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Embedding Dimensions"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Maximum Hits@20"</code><code class="p">);</code>
</pre>

<figure><div id="ddi-sweep-embedding-dim" class="figure"><img alt="" src="assets/dlfb_0411.png" width="600" height="320"/>
<h6><span class="label">Figure 4-11. </span>Maximum Hits@20 achieved by models with varying embedding dimensions, highlighting the impact of embedding size on performance.</h6>
</div></figure>

<p>The model with embedding dimensions set to 512 seem to achieve a perfect Hits@20 of ~1 on the training set, suggesting that it has sufficient capacity to fully (and even overly) fit the training data.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Increasing the embedding size and training the model for longer, as we have done in this section, comes with higher computational costs. Whether this trade-off is worthwhile depends on the resources available and the relative importance of improving model performance in your specific use case.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html43" data-type="indexterm" id="id810"/></p>
</div>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Varying multiple hyperparameters"><div class="sect3" id="varying-multiple-hyperparameters">
<h3>Varying multiple hyperparameters</h3>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="improving the model" data-tertiary="varying multiple hyperparameters" data-type="indexterm" id="ch04_graphs.html44"/>For this experiment, we broaden our search by sweeping over multiple model hyperparameters simultaneously. This builds on the previous experiment but explores a wider region of the hyperparameter space in the hopes of discovering better-performing configurations.</p>

<p>We will vary the following parameters:</p>

<ul class="simple">
	<li>
	<p>Dropout rate: 0, 0.3, or 0.5</p>
	</li>
	<li>
	<p>Self-edges in the last convolutional layer: Whether to include self-edges (<code>last_layer_self</code>: True or False)</p>
	</li>
	<li>
	<p>Degree normalization: Whether to normalize node embeddings by their degree (<code>degree_norm</code>: True or False)</p>
	</li>
	<li>
	<p>Number of MLP layers in the link predictor: 1, 2, or 3 (<code>n_mlp_layers</code>)</p>
	</li>
</ul>

<p>For this experiment, we fix the embedding dimension at 512, as it was among the best-performing settings in the earlier sweep. While an embedding size of 256 also performed reasonably well and would reduce computational cost, we opt for 512 in anticipation of scaling to larger graphs. A larger embedding size offers greater capacity to model complex relational patterns.</p>

<p>Let’s set up the sweep:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">itertools</code>

<code class="n">model_params</code> <code class="o">=</code> <code class="p">{</code>
  <code class="s2">"n_nodes"</code><code class="p">:</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">n_nodes</code><code class="p">,</code>
  <code class="s2">"embedding_dim"</code><code class="p">:</code> <code class="mi">512</code><code class="p">,</code>
<code class="p">}</code>

<code class="n">model_params_sweep</code> <code class="o">=</code> <code class="p">{</code>
  <code class="s2">"dropout_rate"</code><code class="p">:</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mf">0.3</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">],</code>
  <code class="s2">"last_layer_self"</code><code class="p">:</code> <code class="p">[</code><code class="kc">True</code><code class="p">,</code> <code class="kc">False</code><code class="p">],</code>
  <code class="s2">"degree_norm"</code><code class="p">:</code> <code class="p">[</code><code class="kc">True</code><code class="p">,</code> <code class="kc">False</code><code class="p">],</code>
  <code class="s2">"n_mlp_layers"</code><code class="p">:</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code>
<code class="p">}</code>
<code class="n">keys</code><code class="p">,</code> <code class="n">values</code> <code class="o">=</code> <code class="nb">zip</code><code class="p">(</code><code class="o">*</code><code class="n">model_params_sweep</code><code class="o">.</code><code class="n">items</code><code class="p">())</code>
<code class="n">model_param_combn</code> <code class="o">=</code> <code class="p">[</code>
  <code class="nb">dict</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">keys</code><code class="p">,</code> <code class="n">combo</code><code class="p">))</code> <code class="k">for</code> <code class="n">combo</code> <code class="ow">in</code> <code class="n">itertools</code><code class="o">.</code><code class="n">product</code><code class="p">(</code><code class="o">*</code><code class="n">values</code><code class="p">)</code>
<code class="p">]</code>
<code class="nb">print</code><code class="p">(</code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">model_param_combn</code><code class="p">))</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">
    dropout_rate  last_layer_self  degree_norm  n_mlp_layers
0   0.0           True             True         1
1   0.0           True             True         2
2   0.0           True             True         3
..  ...           ...              ...          ...
33  0.5           False            False        1
34  0.5           False            False        2
35  0.5           False            False        3

[36 rows x 4 columns]
</pre>

<p class="pagebreak-before">To train models with each parameter combination, we use the following approach:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">name_from_params</code><code class="p">(</code><code class="n">params</code><code class="p">:</code> <code class="nb">dict</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
  <code class="sd">"""Generates a string from a parameters dictionary"""</code>
  <code class="k">return</code> <code class="s2">"_"</code><code class="o">.</code><code class="n">join</code><code class="p">([</code><code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">k</code><code class="si">}</code><code class="s2">:</code><code class="si">{</code><code class="n">v</code><code class="si">}</code><code class="s2">"</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code> <code class="n">v</code> <code class="ow">in</code> <code class="n">params</code><code class="o">.</code><code class="n">items</code><code class="p">()])</code>


<code class="n">all_metrics</code> <code class="o">=</code> <code class="p">[]</code>

<code class="k">for</code> <code class="n">combn</code> <code class="ow">in</code> <code class="n">model_param_combn</code><code class="p">:</code>
  <code class="n">model</code> <code class="o">=</code> <code class="n">DdiModel</code><code class="p">(</code><code class="o">**</code><code class="p">{</code><code class="o">**</code><code class="n">combn</code><code class="p">,</code> <code class="o">**</code><code class="n">model_params</code><code class="p">})</code>
  <code class="n">_</code><code class="p">,</code> <code class="n">metrics</code> <code class="o">=</code> <code class="n">train</code><code class="p">(</code>
    <code class="n">state</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
      <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code>
      <code class="n">dummy_input</code><code class="o">=</code><code class="p">{</code>
        <code class="s2">"graph"</code><code class="p">:</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">graph</code><code class="p">,</code>
        <code class="s2">"pairs"</code><code class="p">:</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">pairs</code><code class="o">.</code><code class="n">get_dummy_input</code><code class="p">(),</code>
      <code class="p">},</code>
      <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adam</code><code class="p">(</code><code class="mf">0.001</code><code class="p">),</code>
    <code class="p">),</code>
    <code class="o">**</code><code class="n">training_params</code><code class="p">,</code>
    <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="sa">f</code><code class="s2">"graphs/models/sweep_all_</code><code class="si">{</code><code class="n">name_from_params</code><code class="p">(</code><code class="n">combn</code><code class="p">)</code><code class="si">}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">)</code>
  <code class="n">df</code> <code class="o">=</code> <code class="n">to_df</code><code class="p">(</code><code class="n">metrics</code><code class="p">)</code><code class="o">.</code><code class="n">assign</code><code class="p">(</code><code class="o">**</code><code class="n">combn</code><code class="p">)</code>
  <code class="n">all_metrics</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">df</code><code class="p">)</code>

<code class="n">all_metrics_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">(</code><code class="n">all_metrics</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
</pre>

<p>Similar to the earlier experiment, this loop automates the process of training models across all parameter combinations. Each model’s performance is evaluated, and metrics such as Hits@20 are recorded for later analysis.</p>

<p>We then calculate the maximum Hits@20 metric for each parameter combination and split. Additionally, we generate a more readable representation for the convolutional layer configurations used in the encoder. This systematic exploration helps us identify the most effective combinations of hyperparameters for optimizing the model.</p>

<p class="pagebreak-before">We then extract the maximum Hits@20 metric for each parameter combination and split:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">conv_layer_annot</code><code class="p">(</code><code class="n">row</code><code class="p">):</code>
  <code class="k">if</code> <code class="n">row</code><code class="p">[</code><code class="s2">"last_layer_self"</code><code class="p">]</code> <code class="ow">and</code> <code class="n">row</code><code class="p">[</code><code class="s2">"degree_norm"</code><code class="p">]:</code>
    <code class="k">return</code> <code class="s2">"with self-edges and norm"</code>
  <code class="k">elif</code> <code class="n">row</code><code class="p">[</code><code class="s2">"last_layer_self"</code><code class="p">]:</code>
    <code class="k">return</code> <code class="s2">"with self-edges, no norm"</code>
  <code class="k">elif</code> <code class="n">row</code><code class="p">[</code><code class="s2">"degree_norm"</code><code class="p">]:</code>
    <code class="k">return</code> <code class="s2">"with norm, no self-edges"</code>
  <code class="k">else</code><code class="p">:</code>
    <code class="k">return</code> <code class="s2">"no self-edges and no norm"</code>


<code class="n">data</code> <code class="o">=</code> <code class="n">all_metrics_df</code>
<code class="n">data</code> <code class="o">=</code> <code class="n">data</code><code class="p">[(</code><code class="n">data</code><code class="p">[</code><code class="s2">"metric"</code><code class="p">]</code> <code class="o">==</code> <code class="s2">"hits@20"</code><code class="p">)]</code>
<code class="n">data</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code>
  <code class="p">[</code><code class="s2">"metric"</code><code class="p">,</code> <code class="s2">"split"</code><code class="p">,</code> <code class="o">*</code><code class="nb">list</code><code class="p">(</code><code class="n">model_params_sweep</code><code class="o">.</code><code class="n">keys</code><code class="p">())],</code> <code class="n">as_index</code><code class="o">=</code><code class="kc">False</code>
<code class="p">)[</code><code class="s2">"mean"</code><code class="p">]</code><code class="o">.</code><code class="n">max</code><code class="p">()</code>
<code class="n">data</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="p">[</code><code class="s2">"split"</code><code class="p">,</code> <code class="s2">"mean"</code><code class="p">])</code>
<code class="n">data</code><code class="p">[</code><code class="s2">"conv_layer"</code><code class="p">]</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">conv_layer_annot</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
</pre>

<p>Next, in <a data-type="xref" href="#ddi-sweep-all">Figure 4-12</a>, we visualize an overview of model performance across different hyperparameter combinations. This plot contains a lot of information, so let’s first clarify how to interpret it. Each pair of points represents the training and validation Hits@20 scores for a single model configuration. Our goal is to identify configurations where validation performance is high and closely matches training performance—suggesting good generalization without overfitting. This is particularly important because the dataset is split by protein targets, meaning that the validation set contains drugs targeting different proteins than those seen during training.</p>

<div data-type="tip"><h6>Tip</h6>
<p>Models that show similar performance on both training and validation sets are more likely to generalize well. Prioritizing these configurations can help guide robust model selection.</p>
</div>

<pre data-code-language="python" data-type="programlisting">
<code class="n">fig</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">relplot</code><code class="p">(</code>
  <code class="n">data</code><code class="o">=</code><code class="n">data</code><code class="p">,</code>
  <code class="n">x</code><code class="o">=</code><code class="s2">"conv_layer"</code><code class="p">,</code>
  <code class="n">y</code><code class="o">=</code><code class="s2">"mean"</code><code class="p">,</code>
  <code class="n">row</code><code class="o">=</code><code class="s2">"dropout_rate"</code><code class="p">,</code>
  <code class="n">col</code><code class="o">=</code><code class="s2">"n_mlp_layers"</code><code class="p">,</code>
  <code class="n">hue</code><code class="o">=</code><code class="s2">"split"</code><code class="p">,</code>
  <code class="n">palette</code><code class="o">=</code><code class="n">DEFAULT_SPLIT_COLORS</code><code class="p">,</code>
  <code class="n">facet_kws</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">margin_titles</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">despine</code><code class="o">=</code><code class="kc">False</code><code class="p">),</code>
  <code class="n">height</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
<code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">figure</code><code class="o">.</code><code class="n">subplots_adjust</code><code class="p">(</code><code class="n">wspace</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">hspace</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="k">for</code> <code class="n">ax</code> <code class="ow">in</code> <code class="n">fig</code><code class="o">.</code><code class="n">axes</code><code class="o">.</code><code class="n">flat</code><code class="p">:</code>
  <code class="k">for</code> <code class="n">label</code> <code class="ow">in</code> <code class="n">ax</code><code class="o">.</code><code class="n">get_xticklabels</code><code class="p">():</code>
    <code class="n">label</code><code class="o">.</code><code class="n">set_rotation</code><code class="p">(</code><code class="mi">45</code><code class="p">)</code>
    <code class="n">label</code><code class="o">.</code><code class="n">set_ha</code><code class="p">(</code><code class="s2">"right"</code><code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_axis_labels</code><code class="p">(</code><code class="s2">""</code><code class="p">,</code> <code class="s2">"maximum Hits@20"</code><code class="p">);</code>
</pre>

<figure><div id="ddi-sweep-all" class="figure"><img alt="" src="assets/dlfb_0412.png" width="600" height="587"/>
<h6><span class="label">Figure 4-12. </span>Maximum Hits@20 achieved by each configuration in our hyper-parameter grid. Rows correspond to dropout_rate (0.0, 0.3, 0.5); columns vary the number of MLP layers (1–3) in the link predictor. Each x-axis category toggles self-loops and degree normalization. The best generalization comes from the minimal model (far left, <span class="keep-together">top row</span>).</h6>
</div></figure>

<p>There are many models in this plot, but one stands out in the upper-left corner: the best-performing model without dropout. It uses a simple convolutional setup (no self-edges, no normalization) and a single-layer link predictor. It’s notable that such a minimal configuration performs so well, suggesting that simpler models can still be highly effective.</p>

<p>Other observations from the plot:</p>

<ul>
  <li><p>Degree normalization hurts almost everywhere. Both “with norm” columns show a noticeable drop in validation performance relative to their “no norm” <span class="keep-together">counterparts</span>.</p></li>
  <li><p>Deeper MLP heads overfit more. Moving from one to three MLP layers widens the train–valid gap and very rarely yields a higher peak validation score.</p></li>
  <li><p>Dropout does not consistently help, though some models with high dropout perform well, suggesting it’s not strictly harmful either.</p></li>
  <li><p>Self-edges make little difference. The two “with self-edges” categories track their “no self-edges” twins closely, suggesting that this graph already conveys enough reciprocal information. Overall, the task appears quite forgiving—many configurations exceed 0.8 on validation—but the simplest, lowest-capacity model remains the most reliable choice.</p></li>
  </ul>

<p>Let’s now inspect the learning curves for the best-performing configuration, shown in <a data-type="xref" href="#ddi-best-model">Figure 4-13</a>.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">dlfb.utils.metric_plots</code> <code class="kn">import</code> <code class="n">from_df</code>

<code class="n">metrics</code> <code class="o">=</code> <code class="n">all_metrics_df</code>
<code class="n">metrics</code> <code class="o">=</code> <code class="n">from_df</code><code class="p">(</code>
  <code class="n">metrics</code><code class="p">[</code>
    <code class="p">(</code><code class="n">metrics</code><code class="p">[</code><code class="s2">"dropout_rate"</code><code class="p">]</code> <code class="o">==</code> <code class="mf">0.0</code><code class="p">)</code>
    <code class="o">&amp;</code> <code class="p">(</code><code class="n">metrics</code><code class="p">[</code><code class="s2">"last_layer_self"</code><code class="p">]</code> <code class="o">==</code> <code class="kc">False</code><code class="p">)</code>
    <code class="o">&amp;</code> <code class="p">(</code><code class="n">metrics</code><code class="p">[</code><code class="s2">"degree_norm"</code><code class="p">]</code> <code class="o">==</code> <code class="kc">False</code><code class="p">)</code>
    <code class="o">&amp;</code> <code class="p">(</code><code class="n">metrics</code><code class="p">[</code><code class="s2">"n_mlp_layers"</code><code class="p">]</code> <code class="o">==</code> <code class="mi">1</code><code class="p">)</code>
  <code class="p">]</code>
<code class="p">)</code>

<code class="n">plot_learning</code><code class="p">(</code><code class="n">metrics</code><code class="p">);</code>
</pre>

<p>Recall that the previous metrics represented maximum values—meaning they may not have occurred at the same point in training and do not rule out eventual overfitting if training were to proceed. Plotting the full learning curves provides additional insight. In <a data-type="xref" href="#ddi-best-model">Figure 4-13</a>, we see strong learning on the training set, but the configuration appears prone to overfitting without early stopping. This is evident from the widening gap between training and validation metrics over time.</p>

<figure><div id="ddi-best-model" class="figure"><img alt="" src="assets/dlfb_0413.png" width="600" height="296"/>
<h6><span class="label">Figure 4-13. </span>Learning curves for loss (left) and Hits@20 (right) of the best-performing configuration. The model achieves near-perfect training performance, but the validation metric plateaus and then declines, indicating overfitting.</h6>
</div></figure>


<p>Let’s also examine the learning curves of an alternative high-performing model in <a data-type="xref" href="#ddi-alt-best-model">Figure 4-14</a>. This configuration comes from a different region of the hyperparameter space: it includes more layers in the link predictor and applies a high dropout rate to encourage generalization. It also uses a different convolutional setup. Notably, the training and validation curves remain closer together, suggesting that this model learns a more generalizable representation and is less susceptible to overfitting:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">metrics</code> <code class="o">=</code> <code class="n">all_metrics_df</code>
<code class="n">metrics</code> <code class="o">=</code> <code class="n">from_df</code><code class="p">(</code>
  <code class="n">metrics</code><code class="p">[</code>
    <code class="p">(</code><code class="n">metrics</code><code class="p">[</code><code class="s2">"dropout_rate"</code><code class="p">]</code> <code class="o">==</code> <code class="mf">0.5</code><code class="p">)</code>
    <code class="o">&amp;</code> <code class="n">metrics</code><code class="p">[</code><code class="s2">"last_layer_self"</code><code class="p">]</code>
    <code class="o">&amp;</code> <code class="p">(</code><code class="n">metrics</code><code class="p">[</code><code class="s2">"degree_norm"</code><code class="p">]</code> <code class="o">==</code> <code class="kc">False</code><code class="p">)</code>
    <code class="o">&amp;</code> <code class="p">(</code><code class="n">metrics</code><code class="p">[</code><code class="s2">"n_mlp_layers"</code><code class="p">]</code> <code class="o">==</code> <code class="mi">2</code><code class="p">)</code>
  <code class="p">]</code>
<code class="p">)</code>

<code class="n">plot_learning</code><code class="p">(</code><code class="n">metrics</code><code class="p">);</code>
</pre>

<figure><div id="ddi-alt-best-model" class="figure"><img alt="" src="assets/dlfb_0414.png" width="600" height="288"/>
<h6><span class="label">Figure 4-14. </span>Learning curves for loss (left) and Hits@20 (right) of an alternative high-performing model. This configuration demonstrates improved generalization, with train and validation metrics remaining stable and closely aligned.</h6>
</div></figure>

<p>While it’s tempting to focus only on the top headline metrics to identify the best-performing configuration, examining training dynamics can offer deeper insight into model performance. Based on both maximum metrics and learning curve behavior, we would recommend this latest model for its strong performance and stability.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>There are many additional hyperparameters that could be explored—including those already present in our model and training loop, as well as others not yet considered. For example, we could experiment with alternative negative sampling strategies, vary the ratio of negative to positive pairs, or increase the depth of the graph convolutional layers. However, for this smaller dataset, the current performance is strong enough that we consider further tuning unnecessary<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html44" data-type="indexterm" id="id811"/>.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html42" data-type="indexterm" id="id812"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html41" data-type="indexterm" id="id813"/></p>
</div>
</div></section>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Train on a Larger Dataset"><div class="sect2" id="training-on-a-larger-dataset">
<h2>Train on a Larger Dataset</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="improving the model" data-tertiary="training on larger dataset" data-type="indexterm" id="ch04_graphs.html45"/><a contenteditable="false" data-primary="training" data-secondary="drug–drug interaction graphs" data-tertiary="training on larger dataset" data-type="indexterm" id="ch04_graphs.html46"/>Finally, we train a model on a much larger DDI dataset—scaling up from using roughly one-tenth to approximately one-half of the available data.</p>
<p>As noted earlier, the optimal model configuration often depends on dataset size and graph connectivity. With this larger dataset, additional hyperparameter exploration revealed that <em>degree normalization</em> becomes critical for strong performance. Moreover, a moderate dropout rate of 0.3 struck the best balance between regularization and learning capacity for this setting.</p>
<p class="pagebreak-before">Here is the full <code>DdiModel</code> setup:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">node_limit</code> <code class="o">=</code> <code class="mi">2134</code>
<code class="n">rng</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">rng</code><code class="p">,</code> <code class="n">rng_dataset</code><code class="p">,</code> <code class="n">rng_init</code><code class="p">,</code> <code class="n">rng_train</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">4</code><code class="p">)</code>
<code class="n">dataset_splits</code> <code class="o">=</code> <code class="n">DatasetBuilder</code><code class="p">(</code><code class="n">path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"graphs/datasets"</code><code class="p">))</code><code class="o">.</code><code class="n">build</code><code class="p">(</code>
  <code class="n">node_limit</code><code class="p">,</code> <code class="n">rng_dataset</code>
<code class="p">)</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">DdiModel</code><code class="p">(</code>
  <code class="n">n_nodes</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">n_nodes</code><code class="p">,</code>
  <code class="n">embedding_dim</code><code class="o">=</code><code class="mi">512</code><code class="p">,</code>
  <code class="n">dropout_rate</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code>
  <code class="n">last_layer_self</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
  <code class="n">degree_norm</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
  <code class="n">n_mlp_layers</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
<code class="p">)</code>

<code class="n">_</code><code class="p">,</code> <code class="n">metrics</code> <code class="o">=</code> <code class="n">train</code><code class="p">(</code>
  <code class="n">state</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code>
    <code class="n">dummy_input</code><code class="o">=</code><code class="p">{</code>
      <code class="s2">"graph"</code><code class="p">:</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">graph</code><code class="p">,</code>
      <code class="s2">"pairs"</code><code class="p">:</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">pairs</code><code class="o">.</code><code class="n">get_dummy_input</code><code class="p">(),</code>
    <code class="p">},</code>
    <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adam</code><code class="p">(</code><code class="mf">0.001</code><code class="p">),</code>
  <code class="p">),</code>
  <code class="n">rng</code><code class="o">=</code><code class="n">rng_train</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">,</code>
  <code class="n">num_epochs</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code>
  <code class="n">eval_every</code><code class="o">=</code><code class="mi">25</code><code class="p">,</code>
  <code class="n">loss_fn</code><code class="o">=</code><code class="n">auc_loss</code><code class="p">,</code>
  <code class="n">norm_loss</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
  <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"graphs/models/larger_model"</code><code class="p">),</code>
<code class="p">)</code>
</pre>

<p>The resulting learning curves in <a data-type="xref" href="#ddi-large-best-model">Figure 4-15</a> look quite different from the smaller-set runs. Validation Hits@20 climbs to ∼0.9—on par with our best earlier models—but training Hits@20 remains unexpectedly low, hovering near 0.1. This wide gap likely reflects the increased difficulty of distinguishing positives from a much larger and more diverse set of negatives in the denser graph. It may also indicate that even with an AUC-like loss, nailing top-k ranking metrics like Hits@20 remains challenging at scale. Either way, further work—such as improved negative sampling or top-k–specific objectives—may be needed to fully leverage the larger dataset, and we encourage you to explore further.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">plot_learning</code><code class="p">(</code><code class="n">metrics</code><code class="p">);</code>
</pre>

<figure><div id="ddi-large-best-model" class="figure"><img alt="" src="assets/dlfb_0415.png" width="600" height="288"/>
<h6><span class="label">Figure 4-15. </span>Learning curves for loss (left) and Hits@20 (right) of the best-performing model trained on the larger dataset. Maximum validation performance is high but does not show significant improvement over the performance of models trained on smaller datasets, suggesting further hyperparameter tuning or model adjustments may be needed.</h6>
</div></figure>


<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The strong validation performance suggests this model could scale to even larger datasets. However, scaling beyond 50% of the dataset becomes difficult due to memory constraints. With an embedding size of 512 for all nodes, the current implementation can run into out-of-memory (OOM) issues during XLA compilation.</p>
<p>A better solution would be to adopt a sampling strategy, as used in GraphSAGE, where the model processes subgraphs in batches. This makes it feasible to scale to larger datasets without reducing embedding size or compromising performance. However, implementing this is beyond the scope of the current chapter.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html46" data-type="indexterm" id="id814"/><a contenteditable="false" data-primary="" data-startref="ch04_graphs.html45" data-type="indexterm" id="id815"/></p>
</div>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Extensions"><div class="sect2" id="extensions">
<h2>Extensions</h2>

<p><a contenteditable="false" data-primary="drug–drug interactions, understanding using graphs" data-secondary="improving the model" data-tertiary="extensions" data-type="indexterm" id="ch04_graphs.html47"/>There are many ways to extend the model and training setup explored in this chapter:</p>

<dl>
<dt>Graph sampling for scalability</dt> 
<dd><p>Replace full-batch training with neighborhood sampling to scale to larger graphs without exceeding memory constraints.</p></dd>
<dt>Incorporate node features</dt> 
<dd><p>Integrate drug-specific information, such as chemical structure or pharmacological annotations, to enrich the learned embeddings and improve prediction <span class="keep-together">accuracy</span>.</p></dd>
<dt>Improve negative sampling</dt> 
<dd><p>Use more informative strategies like hard negative mining, where the model is shown negative pairs it finds confusing (e.g., those with high predicted interaction scores). This encourages the model to focus on challenging distinctions and improves generalization.</p></dd>
<dt>Try alternative GNN architectures</dt> 
<dd><p>Explore other models like Graph Attention Networks, which weigh the importance of neighboring nodes, which are theoretically more expressive.</p></dd>
<dt>Transfer to new biological problems</dt> 
<dd><p>Apply the same modeling framework to other interaction networks, such as gene regulatory, protein-protein, or drug-target interaction graphs.</p></dd>
</dl>

<p>Additionally, here are some analysis ideas you could explore:</p>
<dl>
<dt>Error analysis by drug class</dt> 
<dd><p>Break down model performance by therapeutic class or chemical category to identify where the model struggles most.</p></dd>
<dt>Prediction certainty</dt> 
<dd><p>Examine the distribution of predicted probabilities. Which drug pairs is the model highly confident about (close to 0 or 1)? Which fall near 0.5? Investigate whether uncertain predictions share common characteristics (e.g., unusual structures, sparse connectivity).</p></dd>
<dt>Embedding visualization</dt> 
<dd><p>Use dimensionality reduction methods (e.g., t-SNE, UMAP) to project node embeddings into 2D and inspect whether drugs with similar functions cluster together.</p></dd>
<dt>Temporal validation</dt> 
<dd><p>If drug interaction timestamps are available, evaluate the model on future data after training on past interactions. This mimics a real-world deployment <span class="keep-together">scenario</span>.</p></dd>
<dt>Counterfactual analysis</dt> 
<dd><p>Perturb the graph structure—for example, remove a known interaction or introduce a plausible but incorrect one—and observe how predictions change. This helps probe model sensitivity and identify influential edges<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html47" data-type="indexterm" id="id816"/>.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html39" data-type="indexterm" id="id817"/></p></dd>
</dl>

</div></section>
</div></section>
<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Summary"><div class="sect1" id="conclusion">
<h1>Summary</h1>

<p>In this chapter, we developed graph neural network models to predict links between nodes, applying them to the biologically meaningful task of drug–drug interaction (DDI) prediction. Starting with a simple architecture, we systematically explored model components and training strategies, eventually achieving strong validation performance through careful tuning of the loss function and key hyperparameters. Along the way, we introduced practical tools for evaluating performance, diagnosing overfitting, and scaling to larger datasets. These techniques are broadly applicable beyond DDIs, extending to a wide range of biological graph problems.</p>
<p>Our results show that even relatively simple graph models, when thoughtfully designed and optimized, can learn meaningful biological structure. As graph-based data becomes increasingly central in biology, the approaches introduced here provide a solid foundation for tackling more complex tasks—in drug discovery, genomics, systems biology, and beyond.<a contenteditable="false" data-primary="" data-startref="ch04_graphs.html0" data-type="indexterm" id="id818"/></p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id726"><sup><a href="ch04.html#id726-marker">1</a></sup> Udrescu, L. et al. <a href="https://oreil.ly/rdShs">“Clustering Drug–Drug Interaction Networks with Energy Model Layouts: Community Analysis and Drug Repurposing”</a>. <em>Scientific Reports</em>, 6 (2016): 32745.</p><p data-type="footnote" id="id727"><sup><a href="ch04.html#id727-marker">2</a></sup> Hu, Weihua, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020. <a href="https://oreil.ly/eK20s">“Open Graph Benchmark: Datasets for Machine Learning on Graphs”</a>. arXiv.Org. May 2, 2020.</p><p data-type="footnote" id="id740"><sup><a href="ch04.html#id740-marker">3</a></sup> Gilmer et al., “Neural Message Passing for Quantum Chemistry,” <em>Proceedings of the 34th International Conference on Machine Learning</em> (Sydney, NSW, Australia), vol. 70 (2017): 1263–1272, JMLR.org.</p><p data-type="footnote" id="id741"><sup><a href="ch04.html#id741-marker">4</a></sup> F. Scarselli et al., “The Graph Neural Network Model,” <em>IEEE Transactions on Neural Networks</em> 20, no. 1 (2009): 61–80.</p><p data-type="footnote" id="id754"><sup><a href="ch04.html#id754-marker">5</a></sup> Hamilton, W. L., Ying, Z., &amp; Leskovec, J. (2017). <a href="https://oreil.ly/8mNRd">Inductive representation learning on large graphs</a>. <em>Neural Information Processing Systems</em>, 30, 1024–1034.</p><p data-type="footnote" id="id764"><sup><a href="ch04.html#id764-marker">6</a></sup> Google DeepMind. <a href="https://oreil.ly/r6MCp">“Intro to Graph Neural Nets with JAX/Jraph”</a>.</p><p data-type="footnote" id="id808"><sup><a href="ch04.html#id808-marker">7</a></sup> Wang, Z., Zhou, Y., Hong, L., Zou, Y., Su, H., &amp; Chen, S. (2021). <a href="https://doi.org/10.48550/arxiv.2112.02936">Pairwise learning for neural link prediction</a>. arXiv (Cornell University).</p></div></div></section></div></div></body></html>