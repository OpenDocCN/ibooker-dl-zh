["```py\nimport asyncio\nfrom PyPDF2 import PdfReader\nimport semantic_kernel as sk\nfrom semantic_kernel.connectors.ai.open_ai import \n↪(AzureChatCompletion,AzureTextEmbedding)\nfrom semantic_kernel.memory.semantic_text_memory\n↪import SemanticTextMemory\nfrom semantic_kernel.core_plugins.text_memory_plugin \n↪import TextMemoryPlugin\nfrom semantic_kernel.connectors.memory.chroma import \n↪ChromaMemoryStore\n\n# Load environment variables\nAOAI_KEY = os.getenv(\"AOAI_KEY\")\nAOAI_ENDPOINT = os.getenv(\"AOAI_ENDPOINT\")\nAOAI_MODEL = \"gpt-35-turbo\"\nAOAI_EMBEDDINGS = \"text-embedding-ada-002\"\nAPI_VERSION = '2023-09-15-preview'\n\nPERSIST_DIR = os.getenv(\"PERSIST_DIR\")\nVECTOR_DB = os.getenv(\"VECTOR_DB\")\n\nDOG_BOOKS = \"./data/dog_books\"\nDEBUG = False\nVECTOR_DB = \"dog_books\"\nPERSIST_DIR = \"./storage\"\nALWAYS_CREATE_VECTOR_DB = False\n\n# Load PDFs and extract text\ndef load_pdfs():\n    docs = []\n    total_docs = 0\n    total_pages = 0\n    filenames = [filename for filename in \n      ↪os.listdir(DOG_BOOKS) if filename.endswith(\".pdf\")]\n    with tqdm(total=len(filenames), desc=\"Processing PDFs\") \n      ↪as pbar_outer:\n        for filename in filenames:\n            pdf_path = os.path.join(DOG_BOOKS, filename)\n            with open(pdf_path, \"rb\") as file:\n                pdf = PdfReader(file, strict=False)\n                j = 0\n                total_docs += 1\n                with tqdm(total=len(pdf.pages), \n                  ↪desc=\"Loading Pages\") as pbar_inner:\n                    for page in pdf.pages:\n                        total_pages += 1\n                        j += 1\n                        docs.append(page.extract_text())\n                        pbar_inner.update()\n                pbar_outer.update()\n    print(f\"Processed {total_docs} PDFs with {total_pages} pages.\")\n    return docs\n```", "```py\n# Populate the DB with the PDFs\nasync def populate_db(memory: SemanticTextMemory, docs) -> None:\n    for i, doc in enumerate(tqdm_asyncio.tqdm(docs, desc=\"Populating DB\")):\n        if doc:  #Check if doc is not empty\n            try:\n                await memory.save_information(VECTOR_DB,id=str(i),text=doc)\n            except Exception as e:\n                print(f\"Failed to save information for doc {i}: {e}\")\n                continue  # Skip to the next iteration\n\n# Load the vector DB\nasync def load_vector_db(memory: SemanticTextMemory, \n  ↪vector_db_name: str) -> None:\n    if not ALWAYS_CREATE_VECTOR_DB:\n        collections = await memory.get_collections()\n        if vector_db_name in collections:\n            print(f\" Vector DB {vector_db_name} exists in the \n                  ↪collections. We will reuse this.\")\n            return\n\n    print(f\" Vector DB {vector_db_name} does not exist in the collections.\")\n    print(\"Reading the pdfs...\")\n\n    pdf_docs = load_pdfs()\n    print(\"Total PDFs loaded: \", len(pdf_docs))\n    print(\"Creating embeddings and vector db of the PDFs...\")\n    # This may take some time as we call embedding API for each row\n    await populate_db(memory, pdf_docs)\n```", "```py\nasync def main():\n    # Setup Semantic Kernel\n    kernel = sk.Kernel()\n    kernel.add_service(AzureChatCompletion(\n            service_id=\"chat_completion\",\n            deployment_name=AOAI_MODEL,\n            endpoint=AOAI_ENDPOINT,\n            api_key=AOAI_KEY,\n            api_version=API_VERSION))\n\n    kernel.add_service(AzureTextEmbedding(\n            service_id=\"text_embedding\",\n            deployment_name=AOAI_EMBEDDINGS,\n            endpoint=AOAI_ENDPOINT,\n            api_key=AOAI_KEY))\n\n    # Specify the type of memory to attach to SK. \n    # Here we will use Chroma as it is easy to run it locally\n    # You can specify location of Chroma DB files.\n    store = ChromaMemoryStore(persist_directory=PERSIST_DIR)\n    memory = SemanticTextMemory(storage=store, \n    ↪embeddings_generator = kernel.get_service(\"text_embedding\"))\n    kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPluginACDB\")\n\n    await load_vector_db(memory, VECTOR_DB)\n\n    while True:\n        prompt = check_prompt(input('Ask a question against \n        ↪the PDF (type \"quit\" to exit):'))\n\n        # Query the memory for most relevant match using\n        # search_async specifying relevance score and \n        # \"limit\" of number of closest documents\n        result = await memory.search(collection=VECTOR_DB, \n        ↪limit=3, min_relevance_score=0.7, query=prompt)\n        if result:\n            print(result[0].text)\n        else:\n            print(\"No matches found.\")\n\n        print(\"-\" * 80)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```", "```py\nfrom langchain_community.vector stores import FAISS\nfrom langchain_community.docstore.document import Document\nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain.text_splitter import CharacterTextSplitter\n...\n\ndef create_index():\n    # load the documents and create the index\n    docs = load_pdfs()\n\n    text_splitter = CharacterTextSplitter(\n        separator=\"\\n\",\n        chunk_size=2048,\n        chunk_overlap=200,\n        length_function=len\n    )\n\n    # Convert the chunks of text into embeddings\n    print(\"Chunking and creating embeddings...\")\n    chunks = text_splitter.split_documents(docs)\n    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_KEY)\n    vectordb = FAISS.from_documents(chunks, embeddings)\n\n    return vectordb\n\ndef main():\n    vectordb = create_index()\n    llm = OpenAI(openai_api_key=OPENAI_KEY)\n    chain = load_qa_chain(llm, chain_type='stuff')\n\n    while True:\n        prompt = check_prompt(input(\n            'Ask a question against the PDF (type \"quit\" to exit):'))\n        docs = vectordb.similarity_search(prompt, k=3, fetch_k=10)\n        response = chain.invoke({'input_documents': docs,\n                                 'question': prompt},\n                                return_only_outputs=True)\n        print(f\"Answer:\\n {response['output_text']}\")\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\nfrom llama_index.core import (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    StorageContext,\n    load_index_from_storage,\n    Settings\n)\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.readers.file import PDFReader\n\nPERSIST_DIR = \"./storage/llamaindex\"\nDOG_BOOKS = \"./data/dog_books/\"\n\nOPENAI_KEY = os.getenv('OPENAI_API_BOOK_KEY')                       #1\nSettings.embed_model = OpenAIEmbedding(api_key=OPENAI_KEY)         #1\n\ndef load_or_create_index():\n    if not os.path.exists(PERSIST_DIR):                            #2\n        try:\n            parser = PDFReader()\n            file_extractor = {\".pdf\": parser}\n\n            # load only PDFs\n            required_exts = [\".pdf\"]                               #3\n            documents = SimpleDirectoryReader(DOG_BOOKS, \n                           ↪file_extractor=file_extractor, \n                           ↪required_exts=required_exts).load_data()\n            index = VectorStoreIndex.from_documents(             #4\n                           ↪documents, show_progress=True)\n\n            # store the index for later\n            index.storage_context.persist(persist_dir=PERSIST_DIR)   #5\n\n            print(\"Index created and stored in\", PERSIST_DIR)\n        except Exception as e:\n            print(\"Error while creating index:\", e)\n            exit()\n    else:\n        print(\"Loading existing index from\", PERSIST_DIR)\n\n        try:\n            # load the existing index\n            storage_context = StorageContext.from_defaults( \n                              ↪persist_dir=PERSIST_DIR)\n            index = load_index_from_storage(storage_context)  #6\n        except Exception as e:\n            print(\"Error while loading index:\", e)\n            exit()\n    return index\n\ndef main():\n    index = load_or_create_index()\n    query_engine = index.as_query_engine()\n\n    while True:\n        prompt = input(\"Ask a question about dogs:\")\n        response = query_engine.query(prompt)\n        print(response)\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport openai\n...\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", \n                                             torch_dtype=\"auto\",\n                                             trust_remote_code=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", \n                                          trust_remote_code=True)\n\ndef check_dog_question(question):\n    prompt = f\"Instruct: Is there anything about dogs in the \n             ↪question below? If yes, answer with 'yes' else \n             ↪'no'.\\nQuestion:{question}\\nOutput: \"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\",\n                       return_attention_mask=False,\n                       add_special_tokens=False)\n    outputs = model.generate(**inputs,\n                             max_length=500,\n                             pad_token_id=tokenizer.eos_token_id)\n    text = tokenizer.batch_decode(outputs)[0]\n    regex = \"^Output: Yes$\"\n    match = re.search(regex, text, re.MULTILINE)\n    if match:\n        return True\n\n    return False\n\ndef handle_dog_question(question):\n    print( \"This is a response from RAG and GPT4\")\n\n    # Call OpenAI's GPT-4 to answer the question\n    openai.api_key = \"YOUR_API_KEY\"\n    response = openai.Completion.create(\n      …\n    )\n    return response\n\nif __name__==\"__main__\": \n    # Loop until the user enters \"quit\"\n    while True:\n        # Take user input\n        user_prompt = input(\n           \"What is your question (or type 'quit' to exit):\")\n\n        if check_dog_question(user_prompt):\n            print(handle_dog_question(user_prompt))\n        else:\n            print(\"You did not ask about dogs\")\n```", "```py\ntry:\n    response = openai_client.chat_completions(\n        messages=message_list,\n        openai_settings=ChatCompletionsSettings(\n            **bot_config[\"approach_classifier\"][\"openai_settings\"]\n        ),\n        api_base=f\"https://{AZURE_OPENAI_SERVICE}.openai.azure.com\",\n        api_key=AZURE_OPENAI_KEY,\n    )\n    except openai.error.InvalidRequestError as e:\n        self.logger.error(f\"AOAI API Error: {e}\", exc_info=True)\n        raise e\n\n    classification_response: str = response[\"choices\"][0] \n    ↪[\"message\"][\"content\"]\n    self.log_aoai_response_details(\n        f'Classification Prompt:{history[-1][\"utterance\"]}',\n        f\"Response: {classification_response}\",\n        response,\n    )\n    if classification_response == \"1\":\n        return ApproachType.structured\n    elif classification_response == \"2\":\n        return ApproachType.unstructured\n    elif classification_response == \"3\":\n        return ApproachType.chit_chat\n    elif classification_response == \"4\":\n        # Continuation: Return last question type from history\n        ...\n        else:\n            return ApproachType.unstructured\n    elif classification_response == \"5\":\n        # User has typed something that violates guardrails\n        return ApproachType.inappropriate\n    else:\n        return ApproachType.unstructured\n```", "```py\nYou are an intent classifier for Microsoft Surface product Sales \n↪and Marketing teams. The user will input a statement. You will focus \n↪on the main intent of the user statement and you respond with only \n↪one of four values - '1', '2', '3', '4', or '5'. \n\nBelow is a list of Rules that you must adhere to:\nRules:\nA: Stricly answer questions relating to Microsoft Surface products.\nB: For tabular information return it as an html table. \nC: Do not use markdown format in your responses.\nD: Do not disclose or respond to any proprietary information, IP, \n      ↪secrets, keys, data center, and infrastructure details in \n      ↪your response.\nE: Do not mention or compare to any competitors (i.e. Apple MacBook, \n      ↪Lenovo, HP, etc).\nF: Note if the user asks something illegal, harmful or malicious.\n\nYou will not try to respond to the user's question, you will just \n    ↪classify the user statement based on the below classification rule:\n- For questions about past sales, prices, stores or stock of products \n    ↪such as devices and laptops, respond with 1\n- For questions on specifications of products/devices/laptops or \n    ↪marketing them, respond with 2\n- If the question is idle chit-chat, pleasantries such as greetings, \n    ↪or sligthly off topic but doesn't break the rules, respond with 3\n- If the user is asking for more details about a previous question, \n    ↪respond with 4\n- If the message is not in compliance with Rule F, respond with 5\n\nExamples:\nUser: How much stock of this are we currently carrying?\nAssistant: 1\n\nUser: Give me its specifications\nAssistant: 2\n\nUser: How many MacBook Air do we have in stock?\nAssistant: 3\n\nUser: Tell me more about it\nAssistant: 4\n...\n```"]