<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 7. Dimensionality Reduction"><div class="chapter" id="dimensionality_chapter">
<h1><span class="label">Chapter 7. </span>Dimensionality Reduction</h1>


<p>Many<a data-type="indexterm" data-primary="dimensionality reduction" id="xi_dimensionalityreduction735_1"/> machine learning problems involve thousands or even millions of features for each training instance. Not only do all these features make training extremely slow, but they can also make it much harder to find a good solution, as you will see. This problem is often referred to as the <em>curse of dimensionality</em>.<a data-type="indexterm" data-primary="curse of dimensionality" id="id1862"/><a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="curse of dimensionality" id="id1863"/></p>

<p>Fortunately, in real-world problems, it is often possible to reduce the number of features considerably, turning an intractable problem into a tractable one. For example, consider the MNIST images (introduced in <a data-type="xref" href="ch03.html#classification_chapter">Chapter¬†3</a>): the pixels on the image borders are almost always white, so you could completely drop these pixels from the training set without losing much information. As we saw in the previous chapter, <a data-type="xref" href="ch06.html#mnist_feature_importance_plot">Figure¬†6-6</a> confirms that these pixels are utterly unimportant for the classification task. Additionally, two neighboring pixels are often highly correlated: if you merge them into a single pixel (e.g., by taking the mean of the two pixel intensities), you will not lose much information, removing redundancy and sometimes even noise.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Reducing dimensionality can also drop some useful information, just like compressing an image to JPEG can degrade its quality:<a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="information loss from" id="id1864"/> it can make your system perform slightly worse, especially if you reduce dimensionality too much. Moreover, some models‚Äîsuch as neural networks‚Äîcan handle high-dimensional data efficiently and learn to reduce its dimensionality while preserving the useful information for the task at hand. In short, adding an extra preprocessing step for dimensionality reduction will not always help.</p>
</div>

<p>Apart from speeding up training and possibly improving your model‚Äôs performance, dimensionality reduction<a data-type="indexterm" data-primary="visualization of data" data-secondary="dimensionality reduction" id="id1865"/> is also extremely useful for data visualization. Reducing the number of dimensions down to two (or three) makes it possible to plot a condensed view of a high-dimensional training set on a graph and often gain some important insights by visually detecting patterns, such as clusters. Moreover, data visualization is essential to communicate your conclusions to people who are not data scientists‚Äîin particular, decision makers who will use your results.</p>

<p>In this chapter we will first discuss the curse of dimensionality and get a sense of what goes on in high-dimensional space. Then we will consider the two main approaches to dimensionality reduction (projection and manifold learning), and we will go through three of the most popular dimensionality reduction techniques: PCA, random projection, and locally linear embedding (LLE).</p>






<section data-type="sect1" data-pdf-bookmark="The Curse of Dimensionality"><div class="sect1" id="id116">
<h1>The Curse of Dimensionality</h1>

<p>We<a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="curse of dimensionality" id="xi_dimensionalityreductioncurseofdimensionality7143_1"/> are so used to living in three dimensions‚Å†<sup><a data-type="noteref" id="id1866-marker" href="ch07.html#id1866">1</a></sup> that our intuition fails us when we try to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to picture in our minds (see <a data-type="xref" href="#hypercube_wikipedia">Figure¬†7-1</a>), let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space.</p>

<figure class="width-80"><div id="hypercube_wikipedia" class="figure">
<img src="assets/hmls_0701.png" alt="Diagram illustrating the progression from a point to a tesseract, demonstrating the concept of hypercubes from 0D to 4D." width="1153" height="381"/>
<h6><span class="label">Figure 7-1. </span>Point, segment, square, cube, and tesseract (0D to 4D hypercubes)‚Å†<sup><a data-type="noteref" id="id1867-marker" href="ch07.html#id1867">2</a></sup></h6>
</div></figure>

<p>It turns out that many things behave very differently in high-dimensional space. For example, if you pick a random point in a unit square (a 1 √ó 1 square), it will have only about a 0.4% chance of being located less than 0.001 from a border (in other words, it is very unlikely that a random point will be ‚Äúextreme‚Äù along any dimension). But in a 10,000-dimensional unit hypercube, this probability is greater than 99.999999%. Most points in a high-dimensional hypercube are very close to the border.‚Å†<sup><a data-type="noteref" id="id1868-marker" href="ch07.html#id1868">3</a></sup></p>

<p>Here is a more troublesome difference: if you pick two points randomly in a unit square, the distance between these two points will be, on average, roughly 0.52. If you pick two random points in a 3D unit cube, the average distance will be roughly 0.66. But what about two points picked randomly in a 1,000,000-dimensional unit hypercube? The average distance, believe it or not, will be about 408.25 (roughly <math alttext="StartRoot StartFraction 1 comma 000 comma 000 Over 6 EndFraction EndRoot">
  <msqrt>
    <mfrac><mrow><mn>1</mn><mo lspace="0%" rspace="0%">,</mo><mn>000</mn><mo lspace="0%" rspace="0%">,</mo><mn>000</mn></mrow> <mn>6</mn></mfrac>
  </msqrt>
</math>)! This is counterintuitive: how can two points be so far apart when they both lie within the same unit hypercube? Well, there‚Äôs just plenty of space in high dimensions.</p>

<p>As a result, high-dimensional datasets are often very sparse: most training instances are likely to be far away from each other, so training methods based on distance or similarity (such as <em>k</em>-nearest neighbors) will be much less effective. And some types of models will not be usable at all because they scale poorly with the dataset‚Äôs dimensionality (e.g., SVMs or dense neural networks). And new instances will likely be far away from any training instance, making predictions much less reliable than in lower dimensions since they will be based on much larger extrapolations. Since patterns in the data will become harder to identify, models will tend to fit the noise more frequently than in lower dimensions; regularization will become all the more important. Lastly, models will become even harder to interpret.</p>

<p>In theory, some of these issues can be resolved by increasing the size of the training set to reach a sufficient density of training instances. Unfortunately, in practice, the number of training instances required to reach a given density grows exponentially with the number of dimensions. With just 100 features‚Äîsignificantly fewer than in the MNIST problem‚Äîall ranging from 0 to 1, you would need more training instances than atoms in the observable universe in order for training instances to be within 0.1 of each other on average, assuming they were spread out uniformly across all dimensions.<a data-type="indexterm" data-startref="xi_dimensionalityreductioncurseofdimensionality7143_1" id="id1869"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Main Approaches for Dimensionality Reduction"><div class="sect1" id="id397">
<h1>Main Approaches for Dimensionality Reduction</h1>

<p>Before<a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="approaches to" id="xi_dimensionalityreductionapproachesto7307_1"/> diving into specific dimensionality reduction algorithms, let‚Äôs look at the two main approaches to reducing dimensionality: projection and manifold learning.</p>








<section data-type="sect2" data-pdf-bookmark="Projection"><div class="sect2" id="id117">
<h2>Projection</h2>

<p>In<a data-type="indexterm" data-primary="projection, dimensionality reduction" id="xi_projectiondimensionalityreduction7333_1"/> most real-world problems, training instances are <em>not</em> spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated (as discussed earlier for MNIST). As a result, all training instances lie within (or close to) a much lower-dimensional <em>subspace</em> of the high-dimensional space. This sounds abstract, so let‚Äôs look at an example. In <a data-type="xref" href="#dataset_3d_plot">Figure¬†7-2</a>, a 3D dataset is represented by small spheres (I will refer to this figure several times in the following sections).</p>

<p>Notice that all training instances lie close to a plane: this is a lower-dimensional (2D) subspace of the higher-dimensional (3D) space. If we project every training instance perpendicularly onto this subspace (as represented by the short dashed lines connecting the instances to the plane), we get the new 2D dataset shown in <a data-type="xref" href="#dataset_2d_plot">Figure¬†7-3</a>. Ta-da! We have just reduced the dataset‚Äôs dimensionality from 3D to 2D. Note that the axes correspond to new features <em>z</em><sub>1</sub> and <em>z</em><sub>2</sub>: they are the coordinates of the projections on the plane.</p>

<figure class="width-75"><div id="dataset_3d_plot" class="figure">
<img src="assets/hmls_0702.png" alt="A 3D scatter plot showing data points clustered near a 2D plane, illustrating a lower-dimensional subspace in higher-dimensional space." width="2077" height="1835"/>
<h6><span class="label">Figure 7-2. </span>A 3D dataset lying close to a 2D subspace</h6>
</div></figure>

<figure class="width-70"><div id="dataset_2d_plot" class="figure">
<img src="assets/hmls_0703.png" alt="Scatter plot showing the 2D dataset with axes labeled as new features z1 and z2, illustrating dimensionality reduction from 3D to 2D." width="1795" height="1044"/>
<h6><span class="label">Figure 7-3. </span>The new 2D dataset after projection</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Manifold Learning"><div class="sect2" id="id118">
<h2>Manifold Learning</h2>

<p>Although<a data-type="indexterm" data-primary="manifold learning, dimension reduction" id="xi_manifoldlearningdimensionreduction7479_1"/><a data-type="indexterm" data-startref="xi_projectiondimensionalityreduction7333_1" id="id1870"/><a data-type="indexterm" data-primary="Swiss roll dataset" id="xi_Swissrolldataset7479_1"/> projection is fast and often works well, it‚Äôs not always the best approach to dimensionality reduction. In many cases the subspace may twist and turn, such as in the Swiss roll dataset represented in <a data-type="xref" href="#swiss_roll_plot">Figure¬†7-4</a>: this is a toy dataset containing 3D points in the shape of a Swiss roll.</p>

<figure class="width-70"><div id="swiss_roll_plot" class="figure">
<img src="assets/hmls_0704.png" alt="3D scatter plot of the Swiss roll dataset, illustrating points arranged in a spiral shape, used to demonstrate challenges in dimensionality reduction." width="1351" height="996"/>
<h6><span class="label">Figure 7-4. </span>Swiss roll dataset</h6>
</div></figure>

<p>Simply projecting onto a plane (e.g., by dropping <em>x</em><sub>3</sub>) would squash different layers of the Swiss roll together, as shown on the left side of <a data-type="xref" href="#squished_swiss_roll_plot">Figure¬†7-5</a>. What you probably want instead is to unroll the Swiss roll to obtain the 2D dataset on the righthand side of <a data-type="xref" href="#squished_swiss_roll_plot">Figure¬†7-5</a>.</p>

<figure><div id="squished_swiss_roll_plot" class="figure">
<img src="assets/hmls_0705.png" alt="Diagram showing a squashed Swiss roll dataset on the left, where layers overlap, versus an unrolled version on the right, where the data is spread out in two dimensions." width="2875" height="1071"/>
<h6><span class="label">Figure 7-5. </span>Squashing by projecting onto a plane (left) versus unrolling the Swiss roll (right)</h6>
</div></figure>

<p>The Swiss roll is an example of a 2D <em>manifold</em>. Put simply, a 2D manifold is a 2D shape that can be bent and twisted in a higher-dimensional space. More generally, a <em>d</em>-dimensional manifold is a part of an <em>n</em>-dimensional space (where <em>d</em> &lt; <em>n</em>) that locally resembles a <em>d</em>-dimensional hyperplane. In the case of the Swiss roll, <em>d</em> = 2 and <em>n</em> = 3: it locally resembles a 2D plane, but it is rolled in the third dimension.</p>

<p>Many dimensionality reduction algorithms (e.g., LLE, Isomap, t-SNE, or UMAP), work by modeling the manifold on which the training instances lie; this is called <em>manifold learning</em>. It relies on the <em>manifold assumption</em>, also called the <em>manifold hypothesis</em>,<a data-type="indexterm" data-primary="manifold hypothesis" id="id1871"/> which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. This assumption is very often empirically observed.</p>

<p>Once again, think about the MNIST dataset: all handwritten digit images have some similarities. They are made of connected lines, the borders are white, and they are more or less centered. If you randomly generated images, only a ridiculously tiny fraction of them would look like handwritten digits. In other words, the degrees of freedom available to you if you try to create a digit image are dramatically lower than the degrees of freedom you have if you are allowed to generate any image you want. These constraints tend to squeeze the dataset into a lower-dimensional manifold.</p>

<p>The manifold assumption is often accompanied by another implicit assumption: that the task at hand (e.g., classification or regression) will be simpler if expressed in the lower-dimensional space of the manifold. For example, in the top row of <a data-type="xref" href="#manifold_decision_boundary_plot">Figure¬†7-6</a> the Swiss roll is split into two classes: in the 3D space (on the left) the decision boundary would be fairly complex, but in the 2D unrolled manifold space (on the right) the decision boundary is a straight line.</p>

<p>However, this implicit assumption does not always hold. For example, in the bottom row of <a data-type="xref" href="#manifold_decision_boundary_plot">Figure¬†7-6</a>, the decision boundary is located at <em>x</em><sub>1</sub> = 5. This decision boundary looks very simple in the original 3D space (a vertical plane), but it looks more complex in the unrolled manifold (a collection of four independent line segments).</p>

<p>In short, reducing the dimensionality of your training set before training a model will usually speed up training, but it may not always lead to a better or simpler solution; it all depends on the dataset. Dimensionality reduction is typically more effective when the dataset is small relative to the number of features, especially if it‚Äôs noisy, or many features are highly correlated to one another (i.e., redundant). And if you have domain knowledge about the process that generated the data, and you know it‚Äôs simple, then the manifold assumption certainly holds, and dimensionality reduction is likely to help.</p>

<p>Hopefully, you now have a good sense of what the curse of dimensionality is and how dimensionality reduction algorithms can fight it, especially when the manifold assumption holds. The rest of this chapter will go through some of the most popular algorithms for dimensionality reduction.<a data-type="indexterm" data-startref="xi_dimensionalityreductionapproachesto7307_1" id="id1872"/><a data-type="indexterm" data-startref="xi_manifoldlearningdimensionreduction7479_1" id="id1873"/><a data-type="indexterm" data-startref="xi_Swissrolldataset7479_1" id="id1874"/></p>

<figure><div id="manifold_decision_boundary_plot" class="figure">
<img src="assets/hmls_0706.png" alt="Diagrams illustrating how dimensionality reduction affects decision boundaries, showing a complex spiral structure on the left and its simplified lower-dimensional projections on the right." width="2084" height="1521"/>
<h6><span class="label">Figure 7-6. </span>The decision boundary may not always be simpler with lower dimensions</h6>
</div></figure>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="PCA"><div class="sect1" id="id119">
<h1>PCA</h1>

<p><em>Principal component analysis</em> (PCA)<a data-type="indexterm" data-primary="principal component analysis (PCA)" id="xi_principalcomponentanalysisPCA77937_1"/> is by far the most popular dimensionality reduction algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it, as shown back in <a data-type="xref" href="#dataset_3d_plot">Figure¬†7-2</a>.</p>








<section data-type="sect2" data-pdf-bookmark="Preserving the Variance"><div class="sect2" id="id398">
<h2>Preserving the Variance</h2>

<p>Before <a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="preserving variance" id="id1875"/><a data-type="indexterm" data-primary="variance" data-secondary="preserving" id="id1876"/>you can project the training set onto a lower-dimensional hyperplane, you first need to choose the right hyperplane. For example, a simple 2D dataset is represented on the left in <a data-type="xref" href="#pca_best_projection_plot">Figure¬†7-7</a>, along with three different axes (i.e., 1D hyperplanes). On the right is the result of the projection of the dataset onto each of these axes. As you can see, the projection onto the solid line preserves the maximum variance (top), while the projection onto the dotted line preserves very little variance (bottom), and the projection onto the dashed line preserves an intermediate amount of variance (middle).</p>

<figure><div id="pca_best_projection_plot" class="figure">
<img src="assets/hmls_0707.png" alt="Diagram of PCA showing a 2D dataset on the left projected onto three different 1D axes. The solid line preserves the most variance, the dashed line an intermediate amount, and the dotted line the least, as depicted by the spread of points on the right." width="2268" height="1071"/>
<h6><span class="label">Figure 7-7. </span>Selecting the subspace on which to project</h6>
</div></figure>

<p>It seems reasonable to select the axis that preserves the maximum amount of variance, as it will most likely lose less information than the other projections. Consider your shadow on the ground when the sun is directly overhead: it‚Äôs a small blob that doesn‚Äôt look anything like you. But your shadow on a wall at sunrise is much larger and it <em>does</em> look like you. Another way to justify choosing the axis that maximizes the variance is that it is also the axis that minimizes the mean squared distance between the original dataset and its projection onto that axis. This is the rather simple idea behind PCA, introduced way back <a href="https://homl.info/pca">in 1901</a>!‚Å†<sup><a data-type="noteref" id="id1877-marker" href="ch07.html#id1877">4</a></sup></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Principal Components"><div class="sect2" id="id120">
<h2>Principal Components</h2>

<p>PCA<a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="finding principal components" id="xi_principalcomponentanalysisPCAfindingprincipalcomponents7914_1"/><a data-type="indexterm" data-primary="principal components (PCs)" id="xi_principalcomponentsPCs7914_1"/> identifies the axis that accounts for the largest amount of variance in the training set. In <a data-type="xref" href="#pca_best_projection_plot">Figure¬†7-7</a>, it is the solid line. It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of the remaining variance. In this 2D example there is no choice: it is the dotted line. If it were a higher-dimensional dataset, PCA would also find a third axis, orthogonal to both previous axes, and a fourth, a fifth, and so on‚Äîas many axes as the number of dimensions in the dataset.</p>

<p>The <em>i</em><sup>th</sup> axis is called the <em>i</em><sup>th</sup> <em>principal component</em> (PC) of the data. In <a data-type="xref" href="#pca_best_projection_plot">Figure¬†7-7</a>, the first PC is the axis on which vector <strong>c</strong><sub><strong>1</strong></sub> lies, and the second PC is the axis on which vector <strong>c</strong><sub><strong>2</strong></sub> lies. In <a data-type="xref" href="#dataset_3d_plot">Figure¬†7-2</a>, the first two PCs are on the projection plane, and the third PC is the axis orthogonal to that plane. After the projection, back in <a data-type="xref" href="#dataset_2d_plot">Figure¬†7-3</a>, the first PC corresponds to the <em>z</em><sub>1</sub> axis, and the second PC corresponds to the <em>z</em><sub>2</sub> axis.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>For each principal component, PCA finds a zero-centered unit vector pointing along the direction of the PC. Unfortunately, its direction is not guaranteed: if you perturb the training set slightly and run PCA again, the unit vector may point in the opposite direction. In fact, a pair of unit vectors may even rotate or swap if the variances along these two axes are very close. So if you use PCA as a preprocessing step before a model, make sure you always retrain the model entirely every time you update the PCA transformer: if you don‚Äôt and if the PCA‚Äôs output doesn‚Äôt align with the previous version, the model will be very confused.</p>
</div>

<p>So how can you find the principal components of a training set? Luckily, there is a standard matrix factorization technique called <em>singular value decomposition</em> (SVD)<a data-type="indexterm" data-primary="singular value decomposition (SVD)" id="id1878"/><a data-type="indexterm" data-primary="SVD (singular value decomposition)" id="id1879"/> that can decompose the training set matrix <strong>X</strong> into the product of three matrices <strong>U</strong> <strong>Œ£</strong> <strong>V</strong><sup>‚ä∫</sup>, where <strong>V</strong> contains the unit vectors that define all the principal components that you are looking for, in the correct order, as shown in <a data-type="xref" href="#principal_components_matrix">Equation 7-1</a>.‚Å†<sup><a data-type="noteref" id="id1880-marker" href="ch07.html#id1880">5</a></sup></p>
<div id="principal_components_matrix" data-type="equation">
<h5><span class="label">Equation 7-1. </span>Principal components matrix</h5>
<math alttext="bold upper V equals Start 3 By 4 Matrix 1st Row 1st Column bar 2nd Column bar 3rd Column Blank 4th Column bar 2nd Row 1st Column bold c 1 2nd Column bold c 2 3rd Column midline-horizontal-ellipsis 4th Column bold c Subscript n Baseline 3rd Row 1st Column bar 2nd Column bar 3rd Column Blank 4th Column bar EndMatrix" display="block">
  <mrow>
    <mi>ùêï</mi>
    <mo>=</mo>
    <mo>(</mo>
    <mtable>
      <mtr>
        <mtd>
          <mo>‚à£</mo>
        </mtd>
        <mtd>
          <mo>‚à£</mo>
        </mtd>
        <mtd/>
        <mtd>
          <mo>‚à£</mo>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <msub><mi>ùêú</mi> <mn>1</mn> </msub>
        </mtd>
        <mtd>
          <msub><mi>ùêú</mi> <mn>2</mn> </msub>
        </mtd>
        <mtd>
          <mo>‚ãØ</mo>
        </mtd>
        <mtd>
          <msub><mi>ùêú</mi> <mi>ùëõ</mi> </msub>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mo>‚à£</mo>
        </mtd>
        <mtd>
          <mo>‚à£</mo>
        </mtd>
        <mtd/>
        <mtd>
          <mo>‚à£</mo>
        </mtd>
      </mtr>
    </mtable>
    <mo>)</mo>
  </mrow>
</math>
</div>

<p>The following Python code uses NumPy‚Äôs <code translate="no">svd()</code> function to obtain all the principal components of the 3D training set represented back in <a data-type="xref" href="#dataset_3d_plot">Figure¬†7-2</a>, then it extracts the two unit vectors that define the first two PCs:<a data-type="indexterm" data-startref="xi_principalcomponentanalysisPCAfindingprincipalcomponents7914_1" id="id1881"/><a data-type="indexterm" data-startref="xi_principalcomponentsPCs7914_1" id="id1882"/></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="n">X</code> <code class="o">=</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># create a small 3D dataset</code>
<code class="n">X_centered</code> <code class="o">=</code> <code class="n">X</code> <code class="o">-</code> <code class="n">X</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">U</code><code class="p">,</code> <code class="n">s</code><code class="p">,</code> <code class="n">Vt</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linalg</code><code class="o">.</code><code class="n">svd</code><code class="p">(</code><code class="n">X_centered</code><code class="p">)</code>
<code class="n">c1</code> <code class="o">=</code> <code class="n">Vt</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
<code class="n">c2</code> <code class="o">=</code> <code class="n">Vt</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>PCA assumes that the dataset is centered around the origin. As you will see, Scikit-Learn‚Äôs PCA classes take care of centering the data for you. If you implement PCA yourself (as in the preceding example), or if you use other libraries, don‚Äôt forget to center the data first.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Projecting Down to d Dimensions"><div class="sect2" id="id399">
<h2>Projecting Down to d Dimensions</h2>

<p>Once<a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="projecting down to d dimensions" id="id1883"/> you have identified all the principal components, you can reduce the dimensionality of the dataset down to <em>d</em> dimensions by projecting it onto the hyperplane defined by the first <em>d</em> principal components (we will discuss how to choose the number of dimensions <em>d</em> shortly). Selecting this hyperplane ensures that the projection will preserve as much variance as possible. For example, in <a data-type="xref" href="#dataset_3d_plot">Figure¬†7-2</a> the 3D dataset is projected down to the 2D plane defined by the first two principal <span class="keep-together">components</span>, preserving a large part of the dataset‚Äôs variance. As a result, the 2D projection looks very much like the original 3D dataset.</p>

<p>To project the training set onto the hyperplane and obtain a reduced dataset <strong>X</strong><sub><em>d</em>-proj</sub> of dimensionality <em>d</em>, compute the matrix multiplication of the training set matrix <strong>X</strong> by the matrix <strong>W</strong><sub><em>d</em></sub>, defined as the matrix containing the first <em>d</em> columns of <strong>V</strong>, as shown in <a data-type="xref" href="#pca_projection">Equation 7-2</a>.</p>
<div class="fifty-percent" id="pca_projection" data-type="equation"><h5><span class="label">Equation 7-2. </span>Projecting the training set down to <em>d</em> dimensions</h5>
<math alttext="bold upper X Subscript d hyphen proj Baseline equals bold upper X bold upper W Subscript d">
  <mrow>
    <msub><mi>ùêó</mi> <mrow><mi>d</mi><mtext>-proj</mtext></mrow> </msub>
    <mo>=</mo>
    <mi>ùêó</mi>
    <msub><mi>ùêñ</mi> <mi>d</mi> </msub>
  </mrow>
</math>
</div>

<p>The following Python code projects the training set onto the plane defined by the first two principal components:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">W2</code> <code class="o">=</code> <code class="n">Vt</code><code class="p">[:</code><code class="mi">2</code><code class="p">]</code><code class="o">.</code><code class="n">T</code>
<code class="n">X2D</code> <code class="o">=</code> <code class="n">X_centered</code> <code class="o">@</code> <code class="n">W2</code></pre>

<p>There you have it! You now know how to reduce the dimensionality of any dataset by projecting it down to any number of dimensions, while preserving as much variance as possible.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Using Scikit-Learn"><div class="sect2" id="id400">
<h2>Using Scikit-Learn</h2>

<p>Scikit-Learn‚Äôs <code translate="no">PCA</code><a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="Scikit-Learn for" id="id1884"/><a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="PCA implementation" id="id1885"/><a data-type="indexterm" data-primary="sklearn" data-secondary="decomposition.PCA" id="id1886"/> class uses SVD to implement PCA, just like we did earlier in this chapter. The following code applies PCA to reduce the dimensionality of the dataset down to two dimensions (note that it automatically takes care of centering the data):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">PCA</code>

<code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">X2D</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

<p>After fitting the <code translate="no">PCA</code> transformer to the dataset, its <code translate="no">components_</code> attribute holds the transpose of <strong>W</strong><sub><em>d</em></sub>: it contains one row for each of the first <em>d</em> principal components.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Explained Variance Ratio"><div class="sect2" id="id121">
<h2>Explained Variance Ratio</h2>

<p>Another<a data-type="indexterm" data-primary="explained variance ratio" id="id1887"/><a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="explained variance ratio" id="id1888"/><a data-type="indexterm" data-primary="variance" data-secondary="explained" id="xi_varianceexplained71588_1"/> useful piece of information is the <em>explained variance ratio</em> of each principal component, available via the <code translate="no">explained_variance_ratio_</code> variable. The ratio indicates the proportion of the dataset‚Äôs variance that lies along each principal component. For example, let‚Äôs look at the explained variance ratios of the first two components of the 3D dataset represented in <a data-type="xref" href="#dataset_3d_plot">Figure¬†7-2</a>:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_ratio_</code><code class="w"/>
<code class="go">array([0.82279334, 0.10821224])</code></pre>

<p>This output tells us that about 82% of the dataset‚Äôs variance lies along the first PC, and about 11% lies along the second PC. This leaves about 7% for the third PC, so it is reasonable to assume that the third PC probably carries little information.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Choosing the Right Number of Dimensions"><div class="sect2" id="id122">
<h2>Choosing the Right Number of Dimensions</h2>

<p>Instead<a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="choosing the right number of dimensions" id="xi_dimensionalityreductionchoosingtherightnumberofdimensions71698_1"/><a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="choosing number of dimensions" id="xi_principalcomponentanalysisPCAchoosingnumberofdimensions71698_1"/> of arbitrarily choosing the number of dimensions to reduce down to, it is simpler to choose the number of dimensions that add up to a sufficiently large portion of the variance‚Äîsay, 95%. (An exception to this rule, of course, is if you are reducing dimensionality for data visualization<a data-type="indexterm" data-primary="visualization of data" data-secondary="dimensionality reduction" id="id1889"/>, in which case you will want to reduce the dimensionality down to 2 or 3.)</p>

<p>The following code loads and splits the MNIST dataset (introduced in <a data-type="xref" href="ch03.html#classification_chapter">Chapter¬†3</a>) and performs PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve 95% of the training set‚Äôs variance:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">fetch_openml</code>

<code class="n">mnist</code> <code class="o">=</code> <code class="n">fetch_openml</code><code class="p">(</code><code class="s1">'mnist_784'</code><code class="p">,</code> <code class="n">as_frame</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code> <code class="o">=</code> <code class="n">mnist</code><code class="o">.</code><code class="n">data</code><code class="p">[:</code><code class="mi">60_000</code><code class="p">],</code> <code class="n">mnist</code><code class="o">.</code><code class="n">target</code><code class="p">[:</code><code class="mi">60_000</code><code class="p">]</code>
<code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">mnist</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="mi">60_000</code><code class="p">:],</code> <code class="n">mnist</code><code class="o">.</code><code class="n">target</code><code class="p">[</code><code class="mi">60_000</code><code class="p">:]</code>

<code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">()</code>
<code class="n">pca</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>
<code class="n">cumsum</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">cumsum</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_ratio_</code><code class="p">)</code>
<code class="n">d</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">cumsum</code> <code class="o">&gt;=</code> <code class="mf">0.95</code><code class="p">)</code> <code class="o">+</code> <code class="mi">1</code>  <code class="c1"># d equals 154</code></pre>

<p>You could then set <code translate="no">n_components=d</code> and run PCA again, but there‚Äôs a better option. Instead of specifying the number of principal components you want to preserve, you can set <code translate="no">n_components</code> to be a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mf">0.95</code><code class="p">)</code>
<code class="n">X_reduced</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code></pre>

<p>The actual number of components is determined during training, and it is stored in the <code translate="no">n_components_</code> attribute:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">pca</code><code class="o">.</code><code class="n">n_components_</code><code class="w"/>
<code class="go">np.int64(154)</code></pre>

<p>Yet another option is to plot<a data-type="indexterm" data-primary="explained variance, plotting" id="xi_explainedvarianceplotting720330_1"/> the explained variance as a function of the number of dimensions (simply plot <code translate="no">cumsum</code>; see <a data-type="xref" href="#explained_variance_plot">Figure¬†7-8</a>). There will usually be an elbow in the curve, where the explained variance stops growing fast. In this case, you can see that reducing the dimensionality down to about 100 dimensions wouldn‚Äôt lose too much explained variance.</p>

<figure class="width-65"><div id="explained_variance_plot" class="figure">
<img src="assets/hmls_0708.png" alt="Graph showing explained variance as a function of dimensions, with an elbow indicating diminishing returns beyond 100 dimensions." width="1671" height="1060"/>
<h6><span class="label">Figure 7-8. </span>Explained variance as a function of the number of dimensions</h6>
</div></figure>

<p>Alternatively, if you are using dimensionality reduction as a preprocessing step for a supervised learning task (e.g., classification), then you can tune the number of dimensions as you would any other hyperparameter<a data-type="indexterm" data-primary="hyperparameters" data-secondary="dimensionality reduction" id="id1890"/> (see <a data-type="xref" href="ch02.html#project_chapter">Chapter¬†2</a>). For example, the following code example creates a two-step pipeline, first reducing dimensionality using PCA, then classifying using a random forest. Next, it uses <code translate="no">RandomizedSearchCV</code><a data-type="indexterm" data-primary="sklearn" data-secondary="model_selection.RandomizedSearchCV" id="id1891"/> to find a good combination of hyperparameters for both PCA and the random forest classifier. This example does a quick search, tuning only 2 hyperparameters, training on just 1,000 instances, and running for just 10 iterations, but feel free to do a more thorough search if you have the time:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">RandomizedSearchCV</code>
<code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">make_pipeline</code>

<code class="n">clf</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">PCA</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">),</code>
                    <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">))</code>
<code class="n">param_distrib</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"pca__n_components"</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">80</code><code class="p">),</code>
    <code class="s2">"randomforestclassifier__n_estimators"</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">50</code><code class="p">,</code> <code class="mi">500</code><code class="p">)</code>
<code class="p">}</code>
<code class="n">rnd_search</code> <code class="o">=</code> <code class="n">RandomizedSearchCV</code><code class="p">(</code><code class="n">clf</code><code class="p">,</code> <code class="n">param_distrib</code><code class="p">,</code> <code class="n">n_iter</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
                                <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">rnd_search</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:</code><code class="mi">1000</code><code class="p">],</code> <code class="n">y_train</code><code class="p">[:</code><code class="mi">1000</code><code class="p">])</code></pre>

<p>Let‚Äôs look at the best hyperparameters found:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="nb">print</code><code class="p">(</code><code class="n">rnd_search</code><code class="o">.</code><code class="n">best_params_</code><code class="p">)</code><code class="w"/>
<code class="go">{'randomforestclassifier__n_estimators': np.int64(475),</code>
<code class="go"> 'pca__n_components': np.int64(57)}</code></pre>

<p>It‚Äôs interesting to note how low the optimal number of components is: we reduced a 784-dimensional dataset to just 57 dimensions! This is tied to the fact that we used a random forest, which is a pretty powerful model. If we used a linear model instead, such as an <code translate="no">SGDClassifier</code>, the search would find that we need to preserve more dimensions (about 75).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You may also care about the model‚Äôs size and speed, not just it‚Äôs performance. The fewer dimensions, the smaller the model, and the faster training and inference will be. But if you shrink the data too much, then you will lose too much signal and your model will underfit. You need to choose the right balance of speed, size, and performance for your particular use case.<a data-type="indexterm" data-startref="xi_dimensionalityreductionchoosingtherightnumberofdimensions71698_1" id="id1892"/><a data-type="indexterm" data-startref="xi_explainedvarianceplotting720330_1" id="id1893"/><a data-type="indexterm" data-startref="xi_principalcomponentanalysisPCAchoosingnumberofdimensions71698_1" id="id1894"/><a data-type="indexterm" data-startref="xi_varianceexplained71588_1" id="id1895"/></p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="PCA for Compression"><div class="sect2" id="id123">
<h2>PCA for Compression</h2>

<p>After<a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="for compression" data-secondary-sortas="compression" id="xi_principalcomponentanalysisPCAforcompression72426_1"/> dimensionality reduction, the training set takes up much less space. For example, after applying PCA to the MNIST dataset while preserving 95% of its variance, we are left with 154 features, instead of the original 784 features. So the dataset is now less than 20% of its original size, and we only lost 5% of its variance! This is a reasonable compression ratio, and it‚Äôs easy to see how such a size reduction would speed up a classification algorithm tremendously.</p>

<p>It is also possible to decompress the reduced dataset back to 784 dimensions by applying the inverse transformation of the PCA projection. This won‚Äôt give you back the original data, since the projection lost a bit of information (within the 5% variance that was dropped), but it will likely be close to the original data. The mean squared distance between the original data and the reconstructed data (compressed and then decompressed) is called the <em>reconstruction error</em>.<a data-type="indexterm" data-primary="reconstruction error" id="id1896"/></p>

<p>The <code translate="no">inverse_transform()</code><a data-type="indexterm" data-primary="inverse_transform()" id="id1897"/> method lets us decompress the reduced MNIST dataset back to 784 dimensions:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">X_recovered</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">inverse_transform</code><code class="p">(</code><code class="n">X_reduced</code><code class="p">)</code></pre>

<p><a data-type="xref" href="#mnist_compression_plot">Figure¬†7-9</a> shows a few digits from the original training set (on the left), and the corresponding digits after compression and decompression. You can see that there is a slight image quality loss, but the digits are still mostly intact.</p>

<figure class="width-60"><div id="mnist_compression_plot" class="figure">
<img src="assets/hmls_0709.png" alt="Comparison of original MNIST digits with their compressed and decompressed versions, showing slight quality loss while preserving 95% of the variance." width="1201" height="646"/>
<h6><span class="label">Figure 7-9. </span>MNIST compression that preserves 95% of the variance</h6>
</div></figure>

<p>The equation for the inverse transformation is shown in <a data-type="xref" href="#inverse_pca">Equation 7-3</a>.<a data-type="indexterm" data-startref="xi_principalcomponentanalysisPCAforcompression72426_1" id="id1898"/></p>
<div class="fifty-percent" id="inverse_pca" data-type="equation"><h5><span class="label">Equation 7-3. </span>PCA inverse transformation, back to the original number of dimensions</h5>
<math alttext="bold upper X Subscript recovered Baseline equals bold upper X Subscript d hyphen proj Baseline bold upper W Subscript d Baseline Superscript upper T">
  <mrow>
    <msub><mi>ùêó</mi> <mtext>recovered</mtext> </msub>
    <mo>=</mo>
    <msub><mi>ùêó</mi> <mrow><mi>d</mi><mtext>-proj</mtext></mrow> </msub>
    <msup><mrow><msub><mi>ùêñ</mi> <mi>d</mi> </msub></mrow> <mtext>T</mtext> </msup>
  </mrow>
</math>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Randomized PCA"><div class="sect2" id="id124">
<h2>Randomized PCA</h2>

<p>If<a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="randomized PCA" id="id1899"/><a data-type="indexterm" data-primary="randomized PCA" id="id1900"/> you set the <code translate="no">svd_solver</code><a data-type="indexterm" data-primary="singular value decomposition (SVD)" id="id1901"/><a data-type="indexterm" data-primary="SVD (singular value decomposition)" id="id1902"/> hyperparameter to <code translate="no">"randomized"</code>, Scikit-Learn uses a stochastic algorithm called <em>randomized PCA</em> that quickly finds an approximation of the first <em>d</em> principal components. Its computational complexity is <em>O</em>(<em>m</em> √ó <em>d</em><sup>2</sup>) + <em>O</em>(<em>d</em><sup>3</sup>), instead of <em>O</em>(<em>m</em> √ó <em>n</em><sup>2</sup>) + <em>O</em>(<em>n</em><sup>3</sup>) for the full SVD approach, so it is dramatically faster than full SVD when <em>d</em> is much smaller than <em>n</em>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">rnd_pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">154</code><code class="p">,</code> <code class="n">svd_solver</code><code class="o">=</code><code class="s2">"randomized"</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">X_reduced</code> <code class="o">=</code> <code class="n">rnd_pca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>By default, <code translate="no">svd_solver</code> is set to <code translate="no">"auto"</code>: if the input data has few features (<em>n</em> &lt; 1,000) and at least 10 times more samples (<em>m</em> &gt; 10<em>n</em> ), then the <code translate="no">"covariance_eigh"</code> solver is used, which is very fast in these conditions. Otherwise, if max(<em>m</em>, <em>n</em>) &gt; 500 and <code translate="no">n_components</code> is an integer smaller than 80% of min(<em>m</em>, <em>n</em>), it uses the <code translate="no">"randomized"</code> solver. In other cases, it uses the full SVD approach. If you want to force Scikit-Learn to use full SVD, trading compute time for a slightly more precise result, you can set <code translate="no">svd_solver="full"</code>.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Incremental PCA"><div class="sect2" id="id125">
<h2>Incremental PCA</h2>

<p>One<a data-type="indexterm" data-primary="incremental PCA (IPCA)" id="id1903"/><a data-type="indexterm" data-primary="IPCA (incremental PCA)" id="id1904"/><a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="incremental PCA" id="xi_principalcomponentanalysisPCAincrementalPCA72804_1"/> problem with the preceding implementations of PCA is that they require the whole training set to fit in memory in order for the algorithm to run. Fortunately, <em>incremental PCA</em> (IPCA) algorithms have been developed that allow you to split the training set into mini-batches and feed these in one mini-batch at a time. This is useful for large training sets and for applying PCA online (i.e., on the fly, as new instances arrive).</p>

<p>The following code splits the MNIST training set into 100 mini-batches (using NumPy‚Äôs <code translate="no">array_split()</code> function) and feeds them to Scikit-Learn‚Äôs <code translate="no">IncrementalPCA</code> class‚Å†<sup><a data-type="noteref" id="id1905-marker" href="ch07.html#id1905">6</a></sup> to reduce the dimensionality of the MNIST dataset down to 154 dimensions, just like before. Note that you must call the <code translate="no">partial_fit()</code> method with each mini-batch, rather than the <code translate="no">fit()</code> method with the whole training set:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">IncrementalPCA</code>

<code class="n">n_batches</code> <code class="o">=</code> <code class="mi">100</code>
<code class="n">inc_pca</code> <code class="o">=</code> <code class="n">IncrementalPCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">154</code><code class="p">)</code>
<code class="k">for</code> <code class="n">X_batch</code> <code class="ow">in</code> <code class="n">np</code><code class="o">.</code><code class="n">array_split</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">n_batches</code><code class="p">):</code>
    <code class="n">inc_pca</code><code class="o">.</code><code class="n">partial_fit</code><code class="p">(</code><code class="n">X_batch</code><code class="p">)</code>

<code class="n">X_reduced</code> <code class="o">=</code> <code class="n">inc_pca</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code></pre>

<p>Alternatively, you can use NumPy‚Äôs <code translate="no">memmap</code> class, which allows you to manipulate a large array stored in a binary file on disk as if it were entirely in memory; the class loads only the data it needs in memory, when it needs it. To demonstrate this, let‚Äôs first create a memory-mapped (memmap) file and copy the MNIST training set to it, then call <code translate="no">flush()</code> to ensure that any data still in the cache gets saved to disk. In real life, <code translate="no">X_train</code> would typically not fit in memory, so you would load it chunk by chunk and save each chunk to the right part of the memmap array:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">filename</code> <code class="o">=</code> <code class="s2">"my_mnist.mmap"</code>
<code class="n">X_mmap</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">memmap</code><code class="p">(</code><code class="n">filename</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="s1">'float32'</code><code class="p">,</code> <code class="n">mode</code><code class="o">=</code><code class="s1">'write'</code><code class="p">,</code> <code class="n">shape</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="n">X_mmap</code><code class="p">[:]</code> <code class="o">=</code> <code class="n">X_train</code>  <code class="c1"># could be a loop instead, saving the data chunk by chunk</code>
<code class="n">X_mmap</code><code class="o">.</code><code class="n">flush</code><code class="p">()</code></pre>

<p>Next, we can load the memmap file and use it like a regular NumPy array. Let‚Äôs use the <code translate="no">IncrementalPCA</code> class to reduce its dimensionality. Since this algorithm uses only a small part of the array at any given time, memory usage remains under control. This makes it possible to call the usual <code translate="no">fit()</code> method instead of <code translate="no">partial_fit()</code>, which is quite convenient:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">X_mmap</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">memmap</code><code class="p">(</code><code class="n">filename</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="s2">"float32"</code><code class="p">,</code> <code class="n">mode</code><code class="o">=</code><code class="s2">"readonly"</code><code class="p">)</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">784</code><code class="p">)</code>
<code class="n">batch_size</code> <code class="o">=</code> <code class="n">X_mmap</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">//</code> <code class="n">n_batches</code>
<code class="n">inc_pca</code> <code class="o">=</code> <code class="n">IncrementalPCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">154</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">)</code>
<code class="n">inc_pca</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_mmap</code><code class="p">)</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Only the raw binary data is saved to disk, so you need to specify the data type and shape of the array when you load it. If you omit the shape, <code translate="no">np.memmap()</code> returns a 1D array.</p>
</div>

<p>For very high-dimensional datasets, PCA can be too slow. As you saw earlier, even if you use randomized PCA, its computational complexity is still <em>O</em>(<em>m</em> √ó <em>d</em><sup>2</sup>) + <em>O</em>(<em>d</em><sup>3</sup>), so the target number of dimensions <em>d</em> must not be too large. If you are dealing with a dataset with tens of thousands of features or more (e.g., images), then training may become much too slow: in this case, you should consider using random projection instead.<a data-type="indexterm" data-startref="xi_principalcomponentanalysisPCA77937_1" id="id1906"/><a data-type="indexterm" data-startref="xi_principalcomponentanalysisPCAincrementalPCA72804_1" id="id1907"/></p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Random Projection"><div class="sect1" id="id126">
<h1>Random Projection</h1>

<p>As<a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="random projection algorithm" id="xi_dimensionalityreductionrandomprojectionalgorithm73213_1"/><a data-type="indexterm" data-primary="random projection algorithm" id="xi_randomprojectionalgorithm73213_1"/> its name suggests, the random projection algorithm projects the data to a lower-dimensional space using a random linear projection. This may sound crazy, but it turns out that such a random projection is actually very likely to preserve distances fairly well, as was demonstrated mathematically by William B. Johnson and Joram Lindenstrauss in a famous lemma. So, two similar instances will remain similar after the projection, and two very different instances will remain very different.</p>

<p>Obviously, the more dimensions you drop, the more information is lost, and the more distances get distorted. So how can you choose the optimal number of dimensions? Well, Johnson and Lindenstrauss came up with an equation that determines the minimum number of dimensions to preserve in order to ensure‚Äîwith high probability‚Äîthat distances won‚Äôt change by more than a given tolerance. For example, if you have a dataset containing <em>m</em> = 5,000 instances with <em>n</em> = 20,000 features each, and you don‚Äôt want the squared distance between any two instances to change by more than <em>Œµ</em> = 10%,<sup><a data-type="noteref" id="id1908-marker" href="ch07.html#id1908">7</a></sup> then you should project the data down to <em>d</em> dimensions, with <em>d</em> ‚â• 4 log(<em>m</em>) / (¬Ω <em>Œµ</em>¬≤ - ‚Öì <em>Œµ</em>¬≥), which is 7,300 dimensions. That‚Äôs quite a significant dimensionality reduction! Notice that the equation does not use <em>n</em>, it only relies on <em>m</em> and <em>Œµ</em>. This equation is implemented by the <code translate="no">johnson_lindenstrauss_min_dim()</code> function:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.random_projection</code> <code class="kn">import</code> <code class="n">johnson_lindenstrauss_min_dim</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">m</code><code class="p">,</code> <code class="n">Œµ</code> <code class="o">=</code> <code class="mi">5_000</code><code class="p">,</code> <code class="mf">0.1</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">d</code> <code class="o">=</code> <code class="n">johnson_lindenstrauss_min_dim</code><code class="p">(</code><code class="n">m</code><code class="p">,</code> <code class="n">eps</code><code class="o">=</code><code class="n">Œµ</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">d</code><code class="w"/>
<code class="go">7300</code></pre>

<p>Now we can just generate a random matrix <strong>P</strong> of shape [<em>d</em>, <em>n</em>], where each item is sampled randomly from a Gaussian distribution with mean 0 and variance 1 / <em>d</em>, and use it to project a dataset from <em>n</em> dimensions down to <em>d</em>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">n</code> <code class="o">=</code> <code class="mi">20_000</code>
<code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">default_rng</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">P</code> <code class="o">=</code> <code class="n">rng</code><code class="o">.</code><code class="n">standard_normal</code><code class="p">((</code><code class="n">d</code><code class="p">,</code> <code class="n">n</code><code class="p">))</code> <code class="o">/</code> <code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">d</code><code class="p">)</code>  <code class="c1"># std dev = sqrt(variance)</code>

<code class="n">X</code> <code class="o">=</code> <code class="n">rng</code><code class="o">.</code><code class="n">standard_normal</code><code class="p">((</code><code class="n">m</code><code class="p">,</code> <code class="n">n</code><code class="p">))</code>  <code class="c1"># generate a fake dataset</code>
<code class="n">X_reduced</code> <code class="o">=</code> <code class="n">X</code> <code class="o">@</code> <code class="n">P</code><code class="o">.</code><code class="n">T</code></pre>

<p>That‚Äôs all there is to it! It‚Äôs simple and efficient, and training is almost instantaneous: the only thing the algorithm needs to create the random matrix is the dataset‚Äôs shape. The data itself is not used at all. This makes random projection particularly well suited for very high-dimensional data such as text or genomics with millions of features, or very sparse data, for which even randomized PCA<a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="randomized PCA" id="id1909"/><a data-type="indexterm" data-primary="randomized PCA" id="id1910"/> may take too long to train and require too much memory. At inference time, random projection is just as fast as PCA (i.e., one matrix multiplication). That said, random projection is not a silver bullet: it loses a bit more signal than PCA, so there‚Äôs a trade-off between training speed and performance.</p>

<p>Scikit-Learn offers a <code translate="no">GaussianRandomProjection</code> class<a data-type="indexterm" data-primary="GaussianRandomProjection" id="id1911"/><a data-type="indexterm" data-primary="sklearn" data-secondary="random_projection.GaussianRandomProjection" id="id1912"/> to do exactly what we just did: when you call its <code translate="no">fit()</code> method, it uses <code translate="no">johnson_lindenstrauss_min_dim()</code> to determine the output dimensionality, then it generates a random matrix, which it stores in the <code translate="no">components_</code> attribute. Then when you call <code translate="no">transform()</code>, it uses this matrix to perform the projection. When creating the transformer, you can set <code translate="no">eps</code> to tweak <em>Œµ</em> (it defaults to 0.1), and <code translate="no">n_components</code> to force a specific target dimensionality <em>d</em> (you will probably want to fine-tune these hyperparameters using cross-validation). The following code example gives the same result as the preceding code (you can also verify that <code translate="no">gaussian_rnd_proj.components_</code> is equal to <code translate="no">P</code>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.random_projection</code> <code class="kn">import</code> <code class="n">GaussianRandomProjection</code>

<code class="n">gaussian_rnd_proj</code> <code class="o">=</code> <code class="n">GaussianRandomProjection</code><code class="p">(</code><code class="n">eps</code><code class="o">=</code><code class="n">Œµ</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">X_reduced</code> <code class="o">=</code> <code class="n">gaussian_rnd_proj</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>  <code class="c1"># same result as above</code></pre>

<p>Scikit-Learn also provides a second random projection transformer, known as 
<span class="keep-together"><code translate="no">SparseRandomProjection</code><a data-type="indexterm" data-primary="sklearn" data-secondary="random_projection.SparseRandomProjection" id="id1913"/><a data-type="indexterm" data-primary="SparseRandomProjection" id="id1914"/>.</span> It determines the target dimensionality in the same way, generates a random matrix of the same shape, and performs the projection identically. The main difference is that the random matrix is sparse. This means it uses much less memory: about 25 MB instead of almost 1.2 GB in the preceding example! And it‚Äôs also much faster, both to generate the random matrix and to reduce dimensionality: about 50% faster in this case. Moreover, if the input is sparse, the transformation keeps it sparse (unless you set <code translate="no">dense_output=True</code>). Lastly, it enjoys the same distance-preserving property as the previous approach, and the quality of the dimensionality reduction is comparable (only very slightly less accurate). In short, it‚Äôs usually preferable to use this transformer instead of the first one, especially for large or sparse datasets.</p>

<p>The ratio <em>r</em> of nonzero items in the sparse random matrix is called its <em>density</em>.<a data-type="indexterm" data-primary="density, of sparse random matrix" id="id1915"/> By default, it is equal to <math alttext="StartFraction 1 Over StartRoot n EndRoot EndFraction">
  <mfrac><mn>1</mn> <msqrt><mi>n</mi></msqrt></mfrac>
</math>. With 20,000 features, this means that only 1 in ~141 cells in the random matrix is nonzero: that‚Äôs quite sparse! You can set the <code translate="no">density</code> hyperparameter to another value if you prefer. Each cell in the sparse random matrix has a probability <em>r</em> of being nonzero, and each nonzero value is either ‚Äì<em>v</em> or +<em>v</em> (both equally likely), where <em>v</em> = <math alttext="StartFraction 1 Over StartRoot d r EndRoot EndFraction">
  <mfrac><mn>1</mn> <msqrt><mrow><mi>d</mi><mi>r</mi></mrow></msqrt></mfrac>
</math>.</p>

<p>If you want to perform the inverse transform, you first need to compute the pseudoinverse of the components matrix using SciPy‚Äôs <code translate="no">pinv()</code> function, then multiply the reduced data by the transpose of the pseudoinverse:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">components_pinv</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linalg</code><code class="o">.</code><code class="n">pinv</code><code class="p">(</code><code class="n">gaussian_rnd_proj</code><code class="o">.</code><code class="n">components_</code><code class="p">)</code>
<code class="n">X_recovered</code> <code class="o">=</code> <code class="n">X_reduced</code> <code class="o">@</code> <code class="n">components_pinv</code><code class="o">.</code><code class="n">T</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Computing the pseudoinverse may take a very long time if the components matrix is large, as the computational complexity of <code translate="no">pinv()</code> is <em>O</em>(<em>dn</em>¬≤) if <em>d</em> &lt; <em>n</em>, or <em>O</em>(<em>nd</em>¬≤) otherwise.</p>
</div>

<p>In summary, random projection is a simple, fast, memory-efficient, and surprisingly powerful dimensionality reduction algorithm that you should keep in mind, especially when you deal with high-dimensional datasets.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Random projection is not always used to reduce the dimensionality of large datasets. For example, a <a href="https://homl.info/flies">2017 paper</a>‚Å†<sup><a data-type="noteref" id="id1916-marker" href="ch07.html#id1916">8</a></sup> by Sanjoy Dasgupta et al. showed that the brain of a fruit fly implements an analog of random projection to map dense low-dimensional olfactory inputs to sparse high-dimensional binary outputs: for each odor, only a small fraction of the output neurons get activated, but similar odors activate many of the same neurons. This is similar to a well-known algorithm called <em>locality sensitive hashing</em> (LSH)<a data-type="indexterm" data-primary="locality sensitive hashing (LSH)" id="id1917"/><a data-type="indexterm" data-primary="LSH (locality sensitive hashing)" id="id1918"/>, which is typically used in search engines to group similar documents (see <a data-type="xref" href="ch17.html#speedup_chapter">Chapter¬†17</a>).<a data-type="indexterm" data-startref="xi_dimensionalityreductionrandomprojectionalgorithm73213_1" id="id1919"/><a data-type="indexterm" data-startref="xi_randomprojectionalgorithm73213_1" id="id1920"/></p>
</div>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="LLE"><div class="sect1" id="id127">
<h1>LLE</h1>

<p><a href="https://homl.info/lle"><em>Locally linear embedding</em> (LLE)</a>‚Å†<sup><a data-type="noteref" id="id1921-marker" href="ch07.html#id1921">9</a></sup> <a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="LLE" id="xi_dimensionalityreductionLLE7379216_1"/><a data-type="indexterm" data-primary="LLE (locally linear embedding)" id="xi_LLElocallylinearembedding7379216_1"/><a data-type="indexterm" data-primary="locally linear embedding (LLE)" id="xi_locallylinearembeddingLLE7379216_1"/><a data-type="indexterm" data-primary="embeddings" data-secondary="LLE" id="xi_embeddingsLLE7379216_1"/>is a <em>nonlinear dimensionality reduction</em> (NLDR)<a data-type="indexterm" data-primary="nonlinear dimensionality reduction (NLDR)" id="xi_nonlineardimensionalityreductionNLDR7379264_1"/><a data-type="indexterm" data-primary="NLDR (nonlinear dimensionality reduction)" id="xi_NLDRnonlineardimensionalityreduction7379264_1"/> technique. It is a manifold learning technique that does not rely on projections, unlike PCA and random projection. In a nutshell, LLE first determines how each training instance linearly relates to its nearest neighbors, then it looks for a low-dimensional representation of the training set where these local relationships are best preserved (more details shortly). This approach makes it particularly good at unrolling twisted manifolds, especially when there is not too much noise. However, it does not scale well so it is mostly for small or medium sized datasets.</p>

<p>The following code makes a Swiss roll, then uses Scikit-Learn‚Äôs <code translate="no">LocallyLinearEmbedding</code><a data-type="indexterm" data-primary="sklearn" data-secondary="manifold.LocallyLinearEmbedding" id="id1922"/> class to unroll it:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_swiss_roll</code>
<code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="kn">import</code> <code class="n">LocallyLinearEmbedding</code>

<code class="n">X_swiss</code><code class="p">,</code> <code class="n">t</code> <code class="o">=</code> <code class="n">make_swiss_roll</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">noise</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">lle</code> <code class="o">=</code> <code class="n">LocallyLinearEmbedding</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">n_neighbors</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">X_unrolled</code> <code class="o">=</code> <code class="n">lle</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_swiss</code><code class="p">)</code></pre>

<p>The variable <code translate="no">t</code> is a 1D NumPy array containing the position of each instance along the rolled axis of the Swiss roll. We don‚Äôt use it in this example, but it can be used as a target for a nonlinear regression task. The resulting 2D dataset is shown in <a data-type="xref" href="#lle_unrolling_plot">Figure¬†7-10</a>.</p>

<figure class="width-65"><div id="lle_unrolling_plot" class="figure">
<img src="assets/hmls_0710.png" alt="Visualization of an unrolled Swiss roll dataset using Locally Linear Embedding (LLE), showing the transformation of data points into a stretched band with preserved local distances." width="1795" height="1311"/>
<h6><span class="label">Figure 7-10. </span>Unrolled Swiss roll using LLE</h6>
</div></figure>

<p>As you can see, the Swiss roll is completely unrolled, and the distances between instances are locally well preserved. However, distances are not preserved on a larger scale: the unrolled Swiss roll should be a rectangle, not this kind of stretched and twisted band. Nevertheless, LLE did a pretty good job of modeling the manifold.</p>

<p>Here‚Äôs how LLE works: for each training instance <strong>x</strong><sup>(<em>i</em>)</sup>, the algorithm identifies its <em>k</em>-nearest neighbors (in the preceding code <em>k</em> = 10), then tries to reconstruct <strong>x</strong><sup>(<em>i</em>)</sup> as a linear function of these neighbors. More specifically, it tries to find the weights <em>w</em><sub><em>i,j</em></sub> such that the squared distance between <strong>x</strong><sup>(<em>i</em>)</sup> and <math alttext="sigma-summation Underscript j equals 1 Overscript m Endscripts w Subscript i comma j Baseline bold x Superscript left-parenthesis j right-parenthesis">
  <mrow>
    <msubsup><mo>‚àë</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </msubsup>
    <msub><mi>w</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi></mrow> </msub>
    <msup><mrow><mi>ùê±</mi></mrow> <mfenced open="(" close=")"><mi>j</mi></mfenced> </msup>
  </mrow>
</math> is as small as possible, assuming <em>w</em><sub><em>i,j</em></sub> = 0 if <strong>x</strong><sup>(<em>j</em>)</sup> is not one of the <em>k</em>-nearest neighbors of <strong>x</strong><sup>(<em>i</em>)</sup>. Thus the first step of LLE is the constrained optimization problem described in <a data-type="xref" href="#lle_first_step">Equation 7-4</a>, where <strong>W</strong> is the weight matrix containing all the weights <em>w</em><sub><em>i,j</em></sub>. The second constraint simply normalizes the weights for each training instance <strong>x</strong><sup>(<em>i</em>)</sup>.</p>
<div id="lle_first_step" data-type="equation"><h5><span class="label">Equation 7-4. </span>LLE step 1: linearly modeling local relationships</h5>
<math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mover accent="true"><mi mathvariant="bold">W</mi> <mo>^</mo></mover>
          <mo>=</mo>
          <munder><mo form="prefix">argmin</mo> <mi mathvariant="bold">W</mi></munder>
          <mstyle scriptlevel="0" displaystyle="true">
            <munderover><mo>‚àë</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </munderover>
          </mstyle>
          <msup><mfenced separators="" open="(" close=")"><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>-</mo><munderover><mo>‚àë</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </munderover><msub><mi>w</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi></mrow> </msub><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msup></mfenced> <mn>2</mn> </msup>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mtext>subject</mtext>
          <mspace width="4.pt"/>
          <mtext>to</mtext>
          <mspace width="4.pt"/>
          <mfenced separators="" open="{" close="">
            <mtable>
              <mtr>
                <mtd columnalign="left">
                  <mrow>
                    <msub><mi>w</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi></mrow> </msub>
                    <mo>=</mo>
                    <mn>0</mn>
                  </mrow>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mtext>if</mtext>
                    <mspace width="4.pt"/>
                    <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msup>
                    <mspace width="4.pt"/>
                    <mtext>is</mtext>
                    <mspace width="4.pt"/>
                    <mtext>not</mtext>
                    <mspace width="4.pt"/>
                    <mtext>one</mtext>
                    <mspace width="4.pt"/>
                    <mtext>of</mtext>
                    <mspace width="4.pt"/>
                    <mtext>the</mtext>
                    <mspace width="4.pt"/>
                    <mi>k</mi>
                    <mspace width="4.pt"/>
                    <mtext>n.n.</mtext>
                    <mspace width="4.pt"/>
                    <mtext>of</mtext>
                    <mspace width="4.pt"/>
                    <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd columnalign="left">
                  <mrow>
                    <munderover><mo>‚àë</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </munderover>
                    <msub><mi>w</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi></mrow> </msub>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mtext>for</mtext>
                    <mspace width="4.pt"/>
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo lspace="0%" rspace="0%">,</mo>
                    <mn>2</mn>
                    <mo lspace="0%" rspace="0%">,</mo>
                    <mo>‚ãØ</mo>
                    <mo lspace="0%" rspace="0%">,</mo>
                    <mi>m</mi>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>After this step, the weight matrix <math alttext="ModifyingAbove bold upper W With caret">
  <mover accent="true"><mi>ùêñ</mi> <mo>^</mo></mover>
</math> (containing the weights <math alttext="ModifyingAbove w With caret Subscript i comma j">
  <msub><mover accent="true"><mi>w</mi> <mo>^</mo></mover> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi></mrow> </msub>
</math>) encodes the local linear relationships between the training instances. The second step is to map the training instances into a <em>d</em>-dimensional space (where <em>d</em> &lt; <em>n</em>) while preserving these local relationships as much as possible. If <strong>z</strong><sup>(<em>i</em>)</sup> is the image of <strong>x</strong><sup>(<em>i</em>)</sup> in this <em>d</em>-dimensional space, then we want the squared distance between <strong>z</strong><sup>(<em>i</em>)</sup> and <math alttext="sigma-summation Underscript j equals 1 Overscript m Endscripts ModifyingAbove w With caret Subscript i comma j Baseline bold z Superscript left-parenthesis j right-parenthesis">
  <mrow>
    <msubsup><mo>‚àë</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </msubsup>
    <msub><mover accent="true"><mi>w</mi> <mo>^</mo></mover> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi></mrow> </msub>
    <msup><mrow><mi>ùê≥</mi></mrow> <mfenced open="(" close=")"><mi>j</mi></mfenced> </msup>
  </mrow>
</math> to be as small as possible. This idea leads to the unconstrained optimization problem described in <a data-type="xref" href="#lle_second_step">Equation 7-5</a>. It looks very similar to the first step, but instead of keeping the instances fixed and finding the optimal weights, we are doing the reverse: keeping the weights fixed and finding the optimal position of the instances‚Äô images in the low-dimensional space. Note that <strong>Z</strong> is the matrix containing all <strong>z</strong><sup>(<em>i</em>)</sup>.</p>
<div id="lle_second_step" data-type="equation">
<h5><span class="label">Equation 7-5. </span>LLE step 2: reducing dimensionality while preserving relationships</h5>
<math alttext="ModifyingAbove bold upper Z With caret equals argmin Underscript bold upper Z Endscripts sigma-summation Underscript i equals 1 Overscript m Endscripts left-parenthesis bold z Superscript left-parenthesis i right-parenthesis Baseline minus sigma-summation Underscript j equals 1 Overscript m Endscripts ModifyingAbove w With caret Subscript i comma j Baseline bold z Superscript left-parenthesis j right-parenthesis Baseline right-parenthesis squared" display="block">
  <mstyle scriptlevel="0" displaystyle="true">
    <mrow>
      <mover accent="true"><mi>ùêô</mi> <mo>^</mo></mover>
      <mo>=</mo>
      <munder><mtext>argmin</mtext> <mi>ùêô</mi></munder>
      <mspace width="0.222222em"/>
      <munderover><mo>‚àë</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </munderover>
      <msup><mrow><mo>(</mo><msup><mrow><mi>ùê≥</mi></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>-</mo><munderover><mo>‚àë</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </munderover><msub><mover accent="true"><mi>w</mi> <mo>^</mo></mover> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi></mrow> </msub><msup><mrow><mi>ùê≥</mi></mrow> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msup><mo>)</mo></mrow> <mn>2</mn> </msup>
    </mrow>
  </mstyle>
</math>
</div>

<p class="pagebreak-before">Scikit-Learn‚Äôs LLE implementation has the following computational complexity: <span class="keep-together"><em>O</em>(<em>m</em> log(<em>m</em>)<em>n</em> log(<em>k</em>))</span> for finding the <em>k</em>-nearest neighbors, <em>O</em>(<em>mnk</em><sup>3</sup>) for optimizing the weights, and <em>O</em>(<em>dm</em><sup>2</sup>) for constructing the low-dimensional representations. Unfortunately, the <em>m</em><sup>2</sup> in the last term makes this algorithm scale poorly to very large datasets.</p>

<p>As you can see, LLE is quite different from the projection techniques, and it‚Äôs significantly more complex, but it can also construct much better low-dimensional representations, especially if the data is nonlinear.<a data-type="indexterm" data-startref="xi_dimensionalityreductionLLE7379216_1" id="id1923"/><a data-type="indexterm" data-startref="xi_LLElocallylinearembedding7379216_1" id="id1924"/><a data-type="indexterm" data-startref="xi_locallylinearembeddingLLE7379216_1" id="id1925"/><a data-type="indexterm" data-startref="xi_nonlineardimensionalityreductionNLDR7379264_1" id="id1926"/><a data-type="indexterm" data-startref="xi_embeddingsLLE7379216_1" id="id1927"/><a data-type="indexterm" data-startref="xi_NLDRnonlineardimensionalityreduction7379264_1" id="id1928"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Other Dimensionality Reduction Techniques"><div class="sect1" id="id128">
<h1>Other Dimensionality Reduction Techniques</h1>

<p>Before we conclude this chapter,  let‚Äôs take a quick look at a few other popular dimensionality reduction techniques available in Scikit-Learn:</p>
<dl>
<dt><code translate="no">sklearn.manifold.MDS</code></dt>
<dd>
<p><em>Multidimensional scaling</em> (MDS)<a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="multidimensional scaling" id="id1929"/><a data-type="indexterm" data-primary="MDS (multidimensional scaling)" id="id1930"/><a data-type="indexterm" data-primary="multidimensional scaling (MDS)" id="id1931"/><a data-type="indexterm" data-primary="sklearn" data-secondary="manifold.MDS" id="id1932"/> reduces dimensionality while trying to preserve the distances between the instances. Random projection does that for high-dimensional data, but it doesn‚Äôt work well on low-dimensional data.</p>
</dd>
<dt><code translate="no">sklearn.manifold.Isomap</code></dt>
<dd>
<p><em>Isomap</em> <a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="Isomap" id="id1933"/><a data-type="indexterm" data-primary="Isomap" id="id1934"/><a data-type="indexterm" data-primary="sklearn" data-secondary="manifold.Isomap" id="id1935"/>creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality while trying to preserve the <em>geodesic distances</em><a data-type="indexterm" data-primary="geodesic distance" id="id1936"/> between the instances. The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between these nodes. This approach works best when the data lies on a fairly smooth and low-dimensional manifold with a single global structure (e.g., the Swiss roll).</p>
</dd>
<dt><code translate="no">sklearn.manifold.TSNE</code></dt>
<dd>
<p><em>t-distributed stochastic neighbor embedding</em> (t-SNE)<a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="t-distributed stochastic neighbor embedding" id="id1937"/><a data-type="indexterm" data-primary="sklearn" data-secondary="manifold.TSNE" id="id1938"/><a data-type="indexterm" data-primary="t-distributed stochastic neighbor embedding (t-SNE)" data-primary-sortas="tdistributed stochastic neighbor embedding (t-SNE)" id="id1939"/><a data-type="indexterm" data-primary="t-SNE (t-distributed stochastic neighbor embedding)" data-primary-sortas="tSNE (t-distributed stochastic neighbor embedding)" id="id1940"/><a data-type="indexterm" data-primary="visualization of data" data-secondary="t-SNE" id="id1941"/><a data-type="indexterm" data-primary="embeddings" data-secondary="t-SNE" id="id1942"/> reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space. For example, in the exercises at the end of this chapter you will use t-SNE to visualize a 2D map of the MNIST images. However, it is not meant to be used as a preprocessing stage for an ML model.</p>
</dd>
<dt><code translate="no">sklearn.discriminant_analysis.LinearDiscriminantAnalysis</code></dt>
<dd>
<p><em>Linear discriminant analysis</em> (LDA)<a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="linear discriminant analysis" id="id1943"/><a data-type="indexterm" data-primary="LDA (linear discriminant analysis)" id="id1944"/><a data-type="indexterm" data-primary="linear models" data-secondary="LDA" id="id1945"/><a data-type="indexterm" data-primary="linear models" data-secondary="logistic regression" data-see="logistic regression" id="id1946"/><a data-type="indexterm" data-primary="linear models" data-secondary="PCA" data-see="principal component analysis" id="id1947"/><a data-type="indexterm" data-primary="linear discriminant analysis (LDA)" id="id1948"/><a data-type="indexterm" data-primary="sklearn" data-secondary="discriminant_analysis.LinearDiscriminantAnalysis" id="id1949"/> is a linear classification algorithm that, during training, learns the most discriminative axes between the classes. These axes can then be used to define a hyperplane onto which to project the data. The benefit of this approach is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm (unless LDA alone is sufficient).</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p><em>Uniform Manifold Approximation and Projection</em> (UMAP)<a data-type="indexterm" data-primary="UMAP (Uniform Manifold Approximation and Projection)" id="id1950"/><a data-type="indexterm" data-primary="Uniform Manifold Approximation and Projection (UMAP)" id="id1951"/> is another popular dimensionality reduction technique for visualization. While t-SNE is better at preserving the local structure, especially clusters, UMAP tries to preserve both the local and global structures. Moreover, it scales better to large datasets. Sadly, it is not available in Scikit-Learn, but there‚Äôs a good implementation in the <a href="https://umap-learn.readthedocs.io">umap-learn package</a>.</p>
</div>

<p><a data-type="xref" href="#other_dim_reduction_plot">Figure¬†7-11</a> shows the results of MDS, Isomap, and t-SNE on the Swiss roll. MDS manages to flatten the Swiss roll without losing its global curvature, while Isomap drops it entirely. Depending on the downstream task, preserving the large-scale structure may be good or bad. t-SNE does a reasonable job of flattening the Swiss roll, preserving a bit of curvature, and it also amplifies clusters, tearing the roll apart. Again, this might be good or bad, depending on the downstream task.<a data-type="indexterm" data-startref="xi_dimensionalityreduction735_1" id="id1952"/></p>

<figure><div id="other_dim_reduction_plot" class="figure">
<img src="assets/hmls_0711.png" alt="The diagram compares MDS, Isomap, and t-SNE techniques for reducing the Swiss roll dataset to two dimensions, illustrating different ways the global and local structures are preserved." width="3175" height="1068"/>
<h6><span class="label">Figure 7-11. </span>Using various techniques to reduce the Swiss roll to 2D</h6>
</div></figure>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="id731">
<h1>Exercises</h1>
<ol>
<li>
<p>What are the main motivations for reducing a dataset‚Äôs dimensionality? What are the main drawbacks?</p>
</li>
<li>
<p>What is the curse of dimensionality?</p>
</li>
<li>
<p>Once a dataset‚Äôs dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?</p>
</li>
<li>
<p>Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?</p>
</li>
<li>
<p>Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?</p>
</li>
<li>
<p>In what cases would you use regular PCA, incremental PCA, randomized PCA, or random projection?</p>
</li>
<li>
<p>How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?</p>
</li>
<li>
<p>Does it make any sense to chain two different dimensionality reduction 
<span class="keep-together">algorithms?</span></p>
</li>
<li>
<p>Load the MNIST dataset (introduced in <a data-type="xref" href="ch03.html#classification_chapter">Chapter¬†3</a>) and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing). Train a random forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set. Next, use PCA to reduce the dataset‚Äôs dimensionality, with an explained variance ratio of 95%. Train a new random forest classifier on the reduced dataset and see how long it takes. Was training much faster? Next, evaluate the classifier on the test set. How does it compare to the previous classifier? Try again with an <code translate="no">SGDClassifier</code>. How much does PCA help now?</p>
</li>
<li>
<p>Use t-SNE to reduce the first 5,000 images of the MNIST dataset down to 2 dimensions and plot the result using Matplotlib. You can use a scatterplot using 10 different colors to represent each image‚Äôs target class. Alternatively, you can replace each dot in the scatterplot with the corresponding instance‚Äôs class (a digit from 0 to 9), or even plot scaled-down versions of the digit images themselves (if you plot all digits the visualization will be too cluttered, so you should either draw a random sample or plot an instance only if no other instance has already been plotted at a close distance). You should get a nice visualization with well-separated clusters of digits. Try using other dimensionality reduction algorithms, such as PCA, LLE, or MDS, and compare the resulting visualizations.</p>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter‚Äôs notebook, at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id1866"><sup><a href="ch07.html#id1866-marker">1</a></sup> Well, four dimensions if you count time, and a few more if you are a string theorist.</p><p data-type="footnote" id="id1867"><sup><a href="ch07.html#id1867-marker">2</a></sup> Watch a rotating tesseract projected into 3D space at <a href="https://homl.info/30" class="bare"><em class="hyperlink">https://homl.info/30</em></a>. Image by Wikipedia user NerdBoy1392 (<a href="https://oreil.ly/pMbrK">Creative Commons BY-SA 3.0</a>). Reproduced from <a href="https://en.wikipedia.org/wiki/Tesseract" class="bare"><em class="hyperlink">https://en.wikipedia.org/wiki/Tesseract</em></a>.</p><p data-type="footnote" id="id1868"><sup><a href="ch07.html#id1868-marker">3</a></sup> Fun fact: anyone you know is probably an extremist in at least one dimension (e.g., how much sugar they put in their coffee), if you consider enough dimensions.</p><p data-type="footnote" id="id1877"><sup><a href="ch07.html#id1877-marker">4</a></sup> Karl Pearson, ‚ÄúOn Lines and Planes of Closest Fit to Systems of Points in Space‚Äù, <em>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</em> 2, no. 11 (1901): 559‚Äì572.</p><p data-type="footnote" id="id1880"><sup><a href="ch07.html#id1880-marker">5</a></sup> The proof that SVD happens to give us exactly the principal components we need for PCA requires some prerequisite math knowledge, such as eigenvectors and covariance matrices. If you are curious, you will find all the details in this <a href="https://homl.info/pca2">2014 paper by Jonathon Shlens</a>.</p><p data-type="footnote" id="id1905"><sup><a href="ch07.html#id1905-marker">6</a></sup> Scikit-Learn uses the <a href="https://homl.info/32">algorithm</a> described in David A. Ross et al., ‚ÄúIncremental Learning for Robust Visual Tracking‚Äù, <em>International Journal of Computer Vision</em> 77, no. 1‚Äì3 (2008): 125‚Äì141.</p><p data-type="footnote" id="id1908"><sup><a href="ch07.html#id1908-marker">7</a></sup> <em>Œµ</em> is the Greek letter epsilon, often used for tiny values.</p><p data-type="footnote" id="id1916"><sup><a href="ch07.html#id1916-marker">8</a></sup> Sanjoy Dasgupta et al., ‚ÄúA neural algorithm for a fundamental computing problem‚Äù, <em>Science</em> 358, no. 6364 (2017): 793‚Äì796.</p><p data-type="footnote" id="id1921"><sup><a href="ch07.html#id1921-marker">9</a></sup> Sam T. Roweis and Lawrence K. Saul, ‚ÄúNonlinear Dimensionality Reduction by Locally Linear Embedding‚Äù, <em>Science</em> 290, no. 5500 (2000): 2323‚Äì2326.</p></div></div></section></div></div></body></html>