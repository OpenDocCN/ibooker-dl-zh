["```py\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,     # Vocabulary size\n    \"context_length\": 1024,  # Context length\n    \"emb_dim\": 768,          # Embedding dimension\n    \"n_heads\": 12,           # Number of attention heads\n    \"n_layers\": 12,          # Number of layers\n    \"drop_rate\": 0.1,        # Dropout rate\n    \"qkv_bias\": False        # Query-Key-Value bias\n}\n```", "```py\nimport torch\nimport torch.nn as nn\n\nclass DummyGPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n        self.trf_blocks = nn.Sequential(               #1\n            *[DummyTransformerBlock(cfg)               #1\n              for _ in range(cfg[\"n_layers\"])]         #1\n        )                                              #1\n        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])     #2\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n        )\n\n    def forward(self, in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(\n            torch.arange(seq_len, device=in_idx.device)\n        )\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits\n\nclass DummyTransformerBlock(nn.Module):    #3\n    def __init__(self, cfg):\n        super().__init__()\n\n    def forward(self, x):     #4\n        return x\n\nclass DummyLayerNorm(nn.Module):           #5\n    def __init__(self, normalized_shape, eps=1e-5):    #6\n        super().__init__()\n\n    def forward(self, x):\n        return x\n```", "```py\nimport tiktoken\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\nbatch = []\ntxt1 = \"Every effort moves you\"\ntxt2 = \"Every day holds a\"\n\nbatch.append(torch.tensor(tokenizer.encode(txt1)))\nbatch.append(torch.tensor(tokenizer.encode(txt2)))\nbatch = torch.stack(batch, dim=0)\nprint(batch)\n```", "```py\ntensor([[6109,  3626,  6100,   345],    #1\n        [6109,  1110,  6622,   257]])\n```", "```py\ntorch.manual_seed(123)\nmodel = DummyGPTModel(GPT_CONFIG_124M)\nlogits = model(batch)\nprint(\"Output shape:\", logits.shape)\nprint(logits)\n```", "```py\nOutput shape: torch.Size([2, 4, 50257])\ntensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n         [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n\n        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n       grad_fn=<UnsafeViewBackward0>)\n```", "```py\ntorch.manual_seed(123)\nbatch_example = torch.randn(2, 5)     #1\nlayer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\nout = layer(batch_example)\nprint(out)\n```", "```py\ntensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n       grad_fn=<ReluBackward0>)\n```", "```py\nmean = out.mean(dim=-1, keepdim=True)\nvar = out.var(dim=-1, keepdim=True)\nprint(\"Mean:\\n\", mean)\nprint(\"Variance:\\n\", var)\n```", "```py\nMean:\n  tensor([[0.1324],\n          [0.2170]], grad_fn=<MeanBackward1>)\nVariance:\n  tensor([[0.0231],\n          [0.0398]], grad_fn=<VarBackward0>)\n```", "```py\nout_norm = (out - mean) / torch.sqrt(var)\nmean = out_norm.mean(dim=-1, keepdim=True)\nvar = out_norm.var(dim=-1, keepdim=True)\nprint(\"Normalized layer outputs:\\n\", out_norm)\nprint(\"Mean:\\n\", mean)\nprint(\"Variance:\\n\", var)\n```", "```py\nNormalized layer outputs:\n tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n       grad_fn=<DivBackward0>)\nMean:\n tensor([[-5.9605e-08],\n        [1.9868e-08]], grad_fn=<MeanBackward1>)\nVariance:\n tensor([[1.],\n        [1.]], grad_fn=<VarBackward0>)\n```", "```py\ntorch.set_printoptions(sci_mode=False)\nprint(\"Mean:\\n\", mean)\nprint(\"Variance:\\n\", var)\n```", "```py\nMean:\n tensor([[    0.0000],\n        [    0.0000]], grad_fn=<MeanBackward1>)\nVariance:\n tensor([[1.],\n        [1.]], grad_fn=<VarBackward0>)\n```", "```py\nclass LayerNorm(nn.Module):\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n        return self.scale * norm_x + self.shift\n```", "```py\nln = LayerNorm(emb_dim=5)\nout_ln = ln(batch_example)\nmean = out_ln.mean(dim=-1, keepdim=True)\nvar = out_ln.var(dim=-1, unbiased=False, keepdim=True)\nprint(\"Mean:\\n\", mean)\nprint(\"Variance:\\n\", var)\n```", "```py\nMean:\n tensor([[    -0.0000],\n        [     0.0000]], grad_fn=<MeanBackward1>)\nVariance:\n tensor([[1.0000],\n        [1.0000]], grad_fn=<VarBackward0>)\n```", "```py\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n            (x + 0.044715 * torch.pow(x, 3))\n        ))\n```", "```py\nimport matplotlib.pyplot as plt\ngelu, relu = GELU(), nn.ReLU()\n\nx = torch.linspace(-3, 3, 100)     #1\ny_gelu, y_relu = gelu(x), relu(x)\nplt.figure(figsize=(8, 3))\nfor i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n    plt.subplot(1, 2, i)\n    plt.plot(x, y)\n    plt.title(f\"{label} activation function\")\n    plt.xlabel(\"x\")\n    plt.ylabel(f\"{label}(x)\")\n    plt.grid(True)\nplt.tight_layout()\nplt.show()\n```", "```py\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n            GELU(),\n            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n```", "```py\nffn = FeedForward(GPT_CONFIG_124M)\nx = torch.rand(2, 3, 768)          #1\nout = ffn(x)\nprint(out.shape)\n```", "```py\ntorch.Size([2, 3, 768])\n```", "```py\nclass ExampleDeepNeuralNetwork(nn.Module):\n    def __init__(self, layer_sizes, use_shortcut):\n        super().__init__()\n        self.use_shortcut = use_shortcut\n        self.layers = nn.ModuleList([       #1\n            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), \n                          GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), \n                          GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), \n                          GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), \n                          GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), \n                          GELU())\n        ])\n\n    def forward(self, x):\n        for layer in self.layers:\n            layer_output = layer(x)         #2\n            if self.use_shortcut and x.shape == layer_output.shape:    #3\n                x = x + layer_output\n            else:\n                x = layer_output\n        return x\n```", "```py\nlayer_sizes = [3, 3, 3, 3, 3, 1]  \nsample_input = torch.tensor([[1., 0., -1.]])\ntorch.manual_seed(123)                            #1\nmodel_without_shortcut = ExampleDeepNeuralNetwork(\n    layer_sizes, use_shortcut=False\n)\n```", "```py\ndef print_gradients(model, x):\n    output = model(x)             #1\n    target = torch.tensor([[0.]])\n\n    loss = nn.MSELoss()\n    loss = loss(output, target)    #2\n\n    loss.backward()          #3\n\n    for name, param in model.named_parameters():\n        if 'weight' in name:\n            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n```", "```py\nprint_gradients(model_without_shortcut, sample_input)\n```", "```py\nlayers.0.0.weight has gradient mean of 0.00020173587836325169\nlayers.1.0.weight has gradient mean of 0.0001201116101583466\nlayers.2.0.weight has gradient mean of 0.0007152041653171182\nlayers.3.0.weight has gradient mean of 0.001398873864673078\nlayers.4.0.weight has gradient mean of 0.005049646366387606\n```", "```py\ntorch.manual_seed(123)\nmodel_with_shortcut = ExampleDeepNeuralNetwork(\n    layer_sizes, use_shortcut=True\n)\nprint_gradients(model_with_shortcut, sample_input)\n```", "```py\nlayers.0.0.weight has gradient mean of 0.22169792652130127\nlayers.1.0.weight has gradient mean of 0.20694105327129364\nlayers.2.0.weight has gradient mean of 0.32896995544433594\nlayers.3.0.weight has gradient mean of 0.2665732502937317\nlayers.4.0.weight has gradient mean of 1.3258541822433472\n```", "```py\nfrom chapter03 import MultiHeadAttention\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention(\n            d_in=cfg[\"emb_dim\"],\n            d_out=cfg[\"emb_dim\"],\n            context_length=cfg[\"context_length\"],\n            num_heads=cfg[\"n_heads\"], \n            dropout=cfg[\"drop_rate\"],\n            qkv_bias=cfg[\"qkv_bias\"])\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n\n    def forward(self, x):\n #1\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut      #2\n\n        shortcut = x         #3\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut      #4\n        return x\n```", "```py\ntorch.manual_seed(123)\nx = torch.rand(2, 4, 768)                   #1\nblock = TransformerBlock(GPT_CONFIG_124M)\noutput = block(x)\n\nprint(\"Input shape:\", x.shape)\nprint(\"Output shape:\", output.shape)\n```", "```py\nInput shape: torch.Size([2, 4, 768])\nOutput shape: torch.Size([2, 4, 768])\n```", "```py\nclass GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n        )\n\n    def forward(self, in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n #1\n        pos_embeds = self.pos_emb(\n            torch.arange(seq_len, device=in_idx.device)\n        )\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits\n```", "```py\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\n\nout = model(batch)\nprint(\"Input batch:\\n\", batch)\nprint(\"\\nOutput shape:\", out.shape)\nprint(out)\n```", "```py\nInput batch:\n tensor([[6109,  3626,  6100,   345],      #1\n         [6109,  1110,  6622,   257]])     #2\n\nOutput shape: torch.Size([2, 4, 50257])\ntensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n\n        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n       grad_fn=<UnsafeViewBackward0>)\n```", "```py\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total number of parameters: {total_params:,}\")\n```", "```py\nTotal number of parameters: 163,009,536\n```", "```py\nprint(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\nprint(\"Output layer shape:\", model.out_head.weight.shape)\n```", "```py\nToken embedding layer shape: torch.Size([50257, 768])\nOutput layer shape: torch.Size([50257, 768])\n```", "```py\ntotal_params_gpt2 = (\n    total_params - sum(p.numel()\n    for p in model.out_head.parameters())\n)\nprint(f\"Number of trainable parameters \"\n      f\"considering weight tying: {total_params_gpt2:,}\"\n)\n```", "```py\nNumber of trainable parameters considering weight tying: 124,412,160\n```", "```py\ntotal_size_bytes = total_params * 4       #1\ntotal_size_mb = total_size_bytes / (1024 * 1024)     #2\nprint(f\"Total size of the model: {total_size_mb:.2f} MB\")\n```", "```py\nTotal size of the model: 621.83 MB\n```", "```py\ndef generate_text_simple(model, idx,                 #1\n                         max_new_tokens, context_size): \n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]    #2\n        with torch.no_grad():\n            logits = model(idx_cond)\n\n        logits = logits[:, -1, :]                    #3\n        probas = torch.softmax(logits, dim=-1)           #4\n        idx_next = torch.argmax(probas, dim=-1, keepdim=True)    #5\n        idx = torch.cat((idx, idx_next), dim=1)     #6\n\n    return idx\n```", "```py\nstart_context = \"Hello, I am\"\nencoded = tokenizer.encode(start_context)\nprint(\"encoded:\", encoded)\nencoded_tensor = torch.tensor(encoded).unsqueeze(0)    #1\nprint(\"encoded_tensor.shape:\", encoded_tensor.shape)\n```", "```py\nencoded: [15496, 11, 314, 716]\nencoded_tensor.shape: torch.Size([1, 4])\n```", "```py\nmodel.eval()                  #1\nout = generate_text_simple(\n    model=model,\n    idx=encoded_tensor, \n    max_new_tokens=6, \n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\nprint(\"Output:\", out)\nprint(\"Output length:\", len(out[0]))\n```", "```py\nOutput: tensor([[15496,    11,   314,   716, 27018, 24086, 47843,\n30961, 42348,  7267]])\nOutput length: 10\n```", "```py\ndecoded_text = tokenizer.decode(out.squeeze(0).tolist())\nprint(decoded_text)\n```", "```py\nHello, I am Featureiman Byeswickattribute argue\n```"]