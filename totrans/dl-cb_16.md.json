["```py\n16.1 Simple Text Generation\n16.2 Prepare Keras Model for TensorFlow Serving\n16.3 Prepare Model for iOS\n```", "```py\nmovie = model.get_layer('movie_embedding')\nmovie_weights = movie.get_weights()[0]\nmovie_lengths = np.linalg.norm(movie_weights, axis=1)\nnormalized_movies = (movie_weights.T / movie_lengths).T\nnbrs = NearestNeighbors(n_neighbors=10, algorithm='ball_tree').fit(\n    normalized_movies)\nwith open('data/movie_model.pkl', 'wb') as fout:\n    pickle.dump({\n        'nbrs': nbrs,\n        'normalized_movies': normalized_movies,\n        'movie_to_idx': movie_to_idx\n    }, fout)\n```", "```py\nwith open('data/movie_model.pkl', 'rb') as fin:\n    m = pickle.load(fin)\nmovie_names = [x[0] for x in sorted(movie_to_idx.items(),\n               key=lambda t:t[1])]\ndistances, indices = m['nbrs'].kneighbors(\n    [m['normalized_movies'][m['movie_to_idx']['Rogue One']]])\nfor idx in indices[0]:\n    print(movie_names[idx])\n```", "```py\nRogue One\nPrometheus (2012 film)\nStar Wars: The Force Awakens\nRise of the Planet of the Apes\nStar Wars sequel trilogy\nMan of Steel (film)\nInterstellar (film)\nSuperman Returns\nThe Dark Knight Trilogy\nJurassic World\n```", "```py\nCREATE EXTENSION cube;\n```", "```py\nDROP TABLE IF EXISTS movie;\nCREATE TABLE movie (\n               movie_name TEXT PRIMARY KEY,\n               embedding FLOAT[] NOT NULL DEFAULT '{}'\n);\nCREATE INDEX movie_embedding ON movie USING gin(embedding);\nCREATE INDEX movie_movie_name_pattern\n    ON movie USING btree(lower(movie_name) text_pattern_ops);\n```", "```py\nconnection_str = \"dbname='*`%s`*' user='*`%s`*' password='*`%s`*' host='*`%s`*'\"\nconn = psycopg2.connect(connection_str % (DB_NAME, USER, PWD, HOST))\n\n```", "```py\nwith conn.cursor() as cursor:\n    for movie, embedding in zip(movies, normalized_movies):\n        cursor.execute('INSERT INTO movie (movie_name, embedding)'\n                       ' VALUES (%s, %s)',\n               (movie[0], embedding.tolist()))\nconn.commit()\n```", "```py\ndef recommend_movies(conn, q):\n    with conn.cursor() as cursor:\n        cursor.execute('SELECT movie_name, embedding FROM movie'\n                       '    WHERE lower(movie_name) LIKE %s'\n                       '    LIMIT 1',\n                       ('%' + q.lower() + '%',))\n        if cursor.rowcount == 0:\n            return []\n        movie_name, embedding = cursor.fetchone()\n        cursor.execute('SELECT movie_name, '\n                       '       cube_distance(cube(embedding), '\n                       '                     cube(%s)) as distance '\n                       '    FROM movie'\n                       '    ORDER BY distance'\n                       '    LIMIT 5',\n                       (embedding,))\n        return list(cursor.fetchall())\n```", "```py\nmodel = gensim.models.KeyedVectors.load_word2vec_format(\n    MODEL, binary=True)\n```", "```py\nsvd = TruncatedSVD(n_components=100, random_state=42,\n                   n_iter=40)\nreduced = svd.fit_transform(model.syn0norm)\n```", "```py\nreduced_lengths = np.linalg.norm(reduced, axis=1)\nnormalized_reduced = reduced.T / reduced_lengths).T\n```", "```py\ndef most_similar(norm, positive):\n    vec = norm[model.vocab[positive].index]\n    dists = np.dot(norm, vec)\n    most_extreme = np.argpartition(-dists, 10)[:10]\n    res = ((model.index2word[idx], dists[idx]) for idx in most_extreme)\n    return list(sorted(res, key=lambda t:t[1], reverse=True))\nfor word, score in most_similar(normalized_reduced, 'espresso'):\n    print(word, score)\n```", "```py\nespresso 1.0\ncappuccino 0.856463080029\nchai_latte 0.835657488972\nlatte 0.800340435865\nmacchiato 0.798796776324\nespresso_machine 0.791469456128\nLavazza_coffee 0.790783985201\nmocha 0.788645681469\nespressos 0.78424218748\nmartini 0.784037414689\n```", "```py\napp = Flask(__name__)\n```", "```py\n@app.route('/', methods=['GET', 'POST'])\ndef return_size():\n  if request.method == 'POST':\n    file = request.files['file']\n    if file:\n      image = Image.open(file)\n      width, height = image.size\n      return jsonify(results={'width': width, 'height': height})\n  return '''\n <h1>Upload new File</h1>\n <form action=\"\" method=post enctype=multipart/form-data>\n <p><input type=file name=file>\n <input type=submit value=Upload>\n </form>\n '''\n```", "```py\napp.run(port=5050, host='0.0.0.0')\n```", "```py\nwith open('data/image_similarity.pck', 'rb') as fin:\n    p = pickle.load(fin)\n    image_names = p['image_names']\n    nbrs = p['nbrs']\nbase_model = InceptionV3(weights='imagenet', include_top=True)\nmodel = Model(inputs=base_model.input,\n              outputs=base_model.get_layer('avg_pool').output)\n```", "```py\n      img = Image.open(file)\n      target_size = int(max(model.input.shape[1:]))\n      img = img.resize((target_size, target_size), Image.ANTIALIAS)\n      pre_processed = preprocess_input(\n          np.asarray([image.img_to_array(img)]))\n      vec = model.predict(pre_processed)\n      distances, indices = nbrs.kneighbors(vec)\n      res = [{'distance': dist,\n              'image_name': image_names[idx]}\n             for dist, idx in zip(distances[0], indices[0])]\n      return jsonify(results=res)\n```", "```py\ndef simple_view(request):\n    d = {}\n    update_date(request, d)\n    if request.FILES.get('painting'):\n        data = request.FILES['painting'].read()\n        files = {'file': data}\n        reply = requests.post('http://localhost:5050',\n                              files=files).json()\n        res = reply['results']\n        if res:\n            d['most_similar'] = res[0]['image_name']\n    return render(request, 'template_path/template.html', d)\n```", "```py\ncheckpoint_path = tf.train.latest_checkpoint(model_path)\ntrain_options = training_utils.TrainOptions.load(model_path)\nmodel_cls = locate(train_options.model_class) or \\\n  getattr(models, train_options.model_class)\nmodel_params = train_options.model_params\nmodel = model_cls(\n    params=model_params,\n    mode=tf.contrib.learn.ModeKeys.INFER)\nsource_tokens_ph = tf.placeholder(dtype=tf.string, shape=(1, None))\nsource_len_ph = tf.placeholder(dtype=tf.int32, shape=(1,))\nmodel(\n  features={\n    \"source_tokens\": source_tokens_ph,\n    \"source_len\": source_len_ph\n  },\n  labels=None,\n  params={\n  }\n)\n```", "```py\n  saver = tf.train.Saver()\n  def _session_init_op(_scaffold, sess):\n      saver.restore(sess, checkpoint_path)\n      tf.logging.info(\"Restored model from %s\", checkpoint_path)\n  scaffold = tf.train.Scaffold(init_fn=_session_init_op)\n  session_creator = tf.train.ChiefSessionCreator(scaffold=scaffold)\n  sess = tf.train.MonitoredSession(\n      session_creator=session_creator,\n      hooks=[DecodeOnce({}, callback_func=_save_prediction_to_dict)])\n  return sess, source_tokens_ph, source_len_pht\n```", "```py\n@app.route('/', methods=['GET'])\ndef handle_request():\n  input = request.args.get('input', '')\n  if input:\n    tf.reset_default_graph()\n    source_tokens = input.split() + [\"SEQUENCE_END\"]\n    session.run([], {\n        source_tokens_ph: [source_tokens],\n        source_len_ph: [len(source_tokens)]\n      })\n    return prediction_dict.pop(_tokens_to_str(source_tokens))\n```", "```py\nmodel.save('keras_js/nietzsche.h5')\nwith open('keras_js/chars.js', 'w') as fout:\n    fout.write('maxlen = ' + str(maxlen) + '\\n')\n    fout.write('num_chars = ' + str(len(chars)) + '\\n')\n    fout.write('char_indices = ' + json.dumps(char_indices, indent=2) + '\\n')\n    fout.write('indices_char = ' + json.dumps(indices_char, indent=2) + '\\n')\n```", "```py\ngit clone https://github.com/transcranial/keras-js.git\n```", "```py\npython <*git-root*>/keras-js/python/encoder.py nietzsche.h5\n\n```", "```py\n<script src=\"https://unpkg.com/keras-js\"></script>\n<script src=\"chars.js\"></script>\n```", "```py\n<textarea cols=\"60\" rows=\"4\" id=\"textArea\">\n   i am all for progress, it is\n</textarea><br/>\n<button onclick=\"runModel(250)\" disabled id=\"buttonGo\">Go!</button>\n```", "```py\nconst model = new KerasJS.Model({\n      filepath: 'sayings.bin',\n      gpu: true\n    })\n\n    model.ready().then(() => {\n      document.getElementById(\"buttonGo\").disabled = false\n    })\n```", "```py\n    function encode(st) {\n      var x = new Float32Array(num_chars * st.length);\n      for(var i = 0; i < st.length; i++) {\n        idx = char_indices[ch = st[i]];\n        x[idx + i * num_chars] = 1;\n      }\n      return x;\n    };\n```", "```py\nreturn model.predict(inputData).then(outputData => {\n    ...\n    ...\n  })\n```", "```py\n      var maxIdx = -1;\n      var maxVal = 0.0;\n      for (var idx = 0; idx < output.length; idx ++) {\n        if (output[idx] > maxVal) {\n          maxVal = output[idx];\n          maxIdx = idx;\n        }\n      }\n```", "```py\n     var nextChar = indices_char[\"\" + maxIdx];\n      document.getElementById(\"textArea\").value += nextChar;\n      if (steps > 0) {\n        runModel(steps - 1);\n      }\n```", "```py\nK.set_learning_phase(0)\nchar_cnn = load_model('zoo/07.2 char_cnn_model.h5')\nconfig = char_cnn.get_config()\nif not 'config' in config:\n    config = {'config': config,\n              'class_name': 'Model'}\n\nweights = char_cnn.get_weights()\n```", "```py\ntweet = (\"There's a house centipede in my closet and \"\n         \"since Ryan isn't here I have to kill it....\")\nencoded = np.zeros((1, max_sequence_len, len(char_to_idx)))\nfor idx, ch in enumerate(tweet):\n    encoded[0, idx, char_to_idx[ch]] = 1\n\nres = char_cnn.predict(encoded)\nemojis[np.argmax(res)]\n```", "```py\nu'\\ude03'\n```", "```py\nnew_model = model_from_config(config)\nnew_model.set_weights(weights)\n```", "```py\ninput_info = utils.build_tensor_info(new_model.inputs[0])\noutput_info = utils.build_tensor_info(new_model.outputs[0])\nprediction_signature = signature_def_utils.build_signature_def(\n          inputs={'input': input_info},\n          outputs={'output': output_info},\n          method_name=signature_constants.PREDICT_METHOD_NAME)\n```", "```py\noutpath = 'zoo/07.2 char_cnn_model.tf_model/1'\nshutil.rmtree(outpath)\n\nlegacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\nbuilder = tf.saved_model.builder.SavedModelBuilder(outpath)\nbuilder.add_meta_graph_and_variables(\n      sess, [tf.saved_model.tag_constants.SERVING],\n      signature_def_map={\n           'emoji_suggest': prediction_signature,\n      },\n      legacy_init_op=legacy_init_op)\nbuilder.save()\n```", "```py\ntensorflow_model_server \\\n    --model_base_path=\"char_cnn_model.tf_model/\" \\\n    --model_name=\"char_cnn_model\"\n```", "```py\nrequest = predict_pb2.PredictRequest()\nrequest.model_spec.name = 'char_cnn_model'\nrequest.model_spec.signature_name = 'emoji_suggest'\nrequest.inputs['input'].CopyFrom(tf.contrib.util.make_tensor_proto(\n    encoded.astype('float32'), shape=[1, max_sequence_len, len(char_to_idx)]))\n\nchannel = implementations.insecure_channel('localhost', 8500)\nstub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\nresult = stub.Predict(request, 5)\n```", "```py\nresponse = np.array(result.outputs['output'].float_val)\nprediction = np.argmax(response)\nemojis[prediction]\n```", "```py\nvirtualenv venv2\nsource venv2/bin/activate\npip install coremltools\npip install h5py\npip install keras==2.0.6\npip install tensorflow==1.2.1\n```", "```py\nfrom keras.models import load_model\nimport coremltools\n```", "```py\nkeras_model = load_model('zoo/09.3 retrained pet recognizer.h5')\nclass_labels = json.load(open('zoo/09.3 pet_labels.json'))\n```", "```py\ncoreml_model = coremltools.converters.keras.convert(\n    keras_model,\n    image_input_names=\"input_1\",\n    class_labels=class_labels,\n    image_scale=1/255.)\ncoreml_model.save('zoo/PetRecognizer.mlmodel')\n```"]