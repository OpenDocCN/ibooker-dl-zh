- en: 10 Convolutions in neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络中的10种卷积
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The graphical and algebraic view of neural networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的图形和代数视图
- en: Two-dimensional and three-dimensional convolution with custom weights
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带自定义权重的二维和三维卷积
- en: Adding convolution layers to a neural network
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向神经网络添加卷积层
- en: Image analysis typically involves identifying *local* patterns. For instance,
    to do face recognition, we need to analyze local patterns of neighboring pixels
    corresponding to eyes, noses, and ears. The subject of the photograph may be standing
    on a beach in front of the ocean, but the big picture involving sand and water
    is irrelevant.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分析通常涉及识别*局部*模式。例如，为了进行面部识别，我们需要分析眼睛、鼻子和耳朵对应的相邻像素的局部模式。照片的主题可能站在海滩上，面对着大海，但涉及沙子和水的整体画面是不相关的。
- en: 'Convolution is a specialized operation that examines local patterns in an input
    signal. These operators are typically used to analyze images: that is, the input
    is a 2D array of pixels. To illustrate this, we examine a few examples of special-purpose
    convolution operations that detect the edges, corners, and average illumination
    in a small neighborhood of pixels from an image. Once we have detected such local
    properties, we can combine them and recognize higher-level patterns like ears,
    noses, and eyes. We can combine those in turn to detect still higher-level structures
    like faces. The system naturally lends itself to multilayer convolutional neural'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是一种特殊的操作，它检查输入信号中的局部模式。这些操作通常用于分析图像：即输入是一个像素的二维数组。为了说明这一点，我们考察了一些特殊用途的卷积操作示例，这些操作检测图像中像素小邻域的边缘、角落和平均光照。一旦我们检测到这些局部属性，我们就可以将它们组合起来，识别更高层次的模式，如耳朵、鼻子和眼睛。我们可以依次将它们组合起来，以检测更高层次的结构，如面部。系统自然地适合多层卷积神经网络
- en: networks—the lowest layers(closest to the input) detect edges and corners, and
    the next layers detect ears, eyes, noses, and so forth.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 网络——最低层（最接近输入）检测边缘和角落，下一层检测耳朵、眼睛、鼻子等等。
- en: 'In section [8.3](../Text/08.xhtml#sec-linear-layer), we discussed the *linear*
    neural network layer (aka *fully connected* layer). There, every output is connected
    to *all* inputs. This means an output is derived by taking a weighted linear combination
    of *all* input values. In other words, the output is derived from a *global* view
    of the input. Convolution layers are different. These are characterized by:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[8.3](../Text/08.xhtml#sec-linear-layer)节中，我们讨论了*线性*神经网络层（也称为*全连接*层）。在那里，每个输出都与*所有*输入相连。这意味着输出是通过取所有输入值的加权线性组合得到的。换句话说，输出是从输入的*全局*视角得到的。卷积层不同。这些层的特点是：
- en: '*Local connections*—Only a small subset of neighboring input values are connected
    to one output value. Thus, each output is a weighted linear combination of only
    a small set of *adjacent* input values. As a consequence, only local patterns
    in the input are captured.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*局部连接*—只有一小部分相邻的输入值与一个输出值相连。因此，每个输出值只是少量相邻输入值的加权线性组合。结果，只有输入中的局部模式被捕捉到。'
- en: '*Shared weights*—The same weights are slid over the entire input. Consequently,'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*共享权重*—相同的权重在整个输入上滑动。因此，'
- en: The number of weights is drastically reduced. Since convolution is typically
    used on images where the input size is large number of pixels), fully connected
    layers are prohibitively expensive. Convolution repeats a (usually small) number
    of weights across the input, thereby keeping the number of weights manageable.
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重的数量大大减少。由于卷积通常用于图像上，输入大小是大量像素），全连接层成本过高。卷积在输入上重复（通常是小的）数量的权重，从而保持权重的数量可管理。
- en: The nature of the local pattern extracted is fixed all over the input. If the
    convolution is an edge detector, it extracts edges all over the input. We cannot
    have an edge detector at one region of the input and a corner detector at another
    region, for instance. Of course, in a multilayered network, we can use different
    convolution layers to capture different local patterns. In particular, successive
    layers can capture local patterns in local patterns of the input, and so on, thereby
    capturing increasingly complex and increasingly global patterns at higher layers
    of the network.
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取的局部模式在输入的整个区域都是固定的。如果卷积是一个边缘检测器，它将在整个输入中提取边缘。我们不能在输入的一个区域有边缘检测器，而在另一个区域有角点检测器，例如。当然，在多层网络中，我们可以使用不同的卷积层来捕捉不同的局部模式。特别是，连续的层可以捕捉输入局部模式中的局部模式，依此类推，从而在网络的更高层捕捉到越来越复杂和越来越全局的模式。
- en: The exact local pattern captured depends on the weights of the convolution operator.
    We don’t know exactly what local patterns of the input to capture to recognize
    a specific higher-level structure of interest (such as a face). This means we
    do not want to *specify* the weights of the convolutions. The whole point of neural
    networks is to avoid such tailored feature engineering. Rather, we want to *learn*—through
    the process of *training* described in chapter [8](../Text/08.xhtml#ch-training-neural-networks)—the
    weights of the convolution layers. Losses can be backpropagated through convolution
    just as they are through fully connected (FC) layers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 捕捉到的确切局部模式取决于卷积算子的权重。我们不知道要捕捉输入的哪些局部模式来识别感兴趣的特定高级结构（例如人脸）。这意味着我们不想*指定*卷积的权重。神经网络的全部目的就是避免这种定制特征工程。相反，我们希望通过第[8](../Text/08.xhtml#ch-training-neural-networks)章中描述的训练过程*学习*卷积层的权重。损失可以通过卷积像通过全连接层一样进行反向传播。
- en: Just like FC layers, convolution layers can be expressed as matrix-vector multiplications.
    The structure of the weight matrix is a special case of equation [8.8](../Text/08.xhtml#eq-MLP-weight-matrix),
    but it is a matrix all the same. Consequently, the forward propagation equation
    [8.7](../Text/08.xhtml#eq-linlayer-forwardprop) and backpropagation equations
    [8.31](../Text/08.xhtml#eq-auxvar-vector) and [8.33](../Text/08.xhtml#eq-partialderiv-loss-wt-vector)
    are still applicable. Forward propagation and backpropagation (training) through
    convolution proceed exactly as they do with FC layers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 就像全连接层一样，卷积层可以表示为矩阵-向量乘法。权重矩阵的结构是方程[8.8](../Text/08.xhtml#eq-MLP-weight-matrix)的特殊情况，但本质上它是一个矩阵。因此，前向传播方程[8.7](../Text/08.xhtml#eq-linlayer-forwardprop)和反向传播方程[8.31](../Text/08.xhtml#eq-auxvar-vector)以及[8.33](../Text/08.xhtml#eq-partialderiv-loss-wt-vector)仍然适用。通过卷积进行的前向传播和反向传播（训练）过程与全连接层完全相同。
- en: Since the convolution is learned—as opposed to specified—in a neural network,
    there is no telling what local patterns such layers will learn to extract (although,
    in practice, the initial layers often learn to recognize edges and corners). All
    we know is that each output in a given layer is derived from only a *small subset
    of spatially adjoint* input values from previous layers. The final output is derived
    from a hierarchical local examination of the input.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于卷积是在神经网络中学习得到的——而不是预先指定的——我们无法预测这样的层将学习提取哪些局部模式（尽管在实践中，初始层通常学会识别边缘和角点）。我们所知道的是，给定层中的每个输出仅来自前一层中空间相邻的*一小部分*输入值。最终输出是从对输入的分层局部检查中得到的。
- en: NOTE Fully functional code for chapter 10, runnable via Jupyter Notebook, is
    available at our public GitHub repository at [http://mng.bz/M2lW](http://mng.bz/M2lW).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：第10章的完整功能代码，可通过Jupyter Notebook运行，可在我们的公共GitHub仓库中找到，网址为[http://mng.bz/M2lW](http://mng.bz/M2lW)。
- en: '10.1 One-dimensional convolution: Graphical and algebraical view'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 一维卷积：图形和代数视图
- en: As always, we examine the process of convolution with a set of examples. We
    examine convolutions in one, two, and three dimensions, but we start with one
    dimension for ease of understanding.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们通过一系列示例来检验卷积的过程。我们检验了一维、二维和三维的卷积，但为了便于理解，我们首先从一维开始。
- en: The best way to visualize 1D convolution is to imagine a stretched, straightened
    rope (the input array) over which a measuring ruler (the kernel) is sliding.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最直观地可视化一维卷积的方法是想象一根被拉伸、拉直的绳子（输入数组），在其上滑动一个测量尺（核）。
- en: In figures [10.1](#fig-conv1d-smoothing-stride1), [10.2](#fig-conv1d-smoothing-stride2),
    and [10.3](#fig-conv1d-edge-detection-stride1), the ruler kernel) is shown as
    shaded boxes, while the rope (input array) is shown as a sequence of white boxes.
    Successive steps in the figure represent successive positions (aka slide stops)
    of the sliding ruler. Notice that the shaded portion occupies a different position
    in each step.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图[10.1](#fig-conv1d-smoothing-stride1)，[10.2](#fig-conv1d-smoothing-stride2)，和[10.3](#fig-conv1d-edge-detection-stride1)中，尺子核以阴影框的形式显示，而绳子（输入数组）以一系列白色框的形式显示。图中的连续步骤代表滑动尺子的连续位置（也称为滑动停止点）。请注意，阴影部分在每个步骤中占据不同的位置。
- en: Rulers in successive positions during sliding can overlap. They overlap by varying
    amounts in figures [10.1](#fig-conv1d-smoothing-stride1), [10.2](#fig-conv1d-smoothing-stride2),
    and [10.3](#fig-conv1d-edge-detection-stride1).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在滑动过程中，连续位置的尺子可以重叠。在图[10.1](#fig-conv1d-smoothing-stride1)，[10.2](#fig-conv1d-smoothing-stride2)，和[10.3](#fig-conv1d-edge-detection-stride1)中，它们以不同的量重叠。
- en: The rope and the ruler are discrete 1D arrays in reality. At each slide stop,
    the ruler array elements rest on a subset of rope array elements.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在现实中，绳子和尺子是离散的1D数组。在每个滑动停止点，尺子数组的元素位于绳子数组元素的一个子集上。
- en: We multiply each input array element by the kernel element resting on it and
    sum the products. This is equivalent to taking a weighted sum of the input (rope)
    elements that fall under the current position of the kernel (ruler), with the
    kernel elements serving as weights. This weighted sum is emitted as a single output
    element. One output element results from each slide stop of the tile. As the ruler
    slides over the entire rope, left to right, a 1D output array is generated.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将每个输入数组元素与其上的核元素相乘，并将乘积相加。这相当于对落在当前核（尺子）位置下的输入（绳子）元素进行加权求和，其中核元素作为权重。这个加权求和作为一个单独的输出元素发出。每个瓷砖的滑动停止点产生一个输出元素。当尺子从左到右在整个绳子上滑动时，生成一个1D输出数组。
- en: '![](../../OEBPS/Images/CH10_F01_Chaudhury.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F01_Chaudhury.png)'
- en: Figure 10.1 1D convolution with a *local averaging kernel* of size 3, stride
    1, and valid padding on the input array of size 7
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 大小为3的*局部平均核*的1D卷积，步长为1，有效填充，在大小为7的输入数组上
- en: '![](../../OEBPS/Images/CH10_F02_Chaudhury.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F02_Chaudhury.png)'
- en: Figure 10.2 1D convolution with a *local averaging kernel* of size 3, stride
    2, and valid padding
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 大小为3的*局部平均核*的1D卷积，步长为2，有效填充
- en: '![](../../OEBPS/Images/CH10_F03_Chaudhury.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F03_Chaudhury.png)'
- en: Figure 10.3 1D convolution with an *edge-detection kernel* of size 2, stride
    1, valid padding. Not all slide stops (that is, steps) are shown.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 大小为2的*边缘检测核*的1D卷积，步长为1，有效填充。并非所有滑动停止点（即步骤）都显示出来。
- en: 'The following entities are defined for 1D convolution:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下实体用于1D卷积：
- en: '*Input*—A one-dimensional array. We typically use the symbol *n* to represent
    input array length in 1D convolution. In figure [10.1](#fig-conv1d-smoothing-stride1),
    *n* = 7.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入*——一个一维数组。在1D卷积中，我们通常用符号*n*来表示输入数组长度。在图[10.1](#fig-conv1d-smoothing-stride1)中，*n*
    = 7。'
- en: '*Output*—A one-dimensional array. We typically use the symbol *o* to represent
    the output array length in 1D convolution. In figure [10.1](#fig-conv1d-smoothing-stride1),
    *o* = 5. Section [10.2](#sec-conv1d-out-size) shows how to calculate the output
    size from the independent parameters.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出*——一个一维数组。在1D卷积中，我们通常用符号*o*来表示输出数组长度。在图[10.1](#fig-conv1d-smoothing-stride1)中，*o*
    = 5。第[10.2](#sec-conv1d-out-size)节展示了如何从独立参数计算输出大小。'
- en: '*Kernel*—A small array of weights whose size is a parameter of the convolution.
    We typically use the symbol *k* to represent the kernel size in 1D convolution.
    In figure [10.1](#fig-conv1d-smoothing-stride1), *k* = 3; in figure [10.3](#fig-conv1d-edge-detection-stride1),
    *k* = 2.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*核*——一个权重的小数组，其大小是卷积的参数。在1D卷积中，我们通常用符号*k*来表示核大小。在图[10.1](#fig-conv1d-smoothing-stride1)中，*k*
    = 3；在图[10.3](#fig-conv1d-edge-detection-stride1)中，*k* = 2。'
- en: '*Stride*—The number of input elements over which the kernel slides after completing
    a single step. We typically use the symbol *s* to represent the stride in 1D convolution.
    This is a parameter of the convolution. In figure [10.1](#fig-conv1d-smoothing-stride1),
    *s* = 1; in figure [10.2](#fig-conv1d-smoothing-stride2), stride is 2. A stride
    of 1 means there is a slide stop at each successive element of the input. So,
    the output has roughly the same number of elements as the input (they may not
    be exactly equal because of padding, explained next). A stride of 2 means there
    is a slide stop at every *other* input element. So, the output size is roughly
    half of the input size. A stride of 3 means the output size is roughly one-third
    the input size.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步长*—在完成单步之后，内核在输入元素上滑动的数量。我们通常用符号 *s* 来表示一维卷积中的步长。这是卷积的一个参数。在图 [10.1](#fig-conv1d-smoothing-stride1)
    中，*s* = 1；在图 [10.2](#fig-conv1d-smoothing-stride2) 中，步长为 2。步长为 1 意味着在输入的每个后续元素处都有一个滑动停止。因此，输出元素的数量大致与输入相同（它们可能不完全相等，因为接下来会解释的填充）。步长为
    2 意味着在每隔一个输入元素处都有一个滑动停止。因此，输出大小大致是输入大小的一半。步长为 3 意味着输出大小大致是输入大小的三分之一。'
- en: '*Padding*—As the kernel slides toward the extremity of the input array, parts
    of it may fall outside the input array. In other words, part of the kernel falls
    over ghost input elements. Figure [10.4](#fig-conv1d-smoothing-stride2-zeropad)
    shows such a situation: the ghost input array elements are shown with dashed boxes.
    There are multiple strategies to deal with this:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*填充*—当内核向输入数组的末端滑动时，其部分可能超出输入数组。换句话说，内核的一部分落在幽灵输入元素上。图 [10.4](#fig-conv1d-smoothing-stride2-zeropad)
    展示了这种情况：幽灵输入数组元素用虚线框表示。有多种策略来处理这种情况：'
- en: '*Valid padding*—We stop sliding whenever any element of the kernel falls outside
    the input array. No ghost input elements are involved; the *entire* kernel always
    falls on valid input elements (hence the name *valid*). Note that this implies
    we will have fewer outputs than inputs. If we try to generate an output corresponding
    to, say, the last input element, all but the first kernel element will fall outside
    the input on ghost elements. So, we have to stop when the right-most kernel element
    falls on the right-most input element (see figures [10.1](#fig-conv1d-smoothing-stride1),
    [10.2](#fig-conv1d-smoothing-stride2), and [10.3](#fig-conv1d-edge-detection-stride1)).
    At this point, the left-most kernel element falls on the (*N* − *k*)th input element.
    We do not generate output for the last *k* inputs. Hence, even with a stride of
    1 for valid padding, the output is slightly smaller than the input.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*有效填充*—每当内核的任何元素超出输入数组时，我们就停止滑动。没有涉及幽灵输入元素；内核的 *整个* 总是落在有效的输入元素上（因此得名 *valid*）。请注意，这暗示我们将有比输入更少的输出。如果我们尝试生成与，比如说，最后一个输入元素对应的输出，除了第一个内核元素外，所有内核元素都将落在幽灵元素上的输入之外。因此，我们必须在最右侧的内核元素落在最右侧的输入元素上时停止（参见图
    [10.1](#fig-conv1d-smoothing-stride1)、[10.2](#fig-conv1d-smoothing-stride2) 和
    [10.3](#fig-conv1d-edge-detection-stride1)）。此时，最左侧的内核元素落在第 (*N* − *k*) 个输入元素上。我们不生成最后
    *k* 个输入的输出。因此，即使对于有效填充的步长为 1，输出也略小于输入。'
- en: '*Same (zero) padding*—Here, we do not want to stop early. If the stride is
    1, the output size matches the input size (hence the name *same*). We continue
    to slide the kernel until its left end falls on the right-most input. At this
    point, all but the left-most kernel element is falling on ghost input elements.
    We pretend these ghost input elements have a value of 0 (*zero padding*).'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*相同（零）填充*—在这里，我们不希望提前停止。如果步长为 1，输出大小与输入大小匹配（因此得名 *same*）。我们继续滑动内核，直到其左端落在最右侧的输入上。此时，除了最左侧的内核元素外，所有元素都落在幽灵输入元素上。我们假设这些幽灵输入元素具有
    0（*零填充*）的值。'
- en: '![](../../OEBPS/Images/CH10_F04_Chaudhury.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F04_Chaudhury.png)'
- en: Figure 10.4 1D convolution with a *local averaging kernel* of size 3, stride
    2, and zero padding
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 3x 大小、步长 2、零填充的 1D 卷积
- en: 'Let’s denote the input array’s domain by *S*. It’s a 1D grid:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 *S* 来表示输入数组的域。它是一个一维网格：
- en: '*S* = [0, *W* − 1]'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*S* = [0, *W* − 1]'
- en: 'Every point in *S* is associated with a value *X[x]*. Together, these values
    form the input *X*. On this grid of input points, we define a subgrid *S[o]* of
    output points. *S[o]* is obtained from *S* by applying stride-based stepping on
    the input. Assuming *s* = [*s[W]*] denotes the stride, the first slide stop has
    the top-left corner of the rope at (*x* = 0). The next slide stop is at (*x* =
    *s[W]*), and the next is (*x* = 2*s[W]*), and so on. When we reach the right end,
    we stop. Overall, the output grid consists of the slide-stop points at which the
    top-left corner of the kernel (ruler) rests as it sweeps over the input volume:
    that is, *S[o]* = {(*x* = 0), (*x* = *s[W]*)⋯,}. There is an output for each point
    in *S[o]*.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*S* 中的每一个点都与一个值 *X[x]* 相关联。这些值共同构成了输入 *X*。在这个输入点网格上，我们定义了一个输出点子网格 *S[o]*。*S[o]*
    通过在输入上应用基于步长的跳跃从 *S* 中获得。假设 *s* = [*s[W]*] 表示步长，第一个滑动停止点的绳索左上角在 (*x* = 0)。下一个滑动停止点在
    (*x* = *s[W]*)，然后是 (*x* = 2*s[W]*)，以此类推。当我们到达右侧时，我们停止。总的来说，输出网格由核（尺子）在输入体积上扫过时其左上角所停的滑动停止点组成：即
    *S[o]* = {(*x* = 0), (*x* = *s[W]*)⋯,}。*S[o]* 中的每个点都有一个输出。'
- en: 'Equation [10.1](#eq-1dconv-output) shows how a single output value is generated
    in 1D convolution. *X* denotes input, *Y* denotes output, and *W* denote kernel
    weights:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 [10.1](#eq-1dconv-output) 展示了在 1D 卷积中单个输出值的生成过程。*X* 表示输入，*Y* 表示输出，而 *W*
    表示核权重：
- en: '![](../../OEBPS/Images/eq_10-01.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_10-01.png)'
- en: Equation 10.1
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 10.1
- en: Note that when the kernel of dimension *k[W]* (ruler) has its origin on *x*,
    it covers all input pixels in the domain [*x*..(*x* + *k[W]*)]. These are the
    pixels participating in equation [10.1](#eq-1dconv-output). Each of these input
    pixels is multiplied by the kernel element covering it. Match equation [10.1](#eq-1dconv-output)
    with figures [10.1](#fig-conv1d-smoothing-stride1), [10.2](#fig-conv1d-smoothing-stride2),
    and [10.3](#fig-conv1d-edge-detection-stride1).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当尺寸为 *k[W]*（尺子）的核在其原点 *x* 上时，它覆盖了域 [*x*..(*x* + *k[W]*)] 内的所有输入像素。这些是参与方程
    [10.1](#eq-1dconv-output) 的像素。每个输入像素都乘以覆盖它的核元素。将方程 [10.1](#eq-1dconv-output) 与图
    [10.1](#fig-conv1d-smoothing-stride1)、[10.2](#fig-conv1d-smoothing-stride2) 和
    [10.3](#fig-conv1d-edge-detection-stride1) 进行匹配。
- en: 10.1.1 Curve smoothing via 1D convolution
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 通过 1D 卷积进行曲线平滑
- en: In this section, we look at how to perform local averaging via convolution,
    from a physical and algebraic viewpoint, to get a comprehensive understanding.
    The 1D kernel with weight vector ![](../../OEBPS/Images/eq_10-01-a2.png) (shown
    in figure [10.1](#fig-conv1d-smoothing-stride1)) essentially takes the moving
    average of successive sets of three input values. As such, it is a local averaging
    (aka smoothing) operator. This becomes apparent if we examine the plots of the
    raw input vector with regard to the input vector convolved with the kernel (figure
    [10.5](#fig-1d-smoothing-graph)). The input (solid line) weaves up and down, while
    the output is a smooth curve (dashed line) through the mean position of the input.
    In general, the output produced by convolving by a kernel with all equal weights
    (the weights must be normalized, meaning the sum of the weights is 1) is a smoothed
    (locally averaged) version of the input. Why do we want to smooth an input vector?
    Because it captures the broad trend in the input data while eliminating short-term
    fluctuations (often caused by noise). If you are familiar with Fourier transforms
    and frequency domains, you can see that this is essentially a low-pass filter,
    eliminating short-term, high-frequency noise and capturing the longer-term, low-frequency
    variation in the input data array.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们从物理和代数角度探讨如何通过卷积进行局部平均，以获得全面的理解。权重向量为 ![](../../OEBPS/Images/eq_10-01-a2.png)
    的 1D 核（如图 [10.1](#fig-conv1d-smoothing-stride1) 所示）本质上是对连续的三组输入值进行移动平均。因此，它是一个局部平均（也称为平滑）算子。如果我们检查原始输入向量与通过核卷积的输入向量的图（图
    [10.5](#fig-1d-smoothing-graph)），这一点就会变得明显。输入（实线）上下波动，而输出则是一条通过输入平均位置的平滑曲线（虚线）。一般来说，通过具有所有相等权重的核（权重必须归一化，即权重的总和为
    1）卷积产生的输出是输入的平滑（局部平均）版本。我们为什么要平滑输入向量呢？因为它捕捉了输入数据中的广泛趋势，同时消除了短期波动（通常由噪声引起）。如果你熟悉傅里叶变换和频域，你可以看到这本质上是一个低通滤波器，消除短期、高频噪声，并捕捉输入数据数组中的长期、低频变化。
- en: '![](../../OEBPS/Images/CH10_F05_Chaudhury.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH10_F05_Chaudhury.png)'
- en: Figure 10.5 Graph of the input array solid) and output array (dashed) from figure
    [10.1](#fig-conv1d-smoothing-stride1). Note that the averaged) version of the
    input. Such local soothing captures the low-frequency (long-term) broad trend
    of the function by eliminating high-frequency (short-term) noise.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 从图[10.1](#fig-conv1d-smoothing-stride1)中输入数组（实线）和输出数组（虚线）的图形。注意这是输入的平均值版本。这种局部平滑通过消除高频（短期）噪声来捕捉函数的低频（长期）广泛趋势。
- en: 10.1.2 Curve edge detection via 1D convolution
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 通过1D卷积进行曲线边缘检测
- en: As mentioned earlier, a convolution’s physical effect on an input array radically
    changes with the weights of the convolution kernel. Now let’s examine a very different
    kernel that detects edges in the input data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，卷积对输入数组的影响在卷积核的权重上发生了根本性的变化。现在让我们检查一个完全不同的核，它可以检测输入数据中的边缘。
- en: An *edge* is defined as a sharp change in the values in an input array. For
    instance, if two successive elements in the input array have a large absolute
    difference in values, that is an edge. If we graph the input array (that is, plot
    the input array values in the *y* axis against the array indices), an edge will
    appear in the graph. For instance, consider the input array in figure [10.3](#fig-conv1d-edge-detection-stride1)
    (graphed in figure 10.6). At indices 0 to 3, we have values in the neighborhood
    of 10. At index 4, the value jumps to 51. We say there is an edge between indices
    3 and 4. The values then remain in the neighborhood of 50 at indices 4 to 7. Then
    they jump back to the neighborhood of 10 in the remaining indices. We say there
    is another edge between indices 7 and 8. The convolution we examine here will
    emit a high response (output value) exactly at the indices of the jump—3 and 7—while
    emitting a low response at other indices. This is an edge-detection convolution
    (filter). Why do we want to detect edges? Because edges are important for understanding
    images. Locations at which the signal changes rapidly provide more semantic clues
    than flat uniform regions. Experiments on the human visual cortex have established
    that humans pay more attention to locations where color or shade changes rapidly
    than to flat regions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*边缘*被定义为输入数组中值的急剧变化。例如，如果输入数组中连续的两个元素在值上有很大的绝对差异，那么这就是一个边缘。如果我们绘制输入数组（即在*y*轴上绘制输入数组值与数组索引的关系图），则图中将出现边缘。例如，考虑图[10.3](#fig-conv1d-edge-detection-stride1)中的输入数组（如图10.6所示）。在索引0到3之间，我们有值在10的附近。在索引4处，值跳到51。我们说在索引3和4之间存在边缘。然后这些值在索引4到7之间保持在50的附近。然后它们在剩余的索引中跳回到10的附近。我们说在索引7和8之间存在另一个边缘。我们在这里检查的卷积将在跳跃的索引3和7处产生高响应（输出值），在其他索引处产生低响应。这是一个边缘检测卷积（滤波器）。我们为什么要检测边缘呢？因为边缘对于理解图像很重要。信号快速变化的地点比平坦均匀区域提供了更多的语义线索。对人类视觉皮层的实验表明，人类比平坦区域更关注颜色或阴影快速变化的地点。
- en: '![](../../OEBPS/Images/CH10_F06_Chaudhury.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH10_F06_Chaudhury.png)'
- en: Figure 10.6 Graph of the input array solid) and output array (dashed) from figure
    [10.3](#fig-conv1d-edge-detection-stride1). The output the input. Edges provide
    vital clues for understanding the signal.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 从图[10.3](#fig-conv1d-edge-detection-stride1)中输入数组（实线）和输出数组（虚线）的图形。输出是输入。边缘提供了理解信号的重要线索。
- en: 10.1.3 One-dimensional convolution as matrix multiplication
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.3 一维卷积作为矩阵乘法
- en: Algebraically, the convolution with a kernel of size 3, stride 1, and valid
    padding can be shown as follows. Let the input array be
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从代数上讲，大小为3、步长为1、有效填充的卷积可以用以下方式表示。设输入数组为
- en: '![](../../OEBPS/Images/AR_x.png) = [*x*[0]   *x*[1]   *x*[2]   *x*[3]   *x*[4]  …  *x*[*n*–3]
      *x*[*n*–2]   *x*[*n*-1]]'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_x.png) = [*x*[0] x*[1] x*[2] x*[3] x*[4] … x*[*n*-3]
    x*[*n*-2] x*[*n*-1]]'
- en: The convolving kernel is a matrix of weights of size 3; let’s denote it as
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积核是一个大小为3的权重矩阵；让我们称它为
- en: '![](../../OEBPS/Images/AR_w.png) = [*w*[0]   *w*[1]   *w*[2]]'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_w.png) = [*w*[0] w*[1] w*[2]]'
- en: 'As shown in figure [10.1](#fig-conv1d-smoothing-stride1), in step 0 of the
    convolution, we place this kernel on the 0th element of the input *x*[0]. Thus,
    *w*[0] falls on *x*[0], *w*[1] falls on *x*[1], and *w*[2] falls on *x*[2]:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[10.1](#fig-conv1d-smoothing-stride1)所示，在卷积的步骤0中，我们将这个核放在输入*x*[0]的0号元素上。因此，*w*[0]落在*x*[0]上，*w*[1]落在*x*[1]上，*w*[2]落在*x*[2]上：
- en: '[**x[0]   x[1]   x[2]**   *x*[3]   *x*[4]  …  *x*[*n*–3]   *x*[*n*–2]   *x*[*n*-1]],'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[**x[0] x[1] x[2]** x*[3] x*[4] … x*[*n*-3] x*[*n*-2] x*[*n*-1]]'
- en: 'where the bold typeface identifies the input elements aligned with kernel weights.
    We multiply elements on corresponding positions and sum them, yielding the 0th
    element of the output *y*[0] = *w*[0]*x*[0] + *w*[1]*x*[1] + *w*[2]*x*[2]. Then
    we shift the kernel by one position (assuming the stride is 1; if the stride were
    2, we would move the kernel two positions, and so on). So *w*[0] falls on *x*[1],
    *w*[1] falls on *x*[2], and *w*[2] falls on *x*[3]:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中粗体字样标识了与核权重对齐的输入元素。我们在对应位置乘以元素并将它们相加，得到输出 *y*[0] 的第 0 个元素 = *w*[0]*x*[0] +
    *w*[1]*x*[1] + *w*[2]*x*[2]。然后我们将核向右移动一个位置（假设步长为 1；如果步长为 2，我们将核移动两个位置，依此类推）。因此
    *w*[0] 落在 *x*[1] 上，*w*[1] 落在 *x*[2] 上，*w*[2] 落在 *x*[3] 上：
- en: '[*x*[0]   **x[1]   x[2]   x[3]   x[4]**  …  *x*[*n*–3]   *x*[*n*–2]   *x*[*n*-1]]'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[*x*[0]   **x[1]   x[2]   x[3]   x[4]**  …  *x*[*n*–3]   *x*[*n*–2]   *x*[*n*-1]]'
- en: 'Again, we multiply elements at corresponding positions and sum them, yielding
    the first element of the output *y*[1] = *w*[0]*x*[1] + *w*[1]*x*[2] + *w*[2]*x*[2].
    Similarly, in the next step, we right-shift the kernel one more time:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们在对应位置乘以元素并将它们相加，得到输出 *y*[1] 的第一个元素 = *w*[0]*x*[1] + *w*[1]*x*[2] + *w*[2]*x*[2]。同样，在下一步中，我们将核向右移动一个位置：
- en: '[*x*[0]   *x*[1]   **x[2]   x[3]   x[4]**  …  *x[n]*]'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[*x*[0]   *x*[1]   **x[2]   x[3]   x[4]**  …  *x[n]*]'
- en: The corresponding output is *y*[2] = *w*[0]*x*[2] + *w*[1]*x*[3] + *w*[2]*x*[4].
    Overall, a stride 1, valid padding convolution of a vector ![](../../OEBPS/Images/AR_x.png)
    with a weight kernel ![](../../OEBPS/Images/AR_w.png) yields the output
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的输出是 *y*[2] = *w*[0]*x*[2] + *w*[1]*x*[3] + *w*[2]*x*[4]。总体而言，步长 1，有效填充卷积的向量
    ![](../../OEBPS/Images/AR_x.png) 与权重核 ![](../../OEBPS/Images/AR_w.png) 产生输出
- en: '![](../../OEBPS/Images/eq_10-01-b.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_10-01-b.png)'
- en: Can you see what is happening? We are effectively taking linear combinations
    (see section [2.9](02.xhtml#sec-lin-dep)) of successive sets of *kernel*_*size*
    here, 3) input elements. In other words, the output is a *moving weighted local
    sum* of the input array elements. Depending on the actual weights, we are extracting
    local properties of the input.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你能看出发生了什么吗？我们实际上在这里正在取连续的 *核* _大小_ 的线性组合（参见第 [2.9](02.xhtml#sec-lin-dep) 节），即
    3 个输入元素。换句话说，输出是输入数组元素的 *移动加权局部和*。根据实际的权重，我们正在提取输入的局部特性。
- en: For *valid* padding, the last output is yielded by
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *有效* 填充，最后一个输出由
- en: '[*x*[0]   *x*[1]   *x*[2]   *x*[3]   *x*[4]  …  **x[n–3]   x[n–2]   x[n-1]**]'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[*x*[0]   *x*[1]   *x*[2]   *x*[3]   *x*[4]  …  **x[n–3]   x[n–2]   x[n-1]**]'
- en: which generates the output
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 生成输出
- en: '*y*[*n* − 3] = *w*[0]*x*[*n* − 3] + *w*[1]*x*[*n* − 2] + *w*[2]*x*[*n* − 1]'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*y*[*n* − 3] = *w*[0]*x*[*n* − 3] + *w*[1]*x*[*n* − 2] + *w*[2]*x*[*n* − 1]'
- en: For the *same* zero padding, the last output is yielded by
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *相同* 的零填充，最后一个输出由
- en: '[*x*[0]   *x*[1]   *x*[2]   *x*[3]   *x*[4]  …   **x[n-1]   0**   **0**]'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[*x*[0]   *x*[1]   *x*[2]   *x*[3]   *x*[4]  …   **x[n-1]   0**   **0**]'
- en: which generates the output
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 生成输出
- en: '*y*[*n*−1] = *w*[0] ⋅ *x*[*n*−1] + *w*[1] ⋅ 0 + *w*[2] ⋅ 0'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*y*[*n*−1] = *w*[0] ⋅ *x*[*n*−1] + *w*[1] ⋅ 0 + *w*[2] ⋅ 0'
- en: In section [8.3.1](../Text/08.xhtml#sec-linlayer-matmult), we saw that the FC
    (aka linear) layer can be expressed as a multiplication of the input vector by
    a weight matrix. Now, we will express convolution as matrix-vector multiplication.
    The weight matrix has a block-diagonal structure, as shown in equation [10.2](#eq-conv1d-as-malt-mult-k3s1valid).
    It is a special case of equation [8.8](../Text/08.xhtml#eq-MLP-weight-matrix).
    As such, the forward propagation equation [8.7](../Text/08.xhtml#eq-linlayer-forwardprop)
    and backpropagation equations [8.31](../Text/08.xhtml#eq-auxvar-vector) and [8.33](../Text/08.xhtml#eq-partialderiv-loss-wt-vector)
    are still applicable. Thus, forward propagation and backpropagation training)
    through convolution proceeds exactly as with FC layers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [8.3.1](../Text/08.xhtml#sec-linlayer-matmult) 节中，我们看到了全连接层（也称为线性层）可以表示为输入向量与权重矩阵的乘积。现在，我们将卷积表示为矩阵-向量乘积。权重矩阵具有分块对角结构，如方程式
    [10.2](#eq-conv1d-as-malt-mult-k3s1valid) 所示。它是方程式 [8.8](../Text/08.xhtml#eq-MLP-weight-matrix)
    的一个特例。因此，前向传播方程式 [8.7](../Text/08.xhtml#eq-linlayer-forwardprop) 和反向传播方程式 [8.31](../Text/08.xhtml#eq-auxvar-vector)
    以及 [8.33](../Text/08.xhtml#eq-partialderiv-loss-wt-vector) 仍然适用。因此，通过卷积进行的前向传播和反向传播训练与
    FC 层完全相同。
- en: 'Equation [10.2](#eq-conv1d-as-malt-mult-k3s1valid) expresses *kernel*_*size*
    3, stride 1, valid padding convolution as a multiplication of a weight matrix
    *W* with input vector ![](../../OEBPS/Images/AR_x.png):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 [10.2](#eq-conv1d-as-malt-mult-k3s1valid) 表达了 *核* _大小_ 3，步长 1，有效填充卷积作为权重矩阵
    *W* 与输入向量 ![](../../OEBPS/Images/AR_x.png) 的乘积：
- en: '![](../../OEBPS/Images/eq_10-02.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_10-02.png)'
- en: Equation 10.2
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 10.2
- en: 'Notice the *sparse*, *block-diagonal* nature of the weight matrix in equation
    [10.2](#eq-conv1d-as-malt-mult-k3s1valid). This is characteristic of convolution
    weight matrices. Each row contains all the kernel weights at contiguous positions.
    The size of the kernel is typically much less than the input vector size. Of course,
    for matrix multiplication to be possible, the number of columns in the weight
    matrix must match the size of the input vector. Thus, there are many vacant positions
    in the row besides those occupied by kernel weights. We fill these vacant elements
    with zeros. Each row of the weight matrix thus has all the kernel weights appearing
    somewhere contiguously, and the rest of the row is filled with zeros. *The position
    of kernel weights shifts rightward with each successive row*. This is what gives
    the block-diagonal appearance to the weight matrix. It also simulates the sliding
    of the kernel required for convolution. Each row represents a specific slide stop
    and generates one element of the output vector. Since the kernel is at a fixed
    position of the row and all other row elements are zero, only the input elements
    corresponding to the kernel positions are picked up. Other input elements are
    multiplied by zero: that is, they are ignored.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意方程式 [10.2](#eq-conv1d-as-malt-mult-k3s1valid) 中权重矩阵的 *稀疏*、*块对角*性质。这是卷积权重矩阵的特征。每一行包含所有在连续位置的核权重。核的大小通常远小于输入向量的大小。当然，为了进行矩阵乘法，权重矩阵的列数必须与输入向量的大小相匹配。因此，除了占据核权重的位置外，还有许多空位。我们用零填充这些空位。因此，权重矩阵的每一行都有所有核权重在某个连续位置出现，其余行则用零填充。*核权重的位置随着每一行的连续而向右移动*。这就是给权重矩阵带来块对角外观的原因。它也模拟了卷积所需的核滑动。每一行代表一个特定的滑动停止点，并生成输出向量中的一个元素。由于核位于行的固定位置，而其他行元素都是零，因此只有与核位置对应的输入元素被选中。其他输入元素乘以零：即，它们被忽略。
- en: 'Equation [10.2](#eq-conv1d-as-malt-mult-k3s1valid) depicts a stride of 1. For
    instance, if the stride is 2, the kernel weights will shift by two positions in
    successive rows. This is shown in equation [10.3](#eq-conv1d-as-malt-mult-k3s2valid):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 [10.2](#eq-conv1d-as-malt-mult-k3s1valid) 描述了步长为 1。例如，如果步长为 2，核权重将在连续行中移动两个位置。这显示在方程式
    [10.3](#eq-conv1d-as-malt-mult-k3s2valid) 中：
- en: '![](../../OEBPS/Images/eq_10-03.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_10-03.png)'
- en: Equation 10.3
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 10.3
- en: Note that while equation [10.3](#eq-conv1d-as-malt-mult-k3s2valid) provides
    a conceptual matrix-multiplication view of convolution, it is not the most efficient
    way of implementing convolution. PyTorch and other deep learning software have
    extremely efficient ways of implementing convolution.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然方程式 [10.3](#eq-conv1d-as-malt-mult-k3s2valid) 提供了卷积的概念矩阵乘法视图，但这并不是实现卷积的最高效方式。PyTorch
    和其他深度学习软件都有非常高效的卷积实现方式。
- en: 10.1.4 PyTorch- One-dimensional convolution with custom weights
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.4 PyTorch - 使用自定义权重的单维卷积
- en: We have discussed the convolution of a 1D input vector with two specific 1D
    kernels. We have seen that a kernel with uniform weights, such as ![](../../OEBPS/Images/eq_10-03-a2.png),
    results in local smoothing of the input vector, whereas a kernel with antisymmetric
    weights, such as ![](../../OEBPS/Images/eq_10-03-b2.png), results in an output
    vector that spikes at the edge locations in the input vector. Now we will see
    how to set the weights of a 1D kernel and perform 1D convolution with that kernel
    in PyTorch.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了 1D 输入向量与两个特定 1D 核的卷积。我们看到了，具有均匀权重的核，例如 ![图片](../../OEBPS/Images/eq_10-03-a2.png)，会导致输入向量的局部平滑，而具有反对称权重的核，例如
    ![图片](../../OEBPS/Images/eq_10-03-b2.png)，会导致输出向量在输入向量的边缘位置出现峰值。现在我们将看到如何在 PyTorch
    中设置 1D 核的权重，并使用该核进行 1D 卷积。
- en: NOTE This is *not* a typical PyTorch operation. The more typical operation is
    to create a neural network with a convolution layer (where we specify the size,
    stride, and padding but not the weights) and then train the network so that the
    weights are learned. We usually don’t care about the exact values of the learned
    weight. Then why are we discussing how to set the weights of a kernel in PyTorch?
    Mainly to show how convolution works in PyTorch, the various parameters of the
    convolution object, and so forth.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：这不是典型的 PyTorch 操作。更典型的操作是创建一个具有卷积层（我们指定大小、步长和填充，但不指定权重）的神经网络，然后训练网络以便学习权重。我们通常不关心学习到的权重的确切值。那么我们为什么要讨论在
    PyTorch 中如何设置核的权重呢？主要是为了展示卷积在 PyTorch 中的工作方式，卷积对象的各个参数等等。
- en: Listing 10.1 PyTorch code for 1D local averaging convolution
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.1 PyTorch 1D 局部平均卷积的代码
- en: '[PRE0]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Instantiates a noisy input vector. Follows equation *y* = 5*x*
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ① 实例化一个带噪声的输入向量。遵循方程 *y* = 5*x*
- en: ② Instantiates the weights of the convolutional kernel
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ② 实例化卷积核的权重
- en: ③ PyTorch expects inputs and weights to be of the form *N* × *C* × *L*, where
    *N* is the batch size, *C* is the number of channels, and *L* is the sequence
    length. Here, *N* and *C* are 1\. torch.unsqueeze converts our *L*-length vector
    into a 1 × 1 × *L* tensor.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ③ PyTorch 预期输入和权重为 *N* × *C* × *L* 的形式，其中 *N* 是批大小，*C* 是通道数，*L* 是序列长度。在这里，*N*
    和 *C* 是 1。torch.unsqueeze 将我们的 *L*-长度向量转换为 1 × 1 × *L* 张量。
- en: ④ Instantiates the smoothing kernel
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 实例化平滑核
- en: ⑤ Sets the kernel weights
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 设置核权重
- en: ⑥ Instructs PyTorch to not compute gradients since we currently don’t require
    them
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 指示 PyTorch 不计算梯度，因为我们目前不需要它们
- en: ⑦ Runs the convolution
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 执行卷积
- en: Listing 10.2 PyTorch code for 1D edge detection
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.2 PyTorch 1D 边缘检测的代码
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Instantiates the input vector with edges
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ① 实例化带有边缘的输入向量
- en: ② Instantiates the weights of the edge-detection kernel
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ② 实例化边缘检测核的权重
- en: ③ Converts the inputs and weights to 1 × 1× *L*
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将输入和权重转换为 1 × 1 × *L*
- en: ④ Instantiates the edge-detection kernel
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 实例化边缘检测核
- en: ⑤ Sets the kernel weights
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 设置核权重
- en: ⑥ Instructs PyTorch to not compute gradients since we currently don’t require
    them
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 指示 PyTorch 不计算梯度，因为我们目前不需要它们
- en: ⑦ Runs the convolution
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 执行卷积
- en: These listings show how to perform 1D convolution in PyTorch using the `torch.nn.`
    `Conv1d` class. This is typically used in larger neural networks like those in
    subsequent chapters. We can alternatively use `torch.nn.functional.conv1d` to
    directly invoke the mathematical convolution operation. This takes input and weight
    tensors and returns the convolved output tensor, as shown in listing 10.3.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些列表展示了如何在 PyTorch 中使用 `torch.nn.Conv1d` 类执行 1D 卷积。这通常用于更大的神经网络，如后续章节中的那些。我们还可以使用
    `torch.nn.functional.conv1d` 直接调用数学卷积操作。这需要输入和权重张量，并返回卷积输出张量，如列表 10.3 所示。
- en: Listing 10.3 PyTorch code directly invoking the convolution function
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.3 直接调用卷积函数的 PyTorch 代码
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Instantiates the input tensor
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ① 实例化输入张量
- en: ② Instantiates the weight tensor
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ② 实例化权重张量
- en: ③ Converts the inputs and weights to 1 × 1 × *L*
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将输入和权重转换为 1 × 1 × *L*
- en: ④ Runs the convolution
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 执行卷积
- en: 10.2 Convolution output size
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 卷积输出大小
- en: Consider a kernel of size *k* sliding over an input of size *n* with stride
    *s*. Given a kernel of size *k*, if the left end is at index *l*, the right end
    is at index *l* + (*k* − 1). Each shift advances the left (as well as the right)
    end of the kernel by *s*. If the initial position of the kernel was at index 0,
    then after *m* shifts, the left end is at *ms*. Correspondingly, the right end
    is at *ms* + (*k* − 1). Assuming valid padding, this right-end position must be
    less than or equal to (*n* − 1) (the last valid position of the input array).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个大小为 *k* 的核在大小为 *n* 的输入上滑动，步长为 *s*。给定一个大小为 *k* 的核，如果左端在索引 *l*，则右端在索引 *l*
    + (*k* − 1)。每次平移将核的左端（以及右端）向前移动 *s*。如果核的初始位置在索引 0，那么经过 *m* 次平移后，左端在 *ms*。相应地，右端在
    *ms* + (*k* − 1)。假设有效填充，这个右端位置必须小于或等于 (*n* − 1)（输入数组的最后一个有效位置）。
- en: How many times can we shift before the kernel spills out of the input? In other
    words, what is the maximum possible value of *m*, such that
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以平移多少次，直到核溢出输入？换句话说，*m* 的最大可能值是多少，使得
- en: '*ms* + (*k* − 1) ≤ (*n* − 1)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*ms* + (*k* − 1) ≤ (*n* − 1)'
- en: The answer is
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是
- en: '![](../../OEBPS/Images/eq_10-03-c.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_10-03-c.png)'
- en: But each shift produces one output value. The output size of the convolution,
    *o*, with valid padding, is *m* + 1 (the plus one is to account for the initial
    position). Hence,
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 但每次平移都产生一个输出值。卷积的有效填充输出大小 *o* 是 *m* + 1（加一是为了考虑到初始位置）。因此，
- en: '![](../../OEBPS/Images/eq_10-03-d.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_10-03-d.png)'
- en: If we are zero-padding with *p* zeroes on each side of the input, the input
    size becomes *n* + 2*p*. The corresponding output size is
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在输入的每侧填充 *p* 个零，则输入大小变为 *n* + 2*p*。相应的输出大小是
- en: '![](../../OEBPS/Images/eq_10-04.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_10-04.png)'
- en: Equation 10.4
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 10.4
- en: This can be extended to an arbitrary number of dimensions by repeating it for
    each dimension.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过对每个维度重复执行来扩展到任意数量的维度。
- en: '10.3 Two-dimensional convolution: Graphical and algebraic view'
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 二维卷积：图形和代数视图
- en: It is often said that an image is worth a thousand words. What is an image?
    As far as deep learning is concerned, it is a discrete two-dimensional entity—a
    2D array of pixel values describing a scene at a fixed time. Each pixel represents
    a color intensity value. The color value can be a single element representing
    a gray level, or it can be three-dimensional, corresponding to R(ed), G(reen),
    B(lue) intensity values. (You may want to revisit section [2.3](02.xhtml#sec-matrices)
    before proceeding.)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 人们常说，一张图片胜过千言万语。那么什么是图片呢？就深度学习而言，它是一个离散的二维实体——一个描述固定时间场景的像素值二维数组。每个像素代表一个颜色强度值。颜色值可以是一个表示灰度的单元素，也可以是三维的，对应于R(红)、G(绿)、B(蓝)强度值。（在继续之前，你可能需要回顾第[2.3](02.xhtml#sec-matrices)节。）
- en: '![](../../OEBPS/Images/CH10_F07_Chaudhury.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F07_Chaudhury.png)'
- en: Figure 10.7 2D convolution with a *local averaging kernel* of size [3,3], stride
    [1,1], and valid padding. Each pixel is shown as a small rectangle, with the pixel’s
    gray level written in the rectangle. The shaded order. Successive steps indicate
    slide stops. For each pixel that is overlapped by the kernel, the weight of the
    kernel element falling on it is written in small font.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 2D卷积使用大小为[3,3]，步长为[1,1]，有效填充的局部平均核。每个像素以一个小矩形表示，矩形中写有像素的灰度值。阴影顺序。连续步骤表示滑动停止。对于被核覆盖的每个像素，落在其上的核元素的权重以小字体写出。
- en: At the time of this writing, image analysis is the most popular application
    of convolution. These applications use convolution to extract local patterns.
    How do we do this? In particular, can we rasterize the image (thus converting
    it into a vector) and use one-dimensional convolution?
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，图像分析是卷积最流行的应用。这些应用使用卷积来提取局部模式。我们是如何做到这一点的呢？特别是，我们能否将图像光栅化（从而将其转换为矢量）并使用一维卷积？
- en: The answer is *no*. To see why, examine figure [10.7](#fig-conv2d-smoothing-stride1).
    What is the spatial neighborhood of the pixel at location (*x* = 0, *y* = 0)?
    If we define the *neighborhood* of a pixel as the set of pixels within a Manhattan
    distance of [2,2] with that pixel at the top-left corner, the neighborhood of
    (*x* = 0, *y* = 0) consists of the set of pixels covered by the shaded rectangle
    in figure [10.7](#fig-conv2d-smoothing-stride1), step 0. But these pixels *will
    not be neighboring elements in a rasterized array representation of the image*.
    For instance, the pixel (*x* = 0, *y* =1), with value 6, is the fifth element
    in the rasterized array and, as such, will *not* be considered a neighbor of (*x*
    = 0, *y* = 0), which is the 0th element in the rasterized array. Two-dimensional
    neighborhoods are *not* preserved by rasterization. So, two-dimensional convolution
    has to be a specialized operation beyond merely rasterizing 2D arrays into 1D
    and applying 1D convolution.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是*不*。要了解原因，请查看图[10.7](#fig-conv2d-smoothing-stride1)。像素在位置(*x* = 0, *y* =
    0)的空间邻域是什么？如果我们定义像素的*邻域*为包含该像素在左上角的曼哈顿距离为[2,2]的像素集合，那么(*x* = 0, *y* = 0)的邻域由图[10.7](#fig-conv2d-smoothing-stride1)中阴影矩形覆盖的像素集合组成，步骤0。但这些像素*不会是图像光栅化数组表示中的相邻元素*。例如，像素(*x*
    = 0, *y* =1)，其值为6，是光栅化数组中的第五个元素，因此*不会*被视为(*x* = 0, *y* = 0)的邻居，(*x* = 0, *y* =
    0)是光栅化数组中的第0个元素。二维邻域在光栅化过程中*不会*被保留。因此，二维卷积必须是一个超越仅仅将2D数组光栅化到1D并应用1D卷积的专门操作。
- en: Euclidean distance and Manhattan distance
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离和曼哈顿距离
- en: Euclidean distance measures the straight line distance between two points, whereas
    Manhattan distance measures the distance between two points with a constraint
    that you can only walk parallel to the axes just like on the streets of Manhattan).
    Let’s look at an example.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离衡量两点之间的直线距离，而曼哈顿距离衡量两点之间的距离，但有一个约束，即你只能沿着轴平行行走，就像在曼哈顿的街道上一样）。让我们看看一个例子。
- en: Consider two points A (3, 3) and B (6, 7). The Euclidean distance between A
    and B is the length of the line segment AB, which can be computed as √((6 - 3)²
    + (7 - 3)²) = 5. The Manhattan distance between A and B is (6−3) + (7−3) = 3 +
    4 = 7. In this chapter, we represent the Manhattan distance as [3, 4] to capture
    the distance along each axis separately.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两个点A (3, 3)和B (6, 7)。A和B之间的欧几里得距离是线段AB的长度，可以计算为√((6 - 3)² + (7 - 3)²) = 5。A和B之间的曼哈顿距离是(6−3)
    + (7−3) = 3 + 4 = 7。在本章中，我们用[3, 4]表示曼哈顿距离，以分别捕捉每个轴上的距离。
- en: 'The best way to visualize 2D convolution is to imagine a wall (the input image)
    over which a tile the kernel) is sliding:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化二维卷积的最佳方式是想象一块墙（输入图像）上滑过一个瓦片（核）：
- en: In figures [10.7](#fig-conv2d-smoothing-stride1), [10.8](#fig-conv2d-smoothing-stride2)
    and [10.9](#fig-conv2d-edge-detection-stride1), the shaded rectangle depicts the
    tile (kernel), while the larger white rectangle containing it depicts the wall
    (input image). Successive steps in the figure represent successive positions (aka
    slide stops) of the sliding tile. Notice that the shaded rectangle occupies a
    different position in each step.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图 [10.7](#fig-conv2d-smoothing-stride1)、[10.8](#fig-conv2d-smoothing-stride2)
    和 [10.9](#fig-conv2d-edge-detection-stride1) 中，阴影矩形表示瓦片（核），而包含它的较大白色矩形表示墙（输入图像）。图中的连续步骤代表滑动瓦片的连续位置（也称为滑动停止）。请注意，阴影矩形在每个步骤中都占据不同的位置。
- en: Tiles in successive positions during sliding can overlap. They overlap by varying
    amounts in figures [10.7](#fig-conv2d-smoothing-stride1), [10.8](#fig-conv2d-smoothing-stride2),
    and [10.9](#fig-conv2d-edge-detection-stride1).
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在滑动过程中，连续位置的瓦片可以重叠。在图 [10.7](#fig-conv2d-smoothing-stride1)、[10.8](#fig-conv2d-smoothing-stride2)
    和 [10.9](#fig-conv2d-edge-detection-stride1) 中，它们重叠的程度不同。
- en: The wall and the tile are discrete 2D arrays in reality. At each slide stop,
    the tile array elements rest on a subset of wall array elements.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在现实中，墙和瓦片是离散的二维数组。在每个滑动停止时，瓦片数组元素位于墙数组元素的一个子集上。
- en: We multiply each input array element by the kernel element resting on it and
    sum the products. This is equivalent to taking a weighted sum of the input (wall)
    elements that fall under the current position of the kernel (tile), with the kernel
    elements serving as weights. This weighted sum is emitted as a single output element.
    One output element results from each slide stop of the tile. As the tile slides
    over the entire wall, left to right and top to bottom, a 2D output array is generated.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将每个输入数组元素与其上方的核元素相乘，并将乘积相加。这相当于对输入（墙）元素进行加权求和，这些元素落在当前核（瓦片）的位置下，核元素作为权重。这个加权求和作为一个单独的输出元素发出。每个瓦片滑动停止都会产生一个输出元素。当瓦片在整面墙上从左到右、从上到下滑动时，就生成了一个二维输出数组。
- en: 'In 2D convolution, the input array, kernel size, and stride are all 2D vectors.
    Just as in 1D convolution, the following entities are defined for 2D convolution:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维卷积中，输入数组、核大小和步长都是二维向量。正如在 1D 卷积中一样，以下实体在二维卷积中定义：
- en: '![](../../OEBPS/Images/CH10_F08_Chaudhury.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH10_F08_Chaudhury.png)'
- en: Figure 10.8 2D convolution with a *local averaging kernel* of size [3,3], stride
    [2,2], and valid padding
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 使用大小为 [3,3]、步长为 [2,2] 的 *局部平均核* 的二维卷积
- en: '![](../../OEBPS/Images/CH10_F09_Chaudhury.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH10_F09_Chaudhury.png)'
- en: Figure 10.9 2D convolution with an *edge-detection kernel* of size 2, stride
    1, and valid padding. Not all slide stops that is, steps) are shown. Notice how
    the output is zero at a uniform location but spikes when one-half of the kernel
    falls on low values while the other half falls on high values.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9 使用大小为 2 的 *边缘检测核*、步长 1 和有效填充的二维卷积。并非所有滑动停止（即步数）都显示出来。注意输出在均匀位置为零，但在核的一半落在低值而另一半落在高值时会出现峰值。
- en: '*Input*—A two-dimensional array. We typically use the symbol [*H*, *W*] indicating
    the height and width of the array, respectively) to represent the input array
    size in 2D convolution. In figure [10.7](#fig-conv2d-smoothing-stride1), *H* =
    5, *W* = 5.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入*—一个二维数组。我们通常使用符号 [*H*, *W*]（分别表示数组的高度和宽度）来表示二维卷积中输入数组的大小。在图 [10.7](#fig-conv2d-smoothing-stride1)
    中，*H* = 5，*W* = 5。'
- en: '*Output*—A two-dimensional array. We typically use the symbol ![](../../OEBPS/Images/AR_osm.png)
    = [*o[H]*, *o[W]*] to represent output array dimensions in 2D convolution. For
    instance, in figure [10.7](#fig-conv2d-smoothing-stride1), ![](../../OEBPS/Images/AR_osm.png)
    = [3,3]. In section [10.2](#sec-conv1d-out-size), we saw how to compute the output
    size for a single dimension. We have to repeat that computation once per dimension
    to obtain the output size in higher dimensions.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出*—一个二维数组。我们通常使用符号 ![](../../OEBPS/Images/AR_osm.png) = [*o[H]*, *o[W]*]
    来表示二维卷积中输出数组的维度。例如，在图 [10.7](#fig-conv2d-smoothing-stride1) 中，![](../../OEBPS/Images/AR_osm.png)
    = [3,3]。在 [10.2](#sec-conv1d-out-size) 节中，我们看到了如何计算单维度的输出大小。我们必须对每个维度重复该计算一次，以获得高维度的输出大小。'
- en: '*Kernel*—A small two-dimensional array of weights whose size is a parameter
    of the convolution. We typically use the symbol ![](../../OEBPS/Images/AR_k.png)
    = [*k[H]*, *k[W]*] to represent the kernel size (height, width) in 2D convolution.
    If (*x*, *y*) denotes the current position of the top-left corner of the 2D kernel,
    the bottom-right corner is at (*x* + *k[W]* − 1, *y* + *k[H]* − 1). In figure
    [10.7](#fig-conv2d-smoothing-stride1), ![](../../OEBPS/Images/AR_k.png) = [3,3];
    in figure [10.9](#fig-conv2d-edge-detection-stride1), ![](../../OEBPS/Images/AR_k.png)
    = [2,2].'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*内核*—一个小的二维权重数组，其大小是卷积的一个参数。我们通常使用符号 ![](../../OEBPS/Images/AR_k.png) = [*k[H]*,
    *k[W]*] 来表示二维卷积中的内核大小（高度，宽度）。如果 (*x*, *y*) 表示二维内核左上角当前的位置，则右下角位于 (*x* + *k[W]*
    − 1, *y* + *k[H]* − 1)。在图 [10.7](#fig-conv2d-smoothing-stride1) 中，![](../../OEBPS/Images/AR_k.png)
    = [3,3]；在图 [10.9](#fig-conv2d-edge-detection-stride1) 中，![](../../OEBPS/Images/AR_k.png)
    = [2,2]。'
- en: '*Stride*—The number of input elements over which the kernel slides on completing
    a single step. We typically use the symbol ![](../../OEBPS/Images/AR_s.png) =
    [*s[H]*, *s[W]*] to represent the stride size (height, width) in 2D convolution.
    If (*x*, *y*) denotes the current position of the top-left corner of the 2D kernel,
    the next shift will put the top-left corner of the kernel at (*x* + *s[W]*, *y*)
    see, for instance, the transition from step 0 to step 1 or step 1 to step 2 in
    figure [10.7](#fig-conv2d-smoothing-stride1)). If this transition causes portions
    of the tile to fall outside the wall—that is, *x* + *s[W]* ≥ *W*—we set the next
    slide position such that the top-left corner of the kernel falls on (0, *y* +
    1) (see, for instance, the transition from step 2 to step 3 or step 5 to step
    6 in figure [10.7](#fig-conv2d-smoothing-stride1)). If *y* + *s[H]* ≥ *H*, we
    stop sliding. Stride size is a parameter of the convolution. In figure [10.7](#fig-conv2d-smoothing-stride1),
    ![](../../OEBPS/Images/AR_s.png) = [1,1]; in figure [10.8](#fig-conv2d-smoothing-stride2),
    stride is ![](../../OEBPS/Images/AR_s.png) = [2,2]. As in the 1D case, a stride
    of ![](../../OEBPS/Images/AR_s.png) = [1,1] means there is a slide stop at each
    successive element of the input. So, the output has roughly the same number of
    elements as the input (they may not be exactly equal because of padding). A stride
    of ![](../../OEBPS/Images/AR_s.png) = [2,2] means each row of the input will yield
    half the row size worth of output elements, and each column will generate half
    the column size worth of output elements. Hence, the output size is roughly a
    quarter of the input size. Overall, the reduction factor of the input-to-output
    size roughly matches the product of the elements in the stride vector.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步长*—内核在完成单步滑动后覆盖的输入元素数量。我们通常使用符号 ![](../../OEBPS/Images/AR_s.png) = [*s[H]*,
    *s[W]*] 来表示二维卷积中的步长大小（高度，宽度）。如果 (*x*, *y*) 表示二维内核左上角当前的位置，则下一次移动将使内核的左上角位于 (*x*
    + *s[W]*, *y*)。例如，在图 [10.7](#fig-conv2d-smoothing-stride1) 中，从步骤 0 到步骤 1 或步骤 1
    到步骤 2 的转换。如果这种转换导致瓦片的部分超出墙壁——即 *x* + *s[W]* ≥ *W*——我们将下一次滑动位置设置为内核的左上角落在 (0, *y*
    + 1) 上。例如，在图 [10.7](#fig-conv2d-smoothing-stride1) 中，从步骤 2 到步骤 3 或步骤 5 到步骤 6 的转换。如果
    *y* + *s[H]* ≥ *H*，我们停止滑动。步长大小是卷积的一个参数。在图 [10.7](#fig-conv2d-smoothing-stride1)
    中，![](../../OEBPS/Images/AR_s.png) = [1,1]；在图 [10.8](#fig-conv2d-smoothing-stride2)
    中，步长是 ![](../../OEBPS/Images/AR_s.png) = [2,2]。与一维情况一样，步长 ![](../../OEBPS/Images/AR_s.png)
    = [1,1] 表示在输入的每个连续元素处都有一个滑动停止。因此，输出元素的数量大致与输入相同（它们可能不完全相等，因为填充）。步长 ![](../../OEBPS/Images/AR_s.png)
    = [2,2] 表示输入的每一行将产生一半行大小的输出元素，每一列将产生一半列大小的输出元素。因此，输出大小大致是输入大小的四分之一。总的来说，输入到输出大小的减少因子大致等于步长向量中元素的乘积。'
- en: '*Padding*—As the kernel slides toward the extremity of the input array along
    the width and/or height, parts of it may fall outside the input array. In other
    words, part of the kernel falls over ghost input elements. As in the 1D case,
    we deal with this via padding. Padding strategies in 2D convolution are straightforward
    extensions from 1D:'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*填充*—当内核沿着宽度或高度滑动到输入数组的边缘时，其部分可能超出输入数组。换句话说，内核的一部分覆盖了幽灵输入元素。与一维情况一样，我们通过填充来处理这种情况。二维卷积中的填充策略是一维的简单扩展：'
- en: '*Valid padding*—We stop sliding whenever any element of the kernel falls outside
    the input array, either in width and/or in height. No ghost input elements are
    involved; the *entire* kernel always falls on valid input elements (hence the
    name *valid*).'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*有效填充*—每当内核的任何元素超出输入数组时（无论是宽度还是高度），我们就停止滑动。没有涉及幽灵输入元素；内核的*整个*部分始终落在有效输入元素上（因此得名*有效*）。'
- en: '*Same (zero) padding*—Here, we do not want to stop early. We keep sliding as
    long as the top-left corner of the kernel falls on a valid input position. So,
    if the stride is 1, 1, the output size will match the input size (hence the name
    *same*). When we slide near the end of an input row (right extremity of the input),
    the right-most columns of the kernel will fall outside the input. Similarly, when
    we slide toward the bottom of the input, the bottom rows of the kernel will fall
    outside. If we slide near the bottom-right corner of the input, both the right-most
    columns and bottommost rows will fall outside the input. The rule is that all
    ghost input values outside the true boundaries of the input array are replaced
    by zeros.'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*相同（零）填充*——在这里，我们不想提前停止。只要内核的左上角落在有效的输入位置上，我们就继续滑动。所以，如果步长是1，1，输出大小将与输入大小匹配（因此得名*same*）。当我们接近输入行的末尾（输入的右端点）时，内核的最右侧列将超出输入范围。同样，当我们向输入的底部滑动时，内核的底部行将超出范围。如果我们接近输入的右下角，最右侧的列和最底部的行都将超出输入范围。规则是，所有在输入数组真实边界之外的幽灵输入值都被零替换。'
- en: Let’s denote the input image domain by *S*. It is a 2D grid whose domain is
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用*S*表示输入图像域。它是一个二维网格，其域是
- en: '*S* = [0, *H* − 1] × [0, *W* − 1]'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*S* = [0, *H* − 1] × [0, *W* − 1]'
- en: 'Every point in *S* is a pixel with a color value (which can be a scalar—a gray-level
    value—or a vector of three values, R, G, B. On this grid of input points, we define
    a subgrid *S[o]* of output points. *S[o]* is obtained from *S* by applying stride-based
    stepping on the input. Assuming ![](../../OEBPS/Images/AR_s.png) = [*s[H]*, *s[W]*]
    denotes the 2D stride vector, the first slide stop has the top-left corner of
    the brick at ![](../../OEBPS/Images/AR_p.png)[0] ≡ (*y* = 0, *x* = 0). The next
    slide stop is at ![](../../OEBPS/Images/AR_p.png)[1] ≡ (*y* = 0, *x* = *s[W]*),
    and the next is at ![](../../OEBPS/Images/AR_p.png)[2] ≡ (*y* = 0, *x* = 2*s[W]*).
    When we reach the right end, we increment *y*. Overall, the output grid consists
    of the slide-stop points where the top-left corner of the kernel (brick) rests
    as it sweeps over the input volume: *S[o]* = {![](../../OEBPS/Images/AR_p.png)[0],
    ![](../../OEBPS/Images/AR_p.png)[1], … }. There is an output for each point in
    *S[o]*.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*S*中的每个点都是一个具有颜色值（可以是标量——灰度值或三个值的向量，R、G、B）的像素。在这个输入点网格上，我们定义了一个输出点子网格*S[o]*。*S[o]*是通过在输入上应用基于步长的移动从*S*获得的。假设![](../../OEBPS/Images/AR_s.png)
    = [*s[H]*, *s[W]*]表示2D步长向量，第一个滑动停止点在![](../../OEBPS/Images/AR_p.png)[0] ≡ (*y*
    = 0, *x* = 0)。下一个滑动停止点在![](../../OEBPS/Images/AR_p.png)[1] ≡ (*y* = 0, *x* = *s[W]*)，下一个是![](../../OEBPS/Images/AR_p.png)[2]
    ≡ (*y* = 0, *x* = 2*s[W]*)。当我们到达右侧时，我们增加*y*。总的来说，输出网格由内核（砖块）在输入体积上滑动时其左上角停留的滑动停止点组成：*S[o]*
    = {![](../../OEBPS/Images/AR_p.png)[0], ![](../../OEBPS/Images/AR_p.png)[1], …
    }。*S[o]*中的每个点都有一个输出。'
- en: 'The kernel also has two dimensions (in practice, it has two more dimensions
    corresponding to the input channels and batch—we are ignoring them now for simplicity—as
    discussed in section [10.3.3](#sec-pytorch-2d-conv)). Equation [10.5](#eq-2dconv-output)
    shows how a single output value is generated in 2D convolution. *X* denotes input,
    *Y* denotes output, and *W* denote kernel weights:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 内核也有两个维度（实际上，它对应于输入通道和批次的两个额外维度——为了简单起见，我们现在忽略它们——如第[10.3.3](#sec-pytorch-2d-conv)节所述）。方程[10.5](#eq-2dconv-output)展示了如何在二维卷积中生成单个输出值。*X*表示输入，*Y*表示输出，*W*表示内核权重：
- en: '![](../../OEBPS/Images/eq_10-05.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_10-05.png)'
- en: Equation 10.5
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 方程10.5
- en: Note that the kernel (tile) has its origin on *X*[*y*, *x*]. Its dimensions
    are (*k[H]*, *k[W]*). So, it covers all input pixels in the domain [*y*..(*y*
    + *k[H]*)] × [*x*..(*x* + *k[W]*)]. These are the pixels participating in equation
    [10.5](#eq-2dconv-output). Each of these input pixels is multiplied by the kernel
    element covering it. Match equation [10.5](#eq-2dconv-output) with figures [10.7](#fig-conv2d-smoothing-stride1),
    [10.8](#fig-conv2d-smoothing-stride2), and [10.9](#fig-conv2d-edge-detection-stride1).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，内核（瓦片）的起点在*X*[*y*, *x*]。其维度是(*k[H]*, *k[W]*)。因此，它覆盖了域[*y*..(*y* + *k[H]*)]
    × [*x*..(*x* + *k[W]*)]中的所有输入像素。这些是参与方程[10.5](#eq-2dconv-output)的像素。每个输入像素都乘以覆盖它的内核元素。将方程[10.5](#eq-2dconv-output)与图[10.7](#fig-conv2d-smoothing-stride1)、[10.8](#fig-conv2d-smoothing-stride2)和[10.9](#fig-conv2d-edge-detection-stride1)进行匹配。
- en: 10.3.1 Image smoothing via 2D convolution
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 通过二维卷积进行图像平滑
- en: In section [10.1.1](#sec-1d_local_smoothing), we discussed one-dimensional local
    smoothing. We observed how it gets rid of local fluctuations so that longer-term
    patterns are discernible more cleanly. The same thing happens in two dimensions.
    Figure [10.10](#fig-conv2d-smoothing-example-input) shows an image with some text
    written on a background with salt-and-pepper noise. The noise has no semantic
    significance; it is the text that needs to be analyzed (perhaps via optical character
    recognition). We can eliminate the noise via 2D convolution using a kernel with
    uniform weights, such as
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [10.1.1](#sec-1d_local_smoothing) 节中，我们讨论了一维局部平滑。我们观察到它如何消除局部波动，以便更清晰地识别长期模式。在二维中也会发生同样的事情。图
    [10.10](#fig-conv2d-smoothing-example-input) 显示了一幅在带有盐和胡椒噪声的背景上写有文字的图像。噪声没有语义意义；需要分析的是文字（可能通过光学字符识别）。我们可以通过使用具有均匀权重的核（如）进行二维卷积来消除噪声。
- en: '![](../../OEBPS/Images/eq_10-05-a.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_10-05-a.png)'
- en: The resulting denoised/smooth image is shown in figure [10.11](#fig-conv2d-smoothing-example-vert).
    What does the uniform kernel do? To see that, look at figure [10.8](#fig-conv2d-smoothing-stride2).
    It should be obvious that the kernel causes each output pixel to be a weighted
    local average of the neighboring 3 × 3 input pixels.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的去噪/平滑图像显示在图 [10.11](#fig-conv2d-smoothing-example-vert) 中。均匀核的作用是什么？为了了解这一点，请看图
    [10.8](#fig-conv2d-smoothing-stride2)。很明显，核使得每个输出像素成为相邻 3 × 3 输入像素的加权局部平均值。
- en: '![](../../OEBPS/Images/CH10_F10a_Chaudhury.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F10a_Chaudhury.jpg)'
- en: (a) Input image
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 输入图像
- en: '![](../../OEBPS/Images/CH10_F10b_Chaudhury.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F10b_Chaudhury.jpg)'
- en: (b) Smoothed/denoised output image
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 平滑/去噪输出图像
- en: Figure 10.10 Denoising/smoothing a noisy image by applying 2D convolution ![](../../OEBPS/Images/eq_10-05-b2.png)
    to figure [10.11a](#fig-conv2d-edgedetect-example-input)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10 通过应用 2D 卷积 ![](../../OEBPS/Images/eq_10-05-b2.png) 到图 [10.11a](#fig-conv2d-edgedetect-example-input)
    来去噪/平滑噪声图像
- en: NOTE Fully functional code for image smoothing, executable via Jupyter Notebook,
    can be found at [http://mng.bz/aDM7](http://mng.bz/aDM7).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：完全功能的图像平滑代码，可通过 Jupyter Notebook 执行，可在[http://mng.bz/aDM7](http://mng.bz/aDM7)找到。
- en: 10.3.2 Image edge detection via 2D convolution
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2 通过二维卷积进行图像边缘检测
- en: 'Not all pixels in an image have equal semantic importance. Imagine a photograph
    of a person standing in front of a white wall. The pixels belonging to the wall
    are uniform in color and uninteresting. The pixels that yield the most semantic
    clues are those belonging to the silhouette: the edge pixels. This agrees with
    the science of human vision, where, as we mentioned earlier, experiments indicate
    that the human brain pays more attention to regions with sharp changes in color.
    Humans treat sound in a very similar fashion, ignoring uniform buzz such sounds
    often induce sleep) but becoming alert when the volume or frequency of the sound
    changes. Thus, identifying edges in an image is vital for image understanding.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图像中的不是所有像素都具有相同的语义重要性。想象一下一张人站在白色墙前的照片。墙上的像素颜色均匀且无趣。提供最多语义线索的像素属于轮廓：边缘像素。这与人类视觉科学相一致，正如我们之前提到的，实验表明，人类大脑更关注颜色变化明显的区域。人类对待声音的方式也非常相似，忽略均匀的嗡嗡声（这种声音往往会导致睡眠）但会变得警觉，当声音的音量或频率发生变化时。因此，识别图像中的边缘对于图像理解至关重要。
- en: Edges are local phenomena. As such, they can be identified by 2D convolution
    with specially chosen kernels. For instance, the vertical edges in figure [10.11b](#fig-conv2d-edgedetect-example-vert)
    were produced by performing 2D convolution on the image in figure [10.11a](#fig-conv2d-edgedetect-example-input)
    using the kernel
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘是局部现象。因此，可以通过使用特别选择的核进行二维卷积来识别它们。例如，图 [10.11b](#fig-conv2d-edgedetect-example-vert)
    中的垂直边缘是通过在图 [10.11a](#fig-conv2d-edgedetect-example-input) 中的图像上使用核
- en: '![](../../OEBPS/Images/eq_10-05-c.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_10-05-c.png)'
- en: Likewise, the vertical edges in figure [10.11c](#fig-conv2d-edgedetect-example-horz)
    were produced by performing 2D convolution on the image in figure [10.11a](#fig-conv2d-edgedetect-example-input)
    using the kernel
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，图 [10.11c](#fig-conv2d-edgedetect-example-horz) 中的垂直边缘是通过在图 [10.11a](#fig-conv2d-edgedetect-example-input)
    中的图像上使用核
- en: '![](../../OEBPS/Images/eq_10-05-d.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_10-05-d.png)'
- en: '![](../../OEBPS/Images/CH10_F11a_Chaudhury.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F11a_Chaudhury.png)'
- en: (a) Input image
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 输入图像
- en: '![](../../OEBPS/Images/CH10_F11b_Chaudhury.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F11b_Chaudhury.png)'
- en: (b) Vertical edges detected by applying 2D convolution ![](../../OEBPS/Images/eq_10-05-e2.png)
    to figure [10.11a](#fig-conv2d-edgedetect-example-input)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 通过对图 [10.11a](#fig-conv2d-edgedetect-example-input) 应用二维卷积 ![此处应有图片](../../OEBPS/Images/eq_10-05-e2.png)
    检测到的垂直边缘
- en: '![](../../OEBPS/Images/CH10_F11c_Chaudhury.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![此处应有图片](../../OEBPS/Images/CH10_F11c_Chaudhury.png)'
- en: (c) Horizontal edges detected by applying 2D convolution ![](../../OEBPS/Images/eq_10-05-f2.png)
    to figure [10.11a](#fig-conv2d-edgedetect-example-input)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 通过对图 [10.11a](#fig-conv2d-edgedetect-example-input) 应用二维卷积 ![此处应有图片](../../OEBPS/Images/eq_10-05-f2.png)
    检测到的水平边缘
- en: Figure 10.11 image often helps us analyze the image.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11 的图像经常帮助我们分析图像。
- en: How do these kernels identify edges? To see this, look at figure [10.9](#fig-conv2d-edge-detection-stride1).
    In a neighborhood with equal pixel values (for example, a flat wall), the kernel
    in figure [10.11b](#fig-conv2d-edgedetect-example-vert) will yield zero (the positive
    and negative kernel elements fall on equal values, and their weighted sum is zero).
    Thus this kernel suppresses uniform regions. On the other hand, it has a high
    response if there is a sharp jump in color (the negative and positive halves of
    the kernel fall on very different values, and the weighted sum is a large negative
    or large positive).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这些核如何识别边缘？为了了解这一点，请看图 [10.9](#fig-conv2d-edge-detection-stride1)。在一个具有相等像素值的邻域（例如，平坦的墙壁），图
    [10.11b](#fig-conv2d-edgedetect-example-vert) 中的核将产生零（正负核元素落在相等值上，它们的加权总和为零）。因此，这个核抑制了均匀区域。另一方面，如果颜色有急剧的变化（核的负半部和正半部落在非常不同的值上，加权总和是一个大的负数或大的正数），它会有很高的响应。
- en: NOTE Fully functional code for edge detection, executable via Jupyter Notebook,
    can be found at [http://mng.bz/g4JV](http://mng.bz/g4JV).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：完全功能的边缘检测代码，可通过 Jupyter Notebook 执行，可以在 [http://mng.bz/g4JV](http://mng.bz/g4JV)
    找到。
- en: 10.3.3 PyTorch- 2D convolution with custom weights
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.3 PyTorch- 使用自定义权重进行二维卷积
- en: We have discussed the convolution of 2D input arrays with two specific 2D kernels.
    We have seen that a kernel with uniform weights, such as ![](../../OEBPS/Images/eq_10-05-g2.png),
    results in local smoothing of the input array, whereas a kernel with antisymmetric
    weights, such as ![](../../OEBPS/Images/eq_10-05-h2.png), results in an output
    array that spikes at the edge locations in the input array. Now we will see how
    to set the weights of a 2D kernel and perform 2D convolution with that kernel
    in PyTorch.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了二维输入数组与两个特定二维核的卷积。我们注意到，具有均匀权重的核，例如 ![此处应有图片](../../OEBPS/Images/eq_10-05-g2.png)，会导致输入数组的局部平滑，而具有反对称权重的核，例如
    ![此处应有图片](../../OEBPS/Images/eq_10-05-h2.png)，会导致输出数组在输入数组的边缘位置出现峰值。现在我们将看到如何在
    PyTorch 中设置二维核的权重并使用该核进行二维卷积。
- en: NOTE This is *not* a typical PyTorch operation. The more typical operation is
    to create a neural network with a convolution layer (where we specify the size,
    stride, and padding but not the weights) and then train the network so that the
    weights are learned. We usually don’t care about the exact values of the learned
    weight. A sample neural network with a 2D convolution layer can be seen in section
    [10.6](#sec-conv2or3d-in-NN).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这不是典型的 PyTorch 操作。更典型的操作是创建一个具有卷积层的神经网络（我们指定大小、步长和填充，但不指定权重），然后训练网络以便学习权重。我们通常不关心学习到的权重的确切值。一个具有二维卷积层的示例神经网络可以在第
    [10.6](#sec-conv2or3d-in-NN) 节中看到。
- en: 'Listing [10.4](#id-2DlocalAveragingConvolution) shows local averaging convolution
    in two dimensions. While we saw in section that input arrays are 2D tensors of
    shape *H* × *W*, the PyTorch interface to convolution expects 4D tensors of shape
    *N* × *C* × *H* × *W* as input:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 [10.4](#id-2DlocalAveragingConvolution) 展示了二维局部平均卷积。虽然我们在章节中看到输入数组是形状为 *H*
    × *W* 的二维张量，但 PyTorch 的卷积接口期望输入为形状为 *N* × *C* × *H* × *W* 的四维张量：
- en: The first dimension, *N*, stands for the batch size. In a real neural network,
    inputs are fed in minibatches instead of one input instance at a time (this is
    for efficiency reasons, as discussed in section [9.2.2](../Text/09.xhtml#sec-SGD)).
    *N* stands for the number of input images contained in the minibatch.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个维度，*N*，代表批大小。在真实的神经网络中，输入是以小批量而不是单个输入实例的方式提供的（这是为了效率原因，如第 [9.2.2](../Text/09.xhtml#sec-SGD)
    节所述）。*N* 代表小批量中包含的输入图像数量。
- en: The second dimension, *C*, stands for the number of channels. For the input
    to the entire neural network, in the case of RGB images, we have three channels
    R (red), G (green), and B (blue); in the case of grayscale images, we only have
    a single channel. For other layers, the number of channels can be anything, depending
    on the neural network’s architecture. Typically, layers further from the input
    and closer to the output have more channels. Only channels at the grand input
    have fixed, clearly discernible physical significance (like R, G, B). Channels
    at the input to successive layers do not.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二维，*C*，代表通道数。对于整个神经网络的输入，在RGB图像的情况下，我们有三个通道R（红色）、G（绿色）和B（蓝色）；在灰度图像的情况下，我们只有一个通道。对于其他层，通道数可以是任何东西，这取决于神经网络的架构。通常，离输入更远、离输出更近的层有更多的通道。只有在大输入处的通道具有固定的、明显可辨别的物理意义（如R、G、B）。连续层输入处的通道则没有。
- en: The third dimension, *H*, stands for the height.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三维，*H*，代表高度。
- en: The fourth dimension, *W*, stands for the width.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四维，*W*，代表宽度。
- en: The weight tensor of a PyTorch `Conv2D` object has to be a 4D tensor. The listing
    shows a single grayscale image of size 5 × 5 as input. Hence *N* = 1, *C* = 1,
    *H* = 5, and *W* = 5\. *x* is instantiated as a 2D tensor of size 5 × 5. To convert
    it to a 4D tensor, we use the `torch.unsqueeze()` function, which adds an extra
    dimension to the input.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的`Conv2D`对象的权重张量必须是一个4D张量。列表显示了一个大小为5 × 5的单色图像作为输入。因此，*N* = 1，*C* = 1，*H*
    = 5，*W* = 5。*x*被实例化为一个5 × 5的二维张量。为了将其转换为4D张量，我们使用`torch.unsqueeze()`函数，它向输入添加一个额外的维度。
- en: Listing 10.4 PyTorch code for 2D local averaging convolution
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.4 PyTorch 2D局部平均卷积代码
- en: '[PRE3]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Loads a noisy grayscale input image
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ① 加载带有噪声的灰度输入图像
- en: ② Instantiates the weights of the convolutional kernel
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ② 实例化卷积核的权重
- en: ③ PyTorch expects inputs and weights to be of the form *N* × *C* × *H* × *W*,
    where *N* is the batch size, *C* is the number of channels, *H* is the height,
    and *W* is the width. Here, *N* = 1 because we have a single image. *C* = 1 because
    we are considering a grayscale image. *H* and *W* are both 5 because the input
    is a 5 × 5 array. unsqueeze converts our 5 × 5 tensor into a 1 × 1 × 5 × 5 tensor.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ③ PyTorch期望输入和权重以*N* × *C* × *H* × *W*的形式存在，其中*N*是批量大小，*C*是通道数，*H*是高度，*W*是宽度。在这里，*N*
    = 1，因为我们只有一个图像。*C* = 1，因为我们考虑的是灰度图像。*H*和*W*都是5，因为输入是一个5 × 5的数组。unsqueeze将我们的5
    × 5张量转换为1 × 1 × 5 × 5张量。
- en: ④ Instantiates the 2D smoothing kernel
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 实例化2D平滑核
- en: ⑤ Sets the kernel weights
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 设置内核权重
- en: ⑥ Instructs PyTorch to not compute gradients since we currently don’t require
    them
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 指示PyTorch不计算梯度，因为我们目前不需要它们
- en: ⑦ Runs the convolution
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 运行卷积
- en: Listing 10.5 PyTorch code for 2D edge detection
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.5 PyTorch 2D边缘检测代码
- en: '[PRE4]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Loads a grayscale input image with edges
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ① 加载带有边缘的灰度输入图像
- en: ② Instantiates the weights of the convolutional kernel
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ② 实例化卷积核的权重
- en: ③ Converts the inputs to 1 × 1 × 4 × 4
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将输入转换为1 × 1 × 4 × 4
- en: ④ Instantiates a 2D edge-detection kernel
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 实例化一个2D边缘检测核
- en: ⑤ Sets the kernel weights
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 设置内核权重
- en: ⑥ Instructs PyTorch to not compute gradients since we currently don’t require
    them
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 指示PyTorch不计算梯度，因为我们目前不需要它们
- en: ⑦ Runs the convolution
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 运行卷积操作
- en: 10.3.4 Two-dimensional convolution as matrix multiplication
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.4 二维卷积作为矩阵乘法
- en: In section [10.1.3](#sec-conv1d-mat), we saw how 1D convolution can be viewed
    as multiplying the input vector by a block-diagonal matrix (shown in equation
    [10.3](#eq-conv1d-as-malt-mult-k3s2valid)). The idea can be extended to higher
    dimensions, although the matrix of weights becomes significantly more complex.
    Nonetheless, it is important to have a mental picture of this matrix. Among other
    things, it will help us better understand transposed convolution. In this matrix
    multiplication-oriented view of 2D convolution, the input image is represented
    as a rasterized 1D vector. Thus, an input matrix of size *m* × *n* becomes an
    *mn*-sized vector. The corresponding weight matrix has rows of length *mn*. Each
    row corresponds to a specific slide stop.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在[10.1.3](#sec-conv1d-mat)节中，我们看到了如何将1D卷积视为通过一个分块对角矩阵（如方程[10.3](#eq-conv1d-as-malt-mult-k3s2valid)所示）乘以输入向量。这个想法可以扩展到更高维度，尽管权重矩阵变得显著更复杂。尽管如此，了解这个矩阵的直观形象是很重要的。在众多事情中，它将帮助我们更好地理解转置卷积。在这个以矩阵乘法为导向的二维卷积视图中，输入图像被表示为一个光栅化的1D向量。因此，一个大小为*m*
    × *n*的输入矩阵变成一个*m*n*-大小的向量。相应的权重矩阵有长度为*m*n*的行。每一行对应一个特定的滑动停止点。
- en: For ease of understanding, let’s consider an input image with [*H*, *W*] = [4,4]
    (never mind that this image is unrealistically small). On this image, we are performing
    2D convolution with a [*k[H]*, *k[W]*] = [2,2] kernel with stride [*s[H]*, *s[W]*]
    = [1,1]. The situation is exactly as shown in figure [10.9](#fig-conv2d-edge-detection-stride1).
    The input image *X* with size *H* = 4, *W* = 4
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于理解，让我们考虑一个 [*H*, *W*] = [4,4] 的输入图像（不必在意这个图像是不切实际的很小）。在这张图像上，我们正在执行 [*k[H]*,
    *k[W]*] = [2,2] 的内核和步长 [*s[H]*, *s[W]*] = [1,1] 的 2D 卷积。情况正好如图 [10.9](#fig-conv2d-edge-detection-stride1)
    所示。大小为 *H* = 4, *W* = 4 的输入图像 *X*
- en: '![](../../OEBPS/Images/eq_10-05-i.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_10-05-i.png)'
- en: 'rasterizes to the input vector ![](../../OEBPS/Images/AR_x.png) of length 4
    * 4 = 16. Let the kernel weights be denoted as ![](../../OEBPS/Images/eq_10-05-j2.png)
    Consider the successive slide stops (steps in figure [10.9](#fig-conv2d-edge-detection-stride1)).
    The exact elements of the rasterized input vector that are multiplied by kernel
    weights for a specific step are shown below—these correspond to the shaded items
    for the same steps in figure [10.9](#fig-conv2d-edge-detection-stride1): 2D convolution
    between an image *X* and a kernel *W*, denoted *Y* = *W* ⊛ *X*, in the special
    case of an input image with [*H*, *W*] = [4,4]. For this image, 2D convolution
    with a [*k[H]*, *k[W]*] = [2,2] kernel with stride [*s[H]*, *s[W]*] = [1,1] and
    valid padding can be expressed as the following matrix multiplication:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 光栅化到长度为 4 * 4 = 16 的输入向量 ![图片](../../OEBPS/Images/AR_x.png)。设内核权重为 ![图片](../../OEBPS/Images/eq_10-05-j2.png)。考虑连续的滑动停止（图
    [10.9](#fig-conv2d-edge-detection-stride1) 中的步骤）。下面显示了在特定步骤中乘以内核权重的光栅化输入向量的确切元素——这些对应于图
    [10.9](#fig-conv2d-edge-detection-stride1) 中相同步骤的阴影项：图像 *X* 和内核 *W* 之间的 2D 卷积，记为
    *Y* = *W* ⊛ *X*，在输入图像 [*H*, *W*] = [4,4] 的特殊情况下。对于此图像，使用 [*k[H]*, *k[W]*] = [2,2]
    的内核和步长 [*s[H]*, *s[W]*] = [1,1] 的有效填充的 2D 卷积可以表示为以下矩阵乘法：
- en: '![](../../OEBPS/Images/eq_10-05-k.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_10-05-k.png)'
- en: This can be expressed as
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以表示为
- en: '![](../../OEBPS/Images/eq_10-06.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_10-06.png)'
- en: Equation 10.6
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 10.6
- en: 'Note the following:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以下内容：
- en: The 2D convolution weight matrix shown in equation [10.6](#eq-2dconvwt-mat)
    is for the special case, but it illustrates the general principle.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方程式 [10.6](#eq-2dconvwt-mat) 中所示的 2D 卷积权重矩阵是特殊情况，但它说明了一般原理。
- en: The 2D convolution weight matrix is block diagonal, just like the 1D version.
    The kernel weights are placed precisely to emulate figure [10.9](#fig-conv2d-edge-detection-stride1).
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2D 卷积权重矩阵是块对角线形式，就像 1D 版本一样。内核权重被精确放置以模拟图 [10.9](#fig-conv2d-edge-detection-stride1)。
- en: The convolution weight matrix has 9 rows and 16 columns. Thus it takes a input
    vector rasterized from a 4 × 4 input image) and generates a output matrix (which
    can be folded into a 3 × 3 convolution output image.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积权重矩阵有 9 行和 16 列。因此，它接受一个从 4 × 4 输入图像光栅化的输入向量，并生成一个输出矩阵（可以折叠成一个 3 × 3 卷积输出图像）。
- en: 10.4 Three-dimensional convolution
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 三维卷积
- en: If a picture is worth a thousand words, a video is worth 10,000 words. Videos
    are a rich source of information about dynamic real-life scenes. As deep learning-based
    image analysis (2D convolution) is becoming more and more successful, at the time
    of this writing, video analysis is becoming the next research frontier to conquer.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一张图片值千言万语，那么一段视频就值一万言。视频是关于动态现实场景的丰富信息来源。随着基于深度学习的图像分析（2D 卷积）越来越成功，在撰写本文时，视频分析已成为下一个需要征服的研究前沿。
- en: 'Videos are essentially three-dimensional entities. The representation is *discrete*
    in all three dimensions. The three dimensions correspond to *space*, which is
    two-dimensional, having *height* and *width*, and *time*. A video consists of
    a *sequence of frames*. Each frame is an image: a discrete 2D array of pixels.
    A frame represents the entire video scene at a specific (sampled) point. A pixel
    in a frame represents the color of a sampled location in space belonging to the
    scene at the time corresponding to the frame. Thus a video is a sequence of frames
    representing the dynamic scene at a sampled set of discrete points (pixels) in
    space and time. The video extends over a *spatio-temporal volume* (aka *ST volume*),
    which can be imagined as a cuboid. Each cross-section is a rectangle representing
    a frame. This is shown in figure [10.12](#fig-st-vol-with-frame).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 视频本质上是一个三维实体。在所有三个维度上，其表示是*离散*的。这三个维度对应于*空间*，它是二维的，具有*高度*和*宽度*，以及*时间*。视频由一系列*帧*组成。每一帧是一个图像：一个离散的2D像素数组。一帧代表特定（采样）点的整个视频场景。帧中的一个像素代表场景在帧对应的时间点上的一个采样空间位置的颜色。因此，视频是一系列帧，代表在空间和时间的一组离散点（像素）上的动态场景。视频跨越一个*时空体积*（也称为*ST体积*），可以想象为一个长方体。每个横截面是一个代表帧的矩形。这如图[10.12](#fig-st-vol-with-frame)所示。
- en: '![](../../OEBPS/Images/CH10_F12_Chaudhury.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F12_Chaudhury.jpg)'
- en: Figure 10.12 A spatio-temporal volume light-shaded cuboid) representing a video.
    Individual frames of the video are cross-sectional rectangles in this ST volume.
    A single frame is also shown in darker shading.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12 一个时空体积光阴影长方体（代表视频）。在这个时空体积中，视频的单独帧是横截面矩形。一个单独的帧也以较暗的阴影显示。
- en: To analyze the video, we need to extract local patterns from this 3D volume.
    Can we do it via repeated 2D convolutions?
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析视频，我们需要从这个3D体积中提取局部模式。我们能否通过重复的2D卷积来实现？
- en: 'The answer is *no*. There is extra information when we view the successive
    frames *together*, which is absent when we view the frames one at a time. For
    instance, imagine you are presented with an image of a half-open door. From that
    single image, can you determine whether the door is *opening* or *closing*? No,
    you cannot. To make that determination, we need to see several successive frames.
    In other words, analyzing a video one frame at a time robs us of a vital modality
    of information: *motion*, which can be understood only if we analyze multiple
    successive frames together. This is why we need 3D convolution.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是*不*。当我们一起查看连续的帧时，会有额外的信息，而当我们逐个查看帧时，这些信息是不存在的。例如，想象你被展示了一个半开的门的图片。从这张单独的图片中，你能确定门是*打开*还是*关闭*的吗？不能。为了做出这个判断，我们需要看到几个连续的帧。换句话说，逐帧分析视频剥夺了我们一个重要的信息模态：*运动*，这只有在分析多个连续帧时才能理解。这就是为什么我们需要3D卷积。
- en: The best way to visualize a 3D convolution is to imagine a *brick* sliding over
    the entire volume of a *room*. The room corresponds to the ST volume of the video
    input to the convolution. The brick corresponds to the kernel. While sliding,
    the brick stops at successive positions; we call these slide stops. Figure [10.13](#fig-3d-conv-with-kernel)
    shows four slide stops at different positions. Each slide stop emits one output
    point. As the brick sweeps over the entire input ST volume, an output ST volume
    is generated. At each slide stop, we multiply each input pixel value by the kernel
    element covering it and take a sum of the products. This is effectively a weighted
    sum of all the input (room) elements covered by the kernel (brick), with the covering
    kernel elements serving as the weights.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最直观地可视化3D卷积的方式是想象一个*砖块*在整个*房间*的体积上滑动。房间对应于卷积的视频输入的时空体积。砖块对应于核。在滑动过程中，砖块会停在连续的位置；我们称之为滑块停止。图[10.13](#fig-3d-conv-with-kernel)显示了不同位置的四个滑块停止。每个滑块停止发出一个输出点。当砖块扫过整个输入时空体积时，生成一个输出时空体积。在每个滑块停止处，我们将每个输入像素值乘以覆盖它的核元素，并对这些乘积求和。这实际上是对所有被核（砖块）覆盖的输入（房间）元素的有效加权求和，覆盖的核元素作为权重。
- en: '![](../../OEBPS/Images/CH10_F13a_Chaudhury.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F13a_Chaudhury.png)'
- en: (a) slide stop *x* = 0, *y* = 0, *t* = 0.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 滑块停止 *x* = 0, *y* = 0, *t* = 0。
- en: '![](../../OEBPS/Images/CH10_F13b_Chaudhury.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F13b_Chaudhury.png)'
- en: (b) slide stop *x* = 0, *y* = 0, *t* = 0.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 滑块停止 *x* = 0, *y* = 0, *t* = 0。
- en: '![](../../OEBPS/Images/CH10_F13c_Chaudhury.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F13c_Chaudhury.png)'
- en: (c) slide stop *x* = 0, *y* = 0, *t* = 0.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 滑块停止 *x* = 0, *y* = 0, *t* = 0。
- en: '![](../../OEBPS/Images/CH10_F13d_Chaudhury.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F13d_Chaudhury.png)'
- en: (d) slide stop *x* = 0, *y* = 0, *t* = 0.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 滑动停止 *x* = 0, *y* = 0, *t* = 0。
- en: Figure 10.13 Spatio-temporal view of 3D convolution. The larger, light-shaded
    cuboid on the left of each figure represents the input ST volume (room). The small,
    dark-shaded cuboid inside the room represents the kernel brick). The brick slides
    all over the room’s internal volume. Neighboring positions of the brick may overlap
    in volume. Each position of the brick represents a slide stop; a weighted sum
    is taken of all points in the room (input points) covered by the brick. The brick
    point kernel value) covering each input point serves as the weight. Four different
    slide stops are shown. Each slide stop generates a single output point. As the
    brick sweeps the input volume, an output ST volume the smaller light-shaded cuboid)
    is generated.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.13 3D 卷积的空间时间视图。每个图左侧较大的浅色长方体代表输入 ST 体积（房间）。房间内部的小深色长方体代表核砖块。砖块在整个房间内部体积上滑动。砖块的相邻位置可能在体积上重叠。砖块的每个位置代表一个滑动停止点；对房间（输入点）上所有被砖块覆盖的点进行加权求和。覆盖每个输入点的核点值（核值）作为权重。显示了四个不同的滑动停止点。每个滑动停止点生成一个单独的输出点。当砖块扫过输入体积时，生成一个输出
    ST 体积（较小的浅色长方体）。
- en: Let’s denote the input ST volume by *S*. It is a 3D grid whose domain is
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 *S* 表示输入 ST 体积。它是一个三维网格，其域为
- en: '*S* = [0, *T* − 1] × [0, *H* − 1] × [0, *W* − 1]'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '*S* = [0, *T* − 1] × [0, *H* − 1] × [0, *W* − 1]'
- en: Every point in *S* is a pixel with a color value (which can be a scalar—a gray-level
    value—or a vector of three values, R, G, B. On this grid of input points, we define
    a subgrid *S[o]* of output points. *S[o]* is obtained from *S* by applying stride-based
    stepping on the input. Assuming ![](../../OEBPS/Images/AR_s.png) = [*s^T*, *s[K]*,
    *s[W]*] denotes the 3D stride vector, the first slide stop has the top-left corner
    of the brick at ![](../../OEBPS/Images/AR_p.png)[0] ≡ (*t* = 0, *y* = 0, *x* =
    0). The next slide stop is at ![](../../OEBPS/Images/AR_p.png)[1] ≡ (*t* = 0,
    *y* = 0, *x* = *s[W]*), and the next is at ![](../../OEBPS/Images/AR_p.png)[2]
    ≡ (*t* = 0, *y* = 0, *x* = 2*s[W]*). When we reach the right end, we increment
    *y*. When we reach the bottom, we increment *t*. When we reach the end of the
    room, we stop. *S[o]* = {![](../../OEBPS/Images/AR_p.png)[0], ![](../../OEBPS/Images/AR_p.png)[1]
    … } are the points at which the top-left corner of the kernel (brick) rests as
    it sweeps over the input volume. There is an output for each point in *S[o]*.
    The kernel also has three dimensions (in practice, it has two more dimensions
    corresponding to the input channels and batch—we are ignoring them now for simplicity—as
    discussed in section [10.4.2.1](#sec-pytorch-3d-conv)).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '*S* 中的每个点都是一个具有颜色值（可以是标量——灰度值或三个值的向量，R、G、B）的像素。在这个输入点网格上，我们定义了一个输出点子网格 *S[o]*。*S[o]*
    通过对输入应用基于步长的步进从 *S* 中获得。假设 ![](../../OEBPS/Images/AR_s.png) = [*s^T*, *s[K]*,
    *s[W]*] 表示 3D 步长向量，第一个滑动停止点在 ![](../../OEBPS/Images/AR_p.png)[0] ≡ (*t* = 0, *y*
    = 0, *x* = 0)。下一个滑动停止点在 ![](../../OEBPS/Images/AR_p.png)[1] ≡ (*t* = 0, *y* =
    0, *x* = *s[W]*)，下一个是 ![](../../OEBPS/Images/AR_p.png)[2] ≡ (*t* = 0, *y* = 0,
    *x* = 2*s[W]*)。当我们到达右侧时，我们增加 *y*。当我们到达底部时，我们增加 *t*。当我们到达房间的尽头时，我们停止。*S[o]* = {![](../../OEBPS/Images/AR_p.png)[0],
    ![](../../OEBPS/Images/AR_p.png)[1] … } 是核（砖块）的左上角在扫过输入体积时的位置。每个 *S[o]* 中的点都有一个输出。核也有三个维度（实际上，它还有两个额外的维度对应于输入通道和批量——我们为了简单起见忽略它们，如第
    [10.4.2.1](#sec-pytorch-3d-conv) 节所述）。'
- en: 'Equation [10.7](#eq-3dconv-output) shows how a single output value is generated
    in 3D convolution. *X* denotes the input, *Y* denotes the output, and *W* denote
    the kernel weights:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 [10.7](#eq-3dconv-output) 展示了在 3D 卷积中单个输出值的生成过程。*X* 表示输入，*Y* 表示输出，*W* 表示核权重：
- en: '![](../../OEBPS/Images/eq_10-07.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_10-07.png)'
- en: Equation 10.7
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 10.7
- en: Note that the kernel (brick) has its origin on *X[t]*, *y*, *x*. Its dimensions
    are (*k^T*, *k[H]*, *k[W]*). So, it covers all input pixels in the domain [*t*..(*t*
    + *k^T*)] × [*y*..(*y* + *k[H]*)] × [*x*..(*x* + *k[W]*)]. These are the pixels
    participating in equation [10.7](#eq-3dconv-output). Each of these input pixels
    is multiplied by the kernel element covering it. Match equation [10.7](#eq-3dconv-output)
    with figure [10.13](#fig-3d-conv-with-kernel).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，核（砖块）的起点在 *X[t]*, *y*, *x* 上。其尺寸为 (*k^T*, *k[H]*, *k[W]*)。因此，它覆盖了域 [*t*..(*t*
    + *k^T*)] × [*y*..(*y* + *k[H]*)] × [*x*..(*x* + *k[W]*)] 中的所有输入像素。这些是参与方程 [10.7](#eq-3dconv-output)
    的像素。每个这些输入像素都乘以覆盖它的核元素。将方程 [10.7](#eq-3dconv-output) 与图 [10.13](#fig-3d-conv-with-kernel)
    对比。
- en: 10.4.1 Video motion detection via 3D convolution
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.1 通过 3D 卷积进行视频运动检测
- en: A moving object in a dynamic scene changes position from one video frame to
    another. Consequently, pixels are covered or uncovered at the boundary of motion.
    Pixels belonging to the background in one frame may be covered by the object in
    a subsequent frame and vice versa. If the background is a different color than
    the object, this will cause a color difference between pixels at identical spatial
    locations at different times, as illustrated in figure [10.14](#fig-3d-conv-motion-img).
    The output of applying convolution to an ST volume is another ST volume. Figure
    [10.15](#fig-3d-conv-output) shows a few frames from the output resulting from
    applying our video motion detector to the input shown in figure [10.14](#fig-3d-conv-motion-img).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在动态场景中，移动物体从一个视频帧移动到另一个视频帧。因此，在运动边界处像素被覆盖或暴露。在一个帧中属于背景的像素可能在后续帧中被物体覆盖，反之亦然。如果背景颜色与物体不同，这将在不同时间相同空间位置的像素之间引起颜色差异，如图[10.14](#fig-3d-conv-motion-img)所示。对ST体积应用卷积的输出是另一个ST体积。图[10.15](#fig-3d-conv-output)显示了将我们的视频运动检测器应用于图[10.14](#fig-3d-conv-motion-img)中所示输入的输出结果。
- en: '![](../../OEBPS/Images/CH10_F14_Chaudhury.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F14_Chaudhury.png)'
- en: Figure 10.14 Successive frames of a synthetic video of a moving ball, shown
    in a superimposed fashion with gradually increasing opacity for illustration purposes
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14 移动球合成视频的连续帧，为说明目的以叠加方式显示，并逐渐增加不透明度
- en: 'How does a kernel extract motion information from a set of successive frames?
    As mentioned earlier, motion causes pixels at the same position in successive
    frames to have different colors. However, a single isolated pair of pixels may
    have different colors due to noise—we cannot draw any conclusions from that. If
    we average the pixel values in a small neighborhood in one frame and average the
    pixel values in the same neighborhood in the subsequent frames, and these two
    averages are different, that is a more reliable way to estimate motion. Following
    is a 2 × 3 × 3 3D kernel to do exactly that—average pixel values in a 3 × 3 spatial
    neighborhood in two successive frames and subtract one from the other:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 内核是如何从一系列连续帧中提取运动信息的？如前所述，运动会导致连续帧中相同位置的像素具有不同的颜色。然而，由于噪声，单个孤立的像素对可能具有不同的颜色——我们不能从这个中得出任何结论。如果我们在一个帧的小邻域内平均像素值，并在后续帧的相同邻域内平均像素值，并且这两个平均值不同，那么这是一种更可靠的估计运动的方法。以下是一个2
    × 3 × 3的三维核，正是为了做到这一点——在两个连续帧的3 × 3空间邻域内平均像素值，并从其中一个减去另一个：
- en: '![](../../OEBPS/Images/eq_10-07-a.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_10-07-a.png)'
- en: The result of the subtraction is high in regions of motion and low in regions
    of no motion. In this context, it is worthwhile to note that since the object
    is of uniform color, pixels within the object are indistinguishable. Consequently,
    no motion is observed at the center of the object; motion is observed only at
    the boundary. A few individual frames of the result of this 3D convolution are
    shown in figure [10.15](#fig-3d-conv-output).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 减法的结果在运动区域较高，在无运动区域较低。在此背景下，值得注意的是，由于物体颜色均匀，物体内部的像素是不可区分的。因此，在物体的中心没有观察到运动；只有在边界处观察到运动。图[10.15](#fig-3d-conv-output)显示了此3D卷积结果的几个帧。
- en: NOTE Fully functional code for video motion detection, executable via Jupyter
    Notebook, can be found at [http://mng.bz/enJQ](http://mng.bz/enJQ).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：完全功能的视频运动检测代码，可通过Jupyter Notebook执行，可以在[http://mng.bz/enJQ](http://mng.bz/enJQ)找到。
- en: '![](../../OEBPS/Images/CH10_F15a_Chaudhury.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F15a_Chaudhury.png)'
- en: (a) Output frame 0
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 输出帧 0
- en: '![](../../OEBPS/Images/CH10_F15b_Chaudhury.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F15b_Chaudhury.png)'
- en: (b) Output frame 1
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 输出帧 1
- en: '![](../../OEBPS/Images/CH10_F15c_Chaudhury.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F15c_Chaudhury.png)'
- en: (c) Output frame 2
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 输出帧 2
- en: '![](../../OEBPS/Images/CH10_F15d_Chaudhury.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F15d_Chaudhury.png)'
- en: (d) Output frame 3
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 输出帧 3
- en: Figure 10.15 Result of applying a 3D convolution motion detector to the synthetic
    video of a moving ball. Gray signifies “no motion"; most of the output frames
    are gray. White and black signify motion.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.15 将3D卷积运动检测器应用于移动球合成视频的结果。灰色表示“无运动”；大部分输出帧都是灰色。白色和黑色表示运动。
- en: 10.4.2 PyTorch- Three-dimensional convolution with custom weights
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.2 PyTorch- 使用自定义权重的三维卷积
- en: 'In section [10.4.1](#sec-conv3d-motion-detect), we saw how to detect motion
    in a sequence of input images using 3D convolutions. In this section, we see how
    to implement this in PyTorch. The PyTorch interface to 3D convolutions expects
    5-dimensional input tensors of the form *N* × *C* × *D* × *H* × *W*. In addition
    to the dimensions discussed in section [10.4](#sec-conv-3d), there is an additional
    dimension for the input channels. Thus, there is a separate brick for each input
    channel. We are combining (taking the weighted sum of) them all:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[10.4.1](#sec-conv3d-motion-detect)节中，我们看到了如何使用3D卷积在输入图像序列中检测运动。在本节中，我们将看到如何在PyTorch中实现这一点。PyTorch对3D卷积的接口期望输入张量是5维的，形式为*N*
    × *C* × *D* × *H* × *W*。除了第[10.4](#sec-conv-3d)节中讨论的维度外，还有一个额外的输入通道维度。因此，每个输入通道都有一个单独的砖块。我们正在将它们全部组合（取它们的加权总和）：
- en: As discussed in the case of 2D convolutions (section [10.3.3](#sec-pytorch-2d-conv)),
    the first dimension *N* stands for the batch size minibatches are fed to a real
    neural network instead of individual input instances for efficiency reasons),
    and *C* stands for the number of input channels.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如同在2D卷积的案例中讨论的那样（第[10.3.3](#sec-pytorch-2d-conv)节），第一个维度*N*代表批量大小（为了效率，将批量的小批量输入到实际神经网络中，而不是单个输入实例），而*C*代表输入通道数。
- en: '*D* stands for the sequence length. In our motion detector example, *D* represents
    the number of successive image frames fed to the 3D convolution layer.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*D*代表序列长度。在我们的运动检测器示例中，*D*代表输入到3D卷积层的连续图像帧的数量。'
- en: The third dimension, *H*, stands for height, and the fourth dimension, *W*,
    stands for width.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个维度*H*代表高度，第四个维度*W*代表宽度。
- en: In our motion detector example, we have a sequence of five grayscale images
    as input, each with height = 320 and width = 320\. Since we are considering only
    a single image sequence, *N* = 1\. All images are grayscale, which implies that
    *C* = 1\. The sequence length, *D*, is equal to 5\. *H* and *W* are both 320.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的运动检测器示例中，我们有一个由五个灰度图像组成的序列作为输入，每个图像的高度为320，宽度为320。由于我们只考虑单个图像序列，*N* = 1。所有图像都是灰度的，这意味着*C*
    = 1。序列长度*D*等于5。*H*和*W*都是320。
- en: 'PyTorch expects the 3D kernels to be of the form *C[out]* × *C[in]* × *k[T]*
    × *k[H]* × *k[W]*:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch期望3D核的形式为*C[out]* × *C[in]* × *k[T]* × *k[H]* × *k[W]*：
- en: The first dimension, *C[out]*, represents the number of output channels. You
    can think of the convolutional kernel as a bank of 3D filters, where each filter
    produces one output channel. *C[out]* is the number of 3D filters in the bank.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个维度*C[out]*代表输出通道数。你可以将卷积核视为一个3D滤波器库，其中每个滤波器产生一个输出通道。*C[out]*是库中3D滤波器的数量。
- en: The second dimension, *C[in]*, represents the number of input channels. This
    depends on the number of channels in the input tensor. When we are dealing with
    grayscale images, *C[in]* is 1 at the grand input. For RGB images, *C[in]* is
    3 at the grand input. For layers further from the input, *C[in]* equals the number
    of channels in the tensor fed to that layer.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个维度*C[in]*代表输入通道数。这取决于输入张量中的通道数。当我们处理灰度图像时，在顶层输入中*C[in]*为1。对于RGB图像，在顶层输入中*C[in]*为3。对于输入层之后的层，*C[in]*等于输入到该层的张量中的通道数。
- en: The third, fourth, and fifth dimensions, *k[T]*, *k[H]*, and *k[W]*, represent
    the kernel sizes along the *T*, *H*, and *W* dimensions, respectively
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三、第四和第五维度*k[T]*、*k[H]*和*k[W]*分别代表沿*T*、*H*和*W*维度的核大小
- en: In our motion detector example, we have a single kernel with *k[T]*=2, *k[H]*=3,
    and *k[W]* = 3. Since we only have a single kernel, *C[out]* = 1. And since we
    are dealing with grayscale images, *C[in]* is also 1.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的运动检测器示例中，我们有一个单个核，*k[T]*=2，*k[H]*=3，*k[W]* = 3。由于我们只有一个核，*C[out]* = 1。并且由于我们处理的是灰度图像，*C[in]*也是1。
- en: Listing 10.6 PyTorch code for 3D convolution
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.6 PyTorch 3D卷积的代码
- en: '[PRE5]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Loads a sequence of five grayscale images with shape 320 × 320
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ① 加载五个形状为320 × 320的灰度图像序列
- en: ② Converts to a tensor of shape *T* × *H* × *W* = 5 × 320 × 320
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ② 转换为形状为*T* × *H* × *W* = 5 × 320 × 320的张量
- en: ③ Instantiates a 2D smoothing kernel of shape 3 × 3. Pads an extra dimension
    so that two 2D kernels can be stacked together to form a 3D kernel.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 实例化一个形状为3 × 3的2D平滑核。添加一个额外的维度，以便可以将两个2D核堆叠在一起形成一个3D核。
- en: ④ Concatenates the 2D smoothing kernel and its inverted version along the first
    dimension to form a 3D kernel of shape 2 × 3 × 3
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 沿着第一个维度将2D平滑核及其反转版本连接起来，形成一个形状为2 × 3 × 3的3D核
- en: ⑤ Converts the input tensor to *N* × *C* × *T* × *H* × *W* = 1 × 1 × 5 × 320
    × 320
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将输入张量转换为*N* × *C* × *T* × *H* × *W* = 1 × 1 × 5 × 320 × 320
- en: ⑥ Converts the 3D kernel to *C[out]* × *C[in]* × *k[T]* × *k[H]* × *k[W]* =
    1 × 1 × 2 × 3 × 3
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 将3D核转换为 *C[out]* × *C[in]* × *k[T]* × *k[H]* × *k[W]* = 1 × 1 × 2 × 3 × 3
- en: ⑦ Instantiates and sets the weights of the Conv3d layer
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 实例化并设置Conv3d层的权重
- en: ⑧ Instructs PyTorch to not compute gradients since we currently don’t require
    them
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 指示PyTorch不计算梯度，因为我们目前不需要它们
- en: ⑨ Runs the convolution
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 执行卷积操作
- en: 10.5 Transposed convolution or fractionally strided convolution
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.5 转置卷积或分数步长卷积
- en: 'As usual, we examine this topic with an example. Consider a 1D convolution
    with kernel ![](../../OEBPS/Images/AR_w.png) = [*w*[0]   *w*[1]   *w*[2]] of size
    3, with valid padding. Let’s consider a special case where the input size *n*
    is 5. Following equation [10.2](#eq-conv1d-as-malt-mult-k3s1valid), this convolution
    can be expressed as a multiplication of a block-diagonal matrix *W* constructed
    from the weights vector ![](../../OEBPS/Images/AR_w.png), with input vector ![](../../OEBPS/Images/AR_x.png)
    as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，我们通过一个例子来探讨这个主题。考虑一个大小为3的核 ![](../../OEBPS/Images/AR_w.png) = [*w*[0]
      *w*[1]   *w*[2]] 的1D卷积，使用有效填充。让我们考虑一个特殊情况，其中输入大小 *n* 为5。根据方程 [10.2](#eq-conv1d-as-malt-mult-k3s1valid)，这个卷积可以表示为从权重向量
    ![](../../OEBPS/Images/AR_w.png) 构造的块对角矩阵 *W* 与输入向量 ![](../../OEBPS/Images/AR_x.png)
    的乘积，如下所示：
- en: '![](../../OEBPS/Images/eq_10-07-b.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_10-07-b.png)'
- en: What happens if we multiply the output vector ![](../../OEBPS/Images/AR_y.png)
    by the transposed matrix *W^T*?
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将输出向量 ![](../../OEBPS/Images/AR_y.png) 乘以转置矩阵 *W^T*，会发生什么？
- en: '![](../../OEBPS/Images/eq_10-07-c.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_10-07-c.png)'
- en: 'Following are some observations:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些观察结果：
- en: We haven’t quite recovered ![](../../OEBPS/Images/AR_x.png) from ![](../../OEBPS/Images/AR_y.png),
    but we have generated a vector, *x̃*, the same size as ![](../../OEBPS/Images/AR_x.png).
    Multiplying by the transpose of the weight matrix of the convolution performs
    a kind of upsampling, undoing the downsampling resulting from the forward convolution.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们并没有完全从 ![](../../OEBPS/Images/AR_x.png) 中恢复 ![](../../OEBPS/Images/AR_y.png)，但我们已经生成了一个与
    ![](../../OEBPS/Images/AR_x.png) 大小相同的向量，*x̃*。通过乘以卷积权重矩阵的转置执行一种上采样，撤销正向卷积产生的下采样。
- en: 'It is impossible to recover ![](../../OEBPS/Images/AR_x.png) from ![](../../OEBPS/Images/AR_y.png).
    This is because when constructing ![](../../OEBPS/Images/AR_y.png) from ![](../../OEBPS/Images/AR_x.png),
    we multiplied by *W* and converted a vector with five independent elements to
    a vector with three independent elements—some information was irretrievably lost.
    This intuition is consistent with the fact that a 5 × 3 matrix *W* is *non-invertible*:
    there is no *W*^(−1), so there is no way to get ![](../../OEBPS/Images/AR_x.png)
    = *W*^(−1)![](../../OEBPS/Images/AR_y.png).'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 ![](../../OEBPS/Images/AR_y.png) 中恢复 ![](../../OEBPS/Images/AR_x.png) 是不可能的。这是因为当我们从
    ![](../../OEBPS/Images/AR_x.png) 构造 ![](../../OEBPS/Images/AR_y.png) 时，我们乘以 *W*
    并将具有五个独立元素的向量转换为具有三个独立元素的向量——一些信息已经无法恢复。这种直觉与事实一致，即5 × 3矩阵 *W* 是 *不可逆的*：没有 *W*^(−1)，因此无法得到
    ![](../../OEBPS/Images/AR_x.png) = *W*^(−1)![](../../OEBPS/Images/AR_y.png)。
- en: During transpose convolution, we are distributing elements of ![](../../OEBPS/Images/AR_y.png)
    back to the elements of *x̃* in the same proportion as when we were doing the
    forward convolution (see figure [10.16](#fig-transpose-conv1d)). This should remind
    you of backpropagation from chapter [8](../Text/08.xhtml#ch-training-neural-networks).
    There, in equation [8.24](../Text/08.xhtml#eq-MLP-out-nosubscript-1) right-hand
    side), we saw that for linear layers, forward propagation amounts to multiplying
    by an arbitrary weight matrix *W* (shown in equation [8.8](../Text/08.xhtml#eq-MLP-weight-matrix)).
    Backpropagation involves multiplying by the transpose of the same weight matrix
    (equation [8.31](../Text/08.xhtml#eq-auxvar-vector)). The backpropagation does
    a *proportional blame distribution*—the loss is distributed back to the inputs
    in the same proportion as their contribution in creating the output. The same
    thing is happening here. Thus, multiplying by the transposed weight matrix in
    general distributes the output back in the same ratio in which it contributes
    to the output.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在转置卷积过程中，我们将![](../../OEBPS/Images/AR_y.png)的元素以与进行正向卷积时相同的比例分配回*x̃*中的元素（参见图[10.16](#fig-transpose-conv1d)）。这应该会让你想起第[8](../Text/08.xhtml#ch-training-neural-networks)章中的反向传播。在那里，在方程[8.24](../Text/08.xhtml#eq-MLP-out-nosubscript-1)的右侧，我们看到了对于线性层，正向传播相当于乘以一个任意的权重矩阵*W*（如方程[8.8](../Text/08.xhtml#eq-MLP-weight-matrix)所示）。反向传播涉及乘以相同的权重矩阵的转置（方程[8.31](../Text/08.xhtml#eq-auxvar-vector)）。反向传播进行的是*成比例的责任分配*——损失以与它们在创建输出中的贡献成比例的比例分配回输入。这里发生的情况也是一样。因此，乘以转置权重矩阵通常以与它对输出贡献相同的比例分配输出。
- en: '![](../../OEBPS/Images/CH10_F16_Chaudhury.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F16_Chaudhury.png)'
- en: Figure 10.16 1D convolution and its transpose
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.16 1D卷积及其转置
- en: The idea extends to higher dimensions. Figure [10.17](#fig-transpose-conv2d-stride-1-padding-0)
    illustrates a 2D transpose convolution operation.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这种思想可以扩展到更高维度。图[10.17](#fig-transpose-conv2d-stride-1-padding-0)说明了2D转置卷积操作。
- en: '![](../../OEBPS/Images/CH10_F17_Chaudhury.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH10_F17_Chaudhury.png)'
- en: Figure 10.17 2D convolution and its transpose
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.17 2D卷积及其转置
- en: '10.5.1 Application of transposed convolution: Autoencoders and embeddings'
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5.1 转置卷积的应用：自编码器和嵌入
- en: Transposed convolution is typically required in autoencoders. We provide a very
    brief outline of autoencoders at this point to explain why they need transposed
    convolution. Most of the neural networks we have looked at so far are examples
    of supervised classifiers in that they take an input and directly output the class
    to which the input belongs. This is not the only paradigm possible. As hinted
    in section [6.9](../Text/06.xhtml#sec-GMM), we can also map an input to a vector
    often called the *embedding*, aka *descriptor vector*) that captures the essential
    aspects of the class of interest and throws away the variable aspects. For instance,
    if the class of interest is a human, then given an image, the embedding will only
    capture the features that recognize the humans in the image and ignore the background
    (sky, sea, forest, building, and so on).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 转置卷积通常在自编码器中是必需的。在此处，我们简要概述自编码器，以解释为什么它们需要转置卷积。到目前为止，我们查看的大多数神经网络都是监督分类器的例子，它们接收输入并直接输出输入所属的类别。这不是唯一可能的范式。如第[6.9](../Text/06.xhtml#sec-GMM)节所暗示，我们还可以将输入映射到一个向量，通常称为*嵌入*，也称为*描述向量*，它捕捉感兴趣类别的本质方面，并丢弃可变方面。例如，如果感兴趣的类别是人类，那么给定一个图像，嵌入将只捕获识别图像中人类的特征，而忽略背景（天空、海洋、森林、建筑等）。
- en: The mapping from input to embedding is done by a neural network called an *encoder*.
    If the input is an image, the encoder typically contains a sequence of convolution
    layers.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入到嵌入的映射是通过一个称为*编码器*的神经网络完成的。如果输入是图像，编码器通常包含一系列卷积层。
- en: 'How do we train this neural network? How do we define its loss? Well, one possibility
    is that the embedding must maintain fidelity to the original input: that is, we
    should be able to reconstruct (at least approximately) the input from the embedding.
    Remember, the embedding is smaller in size (with fewer degrees of freedom) than
    the input, so perfect reconstruction is impossible. Still, we can define loss
    as the difference (for example, Euclidean distance or binarized cross-entropy
    loss) between the original input and the reconstructed input.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何训练这个神经网络？我们如何定义它的损失？好吧，一种可能性是嵌入必须保持对原始输入的忠实度：也就是说，我们应该能够从嵌入中重建（至少是近似地）输入。记住，嵌入的大小（自由度较少）比输入小，因此完美的重建是不可能的。尽管如此，我们可以定义损失为原始输入和重建输入之间的差异（例如，欧几里得距离或二值交叉熵损失）。
- en: How do we reconstruct the input from the embedding? This is where transposed
    convolution comes in. Remember, we did convolution (perhaps many times) in our
    encoder to generate the embedding. We can do a set of transposed convolutions
    on the embedding to generate a tensor of the same size as the input. The network
    to do this reconstruction is called the *decoder*. The decoder generates our reconstructed
    input.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何从嵌入中重建输入？这就是转置卷积发挥作用的地方。记住，我们在编码器中进行了卷积（可能多次）以生成嵌入。我们可以在嵌入上执行一系列转置卷积以生成与输入相同大小的张量。执行此重建的网络称为*解码器*。解码器生成我们的重建输入。
- en: We define a loss as the difference between the original and reconstructed input.
    We can train to minimize the loss and learn the weights of both the encoder and
    decoder. This is called *end-to-end learning*, and the encoder-decoder pair is
    called an *autoencoder*.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义损失为原始输入和重建输入之间的差异。我们可以训练以最小化损失并学习编码器和解码器的权重。这被称为*端到端学习*，编码器-解码器对被称为*自动编码器*。
- en: 'We train the autoencoder with many data instances, all belonging to the class
    of interest. Since it does not have the luxury of remembering the entire image
    (the embedding being smaller in size than the input), it is forced to learn how
    to retain the features common to all the training images: that is, the features
    that describe the class of interest. In our example, the autoencoder will learn
    to retain features that identify a human and drop the background. Note that this
    could also lead to a very effective *compression technique*—the embedding is a
    compact representation of the image in which only the objects of interest have
    been retained.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用许多数据实例来训练自动编码器，所有这些实例都属于感兴趣的类别。由于它没有记住整个图像的奢侈（嵌入的大小比输入小），它被迫学习如何保留所有训练图像的共同特征：即描述感兴趣类别的特征。在我们的例子中，自动编码器将学习保留识别人类的特征并丢弃背景。请注意，这也可能导致一种非常有效的*压缩技术*——嵌入是图像的紧凑表示，其中只保留了感兴趣的物体。
- en: 10.5.2 Transposed convolution output size
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5.2 转置卷积输出大小
- en: 'The output size of transposed convolution can be obtained by inverting equation
    [10.8](#eq-conv-out-size):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反转方程式 [10.8](#eq-conv-out-size) 可以获得转置卷积的输出大小：
- en: '*o*^′ = (*n*^′−1)*s* + *k* − 2*p*'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '*o*^′ = (*n*^′−1)*s* + *k* − 2*p*'
- en: Equation 10.8
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 10.8
- en: For instance, transposed convolution with stride *s* = 1 on a ![](../../OEBPS/Images/AR_y.png)
    of size *n*^′ = 3 with valid padding *p* = 0) and a kernel of size *k* = 3 creates
    an output *x̃* of size *o*^′ = 5.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在大小为 *n*^′ = 3 的 ![](../../OEBPS/Images/AR_y.png) 上使用步长 *s* = 1 的转置卷积（有效填充
    *p* = 0）和一个大小为 *k* = 3 的内核，会创建一个大小为 *o*^′ = 5 的输出 *x̃*。
- en: 10.5.3 Upsampling via transpose convolution
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5.3 通过转置卷积进行上采样
- en: In the previous section, we briefly discussed autoencoders, where an encoder
    network maps an input image into an embedding and a decoder network tries to reconstruct
    the input image from the embedding. The encoder network converts a higher-resolution
    input into a lower-resolution embedding by passing the input through a series
    of convolution and pooling layers (we discuss pooling layers in detail in the
    next chapter). The decoder network, which tries to reconstruct the original image
    from the embedding, has to upscale/upsample a lower-resolution input into a higher-resolution
    output.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们简要讨论了自动编码器，其中编码器网络将输入图像映射到嵌入，解码器网络试图从嵌入中重建输入图像。编码器网络通过一系列卷积和池化层将高分辨率输入转换为低分辨率嵌入（我们将在下一章详细讨论池化层）。试图从嵌入中重建原始图像的解码器网络必须将低分辨率输入上采样/上采样到高分辨率输出。
- en: 'Many interpolation techniques, such as nearest neighbor, bilinear, and bicubic
    interpolation, can be used to perform this upsampling operation. These techniques
    typically use predefined mathematical functions to map lower-resolution inputs
    to higher-resolution outputs. However, a more optimal way to perform upsampling
    is through transpose convolutions, where the mapping function is learned during
    the training process instead of being predefined. The neural network will learn
    the best way to distribute the input elements across a higher-resolution output
    map so that the final reconstruction error is minimized (that is, the final output
    is as close to the original input image as possible). We do not get into the details
    of training an autoencoder in this chapter; however, we show how input images
    can be upsampled using transpose convolutions:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 许多插值技术，如最近邻、双线性插值和双三次插值，可以用于执行此上采样操作。这些技术通常使用预定义的数学函数将低分辨率输入映射到高分辨率输出。然而，执行上采样的更优方式是通过转置卷积，其中映射函数是在训练过程中学习的，而不是预先定义的。神经网络将学习最佳方式将输入元素分布到更高分辨率的输出图中，以便最终重建误差最小化（即最终输出尽可能接近原始输入图像）。在本章中，我们不深入讨论自动编码器的训练细节；然而，我们展示了如何使用转置卷积对输入图像进行上采样：
- en: The input array is converted to a 4D tensor of shape *N* × *C[in]* × *H* × *W*,
    where *N* is the batch size, *C[in]* is the number of input channels, *H* is the
    height, and *W* is the width.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数组被转换为形状为 *N* × *C[in]* × *H* × *W* 的 4D 张量，其中 *N* 是批处理大小，*C[in]* 是输入通道数，*H*
    是高度，*W* 是宽度。
- en: The kernel is a 4D tensor of shape *C[in]* × *C[out]* × *k[H]* × *k[W]*, where
    *C[in]* is the number of input channels, *C[out]* is the number of output channels,
    *k[H]* is the kernel height, and *k[W]* is the kernel width. Note how this differs
    from the regular 2D convolutional kernel, which is expected to be of shape *C[out]*
    × *C[in]* × *k[H]* × *k[W]*. Essentially, the input and output channel dimensions
    are interchanged.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核是一个形状为 *C[in]* × *C[out]* × *k[H]* × *k[W]* 的 4D 张量，其中 *C[in]* 是输入通道数，*C[out]*
    是输出通道数，*k[H]* 是核高度，*k[W]* 是核宽度。注意这与常规 2D 卷积核不同，常规 2D 卷积核的形状应为 *C[out]* × *C[in]*
    × *k[H]* × *k[W]*。本质上，输入和输出通道维度是互换的。
- en: Figure [10.18](#fig-transpose-conv2d-stride-2) shows an example with input of
    shape 1 × 1 × 2 × 2. The kernel is of shape 1 × 1 × 2 × 2. Transpose convolution
    with stride 2 results in an output of shape 1 × 1 × 4 × 4.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [10.18](#fig-transpose-conv2d-stride-2) 展示了一个输入形状为 1 × 1 × 2 × 2 的示例。卷积核的形状为
    1 × 1 × 2 × 2。步长为 2 的转置卷积会产生一个形状为 1 × 1 × 4 × 4 的输出。
- en: '![](../../OEBPS/Images/CH10_F18_Chaudhury.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH10_F18_Chaudhury.png)'
- en: Figure 10.18 Upscaling using 2D transpose convolution with stride 2
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.18 使用步长 2 的 2D 转置卷积进行上采样
- en: NOTE Fully functional code for transpose convolution, executable via Jupyter
    Notebook, can be found at [http://mng.bz/radD](http://mng.bz/radD).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：完全功能的转置卷积代码，可通过 Jupyter Notebook 执行，可在 [http://mng.bz/radD](http://mng.bz/radD)
    找到。
- en: Listing 10.7 PyTorch code for upsampling using transpose convolutions
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.7 使用转置卷积进行上采样的 PyTorch 代码
- en: '[PRE6]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① Instantiates the input tensor
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ① 实例化输入张量
- en: ② Instantiates the weights of the kernel
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ② 实例化核权重
- en: ③ Converts the input tensor to *N* × *C[in]* × *H* × *W* = 1 × 1 × 2 × 2
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将输入张量转换为 *N* × *C[in]* × *H* × *W* = 1 × 1 × 2 × 2
- en: ④ Converts the kernel to *C[in]* × *C[out]* × *k[H]* × *k[W]* = 1 × 1 × 2 ×
    2
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将核转换为 *C[in]* × *C[out]* × *k[H]* × *k[W]* = 1 × 1 × 2 × 2
- en: ⑤ Instantiates the transpose convolution layer
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 实例化转置卷积层
- en: ⑥ Sets the kernel weights
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 设置核权重
- en: ⑦ Instructs PyTorch to not compute gradients since we currently don’t require
    them
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 指示 PyTorch 不计算梯度，因为我们目前不需要它们
- en: ⑧ Runs the transpose convolution. *y* is of shape 4 × 4.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 执行转置卷积。*y* 的形状为 4 × 4。
- en: 10.6 Adding convolution layers to a neural network
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.6 将卷积层添加到神经网络中
- en: Until now, we have been discussing convolution layers with custom weights that
    we set. While this gives us a conceptual understanding of how convolution works,
    in real neural networks, we do *not* set the convolution weights ourselves. Rather,
    we expect the weights to be learned from loss minimization via backpropagation,
    as described in chapters [8](../Text/08.xhtml#ch-training-neural-networks) and
    [9](../Text/09.xhtml#ch-loss-optim-reg). We look at popular neural network architectures
    in the next chapter. But from a programming point of view, the most important
    thing to learn is how to add a convolution layer to a neural network. This is
    what we learn in the following section.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在讨论具有自定义权重的卷积层，这些权重是我们设置的。虽然这让我们对卷积的工作原理有了概念性的理解，但在实际的神经网络中，我们**并不**自己设置卷积权重。相反，我们期望权重通过反向传播中的损失最小化来学习，正如第[8](../Text/08.xhtml#ch-training-neural-networks)章和第[9](../Text/09.xhtml#ch-loss-optim-reg)章所描述的。我们将在下一章中查看流行的神经网络架构。但从编程的角度来看，最重要的学习内容是如何将卷积层添加到神经网络中。这就是我们在下一节要学习的内容。
- en: As part of setting up the neural network, we specify its dimensions but not
    the weights. We also initialize the weight values. The weight values are updated
    during the backpropagation (the `loss.backward()` call) somewhat behind the scene
    (although PyTorch allows us to view their values if we choose to).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置神经网络的过程中，我们指定其维度但不指定权重。我们还初始化权重值。权重值在反向传播（`loss.backward()`调用）过程中稍后更新（尽管PyTorch允许我们选择查看它们的值）。
- en: 10.6.1 PyTorch- Adding convolution layers to a neural network
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.6.1 在PyTorch中向神经网络添加卷积层
- en: 'Let’s see how a convolutional layer is implemented as part of a larger neural
    network in PyTorch (the full neural network architecture is discussed in detail
    in the next chapter):'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在PyTorch（完整的神经网络架构将在下一章中详细讨论）中将卷积层作为更大神经网络的一部分来实现：
- en: A neural network typically subclasses the `torch.nn.Module` base class and implements
    the `forward()` method. The layers of the neural network are instantiated in the
    `__init__()` function.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络通常子类化`torch.nn.Module`基类并实现`forward()`方法。神经网络的层在`__init__()`函数中实例化。
- en: '`torch.nn.Sequential` is used to chain multiple layers one after another. The
    output of the first layer is fed into the second layer, and so on.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`torch.nn.Sequential`将多个层依次连接起来。第一层的输出被馈送到第二层，依此类推。
- en: Each `torch.nn.Conv2d()` represents a single convolutional layer. Our code snippet
    instantiates three such convolutional layers with other layers in between (details
    are covered in the next chapter).
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个`torch.nn.Conv2d()`代表一个单独的卷积层。我们的代码片段实例化了三个这样的卷积层，层之间有其他层（详细信息将在下一章中介绍）。
- en: Listing 10.8 PyTorch code for a sample convolutional neural network
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.8 PyTorch示例卷积神经网络的代码
- en: '[PRE7]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① torch.nn.Sequential is used to chain a sequence of layers together.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用`torch.nn.Sequential`将一系列层连接起来。
- en: ② Instantiates the convolutional layer
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ② 实例化卷积层
- en: ③ Implements the forward pass
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 实现前向传播
- en: ④ Runs the convolution
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 执行卷积
- en: 10.7 Pooling
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.7 池化
- en: Until now, we have seen how a convolution layer slides over an input image and
    generates an output feature map that contains important features that describe
    the image. We looked at this in 1D, 2D, and 3D settings. In a typical deep neural
    network, multiple such convolution layers are stacked one after another to recognize
    more and more complex structures in the image. (We talk more about this in the
    next chapter.) A major drawback of the convolution layer is that it is very sensitive
    to the location of the features in the input. Minor variations in the position
    of input features can result in a different output feature map. Such variations
    can occur in the real world due to camera angle changes, rotations, crops, objects
    being present at varying distances from the camera, and so on. How do we handle
    such variations and make the neural network more robust?
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了卷积层如何在输入图像上滑动并生成包含描述图像的重要特征的输出特征图。我们在1D、2D和3D设置中探讨了这一点。（我们将在下一章中更多地讨论这一点。）卷积层的一个主要缺点是它对输入中特征的定位非常敏感。输入特征位置的微小变化可能导致不同的输出特征图。这种变化可能由于相机角度变化、旋转、裁剪、物体以不同距离从相机存在等原因在现实世界中发生。我们如何处理这种变化并使神经网络更加鲁棒？
- en: One way to do so is via downsampling. A lower-resolution version of the feature
    map still contains the important features but at a lower precision/granularity.
    So even if important features are present at slightly varying locations in higher-resolution
    feature maps, they will be more or less at the same location in the lower-resolution
    feature maps. This is also known as *local translation invariance*.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 一种实现方式是通过下采样。特征图的一个低分辨率版本仍然包含重要的特征，但精度/粒度较低。因此，即使重要特征在高分辨率特征图中位于略微不同的位置，它们在低分辨率特征图中也大致位于相同的位置。这也被称为*局部平移不变性*。
- en: 'In convolution neural networks, the downsampling operation is performed by
    *pooling* layers. Pooling layers essentially slide a small filter across the entire
    image. At each filter location, they capture a summary of the local patch using
    a pooling operation. The two most popular types of pooling operations are as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积神经网络中，下采样操作是通过*池化*层来执行的。池化层本质上是在整个图像上滑动一个小滤波器。在每个滤波器位置，它们使用池化操作捕获局部补丁的摘要。以下是最流行的两种池化操作类型：
- en: '*Max pooling*—Calculates the maximum value for each patch'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最大池化*—计算每个补丁的最大值'
- en: '*Average pooling*—Calculates the average value for each patch'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*平均池化*—计算每个补丁的平均值'
- en: Figure [10.19](#fig-2d-maxpool-stride-2) illustrates this in detail. The size
    of the output feature map depends on the kernel size and the stride of the pooling
    layer. For example, if we use a 2 × 2 kernel with a stride of 2, as in figures
    [10.19](#fig-2d-maxpool-stride-2) and [10.20](#fig-2d-avgpool-stride-2), the output
    feature map becomes half the size of the input feature map. Similarly, using a
    3 × 3 kernel with stride = 3 makes the output feature map one-third the size.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 图[10.19](#fig-2d-maxpool-stride-2)详细说明了这一点。输出特征图的大小取决于核大小和池化层的步长。例如，如果我们使用2
    × 2核和步长为2，如图[10.19](#fig-2d-maxpool-stride-2)和[10.20](#fig-2d-avgpool-stride-2)所示，输出特征图变为输入特征图大小的一半。同样，使用3
    × 3核和步长= 3使得输出特征图大小为输入特征图的三分之一。
- en: '![](../../OEBPS/Images/CH10_F19_Chaudhury.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH10_F19_Chaudhury.png)'
- en: Figure 10.19 Max pooling using a 2 × 2 kernel with stride 2\. The resulting
    output feature map is half the size of the input feature map. Each value of the
    output feature map is a max of the corresponding local patch in the input feature
    map.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.19 使用2 × 2核和步长2的最大池化。结果输出特征图是输入特征图大小的一半。输出特征图中的每个值是输入特征图中相应局部补丁的最大值。
- en: Listing 10.9 PyTorch code for max and average pooling
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.9 PyTorch代码用于最大池化和平均池化
- en: '[PRE8]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Instantiates a 4 × 4 input tensor
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ① 实例化一个4 × 4输入张量
- en: ② Instantiates a 2 × 2 max pooling layer with stride 2
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ② 实例化一个步长为2的2 × 2最大池化层
- en: ③ Output feature map is of size 2 × 2
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 输出特征图大小为2 × 2
- en: ④ Instantiates a 2 × 2 average pooling layer with stride 2
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 实例化一个步长为2的2 × 2平均池化层
- en: ⑤ Output feature map is of size 2 × 2
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 输出特征图大小为2 × 2
- en: '![](../../OEBPS/Images/CH10_F20_Chaudhury.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH10_F20_Chaudhury.png)'
- en: Figure 10.20 Average pooling using a 2 × 2 kernel with stride 2\. The resulting
    output feature map is corresponding local patch in the input feature map.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.20 使用2 × 2核和步长2的平均池化。结果输出特征图对应于输入特征图中的局部补丁。
- en: Summary
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we took an in-depth look at 1D, 2D, and 3D convolutions and
    their application to image and video analysis:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了1D、2D和3D卷积及其在图像和视频分析中的应用：
- en: Convolutional layers help capture local patterns in input data because they
    connect only a small set of adjacent input values to an output value. This is
    different from the fully connected layers (aka linear layers) discussed in the
    previous chapters, where all inputs are connected to every output value.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层有助于捕捉输入数据中的局部模式，因为它们只将一小组相邻的输入值连接到输出值。这与前几章中讨论的完全连接层（也称为线性层）不同，其中所有输入都连接到每个输出值。
- en: A convolution operation involves sliding a kernel over an input array. It can
    conceptually be viewed as a matrix multiplication though it is not implemented
    this way for efficiency reasons). The kernel size, stride, and padding affect
    the size of the output.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积操作涉及在输入数组上滑动一个核。虽然从概念上可以将其视为矩阵乘法，但由于效率原因，实际上并不是这样实现的。核大小、步长和填充会影响输出的大小。
- en: The number of input elements over which the kernel slides upon completing a
    single step is known as *stride*.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核在完成单步滑动后覆盖的输入元素数量称为*步长*。
- en: As the kernel reaches the extremities of the input array, parts of it may fall
    outside the array. To deal with such cases, multiple *padding* strategies can
    be applied. In *valid padding*, the convolution operation stops when even a single
    kernel element falls outside the input array. In *same (zero) padding*, an input
    value of zero is assumed for all kernel elements that are outside the input array.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当内核达到输入数组的末端时，其部分可能超出数组范围。为了处理这种情况，可以应用多种*填充*策略。在*有效填充*中，当内核的任何一个元素超出输入数组时，卷积操作就会停止。在*相同（零）填充*中，假设所有超出输入数组的内核元素输入值为零。
- en: 1D convolutions can conceptually be viewed as sliding a measuring ruler (1D
    kernel) across a stretched, straightened rope (1D input array). Real-world applications
    of 1D convolutions include smoothing and edge detection in curves.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从概念上讲，1D卷积可以看作是将测量尺（1D内核）在拉直的绳索（1D输入数组）上滑动。1D卷积的实际应用包括曲线中的平滑和边缘检测。
- en: 2D convolutions can conceptually be viewed as sliding a tile (2D kernel) over
    the entire surface area of a wall (2D input array). Real-world applications of
    2D convolutions include smoothing and edge detection in images.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从概念上讲，2D卷积可以看作是将瓦片（2D内核）在整个墙壁（2D输入数组）的表面积上滑动。2D卷积的实际应用包括图像中的平滑和边缘检测。
- en: 3D convolutions can conceptually be viewed as sliding a brick (3D kernel) over
    the entire volume of a room 3D input array). Real-world applications of 3D convolutions
    include motion detection in an image sequence.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从概念上讲，3D卷积可以看作是将砖块（3D内核）在整个房间3D输入数组的整个体积上滑动。3D卷积的实际应用包括图像序列中的运动检测。
- en: In transpose convolutions, the input array elements are multiplied by the kernel
    weights and then distributed across the output array. Real-world applications
    of transpose convolutions include upsampling, where lower-resolution inputs are
    converted into higher-resolution outputs. Autoencoders use transpose convolutions
    to reconstruct images from embeddings.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在转置卷积中，输入数组元素与内核权重相乘，然后分布到输出数组中。转置卷积的实际应用包括上采样，将低分辨率输入转换为高分辨率输出。自动编码器使用转置卷积从嵌入中重建图像。
- en: Pooling layers essentially slide a kernel across the input, capturing a summary
    of the local patch at each kernel location. They help improve the robustness of
    convolutional neural networks to minor variations in input features. The two most
    popular pooling operations are max pooling (calculates the maximum value of the
    local patch) and average pooling (calculates the average value of the local patch).
    Pooling layers result in downsampling of the input array. The output size depends
    on the size and stride of the pooling kernel.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层本质上是在输入上滑动内核，捕获每个内核位置的局部区域的摘要。它们有助于提高卷积神经网络对输入特征微小变化的鲁棒性。最流行的两种池化操作是最大池化（计算局部区域的最大值）和平均池化（计算局部区域的平均值）。池化层导致输入数组下采样。输出大小取决于池化内核的大小和步长。
