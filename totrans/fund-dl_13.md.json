["```py\ndef calculate_naive_returns(rewards):\n\"\"\" Calculates a list of naive returns given a \n    list of rewards.\"\"\"\n\u00a0 \u00a0 total_returns = np.zeros(len(rewards))\n\u00a0 \u00a0 total_return = 0.0\n\u00a0 \u00a0 for t in range(len(rewards), 0):\n\u00a0 \u00a0 \u00a0 \u00a0 total_return = total_return + reward\n\u00a0 \u00a0 \u00a0 \u00a0 total_returns[t] = total_return\n\u00a0 \u00a0 return total_returns\n```", "```py\ndef discount_rewards(rewards, gamma=0.98):\n\u00a0 \u00a0 discounted_returns = [0 for _ in rewards]\n\u00a0 \u00a0 discounted_returns[-1] = rewards[-1]\n\u00a0 \u00a0 for t in range(len(rewards)-2, -1, -1): # iterate backwards\n\u00a0 \u00a0 \u00a0 \u00a0 discounted_returns[t] = rewards[t] + \n\u00a0 \u00a0 \u00a0 \u00a0   discounted_returns[t+1]*gamma\n\u00a0 \u00a0 return discounted_returns\n\n```", "```py\ndef epsilon_greedy_action(action_distribution,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0epsilon=1e-1):\n\u00a0\u00a0\u00a0\u00a0action_distribution = action_distribution.detach().numpy()\n\u00a0\u00a0\u00a0\u00a0if random.random() < epsilon:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return np.argmax(np.random.random(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action_distribution.shape))\n\u00a0\u00a0\u00a0\u00a0else:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return np.argmax(action_distribution)\n\n```", "```py\ndef epsilon_greedy_action_annealed(action_distribution,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0percentage,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0epsilon_start=1.0,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0epsilon_end=1e-2):\n\u00a0\u00a0\u00a0\u00a0action_distribution = action_distribution.detach().numpy()\n\u00a0\u00a0\u00a0\u00a0annealed_epsilon = (epsilon_start*(1.0-percentage) +\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0epsilon_end*percentage)\n\u00a0\u00a0\u00a0\u00a0if random.random() < annealed_epsilon:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return np.argmax(np.random.random(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action_distribution.shape))\n\u00a0\u00a0\u00a0\u00a0else:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return np.argmax(action_distribution)\n\n```", "```py\nfrom torch import optim\nclass PGAgent(object):\n\u00a0\u00a0\u00a0\u00a0def __init__(self, state_size, num_actions,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0hidden_size,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0learning_rate=1e-3,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0explore_exploit_setting= \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0'epsilon_greedy_annealed_1.0->0.001'):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.state_size = state_size\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.num_actions = num_actions\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.hidden_size = hidden_size\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.learning_rate = learning_rate\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.explore_exploit_setting = \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0explore_exploit_setting\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.build_model()\n\n\u00a0\u00a0\u00a0\u00a0def build_model(self):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.model = torch.nn.Sequential(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(self.state_size, self.hidden_size),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(self.hidden_size, self.hidden_size),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(self.hidden_size, self.num_actions),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Softmax(dim=0))\n\n\u00a0\u00a0\u00a0\u00a0def train(self, state, action_input, reward_input):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0state = torch.tensor(state).float()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action_input = torch.tensor(action_input).long()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0reward_input = torch.tensor(reward_input).float()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.output = self.model(state)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Select the logits related to the action taken\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0logits_for_actions = self.output.gather(1,\n\u00a0                                          action_input.view(-1,1))\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.loss = -torch.mean(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0torch.log(logits_for_actions) * reward_input)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.loss.backward()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.optimizer = optim.Adam(self.model.parameters())\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.optimizer.step()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.optimizer.zero_grad()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return self.loss.item()\n\n\u00a0\u00a0\u00a0\u00a0def sample_action_from_distribution(self,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action_distribution,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0epsilon_percentage):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Choose an action based on the action probability\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# distribution and an explore vs exploit\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if self.explore_exploit_setting == 'greedy':\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action = epsilon_greedy_action(action_distribution,\n\u00a0                                            0.00)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0elif self.explore_exploit_setting == 'epsilon_greedy_0.05':\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action = epsilon_greedy_action(action_distribution,\n\u00a0                                            0.05)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0elif self.explore_exploit_setting == 'epsilon_greedy_0.25':\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action = epsilon_greedy_action(action_distribution,\n\u00a0                                            0.25)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0elif self.explore_exploit_setting == 'epsilon_greedy_0.50':\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action = epsilon_greedy_action(action_distribution,\n\u00a0                                            0.50)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0elif self.explore_exploit_setting == 'epsilon_greedy_0.90':\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action = epsilon_greedy_action(action_distribution,\n\u00a0                                            0.90)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0elif self.explore_exploit_setting == \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0'epsilon_greedy_annealed_1.0->0.001':\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action = epsilon_greedy_action_annealed(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action_distribution,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0epsilon_percentage, 1.0,0.001)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0elif self.explore_exploit_setting == \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0'epsilon_greedy_annealed_0.5->0.001':\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action = epsilon_greedy_action_annealed(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action_distribution,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0epsilon_percentage, 0.5, 0.001)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0elif self.explore_exploit_setting == \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0'epsilon_greedy_annealed_0.25->0.001':\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action = epsilon_greedy_action_annealed(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action_distribution,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0epsilon_percentage, 0.25, 0.001)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return action\n\n\u00a0\u00a0\u00a0\u00a0def predict_action(self, state, epsilon_percentage):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action_distribution = self.model(\n\u00a0                                torch.from_numpy(state).float())\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action = self.sample_action_from_distribution(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action_distribution, epsilon_percentage)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return action\n\n```", "```py\nclass EpisodeHistory(object):\n\n\u00a0\u00a0\u00a0\u00a0def __init__(self):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.states = []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.actions = []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.rewards = []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.state_primes = []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.discounted_returns = []\n\n\u00a0\u00a0\u00a0\u00a0def add_to_history(self, state, action, reward,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0state_prime):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.states.append(state)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.actions.append(action)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.rewards.append(reward)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.state_primes.append(state_prime)\n\nclass Memory(object):\n\n\u00a0\u00a0\u00a0\u00a0def __init__(self):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.states = []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.actions = []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.rewards = []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.state_primes = []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.discounted_returns = []\n\n\u00a0\u00a0\u00a0\u00a0def reset_memory(self):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.states = []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.actions = []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.rewards = []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.state_primes = []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.discounted_returns = []\n\n\u00a0\u00a0\u00a0\u00a0def add_episode(self, episode):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.states += episode.states\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.actions += episode.actions\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.rewards += episode.rewards\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.discounted_returns += episode.discounted_returns\n\n```", "```py\n# Configure Settings\n#total_episodes = 5000\ntotal_episodes = 16\ntotal_steps_max = 10000\nepsilon_stop = 3000\ntrain_frequency = 8\nmax_episode_length = 500\nrender_start = -1\nshould_render = False\n\nexplore_exploit_setting = 'epsilon_greedy_annealed_1.0->0.001'\n\nenv = gym.make('CartPole-v0')\nstate_size = env.observation_space.shape[0]\u00a0\u00a0# 4 for\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# CartPole-v0\nnum_actions = env.action_space.n\u00a0\u00a0# 2 for CartPole-v0\n\nsolved = False\nagent = PGAgent(state_size=state_size,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0num_actions=num_actions,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0hidden_size=16,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0explore_exploit_setting= \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0explore_exploit_setting)\n\nepisode_rewards = []\nbatch_losses = []\n\nglobal_memory = Memory()\nsteps = 0\nfor i in range(total_episodes):\n\u00a0\u00a0state = env.reset()\n\u00a0\u00a0episode_reward = 0.0\n\u00a0\u00a0episode_history = EpisodeHistory()\n\u00a0\u00a0epsilon_percentage = float(min(i/float(epsilon_stop), 1.0))\n\n\u00a0\u00a0for j in range(max_episode_length):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action = agent.predict_action(state, epsilon_percentage)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0state_prime, reward, terminal, _ = env.step(action)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0episode_history.add_to_history(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0state, action, reward, state_prime)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0state = state_prime\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0episode_reward += reward\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0steps += 1\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if j == (max_episode_length - 1):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0terminal = True\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if terminal:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0episode_history.discounted_returns = \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0discount_rewards(episode_history.rewards)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0global_memory.add_episode(episode_history)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# every 8th episode train the NN\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# train on all actions from episodes in memory, \n\u00a0         # then reset memory\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if np.mod(i, train_frequency) == 0:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0reward_input = global_memory.discounted_returns\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action_input = global_memory.actions\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0state = global_memory.states\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# train step\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0batch_loss = agent.train(state, action_input, \n\u00a0                                    reward_input)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# print(f'Batch loss: {batch_loss}')\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# batch_losses.append(batch_loss)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0global_memory.reset_memory()\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0episode_rewards.append(episode_reward)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if i % 10 == 0:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0mean_rewards = torch.mean(torch.tensor(\n\u00a0                                        episode_rewards[:-10]))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if mean_rewards > 10.0:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0solved = True\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0else:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0solved = False\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0print(f'Solved: {solved} Mean Reward: {mean_rewards}')\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0break # stop playing if terminal\n\n\u00a0\u00a0print(f'Episode[{i}]: {len(episode_history.actions)} \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0actions {episode_reward} reward')\n\n```", "```py\n# DQNAgent\n\nclass DQNAgent(object):\n\n\u00a0\u00a0\u00a0\u00a0def __init__(self, num_actions,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0learning_rate=1e-3, history_length=4,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0screen_height=84, screen_width=84,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0gamma=0.99):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.num_actions = num_actions\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.learning_rate = learning_rate\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.history_length = history_length\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.screen_height = screen_height\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.screen_width = screen_width\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.gamma = gamma\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.build_prediction_network()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.build_target_network()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#self.build_training()\n\n\u00a0\u00a0\u00a0\u00a0def build_prediction_network(self):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.model_predict = nn.Sequential(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Conv2d(4, 32, kernel_size=8 , stride=4),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Conv2d(32, 64, kernel_size=4, stride=2),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Conv2d(64, 64, kernel_size=3, stride=1),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Flatten(),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(3136, 512),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(512, self.num_actions)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0)\n\n\u00a0\u00a0\u00a0\u00a0def build_target_network(self):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.model_target = nn.Sequential(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Conv2d(4, 32, kernel_size=8 , stride=4),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Conv2d(32, 64, kernel_size=4, stride=2),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Conv2d(64, 64, kernel_size=3, stride=1),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Flatten(),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(3136, 512),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(512, self.num_actions)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0)\n\n\u00a0\u00a0\u00a0\u00a0def sample_and_train_pred(self, replay_table, batch_size):\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0s_t, action, reward, s_t_plus_1, terminal = \\\n\u00a0                  replay_table.sample_batch(batch_size)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# given state_t, find q_t (predict_model) and \n\u00a0       #  q_t+1 (target_model)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# do it in batches\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Find q_t_plus_1\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0input_t = torch.from_numpy(s_t_plus_1).float()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model_t = self.model_target.float()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0q_t_plus_1 = model_t(input_t)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0terminal = torch.tensor(terminal).float()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0max_q_t_plus_1, _ = torch.max(q_t_plus_1, dim=1)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0reward = torch.from_numpy(reward).float()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0target_q_t = (1\\. - terminal) * self.gamma * \\\n\u00a0                    max_q_t_plus_1 + reward\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Find q_t, and q_of_action\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0input_p = torch.from_numpy(s_t).float()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model_p = self.model_predict.float()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0q_t = model_p(input_p)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action = torch.from_numpy(action)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action_one_hot = nn.functional.one_hot(action,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.num_actions)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0q_of_action = torch.sum(q_t * action_one_hot)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Compute loss\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.delta = (target_q_t - q_of_action)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.loss = torch.mean(self.delta)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Update predict_model gradients (only)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.optimizer = optim.Adam(self.model_predict.parameters(),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0lr = self.learning_rate)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.loss.backward()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.optimizer.step()\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return q_t\n\n\u00a0\u00a0\u00a0\u00a0def predict_action(self, state, epsilon_percentage):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0input_p = torch.from_numpy(state).float().unsqueeze(dim=0)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model_p = self.model_predict.float()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action_distribution = model_p(input_p)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# sample from action distribution\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action = epsilon_greedy_action_annealed(\n\u00a0                                      action_distribution.detach(),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0epsilon_percentage)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return action\n\n\u00a0\u00a0\u00a0\u00a0def process_state_into_stacked_frames(self,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0frame,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0past_frames,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0past_state=None):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0full_state = np.zeros((self.history_length,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.screen_width,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.screen_height))\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if past_state is not None:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0for i in range(len(past_state)-1):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0full_state[i, :, :] = past_state[i+1, :, :]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0full_state[-1, :, :] = self.preprocess_frame(frame,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0(self.screen_width,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.screen_height)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0else:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0all_frames = past_frames + [frame]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0for i, frame_f in enumerate(all_frames):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0full_state[i, :, :] = self.preprocess_frame(frame_f,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0(self.screen_width,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.screen_height)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return full_state\n\n\u00a0\u00a0\u00a0\u00a0def to_grayscale(self, x):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return np.dot(x[...,:3], [0.299, 0.587, 0.114])\n\n\u00a0\u00a0\u00a0\u00a0def preprocess_frame(self, im, shape):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0cropped = im[16:201,:] # (185, 160, 3)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0grayscaled = self.to_grayscale(cropped) # (185, 160)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# resize to (84,84)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0resized = np.array(Image.fromarray(grayscaled).resize(shape))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0mean, std = 40.45, 64.15\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0frame = (resized-mean)/std\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return frame\n\n```", "```py\nclass ExperienceReplayTable(object):\n\n\u00a0\u00a0\u00a0\u00a0def __init__(self, table_size=50000):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.states = []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.actions = []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.rewards = []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.state_primes = []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.terminals = []\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.table_size = table_size\n\n\u00a0\u00a0\u00a0\u00a0def add_episode(self, episode):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.states += episode.states\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.actions += episode.actions\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.rewards += episode.rewards\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.state_primes += episode.state_primes\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.terminals += episode.terminals\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.purge_old_experiences()\n\n\u00a0\u00a0\u00a0\u00a0def purge_old_experiences(self):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0while len(self.states) > self.table_size:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.states.pop(0)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.actions.pop(0)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.rewards.pop(0)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.state_primes.pop(0)\n\n\u00a0\u00a0\u00a0\u00a0def sample_batch(self, batch_size):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0s_t, action, reward, s_t_plus_1, terminal = [], [], [], \n\u00a0                                                   [], []\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0rands = np.arange(len(self.states))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0np.random.shuffle(rands)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0rands = rands[:batch_size]\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0for r_i in rands:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0s_t.append(self.states[r_i])\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action.append(self.actions[r_i])\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0reward.append(self.rewards[r_i])\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0s_t_plus_1.append(self.state_primes[r_i])\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0terminal.append(self.terminals[r_i])\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return (np.array(s_t), np.array(action), np.array(reward),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0np.array(s_t_plus_1), np.array(terminal))\n\n```", "```py\nlearn_start = 4\ntotal_episodes = 32\nepsilon_stop = 32\ntrain_frequency = 2\ntarget_frequency = 4\nbatch_size = 4\nmax_episode_length = 1000\nenv = gym.make('Breakout-v4')\nnum_actions = env.action_space.n\nsolved = False\n\nagent = DQNAgent(num_actions=num_actions,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0learning_rate=1e-4,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0history_length=4,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0gamma=0.98)\n\nepisode_rewards = []\nq_t_list = []\nbatch_losses = []\npast_frames_last_time = None\n\nreplay_table = ExperienceReplayTable()\nglobal_step_counter = 0\n\nfor i in range(total_episodes):\n\u00a0\u00a0\u00a0\u00a0# Get initial frame -> state\n\u00a0\u00a0\u00a0\u00a0frame = env.reset() # np.array of shape (210, 160, 3)\n\u00a0\u00a0\u00a0\u00a0# past_frames is a list of past 3 frames (np.arrays)\n\u00a0\u00a0\u00a0\u00a0past_frames = [copy.deepcopy(frame) for _ in range(\n\u00a0                                          agent.history_length-1)]\n\u00a0\u00a0\u00a0\u00a0state = agent.process_state_into_stacked_frames(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0frame, past_frames, past_state=None) # state is (4,84,84)\n\n\u00a0\u00a0\u00a0\u00a0# initialize episode history (s_t, a, r, s_t+1, terminal)\n\u00a0\u00a0\u00a0\u00a0episode_reward = 0.0\n\u00a0\u00a0\u00a0\u00a0episode_history = EpisodeHistory()\n\u00a0\u00a0\u00a0\u00a0epsilon_percentage = float(min(i/float(epsilon_stop), 1.0))\n\n\u00a0\u00a0\u00a0\u00a0for j in range(max_episode_length):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# predict action or choose random action at first\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if global_step_counter < learn_start:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action = np.argmax(np.random.random((agent.num_actions)))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0else:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action = agent.predict_action(state, epsilon_percentage)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# take action, get next frame (-> next state), reward, \n\u00a0       # and terminal\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0reward = 0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0frame_prime, reward, terminal, _ = env.step(action)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if terminal == True:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0reward -= 1\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# get next state from next frame and past frames\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0state_prime = agent.process_state_into_stacked_frames(\n\u00a0                                                  frame_prime,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0past_frames,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0past_state=state)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Update past_frames with frame_prime for next time\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0past_frames.append(frame_prime)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0past_frames = past_frames[len(past_frames)- \\\n\u00a0                           agent.history_length:]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0past_frames_last_time = past_frames\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Add to episode history (state, action, reward, \n\u00a0       #  state_prime, terminal)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0episode_history.add_to_history(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0state, action, reward, state_prime, terminal)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0state = state_prime\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0episode_reward += reward\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0global_step_counter += 1\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0\u00a0Do not train predict_model until we have enough\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0\u00a0\u00a0episodes in episode history\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if global_step_counter > learn_start:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if global_step_counter % train_frequency == 0:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if(len(replay_table.actions) != 0):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0q_t = agent.sample_and_train_pred(replay_table, \n\u00a0                                                 batch_size)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0q_t_list.append(q_t)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if global_step_counter % target_frequency == 0:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0agent.model_target.load_state_dict(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0agent.model_predict.state_dict())\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# If terminal or max episodes reached,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0\u00a0\u00a0add episode_history to replay table\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if j == (max_episode_length - 1):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0terminal = True\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if terminal:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0replay_table.add_episode(episode_history)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0episode_rewards.append(episode_reward)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0break\n\u00a0\u00a0\u00a0\u00a0print(f'Episode[{i}]: {len(episode_history.actions)} \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0actions {episode_reward} reward')\n\n```"]