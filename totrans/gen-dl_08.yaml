- en: Chapter 5\. Autoregressive Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。自回归模型
- en: So far, we have explored two different families of generative models that have
    both involved latent variables—variational autoencoders (VAEs) and generative
    adversarial networks (GANs). In both cases, a new variable is introduced with
    a distribution that is easy to sample from and the model learns how to *decode*
    this variable back into the original domain.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了两种涉及潜变量的生成模型家族——变分自动编码器（VAEs）和生成对抗网络（GANs）。在这两种情况下，引入了一个新变量，其分布易于抽样，模型学习如何将此变量*解码*回原始领域。
- en: We will now turn our attention to *autoregressive models*—a family of models
    that simplify the generative modeling problem by treating it as a sequential process.
    Autoregressive models condition predictions on previous values in the sequence,
    rather than on a latent random variable. Therefore, they attempt to explicitly
    model the data-generating distribution rather than an approximation of it (as
    in the case of VAEs).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将把注意力转向*自回归模型*——一类通过将生成建模问题简化为一个顺序过程的模型家族。自回归模型将预测条件放在序列中的先前值上，而不是在潜在随机变量上。因此，它们试图明确地对数据生成分布建模，而不是对其进行近似（如VAEs的情况）。
- en: 'In this chapter we shall explore two different autoregressive models: long
    short-term memory networks and PixelCNN. We will apply the LSTM to text data and
    the PixelCNN to image data. We will cover another highly successful autoregressive
    model, the Transformer, in detail in [Chapter 9](ch09.xhtml#chapter_transformer).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨两种不同的自回归模型：长短期记忆网络和PixelCNN。我们将把LSTM应用于文本数据，将PixelCNN应用于图像数据。我们将在[第9章](ch09.xhtml#chapter_transformer)中详细介绍另一个非常成功的自回归模型Transformer。
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: To understand how an LSTM works, we will first pay a visit to a strange prison,
    where the inmates have formed a literary society…​
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解LSTM的工作原理，我们将首先访问一个奇怪的监狱，那里的囚犯们组成了一个文学社团...​
- en: 'The story of Mr. Sopp and his crowdsourced fables is an analogy for one of
    the most notorious autoregressive techniques for sequential data such as text:
    the long short-term memory network.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Sopp先生及其众包寓言的故事是对一种臭名昭著的用于文本等序列数据的自回归技术的类比：长短期记忆网络。
- en: Long Short-Term Memory Network (LSTM)
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆网络（LSTM）
- en: An LSTM is a particular type of recurrent neural network (RNN). RNNs contain
    a recurrent layer (or *cell*) that is able to handle sequential data by making
    its own output at a particular timestep form part of the input to the next timestep.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM是一种特殊类型的循环神经网络（RNN）。RNN包含一个循环层（或*单元），能够通过使其在特定时间步的输出成为下一个时间步的输入的一部分来处理序列数据。
- en: When RNNs were first introduced, recurrent layers were very simple and consisted
    solely of a tanh operator that ensured that the information passed between timesteps
    was scaled between –1 and 1\. However, this approach was shown to suffer from
    the vanishing gradient problem and didn’t scale well to long sequences of data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当RNN首次引入时，循环层非常简单，仅包含一个tanh运算符，确保在时间步之间传递的信息在-1和1之间缩放。然而，这种方法被证明存在梯度消失问题，并且在处理长序列数据时不具备良好的可扩展性。
- en: LSTM cells were first introduced in 1997 in a paper by Sepp Hochreiter and Jürgen
    Schmidhuber.^([1](ch05.xhtml#idm45387018222496)) In the paper, the authors describe
    how LSTMs do not suffer from the same vanishing gradient problem experienced by
    vanilla RNNs and can be trained on sequences that are hundreds of timesteps long.
    Since then, the LSTM architecture has been adapted and improved, and variations
    such as gated recurrent units (discussed later in this chapter) are now widely
    utilized and available as layers in Keras.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM单元最初是在1997年由Sepp Hochreiter和Jürgen Schmidhuber的一篇论文中首次引入的。^([1](ch05.xhtml#idm45387018222496))在这篇论文中，作者描述了LSTM不会像普通RNN那样遭受梯度消失问题，并且可以在数百个时间步长的序列上进行训练。自那时以来，LSTM架构已经被改进和改良，变体如门控循环单元（本章后面讨论）现在被广泛应用并作为Keras中的层可用。
- en: LSTMs have been applied to a wide range of problems involving sequential data,
    including time series forecasting, sentiment analysis, and audio classification.
    In this chapter we will be using LSTMs to tackle the challenge of text generation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM已经应用于涉及序列数据的各种问题，包括时间序列预测、情感分析和音频分类。在本章中，我们将使用LSTM来解决文本生成的挑战。
- en: Running the Code for This Example
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行此示例的代码
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/05_autoregressive/01_lstm/lstm.ipynb*
    in the book repository.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的代码可以在位于书籍存储库中的Jupyter笔记本中找到，路径为*notebooks/05_autoregressive/01_lstm/lstm.ipynb*。
- en: The Recipes Dataset
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 食谱数据集
- en: We’ll be using the [Epicurious Recipes dataset](https://oreil.ly/laNUt) that
    is available through Kaggle. This is a set of over 20,000 recipes, with accompanying
    metadata such as nutritional information and ingredient lists.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用通过Kaggle提供的[Epicurious食谱数据集](https://oreil.ly/laNUt)。这是一个包含超过20,000个食谱的数据集，附带有营养信息和配料清单等元数据。
- en: You can download the dataset by running the Kaggle dataset downloader script
    in the book repository, as shown in [Example 5-1](#downloading-recipe-dataset).
    This will save the recipes and accompanying metadata locally to the */data* folder.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在书籍存储库中运行Kaggle数据集下载脚本来下载数据集，如[示例5-1](#downloading-recipe-dataset)所示。这将把食谱和相关元数据保存到本地的*/data*文件夹中。
- en: Example 5-1\. Downloading the Epicurious Recipe dataset
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-1。下载Epicurious食谱数据集
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`[Example 5-2](#example0601) shows how the data can be loaded and filtered
    so that only recipes with a title and a description remain. An example of a recipe
    text string is given in [Example 5-3](#text_clean).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`[示例5-2](#example0601)展示了如何加载和过滤数据，以便只保留具有标题和描述的食谱。示例中给出了一个食谱文本字符串，详见[示例5-3](#text_clean)。'
- en: Example 5-2\. Loading the data
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-2。加载数据
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Example 5-3\. A text string from the Recipes dataset
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-3。来自食谱数据集的文本字符串
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Before taking a look at how to build an LSTM network in Keras, we must first
    take a quick detour to understand the structure of text data and how it is different
    from the image data that we have seen so far in this book.`  `## Working with
    Text Data
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在看如何在Keras中构建LSTM网络之前，我们必须先快速了解文本数据的结构以及它与本书中迄今为止看到的图像数据有何不同。## 处理文本数据
- en: 'There are several key differences between text and image data that mean that
    many of the methods that work well for image data are not so readily applicable
    to text data. In particular:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 文本和图像数据之间存在几个关键差异，这意味着许多适用于图像数据的方法并不适用于文本数据。特别是：
- en: Text data is composed of discrete chunks (either characters or words), whereas
    pixels in an image are points in a continuous color spectrum. We can easily make
    a green pixel more blue, but it is not obvious how we should go about making the
    word *cat* more like the word *dog*, for example. This means we can easily apply
    backpropagation to image data, as we can calculate the gradient of our loss function
    with respect to individual pixels to establish the direction in which pixel colors
    should be changed to minimize the loss. With discrete text data, we can’t obviously
    apply backpropagation in the same way, so we need to find a way around this problem.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据由离散块（字符或单词）组成，而图像中的像素是连续色谱中的点。我们可以轻松地将绿色像素变成蓝色，但我们不清楚应该如何使单词“猫”更像单词“狗”，例如。这意味着我们可以轻松地将反向传播应用于图像数据，因为我们可以计算损失函数相对于单个像素的梯度，以确定像素颜色应该如何改变以最小化损失的方向。对于离散文本数据，我们不能明显地以同样的方式应用反向传播，因此我们需要找到解决这个问题的方法。
- en: 'Text data has a time dimension but no spatial dimension, whereas image data
    has two spatial dimensions but no time dimension. The order of words is highly
    important in text data and words wouldn’t make sense in reverse, whereas images
    can usually be flipped without affecting the content. Furthermore, there are often
    long-term sequential dependencies between words that need to be captured by the
    model: for example, the answer to a question or carrying forward the context of
    a pronoun. With image data, all pixels can be processed simultaneously.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据具有时间维度但没有空间维度，而图像数据具有两个空间维度但没有时间维度。文本数据中单词的顺序非常重要，单词倒过来就没有意义，而图像通常可以翻转而不影响内容。此外，单词之间通常存在长期的顺序依赖关系，模型需要捕捉这些依赖关系：例如，回答问题或延续代词的上下文。对于图像数据，所有像素可以同时处理。
- en: Text data is highly sensitive to small changes in the individual units (words
    or characters). Image data is generally less sensitive to changes in individual
    pixel units—a picture of a house would still be recognizable as a house even if
    some pixels were altered—but with text data, changing even a few words can drastically
    alter the meaning of the passage, or make it nonsensical. This makes it very difficult
    to train a model to generate coherent text, as every word is vital to the overall
    meaning of the passage.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据对个体单位（单词或字符）的微小变化非常敏感。图像数据通常对个体像素单位的变化不太敏感——即使一些像素被改变，房子的图片仍然可以被识别为房子——但是对于文本数据，即使改变几个单词也可能极大地改变段落的含义，或使其毫无意义。这使得训练模型生成连贯文本非常困难，因为每个单词对段落的整体含义至关重要。
- en: Text data has a rules-based grammatical structure, whereas image data doesn’t
    follow set rules about how the pixel values should be assigned. For example, it
    wouldn’t make grammatical sense in any context to write “The cat sat on the having.”
    There are also semantic rules that are extremely difficult to model; it wouldn’t
    make sense to say “I am in the beach,” even though grammatically, there is nothing
    wrong with this statement.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据具有基于规则的语法结构，而图像数据不遵循有关如何分配像素值的固定规则。例如，在任何情况下写“猫坐在上面”都没有语法意义。还有一些语义规则极其难以建模；即使从语法上讲，“我在海滩上”这个陈述没有问题，但意义上是不通顺的。
- en: Advances in Text-Based Generative Deep Learning
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于文本的生成式深度学习的进展
- en: Until recently, most of the most sophisticated generative deep learning models
    have focused on image data, because many of the challenges presented in the preceding
    list were beyond the reach of even the most advanced techniques. However, in the
    last five years astonishing progress has been made in the field of text-based
    generative deep learning, thanks to the introduction of the Transformer model
    architecture, which we will explore in [Chapter 9](ch09.xhtml#chapter_transformer).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，大多数最复杂的生成式深度学习模型都集中在图像数据上，因为前面列表中提到的许多挑战甚至超出了最先进技术的范围。然而，在过去的五年中，在基于文本的生成式深度学习领域取得了惊人的进展，这要归功于Transformer模型架构的引入，我们将在第9章中探讨。
- en: With these points in mind, let’s now take a look at the steps we need to take
    in order to get the text data into the right shape to train an LSTM network.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些要点，让我们现在来看看我们需要采取哪些步骤，以便将文本数据整理成适合训练LSTM网络的形式。
- en: Tokenization
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记化
- en: The first step is to clean up and tokenize the text. *Tokenization* is the process
    of splitting the text up into individual units, such as words or characters.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是清理和标记化文本。标记化是将文本分割成单独的单位，如单词或字符的过程。
- en: How you tokenize your text will depend on what you are trying to achieve with
    your text generation model. There are pros and cons to using both word and character
    tokens, and your choice will affect how you need to clean the text prior to modeling
    and the output from your model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如何对文本进行标记化取决于您尝试使用文本生成模型实现什么目标。使用单词和字符标记都有利弊，您的选择将影响您在建模之前需要如何清理文本以及模型输出。
- en: 'If you use word tokens:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用单词标记：
- en: All text can be converted to lowercase, to ensure capitalized words at the start
    of sentences are tokenized the same way as the same words appearing in the middle
    of a sentence. In some cases, however, this may not be desirable; for example,
    some proper nouns, such as names or places, may benefit from remaining capitalized
    so that they are tokenized independently.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有文本都可以转换为小写，以确保句子开头的大写单词与句子中间出现的相同单词以相同方式进行标记化。然而，在某些情况下，这可能不是理想的；例如，一些专有名词，如姓名或地点，可能受益于保持大写，以便它们被独立标记化。
- en: The text *vocabulary* (the set of distinct words in the training set) may be
    very large, with some words appearing very sparsely or perhaps only once. It may
    be wise to replace sparse words with a token for *unknown word*, rather than including
    them as separate tokens, to reduce the number of weights the neural network needs
    to learn.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本*词汇*（训练集中不同单词的集合）可能非常庞大，有些单词可能非常稀疏，甚至可能只出现一次。将稀疏单词替换为*未知单词*的标记可能是明智的选择，而不是将它们作为单独的标记包含在内，以减少神经网络需要学习的权重数量。
- en: Words can be *stemmed*, meaning that they are reduced to their simplest form,
    so that different tenses of a verb remained tokenized together. For example, *browse*,
    *browsing*, *browses*, and *browsed* would all be stemmed to *brows*.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词可以进行*词干处理*，意味着它们被简化为最简单的形式，以便动词的不同时态保持标记化在一起。例如，*browse*、*browsing*、*browses*和*browsed*都将被词干处理为*brows*。
- en: You will need to either tokenize the punctuation, or remove it altogether.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要将标点标记化，或者完全删除它。
- en: Using word tokenization means that the model will never be able to predict words
    outside of the training vocabulary.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单词标记化意味着模型永远无法预测训练词汇表之外的单词。
- en: 'If you use character tokens:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用字符标记：
- en: The model may generate sequences of characters that form new words outside of
    the training vocabulary—this may be desirable in some contexts, but not in others.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型可能生成字符序列，形成训练词汇表之外的新单词——在某些情况下，这可能是可取的，但在其他情况下则不是。
- en: Capital letters can either be converted to their lowercase counterparts, or
    remain as separate tokens.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大写字母可以转换为它们的小写对应词，也可以保留为单独的标记。
- en: The vocabulary is usually much smaller when using character tokenization. This
    is beneficial for model training speed as there are fewer weights to learn in
    the final output layer.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用字符标记时，词汇量通常较小。这对模型训练速度有益，因为最终输出层中需要学习的权重较少。
- en: For this example, we’ll use lowercase word tokenization, without word stemming.
    We’ll also tokenize punctuation marks, as we would like the model to predict when
    it should end sentences or use commas, for example.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用小写单词标记化，不进行词干处理。我们还将标记化标点符号，因为我们希望模型能够预测何时结束句子或使用逗号，例如。
- en: The code in [Example 5-4](#tokenisation) cleans and tokenizes the text.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例5-4](#tokenisation)中的代码清理并标记文本。'
- en: Example 5-4\. Tokenization
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-4。标记化
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](Images/1.png)](#co_autoregressive_models_CO1-1)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_autoregressive_models_CO1-1)'
- en: Pad the punctuation marks, to treat them as separate words.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 填充标点符号，将它们视为单独的单词。
- en: '[![2](Images/2.png)](#co_autoregressive_models_CO1-2)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_autoregressive_models_CO1-2)'
- en: Convert to a TensorFlow Dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 转换为TensorFlow数据集。
- en: '[![3](Images/3.png)](#co_autoregressive_models_CO1-3)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_autoregressive_models_CO1-3)'
- en: Create a Keras `TextVectorization` layer to convert text to lowercase, give
    the most prevalent 10,000 words a corresponding integer token, and trim or pad
    the sequence to 201 tokens long.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个Keras `TextVectorization`层，将文本转换为小写，为最常见的10,000个单词分配相应的整数标记，并将序列修剪或填充到201个标记长。
- en: '[![4](Images/4.png)](#co_autoregressive_models_CO1-4)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_autoregressive_models_CO1-4)'
- en: Apply the `TextVectorization` layer to the training data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将`TextVectorization`层应用于训练数据。
- en: '[![5](Images/5.png)](#co_autoregressive_models_CO1-5)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_autoregressive_models_CO1-5)'
- en: The `vocab` variable stores a list of the word tokens.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`vocab`变量存储一个单词标记列表。'
- en: An example of a recipe after tokenization is shown in [Example 5-5](#text_tokenised).
    The sequence length that we use to train the model is a parameter of the training
    process. In this example we choose to use a sequence length of 200, so we pad
    or clip the recipe to one more than this length, to allow us to create the target
    variable (more on this in the next section). To achieve this desired length, the
    end of the vector is padded with zeros.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记化后，一个配方的示例显示在[示例5-5](#text_tokenised)中。我们用于训练模型的序列长度是训练过程的一个参数。在这个示例中，我们选择使用长度为200的序列长度，因此我们将配方填充或裁剪到比这个长度多一个，以便我们创建目标变量（在下一节中详细介绍）。为了实现这个期望的长度，向量的末尾用零填充。
- en: Stop Tokens
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停止标记
- en: The `0` token is known as a the *stop token*, signifying that the text string
    has come to an end.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`0`标记被称为*停止标记*，表示文本字符串已经结束。'
- en: Example 5-5\. The recipe from [Example 5-3](#text_clean) tokenized
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-5。[示例5-3](#text_clean)中的配方进行了标记化
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In [Example 5-6](#tokens), we can see a subset of the list of tokens mapped
    to their respective indices. The layer reserves the `0` token for padding (i.e.,
    it is the stop token) and the `1` token for unknown words that fall outside the
    top 10,000 words (e.g., persillade). The other words are assigned tokens in order
    of frequency. The number of words to include in the vocabulary is also a parameter
    of the training process. The more words included, the fewer *unknown* tokens you
    will see in the text; however, your model will need to be larger to accommodate
    the larger vocabulary size.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例5-6](#tokens)中，我们可以看到一部分标记列表映射到它们各自的索引。该层将`0`标记保留为填充（即停止标记），将`1`标记保留为超出前10000个单词的未知单词（例如，persillade）。其他单词按频率顺序分配标记。要包含在词汇表中的单词数量也是训练过程的一个参数。包含的单词越多，您在文本中看到的*未知*标记就越少；但是，您的模型需要更大以容纳更大的词汇量。
- en: Example 5-6\. The vocabulary of the `TextVectorization` layer
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-6。`TextVectorization`层的词汇表
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Creating the Training Set
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建训练集
- en: Our LSTM will be trained to predict the next word in a sequence, given a sequence
    of words preceding this point. For example, we could feed the model the tokens
    for *grilled chicken with boiled* and would expect the model to output a suitable
    next word (e.g., *potatoes*, rather than *bananas*).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的LSTM将被训练以预测序列中的下一个单词，给定此点之前的一系列单词。例如，我们可以向模型提供*烤鸡配煮熟的*的标记，期望模型输出一个合适的下一个单词（例如*土豆*，而不是*香蕉*）。
- en: We can therefore simply shift the entire sequence by one token in order to create
    our target variable.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以简单地将整个序列向后移动一个标记，以创建我们的目标变量。
- en: The dataset generation step can be achieved with the code in [Example 5-7](#example0602).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集生成步骤可以通过[示例5-7](#example0602)中的代码实现。
- en: Example 5-7\. Creating the training dataset
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-7。创建训练数据集
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](Images/1.png)](#co_autoregressive_models_CO2-1)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_autoregressive_models_CO2-1)'
- en: Create the training set consisting of recipe tokens (the input) and the same
    vector shifted by one token (the target).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 创建包含食谱标记（输入）和相同向量向后移动一个标记（目标）的训练集。
- en: The LSTM Architecture
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LSTM架构
- en: The architecture of the overall LSTM model is shown in [Table 5-1](#lstm_summary).
    The input to the model is a sequence of integer tokens and the output is the probability
    of each word in the 10,000-word vocabulary appearing next in the sequence. To
    understand how this works in detail, we need to introduce two new layer types,
    `Embedding` and `LSTM`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 整个LSTM模型的架构如[表5-1](#lstm_summary)所示。模型的输入是一系列整数标记，输出是10,000个词汇表中每个单词在序列中出现的概率。为了详细了解这是如何工作的，我们需要介绍两种新的层类型，即`Embedding`和`LSTM`。
- en: Table 5-1\. Model summary of the LSTM
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表5-1。LSTM模型的摘要
- en: '| Layer (type) | Output shape | Param # |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 层（类型） | 输出形状 | 参数 # |'
- en: '| --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| InputLayer | (None, None) | 0 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| InputLayer | (None, None) | 0 |'
- en: '| Embedding | (None, None, 100) | 1,000,000 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Embedding | (None, None, 100) | 1,000,000 |'
- en: '| LSTM | (None, None, 128) | 117,248 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| LSTM | (None, None, 128) | 117,248 |'
- en: '| Dense | (None, None, 10000) | 1,290,000 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Dense | (None, None, 10000) | 1,290,000 |'
- en: '| Total params | 2,407,248 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 总参数 | 2,407,248 |'
- en: '| Trainable params | 2,407,248 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 可训练参数 | 2,407,248 |'
- en: '| Non-trainable params | 0 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 不可训练参数 | 0 |'
- en: The Input Layer of the LSTM
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM的输入层
- en: Notice that the `Input` layer does not need us to specify the sequence length
    in advance. Both the batch size and the sequence length are flexible (hence the
    `(None, None)` shape). This is because all downstream layers are agnostic to the
    length of the sequence being passed through.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`Input`层不需要我们提前指定序列长度。批处理大小和序列长度都是灵活的（因此形状为`(None, None)`）。这是因为所有下游层对通过的序列长度都是不可知的。
- en: The Embedding Layer
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入层
- en: An *embedding layer* is essentially a lookup table that converts each integer
    token into a vector of length `embedding_size`, as shown in [Figure 5-2](#embedding).
    The lookup vectors are learned by the model as *weights*. Therefore, the number
    of weights learned by this layer is equal to the size of the vocabulary multiplied
    by the dimension of the embedding vector (i.e., 10,000 × 100 = 1,000,000).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*嵌入层*本质上是一个查找表，将每个整数标记转换为长度为`embedding_size`的向量，如[图5-2](#embedding)所示。模型通过*权重*学习查找向量。因此，该层学习的权重数量等于词汇表的大小乘以嵌入向量的维度（即10,000
    × 100 = 1,000,000）。'
- en: '![](Images/gdl2_0502.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0502.png)'
- en: Figure 5-2\. An embedding layer is a lookup table for each integer token
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2。嵌入层是每个整数标记的查找表
- en: We embed each integer token into a continuous vector because it enables the
    model to learn a representation for each word that is able to be updated through
    backpropagation. We could also just one-hot encode each input token, but using
    an embedding layer is preferred because it makes the embedding itself trainable,
    thus giving the model more flexibility in deciding how to embed each token to
    improve its performance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每个整数标记嵌入到连续向量中，因为这使得模型能够学习每个单词的表示，这些表示可以通过反向传播进行更新。我们也可以只对每个输入标记进行独热编码，但使用嵌入层更可取，因为它使得嵌入本身是可训练的，从而使模型在决定如何嵌入每个标记以提高性能时更加灵活。
- en: Therefore, the `Input` layer passes a tensor of integer sequences of shape `[batch_size,
    seq_length]` to the `Embedding` layer, which outputs a tensor of shape `[batch_size,
    seq_length, embedding_size]`. This is then passed on to the `LSTM` layer ([Figure 5-3](#embedding_layer)).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`Input`层将形状为`[batch_size, seq_length]`的整数序列张量传递给`Embedding`层，后者输出形状为`[batch_size,
    seq_length, embedding_size]`的张量。然后将其传递给`LSTM`层([图5-3](#embedding_layer))。
- en: '![](Images/gdl2_0503.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0503.png)'
- en: Figure 5-3\. A single sequence as it flows through an embedding layer
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-3。单个序列在嵌入层中流动
- en: The LSTM Layer
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LSTM层
- en: To understand the LSTM layer, we must first look at how a general recurrent
    layer works.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解LSTM层，我们首先必须看一下通用循环层的工作原理。
- en: A recurrent layer has the special property of being able to process sequential
    input data <math alttext="x 1 comma ellipsis comma x Subscript n Baseline"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math>
    . It consists of a cell that updates its *hidden state*, <math alttext="h Subscript
    t"><msub><mi>h</mi> <mi>t</mi></msub></math> , as each element of the sequence
    <math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math> is passed
    through it, one timestep at a time.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 循环层具有特殊属性，能够处理顺序输入数据<math alttext="x 1 comma ellipsis comma x Subscript n Baseline"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math>。随着序列中的每个元素<math
    alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>逐个时间步通过，它会更新其*隐藏状态*<math
    alttext="h Subscript t"><msub><mi>h</mi> <mi>t</mi></msub></math>。
- en: The hidden state is a vector with length equal to the number of *units* in the
    cell—it can be thought of as the cell’s current understanding of the sequence.
    At timestep <math alttext="t"><mi>t</mi></math> , the cell uses the previous value
    of the hidden state, <math alttext="h Subscript t minus 1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    , together with the data from the current timestep <math alttext="x Subscript
    t"><msub><mi>x</mi> <mi>t</mi></msub></math> to produce an updated hidden state
    vector, <math alttext="h Subscript t"><msub><mi>h</mi> <mi>t</mi></msub></math>
    . This recurrent process continues until the end of the sequence. Once the sequence
    is finished, the layer outputs the final hidden state of the cell, <math alttext="h
    Subscript n"><msub><mi>h</mi> <mi>n</mi></msub></math> , which is then passed
    on to the next layer of the network. This process is shown in [Figure 5-4](#lstm_rolled).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏状态是一个向量，其长度等于细胞中的*单元*数——它可以被视为细胞对序列的当前理解。在时间步<math alttext="t"><mi>t</mi></math>，细胞使用先前的隐藏状态值<math
    alttext="h Subscript t minus 1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>，以及当前时间步的数据<math
    alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>，产生一个更新的隐藏状态向量<math
    alttext="h Subscript t"><msub><mi>h</mi> <mi>t</mi></msub></math>。这个循环过程持续到序列结束。一旦序列结束，该层输出细胞的最终隐藏状态<math
    alttext="h Subscript n"><msub><mi>h</mi> <mi>n</mi></msub></math>，然后传递给网络的下一层。这个过程在[图5-4](#lstm_rolled)中显示。
- en: '![](Images/gdl2_0504.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0504.png)'
- en: Figure 5-4\. A simple diagram of a recurrent layer
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-4。循环层的简单图示
- en: To explain this in more detail, let’s unroll the process so that we can see
    exactly how a single sequence is fed through the layer ([Figure 5-5](#lstm_layer)).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更详细地解释这一点，让我们展开这个过程，这样我们就可以看到单个序列是如何通过该层传递的（[图5-5](#lstm_layer)）。
- en: Cell Weights
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 细胞权重
- en: It’s important to remember that all of the cells in this diagram share the same
    weights (as they are really the same cell). There is no difference between this
    diagram and [Figure 5-4](#lstm_rolled); it’s just a different way of drawing the
    mechanics of a recurrent layer.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，这个图中的所有细胞共享相同的权重（因为它们实际上是相同的细胞）。这个图与[图5-4](#lstm_rolled)没有区别；只是以不同的方式绘制了循环层的机制。
- en: '![](Images/gdl2_0505.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0505.png)'
- en: Figure 5-5\. How a single sequence flows through a recurrent layer
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-5。单个序列如何流经循环层
- en: Here, we represent the recurrent process by drawing a copy of the cell at each
    timestep and show how the hidden state is constantly being updated as it flows
    through the cells. We can clearly see how the previous hidden state is blended
    with the current sequential data point (i.e., the current embedded word vector)
    to produce the next hidden state. The output from the layer is the final hidden
    state of the cell, after each word in the input sequence has been processed.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过在每个时间步绘制细胞的副本来表示循环过程，并展示隐藏状态如何在流经细胞时不断更新。我们可以清楚地看到先前的隐藏状态如何与当前的顺序数据点（即当前嵌入的单词向量）混合以产生下一个隐藏状态。该层的输出是细胞的最终隐藏状态，在输入序列中的每个单词都被处理后。
- en: Warning
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The fact that the output from the cell is called a *hidden* state is an unfortunate
    naming convention—it’s not really hidden, and you shouldn’t think of it as such.
    Indeed, the last hidden state is the overall output from the layer, and we will
    be making use of the fact that we can access the hidden state at each individual
    timestep later in this chapter.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 细胞的输出被称为*隐藏*状态是一个不幸的命名惯例——它并不真正隐藏，你不应该这样认为。事实上，最后一个隐藏状态是该层的整体输出，我们将利用这一点，稍后在本章中我们可以访问每个时间步的隐藏状态。
- en: The LSTM Cell
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LSTM细胞
- en: Now that we have seen how a generic recurrent layer works, let’s take a look
    inside an individual LSTM cell.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了一个通用循环层是如何工作的，让我们来看看单个LSTM细胞的内部。
- en: The job of the LSTM cell is to output a new hidden state, <math alttext="h Subscript
    t"><msub><mi>h</mi> <mi>t</mi></msub></math> , given its previous hidden state,
    <math alttext="h Subscript t minus 1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    , and the current word embedding, <math alttext="x Subscript t"><msub><mi>x</mi>
    <mi>t</mi></msub></math> . To recap, the length of <math alttext="h Subscript
    t"><msub><mi>h</mi> <mi>t</mi></msub></math> is equal to the number of units in
    the LSTM. This is a parameter that is set when you define the layer and has nothing
    to do with the length of the sequence.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM细胞的工作是输出一个新的隐藏状态，<math alttext="h Subscript t"><msub><mi>h</mi> <mi>t</mi></msub></math>，给定其先前的隐藏状态，<math
    alttext="h Subscript t minus 1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>，和当前的单词嵌入，<math
    alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>。回顾一下，<math alttext="h
    Subscript t"><msub><mi>h</mi> <mi>t</mi></msub></math>的长度等于LSTM中的单元数。这是在定义层时设置的一个参数，与序列的长度无关。
- en: Warning
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Make sure you do not confuse the term *cell* with *unit*. There is one cell
    in an LSTM layer that is defined by the number of units it contains, in the same
    way that the prisoner cell from our earlier story contained many prisoners. We
    often draw a recurrent layer as a chain of cells unrolled, as it helps to visualize
    how the hidden state is updated at each timestep.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 确保不要混淆术语*细胞*和*单元*。在LSTM层中有一个细胞，由它包含的单元数定义，就像我们早期故事中的囚犯细胞包含许多囚犯一样。我们经常将循环层绘制为展开的细胞链，因为这有助于可视化如何在每个时间步更新隐藏状态。
- en: An LSTM cell maintains a cell state, <math alttext="upper C Subscript t"><msub><mi>C</mi>
    <mi>t</mi></msub></math> , which can be thought of as the cell’s internal beliefs
    about the current status of the sequence. This is distinct from the hidden state,
    <math alttext="h Subscript t"><msub><mi>h</mi> <mi>t</mi></msub></math> , which
    is ultimately output by the cell after the final timestep. The cell state is the
    same length as the hidden state (the number of units in the cell).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look more closely at a single cell and how the hidden state is updated
    ([Figure 5-6](#lstm_cell)).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'The hidden state is updated in six steps:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: The hidden state of the previous timestep, <math alttext="h Subscript t minus
    1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    , and the current word embedding, <math alttext="x Subscript t"><msub><mi>x</mi>
    <mi>t</mi></msub></math> , are concatenated and passed through the *forget* gate.
    This gate is simply a dense layer with weights matrix <math alttext="upper W Subscript
    f"><msub><mi>W</mi> <mi>f</mi></msub></math> , bias <math alttext="b Subscript
    f"><msub><mi>b</mi> <mi>f</mi></msub></math> , and a sigmoid activation function.
    The resulting vector, <math alttext="f Subscript t"><msub><mi>f</mi> <mi>t</mi></msub></math>
    , has length equal to the number of units in the cell and contains values between
    0 and 1 that determine how much of the previous cell state, <math alttext="upper
    C Subscript t minus 1"><msub><mi>C</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    , should be retained.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0506.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. An LSTM cell
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The concatenated vector is also passed through an *input* gate that, like the
    forget gate, is a dense layer with weights matrix <math alttext="upper W Subscript
    i"><msub><mi>W</mi> <mi>i</mi></msub></math> , bias <math alttext="b Subscript
    i"><msub><mi>b</mi> <mi>i</mi></msub></math> , and a sigmoid activation function.
    The output from this gate, <math alttext="i Subscript t"><msub><mi>i</mi> <mi>t</mi></msub></math>
    , has length equal to the number of units in the cell and contains values between
    0 and 1 that determine how much new information will be added to the previous
    cell state, <math alttext="upper C Subscript t minus 1"><msub><mi>C</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    .
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The concatenated vector is passed through a dense layer with weights matrix
    <math alttext="upper W Subscript upper C"><msub><mi>W</mi> <mi>C</mi></msub></math>
    , bias <math alttext="b Subscript upper C"><msub><mi>b</mi> <mi>C</mi></msub></math>
    , and a tanh activation function to generate a vector <math alttext="upper C overTilde
    Subscript t"><msub><mover accent="true"><mi>C</mi> <mo>˜</mo></mover> <mi>t</mi></msub></math>
    that contains the new information that the cell wants to consider keeping. It
    also has length equal to the number of units in the cell and contains values between
    –1 and 1.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <math alttext="f Subscript t"><msub><mi>f</mi> <mi>t</mi></msub></math> and
    <math alttext="upper C Subscript t minus 1"><msub><mi>C</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    are multiplied element-wise and added to the element-wise multiplication of <math
    alttext="i Subscript t"><msub><mi>i</mi> <mi>t</mi></msub></math> and <math alttext="upper
    C overTilde Subscript t"><msub><mover accent="true"><mi>C</mi> <mo>˜</mo></mover>
    <mi>t</mi></msub></math> . This represents forgetting parts of the previous cell
    state and then adding new relevant information to produce the updated cell state,
    <math alttext="upper C Subscript t"><msub><mi>C</mi> <mi>t</mi></msub></math>
    .
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The concatenated vector is passed through an *output* gate: a dense layer with
    weights matrix <math alttext="upper W Subscript o"><msub><mi>W</mi> <mi>o</mi></msub></math>
    , bias <math alttext="b Subscript o"><msub><mi>b</mi> <mi>o</mi></msub></math>
    , and a sigmoid activation. The resulting vector, <math alttext="o Subscript t"><msub><mi>o</mi>
    <mi>t</mi></msub></math> , has length equal to the number of units in the cell
    and stores values between 0 and 1 that determine how much of the updated cell
    state, <math alttext="upper C Subscript t"><msub><mi>C</mi> <mi>t</mi></msub></math>
    , to output from the cell.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接后的向量通过一个*输出*门传递：一个带有权重矩阵<math alttext="upper W Subscript o"><msub><mi>W</mi>
    <mi>o</mi></msub></math>、偏置<math alttext="b Subscript o"><msub><mi>b</mi> <mi>o</mi></msub></math>和sigmoid激活函数的稠密层。得到的向量<math
    alttext="o Subscript t"><msub><mi>o</mi> <mi>t</mi></msub></math>的长度等于单元格中的单元数，并存储介于0和1之间的值，确定要从单元格中输出的更新后的单元格状态<math
    alttext="upper C Subscript t"><msub><mi>C</mi> <mi>t</mi></msub></math>的多少。
- en: <math alttext="o Subscript t"><msub><mi>o</mi> <mi>t</mi></msub></math> is multiplied
    element-wise with the updated cell state, <math alttext="upper C Subscript t"><msub><mi>C</mi>
    <mi>t</mi></msub></math> , after a tanh activation has been applied to produce
    the new hidden state, <math alttext="h Subscript t"><msub><mi>h</mi> <mi>t</mi></msub></math>
    .
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <math alttext="o Subscript t"><msub><mi>o</mi> <mi>t</mi></msub></math>与更新后的单元格状态<math
    alttext="upper C Subscript t"><msub><mi>C</mi> <mi>t</mi></msub></math>进行逐元素相乘，然后应用tanh激活函数产生新的隐藏状态<math
    alttext="h Subscript t"><msub><mi>h</mi> <mi>t</mi></msub></math>。
- en: The Keras LSTM Layer
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras LSTM层
- en: All of this complexity is wrapped up within the `LSTM` layer type in Keras,
    so you don’t have to worry about implementing it yourself!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些复杂性都包含在Keras的`LSTM`层类型中，因此您不必担心自己实现它！
- en: Training the LSTM
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练LSTM
- en: The code to build, compile, and train the LSTM is given in [Example 5-8](#example0603).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 构建、编译和训练LSTM的代码在[Example 5-8](#example0603)中给出。
- en: Example 5-8\. Building, compiling, and training the LSTM
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 5-8\. 构建、编译和训练LSTM
- en: '[PRE7]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](Images/1.png)](#co_autoregressive_models_CO3-1)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_autoregressive_models_CO3-1)'
- en: The `Input` layer does not need us to specify the sequence length in advance
    (it can be flexible), so we use `None` as a placeholder.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`Input`层不需要我们提前指定序列长度（可以是灵活的），所以我们使用`None`作为占位符。'
- en: '[![2](Images/2.png)](#co_autoregressive_models_CO3-2)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_autoregressive_models_CO3-2)'
- en: The `Embedding` layer requires two parameters, the size of the vocabulary (10,000
    tokens) and the dimensionality of the embedding vector (100).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`Embedding`层需要两个参数，词汇量的大小（10,000个标记）和嵌入向量的维度（100）。'
- en: '[![3](Images/3.png)](#co_autoregressive_models_CO3-3)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_autoregressive_models_CO3-3)'
- en: The LSTM layers require us to specify the dimensionality of the hidden vector
    (128). We also choose to return the full sequence of hidden states, rather than
    just the hidden state at the final timestep.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM层要求我们指定隐藏向量的维度（128）。我们还选择返回完整的隐藏状态序列，而不仅仅是最终时间步的隐藏状态。
- en: '[![4](Images/4.png)](#co_autoregressive_models_CO3-4)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_autoregressive_models_CO3-4)'
- en: The `Dense` layer transforms the hidden states at each timestep into a vector
    of probabilities for the next token.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dense`层将每个时间步的隐藏状态转换为下一个标记的概率向量。'
- en: '[![5](Images/5.png)](#co_autoregressive_models_CO3-5)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_autoregressive_models_CO3-5)'
- en: The overall `Model` predicts the next token, given an input sequence of tokens.
    It does this for each token in the sequence.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 整体的`Model`在给定一系列标记的输入序列时预测下一个标记。它为序列中的每个标记执行此操作。
- en: '[![6](Images/6.png)](#co_autoregressive_models_CO3-6)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_autoregressive_models_CO3-6)'
- en: The model is compiled with `SparseCategoricalCrossentropy` loss—this is the
    same as categorical cross-entropy, but is used when the labels are integers rather
    than one-hot encoded vectors.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用`SparseCategoricalCrossentropy`损失进行编译——这与分类交叉熵相同，但在标签为整数而不是独热编码向量时使用。
- en: '[![7](Images/7.png)](#co_autoregressive_models_CO3-7)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_autoregressive_models_CO3-7)'
- en: The model is fit to the training dataset.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 模型适合训练数据集。
- en: In [Figure 5-7](#lstm_training) you can see the first few epochs of the LSTM
    training process—notice how the example output becomes more comprehensible as
    the loss metric falls. [Figure 5-8](#lstm_loss) shows the cross-entropy loss metric
    falling throughout the training process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Figure 5-7](#lstm_training)中，您可以看到LSTM训练过程的前几个时期——请注意随着损失指标下降，示例输出变得更加易懂。[Figure
    5-8](#lstm_loss)显示了整个训练过程中交叉熵损失指标的下降。
- en: '![](Images/gdl2_0507.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0507.png)'
- en: Figure 5-7\. The first few epochs of the LSTM training process
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 5-7\. LSTM训练过程的前几个时期
- en: '![](Images/gdl2_0508.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0508.png)'
- en: Figure 5-8\. The cross-entropy loss metric of the LSTM training process by epoch
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 5-8\. LSTM训练过程中的交叉熵损失指标按时期
- en: Analysis of the LSTM
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LSTM的分析
- en: 'Now that we have compiled and trained the LSTM, we can start to use it to generate
    long strings of text by applying the following process:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经编译和训练了LSTM，我们可以开始使用它通过以下过程生成长文本字符串：
- en: Feed the network with an existing sequence of words and ask it to predict the
    following word.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用现有的单词序列喂给网络，并要求它预测下一个单词。
- en: Append this word to the existing sequence and repeat.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这个单词附加到现有序列并重复。
- en: The network will output a set of probabilities for each word that we can sample
    from. Therefore, we can make the text generation stochastic, rather than deterministic.
    Moreover, we can introduce a *temperature* parameter to the sampling process to
    indicate how deterministic we would like the process to be.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 网络将为每个单词输出一组概率，我们可以从中进行采样。因此，我们可以使文本生成具有随机性，而不是确定性。此外，我们可以引入一个*温度*参数到采样过程中，以指示我们希望过程有多确定性。
- en: The Temperature Parameter
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 温度参数
- en: A temperature close to 0 makes the sampling more deterministic (i.e., the word
    with the highest probability is very likely to be chosen), whereas a temperature
    of 1 means each word is chosen with the probability output by the model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接近0的温度使采样更加确定性（即，具有最高概率的单词很可能被选择），而温度为1意味着每个单词都以模型输出的概率被选择。
- en: This is achieved with the code in [Example 5-9](#textgenerator_callback), which
    creates a callback function that can be used to generate text at the end of each
    training epoch.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过在[示例5-9](#textgenerator_callback)中的代码实现的，该代码创建了一个回调函数，可以在每个训练周期结束时用于生成文本。
- en: Example 5-9\. The `TextGenerator` callback function
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-9。`TextGenerator`回调函数
- en: '[PRE8]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](Images/1.png)](#co_autoregressive_models_CO4-1)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_autoregressive_models_CO4-1)'
- en: Create an inverse vocabulary mapping (from word to token).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个反向词汇映射（从单词到标记）。
- en: '[![2](Images/2.png)](#co_autoregressive_models_CO4-2)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_autoregressive_models_CO4-2)'
- en: This function updates the probabilities with a `temperature` scaling factor.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数使用`temperature`缩放因子更新概率。
- en: '[![3](Images/3.png)](#co_autoregressive_models_CO4-3)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_autoregressive_models_CO4-3)'
- en: The start prompt is a string of words that you would like to give the model
    to start the generation process (for example, *recipe for*). The words are first
    converted to a list of tokens.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 起始提示是您想要给模型以开始生成过程的一串单词（例如，*recipe for*）。首先将这些单词转换为标记列表。
- en: '[![4](Images/4.png)](#co_autoregressive_models_CO4-4)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_autoregressive_models_CO4-4)'
- en: The sequence is generated until it is `max_tokens` long or a stop token (0)
    is produced.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 序列生成直到达到`max_tokens`长度或产生停止令牌（0）为止。
- en: '[![5](Images/5.png)](#co_autoregressive_models_CO4-5)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_autoregressive_models_CO4-5)'
- en: The model outputs the probabilities of each word being next in the sequence.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出每个单词成为序列中下一个单词的概率。
- en: '[![6](Images/6.png)](#co_autoregressive_models_CO4-6)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_autoregressive_models_CO4-6)'
- en: The probabilities are passed through the sampler to output the next word, parameterized
    by `temperature`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 概率通过采样器传递以输出下一个单词，由`temperature`参数化。
- en: '[![7](Images/7.png)](#co_autoregressive_models_CO4-7)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_autoregressive_models_CO4-7)'
- en: We append the new word to the prompt text, ready for the next iteration of the
    generative process.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将新单词附加到提示文本中，准备进行生成过程的下一次迭代。
- en: Let’s take a look at this in action, at two different temperature values ([Figure 5-9](#lstm_examples)).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这在实际中是如何运作的，使用两个不同的温度值（[图5-9](#lstm_examples)）。
- en: '![](Images/gdl2_0509.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0509.png)'
- en: Figure 5-9\. Generated outputs at `temperature = 1.0` and `temperature = 0.2`
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-9。在`temperature = 1.0`和`temperature = 0.2`时生成的输出
- en: There are a few things to note about these two passages. First, both are stylistically
    similar to a recipe from the original training set. They both open with a recipe
    title and contain generally grammatically correct constructions. The difference
    is that the generated text with a temperature of 1.0 is more adventurous and therefore
    less accurate than the example with a temperature of 0.2\. Generating multiple
    samples with a temperature of 1.0 will therefore lead to more variety, as the
    model is sampling from a probability distribution with greater variance.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这两段文字有几点需要注意。首先，两者在风格上与原始训练集中的食谱相似。它们都以食谱标题开头，并包含通常语法正确的结构。不同之处在于，温度为1.0的生成文本更加冒险，因此比温度为0.2的示例不够准确。因此，使用温度为1.0生成多个样本将导致更多的变化，因为模型正在从具有更大方差的概率分布中进行抽样。
- en: To demonstrate this, [Figure 5-10](#lstm_probs) shows the top five tokens with
    the highest probabilities for a range of prompts, for both temperature values.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明这一点，[图5-10](#lstm_probs)显示了一系列提示的前五个具有最高概率的标记，对于两个温度值。
- en: '![](Images/gdl2_0510.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0510.png)'
- en: Figure 5-10\. Distribution of word probabilities following various sequences,
    for temperature values of 1.0 and 0.2
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-10。在不同序列后的单词概率分布，对于温度值为1.0和0.2
- en: The model is able to generate a suitable distribution for the next most likely
    word across a range of contexts. For example, even though the model was never
    told about parts of speech such as nouns, verbs, or numbers, it is generally able
    to separate words into these classes and use them in a way that is grammatically
    correct.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型能够在一系列上下文中生成下一个最可能的单词的适当分布。例如，即使模型从未被告知过名词、动词或数字等词类，它通常能够将单词分为这些类别并以语法正确的方式使用它们。
- en: Moreover, the model is able to select an appropriate verb to begin the recipe
    instructions, depending on the preceding title. For roasted vegetables, it selects
    `preheat`, `prepare`, `heat`, `put`, or `combine` as the most likely possibilities,
    whereas for ice cream it selects `in`, `combine`, `stir`, `whisk`, and `mix`.
    This shows that the model has some contextual understanding of the differences
    between recipes depending on their ingredients.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，该模型能够选择一个适当的动词来开始食谱说明，这取决于前面的标题。对于烤蔬菜，它选择`preheat`、`prepare`、`heat`、`put`或`combine`作为最可能的可能性，而对于冰淇淋，它选择`in`、`combine`、`stir`、`whisk`和`mix`。这表明该模型对于根据其成分而异的食谱之间的差异具有一定的上下文理解。
- en: Notice also how the probabilities for the `temperature = 0.2` examples are much
    more heavily weighted toward the first choice token. This is the reason why there
    is generally less variety in generations when the temperature is lower.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意`temperature = 0.2`示例的概率更加倾向于第一个选择标记。这就是为什么当温度较低时，生成的变化通常较少的原因。
- en: While our basic LSTM model is doing a great job at generating realistic text,
    it is clear that it still struggles to grasp some of the semantic meaning of the
    words that it is generating. It introduces ingredients that are not likely to
    work well together (for example, sour Japanese potatoes, pecan crumbs, and sorbet)!
    In some cases, this may be desirable—say, if we want our LSTM to generate interesting
    and unique patterns of words—but in other cases, we will need our model to have
    a deeper understanding of the ways in which words can be grouped together and
    a longer memory of ideas introduced earlier in the text.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll explore some of the ways that we can improve our
    basic LSTM network. In [Chapter 9](ch09.xhtml#chapter_transformer), we’ll take
    a look at a new kind of autoregressive model, the Transformer, which takes language
    modeling to the next level.`  `# Recurrent Neural Network (RNN) Extensions
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: The model in the preceding section is a simple example of how an LSTM can be
    trained to learn how to generate text in a given style. In this section we will
    explore several extensions to this idea.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Stacked Recurrent Networks
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The network we just looked at contained a single LSTM layer, but we can also
    train networks with stacked LSTM layers, so that deeper features can be learned
    from the text.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, we simply introduce another LSTM layer after the first. The
    second LSTM layer can then use the hidden states from the first layer as its input
    data. This is shown in [Figure 5-11](#lstm_multilayer), and the overall model
    architecture is shown in [Table 5-2](#lstm_multilayer_arch).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0511.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-11\. Diagram of a multilayer RNN: g[t] denotes hidden states of the
    first layer and h[t] denotes hidden states of the second layer'
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Table 5-2\. Model summary of the stacked LSTM
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer (type) | Output shape | Param # |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| InputLayer | (None, None) | 0 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| Embedding | (None, None, 100) | 1,000,000 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| LSTM | (None, None, 128) | 117,248 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| LSTM | (None, None, 128) | 131,584 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| Dense | (None, None, 10000) | 1,290,000 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| Total params | 2,538,832 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| Trainable params | 2,538,832 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| Non-trainable params | 0 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: The code to build the stacked LSTM is given in [Example 5-10](#example0605).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-10\. Building a stacked LSTM
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Gated Recurrent Units
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another type of commonly used RNN layer is the *gated recurrent unit* (GRU).^([2](ch05.xhtml#idm45387016270848))
    The key differences from the LSTM unit are as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: The *forget* and *input* gates are replaced by *reset* and *update* gates.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is no *cell state* or *output* gate, only a *hidden state* that is output
    from the cell.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The hidden state is updated in four steps, as illustrated in [Figure 5-12](#gru_cell).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0512.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: Figure 5-12\. A single GRU cell
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The process is as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: The hidden state of the previous timestep, <math alttext="h Subscript t minus
    1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    , and the current word embedding, <math alttext="x Subscript t"><msub><mi>x</mi>
    <mi>t</mi></msub></math> , are concatenated and used to create the *reset* gate.
    This gate is a dense layer, with weights matrix <math alttext="upper W Subscript
    r"><msub><mi>W</mi> <mi>r</mi></msub></math> and a sigmoid activation function.
    The resulting vector, <math alttext="r Subscript t"><msub><mi>r</mi> <mi>t</mi></msub></math>
    , has length equal to the number of units in the cell and stores values between
    0 and 1 that determine how much of the previous hidden state, <math alttext="h
    Subscript t minus 1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    , should be carried forward into the calculation for the new beliefs of the cell.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The reset gate is applied to the hidden state, <math alttext="h Subscript t
    minus 1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    , and concatenated with the current word embedding, <math alttext="x Subscript
    t"><msub><mi>x</mi> <mi>t</mi></msub></math> . This vector is then fed to a dense
    layer with weights matrix <math alttext="upper W"><mi>W</mi></math> and a tanh
    activation function to generate a vector, <math alttext="h overTilde Subscript
    t"><msub><mover accent="true"><mi>h</mi> <mo>˜</mo></mover> <mi>t</mi></msub></math>
    , that stores the new beliefs of the cell. It has length equal to the number of
    units in the cell and stores values between –1 and 1.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置门应用于隐藏状态，<math alttext="h Subscript t minus 1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>，并与当前单词嵌入<math
    alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>连接。然后将该向量馈送到具有权重矩阵<math
    alttext="upper W"><mi>W</mi></math>和tanh激活函数的密集层，以生成一个向量<math alttext="h overTilde
    Subscript t"><msub><mover accent="true"><mi>h</mi> <mo>˜</mo></mover> <mi>t</mi></msub></math>，其中存储了细胞的新信念。它的长度等于细胞中的单元数，并存储在-1和1之间的值。
- en: The concatenation of the hidden state of the previous timestep, <math alttext="h
    Subscript t minus 1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    , and the current word embedding, <math alttext="x Subscript t"><msub><mi>x</mi>
    <mi>t</mi></msub></math> , are also used to create the *update* gate. This gate
    is a dense layer with weights matrix <math alttext="upper W Subscript z"><msub><mi>W</mi>
    <mi>z</mi></msub></math> and a sigmoid activation. The resulting vector, <math
    alttext="z Subscript t"><msub><mi>z</mi> <mi>t</mi></msub></math> , has length
    equal to the number of units in the cell and stores values between 0 and 1, which
    are used to determine how much of the new beliefs, <math alttext="h overTilde
    Subscript t"><msub><mover accent="true"><mi>h</mi> <mo>˜</mo></mover> <mi>t</mi></msub></math>
    , to blend into the current hidden state, <math alttext="h Subscript t minus 1"><msub><mi>h</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math> .
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前一个时间步的隐藏状态<math alttext="h Subscript t minus 1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>和当前单词嵌入<math
    alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>的连接也用于创建*更新*门。该门是一个具有权重矩阵<math
    alttext="upper W Subscript z"><msub><mi>W</mi> <mi>z</mi></msub></math>和sigmoid激活的密集层。生成的向量<math
    alttext="z Subscript t"><msub><mi>z</mi> <mi>t</msub></math>的长度等于细胞中的单元数，并存储在0和1之间的值，用于确定新信念<math
    alttext="h overTilde Subscript t"><msub><mover accent="true"><mi>h</mi> <mo>˜</mo></mover>
    <mi>t</mi></msub></math>的多少要混合到当前隐藏状态<math alttext="h Subscript t minus 1"><msub><mi>h</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>中。
- en: The new beliefs of the cell, <math alttext="h overTilde Subscript t"><msub><mover
    accent="true"><mi>h</mi> <mo>˜</mo></mover> <mi>t</mi></msub></math> , and the
    current hidden state, <math alttext="h Subscript t minus 1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    , are blended in a proportion determined by the update gate, <math alttext="z
    Subscript t"><msub><mi>z</mi> <mi>t</mi></msub></math> , to produce the updated
    hidden state, <math alttext="h Subscript t"><msub><mi>h</mi> <mi>t</mi></msub></math>
    , that is output from the cell.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 细胞的新信念<math alttext="h overTilde Subscript t"><msub><mover accent="true"><mi>h</mi>
    <mo>˜</mo></mover> <mi>t</mi></msub></math>和当前隐藏状态<math alttext="h Subscript t
    minus 1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>按照更新门<math
    alttext="z Subscript t"><msub><mi>z</mi> <mi>t</msub></math>确定的比例混合，以产生更新后的隐藏状态<math
    alttext="h Subscript t"><msub><mi>h</mi> <mi>t</msub></math>，从细胞中输出。
- en: Bidirectional Cells
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双向细胞
- en: 'For prediction problems where the entire text is available to the model at
    inference time, there is no reason to process the sequence only in the forward
    direction—it could just as well be processed backward. A `Bidirectional` layer
    takes advantage of this by storing two sets of hidden states: one that is produced
    as a result of the sequence being processed in the usual forward direction and
    another that is produced when the sequence is processed backward. This way, the
    layer can learn from information both preceding and succeeding the given timestep.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预测问题，在推断时模型可以获得整个文本，没有理由只在正向方向处理序列 - 它同样可以被反向处理。`Bidirectional`层通过存储两组隐藏状态来利用这一点：一组是由序列在通常的正向方向处理时产生的，另一组是在序列被反向处理时产生的。这样，该层可以从给定时间步之前和之后的信息中学习。
- en: In Keras, this is implemented as a wrapper around a recurrent layer, as shown
    in [Example 5-11](#example0606).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，这被实现为对循环层的包装，如[示例5-11](#example0606)所示。
- en: Example 5-11\. Building a bidirectional GRU layer
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-11。构建双向GRU层
- en: '[PRE10]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Hidden State
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐藏状态
- en: The hidden states in the resulting layer are vectors of length equal to double
    the number of units in the wrapped cell (a concatenation of the forward and backward
    hidden states). Thus, in this example the hidden states of the layer are vectors
    of length 200.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 结果层中的隐藏状态是长度等于包装细胞中单元数两倍的向量（正向和反向隐藏状态的连接）。因此，在此示例中，该层的隐藏状态是长度为200的向量。
- en: So far, we have only applied autoregressive models (LSTMs) to text data. In
    the next section, we will see how autoregressive models can also be used to generate
    images.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只将自回归模型（LSTMs）应用于文本数据。在下一节中，我们将看到如何使用自回归模型来生成图像。
- en: PixelCNN
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PixelCNN
- en: In 2016, van den Oord et al.^([3](ch05.xhtml#idm45387016135616)) introduced
    a model that generates images pixel by pixel by predicting the likelihood of the
    next pixel based on the pixels before it. The model is called *PixelCNN*, and
    it can be trained to generate images autoregressively.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，van den Oord等人^([3](ch05.xhtml#idm45387016135616))提出了一种通过预测下一个像素的可能性来逐像素生成图像的模型。该模型称为*PixelCNN*，可以训练以自回归方式生成图像。
- en: There are two new concepts that we need to introduce to understand the PixelCNN—*masked
    convolutional layers* and *residual blocks*.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要介绍两个新概念来理解PixelCNN - *掩码卷积层*和*残差块*。
- en: Running the Code for This Example
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行此示例的代码
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/05_autoregressive/02_pixelcnn/pixelcnn.ipynb*
    in the book repository.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的代码可以在位于书籍存储库中的Jupyter笔记本中找到，路径为*notebooks/05_autoregressive/02_pixelcnn/pixelcnn.ipynb*。
- en: The code has been adapted from the excellent [PixelCNN tutorial](https://keras.io/examples/generative/pixelcnn)
    created by ADMoreau, available on the Keras website.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '该代码改编自由ADMoreau创建的出色的[PixelCNN教程](https://keras.io/examples/generative/pixelcnn)，可在Keras网站上找到。 '
- en: Masked Convolutional Layers
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 掩码卷积层
- en: As we saw in [Chapter 2](ch02.xhtml#chapter_deep_learning), a convolutional
    layer can be used to extract features from an image by applying a series of filters.
    The output of the layer at a particular pixel is a weighted sum of the filter
    weights multiplied by the preceding layer values over a small square centered
    on the pixel. This method can detect edges and textures and, at deeper layers,
    shapes and higher-level features.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第2章](ch02.xhtml#chapter_deep_learning)中看到的，卷积层可以通过应用一系列滤波器从图像中提取特征。在特定像素处的层的输出是滤波器权重乘以围绕像素中心的小正方形上一层值的加权和。这种方法可以检测边缘和纹理，而在更深的层中，可以检测形状和更高级的特征。
- en: Whilst convolutional layers are extremely useful for feature detection, they
    cannot directly be used in an autoregressive sense, because there is no ordering
    placed on the pixels. They rely on the fact that all pixels are treated equally—no
    pixel is treated as the *start* or *end* of the image. This is in contrast to
    the text data that we have already seen in this chapter, where there is a clear
    ordering to the tokens so recurrent models such as LSTMs can be readily applied.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然卷积层在特征检测方面非常有用，但不能直接以自回归的方式使用，因为像素上没有顺序。它们依赖于所有像素都被平等对待的事实——没有像素被视为图像的*开始*或*结束*。这与我们在本章中已经看到的文本数据形成对比，其中令牌有明确的顺序，因此可以轻松应用循环模型，如LSTM。
- en: For us to be able to apply convolutional layers to image generation in an autoregressive
    sense, we must first place an ordering on the pixels and ensure that the filters
    are only able to see pixels that precede the pixel in question. We can then generate
    images one pixel at a time, by applying convolutional filters to the current image
    to predict the value of the next pixel from all preceding pixels.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够以自回归的方式将卷积层应用于图像生成，我们必须首先对像素进行排序，并确保滤波器只能看到在问题像素之前的像素。然后，我们可以通过将卷积滤波器应用于当前图像来一次生成一个像素，以预测下一个像素的值。
- en: We first need to choose an ordering for the pixels—a sensible suggestion is
    to order the pixels from top left to bottom right, moving first along the rows
    and then down the columns.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要为像素选择一个顺序——一个明智的建议是按照从左上到右下的顺序对像素进行排序，首先沿着行移动，然后沿着列向下移动。
- en: We then mask the convolutional filters so that the output of the layer at each
    pixel is only influenced by pixel values that precede the pixel in question. This
    is achieved by multiplying a mask of ones and zeros with the filter weights matrix,
    so that the values of any pixels that are after the target pixel are zeroed.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对卷积滤波器进行掩码处理，以便每个像素处的层的输出仅受到在问题像素之前的像素值的影响。这是通过将一个由1和0组成的掩码与滤波器权重矩阵相乘来实现的，以便在目标像素之后的任何像素的值都被置为零。
- en: 'There are actually two different kinds of masks in a PixelCNN, as shown in
    [Figure 5-13](#conv_filter_mask):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在PixelCNN中实际上有两种不同类型的掩码，如[图5-13](#conv_filter_mask)所示：
- en: Type A, where the value of the central pixel is masked
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类型A，中心像素的值被掩码
- en: Type B, where the value of the central pixel is *not* masked
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类型B，中心像素的值*未*被掩码
- en: '![](Images/gdl2_0513.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0513.png)'
- en: 'Figure 5-13\. Left: a convolutional filter mask; right: a mask applied to a
    set of pixels to predict the distribution of the central pixel value (source:
    [van den Oord et al., 2016](https://arxiv.org/pdf/1606.05328))'
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-13。左：卷积滤波器掩码；右：应用于一组像素以预测中心像素值分布的掩码（来源：[van den Oord等人，2016](https://arxiv.org/pdf/1606.05328))
- en: The initial masked convolutional layer (i.e., the one that is applied directly
    to the input image) cannot use the central pixel, because this is precisely the
    pixel we want the network to guess! However, subsequent layers can use the central
    pixel because this will have been calculated only as a result of information from
    preceding pixels in the original input image.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 初始的掩码卷积层（即直接应用于输入图像的层）不能使用中心像素，因为这正是我们希望网络猜测的像素！然而，后续层可以使用中心像素，因为这将仅根据原始输入图像中前面像素的信息计算出来。
- en: We can see in [Example 5-12](#masked_conv_layer) how a `MaskedConvLayer` can
    be built using Keras.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[示例5-12](#masked_conv_layer)中看到如何使用Keras构建`MaskedConvLayer`。
- en: Example 5-12\. A `MaskedConvLayer` in Keras
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-12。Keras中的`MaskedConvLayer`
- en: '[PRE11]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](Images/1.png)](#co_autoregressive_models_CO5-1)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_autoregressive_models_CO5-1)'
- en: The `MaskedConvLayer` is based on the normal `Conv2D` layer.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`MaskedConvLayer`基于普通的`Conv2D`层。'
- en: '[![2](Images/2.png)](#co_autoregressive_models_CO5-2)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_autoregressive_models_CO5-2)'
- en: The mask is initialized with all zeros.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码初始化为全零。
- en: '[![3](Images/3.png)](#co_autoregressive_models_CO5-3)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_autoregressive_models_CO5-3)'
- en: The pixels in the preceding rows are unmasked with ones.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 前面行中的像素将被一个1解除掩码。
- en: '[![4](Images/4.png)](#co_autoregressive_models_CO5-4)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_autoregressive_models_CO5-4)'
- en: The pixels in the preceding columns that are in the same row are unmasked with
    ones.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 前面列中在同一行中的像素将被一个1解除掩码。
- en: '[![5](Images/5.png)](#co_autoregressive_models_CO5-5)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_autoregressive_models_CO5-5)'
- en: If the mask type is B, the central pixel is unmasked with a one.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果掩码类型为B，则中心像素将被一个1解除掩码。
- en: '[![6](Images/6.png)](#co_autoregressive_models_CO5-6)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_autoregressive_models_CO5-6)'
- en: The mask is multiplied with the filter weights.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码与滤波器权重相乘。
- en: Note that this simplified example assumes a grayscale image (i.e., with one
    channel). If we have color images, we’ll have three color channels that we can
    also place an ordering on so that, for example, the red channel precedes the blue
    channel, which precedes the green channel.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Residual Blocks
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have seen how to mask the convolutional layer, we can start to build
    our PixelCNN. The core building block that we will use is the residual block.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: A *residual block* is a set of layers where the output is added to the input
    before being passed on to the rest of the network. In other words, the input has
    a *fast-track* route to the output, without having to go through the intermediate
    layers—this is called a *skip connection*. The rationale behind including a skip
    connection is that if the optimal transformation is just to keep the input the
    same, this can be achieved by simply zeroing the weights of the intermediate layers.
    Without the skip connection, the network would have to find an identity mapping
    through the intermediate layers, which is much harder.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: A diagram of the residual block in our PixelCNN is shown in [Figure 5-14](#residual_block_pixelcnn).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0514.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: Figure 5-14\. A PixelCNN residual block (the numbers of filters are next to
    the arrows and the filter sizes are next to the layers)
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can build a `ResidualBlock` using the code shown in [Example 5-13](#residual_block_code_pixelcnn).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-13\. A `ResidualBlock`
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](Images/1.png)](#co_autoregressive_models_CO6-1)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: The initial `Conv2D` layer halves the number of channels.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_autoregressive_models_CO6-2)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: The Type B `MaskedConv2D` layer with kernel size of 3 only uses information
    from five pixels—three pixels in the row above the focus pixel, one to the left,
    and the focus pixel itself.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_autoregressive_models_CO6-3)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: The final `Conv2D` layer doubles the number of channels to again match the input
    shape.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_autoregressive_models_CO6-4)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: The output from the convolutional layers is added to the input—this is the skip
    connection.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Training the PixelCNN
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Example 5-14](#pixelcnn_architecture) we put together the whole PixelCNN
    network, approximately following the structure laid out in the original paper.
    In the original paper, the output layer is a 256-filter `Conv2D` layer, with softmax
    activation. In other words, the network tries to re-create its input by predicting
    the correct pixel values, a bit like an autoencoder. The difference is that the
    PixelCNN is constrained so that no information from earlier pixels can flow through
    to influence the prediction for each pixel, due to the way that network is designed,
    using `MaskedConv2D` layers.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: A challenge with this approach is that the network has no way to understand
    that a pixel value of, say, 200 is very close to a pixel value of 201\. It must
    learn every pixel output value independently, which means training can be very
    slow, even for the simplest datasets. Therefore, in our implementation, we instead
    simplify the input so that each pixel can take only one of four values. This way,
    we can use a 4-filter `Conv2D` output layer instead of 256.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-14\. The PixelCNN architecture
  id: totrans-283
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](Images/1.png)](#co_autoregressive_models_CO7-1)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: The model `Input` is a grayscale image of size 16 × 16 × 1, with inputs scaled
    between 0 and 1.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_autoregressive_models_CO7-2)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: The first Type A `MaskedConv2D` layer with a kernel size of 7 uses information
    from 24 pixels—21 pixels in the three rows above the focus pixel and 3 to the
    left (the focus pixel itself is not used).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_autoregressive_models_CO7-3)'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Five `ResidualBlock` layer groups are stacked sequentially.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_autoregressive_models_CO7-4)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Two Type B `MaskedConv2D` layers with a kernel size of 1 act as `Dense` layers
    across the number of channels for each pixel.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_autoregressive_models_CO7-5)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: The final `Conv2D` layer reduces the number of channels to four—the number of
    pixel levels for this example.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_autoregressive_models_CO7-6)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: The `Model` is built to accept an image and output an image of the same dimensions.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_autoregressive_models_CO7-7)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Fit the model—`input_data` is scaled in the range [0, 1] (floats); `output_data`
    is scaled in the range [0, 3] (integers).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of the PixelCNN
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can train our PixelCNN on images from the Fashion-MNIST dataset that we encountered
    in [Chapter 3](ch03.xhtml#chapter_vae). To generate new images, we need to ask
    the model to predict the next pixel given all preceding pixels, one pixel at a
    time. This is a very slow process compared to a model such as a variational autoencoder!
    For a 32 × 32 grayscale image, we need to make 1,024 predictions sequentially
    using the model, compared to the single prediction that we need to make for a
    VAE. This is one of the major downsides to autoregressive models such as a PixelCNN—they
    are slow to sample from, because of the sequential nature of the sampling process.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, we use an image size of 16 × 16, rather than 32 × 32, to speed
    up the generation of new images. The generation callback class is shown in [Example 5-15](#pixelcnn_generation).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-15\. Generating new images using the PixelCNN
  id: totrans-302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](Images/1.png)](#co_autoregressive_models_CO8-1)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Start with a batch of empty images (all zeros).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_autoregressive_models_CO8-2)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Loop over the rows, columns, and channels of the current image, predicting the
    distribution of the next pixel value.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_autoregressive_models_CO8-3)'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Sample a pixel level from the predicted distribution (for our example, a level
    in the range [0, 3]).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_autoregressive_models_CO8-4)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Convert the pixel level to the range [0, 1] and overwrite the pixel value in
    the current image, ready for the next iteration of the loop.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 5-15](#pixelcnn_output), we can see several images from the original
    training set, alongside images that have been generated by the PixelCNN.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0515.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
- en: Figure 5-15\. Example images from the training set and generated images created
    by the PixelCNN model
  id: totrans-314
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The model does a great job of re-creating the overall shape and style of the
    original images! It is quite amazing that we can treat images as a series of tokens
    (pixel values) and apply autoregressive models such as a PixelCNN to produce realistic
    samples.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, one of the downsides to autoregressive models is that
    they are slow to sample from, which is why a simple example of their application
    is presented in this book. However, as we shall see in [Chapter 10](ch10.xhtml#chapter_image_generation),
    more complex forms of autoregressive model can be applied to images to produce
    state-of-the-art outputs. In such cases, the slow generation speed is a necessary
    price to pay in return for exceptional-quality outputs.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Since the original paper was published, several improvements have been made
    to the architecture and training process of the PixelCNN. The following section
    introduces one of those changes—using mixture distributions—and demonstrates how
    to train a PixelCNN model with this improvement using a built-in TensorFlow function.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Mixture Distributions
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our previous example, we reduced the output of the PixelCNN to just 4 pixel
    levels to ensure the network didn’t have to learn a distribution over 256 independent
    pixel values, which would slow the training process. However, this is far from
    ideal—for color images, we wouldn’t want our canvas to be restricted to only a
    handful of possible colors.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: To get around this problem, we can make the output of the network a *mixture
    distribution*, instead of a softmax over 256 discrete pixel values, following
    the ideas presented by Salimans et al.^([4](ch05.xhtml#idm45387015116960)) A mixture
    distribution is quite simply a mixture of two or more other probability distributions.
    For example, we could have a mixture distribution of five logistic distributions,
    each with different parameters. The mixture distribution also requires a discrete
    categorical distribution that denotes the probability of choosing each of the
    distributions included in the mix. An example is shown in [Figure 5-16](#mixture_distribution).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0516.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
- en: Figure 5-16\. A mixture distribution of three normal distributions with different
    parameters—the categorical distribution over the three normal distributions is
    `[0.5, 0.3, 0.2]`
  id: totrans-322
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To sample from a mixture distribution, we first sample from the categorical
    distribution to choose a particular subdistribution and then sample from this
    in the usual way. This way, we can create complex distributions with relatively
    few parameters. For example, the mixture distribution in [Figure 5-16](#mixture_distribution)
    only requires eight parameters—two for the categorical distribution and a mean
    and variance for each of the three normal distributions. This is compared to the
    255 parameters that would define a categorical distribution over the entire pixel
    range.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Conveniently, the TensorFlow Probability library provides a function that allows
    us to create a PixelCNN with mixture distribution output in a single line. [Example 5-16](#pixelcnn_generation2)
    illustrates how to build a PixelCNN using this function.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Running the Code for This Example
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this example can be found in the Jupyter notebook in *notebooks/05_autoregressive/03_pixelcnn_md/pixelcnn_md.ipynb*
    in the book repository.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-16\. Building a PixelCNN using the TensorFlow function
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](Images/1.png)](#co_autoregressive_models_CO9-1)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Define the PixelCNN as a distribution—i.e., the output layer is a mixture distribution
    made up of five logistic distributions.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_autoregressive_models_CO9-2)'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: The input is a grayscale image of size 32 × 32 × 1.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_autoregressive_models_CO9-3)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: The `Model` takes a grayscale image as input and outputs the log-likelihood
    of the image under the mixture distribution calculated by the PixelCNN.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_autoregressive_models_CO9-4)'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: The loss function is the mean negative log-likelihood over the batch of input
    images.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: The model is trained in the same way as before, but this time accepting integer
    pixel values as input, in the range [0, 255]. Outputs can be generated from the
    distribution using the `sample` function, as shown in [Example 5-17](#pixelcnn_sampling).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-17\. Sampling from the PixelCNN mixture distribution
  id: totrans-338
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Example generated images are shown in [Figure 5-17](#pixel_cnn_mixture_outputs).
    The difference from our previous examples is that now the full range of pixel
    values is being utilized.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0517.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
- en: Figure 5-17\. Outputs from the PixelCNN using a mixture distribution output
  id: totrans-342
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we have seen how autoregressive models such as recurrent neural
    networks can be applied to generate text sequences that mimic a particular style
    of writing, and also how a PixelCNN can generate images in a sequential fashion,
    one pixel at a time.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: We explored two different types of recurrent layers—long short-term memory (LSTM)
    and gated recurrent unit (GRU)—and saw how these cells can be stacked or made
    bidirectional to form more complex network architectures. We built an LSTM to
    generate realistic recipes using Keras and saw how to manipulate the temperature
    of the sampling process to increase or decrease the randomness of the output.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: We also saw how images can be generated in an autoregressive manner, using a
    PixelCNN. We built a PixelCNN from scratch using Keras, coding the masked convolutional
    layers and residual blocks to allow information to flow through the network so
    that only preceding pixels could be used to generate the current pixel. Finally,
    we discussed how the TensorFlow Probability library provides a standalone `PixelCNN`
    function that implements a mixture distribution as the output layer, allowing
    us to further improve the learning process.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到了如何以自回归方式生成图像，使用了 PixelCNN。我们使用 Keras 从头开始构建了一个 PixelCNN，编写了掩膜卷积层和残差块，以允许信息在网络中流动，从而只能使用前面的像素来生成当前的像素。最后，我们讨论了
    TensorFlow Probability 库提供了一个独立的 `PixelCNN` 函数，实现了混合分布作为输出层，使我们能够进一步改进学习过程。
- en: In the next chapter we will explore another generative modeling family that
    explicitly models the data-generating distribution—normalizing flow models.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨另一种生成建模家族，明确地对数据生成分布进行建模—正规化流模型。
- en: '^([1](ch05.xhtml#idm45387018222496-marker)) Sepp Hochreiter and Jürgen Schmidhuber,
    “Long Short-Term Memory,” *Neural Computation* 9 (1997): 1735–1780, [*https://www.bioinf.jku.at/publications/older/2604.pdf*](https://www.bioinf.jku.at/publications/older/2604.pdf).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch05.xhtml#idm45387018222496-marker)) Sepp Hochreiter 和 Jürgen Schmidhuber,
    “长短期记忆,” *神经计算* 9 (1997): 1735–1780, [*https://www.bioinf.jku.at/publications/older/2604.pdf*](https://www.bioinf.jku.at/publications/older/2604.pdf).'
- en: ^([2](ch05.xhtml#idm45387016270848-marker)) Kyunghyun Cho et al., “Learning
    Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation,”
    June 3, 2014, [*https://arxiv.org/abs/1406.1078*](https://arxiv.org/abs/1406.1078).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.xhtml#idm45387016270848-marker)) Kyunghyun Cho 等人, “使用 RNN 编码器-解码器学习短语表示进行统计机器翻译,”
    2014年6月3日, [*https://arxiv.org/abs/1406.1078*](https://arxiv.org/abs/1406.1078).
- en: ^([3](ch05.xhtml#idm45387016135616-marker)) Aaron van den Oord et al., “Pixel
    Recurrent Neural Networks,” August 19, 2016, [*https://arxiv.org/abs/1601.06759*](https://arxiv.org/abs/1601.06759).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch05.xhtml#idm45387016135616-marker)) Aaron van den Oord 等人, “像素递归神经网络,”
    2016年8月19日, [*https://arxiv.org/abs/1601.06759*](https://arxiv.org/abs/1601.06759).
- en: '^([4](ch05.xhtml#idm45387015116960-marker)) Tim Salimans et al., “PixelCNN++:
    Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other
    Modifications,” January 19, 2017, [*http://arxiv.org/abs/1701.05517*](http://arxiv.org/abs/1701.05517).`'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch05.xhtml#idm45387015116960-marker)) Tim Salimans 等人, “PixelCNN++: 使用离散化逻辑混合似然和其他修改改进
    PixelCNN,” 2017年1月19日, [*http://arxiv.org/abs/1701.05517*](http://arxiv.org/abs/1701.05517).'
