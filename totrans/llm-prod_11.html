<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">12</span></span> <span class="chapter-title-text">Production, an ever-changing landscape: Things are just getting started</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">A brief overview of LLMs in production</li>
<li class="readable-text" id="p3">The future of LLMs as a technology and several exciting fields of research into it</li>
<li class="readable-text" id="p4">Our closing remarks</li>
</ul>
</div>
<div class="readable-text" id="p5">
<blockquote>
<div>
     The Web as I envisaged it, we have not seen it yet. The future is still so much bigger than the past. 
     <div class="quote-cite">
       —Tim Berners-Lee (inventor of www) 
     </div>
</div>
</blockquote>
</div>
<div class="readable-text" id="p6">
<p>Wow! We’ve really covered a lot of ground in this book. Is your head just about ready to explode? Because ours are, and we wrote the book. Writing this book has been no easy feat, as the industry has been constantly changing—and fast. Trying to stay on top of what’s happening with LLMs has been like trying to build a house on quicksand; you finish one level, and it seems to have already sunk before you can start the next. We know that portions of this book will inevitably become out of date, and that’s why we tried our best to stick to core concepts, the sturdy rocks in the sand, that will never change.</p>
</div>
<div class="readable-text intended-text" id="p7">
<p>In this chapter, we wanted to take a step back and review some of the major takeaways we hope you will walk away with. We’ve spent a lot of time getting into the weeds and paying attention to details, so let’s reflect for a moment to see the whole picture and review what we’ve covered. After that, we’ll take a minute to discuss the future of the field and where we can expect to see some of the next major breakthroughs. Finally, we’ll leave you with our final thoughts. </p>
</div>
<div class="readable-text" id="p8">
<h2 class="readable-text-h2" id="sigil_toc_id_182"><span class="num-string">12.1</span> A thousand-foot view</h2>
</div>
<div class="readable-text" id="p9">
<p>We have gone over a lot of material in this book—from making a bag-of-words model to serving an LLM API on a Raspberry Pi. If you made it all the way through the whole book, that’s an accomplishment. Great work! We are not going to recap everything, but we wanted to take a second to see the forest from the trees, as it were. To summarize much of what we’ve covered, we can split most of the ideas into four distinct but very closely tied quadrants: Preparation, Training, Serving, and Developing. You can see these quadrants in figure 12.1. You’ll notice that along with these sections, there’s a fifth one distinct from the others, which we labeled Undercurrents. These are elements that seem to affect all of the other quadrants to varying degrees and things you’ll have to worry about during each stage of an LLM product life cycle. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p10">
<img alt="figure" height="567" src="../Images/12-1.png" width="1012"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.1</span> LLM product life cycle. Here are all the key concepts discussed in the book, along with where they generally fit within the production environment. Undercurrents are important elements that show up in every part of the life cycle—for example, linguistics informs preparation, creates metrics in training and serving, and influences prompting and development.</h5>
</div>
<div class="readable-text" id="p11">
<p>Hopefully, if it wasn’t clear when we were talking about a concept in an earlier chapter, it’s clear now exactly where that concept fits in a production life cycle. You’ll notice that we’ve likely put some elements in places that your current production environment doesn’t reflect—for example, provisioning of the MLOps infrastructure doesn’t often actually happen within the preparation stage but is rather haphazardly thrown together the first time that serving needs to happen. We get it. But during preparation is where we feel it <em>should</em> happen. Take a moment to digest all that you’ve learned while reading this book, and consider how the pieces all come together.</p>
</div>
<div class="readable-text intended-text" id="p12">
<p>Given this abstract and idealized version of a production life cycle, let’s move to the things not currently included there. What might we need to add to our development portion five years down the line, especially given how fast the field moves now?</p>
</div>
<div class="readable-text" id="p13">
<h2 class="readable-text-h2" id="sigil_toc_id_183"><span class="num-string">12.2</span> The future of LLMs</h2>
</div>
<div class="readable-text" id="p14">
<p>When we wrote this book, we made a conscious effort to focus on the foundational knowledge you will need to understand how LLMs work and how to deploy them to production. This information is crucial, as production looks very different for every single use case. Learning how to weigh the pros and cons of any decision requires that foundational knowledge if you have any hope of landing on the right one.</p>
</div>
<div class="readable-text intended-text" id="p15">
<p>Adjacent to this decision, we didn’t want this book to be all theory. We wanted it to be hands-on, with enough examples that you as a reader wouldn’t just know how things worked but would get a sense of how they feel—like getting a feel for how long it takes to load a 70B model onto a GPU, sensing what the experience will be like for your user if you run the model on an edge device, and feeling the soft glow of your computer monitor as you hide in a dark cave pouring over code and avoiding the warm sun on a nice spring day.</p>
</div>
<div class="readable-text intended-text" id="p16">
<p>One of the hardest decisions we made when we wrote this book was deciding to focus on the here and now. We decided to focus on the best methods that we actually see people using in production today. This decision was hard because over the course of writing this book, there have been many mind-blowing research papers we’ve been convinced will “change everything.” However, for one reason or another, that research has yet to make it to production. In this section, we are going to change that restriction and talk about what’s up and coming regardless of the current state of the industry. But it’s not just research; public opinions, lawsuits, and political landscapes often shape the future of technology as well. We’ll be looking at where we see LLMs going in the next several years and mention some of the directions they could take.</p>
</div>
<div class="readable-text" id="p17">
<h3 class="readable-text-h3" id="sigil_toc_id_184"><span class="num-string">12.2.1</span> Government and regulation</h3>
</div>
<div class="readable-text" id="p18">
<p>At the beginning of this book, we promised to show you how to create LLM products, not just demos. While we believe we have done just that, there’s one important detail we’ve been ignoring: the fact that products live in the real world. While demos just have to work in isolation, products have to work in general. Products are meant to be sold, and once there’s an exchange of currency, expectations are set, reputations are on the line, and ultimately, governments are going to get involved.</p>
</div>
<div class="readable-text intended-text" id="p19">
<p>While a team can’t build for future regulations that may never come, it’s important to be aware of the possible legal ramifications of the products you build. One lost lawsuit can set a precedent that brings a tidal wave of copycat lawsuits. Since products live in the real world, it is best that we pay attention to that world.</p>
</div>
<div class="readable-text intended-text" id="p20">
<p>One of us had the opportunity to participate in Utah’s legislative process for Utah’s SB-149 Artificial Intelligence Amendments bill. This bill is primarily concerned with introducing liability to actors using LLMs to skirt consumer protection laws in the state. At the moment, every legislative body is attempting to figure out where its jurisdiction starts and ends concerning AI and how to deal with the increased responsibility it has to protect citizens and corporations within its constituency. In Utah, the state government takes a very serious and business-first approach to AI and LLMs. Throughout the process and the bill itself, the legislature cannot create definitions that aren’t broken with “behold, a man” Diogenes-style examples, and we will need every bit of good faith to navigate the new world that LLMs bring to regulatory bodies. How do you define AI? The bill defines it as follows:</p>
</div>
<div class="readable-text" id="p21">
<blockquote>
<div>
     “Artificial intelligence” means a machine-based system that makes predictions, recommendations, or decisions influencing real or virtual environments.
    </div>
</blockquote>
</div>
<div class="readable-text" id="p22">
<p>This could be anything from a piecewise function to an LLM agent, meaning that your marketing team will not be liable for claims that your <code>if</code> statements are AI within the state. That said, the bill contains a thorough and well-thought-out definition of a deceptive act by a supplier, along with the formulation of an AI analysis and research program to help the state assess risks and policy in a more long-term capacity, which seems novel and unique to Utah. The Utah state legislature was able to refine this bill by consulting with researchers, experts, c-level executives, and business owners within the state, and we’d encourage the reader to participate in creating worthwhile and meaningful regulations within your communities and governments. This is the only way to make sure that court systems are prepared to impose consequences where they are due in the long term.</p>
</div>
<div class="readable-text" id="p23">
<h4 class="readable-text-h4 sigil_not_in_toc">Copyright</h4>
</div>
<div class="readable-text" id="p24">
<p>At the forefront of legal concerns is that of copyright infringement. LLMs trained on enough data can impersonate or copy the style of an author or creator or even straight-up word-for-word plagiarize. While this is exciting when considering building your own ghostwriter to help you in your creative process, it’s much less so when you realize a competitor could do the same.</p>
</div>
<div class="readable-text intended-text" id="p25">
<p>Probably the biggest lawsuit to pay attention to is that of <em>The New York Times</em> v. OpenAI.<a href="#footnote-97"><sup class="footnote-reference" id="footnote-source-1">1</sup></a> <em>The New York Times</em> is in the process of legal action against OpenAI, stating their chatbots were trained on the <em>Times</em>’ intellectual property without consent. It gives evidence that the chatbots are giving word-for-word responses identical to proprietary information found in articles a user would normally have to pay to see. As a result, there is the concern that fewer users will visit their site, reducing ad revenue. Essentially, they stole their data and are now using it as a competitor in the information space.</p>
</div>
<div class="readable-text intended-text" id="p26">
<p>Bystanders to the fight worry that if the <em>Times</em> wins, it may significantly hamper the development of AI and cause the United States to lose its position as the leader in the global AI development race. The more AI companies are exposed to copyright liability, the greater risk and thus loss to competition, which means less innovation. Conversely, they also worry that if the <em>Times</em> loses, it will further cut into the already struggling journalism business, where it’s already hard enough to find quality reports you can trust. This, too, would severely hurt AI development, which is always starving for good clean data. It appears to be a lose–lose situation for the AI field.</p>
</div>
<div class="readable-text intended-text" id="p27">
<p>Regardless of who wins or loses the lawsuit, it’s pretty clear that current copyright laws never took into consideration that robots would eventually copy us. We need new laws, and it’s unclear whether our lawmakers are technically capable enough to meet the challenge. So again, we’d encourage you to participate in the creation process of regulations within your own communities.</p>
</div>
<div class="readable-text" id="p28">
<h4 class="readable-text-h4 sigil_not_in_toc">AI detection</h4>
</div>
<div class="readable-text" id="p29">
<p>One area of concern that continues to break our hearts comes from the rise of “AI detection” products. Let us just state from the start: these products are all snake oils and shams. There’s no reliable way to determine whether a piece of text was written by a human or a bot. By this point in the book, we expect most readers to have come to this conclusion as well. The reason is simple: if we can reliably determine what is and isn’t generated text, we can create a new model to beat the detector. This is the whole point of adversarial machine learning.</p>
</div>
<div class="readable-text intended-text" id="p30">
<p>There has been a running gag online that anything you read with the word “delve” in it must be written by an LLM (e.g., <a href="https://mng.bz/o0nr">https://mng.bz/o0nr</a>). The word <em>delve</em> is statistically more likely to occur in generated text than in human speech, but that brings up the obvious questions: Which model? Which prompt? The human hubris to believe one can identify generated content simply by looking for particular words is laughable. But, of course, if people vainly believe this obvious falsehood, it’s no surprise they are willing to believe a more complex system or algorithm will be able to do it even better.</p>
</div>
<div class="readable-text intended-text" id="p31">
<p>The reason it breaks our hearts, though, is because we’ve read story after story of students getting punished, given failing grades on papers, forced to drop out of classes, and given plagiarism marks on their transcripts. Now, we don’t know the details of every case, but as experts in the technology in question, we choose to believe the students more often than not.</p>
</div>
<div class="readable-text intended-text" id="p32">
<p>The fact that a paper marked by an “AI detection” system as having a high probability of being written by AI is put in the same category as plagiarism is also ridiculous. Now, we don’t condone cheating, but LLMs are a new tool. They help us with language the way calculators help us with math. We have figured out ways to teach and evaluate students’ progress without creating “calculator detection” systems. We can do it again.</p>
</div>
<div class="readable-text intended-text" id="p33">
<p>Look, it’s not that it’s impossible to identify generated content. One investigation found that by simply searching for phrases like “As an AI language model” or “As of my last knowledge update,” they found hundreds of published papers in scientific journals written with the help of LLMs.<a href="#footnote-98"><sup class="footnote-reference" id="footnote-source-2">2</sup></a> Some phrases are obvious signs, but these are only identified due to the pure laziness of the authors.</p>
</div>
<div class="readable-text intended-text" id="p34">
<p>The worst part of all this is that since these detection systems are fake, bad, and full of false positives, they seem to be enforced arbitrarily and randomly at the teacher’s discretion. It’s hard to believe that a majority of papers aren’t flagged, so why is only a select group of students called out for it? It’s because these systems appear to have become a weapon of power and discrimination for teachers who will wield them to punish students they don’t like—not to mention the obvious hypocrisy since we could guess that some of these teachers are the same ones publishing papers with phrases like “As an AI language model” in them.</p>
</div>
<div class="readable-text" id="p35">
<h4 class="readable-text-h4 sigil_not_in_toc">Bias and ethics</h4>
</div>
<div class="readable-text" id="p36">
<p>This isn’t the first time we have spoken about bias and ethics found inside LLMs, but this time, let’s take a slightly deeper dive into what the discussion deserves. Let’s say a person is tied to some trolley tracks, you do nothing, and the trolley runs them over, ending their life. Are you responsible? This thought experiment, called “The Trolley Problem,” has been discussed ad nauseam; there’s even a video game (Trolley Problem Inc. from Read Graves) that poses dozens of variations based on published papers. We won’t even attempt to answer the question, but we will give you a brief rundown on how you might be able to decide the answer for yourself.</p>
</div>
<div class="readable-text intended-text" id="p37">
<p>There are way more than two ways you can analyze this, but we’ll only focus on two—the moral and the ethical—and we’ll reduce these because this isn’t a philosophy book. Morality here helps you determine fault based on a belief of what is good/not good. Ethics help us determine consequences within the practical framework of the legal system that exists within the societies we live in. If you are morally responsible for the death of the person on the tracks, you believe that it was ultimately your fault, that your actions are the cause of the disliving. This is different from ethical responsibility, which would mean that you deserve legal and societal consequences for that action. They can agree, but they don’t have to. Changing the context can help clarify the distinction: if you tell someone that a knife isn’t sharp and they cut themselves on it while checking, morally, it’s likely your fault they were in that situation, but ethically, you will avoid an attempted murder charge.</p>
</div>
<div class="readable-text intended-text" id="p38">
<p>Algorithms create thousands of these situations where our morality and our ethics likely don’t agree. There’s an old example of moral and ethical responsibility in the Talmud that decides that a person is not a murderer if they push another person into water or fire and the pushed person fails to escape.<a href="#footnote-99"><sup class="footnote-reference" id="footnote-source-3">3</sup></a> Depending on your beliefs and the law you live under, Meta could be either morally or ethically at fault for genocide (not joking<a href="#footnote-100"><sup class="footnote-reference" id="footnote-source-4">4</sup></a>) in Myanmar. Meta didn’t even do the pushing into the fire in that scenario; their algorithm did. This is obviously a charged and brutal example, but LLMs create a very real scenario where ML practitioners need practical, consistent, and defensible frameworks of both morality and ethics, or they risk real tragedy under their watch. Obviously, we aren’t the arbiters of morality and aren’t going to judge you about where you find yourself there, but you should still consider the broader context of any system you create.</p>
</div>
<div class="readable-text" id="p39">
<h4 class="readable-text-h4 sigil_not_in_toc">Laws are coming</h4>
</div>
<div class="readable-text" id="p40">
<p>One thing we <em>can </em>be sure about is that regulation will come, and companies will be held responsible for what their AI agents do. Air Canada found this out the hard way when the courts ruled against it, saying the company had to honor a refund policy that its chatbot had completely made up (<a href="https://mng.bz/pxvG">https://mng.bz/pxvG</a>). The bot gave incorrect information. It did link the customer to the correct refund policy; however, the courts rightly questioned “why customers should have to double-check information found in one part of its website on another part of its website.”</p>
</div>
<div class="readable-text intended-text" id="p41">
<p>We’ve seen similar cases where users have used prompt engineering to trick Chevy’s LLM chatbot into selling a 2024 Tahoe for $1 (<a href="https://mng.bz/XVmG">https://mng.bz/XVmG</a>), and DPD needed to “shut down its AI element” after a customer got it to admit to being the worst delivery company in the world.<a href="#footnote-101"><sup class="footnote-reference" id="footnote-source-5">5</sup></a> As we said earlier, it’s difficult to tell, even with existing legislation, what is ethically allowable for an LLM to do. Of course, it brings up the question of whether, if the chatbot was licensed and equipped to sell cars and did complete such a transaction, the customer’s bad faith interaction would actually matter, or whether a company would still be held ethically responsible for upholding such a transaction.</p>
</div>
<div class="readable-text intended-text" id="p42">
<p>Being held responsible for what an LLM generates is enough to make you think twice about many applications you may consider using it for. The higher the risk, the more time you should take to pause and consider potential legal ramifications. We highly recommend dialing in your prompt engineering system, setting up guard rails to keep your agent on task, and absolutely being sure to save your logs and keep your customer chat history.</p>
</div>
<div class="readable-text" id="p43">
<h3 class="readable-text-h3" id="sigil_toc_id_185"><span class="num-string">12.2.2</span> LLMs are getting bigger</h3>
</div>
<div class="readable-text" id="p44">
<p>Another thing we can be sure of is that we will continue to see models getting bigger and bigger for the near future. Since larger models continue to display emergent behavior, there’s no reason for companies to stop taking this approach when simply throwing money at the problem seems to generate more money. Not to mention, for companies that have invested the most, larger models are harder to replicate. As you’ve probably found, the best way for smaller companies to compete is to create smaller, specialized models. Ultimately, as long as we have large-enough training datasets to accommodate more parameters, we can expect to see more parameters stuffed into a model, but the question of whether we’ve ever had adequate data to demonstrate “general intelligence” (as in AGI) is as murky as ever.</p>
</div>
<div class="readable-text" id="p45">
<h4 class="readable-text-h4 sigil_not_in_toc">Larger context windows</h4>
</div>
<div class="readable-text" id="p46">
<p>It’s not just larger models. We are really excited to see context lengths grow as well. When we started working on this book, they were a real limitation. It was rare to see models with context lengths greater than 10K tokens. ChatGPT only offered lengths up to 4,096 tokens at the time. A year later, and we see models like Gemini 1.5 Pro offering a context length of up to 1 million tokens, with researchers indicating that it can handle up to 10 million tokens in test cases (<a href="https://mng.bz/YV4N">https://mng.bz/YV4N</a>). To put it in perspective, the entire seven-book Harry Potter series is 1,084,170 words (I didn’t count them; <a href="https://wordsrated.com/harry-potter-stats/">https://wordsrated.com/harry-potter-stats/</a>), which would come out to roughly 1.5 million tokens depending on your tokenizer. At these lengths, it’s hard to believe there are any limitations.</p>
</div>
<div class="readable-text intended-text" id="p47">
<p>Obviously, there still are. These larger models with near infinite context windows generally have you paying per token. If the model doesn’t force your users to send smaller queries, your wallet will. Not to mention, if you are reading this book, you are likely more interested in smaller open source models you can deploy yourself, and many of these definitely still have limiting context sizes you have to work with. Don’t worry, though; right now and in the future, even smaller models will have million-sized context windows. There’s a lot of interesting research going into this area. If you are interested, we recommend you check out RoPE,<a href="#footnote-102"><sup class="footnote-reference" id="footnote-source-6">6</sup></a> YaRN,<a href="#footnote-103"><sup class="footnote-reference" id="footnote-source-7">7</sup></a> and Hyena.<a href="#footnote-104"><sup class="footnote-reference" id="footnote-source-8">8</sup></a></p>
</div>
<div class="readable-text" id="p48">
<h4 class="readable-text-h4 sigil_not_in_toc">The next attention</h4>
</div>
<div class="readable-text" id="p49">
<p>Of course, larger context windows are great, but they come at a cost. Remember, at the center of an LLM lies the attention algorithm, which is quadratic in complexity—meaning the more data we throw at it, the more compute we have to throw at it as well. One challenge driving the research community is finding the next attention algorithm that doesn’t suffer from this same problem. Can we build transformers with a new algorithm that is only linear in complexity? That is the billion-dollar question right now.</p>
</div>
<div class="readable-text intended-text" id="p50">
<p>There are lots of competing innovations in this field, and we don’t even have time to discuss all of our absolute favorites. Two of those favorites are MAMBA, an alternative to transformers, and KAN, an alternative to multilayer perceptrons (MLPs). MAMBA, in particular, is an improvement on state space models (SSMs) incorporated into an attention-free neural network architecture.<a href="#footnote-105"><sup class="footnote-reference" id="footnote-source-9">9</sup></a> By itself, it isn’t all that impressive, as it took lots of hardware hacking to make it somewhat performant. However, later JAMBA came out, a MAMBA-style model that uses hybrid SSM-transformer layers and joint attention.<a href="#footnote-106"><sup class="footnote-reference" id="footnote-source-10">10</sup></a> The hybrid approach appears to give us the best of both worlds.</p>
</div>
<div class="readable-text intended-text" id="p51">
<p>So you can experience it for yourself, in listing 12.1, we will finetune and run inference on a JAMBA model. This model is a mixture-of-experts model with 52B parameters, and the implementation will allow for 140K context lengths on an 80 GB GPU, which is much better performance than you’d get with an attention model alone. This example was adapted right from the Hugging Face model card, so the syntax should look very familiar compared to every other simple transformer implementation, and we are very grateful for the ease of trying out brand-new stuff. </p>
</div>
<div class="readable-text intended-text" id="p52">
<p>For the training portion, unfortunately, the model is too big, even in half precision, to fit on a single 80 GB GPU, so you’ll have to use Accelerate to parallelize it between several GPUs to complete training. If you don’t have that compute just lying around, you can complete the imports up to the tokenizer and skip to after the training portion, changing very little. We aren’t doing anything fancy; the dataset we’ll use for training is just a bunch of famous quotes in English from various authors retrieved from Goodreads consisting of quote, author, and tags, so don’t feel like you are missing out if you decide to skip finetuning. We’ll start by loading the tokenizer, model, and dataset.</p>
</div>
<div class="browsable-container listing-container" id="p53">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.1</span> Finetuning and inferencing JAMBA </h5>
<div class="code-area-container">
<pre class="code-area">from trl import SFTTrainer
from peft import LoraConfig
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
)
from transformers import BitsAndBytesConfig
import torch
from datasets import load_dataset

tokenizer = AutoTokenizer.from_pretrained("ai21labs/Jamba-v0.1")
model = AutoModelForCausalLM.from_pretrained(
    "ai21labs/Jamba-v0.1", device_map="auto"
)

dataset = load_dataset("Abirate/english_quotes", split="train")</pre>
</div>
</div>
<div class="readable-text" id="p54">
<p>Once all of those are in memory (you can stream the dataset if your hardware is limited), we’ll create training arguments and a LoRA config to help the finetuning work on even smaller hardware:</p>
</div>
<div class="browsable-container listing-container" id="p55">
<div class="code-area-container">
<pre class="code-area">training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    logging_dir="./logs",
    logging_steps=10,
    learning_rate=2e-3,
)
lora_config = LoraConfig(
    r=8,
    target_modules=["embed_tokens", "x_proj", "in_proj", "out_proj"],
    task_type="CAUSAL_LM",
    bias="none",
)</pre>
</div>
</div>
<div class="readable-text" id="p56">
<p>And now, for the finale, similar to sklearn’s <code>model.fit()</code>, transformers’ <code>trainer.train()</code> has become a moniker for why anyone can learn how to interact with state-of-the-art ML models. Once training completes (it took a little under an hour for us), we’ll save local versions of the tokenizer and the model and delete the model in memory:</p>
</div>
<div class="browsable-container listing-container" id="p57">
<div class="code-area-container">
<pre class="code-area">trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    peft_config=lora_config,
    train_dataset=dataset,
    dataset_text_field="quote",
)

trainer.train()

tokenizer.save_pretrained("./JAMBA/")
model.save_pretrained("./JAMBA/")

del model</pre>
</div>
</div>
<div class="readable-text" id="p58">
<p>Next, we’ll reload the model, but in a memory-efficient way, to be used for inference. With an 80 GB GPU and loading in 8bit with this BitsandBytes config, you can now fit the model and a significant amount of data on a single GPU. Loading in 4bit allows that on any type of A100 or two 3090s, similar to a 70B parameter transformer. Using quantization to get it down to a 1-bit model, you can fit this model and a significant amount of data on a single 3090. We’ll use the following 8bit inference implementation and run inference on it:</p>
</div>
<div class="browsable-container listing-container" id="p59">
<div class="code-area-container">
<pre class="code-area">quantization_config = BitsAndBytesConfig(
    load_in_8bit=True, llm_int8_skip_modules=["mamba"]
)
model = AutoModelForCausalLM.from_pretrained(
    "ai21labs/Jamba-v0.1",
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
    quantization_config=quantization_config,
)
input_ids = tokenizer(
    "In the recent Super Bowl LVIII,", return_tensors="pt"
).to(model.device)["input_ids"]

outputs = model.generate(input_ids, max_new_tokens=216)

print(tokenizer.batch_decode(outputs))</pre>
</div>
</div>
<div class="readable-text" id="p60">
<p>We are blown away almost monthly at this point by the alternatives to various parts of LLM systems that pop up. Here, we’d like to draw your attention way back to where LLMs got their big break: “Attention Is All You Need.”<a href="#footnote-107"><sup class="footnote-reference" id="footnote-source-11">11</sup></a> That paper showed that you could use dumb MLPs to get amazing results, using only attention to bridge the gap. We’re entering a new age where we aren’t focusing on just what we need but what we want for the best results. For example, we want subquadratic drop-in replacements for attention that match or beat flash attention for speed. We want attention-free transformers and millions-long context lengths with no “lost in the middle” problems. We want alternatives to dense MLPs with no drops in accuracy or learning speed. We are, bit by bit, getting all of these and more.</p>
</div>
<div class="readable-text" id="p61">
<h4 class="readable-text-h4 sigil_not_in_toc">Pushing the boundaries of compression</h4>
</div>
<div class="readable-text" id="p62">
<p>After going down to INT4, there are experimental quantization strategies for going even further down to INT2. INT2 70B models still perform decently, much to many peoples’ surprise. Then there’s research suggesting we could potentially go even smaller to 1.58 bits per weight or even 0.68 using ternary and other smaller operators. Want to test it out? Llama3 70B already has 1-bit quantization implementations in GGUF, GPTQ, and AWQ formats, and it only takes up 16.6 GB of memory. Go nuts!</p>
</div>
<div class="readable-text intended-text" id="p63">
<p>There’s another dimension to this, which doesn’t involve compressing models but instead decouples the idea of models being one piece and thinking of models as collections of layers and parameters again. Speculative decoding gives us yet another way of accessing large models quickly. Speculative decoding requires not just enough memory to load one large model but also another smaller model alongside it—think distillation models. An example often used in production these days is Whisper-Large-v3 and Distil-Whisper-Large-V3. Whisper is a multimodal LLM that focuses on the speech-to-text problem, but speculative decoding will work with any two models that have the same architecture and different sizes. </p>
</div>
<div class="readable-text intended-text" id="p64">
<p>This method allows us to sample larger models quicker (sometimes a straight 2× speed boost) by computing several tokens in parallel and by an approximation “assistant” model that allows us to both complete a step and verify whether that step is easy or hard at the same time. The basic idea is this: use the smaller, faster Distil-Whisper model to generate guesses about the end result, and allow Whisper to evaluate those guesses in parallel, ignoring the ones that it would do the same thing on and correcting the ones that it would change. This allows for the speed of a smaller model with the accuracy of a larger one.</p>
</div>
<div class="readable-text intended-text" id="p65">
<p>In listing 12.2, we demonstrate speculative decoding on an English audio dataset. We’ll load Whisper and Distil-Whisper, load the dataset, and then add an <code>assistant_ model</code> to the generation keyword arguments (<code>generate_kwargs</code>). You may ask, how does this system know that the assistant model is only meant to help with decoding, as the name suggests? Well, we load the assistant model with <code>AutoModelForCausalLM</code> instead of the speech sequence-to-sequence version. This way, the model will only help with the easier decoding steps in parallel with the larger one. With that done, we’re free to test.</p>
</div>
<div class="browsable-container listing-container" id="p66">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.2</span> Speculative decoding with Whisper </h5>
<div class="code-area-container">
<pre class="code-area">from transformers import (
    AutoModelForCausalLM,
    AutoModelForSpeechSeq2Seq,
    AutoProcessor,
)
import torch
from datasets import load_dataset

from time import perf_counter
from tqdm import tqdm

from evaluate import load

device = "cuda:0" if torch.cuda.is_available() else "cpu"
print(f"Device: {device}")
attention = "sdpa"
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

model_id = "openai/whisper-large-v3"
assistant_model_id = "distil-whisper/distil-large-v3"
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_id,
    low_cpu_mem_usage=False,
    use_safetensors=True,
    attn_implementation=attention,
    torch_dtype=torch_dtype,
).to(device)
processor = AutoProcessor.from_pretrained(model_id)
assistant_model = AutoModelForCausalLM.from_pretrained(
    assistant_model_id,
    low_cpu_mem_usage=False,
    use_safetensors=True,
    attn_implementation=attention,
    torch_dtype=torch_dtype,
).to(device)

dataset = load_dataset(
    "hf-internal-testing/librispeech_asr_dummy",
    "clean",
    split="validation",
    trust_remote_code=True,
)
wer = load("wer")

generate_kwargs_1 = {
    "language": "en",
    "task": "transcribe",
}
generate_kwargs_2 = {
    "language": "en",
    "task": "transcribe",
    "assistant_model": assistant_model,
}

spec_decoding = False
for i, generate_kwargs in enumerate([generate_kwargs_1, generate_kwargs_2]):
    all_time = 0
    predictions = []
    references = []
    for sample in tqdm(dataset):
        audio = sample["audio"]
        inputs = processor(
            audio["array"],
            sampling_rate=audio["sampling_rate"],
            return_tensors="pt",
        )
        inputs = inputs.to(device=device, dtype=torch_dtype)
        start_time = perf_counter()
        output = model.generate(
            **inputs,
            **generate_kwargs,
        )
        gen_time = perf_counter() - start_time
        all_time += gen_time
        predictions.append(
            processor.batch_decode(
                output, skip_special_tokens=True, normalize=True
            )[0]
        )
        references.append(processor.tokenizer.normalize(sample["text"]))
    score = wer.compute(predictions=predictions, references=references)
    if i &gt; 0:
        spec_decoding = True
    print(f"Speculative Decoding: {spec_decoding}")
    print(f"Time: {all_time}")
    print(f"Word Error Rate: {score}")</pre>
</div>
</div>
<div class="readable-text" id="p67">
<p>In our testing, we observed about 42 seconds for Whisper-Large-V3 to get through all 73 examples with scaled dot product attention. With speculative decoding, that dropped to 18.7 seconds, with the exact same word error rate (WER). So there was an almost 2× speed increase with absolutely zero drop in accuracy. Yeah, pretty nuts.</p>
</div>
<div class="readable-text intended-text" id="p68">
<p>At this point, we were wondering, “Why doesn’t everyone use this for everything all the time?” Here are the drawbacks to this method: first, it works best in smaller sequences. With LLMs, that’s under 128 tokens of generation or around 20 seconds of audio processing. With the larger generations, the speed boost will be negligible. Beyond that, we don’t always have access to perfectly compatible pairs of large and small models, like BERT versus DistilBERT. The last reason is that very few people really know about it, even with its ease of implementation.</p>
</div>
<div class="readable-text intended-text" id="p69">
<p>Ultimately, whether it’s sub-bit quantization, speculative decoding, or other advances, LLMs are pushing research into compression methodologies more than any other technology, and it’s interesting to watch as new techniques change the landscape. As these methods improve, we can push models to smaller and cheaper hardware, making the field even more accessible.</p>
</div>
<div class="readable-text" id="p70">
<h3 class="readable-text-h3" id="sigil_toc_id_186"><span class="num-string">12.2.3</span> Multimodal spaces</h3>
</div>
<div class="readable-text" id="p71">
<p>We are so excited about the possibilities within multimodality. Going back to chapter 2, multimodality is one of the main features of language we haven’t seen as many solutions crop up for, and we’re seeing a shift toward actually attempting to solve phonetics. Audio isn’t the only modality that humans operate in, though. Accordingly, the push toward combining phonetics, semantics, and pragmatics and getting as much context within the same embedding space (for comparison) as the text is very strong. With this in mind, here are some points of interest in the landscape.</p>
</div>
<div class="readable-text intended-text" id="p72">
<p>The first we want to draw attention to is ImageBind, a project showcasing that instead of trying to curtail a model into ingesting every type of data, we can instead squish every type of data into an embedding space the model would already be familiar with and be able to process. You can take a look at the official demo here: <a href="https://imagebind.metademolab.com/">https://imagebind.metademolab.com/</a>.</p>
</div>
<div class="readable-text intended-text" id="p73">
<p>ImageBind builds off what multimodal projection models such as CLIP have already been showcasing for some time: the ability to create and process embeddings is the true power behind deterministic LLM systems. You can use these models for very fast searches, including searches that have been, up to this point, nigh impossible, like asking to find images of animals that make sounds similar to an uploaded audio clip.</p>
</div>
<div class="readable-text intended-text" id="p74">
<p>OneLLM flips this logic the other way around, taking one model and one multimodal encoder to unify and embed eight modalities instead of the ImageBind example of using six different encoders to embed six modalities in the same dimension. It can be found here: <a href="https://onellm.csuhan.com/">https://onellm.csuhan.com/</a>. The big idea behind OneLLM is aligning the unified encoder using language, which offers a unique spin on multimodality that aligns the process of encoding rather than the result.</p>
</div>
<div class="readable-text intended-text" id="p75">
<p>We are extremely excited about the research happening in this area. This research is able to help bridge the gap between phonetics and pragmatics in the model ecosystem and allow for more human-like understanding and interaction, especially in the search field.</p>
</div>
<div class="readable-text" id="p76">
<h3 class="readable-text-h3" id="sigil_toc_id_187"><span class="num-string">12.2.4</span> Datasets</h3>
</div>
<div class="readable-text" id="p77">
<p>One exciting change we are seeing inside the industry due to the introduction of LLMs is that companies are finally starting to understand the importance of governing and managing their data. For some, it’s the drive to finetune their own LLMs and get in on the exciting race to deliver AI products. For others, it’s the fear of becoming obsolete, as the capabilities of these systems far surpass previous technologies; they are finding it’s only their data that provides any type of moat or protection from competition. And for everyone, it’s the worry they’ll make the same mistakes they’ve seen other companies make.</p>
</div>
<div class="readable-text intended-text" id="p78">
<p>LLMs aren’t just a driving factor; they are also helping teams label, tag, organize, and clean their data. Many companies had piles of data they didn’t know what to do with, but with LLM models like CLIP, captioning images has become a breeze. Some companies have found that simply creating embedding spaces of their text, images, audio, and video has allowed them to create meaningful structures for datasets previously unstructured. Structured data is much easier to operate around, opening doors for search, recommendations, and other insights.</p>
</div>
<div class="readable-text intended-text" id="p79">
<p>One aspect we see currently missing in the industry is valuable open source datasets, especially when it comes to evaluations. Many of the current benchmarks used to evaluate models rely on multiple-choice questions, but this is inefficient for anyone trying to create an LLM application. In the real world, when are your users going to ask your model questions in a multiple-choice format? Next to never. People ask freeform questions in conversations and when seeking help since they don’t know the answer themselves. However, these evaluation datasets have become benchmarks simply because they are easy for researchers to gather, compile, and evaluate for accuracy.</p>
</div>
<div class="readable-text intended-text" id="p80">
<p>In addition, we believe another inevitability is the need for more language representation. The world is a tapestry of diverse languages and dialects, each carrying its unique cultural nuances and communicative subtleties. However, many languages remain underrepresented in existing datasets, leading to models that are biased toward more dominant languages. As technology becomes increasingly global, the inclusion of a wider range of languages is crucial. Adding multiple languages not only promotes inclusivity but also enhances the accuracy and applicability of language models in various international contexts, bridging communication gaps and fostering a more connected world. Imagine your startup didn’t need to pay anyone to get accurate information regarding entering China, Russia, or Saudi Arabia to expand your market.</p>
</div>
<div class="readable-text" id="p81">
<h3 class="readable-text-h3" id="sigil_toc_id_188"><span class="num-string">12.2.5</span> Solving hallucination</h3>
</div>
<div class="readable-text" id="p82">
<p>There’s a lot of evidence that LLMs have more information in them than they readily give out and even more evidence that people are generally either terrible or malicious at prompting. As a result, you’ll find that hallucinations are one of the largest roadblocks when trying to develop an application that consistently delivers results. This problem has frustrated many software engineering teams that are used to deterministic computer algorithms and rarely deal with nondeterministic systems. For many statisticians who are more familiar with these types of systems, hallucinations are seen as a feature, not a bug. Regardless of where you stand, there’s a lot of research going into the best ways to handle hallucinations, and this is an area of interest you should be watching.</p>
</div>
<div class="readable-text" id="p83">
<h4 class="readable-text-h4 sigil_not_in_toc">Better prompt engineering</h4>
</div>
<div class="readable-text" id="p84">
<p>One area that’s interesting to watch and has shown great improvement over time is prompt engineering. One prompt engineering tool that helps reduce hallucinations is DSPy. We went over it briefly in chapter 7, but here we’ll give an example of how it works and why it can be a helpful step for solving hallucination in your LLMs. We’ve discussed the fact that LLMs are characteristically bad at math, even simple math, several times throughout the book, and we’ve also discussed why, but we haven’t really discussed solutions other than improving your tokenization. So in listing 12.3, we will show just how good you can coax an LLM to be at math with zero tokenization changes, zero finetuning, and no LoRAs or DoRAs, just optimizing your prompts to tell the model exactly how to answer the questions you’re asking. </p>
</div>
<div class="readable-text intended-text" id="p85">
<p>We’ll do this using the dspy-ai Python package and Llama3-8B-Instruct. We’ll start by loading and quantizing the model to fit on most GPUs and the Grade-School Math 8K dataset. We picked this dataset because it’s a collection of math problems that you, as a person who has graduated elementary (primary) school, likely don’t even need a calculator to solve. We’ll use 200 examples for our train and test (dev) sets, although we’d recommend you play with these numbers to find the best ratio for your use case without data leakage.</p>
</div>
<div class="browsable-container listing-container" id="p86">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.3</span> DSPy for math</h5>
<div class="code-area-container">
<pre class="code-area">from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import BitsAndBytesConfig
import torch
import dspy
from dspy.datasets.gsm8k import GSM8K, gsm8k_metric
from dsp.modules.lm import LM
from dspy.evaluate import Evaluate
from dspy.teleprompt import BootstrapFewShot

model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)


model = AutoModelForCausalLM.from_pretrained(
    model_name, 
    device_map="auto", 
    quantization_config=quantization_config,
    attn_implementation="sdpa",
)
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True,)

gms8k = GSM8K()
gsm8k_trainset, gsm8k_devset = gms8k.train[:30], gms8k.dev[:100]</pre>
</div>
</div>
<div class="readable-text" id="p87">
<p>Now that we have our imports and loading ready, we’ll need to address the fact that we loaded Llama3 using transformers and not DSPy. DSPy expects to interact with models utilizing the OpenAI API, but we have a model loaded locally from Hugging Face, DSPy has recently added HFModel to their package, and it can now be easily imported, rather than needing the wrapper defined next. First, we make a simple function to map any keyword argument differences between the APIs, like <code>max_tokens</code> vs <code>max_new_tokens</code>, and then we create a class that will act as the wrapper for our model to generate answers and optimize the prompt. Once that’s ready, we’ll load DSPy:</p>
</div>
<div class="browsable-container listing-container" id="p88">
<div class="code-area-container">
<pre class="code-area">def openai_to_hf(**kwargs):
    hf_kwargs = {}
    for k, v in kwargs.items():
        if k == "n":
            hf_kwargs["num_return_sequences"] = v
        elif k == "frequency_penalty":
            hf_kwargs["repetition_penalty"] = 1.0 - v
        elif k == "presence_penalty":
            hf_kwargs["diversity_penalty"] = v
        elif k == "max_tokens":
            hf_kwargs["max_new_tokens"] = v
        elif k == "model":
            pass
        else:
            hf_kwargs[k] = v

    return hf_kwargs


class HFModel(LM):
    def __init__(
        self,
        model: AutoModelForCausalLM,
        tokenizer: AutoTokenizer,
        **kwargs
    ):
        """wrapper for Hugging Face models
        Args:
            model (AutoModelForCausalLM): HF model identifier to load and use
            tokenizer: AutoTokenizer
        """
        super().__init__(model)
        self.model = model
        self.tokenizer = tokenizer
        self.drop_prompt_from_output = True
        self.history = []
        self.is_client = False
        self.device = model.device
        self.kwargs = {
            "temperature": 0.3,
            "max_new_tokens": 300,
        }

    def basic_request(self, prompt, **kwargs):
        raw_kwargs = kwargs
        kwargs = {**self.kwargs, **kwargs}
        response = self._generate(prompt, **kwargs)

        history = {
            "prompt": prompt,
            "response": response,
            "kwargs": kwargs,
            "raw_kwargs": raw_kwargs,
        }
        self.history.append(history)

        return response

    def _generate(self, prompt, **kwargs):
        kwargs = {**openai_to_hf(**self.kwargs), **openai_to_hf(**kwargs)}
        if isinstance(prompt, dict):
            try:
                prompt = prompt["messages"][0]["content"]
            except (KeyError, IndexError, TypeError):
                print("Failed to extract 'content' from the prompt.")
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        outputs = self.model.generate(**inputs, **kwargs)    
        if self.drop_prompt_from_output:
            input_length = inputs.input_ids.shape[1]
            outputs = outputs[:, input_length:]
        completions = [
            {"text": c}
            for c in self.tokenizer.batch_decode(
                outputs, skip_special_tokens=True
            )
        ]
        response = {
            "prompt": prompt,
            "choices": completions,
        }
        return response
    def __call__(
        self, prompt, only_completed=True, return_sorted=False, **kwargs
    ):
        assert only_completed, "for now"
        assert return_sorted is False, "for now"

        if kwargs.get("n", 1) &gt; 1 or kwargs.get("temperature", 0.0) &gt; 0.1:
            kwargs["do_sample"] = True

        response = self.request(prompt, **kwargs)
        return [c["text"] for c in response["choices"]]


print("Model set up!")             <span class="aframe-location"/> #1
llama = HFModel(model, tokenizer)

dspy.settings.configure(lm=llama)    <span class="aframe-location"/> #2</pre>
<div class="code-annotations-overlay-container">
     #1 Sets up the LM
     <br/>#2 Sets up ΔSPY to use that LM
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p89">
<p>Now that we are prepared with an LLM to take our math test, let’s test it. We’ll start by establishing a baseline. We’ll define a simple chain-of-thought (CoT)-like prompt in the <code>QASignature</code> class, which we’ll use to define a zero-shot version to use as a baseline. The prompt is likely pretty close to prompts you’ve seen before, so hopefully, this will be a very relevant demonstration of tasks you may be working on. For evaluation, we’re using DSPy’s <code>gsm8k_metric</code>, which we imported at the top to evaluate against, but you could always create your own:</p>
</div>
<div class="browsable-container listing-container" id="p90">
<div class="code-area-container">
<pre class="code-area">class QASignature(dspy.Signature):        <span class="aframe-location"/> #1
    (
    """You are given a question and answer"""
    """and you must think step by step to answer the question. """
    """Only include the answer as the output."""
    )
    question = dspy.InputField(desc="A math question")
    answer = dspy.OutputField(desc="An answer that is a number")


class ZeroShot(dspy.Module):
    def __init__(self):
    super().__init__()
    self.prog = dspy.Predict(QASignature, max_tokens=1000)

    def forward(self, question):
    return self.prog(question=question)


evaluate = Evaluate(       <span class="aframe-location"/> #2
    devset=gsm8k_devset,
    metric=gsm8k_metric,
    num_threads=4,
    display_progress=True,
    display_table=0,
)

print("Evaluating Zero Shot")     <span class="aframe-location"/> #3
evaluate(ZeroShot())</pre>
<div class="code-annotations-overlay-container">
     #1 Δefines the QASignature and CoT
     <br/>#2 Sets up the evaluator, which can be used multiple times
     <br/>#3 Evaluates how the LLM does with no changes
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p91">
<p>The output is</p>
</div>
<div class="browsable-container listing-container" id="p92">
<div class="code-area-container">
<pre class="code-area">29/200 14.5%</pre>
</div>
</div>
<div class="readable-text" id="p93">
<p>With our simple zero-shot CoT prompt, Llama3 gets only 14.5% of the questions correct. This result might not seem very good, but it is actually quite a bit better than just running the model on the questions alone without any prompt, which only yields about 1% to 5% correct.</p>
</div>
<div class="readable-text intended-text" id="p94">
<p>With the baseline out of the way, let’s move on to the bread and butter of DSPy, optimizing the prompt to see where that gets us. There’s been some evolution in what people think of as a CoT prompt since the original paper came out. CoT has evolved in the industry to mean more than just adding “think step by step” in your prompt since this approach is seen more as just basic prompt engineering, whereas allowing the model to few-shot prompt itself to get a rationale for its ultimate output is considered the new CoT, and that’s how the DSPy framework uses those terms. With that explanation, we’ll go ahead and create a <code>CoT</code> class using the <code>dspy.ChainOfThought</code> function and then evaluate it like we did our <code>ZeroShot</code> class:</p>
</div>
<div class="browsable-container listing-container" id="p95">
<div class="code-area-container">
<pre class="code-area">config = dict(max_bootstrapped_demos=2)    <span class="aframe-location"/> #1

class CoT(dspy.Module):
    def __init__(self):
    super().__init__()
    self.prog = dspy.ChainOfThought(QASignature, max_tokens=1000)

    def forward(self, question):
    return self.prog(question=question)

print("Creating Bootstrapped Few Shot Prompt")        <span class="aframe-location"/> #2
teleprompter = BootstrapFewShot(metric=gsm8k_metric, **config)
optimized_cot = teleprompter.compile(
    CoT(), trainset=gsm8k_trainset, valset=gsm8k_devset
)
optimized_cot.save("optimized_llama3_math_cot.json")

print("Evaluating Optimized CoT Prompt")           <span class="aframe-location"/> #3
evaluate(optimized_cot)                  
#149/200 74.5%</pre>
<div class="code-annotations-overlay-container">
     #1 Sets up the optimizer
     <br/>#2 Optimize the prompts
     <br/>#3 Evaluates our “optimized_cot” program
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p96">
<p>Look at that! If it doesn’t astonish you that the accuracy jumped from 14.5% to 74.5% by changing only the prompts—remember we haven’t done any finetuning or training—we don’t know what will. People are speculating whether the age of the prompt engineer is over, but we’d like to think that it’s just begun. That said, the age of “coming up with a clever string and doing no follow-up” has been over and shouldn’t have ever started. In this example, we used arbitrary boundaries, gave the sections of the dataset and the numbers absolutely no thought, and didn’t include any helpful tools or context for the model to access to improve. If we did, you’d see that after applying all the prompt engineering tricks in the book, it isn’t difficult to push the model’s abilities to staggering levels, even on things LLMs are characteristically bad at—like math.</p>
</div>
<div class="readable-text" id="p97">
<h4 class="readable-text-h4 sigil_not_in_toc">Grounding</h4>
</div>
<div class="readable-text" id="p98">
<p>If you are looking for ways to combat hallucinations, you’ll run into the term <em>grounding</em>. Grounding is when we give the LLM necessary context in the prompt. By giving it the information it needs, we are helping to provide a solid base for the generation to build off of, so it’s less likely to dream up visions out of thin air. If this sounds familiar, it should, as we have used one of the most common grounding techniques, RAG, several times in this book.</p>
</div>
<div class="readable-text intended-text" id="p99">
<p>The term <em>RAG</em> (retrieval augmented generation) is, at face value, synonymous with grounding since we are literally retrieving the appropriate context based on the prompt and then using it to augment the text generated from the LLM. However, RAG has become synonymous with using semantic search with a VectorDB for the retrieval portion. Technically, you could use any type of search algorithm or any type of database, but if you tell someone in the industry you have set up a RAG system, they will assume the former architecture.</p>
</div>
<div class="readable-text intended-text" id="p100">
<p>With that clarification, RAG applications are most useful for answering simple questions. Consider the question, “What is Gal Gadot’s husband’s current job?” It’s really two questions in one, “Who is Gal Gadot’s husband?” and once we know that, “What does he do?” RAG alone is pretty terrible at solving these multistep questions, as a similarity vector search will likely return many articles about Gal Gadot and probably none about Jaron Varsano, her husband. </p>
</div>
<div class="readable-text intended-text" id="p101">
<p>We can enhance this approach in an important way that we haven’t touched on yet: using knowledge graphs. Knowledge graphs store information in a structure that captures relationships between entities. This structure consists of nodes that represent objects and edges that represent relationships. A graph database like NEO4J makes it easy to create and query knowledge graphs. And as it turns out, knowledge graphs are amazing at answering more complex multipart questions where you need to connect the dots between linked pieces of information. Why? Because they’ve already connected the dots for us. </p>
</div>
<div class="readable-text intended-text" id="p102">
<p>Many teams who have struggled to get value out of RAG have been able to see large improvements once they transitioned to a graph database from a vector one. This comes with two major hurdles, though. First, we can no longer simply embed our prompts and pull similar matches; we have the much harder task of coming up with a way to turn our prompts into queries our graph database will understand. While there are several methods to take this on, it’s just another NLP problem. Thankfully, as it turns out, LLMs are really good at this! Second, and probably the bigger problem, is that it is much harder to turn your documents into a knowledge graph. This is why vector databases have become so popular—the ease of turning your data into embeddings to search against. Turning your data into a knowledge graph will be a bit more work and take additional expertise, but it can really set you up for success down the road.</p>
</div>
<div class="readable-text intended-text" id="p103">
<p>Right now, few teams are willing to invest in the extra data engineering to prepare their data into a knowledge graph. Most companies are still looking for quick wins, building simple wrappers around LLM APIs. As the industry matures, we believe we’ll start to see organizations shift toward building knowledge graphs from their proprietary data to eke out better performance from their LLM applications.</p>
</div>
<div class="readable-text" id="p104">
<h4 class="readable-text-h4 sigil_not_in_toc">Knowledge editing</h4>
</div>
<div class="readable-text" id="p105">
<p>Another promising field of research to combat hallucinations is <em>knowledge editing</em>. Knowledge editing is the process of efficiently adjusting specific behaviors. Optimally, this would look like surgery where we precisely go in and change the exact model weights that activate when we get incorrect responses, as can be seen in figure 12.2. Knowledge editing can be used for many things, but it is often used to combat factual decay—the fact that, over time, facts change, like who the current Super Bowl winner is or the current president of any individual country. We could retrain or finetune the model, but these are often much heavier solutions that may change the model in unexpected ways when all we want to do is update a fact or two.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p106">
<img alt="figure" height="376" src="../Images/12-2.png" width="622"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.2</span> Knowledge editing is a technique to essentially perform surgery on a model to directly insert, update, or erase information.</h5>
</div>
<div class="readable-text" id="p107">
<p>Knowledge editing is an interesting field of research that we unfortunately didn’t have the space to go into in this book. A host of algorithms and techniques have been created to do it, like ROME, MEND, and GRACE. For those interested in using any of these techniques, we recommend first checking out EasyEdit at <a href="https://github.com/zjunlp/EasyEdit">https://github.com/zjunlp/EasyEdit</a>. EasyEdit is a project that has implemented the most common knowledge editing techniques and provides a framework to utilize them easily. It includes examples, tutorials, and more to get you started.</p>
</div>
<div class="readable-text" id="p108">
<h3 class="readable-text-h3" id="sigil_toc_id_189"><span class="num-string">12.2.6</span> New hardware</h3>
</div>
<div class="readable-text" id="p109">
<p>As with most popular technologies, LLMs have already created a fierce market of competition. While most companies are still competing on capabilities and features, there’s also a clear drive to make them faster and cheaper. We’ve discussed many of these methods you can employ, like quantization and compilation. One we expect to see more of is innovation around hardware.</p>
</div>
<div class="readable-text intended-text" id="p110">
<p>In fact, Sam Altman, CEO of OpenAI, has been trying to raise funds to the tune of $7 trillion dollars to invest in the semiconductor industry.<a href="#footnote-108"><sup class="footnote-reference" id="footnote-source-12">12</sup></a> We’ve talked about the global GPU shortage before, but no one is as annoyed about it as some of the biggest players. The investment would go further than just meeting demand; it would also accelerate development and research into better chips like Application-Specifc Integrated Circuits (ASICs).</p>
</div>
<div class="readable-text intended-text" id="p111">
<p>We’ve talked about and have used GPUs a lot throughout this book, but GPUs weren’t designed for AI; they were designed for graphics. Of course, that fact didn’t stop NVIDIA from briefly becoming the world’s most valuable company.<a href="#footnote-109"><sup class="footnote-reference" id="footnote-source-13">13</sup></a> ASICs are designed for specific tasks; an example would be Google’s TPUs or tensor processing units. ASICs designed to handle AI workloads are NPUs (neural processing units), and chances are, you’ve never heard of, or at least never seen, an NPU chip before. We point this out to show there’s still plenty of room for improvement, and it’s likely we will see a large array of new accelerators in the future, from better GPUs to NPUs and everything in between. For more info, take a look at Cerebras (<a href="https://cerebras.ai/product-chip/">https://cerebras.ai/product-chip/</a>).</p>
</div>
<div class="readable-text intended-text" id="p112">
<p>One of the authors of this book spent a good portion of his career working for Intel and Micron developing the now-discontinued memory technology known as 3D XPoint (3DxP). The details of 3DxP aren’t important for this discussion; what it offered, extremely fast and cheap memory, is. It was sold under the brand name Optane for several years and had even earned the moniker “The Fastest SSD Ever Made.”<a href="#footnote-110"><sup class="footnote-reference" id="footnote-source-14">14</sup></a> This technology proved itself to be almost as fast as RAM but almost as cheap to produce as NAND flash memory and could be used to replace either.</p>
</div>
<div class="readable-text intended-text" id="p113">
<p>Imagine a world where every processor conveniently had 500 GB or even 1 TB of memory space. Most of the limitations we’ve discussed so far would simply disappear. You could load entire LLMs the size of GPT-4 onto one GPU. You wouldn’t have to worry about parallelization or the underutilization problems that come with the extra overhead. Did I mention 3DxP was nonvolatile as well? Load your model once, and you’re done; you’d never need to reload it, even if you had to restart your server, which would make jobs like autoscaling so much easier.</p>
</div>
<div class="readable-text intended-text" id="p114">
<p>3DxP was a technology that had already proven itself in the market as capable, but it nonetheless suffered due to a perceived lack of demand. Consumers didn’t know what to do with this new layer in the memory hierarchy that it provided. Personally, with the arrival of LLMs, the authors see plenty of demand now for a technology like this. We’ll just have to wait and see whether the semiconductor industry decides to reinvest. </p>
</div>
<div class="readable-text" id="p115">
<h3 class="readable-text-h3" id="sigil_toc_id_190"><span class="num-string">12.2.7</span> Agents will become useful</h3>
</div>
<div class="readable-text" id="p116">
<p>Lastly, we believe LLM-based agents will eventually be more than just a novelty that works only in demos. Most agents we’ve seen have simply been feats of magic, or should I say smoke and mirrors, throwing a few prompt engineering tricks at the largest models. The fact that several of them work at all—even in a limited capacity—shines light on the possibilities.</p>
</div>
<div class="readable-text intended-text" id="p117">
<p>We’ve seen several companies chase after the holy grail, building agents to replace software engineers. In fact, you’ll see them try to build agents to replace doctors, sales associates, or managers. But just as many companies and AI experts used to promise we’d have self-driving cars in the near future, that near future keeps on eluding us. Don’t get me wrong: it’s not like we don’t have self-driving cars, but they are much more of an annoyance than anything, and they can only drive in select locations as rideshare vehicles. In a similar fashion, we aren’t too worried about agents replacing any occupation.</p>
</div>
<div class="readable-text intended-text" id="p118">
<p>What we are more interested in are small agents—agents trained and finetuned to do a specialized task but with greater flexibility to hold conversations. Many video game NPCs would benefit from this type of setup where they could not only use an LLM to hold random conversations and provide a more immersive experience but also to decide to take actions that would shape a unique story.</p>
</div>
<div class="readable-text intended-text" id="p119">
<p>We are also likely to see them do smaller tasks well first. For example, LLMs can already read your email and summarize them for you, but a simple agent would go a step further and generate email responses for you. Maybe it wouldn’t actually send them, but simply provide you with the options, and all you’d have to do is pick the one you want, and then it would send it.</p>
</div>
<div class="readable-text intended-text" id="p120">
<p>But mostly, we are excited to see LLM agents replace other bots. For example, who hasn’t uploaded their resume only to find they have to reenter all their information? Either because the resume extraction tool didn’t work well or it didn’t even exist. An LLM agent can not only read your resume and extract the information but also double-check its work and make sure it makes sense. Plus, we haven’t even mentioned the applicant tracking systems that automatically screen resumes based on keywords. These systems are often easily manipulated and terrible at separating the cream from the crop. An LLM agent has a much better chance of performing this task well. Of course, we care about ensuring fair hiring practices, but these systems are already automated and biased to some extent. A better model is an opportunity to reduce that non-useful bias. </p>
</div>
<div class="readable-text intended-text" id="p121">
<p>With this in mind, one way that models might make better agents is through the use of cache embeddings. It’s an interesting idea of something you can do with models that we haven’t really heard anyone talking about, other than Will Gaviro Rojas at a local Utah meetup. Caching embeddings allows you to cut down on repeating the same computations several times to complete several tasks in parallel. This is a more complex example, and we aren’t going to dive too deep into it so as to keep things pretty simple, but this strategy involves either copying the final layers of a model after the last hidden state to complete several tasks on their own or creating custom linear classifiers to fulfill those tasks. In listing 12.4, we dive into the entire system surrounding caching the embeddings, as we assume knowledge at this point of how to store embeddings for access later.</p>
</div>
<div class="readable-text intended-text" id="p122">
<p>We start by loading Llama3-ChatQA in INT4 quantization with BitsandBytes to make sure it fits on smaller consumer GPUs, which should be familiar at the end of this book. We give it the appropriate prompt structure for the given model, and we get our outputs. Then we access the last hidden state or the embeddings with <code>outputs.last_ hidden_states</code> and show how we could either create copies of the relevant layers to put that hidden state through (provided they’re trained to handle this) or create a custom linear classifier in PyTorch that can be fully trained on any classification task.</p>
</div>
<div class="browsable-container listing-container" id="p123">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.4</span> Caching embeddings for multiple smaller models</h5>
<div class="code-area-container">
<pre class="code-area">from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
import torch
from time import perf_counter

model_id = "nvidia/Llama3-ChatQA-1.5-8B"
device = "cuda:0" if torch.cuda.is_available() else "cpu"

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    low_cpu_mem_usage=True,
    use_safetensors=True,
    attn_implementation="sdpa",
    torch_dtype=torch.float16,
)
 system = (                       <span class="aframe-location"/>                                           #1
    "This is a chat between a user and an artificial intelligence "          #1
    "assistant. The assistant gives helpful, detailed, and polite answers "  #1
    "to the user's questions based on the context. The assistant should "    #1
    "also indicate when the answer cannot be found in the context."          #1
)                                                                            #1
question = ( #1
    "Please give a full and complete answer for the question. "
    "Can you help me find a place to eat?"
)
response = (
    "Sure, there are many locations near you that are wonderful "
    "to eat at, have you tried La Dolce Vite?"
)
question_2 = (
    "Please give a full and complete answer for the question. "
    "I'm looking for somewhere near me that serves noodles."
)


prompt = f"""System: {system}

User: {question}

Assistant: {response}

User: {question_2}

Assistant:"""
start = perf_counter()
inputs = tokenizer(tokenizer.bos_token + prompt, return_tensors="pt").to(
    device
)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("&lt;|eot_id|&gt;"),
]
text_outputs = model.generate(
    input_ids=inputs.input_ids,
    attention_mask=inputs.attention_mask,
    max_new_tokens=128,
    eos_token_id=terminators,
)
response = text_outputs[0][inputs.input_ids.shape[-1] :]
end = perf_counter() - start
 print(
    f"\n\nFull Response: {tokenizer.batch_decode(text_outputs)}"
    f"\n\nOnly Answer Response: {tokenizer.decode(response)}"
)
print(f"\nTime to execute: {end}\n")

start = perf_counter()
with torch.no_grad():          
    hidden_outputs = model(        <span class="aframe-location"/> #2
    input_ids=inputs.input_ids,
    attention_mask=inputs.attention_mask,
    output_hidden_states=True,
    )

    embeddings_to_cache = hidden_outputs.hidden_states[-1]

end = perf_counter() - start
print(f"Embeddings: {embeddings_to_cache}")
print(f"\nTime to execute: {end}\n")

for key, module in model._modules.items():
    if key == "lm_head":                        <span class="aframe-location"/> #3
    print(f"This is the layer to pass to by itself:\n{module}")
with torch.no_grad():
    start = perf_counter()
    outputs = model._modules["lm_head"](embeddings_to_cache)
    end = perf_counter() - start
    print(f"Outputs: {outputs}")
    print(f"\nTime to execute: {end}\n")


class CustomLinearClassifier(torch.nn.Module):       <span class="aframe-location"/> #4
    def __init__(self, num_labels):
        super(CustomLinearClassifier, self).__init__()
        self.num_labels = num_labels
        self.dropout = torch.nn.Dropout(0.1)
        self.ff = torch.nn.Linear(4096, num_labels, dtype=torch.float16)

    def forward(self, input_ids=None, targets=None):
    sequence = self.dropout(input_ids)

    logits = self.ff(sequence[:, 0, :].view(-1, 4096))

    if targets is not None:
            loss = torch.nn.functional.cross_entropy(
                logits.view(-1, self.num_labels), targets.view(-1)
            )
            return logits, loss
    else:
            return logits


custom_LMHead = CustomLinearClassifier(128256).to(device)

with torch.no_grad():
    start = perf_counter()
    outputs = custom_LMHead(embeddings_to_cache)
    end = perf_counter() - start
    print(f"Outputs: {outputs}")
    print(f"\nTime to execute: {end}\n")</pre>
<div class="code-annotations-overlay-container">
     #1 Traditional generation
     <br/>#2 Embedding
     <br/>#3 Finds the LM Head layer
     <br/>#4 Custom Trainable classifier
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p124">
<p>This decoupling of the idea of models as monoliths that connect to other systems is very engineering-friendly, allowing for one model to output hundreds of classifications around a single data point, thanks to embeddings. LangChain provides a <code>CacheBackedEmbeddings</code> class to help with caching the vectors quickly and conveniently if you’re working within that class, and we think that name is pretty great for the larger idea as well—backing up your embedding process with caching to be fed to multiple linear classifiers at once. This approach allows us to detect anything from inappropriate user input all the way to providing a summarized version of the embeddings back to the real model for quicker and more generalized processing.</p>
</div>
<div class="readable-text" id="p125">
<h2 class="readable-text-h2" id="sigil_toc_id_191"><span class="num-string">12.3</span> Final thoughts</h2>
</div>
<div class="readable-text" id="p126">
<p>We really hope you enjoyed this book and that you learned something new and useful. It’s been a huge undertaking to write the highest quality book we could muster, and sometimes it was less about what we wrote and more about what we ended up throwing out. Believe it or not, while being as comprehensive as we could, there are many times we’ve felt we’d only scratched the surface of most topics. Thank you for going on this journey with us.</p>
</div>
<div class="readable-text intended-text" id="p127">
<p>We are so excited about where this industry is going. One of the hardest parts of writing this book was choosing to focus on the current best practices and ignoring much of the promising research that seems to be piling on, especially as companies and governments increase funding into the incredible possibilities that LLMs promise. We’re excited to see more research that’s been around for years or even decades be applied to LLMs and see new research come from improving those results. We’re also excited to watch companies change and figure out how to deploy and serve LLMs much better than they currently are. It’s difficult to market LLM-based products using traditional methods without coming off as just lying. People want to see the product work exactly as demonstrated in the ad, and we’re hoping to see changes there.</p>
</div>
<div class="readable-text intended-text" id="p128">
<p>What an exciting time! There’s still so much more to learn and explore. Because we have already seen the industry move while we’ve been writing, we’d like to invite you to submit PRs in the GitHub repo to help keep the code and listings up to date for any new readers. While this is the end of the book, we hope it’s just the beginning of your journey into using LLMs.</p>
</div>
<div class="readable-text" id="p129">
<h2 class="readable-text-h2" id="sigil_toc_id_192">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p130"> LLMs are quickly challenging current laws and regulations and the interpretations thereof. </li>
<li class="readable-text" id="p131"> The fear of LLMs being used for cheating has hurt many students with the introduction of AI detection systems that don’t work. </li>
<li class="readable-text" id="p132"> LLMs are only getting bigger, and we will need solutions like better compression and the next attention algorithm to compensate. </li>
<li class="readable-text" id="p133"> Embeddings are paving the way to multimodal solutions with interesting approaches like ImageBind and OneLLM. </li>
<li class="readable-text" id="p134"> Data is likely to be one of the largest bottlenecks and constraints to future improvements, largely starting with a lack of quality evaluation datasets. </li>
<li class="readable-text" id="p135"> For use cases where they are a problem, hallucinations will continue to be so, but methodologies to curb their effects and frequency of occurrence are becoming quite sophisticated. </li>
<li class="readable-text" id="p136"> LLMs continue to suffer due to GPU shortages and will help drive research and innovation to develop more powerful computing systems. </li>
<li class="readable-text" id="p137"> LLM Agents don’t provide a pathway to AGI, but we will see them graduate from toys to tools. </li>
</ul>
<div class="readable-text footnote-readable-text" id="p138">
<p><a href="#footnote-source-1"><span class="footnote-definition" id="footnote-97">[1]</span></a> M. M. Grynbaum and R. Mac, “The Times Sues OpenAI and Microsoft Over A.I. Use of Copyrighted Work,” The New York Times, December 27, 2023, <a href="https://mng.bz/6Y0D">https://mng.bz/6Y0D</a>. </p>
</div>
<div class="readable-text footnote-readable-text" id="p139">
<p><a href="#footnote-source-2"><span class="footnote-definition" id="footnote-98">[2]</span></a> E. Maiberg, “Scientific journals are publishing papers with AI-generated text,” 404 Media, March 18, 2024, <a href="https://mng.bz/n0og">https://mng.bz/n0og</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p140">
<p><a href="#footnote-source-3"><span class="footnote-definition" id="footnote-99">[3]</span></a> Sanhedrin 76b:11, <a href="https://mng.bz/vJaJ">https://mng.bz/vJaJ</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p141">
<p><a href="#footnote-source-4"><span class="footnote-definition" id="footnote-100">[4]</span></a> “Myanmar army behind Facebook pages spewing hate speech: UN probe,” RFI, March 27, 2024, <a href="https://mng.bz/mR0P">https://mng.bz/mR0P</a>. </p>
</div>
<div class="readable-text footnote-readable-text" id="p142">
<p><a href="#footnote-source-5"><span class="footnote-definition" id="footnote-101">[5]</span></a> A. Guzman, “Company disables AI after bot starts swearing at customer, calls itself the ‘worst delivery firm in the world,’” NY Post, January 20, 2024, <a href="https://mng.bz/yoVq">https://mng.bz/yoVq</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p143">
<p><a href="#footnote-source-6"><span class="footnote-definition" id="footnote-102">[6]</span></a> emozilla, “Dynamically Scaled RoPE further increases performance of long context LLaMA with zero fine-tuning,” Jun. 30, 2023, <a href="https://mng.bz/M1pn">https://mng.bz/M1pn</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p144">
<p><a href="#footnote-source-7"><span class="footnote-definition" id="footnote-103">[7]</span></a> B. Peng, J. Quesnelle, H. Fan, E. Shippole, N. Research, and Eleutherai, “YaRN: Efficient Context Window Extension of Large Language Models.” Available: <a href="https://arxiv.org/pdf/2309.00071">https://arxiv.org/pdf/2309.00071</a> </p>
</div>
<div class="readable-text footnote-readable-text" id="p145">
<p><a href="#footnote-source-8"><span class="footnote-definition" id="footnote-104">[8]</span></a> M. Poli et al., “Hyena Hierarchy: Towards Larger Convolutional Language Models,” Feb. 2023, doi: <a href="https://doi.org/10.48550/arxiv.2302.10866">https://doi.org/10.48550/arxiv.2302.10866</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p146">
<p><a href="#footnote-source-9"><span class="footnote-definition" id="footnote-105">[9]</span></a> A. Gu and T. Dao, “Mamba: Linear-Time Sequence Modeling with Selective State Spaces,” arXiv.org, Dec. 01, 2023, <a href="https://arxiv.org/abs/2312.00752">https://arxiv.org/abs/2312.00752</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p147">
<p><a href="#footnote-source-10"><span class="footnote-definition" id="footnote-106">[10]</span></a> [1]O. Lieber et al., “Jamba: A Hybrid Transformer-Mamba Language Model,” arXiv.org, Mar. 28, 2024, <a href="https://arxiv.org/abs/2403.19887">https://arxiv.org/abs/2403.19887</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p148">
<p><a href="#footnote-source-11"><span class="footnote-definition" id="footnote-107">[11]</span></a> Vaswani et al., Attention Is All You Need,” 2017, <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p149">
<p><a href="#footnote-source-12"><span class="footnote-definition" id="footnote-108">[12]</span></a> K. H. and A. Fitch, “Sam Altman seeks trillions of dollars to reshape business of chips and AI,” Wall Street Journal, February 8, 2024, <a href="https://mng.bz/KDrK">https://mng.bz/KDrK</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p150">
<p><a href="#footnote-source-13"><span class="footnote-definition" id="footnote-109">[13]</span></a> A. Pequeño IV, “Nvidia now world’s most valuable company—Topping Microsoft and Apple,” Forbes, June 18, 2024, <a href="https://mng.bz/9ojl">https://mng.bz/9ojl</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p151">
<p><a href="#footnote-source-14"><span class="footnote-definition" id="footnote-110">[14]</span></a> S. Webster, “Intel Optane SSD DC P5800X review: The fastest SSD ever made,” Tom’s Hardware, August 26, 2022, <a href="https://mng.bz/j0Wx">https://mng.bz/j0Wx</a>.</p>
</div>
</div></body></html>