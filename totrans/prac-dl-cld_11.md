# 第十一章。在 iOS 上使用 Core ML 进行实时对象分类

到目前为止，我们已经看到我们的深度学习模型在桌面、云端和浏览器上运行。尽管这种设置有明显的优势，但并不适用于所有情况。在本章中，我们将探讨在移动设备上使用深度学习模型进行预测。

将计算带到用户设备附近，而不是远程服务器，对于许多原因都是有利的：

延迟和互动性

发送图像，将其在云端处理，然后返回结果可能需要几秒钟，具体取决于网络质量和传输数据的数量。这可能导致糟糕的用户体验。几十年的用户体验研究，包括 Jakob Nielsen 在 1993 年发表在他的书《可用性工程》（Elsevier）中的发现，显示如下：

+   0.1 秒是让用户感觉系统反应即时的极限。

+   1 秒是用户思维流畅保持不间断的极限。

+   10 秒是保持用户注意力集中的极限。

大约两十年后，谷歌发布了一项研究结果，发现如果移动浏览器加载一个网页超过三秒，一半的用户会放弃。甚至忘记三秒，延迟增加 100 毫秒都会导致亚马逊销售额下降 1%。这是很多损失的收入。通过立即在设备上处理来减轻这种情况可以实现丰富和互动的用户体验。实时运行深度学习模型，就像 Snapchat Lenses 所做的那样，可以增加与用户的互动。

全天候可用性和降低云成本

显然，向云端发送的数据越少，对于开发者来说意味着更少的计算成本，从而节省金钱。当应用程序获得关注并发展成大量用户群时，这也减少了扩展成本。对于用户来说，在边缘计算上进行计算也很有帮助，因为他们不需要担心数据计划成本。此外，本地处理意味着全天候可用性，无需担心失去连接的恐惧。

隐私

对于用户来说，本地计算通过不外部共享数据来保护隐私，这些数据可能被挖掘用于用户信息。对于开发者来说，这确保了处理个人可识别信息（PII）时更少的麻烦。随着欧盟的《通用数据保护条例》（GDPR）和其他全球用户数据保护法律的出台，这变得更加重要。

希望这些论点能够说明为什么移动设备上的人工智能很重要。对于构建任何严肃应用程序的人来说，在开发过程中考虑以下几个常见问题：

+   如何将我的模型转换为在智能手机上运行？

+   我的模型是否能在其他平台上运行？

+   如何快速运行我的模型？

+   如何最小化应用程序的大小？

+   如何确保我的应用程序不会耗尽电池？

+   如何在不经历大约两天的应用审核过程的情况下更新我的模型？

+   如何进行 A/B 测试我的模型？

+   我能在设备上训练模型吗？

+   如何保护我的知识产权（即模型）不被盗窃？

在接下来的三章中，我们将探讨如何使用不同的框架在智能手机上运行深度学习算法。在这个过程中，我们会回答这些问题。

在本章中，我们深入探讨了 iOS 设备的移动人工智能世界。我们首先看一下一般的端到端软件生命周期（在图 11-1 中显示），看看不同部分如何相互配合。我们探索 Core ML 生态系统，其历史和提供的功能。接下来，我们在 iOS 设备上部署一个实时对象分类应用程序，并学习性能优化和基准测试。最后，我们分析了一些基于 Core ML 构建的真实应用程序。

是时候看看全局了。

# 移动设备上人工智能的开发生命周期

图 11-1 描述了移动设备上 AI 的典型生命周期。

![移动 AI 开发生命周期](img/00199.jpeg)

###### 图 11-1\. 移动 AI 开发生命周期

让我们更仔细地看一下图 11-1 中的阶段：

1.  *收集数据*: 我们收集的数据应反映应用程序的使用环境。由智能手机摄像头拍摄的真实用户照片往往比专业摄影师拍摄的照片更好作为训练示例。我们可能在第一天没有这些数据，但随着使用量的增长，我们可以逐渐收集更多数据。在许多情况下，一个很好的起点是从搜索引擎下载图像。

1.  *标记数据*: 我们需要与我们希望我们的模型预测的数据样本相关联的标签。高质量（即正确）的标签对于一个好的模型至关重要。

1.  *训练模型*: 我们利用迄今为止拥有的数据和相关标签构建最高准确度的神经网络。

1.  *转换模型*: 将模型从训练框架导出到移动兼容的框架。

1.  *优化性能*: 由于移动设备资源受限，对于内存、能源和处理器的使用效率至关重要。

1.  *部署*: 将模型添加到应用程序中并将其交付给用户。

1.  *监控*: 跟踪应用在现实世界中的使用情况，找到进一步改进的机会。此外，收集愿意同意的用户的真实数据样本，以供这个生命周期使用，然后回到第一步。

在本书的前部分，我们主要探讨了阶段 1、2 和 3，以及一般性能改进。在本章中，我们将重点放在阶段 4、5 和 6 上。在接下来的几章中，我们将探讨所有这些阶段在移动开发的背景下的应用。

###### 提示

在将应用交到真实用户手中之前（如果应用表现不如他们期望的那样，他们可能会讨厌它），通过一种称为“dogfooding”的过程收集反馈是常见做法。俗话说：吃自己的狗粮。这个过程涉及拥有一群忠实用户的内部圈子，他们可以测试早期版本，从而在发布给公众之前识别错误。对于 AI 开发，这个内部圈子也可能贡献数据，并评估 AI 模型在现实世界中的成功。随着模型的改进，它们可以逐渐部署到越来越多的测试用户，最终部署到公众。

让我们开始吧！

# Core ML 简史

Core ML 为在苹果设备（如 iPhone 和 iPad 以及 MacBook、Apple TV 和 Apple Watch）上运行深度神经网络提供了一种最简单的方式。除了易于使用外，它还针对底层硬件架构进行了优化。在过去几年中，替代框架已经变得更好了，但很难超越 Core ML 提供的简单性和性能。

传统上，要在苹果设备上快速运行 CNN，开发人员*需要*使用 Metal 编写，这是一个为游戏开发人员提供的库，用于更好地利用 GPU。不幸的是，使用 Metal 进行开发就像是在汇编语言或 CUDA 代码中编写一样。这是繁琐、容易出错且难以调试的。很少有开发人员敢于踏上这条道路。Amund Tveit 于 2015 年 12 月推出的 DeepLearningKit 是为部署 CNN 构建一个对 Metal 的抽象的努力。

在 2016 年的苹果全球开发者大会（WWDC）上，该公司宣布了 Metal Performance Shaders（MPS），这是建立在 Metal 之上的一个高性能库，用于优化图形和某些计算操作。它抽象了许多底层细节，提供了基本构建模块，如卷积、池化和 ReLU。它允许开发人员通过在代码中组合这些操作来编写深度神经网络。对于来自 Keras 世界的任何人来说，这是一个熟悉而不那么令人畏惧的任务。不幸的是，在编写 MPS 代码时涉及了大量的繁琐工作，因为您需要在每个步骤中手动跟踪输入和输出维度。例如，苹果发布的用于识别 1,000 个对象类别的 InceptionV3 模型的代码示例超过 2,000 行，其中大部分定义了网络。现在想象一下在训练过程中稍微更改模型，然后不得不查看所有 2,000 行代码以在 iOS 代码中反映相同的更新。Forge（2017 年 4 月），由 Matthijs Hollemans 开发的一个库，是为了简化 MPS 开发，减少启动模型所需的样板代码。

当苹果在 2017 年的 WWDC 上宣布 Core ML 时，所有这些困难都消失了。这包括 iOS 上的推理引擎和一个名为 Core ML Tools 的开源 Python 包，用于序列化来自 Keras 和 Caffe 等其他框架的 CNN 模型。构建应用程序的一般工作流程是：在其他软件包中训练模型，将其转换为*.mlmodel*文件，并部署在运行在 Core ML 平台上的 iOS 应用程序中。

Core ML 支持导入使用第一方和第三方框架和文件格式构建的广泛范围的机器学习模型。图 11-2 展示了其中一些（顺时针，从左上角开始），如 TensorFlow、Keras、ONNX、scikit-learn、Caffe2、苹果的 Create ML、LIBSVM 和 TuriCreate（也来自苹果）。ONNX 本身支持各种框架，包括 PyTorch（Facebook）、MXNet（亚马逊）、Cognitive Toolkit（微软）、PaddlePaddle（百度）等，从而确保与天下任何主要框架的兼容性。

![截至 2019 年与 Core ML 兼容的框架](img/00181.jpeg)

###### 图 11-2。截至 2019 年与 Core ML 兼容的框架

# Core ML 的替代方案

根据平台的不同，有几种选项可以实现实时预测。这些包括通用推理框架，如 Core ML（来自苹果）、TensorFlow Lite（来自谷歌）、ML Kit（也来自谷歌）和 Fritz，以及特定芯片加速器框架，包括骁龙神经处理引擎（来自高通）和华为 AI 移动计算平台（用于华为的神经处理单元）。表 11-1 提供了所有这些框架的高级比较。

表 11-1。移动设备 AI 框架的比较

| **框架** | **适用于 iOS** | **适用于 Android** | **动态更新** | **A/B 测试** | **设备上的训练** | **模型加密** |
| --- | --- | --- | --- | --- | --- | --- |
| Core ML | ✓ | — | ✓ | — | ✓ | — |
| TensorFlow Lite | ✓ | ✓ | — | — | 2019 年底发布 | — |
| ML Kit | ✓ | ✓ | ✓ | ✓ | — | — |
| Fritz | ✓ | ✓ | ✓ | ✓ | — | ✓ |

## TensorFlow Lite

2017 年 11 月，谷歌宣布推出了一个名为 TensorFlow Lite 的设备上推理引擎，旨在将 TensorFlow 生态系统扩展到服务器和个人电脑之外。在此之前，TensorFlow 生态系统内的选项包括将整个 TensorFlow 库移植到 iOS（这很沉重且缓慢），以及稍后推出的其略微简化版本称为 TensorFlow Mobile（仍然相当庞大）。

TensorFlow Lite 从头开始重建，针对移动和边缘设备进行优化，优化速度、模型和解释器大小以及功耗。它增加了对 GPU 后端代理的支持，这意味着只要为硬件平台实现了 GPU 支持，TensorFlow Lite 就可以利用 GPU 的强大功能。在 iOS 上，GPU 代理使用 Metal 进行加速。我们在第十三章中更详细地讨论了 TensorFlow Lite。

## ML Kit

ML Kit 是谷歌提供的一个高级库，提供了许多计算机视觉、自然语言处理和人工智能功能，包括运行 TensorFlow Lite 模型的能力。一些功能包括人脸检测、条形码扫描、智能回复、设备上的翻译和语言识别。然而，ML Kit 的主要卖点是与 Google Firebase 的集成。Firebase 提供的功能包括动态模型更新、A/B 测试和基于远程配置驱动的动态模型选择（根据客户选择使用哪个模型的花哨词）。我们在第十三章中更详细地探讨了 ML Kit。

## Fritz

Fritz 是一家创立的初创公司，旨在使移动推断的端到端过程更加简单。它通过提供易于使用的命令行工具，弥合了机器学习从业者和移动工程师之间的鸿沟。一方面，它将 Keras 中的训练直接集成到部署流水线中，因此机器学习工程师可以在训练完成后添加一行 Keras 回调即可立即将模型部署给用户。另一方面，移动工程师可以在不需要部署到物理设备的情况下对模型进行基准测试，通过虚拟模拟模型的性能，评估 Keras 模型与 Core ML 的兼容性，并为每个模型获取分析数据。Fritz 的一个独特卖点是模型保护功能，通过混淆防止模型在手机越狱时进行深度检查。

# 苹果的机器学习架构

为了更好地了解 Core ML 生态系统，有必要看到苹果提供的所有不同 API 以及它们如何相互配合。图 11-3 让我们看到了构成苹果机器学习架构的不同组件。

![苹果为应用开发人员提供的不同级别的 API](img/00240.jpeg)

###### 图 11-3\. 苹果为应用开发人员提供的不同级别的 API

## 基于领域的框架

为了简化机器学习中的常见任务，不需要领域专业知识，苹果提供了许多开箱即用的 API，涵盖视觉、自然语言、语音和声音分析等领域。表 11-2 详细概述了苹果操作系统上可用的功能。

表 11-2\. 苹果操作系统中的开箱即用机器学习功能

| **视觉** | **自然语言** | **其他** |
| --- | --- | --- |

|

+   面部特征点检测

+   图像相似度

+   显著性检测

+   光学字符识别

+   矩形检测

+   人脸检测

+   对象分类

+   条形码检测

+   地平线检测

+   人类和动物检测

+   对象跟踪（用于视频）

|

+   标记化

+   语言识别

+   词性识别

+   文本嵌入

|

+   语音识别（设备上和云端）

+   声音分类

|

## ML 框架

Core ML 具有在深度学习和机器学习模型上运行推断的能力。

## ML 性能基元

以下是苹果堆栈中的一些机器学习基元：

MPS

提供低级和高性能的基元，利用 GPU 来快速运行大多数基于 CNN 的网络。如果 Core ML 不支持某个模型，MPS 提供了所有构建块，使我们能够构建它们。此外，我们可能考虑使用 MPS 手动创建一个模型以提高性能（例如保证模型在 GPU 上运行）。

加速和基本神经网络子程序

加速是苹果对基本线性代数子程序（BLAS）库的实现。它提供了用于高性能大规模数学计算和图像计算的功能，如基本神经网络子程序（BNNS），有助于实现和运行神经网络。

既然我们已经看到了 Core ML 和特定领域 API 如何融入整体架构，让我们看看在 iOS 应用程序上运行机器学习模型所需的工作量是多么少。

###### 提示

苹果提供了几个可下载的模型（图 11-4），用于各种计算机视觉任务，从分类到检测物体（带有边界框），分割（识别像素），深度估计等。您可以在[*https://developer.apple.com/machine-learning/models/*](https://developer.apple.com/machine-learning/models)找到它们。

对于分类，您可以在苹果的机器学习网站上找到许多预训练的 Core ML 模型，包括 MobileNet、SqueezeNet、ResNet-50 和 VGG16。

![苹果机器学习网站上的现成模型](img/00074.jpeg)

###### 图 11-4\. 苹果机器学习网站上的现成模型

# 构建实时物体识别应用程序

虽然我们不打算教授 iOS 开发，但我们想演示在 iOS 设备上实时运行一个能够在 1,000 个 ImageNet 类别中进行分类的物体识别模型。

我们将检查运行此应用程序所需的最少代码量。一般的概述如下：

1.  将*.mlmodel*文件拖放到 Xcode 项目中。

1.  将模型加载到 Vision 容器（`VNCoreMLModel`）中。

1.  基于该容器创建一个请求（`VNCoreMLRequest`），并提供一个在请求完成时将被调用的函数。

1.  创建一个请求处理程序，可以根据提供的图像处理请求。

1.  执行请求并打印结果。

让我们看看如何编写代码来实现这一点：

```py
import CoreML
import Vision

// load the model
let model = try? VNCoreMLModel(for: Resnet50().model)!
// create a request with a callback
let classificationRequest = VNCoreMLRequest(model: model) { 
    (request, error) in
    // print the results once the request is complete
    if let observations = request.results as? [VNClassificationObservation] {
        let results = observations
                      .map{"\($0.identifier) - \($0.confidence)"}
                      .joined(separator: "\n")
        print(results)
    }
}
// create a request handler taking an image as an argument
let requestHandler = VNImageRequestHandler(cgImage: cgImage)
// execute the request
try? requestHandler.perform([classificationRequest])
```

在这段代码中，`cgImage`可以是来自各种来源的图像。它可以是来自照片库或网络的照片。

我们还可以使用摄像头实时场景，并将单个摄像头帧传递到此函数中。普通的 iPhone 摄像头可以每秒拍摄高达 60 帧。

默认情况下，Core ML 沿着较长的边裁剪图像。换句话说，如果模型需要方形尺寸，Core ML 将提取图像中心的最大方形。这可能会让开发人员感到困惑，因为他们发现图像的顶部和底部条被忽略以进行预测。根据情况，我们可能希望使用`.centerCrop`、`.scaleFit`或`.scaleFill`选项，如图 11-5 所示：

```py
classificationRequest.imageCropAndScaleOption = .scaleFill
```

![不同缩放选项如何修改输入图像到 Core ML 模型](img/00054.jpeg)

###### 图 11-5\. 不同缩放选项如何修改输入图像到 Core ML 模型

既然我们已经深入了解了这个主题，还有什么比这更有趣的呢？怎么样在手机上实际运行它！我们在本书的 GitHub 网站上提供了一个完整的应用程序（请参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）位于*code/chapter-11*。通过 iPhone 或 iPad，我们可以相当快速地部署它并进行实验，即使没有 iOS 开发知识也可以。以下是步骤（注意：这需要 Mac）：

1.  从苹果开发者网站或 Mac App Store 下载 Xcode。

1.  插入 iOS 设备。手机在部署过程中需要保持解锁状态。

1.  将当前工作目录更改为`CameraApp`：

    ```py
    $ cd code/chapter-11/CameraApp
    ```

1.  从苹果网站下载 Core ML 模型，使用提供的 Bash 脚本：

    ```py
    $ ./download-coreml-models.sh
    ```

1.  打开 Xcode 项目：

    ```py
    $ open CameraApp.xcodeproj
    ```

1.  在项目层次结构导航器中，点击左上角的 CameraApp 项目，如图 11-6 所示，打开项目信息视图。

    ![Xcode 中的项目信息视图](img/00228.jpeg)

    ###### 图 11-6. Xcode 中的项目信息视图

1.  因为 Xcode 保留了唯一的包标识符，所以使用一个唯一的名称来标识项目。

1.  登录到苹果账户，让 Xcode 签署应用程序并将其部署到设备上。选择一个团队进行签署，如图 11-7 所示。

    ![选择一个团队，让 Xcode 自动管理代码签名](img/00281.jpeg)

    ###### 图 11-7. 选择一个团队，让 Xcode 自动管理代码签名

1.  点击“构建和运行”按钮（右向三角形）将应用程序部署到设备上，如图 11-8 所示。这通常需要大约 30 到 60 秒。

    ![选择设备，点击“构建和运行”按钮来部署应用程序](img/00261.jpeg)

    ###### 图 11-8. 选择设备，点击“构建和运行”按钮来部署应用程序

1.  设备不会立即运行应用程序，因为它没有受信任。前往设置 > 通用 > 配置文件与设备管理，选择包含您信息的行，然后点击“信任{your_email_id}”，如图 11-9 所示。

    ![配置文件和设备管理屏幕](img/00187.jpeg)

    ###### 图 11-9. 配置文件和设备管理屏幕

1.  在主屏幕上找到 CameraApp 并运行应用程序。

    ![应用程序的屏幕截图](img/00167.jpeg)

    ###### 图 11-10. 应用程序的屏幕截图

    输出显示“笔记本电脑，笔记本电脑”预测的置信度为 84%，“笔记本电脑，笔记本电脑”预测的置信度为 11%。

这一切都很有趣。现在让我们转向更严肃的业务：将不同框架的模型转换为 Core ML 模型。

###### 提示

Xcode 的一个很棒之处是当我们将*.mlmodel*文件加载到 Xcode 中时，会显示模型的输入和输出参数，如图 11-11 所示。当我们没有自己训练模型并且不想编写代码（比如在 Keras 中的`model.summary()`）来探索模型架构时，这是特别有帮助的。

![Xcode 模型检查器显示 MobileNetV2 模型的输入和输出](img/00145.jpeg)

###### 图 11-11. Xcode 模型检查器显示 MobileNetV2 模型的输入和输出

# 转换为 Core ML

在我们构建的代码示例中，您看到了*Inceptionv3.mlmodel*文件。您是否想知道这个文件是如何生成的？毕竟，Inception 是由谷歌使用 TensorFlow 训练的。该文件是从*.pb*文件转换为 Core ML 模型的。我们可能也有需要将模型从 Keras、Caffe 或任何其他框架转换为 Core ML。以下是一些工具，可以实现模型转换为 Core ML。

+   Core ML 工具（苹果）：从 Keras（*.h5*）、Caffe（*.caffemodel*）以及 LIBSVM、scikit-learn 和 XGBoost 等机器学习库。

+   tf-coreml（谷歌）：从 TensorFlow（*.pb*）。

+   onnx-coreml（ONNX）：从 ONNX（*.onnx*）。

我们详细查看前两个转换器。

## 从 Keras 转换

Core ML 工具有助于将 Keras、ONNX 和其他模型格式转换为 Core ML 格式（*.mlmodel*）。使用`pip`安装`coremltools`框架：

```py
$ pip install --upgrade coremltools
```

现在，让我们看看如何使用现有的 Keras 模型并将其转换为 Core ML。转换只需一行命令，然后我们可以保存转换后的模型，如下所示：

```py
from tensorflow.keras.applications.resnet50 import ResNet50
model = ResNet50()

import coremltools
coreml_model = coremltools.converters.keras.convert(model)
coreml_model.save("resnet50.mlmodel")
```

就是这样！它再简单不过了。我们将讨论如何转换具有不受支持层（如 MobileNet）的模型，详见第十二章。

## 从 TensorFlow 进行转换

苹果建议使用`tf-coreml`（来自 Google）将基于 TensorFlow 的模型转换为 Core ML。在以下步骤中，我们将一个预训练的 TensorFlow 模型转换为 Core ML。这个过程比我们之前看到的单行代码要复杂一些。

首先，我们使用`pip`执行`tfcoreml`的安装：

```py
$ pip install tfcoreml --user --upgrade
```

要进行转换，我们需要知道模型的第一层和最后一层。我们可以通过使用模型可视化工具（如[Netron](https://oreil.ly/hJoly)）检查模型架构来确定这一点。在 Netron 中加载 MobileNet 模型（*.pb*文件）后，我们可以在图形界面中可视化整个模型。图 11-12 显示了 MobileNet 模型的一个小部分；特别是输出层。

![在 Netron 中查看的 MobileNet 的输出层](img/00146.jpeg)

###### 图 11-12. 在 Netron 中查看的 MobileNet 的输出层

我们只需要将其作为参数插入以下 Python 代码中并运行：

```py
import tfcoreml as tf_converter
tf_converter.convert(tf_model_path = "input_model.pb",
     mlmodel_path = "output_model.mlmodel",
     output_feature_names = ["MobilenetV1/Predictions/Reshape_1:0"])
```

当脚本运行时，我们会看到每个层的每个操作被转换为相应的 Core ML 等效操作。完成后，我们应该在目录中找到导出的*.mlmodel*。

我们的 Core ML 模型现在已经准备好了。将其拖入 Xcode 进行测试。

# 动态模型部署

作为开发人员，我们可能会希望随着时间的推移不断改进我们的模型。毕竟，我们希望我们的用户尽快获得最新和最好的模型。处理这个问题的一种方法是每次部署新模型时向 App Store 发送更新。这种方法效果不是很好，因为每次我们都需要等待大约两天才能获得苹果的批准，可能会导致显著的延迟。

另一种推荐的方法是让应用程序动态下载*.mlmodel*文件，并在用户设备内编译它。有几个原因我们可能想尝试这种方法：

+   我们希望定期更新模型，而不受我们 App Store 发布节奏的限制。

+   我们希望保持应用程序下载大小较小，并让用户最终仅下载与其使用场景相关的模型。这将减少存储需求和带宽成本。苹果在通过蜂窝网络下载 App Store 的应用程序上设置了 200 MB 的限制，因此保持在该限制以下至关重要，以免错失潜在的下载机会。

+   我们希望在不同用户集合上对不同模型进行 A/B 测试，以进一步提高模型的质量。

+   我们希望为不同的用户段、区域和语言环境使用不同的模型。

实现这一目标的过程相当简单：将*.mlmodel*托管在服务器上，并设计我们的应用程序以动态下载文件。在模型位于用户设备上之后，我们可以在该文件上运行`MLModel.compileModel`来生成模型的编译版本：

```py
let compiledModelUrl = try MLModel.compileModel(at: downloadedModelUrl)
let model = try MLModel(contentsOf: compiledModelUrl)
```

请记住，`compiledModelUrl`是一个临时位置的地址。如果您希望在会话之外将模型保存在设备上更长时间，您必须将其移动到永久存储中。

###### 注意

尽管我们可以直接使用 Core ML 手动进行模型管理，但这仍然涉及大量样板代码的编写，包括后端和设备端。我们需要手动管理每个模型的版本、文件存储和配置基础设施，以及过程中的任何错误。这就是 ML Kit 和 Fritz 真正出色的地方，它们提供了这些功能。我们将在第十三章中更详细地讨论这一点。

# 在设备上训练

到目前为止，我们已经看到了一些适合“一刀切”的神经网络的场景。然而，一些用例如果没有个性化模型将无法正常工作。一个例子是一个根据每张图片中人脸识别来组织用户照片库的应用。鉴于普通用户的手机大多充斥着朋友和家人的照片，一个训练有丹尼·德维托和汤姆·汉克斯脸部的通用模型对用户来说并不是很有用（除非当然他们属于德维托或汉克斯家族）。

一个现实生活中的例子是 iOS 系统键盘，它会随着时间学习用户的语言模式，并开始为该用户提供越来越相关的建议。当用户使用俚语、昵称、领域特定术语等时，这变得更加明显，这些可能不是常见语言词典的一部分。基于这些数据的个性化建议对于其他用户来说是无用的。

在这些情况下，我们希望为该用户收集数据并仅为该个人训练个性化模型。实现这一目标的一种方法是收集并发送数据到云端，在那里训练一个新模型，然后将更新后的模型发送回用户。这种方法存在可扩展性、成本和隐私问题。

相反，Core ML 提供了设备上训练的功能，因此用户的数据永远不需要离开设备。当`isUpdatable`属性设置为`true`时，Core ML 模型是可更新的。此外，需要重新训练的特定层集（通常是网络末端）也需要将相同属性设置为`true`。还可以在模型上设置额外的训练参数，如学习率和优化器。

尽管训练可能会消耗 GPU 和神经处理器单元（NPU；更多信息请参阅第十三章），但训练可以在后台进行调度（使用`BackgroundTasks`框架），即使设备处于空闲和充电状态，通常在晚上。这对用户体验的影响最小。

要进行设备上的训练，我们可以调用`MLUpdateTask`函数，以及我们将用于重新训练模型的新数据。我们还需要在函数调用中传入新更新模型的路径。训练完成后，该路径上的模型就准备就绪了：

```py
let modelUrl = bundle.url(forResource: "MyClassifier",
                          withExtension: "mlmodelc")!
let updatedModelUrl = bundle.url(forResource: "MyClassifierUpdated",
                                 withExtension: "mlmodelc")!

let task = try MLUpdateTask(
    forModelAt: modelUrl,
    trainingData: trainData,
    configuration: nil,
    completionHandler: { [weak self] (updateContext) in
          self.model = updateContext.model
          updateContext.model.write(to: updatedModelUrl)
    })

task.resume()
```

## 联邦学习

设备上的训练很棒，除了一个缺点：通用全局模型没有机会改进。开发人员是否可以以某种方式利用每个用户设备上生成的数据来改进全局模型，而无需从其设备传输数据？这就是*联邦学习*的作用。

联邦学习是一种协作分布式训练过程。它通过将增量更新（来自用户设备的个性化模型）发送到云端，并在整个用户群体中聚合这些更新，从而为每个人丰富全局模型，将设备上的训练推进了一步。请记住，这里没有传输任何用户数据，也不可能从聚合的特征集中逆向工程用户数据。通过这种方法，我们能够尊重用户的隐私，同时通过集体参与使每个人受益。

设备上的训练是联邦学习的一个重要步骤。尽管我们还没有达到这一步，但这是行业发展的方向。我们可以期待随着时间的推移看到对联邦学习的更多支持。

TensorFlow Federated 是联邦学习的一个实现，为谷歌的 GBoard 键盘应用提供训练支持。用户设备上的训练发生在它们正在充电时的后台。

# 性能分析

编写原型是一回事。制作适用于生产的应用程序则完全是另一回事。有几个因素会影响用户体验，了解权衡是很重要的。其中一些因素包括支持的设备型号、最低操作系统（OS）版本、处理帧率以及深度学习模型的选择。在本节中，我们探讨了这些因素对产品质量和性能的影响。

## 在 iPhone 上对模型进行基准测试

与基准测试一样，最好在公开可下载的模型上进行实验，以便我们可以轻松下载并进行实践！

首先，我们在 2013 年至 2018 年生产的多款 iPhone 上运行了我们的实时对象分类应用程序。表 11-3 列出了这些实验的结果。

表 11-3。不同 iPhone 版本上不同型号的基准推理时间

| 设备型号 | iPhone 5s | iPhone 6 | iPhone 6s | iPhone 7+ | iPhone X | iPhone XS | iPhone 11 Pro |
| --- | --- | --- | --- | --- | --- | --- | --- |
| **发布年份** | 2013 | 2014 | 2015 | 2016 | 2017 | 2018 | 2019 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| **RAM** | 1 GB | 1 GB | 2 GB | 2 GB | 2 GB | 4 GB | 4GB |
| --- | --- | --- | --- | --- | --- | --- | --- |
| **处理器芯片** | A7 | A8 | A9 | A10 | A11 | A12 | A13 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| **模型** | **准确率（%）** | **大小（MB）** | **FPS** | **FPS** | **FPS** | **FPS** | **FPS** | **FPS** | **FPS** |
| **VGG-16** | 71 | 553 | 0.1 | 0.2 | 4.2 | 5.5 | 6.9 | 27.8 | 34.5 |
| **InceptionV3** | 78 | 95 | 1.4 | 1.5 | 9 | 11.1 | 12.8 | 35.7 | 41.7 |
| **ResNet-50** | 75 | 103 | 1.9 | 1.7 | 11.6 | 13.5 | 14.1 | 38.5 | 50 |
| **MobileNet** | 71 | 17 | 7.8 | 9 | 19.6 | 28.6 | 28.6 | 55.6 | 71.4 |
| **SqueezeNet** | 57 | 5 | 13.3 | 12.4 | 29.4 | 33.3 | 34.5 | 66.7 | 76.9 |

2014 年和 2015 年推理时间之间的差异尤为显著。2015 年发生了什么？如果你猜到了 GPU，你是对的。iPhone 6S 首次引入了专用 GPU，支持诸如“嘿 Siri”之类的功能。

另一方面，让我们看看 2018 年 9 月 iPhone XS 发布月份在美国 iPhone 中的市场份额，如表 11-4 所示。请注意，发布通常在每年的 9 月份进行。

表 11-4。截至 2018 年 9 月的美国 iPhone 设备市场份额（iPhone XS 发布月份；数据来自[Flurry Analytics](https://oreil.ly/L47c0)，排除 iPhone XS、XS Plus 和 XS Max）

| 发布年份 | iPhone 型号 | 百分比 |
| --- | --- | --- |
| 2017 | 8 Plus | 10.8% |
| 2017 | X | 10.3% |
| 2017 | 8 | 8.1% |
| 2016 | 7 | 15.6% |
| 2016 | 7 Plus | 12.9% |
| 2016 | SE | 4.2% |
| 2015 | 6S | 12.5% |
| 2015 | 6S Plus | 6.1% |
| 2014 | 6 | 10.7% |
| 2014 | 6 Plus | 3.3% |
| 2013 | 5S | 3.4% |
| 2013 | 5C | 0.8% |
| 2012 | 5 | 0.8% |
| 2011 | 4S | 0.4% |
| 2010 | 4 | 0.2% |

从表 11-4 中推导出累积百分比，我们得到了表 11-5。该表根据我们选择支持的最旧设备来表达我们的潜在市场份额。例如，2016 年 9 月之后发布的 iPhone（2018 年 9 月之前的两年）总市场份额为 61.9%。 

表 11-5。iPhone 年度累积市场份额

| 年份 | 累积百分比 |
| --- | --- |
| 1 | 29.2% |
| 2 | 61.9% |
| 3 | 80.5% |
| 4 | 94.5% |
| 5 | 98.7% |
| 6 | 99.5% |

结合基准和市场份额，我们有一些设计选择和优化可供选择：

使用更快的模型

在 iPhone 6 上，VGG-16 的运行速度比 MobileNet 慢大约 40 倍。在 iPhone XS 上，它仍然比 MobileNet 慢大约两倍。仅仅选择一个更高效的模型就可以对性能产生显著影响，通常无需牺牲等量的准确性。值得注意的是，MobileNetV2 和 EfficientNet 提供了更好的速度和准确性的组合。

确定支持的最低 FPS

一个在实时摄像头视频上应用滤镜的功能需要能够处理摄像头每秒传送的大量帧，以确保流畅的体验。相比之下，由于用户发起的操作，一个处理单个图像的应用并不需要太过担心性能。很多应用程序会处于两者之间的某个位置。重要的是确定最低必要的 FPS，并进行类似于表 11-5 的基准测试，以确定跨 iPhone 世代的最佳模型。

批处理

GPU 非常擅长并行处理。因此，将一批数据进行处理比对每个项目进行推断更有效并不奇怪。Core ML 利用这一事实通过暴露批处理 API 来实现。一些用户体验，特别是异步和/或内存密集型的体验，可以从这些批处理 API 中获益。例如，在照片库中进行任何批量处理照片。我们可以将一组图像一起发送到批处理 API，以便 Core ML 可以优化 GPU 上的性能，而不是逐个处理图像。

动态模型选择

根据使用情况，我们可能对高准确度并不像对流畅体验（在某个最低 FPS 阈值下）那样在意。在这些情况下，我们可能会在较慢、较旧的设备上选择一个更轻、准确性较低的模型，而在更快、现代化的设备上选择一个更大、更准确的模型。这可以与通过云端进行模型部署结合使用，这样我们就不会不必要地增加应用的大小。

利用“Sherlocking”来获益

“Sherlocking”是一个术语，用于描述第一方供应商（尤其是苹果）通过内置功能使第三方软件变得过时。一个很好的例子是，当苹果发布了 iPhone 内置的手电筒功能时，所有第三方手电筒应用程序（无论是付费还是免费）都变得过时了。举个例子，假设一个应用在 2017 年发布了一个面部跟踪功能。一年后，苹果在 iOS 12 的 Core ML 中添加了一个更准确的面部跟踪 API。由于面部跟踪的神经网络已经内置在操作系统中，该应用可以更新其代码以使用内置的 API。然而，由于 API 在 iOS 11 中仍然不可用，该应用可以采用混合方法，通过使用旧代码路径保持向后兼容性。应用开发人员可以停止将神经网络捆绑到应用中，从而减小其大小，可能减少几兆字节，并动态下载旧模型以供 iOS 11 用户使用。

优雅降级

有时，由于性能需求，较旧的手机可能无法处理较新的深度学习模型。这是一个完全合理的情况，特别是如果该功能是尖端的。在这些情况下，可以采用两种方法之一。第一种是将计算外包到云端。这显然会牺牲互动性和隐私。另一种选择是为这些用户禁用该功能，并向他们显示为什么该功能不可用的消息。

# 测量能源影响

在之前的章节中，我们专注于在服务器上托管分类器。尽管有许多因素会影响设计决策，但能量消耗通常不在其中。然而，在客户端，电池电量往往是有限的，最小化其消耗成为首要任务。用户对产品的感知很大程度上取决于其消耗的能量。还记得以前 GPS 是电池杀手的时代吗？许多需要位置访问的应用程序因为消耗太多电池电量而获得了大量一星评价。我们绝对不希望出现这种情况。

###### 注意

当用户意识到应用程序比预期消耗更多电池时，他们通常会非常慷慨地给出差评，正如图 11-13 中所示的评论所示。

![App Store 中关于 YouTube 和 Snapchat 的评论，抱怨电池消耗过大](img/00102.jpeg)

###### 图 11-13\. App Store 中关于 YouTube 和 Snapchat 的评论，抱怨电池消耗过大

在这里，我们利用 Xcode 的调试导航器中的能量影响选项卡（图 11-14），并生成图 11-15。

![Xcode 调试导航器选项卡](img/00063.jpeg)

###### 图 11-14\. Xcode 调试导航器选项卡

![Xcode 2017 年 iPad Pro 上的能量影响图表（注意：此截图是在不同时间拍摄的，所以数字略有不同）](img/00188.jpeg)

###### 图 11-15\. Xcode 2017 年 iPad Pro 上的能量影响图表（注意：此截图是在不同时间拍摄的，所以数字略有不同）

图 11-15 显示了在 2017 年 iPad Pro 上运行该过程的能量影响很大。其中一个重要原因是，在我们的示例代码中，我们正在处理从摄像头接收到的每一帧。这意味着摄像头捕获的每一帧都会被发送到 GPU 进行处理，导致能量消耗增加。在许多实际应用中，并不需要对每一帧进行分类。即使处理每隔一帧，也可以实现显著的能量节省，而不会对用户体验产生一般性影响。在接下来的部分中，我们将探讨帧处理速率与能量影响之间的关系。

###### 注意

CPU 与 GPU 使用率的比例受模型架构影响，具体取决于卷积操作的数量。在这里，我们对不同模型的平均 CPU 和 GPU 利用率进行了分析（图 11-16）。这些数字可以从 Xcode 的能量影响图表中提取出来。请注意，我们通常更喜欢 GPU 利用率更高的模型以提高性能效率。

![比较 iOS 11 上不同模型的 CPU 和 GPU 利用率](img/00310.jpeg)

###### 图 11-16\. 比较 iOS 11 上不同模型的 CPU 和 GPU 利用率

## 基准负载

正如您所期望的那样，在每帧实时运行 CNN 模型会导致高 GPU/CPU 利用率，从而迅速耗尽电池。用户可能还会注意到手机发热。

与其在每帧上运行分析，不如我们跳过几帧？MobileNet 在 iPad Pro 2017 上分析一帧需要 20 毫秒。因此，它每秒分类约 50 帧。相反，如果我们以 1FPS 运行它，GPU 利用率从 42%降低到仅 7%——降低了超过 83%！对于许多应用程序来说，以 1FPS 处理可能已经足够，同时仍保持设备冷却。例如，一个安全摄像头即使每隔几秒处理一帧，也可能表现得相当不错。

通过调整每秒分析的帧数并测量 GPU 利用率百分比，我们观察到了一个有趣的趋势。从图 11-17 中的图表可以明显看出，FPS 越高，GPU 利用率越高。这对能源消耗有显著影响。对于需要长时间运行的应用程序，减少每秒推断次数可能是有益的。

![调整 FPS 并分析 2017 年 iPad Pro 上的负载](img/00269.jpeg)

###### 图 11-17。调整 FPS 并分析 2017 年 iPad Pro 上的负载

图中显示的值是从 Xcode 仪器的核心动画仪器中得出的。以下是我们用来生成结果的过程：

1.  从 Xcode 中，点击产品，然后选择配置文件。

1.  仪器窗口出现后，选择核心动画仪器，如图 11-18 所示。

    ![Xcode 仪器中的仪器窗口](img/00227.jpeg)

    ###### 图 11-18。Xcode 仪器中的仪器窗口

1.  按下录制按钮以开始在分析模式下运行应用程序。

1.  等待几秒钟开始收集仪器数据。

1.  测量 GPU 硬件利用率列中的值（图 11-19）。

    ![在核心动画工具中实时分析的应用程序](img/00192.jpeg)

    ###### 图 11-19。在核心动画工具中实时分析的应用程序

到目前为止，在本章中，我们已经探讨了使用 Core ML 制作生产级应用程序的技术。在下一节中，我们将讨论现实世界中这类应用程序的示例。

# 减小应用程序大小

应用程序大小对于某些市场的开发人员至关重要。苹果不允许通过蜂窝网络下载大于 200MB 的应用程序。这对于那些经常在外出时使用的应用程序（如 Uber 和 Lyft 等打车应用程序）来说是一个关键的争议点。有趣的一点是：Uber 必须进行一些非常严格的优化，例如大幅减少 Swift 可选项、结构体和协议的数量（否则有助于代码可维护性），以使应用程序二进制大小低于 App Store 限制（当时为 150MB）。如果不这样做，公司将失去很多新用户。

任何我们在应用程序中引入的新 AI 功能都会导致存储使用量增加是理所当然的。我们可以采用一些策略来减少这种影响。

###### 注意

总部位于旧金山的数据分析公司 Segment 想要确定应用程序大小对安装数量的影响有多大。为了进行实验，该公司购买了一个按揭计算器应用程序，该应用程序的应用下载量保持稳定（每天约 50 次）。他们拿到了一个原始大小为 3MB 的应用程序，然后反复地用泰勒·斯威夫特专辑的图片膨胀它（嘿，这是为了科学和研究！）。工程师们观察到，随着公司增加应用程序大小，每日安装量显著下降。当应用程序超过 100MB（当时 App Store 通过蜂窝网络下载的最大限制）时，每日安装量下降了惊人的 44％！此外，该应用程序吸引了一些负面评论，用户对应用程序大小表示怀疑。

这个故事的寓意是，应用程序大小比我们想象的更为重要，我们在发布应用程序之前应该注意我们的应用程序消耗的空间量。

## 避免捆绑模型

如果可能的话，我们应该避免将模型与 App Store 二进制文件捆绑在一起。无论如何都需要下载相同数量的数据，所以只要用户体验不受影响，我们应该在实际使用功能时延迟模型下载。此外，我们应该在 WiFi 环境下下载模型，以保留蜂窝带宽。Microsoft Translator 和 Google Translate 实现了一种形式，它们一开始只进行基于云的翻译。知道旅行者经常使用这些应用（可能没有良好的互联网接入），它们还提供了离线模式，在这种模式下，所需的语言模型会在用户提示的情况下在后台下载。

## 使用量化

正如我们在 第六章 中讨论的，量化是一种很好的策略，可以大幅减小模型大小，同时保持性能。基本上，它将 32 位浮点权重降低到 16 位浮点，将 8 位整数一直降低到 1 位。但是，我们绝对不建议低于 8 位，因为会导致准确性下降。我们可以在几行代码中使用 Core ML Tools 为 Keras 模型实现量化：

```py
import coremltools

model_spec = coremltools.utils.load_spec("MyModel.mlmodel")

*`# 16-bit conversion`*
model_fp16_spec =
coremltools.utils.convert_neural_network_spec_weights_to_fp16(model_spec)
coremltools.utils.save_spec(model_fp16_spec, "MyModel_FP16.mlmodel")

*`# 8-bit or lower quantization`*
num_bits = 8
model_quant_spec =
coremltools.models.neural_network.quantization_utils.quantize_weights(model_spec,
num_bits, "linear")
coremltools.utils.save_spec(model_quant_spec, "MyModel_Quant.mlmodel")
```

展示量化对具有挑战性数据集的影响，其中小的修改可能导致 drastc 改变，我们选择使用 Keras（大约 14 MB 大小）为牛津的 102 种花卉数据集构建分类器，并将其量化为不同的位表示，同时测量其准确性并减小其大小。为了衡量预测的变化，我们比较完整精度模型和量化模型之间的匹配百分比。我们测试了三种量化模式。

+   简单的 `linear` 量化，我们在 第六章 中描述过。在这种策略中，间隔是均匀分布的。

+   使用查找表的线性量化，或 `linear_lut`。在这种技术中，间隔是不均匀分布的，密集区域的间隔更小，有更多的间隔，而稀疏区域的间隔更大，有更少的间隔。由于这些间隔是不均匀的，它们需要存储在查找表中，而不是直接用简单的算术计算。

+   由 *k*-means 生成的查找表，或 `kmeans_lut`，通常用于最近邻分类器。

表 11-6 展示了我们的观察结果。

表 11-6。不同目标位大小和不同量化模式的量化结果

| **量化为** | **百分大小减少（约）** | **与 32 位结果匹配的百分比** |
| --- | --- | --- |
| **`linear`** | **`linear_lut`** | **`kmeans_lut`** |
| --- | --- | --- |
| 16 位 | 50% | 100% | 100% | 100% |
| 8 位 | 75% | 88.37% | 80.62% | 98.45% |
| 4 位 | 88% | 0% | 0% | 81.4% |
| 2 位 | 94% | 0% | 0% | 10.08% |
| 1 位 | 97% | 0% | 0% | 7.75% |

这给了我们一些见解。

+   将表示降低到 16 位对准确性没有任何影响。我们可以将模型大小几乎减半，而准确性几乎没有差异。

+   当用于构建查找表时，*k*-means 优于简单的线性分割方法。即使在 4 位时，也只损失了 20% 的准确性，这是非常了不起的。

+   将量化模型降至 8 位可以使模型大小减小 4 倍，而准确性几乎没有损失（特别是对于 `kmeans_lut` 模式）。

+   将量化降至低于 8 位会导致准确性急剧下降，特别是对于线性模式。

## 使用 Create ML

Create ML 是苹果的一个工具，通过简单地将数据拖放到 Mac 上的 GUI 应用程序中来训练模型。它提供了几个模板，包括对象分类/检测、声音分类和文本分类等，使得即使没有领域专业知识的新手也可以进行训练 AI。它使用迁移学习来调整只有少数几层对我们的任务是必要的。因此，训练过程可以在几分钟内完成。操作系统内置了大部分层（可用于多个任务），而特定任务的层可以作为应用程序的一部分进行发布。最终导出的模型非常小（仅有 17 KB，我们很快就会看到）。我们将在下一章中亲自体验 Create ML。

# 案例研究

让我们看一些使用 Core ML 进行移动推断的真实案例。

## 魔术数独

在 2017 年[ARKit 在 iOS 上推出](https://developer.apple.com/arkit)后不久，来自 Hatchlings 的游戏和移动应用初创公司的[魔术数独](https://magicsudoku.com)应用成为了突破性的热门应用之一。只需将手机对准数独谜题，应用程序就会在纸上显示一个已解决的数独。正如我们此时可能已经猜到的那样，该系统使用 Core ML 来运行基于 CNN 的数字识别器。系统经过以下步骤：

1.  使用 ARKit 获取相机帧。

1.  使用 iOS Vision Framework 查找矩形。

1.  确定它是否为数独网格。

1.  从数独图像中提取 81 个方块。

1.  使用 Core ML 识别每个方块中的数字。

1.  使用数独求解器完成空方块。

1.  使用 ARKit 在原始纸张表面上投影完成的数独，如图 11-20 所示。

![解决 ARKit 的逐步解决方案（图片来源）](img/00153.jpeg)

###### 图 11-20\. 解决 ARKit 的逐步解决方案（[图片来源](https://oreil.ly/gzmb9)）

Hatchlings 团队首先使用了 MNIST 数字识别模型，该模型主要由手写数字组成。但对于印刷字体来说并不稳健。然后，团队拍摄了数千页数独书，使用其流水线提取方块，直到获得了大量不同字体中的单个数字图像。为了标记这些数据，团队请求其粉丝将每个项目分类为 0 到 9 和空类。在 24 小时内，团队获得了 60 万个数字扫描。下一步是训练一个定制的 CNN，因为系统需要快速工作，因为需要对 81 个方块图像进行分类。使用 Core ML 部署这个模型后，应用程序推出并一夜之间成为了热门。

与野外的新用户一起运行带来了以前未预料到的新情况。因为大多数用户面前没有数独谜题，他们经常在计算机屏幕上搜索数独，这对模型总是准确识别是困难的。此外，由于 ARKit 的固定焦距限制，输入图像可能会稍微模糊。为了解决这个问题，Hatchlings 团队收集了额外的计算机屏幕上带有谜题的照片示例，稍微模糊了它们，并使用额外的数据训练了一个新的 CNN。通过 App Store 更新，整个体验变得更加稳健。总之，在推出应用程序或服务时，应用程序构建者需要不断从以前未预料到的新场景中学习。

## Seeing AI

[Seeing AI](https://oreil.ly/hJxUE) 是微软研究设计的一款面向盲人和低视力社区的语音相机应用程序。它使用计算机视觉通过口头语音描述人物、文本、手写、物体、货币等等。大部分图像处理在设备上本地进行，使用 Core ML。其中一个核心用户需求是识别产品——通常可以通过扫描条形码的特写来确定。但对于盲人用户来说，条形码的位置是未知的，使大多数条形码应用程序难以使用。为了解决这个问题，团队构建了一个自定义的 CNN，用包含不同角度、大小、光照和方向的条形码图像进行训练。用户现在尝试在 iPhone 前旋转物体，当 CNN 分类出条形码的存在时（实时运行，逐帧），它会发出可听的蜂鸣声。蜂鸣声的频率直接与相机可见的条形码区域相关。当盲人用户开始将条形码靠近时，蜂鸣声会变得更快。当条形码阅读库能够清晰看到条形码时，应用程序解码通用产品代码并朗读产品名称。过去，盲人用户通常需要购买并携带笨重的激光条形码扫描仪，通常价格超过 1300 美元。事实上，一个慈善机构筹集了数百万美元，捐赠这些硬件条形码阅读器给需要的人。现在，深度学习可以免费解决这个问题。这是一个将计算机视觉和用户体验结合起来解决现实问题的好例子。

## HomeCourt

在生活中，无论是写作、演奏乐器还是烹饪，定期练习是必不可少的。然而，练习的质量远比数量重要。使用数据支持我们的练习并监控我们的进步，对我们获得技能的速度有奇效。这正是总部位于加利福尼亚州圣何塞的初创公司 NEX Team 为篮球练习设定的目标，借助其 HomeCourt 应用程序的帮助。运行该应用程序很简单：将其设置在地面或三脚架上，将相机对准篮球场，然后开始录制。

该应用程序在 Core ML 之上实时运行对象检测器，以跟踪球、人和篮球框。在这些人中，一个重要问题是：谁投篮了？当球靠近篮筐时，应用程序会倒带视频以识别投篮的球员。然后，它对球员进行人体姿势估计，以跟踪球员的动作。如果这还不够令人印象深刻，它使用几何变换将球场的这个 3D 场景转换为 2D 地图（如图 11-21 右上角所示），以跟踪球员投篮的位置。这里需要注意的重要一点是，多个模型同时运行。根据跟踪的对象和球员的身体位置、关节等，应用程序为球员提供每次投篮的释放高度、释放角度、释放位置、释放时间、球员速度等统计数据和可视化。这些统计数据很重要，因为它们与成功尝试有很高的相关性。球员可以在比赛进行时录制整个比赛，然后回家分析每个投篮，以确定他们的弱点，以便在下次改进。

在不到两年的时间里，这家小型初创公司吸引了数十万用户，口碑传播从业余者到专业人士。甚至美国国家篮球协会（NBA）与 NEX Team 合作，帮助改善他们球员的表现，使用 HomeCourt。所有这些都是因为他们看到了一个未满足的需求，并提出了一个创造性的解决方案，使用深度学习。

![HomeCourt 应用程序实时跟踪球员投篮](img/00104.jpeg)

###### 图 11-21。HomeCourt 应用程序在玩家进行比赛时实时跟踪其投篮

## InstaSaber + YoPuppet

你知道《星球大战》系列中最大的利润制造者是谁吗？不是电影票房收入，绝对不是从销售 DVD 中获得的。这里有一个提示：它押韵的是商品。卢卡斯影业（现在是迪士尼）通过销售 R2D2 玩具和 Chewbacca 服装赚了很多钱。尽管永远的最爱仍然是心爱的光剑。然而，与电影中看起来很酷不同，商品光剑大多由塑料制成，实际上看起来不太科幻。此外，你挥舞几次，很可能会不小心打到别人的脸。

2020CV 的创始人哈特·伍勒利决定改变这一点，通过 InstaSaber 应用程序将好莱坞级别的视觉效果带到我们的手机上。只需卷起一张纸，握住它，将手机的摄像头对准它，看着它变成一个发光的光剑，如图 11-22 所示。挥动你的手，不仅可以实时跟踪它，还可以听到与卢克与他的父亲（剧透！）达斯维达战斗时相同的声音效果。

将追踪魔法提升到下一个级别，他开发了 YoPuppet，该应用程序实时跟踪手部关节，构建出模仿手部运动的虚拟木偶。只有当它真正实时无延迟，准确且看起来逼真时，才能产生一种悬置现实的效果。

除了逼真，这些应用程序玩起来也很有趣。难怪 InstaSaber 在网上和新闻中成为一夜爆红，获得数百万次的病毒浏览。AI 潜力甚至让亿万富翁投资者马克·库班说：“你得到了一笔交易”，并投资于 2020CV。

![InstaSaber 和 YoPuppet 的屏幕截图](img/00069.jpeg)

###### 图 11-22。InstaSaber 和 YoPuppet 的屏幕截图

# 摘要

在本章中，我们快速浏览了 Core ML 的世界，它为在 iOS 设备上运行机器学习和深度学习算法提供了推理引擎。通过分析运行 CNN 所需的最小代码，我们构建了一个实时物体识别应用程序，可以对 1,000 个 ImageNet 类别进行分类。在此过程中，我们讨论了一些关于模型转换的有用信息。进入下一个级别，我们学习了关于动态模型部署和设备上训练等实用技术，同时还在各种 iOS 设备上对不同的深度学习模型进行了基准测试，从而更深入地了解了电池和资源约束。我们还看了如何使用模型量化来优化应用程序大小。最后，为了从行业中获得一些灵感，我们探索了 Core ML 在生产应用程序中的使用实例。

在下一章中，我们将通过使用 Create ML（以及其他工具）进行训练，部署我们的自定义训练分类器，并使用 Core ML 运行一个端到端的应用程序。
