- en: Chapter 7\. PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Chapters [6](ch06.html#recurrent) and [5](ch05.html#convolution), you learned
    how convolutional and recurrent neural networks worked by implementing them from
    scratch. Nevertheless, while understanding how they work is necessary, that knowledge
    alone won’t get them to work on a real-world problem; for that, you need to be
    able to implement them in a high-performance library. We could devote an entire
    book to building a high-performance neural network library, but that would be
    a much different (or simply much longer) book, for a much different audience.
    Instead, we’ll devote this last chapter to introducing PyTorch, an increasingly
    popular neural network framework based on automatic differentiation, which we
    introduced at the beginning of [Chapter 6](ch06.html#recurrent).
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the rest of the book, we’ll write our code in a way that maps to the
    mental models of how neural networks work, writing classes for `Layer`s, `Trainer`s,
    and so on. In doing so, we won’t be writing our code in line with common PyTorch
    practices, but we’ll include links on [the book’s GitHub repo](https://oreil.ly/2N4H8jz)
    for you to learn more about expressing neural networks the way PyTorch was designed
    to express them. Before we get there, let’s start by learning the data type at
    the core of PyTorch that enables its automatic differentiation and thus its ability
    to express neural network training cleanly: the `Tensor`.'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last chapter, we showed a simple `NumberWithGrad` accumulate gradients
    by keeping track of the operations performed on it. This meant that if we wrote:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: then `a.grad` would equal `35`, which is actually the partial derivative of
    `e` with respect to `a`.
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch’s `Tensor` class acts like an "`ndarrayWithGrad`“: it is similar to
    `NumberWithGrad`, except with arrays (like `numpy`) instead of just `float`s and
    `int`s. Let’s rewrite the preceding example using a PyTorch `Tensor`. First we’ll
    initialize a `Tensor` manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Note a couple of things here:'
  prefs: []
  type: TYPE_NORMAL
- en: We can initialize a `Tensor` by simply wrapping the data contained in it in
    a `torch.Tensor`, just as we did with `ndarray`s.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When initializing a `Tensor` this way, we have to pass in the argument `requires_grad=True`
    to tell the `Tensor` to accumulate gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once we’ve done this, we can perform computations as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that there’s an extra step here compared to the `NumberWithGrad`
    example: we have to *sum* `e` before calling `backward` on its sum. This is because,
    as we argued in the first chapter, it doesn’t make sense to think of “the derivative
    of a number with respect to an array”: we can, however, reason about what the
    partial derivative of `e_sum` with respect to each element of `a` would be—and
    indeed, we see that the answer is consistent with what we found in the prior chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This feature of PyTorch enables us to define models simply by defining the forward
    pass, computing a loss, and calling `.backward` on the loss to automatically compute
    the derivative of each of the `parameters` with respect to that loss. In particular,
    we don’t have to worry about reusing the same quantity multiple times in the forward
    pass (which was the limitation of the `Operation` framework we used in the first
    few chapters); as this simple example shows, gradients will automatically be computed
    correctly once we call `backward` on the output of our computations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next several sections, we’ll show how the training framework we laid
    out earlier in the book can be implemented with PyTorch’s data types.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we’ve seen, deep learning models have several elements that work together
    to produce a trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: A `Model`, which contains `Layer`s
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An `Optimizer`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `Loss`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `Trainer`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It turns out that with PyTorch, the `Optimizer` and the `Loss` are one-liners,
    and the `Model` and `Layer`s are straightforward as well. Let’s cover each of
    these elements in turn.
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch Elements: Model, Layer, Optimizer, and Loss'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A key feature of PyTorch is the ability to define models and layers as easy-to-use
    objects that handle sending gradients backward and storing parameters automatically,
    simply by having them inherit from the `torch.nn.Module` class. You’ll see how
    these pieces come together later in this chapter; for now, just know that `PyTorchLayer`
    can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'and `PyTorchModel` can also be written this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In other words, each subclass of a `PyTorchLayer` or a `PyTorchModel` will just
    need to implement `__init__` and `forward` methods, which will allow us to use
    them in intuitive ways.^([1](ch07.html#idm45732611298008))
  prefs: []
  type: TYPE_NORMAL
- en: The inference flag
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we saw in [Chapter 4](ch04.html#extensions), because of dropout, we need
    the ability to change our model’s behavior depending on whether we are running
    it in training mode or in inference mode. In PyTorch, we can switch a model or
    layer from training mode (its default behavior) to inference mode by running `m.eval`
    on the model or layer (any object that inherits from `nn.Module`). Furthermore,
    PyTorch has an elegant way to quickly change the behavior of all subclasses of
    a layer using the `apply` function. If we define:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'then we can include:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: in the `forward` method of each subclass of `PyTorchModel` or `PyTorchLayer`
    we define, thus getting the flag we desire.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how this comes together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing Neural Network Building Blocks Using PyTorch: DenseLayer'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now have all the prerequisites to start implementing the `Layer`s we’ve
    seen previously, but with PyTorch operations. A `DenseLayer` layer would be written
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, with `nn.Linear`, we see our first example of a PyTorch operation that
    automatically handles backpropagation for us. This object not only handles the
    weight multiplication and the addition of a bias term on the forward pass but
    also causes `x`’s gradients to accumulate so that the correct derivatives of the
    loss with respect to the parameters can be computed on the backward pass. Note
    also that since all PyTorch operations inherit from `nn.Module`, we can call them
    like mathematical functions: in the preceding case, for example, we write `self.linear(x)`
    rather than `self.linear.forward(x)`. This also holds true for the `DenseLayer`
    itself, as we’ll see when we use it in the upcoming model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Boston Housing Prices Model in PyTorch'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using this `Layer` as a building block, we can implement the now-familiar housing
    prices model from Chapters [2](ch02.html#fundamentals) and [3](ch03.html#deep_learning_from_scratch).
    Recall that this model simply had one hidden layer with a `sigmoid` activation;
    in [Chapter 3](ch03.html#deep_learning_from_scratch), we implemented this within
    our object-oriented framework that had a class for the `Layer`s and a model that
    had a list of length 2 as its `layers` attribute. Similarly, we can define a `HousePricesModel`
    class that inherits from `PyTorchModel` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then instantiate this via:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that it is not conventional to write a separate `Layer` class for PyTorch
    models; it is more common to simply define models in terms of the individual operations
    taking place, using something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When building PyTorch models on your own in the future, you may want to write
    your code in this way rather than creating a separate `Layer` class—and when *reading*
    others’ code, you’ll almost always see something similar to the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: '`Layer`s and `Model`s are more involved than `Optimizer`s and `Loss`es, which
    we’ll cover next.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch Elements: Optimizer and Loss'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Optimizer`s and `Loss`es are implemented in PyTorch as one-liners. For example,
    the `SGDMomentum` loss we covered in [Chapter 4](ch04.html#extensions) can be
    written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In PyTorch, models are passed into the `Optimizer` as an argument; this ensures
    that the optimizer is “pointed at” the correct model’s parameters so it knows
    what to update on each iteration (we did this using the `Trainer` class earlier).
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, the mean squared error loss we saw in [Chapter 2](ch02.html#fundamentals)
    and the `SoftmaxCrossEntropyLoss` we discussed in [Chapter 4](ch04.html#extensions)
    can simply be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Like the preceding `Layer`s, these inherit from `nn.Module`, so they can be
    called in the same way as `Layer`s.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that even though the word *softmax* is not in the name of the `nn.CrossEntropyLoss`
    class, the softmax operation is indeed performed on the inputs, so that we can
    pass in “raw outputs” from the neural network rather than outputs that have already
    passed through the softmax function, just as we did before.
  prefs: []
  type: TYPE_NORMAL
- en: These `Loss`es inherit from `nn.Module`, just like the `Layer`s from earlier,
    so they can be called the same way, using `loss(x)` instead of `loss.forward(x)`,
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch Elements: Trainer'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Trainer` pulls all of these elements together. Let’s consider the requirements
    for the `Trainer`. We know that it has to implement the general pattern for training
    neural networks that we’ve seen many times throughout this book:'
  prefs: []
  type: TYPE_NORMAL
- en: Feed a batch of inputs through the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the outputs and targets into a loss function to compute a loss value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the loss with respect to all of the parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `Optimizer` to update the parameters according to some rule.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With PyTorch, this all works the same way, except there are two small implementation
    caveats:'
  prefs: []
  type: TYPE_NORMAL
- en: By default, `Optimizer`s will retain the gradients of the parameters (what we
    referred to as `param_grads` earlier in the book) after each iteration of a parameter
    update. To clear these gradients before the next parameter update, we’ll call
    `self.optim.zero_grad`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As illustrated previously in the simple automatic differentiation example, to
    kick off the backpropagation, we’ll have to call `loss.backward` after computing
    the loss value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This leads to the following sequence of code that is seen throughout PyTorch
    training loops, and will in fact be used in the `PyTorchTrainer` class. As the
    `Trainer` class from prior chapters did, `PyTorchTrainer` will take in an `Optimizer`,
    a `PyTorchModel`, and a `Loss` (either `nn.MSELoss` or `nn.CrossEntropyLoss`)
    for a batch of data `(X_batch, y_batch)`; with these objects in place as `self.optim`,
    `self.model`, and `self.loss`, respectively, the following five lines of code
    train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Those are the most important lines; still, here’s the rest of the code for
    the `PyTorchTrainer`, much of which is similar to the code for the `Trainer` that
    we saw in prior chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since we’re passing a `Model`, an `Optimizer`, and a `Loss` into the `Trainer`,
    we need to check that the parameters that the `Optimizer` refers to are in fact
    the same as the model’s parameters; `_check_optim_net_aligned` does this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now training the model is as simple as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This code is nearly identical to the code we used to train models using the
    framework we built in the first three chapters. Whether you’re using PyTorch,
    TensorFlow, or Theano under the hood, the elements of training a deep learning
    model remain the same!
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll explore more features of PyTorch by showing how to implement the
    tricks to improve training that we saw in [Chapter 4](ch04.html#extensions).
  prefs: []
  type: TYPE_NORMAL
- en: Tricks to Optimize Learning in PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We learned four tricks to accelerate learning in [Chapter 4](ch04.html#extensions):'
  prefs: []
  type: TYPE_NORMAL
- en: Momentum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight initialization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning rate decay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are all easy to implement in PyTorch. For example, to include momentum
    in our optimizer, we can simply include a `momentum` keyword in `SGD`, so that
    the optimizer becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Dropout is similarly easy. Just as PyTorch has a built-in `Module` `nn.Linear(n_in,
    n_out)` that computes the operations of a `Dense` layer from before, the `Module`
    `nn.Dropout(dropout_prob)` implements the `Dropout` operation, with the caveat
    that the probability passed in is by default the probability of *dropping* a given
    neuron, rather than keeping it as it was in our implementation from before.
  prefs: []
  type: TYPE_NORMAL
- en: 'We don’t need to worry about weight initialization at all: the weights in most
    PyTorch operations involving parameters, including `nn.Linear`, are automatically
    scaled based on the size of the layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, PyTorch has an `lr_scheduler` class that can be used to decay the learning
    rate over the epochs. The key import you need to get started is from `torch.optim
    import lr_scheduler`.^([2](ch07.html#idm45732608539560)) Now you can easily use
    these techniques we covered from first principles in any future deep learning
    project you work on!
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 5](ch05.html#convolution), we systematically covered how convolutional
    neural networks work, focusing in particular on the multichannel convolution operation.
    We saw that the operation transforms the pixels of input images into layers of
    neurons organized into feature maps, where each neuron represents whether a given
    visual feature (defined by a convolutional filter) is present at that location
    in the image. The multichannel convolution operation had the following shapes
    for its two inputs and its output:'
  prefs: []
  type: TYPE_NORMAL
- en: The data input shape `[batch_size, in_channels, image_height, image_width]`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parameters input shape `[in_channels, out_channels, filter_size, filter_size]`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output shape `[batch_size, out_channels, image_height, image_width]`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In terms of this notation, the multichannel convolution operation in PyTorch
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'With this defined, wrapping a `ConvLayer` around this operation is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.html#convolution), we automatically padded the output based
    on the filter size to keep the output image the same size as the input image.
    PyTorch does not do that; to achieve the same behavior we had before, we add an
    argument to the `nn.Conv2d` operation setting `padding` `= filter_size // 2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'From there, all we have to do is define a `PyTorchModel` with its operations
    in the `__init__` function and the sequence of operations defined in the `forward`
    function to begin to train. Next is a simple architecture we can use on the MNIST
    dataset we saw in Chapters [4](ch04.html#extensions) and [5](ch05.html#convolution),
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: A convolutional layer that transforms the input from 1 “channel” to 16 channels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another layer that transforms these 16 channels into 8 (with each channel still
    containing 28 × 28 neurons)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two fully connected layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pattern of several convolutional layers followed by a smaller number of
    fully connected layers is common for convolutional architectures; here, we just
    use two of each:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can train this model the same way we trained the `HousePricesModel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: There is an important caveat related to the `nn.CrossEntropyLoss` class. Recall
    that in the custom framework from previous chapters, our `Loss` class expected
    an input of the same shape as the target. To get this, we one-hot encoded the
    10 distinct values of the target in the MNIST data so that, for each batch of
    data, the target had shape `[batch_size, 10]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With PyTorch’s `nn.CrossEntropyLoss` class—which works exactly the same as
    our `SoftmaxCrossEntropyLoss` from before—we don’t have to do that. This loss
    function expects two `Tensor`s:'
  prefs: []
  type: TYPE_NORMAL
- en: A prediction `Tensor` of size `[batch_size, num_classes]`, just as our `SoftmaxCrossEntropyLoss`
    class did before
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A target `Tensor` of size `[batch_size]` with `num_classes` different values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So in the preceding example, `y_train` is simply an array of size `[60000]`
    (the number of observations in the training set of MNIST), and `y_test` simply
    has size `[10000]` (the number of observations in the test set).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’re dealing with larger datasets, we should cover another best practice.
    It is clearly very memory inefficient to load the entire training and testing
    sets into memory to train the model, as we’re doing with `X_train`, `y_train`,
    `X_test`, and `y_test`. PyTorch has a way around this: the `DataLoader` class.'
  prefs: []
  type: TYPE_NORMAL
- en: DataLoader and Transforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall that in our MNIST modeling in [Chapter 2](ch02.html#fundamentals), we
    applied a simple preprocessing step to the MNIST data, subtracting off the global
    mean and dividing by the global standard deviation to roughly “normalize” the
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Still, this required us to first fully read these two arrays into memory; it
    would be much more efficient to perform this preprocessing on the fly, as batches
    are fed into the neural network. PyTorch has built-in functions that do this,
    and they are especially commonly used with image data—transformations via the
    `transforms` module, and a `DataLoader` via `torch.utils.data`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Previously, we read in the entire training set into `X_train` via:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We then performed transformations on `X_train` to get it to a form where it
    was ready for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch has some convenience functions that allow us to compose many transformations
    to each batch of data as it is read in; this allows us both to avoid reading the
    entire dataset into memory and to use PyTorch’s transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define a list of transformations to perform on each batch of data
    read in. For example, the following transformations convert each MNIST image to
    a `Tensor` (most PyTorch datasets are “PIL images” by default, so `transforms.ToTensor()`
    is often the first transformation in the list), and then “normalize” the dataset—subtracting
    off the mean and then dividing by the standard deviation—using the overall MNIST
    mean and standard deviation of `0.1305` and `0.3081`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`Normalize` actually subtracts the mean and standard deviation *from each channel*
    of the input image. Thus, it is common when dealing with color images with three
    input channels to have a `Normalize` transformation that has two tuples of three
    numbers each—for example, `transforms.Normalize((0.1, 0.3, 0.6), (0.4, 0.2, 0.5))`,
    which would tell the `DataLoader` to:'
  prefs: []
  type: TYPE_NORMAL
- en: Normalize the first channel using a mean of 0.1 and a standard deviation of
    0.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalize the second channel using a mean of 0.3 and a standard deviation of
    0.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalize the third channel using a mean of 0.6 and a standard deviation of
    0.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Second, once these transformations have been applied, we apply these to the
    `dataset` as we read in batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can define a `DataLoader` that takes in this dataset and defines
    rules for successively generating batches of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then modify the `Trainer` to use the `dataloader` to generate the batches
    used to train the network instead of loading the entire dataset into memory and
    then manually generating them using the `batch_generator` function, as we did
    before. On [the book’s website](https://oreil.ly/2N4H8jz),^([3](ch07.html#idm45732607595080))
    I show an example of training a convolutional neural network using these `DataLoader`s.
    The main change in the `Trainer` is simply changing the line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, instead of feeding in the entire training set into the `fit` function,
    we now feed in `DataLoader`s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Using this architecture and calling the `fit` method, as we just did, gets us
    to about 97% accuracy on MNIST after one epoch. More important than the accuracy,
    however, is that you’ve seen how to implement the concepts we reasoned through
    from first principles into a high-performance framework. Now that you understand
    both the underlying concepts and the framework, I encourage you to modify the
    code in [the book’s GitHub repo](https://oreil.ly/2N4H8jz) and try out other convolutional
    architectures, other datasets, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs were one of two advanced architectures we covered earlier in the book;
    let’s now turn to the other one and show how to implement the most advanced RNN
    variant we’ve covered, LSTMs, in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs in PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We saw in the last chapter how to code LSTMs from scratch. We coded an `LSTMLayer`
    to take in an input `ndarray` of size [`batch_size`, `sequence_length`, `feature_size`],
    and output an `ndarray` of size [`batch_size`, `sequence_length`, `feature_size`].
    In addition, each layer took in a hidden state and a cell state, each initialized
    with shape `[1, hidden_size]`, expanded to shape `[batch_size, hidden_size]` when
    a batch is passed in, and then collapsed back down to `[1, hidden_size]` after
    the iteration is complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this, we define the `__init__` method for our `LSTMLayer` as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As with convolutional layers, PyTorch has an `nn.lstm` operation for implementing
    LSTMs. Note that in our custom `LSTMLayer` we store a `DenseLayer` in the `self.fc`
    attribute. You may recall from the last chapter that the last step of an LSTM
    cell is putting the final hidden state through the operations of a `Dense` layer
    (a weight multiplication and addition of a bias) to transform the hidden state
    into dimension `output_size` for each operation. PyTorch does things a bit differently:
    the `nn.lstm` operation simply outputs the hidden states for each time step. Thus,
    to enable our `LSTMLayer` to output a different dimension than its input—as we
    would want all of our neural network layers to be able to do—we add a `DenseLayer`
    at the end to transform the hidden state into dimension `output_size`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this modification, the `forward` function is now straightforward, looking
    similar to the `forward` function of the `LSTMLayer` from [Chapter 6](ch06.html#recurrent):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The key line here, which should look familiar given our implementation of LSTMs
    in [Chapter 6](ch06.html#recurrent), is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Aside from that, there’s some reshaping of the hidden and cell states before
    and after the `self.lstm` function via a helper function `self._transform_hidden_batch`.
    You can see the full function in [the book’s GitHub repo](https://oreil.ly/2N4H8jz).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, wrapping a model around this is easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `nn.CrossEntropyLoss` function expects the first two dimensions to be the
    `batch_size` and the distribution over the classes; the way we’ve been implementing
    our LSTMs, however, we have the distribution over the classes as the last dimension
    (`vocab_size`) coming out of the `LSTMLayer`. To prepare the final model output
    to be fed into the loss, therefore, we move the dimension containing the distribution
    over letters to the second dimension using `out.permute(0, 2, 1)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in [the book’s GitHub repo](https://oreil.ly/2N4H8jz), I show how
    to write a class `LSTMTrainer` to inherit from `PyTorchTrainer` and use it to
    train a `NextCharacterModel` to generate text. We use the same text preprocessing
    that we did in [Chapter 6](ch06.html#recurrent): selecting sequences of text,
    one-hot encoding the letters, and grouping the sequences of one-hot encoded letters
    into batches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'That wraps up how to translate the three neural network architectures for supervised
    learning we saw in this book—fully connected neural networks, convolutional neural
    networks, and recurrent neural networks—into PyTorch. To conclude, we’ll briefly
    cover how neural networks can be used for the other half of machine learning:
    *un*-supervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Postscript: Unsupervised Learning via Autoencoders'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout this book we’ve focused on how deep learning models can be used
    to solve *supervised* learning problems. There is, of course, a whole other side
    to machine learning: unsupervised learning; which involves what is often described
    as “finding structure in data without labels”; I like to think of it, though,
    as finding relationships between characteristics in your data that have not yet
    been measured, whereas supervised learning involves finding relationships between
    characteristics in your data that have already been measured.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you had a dataset of images with no labels. You don’t know much about
    these images—for example, you’re not sure whether there are 10 distinct digits
    represented, or 5, or 20 (these images could be from a strange alphabet)—and you
    want to know the answers to questions like:'
  prefs: []
  type: TYPE_NORMAL
- en: How many distinct digits are there?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which digits are visually similar to one another?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there “outlier” images that are distinctly *dis*similar to other images?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To understand how deep learning can help with this, we’ll have to take a quick
    step back and think conceptually about what deep learning models are trying to
    do.
  prefs: []
  type: TYPE_NORMAL
- en: Representation Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve seen that deep learning models can learn to make accurate predictions.
    They do this by transforming the input they receive into representations that
    are progressively both more abstract and more tuned to directly making predictions
    for whatever the relevant problem is. In particular, the final layer of the network,
    directly before the layer with the predictions themselves (which would have just
    one neuron for a regression problem and *`num_classes`* neurons for a classification
    problem), is the network’s attempt at creating a representation of the input data
    that is as useful as possible for the task of making predictions. This is shown
    in [Figure 7-1](#fig_07_01).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfs_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. The final layer of a neural network, immediately before the predictions,
    represents the network’s representation of the input that it has found most useful
    to the task of predicting
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once trained, then, a model can not only make predictions for new data points,
    *but also generate representations of these data points*. These could then be
    used for clustering, similarity analysis, or outlier detection—in addition to
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: An Approach for Situations with No Labels Whatsoever
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A limitation with this whole approach is that it *requires labels to train
    the model to generate the representations in the first place*. The question is:
    how can we train a model to generate “useful” representations without any labels?
    If we don’t have labels, we need to generate representations of our data using
    the only thing we do have: the training data itself. This is the idea behind a
    class of neural network architectures known as autoencoders, which involve training
    neural networks to *reconstruct* the training data, forcing the network to learn
    the representation of each data point most helpful for this reconstruction.'
  prefs: []
  type: TYPE_NORMAL
- en: Diagram
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Figure 7-2](#fig_07_02) shows a high-level overview of an autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: One set of layers transforms the data into a compressed representation of the
    data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another set of layers transforms this representation into an output of the same
    size and shape as the original data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/dlfs_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. An autoencoder has one set of layers (which can be thought of as
    the “encoder” network) that maps the input to a lower-dimensional representation,
    and another set of layers (which can be thought of as the “decoder” network) that
    maps the lower-dimensional representation back to the input; this structure forces
    the network to learn a lower-dimensional representation that is most useful for
    reconstructing the input
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Implementing such an architecture illustrates some features of PyTorch we haven’t
    had a chance to introduce yet.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an Autoencoder in PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll now show a simple autoencoder that takes in an input image, feeds it
    through two convolutional layers and then a `Dense` layer to generate a representation,
    and then feeds this representation back through a `Dense` layer and two convolutional
    layers to generate an output of the same size as the input. We’ll use this to
    illustrate two common practices when implementing more advanced architectures
    in PyTorch. First, we can include `PyTorchModel`s as attributes of another `PyTorchModel`,
    just as we defined `PyTorchLayer`s as attributes of such models previously. In
    the following example, we’ll implement our autoencoder as having two `PyTorchModel`s
    as attributes: an `Encoder` and a `Decoder`. Once we train the model, we’ll be
    able to use the trained `Encoder` as its own model to generate the representations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the `Encoder` as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'And we define the `Decoder` as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we were using a stride greater than 1, we wouldn’t simply be able to use
    a regular convolution to transform the encoding into an output, as we do here,
    but instead would have to use a *transposed convolution*, where the image size
    of the output of the operation would be larger than the image size of the input.
    See the `nn.ConvTranspose2d` operation in the [PyTorch documentation](https://oreil.ly/306qiV7)
    for more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the `Autoencoder` itself can wrap around these and become:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The `forward` method of the `Autoencoder` illustrates a second common practice
    in PyTorch: since we’ll ultimately want to see the hidden representation that
    the model produces, the `forward` method returns *two* elements: this “encoding,”
    `encoding`, along with the output that will be used to train the network, `x`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, we would have to modify our `Trainer` class to accommodate this;
    specifically, `PyTorchModel` as currently written outputs only a single `Tensor`
    from its `forward` method. As it turns out, modifying it so that it returns a
    `Tuple` of `Tensor`s by default, even if that `Tuple` is only of length 1, will
    both be useful—enabling us to easily write models like the `Autoencoder`—and not
    difficult. All we have to do is three small things: first, make the function signature
    of the `forward` method of our base `PyTorchModel` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Then, at the end of the `forward` method of any model that inherits from the
    `PyTorchModel` base class, we’ll write `return x,` instead of `return x` as we
    were doing before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, we’ll modify our `Trainer` to always take as output the first element
    of whatever the model returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'There is one other notable feature of the `Autoencoder` model: we apply a `Tanh`
    activation function to the last layer, meaning the model output will be between
    –1 and 1\. With any model, the model outputs should be on the same scale as the
    target they are compared to, and here, the target is our input itself. So we should
    scale our input to range from a minimum of –1 and a maximum of 1, as in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can train our model using training code, which by now should look
    familiar (we somewhat arbitrarily use 28 as the dimensionality of the output of
    the encoding):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we run this code and train the model, we can look at both the reconstructed
    images and the image representations simply by passing `X_test_auto` through the
    model (since the `forward` method was defined to return two quantities):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Each element of `reconstructed_images` is a `[1, 28, 28]` `Tensor` and represents
    the neural network’s best attempt to reconstruct the corresponding original image
    after passing it through an autoencoder architecture that forced the image through
    a layer with lower dimensionality. [Figure 7-3](#fig_07_03) shows a randomly chosen
    reconstructed image alongside the original image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfs_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. An image from the MNIST test set alongside the reconstruction of
    that image after it was fed through the autoencoder
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Visually, the images look similar, telling us that the neural network does
    indeed seem to have taken the original images, which were 784 pixels, and mapped
    them to a space of lower dimensionality—specifically, 28—such that most of the
    information about the 784-pixel image is encoded in this vector of length 28\.
    How can we examine the whole dataset to see whether the neural network has indeed
    learned the structure of the image data without seeing the labels? Well, “the
    structure of the data” here means that the underlying data is in fact images of
    10 distinct handwritten digits. Thus, images close to a given image in the new
    28-dimensional space should ideally be of the same digit, or at least visually
    be very similar, since visual similarity is how we as humans distinguish between
    different images. We can test whether this is the case by applying a dimensionality
    reduction technique invented by Laurens van der Maaten when he was a graduate
    student under Geoffrey Hinton (who was one of the “founding fathers” of neural
    networks): *t-Distributed Stochastic Neighbor Embedding*, or t-SNE. t-SNE performs
    its dimensionality reduction in a way that is analogous to how neural networks
    are trained: it starts with an initial lower-dimensional representation and then
    updates it so that, over time, it approaches a solution with the property that
    points that are “close together” in the high-dimensional space are “close together”
    in the low-dimensional space, and vice versa.^([4](ch07.html#idm45732606124216))'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll try the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Feed the 10,000 images through t-SNE and reduce the dimensionality to 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize the resulting two-dimensional space, coloring the different points
    by their *actual* label (which the autoencoder did not see).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Figure 7-4](#fig_07_04) shows the result.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfs_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Result of running t-SNE on 28-dimensional learned space of the
    autoencoder
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It appears that images of each digit are largely grouped together in their
    own separate cluster; this shows that training our autoencoder architecture to
    learn to reconstruct the original images from just a lower-dimensional representation
    has indeed enabled it to discover much of the underlying structure of these images
    without seeing any labels.^([5](ch07.html#idm45732606114968)) And not only are
    the 10 digits represented as distinct clusters, but visually similar digits are
    also closer together: at the top and slightly to the right, we have clusters of
    the digits 3, 5, and 8, and at the bottom we see 4 and 9 clustered tightly together,
    with 7 not far away. Finally, the most distinct digits—0, 1, and 6—form the most
    distinct clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: A Stronger Test for Unsupervised Learning, and a Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What we’ve just seen is a fairly weak test for whether our model has learned
    an underlying structure to the space of input images—by this point, it shouldn’t
    be too surprising that a convolutional neural network can learn representations
    of images of digits with the property that visually similar images have similar
    representations. A stronger test would be to examine if the neural network has
    discovered a “smooth” underlying space: a space in which *any* vector of length
    28, rather than just the vectors resulting from feeding real digits through the
    encoder network, can be mapped to a realistic-looking digit. It turns out that
    our autoencoder cannot do this; [Figure 7-5](#fig_07_05) shows the result of generating
    five random vectors of length 28 and feeding them through the decoder network,
    using the fact that the `Autoencoder` contained a `Decoder` as an attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfs_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. Result of feeding five randomly generated vectors through the decoder
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can see that the resulting images don’t look like digits; thus, while our
    autoencoder can map our data to a lower-dimensional space in a sensible way, it
    doesn’t appear to be able to learn a “smooth” space such as the one described
    a moment ago.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the problem, of training a neural network to learn to represent images
    in a training set in a “smooth” underlying space, is one of the major accomplishments
    of *generative adversarial networks* (GANs). Invented in 2014, GANs are most widely
    known for allowing neural networks to generate realistic-looking images via a
    training procedure in which two neural networks are trained simultaneously. GANs
    were truly pushed forward in 2015, however, when researchers used them with deep
    convolutional architectures in both networks not just to generate realistic-looking
    64 × 64 color images of bedrooms but also to generate a large sample of said images
    from randomly generated 100-dimensional vectors.^([6](ch07.html#idm45732606010168))
    This signaled that the neural networks really had learned an underlying representation
    of the “space” of these unlabeled images. GANs deserve a book of their own, so
    we won’t cover them in more detail than this.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You now have a deep understanding of the mechanics of some of the most popular
    advanced deep learning architectures out there, as well as how to implement these
    architectures in one of the most popular high-performance deep learning frameworks.
    The only thing stopping you from using deep learning models to solve real-world
    problems is practice. Luckily, it has never been easier to read others’ code and
    quickly get up to speed on the details and implementation tricks that make certain
    model architectures work on certain problems. A list of recommended next steps
    is listed in [the book’s GitHub repo](https://oreil.ly/2N4H8jz).
  prefs: []
  type: TYPE_NORMAL
- en: Onward!
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch07.html#idm45732611298008-marker)) Writing `Layer`s and `Model`s in
    this way isn’t the most common or recommended use of PyTorch; we show it here
    because it most closely maps to the concepts we’ve covered so far. To see a more
    common way to build neural network building blocks with PyTorch, see this [introductory
    tutorial from the official documentation](https://oreil.ly/SKB_V).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch07.html#idm45732608539560-marker)) In the [book’s GitHub repo](https://oreil.ly/301qxRk),
    you can find an example of code that implements exponential learning rate decay
    as part of a `PyTorchTrainer`. The documentation for the `ExponentialLR` class
    used there can be found on the [PyTorch website](https://oreil.ly/2Mj9IhH).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch07.html#idm45732607595080-marker)) Look in the “CNNs using PyTorch”
    section.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch07.html#idm45732606124216-marker)) The original 2008 paper is [“Visualizing
    Data using t-SNE”](https://oreil.ly/2KIAaOt), by Laurens van der Maaten and Geoffrey
    Hinton.
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch07.html#idm45732606114968-marker)) Furthermore, we did this without
    much trying: the architecture here is very straightforward, and we’re not using
    any of the tricks we discussed for training neural networks, such as learning
    rate decay, since we’re training for only one epoch. This illustrates that the
    underlying idea of using an autoencoder-like architecture to learn the structure
    of a dataset without labels is a good one in general and didn’t just “happen to
    work” here.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch07.html#idm45732606010168-marker)) Check out the DCGAN paper, [“Unsupervised
    Representation Learning with Deep Convolutional Generative Adversarial Networks”](https://arxiv.org/abs/1511.06434)
    by Alec Radford et al., as well as this [PyTorch documentation](https://oreil.ly/2TEspgG).
  prefs: []
  type: TYPE_NORMAL
