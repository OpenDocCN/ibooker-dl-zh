- en: Chapter 7\. PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。PyTorch
- en: In Chapters [6](ch06.html#recurrent) and [5](ch05.html#convolution), you learned
    how convolutional and recurrent neural networks worked by implementing them from
    scratch. Nevertheless, while understanding how they work is necessary, that knowledge
    alone won’t get them to work on a real-world problem; for that, you need to be
    able to implement them in a high-performance library. We could devote an entire
    book to building a high-performance neural network library, but that would be
    a much different (or simply much longer) book, for a much different audience.
    Instead, we’ll devote this last chapter to introducing PyTorch, an increasingly
    popular neural network framework based on automatic differentiation, which we
    introduced at the beginning of [Chapter 6](ch06.html#recurrent).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[6](ch06.html#recurrent)章和第[5](ch05.html#convolution)章中，您学习了如何通过从头开始实现卷积和循环神经网络来使它们工作。然而，了解它们如何工作是必要的，但仅凭这些知识无法使它们在真实世界的问题上工作；为此，您需要能够在高性能库中实现它们。我们可以致力于构建一个高性能神经网络库的整本书，但那将是一本非常不同（或者只是更长）的书，面向一个非常不同的受众。相反，我们将把这最后一章献给介绍PyTorch，这是一个越来越受欢迎的基于自动微分的神经网络框架，我们在[第6章](ch06.html#recurrent)的开头介绍过。
- en: 'As in the rest of the book, we’ll write our code in a way that maps to the
    mental models of how neural networks work, writing classes for `Layer`s, `Trainer`s,
    and so on. In doing so, we won’t be writing our code in line with common PyTorch
    practices, but we’ll include links on [the book’s GitHub repo](https://oreil.ly/2N4H8jz)
    for you to learn more about expressing neural networks the way PyTorch was designed
    to express them. Before we get there, let’s start by learning the data type at
    the core of PyTorch that enables its automatic differentiation and thus its ability
    to express neural network training cleanly: the `Tensor`.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书的其余部分一样，我们将以与神经网络工作方式相匹配的方式编写代码，编写`Layer`、`Trainer`等类。这样做的同时，我们不会按照常见的PyTorch实践编写代码，但我们将在[本书的GitHub存储库](https://oreil.ly/2N4H8jz)中包含链接，让您了解更多关于如何表达神经网络的信息，以PyTorch设计的方式来表达。在我们开始之前，让我们从学习PyTorch的核心数据类型开始，这种数据类型使其具有自动微分的能力，从而使其能够清晰地表达神经网络训练：`Tensor`。
- en: PyTorch Tensors
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch张量
- en: 'In the last chapter, we showed a simple `NumberWithGrad` accumulate gradients
    by keeping track of the operations performed on it. This meant that if we wrote:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们展示了一个简单的`NumberWithGrad`通过跟踪对其执行的操作来累积梯度。这意味着如果我们写：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: then `a.grad` would equal `35`, which is actually the partial derivative of
    `e` with respect to `a`.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后`a.grad`将等于`35`，这实际上是`e`相对于`a`的偏导数。
- en: 'PyTorch’s `Tensor` class acts like an "`ndarrayWithGrad`“: it is similar to
    `NumberWithGrad`, except with arrays (like `numpy`) instead of just `float`s and
    `int`s. Let’s rewrite the preceding example using a PyTorch `Tensor`. First we’ll
    initialize a `Tensor` manually:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的`Tensor`类就像一个"`ndarrayWithGrad`"：它类似于`NumberWithGrad`，只是使用数组（如`numpy`）而不仅仅是`float`和`int`。让我们使用PyTorch的`Tensor`重新编写前面的示例。首先，我们将手动初始化一个`Tensor`：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Note a couple of things here:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这里注意几点：
- en: We can initialize a `Tensor` by simply wrapping the data contained in it in
    a `torch.Tensor`, just as we did with `ndarray`s.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过简单地将其中包含的数据包装在`torch.Tensor`中来初始化一个`Tensor`，就像我们用`ndarray`做的那样。
- en: When initializing a `Tensor` this way, we have to pass in the argument `requires_grad=True`
    to tell the `Tensor` to accumulate gradients.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当以这种方式初始化`Tensor`时，我们必须传入参数`requires_grad=True`，以告诉`Tensor`累积梯度。
- en: 'Once we’ve done this, we can perform computations as before:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们这样做了，我们就可以像以前一样执行计算：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can see that there’s an extra step here compared to the `NumberWithGrad`
    example: we have to *sum* `e` before calling `backward` on its sum. This is because,
    as we argued in the first chapter, it doesn’t make sense to think of “the derivative
    of a number with respect to an array”: we can, however, reason about what the
    partial derivative of `e_sum` with respect to each element of `a` would be—and
    indeed, we see that the answer is consistent with what we found in the prior chapters:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到与`NumberWithGrad`示例相比，这里有一个额外的步骤：在调用其总和之前，我们必须*对`e`进行求和*，然后调用`backward`。这是因为，正如我们在第一章中所讨论的，想象“一个数字相对于一个数组的导数”是没有意义的：但是，我们可以推断出`e_sum`相对于`a`的每个元素的偏导数是什么，并且，我们看到答案与我们在之前的章节中发现的是一致的：
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This feature of PyTorch enables us to define models simply by defining the forward
    pass, computing a loss, and calling `.backward` on the loss to automatically compute
    the derivative of each of the `parameters` with respect to that loss. In particular,
    we don’t have to worry about reusing the same quantity multiple times in the forward
    pass (which was the limitation of the `Operation` framework we used in the first
    few chapters); as this simple example shows, gradients will automatically be computed
    correctly once we call `backward` on the output of our computations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的这个特性使我们能够通过定义前向传播、计算损失并在损失上调用`.backward`来简单地定义模型，并自动计算每个`参数`相对于该损失的导数。特别是，我们不必担心在前向传递中多次重复使用相同的数量（这是我们在前几章中使用的`Operation`框架的限制）；正如这个简单的例子所示，一旦我们在我们的计算输出上调用`backward`，梯度将自动正确计算。
- en: In the next several sections, we’ll show how the training framework we laid
    out earlier in the book can be implemented with PyTorch’s data types.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将展示如何使用PyTorch的数据类型实现我们在本书中提出的训练框架。
- en: Deep Learning with PyTorch
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch进行深度学习
- en: 'As we’ve seen, deep learning models have several elements that work together
    to produce a trained model:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，深度学习模型有几个元素共同工作以产生一个经过训练的模型：
- en: A `Model`, which contains `Layer`s
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含`Layer`的`Model`
- en: An `Optimizer`
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`Optimizer`
- en: A `Loss`
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`Loss`
- en: A `Trainer`
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`Trainer`
- en: It turns out that with PyTorch, the `Optimizer` and the `Loss` are one-liners,
    and the `Model` and `Layer`s are straightforward as well. Let’s cover each of
    these elements in turn.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，使用PyTorch，`Optimizer`和`Loss`都是一行代码，`Model`和`Layer`也很简单。让我们依次介绍这些元素。
- en: 'PyTorch Elements: Model, Layer, Optimizer, and Loss'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch元素：模型、层、优化器和损失
- en: 'A key feature of PyTorch is the ability to define models and layers as easy-to-use
    objects that handle sending gradients backward and storing parameters automatically,
    simply by having them inherit from the `torch.nn.Module` class. You’ll see how
    these pieces come together later in this chapter; for now, just know that `PyTorchLayer`
    can be written as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的一个关键特性是能够将模型和层定义为易于使用的对象，通过继承`torch.nn.Module`类处理梯度向后传播和自动存储参数。稍后在本章中您将看到这些部分如何结合在一起；现在只需知道`PyTorchLayer`可以这样编写：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'and `PyTorchModel` can also be written this way:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyTorchModel`也可以这样编写：'
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In other words, each subclass of a `PyTorchLayer` or a `PyTorchModel` will just
    need to implement `__init__` and `forward` methods, which will allow us to use
    them in intuitive ways.^([1](ch07.html#idm45732611298008))
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，`PyTorchLayer`或`PyTorchModel`的每个子类只需要实现`__init__`和`forward`方法，这将使我们能够以直观的方式使用它们。
- en: The inference flag
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推断标志
- en: 'As we saw in [Chapter 4](ch04.html#extensions), because of dropout, we need
    the ability to change our model’s behavior depending on whether we are running
    it in training mode or in inference mode. In PyTorch, we can switch a model or
    layer from training mode (its default behavior) to inference mode by running `m.eval`
    on the model or layer (any object that inherits from `nn.Module`). Furthermore,
    PyTorch has an elegant way to quickly change the behavior of all subclasses of
    a layer using the `apply` function. If we define:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第4章](ch04.html#extensions)中看到的，由于dropout，我们需要根据我们是在训练模式还是推断模式下运行模型的能力来改变模型的行为。在PyTorch中，我们可以通过在模型或层（从`nn.Module`继承的任何对象）上运行`m.eval`将模型或层从训练模式（其默认行为）切换到推断模式。此外，PyTorch有一种优雅的方式可以快速更改层的所有子类的行为，即使用`apply`函数。如果我们定义：
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'then we can include:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以包括：
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: in the `forward` method of each subclass of `PyTorchModel` or `PyTorchLayer`
    we define, thus getting the flag we desire.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义的每个`PyTorchModel`或`PyTorchLayer`子类的`forward`方法中，从而获得我们想要的标志。
- en: Let’s see how this comes together.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是如何结合在一起的。
- en: 'Implementing Neural Network Building Blocks Using PyTorch: DenseLayer'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch实现神经网络构建块：DenseLayer
- en: 'We now have all the prerequisites to start implementing the `Layer`s we’ve
    seen previously, but with PyTorch operations. A `DenseLayer` layer would be written
    as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经具备开始使用PyTorch操作实现之前看到的`Layer`的所有先决条件，一个`DenseLayer`层将被写成如下：
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, with `nn.Linear`, we see our first example of a PyTorch operation that
    automatically handles backpropagation for us. This object not only handles the
    weight multiplication and the addition of a bias term on the forward pass but
    also causes `x`’s gradients to accumulate so that the correct derivatives of the
    loss with respect to the parameters can be computed on the backward pass. Note
    also that since all PyTorch operations inherit from `nn.Module`, we can call them
    like mathematical functions: in the preceding case, for example, we write `self.linear(x)`
    rather than `self.linear.forward(x)`. This also holds true for the `DenseLayer`
    itself, as we’ll see when we use it in the upcoming model.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，通过`nn.Linear`，我们看到了PyTorch操作的第一个示例，它会自动处理反向传播。这个对象不仅在前向传播时处理权重乘法和偏置项的加法，还会导致`x`的梯度累积，以便在反向传播时计算出参数相对于损失的正确导数。还要注意，由于所有PyTorch操作都继承自`nn.Module`，我们可以像数学函数一样调用它们：在前面的情况下，例如，我们写`self.linear(x)`而不是`self.linear.forward(x)`。当我们在即将到来的模型中使用`DenseLayer`时，这也适用于`DenseLayer`本身。
- en: 'Example: Boston Housing Prices Model in PyTorch'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：PyTorch中的波士顿房价模型
- en: 'Using this `Layer` as a building block, we can implement the now-familiar housing
    prices model from Chapters [2](ch02.html#fundamentals) and [3](ch03.html#deep_learning_from_scratch).
    Recall that this model simply had one hidden layer with a `sigmoid` activation;
    in [Chapter 3](ch03.html#deep_learning_from_scratch), we implemented this within
    our object-oriented framework that had a class for the `Layer`s and a model that
    had a list of length 2 as its `layers` attribute. Similarly, we can define a `HousePricesModel`
    class that inherits from `PyTorchModel` as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个`Layer`作为构建块，我们可以实现在第[2](ch02.html#fundamentals)章和第[3](ch03.html#deep_learning_from_scratch)章中看到的熟悉的房价模型。回想一下，这个模型只有一个带有`sigmoid`激活函数的隐藏层；在[第3章](ch03.html#deep_learning_from_scratch)中，我们在面向对象的框架中实现了这一点，该框架具有`Layer`的类和一个模型，其`layers`属性是长度为2的列表。类似地，我们可以定义一个`HousePricesModel`类，它继承自`PyTorchModel`，如下所示：
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can then instantiate this via:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以通过以下方式实例化：
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Note that it is not conventional to write a separate `Layer` class for PyTorch
    models; it is more common to simply define models in terms of the individual operations
    taking place, using something like the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为PyTorch模型编写单独的`Layer`类并不是传统的做法；更常见的做法是简单地根据正在发生的各个操作定义模型，使用类似以下的方式：
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: When building PyTorch models on your own in the future, you may want to write
    your code in this way rather than creating a separate `Layer` class—and when *reading*
    others’ code, you’ll almost always see something similar to the preceding code.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来构建自己的PyTorch模型时，您可能希望以这种方式编写代码，而不是创建一个单独的`Layer`类——当*阅读*他人的代码时，您几乎总是会看到类似于前面代码的东西。
- en: '`Layer`s and `Model`s are more involved than `Optimizer`s and `Loss`es, which
    we’ll cover next.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`Layer`和`Model`比`Optimizer`和`Loss`更复杂，我们将在下一节中介绍。'
- en: 'PyTorch Elements: Optimizer and Loss'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch元素：优化器和损失
- en: '`Optimizer`s and `Loss`es are implemented in PyTorch as one-liners. For example,
    the `SGDMomentum` loss we covered in [Chapter 4](ch04.html#extensions) can be
    written as:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`Optimizer`和`Loss`在PyTorch中实现为一行代码。例如，我们在[第4章](ch04.html#extensions)中介绍的`SGDMomentum`损失可以写成：'
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In PyTorch, models are passed into the `Optimizer` as an argument; this ensures
    that the optimizer is “pointed at” the correct model’s parameters so it knows
    what to update on each iteration (we did this using the `Trainer` class earlier).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，模型作为参数传递给`Optimizer`；这确保了优化器“指向”正确的模型参数，以便在每次迭代中知道要更新什么（我们之前使用`Trainer`类做过这个操作）。
- en: 'Furthermore, the mean squared error loss we saw in [Chapter 2](ch02.html#fundamentals)
    and the `SoftmaxCrossEntropyLoss` we discussed in [Chapter 4](ch04.html#extensions)
    can simply be written as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在[第2章]中看到的均方误差损失和我们在[第4章]中讨论的`SoftmaxCrossEntropyLoss`可以简单地写为：
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Like the preceding `Layer`s, these inherit from `nn.Module`, so they can be
    called in the same way as `Layer`s.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的`Layer`一样，这些都继承自`nn.Module`，因此可以像调用`Layer`一样调用它们。
- en: Note
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that even though the word *softmax* is not in the name of the `nn.CrossEntropyLoss`
    class, the softmax operation is indeed performed on the inputs, so that we can
    pass in “raw outputs” from the neural network rather than outputs that have already
    passed through the softmax function, just as we did before.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，即使`nn.CrossEntropyLoss`类的名称中没有*softmax*一词，也确实对输入执行了softmax操作，因此我们可以传入神经网络的“原始输出”，而不是已经通过softmax函数的输出，就像我们以前做的那样。
- en: These `Loss`es inherit from `nn.Module`, just like the `Layer`s from earlier,
    so they can be called the same way, using `loss(x)` instead of `loss.forward(x)`,
    for example.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这些`Loss`也继承自`nn.Module`，就像之前的`Layer`一样，因此可以使用相同的方式调用，例如使用`loss(x)`而不是`loss.forward(x)`。
- en: 'PyTorch Elements: Trainer'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch元素：Trainer
- en: 'The `Trainer` pulls all of these elements together. Let’s consider the requirements
    for the `Trainer`. We know that it has to implement the general pattern for training
    neural networks that we’ve seen many times throughout this book:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`Trainer`将所有这些元素汇集在一起。让我们考虑`Trainer`的要求。我们知道它必须实现我们在本书中多次看到的训练神经网络的一般模式：'
- en: Feed a batch of inputs through the model.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过模型传递一批输入。
- en: Feed the outputs and targets into a loss function to compute a loss value.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出和目标输入到损失函数中以计算损失值。
- en: Compute the gradient of the loss with respect to all of the parameters.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失相对于所有参数的梯度。
- en: Use the `Optimizer` to update the parameters according to some rule.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`Optimizer`根据某种规则更新参数。
- en: 'With PyTorch, this all works the same way, except there are two small implementation
    caveats:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyTorch，这一切都是一样的，只是有两个小的实现注意事项：
- en: By default, `Optimizer`s will retain the gradients of the parameters (what we
    referred to as `param_grads` earlier in the book) after each iteration of a parameter
    update. To clear these gradients before the next parameter update, we’ll call
    `self.optim.zero_grad`.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，`Optimizer`将在每次参数更新迭代后保留参数的梯度（在本书中我们称之为`param_grads`）。在下一次参数更新之前清除这些梯度，我们将调用`self.optim.zero_grad`。
- en: As illustrated previously in the simple automatic differentiation example, to
    kick off the backpropagation, we’ll have to call `loss.backward` after computing
    the loss value.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如在简单的自动微分示例中所示，为了启动反向传播，我们在计算损失值后必须调用`loss.backward`。
- en: 'This leads to the following sequence of code that is seen throughout PyTorch
    training loops, and will in fact be used in the `PyTorchTrainer` class. As the
    `Trainer` class from prior chapters did, `PyTorchTrainer` will take in an `Optimizer`,
    a `PyTorchModel`, and a `Loss` (either `nn.MSELoss` or `nn.CrossEntropyLoss`)
    for a batch of data `(X_batch, y_batch)`; with these objects in place as `self.optim`,
    `self.model`, and `self.loss`, respectively, the following five lines of code
    train the model:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了在PyTorch训练循环中看到的以下代码序列，实际上将在`PyTorchTrainer`类中使用。与之前章节中的`Trainer`类一样，`PyTorchTrainer`将接收一个`Optimizer`，一个`PyTorchModel`和一个`Loss`（可以是`nn.MSELoss`或`nn.CrossEntropyLoss`）用于数据批次`(X_batch,
    y_batch)`；将这些对象放置为`self.optim`，`self.model`和`self.loss`，以下五行代码训练模型：
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Those are the most important lines; still, here’s the rest of the code for
    the `PyTorchTrainer`, much of which is similar to the code for the `Trainer` that
    we saw in prior chapters:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是最重要的行；但是，这是`PyTorchTrainer`的其余代码，其中许多与我们在之前章节中看到的`Trainer`的代码相似：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Since we’re passing a `Model`, an `Optimizer`, and a `Loss` into the `Trainer`,
    we need to check that the parameters that the `Optimizer` refers to are in fact
    the same as the model’s parameters; `_check_optim_net_aligned` does this.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将`Model`，`Optimizer`和`Loss`传入`Trainer`，我们需要检查`Optimizer`引用的参数实际上是否与模型的参数相同；`_check_optim_net_aligned`会执行此操作。
- en: 'Now training the model is as simple as:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在训练模型就像这样简单：
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This code is nearly identical to the code we used to train models using the
    framework we built in the first three chapters. Whether you’re using PyTorch,
    TensorFlow, or Theano under the hood, the elements of training a deep learning
    model remain the same!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码几乎与我们在前三章中使用的训练模型的代码完全相同。无论您使用PyTorch、TensorFlow还是Theano作为底层，训练深度学习模型的要素都是相同的！
- en: Next, we’ll explore more features of PyTorch by showing how to implement the
    tricks to improve training that we saw in [Chapter 4](ch04.html#extensions).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过展示如何实现我们在[第4章]中看到的改进训练的技巧来探索PyTorch的更多特性。
- en: Tricks to Optimize Learning in PyTorch
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在PyTorch中优化学习的技巧
- en: 'We learned four tricks to accelerate learning in [Chapter 4](ch04.html#extensions):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到了四个加速学习的技巧[第4章]：
- en: Momentum
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动量
- en: Dropout
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout
- en: Weight initialization
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重初始化
- en: Learning rate decay
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率衰减
- en: 'These are all easy to implement in PyTorch. For example, to include momentum
    in our optimizer, we can simply include a `momentum` keyword in `SGD`, so that
    the optimizer becomes:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些在PyTorch中都很容易实现。例如，要在我们的优化器中包含动量，我们可以简单地在`SGD`中包含一个`momentum`关键字，使得优化器变为：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Dropout is similarly easy. Just as PyTorch has a built-in `Module` `nn.Linear(n_in,
    n_out)` that computes the operations of a `Dense` layer from before, the `Module`
    `nn.Dropout(dropout_prob)` implements the `Dropout` operation, with the caveat
    that the probability passed in is by default the probability of *dropping* a given
    neuron, rather than keeping it as it was in our implementation from before.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout同样很容易。就像PyTorch有一个内置的`Module` `nn.Linear(n_in, n_out)`来计算之前的`Dense`层的操作一样，`Module`
    `nn.Dropout(dropout_prob)`实现了`Dropout`操作，但传入的概率默认情况下是*丢弃*给定神经元的概率，而不是像之前我们的实现中那样保留它。
- en: 'We don’t need to worry about weight initialization at all: the weights in most
    PyTorch operations involving parameters, including `nn.Linear`, are automatically
    scaled based on the size of the layer.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根本不需要担心权重初始化：PyTorch中大多数涉及参数的操作，包括`nn.Linear`，其权重会根据层的大小自动缩放。
- en: Finally, PyTorch has an `lr_scheduler` class that can be used to decay the learning
    rate over the epochs. The key import you need to get started is from `torch.optim
    import lr_scheduler`.^([2](ch07.html#idm45732608539560)) Now you can easily use
    these techniques we covered from first principles in any future deep learning
    project you work on!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，PyTorch有一个`lr_scheduler`类，可以用来在各个epoch中衰减学习率。你需要开始的关键导入是`from torch.optim
    import lr_scheduler`。现在你可以轻松地在任何未来的深度学习项目中使用我们从头开始介绍的这些技术！
- en: Convolutional Neural Networks in PyTorch
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch中的卷积神经网络
- en: 'In [Chapter 5](ch05.html#convolution), we systematically covered how convolutional
    neural networks work, focusing in particular on the multichannel convolution operation.
    We saw that the operation transforms the pixels of input images into layers of
    neurons organized into feature maps, where each neuron represents whether a given
    visual feature (defined by a convolutional filter) is present at that location
    in the image. The multichannel convolution operation had the following shapes
    for its two inputs and its output:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#convolution)中，我们系统地介绍了卷积神经网络的工作原理，特别关注多通道卷积操作。我们看到该操作将输入图像的像素转换为组织成特征图的神经元层，其中每个神经元表示图像中该位置是否存在给定的视觉特征（由卷积滤波器定义）。多通道卷积操作对其两个输入和输出具有以下形状：
- en: The data input shape `[batch_size, in_channels, image_height, image_width]`
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据输入形状`[batch_size, in_channels, image_height, image_width]`
- en: The parameters input shape `[in_channels, out_channels, filter_size, filter_size]`
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数输入形状`[in_channels, out_channels, filter_size, filter_size]`
- en: The output shape `[batch_size, out_channels, image_height, image_width]`
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出形状`[batch_size, out_channels, image_height, image_width]`
- en: 'In terms of this notation, the multichannel convolution operation in PyTorch
    is:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这种表示法，PyTorch中的多通道卷积操作是：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'With this defined, wrapping a `ConvLayer` around this operation is straightforward:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个定义，将`ConvLayer`包装在这个操作周围就很简单了：
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In [Chapter 5](ch05.html#convolution), we automatically padded the output based
    on the filter size to keep the output image the same size as the input image.
    PyTorch does not do that; to achieve the same behavior we had before, we add an
    argument to the `nn.Conv2d` operation setting `padding` `= filter_size // 2`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#convolution)中，我们根据滤波器大小自动填充输出，以保持输出图像与输入图像相同大小。PyTorch不会这样做；为了实现之前的相同行为，我们在`nn.Conv2d`操作中添加一个参数`padding
    = filter_size // 2`。
- en: 'From there, all we have to do is define a `PyTorchModel` with its operations
    in the `__init__` function and the sequence of operations defined in the `forward`
    function to begin to train. Next is a simple architecture we can use on the MNIST
    dataset we saw in Chapters [4](ch04.html#extensions) and [5](ch05.html#convolution),
    with:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们只需在`__init__`函数中定义一个`PyTorchModel`及其操作，并在`forward`函数中定义操作序列，即可开始训练。下面是一个简单的架构，我们可以在第[4](ch04.html#extensions)章和第[5](ch05.html#convolution)章中看到的MNIST数据集上使用：
- en: A convolutional layer that transforms the input from 1 “channel” to 16 channels
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个将输入从1个“通道”转换为16个通道的卷积层
- en: Another layer that transforms these 16 channels into 8 (with each channel still
    containing 28 × 28 neurons)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个层，将这16个通道转换为8个（每个通道仍然包含28×28个神经元）
- en: Two fully connected layers
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个全连接层
- en: 'The pattern of several convolutional layers followed by a smaller number of
    fully connected layers is common for convolutional architectures; here, we just
    use two of each:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 几个卷积层后跟少量全连接层的模式对于卷积架构是常见的；在这里，我们只使用了两个：
- en: '[PRE21]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then we can train this model the same way we trained the `HousePricesModel`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以像训练`HousePricesModel`一样训练这个模型：
- en: '[PRE22]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: There is an important caveat related to the `nn.CrossEntropyLoss` class. Recall
    that in the custom framework from previous chapters, our `Loss` class expected
    an input of the same shape as the target. To get this, we one-hot encoded the
    10 distinct values of the target in the MNIST data so that, for each batch of
    data, the target had shape `[batch_size, 10]`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 与`nn.CrossEntropyLoss`类相关的一个重要注意事项。回想一下，在前几章的自定义框架中，我们的`Loss`类期望输入与目标的形状相同。为了实现这一点，我们对MNIST数据中目标的10个不同值进行了独热编码，以便对于每批数据，目标的形状为`[batch_size,
    10]`。
- en: 'With PyTorch’s `nn.CrossEntropyLoss` class—which works exactly the same as
    our `SoftmaxCrossEntropyLoss` from before—we don’t have to do that. This loss
    function expects two `Tensor`s:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyTorch的`nn.CrossEntropyLoss`类——其与我们之前的`SoftmaxCrossEntropyLoss`完全相同——我们不需要这样做。这个损失函数期望两个`Tensor`：
- en: A prediction `Tensor` of size `[batch_size, num_classes]`, just as our `SoftmaxCrossEntropyLoss`
    class did before
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个大小为`[batch_size, num_classes]`的预测`Tensor`，就像我们的`SoftmaxCrossEntropyLoss`类之前所做的那样
- en: A target `Tensor` of size `[batch_size]` with `num_classes` different values
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个大小为`[batch_size]`的目标`Tensor`，具有`num_classes`个不同的值
- en: So in the preceding example, `y_train` is simply an array of size `[60000]`
    (the number of observations in the training set of MNIST), and `y_test` simply
    has size `[10000]` (the number of observations in the test set).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在前面的示例中，`y_train`只是一个大小为`[60000]`的数组（MNIST训练集中的观测数量），而`y_test`只是大小为`[10000]`的数组（测试集中的观测数量）。
- en: 'Now that we’re dealing with larger datasets, we should cover another best practice.
    It is clearly very memory inefficient to load the entire training and testing
    sets into memory to train the model, as we’re doing with `X_train`, `y_train`,
    `X_test`, and `y_test`. PyTorch has a way around this: the `DataLoader` class.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们正在处理更大的数据集，我们应该涵盖另一个最佳实践。将整个训练和测试集加载到内存中训练模型显然非常低效，就像我们现在使用的`X_train`、`y_train`、`X_test`和`y_test`一样。PyTorch有一种解决方法：`DataLoader`类。
- en: DataLoader and Transforms
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataLoader和Transforms
- en: 'Recall that in our MNIST modeling in [Chapter 2](ch02.html#fundamentals), we
    applied a simple preprocessing step to the MNIST data, subtracting off the global
    mean and dividing by the global standard deviation to roughly “normalize” the
    data:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在[第2章](ch02.html#fundamentals)中的MNIST建模中，我们对MNIST数据应用了一个简单的预处理步骤，减去全局均值并除以全局标准差，以粗略“规范化”数据：
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Still, this required us to first fully read these two arrays into memory; it
    would be much more efficient to perform this preprocessing on the fly, as batches
    are fed into the neural network. PyTorch has built-in functions that do this,
    and they are especially commonly used with image data—transformations via the
    `transforms` module, and a `DataLoader` via `torch.utils.data`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这要求我们首先完全将这两个数组读入内存；在将批次馈送到神经网络时，执行此预处理将更加高效。PyTorch具有内置函数来执行此操作，特别是在处理图像数据时经常使用——通过`transforms`模块进行转换，以及通过`torch.utils.data`进行`DataLoader`：
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Previously, we read in the entire training set into `X_train` via:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们通过以下方式将整个训练集读入`X_train`中：
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We then performed transformations on `X_train` to get it to a form where it
    was ready for modeling.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对`X_train`执行转换，使其准备好进行建模。
- en: PyTorch has some convenience functions that allow us to compose many transformations
    to each batch of data as it is read in; this allows us both to avoid reading the
    entire dataset into memory and to use PyTorch’s transformations.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch有一些方便的函数，允许我们将许多转换组合到每个读入的数据批次中；这使我们既可以避免将整个数据集读入内存，又可以使用PyTorch的转换。
- en: 'We first define a list of transformations to perform on each batch of data
    read in. For example, the following transformations convert each MNIST image to
    a `Tensor` (most PyTorch datasets are “PIL images” by default, so `transforms.ToTensor()`
    is often the first transformation in the list), and then “normalize” the dataset—subtracting
    off the mean and then dividing by the standard deviation—using the overall MNIST
    mean and standard deviation of `0.1305` and `0.3081`, respectively:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义要对读入的每批数据执行的转换列表。例如，以下转换将每个MNIST图像转换为`Tensor`（大多数PyTorch数据集默认为“PIL图像”，因此`transforms.ToTensor()`通常是列表中的第一个转换），然后使用整体MNIST均值和标准差`0.1305`和`0.3081`对数据集进行“规范化”——先减去均值，然后除以标准差：
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '`Normalize` actually subtracts the mean and standard deviation *from each channel*
    of the input image. Thus, it is common when dealing with color images with three
    input channels to have a `Normalize` transformation that has two tuples of three
    numbers each—for example, `transforms.Normalize((0.1, 0.3, 0.6), (0.4, 0.2, 0.5))`,
    which would tell the `DataLoader` to:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`Normalize`实际上是从输入图像的每个通道中减去均值和标准差。因此，当处理具有三个输入通道的彩色图像时，通常会有一个`Normalize`转换，其中包含两个三个数字的元组，例如`transforms.Normalize((0.1,
    0.3, 0.6), (0.4, 0.2, 0.5))`，这将告诉`DataLoader`：'
- en: Normalize the first channel using a mean of 0.1 and a standard deviation of
    0.4
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用均值为0.1和标准差为0.4来规范化第一个通道
- en: Normalize the second channel using a mean of 0.3 and a standard deviation of
    0.2
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用均值为0.3和标准差为0.2来规范化第二个通道
- en: Normalize the third channel using a mean of 0.6 and a standard deviation of
    0.5
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用均值为0.6和标准差为0.5来规范化第三个通道
- en: 'Second, once these transformations have been applied, we apply these to the
    `dataset` as we read in batches:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，一旦应用了这些转换，我们将其应用于读入的批次的`dataset`：
- en: '[PRE27]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we can define a `DataLoader` that takes in this dataset and defines
    rules for successively generating batches of data:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以定义一个`DataLoader`，它接收这个数据集并定义了连续生成数据批次的规则：
- en: '[PRE28]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can then modify the `Trainer` to use the `dataloader` to generate the batches
    used to train the network instead of loading the entire dataset into memory and
    then manually generating them using the `batch_generator` function, as we did
    before. On [the book’s website](https://oreil.ly/2N4H8jz),^([3](ch07.html#idm45732607595080))
    I show an example of training a convolutional neural network using these `DataLoader`s.
    The main change in the `Trainer` is simply changing the line:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以修改`Trainer`以使用`dataloader`生成用于训练网络的批次，而不是将整个数据集加载到内存中，然后手动使用`batch_generator`函数生成它们，就像我们之前做的那样。在[书的网站](https://oreil.ly/2N4H8jz)上，我展示了使用这些`DataLoader`训练卷积神经网络的示例。`Trainer`中的主要变化只是改变了这一行：
- en: '[PRE29]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'to:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 到：
- en: '[PRE30]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In addition, instead of feeding in the entire training set into the `fit` function,
    we now feed in `DataLoader`s:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们现在不再将整个训练集馈送到`fit`函数中，而是馈送`DataLoader`：
- en: '[PRE31]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Using this architecture and calling the `fit` method, as we just did, gets us
    to about 97% accuracy on MNIST after one epoch. More important than the accuracy,
    however, is that you’ve seen how to implement the concepts we reasoned through
    from first principles into a high-performance framework. Now that you understand
    both the underlying concepts and the framework, I encourage you to modify the
    code in [the book’s GitHub repo](https://oreil.ly/2N4H8jz) and try out other convolutional
    architectures, other datasets, and so on.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种架构并调用`fit`方法，就像我们刚刚做的那样，在一个epoch后我们可以达到大约97%的MNIST准确率。然而，比准确率更重要的是，您已经看到了如何将我们从第一原则推理出的概念实现到高性能框架中。现在您既了解了基本概念又了解了框架，我鼓励您修改[书的GitHub存储库](https://oreil.ly/2N4H8jz)中的代码，并尝试其他卷积架构、其他数据集等。
- en: CNNs were one of two advanced architectures we covered earlier in the book;
    let’s now turn to the other one and show how to implement the most advanced RNN
    variant we’ve covered, LSTMs, in PyTorch.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: CNN是我们在本书中之前介绍的两种高级架构之一；现在让我们转向另一种，并展示如何在PyTorch中实现我们介绍过的最先进的RNN变体之一，即LSTMs。
- en: LSTMs in PyTorch
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch中的LSTMs
- en: We saw in the last chapter how to code LSTMs from scratch. We coded an `LSTMLayer`
    to take in an input `ndarray` of size [`batch_size`, `sequence_length`, `feature_size`],
    and output an `ndarray` of size [`batch_size`, `sequence_length`, `feature_size`].
    In addition, each layer took in a hidden state and a cell state, each initialized
    with shape `[1, hidden_size]`, expanded to shape `[batch_size, hidden_size]` when
    a batch is passed in, and then collapsed back down to `[1, hidden_size]` after
    the iteration is complete.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了如何从头开始编写LSTMs。我们编写了一个`LSTMLayer`，接受大小为[`batch_size`, `sequence_length`,
    `feature_size`]的输入`ndarray`，并输出大小为[`batch_size`, `sequence_length`, `feature_size`]的`ndarray`。此外，每个层接受一个隐藏状态和一个单元状态，每个状态初始化为形状`[1,
    hidden_size]`，当传入一个批次时，扩展为形状`[batch_size, hidden_size]`，然后在迭代完成后缩小回`[1, hidden_size]`。
- en: 'Based on this, we define the `__init__` method for our `LSTMLayer` as:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一点，我们为我们的`LSTMLayer`定义`__init__`方法如下：
- en: '[PRE32]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As with convolutional layers, PyTorch has an `nn.lstm` operation for implementing
    LSTMs. Note that in our custom `LSTMLayer` we store a `DenseLayer` in the `self.fc`
    attribute. You may recall from the last chapter that the last step of an LSTM
    cell is putting the final hidden state through the operations of a `Dense` layer
    (a weight multiplication and addition of a bias) to transform the hidden state
    into dimension `output_size` for each operation. PyTorch does things a bit differently:
    the `nn.lstm` operation simply outputs the hidden states for each time step. Thus,
    to enable our `LSTMLayer` to output a different dimension than its input—as we
    would want all of our neural network layers to be able to do—we add a `DenseLayer`
    at the end to transform the hidden state into dimension `output_size`.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 与卷积层一样，PyTorch有一个`nn.lstm`操作用于实现LSTMs。请注意，在我们自定义的`LSTMLayer`中，我们将一个`DenseLayer`存储在`self.fc`属性中。您可能还记得上一章中，LSTM单元的最后一步是将最终隐藏状态通过`Dense`层的操作（权重乘法和偏置加法）转换为每个操作的维度`output_size`。PyTorch的做法有点不同：`nn.lstm`操作只是简单地输出每个时间步的隐藏状态。因此，为了使我们的`LSTMLayer`能够输出与其输入不同的维度
    - 正如我们希望所有的神经网络层都能够做到的那样 - 我们在最后添加一个`DenseLayer`来将隐藏状态转换为维度`output_size`。
- en: 'With this modification, the `forward` function is now straightforward, looking
    similar to the `forward` function of the `LSTMLayer` from [Chapter 6](ch06.html#recurrent):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种修改，`forward`函数现在变得简单明了，看起来与[第6章](ch06.html#recurrent)中的`LSTMLayer`的`forward`函数相似：
- en: '[PRE33]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The key line here, which should look familiar given our implementation of LSTMs
    in [Chapter 6](ch06.html#recurrent), is:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键一行，应该看起来很熟悉，因为我们在[第6章](ch06.html#recurrent)中实现了LSTMs：
- en: '[PRE34]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Aside from that, there’s some reshaping of the hidden and cell states before
    and after the `self.lstm` function via a helper function `self._transform_hidden_batch`.
    You can see the full function in [the book’s GitHub repo](https://oreil.ly/2N4H8jz).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，在`self.lstm`函数之前和之后对隐藏状态和单元状态进行一些重塑，通过一个辅助函数`self._transform_hidden_batch`。您可以在[书的GitHub存储库](https://oreil.ly/2N4H8jz)中看到完整的函数。
- en: 'Finally, wrapping a model around this is easy:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将模型包装起来很容易：
- en: '[PRE35]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The `nn.CrossEntropyLoss` function expects the first two dimensions to be the
    `batch_size` and the distribution over the classes; the way we’ve been implementing
    our LSTMs, however, we have the distribution over the classes as the last dimension
    (`vocab_size`) coming out of the `LSTMLayer`. To prepare the final model output
    to be fed into the loss, therefore, we move the dimension containing the distribution
    over letters to the second dimension using `out.permute(0, 2, 1)`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.CrossEntropyLoss`函数期望前两个维度是`batch_size`和类别分布；然而，我们一直在实现LSTMs时，类别分布作为最后一个维度（`vocab_size`）从`LSTMLayer`中出来。因此，为了准备最终模型输出以输入损失，我们使用`out.permute(0,
    2, 1)`将包含字母分布的维度移动到第二维度。'
- en: 'Finally, in [the book’s GitHub repo](https://oreil.ly/2N4H8jz), I show how
    to write a class `LSTMTrainer` to inherit from `PyTorchTrainer` and use it to
    train a `NextCharacterModel` to generate text. We use the same text preprocessing
    that we did in [Chapter 6](ch06.html#recurrent): selecting sequences of text,
    one-hot encoding the letters, and grouping the sequences of one-hot encoded letters
    into batches.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在[书的GitHub存储库](https://oreil.ly/2N4H8jz)中，我展示了如何编写一个从`PyTorchTrainer`继承的`LSTMTrainer`类，并使用它来训练`NextCharacterModel`生成文本。我们使用了与[第6章](ch06.html#recurrent)中相同的文本预处理：选择文本序列，对字母进行独热编码，将独热编码的文本序列分组成批次。
- en: 'That wraps up how to translate the three neural network architectures for supervised
    learning we saw in this book—fully connected neural networks, convolutional neural
    networks, and recurrent neural networks—into PyTorch. To conclude, we’ll briefly
    cover how neural networks can be used for the other half of machine learning:
    *un*-supervised learning.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是如何将本书中看到的三种用于监督学习的神经网络架构 - 全连接神经网络、卷积神经网络和循环神经网络 - 转换为PyTorch。最后，我们将简要介绍神经网络如何用于机器学习的另一半：*非*监督学习。
- en: 'Postscript: Unsupervised Learning via Autoencoders'
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附言：通过自动编码器进行无监督学习
- en: 'Throughout this book we’ve focused on how deep learning models can be used
    to solve *supervised* learning problems. There is, of course, a whole other side
    to machine learning: unsupervised learning; which involves what is often described
    as “finding structure in data without labels”; I like to think of it, though,
    as finding relationships between characteristics in your data that have not yet
    been measured, whereas supervised learning involves finding relationships between
    characteristics in your data that have already been measured.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们一直专注于深度学习模型如何用于解决*监督*学习问题。当然，机器学习还有另一面：无监督学习；这通常被描述为“在没有标签的数据中找到结构”;
    我更喜欢将其看作是在数据中找到尚未被测量的特征之间的关系，而监督学习涉及在数据中找到已经被测量的特征之间的关系。
- en: 'Suppose you had a dataset of images with no labels. You don’t know much about
    these images—for example, you’re not sure whether there are 10 distinct digits
    represented, or 5, or 20 (these images could be from a strange alphabet)—and you
    want to know the answers to questions like:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个没有标签的图像数据集。你对这些图像了解不多——例如，你不确定是否有10个不同的数字，或者5个，或者20个（这些图像可能来自一个奇怪的字母表）——你想知道这样的问题的答案：
- en: How many distinct digits are there?
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有多少个不同的数字？
- en: Which digits are visually similar to one another?
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些数字在视觉上相似？
- en: Are there “outlier” images that are distinctly *dis*similar to other images?
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有与其他图像明显*不*相似的“异常”图像？
- en: To understand how deep learning can help with this, we’ll have to take a quick
    step back and think conceptually about what deep learning models are trying to
    do.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解深度学习如何帮助解决这个问题，我们需要快速退后一步，从概念上思考深度学习模型试图做什么。
- en: Representation Learning
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示学习
- en: We’ve seen that deep learning models can learn to make accurate predictions.
    They do this by transforming the input they receive into representations that
    are progressively both more abstract and more tuned to directly making predictions
    for whatever the relevant problem is. In particular, the final layer of the network,
    directly before the layer with the predictions themselves (which would have just
    one neuron for a regression problem and *`num_classes`* neurons for a classification
    problem), is the network’s attempt at creating a representation of the input data
    that is as useful as possible for the task of making predictions. This is shown
    in [Figure 7-1](#fig_07_01).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到深度学习模型可以学习进行准确的预测。它们通过将接收到的输入转换为逐渐更抽象且更直接用于解决相关问题的表示来实现这一点。特别是，在网络的最终层，直接在具有预测本身的层之前（对于回归问题只有一个神经元，对于分类问题有*`num_classes`*个神经元），网络试图创建一个对于预测任务尽可能有用的输入数据表示。这在[图7-1](#fig_07_01)中显示。
- en: '![](assets/dlfs_0701.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfs_0701.png)'
- en: Figure 7-1\. The final layer of a neural network, immediately before the predictions,
    represents the network’s representation of the input that it has found most useful
    to the task of predicting
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1\. 神经网络的最终层，在预测之前，表示网络对于预测任务发现的输入的表示
- en: Once trained, then, a model can not only make predictions for new data points,
    *but also generate representations of these data points*. These could then be
    used for clustering, similarity analysis, or outlier detection—in addition to
    prediction.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，模型不仅可以为新数据点做出预测，*还可以生成这些数据点的表示*。这些表示可以用于聚类、相似性分析或异常检测，除了预测。
- en: An Approach for Situations with No Labels Whatsoever
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种没有任何标签的情况下的方法
- en: 'A limitation with this whole approach is that it *requires labels to train
    the model to generate the representations in the first place*. The question is:
    how can we train a model to generate “useful” representations without any labels?
    If we don’t have labels, we need to generate representations of our data using
    the only thing we do have: the training data itself. This is the idea behind a
    class of neural network architectures known as autoencoders, which involve training
    neural networks to *reconstruct* the training data, forcing the network to learn
    the representation of each data point most helpful for this reconstruction.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这种整体方法的一个限制是*需要标签来训练模型首先生成表示*。问题是：如何训练模型生成“有用”的表示而没有任何标签？如果没有标签，我们需要使用唯一拥有的东西——训练数据本身——来生成数据的表示。这就是一类被称为自动编码器的神经网络架构的理念，它涉及训练神经网络*重建*训练数据，迫使网络学习对于这种重建最有帮助的每个数据点的表示。
- en: Diagram
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图表
- en: '[Figure 7-2](#fig_07_02) shows a high-level overview of an autoencoder:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-2](#fig_07_02)展示了自动编码器的高级概述：'
- en: One set of layers transforms the data into a compressed representation of the
    data.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一组层将数据转换为数据的压缩表示。
- en: Another set of layers transforms this representation into an output of the same
    size and shape as the original data.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一组层将这个表示转换为与原始数据相同大小和形状的输出。
- en: '![](assets/dlfs_0702.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfs_0702.png)'
- en: Figure 7-2\. An autoencoder has one set of layers (which can be thought of as
    the “encoder” network) that maps the input to a lower-dimensional representation,
    and another set of layers (which can be thought of as the “decoder” network) that
    maps the lower-dimensional representation back to the input; this structure forces
    the network to learn a lower-dimensional representation that is most useful for
    reconstructing the input
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2\. 自动编码器有一组层（可以被视为“编码器”网络），将输入映射到一个低维表示，另一组层（可以被视为“解码器”网络）将低维表示映射回输入；这种结构迫使网络学习一个对于重建输入最有用的低维表示
- en: Implementing such an architecture illustrates some features of PyTorch we haven’t
    had a chance to introduce yet.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这样的架构展示了一些我们还没有机会介绍的PyTorch特性。
- en: Implementing an Autoencoder in PyTorch
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在PyTorch中实现自动编码器
- en: 'We’ll now show a simple autoencoder that takes in an input image, feeds it
    through two convolutional layers and then a `Dense` layer to generate a representation,
    and then feeds this representation back through a `Dense` layer and two convolutional
    layers to generate an output of the same size as the input. We’ll use this to
    illustrate two common practices when implementing more advanced architectures
    in PyTorch. First, we can include `PyTorchModel`s as attributes of another `PyTorchModel`,
    just as we defined `PyTorchLayer`s as attributes of such models previously. In
    the following example, we’ll implement our autoencoder as having two `PyTorchModel`s
    as attributes: an `Encoder` and a `Decoder`. Once we train the model, we’ll be
    able to use the trained `Encoder` as its own model to generate the representations.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the `Encoder` as:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'And we define the `Decoder` as:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Note
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we were using a stride greater than 1, we wouldn’t simply be able to use
    a regular convolution to transform the encoding into an output, as we do here,
    but instead would have to use a *transposed convolution*, where the image size
    of the output of the operation would be larger than the image size of the input.
    See the `nn.ConvTranspose2d` operation in the [PyTorch documentation](https://oreil.ly/306qiV7)
    for more.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the `Autoencoder` itself can wrap around these and become:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The `forward` method of the `Autoencoder` illustrates a second common practice
    in PyTorch: since we’ll ultimately want to see the hidden representation that
    the model produces, the `forward` method returns *two* elements: this “encoding,”
    `encoding`, along with the output that will be used to train the network, `x`.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, we would have to modify our `Trainer` class to accommodate this;
    specifically, `PyTorchModel` as currently written outputs only a single `Tensor`
    from its `forward` method. As it turns out, modifying it so that it returns a
    `Tuple` of `Tensor`s by default, even if that `Tuple` is only of length 1, will
    both be useful—enabling us to easily write models like the `Autoencoder`—and not
    difficult. All we have to do is three small things: first, make the function signature
    of the `forward` method of our base `PyTorchModel` class:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Then, at the end of the `forward` method of any model that inherits from the
    `PyTorchModel` base class, we’ll write `return x,` instead of `return x` as we
    were doing before.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, we’ll modify our `Trainer` to always take as output the first element
    of whatever the model returns:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'There is one other notable feature of the `Autoencoder` model: we apply a `Tanh`
    activation function to the last layer, meaning the model output will be between
    –1 and 1\. With any model, the model outputs should be on the same scale as the
    target they are compared to, and here, the target is our input itself. So we should
    scale our input to range from a minimum of –1 and a maximum of 1, as in the following
    code:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Finally, we can train our model using training code, which by now should look
    familiar (we somewhat arbitrarily use 28 as the dimensionality of the output of
    the encoding):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Once we run this code and train the model, we can look at both the reconstructed
    images and the image representations simply by passing `X_test_auto` through the
    model (since the `forward` method was defined to return two quantities):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Each element of `reconstructed_images` is a `[1, 28, 28]` `Tensor` and represents
    the neural network’s best attempt to reconstruct the corresponding original image
    after passing it through an autoencoder architecture that forced the image through
    a layer with lower dimensionality. [Figure 7-3](#fig_07_03) shows a randomly chosen
    reconstructed image alongside the original image.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfs_0703.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. An image from the MNIST test set alongside the reconstruction of
    that image after it was fed through the autoencoder
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Visually, the images look similar, telling us that the neural network does
    indeed seem to have taken the original images, which were 784 pixels, and mapped
    them to a space of lower dimensionality—specifically, 28—such that most of the
    information about the 784-pixel image is encoded in this vector of length 28\.
    How can we examine the whole dataset to see whether the neural network has indeed
    learned the structure of the image data without seeing the labels? Well, “the
    structure of the data” here means that the underlying data is in fact images of
    10 distinct handwritten digits. Thus, images close to a given image in the new
    28-dimensional space should ideally be of the same digit, or at least visually
    be very similar, since visual similarity is how we as humans distinguish between
    different images. We can test whether this is the case by applying a dimensionality
    reduction technique invented by Laurens van der Maaten when he was a graduate
    student under Geoffrey Hinton (who was one of the “founding fathers” of neural
    networks): *t-Distributed Stochastic Neighbor Embedding*, or t-SNE. t-SNE performs
    its dimensionality reduction in a way that is analogous to how neural networks
    are trained: it starts with an initial lower-dimensional representation and then
    updates it so that, over time, it approaches a solution with the property that
    points that are “close together” in the high-dimensional space are “close together”
    in the low-dimensional space, and vice versa.^([4](ch07.html#idm45732606124216))'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上看，这些图像看起来相似，告诉我们神经网络确实似乎已经将原始图像（784像素）映射到了一个较低维度的空间——具体来说是28——这样大部分关于784像素图像的信息都被编码在这个长度为28的向量中。我们如何检查整个数据集，以查看神经网络是否确实学习了图像数据的结构而没有看到标签呢？嗯，“数据的结构”在这里意味着底层数据实际上是10个不同手写数字的图像。因此，在新的28维空间中，接近给定图像的图像理想情况下应该是相同的数字，或者至少在视觉上非常相似，因为视觉相似性是我们作为人类区分不同图像的方式。我们可以通过应用Laurens
    van der Maaten在Geoffrey Hinton的指导下作为研究生时发明的降维技术*t-分布随机邻居嵌入*（t-SNE）来测试是否符合这种情况。t-SNE以类似于神经网络训练的方式进行降维：它从一个初始的低维表示开始，然后更新它，以便随着时间的推移，它接近具有这样的属性的解决方案，即在高维空间中“靠在一起”的点在低维空间中也是“靠在一起”，反之亦然。
- en: 'We’ll try the following:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试以下操作：
- en: Feed the 10,000 images through t-SNE and reduce the dimensionality to 2.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这10,000个图像通过t-SNE并将维度降低到2。
- en: Visualize the resulting two-dimensional space, coloring the different points
    by their *actual* label (which the autoencoder did not see).
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将生成的两维空间可视化，通过它们的*实际*标签对不同的点进行着色（自动编码器没有看到）。
- en: '[Figure 7-4](#fig_07_04) shows the result.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-4](#fig_07_04)显示了结果。'
- en: '![](assets/dlfs_0704.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfs_0704.png)'
- en: Figure 7-4\. Result of running t-SNE on 28-dimensional learned space of the
    autoencoder
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4。在自动编码器的28维学习空间上运行t-SNE的结果
- en: 'It appears that images of each digit are largely grouped together in their
    own separate cluster; this shows that training our autoencoder architecture to
    learn to reconstruct the original images from just a lower-dimensional representation
    has indeed enabled it to discover much of the underlying structure of these images
    without seeing any labels.^([5](ch07.html#idm45732606114968)) And not only are
    the 10 digits represented as distinct clusters, but visually similar digits are
    also closer together: at the top and slightly to the right, we have clusters of
    the digits 3, 5, and 8, and at the bottom we see 4 and 9 clustered tightly together,
    with 7 not far away. Finally, the most distinct digits—0, 1, and 6—form the most
    distinct clusters.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎每个数字的图像主要被分组在自己的独立簇中；这表明训练我们的自动编码器架构学习仅从较低维度表示中重建原始图像确实使其能够发现这些图像的底层结构的大部分，而不需要看到任何标签。不仅10个数字被表示为不同的簇，而且视觉上相似的数字也更接近：在顶部稍微向右，我们有数字3、5和8的簇，底部我们看到4和9紧密聚集在一起，7也不远。最后，最不同的数字—0、1和6—形成最不同的簇。
- en: A Stronger Test for Unsupervised Learning, and a Solution
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习的更强测试，以及解决方案
- en: 'What we’ve just seen is a fairly weak test for whether our model has learned
    an underlying structure to the space of input images—by this point, it shouldn’t
    be too surprising that a convolutional neural network can learn representations
    of images of digits with the property that visually similar images have similar
    representations. A stronger test would be to examine if the neural network has
    discovered a “smooth” underlying space: a space in which *any* vector of length
    28, rather than just the vectors resulting from feeding real digits through the
    encoder network, can be mapped to a realistic-looking digit. It turns out that
    our autoencoder cannot do this; [Figure 7-5](#fig_07_05) shows the result of generating
    five random vectors of length 28 and feeding them through the decoder network,
    using the fact that the `Autoencoder` contained a `Decoder` as an attribute:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到的是一个相当弱的测试，用于检查我们的模型是否已经学习了输入图像空间的底层结构——到这一点，一个卷积神经网络可以学习图像的表示，使得视觉上相似的图像具有相似的表示，这应该不会太令人惊讶。一个更强的测试是检查神经网络是否发现了一个“平滑”的底层空间：一个空间，其中*任何*长度为28的向量，而不仅仅是通过编码器网络传递真实数字得到的向量，都可以映射到一个看起来像真实数字的图像。事实证明，我们的自动编码器无法做到这一点；[图7-5](#fig_07_05)显示了生成五个长度为28的随机向量并通过解码器网络传递它们的结果，利用了`Autoencoder`包含`Decoder`作为属性的事实：
- en: '[PRE44]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](assets/dlfs_0705.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfs_0705.png)'
- en: Figure 7-5\. Result of feeding five randomly generated vectors through the decoder
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-5。通过解码器传递五个随机生成的向量的结果
- en: You can see that the resulting images don’t look like digits; thus, while our
    autoencoder can map our data to a lower-dimensional space in a sensible way, it
    doesn’t appear to be able to learn a “smooth” space such as the one described
    a moment ago.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到生成的图像看起来不像数字；因此，虽然我们的自动编码器可以以合理的方式将数据映射到较低维度空间，但似乎无法学习一个像前面描述的“平滑”空间。
- en: Solving the problem, of training a neural network to learn to represent images
    in a training set in a “smooth” underlying space, is one of the major accomplishments
    of *generative adversarial networks* (GANs). Invented in 2014, GANs are most widely
    known for allowing neural networks to generate realistic-looking images via a
    training procedure in which two neural networks are trained simultaneously. GANs
    were truly pushed forward in 2015, however, when researchers used them with deep
    convolutional architectures in both networks not just to generate realistic-looking
    64 × 64 color images of bedrooms but also to generate a large sample of said images
    from randomly generated 100-dimensional vectors.^([6](ch07.html#idm45732606010168))
    This signaled that the neural networks really had learned an underlying representation
    of the “space” of these unlabeled images. GANs deserve a book of their own, so
    we won’t cover them in more detail than this.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 解决问题，即训练神经网络学习在训练集中表示图像的“平滑”基础空间，是*生成对抗网络*（GANs）的主要成就之一。GANs于2014年发明，最广为人知的是通过同时训练两个神经网络的训练过程，使神经网络能够生成看起来逼真的图像。然而，真正推动GANs的发展是在2015年，当研究人员将它们与深度卷积架构一起使用时，不仅生成了看起来逼真的64×64彩色卧室图像，还从随机生成的100维向量中生成了大量这样的图像样本。这表明神经网络确实已经学会了这些未标记图像的“空间”的基本表示。GANs值得有一本专门的书来介绍，所以我们不会详细介绍它们。
- en: Conclusion
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: You now have a deep understanding of the mechanics of some of the most popular
    advanced deep learning architectures out there, as well as how to implement these
    architectures in one of the most popular high-performance deep learning frameworks.
    The only thing stopping you from using deep learning models to solve real-world
    problems is practice. Luckily, it has never been easier to read others’ code and
    quickly get up to speed on the details and implementation tricks that make certain
    model architectures work on certain problems. A list of recommended next steps
    is listed in [the book’s GitHub repo](https://oreil.ly/2N4H8jz).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您对一些最流行的先进深度学习架构的机制有了深入的了解，以及如何在最流行的高性能深度学习框架之一中实现这些架构。阻止您使用深度学习模型解决实际问题的唯一障碍是实践。幸运的是，阅读他人的代码并迅速掌握使某些模型架构在某些问题上起作用的细节和实现技巧从未如此简单。推荐的下一步列表在书的GitHub存储库中列出（https://oreil.ly/2N4H8jz）。
- en: Onward!
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 继续前进！
- en: ^([1](ch07.html#idm45732611298008-marker)) Writing `Layer`s and `Model`s in
    this way isn’t the most common or recommended use of PyTorch; we show it here
    because it most closely maps to the concepts we’ve covered so far. To see a more
    common way to build neural network building blocks with PyTorch, see this [introductory
    tutorial from the official documentation](https://oreil.ly/SKB_V).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式编写`Layer`和`Model`在PyTorch中并不是最常见或推荐的用法；我们在这里展示它，因为它最接近我们迄今为止涵盖的概念。要查看使用PyTorch构建神经网络构建块的更常见方法，请参阅官方文档中的这个入门教程（https://oreil.ly/SKB_V）。
- en: ^([2](ch07.html#idm45732608539560-marker)) In the [book’s GitHub repo](https://oreil.ly/301qxRk),
    you can find an example of code that implements exponential learning rate decay
    as part of a `PyTorchTrainer`. The documentation for the `ExponentialLR` class
    used there can be found on the [PyTorch website](https://oreil.ly/2Mj9IhH).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在书的GitHub存储库中，您可以找到一个实现指数学习率衰减的代码示例，作为`PyTorchTrainer`的一部分。在那里使用的`ExponentialLR`类的文档可以在PyTorch网站上找到（https://oreil.ly/2Mj9IhH）。
- en: ^([3](ch07.html#idm45732607595080-marker)) Look in the “CNNs using PyTorch”
    section.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 查看“使用PyTorch的CNNs”部分。
- en: ^([4](ch07.html#idm45732606124216-marker)) The original 2008 paper is [“Visualizing
    Data using t-SNE”](https://oreil.ly/2KIAaOt), by Laurens van der Maaten and Geoffrey
    Hinton.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 2008年的原始论文是“使用t-SNE可视化数据”（https://oreil.ly/2KIAaOt），由Laurens van der Maaten和Geoffrey
    Hinton撰写。
- en: '^([5](ch07.html#idm45732606114968-marker)) Furthermore, we did this without
    much trying: the architecture here is very straightforward, and we’re not using
    any of the tricks we discussed for training neural networks, such as learning
    rate decay, since we’re training for only one epoch. This illustrates that the
    underlying idea of using an autoencoder-like architecture to learn the structure
    of a dataset without labels is a good one in general and didn’t just “happen to
    work” here.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们做到这一点并没有费多少力气：这里的架构非常简单，我们没有使用我们讨论过的训练神经网络的任何技巧，比如学习率衰减，因为我们只训练了一个周期。这说明使用类似自动编码器的架构来学习数据集的结构而不使用标签的基本想法是一个好主意，而不仅仅是在这里“碰巧起作用”。
- en: ^([6](ch07.html#idm45732606010168-marker)) Check out the DCGAN paper, [“Unsupervised
    Representation Learning with Deep Convolutional Generative Adversarial Networks”](https://arxiv.org/abs/1511.06434)
    by Alec Radford et al., as well as this [PyTorch documentation](https://oreil.ly/2TEspgG).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 查看DCGAN论文，“使用深度卷积生成对抗网络进行无监督表示学习”（https://arxiv.org/abs/1511.06434）由Alec Radford等人撰写，以及这个PyTorch文档（https://oreil.ly/2TEspgG）。
