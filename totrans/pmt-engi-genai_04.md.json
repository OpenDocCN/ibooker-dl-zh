["```py\nfrom langchain_openai.chat_models import ChatOpenAI\nchat = ChatOpenAI(api_key=\"api_key\")\n```", "```py\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\n\nchat = ChatOpenAI(temperature=0.5)\nmessages = [SystemMessage(content='''Act as a senior software engineer\nat a startup company.'''),\nHumanMessage(content='''Please can you provide a funny joke\nabout software engineers?''')]\nresponse = chat.invoke(input=messages)\nprint(response.content)\n```", "```py\nSure, here's a lighthearted joke for you:\nWhy did the software engineer go broke?\nBecause he lost his domain in a bet and couldn't afford to renew it.\n```", "```py\nresponse = chat(messages=messages)\n```", "```py\nfor chunk in chat.stream(messages):\n    print(chunk.content, end=\"\", flush=True)\n```", "```py\n# 2x lists of messages, which is the same as [messages, messages]\nsynchronous_llm_result = chat.batch([messages]*2)\nprint(synchronous_llm_result)\n```", "```py\n[AIMessage(content='''Sure, here's a lighthearted joke for you:\\n\\nWhy did\nthe software engineer go broke?\\n\\nBecause he kept forgetting to Ctrl+ Z\nhis expenses!'''),\nAIMessage(content='''Sure, here\\'s a lighthearted joke for you:\\n\\nWhy do\nsoftware engineers prefer dark mode?\\n\\nBecause it\\'s easier on their\n\"byte\" vision!''')]\n```", "```py\nfrom langchain_core.runnables.config import RunnableConfig\n\n# Create a RunnableConfig with the desired concurrency limit:\nconfig = RunnableConfig(max_concurrency=5)\n\n# Call the .batch() method with the inputs and config:\nresults = chat.batch([messages, messages], config=config)\n```", "```py\nlanguage = \"Python\"\nprompt = f\"What is the best way to learn coding in {language}?\"\nprint(prompt) # What is the best way to learn coding in Python?\n```", "```py\nchain = prompt | model\n```", "```py\nbad_order_chain = model | prompt\n```", "```py\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_core.prompts import (SystemMessagePromptTemplate,\nChatPromptTemplate)\n\ntemplate = \"\"\"\nYou are a creative consultant brainstorming names for businesses.\n\nYou must follow the following principles:\n{principles}\n\nPlease generate a numerical list of five catchy names for a start-up in the\n{industry} industry that deals with {context}?\n\nHere is an example of the format:\n1\\. Name1\n2\\. Name2\n3\\. Name3\n4\\. Name4\n5\\. Name5\n\"\"\"\n\nmodel = ChatOpenAI()\nsystem_prompt = SystemMessagePromptTemplate.from_template(template)\nchat_prompt = ChatPromptTemplate.from_messages([system_prompt])\n\nchain = chat_prompt | model\n\nresult = chain.invoke({\n    \"industry\": \"medical\",\n    \"context\":'''creating AI solutions by automatically summarizing patient\n records''',\n    \"principles\":'''1\\. Each name should be short and easy to\n remember. 2\\. Each name should be easy to pronounce.\n 3\\. Each name should be unique and not already taken by another company.'''\n})\n\nprint(result.content)\n```", "```py\n1\\. SummarAI\n2\\. MediSummar\n3\\. AutoDocs\n4\\. RecordAI\n5\\. SmartSummarize\n```", "```py\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain.prompts.chat import SystemMessagePromptTemplate\nfrom langchain_openai.chat_models import ChatOpenAI\nprompt=PromptTemplate(\n template='''You are a helpful assistant that translates {input_language} to\n {output_language}.''',\n input_variables=[\"input_language\", \"output_language\"],\n)\nsystem_message_prompt = SystemMessagePromptTemplate(prompt=prompt)\nchat = ChatOpenAI()\nchat.invoke(system_message_prompt.format_messages(\ninput_language=\"English\",output_language=\"French\"))\n```", "```py\nAIMessage(content=\"Vous êtes un assistant utile qui traduit l'anglais en\nfrançais.\", additional_kwargs={}, example=False)\n```", "```py\nfrom langchain_core.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain.output_parsers import PydanticOutputParser\nfrom pydantic.v1 import BaseModel, Field\nfrom typing import List\n\ntemperature = 0.0\n\nclass BusinessName(BaseModel):\n    name: str = Field(description=\"The name of the business\")\n    rating_score: float = Field(description='''The rating score of the\n business. 0 is the worst, 10 is the best.''')\n\nclass BusinessNames(BaseModel):\n    names: List[BusinessName] = Field(description='''A list\n of busines names''')\n\n# Set up a parser + inject instructions into the prompt template:\nparser = PydanticOutputParser(pydantic_object=BusinessNames)\n\nprinciples = \"\"\"\n- The name must be easy to remember.\n- Use the {industry} industry and Company context to create an effective name.\n- The name must be easy to pronounce.\n- You must only return the name without any other text or characters.\n- Avoid returning full stops, \\n, or any other characters.\n- The maximum length of the name must be 10 characters.\n\"\"\"\n\n# Chat Model Output Parser:\nmodel = ChatOpenAI()\ntemplate = \"\"\"Generate five business names for a new start-up company in the\n{industry} industry.\nYou must follow the following principles: {principles}\n{format_instructions}\n\"\"\"\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n\n# Creating the LCEL chain:\nprompt_and_model = chat_prompt | model\n\nresult = prompt_and_model.invoke(\n    {\n        \"principles\": principles,\n        \"industry\": \"Data Science\",\n        \"format_instructions\": parser.get_format_instructions(),\n    }\n)\n# The output parser, parses the LLM response into a Pydantic object:\nprint(parser.parse(result.content))\n```", "```py\nnames=[BusinessName(name='DataWiz', rating_score=8.5),\nBusinessName(name='InsightIQ',\nrating_score=9.2), BusinessName(name='AnalytiQ', rating_score=7.8),\nBusinessName(name='SciData', rating_score=8.1),\nBusinessName(name='InfoMax', rating_score=9.5)]\n```", "```py\nchain = prompt | model | output_parser\n```", "```py\nparser = PydanticOutputParser(pydantic_object=BusinessNames)\nchain = chat_prompt | model | parser\n\nresult = chain.invoke(\n    {\n        \"principles\": principles,\n        \"industry\": \"Data Science\",\n        \"format_instructions\": parser.get_format_instructions(),\n    }\n)\nprint(result)\n```", "```py\nnames=[BusinessName(name='DataTech', rating_score=9.5),...]\n```", "```py\nimport os\nfrom langchain_mistralai.chat_models import ChatMistralAI\nfrom langchain.output_parsers import PydanticOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom pydantic.v1 import BaseModel\nfrom typing import Literal, Union\nfrom langchain_core.output_parsers import StrOutputParser\n\n# 1\\. Define the model:\nmistral_api_key = os.environ[\"MISTRAL_API_KEY\"]\n\nmodel = ChatMistralAI(model=\"mistral-small\", mistral_api_key=mistral_api_key)\n\n# 2\\. Define the prompt:\nsystem_prompt = \"\"\"You are are an expert at analyzing\nbank transactions, you will be categorizing a single\ntransaction.\nAlways return a transaction type and category:\ndo not return None.\nFormat Instructions:\n{format_instructions}\"\"\"\n\nuser_prompt = \"\"\"Transaction Text:\n{transaction}\"\"\"\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            system_prompt,\n        ),\n        (\n            \"user\",\n            user_prompt,\n        ),\n    ]\n)\n\n# 3\\. Define the pydantic model:\nclass EnrichedTransactionInformation(BaseModel):\n    transaction_type: Union[\n        Literal[\"Purchase\", \"Withdrawal\", \"Deposit\",\n        \"Bill Payment\", \"Refund\"], None\n    ]\n    transaction_category: Union[\n        Literal[\"Food\", \"Entertainment\", \"Transport\",\n        \"Utilities\", \"Rent\", \"Other\"],\n        None,\n    ]\n\n# 4\\. Define the output parser:\noutput_parser = PydanticOutputParser(\n    pydantic_object=EnrichedTransactionInformation)\n\n# 5\\. Define a function to try to fix and remove the backslashes:\ndef remove_back_slashes(string):\n    # double slash to escape the slash\n    cleaned_string = string.replace(\"\\\\\", \"\")\n    return cleaned_string\n\n# 6\\. Create an LCEL chain that fixes the formatting:\nchain = prompt | model | StrOutputParser() \\\n| remove_back_slashes | output_parser\n\ntransaction = df.iloc[0][\"Transaction Description\"]\nresult = chain.invoke(\n        {\n            \"transaction\": transaction,\n            \"format_instructions\": \\\n            output_parser.get_format_instructions(),\n        }\n    )\n\n# 7\\. Invoke the chain for the whole dataset:\nresults = []\n\nfor i, row in tqdm(df.iterrows(), total=len(df)):\n    transaction = row[\"Transaction Description\"]\n    try:\n        result = chain.invoke(\n            {\n                \"transaction\": transaction,\n                \"format_instructions\": \\\n                output_parser.get_format_instructions(),\n            }\n        )\n    except:\n        result = EnrichedTransactionInformation(\n            transaction_type=None,\n            transaction_category=None\n        )\n\n    results.append(result)\n\n# 8\\. Add the results to the dataframe, as columns transaction type and\n# transaction category:\ntransaction_types = []\ntransaction_categories = []\n\nfor result in results:\n    transaction_types.append(result.transaction_type)\n    transaction_categories.append(\n        result.transaction_category)\n\ndf[\"mistral_transaction_type\"] = transaction_types\ndf[\"mistral_transaction_category\"] = transaction_categories\ndf.head()\n```", "```py\nTransaction Description\ttransaction_type\ntransaction_category\tmistral_transaction_type\nmistral_transaction_category\n0\tcash deposit at local branch\tDeposit\tOther\tDeposit\nOther\n1\tcash deposit at local branch\tDeposit\tOther\tDeposit\nOther\n2\twithdrew money for rent payment\tWithdrawal\tRent\nWithdrawal\tRent\n3\twithdrew cash for weekend expenses\tWithdrawal\tOther\nWithdrawal\tOther\n4\tpurchased books from the bookstore\tPurchase\tOther\nPurchase\tEntertainment\n```", "```py\n# Evaluate answers using LangChain evaluators:\nfrom langchain.evaluation import load_evaluator\nevaluator = load_evaluator(\"labeled_pairwise_string\")\n\nrow = df.iloc[0]\ntransaction = row[\"Transaction Description\"]\ngpt3pt5_category = row[\"gpt3.5_transaction_category\"]\ngpt3pt5_type = row[\"gpt3.5_transaction_type\"]\nmistral_category = row[\"mistral_transaction_category\"]\nmistral_type = row[\"mistral_transaction_type\"]\nreference_category = row[\"transaction_category\"]\nreference_type = row[\"transaction_type\"]\n\n# Put the data into JSON format for the evaluator:\ngpt3pt5_data = f\"\"\"{{\n \"transaction_category\": \"{gpt3pt5_category}\",\n \"transaction_type\": \"{gpt3pt5_type}\"\n}}\"\"\"\n\nmistral_data = f\"\"\"{{\n \"transaction_category\": \"{mistral_category}\",\n \"transaction_type\": \"{mistral_type}\"\n}}\"\"\"\n\nreference_data = f\"\"\"{{\n \"transaction_category\": \"{reference_category}\",\n \"transaction_type\": \"{reference_type}\"\n}}\"\"\"\n\n# Set up the prompt input for context for the evaluator:\ninput_prompt = \"\"\"You are an expert at analyzing bank\ntransactions,\nyou will be categorizing a single transaction.\nAlways return a transaction type and category: do not\nreturn None.\nFormat Instructions:\n{format_instructions}\nTransaction Text:\n{transaction}\n\"\"\"\n\ntransaction_types.append(transaction_type_score)\ntransaction_categories.append(\n    transaction_category_score)\n\naccuracy_score = 0\n\nfor transaction_type_score, transaction_category_score \\\n    in zip(\n        transaction_types, transaction_categories\n    ):\n    accuracy_score += transaction_type_score['score'] + \\\n    transaction_category_score['score']\n\naccuracy_score = accuracy_score / (len(transaction_types) \\\n    * 2)\nprint(f\"Accuracy score: {accuracy_score}\")\n\nevaluator.evaluate_string_pairs(\n    prediction=gpt3pt5_data,\n    prediction_b=mistral_data,\n    input=input_prompt.format(\n        format_instructions=output_parser.get_format_instructions(),\n        transaction=transaction),\n    reference=reference_data,\n)\n```", "```py\n{'reasoning': '''Both Assistant A and Assistant B provided the exact same\nresponse to the user\\'s question. Their responses are both helpful, relevant,\ncorrect, and demonstrate depth of thought. They both correctly identified the\ntransaction type as \"Deposit\" and the transaction category as \"Other\" based on\nthe transaction text provided by the user. Both responses are also\nwell-formatted according to the JSON schema provided by the user. Therefore,\nit\\'s a tie between the two assistants. \\n\\nFinal Verdict: [[C]]''',\n 'value': None,\n 'score': 0.5}\n```", "```py\nfrom openai import OpenAI\nimport json\nfrom os import getenv\n\ndef schedule_meeting(date, time, attendees):\n    # Connect to calendar service:\n    return { \"event_id\": \"1234\", \"status\": \"Meeting scheduled successfully!\",\n            \"date\": date, \"time\": time, \"attendees\": attendees }\n\nOPENAI_FUNCTIONS = {\n    \"schedule_meeting\": schedule_meeting\n}\n```", "```py\n# Our predefined function JSON schema:\nfunctions = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"type\": \"object\",\n            \"name\": \"schedule_meeting\",\n            \"description\": '''Set a meeting at a specified date and time for\n designated attendees''',\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"date\": {\"type\": \"string\", \"format\": \"date\"},\n                    \"time\": {\"type\": \"string\", \"format\": \"time\"},\n                    \"attendees\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                },\n                \"required\": [\"date\", \"time\", \"attendees\"],\n            },\n        },\n    }\n]\n```", "```py\nclient = OpenAI(api_key=getenv(\"OPENAI_API_KEY\"))\n\n# Start the conversation:\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": '''Schedule a meeting on 2023-11-01 at 14:00\n with Alice and Bob.''',\n    }\n]\n\n# Send the conversation and function schema to the model:\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    messages=messages,\n    tools=functions,\n)\n\nresponse = response.choices[0].message\n\n# Check if the model wants to call our function:\nif response.tool_calls:\n    # Get the first function call:\n    first_tool_call = response.tool_calls[0]\n\n    # Find the function name and function args to call:\n    function_name = first_tool_call.function.name\n    function_args = json.loads(first_tool_call.function.arguments)\n    print(\"This is the function name: \", function_name)\n    print(\"These are the function arguments: \", function_args)\n\n    function = OPENAI_FUNCTIONS.get(function_name)\n\n    if not function:\n        raise Exception(f\"Function {function_name} not found.\")\n\n    # Call the function:\n    function_response = function(**function_args)\n\n    # Share the function's response with the model:\n    messages.append(\n        {\n            \"role\": \"function\",\n            \"name\": \"schedule_meeting\",\n            \"content\": json.dumps(function_response),\n        }\n    )\n\n    # Let the model generate a user-friendly response:\n    second_response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\", messages=messages\n    )\n\n    print(second_response.choices[0].message.content)\n```", "```py\nThese are the function arguments:  {'date': '2023-11-01', 'time': '14:00',\n'attendees': ['Alice', 'Bob']}\nThis is the function name:  schedule_meeting\nI have scheduled a meeting on 2023-11-01 at 14:00 with Alice and Bob.\nThe event ID is 1234.\n```", "```py\n# Start the conversation:\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": '''Schedule a meeting on 2023-11-01 at 14:00 with Alice\n and Bob. Then I want to schedule another meeting on 2023-11-02 at\n 15:00 with Charlie and Dave.'''\n    }\n]\n```", "```py\n# Send the conversation and function schema to the model:\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    messages=messages,\n    tools=functions,\n)\n\nresponse = response.choices[0].message\n\n# Check if the model wants to call our function:\nif response.tool_calls:\n    for tool_call in response.tool_calls:\n        # Get the function name and arguments to call:\n        function_name = tool_call.function.name\n        function_args = json.loads(tool_call.function.arguments)\n        print(\"This is the function name: \", function_name)\n        print(\"These are the function arguments: \", function_args)\n\n        function = OPENAI_FUNCTIONS.get(function_name)\n\n        if not function:\n            raise Exception(f\"Function {function_name} not found.\")\n\n        # Call the function:\n        function_response = function(**function_args)\n\n        # Share the function's response with the model:\n        messages.append(\n            {\n                \"role\": \"function\",\n                \"name\": function_name,\n                \"content\": json.dumps(function_response),\n            }\n        )\n\n    # Let the model generate a user-friendly response:\n    second_response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\", messages=messages\n    )\n\n    print(second_response.choices[0].message.content)\n```", "```py\nThis is the function name:  schedule_meeting\nThese are the function arguments:  {'date': '2023-11-01', 'time': '14:00',\n'attendees': ['Alice', 'Bob']}\nThis is the function name:  schedule_meeting\nThese are the function arguments:  {'date': '2023-11-02', 'time': '15:00',\n'attendees': ['Charlie', 'Dave']}\nTwo meetings have been scheduled:\n1. Meeting with Alice and Bob on 2023-11-01 at 14:00.\n2. Meeting with Charlie and Dave on 2023-11-02 at 15:00.\n```", "```py\nfrom langchain.output_parsers.openai_tools import PydanticToolsParser\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom typing import Optional\n\nclass Article(BaseModel):\n    \"\"\"Identifying key points and contrarian views in an article.\"\"\"\n\n    points: str = Field(..., description=\"Key points from the article\")\n    contrarian_points: Optional[str] = Field(\n        None, description=\"Any contrarian points acknowledged in the article\"\n    )\n    author: Optional[str] = Field(None, description=\"Author of the article\")\n\n_EXTRACTION_TEMPLATE = \"\"\"Extract and save the relevant entities mentioned \\\nin the following passage together with their properties.\n\nIf a property is not present and is not required in the function parameters,\ndo not include it in the output.\"\"\"\n\n# Create a prompt telling the LLM to extract information:\nprompt = ChatPromptTemplate.from_messages(\n    {(\"system\", _EXTRACTION_TEMPLATE), (\"user\", \"{input}\")}\n)\n\nmodel = ChatOpenAI()\n\npydantic_schemas = [Article]\n\n# Convert Pydantic objects to the appropriate schema:\ntools = [convert_to_openai_tool(p) for p in pydantic_schemas]\n\n# Give the model access to these tools:\nmodel = model.bind_tools(tools=tools)\n\n# Create an end to end chain:\nchain = prompt | model | PydanticToolsParser(tools=pydantic_schemas)\n\nresult = chain.invoke(\n    {\n        \"input\": \"\"\"In the recent article titled 'AI adoption in industry,'\n key points addressed include the growing interest ... However, the\n author, Dr. Jane Smith, ...\"\"\"\n    }\n)\nprint(result)\n```", "```py\n[Article(points='The growing interest in AI in various sectors, ...',\ncontrarian_points='Without stringent regulations, ...',\nauthor='Dr. Jane Smith')]\n```", "```py\nfrom langchain.chains.openai_tools import create_extraction_chain_pydantic\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\n# Make sure to use a recent model that supports tools:\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n\nclass Person(BaseModel):\n    \"\"\"A person's name and age.\"\"\"\n\n    name: str = Field(..., description=\"The person's name\")\n    age: int = Field(..., description=\"The person's age\")\n\nchain = create_extraction_chain_pydantic(Person, model)\nchain.invoke({'input':'''Bob is 25 years old. He lives in New York.\nHe likes to play basketball. Sarah is 30 years old. She lives in San\nFrancisco. She likes to play tennis.'''})\n```", "```py\n[Person(name='Bob', age=25), Person(name='Sarah', age=30)]\n```", "```py\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain.output_parsers.pydantic import PydanticOutputParser\nfrom langchain_core.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom pydantic.v1 import BaseModel, Field\nfrom typing import List\n\nclass Query(BaseModel):\n    id: int\n    question: str\n    dependencies: List[int] = Field(\n        default_factory=list,\n        description=\"\"\"A list of sub-queries that must be completed before\n this task can be completed.\n Use a sub query when anything is unknown and we might need to ask\n many queries to get an answer.\n Dependencies must only be other queries.\"\"\"\n    )\n\nclass QueryPlan(BaseModel):\n    query_graph: List[Query]\n```", "```py\n# Set up a chat model:\nmodel = ChatOpenAI()\n\n# Set up a parser:\nparser = PydanticOutputParser(pydantic_object=QueryPlan)\n\ntemplate = \"\"\"Generate a query plan. This will be used for task execution.\n\nAnswer the following query: {query}\n\nReturn the following query graph format:\n{format_instructions}\n\"\"\"\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n\n# Create the LCEL chain with the prompt, model, and parser:\nchain = chat_prompt | model | parser\n\nresult = chain.invoke({\n\"query\":'''I want to get the results from my database. Then I want to find\nout what the average age of my top 10 customers is. Once I have the average\nage, I want to send an email to John. Also I just generally want to send a\nwelcome introduction email to Sarah, regardless of the other tasks.''',\n\"format_instructions\":parser.get_format_instructions()})\n\nprint(result.query_graph)\n```", "```py\n[Query(id=1, question='Get top 10 customers', dependencies=[]),\nQuery(id=2, question='Calculate average age of customers', dependencies=[1]),\nQuery(id=3, question='Send email to John', dependencies=[2]),\nQuery(id=4, question='Send welcome email to Sarah', dependencies=[])]\n```", "```py\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_core.prompts import (\n    FewShotChatMessagePromptTemplate,\n    ChatPromptTemplate,\n)\n\nexamples = [\n    {\n        \"question\": \"What is the capital of France?\",\n        \"answer\": \"Paris\",\n    },\n    {\n        \"question\": \"What is the capital of Spain?\",\n        \"answer\": \"Madrid\",\n    } # ...more examples...\n]\n```", "```py\nexample_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"human\", \"{question}\"),\n        (\"ai\", \"{answer}\"),\n    ]\n)\n\nfew_shot_prompt = FewShotChatMessagePromptTemplate(\n    example_prompt=example_prompt,\n    examples=examples,\n)\n\nprint(few_shot_prompt.format())\n```", "```py\nHuman: What is the capital of France?\nAI: Paris\nHuman: What is the capital of Spain?\nAI: Madrid\n...more examples...\n```", "```py\nfrom langchain_core.output_parsers import StrOutputParser\n\nfinal_prompt = ChatPromptTemplate.from_messages(\n    [(\"system\",'''You are responsible for answering\n questions about countries. Only return the country\n name.'''),\n    few_shot_prompt,(\"human\", \"{question}\"),]\n)\n\nmodel = ChatOpenAI()\n\n# Creating the LCEL chain with the prompt, model, and a StrOutputParser():\nchain = final_prompt | model | StrOutputParser()\n\nresult = chain.invoke({\"question\": \"What is the capital of America?\"})\n\nprint(result)\n```", "```py\nWashington, D.C.\n```", "```py\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\nfrom langchain.prompts.example_selector import LengthBasedExampleSelector\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_core.messages import SystemMessage\nimport tiktoken\n\nexamples = [\n    {\"input\": \"Gollum\", \"output\": \"<Story involving Gollum>\"},\n    {\"input\": \"Gandalf\", \"output\": \"<Story involving Gandalf>\"},\n    {\"input\": \"Bilbo\", \"output\": \"<Story involving Bilbo>\"},\n]\n\nstory_prompt = PromptTemplate(\n    input_variables=[\"input\", \"output\"],\n    template=\"Character: {input}\\nStory: {output}\",\n)\n\ndef num_tokens_from_string(string: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.get_encoding(\"cl100k_base\")\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\nexample_selector = LengthBasedExampleSelector(\n    examples=examples,\n    example_prompt=story_prompt,\n    max_length=1000, # 1000 tokens are to be included from examples\n    # get_text_length: Callable[[str], int] = lambda x: len(re.split(\"\\n| \", x))\n    # You have modified the get_text_length function to work with the\n    # TikToken library based on token usage:\n    get_text_length=num_tokens_from_string,\n)\n```", "```py\ndynamic_prompt = FewShotPromptTemplate(\n    example_selector=example_selector,\n    example_prompt=story_prompt,\n    prefix='''Generate a story for {character} using the\n current Character/Story pairs from all of the characters\n as context.''',\n    suffix=\"Character: {character}\\nStory:\",\n    input_variables=[\"character\"],\n)\n\n# Provide a new character from Lord of the Rings:\nformatted_prompt = dynamic_prompt.format(character=\"Frodo\")\n\n# Creating the chat model:\nchat = ChatOpenAI()\n\nresponse = chat.invoke([SystemMessage(content=formatted_prompt)])\nprint(response.content)\n```", "```py\nFrodo was a young hobbit living a peaceful life in the Shire. However,\nhis life...\n```", "```py\nresult = model.invoke([SystemMessage(content=formatted_prompt)])\n```", "```py\nfrom langchain_core.prompts import PromptTemplate, load_prompt\n\nprompt = PromptTemplate(\n    template='''Translate this sentence from English to Spanish.\n    \\nSentence: {sentence}\\nTranslation:''',\n    input_variables=[\"sentence\"],\n)\n\nprompt.save(\"translation_prompt.json\")\n\n# Loading the prompt template:\nload_prompt(\"translation_prompt.json\")\n# Returns PromptTemplate()\n```", "```py\nfrom langchain_community.document_loaders import Docx2txtLoader\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_community.document_loaders.csv_loader import CSVLoader\nimport glob\nfrom langchain.text_splitter import CharacterTextSplitter\n\n# To store the documents across all data sources:\nall_documents = []\n\n# Load the PDF:\nloader = PyPDFLoader(\"data/principles_of_marketing_book.pdf\")\npages = loader.load_and_split()\nprint(pages[0])\n\n# Add extra metadata to each page:\nfor page in pages:\n    page.metadata[\"description\"] = \"Principles of Marketing Book\"\n\n# Checking that the metadata has been added:\nfor page in pages[0:2]:\n    print(page.metadata)\n\n# Saving the marketing book pages:\nall_documents.extend(pages)\n\ncsv_files = glob.glob(\"data/*.csv\")\n\n# Filter to only include the word Marketing in the file name:\ncsv_files = [f for f in csv_files if \"Marketing\" in f]\n\n# For each .csv file:\nfor csv_file in csv_files:\n    loader = CSVLoader(file_path=csv_file)\n    data = loader.load()\n    # Saving the data to the all_documents list:\n    all_documents.extend(data)\n\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=200, chunk_overlap=0\n)\n\nurls = [\n\n    '''https://storage.googleapis.com/oreilly-content/NutriFusion%20Foods%2\n 0Marketing%20Plan%202022.docx''',\n    '''https://storage.googleapis.com/oreilly-content/NutriFusion%20Foods%2\n 0Marketing%20Plan%202023.docx''',\n]\n\ndocs = []\nfor url in urls:\n    loader = Docx2txtLoader(url.replace('\\n', ''))\n    pages = loader.load()\n    chunks = text_splitter.split_documents(pages)\n\n    # Adding the metadata to each chunk:\n    for chunk in chunks:\n        chunk.metadata[\"source\"] = \"NutriFusion Foods Marketing Plan - 2022/2023\"\n    docs.extend(chunks)\n\n# Saving the marketing book pages:\nall_documents.extend(docs)\n```", "```py\npage_content='Principles of Mark eting'\nmetadata={'source': 'data/principles_of_marketing_book.pdf', 'page': 0}\n{'source': 'data/principles_of_marketing_book.pdf', 'page': 0,\n'description': 'Principles of Marketing Book'}\n{'source': 'data/principles_of_marketing_book.pdf', 'page': 1,\n'description': 'Principles of Marketing Book'}\n```", "```py\nfrom langchain_text_splitters import CharacterTextSplitter\n\ntext = \"\"\"\nBiology is a fascinating and diverse field of science that explores the\nliving world and its intricacies \\n\\n. It encompasses the study of life, its\norigins, diversity, structure, function, and interactions at various levels\nfrom molecules and cells to organisms and ecosystems \\n\\n. In this 1000-word\nessay, we will delve into the core concepts of biology, its history, key\nareas of study, and its significance in shaping our understanding of the\nnatural world. \\n\\n ...(truncated to save space)...\n\"\"\"\n# No chunk overlap:\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\nchunk_size=50, chunk_overlap=0, separator=\"\\n\",\n)\ntexts = text_splitter.split_text(text)\nprint(f\"Number of texts with no chunk overlap: {len(texts)}\")\n\n# Including a chunk overlap:\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\nchunk_size=50, chunk_overlap=48, separator=\"\\n\",\n)\ntexts = text_splitter.split_text(text)\nprint(f\"Number of texts with chunk overlap: {len(texts)}\")\n```", "```py\nNumber of texts with no chunk overlap: 3\nNumber of texts with chunk overlap: 6\n```", "```py\nfrom langchain.text_splitter import TokenTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader\n\ntext_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=50)\nloader = PyPDFLoader(\"data/principles_of_marketing_book.pdf\")\npages = loader.load_and_split(text_splitter=text_splitter)\n\nprint(len(pages)) #737\n```", "```py\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=100,\n    chunk_overlap=20,\n    length_function=len,\n)\n```", "```py\n# Split the text into chunks:\ntexts = text_splitter.split_text(text)\n```", "```py\n# Create documents from the chunks:\nmetadatas = {\"title\": \"Biology\", \"author\": \"John Doe\"}\ndocs = text_splitter.create_documents(texts, metadatas=[metadatas] * len(texts))\n```", "```py\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=300)\nsplitted_docs = text_splitter.split_documents(docs)\n```", "```py\nfrom langchain_core.prompts.chat import ChatPromptTemplate\n\ncharacter_generation_prompt = ChatPromptTemplate.from_template(\n    \"\"\"I want you to brainstorm three to five characters for my short story. The\n genre is {genre}. Each character must have a Name and a Biography.\n You must provide a name and biography for each character, this is very\n important!\n ---\n Example response:\n Name: CharWiz, Biography: A wizard who is a master of magic.\n Name: CharWar, Biography: A warrior who is a master of the sword.\n ---\n Characters: \"\"\"\n)\n\nplot_generation_prompt = ChatPromptTemplate.from_template(\n    \"\"\"Given the following characters and the genre, create an effective\n plot for a short story:\n Characters:\n {characters}\n ---\n Genre: {genre}\n ---\n Plot: \"\"\"\n    )\n\nscene_generation_plot_prompt = ChatPromptTemplate.from_template(\n    \"\"\"Act as an effective content creator.\n Given multiple characters and a plot, you are responsible for\n generating the various scenes for each act.\n\n You must decompose the plot into multiple effective scenes:\n ---\n Characters:\n {characters}\n ---\n Genre: {genre}\n ---\n Plot: {plot}\n ---\n Example response:\n Scenes:\n Scene 1: Some text here.\n Scene 2: Some text here.\n Scene 3: Some text here.\n ----\n Scenes:\n \"\"\"\n)\n```", "```py\nfrom operator import itemgetter\nfrom langchain_core.runnables import RunnablePassthrough\n\nchain = RunnablePassthrough() | {\n    \"genre\": itemgetter(\"genre\"),\n  }\nchain.invoke({\"genre\": \"fantasy\"})\n# {'genre': 'fantasy'}\n```", "```py\nfrom langchain_core.runnables import RunnableLambda\n\nchain = RunnablePassthrough() | {\n    \"genre\": itemgetter(\"genre\"),\n    \"upper_case_genre\": lambda x: x[\"genre\"].upper(),\n    \"lower_case_genre\": RunnableLambda(lambda x: x[\"genre\"].lower()),\n}\nchain.invoke({\"genre\": \"fantasy\"})\n# {'genre': 'fantasy', 'upper_case_genre': 'FANTASY',\n# 'lower_case_genre': 'fantasy'}\n```", "```py\nfrom langchain_core.runnables import RunnableParallel\n\nmaster_chain = RunnablePassthrough() | {\n    \"genre\": itemgetter(\"genre\"),\n    \"upper_case_genre\": lambda x: x[\"genre\"].upper(),\n    \"lower_case_genre\": RunnableLambda(lambda x: x[\"genre\"].lower()),\n}\n\nmaster_chain_two = RunnablePassthrough() | RunnableParallel(\n        genre=itemgetter(\"genre\"),\n        upper_case_genre=lambda x: x[\"genre\"].upper(),\n        lower_case_genre=RunnableLambda(lambda x: x[\"genre\"].lower()),\n)\n\nstory_result = master_chain.invoke({\"genre\": \"Fantasy\"})\nprint(story_result)\n\nstory_result = master_chain_two.invoke({\"genre\": \"Fantasy\"})\nprint(story_result)\n\n# master chain: {'genre': 'Fantasy', 'upper_case_genre': 'FANTASY',\n# 'lower_case_genre': 'fantasy'}\n# master chain two: {'genre': 'Fantasy', 'upper_case_genre': 'FANTASY',\n# 'lower_case_genre': 'fantasy'}\n```", "```py\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Create the chat model:\nmodel = ChatOpenAI()\n\n# Create the subchains:\ncharacter_generation_chain = ( character_generation_prompt\n| model\n| StrOutputParser() )\n\nplot_generation_chain = ( plot_generation_prompt\n| model\n| StrOutputParser() )\n\nscene_generation_plot_chain = ( scene_generation_plot_prompt\n| model\n| StrOutputParser()  )\n```", "```py\nfrom langchain_core.runnables import RunnableParallel\nfrom operator import itemgetter\nfrom langchain_core.runnables import RunnablePassthrough\n\nmaster_chain = (\n    {\"characters\": character_generation_chain, \"genre\":\n    RunnablePassthrough()}\n    | RunnableParallel(\n        characters=itemgetter(\"characters\"),\n        genre=itemgetter(\"genre\"),\n        plot=plot_generation_chain,\n    )\n    | RunnableParallel(\n        characters=itemgetter(\"characters\"),\n        genre=itemgetter(\"genre\"),\n        plot=itemgetter(\"plot\"),\n        scenes=scene_generation_plot_chain,\n    )\n)\n\nstory_result = master_chain.invoke({\"genre\": \"Fantasy\"})\n```", "```py\n{'characters': '''Name: Lyra, Biography: Lyra is a young elf who possesses\n..\\n\\nName: Orion, Biography: Orion is a ..''', 'genre': {'genre':\n'Fantasy'} 'plot': '''In the enchanted forests of a mystical realm, a great\ndarkness looms, threatening to engulf the land and its inhabitants. Lyra,\nthe young elf with a deep connection to nature, ...''', 'scenes': '''Scene\n1: Lyra senses the impending danger in the forest ...\\n\\nScene 2: Orion, on\nhis mission to investigate the disturbances in the forest...\\n\\nScene 9:\nAfter the battle, Lyra, Orion, Seraphina, Finnegan...'''}\n```", "```py\n# Extracting the scenes using .split('\\n') and removing empty strings:\nscenes = [scene for scene in story_result[\"scenes\"].split(\"\\n\") if scene]\ngenerated_scenes = []\nprevious_scene_summary = \"\"\n\ncharacter_script_prompt = ChatPromptTemplate.from_template(\n    template=\"\"\"Given the following characters: {characters} and the genre:\n    {genre}, create an effective character script for a scene.\n\n You must follow the following principles:\n - Use the Previous Scene Summary: {previous_scene_summary} to avoid\n repeating yourself.\n - Use the Plot: {plot} to create an effective scene character script.\n - Currently you are generating the character dialogue script for the\n following scene: {scene}\n\n ---\n Here is an example response:\n SCENE 1: ANNA'S APARTMENT\n\n (ANNA is sorting through old books when there is a knock at the door.\n She opens it to reveal JOHN.)\n ANNA: Can I help you, sir?\n JOHN: Perhaps, I think it's me who can help you. I heard you're\n researching time travel.\n (Anna looks intrigued but also cautious.)\n ANNA: That's right, but how do you know?\n JOHN: You could say... I'm a primary source.\n\n ---\n SCENE NUMBER: {index}\n\n \"\"\",\n)\n\nsummarize_prompt = ChatPromptTemplate.from_template(\n    template=\"\"\"Given a character script, create a summary of the scene.\n Character script: {character_script}\"\"\",\n)\n```", "```py\n# Loading a chat model:\nmodel = ChatOpenAI(model='gpt-3.5-turbo-16k')\n\n# Create the LCEL chains:\ncharacter_script_generation_chain = (\n    {\n        \"characters\": RunnablePassthrough(),\n        \"genre\": RunnablePassthrough(),\n        \"previous_scene_summary\": RunnablePassthrough(),\n        \"plot\": RunnablePassthrough(),\n        \"scene\": RunnablePassthrough(),\n        \"index\": RunnablePassthrough(),\n    }\n    | character_script_prompt\n    | model\n    | StrOutputParser()\n)\n\nsummarize_chain = summarize_prompt | model | StrOutputParser()\n\n# You might want to use tqdm here to track the progress,\n# or use all of the scenes:\nfor index, scene in enumerate(scenes[0:3]):\n\n    # # Create a scene generation:\n    scene_result = character_script_generation_chain.invoke(\n        {\n            \"characters\": story_result[\"characters\"],\n            \"genre\": \"fantasy\",\n            \"previous_scene_summary\": previous_scene_summary,\n            \"index\": index,\n        }\n    )\n\n    # Store the generated scenes:\n    generated_scenes.append(\n        {\"character_script\": scene_result, \"scene\": scenes[index]}\n    )\n\n    # If this is the first scene then we don't have a\n    # previous scene summary:\n    if index == 0:\n        previous_scene_summary = scene_result\n    else:\n        # If this is the second scene or greater then\n        # we can use and generate a summary:\n        summary_result = summarize_chain.invoke(\n            {\"character_script\": scene_result}\n        )\n        previous_scene_summary = summary_result\n```", "```py\nfrom langchain_core.prompts.chat import ChatPromptTemplate\nfrom operator import itemgetter\nfrom langchain_core.runnables import RunnablePassthrough, RunnableLambda\n\nbad_first_input = {\n    \"film_required_age\": 18,\n}\n\nprompt = ChatPromptTemplate.from_template(\n    \"Generate a film title, the age is {film_required_age}\"\n)\n\n# This will error:\nbad_chain = bad_first_input | prompt\n```", "```py\n# All of these chains enforce the runnable interface:\nfirst_good_input = {\"film_required_age\": itemgetter(\"film_required_age\")}\n\n# Creating a dictionary within a RunnableLambda:\nsecond_good_input = RunnableLambda(lambda x: { \"film_required_age\":\nx[\"film_required_age\"] } )\n\nthird_good_input = RunnablePassthrough()\nfourth_good_input = {\"film_required_age\": RunnablePassthrough()}\n# You can also create a chain starting with RunnableParallel(...)\n\nfirst_good_chain = first_good_input | prompt\nsecond_good_chain = second_good_input | prompt\nthird_good_chain = third_good_input | prompt\nfourth_good_chain = fourth_good_input | prompt\n\nfirst_good_chain.invoke({\n    \"film_required_age\": 18\n}) # ...\n```", "```py\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain.chains.summarize import load_summarize_chain\nimport pandas as pd\n```", "```py\ndf = pd.DataFrame(generated_scenes)\n```", "```py\nall_character_script_text = \"\\n\".join(df.character_script.tolist())\n```", "```py\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=1500, chunk_overlap=200\n)\ndocs = text_splitter.create_documents([all_character_script_text])\n```", "```py\nchain = load_summarize_chain(llm=model, chain_type=\"map_reduce\")\n```", "```py\nsummary = chain.invoke(docs)\n```", "```py\nprint(summary['output_text'])\n```", "```py\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.chains.summarize import load_summarize_chain\nimport pandas as pd\n\ndf = pd.DataFrame(generated_scenes)\n\nall_character_script_text = \"\\n\".join(df.character_script.tolist())\n\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=1500, chunk_overlap=200\n)\n\ndocs = text_splitter.create_documents([all_character_script_text])\n\nchain = load_summarize_chain(llm=model, chain_type=\"map_reduce\")\nsummary = chain.invoke(docs)\nprint(summary['output_text'])\n```", "```py\nAurora and Magnus agree to retrieve a hidden artifact, and they enter an\nancient library to find a book that will guide them to the relic...'\n```", "```py\nchain = load_summarize_chain(llm=model, chain_type='refine')\n```"]