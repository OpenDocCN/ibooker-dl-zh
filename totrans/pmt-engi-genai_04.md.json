["```py\nfrom langchain_openai.chat_models import ChatOpenAI\nchat = ChatOpenAI(api_key=\"api_key\")\n```", "```py\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\n\nchat = ChatOpenAI(temperature=0.5)\nmessages = [SystemMessage(content='''Act as a senior software engineer\nat a startup company.'''),\nHumanMessage(content='''Please can you provide a funny joke\nabout software engineers?''')]\nresponse = chat.invoke(input=messages)\nprint(response.content)\n```", "```py\nSure, here's a lighthearted joke for you: `Why` `did` `the` `software` `engineer` `go` `broke``?`\n`Because` `he` `lost` `his` `domain` `in` `a` `bet` `and` `couldn``'t afford to renew it.`\n```", "```py`` ```", "```py response = chat(messages=messages) ```", "```py` ```", "```py```", "````py````", "```py```", "````py````", "```py for chunk in chat.stream(messages):     print(chunk.content, end=\"\", flush=True) ```", "```py # 2x lists of messages, which is the same as [messages, messages] synchronous_llm_result = chat.batch([messages]*2) print(synchronous_llm_result) ```", "```py [AIMessage(content='''Sure, here's a lighthearted joke for you:\\n\\nWhy did the software engineer go broke?\\n\\nBecause he kept forgetting to Ctrl+ Z his expenses!'''), AIMessage(content='''Sure, here\\'s a lighthearted joke for you:\\n\\nWhy do software engineers prefer dark mode?\\n\\nBecause it\\'s easier on their \"byte\" vision!''')] ```", "```py from langchain_core.runnables.config import RunnableConfig  # Create a RunnableConfig with the desired concurrency limit: config = RunnableConfig(max_concurrency=5)  # Call the .batch() method with the inputs and config: results = chat.batch([messages, messages], config=config) ```", "```py language = \"Python\" prompt = f\"What is the best way to learn coding in {language}?\" print(prompt) # What is the best way to learn coding in Python? ```", "```py chain = prompt | model ```", "```py bad_order_chain = model | prompt ```", "```py from langchain_openai.chat_models import ChatOpenAI from langchain_core.prompts import (SystemMessagePromptTemplate, ChatPromptTemplate)  template = \"\"\" You are a creative consultant brainstorming names for businesses.  You must follow the following principles: {principles} `Please generate a numerical list of five catchy names for a start-up in the` `{industry}` `industry that deals with` `{context}``?`  `Here is an example of the format:` `1\\. Name1` `2\\. Name2` `3\\. Name3` `4\\. Name4` `5\\. Name5` `\"\"\"`  `model` `=` `ChatOpenAI``()` `system_prompt` `=` `SystemMessagePromptTemplate``.``from_template``(``template``)` `chat_prompt` `=` `ChatPromptTemplate``.``from_messages``([``system_prompt``])`  `chain` `=` `chat_prompt` `|` `model`  `result` `=` `chain``.``invoke``({`     `\"industry\"``:` `\"medical\"``,`     `\"context\"``:``'''creating AI solutions by automatically summarizing patient`  `records'''``,`     `\"principles\"``:``'''1\\. Each name should be short and easy to`  `remember. 2\\. Each name should be easy to pronounce.`  `3\\. Each name should be unique and not already taken by another company.'''` `})`  `print``(``result``.``content``)` ```", "```py`Output:    ```", "```py    First, you’ll import `ChatOpenAI`, `SystemMessagePromptTemplate`, and `ChatPromptTemplate`. Then, you’ll define a prompt template with specific guidelines under `template`, instructing the LLM to generate business names. `ChatOpenAI()` initializes the chat, while `SystemMessagePromptTemplate.from_template(template)` and `ChatPromptTemplate.from_messages([system_prompt])` create your prompt template.    You create an LCEL `chain` by piping together `chat_prompt` and the `model`, which is then *invoked*. This replaces the `{industries}`, `{context}`, and `{principles}` placeholders in the prompt with the dictionary values within the `invoke` function.    Finally, you extract the LLM’s response as a string accessing the `.content` property on the `result` variable.    # Give Direction and Specify Format    Carefully crafted instructions might include things like “You are a creative consultant brainstorming names for businesses” and “Please generate a numerical list of five to seven catchy names for a start-up.” Cues like these guide your LLM to perform the exact task you require from it.```", "```py```", "````py````", "```py from langchain_core.prompts import PromptTemplate from langchain.prompts.chat import SystemMessagePromptTemplate from langchain_openai.chat_models import ChatOpenAI prompt=PromptTemplate(  template='''You are a helpful assistant that translates {input_language} to  {output_language}.''',  input_variables=[\"input_language\", \"output_language\"], ) system_message_prompt = SystemMessagePromptTemplate(prompt=prompt) chat = ChatOpenAI() chat.invoke(system_message_prompt.format_messages( input_language=\"English\",output_language=\"French\")) ```", "```py AIMessage(content=\"Vous êtes un assistant utile qui traduit l'anglais en `français``.``\", additional_kwargs=``{}``, example=False)` ```", "```py```", "````py``` ````", "`````` # Output Parsers    In [Chapter 3](ch03.html#standard_practices_03), you used regular expressions (regex) to extract structured data from text that contained numerical lists, but it’s possible to do this automatically in LangChain with *output parsers*.    *Output parsers* are a higher-level abstraction provided by LangChain for parsing structured data from LLM string responses. Currently the available output parsers are:    List parser      Returns a list of comma-separated items.      Datetime parser      Parses an LLM output into datetime format.      Enum parser      Parses strings into enum values.      Auto-fixing parser      Wraps another output parser, and if that output parser fails, it will call another LLM to fix any errors.      Pydantic (JSON) parser      Parses LLM responses into JSON output that conforms to a Pydantic schema.      Retry parser      Provides retrying a failed parse from a previous output parser.      Structured output parser      Can be used when you want to return multiple fields.      XML parser      Parses LLM responses into an XML-based format.      As you’ll discover, there are two important functions for LangChain output parsers:    `.get_format_instructions()`      This function provides the necessary instructions into your prompt to output a structured format that can be parsed.      `.parse(llm_output: str)`      This function is responsible for parsing your LLM responses into a predefined format.      Generally, you’ll find that the Pydantic (JSON) parser with `ChatOpenAI()` provides the most flexibility.    The Pydantic (JSON) parser takes advantage of the [Pydantic](https://oreil.ly/QIMih) library in Python. Pydantic is a data validation library that provides a way to validate incoming data using Python type annotations. This means that Pydantic allows you to create schemas for your data and automatically validates and parses input data according to those schemas.    Input:    ```py from langchain_core.prompts.chat import (     ChatPromptTemplate,     SystemMessagePromptTemplate, ) from langchain_openai.chat_models import ChatOpenAI from langchain.output_parsers import PydanticOutputParser from pydantic.v1 import BaseModel, Field from typing import List  temperature = 0.0  class BusinessName(BaseModel):     name: str = Field(description=\"The name of the business\")     rating_score: float = Field(description='''The rating score of the  business. 0 is the worst, 10 is the best.''')  class BusinessNames(BaseModel):     names: List[BusinessName] = Field(description='''A list  of busines names''')  # Set up a parser + inject instructions into the prompt template: parser = PydanticOutputParser(pydantic_object=BusinessNames)  principles = \"\"\" - The name must be easy to remember. - Use the {industry} industry and Company context to create an effective name. - The name must be easy to pronounce. - You must only return the name without any other text or characters. - Avoid returning full stops, \\n, or any other characters. - The maximum length of the name must be 10 characters. \"\"\"  # Chat Model Output Parser: model = ChatOpenAI() template = \"\"\"Generate five business names for a new start-up company in the {industry} industry. You must follow the following principles: {principles} `{format_instructions}` ``` `\"\"\"` `system_message_prompt` `=` `SystemMessagePromptTemplate``.``from_template``(``template``)` `chat_prompt` `=` `ChatPromptTemplate``.``from_messages``([``system_message_prompt``])`  `# Creating the LCEL chain:` `prompt_and_model` `=` `chat_prompt` `|` `model`  `result` `=` `prompt_and_model``.``invoke``(`     `{`         `\"principles\"``:` `principles``,`         `\"industry\"``:` `\"Data Science\"``,`         `\"format_instructions\"``:` `parser``.``get_format_instructions``(),`     `}` `)` `# The output parser, parses the LLM response into a Pydantic object:` `print``(``parser``.``parse``(``result``.``content``))` ```py ```   ```py`` ````Output:    ```py names=[BusinessName(name='DataWiz', rating_score=8.5), BusinessName(name='InsightIQ', rating_score=9.2), BusinessName(name='AnalytiQ', rating_score=7.8), BusinessName(name='SciData', rating_score=8.1), BusinessName(name='InfoMax', rating_score=9.5)] ```    After you’ve loaded the necessary libraries, you’ll set up a ChatOpenAI model. Then create `SystemMessagePromptTemplate` from your template and form a `ChatPromptTemplate` with it. You’ll use the Pydantic models `BusinessName` and `BusinessNames` to structure your desired output, a list of unique business names. You’ll create a `Pydantic` parser for parsing these models and format the prompt using user-inputted variables by calling the `invoke` function. Feeding this customized prompt to your model, you’re enabling it to produce creative, unique business names by using the `parser`.    It’s possible to use output parsers inside of LCEL by using this syntax:    ```py chain = prompt | model | output_parser ```    Let’s add the output parser directly to the chain.    Input:    ```py parser = PydanticOutputParser(pydantic_object=BusinessNames) chain = chat_prompt | model | parser  result = chain.invoke(     {         \"principles\": principles,         \"industry\": \"Data Science\",         \"format_instructions\": parser.get_format_instructions(),     } ) print(result) ```    Output:    ```py names=[BusinessName(name='DataTech', rating_score=9.5),...] ```    The chain is now responsible for prompt formatting, LLM calling, and parsing the LLM’s response into a `Pydantic` object.    # Specify Format    The preceding prompts use Pydantic models and output parsers, allowing you explicitly tell an LLM your desired response format.    It’s worth knowing that by asking an LLM to provide structured JSON output, you can create a flexible and generalizable API from the LLM’s response. There are limitations to this, such as the size of the JSON created and the reliability of your prompts, but it still is a promising area for LLM applications.    ###### Warning    You should take care of edge cases as well as adding error handling statements, since LLM outputs might not always be in your desired format.    Output parsers save you from the complexity and intricacy of regular expressions, providing easy-to-use functionalities for a variety of use cases. Now that you’ve seen them in action, you can utilize output parsers to effortlessly structure and retrieve the data you need from an LLM’s output, harnessing the full potential of AI for your tasks.    Furthermore, using parsers to structure the data extracted from LLMs allows you to easily choose how to organize outputs for more efficient use. This can be useful if you’re dealing with extensive lists and need to sort them by certain criteria, like business names.```py` `````  ```py``````", "``````py```` # LangChain Evals    As well as output parsers to check for formatting errors, most AI systems also make use of *evals*, or evaluation metrics, to measure the performance of each prompt response. LangChain has a number of off-the-shelf evaluators, which can be directly be logged in their [LangSmith](https://oreil.ly/0Fn94) platform for further debugging, monitoring, and testing. [Weights and Biases](https://wandb.ai/site) is alternative machine learning platform that offers similar functionality and tracing capabilities for LLMs.    Evaluation metrics are useful for more than just prompt testing, as they can be used to identify positive and negative examples for retrieval as well as to build datasets for fine-tuning custom models.    Most eval metrics rely on a set of test cases, which are input and output pairings where you know the correct answer. Often these reference answers are created or curated manually by a human, but it’s also common practice to use a smarter model like GPT-4 to generate the ground truth answers, which has been done for the following example. Given a list of descriptions of financial transactions, we used GPT-4 to classify each transaction with a `transaction_category` and `transaction_type`. The process can be found in the `langchain-evals.ipynb` Jupyter Notebook in the [GitHub repository](https://oreil.ly/a4Hut) for the book.    With the GPT-4 answer being taken as the correct answer, it’s now possible to rate the accuracy of smaller models like GPT-3.5-turbo and Mixtral 8x7b (called `mistral-small` in the API). If you can achieve good enough accuracy with a smaller model, you can save money or decrease latency. In addition, if that model is available open source like [Mistral’s model](https://oreil.ly/Ec578), you can migrate that task to run on your own servers, avoiding sending potentially sensitive data outside of your organization. We recommend testing with an external API first, before going to the trouble of self-hosting an OS model.    [Remember to sign up](https://mistral.ai) and subscribe to obtain an API key; then expose that as an environment variable by typing in your terminal:    *   `**export MISTRAL_API_KEY=api-key**`    The following script is part of a [notebook](https://oreil.ly/DqDOf) that has previously defined a dataframe `df`. For brevity let’s investigate only the evaluation section of the script, assuming a dataframe is already defined.    Input:    ```py import os from langchain_mistralai.chat_models import ChatMistralAI from langchain.output_parsers import PydanticOutputParser from langchain_core.prompts import ChatPromptTemplate from pydantic.v1 import BaseModel from typing import Literal, Union from langchain_core.output_parsers import StrOutputParser  # 1\\. Define the model: mistral_api_key = os.environ[\"MISTRAL_API_KEY\"]  model = ChatMistralAI(model=\"mistral-small\", mistral_api_key=mistral_api_key)  # 2\\. Define the prompt: system_prompt = \"\"\"You are are an expert at analyzing bank transactions, you will be categorizing a single transaction. Always return a transaction type and category: do not return None. Format Instructions: {format_instructions}\"\"\"  user_prompt = \"\"\"Transaction Text: {transaction}\"\"\"  prompt = ChatPromptTemplate.from_messages(     [         (             \"system\",             system_prompt,         ),         (             \"user\",             user_prompt,         ),     ] )  # 3\\. Define the pydantic model: class EnrichedTransactionInformation(BaseModel):     transaction_type: Union[         Literal[\"Purchase\", \"Withdrawal\", \"Deposit\",         \"Bill Payment\", \"Refund\"], None     ]     transaction_category: Union[         Literal[\"Food\", \"Entertainment\", \"Transport\",         \"Utilities\", \"Rent\", \"Other\"],         None,     ]   # 4\\. Define the output parser: output_parser = PydanticOutputParser(     pydantic_object=EnrichedTransactionInformation)  # 5\\. Define a function to try to fix and remove the backslashes: def remove_back_slashes(string):     # double slash to escape the slash     cleaned_string = string.replace(\"\\\\\", \"\")     return cleaned_string  # 6\\. Create an LCEL chain that fixes the formatting: chain = prompt | model | StrOutputParser() \\ | remove_back_slashes | output_parser  transaction = df.iloc[0][\"Transaction Description\"] result = chain.invoke(         {             \"transaction\": transaction,             \"format_instructions\": \\             output_parser.get_format_instructions(),         }     )  # 7\\. Invoke the chain for the whole dataset: results = []  for i, row in tqdm(df.iterrows(), total=len(df)):     transaction = row[\"Transaction Description\"]     try:         result = chain.invoke(             {                 \"transaction\": transaction,                 \"format_instructions\": \\                 output_parser.get_format_instructions(),             }         )     except:         result = EnrichedTransactionInformation(             transaction_type=None,             transaction_category=None         )      results.append(result)  # 8\\. Add the results to the dataframe, as columns transaction type and # transaction category: transaction_types = [] transaction_categories = []  for result in results:     transaction_types.append(result.transaction_type)     transaction_categories.append(         result.transaction_category)  df[\"mistral_transaction_type\"] = transaction_types df[\"mistral_transaction_category\"] = transaction_categories df.head() ```    Output:    ```py Transaction Description\ttransaction_type transaction_category\tmistral_transaction_type mistral_transaction_category 0\tcash deposit at local branch\tDeposit\tOther\tDeposit Other 1\tcash deposit at local branch\tDeposit\tOther\tDeposit Other 2\twithdrew money for rent payment\tWithdrawal\tRent Withdrawal\tRent 3\twithdrew cash for weekend expenses\tWithdrawal\tOther Withdrawal\tOther 4\tpurchased books from the bookstore\tPurchase\tOther Purchase\tEntertainment ```    The code does the following:    1.  `from langchain_mistralai.chat_models import ChatMistralAI`: We import LangChain’s Mistral implementation.           2.  `from langchain.output_parsers import PydanticOutputParser`: Imports the `PydanticOutputParser` class, which is used for parsing output using Pydantic models. We also import a string output parser to handle an interim step where we remove backslashes from the JSON key (a common problem with responses from Mistral).           3.  `mistral_api_key = os.environ[\"MISTRAL_API_KEY\"]`: Retrieves the Mistral API key from the environment variables. This needs to be set prior to running the notebook.           4.  `model = ChatMistralAI(model=\"mistral-small\", mistral_api_key=mistral_api_key)`: Initializes an instance of `ChatMistralAI` with the specified model and API key. Mistral Small is what they call the Mixtral 8x7b model (also available open source) in their API.           5.  `system_prompt` and `user_prompt`: These lines define templates for the system and user prompts used in the chat to classify the transactions.           6.  `class EnrichedTransactionInformation(BaseModel)`: Defines a Pydantic model `EnrichedTransactionInformation` with two fields: `transaction_type` and `transaction_category`, each with specific allowed values and the possibility of being `None`. This is what tells us if the output is in the correct format.           7.  `def remove_back_slashes(string)`: Defines a function to remove backslashes from a string.           8.  `chain = prompt | model | StrOutputParser() | remove_back_slashes | output_parser`: Updates the chain to include a string output parser and the `remove_back_slashes` function before the original output parser.           9.  `transaction = df.iloc[0][\"Transaction Description\"]`: Extracts the first transaction description from a dataframe `df`. This dataframe is loaded earlier in the [Jupyter Notebook](https://oreil.ly/-koAO) (omitted for brevity).           10.  `for i, row in tqdm(df.iterrows(), total=len(df))`: Iterates over each row in the dataframe `df`, with a progress bar.           11.  `result = chain.invoke(...)`: Inside the loop, the chain is invoked for each transaction.           12.  `except`: In case of an exception, a default `EnrichedTransactionInformation` object with `None` values is created. These will be treated as errors in evaluation but will not break the processing loop.           13.  `df[\"mistral_transaction_type\"] = transaction_types`, `df[\"mistral_transaction_category\"] = transaction_categories`: Adds the transaction types and categories as new columns in the dataframe, which we then display with `df.head()`.              With the responses from Mistral saved in the dataframe, it’s possible to compare them to the transaction categories and types defined earlier to check the accuracy of Mistral. The most basic LangChain eval metric is to do an exact string match of a prediction against a reference answer, which returns a score of 1 if correct, and a 0 if incorrect. The notebook gives an example of how to [implement this](https://oreil.ly/vPUfI), which shows that Mistral’s accuracy is 77.5%. However, if all you are doing is comparing strings, you probably don’t need to implement it in LangChain.    Where LangChain is valuable is in its standardized and tested approaches to implementing more advanced evaluators using LLMs. The evaluator `labeled_pairwise_string` compares two outputs and gives a reason for choosing between them, using GPT-4\\. One common use case for this type of evaluator is to compare the outputs from two different prompts or models, particularly if the models being tested are less sophisticated than GPT-4\\. This evaluator using GPT-4 does still work for evaluating GPT-4 responses, but you should manually review the reasoning and scores to ensure it is doing a good job: if GPT-4 is bad at a task, it may also be bad at evaluating that task. In [the notebook](https://oreil.ly/9O7Mb), the same transaction classification was run again with the model changed to `model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", model_kwargs={\"response_format\": {\"type\": \"json_object\"}},)`. Now it’s possible to do pairwise comparison between the Mistral and GPT-3.5 responses, as shown in the following example. You can see in the output the reasoning that is given to justify the score.    Input:    ```py # Evaluate answers using LangChain evaluators: from langchain.evaluation import load_evaluator evaluator = load_evaluator(\"labeled_pairwise_string\")  row = df.iloc[0] transaction = row[\"Transaction Description\"] gpt3pt5_category = row[\"gpt3.5_transaction_category\"] gpt3pt5_type = row[\"gpt3.5_transaction_type\"] mistral_category = row[\"mistral_transaction_category\"] mistral_type = row[\"mistral_transaction_type\"] reference_category = row[\"transaction_category\"] reference_type = row[\"transaction_type\"]  # Put the data into JSON format for the evaluator: gpt3pt5_data = f\"\"\"{{ `\"transaction_category\": \"``{``gpt3pt5_category``}``\",`  `\"transaction_type\": \"``{``gpt3pt5_type``}``\"` `}}``\"\"\"`  `mistral_data` `=` `f``\"\"\"``{{` ``````", "````` `\"transaction_category\": \"``{``reference_category``}``\",`  `\"transaction_type\": \"``{``reference_type``}``\"` `}}``\"\"\"`  `# Set up the prompt input for context for the evaluator:` `input_prompt` `=` `\"\"\"You are an expert at analyzing bank` `transactions,` `you will be categorizing a single transaction.` `Always return a transaction type and category: do not` `return None.` `Format Instructions:` `{format_instructions}` ```py` `Transaction Text:` `{transaction}` ``` `\"\"\"`  `transaction_types``.``append``(``transaction_type_score``)` `transaction_categories``.``append``(`     `transaction_category_score``)`  `accuracy_score` `=` `0`  `for` `transaction_type_score``,` `transaction_category_score` \\     `in` `zip``(`         `transaction_types``,` `transaction_categories`     `):`     `accuracy_score` `+=` `transaction_type_score``[``'score'``]` `+` \\     `transaction_category_score``[``'score'``]`  `accuracy_score` `=` `accuracy_score` `/` `(``len``(``transaction_types``)` \\     `*` `2``)` `print``(``f``\"Accuracy score:` `{``accuracy_score``}``\"``)`  `evaluator``.``evaluate_string_pairs``(`     `prediction``=``gpt3pt5_data``,`     `prediction_b``=``mistral_data``,`     `input``=``input_prompt``.``format``(`         `format_instructions``=``output_parser``.``get_format_instructions``(),`         `transaction``=``transaction``),`     `reference``=``reference_data``,` `)` ```py ```` ```py`` `````", "```   ```", "````Output:    ```py {'reasoning': '''Both Assistant A and Assistant B provided the exact same response to the user\\'s question. Their responses are both helpful, relevant, correct, and demonstrate depth of thought. They both correctly identified the transaction type as \"Deposit\" and the transaction category as \"Other\" based on the transaction text provided by the user. Both responses are also well-formatted according to the JSON schema provided by the user. Therefore, it\\'s a tie between the two assistants. \\n\\nFinal Verdict: [[C]]''',  'value': None,  'score': 0.5} ```    This code demonstrates the simple exact string matching evaluator from LangChain:    1.  `evaluator = load_evaluator(\"labeled_pairwise_string\")`: This is a helper function that can be used to load any LangChain evaluator by name. In this case, it is the `labeled_pairwise_string` evaluator being used.           2.  `row = df.iloc[0]`: This line and the seven lines that follow get the first row and extract the values for the different columns needed. It includes the transaction description, as well as the Mistral and GPT-3.5 transaction category and types. This is showcasing a single transaction, but this code can easily run in a loop through each transaction, replacing this line with an `iterrows` function `for i, row in tqdm(df.iterrows(), total=len(df)):`, as is done later in [the notebook](https://oreil.ly/dcCOO).           3.  `gpt3pt5_data = f\"\"\"{{`: To use the pairwise comparison evaluator, we need to pass the results in a way that is formatted correctly for the prompt. This is done for Mistral and GPT-3.5, as well as the reference data.           4.  `input_prompt = \"\"\"You are an expert...`: The other formatting we have to get right is in the prompt. To get accurate evaluation scores, the evaluator needs to see the instructions that were given for the task.           5.  `evaluator.evaluate_string_pairs(...`: All that remains is to run the evaluator by passing in the `prediction` and `prediction_b` (GPT-3.5 and Mistral, respectively), as well as the `input` prompt, and `reference` data, which serves as the ground truth.           6.  Following this code [in the notebook](https://oreil.ly/hW8Wr), there is an example of looping through and running the evaluator on every row in the dataframe and then saving the results and reasoning back to the dataframe.              This example demonstrates how to use a LangChain evaluator, but there are many different kinds of evaluator available. String distance ([Levenshtein](https://oreil.ly/Al5G3)) or [embedding distance](https://oreil.ly/0p_nE) evaluators are often used in scenarios where answers are not an exact match for the reference answer, but only need to be close enough semantically. Levenshtein distance allows for fuzzy matches based on how many single-character edits would be needed to transform the predicted text into the reference text, and embedding distance makes use of vectors (covered in [Chapter 5](ch05.html#vector_databases_05)) to calculate similarity between the answer and reference.    The other kind of evaluator we often use in our work is pairwise comparisons, which are useful for comparing two different prompts or models, using a smarter model like GPT-4\\. This type of comparison is helpful because reasoning is provided for each comparison, which can be useful in debugging why one approach was favored over another. The [notebook for this section](https://oreil.ly/iahTJ) shows an example of using a pairwise comparison evaluator to check GPT-3.5-turbo’s accuracy versus Mixtral 8x7b.    # Evaluate Quality    Without defining an appropriate set of eval metrics to define success, it can be difficult to tell if changes to the prompt or wider system are improving or harming the quality of responses. If you can automate eval metrics using smart models like GPT-4, you can iterate faster to improve results without costly or time-consuming manual human review.```py` ````", "```py```", "``` ```", "````` # OpenAI Function Calling    *Function calling* provides an alternative method to output parsers, leveraging fine-tuned OpenAI models. These models identify when a function should be executed and generate a JSON response with the *name and arguments* for a predefined function. Several use cases include:    Designing sophisticated chat bots      Capable of organizing and managing schedules. For example, you can define a function to schedule a meeting: `schedule_meeting(date: str, time: str, attendees: List[str])`.      Convert natural language into actionable API calls      A command like “Turn on the hallway lights” can be converted to `control_device(device: str, action: 'on' | 'off')` for interacting with your home automation API.      Extracting structured data      This could be done by defining a function such as `extract_contextual_data(context: str, data_points: List[str])` or `search_database(query: str)`.      Each function that you use within function calling will require an appropriate *JSON schema*. Let’s explore an example with the `OpenAI` package:    ```py from openai import OpenAI import json from os import getenv  def schedule_meeting(date, time, attendees):     # Connect to calendar service:     return { \"event_id\": \"1234\", \"status\": \"Meeting scheduled successfully!\",             \"date\": date, \"time\": time, \"attendees\": attendees }  OPENAI_FUNCTIONS = {     \"schedule_meeting\": schedule_meeting } ```    After importing `OpenAI` and `json`, you’ll create a function named `schedule_meeting`. This function is a mock-up, simulating the process of scheduling a meeting, and returns details such as `event_id`, `date`, `time`, and `attendees`. Following that, make an `OPENAI_FUNCTIONS` dictionary to map the function name to the actual function for ease of reference.    Next, define a `functions` list that provides the function’s JSON schema. This schema includes its name, a brief description, and the parameters it requires, guiding the LLM on how to interact with it:    ```py # Our predefined function JSON schema: functions = [     {         \"type\": \"function\",         \"function\": {             \"type\": \"object\",             \"name\": \"schedule_meeting\",             \"description\": '''Set a meeting at a specified date and time for  designated attendees''',             \"parameters\": {                 \"type\": \"object\",                 \"properties\": {                     \"date\": {\"type\": \"string\", \"format\": \"date\"},                     \"time\": {\"type\": \"string\", \"format\": \"time\"},                     \"attendees\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},                 },                 \"required\": [\"date\", \"time\", \"attendees\"],             },         },     } ] ```    # Specify Format    When using function calling with your OpenAI models, always ensure to define a detailed JSON schema (including the name and description). This acts as a blueprint for the function, guiding the model to understand when and how to properly invoke it.    After defining the functions, let’s make an OpenAI API request. Set up a `messages` list with the user query. Then, using an OpenAI `client` object, you’ll send this message and the function schema to the model. The LLM analyzes the conversation, discerns a need to trigger a function, and provides the function name and arguments. The `function` and `function_args` are parsed from the LLM response. Then the function is executed, and its results are added back into the conversation. Then you call the model again for a user-friendly summary of the entire process.    Input:    ```py client = OpenAI(api_key=getenv(\"OPENAI_API_KEY\"))  # Start the conversation: messages = [     {         \"role\": \"user\",         \"content\": '''Schedule a meeting on 2023-11-01 at 14:00  with Alice and Bob.''',     } ]  # Send the conversation and function schema to the model: response = client.chat.completions.create(     model=\"gpt-3.5-turbo-1106\",     messages=messages,     tools=functions, )  response = response.choices[0].message  # Check if the model wants to call our function: if response.tool_calls:     # Get the first function call:     first_tool_call = response.tool_calls[0]      # Find the function name and function args to call:     function_name = first_tool_call.function.name     function_args = json.loads(first_tool_call.function.arguments)     print(\"This is the function name: \", function_name)     print(\"These are the function arguments: \", function_args)      function = OPENAI_FUNCTIONS.get(function_name)      if not function:         raise Exception(f\"Function {function_name} not found.\")      # Call the function:     function_response = function(**function_args)      # Share the function's response with the model:     messages.append(         {             \"role\": \"function\",             \"name\": \"schedule_meeting\",             \"content\": json.dumps(function_response),         }     )      # Let the model generate a user-friendly response:     second_response = client.chat.completions.create(         model=\"gpt-3.5-turbo-0613\", messages=messages     )      print(second_response.choices[0].message.content) ```    Output:    ```py These are the function arguments:  {'date': '2023-11-01', 'time': '14:00', 'attendees': ['Alice', 'Bob']} This is the function name:  schedule_meeting I have scheduled a meeting on 2023-11-01 at 14:00 with Alice and Bob. The event ID is 1234. ```    Several important points to note while function calling:    *   It’s possible to have many functions that the LLM can call.           *   OpenAI can hallucinate function parameters, so be more explicit within the `system` message to overcome this.           *   The `function_call` parameter can be set in various ways:               *   To mandate a specific function call: `tool_choice: {\"type: \"function\", \"function\": {\"name\": \"my_function\"}}}`.                       *   For a user message without function invocation: `tool_choice: \"none\"`.                       *   By default (`tool_choice: \"auto\"`), the model autonomously decides if and which function to call.                      # Parallel Function Calling    You can set your chat messages to include intents that request simultaneous calls to multiple tools. This strategy is known as *parallel function calling*.    Modifying the previously used code, the `messages` list is updated to mandate the scheduling of two meetings:    ```py # Start the conversation: messages = [     {         \"role\": \"user\",         \"content\": '''Schedule a meeting on 2023-11-01 at 14:00 with Alice  and Bob. Then I want to schedule another meeting on 2023-11-02 at  15:00 with Charlie and Dave.'''     } ] ```    Then, adjust the previous code section by incorporating a `for` loop.    Input:    ```py # Send the conversation and function schema to the model: response = client.chat.completions.create(     model=\"gpt-3.5-turbo-1106\",     messages=messages,     tools=functions, )  response = response.choices[0].message  # Check if the model wants to call our function: if response.tool_calls:     for tool_call in response.tool_calls:         # Get the function name and arguments to call:         function_name = tool_call.function.name         function_args = json.loads(tool_call.function.arguments)         print(\"This is the function name: \", function_name)         print(\"These are the function arguments: \", function_args)          function = OPENAI_FUNCTIONS.get(function_name)          if not function:             raise Exception(f\"Function {function_name} not found.\")          # Call the function:         function_response = function(**function_args)          # Share the function's response with the model:         messages.append(             {                 \"role\": \"function\",                 \"name\": function_name,                 \"content\": json.dumps(function_response),             }         )      # Let the model generate a user-friendly response:     second_response = client.chat.completions.create(         model=\"gpt-3.5-turbo-0613\", messages=messages     )      print(second_response.choices[0].message.content) ```    Output:    ```py This is the function name:  schedule_meeting These are the function arguments:  {'date': '2023-11-01', 'time': '14:00', 'attendees': ['Alice', 'Bob']} This is the function name:  schedule_meeting These are the function arguments:  {'date': '2023-11-02', 'time': '15:00', 'attendees': ['Charlie', 'Dave']} Two meetings have been scheduled: 1. Meeting with Alice and Bob on 2023-11-01 at 14:00. 2. Meeting with Charlie and Dave on 2023-11-02 at 15:00. ```    From this example, it’s clear how you can effectively manage multiple function calls. You’ve seen how the `schedule_meeting` function was called twice in a row to arrange different meetings. This demonstrates how flexibly and effortlessly you can handle varied and complex requests using AI-powered tools.    # Function Calling in LangChain    If you’d prefer to avoid writing JSON schema and simply want to extract structured data from an LLM response, then LangChain allows you to use function calling with Pydantic.    Input:    ```py from langchain.output_parsers.openai_tools import PydanticToolsParser from langchain_core.utils.function_calling import convert_to_openai_tool from langchain_core.prompts import ChatPromptTemplate from langchain_openai.chat_models import ChatOpenAI from langchain_core.pydantic_v1 import BaseModel, Field from typing import Optional  class Article(BaseModel):     \"\"\"Identifying key points and contrarian views in an article.\"\"\"      points: str = Field(..., description=\"Key points from the article\")     contrarian_points: Optional[str] = Field(         None, description=\"Any contrarian points acknowledged in the article\"     )     author: Optional[str] = Field(None, description=\"Author of the article\")  _EXTRACTION_TEMPLATE = \"\"\"Extract and save the relevant entities mentioned \\ in the following passage together with their properties.  If a property is not present and is not required in the function parameters, do not include it in the output.\"\"\"  # Create a prompt telling the LLM to extract information: prompt = ChatPromptTemplate.from_messages(     {(\"system\", _EXTRACTION_TEMPLATE), (\"user\", \"{input}\")} )  model = ChatOpenAI()  pydantic_schemas = [Article]  # Convert Pydantic objects to the appropriate schema: tools = [convert_to_openai_tool(p) for p in pydantic_schemas]  # Give the model access to these tools: model = model.bind_tools(tools=tools)  # Create an end to end chain: chain = prompt | model | PydanticToolsParser(tools=pydantic_schemas)  result = chain.invoke(     {         \"input\": \"\"\"In the recent article titled 'AI adoption in industry,'  key points addressed include the growing interest ... However, the  author, Dr. Jane Smith, ...\"\"\"     } ) print(result) ```    Output:    ```py [Article(points='The growing interest in AI in various sectors, ...', contrarian_points='Without stringent regulations, ...', author='Dr. Jane Smith')] ```    You’ll start by importing various modules, including `PydanticToolsParser` and `ChatPromptTemplate`, essential for parsing and templating your prompts. Then, you’ll define a Pydantic model, `Article`, to specify the structure of the information you want to extract from a given text. With the use of a custom prompt template and the ChatOpenAI model, you’ll instruct the AI to extract key points and contrarian views from an article. Finally, the extracted data is neatly converted into your predefined Pydantic model and printed out, allowing you to see the structured information pulled from the text.    There are several key points, including:    Converting Pydantic schema to OpenAI tools      `tools = [convert_to_openai_tool(p) for p in pydantic_schemas]`      Binding the tools directly to the LLM      `model = model.bind_tools(tools=tools)`      Creating an LCEL chain that contains a tools parser      `chain = prompt | model | PydanticToolsParser(tools=pydantic_schemas)`      # Extracting Data with LangChain    The `create_extraction_chain_pydantic` function provides a more concise version of the previous implementation. By simply inserting a Pydantic model and an LLM that supports function calling, you can easily achieve parallel function calling.    Input:    ```py from langchain.chains.openai_tools import create_extraction_chain_pydantic from langchain_openai.chat_models import ChatOpenAI from langchain_core.pydantic_v1 import BaseModel, Field  # Make sure to use a recent model that supports tools: model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")  class Person(BaseModel):     \"\"\"A person's name and age.\"\"\"      name: str = Field(..., description=\"The person's name\")     age: int = Field(..., description=\"The person's age\")  chain = create_extraction_chain_pydantic(Person, model) chain.invoke({'input':'''Bob is 25 years old. He lives in New York. He likes to play basketball. Sarah is 30 years old. She lives in San Francisco. She likes to play tennis.'''}) ```    Output:    ```py [Person(name='Bob', age=25), Person(name='Sarah', age=30)] ```    The `Person` Pydantic model has two properties, `name` and `age`; by calling the `create_extraction_chain_pydantic` function with the input text, the LLM invokes the same function twice and creates two `People` objects.    # Query Planning    You may experience problems when user queries have multiple intents with intricate dependencies. *Query planning* is an effective way to parse a user’s query into a series of steps that can be executed as a query graph with relevant dependencies:    ```py from langchain_openai.chat_models import ChatOpenAI from langchain.output_parsers.pydantic import PydanticOutputParser from langchain_core.prompts.chat import (     ChatPromptTemplate,     SystemMessagePromptTemplate, ) from pydantic.v1 import BaseModel, Field from typing import List  class Query(BaseModel):     id: int     question: str     dependencies: List[int] = Field(         default_factory=list,         description=\"\"\"A list of sub-queries that must be completed before  this task can be completed.  Use a sub query when anything is unknown and we might need to ask  many queries to get an answer.  Dependencies must only be other queries.\"\"\"     )  class QueryPlan(BaseModel):     query_graph: List[Query] ```    Defining `QueryPlan` and `Query` allows you to first ask an LLM to parse a user’s query into multiple steps. Let’s investigate how to create the query plan.    Input:    ```py # Set up a chat model: model = ChatOpenAI()  # Set up a parser: parser = PydanticOutputParser(pydantic_object=QueryPlan)  template = \"\"\"Generate a query plan. This will be used for task execution.  Answer the following query: {query} `Return the following query graph format:` `{format_instructions}` ``` `\"\"\"` `system_message_prompt` `=` `SystemMessagePromptTemplate``.``from_template``(``template``)` `chat_prompt` `=` `ChatPromptTemplate``.``from_messages``([``system_message_prompt``])`  `# Create the LCEL chain with the prompt, model, and parser:` `chain` `=` `chat_prompt` `|` `model` `|` `parser`  `result` `=` `chain``.``invoke``({` `\"query\"``:``'''I want to get the results from my database. Then I want to find` `out what the average age of my top 10 customers is. Once I have the average` `age, I want to send an email to John. Also I just generally want to send a` `welcome introduction email to Sarah, regardless of the other tasks.'''``,` `\"format_instructions\"``:``parser``.``get_format_instructions``()})`  `print``(``result``.``query_graph``)` ```py ```   ```py`` ````Output:    ```py [Query(id=1, question='Get top 10 customers', dependencies=[]), Query(id=2, question='Calculate average age of customers', dependencies=[1]), Query(id=3, question='Send email to John', dependencies=[2]), Query(id=4, question='Send welcome email to Sarah', dependencies=[])] ```    Initiate a `ChatOpenAI` instance and create a `PydanticOutputParser` for the `QueryPlan` structure. Then the LLM response is called and parsed, producing a structured `query_graph` for your tasks with their unique dependencies.```py` `````", "```py```", "```py```", "``` from langchain_openai.chat_models import ChatOpenAI from langchain_core.prompts import (     FewShotChatMessagePromptTemplate,     ChatPromptTemplate, )  examples = [     {         \"question\": \"What is the capital of France?\",         \"answer\": \"Paris\",     },     {         \"question\": \"What is the capital of Spain?\",         \"answer\": \"Madrid\",     } # ...more examples... ] ```", "``` example_prompt = ChatPromptTemplate.from_messages(     [         (\"human\", \"{question}\"),         (\"ai\", \"{answer}\"),     ] )  few_shot_prompt = FewShotChatMessagePromptTemplate(     example_prompt=example_prompt,     examples=examples, )  print(few_shot_prompt.format()) ```", "``` Human: What is the capital of France? AI: Paris Human: What is the capital of Spain? AI: Madrid ...more examples... ```", "``` from langchain_core.output_parsers import StrOutputParser  final_prompt = ChatPromptTemplate.from_messages(     [(\"system\",'''You are responsible for answering  questions about countries. Only return the country  name.'''),     few_shot_prompt,(\"human\", \"{question}\"),] )  model = ChatOpenAI()  # Creating the LCEL chain with the prompt, model, and a StrOutputParser(): chain = final_prompt | model | StrOutputParser()  result = chain.invoke({\"question\": \"What is the capital of America?\"})  print(result) ```", "``` Washington, D.C. ```", "``` from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate from langchain.prompts.example_selector import LengthBasedExampleSelector from langchain_openai.chat_models import ChatOpenAI from langchain_core.messages import SystemMessage import tiktoken  examples = [     {\"input\": \"Gollum\", \"output\": \"<Story involving Gollum>\"},     {\"input\": \"Gandalf\", \"output\": \"<Story involving Gandalf>\"},     {\"input\": \"Bilbo\", \"output\": \"<Story involving Bilbo>\"}, ]  story_prompt = PromptTemplate(     input_variables=[\"input\", \"output\"],     template=\"Character: {input}\\nStory: {output}\", )  def num_tokens_from_string(string: str) -> int:     \"\"\"Returns the number of tokens in a text string.\"\"\"     encoding = tiktoken.get_encoding(\"cl100k_base\")     num_tokens = len(encoding.encode(string))     return num_tokens  example_selector = LengthBasedExampleSelector(     examples=examples,     example_prompt=story_prompt,     max_length=1000, # 1000 tokens are to be included from examples     # get_text_length: Callable[[str], int] = lambda x: len(re.split(\"\\n| \", x))     # You have modified the get_text_length function to work with the     # TikToken library based on token usage:     get_text_length=num_tokens_from_string, ) ```", "``` dynamic_prompt = FewShotPromptTemplate(     example_selector=example_selector,     example_prompt=story_prompt,     prefix='''Generate a story for {character} using the  current Character/Story pairs from all of the characters  as context.''',     suffix=\"Character: {character}\\nStory:\",     input_variables=[\"character\"], )  # Provide a new character from Lord of the Rings: formatted_prompt = dynamic_prompt.format(character=\"Frodo\")  # Creating the chat model: chat = ChatOpenAI()  response = chat.invoke([SystemMessage(content=formatted_prompt)]) print(response.content) ```", "``` Frodo was a young hobbit living a peaceful life in the Shire. However, his life... ```", "``` result = model.invoke([SystemMessage(content=formatted_prompt)]) ```", "``` from langchain_core.prompts import PromptTemplate, load_prompt  prompt = PromptTemplate(     template='''Translate this sentence from English to Spanish.     \\nSentence: {sentence}\\nTranslation:''',     input_variables=[\"sentence\"], )  prompt.save(\"translation_prompt.json\")  # Loading the prompt template: load_prompt(\"translation_prompt.json\") # Returns PromptTemplate() ```", "``` from langchain_community.document_loaders import Docx2txtLoader from langchain_community.document_loaders import PyPDFLoader from langchain_community.document_loaders.csv_loader import CSVLoader import glob from langchain.text_splitter import CharacterTextSplitter  # To store the documents across all data sources: all_documents = []  # Load the PDF: loader = PyPDFLoader(\"data/principles_of_marketing_book.pdf\") pages = loader.load_and_split() print(pages[0])  # Add extra metadata to each page: for page in pages:     page.metadata[\"description\"] = \"Principles of Marketing Book\"  # Checking that the metadata has been added: for page in pages[0:2]:     print(page.metadata)  # Saving the marketing book pages: all_documents.extend(pages)  csv_files = glob.glob(\"data/*.csv\")  # Filter to only include the word Marketing in the file name: csv_files = [f for f in csv_files if \"Marketing\" in f]  # For each .csv file: for csv_file in csv_files:     loader = CSVLoader(file_path=csv_file)     data = loader.load()     # Saving the data to the all_documents list:     all_documents.extend(data)  text_splitter = CharacterTextSplitter.from_tiktoken_encoder(     chunk_size=200, chunk_overlap=0 )  urls = [      '''https://storage.googleapis.com/oreilly-content/NutriFusion%20Foods%2  0Marketing%20Plan%202022.docx''',     '''https://storage.googleapis.com/oreilly-content/NutriFusion%20Foods%2  0Marketing%20Plan%202023.docx''', ]  docs = [] for url in urls:     loader = Docx2txtLoader(url.replace('\\n', ''))     pages = loader.load()     chunks = text_splitter.split_documents(pages)      # Adding the metadata to each chunk:     for chunk in chunks:         chunk.metadata[\"source\"] = \"NutriFusion Foods Marketing Plan - 2022/2023\"     docs.extend(chunks)  # Saving the marketing book pages: all_documents.extend(docs) ```", "``` page_content='Principles of Mark eting' metadata={'source': 'data/principles_of_marketing_book.pdf', 'page': 0} {'source': 'data/principles_of_marketing_book.pdf', 'page': 0, 'description': 'Principles of Marketing Book'} {'source': 'data/principles_of_marketing_book.pdf', 'page': 1, 'description': 'Principles of Marketing Book'} ```", "``` from langchain_text_splitters import CharacterTextSplitter  text = \"\"\" Biology is a fascinating and diverse field of science that explores the living world and its intricacies \\n\\n. It encompasses the study of life, its origins, diversity, structure, function, and interactions at various levels from molecules and cells to organisms and ecosystems \\n\\n. In this 1000-word essay, we will delve into the core concepts of biology, its history, key areas of study, and its significance in shaping our understanding of the natural world. \\n\\n ...(truncated to save space)... \"\"\" # No chunk overlap: text_splitter = CharacterTextSplitter.from_tiktoken_encoder( chunk_size=50, chunk_overlap=0, separator=\"\\n\", ) texts = text_splitter.split_text(text) print(f\"Number of texts with no chunk overlap: {len(texts)}\")  # Including a chunk overlap: text_splitter = CharacterTextSplitter.from_tiktoken_encoder( chunk_size=50, chunk_overlap=48, separator=\"\\n\", ) texts = text_splitter.split_text(text) print(f\"Number of texts with chunk overlap: {len(texts)}\") ```", "``` Number of texts with no chunk overlap: 3 Number of texts with chunk overlap: 6 ```", "``` from langchain.text_splitter import TokenTextSplitter from langchain_community.document_loaders import PyPDFLoader  text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=50) loader = PyPDFLoader(\"data/principles_of_marketing_book.pdf\") pages = loader.load_and_split(text_splitter=text_splitter)  print(len(pages)) #737 ```", "``` from langchain_text_splitters import RecursiveCharacterTextSplitter  text_splitter = RecursiveCharacterTextSplitter(     chunk_size=100,     chunk_overlap=20,     length_function=len, ) ```", "``` # Split the text into chunks: texts = text_splitter.split_text(text) ```", "``` # Create documents from the chunks: metadatas = {\"title\": \"Biology\", \"author\": \"John Doe\"} docs = text_splitter.create_documents(texts, metadatas=[metadatas] * len(texts)) ```", "``` text_splitter = RecursiveCharacterTextSplitter(chunk_size=300) splitted_docs = text_splitter.split_documents(docs) ```", "``` from langchain_core.prompts.chat import ChatPromptTemplate  character_generation_prompt = ChatPromptTemplate.from_template(     \"\"\"I want you to brainstorm three to five characters for my short story. The  genre is {genre}. Each character must have a Name and a Biography.  You must provide a name and biography for each character, this is very  important!  ---  Example response:  Name: CharWiz, Biography: A wizard who is a master of magic.  Name: CharWar, Biography: A warrior who is a master of the sword.  ---  Characters: \"\"\" )  plot_generation_prompt = ChatPromptTemplate.from_template(     \"\"\"Given the following characters and the genre, create an effective  plot for a short story:  Characters:  {characters}  ---  Genre: {genre}  ---  Plot: \"\"\"     )  scene_generation_plot_prompt = ChatPromptTemplate.from_template(     \"\"\"Act as an effective content creator.  Given multiple characters and a plot, you are responsible for  generating the various scenes for each act.   You must decompose the plot into multiple effective scenes:  ---  Characters:  {characters}  ---  Genre: {genre}  ---  Plot: {plot}  ---  Example response:  Scenes:  Scene 1: Some text here.  Scene 2: Some text here.  Scene 3: Some text here.  ----  Scenes:  \"\"\" ) ```", "``` from operator import itemgetter from langchain_core.runnables import RunnablePassthrough  chain = RunnablePassthrough() | {     \"genre\": itemgetter(\"genre\"),   } chain.invoke({\"genre\": \"fantasy\"}) # {'genre': 'fantasy'} ```", "``` from langchain_core.runnables import RunnableLambda  chain = RunnablePassthrough() | {     \"genre\": itemgetter(\"genre\"),     \"upper_case_genre\": lambda x: x[\"genre\"].upper(),     \"lower_case_genre\": RunnableLambda(lambda x: x[\"genre\"].lower()), } chain.invoke({\"genre\": \"fantasy\"}) # {'genre': 'fantasy', 'upper_case_genre': 'FANTASY', # 'lower_case_genre': 'fantasy'} ```", "``` from langchain_core.runnables import RunnableParallel  master_chain = RunnablePassthrough() | {     \"genre\": itemgetter(\"genre\"),     \"upper_case_genre\": lambda x: x[\"genre\"].upper(),     \"lower_case_genre\": RunnableLambda(lambda x: x[\"genre\"].lower()), }  master_chain_two = RunnablePassthrough() | RunnableParallel(         genre=itemgetter(\"genre\"),         upper_case_genre=lambda x: x[\"genre\"].upper(),         lower_case_genre=RunnableLambda(lambda x: x[\"genre\"].lower()), )  story_result = master_chain.invoke({\"genre\": \"Fantasy\"}) print(story_result)  story_result = master_chain_two.invoke({\"genre\": \"Fantasy\"}) print(story_result)  # master chain: {'genre': 'Fantasy', 'upper_case_genre': 'FANTASY', # 'lower_case_genre': 'fantasy'} # master chain two: {'genre': 'Fantasy', 'upper_case_genre': 'FANTASY', # 'lower_case_genre': 'fantasy'} ```", "``` from langchain_openai.chat_models import ChatOpenAI from langchain_core.output_parsers import StrOutputParser  # Create the chat model: model = ChatOpenAI()  # Create the subchains: character_generation_chain = ( character_generation_prompt | model | StrOutputParser() )  plot_generation_chain = ( plot_generation_prompt | model | StrOutputParser() )  scene_generation_plot_chain = ( scene_generation_plot_prompt | model | StrOutputParser()  ) ```", "``` from langchain_core.runnables import RunnableParallel from operator import itemgetter from langchain_core.runnables import RunnablePassthrough  master_chain = (     {\"characters\": character_generation_chain, \"genre\":     RunnablePassthrough()}     | RunnableParallel(         characters=itemgetter(\"characters\"),         genre=itemgetter(\"genre\"),         plot=plot_generation_chain,     )     | RunnableParallel(         characters=itemgetter(\"characters\"),         genre=itemgetter(\"genre\"),         plot=itemgetter(\"plot\"),         scenes=scene_generation_plot_chain,     ) )  story_result = master_chain.invoke({\"genre\": \"Fantasy\"}) ```", "``` {'characters': '''Name: Lyra, Biography: Lyra is a young elf who possesses ..\\n\\nName: Orion, Biography: Orion is a ..''', 'genre': {'genre': 'Fantasy'} 'plot': '''In the enchanted forests of a mystical realm, a great darkness looms, threatening to engulf the land and its inhabitants. Lyra, the young elf with a deep connection to nature, ...''', 'scenes': '''Scene 1: Lyra senses the impending danger in the forest ...\\n\\nScene 2: Orion, on his mission to investigate the disturbances in the forest...\\n\\nScene 9: After the battle, Lyra, Orion, Seraphina, Finnegan...'''} ```", "``` # Extracting the scenes using .split('\\n') and removing empty strings: scenes = [scene for scene in story_result[\"scenes\"].split(\"\\n\") if scene] generated_scenes = [] previous_scene_summary = \"\"  character_script_prompt = ChatPromptTemplate.from_template(     template=\"\"\"Given the following characters: {characters} and the genre:     {genre}, create an effective character script for a scene.   You must follow the following principles:  - Use the Previous Scene Summary: {previous_scene_summary} to avoid  repeating yourself.  - Use the Plot: {plot} to create an effective scene character script.  - Currently you are generating the character dialogue script for the  following scene: {scene} `---`  `Here is an example response:`  `SCENE 1: ANNA'S APARTMENT`   `(ANNA is sorting through old books when there is a knock at the door.`  `She opens it to reveal JOHN.)`  `ANNA: Can I help you, sir?`  `JOHN: Perhaps, I think it's me who can help you. I heard you're`  `researching time travel.`  `(Anna looks intrigued but also cautious.)`  `ANNA: That's right, but how do you know?`  `JOHN: You could say... I'm a primary source.`   `---`  `SCENE NUMBER:` `{index}` ```", "``` ```", "````` ```py`Technically, you could generate all of the scenes asynchronously. However, it’s beneficial to know what each character has done in the *previous scene to avoid repeating points*.    Therefore, you can create two LCEL chains, one for generating the character scripts per scene and the other for summarizations of previous scenes:    ``` # Loading a chat model: model = ChatOpenAI(model='gpt-3.5-turbo-16k')  # Create the LCEL chains: character_script_generation_chain = (     {         \"characters\": RunnablePassthrough(),         \"genre\": RunnablePassthrough(),         \"previous_scene_summary\": RunnablePassthrough(),         \"plot\": RunnablePassthrough(),         \"scene\": RunnablePassthrough(),         \"index\": RunnablePassthrough(),     }     | character_script_prompt     | model     | StrOutputParser() )  summarize_chain = summarize_prompt | model | StrOutputParser()  # You might want to use tqdm here to track the progress, # or use all of the scenes: for index, scene in enumerate(scenes[0:3]):      # # Create a scene generation:     scene_result = character_script_generation_chain.invoke(         {             \"characters\": story_result[\"characters\"],             \"genre\": \"fantasy\",             \"previous_scene_summary\": previous_scene_summary,             \"index\": index,         }     )      # Store the generated scenes:     generated_scenes.append(         {\"character_script\": scene_result, \"scene\": scenes[index]}     )      # If this is the first scene then we don't have a     # previous scene summary:     if index == 0:         previous_scene_summary = scene_result     else:         # If this is the second scene or greater then         # we can use and generate a summary:         summary_result = summarize_chain.invoke(             {\"character_script\": scene_result}         )         previous_scene_summary = summary_result ```py    First, you’ll establish a `character_script_generation_chain` in your script, utilizing various runnables like `RunnablePassthrough` for smooth data flow. Crucially, this chain integrates model = `ChatOpenAI(model='gpt-3.5-turbo-16k')`, a powerful model with a generous 16k context window, ideal for extensive content generation tasks. When invoked, this chain adeptly generates character scripts, drawing on inputs such as character profiles, genre, and scene specifics.    You dynamically enrich each scene by adding the summary of the previous scene, creating a simple yet effective buffer memory. This technique ensures continuity and context in the narrative, enhancing the LLM’s ability to generate coherent character scripts.    Additionally, you’ll see how the `StrOutputParser` elegantly converts model outputs into structured strings, making the generated content easily usable.    # Divide Labor    Remember, designing your tasks in a sequential chain greatly benefits from the Divide Labor principle. Breaking tasks down into smaller, manageable chains can increase the overall quality of your output. Each chain in the sequential chain contributes its individual effort toward achieving the overarching task goal.    Using chains gives you the ability to use different models. For example, using a smart model for the ideation and a cheap model for the generation usually gives optimal results. This also means you can have fine-tuned models on each step.```` ```py``  `````", "```py`## Structuring LCEL Chains    In LCEL you must ensure that the first part of your LCEL chain is a *runnable* type. The following code will throw an error:    ```", "```py    A Python dictionary with a value of 18 will not create a runnable LCEL chain. However, all of the following implementations will work:    ```", "```py    Sequential chains are great at incrementally building generated knowledge that is used by future chains, but they often yield slower response times due to their sequential nature. As such, `SequentialChain` data pipelines are best suited for server-side tasks, where immediate responses are not a priority and users aren’t awaiting real-time feedback.    ## Document Chains    Let’s imagine that before accepting your generated story, the local publisher has requested that you provide a summary based on all of the character scripts. This is a good use case for *document chains* because you need to provide an LLM with a large amount of text that wouldn’t fit within a single LLM request due to the context length restrictions.    Before delving into the code, let’s first get a sense of the broader picture. The script you are going to see performs a text summarization task on a collection of scenes.    Remember to install Pandas with `pip install pandas`.    Now, let’s start with the first set of code:    ```", "```py    These lines are importing all the necessary tools you need. `CharacterTextSplitter` and `load_summarize_chain` are from the LangChain package and will help with text processing, while Pandas (imported as `pd`) will help manipulate your data.    Next, you’ll be dealing with your data:    ```", "```py    Here, you create a Pandas DataFrame from the `generated_scenes` variable, effectively converting your raw scenes into a tabular data format that Pandas can easily manipulate.    Then you need to consolidate your text:    ```", "```py    In this line, you’re transforming the `character_script` column from your DataFrame into a single text string. Each entry in the column is converted into a list item, and all items are joined together with new lines in between, resulting in a single string that contains all character scripts.    Once you have your text ready, you prepare it for the summarization process:    ```", "```py    Here, you create a `CharacterTextSplitter` instance using its class method `from_tiktoken_encoder`, with specific parameters for chunk size and overlap. You then use this text splitter to split your consolidated script text into chunks suitable for processing by your summarization tool.    Next, you set up your summarization tool:    ```", "```py    This line is about setting up your summarization process. You’re calling a function that loads a summarization chain with a chat model in a `map-reduce` style approach.    Then you run the summarization:    ```", "```py    This is where you actually perform the text summarization. The `invoke` method executes the summarization on the chunks of text you prepared earlier and stores the summary into a variable.    Finally, you print the result:    ```", "```py    This is the culmination of all your hard work. The resulting summary text is printed to the console for you to see.    This script takes a collection of scenes, consolidates the text, chunks it up, summarizes it, and then prints the summary:    ```", "```py    Output:    ```", "```py    It’s worth noting that even though you’ve used a `map_reduce` chain, there are four core chains for working with `Document` objects within LangChain.    ## Stuff    The document insertion chain, also referred to as the *stuff* chain (drawing from the concept of *stuffing* or *filling*), is the simplest approach among various document chaining strategies. [Figure 4-5](#figure-4-5) illustrates the process of integrating multiple documents into a single LLM request.  ![Stuff Documents Chain](assets/pega_0405.png)  ###### Figure 4-5\\. Stuff documents chain    ## Refine    The refine documents chain ([Figure 4-6](#figure-4-6)) creates an LLM response through a cyclical process that *iteratively updates its output*. During each loop, it combines the current output (derived from the LLM) with the current document. Another LLM request is made to *update the current output*. This process continues until all documents have been processed.  ![Refine Documents Chain](assets/pega_0406.png)  ###### Figure 4-6\\. Refine documents chain    ## Map Reduce    The map reduce documents chain in [Figure 4-7](#figure-4-7) starts with an LLM chain to each separate document (a process known as the Map step), interpreting the resulting output as a newly generated document.    Subsequently, all these newly created documents are introduced to a distinct combine documents chain to formulate a singular output (a process referred to as the Reduce step). If necessary, to ensure the new documents seamlessly fit into the context length, an optional compression process is used on the mapped documents. If required, this compression happens recursively.  ![Map Reduce Chain](assets/pega_0407.png)  ###### Figure 4-7\\. Map reduce documents chain    ## Map Re-rank    There is also map re-rank, which operates by executing an initial prompt on each document. This not only strives to fulfill a given task but also assigns a confidence score reflecting the certainty of its answer. The response with the highest confidence score is then selected and returned.    [Table 4-1](#table-4-1) demonstrates the advantages and disadvantages for choosing a specific document chain strategy.      Table 4-1\\. Overview of document chain strategies   | Approach | Advantages | Disadvantages | | --- | --- | --- | | Stuff Documents Chain | Simple to implement. Ideal for scenarios with small documents and few inputs. | May not be suitable for handling large documents or multiple inputs due to prompt size limitation. | | Refine Documents Chain | Allows iterative refining of the response. More control over each step of response generation. Good for progressive extraction tasks. | Might not be optimal for real-time applications due to the loop process. | | Map Reduce Documents Chain | Enables independent processing of each document. Can handle large datasets by reducing them into manageable chunks. | Requires careful management of the process. Optional compression step can add complexity and loses document order. | | Map Re-rank Documents Chain | Provides a confidence score for each answer, allowing for better selection of responses. | The ranking algorithm can be complex to implement and manage. May not provide the best answer if the scoring mechanism is not reliable or well-tuned. |    You can read more about how to implement different document chains in [LangChain’s comprehensive API](https://oreil.ly/FQUK_) and [here](https://oreil.ly/9xr_6).    Also, it’s possible to simply change the chain type within the `load_summarize_chain` function:    ```", "```py    There are newer, more customizable approaches to creating summarization chains using LCEL, but for most of your needs `load_summarize_chain` provides sufficient results.```", "```py``  `` `# Summary    In this chapter, you comprehensively reviewed the LangChain framework and its essential components. You learned about the importance of document loaders for gathering data and the role of text splitters in handling large text blocks.    Moreover, you were introduced to the concepts of task decomposition and prompt chaining. By breaking down complex problems into smaller tasks, you saw the power of problem isolation. Furthermore, you now grasp how prompt chaining can combine multiple inputs/outputs for richer idea generation.    In the next chapter, you’ll learn about vector databases, including how to integrate these with documents from LangChain, and this ability will serve a pivotal role in enhancing the accuracy of knowledge extraction from your data.` `` ```", "```py ```", "```py` ```", "```py`` ```", "```py```", "``````py```` ```py``````", "``````py``````", "```py```", "````py``` ````", "```````py` ``````py```````", "``````py``````"]