- en: Chapter 10\. AI in the Browser with TensorFlow.js and ml5.js
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Written in collaboration with guest author: Zaid Alyafeai'
  prefs: []
  type: TYPE_NORMAL
- en: You’re a developer who dreams big. You have a kickass AI model that you would
    like a lot of people to try. How many is a lot? Ten thousand? A million? No, silly.
    You like to dream big. How about 100 million people? That’s a nice round number.
    Now convincing 100 million people to download and install an app and make space
    for it on their phones is not an easy sell. But what if we told you that they
    all have an app already installed, just for you. No downloads. No installs. No
    app stores. What kind of black magic is this!? Of course, it’s the web browser.
    And as a bonus, it also runs on your PC.
  prefs: []
  type: TYPE_NORMAL
- en: This is what Google did with its home page when it decided to launch its first-ever
    AI doodle to their billions of users ([Figure 10-1](part0012.html#the_bach_music_harmonizer_doodle_from_go)).
    And what better theme to pick for it than the music of J.S. Bach. (Bach’s parents
    wanted to call him J.S. Bach, 310 years before JavaScript was even created. They
    had quite the foresight!)
  prefs: []
  type: TYPE_NORMAL
- en: To explain briefly, the doodle allowed anyone to write one line (voice) of random
    notes for two measures using mouse clicks. When the user clicked a button labeled
    Harmonize, the input would then be processed against hundreds of musical pieces
    written by Bach that contain between two and four lines (voices) of music. The
    system would figure out which notes would sound best along with the user’s input
    to create a much richer Bach-like sounding musical piece. The entire process ran
    in the browser, so Google would not need to scale up its machine learning prediction
    infrastructure at all.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Bach music harmonizer doodle from Google](../images/00222.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. The Bach music harmonizer [doodle](https://oreil.ly/BYFfg) from
    Google
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In addition to the cost savings and the ability to run on any platform, with
    a browser we can provide users with a richer, more interactive experience because
    network latency is not a factor. And of course, because everything can be run
    locally after the model is downloaded, the end user can benefit from the privacy
    of their data.
  prefs: []
  type: TYPE_NORMAL
- en: Given that JavaScript is the language of the web browser, it’s useful for us
    to delve into JavaScript-based deep learning libraries that can run our trained
    model within users’ browsers. And that’s exactly what we do in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we focus on implementing deep learning models in the browser. First, we
    look at a brief history of different JavaScript-based deep learning frameworks
    before moving on to TensorFlow.js and eventually a higher-level abstraction for
    it called ml5.js. We also examine a few complex browser-based applications such
    as detecting the body pose of a person or converting a hand-drawn doodle to a
    photograph (using GANs). Finally, we talk about some practical considerations
    and showcase some real-world case studies.
  prefs: []
  type: TYPE_NORMAL
- en: 'JavaScript-Based Machine Learning Libraries: A Brief History'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the breakthrough of deep learning in recent years, many attempts have
    been made to make AI accessible to a wider range of people in the form of web-based
    libraries. [Table 10-1](part0012.html#historical_overview_of_different_javascr)
    offers a brief overview of the different libraries in the order in which they
    were first released.
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-1\. Historical overview of different JavaScript-based deep learning
    libraries (data captured as of August 2019)
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Active years** | **★ on GitHub** | **Known for** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| brain.js | 2015–present | 9,856 | Neural networks, RNNs, LSTMs, and GRUs
    |'
  prefs: []
  type: TYPE_TB
- en: '| ConvNetJS | 2014–2016 | 9,735 | Neural networks, CNNs |'
  prefs: []
  type: TYPE_TB
- en: '| Synaptic | 2014–present | 6,571 | Neural networks, LSTMs |'
  prefs: []
  type: TYPE_TB
- en: '| MXNetJS | 2015–2017 | 420 | Running MXNet models |'
  prefs: []
  type: TYPE_TB
- en: '| Keras.js | 2016–2017 | 4,562 | Running Keras models |'
  prefs: []
  type: TYPE_TB
- en: '| CaffeJS | 2016–2017 | 115 | Running Caffe models |'
  prefs: []
  type: TYPE_TB
- en: '| TensorFlow.js (formerly known as deeplearn.js) | 2017–present | 11,282 |
    Running TensorFlow models on GPU |'
  prefs: []
  type: TYPE_TB
- en: '| ml5.js | 2017–present | 2,818 | Easy to use on top of TF.js. |'
  prefs: []
  type: TYPE_TB
- en: '| ONNX.js | 2018–present | 853 | Speed, running ONNX models |'
  prefs: []
  type: TYPE_TB
- en: Let’s go through a few of these libraries in more detail and see how they evolved.
  prefs: []
  type: TYPE_NORMAL
- en: ConvNetJS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[ConvNetJS](https://oreil.ly/URdv9) is a JavaScript library that was designed
    in 2014 by Andrej Karpathy as part of a course during his Ph.D. at Stanford University.
    It trained CNNs in the browser, an exciting proposition, especially in 2014, considering
    the AI hype was starting to take off, and a developer wouldn’t have had to go
    through an elaborate and painful setup process to get running. ConvNetJS helped
    introduce AI to so many people for the first time with interactive training demonstrations
    in the browser.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In fact, when MIT scientist Lex Fridman taught his popular self-driving course
    in 2017, he challenged students worldwide to train a simulated autonomous car
    using reinforcement learning—in the browser using ConvNetJS—as shown in [Figure 10-2](part0012.html#screenshot_of_deeptrafficcomma_training).
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of DeepTraffic, training a car with reinforcement learning using
    ConvNetJS](../images/00290.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. Screenshot of DeepTraffic training a car with reinforcement learning
    using ConvNetJS
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Keras.js
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keras.js was introduced in 2016 by Leon Chen. It was a Keras port made to work
    in the browser by using JavaScript. Keras.js used WebGL to run computations on
    the GPU. It used shaders (special operations for pixel rendering) to run inferences,
    which made them run much faster than using just the CPU. Additionally, Keras.js
    could run on a Node.js server on a CPU to provide server-based inferences. Keras.js
    implemented a handful of convolutional, dense, pooling, activation, and RNN layers.
    It is no longer under active development.
  prefs: []
  type: TYPE_NORMAL
- en: ONNX.js
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Created by Microsoft in 2018, ONNX.js is a JavaScript library for running ONNX
    models in browsers and on Node.js. ONNX is an open standard for representing machine
    learning models that is a collaboration between Microsoft, Facebook, Amazon, and
    others. ONNX.js is surprisingly fast. In fact, faster than even TensorFlow.js
    (discussed in the next section) in early benchmarks, as shown in [Figure 10-3](part0012.html#benchmarking_data_for_resnet-50-id00001)
    and [Figure 10-4](part0012.html#benchmarking_data_for_resnet-50_on_diffe). This
    could be attributed to the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: ONNX.js utilizes WebAssembly (from Mozilla) for execution on the CPU and WebGL
    on the GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WebAssembly allows it to run C/C++ and Rust programs in the web browser while
    providing near-native performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WebGL provides GPU-accelerated computations like image processing within the
    browser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although browsers tend to be single-threaded, ONNX.js uses Web Workers to provide
    a multithreaded environment in the background for parallelizing data operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Benchmarking data for ResNet-50 on different JavaScript machine learning
    libraries on CPU (data source: https://github.com/microsoft/onnxjs)](../images/00297.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. Benchmarking data for ResNet-50 on different JavaScript machine
    learning libraries on CPU ([data source](https://github.com/microsoft/onnxjs))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Benchmarking data for ResNet-50 on different JavaScript machine learning
    libraries on GPU (data source: https://github.com/microsoft/onnxjs)](../images/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. Benchmarking data for ResNet-50 on different JavaScript machine
    learning libraries on GPU ([data source](https://github.com/microsoft/onnxjs))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TensorFlow.js
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some libraries offered the ability to train within the browser (e.g., ConvNetJS),
    whereas other libraries offered blazing-fast performance (e.g., the now-defunct
    TensorFire). deeplearn.js from Google was the first library that supported fast
    GPU accelerated operations using WebGL while also providing the ability to define,
    train, and infer within the browser. It offered both an immediate execution model
    (for inference) as well as a delayed execution model for training (like in TensorFlow
    1.x). Originally released in 2017, this project became the core of TensorFlow.js
    (released in 2018). It is considered an integral part of the TensorFlow ecosystem,
    and as a result, it is currently the most actively developed JavaScript deep learning
    library. Considering this fact, we focus on TensorFlow.js in this chapter. To
    make TensorFlow.js even simpler to use, we also look at ml5.js, which is built
    on top of TensorFlow.js and abstracts away its complexities, exposing a simple
    API with ready-to-use models from GANs to PoseNet.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow.js Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let’s take a look at the high-level architecture of TensorFlow.js (see
    [Figure 10-5](part0012.html#a_high-level_overview_of_the_tensorflowd)). TensorFlow.js
    runs directly in the browser on desktop and mobile. It utilizes WebGL for GPU
    acceleration, but also can fall back to the browser’s runtime for execution on
    the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'It consists of two APIs: the Operations API and the Layers API. The Operations
    API provides access to lower-level operations such as tensor arithmetic and other
    mathematical operations. The Layers API builds on top of the Operations API to
    provide layers such as convolution, ReLU, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A high-level overview of the TensorFlow.js and ml5.js ecosystem](../images/00037.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. A high-level overview of the TensorFlow.js and ml5.js ecosystem
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Beyond the browser, TensorFlow.js can also run on a Node.js server. Additionally,
    ml5.js uses TensorFlow.js to provide an even higher-level API along with several
    prebuilt models. Having access to all of these APIs at different levels of abstraction
    allows us to build web apps, not only to do simple inference, but also to train
    models within the browser itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are some common questions that come up during the development life
    cycle for browser-based AI:'
  prefs: []
  type: TYPE_NORMAL
- en: How do I run pretrained models in the browser? Can I use my webcam feed for
    real-time interactivity?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can I create models for the browser from my TensorFlow trained models?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can I even train a model in the browser?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do different hardware and browsers affect performance?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We answer each of these questions in this chapter, starting with TensorFlow.js
    before moving on to ml5.js. We explore some rich built-in functionality contributed
    by the ml5.js community, which would otherwise take a lot of effort and expertise
    to implement directly on TensorFlow.js. We also look at approaches to benchmarking
    before looking at some motivating examples built by creative developers.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s take a look at how to take advantage of pretrained models to make
    inferences within the browser.
  prefs: []
  type: TYPE_NORMAL
- en: Running Pretrained Models Using TensorFlow.js
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow.js offers lots of pretrained models that we can directly run in the
    browser. Some examples include MobileNet, SSD, and PoseNet. In the following example,
    we load a pretrained MobileNet model. The full code is located on the book’s GitHub
    repository (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    at *code/chapter-10/mobilenet-example/*.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the latest bundle of the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We then import the MobileNet model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can make predictions using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 10-6](part0012.html#class_prediction_output_in_the_browser) shows a
    sample output.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Class prediction output in the browser](../images/00285.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. Class prediction output in the browser
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can alternatively load a model using a JSON file URL in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The JSON file contains the architecture, the parameter names of the model, and
    the paths to the smaller sharded weight files. The sharding allows the files to
    be small enough to be cached by web browsers, which would make loading faster
    for any subsequent time the model would be needed.
  prefs: []
  type: TYPE_NORMAL
- en: Model Conversion for the Browser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we examined how to load a pretrained model that is
    already in the JSON format. In this section, we learn how to convert a pretrained
    Keras model (.`h5` format) to JSON that is compatible with TensorFlow.js. To do
    this, we need to install the conversion tool using `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Assuming that our trained Keras model is stored in a folder named *keras_model*,
    we would be able to use the following command to convert it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the `web_model` directory will contain the `.json` and `.shard` files that
    we can easily load using the `tf.loadLayersModel` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: That’s it! Bringing our trained model to the browser is an easy task. For cases
    in which we don’t already have an existing trained model, TensorFlow.js also allows
    us to train models directly in the browser. In the next section, we explore this
    by creating an end-to-end example of training a model using a webcam feed.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Loading a model locally requires running a web server. There are tons of options
    that we can use, ranging from the LAMP (Linux, Apache, MySQL, PHP) stack to installing
    `http-server` using npm, to even running Internet Information Services (IIS) on
    Windows to test model loading locally. Even Python 3 can run a simple web server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Training in the Browser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous example used a pretrained model. Let’s take it up a notch and train
    our own models directly in the browser using input from the webcam. Like in some
    of the previous chapters, we look at a simple example in which we exploit transfer
    learning to make the training process faster.
  prefs: []
  type: TYPE_NORMAL
- en: Adapted from Google’s Teachable Machine, we use transfer learning to construct
    a simple binary classification model, which will be trained using the webcam feed.
    To build this, we need a feature extractor (converts input images into features
    or embeddings), and then attach a network that converts these features into a
    prediction. Finally, we can train it with webcam inputs. The code is referenced
    in the book’s GitHub repository (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    at *code/chapter-10/teachable-machine*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Google Creative Lab built a fun interactive website called [Teachable Machine](https://oreil.ly/jkM6W)
    where the user can train three classes on any type of classification problem by
    simply showing those objects in front of the webcam. The three classes are given
    simple labels of green, purple, and orange. During prediction, rather than showing
    bland-looking class probabilities in text on a web page (or even worse, in the
    console), Teachable Machine shows GIFs of cute animals or plays different sounds
    based on the class that is being predicted. As you can imagine, this would be
    a fun and engaging experience for kids in a classroom, and it would serve as a
    wonderful tool to introduce them to AI.
  prefs: []
  type: TYPE_NORMAL
- en: '![Training live in the browser with Teachable Machine](../images/00267.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. Training live in the browser with Teachable Machine
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Feature Extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we explored in some of the early chapters in this book, training a large
    model from scratch is a slow process. It’s a lot cheaper and quicker to use a
    pretrained model and use transfer learning to customize the model for our use
    case. We’ll use that model to extract high-level features (embeddings) from the
    input image, and use those features to train our custom model.
  prefs: []
  type: TYPE_NORMAL
- en: 'For extracting the features, we load and use a pretrained MobileNet model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s inspect the inputs and outputs of the model. We know that the model was
    trained on ImageNet and the final layer predicts a probability of each one of
    the 1,000 classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To extract the features, we select a layer closer to the output. Here we select
    `conv_pw_13_relu` and make it the output of the model; that is, remove the dense
    layers at the end. The model we created is called a feature extraction model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll keep the feature extraction model unmodified during our training process.
    Instead, we add a trainable set of layers on top of it to build our classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Data Collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we collect images using the webcam feed and process them for feature
    extraction. The `capture()` function in the Teachable Machine is responsible for
    setting up the `webcamImage` for storing the captured images from the webcam in
    memory. Now, let’s preprocess them to make them applicable to the feature extraction
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After we capture the images, we can add the image and label to the training
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we train the model, much as we trained in [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b).
    Just like in Keras and TensorFlow, we add an optimizer and define the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'One important thing to keep in mind is that on the GPU, memory allocated by
    TensorFlow.js is not released when a `tf.tensor` object goes out of scope. One
    solution is to call the `dispose()` method on every single object created. However,
    that would make the code more difficult to read, particularly when chaining is
    involved. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To cleanly dispose of all memory, we’d need to break it down into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead, we could simply use `tf.tidy()` to do the memory management for us,
    while keeping our code clean and readable. Our first line simply needs to be wrapped
    inside the `tf.tidy()` block, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: With a CPU backend, the objects are automatically garbage collected by the browser.
    Calling `.dispose()` there does not have any effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a simple use case, let’s train the model to detect emotions. To do so, we
    need to simply add training examples belonging to either one of two classes: happy
    or sad. Using this data, we can start training. [Figure 10-8](part0012.html#predictions_by_our_model_on_a_webcam_fee)
    shows the final result after training the model on 30 images per each class.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Predictions by our model on a webcam feed within a browser](../images/00245.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-8\. Predictions by our model on a webcam feed within a browser
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Usually, when using a webcam for predictions, the UI tends to freeze. This is
    because the computation happens on the same thread as the UI rendering. Calling
    `await` `tf.nextFrame()` will release the UI thread, which will make the web page
    responsive and prevent the tab/browser from freezing.
  prefs: []
  type: TYPE_NORMAL
- en: GPU Utilization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can see the CPU/GPU utilization during training and inference using the Chrome
    profiler. In the previous example, we recorded the utilization for 30 seconds
    and observed the GPU usage. In [Figure 10-9](part0012.html#gpu_utilization_shown_in_the_google_chro),
    we see that the GPU is used a quarter of the time.
  prefs: []
  type: TYPE_NORMAL
- en: '![GPU utilization shown in the Google Chrome profiler view](../images/00129.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-9\. GPU utilization shown in the Google Chrome profiler view
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So far, we’ve discussed how to do everything from scratch, including loading
    the model, capturing video from the webcam, collecting the training data, training
    the model, and running an inference. Wouldn’t it be great if all these steps could
    be taken care of under the hood and we could just focus on what to do with the
    results of the inference? In the next section, we discuss exactly that with ml5.js.
  prefs: []
  type: TYPE_NORMAL
- en: ml5.js
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ml5.js is a higher abstraction of TensorFlow.js that makes it easy to use existing
    pretrained deep learning models in a unified way, with a minimal number of lines
    of code. The package comes with a wide range of built-in models, ranging from
    image segmentation to sound classification to text generation, as shown in [Table 10-2](part0012.html#selected_built-in_models_in_ml5dotjscomm).
    Further, ml5.js reduces the steps related to preprocessing, postprocessing, and
    so on, letting us concentrate on building the application that we want with these
    models. For each of these functionalities, ml5js comes with a [demonstration](https://ml5js.org/reference)
    and reference code.
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-2\. Selected built-in models in ml5.js, showing the range of functionalities
    in text, image, and sound
  prefs: []
  type: TYPE_NORMAL
- en: '| **Functionality** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PoseNet | Detect the location of human joints |'
  prefs: []
  type: TYPE_TB
- en: '| U-Net | Object segmentation; e.g., removing object background |'
  prefs: []
  type: TYPE_TB
- en: '| Style Transfer | Transfers style of one image to another |'
  prefs: []
  type: TYPE_TB
- en: '| Pix2Pix | Image-to-image translation; e.g., black-and-white to color |'
  prefs: []
  type: TYPE_TB
- en: '| Sketch RNN | Creates doodles based on an incomplete sketch |'
  prefs: []
  type: TYPE_TB
- en: '| YOLO | Object detection; e.g., locates faces with bounding boxes |'
  prefs: []
  type: TYPE_TB
- en: '| Sound Classifier | Recognizes audio; e.g., whistle, clap, “one,” “stop,”
    etc. |'
  prefs: []
  type: TYPE_TB
- en: '| Pitch Detector | Estimates pitch of sound |'
  prefs: []
  type: TYPE_TB
- en: '| Char RNN | Generates new text based on training on a large corpus of text
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment Classifier | Detects sentiment of a sentence |'
  prefs: []
  type: TYPE_TB
- en: '| Word2Vec | Produces word embeddings to identify word relations |'
  prefs: []
  type: TYPE_TB
- en: '| Feature Extractor | Generates features or embeddings from input |'
  prefs: []
  type: TYPE_TB
- en: '| kNN Classifier | Creates a fast classifier using *k*-Nearest Neighbor |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s see it in action. First, we import the latest bundle of ml5.js, which
    is similar to TensorFlow.js:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we no longer need to import anything related to TensorFlow.js, because
    it already comes included with ml5.js. We create a simple example where we use
    the same MobileNet scenario as earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Done! In effectively three lines, a pretrained model is running in our browser.
    Now, let’s open the browser’s console to inspect the output presented in [Figure 10-10](part0012.html#the_top_predicted_classes_with_the_proba).
  prefs: []
  type: TYPE_NORMAL
- en: '![The top predicted classes with the probability of each class](../images/00151.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-10\. The top predicted classes with the probability of each class
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you are unfamiliar with the browser console, you can simply access it by
    right-clicking anywhere within the browser window and selecting “Inspect element.”
    A separate window opens with the console inside it.
  prefs: []
  type: TYPE_NORMAL
- en: We can find the full source code for the previous example at *code/chapter-10/ml5js*.
  prefs: []
  type: TYPE_NORMAL
- en: Note that ml5.js uses callbacks to manage asynchronous calls of the models.
    A callback is a function that is executed after the accompanying call is finished.
    For instance, in the last code snippet, after the model is loaded, the `modelLoaded`
    function is called, indicating that the model is loaded into memory.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: p5.js is a library that works nicely in conjunction with ml5.js and makes it
    super easy to make model predictions in real time using a live video stream. You
    can find a code snippet demonstrating the power of p5.js at *code/chapter-10/p5js-webcam/*.
  prefs: []
  type: TYPE_NORMAL
- en: ml5.js natively supports p5.js elements and objects. You can use p5.js elements
    for drawing objects, capturing webcam feeds, and more. Then, you can easily use
    such elements as input to the ml5.js callback functions.
  prefs: []
  type: TYPE_NORMAL
- en: PoseNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far in this book, we have primarily explored image classification problems.
    In later chapters, we take a look at object detection and segmentation problems.
    These types of problems form a majority of computer-vision literature. In this
    section, however, we choose to take a break from the usual and deal with a different
    kind of problem: keypoint detection. This has significant applications in a variety
    of areas including health care, fitness, security, gaming, augmented reality,
    and robotics. For instance, to encourage a healthy lifestyle through exercise,
    Mexico City installed kiosks that detect the squat pose and offer free subway
    tickets to passengers who can do at least 10 squats. In this section, we explore
    how to run something so powerful in our humble web browser.'
  prefs: []
  type: TYPE_NORMAL
- en: The PoseNet model offers real-time pose estimation in the browser. A “pose”
    consists of the position of different keypoints (including joints) in the human
    body such as the top of the head, eyes, nose, neck, wrists, elbows, knees, ankles,
    shoulders, and hips. You can use PoseNet for single or multiple poses that might
    exist in the same frame.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s build an example using PoseNet, which is readily available in ml5.js,
    to detect and draw keypoints (with the help of p5.js).
  prefs: []
  type: TYPE_NORMAL
- en: '![Keypoints drawn using PoseNet on a picture of former President Obama in a
    snowball fight](../images/00125.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-11\. Keypoints drawn using PoseNet on a picture of former President
    Obama in a snowball fight
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can find the code to detect keypoints for a still image at *code/chapter-10/posenet/single.html*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We can also run a similar script (at *code/chapter-10/posenet/webcam.html*)
    on our webcam.
  prefs: []
  type: TYPE_NORMAL
- en: Now let us look at another example that is supported by ml5.js.
  prefs: []
  type: TYPE_NORMAL
- en: pix2pix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Hasta la vista, baby!”
  prefs: []
  type: TYPE_NORMAL
- en: 'This is one of the most memorable lines in the history of film. Coincidentally,
    it was uttered by an AI cyborg in the 1991 classic *Terminator 2: Judgment Day*.
    By the way, its translation is “Goodbye, baby!” Language translation technology
    has come a long way since then. It used to be that language translation was built
    on phrase substitution rules. And now, it’s replaced by much better performing
    deep learning systems that understand the context of the sentence to convert it
    into a similar meaning sentence in the target language.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a thought: if we can translate from sentence one to sentence two, could
    we translate a picture from one setting to another? Could we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert an image from low resolution to higher resolution?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert an image from black and white to color?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert an image from daytime to nighttime view?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert a satellite image of the earth into a map view?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert an image from a hand-drawn sketch into a photograph?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well, image translation is not science fiction anymore. In 2017, [Philip Isola
    et al.](https://oreil.ly/g5R60) developed a way to convert a picture into another
    picture, conveniently naming it pix2pix. By learning from several pairs of before
    and after pictures, the pix2pix model is able to generate highly realistic renderings
    based on the input image. For example, as demonstrated in [Figure 10-12](part0012.html#example_of_input_and_output_pairs_on_pix),
    given a pencil sketch of a bag, it can recreate the photo of a bag. Additional
    applications include image segmentation, synthesizing artistic imagery, and more.
  prefs: []
  type: TYPE_NORMAL
- en: '![Example of input and output pairs on pix2pix](../images/00135.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-12\. Example of input and output pairs on pix2pix
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Imagine a scenario with a bank teller and a currency counterfeiter. The job
    of the bank teller is to spot fake currency bills, whereas the counterfeiter’s
    goal is to make it as difficult as possible for the bank teller to identify the
    fakes. They are clearly in an adversarial situation. Each time the cop spots the
    fake bills, the counterfeiter learns his mistake, takes it as an opportunity to
    improve (growth mindset after all), and tries to make it even more difficult for
    the bank teller to thwart him next time. This forces the bank teller to get better
    at recognizing fakes over time. This feedback cycle forces both of them to get
    better at what they do. This is the underlying principle driving GANs.
  prefs: []
  type: TYPE_NORMAL
- en: As [Figure 10-13](part0012.html#a_flowchart_for_a_gan) illustrates, GANs consist
    of two networks, a Generator and a Discriminator, which have the same adversarial
    relationship as the counterfeiter and the bank teller. The Generator’s job is
    to generate realistic-looking output, very similar to the training data. The Discriminator’s
    responsibility is to identify whether the data passed to it by the Generator was
    real or fake. The output of the Discriminator is fed back into the Generator to
    begin the next cycle. Each time the Discriminator correctly identifies a generated
    output as a fake, it forces the Generator to get better in the next cycle.
  prefs: []
  type: TYPE_NORMAL
- en: It is worthwhile to note that GANs typically do not have control over the data
    to be generated. However, there are variants of GANs, such as *conditional GANs*,
    that allow for labels to be part of the input, providing more control over the
    output generation; that is, conditioning the output. pix2pix is an example of
    a conditional GAN.
  prefs: []
  type: TYPE_NORMAL
- en: '![A flowchart for a GAN](../images/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-13\. A flowchart for a GAN
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We used pix2pix to create a simple sketching app that works in the browser.
    The output images are really interesting to look at. Consider the examples shown
    in [Figure 10-14](part0012.html#sketches_to_image_example) and [Figure 10-15](part0012.html#we_can_create_colored_blueprints_left_pa).
  prefs: []
  type: TYPE_NORMAL
- en: '![Sketches to image example](../images/00004.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-14\. Sketch-to-image example
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![We can create colored blueprints (left) and it will convert them to realistic-looking
    human faces (right)](../images/00251.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-15\. We can create colored blueprints (left) and pix2pix will convert
    them to realistic-looking human faces (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Fun fact: Ian Goodfellow came up with the idea for GANs while at a bar. This
    adds yet another item to the list of inventions, organizations, and companies
    whose ideas originated over drinks, including the creation of the RSA Algorithm,
    Southwest Airlines, and the game of Quidditch.'
  prefs: []
  type: TYPE_NORMAL
- en: pix2pix works by training on pairs of images. In [Figure 10-16](part0012.html#training_pairs_for_pix2pixcolon_a_bamper),
    the image on the left is the input image or the conditional input. The image on
    the right is the target image, the realistic output that we want to generate (if
    you are reading the print version, you will not see the color image on the right).
  prefs: []
  type: TYPE_NORMAL
- en: '![Training pairs for pix2pix: a B&W image and its original color image](../images/00230.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-16\. Training pairs for pix2pix: a B&W image and its original color
    image'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'One of the easier ports for training pix2pix is the TensorFlow-based implementation
    by [Christopher Hesse](https://oreil.ly/r-d1l). We can use a very simple script
    to train our own model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After training has finished, we can save the model using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we can use this simple code to load the saved weights to ml5.js.
    Note the transfer function that is used to retrieve the output in a canvas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We can also draw strokes and allow real-time sketching. For instance, [Figure 10-17](part0012.html#pix2pixcolon_edges_to_pikachu_by_yining)
    shows an example that draws Pikachu.
  prefs: []
  type: TYPE_NORMAL
- en: '![Pix2Pix: Edges to Pikachu by Yining Shi, built on ml5.js](../images/00311.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-17\. [Pix2Pix: Edges to Pikachu](https://oreil.ly/HIaSy) by Yining
    Shi, built on ml5.js'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Benchmarking and Practical Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As people who care deeply about how our end users perceive our product, it’s
    important for us to treat them right. Two factors play a large role in how users
    experience our product: the model size, and the inference time based on the hardware.
    Let’s take a closer look at each factor.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A typical MobileNet model is 16 MB. Loading this on a standard home or office
    network might just take a few seconds. Loading the same model on a mobile network
    would take even longer. The clock is ticking, and the user is becoming impatient.
    And this is before the model even gets a chance to start inference. Waiting for
    big models to load is more detrimental to the UX than their runtime, especially
    where internet speeds are not as fast as a broadband paradise like Singapore.
    There are a few strategies that can help:'
  prefs: []
  type: TYPE_NORMAL
- en: Pick the smallest model for the job
  prefs: []
  type: TYPE_NORMAL
- en: Among pretrained networks, EfficientNet, MobileNet, or SqueezeNet tend to be
    the smallest (in order of decreasing accuracy).
  prefs: []
  type: TYPE_NORMAL
- en: Quantize the model
  prefs: []
  type: TYPE_NORMAL
- en: Reduce the model size using the TensorFlow Model Optimization Toolkit before
    exporting to TensorFlow.js.
  prefs: []
  type: TYPE_NORMAL
- en: Build our own tiny model architecture
  prefs: []
  type: TYPE_NORMAL
- en: If the final product does not need heavy ImageNet-level classification, we could
    build our own smaller model. When Google made the J.S. Bach doodle on the Google
    home page, its model was only 400 KB, loading almost instantly.
  prefs: []
  type: TYPE_NORMAL
- en: Inference Time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Considering our model is accessible in a browser running on a PC or a mobile
    phone, we would want to pay careful attention to the UX, especially on the slowest
    hardware. During our benchmarking process, we ran *chapter10/code/benchmark.html*
    within various browsers on different devices. [Figure 10-18](part0012.html#inference_time_for_mobilenet_v1_in_chrom)
    presents the results of these experiments.
  prefs: []
  type: TYPE_NORMAL
- en: '![Inference time for MobileNetV1 in Chrome on different devices](../images/00233.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-18\. Inference time for MobileNetV1 in Chrome on different devices
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 10-18](part0012.html#inference_time_for_mobilenet_v1_in_chrom) implies
    the faster the hardware, the faster the model inference. Apple appears to be outdoing
    Android in terms of GPU performance. Though, clearly, it is not an “apples-to-apples
    comparison.”'
  prefs: []
  type: TYPE_NORMAL
- en: Out of curiosity, do different browsers run inference at the same speed? Let’s
    find that out on an iPhone X; [Figure 10-19](part0012.html#inference_time_in_different_browsers_on)
    shows the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![Inference time in different browsers on iPhone X](../images/00107.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-19\. Inference time in different browsers on iPhone X
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 10-19](part0012.html#inference_time_in_different_browsers_on) shows
    us the same speed in all browsers on an iPhone. This shouldn’t be surprising,
    because all of these browsers use iPhone’s WebKit-based built-in browser control
    called `WKWebView`. How about on a MacBook Pro? Take a look at [Figure 10-20](part0012.html#inference_time_in_different_bro-id00001)
    to see.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Inference time in different browsers on an i7 @ 2.6 GHz macOS 10.14 machine](../images/00090.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-20\. Inference time in different browsers on an i7 @ 2.6 GHz macOS
    10.14 machine
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The results might be surprising. Chrome is almost double the speed of Firefox
    in this example. Why is that? Opening a GPU monitor showed that Chrome had much
    higher GPU utilization compared to Firefox and slightly higher than Safari. The
    higher the utilization, the faster the inference. What this means is that depending
    on the operating system, browsers might have different optimizations to speed
    up the inference on the GPU, leading to different running times.
  prefs: []
  type: TYPE_NORMAL
- en: One key point to note is that these tests were performed on top-of-the-line
    devices. They do not necessarily reflect the kind of device an average user might
    have. This also has implications for battery usage, if run for prolonged periods
    of time. Accordingly, we need to set appropriate expectations regarding performance,
    particularly for real-time user experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Case Studies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know all the ingredients for deep learning on the browser, let’s
    see what the industry is cooking.
  prefs: []
  type: TYPE_NORMAL
- en: Semi-Conductor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Have you ever dreamed of conducting the New York Philharmonic Orchestra? With
    [Semi-Conductor](https://oreil.ly/sNOFg), your dream is half-way fulfilled. Open
    the website, stand in front of the webcam, wave your arms, and watch the entire
    orchestra perform Mozart’s Eine Kleine Nachtmusik at your whim! As you might have
    guessed, it’s using PoseNet to track the arm movements and using those movements
    to set the tempo, volume, and the section of instruments ([Figure 10-21](part0012.html#control_an_orchestra_by_waving_your_arms))
    playing the music (including violins, violas, cellos, and double bass). Built
    by the Google Creative Lab in Sydney, Australia, it uses a prerecorded musical
    piece broken up into tiny fragments, and each fragment is played at the scored
    speed and volume depending on the arm movements. Moving hands up increases volume,
    moving faster increases tempo. This interactive experience is possible only because
    PoseNet is able to run inferences at several frames a second (on a regular laptop).
  prefs: []
  type: TYPE_NORMAL
- en: '![Control an orchestra by waving your arms on the Semi-Conductor demonstrator](../images/00010.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-21\. Control an orchestra by waving your arms on the Semi-Conductor
    demonstrator
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TensorSpace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CNNs can often feel…well, convoluted. Often treated as a black box, they can
    be difficult to understand. What do the filters look like? What activates them?
    Why did they make a certain prediction? They are shrouded in mystery. As with
    anything complex, visualizations can help open this black box and make it easier
    to understand. And that’s where [TensorSpace](https://tensorspace.org), the library
    that “presents tensors in space,” comes in.
  prefs: []
  type: TYPE_NORMAL
- en: It allows us to load models in 3D space, explore their structures in the browser,
    zoom and rotate through them, feed inputs, and understand how the image is processed
    and passed layer by layer all the way to the final prediction layer. The filters
    can finally be opened up to manual inspection without the need for any installation.
    And, as [Figure 10-22](part0012.html#lenet_model_visualized_inside_tensorspac)
    teases, if you’re feeling savvy, you could even load this in virtual reality against
    any background!
  prefs: []
  type: TYPE_NORMAL
- en: '![LeNet model visualized inside TensorSpace](../images/00316.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-22\. LeNet model visualized inside TensorSpace
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Metacar
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Self-driving cars are a complex beast. And using reinforcement learning to train
    them can take a lot of time, money, and monkeywrenching (not counting the crashes,
    initially). What if we could train them in the browser itself? [Metacar](https://metacar-project.com)
    solves this by providing a simulated 2D environment to train toy cars with reinforcement
    learning, all in the browser, as depicted in [Figure 10-23](part0012.html#metacar_environment_for_training_with_re).
    Just as with video games you progress to ever-more difficult levels, Metacar allows
    building multiple levels to improve the performance of your car. Utilizing TensorFlow.js,
    this is aimed at making reinforcement learning more accessible (in which we dive
    into greater detail in [Chapter 17](part0020.html#J2B83-13fa565533764549a6f0ab7f11eed62b)
    while building a small-scale autonomous car).
  prefs: []
  type: TYPE_NORMAL
- en: '![Metacar environment for training with reinforcement learning](../images/00084.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-23\. Metacar environment for training with reinforcement learning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Airbnb’s Photo Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Airbnb, the online property rental company, requires homeowners and renters
    to upload pictures of themselves for their profiles. Unfortunately, a few people
    try to find the most readily available picture they have—their driver’s license
    or passport. Given the confidential nature of the information, Airbnb uses a neural
    network running on TensorFlow.js to detect sensitive images and prevent their
    upload to the server.
  prefs: []
  type: TYPE_NORMAL
- en: GAN Lab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to [TensorFlow Playground](https://oreil.ly/vTpmu) (an in-browser neural
    network visualization tool), [GAN Lab](https://oreil.ly/aQgga) ([Figure 10-24](part0012.html#screenshot_of_gan_lab))
    is an elegant visualization tool for understanding GANs using TensorFlow.js. Visualizing
    GANs is a difficult process, so to simplify it, GAN Lab attempts to learn simple
    distributions and visualizes the generator and discriminator network output. For
    instance, the real distribution could be points representing a circle in 2D space.
    The generator starts from a random Gaussian distribution and gradually tries to
    generate the original distribution. This project is a collaboration between Georgia
    Tech and Google Brain/PAIR (People + AI Research).
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of GAN Lab](../images/00105.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-24\. Screenshot of GAN Lab
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We first examined the evolution of JavaScript-based deep learning libraries
    and chose TensorFlow.js as the candidate to focus on. We ran pretrained models
    in real time on the webcam feed, and then even trained models in the browser.
    Tools like Chrome’s profiler gave us insight into GPU usage. Then, to simplify
    development further, we used ml5.js, which enabled us to build demos like PoseNet
    and pix2pix in just a few lines of code. Finally, we benchmarked the performance
    of these models and libraries in the real world, ending with some interesting
    case studies.
  prefs: []
  type: TYPE_NORMAL
- en: One huge benefit of running neural networks in the browser is the vast reach
    that browsers have compared to any smartphone platform. Add to that the advantage
    of not having to overcome the user’s reluctance to install yet another app. This
    also makes for a quick prototyping platform that enables inexpensive validation
    of hypotheses before investing significant amounts of time and money to build
    native experiences. TensorFlow.js, in conjunction with ml5.js, has accelerated
    the process of bringing the power of AI to the browser and broaden its reach to
    the masses.
  prefs: []
  type: TYPE_NORMAL
