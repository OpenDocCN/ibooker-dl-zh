- en: Chapter 14\. Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In May 2018, I began work on the first edition of this book. Five years later,
    I am more excited than ever about the endless possibilities and potential impact
    of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: In this time we have seen incredible progress in this field, with seemingly
    limitless potential for real-world applications. I am filled with a sense of awe
    and wonder at what we have been able to achieve so far and eagerly anticipate
    witnessing the effect that generative AI will have on the world in the coming
    years. Generative deep learning has the power to shape the future in ways we can’t
    even begin to imagine.
  prefs: []
  type: TYPE_NORMAL
- en: What’s more, as I have been researching content for this book, it has become
    ever clearer to me that this field isn’t just about creating images, text, or
    music. I believe that at the core of generative deep learning lies the secret
    of intelligence itself.
  prefs: []
  type: TYPE_NORMAL
- en: The first section of this chapter summarizes how we have reached this point
    in our generative AI journey. We will walk through a timeline of generative AI
    developments since 2014 in chronological order, so that you can see where each
    technique fits into the history of generative AI to date. The second section explains
    where we currently stand in terms of state-of-the-art generative AI. We will discuss
    current trends in the approach to generative deep learning and the current off-the-shelf
    models available to the general public. Next, we will explore the future of generative
    AI and the opportunities and challenges that lie ahead. We will consider what
    generative AI might look like five years in the future and its potential impact
    on society and business, and address some of the main ethical and practical concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Timeline of Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Figure 14-1](#timeline) is a timeline of the key developments in generative
    modeling that we have explored together in this book. The colors represent different
    model types.'
  prefs: []
  type: TYPE_NORMAL
- en: To field of generative AI stands on the shoulders of earlier developments in
    deep learning, such as backpropagation and convolutional neural networks, which
    unlocked the possibility for models to learn complex relationships across large
    datasets at scale. In this section, we will study the modern history of generative
    AI, from 2014 onwards, that has moved at such breathtaking speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help us understand how everything fits together, we can loosely break down
    this history into three main eras:'
  prefs: []
  type: TYPE_NORMAL
- en: '2014–2017: The VAE and GAN era'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '2018–2019: The Transformer era'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '2020–2022: The Big Model era'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1401.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14-1\. A brief history of generative AI from 2014 to 2023 (note: some
    important developments such as LSTMs and early energy-based models [e.g., Boltzmann
    machines] precede this timeline)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '2014–2017: The VAE and GAN Era'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The invention of the VAE in December 2013 can perhaps be thought of as the spark
    that lit the generative AI touchpaper. This paper showed how it was possible to
    generate not only simple images such as MNIST digits but also more complex images
    such as faces in a latent space that could be smoothly traversed. It was followed
    in 2014 by the introduction of the GAN, an entirely new adversarial framework
    for tackling generative modeling problems.
  prefs: []
  type: TYPE_NORMAL
- en: The following three years were dominated by progressively more impressive extensions
    of the GAN portfolio. In addition to fundamental changes to the GAN model architecture
    (DCGAN, 2015), loss function (Wasserstein GAN, 2017), and training process (ProGAN,
    2017), new domains were tackled using GANs, such as image-to-image translation
    (pix2pix, 2016, and CycleGAN, 2017) and music generation (MuseGAN, 2017).
  prefs: []
  type: TYPE_NORMAL
- en: During this era, important VAE improvements were also introduced, such as VAE-GAN
    (2015) and later VQ-VAE (2017), and applications to reinforcement learning were
    seen in the “World Models” paper (2018).
  prefs: []
  type: TYPE_NORMAL
- en: Established autoregressive models such as LSTMs and GRUs remained the dominant
    force in text generation over this time. The same autoregressive ideas were also
    being used to generate images, with PixelRNN (2016) and PixelCNN (2016) introduced
    as new ways to think about image generation. Other approaches to image generation
    were also being tested, such as the RealNVP model (2016) that paved the way for
    later types of normalizing flow models.
  prefs: []
  type: TYPE_NORMAL
- en: In June 2017, a groundbreaking paper entitled “Attention Is All You Need” was
    published that would usher in the next era of generative AI, focused around Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: '2018–2019: The Transformer Era'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the heart of a Transformer is the attention mechanism that negates the need
    for the recurrent layers present in older autoregressive models such as LSTMs.
    The Transformer quickly rose to prominence with the introduction of GPT (a decoder-only
    Transformer) and BERT (an encoder-only Transformer) in 2018\. The following year
    saw progressively larger language models being built that excelled at a wide range
    of tasks by treating them as pure text-to-text generation problems, with GPT-2
    (2018, 1.5B parameters) and T5 (2019, 11B parameters) being standout examples.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers were also starting to be successfully applied to music generation,
    with the introduction of, for example, the Music Transformer (2018) and MuseNet
    (2019) models.
  prefs: []
  type: TYPE_NORMAL
- en: Over these two years, several impressive GANs were also released that cemented
    the technique’s place as the state-of-the-art approach for image generation. In
    particular, SAGAN (2018) and the larger BigGAN (2018) incorporated the attention
    mechanism into the GAN framework with incredible results, and StyleGAN (2018)
    and later StyleGAN2 (2019) showed how images could be generated with amazing fine-grained
    control over the style and content of a particular image.
  prefs: []
  type: TYPE_NORMAL
- en: Another field of generative AI that was gathering momentum was score-based models
    (NCSN, 2019), which would eventually pave the way for the next seismic shift in
    the generative AI landscape—diffusion models.
  prefs: []
  type: TYPE_NORMAL
- en: '2020–2022: The Big Model Era'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This era saw the introduction of several models that merged ideas across different
    generative modeling families and turbo-charged existing architectures. For example,
    the VQ-GAN (2020) brought the GAN discriminator into the VQ-VAE architecture and
    the Vision Transformer (2020) showed how it was possible to train a Transformer
    to operate over images. 2022 saw the release of StyleGAN-XL, a further update
    to the StyleGAN architecture that enables 1,024 × 1,024–pixel images to be generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two models were introduced in 2020 that would lay the foundations for all future
    large image generation models: DDPM and DDIM. Suddenly, diffusion models were
    a rival for GANs in terms of image generation quality, as explicitly stated in
    the title of the 2021 paper “Diffusion Models Beat GANs on Image Synthesis.” The
    image quality of diffusion models is unbelievably good and they only require a
    single U-Net network to be trained, rather than the dual-network setup of a GAN,
    making the training process much more stable.'
  prefs: []
  type: TYPE_NORMAL
- en: Around the same time, GPT-3 (2020) was released—an enormous 175B parameter Transformer
    that can generate text on just about any topic in a way that seems almost impossible
    to comprehend. The model was released through a web application and API, allowing
    companies to build products and services on top of it. ChatGPT (2022) is a web
    application and API wrapper around the latest version of GPT from OpenAI that
    allows users to have natural conversations with the AI about any topic.
  prefs: []
  type: TYPE_NORMAL
- en: Over 2021 and 2022, a flurry of other large language models were released to
    rival GPT-3, including Megatron-Turing NLG (2021) by Microsoft and NVIDIA, Gopher
    (2021) and Chinchilla by DeepMind (2022), LaMDA (2022) and PaLM (2022) by Google,
    and Luminous (2022) by Aleph Alpha. Some open source models were also released,
    such as GPT-Neo (2021), GPT-J (2021), and GPT-NeoX (2022) by EleutherAI; the 66B
    parameter OPT model (2022) by Meta; the fine-tuned Flan-T5 model (2022) by Google,
    BLOOM (2022) by Hugging Face; and others. Each of these models is a variation
    of a Transformer, trained on a huge corpus of data.
  prefs: []
  type: TYPE_NORMAL
- en: The rapid rise of powerful Transformers for text generation and state-of-the-art
    diffusion models for image generation has meant that much of the focus of the
    last two years of generative AI development has been on multimodal models—that
    is, models that operate over more than one domain (for example, text-to-image
    models).
  prefs: []
  type: TYPE_NORMAL
- en: 'This trend was established in 2021 when OpenAI released DALL.E, a text-to-image
    model based upon a discrete VAE (similar to VQ-VAE) and CLIP (a Transformer model
    that predicts image/text pairs). This was followed by GLIDE (2021) and DALL.E
    2 (2022), which updated the generative part of the model to use a diffusion model
    rather than a discrete VAE, with truly impressive results. This era also saw the
    release of three text-to-image models from Google: Imagen (2022, using Transformer
    and diffusion models), Parti (2022, using Transformers and a ViT-VQGAN model),
    and later MUSE (2023, using Transformers and VQ-GANs). DeepMind also released
    Flamingo (2022), a visual language model that builds upon their large language
    model Chinchilla by allowing images to be used as part of the prompt data.'
  prefs: []
  type: TYPE_NORMAL
- en: Another important diffusion advancement introduced in 2021 was latent diffusion,
    where a diffusion model is trained within the latent space of an autoencoder.
    This technique powers the Stable Diffusion model, released as a joint collaboration
    between Stability AI, CompVis, and Runway in 2022\. Unlike with DALL.E 2, Imagen,
    and Flamingo, the code and model weights of Stable Diffusion are open source,
    meaning anyone can run the model on their own hardware.
  prefs: []
  type: TYPE_NORMAL
- en: The Current State of Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we come to end of our journey through the history of generative AI, it is
    important to now reflect on where we stand in terms of current state-of-the-art
    applications and models. Let’s take a moment to assess our progress and key accomplishments
    in the field to date.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI for text is now almost entirely focused on building large language
    models (LLMs), whose sole purpose is to directly model language from a huge corpus
    of text—that is, they are trained to predict the next word, in the style of a
    decoder Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: The large language model approach has been adopted so widely because of its
    flexibility and ability to excel at a wide range of tasks. The same model can
    be used for question answering, text summarization, content creation, and many
    other examples because ultimately each use case can be framed as a text-to-text
    problem, where the specific task instructions (the *prompt*) are given as part
    of the input to the model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take [GPT-3](https://oreil.ly/Pga1w) as an example. [Figure 14-2](#gpt3-examples)
    shows how the same model can be used for text summarization and content creation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-2\. Output from GPT-3—the non-highlighted text is the prompt and the
    green highlighted text is the output from GPT-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice how in both cases, the prompt contains the relevant instructions. The
    job of GPT-3 is just to continue the prompt, one token at a time. It doesn’t have
    a database of facts from which it can look up information, or snippets of text
    that it can copy into its answers. It is only asked to predict what token is most
    likely to follow the existing tokens and then append this prediction to the prompt
    to generate the next token, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Incredibly, this simple design is enough for the language model to excel at
    a range of tasks, as shown in [Figure 14-2](#gpt3-examples). Moreover, it gives
    the language model incredible flexibility to generate realistic text as a response
    to any prompt—imagination is often the limiting factor!
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 14-3](#languagemodelsize) shows how large language models have grown
    in size since the original GPT model was published in 2018\. The number of parameters
    grew exponentially until late 2021, with Megatron-Turing NLG reaching 530B parameters.
    Recently, more emphasis has been placed on building more efficient language models
    that use fewer parameters, as larger models are more costly and slower to serve
    in a production environment.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-3\. The size of large language models (orange) and multimodal models
    (pink) in number of parameters over time
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: OpenAI’s GPT collection (GPT-3, GPT-3.5, GPT-4, etc.) is still considered by
    many to be the most powerful state-of-the-art suite of language models available
    for personal and commercial use. They are each available through a [web application](https://platform.openai.com/playground)
    and [API](https://openai.com/api).
  prefs: []
  type: TYPE_NORMAL
- en: Another recent addition to the large language model family is *Large Language
    Model Meta AI* (LLaMA) from Meta,^([1](ch14.xhtml#idm45387000972896)) a suite
    of models ranging from 7B to 65B parameters in size that are trained purely on
    publicly available datasets.
  prefs: []
  type: TYPE_NORMAL
- en: A summary of some of the most powerful LLMs in existence today is shown in [Table 14-1](#llms).
    Some, like LLaMA, are families of models of different sizes—in this case, the
    size of the largest model is shown here. Pre-trained weights are fully open source
    for some of the models, meaning that they are free for anyone to use and build
    upon.
  prefs: []
  type: TYPE_NORMAL
- en: Table 14-1\. Large language models
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Date | Developer | # parameters | Open source |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3 | May 2020 | OpenAI | 175,000,000,000 | No |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-Neo | Mar 2021 | EleutherAI | 2,700,000,000 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-J | Jun 2021 | EleutherAI | 6,000,000,000 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Megatron-Turing NLG | Oct 2021 | Microsoft & NVIDIA | 530,000,000,000 | No
    |'
  prefs: []
  type: TYPE_TB
- en: '| Gopher | Dec 2021 | DeepMind | 280,000,000,000 | No |'
  prefs: []
  type: TYPE_TB
- en: '| LaMDA | Jan 2022 | Google | 137,000,000,000 | No |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-NeoX | Feb 2022 | EleutherAI | 20,000,000,000 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Chinchilla | Mar 2022 | DeepMind | 70,000,000,000 | No |'
  prefs: []
  type: TYPE_TB
- en: '| PaLM | Apr 2022 | Google | 540,000,000,000 | No |'
  prefs: []
  type: TYPE_TB
- en: '| Luminous | Apr 2022 | Aleph Alpha | 70,000,000,000 | No |'
  prefs: []
  type: TYPE_TB
- en: '| OPT | May 2022 | Meta | 175,000,000,000 | Yes (66B) |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM | Jul 2022 | Hugging Face collaboration | 175,000,000,000 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Flan-T5 | Oct 2022 | Google | 11,000,000,000 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | Nov 2022 | OpenAI | Unknown | No |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA | Feb 2023 | Meta | 65,000,000,000 | No |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | Mar 2023 | OpenAI | Unknown | No |'
  prefs: []
  type: TYPE_TB
- en: Despite the impressive applications of large language models, there remain significant
    challenges to overcome. Most notably, they are prone to inventing facts and cannot
    reliably apply logical thought processes, as shown in [Figure 14-4](#languagemodelproblems).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-4\. While large language models excel at some tasks, they are also
    prone to mistakes related to factual or logical reasoning (GPT-3 output shown)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is important to remember that LLMs are trained only to predict the next word.
    They have no other connection to reality that would allow them to reliably identify
    factual or logical fallacies. Therefore, we must be extremely cautious about how
    we use these powerful text prediction models in production—they cannot yet be
    reliably utilized for anything that requires precise reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-Code Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another application of large language models is code generation. In July 2021,
    OpenAI introduced a model called *Codex*, a GPT language model that had been fine-tuned
    on code from GitHub.^([2](ch14.xhtml#idm45387000894496)) The model was able to
    successfully write novel coded solutions to a range of problems, prompted only
    with a comment on the problem to be solved, or a function name. The technology
    today powers [GitHub Copilot](https://oreil.ly/P5WXo), an AI pair programmer that
    can be used to suggest code in real time as you type. Copilot is a paid subscription-based
    service, with a free trial period.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 14-5](#codex) shows two examples of autogenerated completions. The
    first example is a function that fetches tweets from a given user, using the Twitter
    API. Given the function name and parameter, Copilot is able to autocomplete the
    rest of the function definition. The second example asks Copilot to parse a list
    of expenses, by additionally including a free text description in the docstring
    that explains the format of the input parameter and specific instructions related
    to the task. Copilot is able to autocomplete the entire function from the description
    alone.'
  prefs: []
  type: TYPE_NORMAL
- en: This remarkable technology is already beginning to change how programmers approach
    a given task. A significant proportion of a programmer’s time is usually spent
    searching for examples of existing solutions, reading community Q&A forums such
    as Stack Overflow, and looking up syntax in package documentation. This means
    leaving the interactive development environment (IDE) through which you are coding,
    switching to a web browser, and copying and pasting code snippets from the web
    to see if they solve your specific problem. Copilot removes the need to do this
    in many cases, because you can simply tab through potential solutions generated
    by the AI from within the IDE, after writing a brief description of what you are
    looking to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1405.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14-5\. Two examples of GitHub Copilot capabilities (source: GitHub Copilot)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Text-to-Image Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: State-of-the-art image generation is currently dominated by large multimodal
    models that convert a given text prompt into an image. Text-to-image models are
    highly useful as they allow users to easily manipulate generated images via natural
    language. This is in contrast to models such as StyleGAN, which, while extremely
    impressive, does not have a text interface through which you can describe the
    image that you want to be generated.
  prefs: []
  type: TYPE_NORMAL
- en: Three important text-to-image generation models that are currently available
    for commercial and personal use are DALL.E 2, Midjourney, and Stable Diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: DALL.E 2 by OpenAI is a pay-as-you-go service that is available through a web
    application and [API](https://labs.openai.com). [Midjourney](https://midjourney.com)
    provides a subscription-based text-to-image service through its Discord channel.
    Both DALL.E 2 and Midjourney offer free credits to those joining the platform
    for early experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: Midjourney
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Midjourney is the service used to create the illustrations for the stories in
    [Part II](part02.xhtml#part_methods) of this book!
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion is different because it is fully open source. The model weights
    and code to train the model are available on [GitHub](https://oreil.ly/C47vN),
    so anyone can run the model on their own hardware. The dataset used to train Stable
    Diffusion is also open source. This dataset, called [LAION-5B](https://oreil.ly/2O758),
    contains 5.85 billion image-text pairs and is currently the largest openly accessible
    image-text dataset in the world.
  prefs: []
  type: TYPE_NORMAL
- en: An important corollary of this approach is that the baseline Stable Diffusion
    model can be built upon and adapted to different use cases. An excellent demonstration
    of this is ControlNet, a neural network structure that allows fine-grained control
    of the output from Stable Diffusion by adding extra conditions.^([3](ch14.xhtml#idm45387000869232))
    For example, output images can be conditioned on a [Canny edge map](https://oreil.ly/8v9Ym)
    of a given input image, as shown in [Figure 14-6](#controlnet_canny).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1406.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14-6\. Conditioning the output of Stable Diffusion using a Canny edge
    map and ControlNet (source: [Lvmin Zhang, ControlNet](https://github.com/lllyasviel/ControlNet))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ControlNet contains a trainable copy of the Stable Diffusion encoder, alongside
    a locked copy of the full Stable Diffusion model. The job of this trainable encoder
    is to learn how to handle the input condition (e.g., the Canny edge map), whilst
    the locked copy retains the power of the original model. This way, Stable Diffusion
    can be fine-tuned using only a small number of image pairs. Zero convolutions
    are simply 1 × 1 convolutions where all weights and biases are zero, so that before
    training, ControlNet does not have any effect.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1407.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14-7\. The ControlNet architecture, with the trainable copies of the
    Stable Diffusion encoder blocks highlighted in blue (source: [Lvmin Zhang, ControlNet](https://github.com/lllyasviel/ControlNet))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Another advantage of Stable Diffusion is that it is able to run on a single
    modestly sized GPU with only 8 GB of VRAM, making it possible to run on edge devices,
    rather than through calls to a cloud service. As text-to-image services are included
    in downstream products, the speed of generation is becoming increasingly more
    important. This is one reason why the size of multimodal models is generally trending
    downward (see [Figure 14-3](#languagemodelsize)).
  prefs: []
  type: TYPE_NORMAL
- en: Example outputs for all three models can be seen in [Figure 14-8](#multimodal_comparison).
    All of these models are exceptional and are able to capture the content and style
    of the given description.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1408.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-8\. Outputs from Stable Diffusion v2.1, Midjourney, and DALL.E 2 for
    the same prompt
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A summary of some of the most powerful text-to-image models in existence today
    is shown in [Table 14-2](#text_to_image).
  prefs: []
  type: TYPE_NORMAL
- en: Table 14-2\. Text-to-image models
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Date | Developer | # parameters | Open source |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DALL.E 2 | Apr 2022 | OpenAI | 3,500,000,000 | No |'
  prefs: []
  type: TYPE_TB
- en: '| Imagen | May 2022 | Google | 4,600,000,000 | No |'
  prefs: []
  type: TYPE_TB
- en: '| Parti | Jun 2022 | Google | 20,000,000,000 | No |'
  prefs: []
  type: TYPE_TB
- en: '| Stable Diffusion | Aug 2022 | Stability AI, CompVis, and Runway | 890,000,000
    | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| MUSE | Jan 2023 | Google | 3,000,000,000 | No |'
  prefs: []
  type: TYPE_TB
- en: Part of the skill of working with text-to-image models is creating a prompt
    that both describes the content of the image you want to generate and uses keywords
    that encourage the model to produce a particular style or type of image. For example,
    adjectives such as *stunning* or *award-winning* can often be used to improve
    the quality of the generation. However, it is not always the case that the same
    prompt will work well across different models—it depends on the contents of the
    specific text-image dataset used to train the model. The art of uncovering prompts
    that work well for a particular model is known as *prompt engineering*.
  prefs: []
  type: TYPE_NORMAL
- en: Other Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI is rapidly finding applications across a variety of novel domains,
    from reinforcement learning to other kinds of *text-to-X* multimodal models.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in November 2022 Meta published a paper on [*CICERO*](https://oreil.ly/kBQvY),
    an AI agent trained to play the board game *Diplomacy*. In this game, players
    represent different countries in Europe before World War I and must negotiate
    with and deceive each other in order to gain control of the continent. It is a
    highly complex game for an AI agent to master, not least because there is a communicative
    element where players must discuss their plans with other players in order to
    gain allies, coordinate maneuvers, and suggest strategic goals. To achieve this,
    CICERO contains a language model that is able to initiate dialogue and respond
    to messages from other players. Crucially, the dialogue is consistent with the
    agent’s strategic plans, which are generated by another part of the model to adapt
    to the constantly evolving scenario. This includes the ability for the agent to
    bluff when conversing with other players—that is, convince another player to co-operate
    with the agent’s plans, only to then enact an aggressive maneuver against the
    player in a later turn. Remarkably, in an anonymous online *Diplomacy* league
    featuring 40 games, CICERO’s score was more than double the average of the human
    players and it ranked in the top 10% of participants who played multiple games.
    This is an excellent example of how generative AI can be successfully blended
    with reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: The development of embodied large language models is an exciting area of research,
    further exemplified by Google’s [*PaLM-E*](https://palm-e.github.io). This model
    combines the powerful language model PaLM with a Vision Transformer to convert
    visual and sensor data into tokens that can be interleaved with text instructions,
    allowing robots to execute tasks based on text prompts and continuous feedback
    from other sensory modalities. The PaLM-E website showcases the model’s abilities,
    including controlling a robot to arrange blocks and fetch objects based on text
    descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Text-to-video* models involve the creation of videos from text input. This
    field, which builds on the concept of text-to-image modeling, has the additional
    challenge of incorporating a time dimension. For example, in September 2022 Meta
    published [*Make-A-Video*](https://makeavideo.studio), a generative model that
    is able to create a short video given only a text prompt as input. The model is
    also able to add motion between two static images and produce variations of a
    given input video. Interestingly, it is trained only on paired text–image data
    and unsupervised video footage, rather than text–video pairs directly. The unsupervised
    video data is enough for the model to learn how the world moves; it then uses
    the text–image pairs to learn how to map between text image modalities, which
    are then animated. The [*Dreamix* model](https://oreil.ly/F9wdw) is able to perform
    video editing, where an input video is transformed based on a given text prompt
    while retaining other stylistic attributes. For example, a video of a glass of
    milk being poured could be converted to a cup of coffee being poured, while retaining
    the camera angle, background, and lighting elements of the original video.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, *text-to-3D* models extend traditional text-to-image approaches into
    a third dimension. In September 2022 Google published [*DreamFusion*](https://dreamfusion3d.github.io),
    a diffusion model that generates 3D assets given an input text prompt. Crucially,
    the model does not require labeled 3D assets to train on. Instead, the authors
    use a pre-trained 2D text-to-image model (Imagen) as a prior and then train a
    3D Neural Radiance Field (NeRF), such that it is able to produce good images when
    rendered from random angles. Another example is OpenAI’s [*Point-E*](https://openai.com/research/point-e),
    published in December 2022\. Point-E is a pure diffusion-based system that is
    able to generate a 3D point cloud from a given text prompt. While the output produced
    is not as high quality as DreamFusion’s, the advantage of this approach is that
    is much faster than NeRF-based methods—it can produce output in just one to two
    minutes on a single GPU, rather than requiring multiple GPU-hours.
  prefs: []
  type: TYPE_NORMAL
- en: Given the similarities between text and music, it is not surprising that there
    have also been attempts to create *text-to-music* models. [*MusicLM*](https://oreil.ly/qb7II),
    released by Google in January 2023, is a language model that is able to convert
    a text description of a piece of music (e.g., “a calming violin melody backed
    by a distorted guitar riff”) into audio spanning several minutes that accurately
    reflects the description. It builds upon the earlier work [*AudioLM*](https://oreil.ly/0EDRY)
    by adding the ability for the model to be guided by a text prompt; examples that
    you can listen to are available on the Google Research website.
  prefs: []
  type: TYPE_NORMAL
- en: The Future of Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final section, we will explore the potential impact that powerful generative
    AI systems may have on the world we live in—across our everyday lives, in the
    workplace, and within the field of education. We will also lay down the key practical
    and ethical challenges generative AI will face if it is to become a ubiquitous
    tool that makes a significant net positive contribution to society.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI in Everyday Life
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is no doubt that in the future generative AI will play an increasingly
    important role in people’s everyday lives—particularly large language models.
    With OpenAI’s [ChatGPT](https://chat.openai.com/chat), it is already possible
    to generate a perfect cover letter for a job application, a professional email
    response to a colleague, or a funny social media post on a given topic using generative
    AI. This technology is truly interactive: it is able to include specific details
    that you request, respond to feedback, and ask its own questions back if something
    isn’t clear. This style of *personal assistant* AI should be the stuff of science
    fiction, but it isn’t—it’s here right now, for anyone who chooses to use it.'
  prefs: []
  type: TYPE_NORMAL
- en: What are the repercussions of this kind of application becoming mainstream?
    It is likely that the most immediate effect will be an increase in the quality
    of written communication. Access to large language models with a user-friendly
    interface will enable people to translate a sketch of an idea into coherent, high-quality
    paragraphs in seconds. Email writing, social media posts, and even short-form
    instant messaging will be transformed by this technology. It goes beyond removing
    the common barriers associated with spelling, grammar, and readability—it directly
    links our thought processes to usable output, often removing the need to engage
    with the process of constructing sentences at all.
  prefs: []
  type: TYPE_NORMAL
- en: Production of well-formed text is only one use of large language models. People
    will start using these models for idea generation, advice, and information retrieval.
    I believe we can see this as the fourth stage of our ability as a species to acquire,
    share, retrieve, and synthesize information. We started by acquiring information
    from those around us, or physically traveling to new locations to transfer knowledge.
    The invention of the printing press allowed the book to become the primary vessel
    through which ideas were shared. Finally, the birth of the internet allowed us
    to instantaneously search for and retrieve information at the touch of a button.
    Generative AI unlocks a new era of information synthesis that I believe will replace
    many of the current uses of today’s search engines.
  prefs: []
  type: TYPE_NORMAL
- en: For example, OpenAI’s GPT suite of models can provide bespoke holiday destination
    recommendations, as shown in [Figure 14-9](#holidays), or advice on how to respond
    to a difficult situation, or a detailed explanation of an obscure concept. Using
    this technology feels more like asking a friend than typing a query into a search
    engine, and for that reason, people are flocking to it extremely quickly. ChatGPT
    is the fastest-growing tech platform ever; it acquired 1 million users within
    5 days of its launch. For context, it took Instagram 2.5 months to reach the same
    number of users and Facebook 10 months.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1409.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-9\. Output from GPT-3, giving bespoke holiday recommendations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Generative AI in the Workplace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As well as general use, generative AI will find applications in specific jobs
    where creativity is required. A nonexhaustive list of occupations that may benefit
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Advertising
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI can be used to create personalized ad campaigns that target specific
    demographics based on their browsing and purchase history.
  prefs: []
  type: TYPE_NORMAL
- en: Music production
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI can be used to compose and produce original music tracks, allowing
    for a limitless range of possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI can be used to design buildings and structures, taking into account
    factors such as style and constraints around layout.
  prefs: []
  type: TYPE_NORMAL
- en: Fashion design
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI can be used to create unique and diverse clothing designs, taking
    into account trends and wearer preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Automotive design
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI can be used to design and develop new vehicle models and automatically
    find interesting variations on a particular design.
  prefs: []
  type: TYPE_NORMAL
- en: Film and video production
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI can be used to create special effects and animations, as well
    as to generate dialogue for entire scenes or storylines.
  prefs: []
  type: TYPE_NORMAL
- en: Pharmaceutical research
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI can be used to generate new drug compounds, which can aid in the
    development of new treatments.
  prefs: []
  type: TYPE_NORMAL
- en: Creative writing
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI can be used to generate written content, such as fiction stories,
    poetry, news articles, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Game design
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI can be used to design and develop new game levels and content,
    creating an infinite variety of gameplay experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Digital design
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI can be used to create original digital art and animations, as
    well as to design and develop new user interfaces and web designs.
  prefs: []
  type: TYPE_NORMAL
- en: AI is often said to pose an existential threat to jobs in fields such as these,
    but I do not believe that this is actually the case. For me, AI is just another
    tool in the toolbox of these creative roles (albeit a very powerful one), rather
    than a replacement for the role itself. Those who choose to embrace this new technology
    will find that they are able to explore new ideas much faster and iterate over
    concepts in a way that previously was not possible.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI in Education
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One final area of everyday life that I believe will be significantly impacted
    is education. Generative AI challenges the fundamental axioms of education in
    a way that we haven’t seen since the dawn of the internet. The internet gave students
    the ability to retrieve information instantaneously and unambiguously, making
    exams that purely tested memorization and recall seem old-fashioned and irrelevant.
    This prompted a shift in approach, focused on testing students’ ability to synthesize
    ideas in a novel way instead of only testing factual knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: I believe that generative AI will cause another transformative shift in the
    field of education, necessitating a reevaluation and adjustment of current teaching
    methods and assessment criteria. If every student now has access to an essay-writing
    machine in their pocket that can generate novel responses to questions, what is
    the purpose of essay-based coursework?
  prefs: []
  type: TYPE_NORMAL
- en: Many would call for the use of such AI tools to be banned, in the same way that
    plagiarism is banned. However, it’s not that simple, as detecting AI-generated
    text is much harder than detecting plagiarism and even harder to prove beyond
    doubt. Moreover, students could use AI tools to generate a skeleton draft for
    the essay and then add extra detail or update factually incorrect information
    as required. In this case, is it the student’s original work, or the AI’s?
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, these are huge questions that need to be addressed in order for education
    and certifications to maintain their integrity. In my opinion, there is no sense
    in resisting the proliferation of AI tools within education—any such approach
    is doomed to fail, as they will become so widespread in everyday life that trying
    to restrict their use will be futile. Instead, we need to find ways to embrace
    the technology and ask how we can design *open-AI* coursework, in the same way
    that we allow *open-book* coursework, and encourage students to openly research
    material using the internet and AI tools.
  prefs: []
  type: TYPE_NORMAL
- en: The potential for generative AI to assist with the learning process itself is
    also immense and deeply profound. An AI-powered tutor could help a student learn
    a new topic (as shown in [Figure 14-10](#gpt3learning)), overcome a misunderstanding,
    or generate an entirely personalized study plan. The challenge of filtering truth
    from generated fiction is no different from what we currently have with information
    available on the internet and is a life skill that needs further attention across
    the curriculum.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1410.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-10\. Output from GPT-3—an example of how large language models can
    be used for learning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Generative AI can be an incredibly powerful tool to level the playing field
    between those who have access to excellent teachers and the best learning materials
    and those who do not. I am excited to see the progress in this space, as I believe
    it could unlock massive amounts of potential across the globe.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI Ethics and Challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the incredible progress that has been made in the field of generative
    AI, there remain many challenges to overcome. Some of these challenges are practical
    and others ethical.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a major criticism of large language models is that they are prone
    to generate misinformation when asked about a topic that is unfamiliar or contradictory,
    as shown in [Figure 14-4](#languagemodelproblems). The danger with this is that
    it is difficult to know if the information that is contained within a generated
    response is truly accurate. Even if you ask the LLM to explain its reasoning or
    cite sources, it might make up references or spout a series of statements that
    do not logically follow on from one another. This is not an easy problem to solve,
    as the LLM is nothing more than a set of weights that accurately capture the most
    likely next word given a set of input tokens—it does not have a bank of *true*
    information that it can use as a reference.
  prefs: []
  type: TYPE_NORMAL
- en: A potential solution to this problem is to provide large language models with
    the ability to call upon structured tools such as calculators, code compilers,
    and online information sources for tasks that require precise execution or facts.
    For example, [Figure 14-11](#toolformer_ex) shows output from a model called *Toolformer*,
    published by Meta in February 2023.^([4](ch14.xhtml#idm45387000734816))
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1411.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14-11\. An example of how Toolformer is able to autonomously call different
    APIs in order to obtain precise information where necessary (source: [Schick et
    al., 2023](https://arxiv.org/pdf/2302.04761.pdf))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Toolformer is able to explicitly call APIs for information, as part of its generative
    response. For example, it might use the Wikipedia API to retrieve information
    about a particular person, rather than relying on this information being embedded
    in its model weights. This approach is particularly useful for precise mathematical
    operations, where Toolformer can state which operations it would like to enter
    into the calculator API instead of trying to generate the answer autoregressively
    in the useful fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Another prominent ethical concern with generative AI centers on the fact that
    large companies have used huge amounts of data scraped from the web to train their
    models, when consent was not explicitly given by the original creators to do so.
    Often this data is not even publicly released, so it is impossible to know if
    your data is being used to train large language models or multimodal text-to-image
    models. Clearly this is a valid concern, particularly for artists, who may argue
    that it is usage of their artwork for which they are not being paid any royalties
    or commission. Moreover, an artist’s name may be used as a prompt in order to
    generate more artwork that is similar in style to the originals, thereby degrading
    the uniqueness of the content and commoditizing the style.
  prefs: []
  type: TYPE_NORMAL
- en: A solution to this problem is being pioneered by Stability AI, whose multimodal
    model Stable Diffusion is trained on a subset of the open source LAION-5B dataset.
    They have also launched the website [*Have I Been Trained?*](https://haveibeentrained.com)
    where anyone can search for a particular image or text passage within the training
    dataset and opt out of future inclusion in the model training process. This puts
    control back in the hands of the original creators and ensures that there is transparency
    in the data that is being used to create powerful tools like this one. However,
    this practice is not commonplace, and many commercially available generative AI
    models do not make their datasets or model weights open source or provide any
    option to opt out of the training process.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, while generative AI is a powerful tool for communication, productivity,
    and learning across everyday life, in the workplace, and in the field of education,
    there are both advantages and disadvantages to its widespread use. It is important
    to be aware of the potential risks of using the output from a generative AI model
    and to always be sure to use it responsibly. Nevertheless, I remain optimistic
    about the future of generative AI and am eager to see how businesses and people
    adapt to this new and exciting technology.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book we have taken a journey through the last decade of generative modeling
    research, starting out with the basic ideas behind VAEs, GANs, autoregressive
    models, normalizing flow models, energy-based models, and diffusion models and
    building upon these foundations to understand how state-of-the-art techniques
    such as VQ-GAN, Transformers, world models, and multimodal models are now pushing
    the boundaries of what generative models are capable of achieving, across a variety
    of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'I believe that in the future, generative modeling may be the key to a deeper
    form of artificial intelligence that transcends any one particular task and allows
    machines to organically formulate their own rewards, strategies, and perhaps awareness
    within their environment. My beliefs are closely aligned to the principle of *active
    inference*, originally pioneered by Karl Friston. The theory behind active inference
    could easily fill another entire book—and does, in Thomas Parr et al.’s excellent
    *Active Inference: The Free Energy Principle in Mind, Brain, and Behavior* (MIT
    Press), which I highly recommend—so I will only attempt a short explanation here.'
  prefs: []
  type: TYPE_NORMAL
- en: As babies, we are constantly exploring our surroundings, building up a mental
    model of possible futures with no apparent aim other than to develop a deeper
    understanding of the world. There are no labels on the data that we receive—a
    seemingly random stream of light and sound waves that bombard our senses from
    the moment we are born. Even when our someone points to an apple and says *apple*,
    there is no reason for our young brains to associate the two inputs and learn
    that the way in which light entered our eye at that particular moment is in some
    way related to the way the sound waves entered our ear. There is no training set
    of sounds and images, no training set of smells and tastes, and no training set
    of actions and rewards; there’s just an endless stream of extremely noisy data.
  prefs: []
  type: TYPE_NORMAL
- en: And yet here you are now, reading this sentence, perhaps enjoying the taste
    of a cup of coffee in a noisy cafe. You pay no attention to the background noise
    as you concentrate on converting the absence of light on a tiny portion of your
    retina into a sequence of abstract concepts that convey almost no meaning individually
    but, when combined, trigger a wave of parallel representations in your mind’s
    eye—images, emotions, ideas, beliefs, and potential actions all flood your consciousness,
    awaiting your recognition. The same noisy stream of data that was essentially
    meaningless to your infant brain is not so noisy anymore. Everything makes sense
    to you. You see structure everywhere. You are never surprised by the physics of
    everyday life. The world is the way that it is because your brain decided it should
    be that way. In this sense, your brain is an extremely sophisticated generative
    model, equipped with the ability to attend to particular parts of the input data,
    form representations of concepts within a latent space of neural pathways, and
    process sequential data over time.
  prefs: []
  type: TYPE_NORMAL
- en: Active inference is a framework that builds upon this idea to explain how the
    brain processes and integrates sensory information to make decisions and actions.
    It states that an organism has a generative model of the world it inhabits, and
    uses this model to make predictions about future events. In order to reduce the
    surprise caused by discrepancies between the model and reality, the organism adjusts
    its actions and beliefs accordingly. Friston’s key idea is that action and perception
    optimization can be framed as two sides of the same coin, with both seeking to
    minimize a single quantity known as *free energy*.
  prefs: []
  type: TYPE_NORMAL
- en: At the heart of this framework is a generative model of the environment (captured
    within the brain) that is constantly being compared to reality. Crucially, the
    brain is not a passive observer of events. In humans, it is attached to a neck
    and a set of legs that can put its core input sensors in a myriad of positions
    relative to the source of the input data. Therefore, the generated sequence of
    possible futures is not only dependent on its understanding of the physics of
    the environment, but also on its understanding of *itself* and how it acts. This
    feedback loop of action and perception is extremely interesting to me, and I believe
    we have only scratched the surface of what is possible with embodied generative
    models that are able to take actions within a given environment according to the
    principles of active inference.
  prefs: []
  type: TYPE_NORMAL
- en: This is the core idea that I believe will continue to propel generative modeling
    into the spotlight in the next decade, as one of the keys to unlocking artificial
    general intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, I encourage you to continue learning more about generative
    models from all the great material that is available online and in other books.
    Thank you for taking the time to read to the end of this book—I hope you have
    enjoyed reading it as much as I have enjoyed generating it!
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch14.xhtml#idm45387000972896-marker)) Hugo Touvron et al., “LLaMA: Open
    and Efficient Foundation Language Models,” February 27, 2023, [*https://arxiv.org/abs/2302.13971*](https://arxiv.org/abs/2302.13971).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch14.xhtml#idm45387000894496-marker)) Mark Chen et al., “Evaluating Large
    Language Models Trained on Code,” July 7, 2021, [*https://arxiv.org/abs/2107.03374*](https://arxiv.org/abs/2107.03374).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch14.xhtml#idm45387000869232-marker)) Lvmin Zhang and Maneesh Agrawala,
    “Adding Conditional Control to Text-to-Image Diffusion Models,” February 10, 2023,
    [*https://arxiv.org/abs/2302.05543*](https://arxiv.org/abs/2302.05543).
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch14.xhtml#idm45387000734816-marker)) Timo Schick et al., “Toolformer:
    Language Models Can Teach Themselves to Use Tools,” February 9, 2023, [*https://arxiv.org/abs/2302.04761*](https://arxiv.org/abs/2302.04761).'
  prefs: []
  type: TYPE_NORMAL
