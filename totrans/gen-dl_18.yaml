- en: Chapter 14\. Conclusion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章。结论
- en: In May 2018, I began work on the first edition of this book. Five years later,
    I am more excited than ever about the endless possibilities and potential impact
    of generative AI.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年5月，我开始着手第一版这本书的工作。五年后，我对生成AI的无限可能性和潜在影响感到比以往任何时候都更加兴奋。
- en: In this time we have seen incredible progress in this field, with seemingly
    limitless potential for real-world applications. I am filled with a sense of awe
    and wonder at what we have been able to achieve so far and eagerly anticipate
    witnessing the effect that generative AI will have on the world in the coming
    years. Generative deep learning has the power to shape the future in ways we can’t
    even begin to imagine.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段时间里，我们看到了这个领域的惊人进步，对真实世界应用有着看似无限的潜力。我对我们迄今为止所取得的成就感到敬畏和惊叹，并迫不及待地期待着生成AI未来几年将对世界产生的影响。生成深度学习有能力以我们无法想象的方式塑造未来。
- en: What’s more, as I have been researching content for this book, it has become
    ever clearer to me that this field isn’t just about creating images, text, or
    music. I believe that at the core of generative deep learning lies the secret
    of intelligence itself.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着我为这本书研究内容，我越来越清楚地意识到这个领域不仅仅是关于创建图像、文本或音乐。我相信生成深度学习的核心是智能本身的秘密。
- en: The first section of this chapter summarizes how we have reached this point
    in our generative AI journey. We will walk through a timeline of generative AI
    developments since 2014 in chronological order, so that you can see where each
    technique fits into the history of generative AI to date. The second section explains
    where we currently stand in terms of state-of-the-art generative AI. We will discuss
    current trends in the approach to generative deep learning and the current off-the-shelf
    models available to the general public. Next, we will explore the future of generative
    AI and the opportunities and challenges that lie ahead. We will consider what
    generative AI might look like five years in the future and its potential impact
    on society and business, and address some of the main ethical and practical concerns.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一部分总结了我们在生成AI之旅中达到这一点的过程。我们将按时间顺序浏览自2014年以来的生成AI发展时间轴，以便您可以看到每种技术在生成AI历史中的位置。第二部分解释了我们目前在最先进的生成AI方面的位置。我们将讨论生成深度学习方法的当前趋势以及普通公众可以使用的当前现成模型。接下来，我们将探讨生成AI的未来以及前方的机遇和挑战。我们将考虑未来五年生成AI可能会是什么样子，以及它对社会和商业的潜在影响，并解决一些主要的伦理和实际问题。
- en: Timeline of Generative AI
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成AI时间轴
- en: '[Figure 14-1](#timeline) is a timeline of the key developments in generative
    modeling that we have explored together in this book. The colors represent different
    model types.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14-1](#timeline)是我们在本书中一起探索的生成建模关键发展的时间轴。颜色代表不同的模型类型。'
- en: To field of generative AI stands on the shoulders of earlier developments in
    deep learning, such as backpropagation and convolutional neural networks, which
    unlocked the possibility for models to learn complex relationships across large
    datasets at scale. In this section, we will study the modern history of generative
    AI, from 2014 onwards, that has moved at such breathtaking speed.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 生成AI领域建立在深度学习早期发展的基础上，比如反向传播和卷积神经网络，这些技术解锁了模型在大规模数据集上学习复杂关系的可能性。在本节中，我们将研究生成AI的现代历史，从2014年开始，这一历史发展速度惊人。
- en: 'To help us understand how everything fits together, we can loosely break down
    this history into three main eras:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们理解所有内容如何相互关联，我们可以大致将这段历史分为三个主要时代：
- en: '2014–2017: The VAE and GAN era'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2014年至2017年：VAE和GAN时代
- en: '2018–2019: The Transformer era'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2018年至2019年：变压器时代
- en: '2020–2022: The Big Model era'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2020年至2022年：大模型时代
- en: '![](Images/gdl2_1401.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1401.png)'
- en: 'Figure 14-1\. A brief history of generative AI from 2014 to 2023 (note: some
    important developments such as LSTMs and early energy-based models [e.g., Boltzmann
    machines] precede this timeline)'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-1。从2014年到2023年的生成AI简史（注意：一些重要的发展，如LSTM和早期基于能量的模型[例如，玻尔兹曼机]在这个时间轴之前）
- en: '2014–2017: The VAE and GAN Era'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2014年至2017年：VAE和GAN时代
- en: The invention of the VAE in December 2013 can perhaps be thought of as the spark
    that lit the generative AI touchpaper. This paper showed how it was possible to
    generate not only simple images such as MNIST digits but also more complex images
    such as faces in a latent space that could be smoothly traversed. It was followed
    in 2014 by the introduction of the GAN, an entirely new adversarial framework
    for tackling generative modeling problems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: VAE的发明可以说是点燃生成AI火药桶的火花。这篇论文展示了不仅可以生成简单的图像，如MNIST数字，还可以生成更复杂的图像，如面孔，而且可以在一个可以平滑遍历的潜在空间中生成。2014年，GAN的引入紧随其后，这是一种全新的对抗性框架，用于解决生成建模问题。
- en: The following three years were dominated by progressively more impressive extensions
    of the GAN portfolio. In addition to fundamental changes to the GAN model architecture
    (DCGAN, 2015), loss function (Wasserstein GAN, 2017), and training process (ProGAN,
    2017), new domains were tackled using GANs, such as image-to-image translation
    (pix2pix, 2016, and CycleGAN, 2017) and music generation (MuseGAN, 2017).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的三年被逐渐更令人印象深刻的GAN系列扩展所主导。除了对GAN模型架构（DCGAN，2015）、损失函数（Wasserstein GAN，2017）和训练过程（ProGAN，2017）的基本改变外，还使用GAN处理了新的领域，如图像到图像的转换（pix2pix，2016，和CycleGAN，2017）和音乐生成（MuseGAN，2017）。
- en: During this era, important VAE improvements were also introduced, such as VAE-GAN
    (2015) and later VQ-VAE (2017), and applications to reinforcement learning were
    seen in the “World Models” paper (2018).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个时代，还引入了重要的VAE改进，如VAE-GAN（2015）和后来的VQ-VAE（2017），并且在“世界模型”论文中看到了对强化学习的应用。
- en: Established autoregressive models such as LSTMs and GRUs remained the dominant
    force in text generation over this time. The same autoregressive ideas were also
    being used to generate images, with PixelRNN (2016) and PixelCNN (2016) introduced
    as new ways to think about image generation. Other approaches to image generation
    were also being tested, such as the RealNVP model (2016) that paved the way for
    later types of normalizing flow models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段时间内，已建立的自回归模型，如LSTMs和GRUs，仍然是文本生成的主导力量。相同的自回归思想也被用于生成图像，PixelRNN（2016年）和PixelCNN（2016年）被引入作为思考图像生成的新方法。还在测试其他图像生成方法，例如RealNVP模型（2016年），为后来的各种归一化流模型铺平了道路。
- en: In June 2017, a groundbreaking paper entitled “Attention Is All You Need” was
    published that would usher in the next era of generative AI, focused around Transformers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年6月，一篇开创性的论文《注意力就是一切》发表，开启了以Transformer为中心的生成AI的下一个时代。
- en: '2018–2019: The Transformer Era'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2018年至2019年：Transformer时代
- en: At the heart of a Transformer is the attention mechanism that negates the need
    for the recurrent layers present in older autoregressive models such as LSTMs.
    The Transformer quickly rose to prominence with the introduction of GPT (a decoder-only
    Transformer) and BERT (an encoder-only Transformer) in 2018\. The following year
    saw progressively larger language models being built that excelled at a wide range
    of tasks by treating them as pure text-to-text generation problems, with GPT-2
    (2018, 1.5B parameters) and T5 (2019, 11B parameters) being standout examples.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的核心是注意力机制，它消除了旧的自回归模型（如LSTMs）中存在的循环层的需求。Transformer随着2018年GPT（仅解码器Transformer）和BERT（仅编码器Transformer）的推出迅速崭露头角。接下来的一年，逐渐建立了更大的语言模型，通过将它们视为纯文本到文本生成问题，擅长各种任务，其中GPT-2（2018年，15亿参数）和T5（2019年，110亿参数）是杰出的例子。
- en: Transformers were also starting to be successfully applied to music generation,
    with the introduction of, for example, the Music Transformer (2018) and MuseNet
    (2019) models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer也开始成功应用于音乐生成，例如Music Transformer（2018年）和MuseNet（2019年）模型的引入。
- en: Over these two years, several impressive GANs were also released that cemented
    the technique’s place as the state-of-the-art approach for image generation. In
    particular, SAGAN (2018) and the larger BigGAN (2018) incorporated the attention
    mechanism into the GAN framework with incredible results, and StyleGAN (2018)
    and later StyleGAN2 (2019) showed how images could be generated with amazing fine-grained
    control over the style and content of a particular image.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两年里，也发布了几个令人印象深刻的GAN，巩固了该技术作为图像生成的最先进方法的地位。特别是，SAGAN（2018年）和更大的BigGAN（2018年）将注意力机制与GAN框架结合起来，取得了令人难以置信的结果，而StyleGAN（2018年）和后来的StyleGAN2（2019年）展示了如何以惊人的细粒度控制生成图像的风格和内容。
- en: Another field of generative AI that was gathering momentum was score-based models
    (NCSN, 2019), which would eventually pave the way for the next seismic shift in
    the generative AI landscape—diffusion models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个正在积聚动力的生成AI领域是基于分数的模型（NCSN，2019年），最终为生成AI领域的下一个重大变革——扩散模型铺平了道路。
- en: '2020–2022: The Big Model Era'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2020年至2022年：大模型时代
- en: This era saw the introduction of several models that merged ideas across different
    generative modeling families and turbo-charged existing architectures. For example,
    the VQ-GAN (2020) brought the GAN discriminator into the VQ-VAE architecture and
    the Vision Transformer (2020) showed how it was possible to train a Transformer
    to operate over images. 2022 saw the release of StyleGAN-XL, a further update
    to the StyleGAN architecture that enables 1,024 × 1,024–pixel images to be generated.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个时代见证了几个模型的推出，这些模型融合了不同生成建模家族的思想，并加速了现有架构。例如，VQ-GAN（2020年）将GAN鉴别器引入VQ-VAE架构，Vision
    Transformer（2020年）展示了如何训练Transformer在图像上运行的可能性。2022年发布了StyleGAN-XL，这是对StyleGAN架构的进一步更新，可以生成1024×1024像素的图像。
- en: 'Two models were introduced in 2020 that would lay the foundations for all future
    large image generation models: DDPM and DDIM. Suddenly, diffusion models were
    a rival for GANs in terms of image generation quality, as explicitly stated in
    the title of the 2021 paper “Diffusion Models Beat GANs on Image Synthesis.” The
    image quality of diffusion models is unbelievably good and they only require a
    single U-Net network to be trained, rather than the dual-network setup of a GAN,
    making the training process much more stable.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年推出了两个模型，为所有未来大型图像生成模型奠定了基础：DDPM和DDIM。突然之间，扩散模型在图像生成质量方面成为GAN的竞争对手，正如2021年的论文标题“扩散模型在图像合成方面击败了GAN”所明确说明的那样。扩散模型的图像质量令人难以置信地好，它们只需要训练一个单一的U-Net网络，而不是GAN的双网络设置，使训练过程更加稳定。
- en: Around the same time, GPT-3 (2020) was released—an enormous 175B parameter Transformer
    that can generate text on just about any topic in a way that seems almost impossible
    to comprehend. The model was released through a web application and API, allowing
    companies to build products and services on top of it. ChatGPT (2022) is a web
    application and API wrapper around the latest version of GPT from OpenAI that
    allows users to have natural conversations with the AI about any topic.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在同一时间，GPT-3（2020年）发布了——这是一个庞大的1750亿参数的Transformer，可以以一种几乎难以理解的方式生成几乎任何主题的文本。该模型通过一个网络应用程序和API发布，允许公司在其基础上构建产品和服务。ChatGPT（2022年）是一个围绕OpenAI最新版本的GPT的网络应用程序和API封装器，允许用户与AI就任何主题进行自然对话。
- en: Over 2021 and 2022, a flurry of other large language models were released to
    rival GPT-3, including Megatron-Turing NLG (2021) by Microsoft and NVIDIA, Gopher
    (2021) and Chinchilla by DeepMind (2022), LaMDA (2022) and PaLM (2022) by Google,
    and Luminous (2022) by Aleph Alpha. Some open source models were also released,
    such as GPT-Neo (2021), GPT-J (2021), and GPT-NeoX (2022) by EleutherAI; the 66B
    parameter OPT model (2022) by Meta; the fine-tuned Flan-T5 model (2022) by Google,
    BLOOM (2022) by Hugging Face; and others. Each of these models is a variation
    of a Transformer, trained on a huge corpus of data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在2021年和2022年，一大批其他大型语言模型相继发布，以与GPT-3竞争，包括微软和英伟达的Megatron-Turing NLG（2021年），DeepMind的Gopher（2021年）和Chinchilla（2022年），谷歌的LaMDA（2022年）和PaLM（2022年），以及Aleph
    Alpha的Luminous（2022年）。还发布了一些开源模型，如EleutherAI的GPT-Neo（2021年），GPT-J（2021年）和GPT-NeoX（2022年）；Meta的66B参数OPT模型（2022年）；谷歌的Fine-tuned
    Flan-T5模型（2022年）；Hugging Face的BLOOM（2022年）等等。这些模型都是Transformer的变体，训练在大量数据语料库上。
- en: The rapid rise of powerful Transformers for text generation and state-of-the-art
    diffusion models for image generation has meant that much of the focus of the
    last two years of generative AI development has been on multimodal models—that
    is, models that operate over more than one domain (for example, text-to-image
    models).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的Transformer用于文本生成和最先进的扩散模型用于图像生成的迅速崛起意味着过去两年生成AI发展的重点大部分集中在多模态模型上，即在超过一个领域（例如文本到图像模型）上运行的模型。
- en: 'This trend was established in 2021 when OpenAI released DALL.E, a text-to-image
    model based upon a discrete VAE (similar to VQ-VAE) and CLIP (a Transformer model
    that predicts image/text pairs). This was followed by GLIDE (2021) and DALL.E
    2 (2022), which updated the generative part of the model to use a diffusion model
    rather than a discrete VAE, with truly impressive results. This era also saw the
    release of three text-to-image models from Google: Imagen (2022, using Transformer
    and diffusion models), Parti (2022, using Transformers and a ViT-VQGAN model),
    and later MUSE (2023, using Transformers and VQ-GANs). DeepMind also released
    Flamingo (2022), a visual language model that builds upon their large language
    model Chinchilla by allowing images to be used as part of the prompt data.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这一趋势始于2021年，当OpenAI发布了DALL.E，这是一个基于离散VAE（类似于VQ-VAE）和CLIP（一种预测图像/文本对的Transformer模型）的文本到图像模型。随后是GLIDE（2021年）和DALL.E
    2（2022年），更新了模型的生成部分，使用扩散模型而不是离散VAE，取得了真正令人印象深刻的结果。这一时代还见证了谷歌发布的三个文本到图像模型：Imagen（2022年，使用Transformer和扩散模型），Parti（2022年，使用Transformers和ViT-VQGAN模型），以及后来的MUSE（2023年，使用Transformers和VQ-GANs）。DeepMind也发布了Flamingo（2022年），这是一个视觉语言模型，建立在他们的大型语言模型Chinchilla的基础上，允许图像作为提示数据的一部分。
- en: Another important diffusion advancement introduced in 2021 was latent diffusion,
    where a diffusion model is trained within the latent space of an autoencoder.
    This technique powers the Stable Diffusion model, released as a joint collaboration
    between Stability AI, CompVis, and Runway in 2022\. Unlike with DALL.E 2, Imagen,
    and Flamingo, the code and model weights of Stable Diffusion are open source,
    meaning anyone can run the model on their own hardware.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年引入的另一个重要扩散进展是潜在扩散，其中扩散模型在自动编码器的潜在空间内进行训练。这一技术推动了Stable Diffusion模型的诞生，该模型由Stability
    AI、CompVis和Runway在2022年联合合作发布。与DALL.E 2、Imagen和Flamingo不同，Stable Diffusion的代码和模型权重是开源的，这意味着任何人都可以在自己的硬件上运行该模型。
- en: The Current State of Generative AI
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成AI的当前状态
- en: As we come to end of our journey through the history of generative AI, it is
    important to now reflect on where we stand in terms of current state-of-the-art
    applications and models. Let’s take a moment to assess our progress and key accomplishments
    in the field to date.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们结束对生成AI历史的探索时，现在重要的是反思我们在当前最先进应用和模型方面的立足点。让我们花一点时间评估我们在这一领域迄今取得的进展和关键成就。
- en: Large Language Models
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型
- en: Generative AI for text is now almost entirely focused on building large language
    models (LLMs), whose sole purpose is to directly model language from a huge corpus
    of text—that is, they are trained to predict the next word, in the style of a
    decoder Transformer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，文本生成的生成AI几乎完全集中在构建大型语言模型（LLMs）上，它们的唯一目的是直接从大量文本语料库中建模语言，即它们被训练来预测下一个词，以解码器Transformer的风格。
- en: The large language model approach has been adopted so widely because of its
    flexibility and ability to excel at a wide range of tasks. The same model can
    be used for question answering, text summarization, content creation, and many
    other examples because ultimately each use case can be framed as a text-to-text
    problem, where the specific task instructions (the *prompt*) are given as part
    of the input to the model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型方法被广泛采用，因为它具有灵活性和在各种任务上表现出色的能力。同一模型可以用于问答、文本摘要、内容创作等多种示例，因为最终每个用例都可以被构建为一个文本到文本问题，其中特定任务指令（*提示*）作为模型输入的一部分给出。
- en: Let’s take [GPT-3](https://oreil.ly/Pga1w) as an example. [Figure 14-2](#gpt3-examples)
    shows how the same model can be used for text summarization and content creation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以[GPT-3](https://oreil.ly/Pga1w)为例。[图14-2](#gpt3-examples)展示了同一模型如何用于文本摘要和内容创作。
- en: '![](Images/gdl2_1402.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1402.png)'
- en: Figure 14-2\. Output from GPT-3—the non-highlighted text is the prompt and the
    green highlighted text is the output from GPT-3
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-2。来自GPT-3的输出——未突出显示的文本是提示，绿色突出显示的文本是GPT-3的输出
- en: Notice how in both cases, the prompt contains the relevant instructions. The
    job of GPT-3 is just to continue the prompt, one token at a time. It doesn’t have
    a database of facts from which it can look up information, or snippets of text
    that it can copy into its answers. It is only asked to predict what token is most
    likely to follow the existing tokens and then append this prediction to the prompt
    to generate the next token, and so on.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这两种情况下，提示包含相关的指令。GPT-3的任务只是逐个标记地继续提示。它没有一个可以查找信息的事实数据库，也没有可以复制到答案中的文本片段。它只被要求预测接下来最有可能跟随现有标记的标记，然后将这个预测附加到提示中以生成下一个标记，依此类推。
- en: Incredibly, this simple design is enough for the language model to excel at
    a range of tasks, as shown in [Figure 14-2](#gpt3-examples). Moreover, it gives
    the language model incredible flexibility to generate realistic text as a response
    to any prompt—imagination is often the limiting factor!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 令人难以置信的是，这种简单的设计足以使语言模型在各种任务中表现出色，如[图14-2](#gpt3-examples)所示。此外，它赋予了语言模型令人难以置信的灵活性，可以根据任何提示生成逼真的文本作为回应——想象力通常是限制因素！
- en: '[Figure 14-3](#languagemodelsize) shows how large language models have grown
    in size since the original GPT model was published in 2018\. The number of parameters
    grew exponentially until late 2021, with Megatron-Turing NLG reaching 530B parameters.
    Recently, more emphasis has been placed on building more efficient language models
    that use fewer parameters, as larger models are more costly and slower to serve
    in a production environment.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14-3](#languagemodelsize)显示自2018年原始GPT模型发布以来，大型语言模型的规模如何增长。参数数量呈指数增长，直到2021年底，Megatron-Turing
    NLG达到5300亿参数。最近，更多的重点放在构建更高效的语言模型上，这些模型使用更少的参数，因为更大的模型在生产环境中更昂贵且速度较慢。'
- en: '![](Images/gdl2_1403.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1403.png)'
- en: Figure 14-3\. The size of large language models (orange) and multimodal models
    (pink) in number of parameters over time
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-3\. 大型语言模型（橙色）和多模型（粉色）的参数数量随时间变化
- en: OpenAI’s GPT collection (GPT-3, GPT-3.5, GPT-4, etc.) is still considered by
    many to be the most powerful state-of-the-art suite of language models available
    for personal and commercial use. They are each available through a [web application](https://platform.openai.com/playground)
    and [API](https://openai.com/api).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人仍认为OpenAI的GPT系列（GPT-3、GPT-3.5、GPT-4等）是目前个人和商业使用中最强大的最新语言模型套件。它们可以通过[网络应用](https://platform.openai.com/playground)和[API](https://openai.com/api)使用。
- en: Another recent addition to the large language model family is *Large Language
    Model Meta AI* (LLaMA) from Meta,^([1](ch14.xhtml#idm45387000972896)) a suite
    of models ranging from 7B to 65B parameters in size that are trained purely on
    publicly available datasets.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型家族的另一个最新成员是Meta推出的*大型语言模型Meta AI*（LLaMA），^([1](ch14.xhtml#idm45387000972896))，这是一套从7B到65B参数大小的模型系列，纯粹基于公开可用的数据集进行训练。
- en: A summary of some of the most powerful LLMs in existence today is shown in [Table 14-1](#llms).
    Some, like LLaMA, are families of models of different sizes—in this case, the
    size of the largest model is shown here. Pre-trained weights are fully open source
    for some of the models, meaning that they are free for anyone to use and build
    upon.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 今天存在的一些最强大的LLM的摘要显示在[表14-1](#llms)中。有些模型，如LLaMA，是不同规模模型的系列—在这种情况下，最大模型的规模显示在这里。一些模型的预训练权重是完全开源的，这意味着任何人都可以免费使用和构建。
- en: Table 14-1\. Large language models
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表14-1\. 大型语言模型
- en: '| Model | Date | Developer | # parameters | Open source |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 日期 | 开发者 | # 参数 | 开源 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GPT-3 | May 2020 | OpenAI | 175,000,000,000 | No |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3 | 2020年5月 | OpenAI | 1750亿 | 否 |'
- en: '| GPT-Neo | Mar 2021 | EleutherAI | 2,700,000,000 | Yes |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| GPT-Neo | 2021年3月 | EleutherAI | 27亿 | 是 |'
- en: '| GPT-J | Jun 2021 | EleutherAI | 6,000,000,000 | Yes |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| GPT-J | 2021年6月 | EleutherAI | 60亿 | 是 |'
- en: '| Megatron-Turing NLG | Oct 2021 | Microsoft & NVIDIA | 530,000,000,000 | No
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Megatron-Turing NLG | 2021年10月 | 微软和英伟达 | 5300亿 | 否 |'
- en: '| Gopher | Dec 2021 | DeepMind | 280,000,000,000 | No |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Gopher | 2021年12月 | DeepMind | 2800亿 | 否 |'
- en: '| LaMDA | Jan 2022 | Google | 137,000,000,000 | No |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| LaMDA | 2022年1月 | 谷歌 | 1370亿 | 否 |'
- en: '| GPT-NeoX | Feb 2022 | EleutherAI | 20,000,000,000 | Yes |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| GPT-NeoX | 2022年2月 | EleutherAI | 200亿 | 是 |'
- en: '| Chinchilla | Mar 2022 | DeepMind | 70,000,000,000 | No |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Chinchilla | 2022年3月 | DeepMind | 700亿 | 否 |'
- en: '| PaLM | Apr 2022 | Google | 540,000,000,000 | No |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| PaLM | 2022年4月 | 谷歌 | 5400亿 | 否 |'
- en: '| Luminous | Apr 2022 | Aleph Alpha | 70,000,000,000 | No |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Luminous | 2022年4月 | Aleph Alpha | 700亿 | 否 |'
- en: '| OPT | May 2022 | Meta | 175,000,000,000 | Yes (66B) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| OPT | 2022年5月 | Meta | 1750亿 | 是（660亿） |'
- en: '| BLOOM | Jul 2022 | Hugging Face collaboration | 175,000,000,000 | Yes |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM | 2022年7月 | Hugging Face合作 | 1750亿 | 是 |'
- en: '| Flan-T5 | Oct 2022 | Google | 11,000,000,000 | Yes |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5 | 2022年10月 | 谷歌 | 110亿 | 是 |'
- en: '| GPT-3.5 | Nov 2022 | OpenAI | Unknown | No |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 2022年11月 | OpenAI | 未知 | 否 |'
- en: '| LLaMA | Feb 2023 | Meta | 65,000,000,000 | No |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA | 2023年2月 | Meta | 650亿 | 否 |'
- en: '| GPT-4 | Mar 2023 | OpenAI | Unknown | No |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 2023年3月 | OpenAI | 未知 | 否 |'
- en: Despite the impressive applications of large language models, there remain significant
    challenges to overcome. Most notably, they are prone to inventing facts and cannot
    reliably apply logical thought processes, as shown in [Figure 14-4](#languagemodelproblems).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型有令人印象深刻的应用，但仍然存在重大挑战需要克服。最值得注意的是，它们容易虚构事实，无法可靠地应用逻辑思维过程，如[图14-4](#languagemodelproblems)所示。
- en: '![](Images/gdl2_1404.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1404.png)'
- en: Figure 14-4\. While large language models excel at some tasks, they are also
    prone to mistakes related to factual or logical reasoning (GPT-3 output shown)
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-4\. 虽然大型语言模型在某些任务上表现出色，但也容易出现与事实或逻辑推理相关的错误（显示了GPT-3的输出）
- en: It is important to remember that LLMs are trained only to predict the next word.
    They have no other connection to reality that would allow them to reliably identify
    factual or logical fallacies. Therefore, we must be extremely cautious about how
    we use these powerful text prediction models in production—they cannot yet be
    reliably utilized for anything that requires precise reasoning.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，LLMs只是被训练来预测下一个单词。它们与现实没有其他联系，无法可靠地识别事实或逻辑谬误。因此，在生产中使用这些强大的文本预测模型时，我们必须非常谨慎——它们尚不能可靠地用于需要精确推理的任何事情。
- en: Text-to-Code Models
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到代码模型
- en: Another application of large language models is code generation. In July 2021,
    OpenAI introduced a model called *Codex*, a GPT language model that had been fine-tuned
    on code from GitHub.^([2](ch14.xhtml#idm45387000894496)) The model was able to
    successfully write novel coded solutions to a range of problems, prompted only
    with a comment on the problem to be solved, or a function name. The technology
    today powers [GitHub Copilot](https://oreil.ly/P5WXo), an AI pair programmer that
    can be used to suggest code in real time as you type. Copilot is a paid subscription-based
    service, with a free trial period.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型的另一个应用是代码生成。2021年7月，OpenAI推出了一个名为Codex的模型，这是一个在GitHub上的代码上进行了微调的GPT语言模型。该模型能够成功地为一系列问题编写新颖的编码解决方案，只需根据要解决的问题的评论或函数名称进行提示。这项技术如今驱动着GitHub
    Copilot，这是一个可以在您输入时实时建议代码的AI对编程师。Copilot是一个基于订阅的付费服务，提供免费试用期。
- en: '[Figure 14-5](#codex) shows two examples of autogenerated completions. The
    first example is a function that fetches tweets from a given user, using the Twitter
    API. Given the function name and parameter, Copilot is able to autocomplete the
    rest of the function definition. The second example asks Copilot to parse a list
    of expenses, by additionally including a free text description in the docstring
    that explains the format of the input parameter and specific instructions related
    to the task. Copilot is able to autocomplete the entire function from the description
    alone.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14-5](#codex)显示了两个自动生成的完成示例。第一个示例是一个从给定用户那里获取推文的函数，使用Twitter API。给定函数名称和参数，Copilot能够自动完成函数定义的其余部分。第二个示例要求Copilot解析一组费用，还包括在docstring中包含一个自由文本描述，解释输入参数的格式以及与任务相关的具体说明。Copilot能够仅通过描述自动完成整个函数。'
- en: This remarkable technology is already beginning to change how programmers approach
    a given task. A significant proportion of a programmer’s time is usually spent
    searching for examples of existing solutions, reading community Q&A forums such
    as Stack Overflow, and looking up syntax in package documentation. This means
    leaving the interactive development environment (IDE) through which you are coding,
    switching to a web browser, and copying and pasting code snippets from the web
    to see if they solve your specific problem. Copilot removes the need to do this
    in many cases, because you can simply tab through potential solutions generated
    by the AI from within the IDE, after writing a brief description of what you are
    looking to achieve.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这项引人注目的技术已经开始改变程序员处理特定任务的方式。程序员通常会花费相当大的时间搜索现有解决方案的示例，阅读社区问答论坛，如Stack Overflow，并查阅包文档中的语法。这意味着离开交互式开发环境（IDE），切换到Web浏览器，并从Web上复制和粘贴代码片段，以查看它们是否解决了您的特定问题。在许多情况下，Copilot消除了这样做的必要性，因为您只需在IDE中写下您希望实现的简要描述后，就可以通过AI生成的潜在解决方案进行选项卡切换。
- en: '![](Images/gdl2_1405.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1405.png)'
- en: 'Figure 14-5\. Two examples of GitHub Copilot capabilities (source: GitHub Copilot)'
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-5. GitHub Copilot功能的两个示例（来源：GitHub Copilot）
- en: Text-to-Image Models
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到图像模型
- en: State-of-the-art image generation is currently dominated by large multimodal
    models that convert a given text prompt into an image. Text-to-image models are
    highly useful as they allow users to easily manipulate generated images via natural
    language. This is in contrast to models such as StyleGAN, which, while extremely
    impressive, does not have a text interface through which you can describe the
    image that you want to be generated.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，最先进的图像生成主要由将给定文本提示转换为图像的大型多模态模型主导。文本到图像模型非常有用，因为它们允许用户通过自然语言轻松地操纵生成的图像。这与诸如StyleGAN之类的模型形成对比，后者虽然非常令人印象深刻，但没有通过您可以描述要生成的图像的文本界面。
- en: Three important text-to-image generation models that are currently available
    for commercial and personal use are DALL.E 2, Midjourney, and Stable Diffusion.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 目前可供商业和个人使用的三个重要的文本到图像生成模型是DALL.E 2、Midjourney和Stable Diffusion。
- en: DALL.E 2 by OpenAI is a pay-as-you-go service that is available through a web
    application and [API](https://labs.openai.com). [Midjourney](https://midjourney.com)
    provides a subscription-based text-to-image service through its Discord channel.
    Both DALL.E 2 and Midjourney offer free credits to those joining the platform
    for early experimentation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的DALL.E 2是一项按需付费服务，可通过Web应用程序和API获得。Midjourney通过其Discord频道提供基于订阅的文本到图像服务。DALL.E
    2和Midjourney都为那些加入平台进行早期实验的用户提供免费积分。
- en: Midjourney
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Midjourney
- en: Midjourney is the service used to create the illustrations for the stories in
    [Part II](part02.xhtml#part_methods) of this book!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Midjourney是用于本书第II部分故事插图的服务！
- en: Stable Diffusion is different because it is fully open source. The model weights
    and code to train the model are available on [GitHub](https://oreil.ly/C47vN),
    so anyone can run the model on their own hardware. The dataset used to train Stable
    Diffusion is also open source. This dataset, called [LAION-5B](https://oreil.ly/2O758),
    contains 5.85 billion image-text pairs and is currently the largest openly accessible
    image-text dataset in the world.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion不同，因为它是完全开源的。用于训练模型的模型权重和代码都可以在GitHub上找到，因此任何人都可以在自己的硬件上运行该模型。用于训练Stable
    Diffusion的数据集也是开源的。这个名为LAION-5B的数据集包含了58.5亿个图像文本对，目前是世界上最大的公开可访问的图像文本数据集。
- en: An important corollary of this approach is that the baseline Stable Diffusion
    model can be built upon and adapted to different use cases. An excellent demonstration
    of this is ControlNet, a neural network structure that allows fine-grained control
    of the output from Stable Diffusion by adding extra conditions.^([3](ch14.xhtml#idm45387000869232))
    For example, output images can be conditioned on a [Canny edge map](https://oreil.ly/8v9Ym)
    of a given input image, as shown in [Figure 14-6](#controlnet_canny).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个重要推论是，基线稳定扩散模型可以被构建并适应不同的用例。ControlNet就是这一点的一个很好的演示，它是一种神经网络结构，允许通过添加额外条件对稳定扩散的输出进行细粒度控制。例如，输出图像可以根据给定输入图像的[Canny边缘图](https://oreil.ly/8v9Ym)进行条件化，如[图14-6](#controlnet_canny)所示。
- en: '![](Images/gdl2_1406.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1406.png)'
- en: 'Figure 14-6\. Conditioning the output of Stable Diffusion using a Canny edge
    map and ControlNet (source: [Lvmin Zhang, ControlNet](https://github.com/lllyasviel/ControlNet))'
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-6。使用Canny边缘图和ControlNet对稳定扩散输出进行条件化（来源：[Lvmin Zhang, ControlNet](https://github.com/lllyasviel/ControlNet)）
- en: ControlNet contains a trainable copy of the Stable Diffusion encoder, alongside
    a locked copy of the full Stable Diffusion model. The job of this trainable encoder
    is to learn how to handle the input condition (e.g., the Canny edge map), whilst
    the locked copy retains the power of the original model. This way, Stable Diffusion
    can be fine-tuned using only a small number of image pairs. Zero convolutions
    are simply 1 × 1 convolutions where all weights and biases are zero, so that before
    training, ControlNet does not have any effect.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet包含一个可训练的稳定扩散编码器副本，以及一个完整的稳定扩散模型的锁定副本。这个可训练的编码器的任务是学习如何处理输入条件（例如，Canny边缘图），而锁定副本保留了原始模型的功能。这样，稳定扩散可以仅使用少量图像对进行微调。零卷积简单地是所有权重和偏置都为零的1×1卷积，因此在训练之前，ControlNet没有任何效果。
- en: '![](Images/gdl2_1407.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1407.png)'
- en: 'Figure 14-7\. The ControlNet architecture, with the trainable copies of the
    Stable Diffusion encoder blocks highlighted in blue (source: [Lvmin Zhang, ControlNet](https://github.com/lllyasviel/ControlNet))'
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-7。ControlNet架构，可训练的稳定扩散编码器块用蓝色突出显示（来源：[Lvmin Zhang, ControlNet](https://github.com/lllyasviel/ControlNet)）
- en: Another advantage of Stable Diffusion is that it is able to run on a single
    modestly sized GPU with only 8 GB of VRAM, making it possible to run on edge devices,
    rather than through calls to a cloud service. As text-to-image services are included
    in downstream products, the speed of generation is becoming increasingly more
    important. This is one reason why the size of multimodal models is generally trending
    downward (see [Figure 14-3](#languagemodelsize)).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散的另一个优点是，它能够在仅具有8 GB VRAM的单个中等大小GPU上运行，这使得它可以在边缘设备上运行，而不是通过调用云服务。随着文本到图像服务包含在下游产品中，生成速度变得越来越重要。这也是为什么多模态模型的大小通常趋向于减小的原因之一（参见[图14-3](#languagemodelsize)）。
- en: Example outputs for all three models can be seen in [Figure 14-8](#multimodal_comparison).
    All of these models are exceptional and are able to capture the content and style
    of the given description.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 三种模型的示例输出可以在[图14-8](#multimodal_comparison)中看到。所有这些模型都非常出色，能够捕捉给定描述的内容和风格。
- en: '![](Images/gdl2_1408.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1408.png)'
- en: Figure 14-8\. Outputs from Stable Diffusion v2.1, Midjourney, and DALL.E 2 for
    the same prompt
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-8。稳定扩散v2.1、Midjourney和DALL.E 2对相同提示的输出
- en: A summary of some of the most powerful text-to-image models in existence today
    is shown in [Table 14-2](#text_to_image).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 今天存在的一些最强大的文本到图像模型的摘要显示在[表14-2](#text_to_image)中。
- en: Table 14-2\. Text-to-image models
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表14-2。文本到图像模型
- en: '| Model | Date | Developer | # parameters | Open source |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 日期 | 开发者 | # 参数 | 开源 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| DALL.E 2 | Apr 2022 | OpenAI | 3,500,000,000 | No |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| DALL.E 2 | 2022年4月 | OpenAI | 35亿 | 否 |'
- en: '| Imagen | May 2022 | Google | 4,600,000,000 | No |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Imagen | 2022年5月 | 谷歌 | 46亿 | 否 |'
- en: '| Parti | Jun 2022 | Google | 20,000,000,000 | No |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Parti | 2022年6月 | 谷歌 | 200亿 | 否 |'
- en: '| Stable Diffusion | Aug 2022 | Stability AI, CompVis, and Runway | 890,000,000
    | Yes |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 稳定扩散 | 2022年8月 | Stability AI、CompVis和Runway | 8.9亿 | 是 |'
- en: '| MUSE | Jan 2023 | Google | 3,000,000,000 | No |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| MUSE | 2023年1月 | 谷歌 | 30亿 | 否 |'
- en: Part of the skill of working with text-to-image models is creating a prompt
    that both describes the content of the image you want to generate and uses keywords
    that encourage the model to produce a particular style or type of image. For example,
    adjectives such as *stunning* or *award-winning* can often be used to improve
    the quality of the generation. However, it is not always the case that the same
    prompt will work well across different models—it depends on the contents of the
    specific text-image dataset used to train the model. The art of uncovering prompts
    that work well for a particular model is known as *prompt engineering*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本到图像模型的技巧之一是创建一个提示，既描述您想要生成的图像的内容，又使用鼓励模型生成特定风格或类型图像的关键词。例如，诸如*令人惊叹*或*获奖*之类的形容词通常可以用来提高生成的质量。然而，并不总是同一个提示在不同模型上都能很好地工作——这取决于用于训练模型的特定文本-图像数据集的内容。发现适合特定模型的提示的艺术被称为*提示工程*。
- en: Other Applications
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他应用
- en: Generative AI is rapidly finding applications across a variety of novel domains,
    from reinforcement learning to other kinds of *text-to-X* multimodal models.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能正在迅速在各种新领域中找到应用，从强化学习到其他种类的*文本到X*多模态模型。
- en: For example, in November 2022 Meta published a paper on [*CICERO*](https://oreil.ly/kBQvY),
    an AI agent trained to play the board game *Diplomacy*. In this game, players
    represent different countries in Europe before World War I and must negotiate
    with and deceive each other in order to gain control of the continent. It is a
    highly complex game for an AI agent to master, not least because there is a communicative
    element where players must discuss their plans with other players in order to
    gain allies, coordinate maneuvers, and suggest strategic goals. To achieve this,
    CICERO contains a language model that is able to initiate dialogue and respond
    to messages from other players. Crucially, the dialogue is consistent with the
    agent’s strategic plans, which are generated by another part of the model to adapt
    to the constantly evolving scenario. This includes the ability for the agent to
    bluff when conversing with other players—that is, convince another player to co-operate
    with the agent’s plans, only to then enact an aggressive maneuver against the
    player in a later turn. Remarkably, in an anonymous online *Diplomacy* league
    featuring 40 games, CICERO’s score was more than double the average of the human
    players and it ranked in the top 10% of participants who played multiple games.
    This is an excellent example of how generative AI can be successfully blended
    with reinforcement learning.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，2022年11月，Meta发表了一篇关于CICERO的论文，这是一个训练有素的AI代理人，用于玩《外交》这个棋盘游戏。在这个游戏中，玩家代表第一次世界大战前欧洲的不同国家，必须与彼此进行谈判和欺骗，以控制整个大陆。对于AI代理人来说，这是一个非常复杂的游戏，因为其中有一个沟通元素，玩家必须与其他玩家讨论他们的计划，以获得盟友、协调行动并提出战略目标。为了实现这一点，CICERO包含一个能够发起对话并回应其他玩家消息的语言模型。至关重要的是，对话与代理人的战略计划一致，这些计划由模型的另一部分生成，以适应不断变化的情景。这包括代理人在与其他玩家交谈时虚张声势，即说服另一个玩家与代理人合作，然后在后续回合中对该玩家采取激进的行动。值得注意的是，在一个匿名的外交联盟中，涉及40场比赛，CICERO的得分超过了人类玩家的平均水平的两倍以上，并且在参与多场比赛的参与者中排名前10%。这是一个很好的例子，展示了生成式AI如何成功地与强化学习相结合。
- en: The development of embodied large language models is an exciting area of research,
    further exemplified by Google’s [*PaLM-E*](https://palm-e.github.io). This model
    combines the powerful language model PaLM with a Vision Transformer to convert
    visual and sensor data into tokens that can be interleaved with text instructions,
    allowing robots to execute tasks based on text prompts and continuous feedback
    from other sensory modalities. The PaLM-E website showcases the model’s abilities,
    including controlling a robot to arrange blocks and fetch objects based on text
    descriptions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 体现大型语言模型的发展是一个令人兴奋的研究领域，谷歌的PaLM-E模型进一步证明了这一点。该模型将强大的语言模型PaLM与Vision Transformer相结合，将视觉和传感器数据转换为可以与文本指令交错的标记，使机器人能够根据文本提示和来自其他感官模式的持续反馈执行任务。PaLM-E网站展示了该模型的能力，包括控制机器人根据文本描述排列方块和取物品。
- en: '*Text-to-video* models involve the creation of videos from text input. This
    field, which builds on the concept of text-to-image modeling, has the additional
    challenge of incorporating a time dimension. For example, in September 2022 Meta
    published [*Make-A-Video*](https://makeavideo.studio), a generative model that
    is able to create a short video given only a text prompt as input. The model is
    also able to add motion between two static images and produce variations of a
    given input video. Interestingly, it is trained only on paired text–image data
    and unsupervised video footage, rather than text–video pairs directly. The unsupervised
    video data is enough for the model to learn how the world moves; it then uses
    the text–image pairs to learn how to map between text image modalities, which
    are then animated. The [*Dreamix* model](https://oreil.ly/F9wdw) is able to perform
    video editing, where an input video is transformed based on a given text prompt
    while retaining other stylistic attributes. For example, a video of a glass of
    milk being poured could be converted to a cup of coffee being poured, while retaining
    the camera angle, background, and lighting elements of the original video.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到视频模型涉及从文本输入创建视频。这个领域建立在文本到图像建模的概念基础上，还有一个额外的挑战，即融入时间维度。例如，2022年9月，Meta发布了Make-A-Video，这是一个生成模型，可以仅通过文本提示作为输入创建一个短视频。该模型还能在两个静态图像之间添加动作，并生成给定输入视频的变体。有趣的是，它仅在配对的文本-图像数据和无监督视频素材上进行训练，而不是直接在文本-视频对上进行训练。无监督的视频数据足以让模型学习世界如何移动；然后它使用文本-图像对学习如何映射文本图像模态，然后将其动画化。Dreamix模型能够进行视频编辑，根据给定的文本提示转换输入视频，同时保留原始视频的摄像机角度、背景和照明元素。
- en: Similarly, *text-to-3D* models extend traditional text-to-image approaches into
    a third dimension. In September 2022 Google published [*DreamFusion*](https://dreamfusion3d.github.io),
    a diffusion model that generates 3D assets given an input text prompt. Crucially,
    the model does not require labeled 3D assets to train on. Instead, the authors
    use a pre-trained 2D text-to-image model (Imagen) as a prior and then train a
    3D Neural Radiance Field (NeRF), such that it is able to produce good images when
    rendered from random angles. Another example is OpenAI’s [*Point-E*](https://openai.com/research/point-e),
    published in December 2022\. Point-E is a pure diffusion-based system that is
    able to generate a 3D point cloud from a given text prompt. While the output produced
    is not as high quality as DreamFusion’s, the advantage of this approach is that
    is much faster than NeRF-based methods—it can produce output in just one to two
    minutes on a single GPU, rather than requiring multiple GPU-hours.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，*文本到3D*模型将传统的文本到图像方法扩展到第三维。2022年9月，Google发布了[*DreamFusion*](https://dreamfusion3d.github.io)，这是一个扩散模型，根据输入的文本提示生成3D资产。关键是，该模型不需要标记的3D资产进行训练。作者使用一个预先训练的2D文本到图像模型（Imagen）作为先验，然后训练一个3D神经辐射场（NeRF），使其能够在随机角度渲染时产生良好的图像。另一个例子是OpenAI的[*Point-E*](https://openai.com/research/point-e)，于2022年12月发布。Point-E是一个纯扩散系统，能够根据给定的文本提示生成一个3D点云。虽然其输出质量不如DreamFusion，但这种方法的优势在于比基于NeRF的方法快得多——它可以在单个GPU上在一到两分钟内产生输出，而不需要多个GPU小时。
- en: Given the similarities between text and music, it is not surprising that there
    have also been attempts to create *text-to-music* models. [*MusicLM*](https://oreil.ly/qb7II),
    released by Google in January 2023, is a language model that is able to convert
    a text description of a piece of music (e.g., “a calming violin melody backed
    by a distorted guitar riff”) into audio spanning several minutes that accurately
    reflects the description. It builds upon the earlier work [*AudioLM*](https://oreil.ly/0EDRY)
    by adding the ability for the model to be guided by a text prompt; examples that
    you can listen to are available on the Google Research website.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于文本和音乐之间的相似性，不足为奇的是也有人尝试创建*文本到音乐*模型。Google于2023年1月发布的[*MusicLM*](https://oreil.ly/qb7II)是一种语言模型，能够将音乐片段的文本描述（例如“一段由失真吉他伴奏的平静小提琴旋律”）转换为准确反映描述的音频，时长数分钟。它建立在早期工作[*AudioLM*](https://oreil.ly/0EDRY)的基础上，通过添加模型能够由文本提示引导的功能；您可以在Google研究网站上找到可听的示例。
- en: The Future of Generative AI
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成AI的未来
- en: In this final section, we will explore the potential impact that powerful generative
    AI systems may have on the world we live in—across our everyday lives, in the
    workplace, and within the field of education. We will also lay down the key practical
    and ethical challenges generative AI will face if it is to become a ubiquitous
    tool that makes a significant net positive contribution to society.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这最后一部分中，我们将探讨强大的生成AI系统可能对我们生活的世界产生的潜在影响——在我们的日常生活中、工作场所以及教育领域。我们还将阐明生成AI将面临的关键实际和伦理挑战，如果它要成为一个使社会获得显著净正面贡献的无处不在的工具。
- en: Generative AI in Everyday Life
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成AI在日常生活中的应用
- en: 'There is no doubt that in the future generative AI will play an increasingly
    important role in people’s everyday lives—particularly large language models.
    With OpenAI’s [ChatGPT](https://chat.openai.com/chat), it is already possible
    to generate a perfect cover letter for a job application, a professional email
    response to a colleague, or a funny social media post on a given topic using generative
    AI. This technology is truly interactive: it is able to include specific details
    that you request, respond to feedback, and ask its own questions back if something
    isn’t clear. This style of *personal assistant* AI should be the stuff of science
    fiction, but it isn’t—it’s here right now, for anyone who chooses to use it.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，未来生成AI将在人们的日常生活中扮演越来越重要的角色，特别是大型语言模型。通过OpenAI的[ChatGPT](https://chat.openai.com/chat)，已经可以使用生成AI为求职申请生成完美的求职信，为同事生成专业的电子邮件回复，或者在特定主题上生成有趣的社交媒体帖子。这项技术真正是互动的：它能够包含您请求的具体细节，回应反馈，并在某些地方不清楚时提出自己的问题。这种*个人助手*AI的风格应该是科幻小说的内容，但它并不是——它已经出现了，任何选择使用它的人都可以使用。
- en: What are the repercussions of this kind of application becoming mainstream?
    It is likely that the most immediate effect will be an increase in the quality
    of written communication. Access to large language models with a user-friendly
    interface will enable people to translate a sketch of an idea into coherent, high-quality
    paragraphs in seconds. Email writing, social media posts, and even short-form
    instant messaging will be transformed by this technology. It goes beyond removing
    the common barriers associated with spelling, grammar, and readability—it directly
    links our thought processes to usable output, often removing the need to engage
    with the process of constructing sentences at all.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这种应用成为主流的后果是什么？最直接的影响可能是书面沟通质量的提高。使用具有用户友好界面的大型语言模型将使人们能够在几秒钟内将一个想法的草图转化为连贯、高质量的段落。电子邮件写作、社交媒体帖子，甚至短格式即时通讯都将因此技术而发生变革。它不仅消除了与拼写、语法和可读性相关的常见障碍，而且直接将我们的思维过程与可用输出联系起来，通常无需参与构建句子的过程。
- en: Production of well-formed text is only one use of large language models. People
    will start using these models for idea generation, advice, and information retrieval.
    I believe we can see this as the fourth stage of our ability as a species to acquire,
    share, retrieve, and synthesize information. We started by acquiring information
    from those around us, or physically traveling to new locations to transfer knowledge.
    The invention of the printing press allowed the book to become the primary vessel
    through which ideas were shared. Finally, the birth of the internet allowed us
    to instantaneously search for and retrieve information at the touch of a button.
    Generative AI unlocks a new era of information synthesis that I believe will replace
    many of the current uses of today’s search engines.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 生成良好文本只是大型语言模型的一个用途。人们将开始使用这些模型进行创意生成、建议和信息检索。我相信我们可以将这视为作为一个物种获取、分享、检索和综合信息能力的第四阶段。我们开始通过获取周围人的信息或亲自前往新地点来获取信息。印刷术的发明使书籍成为传播思想的主要载体。最后，互联网的诞生使我们能够在触摸按钮时即时搜索和检索信息。生成AI开启了一个新的信息综合时代，我相信它将取代今天搜索引擎的许多当前用途。
- en: For example, OpenAI’s GPT suite of models can provide bespoke holiday destination
    recommendations, as shown in [Figure 14-9](#holidays), or advice on how to respond
    to a difficult situation, or a detailed explanation of an obscure concept. Using
    this technology feels more like asking a friend than typing a query into a search
    engine, and for that reason, people are flocking to it extremely quickly. ChatGPT
    is the fastest-growing tech platform ever; it acquired 1 million users within
    5 days of its launch. For context, it took Instagram 2.5 months to reach the same
    number of users and Facebook 10 months.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，OpenAI的GPT系列模型可以提供定制的假日目的地推荐，如[图14-9](#holidays)所示，或者如何应对困难情况的建议，或者对一个晦涩概念的详细解释。使用这项技术更像是向朋友询问，而不是在搜索引擎中输入查询，因此，人们迅速涌向这项技术。ChatGPT是发展最快的技术平台；在推出后的5天内获得了100万用户。为了对比，Instagram花了2.5个月才达到相同数量的用户，Facebook花了10个月。
- en: '![](Images/gdl2_1409.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1409.png)'
- en: Figure 14-9\. Output from GPT-3, giving bespoke holiday recommendations
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-9。来自GPT-3的输出，提供定制的假日推荐
- en: Generative AI in the Workplace
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作场所中的生成AI
- en: 'As well as general use, generative AI will find applications in specific jobs
    where creativity is required. A nonexhaustive list of occupations that may benefit
    follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 除了一般用途外，生成AI还将在需要创造力的特定工作中找到应用。以下是一些可能受益的职业的非尽头列表：
- en: Advertising
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 广告
- en: Generative AI can be used to create personalized ad campaigns that target specific
    demographics based on their browsing and purchase history.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 生成AI可以用来创建针对特定人群的个性化广告活动，基于他们的浏览和购买历史。
- en: Music production
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐制作
- en: Generative AI can be used to compose and produce original music tracks, allowing
    for a limitless range of possibilities.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 生成AI可以用来创作和制作原创音乐曲目，为无限的可能性提供可能。
- en: Architecture
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 建筑学
- en: Generative AI can be used to design buildings and structures, taking into account
    factors such as style and constraints around layout.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 生成AI可以用来设计建筑和结构，考虑因素如风格和布局约束。
- en: Fashion design
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 时尚设计
- en: Generative AI can be used to create unique and diverse clothing designs, taking
    into account trends and wearer preferences.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 生成AI可以用来创建独特多样的服装设计，考虑到潮流和穿着者的喜好。
- en: Automotive design
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车设计
- en: Generative AI can be used to design and develop new vehicle models and automatically
    find interesting variations on a particular design.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 生成AI可以用来设计和开发新的车型，并自动找到特定设计的有趣变化。
- en: Film and video production
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 电影和视频制作
- en: Generative AI can be used to create special effects and animations, as well
    as to generate dialogue for entire scenes or storylines.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 生成AI可以用来创建特效和动画，以及为整个场景或故事情节生成对话。
- en: Pharmaceutical research
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 制药研究
- en: Generative AI can be used to generate new drug compounds, which can aid in the
    development of new treatments.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 生成AI可以用来生成新的药物化合物，有助于开发新的治疗方法。
- en: Creative writing
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 创意写作
- en: Generative AI can be used to generate written content, such as fiction stories,
    poetry, news articles, and more.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 生成AI可以用来生成书面内容，如小说故事、诗歌、新闻文章等。
- en: Game design
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏设计
- en: Generative AI can be used to design and develop new game levels and content,
    creating an infinite variety of gameplay experiences.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 生成AI可以用来设计和开发新的游戏关卡和内容，创造无限种游戏体验。
- en: Digital design
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 数字设计
- en: Generative AI can be used to create original digital art and animations, as
    well as to design and develop new user interfaces and web designs.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 生成AI可以用来创建原创数字艺术和动画，以及设计和开发新的用户界面和网页设计。
- en: AI is often said to pose an existential threat to jobs in fields such as these,
    but I do not believe that this is actually the case. For me, AI is just another
    tool in the toolbox of these creative roles (albeit a very powerful one), rather
    than a replacement for the role itself. Those who choose to embrace this new technology
    will find that they are able to explore new ideas much faster and iterate over
    concepts in a way that previously was not possible.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 人们经常说AI对这些领域的工作构成存在威胁，但我并不认为事实就是如此。对我来说，AI只是这些创意角色工具箱中的另一个工具（尽管是一个非常强大的工具），而不是角色本身的替代品。选择拥抱这项新技术的人会发现他们能够更快地探索新想法，并以以前不可能的方式迭代概念。
- en: Generative AI in Education
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 教育中的生成AI
- en: One final area of everyday life that I believe will be significantly impacted
    is education. Generative AI challenges the fundamental axioms of education in
    a way that we haven’t seen since the dawn of the internet. The internet gave students
    the ability to retrieve information instantaneously and unambiguously, making
    exams that purely tested memorization and recall seem old-fashioned and irrelevant.
    This prompted a shift in approach, focused on testing students’ ability to synthesize
    ideas in a novel way instead of only testing factual knowledge.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信最终将受到显著影响的另一个日常生活领域是教育。生成式人工智能挑战了教育的基本公理，这是我们自互联网诞生以来从未见过的。互联网使学生能够即时和明确地检索信息，使纯粹测试记忆和回忆的考试显得过时和无关紧要。这促使了一种以测试学生能够以新颖方式综合思想为重点的方法转变，而不仅仅是测试事实知识。
- en: I believe that generative AI will cause another transformative shift in the
    field of education, necessitating a reevaluation and adjustment of current teaching
    methods and assessment criteria. If every student now has access to an essay-writing
    machine in their pocket that can generate novel responses to questions, what is
    the purpose of essay-based coursework?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信生成式人工智能将在教育领域引起另一场变革性转变，需要重新评估和调整当前的教学方法和评估标准。如果每个学生现在都可以在口袋里拥有一个可以对问题生成新颖回答的论文写作机器，那么基于论文的课程的目的是什么？
- en: Many would call for the use of such AI tools to be banned, in the same way that
    plagiarism is banned. However, it’s not that simple, as detecting AI-generated
    text is much harder than detecting plagiarism and even harder to prove beyond
    doubt. Moreover, students could use AI tools to generate a skeleton draft for
    the essay and then add extra detail or update factually incorrect information
    as required. In this case, is it the student’s original work, or the AI’s?
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人呼吁禁止使用这种人工智能工具，就像禁止剽窃一样。然而，情况并不那么简单，因为检测人工智能生成的文本比检测剽窃要困难得多，甚至更难以无疑地证明。此外，学生可以使用人工智能工具为论文生成一个骨架草稿，然后根据需要添加额外细节或更新事实不正确的信息。在这种情况下，是学生的原创作品，还是人工智能的？
- en: Clearly, these are huge questions that need to be addressed in order for education
    and certifications to maintain their integrity. In my opinion, there is no sense
    in resisting the proliferation of AI tools within education—any such approach
    is doomed to fail, as they will become so widespread in everyday life that trying
    to restrict their use will be futile. Instead, we need to find ways to embrace
    the technology and ask how we can design *open-AI* coursework, in the same way
    that we allow *open-book* coursework, and encourage students to openly research
    material using the internet and AI tools.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这些是需要解决的重大问题，以便教育和认证保持其完整性。在我看来，抵制人工智能工具在教育中的传播是毫无意义的-任何这样的方法注定会失败，因为它们将在日常生活中变得如此普遍，以至于试图限制它们的使用将是徒劳的。相反，我们需要找到方法来拥抱这项技术，并询问如何设计*开放式人工智能*课程，就像我们允许*开卷考试*课程一样，并鼓励学生使用互联网和人工智能工具公开研究材料。
- en: The potential for generative AI to assist with the learning process itself is
    also immense and deeply profound. An AI-powered tutor could help a student learn
    a new topic (as shown in [Figure 14-10](#gpt3learning)), overcome a misunderstanding,
    or generate an entirely personalized study plan. The challenge of filtering truth
    from generated fiction is no different from what we currently have with information
    available on the internet and is a life skill that needs further attention across
    the curriculum.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能在辅助学习过程本身方面的潜力也是巨大且深刻的。一个由人工智能驱动的导师可以帮助学生学习新主题（如[图14-10](#gpt3learning)所示），克服误解，或生成完全个性化的学习计划。从生成的虚构中过滤真相的挑战与我们目前在互联网上可用信息所面临的挑战并无二致，这是一个需要跨学科进一步关注的生活技能。
- en: '![](Images/gdl2_1410.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1410.png)'
- en: Figure 14-10\. Output from GPT-3—an example of how large language models can
    be used for learning
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-10。GPT-3的输出-展示了大型语言模型如何用于学习的示例
- en: Generative AI can be an incredibly powerful tool to level the playing field
    between those who have access to excellent teachers and the best learning materials
    and those who do not. I am excited to see the progress in this space, as I believe
    it could unlock massive amounts of potential across the globe.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能可以是一个非常强大的工具，可以在那些有机会接触优秀教师和最佳学习材料的人与那些没有这种机会的人之间拉平竞争场。我对这一领域的进展感到兴奋，因为我相信它可以释放全球范围内的巨大潜力。
- en: Generative AI Ethics and Challenges
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成式人工智能的伦理和挑战
- en: Despite the incredible progress that has been made in the field of generative
    AI, there remain many challenges to overcome. Some of these challenges are practical
    and others ethical.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在生成式人工智能领域取得了令人难以置信的进展，但仍然有许多挑战需要克服。其中一些挑战是实际的，另一些是伦理的。
- en: For example, a major criticism of large language models is that they are prone
    to generate misinformation when asked about a topic that is unfamiliar or contradictory,
    as shown in [Figure 14-4](#languagemodelproblems). The danger with this is that
    it is difficult to know if the information that is contained within a generated
    response is truly accurate. Even if you ask the LLM to explain its reasoning or
    cite sources, it might make up references or spout a series of statements that
    do not logically follow on from one another. This is not an easy problem to solve,
    as the LLM is nothing more than a set of weights that accurately capture the most
    likely next word given a set of input tokens—it does not have a bank of *true*
    information that it can use as a reference.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，大型语言模型的一个主要批评是，当询问一个陌生或矛盾的主题时，它们很容易生成错误信息，如[图14-4](#languagemodelproblems)所示。这种危险在于很难知道生成的回应中包含的信息是否真实准确。即使您要求LLM解释其推理或引用来源，它可能会编造参考文献或说出一系列逻辑上不相连的陈述。这不是一个容易解决的问题，因为LLM只是一组权重，准确捕捉给定一组输入标记时最可能的下一个词-它没有可以用作参考的*真实*信息库。
- en: A potential solution to this problem is to provide large language models with
    the ability to call upon structured tools such as calculators, code compilers,
    and online information sources for tasks that require precise execution or facts.
    For example, [Figure 14-11](#toolformer_ex) shows output from a model called *Toolformer*,
    published by Meta in February 2023.^([4](ch14.xhtml#idm45387000734816))
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一个潜在方案是为大型语言模型提供调用结构化工具的能力，如计算器、代码编译器和在线信息源，用于需要精确执行或事实的任务。例如，[图14-11](#toolformer_ex)展示了Meta于2023年2月发布的名为*Toolformer*的模型的输出。^([4](ch14.xhtml#idm45387000734816))
- en: '![](Images/gdl2_1411.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1411.png)'
- en: 'Figure 14-11\. An example of how Toolformer is able to autonomously call different
    APIs in order to obtain precise information where necessary (source: [Schick et
    al., 2023](https://arxiv.org/pdf/2302.04761.pdf))'
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-11\. Toolformer能够自主调用不同的API以在必要时获取精确信息的示例（来源：[Schick等人，2023](https://arxiv.org/pdf/2302.04761.pdf)）
- en: Toolformer is able to explicitly call APIs for information, as part of its generative
    response. For example, it might use the Wikipedia API to retrieve information
    about a particular person, rather than relying on this information being embedded
    in its model weights. This approach is particularly useful for precise mathematical
    operations, where Toolformer can state which operations it would like to enter
    into the calculator API instead of trying to generate the answer autoregressively
    in the useful fashion.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Toolformer能够明确调用API以获取信息，作为其生成式响应的一部分。例如，它可能使用维基百科API来检索有关特定人物的信息，而不是依赖于这些信息被嵌入在其模型权重中。这种方法特别适用于精确的数学运算，其中Toolformer可以说明它想要输入计算器API的哪些操作，而不是试图以有用的方式自动生成答案。
- en: Another prominent ethical concern with generative AI centers on the fact that
    large companies have used huge amounts of data scraped from the web to train their
    models, when consent was not explicitly given by the original creators to do so.
    Often this data is not even publicly released, so it is impossible to know if
    your data is being used to train large language models or multimodal text-to-image
    models. Clearly this is a valid concern, particularly for artists, who may argue
    that it is usage of their artwork for which they are not being paid any royalties
    or commission. Moreover, an artist’s name may be used as a prompt in order to
    generate more artwork that is similar in style to the originals, thereby degrading
    the uniqueness of the content and commoditizing the style.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI的另一个突出的伦理关注点在于，大公司使用从网络上抓取的大量数据来训练他们的模型，而原始创作者并没有明确同意这样做。通常这些数据甚至没有公开发布，因此无法知道您的数据是否被用来训练大型语言模型或多模态文本到图像模型。显然，这是一个合理的担忧，特别是对于艺术家来说，他们可能会认为这是对他们的艺术作品的使用，而他们并没有得到任何版税或佣金。此外，艺术家的名字可能被用作提示，以生成更多风格类似于原作的艺术作品，从而降低内容的独特性并将风格商品化。
- en: A solution to this problem is being pioneered by Stability AI, whose multimodal
    model Stable Diffusion is trained on a subset of the open source LAION-5B dataset.
    They have also launched the website [*Have I Been Trained?*](https://haveibeentrained.com)
    where anyone can search for a particular image or text passage within the training
    dataset and opt out of future inclusion in the model training process. This puts
    control back in the hands of the original creators and ensures that there is transparency
    in the data that is being used to create powerful tools like this one. However,
    this practice is not commonplace, and many commercially available generative AI
    models do not make their datasets or model weights open source or provide any
    option to opt out of the training process.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的一个解决方案是由Stability AI开创的，他们的多模态模型Stable Diffusion是在开源LAION-5B数据集的一个子集上进行训练的。他们还推出了网站[*Have
    I Been Trained?*](https://haveibeentrained.com)，任何人都可以在训练数据集中搜索特定的图像或文本段落，并选择退出未来的模型训练过程。这将控制权交还给原始创作者，并确保用于创建强大工具如此的数据具有透明度。然而，这种做法并不普遍，许多商业可用的生成式AI模型并不公开其数据集或模型权重，也不提供任何选择退出训练过程的选项。
- en: In conclusion, while generative AI is a powerful tool for communication, productivity,
    and learning across everyday life, in the workplace, and in the field of education,
    there are both advantages and disadvantages to its widespread use. It is important
    to be aware of the potential risks of using the output from a generative AI model
    and to always be sure to use it responsibly. Nevertheless, I remain optimistic
    about the future of generative AI and am eager to see how businesses and people
    adapt to this new and exciting technology.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，虽然生成式AI是一个强大的工具，可用于日常生活、工作场所和教育领域的沟通、生产力和学习，但其广泛使用既有优势也有劣势。重要的是要意识到使用生成式AI模型的输出的潜在风险，并始终确保负责任地使用它。尽管如此，我对生成式AI的未来充满乐观，并迫不及待地想看到企业和人们如何适应这项新的令人兴奋的技术。
- en: Final Thoughts
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后思考
- en: In this book we have taken a journey through the last decade of generative modeling
    research, starting out with the basic ideas behind VAEs, GANs, autoregressive
    models, normalizing flow models, energy-based models, and diffusion models and
    building upon these foundations to understand how state-of-the-art techniques
    such as VQ-GAN, Transformers, world models, and multimodal models are now pushing
    the boundaries of what generative models are capable of achieving, across a variety
    of tasks.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们通过过去十年的生成建模研究之旅，从VAEs、GANs、自回归模型、正规化流模型、基于能量的模型和扩散模型的基本思想开始，建立在这些基础上，了解VQ-GAN、Transformers、世界模型和多模态模型等最新技术如何推动生成模型在各种任务中所能实现的边界。
- en: 'I believe that in the future, generative modeling may be the key to a deeper
    form of artificial intelligence that transcends any one particular task and allows
    machines to organically formulate their own rewards, strategies, and perhaps awareness
    within their environment. My beliefs are closely aligned to the principle of *active
    inference*, originally pioneered by Karl Friston. The theory behind active inference
    could easily fill another entire book—and does, in Thomas Parr et al.’s excellent
    *Active Inference: The Free Energy Principle in Mind, Brain, and Behavior* (MIT
    Press), which I highly recommend—so I will only attempt a short explanation here.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信，在未来，生成建模可能是一种更深层次的人工智能的关键，超越任何特定任务，使机器能够有机地制定自己的奖励、策略，甚至在环境中产生意识。我的信念与Karl
    Friston最初开创的“主动推理”原则密切相关。主动推理背后的理论可以轻松填满另一本完整的书籍——并且确实填满了，就像Thomas Parr等人在《主动推理：心智、大脑和行为中的自由能量原则》（麻省理工学院出版社）中所做的那样，我强烈推荐——所以我只会在这里尝试简短解释。
- en: As babies, we are constantly exploring our surroundings, building up a mental
    model of possible futures with no apparent aim other than to develop a deeper
    understanding of the world. There are no labels on the data that we receive—a
    seemingly random stream of light and sound waves that bombard our senses from
    the moment we are born. Even when our someone points to an apple and says *apple*,
    there is no reason for our young brains to associate the two inputs and learn
    that the way in which light entered our eye at that particular moment is in some
    way related to the way the sound waves entered our ear. There is no training set
    of sounds and images, no training set of smells and tastes, and no training set
    of actions and rewards; there’s just an endless stream of extremely noisy data.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 作为婴儿，我们不断地探索周围环境，建立起可能未来的心智模型，看似没有明显目的，只是为了更深入地理解世界。我们接收到的数据没有标签——从出生那一刻起就不断轰击我们感官的光和声波似乎是随机的。即使有人指着一个苹果说“苹果”，我们年幼的大脑也没有理由将这两个输入联系起来，学习到光线进入眼睛的方式与声波进入耳朵的方式有某种关联。没有声音和图像的训练集，没有气味和味道的训练集，也没有行为和奖励的训练集；只有一个无休止的极其嘈杂的数据流。
- en: And yet here you are now, reading this sentence, perhaps enjoying the taste
    of a cup of coffee in a noisy cafe. You pay no attention to the background noise
    as you concentrate on converting the absence of light on a tiny portion of your
    retina into a sequence of abstract concepts that convey almost no meaning individually
    but, when combined, trigger a wave of parallel representations in your mind’s
    eye—images, emotions, ideas, beliefs, and potential actions all flood your consciousness,
    awaiting your recognition. The same noisy stream of data that was essentially
    meaningless to your infant brain is not so noisy anymore. Everything makes sense
    to you. You see structure everywhere. You are never surprised by the physics of
    everyday life. The world is the way that it is because your brain decided it should
    be that way. In this sense, your brain is an extremely sophisticated generative
    model, equipped with the ability to attend to particular parts of the input data,
    form representations of concepts within a latent space of neural pathways, and
    process sequential data over time.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，此刻你正在阅读这句话，也许正在享受嘈杂咖啡馆里一杯咖啡的味道。你专注于将视网膜上的微小部分的光缺失转化为一系列抽象概念，这些概念单独来看几乎没有意义，但结合起来，会在你的脑海中引发一波平行的表征——图像、情感、想法、信念和潜在行动都涌入你的意识，等待你的认知。对于你的婴儿大脑来说基本无意义的同样嘈杂的数据流现在不再那么嘈杂。一切对你来说都是有意义的。你在任何地方都看到结构。你对日常生活的物理现象从不感到惊讶。世界是因为你的大脑决定它应该是这样。在这个意义上，你的大脑是一个极其复杂的生成模型，具有关注输入数据特定部分、在神经通路的潜在空间内形成概念表征、并随时间处理序列数据的能力。
- en: Active inference is a framework that builds upon this idea to explain how the
    brain processes and integrates sensory information to make decisions and actions.
    It states that an organism has a generative model of the world it inhabits, and
    uses this model to make predictions about future events. In order to reduce the
    surprise caused by discrepancies between the model and reality, the organism adjusts
    its actions and beliefs accordingly. Friston’s key idea is that action and perception
    optimization can be framed as two sides of the same coin, with both seeking to
    minimize a single quantity known as *free energy*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 主动推理是一个基于这一思想的框架，用来解释大脑如何处理和整合感官信息以做出决策和行动。它指出，一个生物体对其所处世界有一个生成模型，并利用这个模型对未来事件进行预测。为了减少模型与现实之间的差异所带来的惊讶，生物体相应地调整其行动和信念。Friston的关键思想是，行动和感知优化可以被看作是同一个硬币的两面，两者都旨在最小化一个称为“自由能量”的量。
- en: At the heart of this framework is a generative model of the environment (captured
    within the brain) that is constantly being compared to reality. Crucially, the
    brain is not a passive observer of events. In humans, it is attached to a neck
    and a set of legs that can put its core input sensors in a myriad of positions
    relative to the source of the input data. Therefore, the generated sequence of
    possible futures is not only dependent on its understanding of the physics of
    the environment, but also on its understanding of *itself* and how it acts. This
    feedback loop of action and perception is extremely interesting to me, and I believe
    we have only scratched the surface of what is possible with embodied generative
    models that are able to take actions within a given environment according to the
    principles of active inference.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这个框架的核心是一个环境的生成模型（在大脑中捕获），它不断地与现实进行比较。关键是，大脑不是事件的被动观察者。在人类中，它连接着一条脖子和一套腿，可以将其核心输入传感器相对于输入数据源放置在多种位置。因此，可能未来的生成序列不仅取决于其对环境物理的理解，还取决于其对*自身*及其行为方式的理解。行动和感知的这种反馈循环对我来说非常有趣，我相信我们只是触及了具有行动推理原则的具体环境中能够采取行动的具体生成模型的潜力表面。
- en: This is the core idea that I believe will continue to propel generative modeling
    into the spotlight in the next decade, as one of the keys to unlocking artificial
    general intelligence.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我认为将在未来十年继续推动生成建模走向聚光灯下的核心理念之一，作为解锁人工通用智能的关键之一。
- en: With that in mind, I encourage you to continue learning more about generative
    models from all the great material that is available online and in other books.
    Thank you for taking the time to read to the end of this book—I hope you have
    enjoyed reading it as much as I have enjoyed generating it!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个基础上，我鼓励您继续从在线和其他书籍中提供的优质材料中学习更多关于生成模型的知识。感谢您抽出时间阅读本书至此，希望您和我一样享受阅读的乐趣！
- en: '^([1](ch14.xhtml#idm45387000972896-marker)) Hugo Touvron et al., “LLaMA: Open
    and Efficient Foundation Language Models,” February 27, 2023, [*https://arxiv.org/abs/2302.13971*](https://arxiv.org/abs/2302.13971).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch14.xhtml#idm45387000972896-marker)) Hugo Touvron等人，“LLaMA: 开放高效的基础语言模型”，2023年2月27日，[*https://arxiv.org/abs/2302.13971*](https://arxiv.org/abs/2302.13971)。'
- en: ^([2](ch14.xhtml#idm45387000894496-marker)) Mark Chen et al., “Evaluating Large
    Language Models Trained on Code,” July 7, 2021, [*https://arxiv.org/abs/2107.03374*](https://arxiv.org/abs/2107.03374).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch14.xhtml#idm45387000894496-marker)) Mark Chen等人，“评估在代码上训练的大型语言模型”，2021年7月7日，[*https://arxiv.org/abs/2107.03374*](https://arxiv.org/abs/2107.03374)。
- en: ^([3](ch14.xhtml#idm45387000869232-marker)) Lvmin Zhang and Maneesh Agrawala,
    “Adding Conditional Control to Text-to-Image Diffusion Models,” February 10, 2023,
    [*https://arxiv.org/abs/2302.05543*](https://arxiv.org/abs/2302.05543).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch14.xhtml#idm45387000869232-marker)) 张旅民和Maneesh Agrawala，“向文本到图像扩散模型添加条件控制”，2023年2月10日，[*https://arxiv.org/abs/2302.05543*](https://arxiv.org/abs/2302.05543)。
- en: '^([4](ch14.xhtml#idm45387000734816-marker)) Timo Schick et al., “Toolformer:
    Language Models Can Teach Themselves to Use Tools,” February 9, 2023, [*https://arxiv.org/abs/2302.04761*](https://arxiv.org/abs/2302.04761).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch14.xhtml#idm45387000734816-marker)) Timo Schick等人，“Toolformer: 语言模型可以自学使用工具”，2023年2月9日，[*https://arxiv.org/abs/2302.04761*](https://arxiv.org/abs/2302.04761)。'
