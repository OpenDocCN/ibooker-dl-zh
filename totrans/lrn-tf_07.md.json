["```py\ndef weight_variable(shape):\n initial = tf.truncated_normal(shape, stddev=0.1)\n return tf.Variable(initial)\n\ndef bias_variable(shape):\n initial = tf.constant(0.1, shape=shape)\n return tf.Variable(initial)\n\ndef conv2d(x, W):\n return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1],\n           padding='SAME')\n\ndef conv_layer(input, shape):\n W = weight_variable(shape)\n b = bias_variable([shape[3]])\n h = tf.nn.relu(conv2d(input, W) + b)\n hp = max_pool_2x2(h)\n return hp\n\ndef max_pool_2x2(x):\n return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n            strides=[1, 2, 2, 1], padding='SAME')\n\nx = tf.placeholder(tf.float32, shape=[None, 784])\nx_image = tf.reshape(x, [-1, 28, 28, 1])\n\nh1 = conv_layer(x_image, shape=[5, 5, 1, 32])\nh2 = conv_layer(h1, shape=[5, 5, 32, 64])\nh3 = conv_layer(h2, shape=[5, 5, 64, 32])\n\n```", "```py\nx = tf.placeholder(tf.float32, shape=[None, 784])\nx_image = tf.reshape(x, [-1, 28, 28, 1])\n\nW1 = tf.truncated_normal([5, 5, 1, 32], stddev=0.1)\nb1 = tf.constant(0.1, shape=[32])\nh1 = tf.nn.relu(tf.nn.conv2d(x_image, W1, \n                strides=[1, 1, 1, 1], padding='SAME') + b1)\nhp1 = tf.nn.max_pool(h1, ksize=[1, 2, 2, 1], \n                     strides=[1, 2, 2, 1], padding='SAME')\nW2 = tf.truncated_normal([5, 5, 32, 64], stddev=0.1)\nb2 = tf.constant(0.1, shape=[64])\nh2 = tf.nn.relu(tf.nn.conv2d(hp1, W2, \n                strides=[1, 1, 1, 1], padding='SAME') + b2)\nhp2 = tf.nn.max_pool(h2, ksize=[1, 2, 2, 1], \n                     strides=[1, 2, 2, 1], padding='SAME')\nW3 = tf.truncated_normal([5, 5, 64, 32], stddev=0.1)\nb3 = tf.constant(0.1, shape=[32])\nh3 = h1 = tf.nn.relu(tf.nn.conv2d(hp2, W3, \n                     strides=[1, 1, 1, 1], padding='SAME') + b3)\nhp3 = tf.nn.max_pool(h3, ksize=[1, 2, 2, 1], \n                     strides=[1, 2, 2, 1], padding='SAME')\n```", "```py\nregressor = learn.LinearRegressor(feature_columns=feature_columns,\n                                 optimizer=optimizer)\nregressor.fit(X, Y, steps=200, batch_size=506)\n\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.contrib import learn\n\n```", "```py\n    model=learn.*`<``some_Estimator``>`*()\n    ```", "```py\n    model.fit()\n\n    ```", "```py\n    model.evaluate()\n\n    ```", "```py\n    model.predict()\n\n    ```", "```py\nfrom sklearn import datasets, metrics, preprocessing\nboston = datasets.load_boston()\nx_data = preprocessing.StandardScaler().fit_transform(boston.data)\ny_data = boston.target\n\n```", "```py\nx = tf.placeholder(tf.float64,shape=(None,13))\ny_true = tf.placeholder(tf.float64,shape=(None))\n\nwith tf.name_scope('inference') as scope:\n  w = tf.Variable(tf.zeros([1,13],dtype=tf.float64,name='weights'))\n  b = tf.Variable(0,dtype=tf.float64,name='bias')\n  y_pred = tf.matmul(w,tf.transpose(x)) + b\n\nwith tf.name_scope('loss') as scope:\n  loss = tf.reduce_mean(tf.square(y_true-y_pred))\n\nwith tf.name_scope('train') as scope:\n  learning_rate = 0.1\n  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  train = optimizer.minimize(loss)\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n  sess.run(init)   \n  for step in range(200):\n    sess.run(train,{x: x_data, y_true: y_data})\n\n  MSE = sess.run(loss,{x: x_data, y_true: y_data})\nprint(MSE)\n\nOut: \nMSE = 21.9036388397\n```", "```py\n    reg = learn.LinearRegressor(\n         feature_columns=feature_columns,\n         optimizer=tf.train.GradientDescentOptimizer(\n          learning_rate=0.1)\n         )\n\n    ```", "```py\n    reg.fit(x_data, boston.target, steps=NUM_STEPS, \n           batch_size=MINIBATCH_SIZE)\n    ```", "```py\n    MSE = regressor.evaluate(x_data, boston.target, steps=1)\n\n    ```", "```py\nNUM_STEPS = 200\nMINIBATCH_SIZE = 506\n\nfeature_columns = learn.infer_real_valued_columns_from_input(x_data)\n\nreg = learn.LinearRegressor(\n     feature_columns=feature_columns,\n     optimizer=tf.train.GradientDescentOptimizer(\n      learning_rate=0.1)\n     )\n\nreg.fit(x_data, boston.target, steps=NUM_STEPS, \n       batch_size=MINIBATCH_SIZE)\n\nMSE = reg.evaluate(x_data, boston.target, steps=1)\n\nprint(MSE)\n\nOut: \n{'loss': 21.902138, 'global_step': 200}\n```", "```py\nimport sys\nimport numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\nDATA_DIR = '/tmp/data' if not 'win32' in sys.platform else \"c:\\\\tmp\\\\data\"\ndata = input_data.read_data_sets(DATA_DIR, one_hot=False)\nx_data, y_data = data.train.images,data.train.labels.astype(np.int32)\nx_test, y_test = data.test.images,data.test.labels.astype(np.int32)\n\n```", "```py\none_hot=False\n\n```", "```py\nNUM_STEPS = 2000\nMINIBATCH_SIZE = 128\n\nfeature_columns = learn.infer_real_valued_columns_from_input(x_data)\n\ndnn = learn.DNNClassifier(\n  feature_columns=feature_columns,\n  hidden_units=[200],\n  n_classes=10,\n  optimizer=tf.train.ProximalAdagradOptimizer(\n  learning_rate=0.2)\n  )\n\ndnn.fit(x=x_data,y=y_data, steps=NUM_STEPS,\n       batch_size=MINIBATCH_SIZE)\n\ntest_acc = dnn.evaluate(x=x_test,y=y_test, steps=1)[\"accuracy\"]\nprint('test accuracy: {}'.format(test_acc))\n\nOut:\ntest accuracy: 0.977\n\n```", "```py\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = dnn.predict(x=x_test,as_iterable=False)\nclass_names = ['0','1','2','3','4','5','6','7','8','9']  \ncnf_matrix = confusion_matrix(y_test, y_pred)\n\n```", "```py\nimport pandas as pd\nN = 10000\n\nweight = np.random.randn(N)*5+70\nspec_id = np.random.randint(0,3,N)\nbias = [0.9,1,1.1]\nheight = np.array([weight[i]/100 + bias[b] for i,b in enumerate(spec_id)])\nspec_name = ['Goblin','Human','ManBears']\nspec = [spec_name[s] for s in spec_id]\n\n```", "```py\ndf = pd.DataFrame({'Species':spec,'Weight':weight,'Height':height})\n\n```", "```py\nfrom tensorflow.contrib import layers\nWeight = layers.real_valued_column(\"Weight\")\n\n```", "```py\nSpecies = layers.sparse_column_with_keys(\n  column_name=\"Species\", keys=['Goblin','Human','ManBears'])\n```", "```py\nreg = learn.LinearRegressor(feature_columns=[Weight,Species])\n\n```", "```py\ndef input_fn(df):\n  feature_cols = {}\n  feature_cols['Weight'] = tf.constant(df['Weight'].values)\n\n  feature_cols['Species'] = tf.SparseTensor(\n  indices=[[i, 0] for i in range(df['Species'].size)],\n  values=df['Species'].values,\n  dense_shape=[df['Species'].size, 1])\n\n  labels = tf.constant(df['Height'].values)\n\n  return feature_cols, labels\n\n```", "```py\nSparseTensor(indices=[[0, 0], [2, 1], [2, 2]], values=[2, 5, 7],\n            dense_shape=[3, 3])\n```", "```py\n[[2, 0, 0]\n [0, 0, 0]\n[0, 5, 7]]\n\n```", "```py\nreg.fit(input_fn=lambda:input_fn(df), steps=50000)\n\n```", "```py\nreg.fit(input_fn=lambda:input_fn(df), steps=10000)\nreg.fit(input_fn=lambda:input_fn(df), steps=10000)\nreg.fit(input_fn=lambda:input_fn(df), steps=10000)\nreg.fit(input_fn=lambda:input_fn(df), steps=10000)\nreg.fit(input_fn=lambda:input_fn(df), steps=10000)\n\n```", "```py\nw_w = reg.get_variable_value('linear/Weight/weight')\nprint('Estimation for Weight: {}'.format(w_w))\n\ns_w = reg.get_variable_value('linear/Species/weights')\nb = reg.get_variable_value('linear/bias_weight')\nprint('Estimation for Species: {}'.format(s_w + b))\n\nOut: \n       Estimation for Weight:  [[0.00992305]]\n        Estimation for Species: [[0.90493023]\n                                 [1.00566959]\n                                 [1.10534406]]\n\n```", "```py\nx_image = tf.reshape(x_data, [-1, 28, 28, 1])\n\n```", "```py\nconv1 = layers.convolution2d(x_image, 32, [5,5],\n           activation_fn=tf.nn.relu,\n            biases_initializer=tf.constant_initializer(0.1),\n           weights_initializer=tf.truncated_normal_initializer(stddev=0.1))\n\n```", "```py\npool1 = layers.max_pool2d(conv1, [2,2])\u200b\n\n```", "```py\nconv2 = layers.convolution2d(pool1, 64, [5,5],\n            activation_fn=tf.nn.relu,\n             biases_initializer=tf.constant_initializer(0.1),\n             weights_initializer=tf.truncated_normal_initializer(stddev=0.1)) \n\npool2 = layers.max_pool2d(conv2, [2,2])\n\n```", "```py\npool2_flat = tf.reshape(pool2, [-1, 7*7*64]) \nfc1 = layers.fully_connected(pool2_flat, 1024,\n         activation_fn=tf.nn.relu,\n          biases_initializer=tf.constant_initializer(0.1),\n          weights_initializer=tf.truncated_normal_initializer(stddev=0.1))\n\n```", "```py\nfc1_drop = layers.dropout(fc1, keep_prob=params[\"dropout\"], \n                           is_training=(mode == 'train'))\ny_conv = layers.fully_connected(fc1_drop, 10, activation_fn=None)\n\n```", "```py\ndef model_fn(x, target, mode, params):\n  y_ = tf.cast(target, tf.float32)\n  x_image = tf.reshape(x, [-1, 28, 28, 1])\n\n  # Conv layer 1\n  conv1 = layers.convolution2d(x_image, 32, [5,5],\n               activation_fn=tf.nn.relu,\n        biases_initializer=tf.constant_initializer(0.1),\n        weights_initializer=tf.truncated_normal_initializer(stddev=0.1))\n  pool1 = layers.max_pool2d(conv1, [2,2])\n\n  # Conv layer 2\n  conv2 = layers.convolution2d(pool1, 64, [5,5],\n               activation_fn=tf.nn.relu,\n        biases_initializer=tf.constant_initializer(0.1),\n        weights_initializer=tf.truncated_normal_initializer(stddev=0.1))\n  pool2 = layers.max_pool2d(conv2, [2,2])\n\n  # FC layer\n  pool2_flat = tf.reshape(pool2, [-1, 7*7*64])\n  fc1 = layers.fully_connected(pool2_flat, 1024,\n             activation_fn=tf.nn.relu,\n       biases_initializer=tf.constant_initializer(0.1),\n       weights_initializer=tf.truncated_normal_initializer(stddev=0.1))\n  fc1_drop = layers.dropout(fc1, keep_prob=params[\"dropout\"],\n    is_training=(mode == 'train'))\n\n  # Readout layer\n  y_conv = layers.fully_connected(fc1_drop, 10, activation_fn=None)\n\n  cross_entropy = tf.reduce_mean(\n    tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_))\n  train_op = tf.contrib.layers.optimize_loss(\n    loss=cross_entropy,\n    global_step=tf.contrib.framework.get_global_step(),\n    learning_rate=params[\"learning_rate\"],\n    optimizer=\"Adam\")\n\n    predictions = tf.argmax(y_conv, 1)\n  return predictions, cross_entropy, train_op\n\n```", "```py\nfrom tensorflow.contrib import layers\n\ndata = input_data.read_data_sets(DATA_DIR, one_hot=True)\nx_data, y_data = data.train.images,np.int32(data.train.labels)\ntf.cast(x_data,tf.float32)\ntf.cast(y_data,tf.float32)\n\nmodel_params = {\"learning_rate\": 1e-4, \"dropout\": 0.5}\n\nCNN = tf.contrib.learn.Estimator(\n  model_fn=model_fn, params=model_params)\n\nprint(\"Starting training for %s steps max\" % 5000)\nCNN.fit(x=data.train.images,\n    y=data.train.labels, batch_size=50,\n    max_steps=5000)\n\ntest_acc = 0\nfor ii in range(5):\n  batch = data.test.next_batch(2000)\n  predictions = list(CNN.predict(batch[0], as_iterable=True))\n  test_acc = test_acc + (np.argmax(batch[1],1) == predictions).mean()\n\nprint(test_acc/5)\n\nOut: \n0.9872\n\n```", "```py\npip install tflearn\n\n```", "```py\nimport tflearn\n\n```", "```py\nfrom tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.conv import conv_2d, max_pool_2d\nfrom tflearn.layers.normalization import local_response_normalization\nfrom tflearn.layers.estimator import regression\n\n# Data loading and basic transformations\nimport tflearn.datasets.mnist as mnist\nX, Y, X_test, Y_test = mnist.load_data(one_hot=True)\nX = X.reshape([-1, 28, 28, 1])\nX_test = X_test.reshape([-1, 28, 28, 1])\n\n# Building the network\nCNN = input_data(shape=[None, 28, 28, 1], name='input')\nCNN = conv_2d(CNN, 32, 5, activation='relu', regularizer=\"L2\")\nCNN = max_pool_2d(CNN, 2)\nCNN = local_response_normalization(CNN)\nCNN = conv_2d(CNN, 64, 5, activation='relu', regularizer=\"L2\")\nCNN = max_pool_2d(CNN, 2)\nCNN = local_response_normalization(CNN)\nCNN = fully_connected(CNN, 1024, activation=None)\nCNN = dropout(CNN, 0.5)\nCNN = fully_connected(CNN, 10, activation='softmax')\nCNN = regression(CNN, optimizer='adam', learning_rate=0.0001,\n          loss='categorical_crossentropy', name='target')\n\n# Training the network\nmodel = tflearn.DNN(CNN,tensorboard_verbose=0,\n                    tensorboard_dir = 'MNIST_tflearn_board/',\n          checkpoint_path = 'MNIST_tflearn_checkpoints/checkpoint')\nmodel.fit({'input': X}, {'target': Y}, n_epoch=3,\n     validation_set=({'input': X_test}, {'target': Y_test}),\n     snapshot_step=1000,show_metric=True, run_id='convnet_mnist')\n\n```", "```py\nevaluation = model.evaluate({'input': X_test},{'target': Y_test})\nprint(evaluation):\n\nOut: \n0.9862\n\n```", "```py\npred = model.predict({'input': X_test})\nprint((np.argmax(Y_test,1)==np.argmax(pred,1)).mean())\n\nOut: \n0.9862\n```", "```py\nfrom tflearn.data_utils import to_categorical, pad_sequences\nfrom tflearn.datasets import imdb\n\n# IMDb dataset loading\ntrain, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000,\n                valid_portion=0.1)\nX_train, Y_train = train\nX_test, Y_test = test\n\n```", "```py\nX_train = pad_sequences(X_train, maxlen=100, value=0.)\nX_test = pad_sequences(X_test, maxlen=100, value=0.)\n\n```", "```py\nRNN = tflearn.input_data([None, 100])\nRNN = tflearn.embedding(RNN, input_dim=10000, output_dim=128)\n\n```", "```py\nRNN = tflearn.lstm(RNN, 128, dropout=0.8)\nRNN = tflearn.fully_connected(RNN, 2, activation='softmax'\n```", "```py\nfrom tflearn.data_utils import to_categorical, pad_sequences\nfrom tflearn.datasets import imdb\n\n# Load data\ntrain, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000,\n                valid_portion=0.1)\nX_train, Y_train = train\nX_test, Y_test = test\n\n# Sequence padding and converting labels to binary vectors\nX_train = pad_sequences(X_train, maxlen=100, value=0.)\nX_test = pad_sequences(X_test, maxlen=100, value=0.)\nY_train = to_categorical(Y_train, nb_classes=2)\nY_test = to_categorical(Y_test, nb_classes=2)\n\n# Building an LSTM network\nRNN = tflearn.input_data([None, 100])\nRNN = tflearn.embedding(RNN, input_dim=10000, output_dim=128)\n\nRNN = tflearn.lstm(RNN, 128, dropout=0.8)\nRNN = tflearn.fully_connected(RNN, 2, activation='softmax')\nRNN = tflearn.regression(RNN, optimizer='adam', learning_rate=0.001,\n            loss='categorical_crossentropy')\n\n# Training the network\nmodel = tflearn.DNN(RNN, tensorboard_verbose=0)\nmodel.fit(X_train, Y_train, validation_set=(X_test, Y_test),\n                                show_metric=True, batch_size=32)\n\n```", "```py\npip install keras\n```", "```py\npython setup.py install\n\n```", "```py\n{\n\u00a0\u00a0\u00a0\u00a0\"image_data_format\": \"channels_last\",\n\u00a0\u00a0\u00a0\u00a0\"epsilon\": 1e-07,\n\u00a0\u00a0\u00a0\u00a0\"floatx\": \"float32\",\n\u00a0\u00a0\u00a0\u00a0\"backend\": \"tensorflow\"\n}\n```", "```py\nfrom keras import backend as K\n```", "```py\ninput = K.placeholder(shape=(10,32))\n```", "```py\ntf.placeholder(shape=(10,32))\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nmodel = Sequential()\n\nmodel.add(Dense(units=64, input_dim=784))\nmodel.add(Activation('softmax'))\n\n```", "```py\nmodel = Sequential([\nDense(64, input_shape=(784,),activation='softmax')\n])\n\n```", "```py\nmodel.compile(loss='categorical_crossentropy',\noptimizer='sgd',\nmetrics=['accuracy'])\n\n```", "```py\noptimizer=keras.optimizers.SGD(lr=0.02, momentum=0.8, nesterov=True))\n\n```", "```py\nfrom keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0,\n                          patience=10, verbose=0, mode='auto')\n\nmodel.fit(x_train, y_train, epochs=10, batch_size=64,\n         callbacks=[TensorBoard(log_dir='/models/autoencoder',)\n         early_stop])\n\nloss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\nclasses = model.predict(x_test, batch_size=64)\n\n```", "```py\ninputs = Input(shape=(784,))\n\n```", "```py\nx = Dense(64, activation='relu')(inputs)\nx = Dense(32, activation='relu')(x)\noutputs = Dense(10, activation='softmax')(x)\n\n```", "```py\nmodel = Model(inputs=inputs, outputs=outputs)\n\n```", "```py\nmodel.compile(optimizer='rmsprop',\nloss='categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=64)\nloss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\nclasses = model.predict(x_test, batch_size=64)\n\n```", "```py\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\nfrom keras.models import Model\nfrom keras.callbacks import TensorBoard, ModelCheckpoint\nfrom keras.datasets import cifar10\nimport numpy as np\n\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nx_train = x_train[np.where(y_train==1)[0],:,:,:]\nx_test = x_test[np.where(y_test==1)[0],:,:,:]\n\n```", "```py\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\n\n```", "```py\nx_train_n = x_train + 0.5 *\\\n np.random.normal(loc=0.0, scale=0.4, size=x_train.shape)\n\nx_test_n = x_test + 0.5 *\\\n np.random.normal(loc=0.0, scale=0.4, size=x_test.shape)\n\nx_train_n = np.clip(x_train_n, 0., 1.)\nx_test_n = np.clip(x_test_n, 0., 1.)\n\n```", "```py\ninp_img = Input(shape=(32, 32, 3))  \n\n```", "```py\nimg = Conv2D(32, (3, 3), activation='relu', padding='same')(inp_img)\nimg = MaxPooling2D((2, 2), padding='same')(img)\nimg = Conv2D(32, (3, 3), activation='relu', padding='same')(img)\nimg = UpSampling2D((2, 2))(img)\ndecoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(img)\n\n```", "```py\nautoencoder = Model(inp_img, decoded)\n\n```", "```py\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n\n```", "```py\ntensorboard=TensorBoard(log_dir='<*`some_path`*>',histogram_freq=0,write_graph=True,write_images=True)model_saver=ModelCheckpoint(filepath='<*`some_path`*>',verbose=0,period=2)autoencoder.fit(x_train_n,x_train,epochs=10,batch_size=64,shuffle=True,validation_data=(x_test_n,x_test),callbacks=[tensorboard,model_saver])\n```", "```py\ninp_img=Input(shape=(32,32,3))img=Conv2D(32,(3,3),activation='relu',padding='same')(inp_img)img=MaxPooling2D((2,2),padding='same')(img)img=Conv2D(32,(3,3),activation='relu',padding='same')(img)img=UpSampling2D((2,2))(img)decoded=Conv2D(3,(3,3),activation='sigmoid',padding='same')(img)autoencoder=Model(inp_img,decoded)Model.load_weights(autoencoder,'*`some_path`*')\n```", "```py\npip install h5py\n\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nW = slim.variable('w',shape=[7, 7, 3 , 3],\ninitializer=tf.truncated_normal_initializer(stddev=0.1),\nregularizer=slim.l2_regularizer(0.07),\ndevice='/CPU:0')\n```", "```py\nnet = slim.conv2d(inputs, 64, [11, 11], 4, padding='SAME',\nweights_initializer=tf.truncated_normal_initializer(stddev=0.01),\nweights_regularizer=slim.l2_regularizer(0.0007), scope='conv1')\n```", "```py\nnet = slim.conv2d(net, 128, [3, 3], scope='con1_1')\nnet = slim.conv2d(net, 128, [3, 3], scope='con1_2')\nnet = slim.conv2d(net, 128, [3, 3], scope='con1_3')\nnet = slim.conv2d(net, 128, [3, 3], scope='con1_4')\nnet = slim.conv2d(net, 128, [3, 3], scope='con1_5')\n\n```", "```py\nnet = slim.repeat(net, 5, slim.conv2d, 128, [3, 3], scope='con1')\n```", "```py\nnet = slim.conv2d(net, 64, [3, 3], scope='con1_1')\nnet = slim.conv2d(net, 64, [1, 1], scope='con1_2')\nnet = slim.conv2d(net, 128, [3, 3], scope='con1_3')\nnet = slim.conv2d(net, 128, [1, 1], scope='con1_4')\nnet = slim.conv2d(net, 256, [3, 3], scope='con1_5')\n```", "```py\nslim.stack(net, slim.conv2d, [(64, [3, 3]), (64, [1, 1]), \n\u00a0                             (128, [3, 3]), (128, [1, 1]),\n\u00a0                             (256, [3, 3])], scope='con')\n```", "```py\nwith slim.arg_scope([slim.conv2d], \n                padding='VALID',\n                activation_fn=tf.nn.relu,\nweights_initializer=tf.truncated_normal_initializer(stddev=0.02)\nweights_regularizer=slim.l2_regularizer(0.0007)):\nnet = slim.conv2d(inputs, 64, [11, 11], scope='con1')\nnet = slim.conv2d(net, 128, [11, 11], padding='VALID', scope='con2')\nnet = slim.conv2d(net, 256, [11, 11], scope='con3')\nnet = slim.conv2d(net, 256, [11, 11], scope='con4')\n\n```", "```py\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                activation_fn=tf.nn.relu,\n                weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n                weights_regularizer=slim.l2_regularizer(0.0005)):\n    net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='con1')\n    net = slim.max_pool2d(net, [2, 2], scope='pool1')\n    net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='con2')\n    net = slim.max_pool2d(net, [2, 2], scope='pool2')\n    net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='con3')\n    net = slim.max_pool2d(net, [2, 2], scope='pool3')\n    net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='con4')\n    net = slim.max_pool2d(net, [2, 2], scope='pool4')\n    net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='con5')\n    net = slim.max_pool2d(net, [2, 2], scope='pool5')\n    net = slim.fully_connected(net, 4096, scope='fc6')\n    net = slim.dropout(net, 0.5, scope='dropout6')\n    net = slim.fully_connected(net, 4096, scope='fc7')\n    net = slim.dropout(net, 0.5, scope='dropout7')\n    net = slim.fully_connected(net, 1000, activation_fn=None, scope='fc8')\n```", "```py\ngit clone https://github.com/tensorflow/models\n\n```", "```py\nimportsyssys.path.append(\"<*`some_path`*> + models/slim\")\n```", "```py\nfromdatasetsimportdataset_utilsimporttensorflowastftarget_dir='<*`some_path`*> + vgg/vgg_checkpoints'\n```", "```py\nimport urllib2\n\nurl = (\"https://somewebpage/somepicture.jpg\")\nim_as_string = urllib2.urlopen(url).read()\nim = tf.image.decode_jpeg(im_as_string, channels=3)\n\n```", "```py\nim = tf.image.decode_png(im_as_string, channels=3)\n\n```", "```py\nfilename_queue = tf.train.string_input_producer(\n                       tf.train.match_filenames_once(\"./images/*.jpg\"))\nimage_reader = tf.WholeFileReader()\n_, image_file = image_reader.read(filename_queue)\nimage = tf.image.decode_jpeg(image_file)\n\n```", "```py\nfrom nets import vgg\nimage_size = vgg.vgg_16.default_image_size\n\n```", "```py\nfrom preprocessing import vgg_preprocessing\nprocessed_im = vgg_preprocessing.preprocess_image(image,\nimage_size,\nimage_size,\nis_training=False)\n\n```", "```py\nprocessed_images= tf.expand_dims(processed_im, 0)\n\n```", "```py\nwith slim.arg_scope(vgg.vgg_arg_scope()):\nlogits, _ = vgg.vgg_16(processed_images,\nnum_classes=1000,\nis_training=False)\nprobabilities = tf.nn.softmax(logits)\n\n```", "```py\ndef vgg_arg_scope(weight_decay=0.0005):\n with slim.arg_scope([slim.conv2d, slim.fully_connected],\n          activation_fn=tf.nn.relu,\n          weights_regularizer=slim.l2_regularizer(weight_decay),\n          biases_initializer=tf.zeros_initializer):\n  with slim.arg_scope([slim.conv2d], padding='SAME') as arg_sc:\n   return arg_sc\n\n```", "```py\nimport os\n\nload_vars = slim.assign_from_checkpoint_fn(\nos.path.join(target_dir, 'vgg_16.ckpt'),\nslim.get_model_variables('vgg_16'))\n```", "```py\nfrom datasets import imagenet\nimagenet.create_readable_names_for_imagenet_labels()\n```", "```py\nnames = []\nwith tf.Session() as sess:\n  load_vars(sess)  \n  network_input, probabilities = sess.run([processed_images,\n                      probabilities])\n  probabilities = probabilities[0, 0:]\n  names_ = imagenet.create_readable_names_for_imagenet_labels()\n  idxs = np.argsort(-probabilities)[:5]\n  probs = probabilities[idxs]\n  classes = np.array(names_.values())[idxs+1]\n  for c,p in zip(classes,probs):\n    print('Class: '+ c + ' |Prob: ' + str(p))\n\n```", "```py\nOutput: Class: lakeside, lakeshore |Prob: 0.365693\nClass: pelican |Prob: 0.163627\nClass: dock, dockage, docking facility |Prob: 0.0608374\nClass: breakwater, groin, groyne, mole, bulwark, seawall, jetty |Prob: 0.0393285\nClass: speedboat |Prob: 0.0391587\n```"]