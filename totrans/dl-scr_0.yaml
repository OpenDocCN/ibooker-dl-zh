- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you’ve tried to learn about neural networks and deep learning, you’ve probably
    encountered an abundance of resources, from blog posts to MOOCs (massive open
    online courses, such as those offered on Coursera and Udacity) of varying quality
    and even some books—I know I did when I started exploring the subject a few years
    ago. However, if you’re reading this preface, it’s likely that each explanation
    of neural networks that you’ve come across is lacking in some way. I found the
    same thing when I started learning: the various explanations were [like blind
    men describing different parts of an elephant](https://oreil.ly/r5YxS), but none
    describing the whole thing. That is what led me to write this book.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: These existing resources on neural networks mostly fall into two categories.
    Some are conceptual and mathematical, containing both the drawings one typically
    finds in explanations of neural networks, of circles connected by lines with arrows
    on the ends, as well as extensive mathematical explanations of what is going on
    so you can “understand the theory.” A prototypical example of this is the very
    good book *Deep Learning* by Ian Goodfellow et al. (MIT Press).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Other resources have dense blocks of code that, if run, appear to show a loss
    value decreasing over time and thus a neural network “learning.” For instance,
    the following example from the PyTorch documentation does indeed define and train
    a simple neural network on randomly generated data:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Explanations like this, of course, don’t give much insight into “what is really
    going on”: the underlying mathematical principles, the individual neural network
    components contained here and how they work together, and so on.^([1](preface01.html#idm45732633101800))'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'What *would* a good explanation of neural networks contain? For an answer,
    it is instructive to look at how other computer science concepts are explained:
    if you want to learn about sorting algorithms, for example, there are textbooks
    that will contain:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: An explanation of the algorithm, in plain English
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A visual explanation of how the algorithm works, of the kind that you would
    draw on a whiteboard during a coding interview
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some mathematical explanation of “why the algorithm works”^([2](preface01.html#idm45732633302504))
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pseudocode implementing the algorithm
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One rarely—or never—finds these elements of an explanation of neural networks
    side by side, even though it seems obvious to me that a proper explanation of
    neural networks should be done this way; this book is an attempt to fill that
    gap.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Neural Networks Requires Multiple Mental Models
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I am not a researcher, and I do not have a Ph.D. I have, however, taught data
    science professionally: I taught a couple of data science bootcamps with a company
    called Metis, and then I traveled around the world for a year with Metis doing
    one- to five-day workshops for companies in many different industries in which
    I explained machine learning and basic software engineering concepts to their
    employees. I’ve always loved teaching and have always been fascinated by the question
    of how best to explain technical concepts, most recently focusing on concepts
    in machine learning and statistics. With neural networks, I’ve found the most
    challenging part is conveying the correct “mental model” for what a neural network
    is, especially since understanding neural networks fully requires not just one
    but *several* mental models, all of which illuminate different (but still essential)
    aspects of how neural networks work. To illustrate this: the following four sentences
    are all correct answers to the question “What is a neural network?”:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: A neural network is a mathematical function that takes in inputs and produces
    outputs.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A neural network is a computational graph through which multidimensional arrays
    flow.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A neural network is made up of layers, each of which can be thought of as having
    a number of “neurons.”
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A neural network is a universal function approximator that can in theory represent
    the solution to any supervised learning problem.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indeed, many of you reading this have probably heard one or more of these before,
    and may have a reasonable understanding of what they mean and what their implications
    are for how neural networks work. To fully understand them, however, we’ll have
    to understand *all* of them and show how they are connected—how is the fact that
    a neural network can be represented as a computational graph connected to the
    notion of “layers,” for example? Furthermore, to make all of this precise, we’ll
    implement all of these concepts from scratch, in Python, and stitch them together
    to make working neural networks that you can train on your laptop. Nevertheless,
    despite the fact that we’ll spend a substantial amount of time on implementation
    details, *the purpose of implementing these models in Python is to solidify and
    make precise our understanding of the concepts; it is not to write as concise
    or performant of a neural network library as possible.*
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: My goal is that after you’ve read this book, you’ll have such a solid understanding
    of all of these mental models (and their implications for how neural networks
    should be *implemented*) that learning related concepts or doing further projects
    in the field will be much easier.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Chapter Outlines
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first three chapters are the most important ones and could themselves form
    a standalone book.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html#foundations) I’ll show how mathematical functions can
    be represented as a series of operations linked together to form a computational
    graph, and show how this representation lets us compute the derivatives of these
    functions’ outputs with respect to their inputs using the chain rule from calculus.
    At the end of this chapter, I’ll introduce a very important operation, the matrix
    multiplication, and show how it can fit into a mathematical function represented
    in this way while still allowing us to compute the derivatives we’ll end up needing
    for deep learning.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In [Chapter 2](ch02.html#fundamentals) we’ll directly use the building blocks
    we created in [Chapter 1](ch01.html#foundations) to build and train models to
    solve a real-world problem: specifically, we’ll use them to build both linear
    regression and neural network models to predict housing prices on a real-world
    dataset. I’ll show that the neural network performs better than the linear regression
    and try to give some intuition for why. The “first principles” approach to building
    the models in this chapter should give you a very good idea of how neural networks
    work, but will also show the limited capability of the step-by-step, purely first-principles-based
    approach to defining deep learning models; this will motivate [Chapter 3](ch03.html#deep_learning_from_scratch).'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In [Chapter 3](ch03.html#deep_learning_from_scratch) we’ll take the building
    blocks from the first-principles-based approach of the first two chapters and
    use them to build the “higher level” components that make up all deep learning
    models: `Layer`s, `Model`s, `Optimizer`s, and so on. We’ll end this chapter by
    training a deep learning model, defined from scratch, on the same dataset from
    [Chapter 2](ch02.html#fundamentals) and showing that it performs better than our
    simple neural network.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As it turns out, there are few theoretical guarantees that a neural network
    with a given architecture will actually find a good solution on a given dataset
    when trained using the standard training techniques we’ll use in this book. In
    [Chapter 4](ch04.html#extensions) we’ll cover the most important “training tricks”
    that generally increase the probability that a neural network will find a good
    solution, and, wherever possible, give some mathematical intuition as to why they
    work.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In [Chapter 5](ch05.html#convolution) I cover the fundamental ideas behind
    convolutional neural networks (CNNs), a kind of neural network architecture specialized
    for understanding images. There are many explanations of CNNs out there, so I’ll
    focus on explaining the absolute essentials of CNNs and how they differ from regular
    neural networks: specifically, how CNNs result in each layer of neurons being
    organized into “feature maps,” and how two of these layers (each made up of multiple
    feature maps) are connected together via convolutional filters. In addition, just
    as we coded the regular layers in a neural network from scratch, we’ll code convolutional
    layers from scratch to reinforce our understanding of how they work.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Throughout the first five chapters, we’ll build up a miniature neural network
    library that defines neural networks as a series of `Layer`s—which are themselves
    made up of a series of `Operation`s—that send inputs forward and gradients backward.
    This is not how most neural networks are implemented in practice; instead, they
    use a technique called *automatic differentiation*. I’ll give a quick illustration
    of automatic differentiation at the beginning of [Chapter 6](ch06.html#recurrent)
    and use it to motivate the main subject of the chapter: *recurrent neural networks*
    (RNNs), the neural network architecture typically used for understanding data
    in which the data points appear sequentially, such as time series data or natural
    language data. I’ll explain the workings of “vanilla RNNs” and of two variants:
    *GRUs* and *LSTMs* (and of course implement all three from scratch); throughout,
    I’ll be careful to distinguish between the elements that are shared across *all*
    of these RNN variants and the specific ways in which these variants differ.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, in [Chapter 7](ch07.html#pytorch), I’ll show how everything we did
    from scratch in Chapters [1](ch01.html#foundations)–[6](ch06.html#recurrent) can
    be implemented using the high-performance, open source neural network library
    PyTorch. Learning a framework like this is essential for progressing your learning
    about neural networks; but diving in and learning a framework without first having
    a solid understanding of how and why neural networks work would severely limit
    your learning in the long term. The goal of the progression of chapters in this
    book is to give you the power to write extremely high-performance neural networks
    (by teaching you PyTorch) while still setting you up for long-term learning and
    success (by teaching you the fundamentals before you learn PyTorch). We’ll conclude
    with a quick illustration of how neural networks can be used for unsupervised
    learning.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: My goal here was to write the book that I wish had existed when I started to
    learn the subject a few years ago. I hope you will find this book helpful. Onward!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Conventions Used in This Book
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following typographical conventions are used in this book:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '*Italic*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Indicates new terms, URLs, email addresses, filenames, and file extensions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '`Constant width`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Used for program listings, as well as within paragraphs to refer to program
    elements such as variable or function names, databases, data types, environment
    variables, statements, and keywords.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '**`Constant width bold`**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Shows commands or other text that should be typed literally by the user.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '*`Constant width italic`*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Used for text that should be replaced with user-supplied values or by values
    determined by context and for comments in code examples.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The Pythagorean Theorem is <math><mrow><msup><mi>a</mi> <mn>2</mn></msup> <mo>+</mo>
    <msup><mi>b</mi> <mn>2</mn></msup> <mo>=</mo> <msup><mi>c</mi> <mn>2</mn></msup></mrow></math>
    .
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This element signifies a general note.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Using Code Examples
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Supplemental material (code examples, exercises, etc.) is available for download
    at [the book’s GitHub repository](https://oreil.ly/deep-learning-github).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: This book is here to help you get your job done. In general, if example code
    is offered with this book, you may use it in your programs and documentation.
    You do not need to contact us for permission unless you’re reproducing a significant
    portion of the code. For example, writing a program that uses several chunks of
    code from this book does not require permission. Selling or distributing a CD-ROM
    of examples from O’Reilly books does require permission. Answering a question
    by citing this book and quoting example code does not require permission. Incorporating
    a significant amount of example code from this book into your product’s documentation
    does require permission.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'We appreciate, but do not require, attribution. An attribution usually includes
    the title, author, publisher, and ISBN. For example: “*Deep Learning from Scratch*
    by Seth Weidman (O’Reilly). Copyright 2019 Seth Weidman, 978-1-492-04141-2.”'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: If you feel your use of code examples falls outside fair use or the permission
    given above, feel free to contact us at [*permissions@oreilly.com*](mailto:permissions@oreilly.com).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: O’Reilly Online Learning
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For almost 40 years, [*O’Reilly Media*](http://oreilly.com) has provided technology
    and business training, knowledge, and insight to help companies succeed.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Our unique network of experts and innovators share their knowledge and expertise
    through books, articles, conferences, and our online learning platform. O’Reilly’s
    online learning platform gives you on-demand access to live training courses,
    in-depth learning paths, interactive coding environments, and a vast collection
    of text and video from O’Reilly and 200+ other publishers. For more information,
    please visit [*http://oreilly.com*](http://oreilly.com).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: How to Contact Us
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please address comments and questions concerning this book to the publisher:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: O’Reilly Media, Inc.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1005 Gravenstein Highway North
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sebastopol, CA 95472
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 800-998-9938 (in the United States or Canada)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 707-829-0515 (international or local)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 707-829-0104 (fax)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a web page for this book, where we list errata, examples, and any additional
    information. You can access this page at [*https://oreil.ly/dl-from-scratch*](https://oreil.ly/dl-from-scratch).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Email [*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com) to comment
    or ask technical questions about this book.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: For more information about our books, courses, conferences, and news, see our
    website at [*http://www.oreilly.com*](http://www.oreilly.com).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'Find us on Facebook: [*http://facebook.com/oreilly*](http://facebook.com/oreilly)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow us on Twitter: [*http://twitter.com/oreillymedia*](http://twitter.com/oreillymedia)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'Watch us on YouTube: [*http://www.youtube.com/oreillymedia*](http://www.youtube.com/oreillymedia)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’d like to thank my editor, Melissa Potter, along with the team at O’Reilly,
    who were meticulous with their feedback and responsive to my questions throughout
    the process.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'I’d like to give a special thanks to several people whose work to make technical
    concepts in machine learning accessible to a wider audience has directly influenced
    me, and a couple of whom I’ve been lucky enough to have gotten to know personally:
    in a randomly generated order, these people are Brandon Rohrer, Joel Grus, Jeremy
    Watt, and Andrew Trask.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: I’d like to thank my boss at Metis and my director at Facebook, who were unreasonably
    supportive of my carving out time to work on this project.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: I’d like to give a special thank you and acknowledgment to Mat Leonard, who
    was my coauthor for a brief period of time before we decided to go our separate
    ways. Mat helped organize the code for the minilibrary associated with the book—`lincoln`—and
    gave me very helpful feedback on some extremely unpolished versions of the first
    two chapters, writing his own versions of large sections of these chapters in
    the process.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I’d like to thank my friends Eva and John, both of whom directly encouraged
    and inspired me to take the plunge and actually start writing. I’d also like to
    thank my many friends in San Francisco who tolerated my general preoccupation
    and worry about the book as well as my lack of availability to hang out for many
    months, and who were unwaveringly supportive when I needed them to be.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想感谢我的朋友Eva和John，他们直接鼓励和启发我开始写作。我还想感谢旧金山的许多朋友，他们容忍了我对这本书的普遍关注和担忧，以及我数月来无法和他们一起出去玩的情况，并且在我需要他们支持时始终如一。
- en: ^([1](preface01.html#idm45732633101800-marker)) To be fair, this example was
    intended as an illustration of the PyTorch library for those who already understand
    neural networks, not as an instructive tutorial. Still, many tutorials follow
    this style, showing only the code along with some brief explanations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 公平地说，这个例子旨在为那些已经了解神经网络的人展示PyTorch库，而不是作为一个教程。尽管如此，许多教程都遵循这种风格，只展示代码以及一些简要的解释。
- en: ^([2](preface01.html#idm45732633302504-marker)) Specifically, in the case of
    sorting algorithms, why the algorithm terminates with a properly sorted list.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在排序算法的情况下，算法为什么会以一个正确排序的列表终止。
