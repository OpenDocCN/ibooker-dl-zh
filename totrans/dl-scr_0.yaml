- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: 'If you’ve tried to learn about neural networks and deep learning, you’ve probably
    encountered an abundance of resources, from blog posts to MOOCs (massive open
    online courses, such as those offered on Coursera and Udacity) of varying quality
    and even some books—I know I did when I started exploring the subject a few years
    ago. However, if you’re reading this preface, it’s likely that each explanation
    of neural networks that you’ve come across is lacking in some way. I found the
    same thing when I started learning: the various explanations were [like blind
    men describing different parts of an elephant](https://oreil.ly/r5YxS), but none
    describing the whole thing. That is what led me to write this book.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试学习神经网络和深度学习，你可能会遇到大量资源，从博客文章到MOOCs（大规模在线开放课程，比如Coursera和Udacity提供的课程）的质量各异，甚至一些书籍——我知道当我几年前开始探索这个主题时也是如此。然而，如果你正在阅读这篇前言，很可能你遇到的每一个神经网络解释都在某种程度上有所欠缺。当我开始学习时，我也发现了同样的问题：各种解释就像盲人描述大象的不同部分一样，但没有一个描述整体。这就是我写这本书的原因。
- en: These existing resources on neural networks mostly fall into two categories.
    Some are conceptual and mathematical, containing both the drawings one typically
    finds in explanations of neural networks, of circles connected by lines with arrows
    on the ends, as well as extensive mathematical explanations of what is going on
    so you can “understand the theory.” A prototypical example of this is the very
    good book *Deep Learning* by Ian Goodfellow et al. (MIT Press).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 关于神经网络的现有资源大多可以分为两类。一些是概念性和数学性的，包含了通常在神经网络解释中找到的圆圈通过箭头线连接的图示，以及详尽的数学解释，让你“理解理论”。这方面的典型例子是Ian
    Goodfellow等人的非常好的书《深度学习》（麻省理工学院出版社）。
- en: 'Other resources have dense blocks of code that, if run, appear to show a loss
    value decreasing over time and thus a neural network “learning.” For instance,
    the following example from the PyTorch documentation does indeed define and train
    a simple neural network on randomly generated data:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 其他资源包含了密集的代码块，如果运行，似乎显示了随着时间减少的损失值，从而神经网络“学习”。例如，PyTorch文档中的以下示例确实定义并训练了一个简单的神经网络，使用随机生成的数据：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Explanations like this, of course, don’t give much insight into “what is really
    going on”: the underlying mathematical principles, the individual neural network
    components contained here and how they work together, and so on.^([1](preface01.html#idm45732633101800))'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这样的解释并没有提供关于“真正发生了什么”的深入见解：这里包含的基本数学原理，个别神经网络组件以及它们如何一起工作等等。
- en: 'What *would* a good explanation of neural networks contain? For an answer,
    it is instructive to look at how other computer science concepts are explained:
    if you want to learn about sorting algorithms, for example, there are textbooks
    that will contain:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的神经网络解释会包含什么？为了得到答案，可以看看其他计算机科学概念是如何解释的：例如，如果你想学习排序算法，有一些教科书会包含：
- en: An explanation of the algorithm, in plain English
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法的解释，用简单的英语
- en: A visual explanation of how the algorithm works, of the kind that you would
    draw on a whiteboard during a coding interview
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法如何工作的视觉解释，就像你在编程面试中画在白板上的那种
- en: Some mathematical explanation of “why the algorithm works”^([2](preface01.html#idm45732633302504))
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些关于“算法为什么有效”的数学解释
- en: Pseudocode implementing the algorithm
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现算法的伪代码
- en: One rarely—or never—finds these elements of an explanation of neural networks
    side by side, even though it seems obvious to me that a proper explanation of
    neural networks should be done this way; this book is an attempt to fill that
    gap.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 很少或者从来没有在神经网络的解释中找到这些元素并排，尽管我认为一个正确的神经网络解释应该这样做；这本书是填补这一空白的尝试。
- en: Understanding Neural Networks Requires Multiple Mental Models
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解神经网络需要多个心智模型
- en: 'I am not a researcher, and I do not have a Ph.D. I have, however, taught data
    science professionally: I taught a couple of data science bootcamps with a company
    called Metis, and then I traveled around the world for a year with Metis doing
    one- to five-day workshops for companies in many different industries in which
    I explained machine learning and basic software engineering concepts to their
    employees. I’ve always loved teaching and have always been fascinated by the question
    of how best to explain technical concepts, most recently focusing on concepts
    in machine learning and statistics. With neural networks, I’ve found the most
    challenging part is conveying the correct “mental model” for what a neural network
    is, especially since understanding neural networks fully requires not just one
    but *several* mental models, all of which illuminate different (but still essential)
    aspects of how neural networks work. To illustrate this: the following four sentences
    are all correct answers to the question “What is a neural network?”:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我不是一名研究员，也没有博士学位。然而，我曾经专业地教授数据科学：我曾经与一家名为Metis的公司一起教授了几个数据科学训练营，然后与Metis一起环游世界一年，在许多不同行业的公司进行为期一到五天的研讨会，向他们的员工解释机器学习和基本软件工程概念。我一直热爱教学，并且一直被如何最好地解释技术概念的问题所吸引，最近专注于机器学习和统计学概念。对于神经网络，我发现最具挑战性的部分是传达正确的“心智模型”，即神经网络是什么，特别是因为完全理解神经网络不仅需要一个而是*几个*心智模型，所有这些模型都阐明神经网络工作的不同（但仍然是必要的）方面。为了说明这一点：以下四个句子都是对问题“神经网络是什么？”的正确回答：
- en: A neural network is a mathematical function that takes in inputs and produces
    outputs.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络是一个数学函数，接受输入并产生输出。
- en: A neural network is a computational graph through which multidimensional arrays
    flow.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络是一个计算图，通过它，多维数组流动。
- en: A neural network is made up of layers, each of which can be thought of as having
    a number of “neurons.”
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络由层组成，每一层可以被认为有一定数量的“神经元”。
- en: A neural network is a universal function approximator that can in theory represent
    the solution to any supervised learning problem.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络是一个通用的函数逼近器，理论上可以表示任何监督学习问题的解决方案。
- en: Indeed, many of you reading this have probably heard one or more of these before,
    and may have a reasonable understanding of what they mean and what their implications
    are for how neural networks work. To fully understand them, however, we’ll have
    to understand *all* of them and show how they are connected—how is the fact that
    a neural network can be represented as a computational graph connected to the
    notion of “layers,” for example? Furthermore, to make all of this precise, we’ll
    implement all of these concepts from scratch, in Python, and stitch them together
    to make working neural networks that you can train on your laptop. Nevertheless,
    despite the fact that we’ll spend a substantial amount of time on implementation
    details, *the purpose of implementing these models in Python is to solidify and
    make precise our understanding of the concepts; it is not to write as concise
    or performant of a neural network library as possible.*
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你们中的许多人可能以前听过其中一个或多个，并且可能对它们的含义以及对神经网络工作方式的影响有一个合理的理解。然而，要完全理解它们，我们必须理解*所有*它们，并展示它们如何相互联系——例如，神经网络可以被表示为计算图的事实如何与“层”这个概念相联系？此外，为了使所有这些精确，我们将从头开始在Python中实现所有这些概念，并将它们组合在一起，以制作可以在您的笔记本电脑上训练的工作神经网络。尽管我们将花费大量时间在实现细节上，*在Python中实现这些模型的目的是巩固和精确化我们对概念的理解；而不是尽可能写出简洁或高性能的神经网络库。*
- en: My goal is that after you’ve read this book, you’ll have such a solid understanding
    of all of these mental models (and their implications for how neural networks
    should be *implemented*) that learning related concepts or doing further projects
    in the field will be much easier.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我的目标是在你阅读完本书后，你将对所有这些心智模型（以及它们对神经网络应该如何*实现*的影响）有坚实的理解，从而学习相关概念或在该领域进行更多项目将会更容易。
- en: Chapter Outlines
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 章节概要
- en: The first three chapters are the most important ones and could themselves form
    a standalone book.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 前三章是最重要的，可以自成一本独立的书。
- en: In [Chapter 1](ch01.html#foundations) I’ll show how mathematical functions can
    be represented as a series of operations linked together to form a computational
    graph, and show how this representation lets us compute the derivatives of these
    functions’ outputs with respect to their inputs using the chain rule from calculus.
    At the end of this chapter, I’ll introduce a very important operation, the matrix
    multiplication, and show how it can fit into a mathematical function represented
    in this way while still allowing us to compute the derivatives we’ll end up needing
    for deep learning.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.html#foundations)中，我将展示数学函数如何被表示为一系列操作链接在一起形成一个计算图，并展示这种表示方式如何让我们使用微积分中的链式法则计算这些函数输出相对于它们的输入的导数。在本章末尾，我将介绍一个非常重要的操作，矩阵乘法，并展示它如何适应这种表示方式中的数学函数，同时仍然允许我们计算深度学习所需的导数。
- en: 'In [Chapter 2](ch02.html#fundamentals) we’ll directly use the building blocks
    we created in [Chapter 1](ch01.html#foundations) to build and train models to
    solve a real-world problem: specifically, we’ll use them to build both linear
    regression and neural network models to predict housing prices on a real-world
    dataset. I’ll show that the neural network performs better than the linear regression
    and try to give some intuition for why. The “first principles” approach to building
    the models in this chapter should give you a very good idea of how neural networks
    work, but will also show the limited capability of the step-by-step, purely first-principles-based
    approach to defining deep learning models; this will motivate [Chapter 3](ch03.html#deep_learning_from_scratch).'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html#fundamentals)中，我们将直接使用我们在[第1章](ch01.html#foundations)中创建的基本组件来构建和训练模型，以解决一个真实世界的问题：具体来说，我们将使用它们来构建线性回归和神经网络模型，以预测一个真实世界数据集上的房价。我将展示神经网络比线性回归表现更好，并尝试解释一些原因。本章中构建模型的“第一原理”方法应该让你对神经网络的工作原理有很好的理解，但也会展示逐步、纯粹基于第一原理的方法定义深度学习模型的有限能力；这将激励我们继续学习[第3章](ch03.html#deep_learning_from_scratch)。
- en: 'In [Chapter 3](ch03.html#deep_learning_from_scratch) we’ll take the building
    blocks from the first-principles-based approach of the first two chapters and
    use them to build the “higher level” components that make up all deep learning
    models: `Layer`s, `Model`s, `Optimizer`s, and so on. We’ll end this chapter by
    training a deep learning model, defined from scratch, on the same dataset from
    [Chapter 2](ch02.html#fundamentals) and showing that it performs better than our
    simple neural network.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.html#deep_learning_from_scratch)中，我们将采用前两章基于第一原理的方法构建的基本组件，并用它们来构建组成所有深度学习模型的“高级”组件：`Layer`、`Model`、`Optimizer`等。我们将通过在[第2章](ch02.html#fundamentals)中相同数据集上训练一个从头定义的深度学习模型来结束本章，并展示它比我们简单的神经网络表现更好。
- en: As it turns out, there are few theoretical guarantees that a neural network
    with a given architecture will actually find a good solution on a given dataset
    when trained using the standard training techniques we’ll use in this book. In
    [Chapter 4](ch04.html#extensions) we’ll cover the most important “training tricks”
    that generally increase the probability that a neural network will find a good
    solution, and, wherever possible, give some mathematical intuition as to why they
    work.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 事实证明，使用标准训练技术训练时，给定架构的神经网络实际上会在给定数据集上找到一个好的解决方案的理论保证很少。在[第4章](ch04.html#extensions)中，我们将介绍最重要的“训练技巧”，通常会增加神经网络找到好解决方案的概率，并在可能的情况下，给出一些数学直觉为什么它们有效。
- en: 'In [Chapter 5](ch05.html#convolution) I cover the fundamental ideas behind
    convolutional neural networks (CNNs), a kind of neural network architecture specialized
    for understanding images. There are many explanations of CNNs out there, so I’ll
    focus on explaining the absolute essentials of CNNs and how they differ from regular
    neural networks: specifically, how CNNs result in each layer of neurons being
    organized into “feature maps,” and how two of these layers (each made up of multiple
    feature maps) are connected together via convolutional filters. In addition, just
    as we coded the regular layers in a neural network from scratch, we’ll code convolutional
    layers from scratch to reinforce our understanding of how they work.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#convolution)中，我涵盖了卷积神经网络（CNNs）背后的基本思想，这是一种专门用于理解图像的神经网络架构。关于CNNs有很多解释，所以我将专注于解释CNNs的绝对基础知识以及它们与常规神经网络的区别：特别是CNNs导致每一层神经元组织成“特征图”，以及两个这些层（每个由多个特征图组成）如何通过卷积滤波器连接在一起。此外，就像我们从头开始编写了神经网络中的常规层一样，我们将从头开始编写卷积层，以加强我们对它们工作原理的理解。
- en: 'Throughout the first five chapters, we’ll build up a miniature neural network
    library that defines neural networks as a series of `Layer`s—which are themselves
    made up of a series of `Operation`s—that send inputs forward and gradients backward.
    This is not how most neural networks are implemented in practice; instead, they
    use a technique called *automatic differentiation*. I’ll give a quick illustration
    of automatic differentiation at the beginning of [Chapter 6](ch06.html#recurrent)
    and use it to motivate the main subject of the chapter: *recurrent neural networks*
    (RNNs), the neural network architecture typically used for understanding data
    in which the data points appear sequentially, such as time series data or natural
    language data. I’ll explain the workings of “vanilla RNNs” and of two variants:
    *GRUs* and *LSTMs* (and of course implement all three from scratch); throughout,
    I’ll be careful to distinguish between the elements that are shared across *all*
    of these RNN variants and the specific ways in which these variants differ.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前五章中，我们将构建一个小型神经网络库，将神经网络定义为一系列由`Layer`组成的层，这些层本身由一系列将输入向前传递和梯度向后传递的`Operation`组成。这不是实践中大多数神经网络的实现方式；相反，它们使用一种称为*自动微分*的技术。我将在[第6章](ch06.html#recurrent)的开头快速说明自动微分，并用它来激发本章的主题：*循环神经网络*（RNNs），这是通常用于理解数据的神经网络架构，其中数据点按顺序出现，如时间序列数据或自然语言数据。我将解释“普通RNNs”的工作原理以及两个变体：*GRUs*和*LSTMs*（当然，我们会从头开始实现这三个）；在整个过程中，我将小心区分这些RNN变体之间共享的元素和这些变体之间的特定差异。
- en: Finally, in [Chapter 7](ch07.html#pytorch), I’ll show how everything we did
    from scratch in Chapters [1](ch01.html#foundations)–[6](ch06.html#recurrent) can
    be implemented using the high-performance, open source neural network library
    PyTorch. Learning a framework like this is essential for progressing your learning
    about neural networks; but diving in and learning a framework without first having
    a solid understanding of how and why neural networks work would severely limit
    your learning in the long term. The goal of the progression of chapters in this
    book is to give you the power to write extremely high-performance neural networks
    (by teaching you PyTorch) while still setting you up for long-term learning and
    success (by teaching you the fundamentals before you learn PyTorch). We’ll conclude
    with a quick illustration of how neural networks can be used for unsupervised
    learning.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在[第7章](ch07.html#pytorch)中，我将展示如何使用高性能、开源的神经网络库PyTorch来实现我们在[第1](ch01.html#foundations)章至[第6](ch06.html#recurrent)章中从头开始做的一切。学习这样的框架对于推进你对神经网络的学习至关重要；但是在深入学习一个框架之前，没有对神经网络的工作原理和原因有扎实的理解，长期来看会严重限制你的学习。本书中章节的进展目标是让你有能力编写极高性能的神经网络（通过教授PyTorch），同时为你长期的学习和成功打下基础（在学习PyTorch之前教授你基础知识）。最后，我们将简要说明神经网络如何用于无监督学习。
- en: My goal here was to write the book that I wish had existed when I started to
    learn the subject a few years ago. I hope you will find this book helpful. Onward!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里的目标是写一本我在几年前开始学习这个主题时希望存在的书。希望你会发现这本书有帮助。继续前进！
- en: Conventions Used in This Book
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书中使用的约定
- en: 'The following typographical conventions are used in this book:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用以下排版约定：
- en: '*Italic*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*斜体*'
- en: Indicates new terms, URLs, email addresses, filenames, and file extensions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表示新术语、URL、电子邮件地址、文件名和文件扩展名。
- en: '`Constant width`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`固定宽度`'
- en: Used for program listings, as well as within paragraphs to refer to program
    elements such as variable or function names, databases, data types, environment
    variables, statements, and keywords.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 用于程序清单，以及在段落中引用程序元素，如变量或函数名称、数据库、数据类型、环境变量、语句和关键字。
- en: '**`Constant width bold`**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**`固定宽度加粗`**'
- en: Shows commands or other text that should be typed literally by the user.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 显示用户应该按照字面输入的命令或其他文本。
- en: '*`Constant width italic`*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*`固定宽度斜体`*'
- en: Used for text that should be replaced with user-supplied values or by values
    determined by context and for comments in code examples.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 用于应该由用户提供的值或由上下文确定的值替换的文本，以及代码示例中的注释。
- en: The Pythagorean Theorem is <math><mrow><msup><mi>a</mi> <mn>2</mn></msup> <mo>+</mo>
    <msup><mi>b</mi> <mn>2</mn></msup> <mo>=</mo> <msup><mi>c</mi> <mn>2</mn></msup></mrow></math>
    .
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 勾股定理是<math><mrow><msup><mi>a</mi> <mn>2</mn></msup> <mo>+</mo> <msup><mi>b</mi>
    <mn>2</mn></msup> <mo>=</mo> <msup><mi>c</mi> <mn>2</mn></msup></mrow></math>。
- en: Note
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This element signifies a general note.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示一般说明。
- en: Using Code Examples
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用代码示例
- en: Supplemental material (code examples, exercises, etc.) is available for download
    at [the book’s GitHub repository](https://oreil.ly/deep-learning-github).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 补充材料（代码示例、练习等）可在[本书的GitHub存储库](https://oreil.ly/deep-learning-github)下载。
- en: This book is here to help you get your job done. In general, if example code
    is offered with this book, you may use it in your programs and documentation.
    You do not need to contact us for permission unless you’re reproducing a significant
    portion of the code. For example, writing a program that uses several chunks of
    code from this book does not require permission. Selling or distributing a CD-ROM
    of examples from O’Reilly books does require permission. Answering a question
    by citing this book and quoting example code does not require permission. Incorporating
    a significant amount of example code from this book into your product’s documentation
    does require permission.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书旨在帮助您完成工作。一般来说，如果本书提供示例代码，您可以在程序和文档中使用它。除非您复制了代码的大部分内容，否则无需联系我们以获得许可。例如，编写一个程序使用本书中的几个代码块不需要许可。出售或分发包含O'Reilly图书示例的CD-ROM需要许可。引用本书并引用示例代码回答问题不需要许可。将本书中大量示例代码整合到产品文档中需要许可。
- en: 'We appreciate, but do not require, attribution. An attribution usually includes
    the title, author, publisher, and ISBN. For example: “*Deep Learning from Scratch*
    by Seth Weidman (O’Reilly). Copyright 2019 Seth Weidman, 978-1-492-04141-2.”'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢，但不要求署名。署名通常包括标题、作者、出版商和ISBN。例如：“*Deep Learning from Scratch* by Seth Weidman
    (O'Reilly). Copyright 2019 Seth Weidman, 978-1-492-04141-2.”
- en: If you feel your use of code examples falls outside fair use or the permission
    given above, feel free to contact us at [*permissions@oreilly.com*](mailto:permissions@oreilly.com).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您认为您对代码示例的使用超出了合理使用范围或上述许可，请随时通过[*permissions@oreilly.com*](mailto:permissions@oreilly.com)与我们联系。
- en: O’Reilly Online Learning
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: O'Reilly在线学习
- en: Note
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: For almost 40 years, [*O’Reilly Media*](http://oreilly.com) has provided technology
    and business training, knowledge, and insight to help companies succeed.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 近40年来，[*O'Reilly Media*](http://oreilly.com)提供技术和商业培训、知识和见解，帮助公司取得成功。
- en: Our unique network of experts and innovators share their knowledge and expertise
    through books, articles, conferences, and our online learning platform. O’Reilly’s
    online learning platform gives you on-demand access to live training courses,
    in-depth learning paths, interactive coding environments, and a vast collection
    of text and video from O’Reilly and 200+ other publishers. For more information,
    please visit [*http://oreilly.com*](http://oreilly.com).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们独特的专家和创新者网络通过书籍、文章、会议和我们的在线学习平台分享他们的知识和专长。O'Reilly的在线学习平台为您提供按需访问实时培训课程、深入学习路径、交互式编码环境以及来自O'Reilly和其他200多家出版商的大量文本和视频。有关更多信息，请访问[*http://oreilly.com*](http://oreilly.com)。
- en: How to Contact Us
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何联系我们
- en: 'Please address comments and questions concerning this book to the publisher:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请将有关本书的评论和问题发送至出版商：
- en: O’Reilly Media, Inc.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O'Reilly Media, Inc.
- en: 1005 Gravenstein Highway North
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1005 Gravenstein Highway North
- en: Sebastopol, CA 95472
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sebastopol, CA 95472
- en: 800-998-9938 (in the United States or Canada)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 800-998-9938（美国或加拿大）
- en: 707-829-0515 (international or local)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0515（国际或本地）
- en: 707-829-0104 (fax)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0104（传真）
- en: We have a web page for this book, where we list errata, examples, and any additional
    information. You can access this page at [*https://oreil.ly/dl-from-scratch*](https://oreil.ly/dl-from-scratch).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这本书创建了一个网页，列出勘误、示例和任何其他信息。您可以在[*https://oreil.ly/dl-from-scratch*](https://oreil.ly/dl-from-scratch)访问此页面。
- en: Email [*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com) to comment
    or ask technical questions about this book.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 发送电子邮件至[*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com)评论或提出有关本书的技术问题。
- en: For more information about our books, courses, conferences, and news, see our
    website at [*http://www.oreilly.com*](http://www.oreilly.com).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有关我们的图书、课程、会议和新闻的更多信息，请访问我们的网站[*http://www.oreilly.com*](http://www.oreilly.com)。
- en: 'Find us on Facebook: [*http://facebook.com/oreilly*](http://facebook.com/oreilly)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在Facebook上找到我们：[*http://facebook.com/oreilly*](http://facebook.com/oreilly)
- en: 'Follow us on Twitter: [*http://twitter.com/oreillymedia*](http://twitter.com/oreillymedia)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在Twitter上关注我们：[*http://twitter.com/oreillymedia*](http://twitter.com/oreillymedia)
- en: 'Watch us on YouTube: [*http://www.youtube.com/oreillymedia*](http://www.youtube.com/oreillymedia)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在YouTube上观看我们：[*http://www.youtube.com/oreillymedia*](http://www.youtube.com/oreillymedia)
- en: Acknowledgments
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: I’d like to thank my editor, Melissa Potter, along with the team at O’Reilly,
    who were meticulous with their feedback and responsive to my questions throughout
    the process.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢我的编辑Melissa Potter，以及O'Reilly团队，在整个过程中他们对我的反馈非常细致，对我的问题也很及时回应。
- en: 'I’d like to give a special thanks to several people whose work to make technical
    concepts in machine learning accessible to a wider audience has directly influenced
    me, and a couple of whom I’ve been lucky enough to have gotten to know personally:
    in a randomly generated order, these people are Brandon Rohrer, Joel Grus, Jeremy
    Watt, and Andrew Trask.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我要特别感谢几位致力于使机器学习中的技术概念更易于为更广泛的受众所理解的人，其中有几位我有幸亲自认识：按随机生成的顺序，这些人是Brandon Rohrer、Joel
    Grus、Jeremy Watt和Andrew Trask。
- en: I’d like to thank my boss at Metis and my director at Facebook, who were unreasonably
    supportive of my carving out time to work on this project.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢Metis的老板和Facebook的主管，他们非常支持我抽出时间来开展这个项目。
- en: I’d like to give a special thank you and acknowledgment to Mat Leonard, who
    was my coauthor for a brief period of time before we decided to go our separate
    ways. Mat helped organize the code for the minilibrary associated with the book—`lincoln`—and
    gave me very helpful feedback on some extremely unpolished versions of the first
    two chapters, writing his own versions of large sections of these chapters in
    the process.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我要特别感谢和致谢Mat Leonard，他曾是我在这个项目中的合著者，虽然我们后来决定各自走开。Mat帮助组织了与本书相关的迷你库`lincoln`的代码，并对前两章极不完善的版本给予了非常有用的反馈，在此过程中还写了自己版本的大部分章节。
- en: Finally, I’d like to thank my friends Eva and John, both of whom directly encouraged
    and inspired me to take the plunge and actually start writing. I’d also like to
    thank my many friends in San Francisco who tolerated my general preoccupation
    and worry about the book as well as my lack of availability to hang out for many
    months, and who were unwaveringly supportive when I needed them to be.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想感谢我的朋友Eva和John，他们直接鼓励和启发我开始写作。我还想感谢旧金山的许多朋友，他们容忍了我对这本书的普遍关注和担忧，以及我数月来无法和他们一起出去玩的情况，并且在我需要他们支持时始终如一。
- en: ^([1](preface01.html#idm45732633101800-marker)) To be fair, this example was
    intended as an illustration of the PyTorch library for those who already understand
    neural networks, not as an instructive tutorial. Still, many tutorials follow
    this style, showing only the code along with some brief explanations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 公平地说，这个例子旨在为那些已经了解神经网络的人展示PyTorch库，而不是作为一个教程。尽管如此，许多教程都遵循这种风格，只展示代码以及一些简要的解释。
- en: ^([2](preface01.html#idm45732633302504-marker)) Specifically, in the case of
    sorting algorithms, why the algorithm terminates with a properly sorted list.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在排序算法的情况下，算法为什么会以一个正确排序的列表终止。
