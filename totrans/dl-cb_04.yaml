- en: Chapter 4\. Building a Recommender System Based on Outgoing Wikipedia Links
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recommender systems are traditionally trained on previously collected ratings
    from users. We want to predict ratings from users, so starting with historical
    ratings feels like a natural fit. However, this requires us to have a substantial
    set of ratings before we can get going and it doesn’t allow us to do a good job
    on new items for which we don’t have ratings yet. Moreover, we deliberately ignore
    the metainformation that we have on items.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter you’ll explore how to build a simple movie recommender system
    based solely on outgoing Wikipedia links. You’ll start by extracting a training
    set from Wikipedia and then train embeddings based on these links. You’ll then
    implement a simple support vector machine classifier to give recommendations.
    Finally, you’ll explore how you can use your newly trained embeddings to predict
    review scores for the movies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in this chapter can be found in these notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 4.1 Collecting the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to obtain a dataset for training for a specific domain, like movies.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parse a Wikipedia dump and extract only the pages that are movies.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The code in this recipe shows how to fetch and extract training data from Wikipedia,
    which is a very useful skill. However, downloading and processing a full dump
    takes a rather long time. The *data* directory of the notebook folder contains
    the top 10,000 movies pre-extracted that we’ll use in the rest of the chapter,
    so you don’t need to run the steps in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by downloading a recent dump from Wikipedia. You can easily do
    this using your favorite browser, and if you don’t need the very latest version,
    you should probably pick a nearby mirror. But you can also do it programmatically.
    Here’s how to get the latest dump pages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll now go through the dumps and find the newest one that has actually finished
    processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the sleep to stay under the rate limiting of Wikipedia. Now let’s fetch
    the dump:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The dump we retrieved is a bz2-compressed XML file. We’ll use `sax` to parse
    the Wikipedia XML. We’re interested in the `<title>` and the `<page>` tags so
    our `Content​Handler` looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: For each `<page>` tag this collects the contents of the title and of the text
    into the `self._values` dictionary and calls `process_article` with the collected
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although Wikipedia started out as a hyperlinked text-based encyclopedia, over
    the years it has developed into a more structured data dump. One way this is done
    is by having pages link back to so-called *category pages*. These links function
    as tags. The page for the film *One Flew Over the Cuckoo’s Nest* links to the
    category page “1975 films,” so we know it is a movie from 1975\. Unfortunately,
    there is no such thing as a category page for just movies. Fortunately, there
    is a better way: Wikipedia templates.'
  prefs: []
  type: TYPE_NORMAL
- en: Templates started out as a way to make sure that pages that contain similar
    information have that information rendered in the same way. The “infobox” template
    is very useful for data processing. Not only does it contain a list of key/value
    pairs applicable to the subject of the page, but it also has a type. One of the
    types is “film,” which makes the task of extracting all movies a lot easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each movie we want to extract the name, the outgoing links and, just because
    we can, the properties stored in the infobox. The aptly named `mwparserfromhell`
    does a decent job of parsing Wikipedia:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now feed the bzipped dump into the parser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s save the results so next time we need the data, we don’t have
    to process for hours:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Wikipedia is not only a great resource to answer questions about almost any
    area of human knowledge; it also is the starting point for many deep learning
    experiments. Knowing how to parse the dumps and extract the relevant bits is a
    skill useful for many projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'At 13 GB the dumps are sizeable downloads. Parsing the Wikipedia markup language
    comes with its own challenges: the language has grown organically over the years
    and doesn’t seem to have a strong underlying design. But with today’s fast connections
    and some great open source libraries to help with the parsing, it has all become
    quite doable.'
  prefs: []
  type: TYPE_NORMAL
- en: In some situations the Wikipedia API might be more appropriate. This REST interface
    to Wikipedia allows you to search and query in a number of powerful ways and only
    fetch the articles that you need. Getting all the movies that way would take a
    long time given the rate limiting, but for smaller domains it is an option.
  prefs: []
  type: TYPE_NORMAL
- en: If you end up parsing Wikipedia for many projects, it might be worth it to first
    import the dump into a database like Postgres so you can query the dataset directly.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Training Movie Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you use link data between entities to produce suggestions like “If you
    liked this, you might also be interested in that”?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Train embeddings using some metainformation as connectors. This recipe builds
    on the previous one by using the movies and links extracted there. To make the
    dataset a bit smaller and less noisy, we’ll work with only the top 10,000 movies
    determined by popularity on Wikipedia.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll treat the outgoing links as the connectors. The intuition here is that
    movies that link to the same page are similar. They might have the same director
    or be of the same genre. As the model trains, it learns not only which movies
    are similar, but also which links are similar. This way it can generalize and
    discover that a link to the year 1978 has a similar meaning as a link to 1979,
    which in turn helps with movie similarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by counting the outgoing links as a quick way to see whether what
    we have is reasonable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Our model’s task is to determine whether a certain link can be found on the
    Wikipedia page of a movie, so we need to feed it labeled examples of matches and
    nonmatches. We’ll keep only links that occur at least three times and build a
    list of all valid (link, movie) pairs, which we’ll store for quick lookups later.
    We keep the same handy as a set for quick lookups later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to introduce our model. Schematically, we take both the `link_id`
    and the `movie_id` as a number and feed those into their respective embedding
    layers. The embedding layer will allocate a vector of `embedding_size` for each
    possible input. We then set the dot product of these two vectors to be the output
    of our model. The model will learn weights such that this dot product will be
    close to the label. These weights will then project movies and links into a space
    such that movies that are similar end up in a similar location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We’ll feed the model using a generator. The generator yields batches of data
    made up of positive and negative examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'We sample the positive samples from the pairs array and then fill it up with
    negative examples. The negative examples are randomly picked and we make sure
    they are not in the `pairs_set`. We then return the data in a format that our
    network expects, an input/output tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Time to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Training times will depend on your hardware, but if you start with the 10,000
    movie dataset they should be fairly short, even on a laptop without GPU acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now extract the movie embeddings from our model by accessing the weights
    of the `movie_embedding` layer. We normalize them so we can use the dot product
    as an approximation of the cosine similarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s see if the embeddings make some sense:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Embeddings are a useful technique, and not just for words. In this recipe we’ve
    trained a simple network and produced embeddings for movies with reasonable results.
    This technique can be applied any time we have a way to connect items. In this
    case we used the outgoing Wikipedia links, but we could also use incoming links
    or the words that appear on the page.
  prefs: []
  type: TYPE_NORMAL
- en: The model we trained here is extremely simple. All we do is ask it to come up
    with an embedding space such that the combination of the vector for the movie
    and the vector for the link can be used to predict whether or not they will co-occur.
    This forces the network to project movies into a space such that similar movies
    end up in a similar location. We can use this space to find similar movies.
  prefs: []
  type: TYPE_NORMAL
- en: In the Word2vec model we use the context of a word to predict the word. In the
    example of this recipe we don’t use the context of the link. For outgoing links
    it doesn’t seem like a particularly useful signal, but if we were using incoming
    links, it might have made sense. Pages linking to movies do this in a certain
    order, and we could use the context of the links to improve our embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we could use the actual Word2vec code and run it over any of
    the pages that link to movies, but keep the links to movies as special tokens.
    This would then create a mixed movie and word embedding space.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Building a Movie Recommender
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you build a recommender system based on embeddings?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a support vector machine to separate the positively ranked items from the
    negatively ranked items.
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous recipe let us cluster movies and make suggestions like “If you
    liked *Rogue One*, you should also check out *Interstellar*.” In a typical recommender
    system we want to show suggestions based on a series of movies that the user has
    rated. As we did in [Chapter 3](ch03.html#word_embeddings), we can use an SVM
    to do just this. Let’s take the best and worst movies according to *Rolling Stone*
    from 2015 and pretend they are user ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Constructing and training a simple SVM classifier based on this is easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now run the new classifier over all the movies in our dataset and print
    the best five and the worst five:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in the previous chapter, we can use support vector machines to efficiently
    construct a classifier that distinguishes between two classes. In this case, we
    have it distinguish between good movies and bad movies based on the embeddings
    that we have previously learned.
  prefs: []
  type: TYPE_NORMAL
- en: Since an SVM finds one or more hyperplanes that separate the “good” examples
    from the “bad” examples, we can use this as the personalization function—the movies
    that are the furthest from the separating hyperplane and on the right side are
    the movies that should be liked best.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Predicting Simple Movie Properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to predict simple movie properties, like Rotten Tomatoes ratings.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a linear regression model on the learned vectors of the embedding model
    to predict movie properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try this for Rotten Tomatoes ratings. Luckily they are already present
    in our data in `movie[-2]` as a string of the form `*N*%`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This should get us data for about half our movies. Let’s train on the first
    80%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s see how we’re doing on the last 20%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'That looks really impressive! But while it is a testament to how effective
    linear regression can be, there is an issue with our data that makes predicting
    the Rotten Tomatoes score easier: we’ve been training on the top 10,000 movies,
    and while popular movies aren’t always better, on average they do get better ratings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get an idea of how well we’re doing by comparing our predictions with
    just always predicting the average score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Our model does perform quite a bit better, but the underlying data makes it
    easy to produce a reasonable result.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Complex problems often need complex solutions, and deep learning can definitely
    give us those. However, starting with the simplest thing that could possibly work
    is often a good approach. It gets us started quickly and gives us an idea of whether
    we’re looking in the right direction: if the simple model doesn’t produce any
    useful results at all it’s not that likely that a complex model will help, whereas
    if the simple model does work there’s a good chance that a more complex model
    can help us achieve better results.'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression models are as simple as they come. The model tries to find
    a set of factors such that the linear combination of these factors and our vectors
    approach the target value as closely as possible. One nice aspect of these models
    compared to most machine learning models is that we can actually see what the
    contribution of each of the factors is.
  prefs: []
  type: TYPE_NORMAL
