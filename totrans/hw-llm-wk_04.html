<html><head></head><body>
  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1" id="chp__constraints"> <span class="chapter-title-numbering"><span class="num-string">5</span></span> <span class="title-text"> How do we constrain the behavior of LLMs?</span> </h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Constraining LLM behavior to make them more useful</li> 
    <li class="readable-text" id="p3">The four areas where we can constrain LLM behavior</li> 
    <li class="readable-text" id="p4"> How fine-tuning allows us to update LLMs</li> 
    <li class="readable-text" id="p5">How reinforcement learning can change the output of LLMs </li> 
    <li class="readable-text" id="p6">Modifying the inputs of an LLM using retrieval augmented generation</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>It may seem counterintuitive that you can make a model more useful by controlling the output the model is allowed to produce, but it is almost always necessary when working with LLMs. This control is necessitated by the fact that when presented with an arbitrary text prompt, an LLM will attempt to generate what it believes to be an appropriate response, regardless of its intended use. Consider a chatbot helping a customer buy a car; you do not want the LLM going off-script and talking to them about athletics or sports just because they asked something related to taking the vehicle to their kid’s soccer games.</p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <p>In this chapter, we will discuss in more detail why you would want to limit, or constrain, the output an LLM produces and the nuances associated with such constraints. Accurately constraining an LLM is one of the hardest things to accomplish because of the nature of how LLMs are trained to complete input based on what they observe in training data. Currently, there are no perfect solutions. We will discuss the four potential places where an LLM’s behavior can be modified:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p9">Before training occurs, curating the data used to train the LLM</li> 
   <li class="readable-text" id="p10">By altering how the LLM is trained</li> 
   <li class="readable-text" id="p11">By fine-tuning the LLM on a set of data</li> 
   <li class="readable-text" id="p12">By writing special code after training is complete to control the outputs of the model</li> 
  </ul> 
  <div class="readable-text" id="p13"> 
   <p>These four cases are summarized in figure <a href="#fig__constraint_stages">5.1</a>. Each stage of developing an LLM feeds into the next. The fine-tuning stage, a second round of training done on a smaller data set, is the most important for how tools like ChatGPT function today and the most likely approach you might use in practice. The first, larger training stage we’ve learned about in chapters 2 to 4 is often referred to as <em>pretraining</em> because it occurs before fine-tuning makes the model useful. The model produced by the pretraining process is sometimes referred to as either a <em>base model</em> or a <em>foundation model</em> because it is a point from which to build a task-specific, or fine-tuned, model.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p14">  
   <img alt="figure" src="../Images/CH05_F01_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__constraint_stages"><span class="num-string">Figure <span class="browsable-reference-id">5.1</span></span> One may intervene to change or constrain an LLM’s behavior in four places. The two stages of model training are shown in the middle of the diagram, where the model’s parameters are altered. On the left, one could also alter the training data before model training. On the right, one could intercept the model outputs after model training and write code to handle specific situations.</h5>
  </div> 
  <div class="readable-text" id="p15"> 
   <p>Due to the importance and effectiveness of fine-tuning, we will spend most of the chapter on that factor and how it may be performed.</p> 
  </div> 
  <div class="readable-text" id="p16"> 
   <h2 class=" readable-text-h2" id="why-do-we-want-to-constrain-behavior"><span class="num-string browsable-reference-id">5.1</span> Why do we want to constrain behavior?</h2> 
  </div> 
  <div class="readable-text" id="p17"> 
   <p>LLMs are incredibly successful because they are the first technology to deliver on the idea of “Tell a computer what to do in plain English, and it does it.” By being very explicit about what you want to happen, establishing a specific level of detail and specifying a certain tone, you can get an LLM to be a shockingly effective tool. This detailed set of instructions is called a <em>prompt</em>, and the art of designing a good prompt has been referred to as <em>prompt engineering</em>. For example, we could develop a prompt for a car-selling bot as demonstrated in figure <a href="#fig__car_prompt">5.2</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p18">  
   <img alt="figure" src="../Images/CH05_F02_Boozallen.jpg"/> 
   <h5 class=" figure-container-h5" id="fig__car_prompt"><span class="num-string">Figure <span class="browsable-reference-id">5.2</span></span> Commercial LLMs like ChatGPT are designed to follow instructions (within some limits) and can perform a lot of low-cognition or pattern-matching tasks with very high efficacy. These tasks include stylized writing, such as pattern matching, and instruction following, such as roleplaying as a car salesperson.</h5>
  </div> 
  <div class="readable-text" id="p19"> 
   <p>You could give an LLM a prompt on organizing data into comma-separated values so that you can copy them into Excel. You could design a prompt about how to categorize free-form survey responses into summarized themes. In all cases, prompting is an exercise in limiting, or constraining, the behavior to a particular task and set of goals. Yet, the tokenization and training techniques we have discussed in the previous chapters do not enable this kind of instruction following.</p> 
  </div> 
  <div class="readable-text" id="p20"> 
   <p>Remembering that models do what they are trained to do is essential. In the case of a standard LLM, this task is to take a text passage and generate a continuation of that document that looks like a typical passage from the training corpus with the provided beginning. It is not trained to answer questions, think, summarize text, hold a conversation, or anything else. To get this desirable instruction-following behavior, we must perform fine-tuning, a second round of training with different objectives that will produce the intended behavior. You may wonder, “Why don’t we train LLMs for the task we want them to perform?” In most deep learning applications, we strongly recommend following the process we defined in chapter 4 to create a loss function that is specific, computable, and smooth. However, for the kinds of tasks that LLMs are good at, there are many reasons why this two-stage training process works well.</p> 
  </div> 
  <div class="readable-text" id="p21"> 
   <p>The first reason involves the breadth of knowledge required to complete specific goals. Think back to the task of a chatbot selling a car. If we aim to build a model that successfully sells cars, it would be great to construct a dataset of only car-relevant facts. But when the potential buyer wants to know if the car can fit all the needed equipment for a hockey player, how easy it will be to clean, whether it will be possible for their arthritic grandparent to get in and out of the passenger seats, or any host of other possible questions someone might have about how their car interacts with their life, you encounter the problem of enumerating every possible question you might receive about cars. There is no way to get all the information required to generate answers for every possible situation. Instead, we rely on the training processes we have discussed so far, which can be considered pretraining, to capture information from an extensive content collection containing text about sports, arthritis, etc. We hope this information helps the model be better prepared or generically helpful in answering broader questions.</p> 
  </div> 
  <div class="readable-text" id="p22"> 
   <p>This is the second and primary reason why we use a two-stage training process: obtaining hundreds of millions of documents that describe a specific problem to use as a part of the pretraining stage would be impossible. At the current state of the art, this massive scale is necessary to create the impressive capabilities seen in GPT. However, with relatively little effort, one can pretrain with hundreds of millions of pieces of general information, such as web pages, to impart models with general knowledge. Subsequently, it is often sufficient to fine-tune with just hundreds of documents to constrain the model to produce something usefully tailored to a task at hand. Obtaining a few hundred documents for a specific problem may be challenging but achievable.</p> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>At a high level, a second fine-tuning training stage can help constrain an LLM to some subset of useful behaviors because the original model is not incentivized to do what we want. In the following sections, we will present concrete examples of the different problems that crop up with base models that will make the reasons why this works evident.</p> 
  </div> 
  <div class="readable-text" id="p24"> 
   <h3 class=" readable-text-h3" id="base-models-are-not-very-usable"><span class="num-string browsable-reference-id">5.1.1</span> Base models are not very usable</h3> 
  </div> 
  <div class="readable-text" id="p25"> 
   <p>Training an LLM following the process described in chapter 4 produces a model typically referred to as a <em>base model</em> because it can serve as a base platform for building applications or fine-tuned models. Unfortunately, base models are not very useful to most people because they don’t expose their underlying knowledge via a user-friendly UI, they can be challenging to keep on-topic, and sometimes they produce unsavory content. Base models are not even trained with the concept of being a chatbot like ChatGPT is.</p> 
  </div> 
  <div class="readable-text" id="p26"> 
   <h3 class=" readable-text-h3" id="not-all-model-outputs-are-desirable"><span class="num-string browsable-reference-id">5.1.2</span> Not all model outputs are desirable</h3> 
  </div> 
  <div class="readable-text" id="p27"> 
   <p>Sometimes, what a model thinks is likely to come next in a document is undesirable. There are several reasons for this, including</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p28"><em>Memorization</em>—Sometimes, LLMs can generate long, exact copies of sequences found in their training data, which is often referred to as <em>memorization</em>, which refers to the idea that the text is being reproduced by memory from the training set. Memorization can be beneficial, such as memorizing the answers to specific factual questions. For example, if someone asks, “When was Abraham Lincoln born?” you want the model to regurgitate “February 12, 1809.” However, it can also be substantially detrimental if it leads a model to infringe copyright. If someone asks for “A copy of <em>Inside Deep Learning</em> by Edward Raff,” and the model produces a verbatim copy, Edward may be upset with you for copyright infringement!</li> 
   <li class="readable-text" id="p29"><em>Bad things on the web</em>—Not everything found on the internet is something you would want to expose a user to. There is a lot of vile and hateful content on the internet, as well as factually incorrect info ranging from common misconceptions to conspiracy theories. While model developers often try to filter out this data before training the model, that’s not always possible.</li> 
   <li class="readable-text" id="p30"><em>Missing and new information</em>—Inconveniently, the world keeps evolving and growing more complex after we train our models. So a model trained oninformation up to 2018 will not know of anything that happened after, such as COVID-19 or the nightmare-fuel invention of necrobotics [1]. But you may want your model to know about these developments to remain useful, without having to pay a considerable cost to retrain your base model from scratch.</li> 
  </ul> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p31"> 
    <h5 class=" callout-container-h5 readable-text-h5">Waiting for the legal system to catch up</h5> 
   </div> 
   <div class="readable-text" id="p32"> 
    <p> We are not your lawyers; this is not a law book! The legal problems around LLMs are complex, and there is a lot of nuance regarding fair use and infringement. Search engines can show you the content of their sources verbatim, but why? A combination of laws explicitly addressing these concerns, such as the Digital Millennium Copyright Act (DMCA), and precedents set by court rulings, such as Field v Google, Inc. (412 F.Supp. 2d 1106 [D. Nev. 2006]), establish acceptable and nonacceptable use over time. However, legislation and court cases take time to create, and the revolution of generative AI does not fit neatly into existing legal understanding.</p> 
   </div> 
   <div class="readable-text" id="p33"> 
    <p>You may want a nice, clean answer about what is and is not forbidden by law in the United States or your own country, and the likely answer is that such certainty does not yet exist for LLMs. Plus, we wouldn’t be caught dead giving such legal advice in print—we don’t even play lawyers on TV!</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p34"> 
   <p>GPT-3.5 and 4 have been improved to avoid answering things they do not know (not always successfully), but we can look to some open-source base models like GPT-Neo to see what happens without proactive countermeasures. For example, if we make up the new fake drug, MELTON-24, and ask “What is MELTON-24, and can it help me sleep better?” we get the unhelpful response: “There is a great number of sleep problems that go with Melatonin, including insomnia and fatigue. This causes insomnia, and why it is important to avoid certain foods that can suppress melatonin.”</p> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>In this case, the similarity of MELTON to melatonin and the prompt of “sleep” were enough for the model to catch onto the melatonin theme. Still, the answer is obviously nonsensical since MELTON-24 does not exist. Ideally, we want the model to recognize and respond, acknowledging its lack of information rather than producing more text like it has done here.</p> 
  </div> 
  <div class="readable-text" id="p36"> 
   <h3 class=" readable-text-h3" id="some-cases-require-specific-formatting"><span class="num-string browsable-reference-id">5.1.3</span> Some cases require specific formatting</h3> 
  </div> 
  <div class="readable-text" id="p37"> 
   <p>If a user asks for data in a specific format, such as a structured text format like JSON (for an example of a common format for exchanging data between computers, see <a href="https://en.wikipedia.org/wiki/JSON">https://en.wikipedia.org/wiki/JSON</a>), and you do not match every opening or closing bracket or encode special characters properly, the output won’t satisfy their goals. It does not matter how sophisticated or close to correct the output may have been; formatting requirements are almost always strict requirements. We presented an example of this kind of problem in chapter 4 when we asked ChatGPT to write code in Modula-3, and it borrowed Python syntax that was invalid for Modula-3. The code won’t compile if it violates syntax rules. An LLM’s probabilistic approach to generating text for specific desired outputs will not guarantee that all desired syntax rules are adhered to 100% of the time.</p> 
  </div> 
  <div class="readable-text" id="p38"> 
   <h2 class=" readable-text-h2" id="fine-tuning-the-primary-method-of-changing-behavior"><span class="num-string browsable-reference-id">5.2</span> Fine-tuning: The primary method of changing behavior</h2> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>Now that we understand various reasons why we want to constrain and control the behavior of an LLM, we are better prepared to introduce new information to the model to address the problem we are trying to solve while avoiding the problem of producing harmful or legally questionable content. Remember, while there are four different places where we can intervene to change behavior, fine-tuning is far more effective than the others. Both closed source options like OpenAI [2] and open source tools like Hugging Face [3], among many others, have varying options for fine-tuning, making it the most accessible method for practitioners.</p> 
  </div> 
  <div class="readable-text" id="p40"> 
   <p>Any fine-tuning method will have the same effect—producing a new variant of an LLM with updated parameters that control its behavior. As a result, it is possible to mix and match different fine-tuning strategies because the fundamental effect they produce is the same: a new set of parameters that can be used as is or altered yet again. One person’s base model could be another person’s fine-tuned model. This happens with many open source LLMs where an initial model (e.g., Llama) will be altered by another party (e.g., you can find many “Instruct Llama” models), which you may then further fine-tune to your data or specific use case.</p> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>The most straightforward way to customize an LLM is by prompting and iteratively refining prompts until the desired behavior is obtained. However, fine-tuning is the next logical step if that does not work well. This step involves a moderate increase in effort and cost, such as collecting the data to fine-tune and acquiring the hardware for running a fine-tuning session.</p> 
  </div> 
  <div class="readable-text" id="p42"> 
   <p>Two fine-tuning methods you should know in particular are <em>supervised</em> fine-tuning (SFT) and the more intimidatingly named <em>reinforcement learning from human feedback</em> (RLHF). SFT is the more straightforward approach and is excellent for incorporating new knowledge into a model or simply giving it a boost in your preferred application domain. RLHF is more complex but provides a strategy for getting an LLM to follow harder and more abstract goals like “be a good chatbot.” </p> 
  </div> 
  <div class="readable-text" id="p43"> 
   <h3 class=" readable-text-h3" id="supervised-fine-tuning"><span class="num-string browsable-reference-id">5.2.1</span> Supervised fine-tuning</h3> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>The most common way to influence a model’s output is SFT. SFT involves taking high-quality, typically human-authored, example content that captures information vital to your task but is not necessarily well reflected in the base model.</p> 
  </div> 
  <div class="readable-text" id="p45"> 
   <p>This often occurs because LLMs are trained on a large amount of generally available content, which may have minimal overlap with your specific needs. If you run a hospital, LLMs have seen very few doctors’ notes. If you run a law firm, an LLM probably has not seen too many deposition transcripts. If you run a repair shop, LLMs probably have not seen all the manuals you might have access to.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p46"> 
   <p> <span class="print-book-callout-head">Warning</span> Fine-tuning is a helpful way to add new information to your model but can also have security ramifications. If you want to build an LLM on medical records, it makes sense to fine-tune the LLM on example medical records. But now there is a risk someone could get your LLM to reproduce sensitive information contained in that fine-tuning data because fundamentally, LLMs attempt to complete input based on the training data they have seen. The bottom line: do not train or fine-tune LLMs on data you want to keep private. </p> 
  </div> 
  <div class="readable-text" id="p47"> 
   <p>Consider again our example of the car company and its sales chatbot. A base model from a third-party source may generally be aware of cars but probably will not know everything about the company’s products. By fine-tuning a model on internal manuals, chat histories, emails, marketing materials, and other internal documents, you could ensure the model is prepared with as much information as possible about your cars. You could even write example documents about the merits of your vehicles over competitors, advantages, scripts, and more to ensure that the LLM is armed with the information you want it to have.</p> 
  </div> 
  <div class="readable-text" id="p48"> 
   <p>The mechanics of SFT are easy to explain. As we’ve alluded to, SFT simply needs more documents. They can be in any format from which text can be extracted. This constitutes all of the work necessary to apply SFT because SFT is just repeating the same training process you learned in chapter 4. Figure <a href="#fig__sft">5.3</a> shows that the process for SFT is the same as you saw previously. The difference is that the initial parameters are random and unhelpful the first time you train the base model. The second time you fine-tune, you start with the base model’s parameters that encode what the base model has learned by observing its training data.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p49">  
   <img alt="figure" src="../Images/CH05_F03_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__sft"><span class="num-string">Figure <span class="browsable-reference-id">5.3</span></span> Supervised fine-tuning (SFT) is a simple approach to improving model results. You repeat the same process used to build the base model. Once the base model is trained on a large amount of general data, you continue training on the smaller specialized data collection.</h5>
  </div> 
  <div class="readable-text" id="p50"> 
   <p>Delightfully, you now have a good understanding of SFT. Like the original training process, it reuses the “predict the next token” task to ensure your model has information from the new documents built inside. As a direct consequence of predicting the next token, SFT also does not allow us to change the incentives of the LLM. For this reason, abstract goals like “Do not curse at the user” are difficult to achieve with SFT.</p> 
  </div> 
  <div class="readable-text" id="p51"> 
   <h4 class=" readable-text-h4" id="fine-tuning-pitfalls"> Fine-tuning pitfalls</h4> 
  </div> 
  <div class="readable-text" id="p52"> 
   <p>By reusing the gradient descent strategy from chapter 4, all fine-tuning methods tend to inherit two problems around an LLM’s ability to return content on which it was trained. Since SFT is so simple, this is a good time for us to review the broader problems with fine-tuning beyond just SFT.</p> 
  </div> 
  <div class="readable-text" id="p53"> 
   <p>There are no guarantees that SFT will retain the information you provide correctly. This problem, known as <em>catastrophic forgetting</em> [4], occurs when you train the model on new data but do not continue training on older data, and the model begins to “forget” that older information. It is not easy to determine what will and will not be forgotten. Catastrophic forgetting has been a recognized problem since 1989 [5]. In other words, fine-tuning is not purely additive; you give up something for it.</p> 
  </div> 
  <div class="readable-text" id="p54"> 
   <h3 class=" readable-text-h3" id="reinforcement-learning-from-human-feedback"><span class="num-string browsable-reference-id">5.2.2</span> Reinforcement learning from human feedback</h3> 
  </div> 
  <div class="readable-text" id="p55"> 
   <p>At the time of writing, RLHF is the dominant paradigm for constraining models. As the name implies, it uses an approach from the field of <em>reinforcement learning</em> (RL). RL is a broad family of techniques where an algorithm must make multiple decisions toward maximizing a long-term goal, as shown in figure <a href="#fig__rl">5.4</a>, where four terms are used with a technical meaning:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p56"><em>Agent</em>—The entity/AI/robot with some overarching goal that it wishes toaccomplish that may take multiple actions to achieve.</li> 
   <li class="readable-text" id="p57"><em>Action</em>—The space of all possible things the agent may be able to perform or engage in to advance the agent’s goals.</li> 
   <li class="readable-text" id="p58"><em>Environment</em>—The place/object/space affected by an action. The environment may or may not change as a result of the action, actions taken by other agents, or the natural continuous change of the environment.</li> 
   <li class="readable-text" id="p59"><em>Reward</em>—The numeric quantification of improvement (which may be negative) that may or may not occur after any given number of actions.</li> 
  </ul> 
  <div class="browsable-container figure-container" id="p60">  
   <img alt="figure" src="../Images/CH05_F04_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__rl"><span class="num-string">Figure <span class="browsable-reference-id">5.4</span></span> RL is about iterative interactions, where the reward for your actions may not materialize for a long time and requires multiple steps to achieve. For a chatbot like ChatGPT, the environment is the conversation with a user, and the actions are the infinite possible texts that ChatGPT might complete. The reward becomes, in some sense, the user’s satisfaction with the chatbot at the end of the conversation.</h5>
  </div> 
  <div class="readable-text" id="p61"> 
   <p>In the example of an LLM being used as a chatbot to interact with people, the users are the environment. The LLM is itself the agent, and the text it can produce is the action. This leaves one final thing to specify: the reward. If we were to get a user to score a +1 for a good conversation with a chatbot (e.g., no foul language, no lying, provided helpful responses) and a -1 for a lousy conversation (e.g., it suggested destroying all humans), then we would be adding human feedback to our reinforcement learning.</p> 
  </div> 
  <div class="readable-text" id="p62"> 
   <p>An astute reader might notice that a reward sounds suspiciously similar to the loss function discussed in chapter 4. In fact, our example of a good and bad conversation falls into the very subjective and difficult-to-quantify regime that we stated was a bad example of a loss function. The +1/-1 reward is not smooth because the value points in one direction or the other, and there is no middle ground, another poor characteristic for a loss function.</p> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>One of the powerful things about RL is that it can work with noncontinuous and hard-to-quantify objectives. We use the term <em>reward</em> instead of <em>loss</em> to imply the difference between these two situations. Generally, the types of objectives that RL can learn are referred to as <em>nondifferentiable</em>. As a result, these objectives can’t be learned using the same mathematical techniques like gradient descent, which we covered when describing how neural networks learn in chapter 4. We will explain how RLHF works specifically in a moment. The caveat lector of RL is that it can be computationally expensive and require a significant amount of data. RL is a notoriously challenging way to learn. It often works worse than other fine-tuning techniques like SFT because RL requires many more examples of the “right” and “wrong” way of doing things than other approaches, and since we are using human feedback to guide RLHF, the results are not always perfect. For example, in figure <a href="#fig__dolphinchat">5.5</a>, RLHF cannot help an LLM understand basic instructions outside of what it has seen explicitly during RLHF training because it does not add any capability to perform basic logic, such as understanding the user’s request to avoid displaying information about dolphins, to the underlying model. </p> 
  </div> 
  <div class="browsable-container figure-container" id="p64">  
   <img alt="figure" src="../Images/CH05_F05_Boozallen.jpg"/> 
   <h5 class=" figure-container-h5" id="fig__dolphinchat"><span class="num-string">Figure <span class="browsable-reference-id">5.5</span></span> RLHF is quite good at getting LLMs to avoid known, specific problems. However, it does not endow the model with new tools to handle novel problems. The desire to talk about the Miami Dolphins as the logical thing to say next after asking about football in Miami violates the first request to avoid ever mentioning dolphins.</h5>
  </div> 
  <div class="readable-text" id="p65"> 
   <p>LLMs do not perform reasoning in the same way that we humans think of reasoning. You can get very far by collecting hundreds of millions of examples of “everything,” but the world is weird. We have little evidence that LLMs can reliably produce satisfying responses when something novel occurs. However, RLHF is the best so far for constraining how an LLM behaves. Despite its challenges, RL presents a way of learning that is not available with gradient-based methods that require differentiable objectives. Most importantly, ChatGPT has shown that RL can work in many cases. So let us dive deeper into how RLHF works.</p> 
  </div> 
  <div class="readable-text" id="p66"> 
   <h3 class=" readable-text-h3" id="fine-tuning-the-big-picture"><span class="num-string browsable-reference-id">5.2.3</span> Fine-tuning: The big picture</h3> 
  </div> 
  <div class="readable-text" id="p67"> 
   <p>SFT and RLHF are the two primary methods of fine-tuning an LLM. SFT can work with thousands of documents or samples, whereas RLHF often requires tens of thousands of examples. That should not stop you from investigating if you have less data, but if you have less data, it may be a better use of your time to develop better prompts.</p> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>More importantly, SFT and RLHF are not mutually exclusive. They both modify the underlying parameters of the model, and you can apply one after the other to obtain the benefits of each approach. They are also not the only fine-tuning methods that currently exist. For example, new fine-tuning methods are being developed that remove concepts from an LLM as a way of forcing a model to ignore data it has learned from after it has been trained [6]. Additional techniques for model alteration will be developed in the coming years. All will likely require you to do some data collection, but they will involve less work overall than trying to build an LLM from scratch yourself.</p> 
  </div> 
  <div class="readable-text" id="p69"> 
   <h2 class=" readable-text-h2" id="the-mechanics-of-rlhf"><span class="num-string browsable-reference-id">5.3</span> The mechanics of RLHF</h2> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>To describe how RLHF works, we will introduce an incomplete version of RLHF, explain why it does not work, and then explain how to fix it. In this section, we will not discuss the detailed math used by RLHF, as it would not give you any particularly great insights into RLHF from a high level. If you want to learn more about the nitty-gritty details, we recommend starting with "Implementing RLHF: Learning to Summarize with trlX" [7] after you’ve completed this chapter.</p> 
  </div> 
  <div class="readable-text" id="p71"> 
   <h3 class=" readable-text-h3" id="beginning-with-a-naive-rlhf"><span class="num-string browsable-reference-id">5.3.1</span> Beginning with a naive RLHF</h3> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>First, let’s look at the incomplete and naive version of RLHF. We have discussed how RL can learn with nondifferentiable objectives. So let us assume that we have a human who will score an LLM’s output with a <em>quality reward</em>, where +1 indicates a good response and -1 is an inadequate response. This quality reward is simply an arbitrary score we assign to the output produced by the LLM to indicate that one example is somehow better than others. </p> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>So if a user requests of an LLM, “Tell me a joke,” and the LLM produces a response of “How many ducks does it take to screw in a light bulb?” we might assign a score of +1 for a (reasonably) good joke. If the LLM instead produces a sentence like “Dogs are evil,” we will assign a score of -1 because it is not even attempting to make a joke. Because RL is difficult to do using simple quality rewards of +1 and -1, we will add additional information for the RL algorithm, such as the probabilities of each generated token. This way, the RL algorithm knows how probable each token may be. This whole process is summarized in figure <a href="#fig__RLHF_naive">5.6</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p74">  
   <img alt="figure" src="../Images/CH05_F06_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__RLHF_naive"><span class="num-string">Figure <span class="browsable-reference-id">5.6</span></span> A naive and incomplete version of RLHF. The dashed lines represent text being sent from one component to another. Since text is incompatible with gradient descent, a more difficult RL algo-rithm must be used instead. This allows us to alter the weights of the LLM based on a quality score for the LLM’s outputs.</h5>
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p75"> 
    <h5 class=" callout-container-h5 readable-text-h5">Why provide RL with probabilities?</h5> 
   </div> 
   <div class="readable-text" id="p76"> 
    <p> It may seem odd that we are providing the RL algorithm with the probabilities of each token. There are deeper mathematical reasons why this is useful, which we will not get into in this chapter. But for some intuition, a good joke often requires misdirection or surprise. If all the probabilities of a sequence are high values (near 1.0), it is probably not a good joke because it’s too predictable.</p> 
   </div> 
   <div class="readable-text" id="p77"> 
    <p>Broadly, across natural language processing, producing good generated text is a balancing act between making something probable (i.e., likely to occur) and not making it too probable (i.e., repetitive).</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p78"> 
   <h3 class=" readable-text-h3" id="the-quality-reward-model"><span class="num-string browsable-reference-id">5.3.2</span> The quality reward model</h3> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>We described the quality reward as human-assigned scores for every prompt completion. Although scoring completions manually in real time would technically work, it would be unreasonable due to the level of effort involved. However, human feedback is still incorporated via the quality reward. Instead, we train a neural network as a <em>reward model</em>. This is accomplished by having people manually collect hundreds of thousands of prompt and completion pairs and scoring them as good or bad. These scorings become the labeled data used to train the reward model, as shown in figure <a href="#fig__reward_model">5.7</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p80">  
   <img alt="figure" src="../Images/CH05_F07_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__reward_model"><span class="num-string">Figure <span class="browsable-reference-id">5.7</span></span> The reward model is trained like a standard supervised classification algorithm. A neural network, which could be an LLM itself or another simpler network like a convolutional or recurrent neural network, is trained to predict how a human would score a prompt completion pair. Because neural networks are differentiable, this training works and provides a tool that stands in as the “human” in RLHF.</h5>
  </div> 
  <div class="readable-text" id="p81"> 
   <p>Collecting hundreds of thousands of scored prompts and completion pairs is expensive but doable (e.g., <a href="https://huggingface.co/datasets/Anthropic/hh-rlhf">https://huggingface.co/datasets/Anthropic/hh-rlhf</a>), especially when using crowd-sourcing tools like Mechanical Turk (<a href="https://www.mturk.com/">https://www.mturk.com/</a>). That is a lot of data to curate manually but orders of magnitude smaller than the billions of tokens used to create the initial base models. These RLHF datasets must be large because you must cover many scenarios, questions, and requests that a user might provide. As we already saw in figure <a href="#fig__dolphinchat">5.5</a> with the dolphin example, RLHF tends to work for relatively straightforward and known topics. So breadth in handling different situations comes directly from breadth in the fine-tuning data.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p82"> 
   <p> <span class="print-book-callout-head">Note</span> We have been using +1/-1 as the example of providing a quality reward because it is the easiest to describe. Since RL does not need gradients, you can use any score relevant to your problem. Using a ranking score, where you compare multiple completions for a given prompt and rank them from best to worst, is more popular and more effective because you are grading multiple completions against each other simultaneously. Regardless, providing positive and negative feedback remains fundamentally the same. </p> 
  </div> 
  <div class="readable-text" id="p83"> 
   <h3 class=" readable-text-h3" id="the-similar-but-different-rlhf-objective"><span class="num-string browsable-reference-id">5.3.3</span> The similar-but-different RLHF objective</h3> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>Once you have trained a reward model, you can create and score as many prompts as you desire for the RLHF process. The human feedback is baked into the reward model and can now be distributed, parallelized, and reused. The only remaining problem is that the current naive version of RLHF is incentivized purely to maximize the quality reward, which is not the sole goal RL must focus on.</p> 
  </div> 
  <div class="readable-text" id="p85"> 
   <p>As a result, the model will start to degrade over time by producing gibberish and nonsensical outputs that are not high quality and would not be valuable to any reader. This degradation is related to a phenomenon called <em>adversarial attacks</em>, where it is surprisingly easy to trick a neural network into absurd decisions with relatively minor changes to the input. Adversarial machine learning (AML) is fast evolving and has its own rabbit hole of complexity, so we’ll defer that discussion to other folks [8]. But the naive implementation of RLHF we describe in figure <a href="#fig__RLHF_naive">5.6</a> essentially performs an adversarial attack against an LLM because it will focus only on maximizing the quality reward, not on being useful to the user. Essentially, this is Goodhart’s law happening to AI/ML: “When a measure becomes a target, it ceases to be a good measure.”</p> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>To address this problem, we must add a second objective to the RL algorithm. We will calculate a second reward for the similarity between the original base LLM’s output and the fine-tuned LLM’s output. Conceptually, this reward can be considered a reward when the fine-tuned LLM produces better output, similar to how the original LLM behaved. It prevents the model from going off the rails by getting too novel. Fundamentally, we want the generated output of the fine-tuned LLM to be grounded by the training data initially observed by the original LLM. We don’t want the fine-tuned model to get so creative that it generates nonsense. This reward is added to the RL algorithm to stabilize the fine-tuning. Figure <a href="#fig__RLHF_full">5.8</a> provides the complete picture of how RLHF works.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p87">  
   <img alt="figure" src="../Images/CH05_F08_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__RLHF_full"><span class="num-string">Figure <span class="browsable-reference-id">5.8</span></span> The full version of RLHF. The dashed lines are text and require RL to update the parameters. The original LLM is the base model without any alterations, while the LLM to fine-tune starts as the base model but is altered to improve the quality of its outputs. The similarity and quality reward components are provided with word probabilities to improve calculation. RL adjusts the parameters by combining the quality and similarity scores.</h5>
  </div> 
  <div class="readable-text" id="p88"> 
   <p>A model that learns to produce gibberish output would receive a high penalty for lack of similarity, discouraging the model from becoming too different. A model that produces the exact same outputs will receive a low quality score, discouraging a lack of change. The balance of both does an excellent job of achieving a Goldilocks effect that allows the model enough flexibility to change without causing it to lose its human-like output.</p> 
  </div> 
  <div class="readable-text" id="p89"> 
   <h2 class=" readable-text-h2" id="other-factors-in-customizing-llm-behavior"><span class="num-string browsable-reference-id">5.4</span> Other factors in customizing LLM behavior</h2> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>Fine-tuning is the dominant means of altering the behavior of an LLM, but fine-tuning is not foolproof and is not the only place where behavior changes can occur. Our focus on fine-tuning is based on the value of RLHF in producing LLM behaviors beyond simple next-token prediction.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p91">  
   <img alt="figure" src="../Images/CH05_F09_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__constraint_stages2"><span class="num-string">Figure <span class="browsable-reference-id">5.9</span></span> In addition to fine-tuning, you can change the model’s behavior by altering the training data, altering the base model training process, or modifying the model outputs by writing code to handle specific situations.</h5>
  </div> 
  <div class="readable-text" id="p92"> 
   <p>The other three stages where LLM behavior can be modified, described in figure <a href="#fig__constraint_stages2">5.9</a>, are not easily accessible to you as a user. However, we will briefly review the other stages now, along with some key details you should know for completeness. These factors can help you understand what is challenging to achieve by fine-tuning and the scope of questions you might want to investigate in your LLM provider.</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <h3 class=" readable-text-h3" id="altering-training-data"><span class="num-string browsable-reference-id">5.4.1</span> Altering training data</h3> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>The adage “garbage in, garbage out” is evergreen in all areas of ML. You may indeed notice that OpenAI [9] and Google [10] provide many low-level technical details about how they develop their LLMs but much less detail on the data used for building the LLM. That is because most of the “secret sauce” in building capable LLMs is around data curation—developing a collection of data representing diverse tasks, high-quality language use, and a spectrum of different situations. The size and quality of the data sets used to train, validate, and test LLMs matter.</p> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>The size and quality of data have become especially pertinent as LLM-generated content works its way into regular use and back online. For example, an estimated 6% to 16% of academic peer reviews are using LLMs [11], and it is highly likely that many copyediting services will soon be using these new technologies. This increased use potentially creates a negative feedback cycle. As the amount of data generated by LLMs grows, there will be proportionally less non-LLM content available for LLMs to train on. This will result in an overall decrease in the diversity of language available and, thus, the novelty that LLMs will be able to capture. In turn, the quality of an LLM trained on newer data is reduced [12]. This problem will likely be significant in keeping LLMs up to date, as curating a high-quality dataset will not be as simple as before.</p> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>There is also the problem that LLMs can only reflect information available at training and are disproportionally more likely to reflect information that is more prevalent in training. If you want an LLM that does not curse or use racist language, you must work to scrub your dataset of all cursing and racist language.</p> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>However, this problem is potentially a double-edged sword. If we want our LLM to know how to recognize and appropriately reject racist or foul language, it must know what racist and foul language are. You can imagine using prompting on an LLM that has never seen any racist text to “teach” the LLM to use racist words in a context that, without knowing <code>X</code> is racist, appears benign. But in the final form, we, as readers who are aware of racism, would recognize the sentence as objectionable. This problem, as of now, has no answer but is something to be mindful of. </p> 
  </div> 
  <div class="readable-text" id="p98"> 
   <p>Altering data is also important, as it is your only chance to influence how tokenization is performed in an LLM. As discussed in chapter 2, different approaches to tokenization have tradeoffs, but the choices you make are forever baked into the model once you start training.</p> 
  </div> 
  <div class="readable-text" id="p99"> 
   <h3 class=" readable-text-h3" id="altering-base-model-training"><span class="num-string browsable-reference-id">5.4.2</span> Altering base model training</h3> 
  </div> 
  <div class="readable-text" id="p100"> 
   <p>Training data privacy must be a significant concern when training or fine-tuning LLMs. Generally, it is possible to reconstruct a model’s training data by crafting inputs into a model in a special way. In some cases, LLMs have been shown to generate the exact passages on which they were trained. This is problematic if the training data contains private information, such as personally identifiable information (PII), private health information (PHI), or some other class of sensitive data. A user of a model could, perhaps unwittingly, provide a prompt that reveals this data verbatim.</p> 
  </div> 
  <div class="readable-text" id="p101"> 
   <p>Initial training of an algorithm is an ideal place to mitigate some of these privacy concerns by using a technique known as <em>differential privacy</em> (DP). DP is complex, so if you want to learn more, we recommend the book <em>Programming Differential Privacy</em> [13]. In short, DP adds a carefully constructed amount of random noise to provide provable guarantees about data privacy in the model training process. DP does not handle everything, but it provides much more protection than what is available with most algorithms today.</p> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>So why hasn’t everyone done just that? Well, adding noise naturally tends to reduce the quality of the result. Large training runs are expensive, costing hundreds of thousands to millions of dollars each. If you had to do <span><img alt="equation image" src="../Images/eq-chapter-5-102-1.png"/></span> more training runs to set your privacy parameters correctly, you would have a million to tens-of-millions-of-dollars problem. But with DP becoming better every year, we suspect it will become more prevalent over time.</p> 
  </div> 
  <div class="readable-text" id="p103"> 
   <h3 class=" readable-text-h3" id="altering-the-outputs"><span class="num-string browsable-reference-id">5.4.3</span> Altering the outputs</h3> 
  </div> 
  <div class="readable-text" id="p104"> 
   <p>Finally, we can examine the tokens being produced and write code to change its behavior based on the combinations of tokens generated by the model. After fine-tuning, this is the second most likely stage that a consumer of LLMs will use to modify their behavior.</p> 
  </div> 
  <div class="readable-text" id="p105"> 
   <p>Earlier in this chapter, we discussed a common need for LLMs to generate output that adheres to a precise format, such as XML or JSON. Implementing formatting requirements like these is a common problem with LLMs. Any single failed prediction results in a failure to generate valid output. You can see an example of this type of failure in figure <a href="#fig__force_output">5.10</a>, where we ask the LLM to complete some Python code; the next token should be a semicolon (;), but it erroneously attempts a newline (\ n) instead. </p> 
  </div> 
  <div class="browsable-container figure-container" id="p106">  
   <img alt="figure" src="../Images/CH05_F10_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__force_output"><span class="num-string">Figure <span class="browsable-reference-id">5.10</span></span> By writing code that enforces a format specification, you can catch invalid output from an LLM as it is being generated. Once detected, having the LLM produce the next most likely token until a valid output is found is a simple way to improve the situation.</h5>
  </div> 
  <div class="readable-text" id="p107"> 
   <p>Various tools exist (e.g., <a href="https://github.com/noamgat/lm-format-enforcer">https://github.com/noamgat/lm-format-enforcer</a>) for specifying strict formats as a part of the LLM’s decoding step. If these tools detect a parse error, they immediately regenerate the last token until a valid output is produced.</p> 
  </div> 
  <div class="readable-text" id="p108"> 
   <p>More sophisticated approaches to selecting the next token are possible. Still, the important lesson here is the ability to use the intermediate outputs to make decisions before generating the entire output. Even simple old-school “go/no-go” lists are valuable tools for catching bad behavior. You do not need to pass an output to the user in true real time; you can always introduce an artificial delay so that you can see more of the response before sending it to the user. This gives you time to chat against bad language filters or other hard-coded checks. If a match occurs, just like in figure <a href="#fig__force_output">5.10</a>, you can regenerate an output or abort the user’s session.</p> 
  </div> 
  <div class="readable-text" id="p109"> 
   <h2 class=" readable-text-h2" id="integrating-llms-into-larger-workflows"><span class="num-string browsable-reference-id">5.5</span> Integrating LLMs into larger workflows</h2> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>At this point in the chapter, we have covered some basic approaches to manipulating an LLM to produce more desirable and consistent outputs. So far, we have focused on techniques that involve the LLM itself, whether through prompting, manipulating training data, or fine-tuning a base model. In this section, we will explore how to tailor the output produced by an LLM by integrating the inputs and outputs of LLMs into multistep chains of operations to achieve more tailored results. This space is quickly evolving, so we will briefly cover one concrete example of integrating an LLM into a broader information retrieval workflow and then discuss a general-purpose tool to show you how to customize LLM outputs using multiple interactions with an LLM.</p> 
  </div> 
  <div class="readable-text" id="p111"> 
   <h3 class=" readable-text-h3" id="sec__rag"><span class="num-string browsable-reference-id">5.5.1</span> Customizing LLMs with retrieval augmented generation</h3> 
  </div> 
  <div class="readable-text" id="p112"> 
   <p><em>Retrieval augmented generation</em> (RAG) is a technique that allows us to produce answers from an LLM while reducing the likelihood of generating nonsensical or otherwise errant explanations. The “retrieval” component of the RAG moniker should give you a helpful hint as to how the technique operates. When a user provides input to a RAG system, it uses an LLM to create a query that is run against a search engine that contains an index of documents. Depending on the use case, this might be an index of general information, such as Google, or a subject-specific index, such as a collection of automotive marketing materials. In response to the query, the search engine generates a list of relevant documents. The RAG system then uses the LLM to extract information from those documents to generate better answers. To do this, the RAG system combines the contents of the retrieved documents with the original user query to create a comprehensive prompt for the LLM that will result in a better response. This method tends to work well because instead of asking an LLM to generate a response based on its training or fine-tuning data, we are now asking the LLM to generate a response to input by summarizing a set of documents relevant to a regular old search engine query and providing that set of relevant documents from which to draw its answers to the LLM. In other words, we’re helping the LLM focus on the data it needs to properly answer a given question. We describe this process in figure <a href="#fig__rag">5.11</a> and compare it with the normal LLM use cases we have described so far.</p> 
  </div> 
  <div class="readable-text" id="p113"> 
   <p>The two most significant benefits of the RAG approach thus far are as follows:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p114">The output of a RAG system is more accurate, factually correct, or otherwise useful to the user’s original question because it is based on specific sources contained in a document index.</li> 
   <li class="readable-text" id="p115">The LLM can generate citations or references to the source documents used to produce its responses, allowing users to validate or correlate against the original source material.</li> 
  </ul> 
  <div class="browsable-container figure-container" id="p116">  
   <img alt="figure" src="../Images/CH05_F11_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__rag"><span class="num-string">Figure <span class="browsable-reference-id">5.11</span></span> On the left, we show the normal use of an LLM of a user asking about how to write JSON. LLMs naturally have the chance of producing errant outputs, which we want to minimize. On the right, we show the RAG approach. By using a search engine, we can find documents that are relevant to a query and combine them into a new prompt, giving the LLM more information and context to produce a better answer.</h5>
  </div> 
  <div class="readable-text" id="p117"> 
   <p>The latter point regarding citations is particularly important. RAG will not solve all of LLMs’ problems because the LLM still generates the final output in a RAG system. The LLM may still produce errors or hallucinations due to content that it cannot find or that doesn’t exist. It is also possible that the LLM will not accurately capture or represent the content of any of the source documents it uses. As a result, the utility of the RAG approach is directly related to the quality of the search it performs and the documents that are returned. The bottom line is that if you can’t build an effective search engine for your problem, you can’t build an effective RAG model.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p118"> 
    <h5 class=" callout-container-h5 readable-text-h5">Context size</h5> 
   </div> 
   <div class="readable-text" id="p119"> 
    <p> When thinking about LLMs, it is important to consider one aspect of LLMs known as the <em>context size</em>. The context size of the LLM determines how many tokens it can computationally handle in a single request for completions. You can think of it as the amount of data that an LLM is able to look at when receiving input in the form of a prompt. For example, GPT-3 has a context size of 2,048 tokens. However, in chatbots, for example, the context is often used to hold a running transcript of the entire conversation, including any LLM outputs. If you have a conversation with GPT-3 that goes beyond 2,048 tokens in length, you’ll find that GPT-3 often loses track of some of the things discussed early on in the chat.</p> 
   </div> 
   <div class="readable-text" id="p120"> 
    <p>Context size is an enabling and limiting factor for RAG use. If a RAG system retrieves an entire book for your LLM to digest, your LLM will require a huge context size to be able to use it. Otherwise, the LLM can only consume the first part of a retrieved document (up to the LLM’s context size) and may miss information. As a result, context size is an important operational characteristic you should consider when choosing a model. Some models today, such as X’s Grok, can handle up to 128,000 tokens as their context size. While large context sizes like Grok’s increase the hard limit of what an LLM can consume, the effectiveness of LLMs when dealing with large amounts of input enabled by larger context sizes is still an active area of study.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p121"> 
   <p>You may notice in figure <a href="#fig__rag">5.11</a> that we have to create a new prompt. We added the prefix “Answer the question:” followed by the postfix “Using the following information:” Hypothetically, you could obtain better results by tweaking this prompt. You may get thoughts about adding some instructions like “Ignore any of the following information if it is not relevant to the original question.” These ideas are starting to get into prompt engineering, the practice of tweaking and modifying the text going into an LLM to change its behavior, as we talked about earlier in chapter 4.</p> 
  </div> 
  <div class="readable-text" id="p122"> 
   <p>Prompt engineering is indeed useful and a good way to combine multiple calls to an LLM to improve results. For example, you could try to improve your search results by asking the LLM to rewrite the question. (This discussion touches on a classic area of information retrieval called <em>query expansion</em>, if you wish to learn more on the topic.) However, prompt engineering can be very brittle: any update to an LLM may change what prompts do or don’t work, and it would be a pain to have to rewrite every prompt—especially as you get into anything more complex, like a RAG model or something even more sophisticated.</p> 
  </div> 
  <div class="readable-text" id="p123"> 
   <h3 class=" readable-text-h3" id="sec__llm_programming"><span class="num-string browsable-reference-id">5.5.2</span> General-purpose LLM programming</h3> 
  </div> 
  <div class="readable-text" id="p124"> 
   <p>Although still new, we are already starting to see programming libraries and other software tools built using LLMs as a component of custom applications. One we particularly like is DSPy (<a href="https://dspy.ai">https://dspy.ai</a>), which can make it easier to build and maintain programs that attempt to alter the inputs to and outputs of an LLM. A good software library will hide details that get in the way of productivity, and DSPy does a good job of abstracting away the following tasks around LLM usage:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p125">Integrating the specific LLM being used</li> 
   <li class="readable-text" id="p126">Implementing common patterns of prompting</li> 
   <li class="readable-text" id="p127">Tweaking the prompts for your desired combination of data, task, and LLM.</li> 
  </ul> 
  <div class="readable-text" id="p128"> 
   <p>This is not a coding book, so a full tutorial on DSPy is out of scope. But it is illustrative to look at the ways DSPy can be used to implement the RAG model we described in section <a href="#sec__rag">5.5.1</a>. It will require that we pick an LLM to use (GPT-3.5, in this case), as well as a database of information (Wikipedia will work well), and define the RAG algorithm. DSPy works by defining a default LLM and database used by all components (unless you intervene), making it easy to separate and replace the parts being used. This process is shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p129"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="dspy_code"><span class="num-string">Listing <span class="browsable-reference-id">5.1</span></span> Simplest RAG in DSPy</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import dspy

llm = dspy.OpenAI(model='gpt-3.5-turbo')  #1




similarity_and_database = dspy.ColBERTv2(   #2
  'wiki17_abstracts'
) 

dspy.settings.configure(  #3
    lm=llm, 
    rm=similarity_and_database
) 

class RAG(dspy.Module):
    def __init__(self, num_passages=3):
        super().__init__()

        self.retrieve = dspy.Retrieve(  #4
            k=num_passages
        ) 

    
        self.generate_answer = dspy.Prediction(  #5
            "question,relevant_documents -&gt; answer"
        )

    
    def forward(self, question):  #6
        documents = self.retrieve(
            question
        ).passages
        
        return self.generate_answer(
            question=question, 
            relevant_documents=documents
        ).answer</pre> 
    <div class="code-annotations-overlay-container"> #1 Uses OpenAI's GPT-3.5, which can be swapped out with other online or local LLMs
     <br/>#2 Uses the ColBERTv2 algorithm to vectorize a copy of Wikipedia
     <br/>#3 Uses the LLM and document database we just created
     <br/>#4 Searches for the three most relevant documents from the database
     <br/>
     <span>#5 Specifies a ``signature" string, which defines the inputs and output of the LLM</span>
     <br/>#6 Calls the functions to build a RAG model. The parameter names match the names in the signature.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p130"> 
   <p>This code sets the aforementioned choices in LLMs and databases as the defaults, making it just as easy to replace OpenAI with another online LLM or a local one such as Llama. The <code>class RAG(dspy.Module):</code> class then defines the RAG algorithm. The initializer only has two parts. </p> 
  </div> 
  <div class="readable-text" id="p131"> 
   <p>First, we need a way to search a database of strings based on vectorized documents, which is defined with <code>ColBERTv2</code>. It uses an older—as in just four years ago (wild how fast the field is moving)—but much faster language model for speed and efficiency. Remember, the larger language model (that is, the more expensive to run) just needs reasonable documents to be retrieved. While ColBERTv2 probably won’t do as good a job as GPT-3.5, it is more than good enough to get you the right documents most of the time. The <code>dspy.Retrieve</code> then uses this default database for searching, so there is no need to specify anything more than how many documents to retrieve. </p> 
  </div> 
  <div class="readable-text" id="p132"> 
   <p>Second, we need to combine the questions and documents into a query for the LLM. In DSPy, the prompt is abstracted away from us. Instead, we write what DSPy calls a <em>signature</em>, which you can think of as the inputs and outputs of a function. These should be given meaningful English names so that DSPy can generate a good prompt for you. (Under the hood, DSPy uses a language model to optimize prompts!) In this case, we have two inputs (<code>question</code> and <code>relevant_documents</code>) separated by a comma. The <code>-&gt;</code> is used to denote the start of the outputs, of which we have only one: the <code>answer</code> to the question. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p133"> 
   <p> <span class="print-book-callout-head">Note</span> DSPy supports some basic types in signatures. For example, you can enforce that the answer must be an integer by denoting <code>"question, relevant</code><code>_documents -&gt; answer:int"</code> in the string. This command will apply the same technique to regenerating on errors that we just learned about in figure <a href="#fig__force_output">5.10</a>. </p> 
  </div> 
  <div class="readable-text" id="p134"> 
   <p>That is all it takes to define our RAG model! The objects are called and passed out in the <code>forward</code> function, but you can modify this code to add additional details if you want. You can convert everything to lowercase, run a spell checker, or use whatever kind of code you want here. This approach lets you mix and match programming rules with LLMs.</p> 
  </div> 
  <div class="readable-text" id="p135"> 
   <p>You can also easily modify the RAG definition to include new constraints and write code to have an LLM perform validation. More importantly, DSPy supports using a training/validation set to tune the prompts better, fine-tune local LLMs, and help you create an empirically tested, improved, and quantified model to achieve your goals without having to spend a lot of time on LLM-specific details. Adopting tools like this early will give you a far more robust solution that allows you to upgrade to newer architectures more easily.</p> 
  </div> 
  <div class="readable-text" id="p136"> 
   <h2 class=" readable-text-h2" id="summary">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p137">You can intervene to change a model’s behavior in four places: the datacollection/tokenization, training the initial base model, fine-tuning the base model, and intercepting the predicted tokens. All four places are important, but fine-tuning is the most effective place for most users to make changes that lower the cost and provide the optimal ability to change the model’s goals.</li> 
   <li class="readable-text" id="p138">Supervised fine-tuning (SFT) performs the normal training process on a smaller bespoke data collection and is useful for refining the model’s knowledge of a particular domain.</li> 
   <li class="readable-text" id="p139">Reinforcement learning from human feedback (RLHF) requires more data, but it allows us to specify objectives more complex than “predict the next token.”</li> 
   <li class="readable-text" id="p140">You can use existing tools like syntax checkers to detect incorrect LLM outputs in cases where the output format must be strict, such as for JSON or XML. Generation and syntax checking can be run in a loop until the output satisfies the necessary syntax constraints.</li> 
   <li class="readable-text" id="p141">Retrieval augmented generation (RAG) is a popular method of augmenting the input of an LLM by first finding relevant content via a search engine or database and then inserting it into the prompt.</li> 
   <li class="readable-text" id="p142">Coding frameworks like DSPy are beginning to emerge that separate the specific LLM, vectorization, and prompt definition from the logic of how inputs and outputs from the LLM are modified for a specific task. This method allows you to build more reliable and repeatable LLM solutions that can quickly adapt to new models and methods.</li> 
  </ul>
 </body></html>