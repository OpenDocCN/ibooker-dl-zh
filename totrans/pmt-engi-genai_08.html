<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 8. Standard Practices for Image Generation with Midjourney"><div class="chapter" id="standard_image_08">
<h1><span class="label">Chapter 8. </span>Standard Practices for Image Generation with Midjourney</h1>


<p>In this chapter, you’ll use standardized techniques to maximize the output and formats from diffusion models. You’ll start by tailoring the prompts to explore all of the common practices used for image generation. All images are generated by Midjourney v5, unless otherwise noted. The techniques discussed were devised to be transferrable to any future or alternative model.</p>






<section data-type="sect1" data-pdf-bookmark="Format Modifiers"><div class="sect1" id="id184">
<h1>Format Modifiers</h1>

<p>The most basic practice in image generation <a data-type="indexterm" data-primary="image generation" data-secondary="Midjourney" data-tertiary="format modifiers" id="mggmjry"/><a data-type="indexterm" data-primary="Midjourney" data-secondary="format modifiers" id="mdjyfd"/><a data-type="indexterm" data-primary="format" data-secondary="modifiers" id="fmmdfr"/>is to specify the format of the image. AI image models are capable of deploying a wide variety of formats, from stock photo, to oil paintings to ancient Egpytian hieroglyphics. The image often looks completely different depending on the format, including the style of the objects or people generated in the image. Many of the images in the training data are stock photos, and this is also one of the most commercially important image categories for image generation.</p>

<p id="format_modifiers_stock_photo">Input:</p>

<pre data-type="programlisting">a stock photo of a business meeting</pre>

<p><a data-type="xref" href="#figure-8-1">Figure 8-1</a> shows the output.</p>

<figure><div id="figure-8-1" class="figure">
<img src="assets/pega_0801.png" alt="pega 0801" width="600" height="600"/>
<h6><span class="label">Figure 8-1. </span>Stock photo of a business meeting</h6>
</div></figure>

<p>The ability to generate infinite royalty-free stock photos for free with open source models like Stable Diffusion, or for a very low cost with DALL-E or Midjourney, is itself a game changer. Each of these images is unique (though may contain similarities to existing images), and therefore they look more premium than reusing the same free stock photos available to everyone else. However, you no longer need to be limited to the stock photography format. If your blog post or website imagery would look better with something more artistic, you can do that with essentially no limits.</p>

<p id="format_modifiers_oil_painting">Input:</p>

<pre data-type="programlisting">an oil painting of a business meeting</pre>

<p class="pagebreak-before"><a data-type="xref" href="#figure-8-2">Figure 8-2</a> shows the output.</p>

<figure><div id="figure-8-2" class="figure">
<img src="assets/pega_0802.png" alt="pega 0802" width="600" height="600"/>
<h6><span class="label">Figure 8-2. </span>Oil painting of a business meeting</h6>
</div></figure>
<div data-type="tip"><h1>Specify Format</h1>
<p>The format we specify significantly <a data-type="indexterm" data-primary="Specify Format principle" data-secondary="format modifiers" id="id1194"/>modifies the results we get from our AI model. Specifying format also improves the reliability of our prompts in terms of giving us the type of visual we require.</p>
</div>

<p>There’s no real limit to how far you can take this technique, and this is one of those domains where it would have helped to go to art school. If you know the name of a specific technique or detail you want to see in your image—for example, impasto, a technique used in oil painting, where paint is laid on an area of the surface thickly, leaving visible brush strokes—you can reference it in the prompt to get closer to your desired result. Google maintains a comprehensive list of popular <a href="https://oreil.ly/OmZbl">artists and art movements</a> that many find useful.</p>

<p class="pagebreak-before" id="format_modifiers_impasto">Input:</p>

<pre data-type="programlisting">an oil painting of a business meeting, textured oil-on-canvas
using thick impasto and swirling dynamic brushstrokes</pre>

<p><a data-type="xref" href="#figure-8-3">Figure 8-3</a> shows the output.</p>

<figure><div id="figure-8-3" class="figure">
<img src="assets/pega_0803.png" alt="pega 0803" width="600" height="600"/>
<h6><span class="label">Figure 8-3. </span>Oil painting of a business meeting with impasto</h6>
</div></figure>

<p>The oil painting of a business meeting is now far more visually interesting and potentially more appealing, depending on your audience. Traditionally, one of the reasons businesses migrated to using stock photography is that it was cheaper than commissioning a painting, but that limitation no longer applies with AI. We can generate essentially any format we like, for example an ancient Egyptian hieroglyph of a business meeting.</p>

<p id="format_modifiers_egyptian_hieroglyph">Input:</p>

<pre data-type="programlisting">an ancient Egyptian hieroglyph of a business meeting</pre>

<p class="pagebreak-before"><a data-type="xref" href="#figure-8-4">Figure 8-4</a> shows the output.</p>

<figure><div id="figure-8-4" class="figure">
<img src="assets/pega_0804.png" alt="pega 0804" width="600" height="600"/>
<h6><span class="label">Figure 8-4. </span>Ancient Egyptian hieroglyph of a business meeting</h6>
</div></figure>

<p>The thing to watch out for with modifying the format is that the style of the image, and even the contents, tend to match what was associated with that format in the training data. For example, in our oil painting there aren’t any computers, because they don’t often appear in oil paintings. Similarly in our hieroglyph the participants in the meeting are wearing ancient Egyptian headdresses. Often you’ll need to combine format modifiers with the other proceeding techniques in order to <a data-type="indexterm" data-primary="image generation" data-secondary="Midjourney" data-tertiary="format modifiers" data-startref="mggmjry" id="id1195"/><a data-type="indexterm" data-primary="Midjourney" data-secondary="format modifiers" data-startref="mdjyfd" id="id1196"/><a data-type="indexterm" data-primary="format" data-secondary="modifiers" data-startref="fmmdfr" id="id1197"/>arrive at what you want.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Art Style Modifiers"><div class="sect1" id="id115">
<h1>Art Style Modifiers</h1>

<p>One of the great powers of AI image models is <a data-type="indexterm" data-primary="image generation" data-secondary="Midjourney" data-tertiary="art style modifiers" id="mgdjsy"/><a data-type="indexterm" data-primary="Midjourney" data-secondary="art style modifiers" id="mjytymd"/><a data-type="indexterm" data-primary="art style modifiers" id="arsydf"/>their ability to replicate any popular art style or artist. The most common examples shared on social media and AI demos are images in the style of Van Gogh, Dali, or Picasso, as well as the art movements they were part of, respectively Post-impressionism, Surrealism, and Cubism. AI art communities have also become influential in determining what contemporary art styles become popular, as is the case with Polish digital artist <a href="https://oreil.ly/nnam3">Greg Rutkowski</a>, known for his fantasy style. However, many artists have taken a stand against AI art, and there is a legal gray area around whether imitating a living artist’s style is considered <em>fair use</em> under copyright law. We recommend AI artists exercise caution when generating AI art in the distinctive style of any living artist and instead stick to artists who died one year ago as a rule of thumb (seek legal counsel for any planned commercial use).</p>

<p id="art_style_modifiers_lewis_carroll">Input:</p>

<pre data-type="programlisting">illustration of a dragon, in the style of Alice's Adventures in Wonderland
by Lewis Carroll</pre>

<p><a data-type="xref" href="#figure-8-5">Figure 8-5</a> shows the output.</p>

<figure><div id="figure-8-5" class="figure">
<img src="assets/pega_0805.png" alt="pega 0805" width="600" height="600"/>
<h6><span class="label">Figure 8-5. </span>Illustration of a dragon, in the style of Lewis Carroll</h6>
</div></figure>
<div data-type="tip"><h1>Give Direction</h1>
<p>Evoking an artist’s name or the name <a data-type="indexterm" data-primary="Give Direction principle" data-secondary="art style modifiers" id="id1198"/>of an art movement is a shortcut toward delivering a specific visual style. So long as the artist or art movement has enough examples in the training data, their nature can be emulated.</p>
</div>

<p>In evoking an artist’s style you’re effectively shortcutting to a part of the <em>latent space</em>, the multidimensional universe of potential model outputs, filtering down to your desired style. Traversing <a data-type="indexterm" data-primary="image generation" data-secondary="Midjourney" data-tertiary="art style modifiers" data-startref="mgdjsy" id="id1199"/><a data-type="indexterm" data-primary="Midjourney" data-secondary="art style modifiers" data-startref="mjytymd" id="id1200"/><a data-type="indexterm" data-primary="art style modifiers" data-startref="arsydf" id="id1201"/>to nearby locations from there can help you arrive at a more pleasing destination than you could get to with random trial and error.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Reverse Engineering Prompts"><div class="sect1" id="id116">
<h1>Reverse Engineering Prompts</h1>

<p>If you didn’t go to art school or don’t <a data-type="indexterm" data-primary="reverse engineering prompts" id="rvgpp"/><a data-type="indexterm" data-primary="Midjourney" data-secondary="Describe" id="id1202"/>know much about film or photography, it can be daunting to try to figure out the art styles, formats, and artists you want to take advantage of. Often you see a picture you like and have no way of describing it in enough detail to re-create it with a prompt. Thankfully, Midjourney’s <code>Describe</code> functionality allows you to reverse engineer a prompt from an image by typing <span class="keep-together"><code><strong>/describe</strong></code></span> and then uploading the image. It works for both AI-generated images and also normal images from other sources, too, as shown in <a data-type="xref" href="#figure-8-6">Figure 8-6</a>, using one of the stock photos from <a data-type="xref" href="ch01.html#five_principles_01">Chapter 1</a>.</p>

<p>Midjourney gives you four options with various artists, art styles, modifiers, and other words, including an estimation of what is happening in the image and what subjects or elements are contained. For example, Midjourney correctly identifies a group of people looking at a laptop in an office, in <a data-type="xref" href="#figure-8-6">Figure 8-6</a>. You can select the option you want by number, and Midjourney will generate an image with that prompt, in the same style as the original. There is similar open source technology available named <a href="https://oreil.ly/fzgno">CLIP Interrogator</a>, though <a data-type="indexterm" data-primary="CLIP Interrogator" id="id1203"/>the richness of the prompt and ability to replicate the style of the uploaded image is lacking compared to Midjourney.</p>

<figure><div id="figure-8-6" class="figure">
<img src="assets/pega_0806.png" alt="pega 0806" width="558" height="800"/>
<h6><span class="label">Figure 8-6. </span>Midjourney Describe, <a href="https://oreil.ly/GdNrt">Mimi Thian</a> on <a href="https://oreil.ly/bEEnJ">Unsplash</a></h6>
</div></figure>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Quality Boosters"><div class="sect1" id="id117">
<h1>Quality Boosters</h1>

<p>One trick that works for image models is <a data-type="indexterm" data-primary="images" data-secondary="quality boosters" id="mgqbyb"/><a data-type="indexterm" data-primary="quality boosters" id="qtybroo"/>to add words that are associated with quality into the prompt. Some art styles are more aesthetic than others, but there is a set of words, known as <em>quality boosters</em>, that seem to improve the image quality without greatly affecting the style, like <em>4k</em>, <em>very beautiful</em>, and <em>trending on artstation</em>. Generative models aren’t trying to make high-quality images; they’re trying to imitate training sets with a wide variety of styles and qualities. If you want high-quality images, you must explicitly ask for them. Start with the subject of your prompt, for example a space whale, and add a modifier to the end, separated by a comma (as in <a data-type="xref" href="#figure-8-7">Figure 8-7</a>).</p>

<p id="art_style_modifiers_artstation">Input:</p>

<pre data-type="programlisting">a space whale, trending on artstation</pre>

<p>Output:</p>

<figure><div id="figure-8-7" class="figure">
<img src="assets/pega_0807.png" alt="pega 0807" width="600" height="353"/>
<h6><span class="label">Figure 8-7. </span>Space whale, trending on artstation</h6>
</div></figure>
<div data-type="tip"><h1>Give Direction</h1>
<p>Using quality boosters can help improve the <a data-type="indexterm" data-primary="Give Direction principle" data-secondary="quality boosters" id="id1204"/>aesthetics of an image through the addition of one or two words to the prompt, without changing the overall style of the image by much.</p>
</div>

<p>The reason these labels work is that they were associated with quality in the training data. When AI image models were trained, they reportedly ingested images from popular design portfolio websites, such as ArtStation, Behance, and DeviantArt. Therefore, the model can approximate that an image that was “trending on artstation” was of higher aesthetic value than normal. Note that sometimes style seeps through that may not be aligned with your creative vision. For example, ArtStation contains a lot of digital art of spaceships, and that perhaps explains why the space whale in <a data-type="xref" href="#figure-8-7">Figure 8-7</a> somewhat resembles a space ship. For a list of quality boosters, art styles, and artists, visit this template created by one of the authors: <a href="https://oreil.ly/afGCQ">Prompt Engineering Template</a>. Google also compiles a comprehensive list of <a href="https://oreil.ly/mhujK">art movements</a>, which can be useful for educating yourself on the names of <a data-type="indexterm" data-primary="images" data-secondary="quality boosters" data-startref="mgqbyb" id="id1205"/><a data-type="indexterm" data-primary="quality boosters" data-startref="qtybroo" id="id1206"/>styles you find appealing.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Negative Prompts"><div class="sect1" id="id118">
<h1>Negative Prompts</h1>

<p>Often two concepts are so <a data-type="indexterm" data-primary="negative prompts" id="ngtvpp"/><a data-type="indexterm" data-primary="image generation" data-secondary="negative prompts" id="igggvpp"/>intertwined in the training data that they appear together frequently when generating images of one of the concepts, even if that’s not what you specified or intended. For example when you ask for oil paintings, you often get the accompanying frame and surrounding wall, because that’s what’s in the images for a large number of museum collections of these paintings.</p>

<p>In Midjourney and Stable Diffusion there <a data-type="indexterm" data-primary="Midjourney" data-secondary="negative prompts" id="mdjgpp"/><a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="negative prompts" id="sdffgvp"/>is the ablity to add <em>negative prompts</em>, which allow you to specify what you don’t want in the image. Negative prompts can be used to effectively separate two intertwined concepts and ensure your image doesn’t contain anything you were hoping to avoid. Taking the example of oil paintings and frames, you can add <code>--no</code> to the end of the prompt, and anything in a comma-separated list after that flag will be negated from the prompt. To fix your frames problem, add “frame” and “wall” as a negative prompt, as shown in <a data-type="xref" href="#figure-8-8">Figure 8-8</a>.</p>

<p id="negative_prompts_rembrandt">Input:</p>

<pre data-type="programlisting">oil painting in the style of Rembrandt --no frame, wall</pre>

<p><a data-type="xref" href="#figure-8-8">Figure 8-8</a> shows the output.</p>

<figure><div id="figure-8-8" class="figure">
<img src="assets/pega_0808.png" alt="pega 0808" width="600" height="389"/>
<h6><span class="label">Figure 8-8. </span>Oil painting in the style of Rembrandt without frame or wall</h6>
</div></figure>
<div data-type="tip"><h1>Give Direction</h1>
<p>Negative prompts can negate unwanted concepts <a data-type="indexterm" data-primary="Give Direction principle" data-secondary="negative prompts" id="id1207"/>in images, directing the model away from areas you are trying to avoid. This doesn’t always work as intended, as often the concepts are too well correlated, but when it does, it can lead to interesting places.</p>
</div>

<p>Negative prompting isn’t fully reliable but can be useful in a wide variety of scenarios. One creative use of this technique is to add the name of a celebrity as a negative prompt to decrease the factors most associated with them. The famous actress Karen Gillan has red hair and has a conventionally feminine look and therefore can be used to make a subject less likely to have red hair or look conventionally feminine.</p>

<p id="decreasechancesRed">Input:</p>

<pre data-type="programlisting">a Scottish female astronaut --no Karen Gillan</pre>

<p><a data-type="xref" href="#figure-8-9">Figure 8-9</a> shows the output.</p>

<figure><div id="figure-8-9" class="figure">
<img src="assets/pega_0809.png" alt="pega 0809" width="600" height="600"/>
<h6><span class="label">Figure 8-9. </span>A less conventionally feminine, less red-haired Scottish female astronaut</h6>
</div></figure>

<p>You can also get very creative and unpredictable outcomes with this technique by taking two inseparable concepts and seeing what happens when you separate them. For example, try taking your favorite cartoon and removing the cartoon style, as depicted in <a data-type="xref" href="#figure-8-10">Figure 8-10</a> with Homer Simpson.</p>

<p id="negative_prompts_homer">Input:</p>

<pre data-type="programlisting">Homer Simpson --no cartoon</pre>

<p><a data-type="xref" href="#figure-8-10">Figure 8-10</a> shows the (horrifying) output.</p>

<figure><div id="figure-8-10" class="figure">
<img src="assets/pega_0810.png" alt="pega 0810" width="600" height="600"/>
<h6><span class="label">Figure 8-10. </span>Homer Simpson without his trademark cartoon style</h6>
</div></figure>

<p>One of the more common historical use cases for negative prompts traditionally is to correct some of the issues with disfigured hands, explicit body parts, or odd-looking eyes, all problems early AI models suffered from. Prompt engineers would add words like <em>nsfw</em>, <em>elongated body</em>, <em>too many digits</em>, <em>not enough fingers</em>, and <em>teeth</em> to the negative prompt in an attempt (often in vain) to guide the model away from these spaces.</p>

<p>While still necessary for older or lesser models like Stable Diffusion v1.5, from Midjourney v5 and Stable Diffusion XL onward this is mostly a solved problem. State-of-the-art models are now more than capable of developing normal-looking <a data-type="indexterm" data-primary="image generation" data-secondary="negative prompts" data-startref="igggvpp" id="id1208"/><a data-type="indexterm" data-primary="negative prompts" data-startref="ngtvpp" id="id1209"/><a data-type="indexterm" data-primary="Midjourney" data-secondary="negative prompts" data-startref="mdjgpp" id="id1210"/><a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="negative prompts" data-startref="sdffgvp" id="id1211"/>images of hands, eyes, and bodies without relying on negative prompts.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Weighted Terms"><div class="sect1" id="id119">
<h1>Weighted Terms</h1>

<p>Negative prompts are useful if you want <a data-type="indexterm" data-primary="weighted terms" id="wghttm"/><a data-type="indexterm" data-primary="image generation" data-secondary="weighted terms" id="imggwgm"/>to completely negate something, but often you just want to dial it down. To mix and match different concepts, it can be helpful to have control over how much of each you want.</p>

<p class="pagebreak-before">By default all words in a prompt have an equal weighting of 1, although words at the beginning of the prompt have a greater effect, which is why we typically put the subject of our image there by convention, i.e., <code>painting of the Golden Gate Bridge</code>. You can change the weights of sections of the prompt in Midjourney by adding a <em>hard break</em> with two colon characters, <code>::</code>, and a number denoting the new weight. With this method you can make an image that is primarily Van Gogh but with a dash of Dali.</p>

<p id="weighted_terms_golden_gate_bridge">Input:</p>

<pre data-type="programlisting">painting of the Golden Gate Bridge::1 in the style of Van
Gogh::0.8, in the style of Dali::0.2</pre>

<p><a data-type="xref" href="#figure-8-11">Figure 8-11</a> shows the output.</p>

<figure><div id="figure-8-11" class="figure">
<img src="assets/pega_0811.png" alt="pega 0811" width="600" height="600"/>
<h6><span class="label">Figure 8-11. </span>Painting of the Golden Gate Bridge in the style of Van Gogh and Dali</h6>
</div></figure>

<p>To see how weights affect the resulting image, you can conduct a grid search by systematically testing each combination of weights at a specific granularity. In this example, the weights changed in 0.2 increments between the two artists from 0 to 1.</p>

<p><a data-type="xref" href="#figure-8-12">Figure 8-12</a> shows the output.</p>

<figure><div id="figure-8-12" class="figure">
<img src="assets/pega_0812.png" alt="pega 0812" width="600" height="522"/>
<h6><span class="label">Figure 8-12. </span>Permutations grid of weights</h6>
</div></figure>
<div data-type="tip"><h1>Evaluate Quality</h1>
<p>Weights introduce many possible <a data-type="indexterm" data-primary="Evaluate Quality principle" data-secondary="weighted terms" id="id1212"/>combinations in a prompt, which can be time-consuming to iterate through one at a time. A grid search approach of systematically generating many possible combinations is recommended to identify where the ideal mix of weightings aligns with your preferences.</p>
</div>

<p>Weights can go higher than 1 as needed for emphasis, or lower if you want to de-emphasize something. You can also add negative weights to the prompt to remove that aspect to varying degress. The <code>--no</code> parameter used for negative prompts is actually just a shortcut for adding <code>::-0.5</code> to that section of the prompt. If you are struggling with something appearing in the image that you don’t want, try stronger negative weights instead of negative prompts. Using the prior example, you could strip any Van Gogh influence out of Dali’s work by adding a -1 weight to Van Gogh and dialing up the Dali weight to 5.</p>

<p class="pagebreak-before" id="negative_weighted_terms">Input:</p>

<pre data-type="programlisting">painting of the Golden Gate Bridge::1 in the style of Van
Gogh::-1, in the style of Dali::5</pre>

<p><a data-type="xref" href="#figure-8-13">Figure 8-13</a> shows the output.</p>

<figure><div id="figure-8-13" class="figure">
<img src="assets/pega_0813.png" alt="pega 0813" width="600" height="600"/>
<h6><span class="label">Figure 8-13. </span>Painting of the Golden Gate Bridge</h6>
</div></figure>

<p>Weights can be a powerful tool in remixing different styles or emphasizing specific elements. There are many permutations of weights, and therefore a more systematic approach must be taken to find an <a data-type="indexterm" data-primary="weighted terms" data-startref="wghttm" id="id1213"/><a data-type="indexterm" data-primary="image generation" data-secondary="weighted terms" data-startref="imggwgm" id="id1214"/>aesthetically interesting space to play in.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Prompting with an Image"><div class="sect1" id="id185">
<h1>Prompting with an Image</h1>

<p>Many AI image generation tools let you <a data-type="indexterm" data-primary="images" data-secondary="prompting with" id="mgppw"/><a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="Img2Img" id="sbdfgig"/><a data-type="indexterm" data-primary="Midjourney" data-secondary="prompting with images" id="mdjyppg"/>prompt the model not just with text but with an image. Supplying an example image of what you’re going for can give you a great baseline for building something more unique and original, while still matching the style you need. In the Stable Diffusion community this is called Img2Img, whereas in Midjourney you simply link to an image in the prompt. The best way to do this is to upload the image into Discord first (for example, this photo by <a href="https://oreil.ly/B6E0Y">Jessica Hearn</a> on <a href="https://oreil.ly/0oO4w">Unsplash</a>), and then right-click and select Copy Link, as depicted in <a data-type="xref" href="#figure-8-14">Figure 8-14</a>. You can later paste that link into the prompt to use as the base image.</p>

<figure><div id="figure-8-14" class="figure">
<img src="assets/pega_0814.png" alt="pega 0814" width="600" height="555"/>
<h6><span class="label">Figure 8-14. </span>Copying the link from an image uploaded to Discord</h6>
</div></figure>

<p>The copied link then should be pasted at the beginning of the Midjourney prompt and accompanied by your text prompt. You don’t need to be as descriptive now that you have given a base image (a picture is worth a thousand words). The image won’t match exactly, but it will be similar to the point of being recognizeable if you know what image was supplied and how it was modified by the prompt.</p>

<p id="prompting_with_image_midjourney">Input:</p>

<pre data-type="programlisting">https://s.mj.run/XkIHsYIdUxc in the style of The Great Gatsby</pre>

<p><a data-type="xref" href="#figure-8-15">Figure 8-15</a> shows the output.</p>

<figure><div id="figure-8-15" class="figure">
<img src="assets/pega_0815.png" alt="pega 0815" width="600" height="600"/>
<h6><span class="label">Figure 8-15. </span>Stock photo in the style of <span class="plaintext">The Great Gatsby</span></h6>
</div></figure>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>The rules and regulations around copyright and fair use with AI are still being developed, so be careful uploading any image you don’t have the rights to.</p>
</div>

<p>This technique works wherever you want a similar vibe, scene, or composition to an image you know of. You can also blend multiple images together with <code>/blend</code> to get something quite unique, and even use the resulting image as input for another prompt. For convenience, there is the <code>--iw</code> parameter, which acts the same as separating the image from the rest of the prompt with <code>::</code> and setting the weight. Prompting with an image is also a common technique for AI video generation with tools such as <a href="https://runwayml.com">RunwayML</a> and <a href="https://pika.art">Pika Labs</a>, given the general unreliability of text to video generation, and because it gives you an opportunity to iterate on the style of the scene without waiting for a whole video to generate and render.</p>
<div data-type="tip"><h1>Provide Examples</h1>
<p>The quickest and easiest way to get the image <a data-type="indexterm" data-primary="images" data-secondary="prompting with" data-startref="mgppw" id="id1215"/><a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="Img2Img" data-startref="sbdfgig" id="id1216"/><a data-type="indexterm" data-primary="Midjourney" data-secondary="prompting with images" data-startref="mdjyppg" id="id1217"/><a data-type="indexterm" data-primary="Provide Examples principle" data-secondary="prompting with images" id="id1218"/>you desire is to upload an image that you want to emulate. This is similar in concept to a one-shot prompt in the text generation space and is similarly useful in guiding the model toward the right output.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Inpainting"><div class="sect1" id="id120">
<h1>Inpainting</h1>

<p>Working with AI image generation tools <a data-type="indexterm" data-primary="image generation" data-secondary="inpainting" id="iminpt"/><a data-type="indexterm" data-primary="inpainting" id="ipnntg"/>is always an iterative process. Rarely do you get the complete final image on the first try. There are usually artifacts that you want to address, or styles that you want to change. For example, say you had generated an image with Midjourney of a woman in a 1920s-style flapper dress but wanted to change what she was wearing without regenerating the entire image.</p>

<p>The solution is <em>inpainting</em>, which is <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="inpainting" id="id1219"/>available in most implementations of <a href="https://oreil.ly/YgL8g">Stable Diffusion</a>, in <a href="https://oreil.ly/7DhZE">Midjourney</a> via a feature called <em>Vary Region</em>, and with <a href="https://oreil.ly/FvGAi">Adobe Photoshop’s</a> <em>Generative Fill</em>. However, DALL-E <a data-type="indexterm" data-primary="DALL-E" data-secondary="inpainting" id="dllptg"/>pioneered this functionality, and it is still our personal preference in terms of the quality of the results. To demonstrate this functionality, first you generate an image with DALL-E in in ChatGPT (Plus), and then you erase the part of the image you want to regenerate.</p>

<p><a data-type="xref" href="#figure-8-16">Figure 8-16</a> shows an image generated by DALL-E with the accompanying prompt below. It is in the process of being edited in DALL-E’s inpainting canvas (currently using ChatGPT) and has had the dress part of the image erased using the inpainting brush, ready for inpainting.</p>

<p id="flapper_dress">Input:</p>

<pre data-type="programlisting">photograph of glamorous woman in a 1920s flapper party,
wearing a sequin dress, wide angle, in color, 3 5 mm, dslr</pre>

<p><a data-type="xref" href="#figure-8-16">Figure 8-16</a> shows the uploaded image with parts erased.</p>

<figure><div id="figure-8-16" class="figure">
<img src="assets/pega_0816.png" alt="pega 0816" width="600" height="341"/>
<h6><span class="label">Figure 8-16. </span>Inpainting in DALL-E</h6>
</div></figure>

<p>Then you add a prompt for what you want to generate within that space. The common advice is to prompt for what you want the whole image to be, but in our experience narrowing down the prompt to just what you want in the erased part of the image gets better results, as in <a data-type="xref" href="#figure-8-17">Figure 8-17</a>, which focuses on the dress itself.</p>

<p id="inpainting_flapper_dress">Input:</p>

<pre data-type="programlisting">Van Gogh style dress, inspired by Starry Night, blue and
yellow swirls, extremely detailed, very well lit, studio
light, 3.5 mm, dslr</pre>

<p><a data-type="xref" href="#figure-8-17">Figure 8-17</a> shows the output.</p>

<figure><div id="figure-8-17" class="figure">
<img src="assets/pega_0817.png" alt="pega 0817" width="600" height="342"/>
<h6><span class="label">Figure 8-17. </span>Van Gogh-style dress</h6>
</div></figure>
<div data-type="tip"><h1>Divide Labor</h1>
<p>It’s important to choose the right model for <a data-type="indexterm" data-primary="Divide Labor principle" data-secondary="inpainting" id="id1220"/>the job. Some image models like Midjourney are good at generating images in a specific style or with a certain aesthetic, while others compete on advanced features like DALL-E’s inpainting. Using multiple models together can expand the scope of what you can accomplish.</p>
</div>

<p>DALL-E has fewer features than most image models but is great at this specific technique. It automatically blends the edges in so that the image fits well with the surroundings. As such you don’t need to be particulary precise with the erase brush, and you’ll still get good results. What’s remarkable is how much these models have progressed in the space of just over a year. <a data-type="xref" href="#figure-8-18">Figure 8-18</a> shows what you would get if you prompted DALL-E with the same prompt used earlier for DALL-E 3 via ChatGPT.</p>

<figure><div id="figure-8-18" class="figure">
<img src="assets/pega_0818.png" alt="pega 0818" width="600" height="600"/>
<h6><span class="label">Figure 8-18. </span>Photograph of woman in a 1920s flapper party</h6>
</div></figure>

<p>DALL-E 3 provides superior quality, but at present, it is available only via API and in ChatGPT; it is not available in the OpenAI Labs interface, which was historically used for inpainting. As image models proliferate, they are diverging in functionality and use cases, and it may take more than one model used in combination to accomplish the task you have at hand. Inpainting is a powerful technique for editing images, whether those images come from <a data-type="indexterm" data-primary="image generation" data-secondary="inpainting" data-startref="iminpt" id="id1221"/><a data-type="indexterm" data-primary="inpainting" data-startref="ipnntg" id="id1222"/><a data-type="indexterm" data-primary="DALL-E" data-secondary="inpainting" data-startref="dllptg" id="id1223"/>other AI models or a real-life photographer.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Outpainting"><div class="sect1" id="id121">
<h1>Outpainting</h1>

<p>Related to inpainting in DALL-E is outpainting, where <a data-type="indexterm" data-primary="image generation" data-secondary="outpainting" id="iggntpt"/><a data-type="indexterm" data-primary="outpainting" id="otptg"/>you generate outside of the frame of the existing image. This technique can in effect <em>zoom out</em> from the existing image to add context around it. This can be used to fill in more detail in an image you have generated or uploaded. Outpainting is no longer available in OpenAI’s labs interface and is not yet available in ChatGPT, but it is called Zoom Out in Midjourney and presents itself as an option for images that have been upscaled, as you can see in <a data-type="xref" href="#figure-8-19">Figure 8-19</a>, where the surroundings of a woman in the flapper dress have been revealed.</p>

<figure><div id="figure-8-19" class="figure">
<img src="assets/pega_0819.png" alt="pega 0819" width="600" height="569"/>
<h6><span class="label">Figure 8-19. </span>Midjourney Zoom Out options</h6>
</div></figure>

<p id="outpainting_flapper_dress">Input:</p>

<pre data-type="programlisting">photograph of glamorous woman in a 1920s flapper party,
wearing a sequin dress, wide angle, in color, 3 5 mm, dslr</pre>

<p><a data-type="xref" href="#figure-8-20">Figure 8-20</a> shows the output.</p>
<div data-type="tip"><h1>Provide Examples</h1>
<p>Often it can be difficult to achieve the <a data-type="indexterm" data-primary="Provide Examples principle" data-secondary="outpainting" id="id1224"/>right style purely with text prompts, particularly if the style is nuanced or you don’t know all the words to describe it. Providing an example of an image to use for inpainting or outpainting is a shortcut to better results.</p>
</div>

<figure><div id="figure-8-20" class="figure">
<img src="assets/pega_0820.png" alt="pega 0820" width="600" height="316"/>
<h6><span class="label">Figure 8-20. </span>Midjourney image before and after zoom</h6>
</div></figure>

<p>As well as creatively expanding on an existing image, outpainting is also helpful if you’re trying to get an image in an aspect ratio other than square, by filling in the gaps. You can run a Custom Zoom and set an aspect ratio as well as prompting what you want in each new section of the image through trial and error until you find something consistent with the rest of the image, or until the full image is in the aspect ratio required (for example, going from portrait to landscape). This technique is also available as <a data-type="indexterm" data-primary="image generation" data-secondary="outpainting" data-startref="iggntpt" id="id1225"/><a data-type="indexterm" data-primary="outpainting" data-startref="otptg" id="id1226"/>an extension in <a href="https://oreil.ly/0c_en">Stable Diffusion</a>, but in our experience it’s less reliable than Midjourney.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Consistent Characters"><div class="sect1" id="id122">
<h1>Consistent Characters</h1>

<p>An underrated use of inpainting and outpainting <a data-type="indexterm" data-primary="image generation" data-secondary="consistent characters" id="mggrcss"/><a data-type="indexterm" data-primary="outpainting" data-secondary="consistent characters and" id="opcsst"/><a data-type="indexterm" data-primary="inpainting" data-secondary="consistency and" id="ipcsscy"/><a data-type="indexterm" data-primary="consistent characters" id="cssty"/>is using an existing image to maintain consistency across generations. One such example is a common method for creating <a href="https://oreil.ly/BaITC">consistent characters</a> by generating two images side by side and inpainting one side at a time. First, generate an image while explicitly dictating that there are two images side by side, in a 2:1 aspect ratio.</p>

<p id="consistent_characters">Input:</p>

<pre data-type="programlisting">two images side by side, rockstar American rough and ready
middle-age jawline actor man, photo booth portrait --ar 2:1</pre>

<p><a data-type="xref" href="#figure-8-21">Figure 8-21</a> shows the output.</p>

<figure><div id="figure-8-21" class="figure">
<img src="assets/pega_0821.png" alt="pega 0821" width="600" height="300"/>
<h6><span class="label">Figure 8-21. </span>Midjourney consistent character</h6>
</div></figure>

<p>The next step is to upscale one of the images, and then mask one-half of the upscaled image in an inpainting canvas, shown in <a data-type="xref" href="#figure-8-22">Figure 8-22</a> using the Midjourney Vary Region feature.</p>

<figure><div id="figure-8-22" class="figure">
<img src="assets/pega_0822.png" alt="pega 0822" width="600" height="391"/>
<h6><span class="label">Figure 8-22. </span>Midjourney Vary Region</h6>
</div></figure>

<p>Finally, reprompt the masked part of the image using inpainting or Vary Region (as it’s known in Midjourney) to dictate a different angle from the original portrait mode.</p>

<p class="pagebreak-before">Input:</p>

<pre data-type="programlisting">side profile image, rockstar American rough and ready
middle-age jawline actor man, photo booth portrait --ar 2:1</pre>

<p><a data-type="xref" href="#figure-8-23">Figure 8-23</a> shows the output.</p>

<figure><div id="figure-8-23" class="figure">
<img src="assets/pega_0823.png" alt="pega 0823" width="600" height="300"/>
<h6><span class="label">Figure 8-23. </span>Consistent characters in side profile</h6>
</div></figure>

<p>This inpainting and generation process can be repeated for multiple angles, with the express purpose of finding new images of a character that looks identical to the original one you generated. Because one-half of the image is always present, the model maintains consistency of the character’s features across generations, allowing you to build a more comprehensive perspective of a single character in different poses and positions. All you need to do to create an image of the character in a new situation is inpaint half of the 2:1 image with the new prompt and crop it in Photoshop (or some equivalent).</p>
<div data-type="tip"><h1>Provide Examples</h1>
<p>Many people think of using a real image as a <a data-type="indexterm" data-primary="image generation" data-secondary="consistent characters" data-startref="mggrcss" id="id1227"/><a data-type="indexterm" data-primary="outpainting" data-secondary="consistent characters and" data-startref="opcsst" id="id1228"/><a data-type="indexterm" data-primary="inpainting" data-secondary="consistency and" data-startref="ipcsscy" id="id1229"/><a data-type="indexterm" data-primary="consistent characters" data-startref="cssty" id="id1230"/><a data-type="indexterm" data-primary="Provide Examples principle" data-secondary="consistent characters" id="id1231"/>baseline when prompting for inpainting, but many of the more advanced AI artists use generated images themselves as inputs to maintain control over the consistency of the characters or objects in their story.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Prompt Rewriting"><div class="sect1" id="id123">
<h1>Prompt Rewriting</h1>

<p>One of the issues you may run into when <a data-type="indexterm" data-primary="meta prompting" id="mtprpt"/><a data-type="indexterm" data-primary="rewriting prompts" data-seealso="meta prompting" id="rwrppmp"/>putting an AI system into production is that you can’t expect the users of your system to be expert prompt engineers. It’s a case of garbage in, garbage out: if they write a substandard prompt, they’ll get poor results and complain about the quality of your product. One <a href="https://oreil.ly/OirCS">common trick in the industry</a> is to rewrite the prompt to make it better and more likely to get impressive results. This is a form of <em>meta prompting</em> where the prompt for one AI model is written by another.</p>

<p>Imagine a simple application where a user inputs a subject and an artist, and then an image is generated of the subject in the style of the artist. The prompt template is <code>a {subject} in the style of {artist}</code>.</p>

<p>Input:</p>

<pre data-type="programlisting">a dachshund dog in the style of Banksy</pre>

<p><a data-type="xref" href="#figure-8-24">Figure 8-24</a> shows the output.</p>

<figure><div id="figure-8-24" class="figure">
<img src="assets/pega_0824.png" alt="pega 0824" width="600" height="600"/>
<h6><span class="label">Figure 8-24. </span>A dachshund dog in the style of Banksy</h6>
</div></figure>

<p>The issue with this prompt is that the expectation would be that the dog would be part of the street painting (in Banksy style), whereas instead it is standing next to it in the image that was generated. To fix this, you can take the user prompt and inject that into a prompt to ChatGPT to find the artist’s medium.</p>

<p>Input:</p>

<pre data-type="programlisting">What's the medium that the artist Banksy mostly used? Respond
in 1-3 words only.</pre>

<p>Output:</p>

<pre data-type="programlisting">Street art</pre>

<p>Finally, you can use this output to rewrite the original user prompt, in the format <code>{medium} of a {subject} in the style of {artist}</code>.</p>

<p>Input:</p>

<pre data-type="programlisting">street art of a dachshund dog in the style of Banksy</pre>

<p><a data-type="xref" href="#figure-8-25">Figure 8-25</a>  shows the output.</p>

<figure><div id="figure-8-25" class="figure">
<img src="assets/pega_0825.png" alt="pega 0825" width="600" height="600"/>
<h6><span class="label">Figure 8-25. </span>Street art of a dachshund dog in the style of Banksy</h6>
</div></figure>

<p>This system can be built out further to include other prompt engineering techniques, such as quality boosters or negative prompts, to make the results more reliable. It’s possible to get good results just asking ChatGPT to rewrite the prompt for DALL-E (which ChatGPT Plus has available as a tool) and then use what it gives you for other models. Some attempts have been made to <a href="https://oreil.ly/9A1NL">train AI models</a> that specialize in generating high-quality prompts, though in our experience this method only brings quality up to average and doesn’t beat expert prompt <span class="keep-together">engineering.</span></p>
<div data-type="tip"><h1>Divide Labor</h1>
<p>Instead of expecting a nontechnical person <a data-type="indexterm" data-primary="meta prompting" data-startref="mtprpt" id="id1232"/><a data-type="indexterm" data-primary="rewriting prompts" data-seealso="meta prompting" data-startref="rwrppmp" id="id1233"/><a data-type="indexterm" data-primary="Divide Labor principle" data-secondary="prompt rewriting" id="id1234"/>to submit a good-quality prompt, simply pass their input to another AI model that can help improve the original prompt.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Meme Unbundling"><div class="sect1" id="vector_databases_08">
<h1>Meme Unbundling</h1>

<p>The main issue with replicating an artist’s style, or <a data-type="indexterm" data-primary="image generation" data-secondary="meme unbundling" id="iggmmbd"/><a data-type="indexterm" data-primary="memes, unbundling" id="mmbdun"/>emulating an art movement, is that it’s relatively unoriginal. Nobody knows what the legal implications of AI art will be, but certainly artists like Greg Rutkowski and others have already spoken out about the immorality of copying their style.</p>

<p>One way to still get the benefit of the shortcut of referring to a successful artist or art movement, while being more original in your style, is to unbundle the memes of an artist. The word <a href="https://oreil.ly/BQYFP"><em>meme</em></a> doesn’t just mean <em>a funny viral image</em>: it refers to any piece of cultural information that gets copied from person to person. If you can decompose an art style into its component parts and characteristics, then you can use these subcomponents to remix your own style. Say, for example, you were trying to make a painting of Times Square in the style of Salvador Dali.</p>

<p id="times_square_dali">Input:</p>

<pre data-type="programlisting">a painting of Times Square in the style of "The Persistence
of Memory" by Salvador Dali</pre>

<p><a data-type="xref" href="#figure-8-26">Figure 8-26</a> shows the output.</p>

<figure><div id="figure-8-26" class="figure">
<img src="assets/pega_0826.png" alt="pega 0826" width="600" height="600"/>
<h6><span class="label">Figure 8-26. </span>Painting of Times Square in the style of “The Persistence of Memory” by Salvador Dali</h6>
</div></figure>

<p>You are already getting good results by evoking Dali’s name, but it’s not quite right. You want a surrealist style, but not an exact copy of Dali’s work. The term <em>unbundling</em> was coined in <a data-type="indexterm" data-primary="unbundling, definition" id="id1235"/>this context by <a href="https://oreil.ly/2qy4E">Bakz T. Future</a>, meaning to extract the characteristics of an artist’s style in order to get something familiar but unique. It works by asking a text generation model (in this case ChatGPT-4) to describe the style of the artist you’re emulating, which you can then use as a prompt.</p>

<p id="meme_unbundling">Input:</p>

<pre data-type="programlisting">Briefly describe the characteristics and artistic style of
"The Persistence of Memory" by Salvador Dali, without
mentioning the artwork or the artist:</pre>

<p>Output:</p>

<pre data-type="programlisting">This is a notable example of Surrealist art, steeped in
dream-like, fantastical imagery that pushes the boundaries of
traditional understanding and interpretation. The painting is
composed with a distinctly precise and detailed technique,
reflecting a deceptive realism.

...

Contrasting with the melting objects, there are other items
in the scene that exhibit solidity and permanence, creating
an uncanny juxtaposition. The painting's composition and
imagery invite deep exploration and various interpretations,
concerning themes of reality, dreams, time, and the
subconscious. Overall, it is a visually striking work that
embodies the ethos of its movement: challenging the viewer's
perceptions and exploring the unconventional and the
irrational.</pre>

<p>You must make sure the prompt insists on not mentioning the artist or the artwork; otherwise, it won’t work as well as a prompt. You want to describe the style to someone who has never seen it so that the description is clean. For reference, <a data-type="xref" href="#figure-8-27">Figure 8-27</a> shows the famous painting ChatGPT is describing.</p>

<figure><div id="figure-8-27" class="figure">
<img src="assets/pega_0827.png" alt="pega 0827" width="600" height="455"/>
<h6><span class="label">Figure 8-27. </span>“The Persistence of Memory” by Salvador Dali</h6>
</div></figure>

<p>Now that you have a description of one of Dali’s most iconic
paintings, you can understand what makes it so visually
appealing. In particular, this sort of explanation can be
useful if you never went to art school and otherwise
wouldn’t know how to describe these elements. From here you
need to abbreviate the description to fit into an image model
prompt.</p>

<p id="convert_prompt">Input:</p>

<pre data-type="programlisting">Convert this description into a shortened prompt for an AI
image generation model such as DALL-E:</pre>

<p>Output:</p>

<pre data-type="programlisting">Surrealist landscape with subdued colors of brown, blue, and
yellow. Depict a barren, craggy shoreline and calm sea
meeting a clear sky. Include amorphous, melting objects
resembling common items, indicating fluidity and impermanence
of time. Contrast these with other solid, permanent elements.
Evoke a sense of stillness, dreamlike atmosphere, and
quietude.</pre>

<p>Take this output and add the subject of your painting, Times Square. It can also help to modify the prompt to make it flow better, as the prompts ChatGPT writes can be too instructive:</p>

<p id="unbundled_dali_times_square">Input:</p>

<pre data-type="programlisting">Painting of Times Square, surrealist landscape with subdued colors
of brown, blue, and yellow, a barren, craggy shoreline and calm
sea meeting a clear sky. Include amorphous, melting objects
resembling common items, indicating fluidity and impermanence
of time. Contrast these with other solid, permanent elements.
Evoke a sense of stillness, dreamlike atmosphere, and quietude.</pre>

<p><a data-type="xref" href="#figure-8-28">Figure 8-28</a> shows the output.</p>

<figure><div id="figure-8-28" class="figure">
<img src="assets/pega_0828.png" alt="pega 0828" width="600" height="600"/>
<h6><span class="label">Figure 8-28. </span>Unbundled Dali memes applied to a painting of Times Square</h6>
</div></figure>
<div data-type="tip"><h1>Give Direction</h1>
<p>Rather than guiding the AI image model toward a <a data-type="indexterm" data-primary="Give Direction principle" data-secondary="meme unbinding" id="id1236"/>specific artist’s work, you can emulate a close approximation by using a description of the artist’s work. This is more transformative and creative approach than simply evoking an artist’s name, and perhaps more ethical.</p>
</div>

<p>This image is still similar to Dali’s work, but it has been transformed through the filter of ChatGPT’s description. Therefore, it’s already more original than what you got when you simply evoked his name: an advantage over the average prompter. But you’re in an even better position now, because you have unbundled Dali’s style into individual <em><a href="https://oreil.ly/BQYFP">memes</a></em> like “surrealist landscape,” “melting objects,” and “dreamlike atmosphere,” and can more easily <a href="https://oreil.ly/wPuMo">remix</a> its component parts to make the image more unique:</p>

<p id="meme_remixing">Input:</p>

<pre data-type="programlisting">Painting of Times Square, surrealist landscape with subdued
colors of orange, red, and green, imposing buildings and calm
river meeting a stormy sky. The amorphous melting dripping
clock in the center of the square indicates the fluidity and
impermanence of time in contrast with other solid, permanent
elements. Evoke a sense of stillness, dreamlike atmosphere,
and quietude.</pre>

<p><a data-type="xref" href="#figure-8-29">Figure 8-29</a> shows the output.</p>

<figure><div id="figure-8-29" class="figure">
<img src="assets/pega_0829.png" alt="pega 0829" width="600" height="600"/>
<h6><span class="label">Figure 8-29. </span>Dali Times Square remixed</h6>
</div></figure>

<p>You have only made small modifications to the color and elements in the painting, but you could go further. It’s also possible to take elements from other popular artists and combine the aspects you like to arrive at something new. This technique only works right now with <a data-type="indexterm" data-primary="image generation" data-secondary="meme unbundling" data-startref="iggmmbd" id="id1237"/><a data-type="indexterm" data-primary="memes, unbundling" data-startref="mmbdun" id="id1238"/>artists and artworks that are famous enough to be readily described from the training data; however, as AI models become multi-modal (i.e., able to generate both images and text), expect to be able to feed in an image and get a description to use for unbundling.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Meme Mapping"><div class="sect1" id="id125">
<h1>Meme Mapping</h1>

<p>One of the most common forms of prompt inspiration is <a data-type="indexterm" data-primary="meme mapping" id="mmmpp"/><a data-type="indexterm" data-primary="mapping memes" id="mppgmes"/>looking at what prompts other prompt engineers are getting results with. The <a href="https://oreil.ly/upQIh">Midjourney Discord community</a> has <a data-type="indexterm" data-primary="Midjourney" data-secondary="Discord community" id="id1239"/>millions of active members, with thousands of new images being generated and automatically shared every day, as do other AI communities including on <a href="https://oreil.ly/EwLNh">Reddit</a> and across various other websites, email newsletters, and social media accounts. One commonly used website is <a href="https://lexica.art">Lexica.art</a>, which has a searchable database (by keyword and similarity) of many Stable Diffusion images and their prompts for inspiration.</p>

<p>While searching and browsing through these sources of inspiration, you’re likely to notice recurring patterns, or memes, in the words that are used for a particular type of image. We call this process of intentionally and systematically finding these patterns <a href="https://oreil.ly/DLqAV"><em>meme mapping</em></a>, and it can be an invaluable tool for identifying useful prompts. For example, you may search Super Mario on Lexica and see lots of examples where people have tried to create a <em>realistic</em> Mario, like the one in <a data-type="xref" href="#figure-8-30">Figure 8-30</a>, which might inspire you to do the same, <a href="https://oreil.ly/WNsRn">starting with a prompt</a> that’s already proven to work, saving you considerable time.</p>

<figure><div id="figure-8-30" class="figure">
<img src="assets/pega_0830.png" alt="pega 0830" width="600" height="328"/>
<h6><span class="label">Figure 8-30. </span>Realistic Mario</h6>
</div></figure>

<p>Alternatively you might apply this meme to a character from a different franchise, and try repurposing some of the prompts used by others to get a realistic effect. Without doing this research, you might not have been aware image models could generate real-world versions of cartoon or game characters, or perhaps never would have thought to try it. You may have never stumbled upon the insight that including “as a Soviet factory worker” in your prompt helps evoke a sense of gritty realism, and may never have encountered the work of the two artists referenced. There is a healthy culture of remixing content in the AI art community, with people learning from other’s prompts, and then paying it forward by sharing their own expertise.</p>

<p>Input:</p>

<pre data-type="programlisting">portrait of Homer Simpson as a Soviet factory worker, gritty,
dirty, beautiful, very detailed, hyperrealistic, medium shot,
very detailed painting by Glenn Fabry, by Joao Ruas --no
cartoon</pre>

<p><a data-type="xref" href="#figure-8-31">Figure 8-31</a> shows the output.</p>

<figure><div id="figure-8-31" class="figure">
<img src="assets/pega_0831.png" alt="pega 0831" width="512" height="512"/>
<h6><span class="label">Figure 8-31. </span>Realistic Homer Simpson</h6>
</div></figure>

<p>This meme mapping process can be <a href="https://oreil.ly/VqyG-">done manually</a>, with the examples copied and pasted into a spreadsheet or productivity tool Notion, although that can be time-consuming. So long as you are respecting a website’s terms and conditions and any legal obligations in your country, it would also be possible to write custom code to programmatically scrape the contents of that website. Once you have all the data in <a data-type="indexterm" data-primary="Google Vision" id="id1240"/><a data-type="indexterm" data-primary="GPT-4 Vision" id="id1241"/><a data-type="indexterm" data-primary="NGrams analysis" id="id1242"/>one place, you could programmatically label the images with an entity recognition model like <a href="https://oreil.ly/EZmRs">Google Vision</a>, a multimodal model like <a href="https://oreil.ly/cOcPR">GPT-4 Vision</a>, or use NLP  such as <span class="keep-together"><a href="https://oreil.ly/GXfDl">NGrams analysis</a></span> on the prompts in order to identify patterns at a larger scale than is <a data-type="indexterm" data-primary="meme mapping" data-startref="mmmpp" id="id1243"/><a data-type="indexterm" data-primary="mapping memes" data-startref="mppgmes" id="id1244"/>possible manually.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Prompt Analysis"><div class="sect1" id="id186">
<h1>Prompt Analysis</h1>

<p>One common mistake is to continue to build <a data-type="indexterm" data-primary="Midjourney" data-secondary="analyzing prompts" id="id1245"/>out longer and longer prompts, without thinking about what parts of the prompt are really necessary. Every word added perturbs the model in some way, adding noise to the resulting output. Often, removing unnecessary words can be as effective as adding new words. To conduct this analysis without lots of trial and error, Midjourney offers a <code>/shorten</code> command that attempts to remove these unnecessary words, leaving only the core tokens that the model pays the most attention to. Click “show details” at the bottom of the response to get token-level weightings and a visual chart.</p>

<p>Input:</p>

<pre data-type="programlisting">portrait of Homer Simpson as a Soviet factory worker, gritty,
dirty, beautiful, very detailed, hyperrealistic, medium shot,
very detailed painting by Glenn Fabry, by Joao Ruas
--no cartoon</pre>

<p>Output:</p>

<pre data-type="programlisting" data-code-language="markdown"><code class="gs">**portrait**</code> (0.08) of <code class="gs">**homer simpson**</code> (1.00) as a
<code class="gs">**soviet**</code> (0.19) <code class="gs">**factory**</code> (0.21) <code class="gs">**worker**</code> (0.08),<code class="w"/>
gritty (0.02), dirty (0.02), beautiful (0.00), very (0.00)<code class="w"/>
detailed (0.01), hyperrealistic (0.01), medium (0.00) shot<code class="w"/>
(0.00), very (0.00) detailed (0.01) painting (0.05) by Glenn<code class="w"/>
Fabry (0.08), by <code class="gs">**Joao Ruas**</code> (0.09)<code class="w"/>

██████████ homer simpson<code class="w"/>
██░░░░░░░░ factory<code class="w"/>
██░░░░░░░░ soviet<code class="w"/>
█░░░░░░░░░ portrait<code class="w"/>
█░░░░░░░░░ worker<code class="w"/>
█░░░░░░░░░ Joao Ruas<code class="w"/>
█░░░░░░░░░ Glenn Fabry<code class="w"/>
█░░░░░░░░░ painting<code class="w"/></pre>

<p>Once you have this analysis, you can use it to cut any noise from the prompt and zero in on what words, or memes, are actually important to the final result.</p>
<div data-type="tip"><h1>Evaluate Quality</h1>
<p>Seeing the weights the model assigns to <a data-type="indexterm" data-primary="Evaluate Quality principle" data-secondary="prompt analysis" id="id1246"/>each token gives you unparalleled insight into how the model works. Often we make assumptions about what’s important in a prompt, and those assumptions can be quite far from reality.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id358">
<h1>Summary</h1>

<p>In this chapter, you learned about standard practices for image generation using diffusion models. You explored format modifiers such as stock photos, oil paintings, and Egyptian hieroglyphs, and how they can be used to create unique and visually appealing images. Additionally, you discovered art style modifiers that allow for the replication of popular art styles or artists, such as Lewis Carroll’s <em>Alice in Wonderland</em> style.</p>

<p>You went deeper into the application of prompt engineering principles, including how to use art-style modifiers to replicate popular art styles and artists, and how mentioning specific artists’ names can help achieve the desired visual style. The concept of negative prompts and weighted terms was introduced, allowing you to specify what you don’t want in an image and control the mixture of different concepts. You also explored the concepts of inpainting and outpainting, where specific parts of an image can be generated separately by erasing and adding prompts. You discovered how these techniques can be further expanded and combined to enhance the reliability and quality of generative AI results.</p>

<p>In the next chapter, you will dive deeper into the world of image generation and explore more advanced use cases. You will learn how to harness the power of Stable Diffusion and AUTOMATIC1111 to improve your image generation skills. Including advanced Stable Diffusion techniques like utilizing ControlNet models for more control over the style and composition of your images, you will discover a wide range of exciting possibilities.</p>
</div></section>
</div></section></div></div></body></html>