- en: Appendix A. Introduction to PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An overview of the PyTorch deep learning library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up an environment and workspace for deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensors as a fundamental data structure for deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mechanics of training deep neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training models on GPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter is designed to equip you with the necessary skills and knowledge
    to put deep learning into practice and implement large language models (LLMs)
    from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: We will introduce PyTorch, a popular Python-based deep learning library, which
    will be our primary tool for the remainder of this book. This chapter will also
    guide you through setting up a deep learning workspace armed with PyTorch and
    GPU support.
  prefs: []
  type: TYPE_NORMAL
- en: Then, you'll learn about the essential concept of tensors and their usage in
    PyTorch. We will also delve into PyTorch's automatic differentiation engine, a
    feature that enables us to conveniently and efficiently use backpropagation, which
    is a crucial aspect of neural network training.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this chapter is meant as a primer for those who are new to deep learning
    in PyTorch. While this chapter explains PyTorch from the ground up, it's not meant
    to be an exhaustive coverage of the PyTorch library. Instead, this chapter focuses
    on the PyTorch fundamentals that we will use to implement LLMs throughout this
    book. If you are already familiar with deep learning, you may skip this appendix
    and directly move on to chapter 2, working with text data.
  prefs: []
  type: TYPE_NORMAL
- en: A.1 What is PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*PyTorch* ([https://pytorch.org/](pytorch.org.html)) is an open-source Python-based
    deep learning library. According to *Papers With Code* ([https://paperswithcode.com/trends](paperswithcode.com.html)),
    a platform that tracks and analyzes research papers, PyTorch has been the most
    widely used deep learning library for research since 2019 by a wide margin. And
    according to the *Kaggle Data Science and Machine Learning Survey 2022* ([https://www.kaggle.com/c/kaggle-survey-2022](c.html)),
    the number of respondents using PyTorch is approximately 40% and constantly grows
    every year.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the reasons why PyTorch is so popular is its user-friendly interface
    and efficiency. However, despite its accessibility, it doesn't compromise on flexibility,
    providing advanced users the ability to tweak lower-level aspects of their models
    for customization and optimization. In short, for many practitioners and researchers,
    PyTorch offers just the right balance between usability and features.
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections, we will define the main features PyTorch has to
    offer.
  prefs: []
  type: TYPE_NORMAL
- en: A.1.1 The three core components of PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyTorch is a relatively comprehensive library, and one way to approach it is
    to focus on its three broad components, which are summarized in figure A.1.
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.1 PyTorch's three main components include a tensor library as a fundamental
    building block for computing, automatic differentiation for model optimization,
    and deep learning utility functions, making it easier to implement and train deep
    neural network models.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/A__image001.png)'
  prefs: []
  type: TYPE_IMG
- en: Firstly, PyTorch is a *tensor library* that extends the concept of array-oriented
    programming library NumPy with the additional feature of accelerated computation
    on GPUs, thus providing a seamless switch between CPUs and GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, PyTorch is an *automatic differentiation engine*, also known as autograd,
    which enables the automatic computation of gradients for tensor operations, simplifying
    backpropagation and model optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, PyTorch is a *deep learning library*, meaning that it offers modular,
    flexible, and efficient building blocks (including pre-trained models, loss functions,
    and optimizers) for designing and training a wide range of deep learning models,
    catering to both researchers and developers.
  prefs: []
  type: TYPE_NORMAL
- en: After defining the term deep learning and installing PyTorch in the two following
    subsections, the remainder of this chapter will go over these three core components
    of PyTorch in more detail, along with hands-on code examples.
  prefs: []
  type: TYPE_NORMAL
- en: A.1.2 Defining deep learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs are often referred to as *AI* models in the news. However, as illustrated
    in the first section of chapter 1 (*1.1 What is an LLM?*) LLMs are also a type
    of deep neural network, and PyTorch is a deep learning library. Sounds confusing?
    Let's take a brief moment and summarize the relationship between these terms before
    we proceed.
  prefs: []
  type: TYPE_NORMAL
- en: AI is fundamentally about creating computer systems capable of performing tasks
    that usually require human intelligence. These tasks include understanding natural
    language, recognizing patterns, and making decisions. (Despite significant progress,
    AI is still far from achieving this level of general intelligence.)
  prefs: []
  type: TYPE_NORMAL
- en: '*Machine learnin*g represents a subfield of AI (as illustrated in figure A.2)
    that focuses on developing and improving learning algorithms. The key idea behind
    machine learning is to enable computers to learn from data and make predictions
    or decisions without being explicitly programmed to perform the task. This involves
    developing algorithms that can identify patterns and learn from historical data
    and improve their performance over time with more data and feedback.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.2 Deep learning is a subcategory of machine learning that is focused
    on the implementation of deep neural networks. In turn, machine learning is a
    subcategory of AI that is concerned with algorithms that learn from data. AI is
    the broader concept of machines being able to perform tasks that typically require
    human intelligence.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/A__image003.png)'
  prefs: []
  type: TYPE_IMG
- en: Machine learning has been integral in the evolution of AI, powering many of
    the advancements we see today, including LLMs. Machine learning is also behind
    technologies like recommendation systems used by online retailers and streaming
    services, email spam filtering, voice recognition in virtual assistants, and even
    self-driving cars. The introduction and advancement of machine learning have significantly
    enhanced AI's capabilities, enabling it to move beyond strict rule-based systems
    and adapt to new inputs or changing environments.
  prefs: []
  type: TYPE_NORMAL
- en: '*Deep learning* is a subcategory of machine learning that focuses on the training
    and application of deep neural networks. These deep neural networks were originally
    inspired by how the human brain works, particularly the interconnection between
    many neurons. The "deep" in deep learning refers to the multiple hidden layers
    of artificial neurons or nodes that allow them to model complex, nonlinear relationships
    in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike traditional machine learning techniques that excel at simple pattern
    recognition, deep learning is particularly good at handling unstructured data
    like images, audio, or text, so deep learning is particularly well suited for
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The typical predictive modeling workflow (also referred to as *supervised learning*)
    in machine learning and deep learning is summarized in figure A.3.
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.3 The supervised learning workflow for predictive modeling consists
    of a training stage where a model is trained on labeled examples in a training
    dataset. The trained model can then be used to predict the labels of new observations.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/A__image005.png)'
  prefs: []
  type: TYPE_IMG
- en: Using a learning algorithm, a model is trained on a training dataset consisting
    of examples and corresponding labels. In the case of an email spam classifier,
    for example, the training dataset consists of emails and their *spam* and *not-spam*
    labels that a human identified. Then, the trained model can be used on new observations
    (new emails) to predict their unknown label (*spam* or *not spam*).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we also want to add a model evaluation between the training and inference
    stages to ensure that the model satisfies our performance criteria before using
    it in a real-world application.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the workflow for training and using LLMs, as we will see later in
    this book, is similar to the workflow depicted in figure A.3 if we train them
    to classify texts. And if we are interested in training LLMs for generating texts,
    which is the main focus of this book, figure A.3 still applies. In this case,
    the labels during pretraining can be derived from the text itself (the next-word
    prediction task introduced in chapter 1). And the LLM will generate entirely new
    text (instead of predicting labels) given an input prompt during inference.
  prefs: []
  type: TYPE_NORMAL
- en: A.1.3 Installing PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyTorch can be installed just like any other Python library or package. However,
    since PyTorch is a comprehensive library featuring CPU- and GPU-compatible codes,
    the installation may require additional explanation.
  prefs: []
  type: TYPE_NORMAL
- en: Python version
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Many scientific computing libraries do not immediately support the newest version
    of Python. Therefore, when installing PyTorch, it's advisable to use a version
    of Python that is one or two releases older. For instance, if the latest version
    of Python is 3.13, using Python 3.10 or 3.11 is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, there are two versions of PyTorch: a leaner version that only
    supports CPU computing and a version that supports both CPU and GPU computing.
    If your machine has a CUDA-compatible GPU that can be used for deep learning (ideally
    an NVIDIA T4, RTX 2080 Ti, or newer), I recommend installing the GPU version.
    Regardless, the default command for installing PyTorch is as follows in a code
    terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Suppose your computer supports a CUDA-compatible GPU. In that case, this will
    automatically install the PyTorch version that supports GPU acceleration via CUDA,
    given that the Python environment you're working on has the necessary dependencies
    (like pip) installed.
  prefs: []
  type: TYPE_NORMAL
- en: AMD GPUs for deep learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As of this writing, PyTorch has also added experimental support for AMD GPUs
    via ROCm. Please see [https://pytorch.org](.html) for additional instructions.
  prefs: []
  type: TYPE_NORMAL
- en: However, to explicitly install the CUDA-compatible version of PyTorch, it's
    often better to specify the CUDA you want PyTorch to be compatible with. PyTorch's
    official website ([https://pytorch.org](.html)) provides commands to install PyTorch
    with CUDA support for different operating systems as shown in figure A.4.
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.4 Access the PyTorch installation recommendation on [https://pytorch.org](.html)
    to customize and select the installation command for your system.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/A__image007.png)'
  prefs: []
  type: TYPE_IMG
- en: (Note that the command shown in figure A.4 will also install the `torchvision`
    and `torchaudio` libraries, which are optional for this book.)
  prefs: []
  type: TYPE_NORMAL
- en: 'As of this writing, this book is based on PyTorch 2.0.1, so it''s recommended
    to use the following installation command to install the exact version to guarantee
    compatibility with this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: However, as mentioned earlier, given your operating system, the installation
    command might slightly differ from the one shown above. Thus, I recommend visiting
    the [https://pytorch.org](pytorch.org.html) website and using the installation
    menu (see figure A4) to select the installation command for your operating system
    and replace `torch` with `torch==2.0.1` in this command.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the version of PyTorch, you can execute the following code in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: PyTorch and Torch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note that the Python library is named "torch" primarily because it's a continuation
    of the Torch library but adapted for Python (hence, "PyTorch"). The name "torch"
    acknowledges the library's roots in Torch, a scientific computing framework with
    wide support for machine learning algorithms, which was initially created using
    the Lua programming language.
  prefs: []
  type: TYPE_NORMAL
- en: If you are looking for additional recommendations and instructions for setting
    up your Python environment or installing the other libraries used later in this
    book, I recommend visiting the supplementary GitHub repository of this book at
    [https://github.com/rasbt/LLMs-from-scratch](rasbt.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing PyTorch, you can check whether your installation recognizes
    your built-in NVIDIA GPU by running the following code in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If the command returns True, you are all set. If the command returns False,
    your computer may not have a compatible GPU, or PyTorch does not recognize it.
    While GPUs are not required for the initial chapters in this book, which are focused
    on implementing LLMs for educational purposes, they can significantly speed up
    deep learning-related computations.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't have access to a GPU, there are several cloud computing providers
    where users can run GPU computations against an hourly cost. A popular Jupyter-notebook-like
    environment is Google Colab ([https://colab.research.google.com](colab.research.google.com.html)),
    which provides time-limited access to GPUs as of this writing. Using the "Runtime"
    menu, it is possible to select a GPU, as shown in the screenshot in figure A.5.
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.5 Select a GPU device for Google Colab under the *Runtime/Change runtime
    type* menu.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/A__image009.png)'
  prefs: []
  type: TYPE_IMG
- en: PyTorch on Apple Silicon
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'If you have an Apple Mac with an Apple Silicon chip (like the M1, M2, M3, or
    newer models), you have the option to leverage its capabilities to accelerate
    PyTorch code execution. To use your Apple Silicon chip for PyTorch, you first
    need to install PyTorch as you normally would. Then, to check if your Mac supports
    PyTorch acceleration with its Apple Silicon chip, you can run a simple code snippet
    in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If it returns `True`, it means that your Mac has an Apple Silicon chip that
    can be used to accelerate PyTorch code.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise A.1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Install and set up PyTorch on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise A.2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Run the supplementary Chapter 2 code at [https://github.com/rasbt/LLMs-from-scratch](rasbt.html)
    that checks whether your environment is set up correctly..
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Understanding tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tensors represent a mathematical concept that generalizes vectors and matrices
    to potentially higher dimensions. In other words, tensors are mathematical objects
    that can be characterized by their order (or rank), which provides the number
    of dimensions. For example, a scalar (just a number) is a tensor of rank 0, a
    vector is a tensor of rank 1, and a matrix is a tensor of rank 2, as illustrated
    in figure A.6
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.6 An illustration of tensors with different ranks. Here 0D corresponds
    to rank 0, 1D to rank 1, and 2D to rank 2\. Note that a 3D vector, which consists
    of 3 elements, is still a rank 1 tensor.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/A__image011.png)'
  prefs: []
  type: TYPE_IMG
- en: From a computational perspective, tensors serve as data containers. For instance,
    they hold multi-dimensional data, where each dimension represents a different
    feature. Tensor libraries, such as PyTorch, can create, manipulate, and compute
    with these multi-dimensional arrays efficiently. In this context, a tensor library
    functions as an array library.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch tensors are similar to NumPy arrays but have several additional features
    important for deep learning. For example, PyTorch adds an automatic differentiation
    engine, simplifying *computing gradients*, as discussed later in section 2.4\.
    PyTorch tensors also support GPU computations to speed up deep neural network
    training, which we will discuss later in section 2.8.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch's has a NumPy-like API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As you will see in the upcoming sections, PyTorch adopts most of the NumPy
    array API and syntax for its tensor operations. If you are new to NumPy, you can
    get a brief overview of the most relevant concepts via my article Scientific Computing
    in Python: Introduction to NumPy and Matplotlib at [https://sebastianraschka.com/blog/2020/numpy-intro.html](2020.html).'
  prefs: []
  type: TYPE_NORMAL
- en: The following subsections will look at the basic operations of the PyTorch tensor
    library, showing how to create simple tensors and going over some of the essential
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: A.2.1 Scalars, vectors, matrices, and tensors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned earlier, PyTorch tensors are data containers for array-like structures.
    A scalar is a 0-dimensional tensor (for instance, just a number), a vector is
    a 1-dimensional tensor, and a matrix is a 2-dimensional tensor. There is no specific
    term for higher-dimensional tensors, so we typically refer to a 3-dimensional
    tensor as just a 3D tensor, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create objects of PyTorch''s `Tensor` class using the `torch.tensor`
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.1 Creating PyTorch tensors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: A.2.2 Tensor data types
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous section, we created tensors from Python integers. In this case,
    PyTorch adopts the default 64-bit integer data type from Python. We can access
    the data type of a tensor via the `.dtype` attribute of a tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If we create tensors from Python floats, PyTorch creates tensors with a 32-bit
    precision by default, as we can see below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This choice is primarily due to the balance between precision and computational
    efficiency. A 32-bit floating point number offers sufficient precision for most
    deep learning tasks, while consuming less memory and computational resources than
    a 64-bit floating point number. Moreover, GPU architectures are optimized for
    32-bit computations, and using this data type can significantly speed up model
    training and inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, it is possible to readily change the precision using a tensor''s
    `.to` method. The following code demonstrates this by changing a 64-bit integer
    tensor into a 32-bit float tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: For more information about different tensor data types available in PyTorch,
    I recommend checking the official documentation at [https://pytorch.org/docs/stable/tensors.html](stable.html).
  prefs: []
  type: TYPE_NORMAL
- en: A.2.3 Common PyTorch tensor operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Comprehensive coverage of all the different PyTorch tensor operations and commands
    is outside the scope of this book. However, we will briefly describe relevant
    operations as we introduce them throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on to the next section covering the concept behind computation
    graphs, below is a list of the most essential PyTorch tensor operations.
  prefs: []
  type: TYPE_NORMAL
- en: We already introduced the `torch.tensor()` function to create new tensors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, the `.shape` attribute allows us to access the shape of a tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see above, `.shape` returns `[2, 3]`, which means that the tensor
    has 2 rows and 3 columns. To reshape the tensor into a 3 by 2 tensor, we can use
    the `.reshape` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'However, note that the more common command for reshaping tensors in PyTorch
    is `.view()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Similar to `.reshape` and `.view`, there are several cases where PyTorch offers
    multiple syntax options for executing the same computation. This is because PyTorch
    initially followed the original Lua Torch syntax convention but then also added
    syntax to make it more similar to NumPy upon popular request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can use `.T` to transpose a tensor, which means flipping it across
    its diagonal. Note that this is similar from reshaping a tensor as you can see
    based on the result below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, the common way to multiply two matrices in PyTorch is the `.matmul`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we can also adopt the `@` operator, which accomplishes the same thing
    more compactly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned earlier, we will introduce additional operations throughout this
    book when needed. For readers who''d like to browse through all the different
    tensor operations available in PyTorch (hint: we won''t need most of these), I
    recommend checking out the official documentation at [https://pytorch.org/docs/stable/tensors.html](stable.html).'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Seeing models as computation graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we covered one of the major three components of PyTorch,
    namely, its tensor library. Next in line is PyTorch's automatic differentiation
    engine, also known as autograd. PyTorch's autograd system provides functions to
    compute gradients in dynamic computational graphs automatically. But before we
    dive deeper into computing gradients in the next section, let's define the concept
    of a computational graph.
  prefs: []
  type: TYPE_NORMAL
- en: A computational graph (or computation graph in short) is a directed graph that
    allows us to express and visualize mathematical expressions. In the context of
    deep learning, a computation graph lays out the sequence of calculations needed
    to compute the output of a neural network -- we will need this later to compute
    the required gradients for backpropagation, which is the main training algorithm
    for neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a concrete example to illustrate the concept of a computation
    graph. The following code implements the forward pass (prediction step) of a simple
    logistic regression classifier, which can be seen as a single-layer neural network,
    returning a score between 0 and 1 that is compared to the true class label (0
    or 1) when computing the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.2 A logistic regression forward pass
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: If not all components in the code above make sense to you, don't worry. The
    point of this example is not to implement a logistic regression classifier but
    rather to illustrate how we can think of a sequence of computations as a computation
    graph, as shown in figure A.7.
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.7 A logistic regression forward pass as a computation graph. The input
    feature *x*[1] is multiplied by a model weight *w*[1] and passed through an activation
    function *σ* after adding the bias. The loss is computed by comparing the model
    output *a* with a given label *y*.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/A__image013.png)'
  prefs: []
  type: TYPE_IMG
- en: In fact, PyTorch builds such a computation graph in the background, and we can
    use this to calculate gradients of a loss function with respect to the model parameters
    (here w1 and b) to train the model, which is the topic of the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Automatic differentiation made easy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we introduced the concept of computation graphs. If
    we carry out computations in PyTorch, it will build such a graph internally by
    default if one of its terminal nodes has the `requires_grad` attribute set to
    `True`. This is useful if we want to compute gradients. Gradients are required
    when training neural networks via the popular backpropagation algorithm, which
    can be thought of as an implementation of the *chain rule* from calculus for neural
    networks, which is illustrated in figure A.8.
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.8 The most common way of computing the loss gradients in a computation
    graph involves applying the chain rule from right to left, which is also called
    reverse-model automatic differentiation or backpropagation. It means we start
    from the output layer (or the loss itself) and work backward through the network
    to the input layer. This is done to compute the gradient of the loss with respect
    to each parameter (weights and biases) in the network, which informs how we update
    these parameters during training.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/A__image015.png)'
  prefs: []
  type: TYPE_IMG
- en: Partial derivatives and gradients
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Figure A.8 shows partial derivatives, which measure the rate at which a function
    changes with respect to one of its variables. A gradient is a vector containing
    all of the partial derivatives of a multivariate function, a function with more
    than one variable as input.
  prefs: []
  type: TYPE_NORMAL
- en: If you are not familiar or don't remember the partial derivatives, gradients,
    or the chain rule from calculus, don't worry. On a high level, all you need to
    know for this book is that the chain rule is a way to compute gradients of a loss
    function with respect to the model's parameters in a computation graph. This provides
    the information needed to update each parameter in a way that minimizes the loss
    function, which serves as a proxy for measuring the model's performance, using
    a method such as gradient descent. We will revisit the computational implementation
    of this training loop in PyTorch in section 2.7, *A typical training loop*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, how is this all related to the second component of the PyTorch library
    we mentioned earlier, the automatic differentiation (autograd) engine? By tracking
    every operation performed on tensors, PyTorch''s autograd engine constructs a
    computational graph in the background. Then, calling the `grad` function, we can
    compute the gradient of the loss with respect to model parameter `w1` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.3 Computing gradients via autograd
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s show the resulting values of the loss with respect to the model''s parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Above, we have been using the grad function "manually," which can be useful
    for experimentation, debugging, and demonstrating concepts. But in practice, PyTorch
    provides even more high-level tools to automate this process. For instance, we
    can call `.backward` on the loss, and PyTorch will compute the gradients of all
    the leaf nodes in the graph, which will be stored via the tensors'' `.grad` attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The outputs are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: If this section is packed with a lot of information and you may be overwhelmed
    by the calculus concepts, don't worry. While this calculus jargon was a means
    to explain PyTorch's autograd component, all you need to take away from this section
    is that PyTorch takes care of the calculus for us via the `.backward` method --
    we won't need to compute any derivatives or gradients by hand in this book.
  prefs: []
  type: TYPE_NORMAL
- en: A.5 Implementing multilayer neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections, we covered PyTorch's tensor and autograd components.
    This section focuses on PyTorch as a library for implementing deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: To provide a concrete example, we focus on a multilayer perceptron, which is
    a fully connected neural network, as illustrated in figure A.9.
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.9 An illustration of a multilayer perceptron with 2 hidden layers.
    Each node represents a unit in the respective layer. Each layer has only a very
    small number of nodes for illustration purposes.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/A__image017.png)'
  prefs: []
  type: TYPE_IMG
- en: When implementing a neural network in PyTorch, we typically subclass the `torch.nn.Module`
    class to define our own custom network architecture. This `Module` base class
    provides a lot of functionality, making it easier to build and train models. For
    instance, it allows us to encapsulate layers and operations and keep track of
    the model's parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Within this subclass, we define the network layers in the `__init__` constructor
    and specify how they interact in the forward method. The forward method describes
    how the input data passes through the network and comes together as a computation
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the backward method, which we typically do not need to implement
    ourselves, is used during training to compute gradients of the loss function with
    respect to the model parameters, as we will see in section 2.7, *A typical training
    loop*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code implements a classic multilayer perceptron with two hidden
    layers to illustrate a typical usage of the `Module` class:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.4 A multilayer perceptron with two hidden layers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then instantiate a new neural network object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'But before using this new `model` object, it is often useful to call `print`
    on the model to see a summary of its structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note that we used the `Sequential` class when we implemented the `NeuralNetwork`
    class. Using Sequential is not required, but it can make our life easier if we
    have a series of layers that we want to execute in a specific order, as is the
    case here. This way, after instantiating `self.layers = Sequential(...)` in the
    `__init__` constructor, we just have to call the `self.layers` instead of calling
    each layer individually in the `NeuralNetwork`'s `forward` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s check the total number of trainable parameters of this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note that each parameter for which `requires_grad=True` counts as a trainable
    parameter and will be updated during training (more on that later in section 2.7,
    *A typical training loop*).
  prefs: []
  type: TYPE_NORMAL
- en: In the case of our neural network model with the two hidden layers above, these
    trainable parameters are contained in the `torch.nn.Linear` layers. A *linear*
    layer multiplies the inputs with a weight matrix and adds a bias vector. This
    is sometimes also referred to as a *feedforward* or *fully connected* layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the `print(model)` call we executed above, we can see that the first
    `Linear` layer is at index position 0 in the layers attribute. We can access the
    corresponding weight parameter matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Since this is a large matrix that is not shown in its entirety, let''s use
    the `.shape` attribute to show its dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: (Similarly, you could access the bias vector via `model.layers[0].bias`.)
  prefs: []
  type: TYPE_NORMAL
- en: The weight matrix above is a 30x50 matrix, and we can see that the `requires_grad`
    is set to `True`, which means its entries are trainable -- this is the default
    setting for weights and biases in `torch.nn.Linear`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if you execute the code above on your computer, the numbers in the
    weight matrix will likely differ from those shown above. This is because the model
    weights are initialized with small random numbers, which are different each time
    we instantiate the network. In deep learning, initializing model weights with
    small random numbers is desired to break symmetry during training -- otherwise,
    the nodes would be just performing the same operations and updates during backpropagation,
    which would not allow the network to learn complex mappings from inputs to outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, while we want to keep using small random numbers as initial values
    for our layer weights, we can make the random number initialization reproducible
    by seeding PyTorch''s random number generator via `manual_seed`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, after we spent some time inspecting the `NeuraNetwork` instance, let''s
    briefly see how it''s used via the forward pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The result is:tensor([[-0.1262, 0.1080, -0.1792]], `grad_fn=<AddmmBackward0>)`
  prefs: []
  type: TYPE_NORMAL
- en: In the code above, we generated a single random training example `X` as a toy
    input (note that our network expects 50-dimensional feature vectors) and fed it
    to the model, returning three scores. When we call `model(x)`, it will automatically
    execute the forward pass of the model.
  prefs: []
  type: TYPE_NORMAL
- en: The forward pass refers to calculating output tensors from input tensors. This
    involves passing the input data through all the neural network layers, starting
    from the input layer, through hidden layers, and finally to the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: These three numbers returned above correspond to a score assigned to each of
    the three output nodes. Notice that the output tensor also includes a `grad_fn`
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Here, `grad_fn=<AddmmBackward0>` represents the last-used function to compute
    a variable in the computational graph. In particular, `grad_fn=<AddmmBackward0>`
    means that the tensor we are inspecting was created via a matrix multiplication
    and addition operation. PyTorch will use this information when it computes gradients
    during backpropagation. The `<AddmmBackward0>` part of `grad_fn=<AddmmBackward0>`
    specifies the operation that was performed. In this case, it is an `Addmm` operation.
    `Addmm` stands for matrix multiplication (`mm`) followed by an addition (`Add`).
  prefs: []
  type: TYPE_NORMAL
- en: If we just want to use a network without training or backpropagation, for example,
    if we use it for prediction after training, constructing this computational graph
    for backpropagation can be wasteful as it performs unnecessary computations and
    consumes additional memory. So, when we use a model for inference (for instance,
    making predictions) rather than training, it is a best practice to use the `torch.no_grad()`
    context manager, as shown below. This tells PyTorch that it doesn't need to keep
    track of the gradients, which can result in significant savings in memory and
    computation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'In PyTorch, it''s common practice to code models such that they return the
    outputs of the last layer (logits) without passing them to a nonlinear activation
    function. That''s because PyTorch''s commonly used loss functions combine the
    softmax (or sigmoid for binary classification) operation with the negative log-likelihood
    loss in a single class. The reason for this is numerical efficiency and stability.
    So, if we want to compute class-membership probabilities for our predictions,
    we have to call the `softmax` function explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The values can now be interpreted as class-membership probabilities that sum
    up to 1\. The values are roughly equal for this random input, which is expected
    for a randomly initialized model without training.
  prefs: []
  type: TYPE_NORMAL
- en: In the following two sections, we will learn how to set up an efficient data
    loader and train the model.
  prefs: []
  type: TYPE_NORMAL
- en: A.6 Setting up efficient data loaders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we defined a custom neural network model. Before we
    can train this model, we have to briefly talk about creating efficient data loaders
    in PyTorch, which we will iterate over when training the model. The overall idea
    behind data loading in PyTorch is illustrated in figure A.10.
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.10 PyTorch implements a Dataset and a DataLoader class. The Dataset
    class is used to instantiate objects that define how each data record is loaded.
    The DataLoader handles how the data is shuffled and assembled into batches.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/A__image019.png)'
  prefs: []
  type: TYPE_IMG
- en: Following the illustration in figure A.10, in this section, we will implement
    a custom Dataset class that we will use to create a training and a test dataset
    that we'll then use to create the data loaders.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating a simple toy dataset of five training examples with
    two features each. Accompanying the training examples, we also create a tensor
    containing the corresponding class labels: three examples below to class 0, and
    two examples belong to class 1\. In addition, we also make a test set consisting
    of two entries. The code to create this dataset is shown below.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.5 Creating a small toy dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Class label numbering
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: PyTorch requires that class labels start with label 0, and the largest class
    label value should not exceed the number of output nodes minus 1 (since Python
    index counting starts at 0\. So, if we have class labels 0, 1, 2, 3, and 4, the
    neural network output layer should consist of 5 nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a custom dataset class, `ToyDataset`, by subclassing from PyTorch's
    `Dataset` parent class, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.6 Defining a custom Dataset class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This custom `ToyDataset` class's purpose is to use it to instantiate a PyTorch
    `DataLoader`. But before we get to this step, let's briefly go over the general
    structure of the `ToyDataset` code.
  prefs: []
  type: TYPE_NORMAL
- en: In PyTorch, the three main components of a custom Dataset class are the `__init__`
    constructor, the `__getitem__` method, and the `__len__` method, as shown in code
    listing A.6 above.
  prefs: []
  type: TYPE_NORMAL
- en: In the `__init__` method, we set up attributes that we can access later in the
    `__getitem__` and `__len__` methods. This could be file paths, file objects, database
    connectors, and so on. Since we created a tensor dataset that sits in memory,
    we are simply assigning `X` and `y` to these attributes, which are placeholders
    for our tensor objects.
  prefs: []
  type: TYPE_NORMAL
- en: In the `__getitem__` method, we define instructions for returning exactly one
    item from the dataset via an `index`. This means the features and the class label
    corresponding to a single training example or test instance. (The data loader
    will provide this `index`, which we will cover shortly.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the `__len__` method constrains instructions for retrieving the length
    of the dataset. Here, we use the `.shape` attribute of a tensor to return the
    number of rows in the feature array. In the case of the training dataset, we have
    five rows, which we can double-check as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we defined a PyTorch Dataset class we can use for our toy dataset,
    we can use PyTorch''s `DataLoader` class to sample from it, as shown in the code
    listing below:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.7 Instantiating data loaders
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: After instantiating the training data loader, we can iterate over it as shown
    below. (The iteration over the `test_loader` works similarly but is omitted for
    brevity.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: As we can see based on the output above, the `train_loader` iterates over the
    training dataset visiting each training example exactly once. This is known as
    a training epoch. Since we seeded the random number generator using `torch.manual_seed(123)`
    above, you should get the exact same shuffling order of training examples as shown
    above. However if you iterate over the dataset a second time, you will see that
    the shuffling order will change. This is desired to prevent deep neural networks
    getting caught in repetitive update cycles during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we specified a batch size of 2 above, but the 3rd batch only contains
    a single example. That''s because we have five training examples, which is not
    evenly divisible by 2\. In practice, having a substantially smaller batch as the
    last batch in a training epoch can disturb the convergence during training. To
    prevent this, it''s recommended to set `drop_last=True`, which will drop the last
    batch in each epoch, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.8 A training loader that drops the last batch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, iterating over the training loader, we can see that the last batch is
    omitted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, let's discuss the setting `num_workers=0` in the `DataLoader`. This
    parameter in PyTorch's `DataLoader` function is crucial for parallelizing data
    loading and preprocessing. When `num_workers` is set to 0, the data loading will
    be done in the main process and not in separate worker processes. This might seem
    unproblematic, but it can lead to significant slowdowns during model training
    when we train larger networks on a GPU. This is because instead of focusing solely
    on the processing of the deep learning model, the CPU must also take time to load
    and preprocess the data. As a result, the GPU can sit idle while waiting for the
    CPU to finish these tasks. In contrast, when `num_workers` is set to a number
    greater than zero, multiple worker processes are launched to load data in parallel,
    freeing the main process to focus on training your model and better utilizing
    your system's resources, which is illustrated in figure A.11
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.11 Loading data without multiple workers (setting `num_workers=0`)
    will create a data loading bottleneck where the model sits idle until the next
    batch is loaded as illustrated in the left subpanel. If multiple workers are enabled,
    the data loader can already queue up the next batch in the background as shown
    in the right subpanel.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/A__image021.png)'
  prefs: []
  type: TYPE_IMG
- en: However, if we are working with very small datasets, setting `num_workers` to
    1 or larger may not be necessary since the total training time takes only fractions
    of a second anyway. On the contrary, if you are working with tiny datasets or
    interactive environments such as Jupyter notebooks, increasing `num_workers` may
    not provide any noticeable speedup. They might, in fact, lead to some issues.
    One potential issue is the overhead of spinning up multiple worker processes,
    which could take longer than the actual data loading when your dataset is small.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, for Jupyter notebooks, setting `num_workers` to greater than 0
    can sometimes lead to issues related to the sharing of resources between different
    processes, resulting in errors or notebook crashes. Therefore, it's essential
    to understand the trade-off and make a calculated decision on setting the `num_workers`
    parameter. When used correctly, it can be a beneficial tool but should be adapted
    to your specific dataset size and computational environment for optimal results.
  prefs: []
  type: TYPE_NORMAL
- en: In my experience, setting `num_workers=4` usually leads to optimal performance
    on many real-world datasets, but optimal settings depend on your hardware and
    the code used for loading a training example defined in the `Dataset` class.
  prefs: []
  type: TYPE_NORMAL
- en: A.7 A typical training loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we''ve discussed all the requirements for training neural networks:
    PyTorch''s tensor library, autograd, the `Module` API, and efficient data loaders.
    Let''s now combine all these things and train a neural network on the toy dataset
    from the previous section. The training code is shown in code listing A.9 below.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.9 Neural network training in PyTorch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the code in listing A.9 above yields the following outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the loss reaches zero after 3 epochs, a sign that the model converged
    on the training set. However, before we evaluate the model's predictions, let's
    go over some of the details of the preceding code listing.
  prefs: []
  type: TYPE_NORMAL
- en: First, note that we initialized a model with two inputs and two outputs. That's
    because the toy dataset from the previous section has two input features and two
    class labels to predict. We used a stochastic gradient descent (`SGD`) optimizer
    with a learning rate (`lr`) of 0.5\. The learning rate is a hyperparameter, meaning
    it's a tunable setting that we have to experiment with based on observing the
    loss. Ideally, we want to choose a learning rate such that the loss converges
    after a certain number of epochs -- the number of epochs is another hyperparameter
    to choose.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise A.3
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: How many parameters does the neural network introduced at the beginning of this
    section have?
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we often use a third dataset, a so-called validation dataset, to
    find the optimal hyperparameter settings. A validation dataset is similar to a
    test set. However, while we only want to use a test set precisely once to avoid
    biasing the evaluation, we usually use the validation set multiple times to tweak
    the model settings.
  prefs: []
  type: TYPE_NORMAL
- en: We also introduced new settings called `model.train()` and `model.eval()`. As
    these names imply, these settings are used to put the model into a training and
    an evaluation mode. This is necessary for components that behave differently during
    training and inference, such as *dropout* or *batch normalization* layers. Since
    we don't have dropout or other components in our `NeuralNetwork` class that are
    affected by these settings, using `model.train()` and `model.eval()` is redundant
    in our code above. However, it's best practice to include them anyway to avoid
    unexpected behaviors when we change the model architecture or reuse the code to
    train a different model.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed earlier, we pass the logits directly into the `cross_entropy` loss
    function, which will apply the softmax function internally for efficiency and
    numerical stability reasons. Then, calling `loss.backward()` will calculate the
    gradients in the computation graph that PyTorch constructed in the background.
    The `optimizer.step()` method will use the gradients to update the model parameters
    to minimize the loss. In the case of the SGD optimizer, this means multiplying
    the gradients with the learning rate and adding the scaled negative gradient to
    the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Preventing undesired gradient accumulation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It is important to include an `optimizer.zero_grad()` call in each update round
    to reset the gradients to zero. Otherwise, the gradients will accumulate, which
    may be undesired.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we trained the model, we can use it to make predictions, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'To obtain the class membership probabilities, we can then use PyTorch''s softmax
    function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Let's consider the first row in the code output above. Here, the first value
    (column) means that the training example has a 99.91% probability of belonging
    to class 0 and a 0.09% probability of belonging to class 1\. (The `set_printoptions`
    call is used here to make the outputs more legible.)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can convert these values into class labels predictions using PyTorch''s
    argmax function, which returns the index position of the highest value in each
    row if we set `dim=1` (setting `dim=0` would return the highest value in each
    column, instead):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that it is unnecessary to compute softmax probabilities to obtain the
    class labels. We could also apply the `argmax` function to the logits (outputs)
    directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Above, we computed the predicted labels for the training dataset. Since the
    training dataset is relatively small, we could compare it to the true training
    labels by eye and see that the model is 100% correct. We can double-check this
    using the == comparison operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `torch.sum`, we can count the number of correct prediction as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Since the dataset consists of 5 training examples, we have 5 out of 5 predictions
    that are correct, which equals 5/5 × 100% = 100% prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: However, to generalize the computation of the prediction accuracy, let's implement
    a `compute_accuracy` function as shown in the following code listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.10 A function to compute the prediction accuracy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Note that the following code listing iterates over a data loader to compute
    the number and fraction of the correct predictions. This is because when we work
    with large datasets, we typically can only call the model on a small part of the
    dataset due to memory limitations. The `compute_accuracy` function above is a
    general method that scales to datasets of arbitrary size since, in each iteration,
    the dataset chunk that the model receives is the same size as the batch size seen
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the internals of the `compute_accuracy` function are similar to
    what we used before when we converted the logits to the class labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then apply the function to the training as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The results is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we can apply the function to the test set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we learned how we can train a neural network using PyTorch.
    Next, let's see how we can save and restore models after training.
  prefs: []
  type: TYPE_NORMAL
- en: A.8 Saving and loading models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we successfully trained a model. Let's now see how
    we can save a trained model to reuse it later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the recommended way how we can save and load models in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: The model's state_dict is a Python dictionary object that maps each layer in
    the model to its trainable parameters (weights and biases). Note that `"model.pth"`
    is an arbitrary filename for the model file saved to disk. We can give it any
    name and file ending we like; however, `.pth` and `.pt` are the most common conventions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we saved the model, we can restore it from disk as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: The `torch.load("model.pth")` function reads the file `"model.pth"` and reconstructs
    the Python dictionary object containing the model's parameters while `model.load_state_dict()`
    applies these parameters to the model, effectively restoring its learned state
    from when we saved it.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the line `model = NeuralNetwork(2, 2)` above is not strictly necessary
    if you execute this code in the same session where you saved a model. However,
    I included it here to illustrate that we need an instance of the model in memory
    to apply the saved parameters. Here, the `NeuralNetwork(2, 2)` architecture needs
    to match the original saved model exactly.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are well equipped to use PyTorch to implement large language models
    in the upcoming chapters. However, before we jump to the next chapter, the last
    section will show you how to train PyTorch models faster using one or more GPUs
    (if available).
  prefs: []
  type: TYPE_NORMAL
- en: A.9 Optimizing training performance with GPUs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this last section of this chapter, we will see how we can utilize GPUs, which
    will accelerate deep neural network training compared to regular CPUs. First,
    we will introduce the main concepts behind GPU computing in PyTorch. Then, we
    will train a model on a single GPU. Finally, we'll then look at distributed training
    using multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: A.9.1 PyTorch computations on GPU devices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you will see, modifying the training loop from section 2.7 to optionally
    run on a GPU is relatively simple and only requires changing three lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Before we make the modifications, it's crucial to understand the main concept
    behind GPU computations within PyTorch. First, we need to introduce the notion
    of devices. In PyTorch, a device is where computations occur, and data resides.
    The CPU and the GPU are examples of devices. A PyTorch tensor resides in a device,
    and its operations are executed on the same device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how this works in action. Assuming that you installed a GPU-compatible
    version of PyTorch as explained in section 2.1.3, Installing PyTorch, we can double-check
    that our runtime indeed supports GPU computing via the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, suppose we have two tensors that we can add as follows -- this computation
    will be carried out on the CPU by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use the `.to()` method[[1]](#_ftn1) to transfer these tensors onto
    a GPU and perform the addition there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the resulting tensor now includes the device information, `device='cuda:0'`,
    which means that the tensors reside on the first GPU. If your machine hosts multiple
    GPUs, you have the option to specify which GPU you'd like to transfer the tensors
    to. You can do this by indicating the device ID in the transfer command. For instance,
    you can use `.to("cuda:0")`, `.to("cuda:1")`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it is important to note that all tensors must be on the same device.
    Otherwise, the computation will fail, as shown below, where one tensor resides
    on the CPU and the other on the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we learned that GPU computations on PyTorch are relatively
    straightforward. All we have to do is transfer the tensors onto the same GPU device,
    and PyTorch will handle the rest. Equipped with this information, we can now train
    the neural network from the previous section on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: A.9.2 Single-GPU training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we are familiar with transferring tensors to the GPU, we can modify
    the training loop from *section 2.7, A typical training loop*, to run on a GPU.
    This requires only changing three lines of code, as shown in code listing A.11
    below.
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.11 A training loop on a GPU
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the above code will output the following, similar to the results obtained
    on the CPU previously in section 2.7:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use `.to("cuda")` instead of `device = torch.device("cuda")`. As
    we saw in section 2.9.1, transferring a tensor to "cuda" instead of `torch.device("cuda")`
    works as well and is shorter. We can also modify the statement to the following,
    which will make the same code executable on a CPU if a GPU is not available, which
    is usually considered best practice when sharing PyTorch code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: In the case of the modified training loop above, we probably won't see a speed-up
    because of the memory transfer cost from CPU to GPU. However, we can expect a
    significant speed-up when training deep neural networks, especially large language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in this section, training a model on a single GPU in PyTorch is relatively
    easy. Next, let''s introduce another concept: training models on multiple GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch on macOS
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: On an Apple Mac with an Apple Silicon chip (like the M1, M2, M3, or newer models)
    instead of a computer with an Nvidia GPU, you can change
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: to take advantage of this chip.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise A.4
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Compare the runtime of matrix multiplication on a CPU to a GPU. At what matrix
    size do you begin to see the matrix multiplication on the GPU being faster than
    on the CPU? Hint: I recommend using the `%timeit` command in Jupyter to compare
    the runtime. For example, given matrices `a` and `b`, run the command `%timeit
    a @ b` in a new notebook cell.'
  prefs: []
  type: TYPE_NORMAL
- en: A.9.3 Training with multiple GPUs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will briefly go over the concept of distributed training.
    Distributed training is the concept of dividing the model training across multiple
    GPUs and machines.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need this? Even when it is possible to train a model on a single GPU
    or machine, the process could be exceedingly time-consuming. The training time
    can be significantly reduced by distributing the training process across multiple
    machines, each with potentially multiple GPUs. This is particularly crucial in
    the experimental stages of model development, where numerous training iterations
    might be necessary to finetune the model parameters and architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-GPU computing is optional
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For this book, it is not required to have access to or use multiple-GPU. This
    section is included for those who are interested in how multi-GPU computing works
    in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will look at the most basic case of distributed training:
    PyTorch''s `DistributedDataParallel` (DDP) strategy. DDP enables parallelism by
    splitting the input data across the available devices and processing these data
    subsets simultaneously.'
  prefs: []
  type: TYPE_NORMAL
- en: How does this work? PyTorch launches a separate process on each GPU, and each
    process receives and keeps a copy of the model -- these copies will be synchronized
    during training. To illustrate this, suppose we have two GPUs that we want to
    use to train a neural network, as shown in figure A.12.
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.12 The model and data transfer in DDP involves two key steps. First,
    we create a copy of the model on each of the GPUs. Then we divide the input data
    into unique minibatches that we pass on to each model copy.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/A__image023.png)'
  prefs: []
  type: TYPE_IMG
- en: Each of the two GPUs will receive a copy of the model. Then, in every training
    iteration, each model will receive a minibatch (or just batch) from the data loader.
    We can use a `DistributedSampler` to ensure that each GPU will receive a different,
    non-overlapping batch when using DDP.
  prefs: []
  type: TYPE_NORMAL
- en: Since each model copy will see a different sample of the training data, the
    model copies will return different logits as outputs and compute different gradients
    during the backward pass. These gradients are then averaged and synchronized during
    training to update the models. This way, we ensure that the models don't diverge,
    as illustrated in figure A.13.
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.13 The forward and backward pass in DDP are executed independently
    on each GPU with its corresponding data subset. Once the forward and backward
    passes are completed, gradients from each model replica (on each GPU) are synchronized
    across all GPUs. This ensures that every model replica has the same updated weights.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/A__image025.png)'
  prefs: []
  type: TYPE_IMG
- en: The benefit of using DDP is the enhanced speed it offers for processing the
    dataset compared to a single GPU. Barring a minor communication overhead between
    devices that comes with DDP use, it can theoretically process a training epoch
    in half the time with two GPUs compared to just one. The time efficiency scales
    up with the number of GPUs, allowing us to process an epoch eight times faster
    if we have eight GPUs, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-GPU computing in interactive environments
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: DDP does not function properly within interactive Python environments like Jupyter
    notebooks, which don't handle multiprocessing in the same way a standalone Python
    script does. Therefore, the following code should be executed as a script, not
    within a notebook interface like Jupyter. This is because DDP needs to spawn multiple
    processes, and each process should have its own Python interpreter instance.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see how this works in practice. For brevity, we will only focus on
    the core parts of the previous code that need to be adjusted for DDP training.
    However, for readers who want to run the code on their own multi-GPU machine or
    a cloud instance of their choice, it is recommended to use the standalone script
    provided in this book's GitHub repository at [https://github.com/rasbt/LLMs-from-scratch](rasbt.html).
  prefs: []
  type: TYPE_NORMAL
- en: First, we will import a few additional submodules, classes, and functions for
    distributed training PyTorch as shown in code listing A.13 below.
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.12 PyTorch utilities for distributed training
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Before we dive deeper into the changes to make the training compatible with
    DDP, let's briefly go over the rationale and usage for these newly imported utilities
    that we need alongside the `DistributedDataParallel` class.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch's `multiprocessing` submodule contains functions such as `multiprocessing.spawn`,
    which we will use to spawn multiple processes and apply a function to multiple
    inputs in parallel. We will use it to spawn one training process per GPU.
  prefs: []
  type: TYPE_NORMAL
- en: If we spawn multiple processes for training, we will need a way to divide the
    dataset among these different processes. For this, we will use the `DistributedSampler`.
  prefs: []
  type: TYPE_NORMAL
- en: The `init_process_group` and `destroy_process_group` are used to initialize
    and quit the distributed training mods. The `init_process_group` function should
    be called at the beginning of the training script to initialize a process group
    for each process in the distributed setup, and `destroy_process_group` should
    be called at the end of the training script to destroy a given process group and
    release its resources.
  prefs: []
  type: TYPE_NORMAL
- en: The following code in listing A.13 below illustrates how these new components
    are used to implement DDP training for the `NeuralNetwork` model we implemented
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.13 Model training with DistributedDataParallel strategy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: Before we run the code from listing A.13, here is a summary of how it works,
    in addition to the annotations above. We have a `__name__ == "__main__"` clause
    at the bottom containing code that is executed when we run the code as a Python
    script instead of importing it as a module. This code first prints the number
    of available GPUs using `torch.cuda.device_count()`, sets a random seed for reproducibility
    and then spawns new processes using PyTorch's `multiprocesses.spawn` function.
    Here, the `spawn` function launches one process per GPU setting `nproces=world_size`,
    where the world size is the number of available GPUs. This `spawn` function launches
    the code in the `main` function we define in the same script with some additional
    arguments provided via `args`. Note that the `main` function has a `rank` argument
    that we don't include in the `mp.spawn()` call. That's because the `rank`, which
    refers to the process ID we use as the GPU ID, is already passed automatically.
  prefs: []
  type: TYPE_NORMAL
- en: The `main` function sets up the distributed environment via `ddp_setup` -- another
    function we defined, loads the training and test sets, sets up the model, and
    carries out the training. Compared to the single-GPU training in section 2.12,
    we now transfer the model and data to the target device via .`to(rank)`, which
    we use to refer to the GPU device ID. Also, we wrap the model via `DDP`, which
    enables the synchronization of the gradients between the different GPUs during
    training. After the training finishes and we evaluate the models, we use `destroy_process_group()`
    to cleanly exit the distributed training and free up the allocated resources.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, we mentioned that each GPU will receive a different subsample of the
    training data. To ensure this, we set `sampler=DistributedSampler(train_ds)` in
    the training loader.
  prefs: []
  type: TYPE_NORMAL
- en: The last function to discuss is `ddp_setup`. It sets the main node's address
    and port to allow for communication between the different processes, initializes
    the process group with the NCCL backend (designed for GPU-to-GPU communication),
    and sets the `rank` (process identifier) and world size (total number of processes).
    Finally, it specifies the GPU device corresponding to the current model training
    process rank.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting available GPUs on a multi-GPU machine
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you wish to restrict the number of GPUs used for training on a multi-GPU
    machine, the simplest way is to use the `CUDA_VISIBLE_DEVICES` environment variable.
    To illustrate this, suppose your machine has multiple GPUs, and you only want
    to use one GPU, for example, the GPU with index 0\. Instead of `python some_script.py`,
    you can run the code from the terminal as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: Or, if your machine has four GPUs and you only want to use the first and third
    GPU, you can use
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: Setting `CUDA_VISIBLE_DEVICES` in this way is a simple and effective way to
    manage GPU allocation without modifying your PyTorch scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now run this code and see how it works in practice by launching the
    code as a script from the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that it should work on both single- and multi-GPU machines. If we run
    this code on a single GPU, we should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: The code output looks similar to the one in section 2.9.2, which is a good sanity
    check.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we run the same command and code on a machine with two GPUs, we should
    see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: As expected, we can see that some batches are processed on the first GPU (`GPU0`)
    and others on the second (`GPU1`). However, we see duplicated output lines when
    printing the training and test accuracies. This is because each process (in other
    words, each GPU) prints the test accuracy independently. Since DDP replicates
    the model onto each GPU and each process runs independently, if you have a print
    statement inside your testing loop, each process will execute it, leading to repeated
    output lines.
  prefs: []
  type: TYPE_NORMAL
- en: If this bothers you, you can fix this using the rank of each process to control
    your print statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'if rank == 0: # only print in the first process'
  prefs: []
  type: TYPE_NORMAL
- en: 'print("Test accuracy: ", accuracy)'
  prefs: []
  type: TYPE_NORMAL
- en: This is, in a nutshell, how distributed training via DDP works. If you are interested
    in additional details, I recommend checking the official API documentation at
    [https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html](generated.html).
  prefs: []
  type: TYPE_NORMAL
- en: Alternative PyTorch APIs for multi-GPU training
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'If you prefer more straightforward ways to use multiple GPUs in PyTorch, you
    can also consider add-on APIs like the open-source Fabric library, which I''ve
    written about in Accelerating PyTorch Model Training: Using Mixed-Precision and
    Fully Sharded Data Parallelism [https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training](p.html).'
  prefs: []
  type: TYPE_NORMAL
- en: A.10 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PyTorch is an open-source library that consists of three core components: a
    tensor library, automatic differentiation functions, and deep learning utilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch's tensor library is similar to array libraries like NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the context of PyTorch, tensors are array-like data structures to represent
    scalars, vectors, matrices, and higher-dimensional arrays.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch tensors can be executed on the CPU, but one major advantage of PyTorch's
    tensor format is its GPU support to accelerate computations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The automatic differentiation (autograd) capabilities in PyTorch allow us to
    conveniently train neural networks using backpropagation without manually deriving
    gradients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deep learning utilities in PyTorch provide building blocks for creating
    custom deep neural networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch includes `Dataset` and `DataLoader` classes to set up efficient data
    loading pipelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's easiest to train models on a CPU or single GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `DistributedDataParallel` is the simplest way in PyTorch to accelerate
    the training if multiple GPUs are available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A.11 Further reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While this chapter should be sufficient to get you up to speed, in addition,
    if you are looking for more comprehensive introductions to deep learning, I recommend
    the following books:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Machine Learning with PyTorch and Scikit-Learn* (2022) by Sebastian Raschka,
    Hayden Liu, and Vahid Mirjalili. ISBN 978-1801819312'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Learning with PyTorch* (2021) by Eli Stevens, Luca Antiga, and Thomas
    Viehmann. ISBN 978-1617295263'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a more thorough introduction to the concepts of tensors, readers can find
    a 15 min video tutorial that I recorded:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lecture 4.1: Tensors in Deep Learning, [https://www.youtube.com/watch?v=JXfDlgrfOBY](www.youtube.com.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you want to learn more about model evaluation in machine learning, I recommend
    my article:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning*
    (2018) by Sebastian Raschka, [https://arxiv.org/abs/1811.12808](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For readers who are interested in a refresher or gentle introduction to calculus,
    I''ve written a chapter on calculus that is freely available on my website:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Introduction to Calculus* by Sebastian Raschka, [https://sebastianraschka.com/pdf/supplementary/calculus.pdf](supplementary.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Why does PyTorch not call `optimizer.zero_grad()` automatically for us in the
    background? In some instances, it may be desirable to accumulate the gradients,
    and PyTorch will leave this as an option for us. If you want to learn more about
    gradient accumulation, please see the following article:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Finetuning Large Language Models On A Single GPU Using Gradient Accumulation*
    by Sebastian Raschka, [https://sebastianraschka.com/blog/2023/llm-grad-accumulation.html](2023.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This chapter covered DDP, which is a popular approach for training deep learning
    models across multiple GPUs. For more advanced use cases where a single model
    doesn''t fit onto the GPU, you may also consider PyTorch''s *Fully Sharded Data
    Parallel* (FSDP) method, which performs distributed data parallelism and distributes
    large layers across different GPUs. For more information, see this overview with
    further links to the API documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing PyTorch Fully Sharded Data Parallel (FSDP) API, [https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/](introducing-pytorch-fully-sharded-data-parallel-api.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A.12 Exercise answers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Exercise A.3:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The network has 2 inputs and 2 outputs. In addition, there are 2 hidden layers
    with 30 and 20 nodes, respectively. Programmatically, we can calculate the number
    of parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also calculate this manually as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'first hidden layer: 2 inputs times 30 hidden units plus 30 bias units.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'second hidden layer: 30 incoming units times 20 nodes plus 20 bias units.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'output layer: 20 incoming nodes times 2 output nodes plus 2 bias units.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, adding all the parameters in each layer results in 2×30+30 + 30×20+20
    + 20×2+2 = 752.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise A.4:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The exact runtime results will be specific to the hardware used for this experiment.
    In my experiments, I observed significant speed-ups even for small matrix multiplications
    as the following one when using a Google Colab instance connected to a V100 GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'On the CPU this resulted in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'When executed on a GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'The result was:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: In this case, on a V100, the computation was approximately four times faster.
  prefs: []
  type: TYPE_NORMAL
- en: '[[1]](#_ftnref1) This is the same .to() method we previously used to change
    a tensor''s datatype in section 2.2.2, Tensor data types.'
  prefs: []
  type: TYPE_NORMAL
