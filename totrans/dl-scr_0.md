# 前言

如果你尝试学习神经网络和深度学习，你可能会遇到大量资源，从博客文章到MOOCs（大规模在线开放课程，比如Coursera和Udacity提供的课程）的质量各异，甚至一些书籍——我知道当我几年前开始探索这个主题时也是如此。然而，如果你正在阅读这篇前言，很可能你遇到的每一个神经网络解释都在某种程度上有所欠缺。当我开始学习时，我也发现了同样的问题：各种解释就像盲人描述大象的不同部分一样，但没有一个描述整体。这就是我写这本书的原因。

关于神经网络的现有资源大多可以分为两类。一些是概念性和数学性的，包含了通常在神经网络解释中找到的圆圈通过箭头线连接的图示，以及详尽的数学解释，让你“理解理论”。这方面的典型例子是Ian Goodfellow等人的非常好的书《深度学习》（麻省理工学院出版社）。

其他资源包含了密集的代码块，如果运行，似乎显示了随着时间减少的损失值，从而神经网络“学习”。例如，PyTorch文档中的以下示例确实定义并训练了一个简单的神经网络，使用随机生成的数据：

```py
# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random input and output data
x = torch.randn(N, D_in, device=device, dtype=dtype)
y = torch.randn(N, D_out, device=device, dtype=dtype)

# Randomly initialize weights
w1 = torch.randn(D_in, H, device=device, dtype=dtype)
w2 = torch.randn(H, D_out, device=device, dtype=dtype)

learning_rate = 1e-6
for t in range(500):
    # Forward pass: compute predicted y
    h = x.mm(w1)
    h_relu = h.clamp(min=0)
    y_pred = h_relu.mm(w2)

    # Compute and print loss
    loss = (y_pred - y).pow(2).sum().item()
    print(t, loss)

    # Backprop to compute gradients of w1 and w2 with respect to loss
    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = h_relu.t().mm(grad_y_pred)
    grad_h_relu = grad_y_pred.mm(w2.t())
    grad_h = grad_h_relu.clone()
    grad_h[h < 0] = 0
    grad_w1 = x.t().mm(grad_h)

    # Update weights using gradient descent
    w1 -= learning_rate * grad_w1
    w2 -= learning_rate * grad_w2
```

当然，这样的解释并没有提供关于“真正发生了什么”的深入见解：这里包含的基本数学原理，个别神经网络组件以及它们如何一起工作等等。

一个好的神经网络解释会包含什么？为了得到答案，可以看看其他计算机科学概念是如何解释的：例如，如果你想学习排序算法，有一些教科书会包含：

+   算法的解释，用简单的英语

+   算法如何工作的视觉解释，就像你在编程面试中画在白板上的那种

+   一些关于“算法为什么有效”的数学解释

+   实现算法的伪代码

很少或者从来没有在神经网络的解释中找到这些元素并排，尽管我认为一个正确的神经网络解释应该这样做；这本书是填补这一空白的尝试。

# 理解神经网络需要多个心智模型

我不是一名研究员，也没有博士学位。然而，我曾经专业地教授数据科学：我曾经与一家名为Metis的公司一起教授了几个数据科学训练营，然后与Metis一起环游世界一年，在许多不同行业的公司进行为期一到五天的研讨会，向他们的员工解释机器学习和基本软件工程概念。我一直热爱教学，并且一直被如何最好地解释技术概念的问题所吸引，最近专注于机器学习和统计学概念。对于神经网络，我发现最具挑战性的部分是传达正确的“心智模型”，即神经网络是什么，特别是因为完全理解神经网络不仅需要一个而是*几个*心智模型，所有这些模型都阐明神经网络工作的不同（但仍然是必要的）方面。为了说明这一点：以下四个句子都是对问题“神经网络是什么？”的正确回答：

+   神经网络是一个数学函数，接受输入并产生输出。

+   神经网络是一个计算图，通过它，多维数组流动。

+   神经网络由层组成，每一层可以被认为有一定数量的“神经元”。

+   神经网络是一个通用的函数逼近器，理论上可以表示任何监督学习问题的解决方案。

实际上，你们中的许多人可能以前听过其中一个或多个，并且可能对它们的含义以及对神经网络工作方式的影响有一个合理的理解。然而，要完全理解它们，我们必须理解*所有*它们，并展示它们如何相互联系——例如，神经网络可以被表示为计算图的事实如何与“层”这个概念相联系？此外，为了使所有这些精确，我们将从头开始在Python中实现所有这些概念，并将它们组合在一起，以制作可以在您的笔记本电脑上训练的工作神经网络。尽管我们将花费大量时间在实现细节上，*在Python中实现这些模型的目的是巩固和精确化我们对概念的理解；而不是尽可能写出简洁或高性能的神经网络库。*

我的目标是在你阅读完本书后，你将对所有这些心智模型（以及它们对神经网络应该如何*实现*的影响）有坚实的理解，从而学习相关概念或在该领域进行更多项目将会更容易。

# 章节概要

前三章是最重要的，可以自成一本独立的书。

1.  在[第1章](ch01.html#foundations)中，我将展示数学函数如何被表示为一系列操作链接在一起形成一个计算图，并展示这种表示方式如何让我们使用微积分中的链式法则计算这些函数输出相对于它们的输入的导数。在本章末尾，我将介绍一个非常重要的操作，矩阵乘法，并展示它如何适应这种表示方式中的数学函数，同时仍然允许我们计算深度学习所需的导数。

1.  在[第2章](ch02.html#fundamentals)中，我们将直接使用我们在[第1章](ch01.html#foundations)中创建的基本组件来构建和训练模型，以解决一个真实世界的问题：具体来说，我们将使用它们来构建线性回归和神经网络模型，以预测一个真实世界数据集上的房价。我将展示神经网络比线性回归表现更好，并尝试解释一些原因。本章中构建模型的“第一原理”方法应该让你对神经网络的工作原理有很好的理解，但也会展示逐步、纯粹基于第一原理的方法定义深度学习模型的有限能力；这将激励我们继续学习[第3章](ch03.html#deep_learning_from_scratch)。

1.  在[第3章](ch03.html#deep_learning_from_scratch)中，我们将采用前两章基于第一原理的方法构建的基本组件，并用它们来构建组成所有深度学习模型的“高级”组件：`Layer`、`Model`、`Optimizer`等。我们将通过在[第2章](ch02.html#fundamentals)中相同数据集上训练一个从头定义的深度学习模型来结束本章，并展示它比我们简单的神经网络表现更好。

1.  事实证明，使用标准训练技术训练时，给定架构的神经网络实际上会在给定数据集上找到一个好的解决方案的理论保证很少。在[第4章](ch04.html#extensions)中，我们将介绍最重要的“训练技巧”，通常会增加神经网络找到好解决方案的概率，并在可能的情况下，给出一些数学直觉为什么它们有效。

1.  在[第5章](ch05.html#convolution)中，我涵盖了卷积神经网络（CNNs）背后的基本思想，这是一种专门用于理解图像的神经网络架构。关于CNNs有很多解释，所以我将专注于解释CNNs的绝对基础知识以及它们与常规神经网络的区别：特别是CNNs导致每一层神经元组织成“特征图”，以及两个这些层（每个由多个特征图组成）如何通过卷积滤波器连接在一起。此外，就像我们从头开始编写了神经网络中的常规层一样，我们将从头开始编写卷积层，以加强我们对它们工作原理的理解。

1.  在前五章中，我们将构建一个小型神经网络库，将神经网络定义为一系列由`Layer`组成的层，这些层本身由一系列将输入向前传递和梯度向后传递的`Operation`组成。这不是实践中大多数神经网络的实现方式；相反，它们使用一种称为*自动微分*的技术。我将在[第6章](ch06.html#recurrent)的开头快速说明自动微分，并用它来激发本章的主题：*循环神经网络*（RNNs），这是通常用于理解数据的神经网络架构，其中数据点按顺序出现，如时间序列数据或自然语言数据。我将解释“普通RNNs”的工作原理以及两个变体：*GRUs*和*LSTMs*（当然，我们会从头开始实现这三个）；在整个过程中，我将小心区分这些RNN变体之间共享的元素和这些变体之间的特定差异。

1.  最后，在[第7章](ch07.html#pytorch)中，我将展示如何使用高性能、开源的神经网络库PyTorch来实现我们在[第1](ch01.html#foundations)章至[第6](ch06.html#recurrent)章中从头开始做的一切。学习这样的框架对于推进你对神经网络的学习至关重要；但是在深入学习一个框架之前，没有对神经网络的工作原理和原因有扎实的理解，长期来看会严重限制你的学习。本书中章节的进展目标是让你有能力编写极高性能的神经网络（通过教授PyTorch），同时为你长期的学习和成功打下基础（在学习PyTorch之前教授你基础知识）。最后，我们将简要说明神经网络如何用于无监督学习。

我在这里的目标是写一本我在几年前开始学习这个主题时希望存在的书。希望你会发现这本书有帮助。继续前进！

# 本书中使用的约定

本书中使用以下排版约定：

*斜体*

表示新术语、URL、电子邮件地址、文件名和文件扩展名。

`固定宽度`

用于程序清单，以及在段落中引用程序元素，如变量或函数名称、数据库、数据类型、环境变量、语句和关键字。

**`固定宽度加粗`**

显示用户应该按照字面输入的命令或其他文本。

*`固定宽度斜体`*

用于应该由用户提供的值或由上下文确定的值替换的文本，以及代码示例中的注释。

勾股定理是<math><mrow><msup><mi>a</mi> <mn>2</mn></msup> <mo>+</mo> <msup><mi>b</mi> <mn>2</mn></msup> <mo>=</mo> <msup><mi>c</mi> <mn>2</mn></msup></mrow></math>。

###### 注意

此元素表示一般说明。

# 使用代码示例

补充材料（代码示例、练习等）可在[本书的GitHub存储库](https://oreil.ly/deep-learning-github)下载。

这本书旨在帮助您完成工作。一般来说，如果本书提供示例代码，您可以在程序和文档中使用它。除非您复制了代码的大部分内容，否则无需联系我们以获得许可。例如，编写一个程序使用本书中的几个代码块不需要许可。出售或分发包含O'Reilly图书示例的CD-ROM需要许可。引用本书并引用示例代码回答问题不需要许可。将本书中大量示例代码整合到产品文档中需要许可。

我们感谢，但不要求署名。署名通常包括标题、作者、出版商和ISBN。例如：“*Deep Learning from Scratch* by Seth Weidman (O'Reilly). Copyright 2019 Seth Weidman, 978-1-492-04141-2.”

如果您认为您对代码示例的使用超出了合理使用范围或上述许可，请随时通过[*permissions@oreilly.com*](mailto:permissions@oreilly.com)与我们联系。

# O'Reilly在线学习

###### 注

近40年来，[*O'Reilly Media*](http://oreilly.com)提供技术和商业培训、知识和见解，帮助公司取得成功。

我们独特的专家和创新者网络通过书籍、文章、会议和我们的在线学习平台分享他们的知识和专长。O'Reilly的在线学习平台为您提供按需访问实时培训课程、深入学习路径、交互式编码环境以及来自O'Reilly和其他200多家出版商的大量文本和视频。有关更多信息，请访问[*http://oreilly.com*](http://oreilly.com)。

# 如何联系我们

请将有关本书的评论和问题发送至出版商：

+   O'Reilly Media, Inc.

+   1005 Gravenstein Highway North

+   Sebastopol, CA 95472

+   800-998-9938（美国或加拿大）

+   707-829-0515（国际或本地）

+   707-829-0104（传真）

我们为这本书创建了一个网页，列出勘误、示例和任何其他信息。您可以在[*https://oreil.ly/dl-from-scratch*](https://oreil.ly/dl-from-scratch)访问此页面。

发送电子邮件至[*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com)评论或提出有关本书的技术问题。

有关我们的图书、课程、会议和新闻的更多信息，请访问我们的网站[*http://www.oreilly.com*](http://www.oreilly.com)。

在Facebook上找到我们：[*http://facebook.com/oreilly*](http://facebook.com/oreilly)

在Twitter上关注我们：[*http://twitter.com/oreillymedia*](http://twitter.com/oreillymedia)

在YouTube上观看我们：[*http://www.youtube.com/oreillymedia*](http://www.youtube.com/oreillymedia)

# 致谢

我要感谢我的编辑Melissa Potter，以及O'Reilly团队，在整个过程中他们对我的反馈非常细致，对我的问题也很及时回应。

我要特别感谢几位致力于使机器学习中的技术概念更易于为更广泛的受众所理解的人，其中有几位我有幸亲自认识：按随机生成的顺序，这些人是Brandon Rohrer、Joel Grus、Jeremy Watt和Andrew Trask。

我要感谢Metis的老板和Facebook的主管，他们非常支持我抽出时间来开展这个项目。

我要特别感谢和致谢Mat Leonard，他曾是我在这个项目中的合著者，虽然我们后来决定各自走开。Mat帮助组织了与本书相关的迷你库`lincoln`的代码，并对前两章极不完善的版本给予了非常有用的反馈，在此过程中还写了自己版本的大部分章节。

最后，我想感谢我的朋友Eva和John，他们直接鼓励和启发我开始写作。我还想感谢旧金山的许多朋友，他们容忍了我对这本书的普遍关注和担忧，以及我数月来无法和他们一起出去玩的情况，并且在我需要他们支持时始终如一。

公平地说，这个例子旨在为那些已经了解神经网络的人展示PyTorch库，而不是作为一个教程。尽管如此，许多教程都遵循这种风格，只展示代码以及一些简要的解释。

具体来说，在排序算法的情况下，算法为什么会以一个正确排序的列表终止。
