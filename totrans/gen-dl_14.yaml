- en: Chapter 10\. Advanced GANs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬10ç« . é«˜çº§GANs
- en: '[ChapterÂ 4](ch04.xhtml#chapter_gan) introduced generative adversarial networks
    (GANs), a class of generative model that has produced state-of-the-art results
    across a wide variety of image generation tasks. The flexibility in the model
    architecture and training process has led academics and deep learning practitioners
    to find new ways to design and train GANs, leading to many different advanced
    *flavors* of the architecture that we shall explore in this chapter.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[ç¬¬4ç« ](ch04.xhtml#chapter_gan)ä»‹ç»äº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ï¼Œè¿™æ˜¯ä¸€ç±»ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨å„ç§å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚æ¨¡å‹æ¶æ„å’Œè®­ç»ƒè¿‡ç¨‹çš„çµæ´»æ€§å¯¼è‡´å­¦æœ¯ç•Œå’Œæ·±åº¦å­¦ä¹ ä»ä¸šè€…æ‰¾åˆ°äº†è®¾è®¡å’Œè®­ç»ƒGANçš„æ–°æ–¹æ³•ï¼Œä»è€Œäº§ç”Ÿäº†è®¸å¤šä¸åŒçš„é«˜çº§æ¶æ„ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬ç« ä¸­æ¢è®¨ã€‚'
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: Explaining all GAN developments and their repercussions in detail could easily
    fill another book. The [GAN Zoo repository](https://oreil.ly/Oy6bR) on GitHub
    contains over 500 distinct examples of GANs with linked papers, ranging from ABC-GAN
    to ZipNet-GAN!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¦ç»†è§£é‡Šæ‰€æœ‰GANå‘å±•åŠå…¶å½±å“å¯èƒ½éœ€è¦å¦ä¸€æœ¬ä¹¦ã€‚GitHubä¸Šçš„[GAN Zooä»£ç åº“](https://oreil.ly/Oy6bR)åŒ…å«äº†500å¤šä¸ªä¸åŒçš„GANç¤ºä¾‹ï¼Œæ¶µç›–äº†ä»ABC-GANåˆ°ZipNet-GANçš„å„ç§GANï¼Œå¹¶é™„æœ‰ç›¸å…³è®ºæ–‡é“¾æ¥ï¼
- en: In this chapter we will cover the main GANs that have been influential in the
    field, including a detailed explanation of the model architecture and training
    process for each.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å¯¹è¯¥é¢†åŸŸäº§ç”Ÿå½±å“çš„ä¸»è¦GANsï¼ŒåŒ…æ‹¬å¯¹æ¯ä¸ªæ¨¡å‹çš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒè¿‡ç¨‹çš„è¯¦ç»†è§£é‡Šã€‚
- en: 'We will first explore three important models from NVIDIA that have pushed the
    boundaries of image generation: ProGAN, StyleGAN, and StyleGAN2\. We will analyze
    each of these models in enough detail to understand the fundamental concepts that
    underpin the architectures and see how they have each built on ideas from earlier
    papers.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é¦–å…ˆæ¢è®¨NVIDIAæ¨åŠ¨å›¾åƒç”Ÿæˆè¾¹ç•Œçš„ä¸‰ä¸ªé‡è¦æ¨¡å‹ï¼šProGANã€StyleGANå’ŒStyleGAN2ã€‚æˆ‘ä»¬å°†å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œè¶³å¤Ÿè¯¦ç»†çš„åˆ†æï¼Œä»¥ç†è§£æ”¯æ’‘æ¶æ„çš„åŸºæœ¬æ¦‚å¿µï¼Œå¹¶çœ‹çœ‹å®ƒä»¬å¦‚ä½•å„è‡ªå»ºç«‹åœ¨æ—©æœŸè®ºæ–‡çš„æƒ³æ³•åŸºç¡€ä¸Šã€‚
- en: 'We will also explore two other important GAN architectures that incorporate
    attention: the Self-Attention GAN (SAGAN) and BigGAN, which built on many of the
    ideas in the SAGAN paper. We have already seen the power of the attention mechanism
    in the context of Transformers in [ChapterÂ 9](ch09.xhtml#chapter_transformer).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å°†æ¢è®¨å¦å¤–ä¸¤ç§é‡è¦çš„GANæ¶æ„ï¼ŒåŒ…æ‹¬å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶çš„Self-Attention GANï¼ˆSAGANï¼‰å’ŒBigGANï¼Œåè€…åœ¨SAGANè®ºæ–‡ä¸­çš„è®¸å¤šæƒ³æ³•åŸºç¡€ä¸Šæ„å»ºã€‚æˆ‘ä»¬å·²ç»åœ¨[ç¬¬9ç« ](ch09.xhtml#chapter_transformer)ä¸­çœ‹åˆ°äº†æ³¨æ„åŠ›æœºåˆ¶åœ¨å˜æ¢å™¨ä¸­çš„å¨åŠ›ã€‚
- en: Lastly, we will cover VQ-GAN and ViT VQ-GAN, which incorporate a blend of ideas
    from variational autoencoders, Transformers, and GANs. VQ-GAN is a key component
    of Googleâ€™s state-of-the-art text-to-image generation model Muse.^([1](ch10.xhtml#idm45387005226448))
    We will explore so-called multimodal models in more detail in [ChapterÂ 13](ch13.xhtml#chapter_multimodal).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å°†ä»‹ç»VQ-GANå’ŒViT VQ-GANï¼Œå®ƒä»¬èåˆäº†å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ã€å˜æ¢å™¨å’ŒGANçš„æ€æƒ³ã€‚VQ-GANæ˜¯è°·æ­Œæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹Museçš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬å°†åœ¨[ç¬¬13ç« ](ch13.xhtml#chapter_multimodal)ä¸­æ›´è¯¦ç»†åœ°æ¢è®¨æ‰€è°“çš„å¤šæ¨¡å‹ã€‚
- en: Training Your Own Models
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ‚¨è‡ªå·±çš„æ¨¡å‹
- en: For conciseness I have chosen not to include code to directly build these models
    in the code repository for this book, but instead will point to publicly available
    implementations where possible, so that you can train your own versions if you
    wish.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘é€‰æ‹©ä¸åœ¨æœ¬ä¹¦çš„ä»£ç åº“ä¸­ç›´æ¥æ„å»ºè¿™äº›æ¨¡å‹çš„ä»£ç ï¼Œè€Œæ˜¯å°†å°½å¯èƒ½æŒ‡å‘å…¬å¼€å¯ç”¨çš„å®ç°ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥æ ¹æ®éœ€è¦è®­ç»ƒè‡ªå·±çš„ç‰ˆæœ¬ã€‚
- en: ProGAN
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ProGAN
- en: ProGAN is a technique developed by NVIDIA Labs in 2017^([2](ch10.xhtml#idm45387005216528))
    to improve both the speed and stability of GAN training. Instead of immediately
    training a GAN on full-resolution images, the ProGAN paper suggests first training
    the generator and discriminator on low-resolution images of, say, 4 Ã— 4 pixels
    and then incrementally adding layers throughout the training process to increase
    the resolution.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ProGANæ˜¯NVIDIAå®éªŒå®¤åœ¨2017å¹´å¼€å‘çš„ä¸€ç§æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜GANè®­ç»ƒçš„é€Ÿåº¦å’Œç¨³å®šæ€§ã€‚ProGANè®ºæ–‡å»ºè®®ï¼Œä¸è¦ç«‹å³åœ¨å…¨åˆ†è¾¨ç‡å›¾åƒä¸Šè®­ç»ƒGANï¼Œè€Œæ˜¯é¦–å…ˆåœ¨ä½åˆ†è¾¨ç‡å›¾åƒï¼ˆä¾‹å¦‚4Ã—4åƒç´ ï¼‰ä¸Šè®­ç»ƒç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨ï¼Œç„¶ååœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥æ·»åŠ å±‚ä»¥å¢åŠ åˆ†è¾¨ç‡ã€‚
- en: Letâ€™s take a look at the concept of *progressive training* in more detail.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°äº†è§£*æ¸è¿›å¼è®­ç»ƒ*çš„æ¦‚å¿µã€‚
- en: Training Your Own ProGAN
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ‚¨è‡ªå·±çš„ProGAN
- en: There is an excellent tutorial by Bharath K on training your own ProGAN using
    Keras available on the [Paperspace blog](https://oreil.ly/b2CJm). Bear in mind
    that training a ProGAN to achieve the results from the paper requires a significant
    amount of computing power.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Bharath Kåœ¨[Paperspaceåšå®¢](https://oreil.ly/b2CJm)ä¸Šæä¾›äº†ä¸€ä¸ªå…³äºä½¿ç”¨Kerasè®­ç»ƒè‡ªå·±çš„ProGANçš„ä¼˜ç§€æ•™ç¨‹ã€‚è¯·è®°ä½ï¼Œè®­ç»ƒProGANä»¥è¾¾åˆ°è®ºæ–‡ä¸­çš„ç»“æœéœ€è¦å¤§é‡çš„è®¡ç®—èƒ½åŠ›ã€‚
- en: Progressive Training
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¸è¿›å¼è®­ç»ƒ
- en: As always with GANs, we build two independent networks, the generator and discriminator,
    with a fight for dominance taking place during the training process.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸GANsä¸€æ ·ï¼Œæˆ‘ä»¬æ„å»ºä¸¤ä¸ªç‹¬ç«‹çš„ç½‘ç»œï¼Œç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œç»Ÿæ²»ä¹‹äº‰ã€‚
- en: In a normal GAN, the generator always outputs full-resolution images, even in
    the early stages of training. It is reasonable to think that this strategy might
    not be optimalâ€”the generator might be slow to learn high-level structures in the
    early stages of training, because it is immediately operating over complex, high-resolution
    images. Wouldnâ€™t it be better to first train a lightweight GAN to output accurate
    low-resolution images and then see if we can build on this to gradually increase
    the resolution?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ™®é€šçš„GANä¸­ï¼Œç”Ÿæˆå™¨æ€»æ˜¯è¾“å‡ºå…¨åˆ†è¾¨ç‡å›¾åƒï¼Œå³ä½¿åœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µä¹Ÿæ˜¯å¦‚æ­¤ã€‚å¯ä»¥åˆç†åœ°è®¤ä¸ºï¼Œè¿™ç§ç­–ç•¥å¯èƒ½ä¸æ˜¯æœ€ä½³çš„â€”â€”ç”Ÿæˆå™¨å¯èƒ½åœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µå­¦ä¹ é«˜çº§ç»“æ„è¾ƒæ…¢ï¼Œå› ä¸ºå®ƒç«‹å³åœ¨å¤æ‚çš„é«˜åˆ†è¾¨ç‡å›¾åƒä¸Šæ“ä½œã€‚é¦–å…ˆè®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„GANä»¥è¾“å‡ºå‡†ç¡®çš„ä½åˆ†è¾¨ç‡å›¾åƒï¼Œç„¶åé€æ¸å¢åŠ åˆ†è¾¨ç‡ï¼Œè¿™æ ·åšä¼šæ›´å¥½å—ï¼Ÿ
- en: This simple idea leads us to *progressive training*, one of the key contributions
    of the ProGAN paper. The ProGAN is trained in stages, starting with a training
    set that has been condensed down to 4 Ã— 4â€“pixel images using interpolation, as
    shown in [FigureÂ 10-1](Images/#condensed_images).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç®€å•çš„æƒ³æ³•å¼•å¯¼æˆ‘ä»¬è¿›å…¥*æ¸è¿›å¼è®­ç»ƒ*ï¼Œè¿™æ˜¯ProGANè®ºæ–‡çš„ä¸€ä¸ªå…³é”®è´¡çŒ®ã€‚ProGANåˆ†é˜¶æ®µè®­ç»ƒï¼Œä»ä¸€ä¸ªå·²ç»é€šè¿‡æ’å€¼å‹ç¼©åˆ°4Ã—4åƒç´ å›¾åƒçš„è®­ç»ƒé›†å¼€å§‹ï¼Œå¦‚[å›¾10-1](Images/#condensed_images)æ‰€ç¤ºã€‚
- en: '![](Images/gdl2_1001.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1001.png)'
- en: Figure 10-1\. Images in the dataset can be compressed to lower resolution using
    interpolation
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾10-1ã€‚æ•°æ®é›†ä¸­çš„å›¾åƒå¯ä»¥ä½¿ç”¨æ’å€¼å‹ç¼©åˆ°è¾ƒä½åˆ†è¾¨ç‡
- en: We can then initially train the generator to transform a latent input noise
    vector <math alttext="z"><mi>z</mi></math> (say, of length 512) into an image
    of shape 4 Ã— 4 Ã— 3\. The matching discriminator will need to transform an input
    image of size 4 Ã— 4 Ã— 3 into a single scalar prediction. The network architectures
    for this first step are shown in [FigureÂ 10-2](#progan_4).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æœ€åˆè®­ç»ƒç”Ÿæˆå™¨ï¼Œå°†æ½œåœ¨è¾“å…¥å™ªå£°å‘é‡<math alttext="z"><mi>z</mi></math>ï¼ˆæ¯”å¦‚é•¿åº¦ä¸º512ï¼‰è½¬æ¢ä¸ºå½¢çŠ¶ä¸º4Ã—4Ã—3çš„å›¾åƒã€‚åŒ¹é…çš„é‰´åˆ«å™¨éœ€è¦å°†å¤§å°ä¸º4Ã—4Ã—3çš„è¾“å…¥å›¾åƒè½¬æ¢ä¸ºå•ä¸ªæ ‡é‡é¢„æµ‹ã€‚è¿™ç¬¬ä¸€æ­¥çš„ç½‘ç»œæ¶æ„å¦‚[å›¾10-2](#progan_4)æ‰€ç¤ºã€‚
- en: The blue box in the generator represents the convolutional layer that converts
    the set of feature maps into an RGB image (`toRGB`), and the blue box in the discriminator
    represents the convolutional layer that converts the RGB images into a set of
    feature maps (`fromRGB`).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå™¨ä¸­çš„è“è‰²æ¡†è¡¨ç¤ºå°†ç‰¹å¾å›¾è½¬æ¢ä¸ºRGBå›¾åƒçš„å·ç§¯å±‚ï¼ˆ`toRGB`ï¼‰ï¼Œé‰´åˆ«å™¨ä¸­çš„è“è‰²æ¡†è¡¨ç¤ºå°†RGBå›¾åƒè½¬æ¢ä¸ºä¸€ç»„ç‰¹å¾å›¾çš„å·ç§¯å±‚ï¼ˆ`fromRGB`ï¼‰ã€‚
- en: '![](Images/gdl2_1002.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1002.png)'
- en: Figure 10-2\. The generator and discriminator architectures for the first stage
    of the ProGAN training process
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾10-2ã€‚ProGANè®­ç»ƒè¿‡ç¨‹çš„ç¬¬ä¸€é˜¶æ®µçš„ç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨æ¶æ„
- en: In the paper, the authors train this pair of networks until the discriminator
    has seen 800,000 real images. We now need to understand how the generator and
    discriminator are expanded to work with 8 Ã— 8â€“pixel images.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®ºæ–‡ä¸­ï¼Œä½œè€…è®­ç»ƒè¿™å¯¹ç½‘ç»œï¼Œç›´åˆ°é‰´åˆ«å™¨çœ‹åˆ°äº†800,000å¼ çœŸå®å›¾åƒã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦äº†è§£å¦‚ä½•æ‰©å±•ç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨ä»¥å¤„ç†8Ã—8åƒç´ å›¾åƒã€‚
- en: To expand the generator and discriminator, we need to blend in additional layers.
    This is managed in two phases, transition and stabilization, as shown in [FigureÂ 10-3](#progan_training_gen).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ‰©å±•ç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨ï¼Œæˆ‘ä»¬éœ€è¦èå…¥é¢å¤–çš„å±‚ã€‚è¿™åœ¨ä¸¤ä¸ªé˜¶æ®µä¸­è¿›è¡Œï¼Œè¿‡æ¸¡å’Œç¨³å®šï¼Œå¦‚[å›¾10-3](#progan_training_gen)æ‰€ç¤ºã€‚
- en: '![](Images/gdl2_1003.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1003.png)'
- en: Figure 10-3\. The ProGAN generator training process, expanding the network from
    4 Ã— 4 images to 8 Ã— 8 (dotted lines represent the rest of the network, not shown)
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾10-3ã€‚ProGANç”Ÿæˆå™¨è®­ç»ƒè¿‡ç¨‹ï¼Œå°†ç½‘ç»œä»4Ã—4å›¾åƒæ‰©å±•åˆ°8Ã—8ï¼ˆè™šçº¿ä»£è¡¨ç½‘ç»œçš„å…¶ä½™éƒ¨åˆ†ï¼Œæœªæ˜¾ç¤ºï¼‰
- en: Letâ€™s first look at the generator. During the *transition phase*, new upsampling
    and convolutional layers are appended to the existing network, with a residual
    connection set up to maintain the output from the existing trained `toRGB` layer.
    Crucially, the new layers are initially masked using a parameter <math alttext="alpha"><mi>Î±</mi></math>
    that is gradually increased from 0 to 1 throughout the transition phase to allow
    more of the new `toRGB` output through and less of the existing `toRGB` layer.
    This is to avoid a *shock* to the network as the new layers take over.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆçœ‹ä¸€ä¸‹ç”Ÿæˆå™¨ã€‚åœ¨*è¿‡æ¸¡é˜¶æ®µ*ä¸­ï¼Œæ–°çš„ä¸Šé‡‡æ ·å’Œå·ç§¯å±‚è¢«é™„åŠ åˆ°ç°æœ‰ç½‘ç»œä¸­ï¼Œå»ºç«‹äº†ä¸€ä¸ªæ®‹å·®è¿æ¥ä»¥ä¿æŒç°æœ‰è®­ç»ƒè¿‡çš„`toRGB`å±‚çš„è¾“å‡ºã€‚å…³é”®çš„æ˜¯ï¼Œæ–°å±‚æœ€åˆä½¿ç”¨ä¸€ä¸ªå‚æ•°<math
    alttext="alpha"><mi>Î±</mi></math>è¿›è¡Œæ©è”½ï¼Œè¯¥å‚æ•°åœ¨æ•´ä¸ªè¿‡æ¸¡é˜¶æ®µé€æ¸ä»0å¢åŠ åˆ°1ï¼Œä»¥å…è®¸æ›´å¤šæ–°çš„`toRGB`è¾“å‡ºé€šè¿‡ï¼Œå‡å°‘ç°æœ‰çš„`toRGB`å±‚ã€‚è¿™æ˜¯ä¸ºäº†é¿å…ç½‘ç»œåœ¨æ–°å±‚æ¥ç®¡æ—¶å‡ºç°*å†²å‡»*ã€‚
- en: Eventually, there is no flow through the old `toRGB` layer and the network enters
    the *stabilization phase*â€”a further period of training where the network can fine-tune
    the output, without any flow through the old `toRGB` layer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œæ—§çš„`toRGB`å±‚ä¸å†æœ‰è¾“å‡ºæµï¼Œç½‘ç»œè¿›å…¥*ç¨³å®šé˜¶æ®µ*â€”â€”è¿›ä¸€æ­¥çš„è®­ç»ƒæœŸé—´ï¼Œç½‘ç»œå¯ä»¥å¾®è°ƒè¾“å‡ºï¼Œè€Œä¸ç»è¿‡æ—§çš„`toRGB`å±‚ã€‚
- en: The discriminator uses a similar process, as shown in [FigureÂ 10-4](#progan_training_dis).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´åˆ«å™¨ä½¿ç”¨ç±»ä¼¼çš„è¿‡ç¨‹ï¼Œå¦‚[å›¾10-4](#progan_training_dis)æ‰€ç¤ºã€‚
- en: '![](Images/gdl2_1004.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1004.png)'
- en: Figure 10-4\. The ProGAN discriminator training process, expanding the network
    from 4 Ã— 4 images to 8 Ã— 8 (dotted lines represent the rest of the network, not
    shown)
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾10-4ã€‚ProGANé‰´åˆ«å™¨è®­ç»ƒè¿‡ç¨‹ï¼Œå°†ç½‘ç»œä»4Ã—4å›¾åƒæ‰©å±•åˆ°8Ã—8ï¼ˆè™šçº¿ä»£è¡¨ç½‘ç»œçš„å…¶ä½™éƒ¨åˆ†ï¼Œæœªæ˜¾ç¤ºï¼‰
- en: Here, we need to blend in additional downscaling and convolutional layers. Again,
    the layers are injected into the networkâ€”this time at the start of the network,
    just after the input image. The existing `fromRGB` layer is connected via a residual
    connection and gradually phased out as the new layers take over during the transition
    phase. The stabilization phase allows the discriminator to fine-tune using the
    new layers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬éœ€è¦èå…¥é¢å¤–çš„é™é‡‡æ ·å’Œå·ç§¯å±‚ã€‚åŒæ ·ï¼Œè¿™äº›å±‚è¢«æ³¨å…¥åˆ°ç½‘ç»œä¸­â€”â€”è¿™æ¬¡æ˜¯åœ¨ç½‘ç»œçš„å¼€å§‹éƒ¨åˆ†ï¼Œå°±åœ¨è¾“å…¥å›¾åƒä¹‹åã€‚ç°æœ‰çš„`fromRGB`å±‚é€šè¿‡æ®‹å·®è¿æ¥è¿æ¥ï¼Œå¹¶åœ¨è¿‡æ¸¡é˜¶æ®µé€æ¸æ·¡å‡ºï¼Œéšç€æ–°å±‚åœ¨è¿‡æ¸¡é˜¶æ®µæ¥ç®¡æ—¶é€æ¸æ·¡å‡ºã€‚ç¨³å®šé˜¶æ®µå…è®¸é‰´åˆ«å™¨ä½¿ç”¨æ–°å±‚è¿›è¡Œå¾®è°ƒã€‚
- en: All transition and stabilization phases last until the discriminator has been
    shown 800,000 real images. Note that even through the network is trained progressively,
    no layers are *frozen*. Throughout the training process, all layers remain fully
    trainable.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿‡æ¸¡å’Œç¨³å®šé˜¶æ®µæŒç»­åˆ°é‰´åˆ«å™¨å·²ç»çœ‹åˆ°äº†800,000å¼ çœŸå®å›¾åƒã€‚è¯·æ³¨æ„ï¼Œå³ä½¿ç½‘ç»œæ˜¯æ¸è¿›è®­ç»ƒçš„ï¼Œä¹Ÿæ²¡æœ‰å±‚è¢«*å†»ç»“*ã€‚åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ‰€æœ‰å±‚éƒ½ä¿æŒå®Œå…¨å¯è®­ç»ƒã€‚
- en: This process continues, growing the GAN from 4 Ã— 4 images to 8 Ã— 8, then 16
    Ã— 16, 32 Ã— 32, and so on, until it reaches full resolution (1,024 Ã— 1,024), as
    shown in [FigureÂ 10-5](#progan).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹ç»§ç»­è¿›è¡Œï¼Œå°†GANä»4Ã—4å›¾åƒæ‰©å±•åˆ°8Ã—8ï¼Œç„¶å16Ã—16ï¼Œ32Ã—32ï¼Œä¾æ­¤ç±»æ¨ï¼Œç›´åˆ°è¾¾åˆ°å®Œæ•´åˆ†è¾¨ç‡ï¼ˆ1,024Ã—1,024ï¼‰ï¼Œå¦‚[å›¾10-5](#progan)æ‰€ç¤ºã€‚
- en: '![](Images/gdl2_1005.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1005.png)'
- en: 'Figure 10-5\. The ProGAN training mechanism, and some example generated faces
    (source: [Karras et al., 2017](https://arxiv.org/abs/1710.10196))'
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾10-5ã€‚ProGANè®­ç»ƒæœºåˆ¶ï¼Œä»¥åŠä¸€äº›ç¤ºä¾‹ç”Ÿæˆçš„äººè„¸ï¼ˆæ¥æºï¼š[Karrasç­‰äººï¼Œ2017](https://arxiv.org/abs/1710.10196)ï¼‰
- en: The overall structure of the generator and discriminator after the full progressive
    training process is complete is shown in [FigureÂ 10-6](#progan_network_diagram).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1006.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-6\. The ProGAN generator and discriminator used to generate 1,024
    Ã— 1,024â€“pixel CelebA faces (source: [Karras et al., 2018](https://arxiv.org/abs/1812.04948))'
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The paper also makes several other important contributions, namely minibatch
    standard deviation, equalized learning rates, and pixelwise normalization, which
    are described briefly in the following sections.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Minibatch standard deviation
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *minibatch standard deviation* layer is an extra layer in the discriminator
    that appends the standard deviation of the feature values, averaged across all
    pixels and across the minibatch as an additional (constant) feature. This helps
    to ensure the generator creates more variety in its outputâ€”if variety is low across
    the minibatch, then the standard deviation will be small, and the discriminator
    can use this feature to distinguish the fake batches from the real batches! Therefore,
    the generator is incentivized to ensure it generates a similar amount of variety
    as is present in the real training data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Equalized learning rates
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All dense and convolutional layers in ProGAN use *equalized learning rates*.
    Usually, weights in a neural network are initialized using a method such as *He
    initialization*â€”a Gaussian distribution where the standard deviation is scaled
    to be inversely proportional to the square root of the number of inputs to the
    layer. This way, layers with a greater number of inputs will be initialized with
    weights that have a smaller deviation from zero, which generally improves the
    stability of the training process.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: The authors of the ProGAN paper found that this was causing problems when used
    in combination with modern optimizers such as Adam or RMSProp. These methods normalize
    the gradient update for each weight, so that the size of the update is independent
    of the scale (magnitude) of the weight. However, this means that weights with
    a larger dynamic range (i.e., layers with fewer inputs) will take comparatively
    longer to adjust than weights with a smaller dynamic range (i.e., layers with
    more inputs). It was found that this causes an imbalance between the speed of
    training of the different layers of the generator and discriminator in ProGAN,
    so they used *equalized learning rates* to solve this problem.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: In ProGAN, weights are initialized using a simple standard Gaussian, regardless
    of the number of inputs to the layer. The normalization is applied dynamically,
    as part of the call to the layer, rather than only at initialization. This way,
    the optimizer sees each weight as having approximately the same dynamic range,
    so it applies the same learning rate. It is only when the layer is called that
    the weight is scaled by the factor from the He initializer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Pixelwise normalization
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lastly, in ProGAN *pixelwise normalization* is used in the generator, rather
    than batch normalization. This normalizes the feature vector in each pixel to
    a unit length and helps to prevent the signal from spiraling out of control as
    it propagates through the network. The pixelwise normalization layer has no trainable
    weights.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Outputs
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the CelebA dataset, ProGAN was also applied to images from the
    Large-scale Scene Understanding (LSUN) dataset with excellent results, as shown
    in [FigureÂ 10-7](#progan_examples). This demonstrated the power of ProGAN over
    earlier GAN architectures and paved the way for future iterations such as StyleGAN
    and StyleGAN2, which we shall explore in the next sections.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1007.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-7\. Generated examples from a ProGAN trained progressively on the
    LSUN dataset at 256 Ã— 256 resolution (source: [Karras et al., 2017](https://arxiv.org/abs/1710.10196))'
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: StyleGAN
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: StyleGAN^([3](ch10.xhtml#idm45387005140128)) is a GAN architecture from 2018
    that builds on the earlier ideas in the ProGAN paper. In fact, the discriminator
    is identical; only the generator is changed.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: StyleGAN^([3](ch10.xhtml#idm45387005140128))æ˜¯2018å¹´çš„ä¸€ä¸ªGANæ¶æ„ï¼Œå»ºç«‹åœ¨ProGANè®ºæ–‡ä¸­çš„æ—©æœŸæ€æƒ³åŸºç¡€ä¸Šã€‚å®é™…ä¸Šï¼Œé‰´åˆ«å™¨æ˜¯ç›¸åŒçš„ï¼›åªæœ‰ç”Ÿæˆå™¨è¢«æ”¹å˜ã€‚
- en: Often when training GANs it is difficult to separate out vectors in the latent
    space corresponding to high-level attributesâ€”they are frequently *entangled*,
    meaning that adjusting an image in the latent space to give a face more freckles,
    for example, might also inadvertently change the background color. While ProGAN
    generates fantastically realistic images, it is no exception to this general rule.
    We would ideally like to have full control of the style of the image, and this
    requires a disentangled separation of features in the latent space.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸åœ¨è®­ç»ƒGANæ—¶ï¼Œå¾ˆéš¾å°†æ½œåœ¨ç©ºé—´ä¸­å¯¹åº”äºé«˜çº§å±æ€§çš„å‘é‡åˆ†ç¦»å‡ºæ¥â€”â€”å®ƒä»¬ç»å¸¸æ˜¯*çº ç¼ åœ¨ä¸€èµ·*ï¼Œè¿™æ„å‘³ç€è°ƒæ•´æ½œåœ¨ç©ºé—´ä¸­çš„å›¾åƒä»¥ä½¿è„¸éƒ¨æ›´å¤šé›€æ–‘ï¼Œä¾‹å¦‚ï¼Œå¯èƒ½ä¹Ÿä¼šæ— æ„ä¸­æ”¹å˜èƒŒæ™¯é¢œè‰²ã€‚è™½ç„¶ProGANç”Ÿæˆäº†æå…¶é€¼çœŸçš„å›¾åƒï¼Œä½†å®ƒä¹Ÿä¸ä¾‹å¤–ã€‚æˆ‘ä»¬ç†æƒ³æƒ…å†µä¸‹å¸Œæœ›å®Œå…¨æ§åˆ¶å›¾åƒçš„é£æ ¼ï¼Œè¿™éœ€è¦åœ¨æ½œåœ¨ç©ºé—´ä¸­å¯¹ç‰¹å¾è¿›è¡Œåˆ†ç¦»ã€‚
- en: 'StyleGAN achieves this by explicitly injecting style vectors into the network
    at different points: some that control high-level features (e.g., face orientation)
    and some that control low-level details (e.g., the way the hair falls across the
    forehead).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: StyleGANé€šè¿‡åœ¨ç½‘ç»œçš„ä¸åŒç‚¹æ˜¾å¼æ³¨å…¥é£æ ¼å‘é‡æ¥å®ç°è¿™ä¸€ç‚¹ï¼šä¸€äº›æ§åˆ¶é«˜çº§ç‰¹å¾ï¼ˆä¾‹å¦‚ï¼Œé¢éƒ¨æ–¹å‘ï¼‰çš„å‘é‡ï¼Œä¸€äº›æ§åˆ¶ä½çº§ç»†èŠ‚ï¼ˆä¾‹å¦‚ï¼Œå¤´å‘å¦‚ä½•è½åœ¨é¢å¤´ä¸Šï¼‰çš„å‘é‡ã€‚
- en: The overall architecture of the StyleGAN generator is shown in [FigureÂ 10-8](#stylegan_arch).
    Letâ€™s walk through this architecture step by step, starting with the mapping network.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: StyleGANç”Ÿæˆå™¨çš„æ•´ä½“æ¶æ„å¦‚[å›¾10-8](#stylegan_arch)æ‰€ç¤ºã€‚è®©æˆ‘ä»¬é€æ­¥èµ°è¿‡è¿™ä¸ªæ¶æ„ï¼Œä»æ˜ å°„ç½‘ç»œå¼€å§‹ã€‚
- en: '![](Images/gdl2_1008.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1008.png)'
- en: 'Figure 10-8\. The StyleGAN generator architecture (source: [Karras et al.,
    2018](https://arxiv.org/abs/1812.04948))'
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾10-8ã€‚StyleGANç”Ÿæˆå™¨æ¶æ„ï¼ˆæ¥æºï¼š[Karras et al., 2018](https://arxiv.org/abs/1812.04948)ï¼‰
- en: Training Your Own StyleGAN
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ‚¨è‡ªå·±çš„StyleGAN
- en: There is an excellent tutorial by Soon-Yau Cheong on training your own StyleGAN
    using Keras available on the [Keras website](https://oreil.ly/MooSe). Bear in
    mind that training a StyleGAN to achieve the results from the paper requires a
    significant amount of computing power.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Soon-Yau Cheongåœ¨[Kerasç½‘ç«™](https://oreil.ly/MooSe)ä¸Šæä¾›äº†ä¸€ä¸ªå…³äºä½¿ç”¨Kerasè®­ç»ƒè‡ªå·±çš„StyleGANçš„ä¼˜ç§€æ•™ç¨‹ã€‚è¯·è®°ä½ï¼Œè¦å®ç°è®ºæ–‡ä¸­çš„ç»“æœï¼Œè®­ç»ƒStyleGANéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚
- en: The Mapping Network
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ˜ å°„ç½‘ç»œ
- en: The *mapping network* <math alttext="f"><mi>f</mi></math> is a simple feed-forward
    network that converts the input noise <math alttext="bold z element-of script
    upper Z"><mrow><mi>ğ³</mi> <mo>âˆˆ</mo> <mi>ğ’µ</mi></mrow></math> into a different
    latent space <math alttext="bold w element-of script upper W"><mrow><mi>ğ°</mi>
    <mo>âˆˆ</mo> <mi>ğ’²</mi></mrow></math> . This gives the generator the opportunity
    to disentangle the noisy input vector into distinct factors of variation, which
    can be easily picked up by the downstream style-generating layers.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ˜ å°„ç½‘ç»œ* <math alttext="f"><mi>f</mi></math> æ˜¯ä¸€ä¸ªç®€å•çš„å‰é¦ˆç½‘ç»œï¼Œå°†è¾“å…¥å™ªå£° <math alttext="bold
    z element-of script upper Z"><mrow><mi>ğ³</mi> <mo>âˆˆ</mo> <mi>ğ’µ</mi></mrow></math>
    è½¬æ¢ä¸ºä¸åŒçš„æ½œåœ¨ç©ºé—´ <math alttext="bold w element-of script upper W"><mrow><mi>ğ°</mi> <mo>âˆˆ</mo>
    <mi>ğ’²</mi></mrow></math>ã€‚è¿™ä½¿å¾—ç”Ÿæˆå™¨æœ‰æœºä¼šå°†å˜ˆæ‚çš„è¾“å…¥å‘é‡åˆ†è§£ä¸ºä¸åŒçš„å˜åŒ–å› ç´ ï¼Œè¿™äº›å› ç´ å¯ä»¥è¢«ä¸‹æ¸¸çš„é£æ ¼ç”Ÿæˆå±‚è½»æ¾æ•æ‰åˆ°ã€‚'
- en: The point of doing this is to separate out the process of choosing a style for
    the image (the mapping network) from the generation of an image with a given style
    (the synthesis network).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·åšçš„ç›®çš„æ˜¯å°†å›¾åƒçš„é£æ ¼é€‰æ‹©è¿‡ç¨‹ï¼ˆæ˜ å°„ç½‘ç»œï¼‰ä¸ç”Ÿæˆå…·æœ‰ç»™å®šé£æ ¼çš„å›¾åƒçš„è¿‡ç¨‹ï¼ˆåˆæˆç½‘ç»œï¼‰åˆ†å¼€ã€‚
- en: The Synthesis Network
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆæˆç½‘ç»œ
- en: 'The synthesis network is the generator of the actual image with a given style,
    as provided by the mapping network. As can be seen from [FigureÂ 10-8](#stylegan_arch),
    the style vector <math alttext="bold w"><mi>ğ°</mi></math> is injected into the
    synthesis network at different points, each time via a differently densely connected
    layer <math alttext="upper A Subscript i"><msub><mi>A</mi> <mi>i</mi></msub></math>
    , which generates two vectors: a bias vector <math alttext="bold y Subscript b
    comma i"><msub><mi>ğ²</mi> <mrow><mi>b</mi><mo>,</mo><mi>i</mi></mrow></msub></math>
    and a scaling vector <math alttext="bold y Subscript s comma i"><msub><mi>ğ²</mi>
    <mrow><mi>s</mi><mo>,</mo><mi>i</mi></mrow></msub></math> . These vectors define
    the specific style that should be injected at this point in the networkâ€”that is,
    they tell the synthesis network how to adjust the feature maps to move the generated
    image in the direction of the specified style.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åˆæˆç½‘ç»œæ˜¯ç”Ÿæˆå…·æœ‰ç»™å®šé£æ ¼çš„å®é™…å›¾åƒçš„ç”Ÿæˆå™¨ï¼Œç”±æ˜ å°„ç½‘ç»œæä¾›ã€‚å¦‚[å›¾10-8](#stylegan_arch)æ‰€ç¤ºï¼Œé£æ ¼å‘é‡ <math alttext="bold
    w"><mi>ğ°</mi></math> è¢«æ³¨å…¥åˆ°åˆæˆç½‘ç»œçš„ä¸åŒç‚¹ï¼Œæ¯æ¬¡é€šè¿‡ä¸åŒçš„å¯†é›†è¿æ¥å±‚ <math alttext="upper A Subscript
    i"><msub><mi>A</mi> <mi>i</mi></msub></math>ï¼Œç”Ÿæˆä¸¤ä¸ªå‘é‡ï¼šä¸€ä¸ªåç½®å‘é‡ <math alttext="bold
    y Subscript b comma i"><msub><mi>ğ²</mi> <mrow><mi>b</mi><mo>,</mo><mi>i</mi></mrow></msub></math>
    å’Œä¸€ä¸ªç¼©æ”¾å‘é‡ <math alttext="bold y Subscript s comma i"><msub><mi>ğ²</mi> <mrow><mi>s</mi><mo>,</mo><mi>i</mi></mrow></msub></math>ã€‚è¿™äº›å‘é‡å®šä¹‰äº†åº”è¯¥åœ¨ç½‘ç»œä¸­çš„è¿™ä¸€ç‚¹æ³¨å…¥çš„ç‰¹å®šé£æ ¼ï¼Œä¹Ÿå°±æ˜¯å‘Šè¯‰åˆæˆç½‘ç»œå¦‚ä½•è°ƒæ•´ç‰¹å¾å›¾ä»¥ä½¿ç”Ÿæˆçš„å›¾åƒæœç€æŒ‡å®šçš„é£æ ¼æ–¹å‘ç§»åŠ¨ã€‚
- en: This adjustment is achieved through *adaptive instance normalization* (AdaIN)
    layers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡*è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–*ï¼ˆAdaINï¼‰å±‚å®ç°è¿™ç§è°ƒæ•´ã€‚
- en: Adaptive instance normalization
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–
- en: 'An AdaIN layer is a type of neural network layer that adjusts the mean and
    variance of each feature map <math alttext="bold x Subscript i"><msub><mi>ğ±</mi>
    <mi>i</mi></msub></math> with a reference style bias <math alttext="bold y Subscript
    b comma i"><msub><mi>ğ²</mi> <mrow><mi>b</mi><mo>,</mo><mi>i</mi></mrow></msub></math>
    and scale <math alttext="bold y Subscript s comma i"><msub><mi>ğ²</mi> <mrow><mi>s</mi><mo>,</mo><mi>i</mi></mrow></msub></math>
    , respectively.^([4](ch10.xhtml#idm45387005090240)) Both vectors are of length
    equal to the number of channels output from the preceding convolutional layer
    in the synthesis network. The equation for adaptive instance normalization is
    as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartLayout 1st Row  AdaIN left-parenthesis bold x Subscript
    i Baseline comma bold y right-parenthesis equals bold y Subscript s comma i Baseline
    StartFraction bold x Subscript i Baseline minus mu left-parenthesis bold x Subscript
    i Baseline right-parenthesis Over sigma left-parenthesis bold x Subscript i Baseline
    right-parenthesis EndFraction plus bold y Subscript b comma i Baseline EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mtext>AdaIN</mtext>
    <mrow><mo>(</mo> <msub><mi>ğ±</mi> <mi>i</mi></msub> <mo>,</mo> <mi>ğ²</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>ğ²</mi> <mrow><mi>s</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mfrac><mrow><msub><mi>ğ±</mi> <mi>i</mi></msub> <mo>-</mo><mi>Î¼</mi><mrow><mo>(</mo><msub><mi>ğ±</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow> <mrow><mi>Ïƒ</mi><mo>(</mo><msub><mi>ğ±</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mfrac> <mo>+</mo> <msub><mi>ğ²</mi> <mrow><mi>b</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></mtd></mtr></mtable></math>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row  AdaIN left-parenthesis bold x Subscript
    i Baseline comma bold y right-parenthesis equals bold y Subscript s comma i Baseline
    StartFraction bold x Subscript i Baseline minus mu left-parenthesis bold x Subscript
    i Baseline right-parenthesis Over sigma left-parenthesis bold x Subscript i Baseline
    right-parenthesis EndFraction plus bold y Subscript b comma i Baseline EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mtext>AdaIN</mtext>
    <mrow><mo>(</mo> <msub><mi>ğ±</mi> <mi>i</mi></msub> <mo>,</mo> <mi>ğ²</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>ğ²</mi> <mrow><mi>s</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mfrac><mrow><msub><mi>ğ±</mi> <mi>i</mi></msub> <mo>-</mo><mi>Î¼</mi><mrow><mo>(</mo><msub><mi>ğ±</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow> <mrow><mi>Ïƒ</mi><mo>(</mo><msub><mi>ğ±</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mfrac> <mo>+</mo> <msub><mi>ğ²</mi> <mrow><mi>b</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></mtd></mtr></mtable></math>
- en: The adaptive instance normalization layers ensure that the style vectors that
    are injected into each layer only affect features at that layer, by preventing
    any style information from leaking through between layers. The authors show that
    this results in the latent vectors <math alttext="bold w"><mi>ğ°</mi></math> being
    significantly more disentangled than the original <math alttext="bold z"><mi>ğ³</mi></math>
    vectors.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Since the synthesis network is based on the ProGAN architecture, it is trained
    progressively. The style vectors at earlier layers in the synthesis network (when
    the resolution of the image is lowestâ€”4 Ã— 4, 8 Ã— 8) will affect coarser features
    than those later in the network (64 Ã— 64 to 1,024 Ã— 1,024â€“pixel resolution). This
    means that not only do we have complete control over the generated image through
    the latent vector <math alttext="bold w"><mi>ğ°</mi></math> , but we can also switch
    the <math alttext="bold w"><mi>ğ°</mi></math> vector at different points in the
    synthesis network to change the style at a variety of levels of detail.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Style mixing
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The authors use a trick known as *style mixing* to ensure that the generator
    cannot utilize correlations between adjacent styles during training (i.e., the
    styles injected at each layer are as disentangled as possible). Instead of sampling
    only a single latent vector <math alttext="bold z"><mi>ğ³</mi></math> , two are
    sampled <math alttext="left-parenthesis bold z bold 1 comma bold z bold 2 right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>ğ³</mi> <mn mathvariant="bold">1</mn></msub> <mo>,</mo> <msub><mi>ğ³</mi>
    <mn mathvariant="bold">2</mn></msub> <mo>)</mo></mrow></math> , corresponding
    to two style vectors <math alttext="left-parenthesis bold w bold 1 comma bold
    w bold 2 right-parenthesis"><mrow><mo>(</mo> <msub><mi>ğ°</mi> <mn mathvariant="bold">1</mn></msub>
    <mo>,</mo> <msub><mi>ğ°</mi> <mn mathvariant="bold">2</mn></msub> <mo>)</mo></mrow></math>
    . Then, at each layer, either <math alttext="left-parenthesis bold w bold 1"><mrow><mo>(</mo>
    <msub><mi>ğ°</mi> <mn mathvariant="bold">1</mn></msub></mrow></math> or <math alttext="bold
    w bold 2 right-parenthesis"><mrow><msub><mi>ğ°</mi> <mn mathvariant="bold">2</mn></msub>
    <mrow><mo>)</mo></mrow></mrow></math> is chosen at random, to break any possible
    correlation between the vectors.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic variation
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The synthesizer network adds noise (passed through a learned broadcasting layer
    <math alttext="upper B"><mi>B</mi></math> ) after each convolution to account
    for stochastic details such as the placement of individual hairs, or the background
    behind the face. Again, the depth at which the noise is injected affects the coarseness
    of the impact on the image.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: This also means that the initial input to the synthesis network can simply be
    a learned constant, rather than additional noise. There is enough stochasticity
    already present in the style inputs and the noise inputs to generate sufficient
    variation in the images.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Outputs from StyleGAN
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[FigureÂ 10-9](#stylegan_w) shows StyleGAN in action.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1009.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-9\. Merging styles between two generated images at different levels
    of detail (source: [Karras et al., 2018](https://arxiv.org/abs/1812.04948))'
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, two images, source A and source B, are generated from two different <math
    alttext="bold w"><mi>ğ°</mi></math> vectors. To generate a merged image, the source
    A <math alttext="bold w"><mi>ğ°</mi></math> vector is passed through the synthesis
    network but, at some point, switched for the source B <math alttext="bold w"><mi>ğ°</mi></math>
    vector. If this switch happens early on (4 Ã— 4 or 8 Ã— 8 resolution), coarse styles
    such as pose, face shape, and glasses from source B are carried across onto source
    A. However, if the switch happens later, only fine-grained detail is carried across
    from source B, such as colors and microstructure of the face, while the coarse
    features from source A are preserved.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: StyleGAN2
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final contribution in this chain of important GAN papers is StyleGAN2.^([5](ch10.xhtml#idm45387005019232))
    This builds further upon the StyleGAN architecture, with some key changes that
    improve the quality of the generated output. In particular, StyleGAN2 generations
    do not suffer as greatly from *artifacts*â€”water dropletâ€“like areas of the image
    that were found to be caused by the adaptive instance normalization layers in
    StyleGAN, as shown in [FigureÂ 10-10](#artifacts_stylegan).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1010.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-10\. An artifact in a StyleGAN-generated image of a face (source:
    [Karras et al., 2019](https://arxiv.org/abs/1912.04958))'
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Both the generator and the discriminator in StyleGAN2 are different from the
    StyleGAN. In the next sections we will explore the key differences between the
    architectures.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Training Your Own StyleGAN2
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official code for training your own StyleGAN using TensorFlow is available
    on [GitHub](https://oreil.ly/alB6w). Bear in mind that training a StyleGAN2 to
    achieve the results from the paper requires a significant amount of computing
    power.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Weight Modulation and Demodulation
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The artifact problem is solved by removing the AdaIN layers in the generator
    and replacing them with weight modulation and demodulation steps, as shown in
    [FigureÂ 10-11](#stylegan2_styleblock). <math alttext="bold w"><mi>ğ°</mi></math>
    represents the weights of the convolutional layer, which are directly updated
    by the modulation and demodulation steps in StyleGAN2 at runtime. In comparison,
    the AdaIN layers of StyleGAN operate on the image tensor as it flows through the
    network.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: The AdaIN layer in StyleGAN is simply an instance normalization followed by
    style modulation (scaling and bias). The idea in StyleGAN2 is to apply style modulation
    and normalization (demodulation) directly to the weights of the convolutional
    layers at runtime, rather than the output from the convolutional layers, as shown
    in [FigureÂ 10-11](#stylegan2_styleblock). The authors show how this removes the
    artifact issue while retaining control of the image style.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1011.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: Figure 10-11\. A comparison between the StyleGAN and StyleGAN2 style blocks
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In StyleGAN2, each dense layer <math alttext="upper A"><mi>A</mi></math> outputs
    a single style vector <math alttext="s Subscript i"><msub><mi>s</mi> <mi>i</mi></msub></math>
    , where <math alttext="i"><mi>i</mi></math> indexes the number of input channels
    in the corresponding convolutional layer. This style vector is then applied to
    the weights of the convolutional layer as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="w Subscript i comma j comma k Superscript prime Baseline equals
    s Subscript i Baseline dot w Subscript i comma j comma k" display="block"><mrow><msubsup><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow> <msup><mo>'</mo></msup></msubsup>
    <mo>=</mo> <msub><mi>s</mi> <mi>i</mi></msub> <mo>Â·</mo> <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow></msub></mrow></math>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="w Subscript i comma j comma k Superscript prime Baseline equals
    s Subscript i Baseline dot w Subscript i comma j comma k" display="block"><mrow><msubsup><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow> <msup><mo>'</mo></msup></msubsup>
    <mo>=</mo> <msub><mi>s</mi> <mi>i</mi></msub> <mo>Â·</mo> <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow></msub></mrow></math>
- en: Here, <math alttext="j"><mi>j</mi></math> indexes the output channels of the
    layer and <math alttext="k"><mi>k</mi></math> indexes the spatial dimensions.
    This is the *modulation* step of the process.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we need to normalize the weights so that they again have a unit standard
    deviation, to ensure stability in the training process. This is the *demodulation*
    step:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="w Subscript i comma j comma k Superscript double-prime Baseline
    equals StartFraction w Subscript i comma j comma k Superscript prime Baseline
    Over StartRoot sigma-summation Underscript i comma k Endscripts w Subscript i
    comma j comma k Superscript prime Baseline squared plus epsilon EndRoot EndFraction"
    display="block"><mrow><msubsup><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow>
    <msup><mrow><mo>'</mo><mo>'</mo></mrow></msup></msubsup> <mo>=</mo> <mfrac><msubsup><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow> <msup><mo>'</mo></msup></msubsup>
    <msqrt><mrow><munder><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></munder>
    <msup><mrow><msubsup><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow>
    <msup><mo>'</mo></msup></msubsup></mrow> <mn>2</mn></msup> <mo>+</mo><mi>Îµ</mi></mrow></msqrt></mfrac></mrow></math>
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="w Subscript i comma j comma k Superscript double-prime Baseline
    equals StartFraction w Subscript i comma j comma k Superscript prime Baseline
    Over StartRoot sigma-summation Underscript i comma k Endscripts w Subscript i
    comma j comma k Superscript prime Baseline squared plus epsilon EndRoot EndFraction"
    display="block"><mrow><msubsup><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow>
    <msup><mrow><mo>'</mo><mo>'</mo></mrow></msup></msubsup> <mo>=</mo> <mfrac><msubsup><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow> <msup><mo>'</mo></msup></msubsup>
    <msqrt><mrow><munder><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></munder>
    <msup><mrow><msubsup><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow>
    <msup><mo>'</mo></msup></msubsup></mrow> <mn>2</mn></msup> <mo>+</mo><mi>Îµ</mi></mrow></msqrt></mfrac></mrow></math>
- en: where <math alttext="epsilon"><mi>Ïµ</mi></math> is a small constant value that
    prevents division by zero.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: In the paper, the authors show how this simple change is enough to prevent water-droplet
    artifacts, while retaining control over the generated images via the style vectors
    and ensuring the quality of the output remains high.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Path Length Regularization
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è·¯å¾„é•¿åº¦æ­£åˆ™åŒ–
- en: Another change made to the StyleGAN architecture is the inclusion of an additional
    penalty term in the loss functionâ€”*this is known as path length regularization*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: StyleGANæ¶æ„çš„å¦ä¸€ä¸ªå˜åŒ–æ˜¯åœ¨æŸå¤±å‡½æ•°ä¸­åŒ…å«äº†é¢å¤–çš„æƒ©ç½šé¡¹â€”â€”*è¿™è¢«ç§°ä¸ºè·¯å¾„é•¿åº¦æ­£åˆ™åŒ–*ã€‚
- en: We would like the latent space to be as smooth and uniform as possible, so that
    a fixed-size step in the latent space in any direction results in a fixed-magnitude
    change in the image.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›æ½œåœ¨ç©ºé—´å°½å¯èƒ½å¹³æ»‘å’Œå‡åŒ€ï¼Œè¿™æ ·åœ¨ä»»ä½•æ–¹å‘ä¸Šæ½œåœ¨ç©ºé—´ä¸­çš„å›ºå®šå¤§å°æ­¥é•¿ä¼šå¯¼è‡´å›¾åƒçš„å›ºå®šå¹…åº¦å˜åŒ–ã€‚
- en: 'To encourage this property, StyleGAN2 aims to minimize the following term,
    alongside the usual Wasserstein loss with gradient penalty:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é¼“åŠ±è¿™ä¸€å±æ€§ï¼ŒStyleGAN2æ—¨åœ¨æœ€å°åŒ–ä»¥ä¸‹æœ¯è¯­ï¼Œä»¥åŠé€šå¸¸çš„WassersteinæŸå¤±å’Œæ¢¯åº¦æƒ©ç½šï¼š
- en: <math alttext="double-struck upper E Subscript w comma y Baseline left-parenthesis
    parallel-to bold upper J Subscript w Superscript down-tack Baseline y parallel-to
    Subscript 2 Baseline minus a right-parenthesis squared" display="block"><mrow><msub><mi>ğ”¼</mi>
    <mrow><mi>ğ‘¤</mi><mo>,</mo><mi>ğ‘¦</mi></mrow></msub> <msup><mfenced separators=""
    open="(" close=")"><msub><mfenced separators="" open="âˆ¥" close="âˆ¥"><msubsup><mi>ğ‰</mi>
    <mi>ğ‘¤</mi> <mi>âŠ¤</mi></msubsup> <mi>ğ‘¦</mi></mfenced> <mn>2</mn></msub> <mo>-</mo><mi>a</mi></mfenced>
    <mn>2</mn></msup></mrow></math>
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E Subscript w comma y Baseline left-parenthesis
    parallel-to bold upper J Subscript w Superscript down-tack Baseline y parallel-to
    Subscript 2 Baseline minus a right-parenthesis squared" display="block"><mrow><msub><mi>ğ”¼</mi>
    <mrow><mi>ğ‘¤</mi><mo>,</mo><mi>ğ‘¦</mi></mrow></msub> <msup><mfenced separators=""
    open="(" close=")"><msub><mfenced separators="" open="âˆ¥" close="âˆ¥"><msubsup><mi>ğ‰</mi>
    <mi>ğ‘¤</mi> <mi>âŠ¤</mi></msubsup> <mi>ğ‘¦</mi></mfenced> <mn>2</mn></msub> <mo>-</mo><mi>a</mi></mfenced>
    <mn>2</mn></msup></mrow></math>
- en: Here, <math alttext="w"><mi>ğ‘¤</mi></math> is a set of style vectors created
    by the mapping network, <math alttext="y"><mi>ğ‘¦</mi></math> is a set of noisy
    images drawn from <math alttext="script upper N left-parenthesis 0 comma bold
    upper I right-parenthesis"><mrow><mi>ğ’©</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>ğˆ</mi>
    <mo>)</mo></mrow></math> , and <math alttext="bold upper J Subscript w Baseline
    equals StartFraction normal partial-differential g Over normal partial-differential
    w EndFraction"><mrow><msub><mi>ğ‰</mi> <mi>ğ‘¤</mi></msub> <mo>=</mo> <mfrac><mrow><mi>âˆ‚</mi><mi>g</mi></mrow>
    <mrow><mi>âˆ‚</mi><mi>ğ‘¤</mi></mrow></mfrac></mrow></math> is the Jacobian of the
    generator network with respect to the style vectors.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ<math alttext="w"><mi>ğ‘¤</mi></math>æ˜¯ç”±æ˜ å°„ç½‘ç»œåˆ›å»ºçš„ä¸€ç»„æ ·å¼å‘é‡ï¼Œ<math alttext="y"><mi>ğ‘¦</mi></math>æ˜¯ä»<math
    alttext="script upper N left-parenthesis 0 comma bold upper I right-parenthesis"><mrow><mi>ğ’©</mi>
    <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>ğˆ</mi> <mo>)</mo></mrow></math>ä¸­ç»˜åˆ¶çš„ä¸€ç»„å˜ˆæ‚å›¾åƒï¼Œ<math
    alttext="bold upper J Subscript w Baseline equals StartFraction normal partial-differential
    g Over normal partial-differential w EndFraction"><mrow><msub><mi>ğ‰</mi> <mi>ğ‘¤</mi></msub>
    <mo>=</mo> <mfrac><mrow><mi>âˆ‚</mi><mi>g</mi></mrow> <mrow><mi>âˆ‚</mi><mi>ğ‘¤</mi></mrow></mfrac></mrow></math>æ˜¯ç”Ÿæˆå™¨ç½‘ç»œç›¸å¯¹äºæ ·å¼å‘é‡çš„é›…å¯æ¯”çŸ©é˜µã€‚
- en: The term <math alttext="parallel-to bold upper J Subscript w Superscript down-tack
    Baseline y parallel-to Subscript 2"><msub><mfenced separators="" open="âˆ¥" close="âˆ¥"><msubsup><mi>ğ‰</mi>
    <mi>ğ‘¤</mi> <mi>âŠ¤</mi></msubsup> <mi>ğ‘¦</mi></mfenced> <mn>2</mn></msub></math>
    measures the magnitude of the images <math alttext="y"><mi>ğ‘¦</mi></math> after
    transformation by the gradients given in the Jacobian. We want this to be close
    to a constant <math alttext="a"><mi>a</mi></math> , which is calculated dynamically
    as the exponential moving average of <math alttext="parallel-to bold upper J Subscript
    w Superscript down-tack Baseline y parallel-to Subscript 2"><msub><mfenced separators=""
    open="âˆ¥" close="âˆ¥"><msubsup><mi>ğ‰</mi> <mi>ğ‘¤</mi> <mi>âŠ¤</mi></msubsup> <mi>ğ‘¦</mi></mfenced>
    <mn>2</mn></msub></math> as the training progresses.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¯è¯­<math alttext="parallel-to bold upper J Subscript w Superscript down-tack
    Baseline y parallel-to Subscript 2"><msub><mfenced separators="" open="âˆ¥" close="âˆ¥"><msubsup><mi>ğ‰</mi>
    <mi>ğ‘¤</mi> <mi>âŠ¤</mi></msubsup> <mi>ğ‘¦</mi></mfenced> <mn>2</mn></msub></math>æµ‹é‡äº†ç»é›…å¯æ¯”çŸ©é˜µç»™å‡ºçš„æ¢¯åº¦å˜æ¢åå›¾åƒ<math
    alttext="y"><mi>ğ‘¦</mi></math>çš„å¹…åº¦ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªå€¼æ¥è¿‘ä¸€ä¸ªå¸¸æ•°<math alttext="a"><mi>a</mi></math>ï¼Œè¿™ä¸ªå¸¸æ•°æ˜¯åŠ¨æ€è®¡ç®—çš„ï¼Œä½œä¸ºè®­ç»ƒè¿›è¡Œæ—¶<math
    alttext="parallel-to bold upper J Subscript w Superscript down-tack Baseline y
    parallel-to Subscript 2"><msub><mfenced separators="" open="âˆ¥" close="âˆ¥"><msubsup><mi>ğ‰</mi>
    <mi>ğ‘¤</mi> <mi>âŠ¤</mi></msubsup> <mi>ğ‘¦</mi></mfenced> <mn>2</mn></msub></math>çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡å€¼ã€‚
- en: The authors find that this additional term makes exploring the latent space
    more reliable and consistent. Moreover, the regularization terms in the loss function
    are only applied once every 16 minibatches, for efficiency. This technique, called
    *lazy regularization*, does not cause a measurable drop in performance.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…å‘ç°ï¼Œè¿™ä¸ªé¢å¤–çš„æœ¯è¯­ä½¿æ¢ç´¢æ½œåœ¨ç©ºé—´æ›´å¯é å’Œä¸€è‡´ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°ä¸­çš„æ­£åˆ™åŒ–é¡¹ä»…åœ¨æ¯16ä¸ªå°æ‰¹æ¬¡ä¸­åº”ç”¨ä¸€æ¬¡ï¼Œä»¥æé«˜æ•ˆç‡ã€‚è¿™ç§æŠ€æœ¯ç§°ä¸º*æ‡’æƒ°æ­£åˆ™åŒ–*ï¼Œä¸ä¼šå¯¼è‡´æ€§èƒ½çš„æ˜æ˜¾ä¸‹é™ã€‚
- en: No Progressive Growing
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ²¡æœ‰æ¸è¿›å¢é•¿
- en: Another major update is in how StyleGAN2 is trained. Rather than adopting the
    usual progressive training mechanism, StyleGAN2 utilizes skip connections in the
    generator and residual connections in the discriminator to train the entire network
    as one. It no longer requires different resolutions to be trained independently
    and blended as part of the training process.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: StyleGAN2è®­ç»ƒçš„å¦ä¸€ä¸ªé‡å¤§æ›´æ–°æ˜¯åœ¨è®­ç»ƒæ–¹å¼ä¸Šã€‚StyleGAN2ä¸å†é‡‡ç”¨é€šå¸¸çš„æ¸è¿›å¼è®­ç»ƒæœºåˆ¶ï¼Œè€Œæ˜¯åˆ©ç”¨ç”Ÿæˆå™¨ä¸­çš„è·³è¿‡è¿æ¥å’Œé‰´åˆ«å™¨ä¸­çš„æ®‹å·®è¿æ¥æ¥å°†æ•´ä¸ªç½‘ç»œä½œä¸ºä¸€ä¸ªæ•´ä½“è¿›è¡Œè®­ç»ƒã€‚å®ƒä¸å†éœ€è¦ç‹¬ç«‹è®­ç»ƒä¸åŒåˆ†è¾¨ç‡ï¼Œå¹¶å°†å…¶ä½œä¸ºè®­ç»ƒè¿‡ç¨‹çš„ä¸€éƒ¨åˆ†æ··åˆã€‚
- en: '[FigureÂ 10-12](#stylegan2_gen_dis) shows the generator and discriminator blocks
    in StyleGAN2.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[å›¾10-12](#stylegan2_gen_dis)å±•ç¤ºäº†StyleGAN2ä¸­çš„ç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨å—ã€‚'
- en: '![](Images/gdl2_1012.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1012.png)'
- en: Figure 10-12\. The generator and discriminator blocks in StyleGAN2
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾10-12ã€‚StyleGAN2ä¸­çš„ç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨å—
- en: The crucial property that we would like to be able to preserve is that the StyleGAN2
    starts by learning low-resolution features and gradually refines the output as
    training progresses. The authors show that this property is indeed preserved using
    this architecture. Each network benefits from refining the convolutional weights
    in the lower-resolution layers in the earlier stages of training, with the skip
    and residual connections used to pass the output through the higher-resolution
    layers mostly unaffected. As training progresses, the higher-resolution layers
    begin to dominate, as the generator discovers more intricate ways to improve the
    realism of the images in order to fool the discriminator. This process is demonstrated
    in [FigureÂ 10-13](#stylegan2_contrib).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿä¿ç•™çš„å…³é”®å±æ€§æ˜¯ï¼ŒStyleGAN2ä»å­¦ä¹ ä½åˆ†è¾¨ç‡ç‰¹å¾å¼€å§‹ï¼Œå¹¶éšç€è®­ç»ƒçš„è¿›è¡Œé€æ¸å®Œå–„è¾“å‡ºã€‚ä½œè€…è¡¨æ˜ï¼Œä½¿ç”¨è¿™ç§æ¶æ„ç¡®å®ä¿ç•™äº†è¿™ä¸€å±æ€§ã€‚åœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µï¼Œæ¯ä¸ªç½‘ç»œéƒ½å—ç›Šäºåœ¨è¾ƒä½åˆ†è¾¨ç‡å±‚ä¸­ç»†åŒ–å·ç§¯æƒé‡ï¼Œè€Œé€šè¿‡è·³è¿‡å’Œæ®‹å·®è¿æ¥å°†è¾“å‡ºä¼ é€’åˆ°è¾ƒé«˜åˆ†è¾¨ç‡å±‚çš„æ–¹å¼åŸºæœ¬ä¸Šä¸å—å½±å“ã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œè¾ƒé«˜åˆ†è¾¨ç‡å±‚å¼€å§‹å ä¸»å¯¼åœ°ä½ï¼Œå› ä¸ºç”Ÿæˆå™¨å‘ç°äº†æ›´å¤æ‚çš„æ–¹æ³•æ¥æ”¹å–„å›¾åƒçš„é€¼çœŸåº¦ï¼Œä»¥æ¬ºéª—é‰´åˆ«å™¨ã€‚è¿™ä¸ªè¿‡ç¨‹åœ¨[å›¾10-13](#stylegan2_contrib)ä¸­å±•ç¤ºã€‚
- en: '![](Images/gdl2_1013.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1013.png)'
- en: Figure 10-13\. The contribution of each resolution layer to the output of the
    generator, by training time (adapted from [Karras et al., 2019](https://arxiv.org/pdf/1912.04958.pdf))
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾10-13ã€‚æ¯ä¸ªåˆ†è¾¨ç‡å±‚å¯¹ç”Ÿæˆå™¨è¾“å‡ºçš„è´¡çŒ®ï¼ŒæŒ‰è®­ç»ƒæ—¶é—´ï¼ˆæ”¹ç¼–è‡ª[Karrasç­‰äººï¼Œ2019](https://arxiv.org/pdf/1912.04958.pdf)ï¼‰
- en: Outputs from StyleGAN2
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StyleGAN2çš„è¾“å‡º
- en: Some examples of StyleGAN2 output are shown in [FigureÂ 10-14](#stylegan2_output).
    To date, the StyleGAN2 architecture (and scaled variations such as StyleGAN-XL^([6](ch10.xhtml#idm45387004898624)))
    remain state of the art for image generation on datasets such as Flickr-Faces-HQ
    (FFHQ) and CIFAR-10, according to the benchmarking website [Papers with Code](https://oreil.ly/VwH2r).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1014.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-14\. Uncurated StyleGAN2 output for the FFHQ face dataset and LSUN
    car dataset (source: [Karras et al., 2019](https://arxiv.org/pdf/1912.04958.pdf))'
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Other Important GANs
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore two more architectures that have also contributed
    significantly to the development of GANsâ€”SAGAN and BigGAN.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Self-Attention GAN (SAGAN)
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Self-Attention GAN (SAGAN)^([7](ch10.xhtml#idm45387004886752)) is a key
    development for GANs as it shows how the attention mechanism that powers sequential
    models such as the Transformer can also be incorporated into GAN-based models
    for image generation. [FigureÂ 10-15](#sagan_attention) shows the self-attention
    mechanism from the paper introducing this architecture.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1015.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-15\. The self-attention mechanism within the SAGAN model (source:
    [Zhang et al., 2018](https://arxiv.org/abs/1805.08318))'
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The problem with GAN-based models that do not incorporate attention is that
    convolutional feature maps are only able to process information locally. Connecting
    pixel information from one side of an image to the other requires multiple convolutional
    layers that reduce the size of the image, while increasing the number of channels.
    Precise positional information is reduced throughout this process in favor of
    capturing higher-level features, making it computationally inefficient for the
    model to learn long-range dependencies between distantly connected pixels. SAGAN
    solves this problem by incorporating the attention mechanism that we explored
    earlier in this chapter into the GAN. The effect of this inclusion is shown in
    [FigureÂ 10-16](Images/#sagan_images).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1016.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-16\. A SAGAN-generated image of a bird (leftmost cell) and the attention
    maps of the final attention-based generator layer for the pixels covered by the
    three colored dots (rightmost cells) (source: [Zhang et al., 2018](https://arxiv.org/abs/1805.08318))'
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The red dot is a pixel that is part of the birdâ€™s body, and so attention naturally
    falls on the surrounding body cells. The green dot is part of the background,
    and here the attention actually falls on the other side of the birdâ€™s head, on
    other background pixels. The blue dot is part of the birdâ€™s long tail and so attention
    falls on other tail pixels, some of which are distant from the blue dot. It would
    be difficult to maintain this long-range dependency for pixels without attention,
    especially for long, thin structures in the image (such as the tail in this case).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Training Your Own SAGAN
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official code for training your own SAGAN using TensorFlow is available
    on [GitHub](https://oreil.ly/rvej0). Bear in mind that training a SAGAN to achieve
    the results from the paper requires a significant amount of computing power.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: BigGAN
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BigGAN,^([8](ch10.xhtml#idm45387004870736)) developed at DeepMind, extends the
    ideas from the SAGAN paper. [FigureÂ 10-17](#biggan_examples) shows some of the
    images generated by BigGAN, trained on the ImageNet dataset at 128 Ã— 128 resolution.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1017.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-17\. Examples of images generated by BigGAN (source: [Brock et al.,
    2018](https://arxiv.org/abs/1809.11096))'
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As well as some incremental changes to the base SAGAN model, there are also
    several innovations outlined in the paper that take the model to the next level
    of sophistication. One such innovation is the so-called *truncation trick*. This
    is where the latent distribution used for sampling is different from the <math
    alttext="z tilde script upper N left-parenthesis 0 comma bold upper I right-parenthesis"><mrow><mi>z</mi>
    <mo>âˆ¼</mo> <mi>ğ’©</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>ğˆ</mi> <mo>)</mo></mrow></math>
    distribution used during training. Specifically, the distribution used during
    sampling is a *truncated normal distribution* (resampling values of <math alttext="z"><mi>z</mi></math>
    that have magnitude greater than a certain threshold). The smaller the truncation
    threshold, the greater the believability of generated samples, at the expense
    of reduced variability. This concept is shown in [FigureÂ 10-18](#truncation).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†å¯¹åŸºæœ¬ SAGAN æ¨¡å‹è¿›è¡Œä¸€äº›å¢é‡æ›´æ”¹å¤–ï¼Œè®ºæ–‡ä¸­è¿˜æ¦‚è¿°äº†å°†æ¨¡å‹æå‡åˆ°æ›´é«˜å±‚æ¬¡çš„å‡ é¡¹åˆ›æ–°ã€‚å…¶ä¸­ä¸€é¡¹åˆ›æ–°æ˜¯æ‰€è°“çš„â€œæˆªæ–­æŠ€å·§â€ã€‚è¿™æ˜¯æŒ‡ç”¨äºé‡‡æ ·çš„æ½œåœ¨åˆ†å¸ƒä¸è®­ç»ƒæœŸé—´ä½¿ç”¨çš„
    <math alttext="z tilde script upper N left-parenthesis 0 comma bold upper I right-parenthesis"><mrow><mi>z</mi>
    <mo>âˆ¼</mo> <mi>ğ’©</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>ğˆ</mi> <mo>)</mo></mrow></math>
    åˆ†å¸ƒä¸åŒã€‚å…·ä½“æ¥è¯´ï¼Œé‡‡æ ·æœŸé—´ä½¿ç”¨çš„åˆ†å¸ƒæ˜¯â€œæˆªæ–­æ­£æ€åˆ†å¸ƒâ€ï¼ˆé‡æ–°é‡‡æ ·å…·æœ‰å¤§äºä¸€å®šé˜ˆå€¼çš„ <math alttext="z"><mi>z</mi></math>
    å€¼ï¼‰ã€‚æˆªæ–­é˜ˆå€¼è¶Šå°ï¼Œç”Ÿæˆæ ·æœ¬çš„å¯ä¿¡åº¦è¶Šé«˜ï¼Œä½†å˜å¼‚æ€§é™ä½ã€‚è¿™ä¸ªæ¦‚å¿µåœ¨[å›¾ 10-18](#truncation)ä¸­å±•ç¤ºã€‚
- en: '![](Images/gdl2_1018.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1018.png)'
- en: 'Figure 10-18\. The truncation trick: from left to right, the threshold is set
    to 2, 1, 0.5, and 0.04 (source: [Brock et al., 2018](https://arxiv.org/abs/1809.11096))'
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 10-18\. æˆªæ–­æŠ€å·§ï¼šä»å·¦åˆ°å³ï¼Œé˜ˆå€¼è®¾ç½®ä¸º 2ã€1ã€0.5 å’Œ 0.04ï¼ˆæ¥æºï¼š[Brock ç­‰äººï¼Œ2018](https://arxiv.org/abs/1809.11096)ï¼‰
- en: Also, as the name suggests, BigGAN is an improvement over SAGAN in part simply
    by being *bigger*. BigGAN uses a batch size of 2,048â€”8 times larger than the batch
    size of 256 used in SAGANâ€”and a channel size that is increased by 50% in each
    layer. However, BigGAN additionally shows that SAGAN can be improved structurally
    by the inclusion of a shared embedding, by orthogonal regularization, and by incorporating
    the latent vector <math alttext="z"><mi>z</mi></math> into each layer of the generator,
    rather than just the initial layer.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚å…¶åç§°æ‰€ç¤ºï¼ŒBigGAN åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯å¯¹ SAGAN çš„æ”¹è¿›ï¼Œä»…ä»…æ˜¯å› ä¸ºå®ƒæ›´â€œå¤§â€ã€‚BigGAN ä½¿ç”¨çš„æ‰¹é‡å¤§å°ä¸º 2,048ï¼Œæ¯” SAGAN ä¸­ä½¿ç”¨çš„
    256 çš„æ‰¹é‡å¤§å°å¤§ 8 å€ï¼Œå¹¶ä¸”æ¯ä¸€å±‚çš„é€šé“å¤§å°å¢åŠ äº† 50%ã€‚ç„¶è€Œï¼ŒBigGAN è¿˜è¡¨æ˜ï¼Œé€šè¿‡åŒ…å«å…±äº«åµŒå…¥ã€æ­£äº¤æ­£åˆ™åŒ–ä»¥åŠå°†æ½œåœ¨å‘é‡ <math alttext="z"><mi>z</mi></math>
    åŒ…å«åˆ°ç”Ÿæˆå™¨çš„æ¯ä¸€å±‚ä¸­ï¼Œè€Œä¸ä»…ä»…æ˜¯åˆå§‹å±‚ï¼Œå¯ä»¥åœ¨ç»“æ„ä¸Šæ”¹è¿› SAGANã€‚
- en: For a full description of the innovations introduced by BigGAN, I recommend
    reading the original paper and [accompanying presentation material](https://oreil.ly/vPn8T).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å…¨é¢äº†è§£ BigGAN å¼•å…¥çš„åˆ›æ–°ï¼Œæˆ‘å»ºè®®é˜…è¯»åŸå§‹è®ºæ–‡å’Œ[ç›¸å…³æ¼”ç¤ºææ–™](https://oreil.ly/vPn8T)ã€‚
- en: Using BigGAN
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ BigGAN
- en: A tutorial for generating images using a pre-trained BigGAN is available on
    [the TensorFlow website](https://oreil.ly/YLbLb).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[ TensorFlow ç½‘ç«™](https://oreil.ly/YLbLb)ä¸Šæä¾›äº†ä¸€ä¸ªä½¿ç”¨é¢„è®­ç»ƒçš„ BigGAN ç”Ÿæˆå›¾åƒçš„æ•™ç¨‹ã€‚
- en: VQ-GAN
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VQ-GAN
- en: Another important type of GAN is the Vector Quantized GAN (VQ-GAN), introduced
    in 2020.^([9](ch10.xhtml#idm45387004838864)) This model architecture builds upon
    an idea introduced in the 2017 paper â€œNeural Discrete Representation Learningâ€^([10](ch10.xhtml#idm45387004834704))â€”namely,
    that the representations learned by a VAE can be discrete, rather than continuous.
    This new type of model, the Vector Quantized VAE (VQ-VAE), was shown to generate
    high-quality images while avoiding some of the issues often seen with traditional
    continuous latent space VAEs, such as *posterior collapse* (where the learned
    latent space becomes uninformative due to an overly powerful decoder).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§é‡è¦çš„ GAN ç±»å‹æ˜¯ 2020 å¹´æ¨å‡ºçš„ Vector Quantized GANï¼ˆVQ-GANï¼‰ã€‚è¿™ç§æ¨¡å‹æ¶æ„å»ºç«‹åœ¨ 2017 å¹´çš„è®ºæ–‡â€œç¥ç»ç¦»æ•£è¡¨ç¤ºå­¦ä¹ â€ä¸­æå‡ºçš„ä¸€ä¸ªæƒ³æ³•ä¹‹ä¸Šï¼Œå³
    VAE å­¦ä¹ åˆ°çš„è¡¨ç¤ºå¯ä»¥æ˜¯ç¦»æ•£çš„ï¼Œè€Œä¸æ˜¯è¿ç»­çš„ã€‚è¿™ç§æ–°å‹æ¨¡å‹ï¼Œå³ Vector Quantized VAEï¼ˆVQ-VAEï¼‰ï¼Œè¢«è¯æ˜å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼ŒåŒæ—¶é¿å…äº†ä¼ ç»Ÿè¿ç»­æ½œåœ¨ç©ºé—´
    VAE ç»å¸¸å‡ºç°çš„ä¸€äº›é—®é¢˜ï¼Œæ¯”å¦‚â€œåéªŒåç¼©â€ï¼ˆå­¦ä¹ åˆ°çš„æ½œåœ¨ç©ºé—´ç”±äºè¿‡äºå¼ºå¤§çš„è§£ç å™¨è€Œå˜å¾—æ— ä¿¡æ¯ï¼‰ã€‚
- en: Tip
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æç¤º
- en: The first version of DALL.E, a text-to-image model released by OpenAI in 2021
    (see [ChapterÂ 13](ch13.xhtml#chapter_multimodal)), utilized a VAE with a discrete
    latent space, similar to VQ-VAE.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI åœ¨ 2021 å¹´å‘å¸ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ DALL.E çš„ç¬¬ä¸€ä¸ªç‰ˆæœ¬ï¼ˆå‚è§[ç¬¬ 13 ç« ](ch13.xhtml#chapter_multimodal)ï¼‰ä½¿ç”¨äº†å…·æœ‰ç¦»æ•£æ½œåœ¨ç©ºé—´çš„
    VAEï¼Œç±»ä¼¼äº VQ-VAEã€‚
- en: By a *discrete latent space*, we mean a learned list of vectors (the *codebook*),
    each associated with a corresponding index. The job of the encoder in a VQ-VAE
    is to collapse the input image to a smaller grid of vectors that can then be compared
    to the codebook. The closest codebook vector to each grid square vector (by Euclidean
    distance) is then taken forward to be decoded by the decoder, as shown in [FigureÂ 10-19](#vqvae).
    The codebook is a list of learned vectors of length <math alttext="d"><mi>d</mi></math>
    (the embedding size) that matches the number of channels in the output of the
    encoder and input to the decoder. For example, <math alttext="e 1"><msub><mi>e</mi>
    <mn>1</mn></msub></math> is a vector that can be interpreted as *background*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡â€œç¦»æ•£æ½œåœ¨ç©ºé—´â€ï¼Œæˆ‘ä»¬æŒ‡çš„æ˜¯ä¸€ä¸ªå­¦ä¹ åˆ°çš„å‘é‡åˆ—è¡¨ï¼ˆâ€œç ä¹¦â€ï¼‰ï¼Œæ¯ä¸ªå‘é‡ä¸ç›¸åº”çš„ç´¢å¼•ç›¸å…³è”ã€‚VQ-VAE ä¸­ç¼–ç å™¨çš„å·¥ä½œæ˜¯å°†è¾“å…¥å›¾åƒæŠ˜å åˆ°ä¸€ä¸ªè¾ƒå°çš„å‘é‡ç½‘æ ¼ä¸­ï¼Œç„¶åå°†å…¶ä¸ç ä¹¦è¿›è¡Œæ¯”è¾ƒã€‚ç„¶åï¼Œå°†æ¯ä¸ªç½‘æ ¼æ–¹æ ¼å‘é‡ï¼ˆé€šè¿‡æ¬§æ°è·ç¦»ï¼‰æœ€æ¥è¿‘çš„ç ä¹¦å‘é‡ä¼ é€’ç»™è§£ç å™¨è¿›è¡Œè§£ç ï¼Œå¦‚[å›¾
    10-19](#vqvae)æ‰€ç¤ºã€‚ç ä¹¦æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º <math alttext="d"><mi>d</mi></math>ï¼ˆåµŒå…¥å¤§å°ï¼‰çš„å­¦ä¹ å‘é‡åˆ—è¡¨ï¼Œä¸ç¼–ç å™¨è¾“å‡ºå’Œè§£ç å™¨è¾“å…¥ä¸­çš„é€šé“æ•°ç›¸åŒ¹é…ã€‚ä¾‹å¦‚ï¼Œ<math
    alttext="e 1"><msub><mi>e</mi> <mn>1</mn></msub></math> æ˜¯ä¸€ä¸ªå¯ä»¥è§£é‡Šä¸ºâ€œèƒŒæ™¯â€çš„å‘é‡ã€‚
- en: '![](Images/gdl2_1019.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_1019.png)'
- en: Figure 10-19\. A diagram of a VQ-VAE
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 10-19\. VQ-VAE çš„ç¤ºæ„å›¾
- en: The codebook can be thought of as a set of learned discrete concepts that are
    shared by the encoder and decoder in order to describe the contents of a given
    image. The VQ-VAE must find a way to make this set of discrete concepts as informative
    as possible so that the encoder can accurately *label* each grid square with a
    particular code vector that is meaningful to the decoder. The loss function for
    a VQ-VAE is therefore the reconstruction loss added to two terms (alignment and
    commitment loss) that ensure that the output vectors from the encoder are as close
    as possible to vectors in the codebook. These terms replace the the KL divergence
    term between the encoded distribution and the standard Gaussian prior in a typical
    VAE.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: However, this architecture poses a questionâ€”how do we sample novel code grids
    to pass to the decoder to generate new images? Clearly, using a uniform prior
    (picking each code with equal probability for each grid square) will not work.
    For example in the MNIST dataset, the top-left grid square is highly likely to
    be coded as *background*, whereas grid squares toward the center of the image
    are not as likely to be coded as such. To solve this problem, the authors used
    another model, an autoregressive PixelCNN (see [ChapterÂ 5](ch05.xhtml#chapter_autoregressive)),
    to predict the next code vector in the grid, given previous code vectors. In other
    words, the prior is learned by the model, rather than static as in the case of
    the vanilla VAE.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Training Your Own VQ-VAE
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is an excellent tutorial by Sayak Paul on training your own VQ-VAE using
    Keras available on the [Keras website](https://oreil.ly/dmcb4).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: The VQ-GAN paper details several key changes to the VQ-VAE architecture, as
    shown in [FigureÂ 10-20](#vqgan).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1020.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-20\. A diagram of a VQ-GAN: the GAN discriminator helps to encourage
    the VAE to generate less blurry images through an additional adversarial loss
    term'
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Firstly, as the name suggests, the authors include a GAN discriminator that
    tries to distinguish between the output from the VAE decoder and real images,
    with an accompanying adversarial term in the loss function. GANs are known to
    produce sharper images than VAEs, so this addition improves the overall image
    quality. Notice that despite the name, the VAE is still present in a VQ-GAN modelâ€”the
    GAN discriminator is an additional component rather than a replacement of the
    VAE. The idea of combining a VAE with a GAN discriminator (VAE-GAN) was first
    introduced by Larsen et al. in their 2015 paper.^([11](ch10.xhtml#idm45387004808112))
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, the GAN discriminator predicts if small patches of the images are
    real or fake, rather than the entire image at once. This idea (*PatchGAN*) was
    applied in the successful *pix2pix* image-to-image model introduced in 2016 by
    Isola et al.^([12](ch10.xhtml#idm45387004801680)) and was also successfully applied
    as part of *CycleGAN*,^([13](ch10.xhtml#idm45387004798080)) another image-to-image
    style transfer model. The PatchGAN discriminator outputs a prediction vector (a
    prediction for each patch), rather than a single prediction for the overall image.
    The benefit of using a PatchGAN discriminator is that the loss function can then
    measure how good the discriminator is at distinguishing images based on their
    *style*, rather than their *content*. Since each individual element of the discriminator
    prediction is based on a small square of the image, it must use the style of the
    patch, rather than its content, to make its decision. This is useful as we know
    that VAEs produce images that are stylistically more blurry than real images,
    so the PatchGAN discriminator can encourage the VAE decoder to generate sharper
    images than it would naturally produce.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Thirdly, rather than use a single MSE reconstruction loss that compares the
    input image pixels with the output pixels from the VAE decoder, VQ-GAN uses a
    *perceptual loss* term that calculates the difference between feature maps at
    intermediate layers of the encoder and corresponding layers of the decoder. This
    idea is from the 2016 paper by Hou et al.,^([14](ch10.xhtml#idm45387004793216))
    where the authors show that this change to the loss function results in more realistic
    image generations.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, instead of PixelCNN, a Transformer is used as the autoregressive part
    of the model, trained to generate sequences of codes. The Transformer is trained
    in a separate phase, after the VQ-GAN has been fully trained. Rather than use
    all previous tokens in a fully autoregressive manner, the authors choose to only
    use tokens that fall within a sliding window around the token to be predicted.
    This ensures that the model scales to larger images, which require a larger latent
    grid size and therefore more tokens to be generated by the Transformer.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: ViT VQ-GAN
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One final extension to the VQ-GAN was made by Yu et al. in their 2021 paper
    entitled â€œVector-Quantized Image Modeling with Improved VQGAN.â€^([15](ch10.xhtml#idm45387004783968))
    Here, the authors show how the convolutional encoder and decoder of the VQ-GAN
    can be replaced with Transformers as shown in [FigureÂ 10-21](#vit_vqgan).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: For the encoder, the authors use a *Vision Transformer* (ViT).^([16](ch10.xhtml#idm45387004780000))
    A ViT is a neural network architecture that applies the Transformer model, originally
    designed for natural language processing, to image data. Instead of using convolutional
    layers to extract features from an image, a ViT divides the image into a sequence
    of patches, which are tokenized and then fed as input to an encoder Transformer.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, in the ViT VQ-GAN, the nonoverlapping input patches (each of size
    8 Ã— 8) are first flattened, then projected into a low-dimensional embedding space,
    where positional embeddings are added. This sequence is then fed to a standard
    encoder Transformer and the resulting embeddings are quantized according to a
    learned codebook. These integer codes are then processed by a decoder Transformer
    model, with the overall output being a sequence of patches that can be stitched
    back together to form the original image. The overall encoder-decoder model is
    trained end-to-end as an autoencoder.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1021.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-21\. A diagram of a ViT VQ-GAN: the GAN discriminator helps to encourage
    the VAE to generate less blurry images through an additional adversarial loss
    term (source: [Yu and Koh, 2022](https://ai.googleblog.com/2022/05/vector-quantized-image-modeling-with.html))^([17](ch10.xhtml#idm45387004774560))'
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As with the original VQ-GAN model, the second phase of training involves using
    an autoregressive decoder Transformer to generate sequences of codes. Therefore
    in total, there are three Transformers in a ViT VQ-GAN, in addition to the GAN
    discriminator and learned codebook. Examples of images generated by the ViT VQ-GAN
    from the paper are shown in [FigureÂ 10-22](#vit_vqgan_ex).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_1022.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-22\. Example images generated by a ViT VQ-GAN trained on ImageNet
    (source: [Yu et al., 2021](https://arxiv.org/pdf/2110.04627.pdf))'
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have taken a tour of some of the most important and influential
    GAN papers since 2017\. In particular, we have explored ProGAN, StyleGAN, StyleGAN2,
    SAGAN, BigGAN, VQ-GAN, and ViT VQ-GAN.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: We started by exploring the concept of progressive training that was pioneered
    in the 2017 ProGAN paper. Several key changes were introduced in the 2018 StyleGAN
    paper that gave greater control over the image output, such as the mapping network
    for creating a specific style vector and synthesis network that allowed the style
    to be injected at different resolutions. Finally, StyleGAN2 replaced the adaptive
    instance normalization of StyleGAN with weight modulation and demodulation steps,
    alongside additional enhancements such as path regularization. The paper also
    showed how the desirable property of gradual resolution refinement could be retained
    without having to the train the network progressively.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: We also saw how the concept of attention could be built into a GAN, with the
    introduction of SAGAN in 2018\. This allows the network to capture long-range
    dependencies, such as similar background colors over opposite sides of an image,
    without relying on deep convolutional maps to spread the information over the
    spatial dimensions of the image. BigGAN was an extension of this idea that made
    several key changes and trained a larger network to improve the image quality
    further.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: In the VQ-GAN paper, the authors show how several different types of generative
    models can be combined to great effect. Building on the original VQ-VAE paper
    that introduced the concept of a VAE with a discrete latent space, VQ-GAN additionally
    includes a discriminator that encourages the VAE to generate less blurry images
    through an additional adversarial loss term. An autoregressive Transformer is
    used to construct a novel sequence of code tokens that can be decoded by the VAE
    decoder to produce novel images. The ViT VQ-GAN paper extends this idea even further,
    by replacing the convolutional encoder and decoder of VQ-GAN with Transformers.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch10.xhtml#idm45387005226448-marker)) Huiwen Chang et al., â€œMuse: Text-to-Image
    Generation via Masked Generative Transformers,â€ January 2, 2023, [*https://arxiv.org/abs/2301.00704*](https://arxiv.org/abs/2301.00704).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch10.xhtml#idm45387005216528-marker)) Tero Karras et al., â€œProgressive
    Growing of GANs for Improved Quality, Stability, and Variation,â€ October 27, 2017,
    [*https://arxiv.org/abs/1710.10196*](https://arxiv.org/abs/1710.10196).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch10.xhtml#idm45387005140128-marker)) Tero Karras et al., â€œA Style-Based
    Generator Architecture for Generative Adversarial Networks,â€ December 12, 2018,
    [*https://arxiv.org/abs/1812.04948*](https://arxiv.org/abs/1812.04948).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch10.xhtml#idm45387005090240-marker)) Xun Huang and Serge Belongie, â€œArbitrary
    Style Transfer in Real-Time with Adaptive Instance Normalization,â€ March 20, 2017,
    [*https://arxiv.org/abs/1703.06868*](https://arxiv.org/abs/1703.06868).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch10.xhtml#idm45387005019232-marker)) Tero Karras et al., â€œAnalyzing and
    Improving the Image Quality of StyleGAN,â€ December 3, 2019, [*https://arxiv.org/abs/1912.04958*](https://arxiv.org/abs/1912.04958).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch10.xhtml#idm45387004898624-marker)) Axel Sauer et al., â€œStyleGAN-XL:
    Scaling StyleGAN to Large Diverse Datasets,â€ February 1, 2022, [*https://arxiv.org/abs/2202.00273v2*](https://arxiv.org/abs/2202.00273v2).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch10.xhtml#idm45387004886752-marker)) Han Zhang et al., â€œSelf-Attention
    Generative Adversarial Networks,â€ May 21, 2018, [*https://arxiv.org/abs/1805.08318*](https://arxiv.org/abs/1805.08318).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch10.xhtml#idm45387004870736-marker)) Andrew Brock et al., â€œLarge Scale
    GAN Training for High Fidelity Natural Image Synthesis,â€ September 28, 2018, [*https://arxiv.org/abs/1809.11096*](https://arxiv.org/abs/1809.11096).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch10.xhtml#idm45387004838864-marker)) Patrick Esser et al., â€œTaming Transformers
    for High-Resolution Image Synthesis,â€ December 17, 2020, [*https://arxiv.org/abs/2012.09841*](https://arxiv.org/abs/2012.09841).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch10.xhtml#idm45387004834704-marker)) Aaron van den Oord et al., â€œNeural
    Discrete Representation Learning,â€ November 2, 2017, [*https://arxiv.org/abs/1711.00937v2*](https://arxiv.org/abs/1711.00937v2).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch10.xhtml#idm45387004808112-marker)) Anders Boesen Lindbo Larsen et
    al., â€œAutoencoding Beyond Pixels Using a Learned Similarity Metric,â€ December
    31, 2015, [*https://arxiv.org/abs/1512.09300*](https://arxiv.org/abs/1512.09300).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch10.xhtml#idm45387004801680-marker)) Phillip Isola et al., â€œImage-to-Image
    Translation with Conditional Adversarial Networks,â€ November 21, 2016, [*https://arxiv.org/abs/1611.07004v3*](https://arxiv.org/abs/1611.07004v3).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch10.xhtml#idm45387004798080-marker)) Jun-Yan Zhu et al., â€œUnpaired Image-to-Image
    Translation using Cycle-Consistent Adversarial Networks,â€ March 30, 2017, [*https://arxiv.org/abs/1703.10593*](https://arxiv.org/abs/1703.10593).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch10.xhtml#idm45387004793216-marker)) Xianxu Hou et al., â€œDeep Feature
    Consistent Variational Autoencoder,â€ October 2, 2016, [*https://arxiv.org/abs/1610.00291*](https://arxiv.org/abs/1610.00291).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch10.xhtml#idm45387004783968-marker)) Jiahui Yu et al., â€œVector-Quantized
    Image Modeling with Improved VQGAN,â€ October 9, 2021, [*https://arxiv.org/abs/2110.04627*](https://arxiv.org/abs/2110.04627).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '^([16](ch10.xhtml#idm45387004780000-marker)) Alexey Dosovitskiy et al., â€œAn
    Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale,â€ October
    22, 2020, [*https://arxiv.org/abs/2010.11929v2*](https://arxiv.org/abs/2010.11929v2).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: ^([17](ch10.xhtml#idm45387004774560-marker)) Jiahui Yu and Jing Yu Koh, â€œVector-Quantized
    Image Modeling with Improved VQGAN,â€ May 18, 2022, [*https://ai.googleblog.com/2022/05/vector-quantized-image-modeling-with.html*](https://ai.googleblog.com/2022/05/vector-quantized-image-modeling-with.html).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
