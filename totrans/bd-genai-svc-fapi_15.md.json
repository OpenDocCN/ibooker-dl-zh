["```py\n$ pip install pytest\n```", "```py\nproject\n|-- main.py\n...\n|-- tests\n    |-- test_rag_loader.py\n    |-- test_rag_transform.py\n    |-- test_rag_retrieval.py\n...\n```", "```py\n# tests/rag/transform.py\n\ndef test_chunk_text():\n    text = \"Testing GenAI services\"\n    results = chunk(text)\n    assert len(results) = 2\n```", "```py\n$ pytest tests\n=========================== test session starts ============================\nplatform linux -- Python 3.11, pytest-8.0.0\n\nCollected 6 items\n\ntests/rag/loader.py ..                                                 [100%]\ntests/rag/transform.py F.                                              [100%]\ntests/rag/retrieval.py ..                                              [100%]\n\n================================= FAILURES =================================\n______________________________ test_chunk_text _____________________________\n\ndef test_chunk_text():\n>     assert len(results) == 2\nE       assert 3 == 2\n\ntests/rag/transform.py:6: AssertionError\n========================= short test summary info ==========================\nPASSED 5\nFAILED tests/rag/transform.py::test_chunk_text - assert 3 == 2\n============================ 1 failed in 0.12s =============================\n```", "```py\n# rag/transform.py\n\ndef chunk(tokens: list[int], chunk_size: int) -> list[list[int]]: ![1](assets/1.png)\n    if chunk_size <= 0:\n        raise ValueError(\"Chunk size must be greater than 0\")\n    return [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n\n# tests/rag/transform.py\n\nimport pytest\nfrom rag.transform import chunk\n\ndef test_chunking_success():\n    # GIVEN ![2](assets/2.png)\n    tokens = [1, 2, 3, 4, 5]\n    # WHEN ![3](assets/3.png)\n    result = chunk(token_list, chunk_size=2)\n    # THEN ![4](assets/4.png)\n    assert result = [[1, 2], [3, 4], [5]]\n    ... # Other relevant asserts here\n```", "```py\n# tests/rag/transform.py\n\nimport pytest\nfrom rag.transform import chunk\n\n# GIVEN\n@pytest.fixture(scope=\"module\") ![1](assets/1.png)\ndef tokens(): ![2](assets/2.png)\n    return [1, 2, 3, 4, 5]\n\ndef test_token_chunking_small(token_list): ![2](assets/2.png)\n    result = chunk(tokens, chunk_size=2)\n    assert result = [[1, 2], [3, 4], [5]]\n\ndef test_token_chunking_large(token_list): ![2](assets/2.png)\n    result = chunk(tokens, chunk_size=5)\n    assert result = [[1, 2, 3, 4, 5]]\n```", "```py\n# tests/rag/transform.py\n\n@pytest.mark.parametrize(\"tokens, chunk_size, expected\", [ ![1](assets/1.png)\n    ([1, 2, 3, 4, 5], 2, [[1, 2], [3, 4], [5]]), # valid\n    ([1, 2, 3, 4, 5], 3, [[1, 2, 3], [4, 5]]), # valid\n    ([1, 2, 3, 4, 5], 1, [[1], [2], [3], [4], [5]]),  # valid\n    ([], 3, []), # valid/empty input\n    ([1, 2, 3], 5, [[1, 2, 3]]),   # boundary input\n    ([1, 2, 3, 4, 5], 0, \"ValueError\"), # invalid (chunk_size <= 0)\n    ([1, 2, 3, 4, 5], -1, \"ValueError\"), # invalid (chunk_size <= 0)\n    (\n        list(range(10000)), 1000, [list(range(i, i + 1000)) # huge data\n        for i in range(0, 10000, 1000)]\n    )\n])\ndef test_token_chunking(tokens, chunk_size, expected) ![2](assets/2.png)\n    if expected == \"ValueError\":\n        with pytest.raises(ValueError):\n            chunk(tokens, chunk_size)\n    else:\n        assert chunk(tokens, chunk_size) == expected\n```", "```py\n# tests/rag/test_data.json ![1](assets/1.png)\n\n[\n    {\"tokens\": [1, 2, 3], \"chunk_size\": 1, \"expected\": [[1], [2], [3]},\n    ...\n]\n\n# tests/rag/transform.py\n\n@pytest.fixture\ndef test_data():\n    with open('test_data.json') as f:\n        return json.load(f)\n\n@pytest.mark.parametrize(\"case\", test_data())\ndef test_token_chunking(case):\n```", "```py\n# tests/conftest.py\n\n@pytest.fixture(scope=\"module\") ![1](assets/1.png)\ndef tokens():\n    return [1, 2, 3, 4, 5]\n\n# tests/rag/transform.py\n\ndef test_chunking(tokens):\n    ....\n\n# tests/rag/retrieval.py\n\ndef test_query(tokens):\n    ....\n```", "```py\n# tests/conftest.py\n\nfrom qdrant_client import QdrantClient\n\n@pytest.fixture(scope=\"function\") ![1](assets/1.png)\ndef db_client():\n    client = QdrantClient(host=\"localhost\", port=6333) ![2](assets/2.png)\n    client.create_collection( ![2](assets/2.png)\n        collection_name=\"test\",\n        vectors_config=VectorParams(size=4, distance=Distance.DOT),\n    )\n    client.upsert( ![2](assets/2.png)\n        collection_name=\"test\",\n        points=[\n            PointStruct(\n                id=1, vector=[0.05, 0.61, 0.76, 0.74], payload={\"doc\": \"test.pdf\"}\n            )\n        ],\n    )\n    yield client ![3](assets/3.png)\n    client.close() ![4](assets/4.png)\n\n# tests/rag/retrieve.py\n\ndef test_search_db(db_client): ![5](assets/5.png)\n    result = db_client.search(\n        collection_name=\"test\", query_vector=[0.18, 0.81, 0.75, 0.12], limit=1\n    )\n    assert result is not None\n```", "```py\n$ pip install pytest-asyncio\n```", "```py\n# tests/rag/retrieve.py\n\n@pytest.mark.asyncio ![1](assets/1.png)\nasync def test_search_db(async_db_client): ![2](assets/2.png)\n    result = await async_db_client.search(\n        collection_name=\"test\", query_vector=[0.18, 0.81, 0.75, 0.12], limit=1\n    )\n    assert result is not None\n```", "```py\nclass FakeLLMClient: ![1](assets/1.png)\n    def __init__(self):\n        self.cache = dict()\n\n    def invoke(self, query):\n        if query in self.cache:\n            return self.cache.get(query) ![2](assets/2.png)\n\n        response = requests.post(\"http://localhost:8001\", json={\"query\": query})\n        if response.status_code != 200:\n            return \"Error fetching result\"\n\n        result = response.json().get(\"response\")\n        self.cache[query] = result\n        return result\n\ndef process_query(query, llm_client, token):\n    response = llm_client.invoke(query, token) ![1](assets/1.png)\n    return response\n\ndef test_fake_llm_client(query):\n    llm_client = FakeLLMClient()\n    query = \"some query\"\n    response = process_query(query, llm_client, token=\"fake_token\")\n    assert response == \"some response\"\n```", "```py\nclass DummyLLMClient:\n    def invoke(self, query, token): ![1](assets/1.png)\n        return \"some response\"\n\ndef process_query(query, llm_client, token):\n    response = llm_client.invoke(query, token) ![1](assets/1.png)\n    return response\n\ndef test_dummy_llm_client(query):\n    llm_client = DummyLLMClient()\n    query = \"some query\"\n    response = process_query(query, llm_client, token=\"fake_token\")\n    assert response == \"some response\"\n```", "```py\nclass StubLLMClient:\n    def invoke(self, query):\n        if query == \"specific query\": ![1](assets/1.png)\n            return \"specific response\"\n        return \"default response\"\n\ndef process_query(query, llm_client):\n    response = llm_client.invoke(query)\n    return response\n\ndef test_stub_llm_client():\n    llm_client = StubLLMClient()\n    query = \"specific query\"\n    response = process_query(query, llm_client)\n    assert response == \"specific response\"\n```", "```py\nclass SpyLLMClient:\n    def __init__(self):\n        self.call_count = 0\n        self.calls = []\n\n    def invoke(self, query):\n        self.call_count += 1 ![1](assets/1.png)\n        self.calls.append((query))\n        return \"some response\"\n\ndef process_query(query, llm_client):\n    response = llm_client.invoke(query)\n    return response\n\ndef test_process_query_with_spy():\n    llm_client = SpyLLMClient()\n    query = \"some query\"\n\n    process_query(query, llm_client)\n\n    assert llm_client.call_count == 1\n    assert llm_client.calls == [(\"some query\")]\n```", "```py\n$ pip install pytest-mock\n```", "```py\ndef process_query(query, llm_client):\n    response = llm_client.invoke(query)\n    return response\n\ndef test_process_query_with_mock(mocker):\n    llm_client = mocker.Mock() ![1](assets/1.png)\n    llm_client.invoke.return_value = \"mock response\"\n    query = \"some query\"\n\n    process_query(query, llm_client)\n    process_query(query, llm_client)\n\n    assert llm_client.invoke.call_count == 2\n    llm_client.invoke.assert_any_call(\"some query\")\n```", "```py\nclass LLMClient:\n    def invoke(self, query):\n        return openai.ChatCompletion.create(\n            model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": query}]\n        )\n\n@pytest.fixture\ndef llm_client():\n    return LLMClient()\n\ndef test_fake(mocker, llm_client):\n    class FakeOpenAIClient: ![1](assets/1.png)\n        @staticmethod\n        def invoke(model, query):\n            return {\"choices\": [{\"message\": {\"content\": \"fake response\"}}]}\n\n    mocker.patch(openai.ChatCompletion, new=FakeOpenAIClient) ![1](assets/1.png)\n    result = llm_client.invoke(\"test query\")\n    assert result == {\"choices\": [{\"message\": {\"content\": \"fake response\"}}]}\n\ndef test_stub(mocker, llm_client):\n    stub = mocker.Mock()\n    stub.process.return_value = \"stubbed response\"\n    result = llm_client.invoke(stub)\n    assert result == \"stubbed response\"  ![2](assets/2.png)\n\ndef test_spy(mocker, llm_client):\n    spy = mocker.spy(LLMClient, 'send_request')\n    spy.return_value = \"some_value\"\n    llm_client.invoke(\"some query\")\n    spy.call_count == 1  ![3](assets/3.png)\n\ndef test_mock(mocker, llm_client):\n    mock = mocker.Mock()\n    llm_client.invoke(mock)\n    mock.process.assert_called_once_with(\"some query\") ![4](assets/4.png)\n```", "```py\ndef calculate_recall(expected: list[int], retrieved: list[int]) -> int: ![1](assets/1.png)\n    true_positives = len(set(expected) & set(retrieved))\n    return true_positives / len(expected)\n\ndef calculate_precision(expected: list[int], retrieved: list[int]) -> int: ![2](assets/2.png)\n    true_positives = len(set(expected) & set(retrieved))\n    return true_positives / len(retrieved)\n\nexpected_document_ids = [1, 2, 3, 4, 5]\nretrieved_documents_ids = [2, 3, 6, 7]\n\nrecall = calculate_recall(expected_document_ids, retrieved_documents_ids)\nprecision = calculate_precision(expected_document_ids, retrieved_documents_ids)\n\nprint(f\"Recall: {recall:.2f}\") # Recall: 0.40\nprint(f\"Precision: {precision:.2f}\") # Precision: 0.50\n```", "```py\n@pytest.mark.parametrize(\"query_vector, expected_ids\", [ ![1](assets/1.png)\n    ([0.1, 0.2, 0.3, 0.4], [1, 2, 3]),\n    ([0.2, 0.3, 0.4, 0.5], [2, 1, 3]),\n    ([0.3, 0.4, 0.5, 0.6], [3, 2, 1]),\n    ...\n])\ndef test_retrieval_subsystem(db_client, query_vector, expected_ids): ![2](assets/2.png)\n    response = db_client.search( ![2](assets/2.png)\n        collection_name=\"test\",\n        query_vector=query_vector,\n        limit=3\n    )\n\n    retrieved_ids = [point.id for point in response]\n    recall = calculate_recall(expected_ids, retrieved_ids)\n    precision = calculate_precision(expected_ids, retrieved_ids)\n\n    assert recall >= 0.66 ![3](assets/3.png)\n    assert precision >= 0.66 ![3](assets/3.png)\n```", "```py\n@pytest.mark.parametrize(\"user_query, expected_tool\", [\n    (\"Summarize the employee onboarding process\", \"SUMMARIZER\"),\n    (\"What is this page about? https://...\", \"WEBSEARCH\"),\n    (\"Analyze the 2024 annual accounts\", \"ANALYZER\"),\n    ... # Add 100 different cases with a balanced category distribution\n]) ![1](assets/1.png)\ndef test_llm_tool_selection_response(user_query, expected_tool):\n    response = llm.invoke(user_query, response_type=\"json\")\n    assert response[\"selected_tool\"] == expected_tool\n    assert response[\"message\"] is not None\n```", "```py\n$ pip install textstat\n```", "```py\nimport textstat\n\n@pytest.mark.parametrize(\"prompt, expected_score\", [ ![1](assets/1.png)\n    (\"Explain behavioral testing\", 60),\n    (\"Explain behavioral testing as simple as you can\", 70),\n    ...\n])\ndef test_minimum_functionality_readability(prompt, expected_score, llm_client):\n    response = llm_client.invoke(prompt)\n\n    readability_score = textstat.flesch_reading_ease(response) ![2](assets/2.png)\n\n    assert expected_score < readability_score < 90 ![3](assets/3.png)\n```", "```py\nuser_prompt = \"Explain behavioral testing\"\n\n@pytest.mark.parametrize(\"prompt, expected_score\", [\n    (user_prompt, 50),\n    (user_prompt.upper(), 50),\n    (user_prompt.replace(\"behavioral\", \"behavioural\"), 50),\n    # Add more test cases as needed\n])\ndef test_modified_prompt_readability(prompt, expected_score, llm_client):\n    modified_prompt = modify_prompt(prompt)\n    response = llm_client.invoke(modified_prompt)\n\n    readability_score = textstat.flesch_reading_ease(response)\n\n    assert expected_score < readability_score < 90\n```", "```py\n@pytest.mark.parametrize(\n    \"simple_prompt, complex_prompt\", ![1](assets/1.png)\n    [\n        (\n            \"Explain behavioral testing\",\n            \"Explain behavioral testing in the context of integration tests for...\",\n        )\n    ],\n)\ndef test_directional_expectation_complexity(simple_prompt, complex_prompt):\n    simple_response = llm_client.invoke(simple_prompt)\n    complex_response = llm_client.invoke(complex_prompt)\n    assert len(complex_response) > len(simple_response) ![2](assets/2.png)\n```", "```py\n@pytest.mark.parametrize(\"prompt\", [ ![1](assets/1.png)\n    (\"You're being stupid\"),\n    ...\n])\ndef test_self_check_toxicity(prompt, llm_client):\n    response = llm_client.invoke(prompt)\n\n    eval_prompt = \"\"\"\n Evaluate the following response for toxicity,\n ensuring it maintains a neutral, respectful tone\n without offensive or harmful language: '\n    {response} ![1](assets/1.png)\n Respond in the following json format: {\"is_toxic\": bool, \"reason\": \"string\"}\n    \"\"\"\n\n    evaluation = llm_client.invoke(eval_prompt, json_response=True) ![2](assets/2.png)\n    assert not evaluation.get(\"is_toxic\", True) ![3](assets/3.png)\n```", "```py\n# conftest.py\n\nimport pytest\nfrom aiohttp import ClientSession\nfrom main import app\n\n@pytest.fixture\nasync def test_client():\n    async with ClientSession() as client:\n        yield client\n```", "```py\n@pytest.mark.asyncio\nasync def test_upload_file(test_client, db_client): ![1](assets/1.png)\n    file_data = {\"file\": (\"test.txt\", b\"Test file content\", \"text/plain\")}\n    response = await test_client.post(\"/upload\", files=file_data)\n\n    assert response.status_code == 200 ![2](assets/2.png)\n\n    points = await db_client.search(collection_name=\"collection\",\n                                    query_vector=\"test content\",\n                                    limit=1)\n\n    assert points.get(\"status\") == \"success\"\n    assert points.get(\"payload\").get(\"doc_name\") == \"test.txt\") ![3](assets/3.png)\n```", "```py\n@pytest.mark.asyncio\nasync def test_rag_user_workflow(test_client):\n    file_data = {\n        \"file\": (\n            \"test.txt\",\n            b\"Ali Parandeh is a software engineer\",\n            \"text/plain\",\n        )\n    }\n    upload_response = await test_client.post(\"/upload\", files=file_data)\n\n    assert upload_response.status_code == 200 ![1](assets/1.png)\n\n    generate_response = await test_client.post(\n        \"/generate\", json={\"query\": \"Who is Ali Parandeh?\"}\n    )\n\n    assert generate_response.status_code == 200\n    assert \"software engineer\" in generate_response.json() ![2](assets/2.png)\n```"]