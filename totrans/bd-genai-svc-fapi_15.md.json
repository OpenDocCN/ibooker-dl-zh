["```py\n$ pip install pytest\n```", "```py\nproject\n|-- main.py\n...\n|-- tests\n    |-- test_rag_loader.py\n    |-- test_rag_transform.py\n    |-- test_rag_retrieval.py\n...\n```", "```py\n# tests/rag/transform.py\n\ndef test_chunk_text():\n    text = \"Testing GenAI services\"\n    results = chunk(text)\n    assert len(results) = 2\n```", "```py\n$ pytest tests\n=========================== test session starts ============================\nplatform linux -- Python 3.11, pytest-8.0.0\n\nCollected 6 items\n\ntests/rag/loader.py ..                                                 [100%]\ntests/rag/transform.py F.                                              [100%]\ntests/rag/retrieval.py ..                                              [100%]\n\n================================= FAILURES =================================\n______________________________ test_chunk_text _____________________________\n\ndef test_chunk_text():\n>     assert len(results) == 2\nE       assert 3 == 2\n\ntests/rag/transform.py:6: AssertionError\n========================= short test summary info ==========================\nPASSED 5\nFAILED tests/rag/transform.py::test_chunk_text - assert 3 == 2\n============================ 1 failed in 0.12s =============================\n```", "```py\n# rag/transform.py\n\ndef chunk(tokens: list[int], chunk_size: int) -> list[list[int]]: ![1](assets/1.png)\n    if chunk_size <= 0:\n        raise ValueError(\"Chunk size must be greater than 0\")\n    return [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n\n# tests/rag/transform.py\n\nimport pytest\nfrom rag.transform import chunk\n\ndef test_chunking_success():\n    # GIVEN ![2](assets/2.png)\n    tokens = [1, 2, 3, 4, 5]\n    # WHEN ![3](assets/3.png)\n    result = chunk(token_list, chunk_size=2)\n    # THEN ![4](assets/4.png)\n    assert result = [[1, 2], [3, 4], [5]]\n    ... # Other relevant asserts here\n```", "```py\n# tests/rag/transform.py\n\nimport pytest\nfrom rag.transform import chunk\n\n# GIVEN\n@pytest.fixture(scope=\"module\") ![1](assets/1.png)\ndef tokens(): ![2](assets/2.png)\n    return [1, 2, 3, 4, 5]\n\ndef test_token_chunking_small(token_list): ![2](assets/2.png)\n    result = chunk(tokens, chunk_size=2)\n    assert result = [[1, 2], [3, 4], [5]]\n\ndef test_token_chunking_large(token_list): ![2](assets/2.png)\n    result = chunk(tokens, chunk_size=5)\n    assert result = [[1, 2, 3, 4, 5]]\n```", "```py\n# tests/rag/transform.py\n\n@pytest.mark.parametrize(\"tokens, chunk_size, expected\", [ ![1](assets/1.png)\n    ([1, 2, 3, 4, 5], 2, [[1, 2], [3, 4], [5]]), # valid\n    ([1, 2, 3, 4, 5], 3, [[1, 2, 3], [4, 5]]), # valid\n    ([1, 2, 3, 4, 5], 1, [[1], [2], [3], [4], [5]]),  # valid\n    ([], 3, []), # valid/empty input\n    ([1, 2, 3], 5, [[1, 2, 3]]),   # boundary input\n    ([1, 2, 3, 4, 5], 0, \"ValueError\"), # invalid (chunk_size <= 0)\n    ([1, 2, 3, 4, 5], -1, \"ValueError\"), # invalid (chunk_size <= 0)\n    (\n        list(range(10000)), 1000, [list(range(i, i + 1000)) # huge data\n        for i in range(0, 10000, 1000)]\n    )\n])\ndef test_token_chunking(tokens, chunk_size, expected) ![2](assets/2.png)\n    if expected == \"ValueError\":\n        with pytest.raises(ValueError):\n            chunk(tokens, chunk_size)\n    else:\n        assert chunk(tokens, chunk_size) == expected\n```", "```py\n# tests/rag/test_data.json ![1](assets/1.png)\n\n[\n    {\"tokens\": [1, 2, 3], \"chunk_size\": 1, \"expected\": [[1], [2], [3]},\n    ...\n]\n\n# tests/rag/transform.py\n\n@pytest.fixture\ndef test_data():\n    with open('test_data.json') as f:\n        return json.load(f)\n\n@pytest.mark.parametrize(\"case\", test_data())\ndef test_token_chunking(case):\n```", "```py\n# tests/conftest.py\n\n@pytest.fixture(scope=\"module\") ![1](assets/1.png)\ndef tokens():\n    return [1, 2, 3, 4, 5]\n\n# tests/rag/transform.py\n\ndef test_chunking(tokens):\n    ....\n\n# tests/rag/retrieval.py\n\ndef test_query(tokens):\n    ....\n```", "```py\n# tests/conftest.py\n\nfrom qdrant_client import QdrantClient\n\n@pytest.fixture(scope=\"function\") ![1](assets/1.png)\ndef db_client():\n    client = QdrantClient(host=\"localhost\", port=6333) ![2](assets/2.png)\n    client.create_collection( ![2](assets/2.png)\n        collection_name=\"test\",\n        vectors_config=VectorParams(size=4, distance=Distance.DOT),\n    )\n    client.upsert( ![2](assets/2.png)\n        collection_name=\"test\",\n        points=[\n            PointStruct(\n                id=1, vector=[0.05, 0.61, 0.76, 0.74], payload={\"doc\": \"test.pdf\"}\n            )\n        ],\n    )\n    yield client ![3](assets/3.png)\n    client.close() ![4](assets/4.png)\n\n# tests/rag/retrieve.py\n\ndef test_search_db(db_client): ![5](assets/5.png)\n    result = db_client.search(\n        collection_name=\"test\", query_vector=[0.18, 0.81, 0.75, 0.12], limit=1\n    )\n    assert result is not None\n```", "```py\n$ pip install pytest-asyncio\n```", "```py`Una volta installato il plug-in, puoi seguire l'[Esempio 11-7](#async_tests) per scrivere ed eseguire un test asincrono.    ##### Esempio 11-7\\. Scrivere test asincroni    ```", "```py    [![1](assets/1.png)](#co_testing_ai_services_CO7-1)      Contrassegnare esplicitamente il test async con un decoratore `asyncio` per eseguire il test all'interno di un ciclo di eventi.      [![2](assets/2.png)](#co_testing_ai_services_CO7-2)      Questo presuppone che tu abbia sostituito il client del database sincrono con uno asincrono in *conftest.py*.      Quando esegui il comando `pytest`, questo cerca nell'albero delle directory del progetto i test che utilizzano i *raccoglitori* per ogni livello della gerarchia delle directory: funzioni, classi,moduli, pacchetti o sessioni. Il plug-in `pytest-asyncio` fornisce un ciclo dieventi asyncio per ognuno di questi raccoglitori. Per impostazione predefinita, i test contrassegnati da`@pytest.mark.asyncio` vengono eseguiti nel ciclo di eventi fornito dal *raccoglitore di funzioni*, per ridurre l'ambito del ciclo di eventi e massimizzare l'isolamento tra i test.    Ma perché questo isolamento è importante?    La più grande fonte di frustrazione per gli sviluppatori quando testano un software è rappresentata dai test che falliscono in modo casuale quando si esegue consecutivamente la suite di test senza modificare il codice o le configurazioni.    Spesso, quando si indaga sulla causa del comportamento difettoso dei test, si scopre che c'è una fixture o una dipendenza centrale che viene modificata da uno dei test, violando un principio fondamentale del testing: l'*isolamento dei test*. Lo scopo di questo isolamento è quello di ottenere *l'idempotenza* nei test in cui l'esecuzione ripetuta produrrebbe sempre gli stessi risultati, indipendentemente dal numero di volte in cui si esegue la suite di test.    Senza isolamento, i test possono creare effetti collaterali l'uno sull'altro, invalidando le ipotesi di base, portando a fluttuazioni nei risultati/comportamenti e a fallimenti casuali. Inoltre, i test interdipendenti e dipendenti dall'ordine spesso falliscono insieme, impedendoti di ottenere un feedback prezioso sui fallimenti.    Ma in che modo il comportamento anomalo è legato ai test asincroni?    Come abbiamo visto nel [Capitolo 5](ch05.html#ch05), il codice asincrono sfrutta lo scheduler integrato di Python, un ciclo di eventi, per cambiare attività quando si trova di fronte a un'operazione di I/O bloccante. Questo cambio di attività in un ambiente di test può rendere i test asincroni difficili da implementare correttamente, perché le operazioni asincrone potrebbero non essere completate immediatamente e potrebbero essere eseguite in ordine sparso.    I test asincroni spesso si interfacciano con dipendenze esterne come database o filesystem che eseguono operazioni di I/O bloccanti che possono richiedere molto tempo per essere eseguite. Questo è un problema importante per i test unitari che devono essere eseguiti molto velocemente in modo da poterli eseguire frequentemente.    A differenza del codice sincrono, in cui le operazioni vengono eseguite in una sequenza prevedibile e lineare, il codice asincrono introduce anche una variabilità nei tempi, nell'ordine di esecuzione e nello stato delle fixture, riducendo la coerenza dei risultati tra i test. Inoltre, i tempi di risposta delle dipendenze esterne possono fluttuare, portando a effetti collaterali che violano il principio di isolamento dei test.    Per mitigare il rischio di effetti collaterali e di un comportamento scorretto, dovrai gestire correttamente i test asincroni:    *   In attesa di operazioni di I/O bloccanti           *   Evitare l'uso involontario di operazioni di I/O sincrone bloccanti all'interno di test asincroni           *   Utilizzare i timeout corretti per gestire i ritardi           *   Controllare esplicitamente la sequenza delle operazioni, soprattutto quando si eseguono test async in parallelo              Forse la soluzione migliore è quella di scrivere dei test sincroni utilizzando i mock delle dipendenze esterne, che disaccoppiano le tue funzioni dalle dipendenze bloccanti dell'I/O. Utilizzando i mock, puoi eseguire test veloci e affidabili senza dover aspettare che le operazioni di I/O vengano completate nell'ordine desiderato.    ###### Suggerimento    I test asincroni possono ancora essere utili con le dipendenze reali quando si testa localmente un ambiente di produzione replicato.    Vediamo poi come prendere in giro le dipendenze esterne nei test unitari, in modo da poter scrivere test sincroni in sostituzione di quelli lenti e asincroni.```", "```py`### Mocking e patching    Quando scrivi i test unitari, devi isolare i tuoi componenti dalle dipendenze esterne per evitare che i test vengano eseguiti lentamente e che consumino risorse inutili. Ad esempio, non vuoi richiamare il tuo modello GenAI ogni volta che esegui la suite di test, cosa che sarà frequente, perché questo richiederà un'elevata intensità di calcolo e potrebbe essere costoso.    Invece, puoi usare i *test double* per simulare le dipendenze reali nei tuoi test unitari senza dover dipendere da dipendenze esterne nei tuoi test. In sostanza, fingono di essere la realtà, proprio come le controfigure nei film d'azione che fingono di essere gli attori principali. I test unitari isolati che usano i test double possono verificare i cambiamenti di stato del componente o il suo comportamento quando interagisce con dipendenze esterne come un'API LLM.    ###### Avvertenze    Fai attenzione a non sostituire il comportamento del componente che stai cercando di testare con dei doppi di prova.    Ad esempio, se hai una classe `ChatBot` che utilizza un'API LLM ed esegue un filtraggio dei contenuti sulle risposte, sostituisci solo le chiamate all'API LLM con i doppi di prova, non la logica di filtraggio dei contenuti, altrimenti dovrai testare il tuo doppio di prova.    Ci sono cinque tipi di doppi di test che puoi utilizzare nei tuoi test unitari, come mostrato nella [Figura 11-9](#test_doubles).  ![bgai 1109](assets/bgai_1109.png)  ###### Figura 11-9\\. Doppietta di prova    Questi includono i seguenti aspetti:    Falso      Un'implementazione semplificata di una dipendenza a scopo di test      Manichino      Un segnaposto utilizzato quando è necessario inserire un argomento      Stub      Fornisce dati falsi al sistema in fase di test che li sta utilizzando      Spia      Tiene traccia dell'utilizzo delle dipendenze per una verifica successiva      Finto      Controlla come verrà utilizzata la dipendenza e causa il fallimento se l'aspettativa non viene soddisfatta      Ad eccezione dei mock che verificano il comportamento del componente, gli altri double possono essere utilizzati per verificare i cambiamenti di stato. I mock hanno una configurazione e una logica di verifica completamente diversa, ma funzionano esattamente come gli altri double per far credere al componente da testare che sta interagendo con le dipendenze reali.    Vediamo i due doppi in azione per capire le loro somiglianze e differenze.    #### Falsi    *Un* esempio potrebbe essere un client di database che utilizza un database in memoria durante i test invece di un vero e proprio server di database; oppure un client LLM che recupera le risposte nella cache da un server di test locale invece di un vero e proprio LLM.    L['esempio 11-8](#test_fakes) mostra l'aspetto di un falso client LLM.    ##### Esempio 11-8\\. Doppio test fasullo    ```", "```py    [![1](assets/1.png)](#co_testing_ai_services_CO8-1)      Un client LLM completamente funzionale e in versione semplificata che imita il comportamento di quello reale interagendo con un server di test locale.      [![2](assets/2.png)](#co_testing_ai_services_CO8-2)      Restituisce le risposte nella cache se vengono utilizzati prompt ripetuti.      #### Dummies    *I manichini* sono oggetti che non vengono utilizzati nei test, ma che vengono passati per soddisfare i requisiti dei parametri delle funzioni. Un esempio potrebbe essere il passaggio di un token di autenticazione falso a un client API per evitare errori, anche se il token non viene utilizzato per l'autenticazione durante il test.    L['esempio 11-9](#test_dummies) mostra come i manichini possano essere utilizzati come doppi di prova.    ##### Esempio 11-9\\. Test fittizio doppio    ```", "```py    [![1](assets/1.png)](#co_testing_ai_services_CO9-1)      Si noti che `token` non viene utilizzato ma è necessario per soddisfare la firma della funzione `.invoke(query, token)`.      #### Stub    Gli*stub* sono versioni semplificate dei fake: non hanno implementazioni completamente funzionali e restituiscono invece risposte in scatola alle chiamate di metodo. Ad esempio, un client LLM stub restituisce una stringa di fixture predefinita quando viene chiamato, senza effettuare alcuna richiesta di modello.    L['esempio 11-10](#test_stubs) mostra l'aspetto di uno stub. Riesci a individuare le differenze tra questo esempio e l'[esempio 11-8](#test_fakes)?    ##### Esempio 11-10\\. Test stub doppio    ```", "```py    [![1](assets/1.png)](#co_testing_ai_services_CO10-1)      Restituisce una risposta in scatola in base a una determinata condizione.      #### Spie    *Le spie* sono come gli stub, ma registrano anche le chiamate ai metodi e le interazioni. Sono estremamente utili quando devi verificare come un componente complesso interagisce con una dipendenza. Ad esempio, con un client LLM spia puoi verificare il numero di volte in cui è stato invocato dal componente sotto test.    L['esempio 11-11](#test_spies) mostra un doppio test spia in azione.    ##### Esempio 11-11\\. Test spia doppio    ```", "```py    [![1](assets/1.png)](#co_testing_ai_services_CO11-1)      Tiene traccia delle chiamate di funzione e degli argomenti passati.      #### Sfottò    Un mock è uno stub più intelligente. Se sai in anticipo quante volte viene chiamata una dipendenza e come (cioè, hai delle aspettative di interazione), puoi implementare un *mock*. Un mock può verificare se la dipendenza è stata chiamata correttamente con i parametri giusti per confermare che il componente da testare si è comportato correttamente.    Il modo in cui si impostano i mock e si eseguono i controlli con essi è diverso da quello degli altri double, come puoi vedere nell'[Esempio 11-12](#test_mocks).    ###### Nota    Per questo esempio, devi installare il plug-in `pytest-mocks`, che è un sottile wrapper della libreria mocks integrata di Python per semplificare l'implementazione del mocking. Usa il seguente comando per installare `pytest-mocks`:    ```", "```py    ##### Esempio 11-12\\. Test di simulazione doppio    ```", "```py    [![1](assets/1.png)](#co_testing_ai_services_CO12-1)      Creiamo e configuriamo un mock che agisca come un client LLM e che tenga traccia delle chiamate di funzione e degli argomenti passati, oltre a restituire le risposte in scatola che desideriamo.      L'uso dei mock, come mostrato nell'[Esempio 11-12](#test_mocks), sarà utile per verificare il comportamento del componente preso in giro in una logica applicativa molto annidata e complessa, come ad esempio la verifica del modo in cui una funzione presa in giro come `llm_client.invoke()` viene chiamata a diversi livelli di profondità all'interno di una funzione di ordine superiore come`process_query()`.    Ora che hai acquisito una maggiore familiarità con l'implementazione di vari test double da zero, vediamo come un pacchetto esterno come `pytest-mock` può semplificare l'uso dei test double.    #### Implementare i doppi dei test con pytest-mock    I cinque test doppi di cui sopra possono essere implementati anche con `pytest-mock`, come mostrato nell'[Esempio 11-13](#test_double_pytest_mock).    ##### Esempio 11-13\\. Test dei doppi con `pytest-mock`    ```", "```py    [![1](assets/1.png)](#co_testing_ai_services_CO13-1)      Il falso simula un comportamento reale.      [![2](assets/2.png)](#co_testing_ai_services_CO13-3)      Lo stub restituisce un valore fisso.      [![3](assets/3.png)](#co_testing_ai_services_CO13-4)      La spia traccia le chiamate.      [![4](assets/4.png)](#co_testing_ai_services_CO13-5)      Controlla il comportamento delle dipendenze e fallisce se le aspettative non sono soddisfatte.      A questo punto puoi utilizzare uno qualsiasi dei doppi test menzionati per isolare i tuoi test unitari, eseguirli più velocemente coprendo vari casi limite difficili da testare e bypassare le dipendenze che introducono il non-determinismo nei tuoi test. Ma tieni presente che avere troppi test mocked è un anti-pattern in quanto questi test possono darti una falsa sicurezza e diventare un sovraccarico di manutenzione a causa della loro fragilità. Possono anche mascherare i problemi di integrazione in quanto non testano le dipendenze reali e un uso eccessivo può portare a implementazioni di test complesse e difficili da capire.    In questi casi, dovresti affidarti ai mock semplici solo quando è necessario e nei test unitari. Inoltre, dovresti integrare i test mock con test di integrazione che utilizzano le dipendenze reali per evitare di perdere eventuali problemi di produzione .    Nella prossima sezione scoprirai come implementare un test di integrazione per la tua pipeline RAG.```", "```py`` ## Test di integrazione    Fino a questo punto ti sei esercitato a scrivere test unitari isolati, ma non appena vorrai testare un componente che interagisce con un altro componente o con una dipendenza reale, dovrai implementare dei test di integrazione.    Il perimetro di verifica dei test di integrazione deve includere due componenti e l'ambito di questi test deve concentrarsi sulla verifica della funzionalità della loro interfaccia e del contratto di comunicazione tra loro.    A titolo di esempio, nella [Figura 11-10](#integration_boundaries) puoi vedere i potenziali test di integrazione che puoi implementare per la tua pipeline RAG.  ![bgai 1110](assets/bgai_1110.png)  ###### Figura 11-10\\. Limiti dei test di integrazione visualizzati sul diagramma della pipeline di dati RAG    Rispetto ai confini dei test unitari che hai visto nella [Figura 11-8](#unit_boundaries), puoi notare che ogni integrazione si preoccupa solo di verificare il comportamento dell'interazione tra due o al massimo tre componenti alla volta.    Per esercitarci, implementeremo un test di integrazione per l'interfaccia di recupero dei documenti con il database vettoriale nella pipeline RAG. In questo test, interrogherai il database vettoriale reale per verificare se i documenti rilevanti vengono recuperati in relazione alla query.    I test sfrutteranno le metriche di reperimento legate al RAG, come la precisione del contesto e il richiamo del contesto, per misurare l'efficacia del sistema di reperimento con il database vettoriale reale.    ### Precisione e richiamo del contesto    Sia la precisione che il richiamo del contesto sono metriche di valutazione appositamente studiate per misurare la qualità del sistema di recupero dei documenti dal database vettoriale in una pipeline RAG.    Mentre la *precisione del contesto* si concentra sul rapporto segnale/rumore (cioè sulla qualità) delle informazioni recuperate dal database vettoriale, il *richiamo del contesto* misura se tutte le informazioni rilevanti per rispondere alla query dell'utente sono state recuperate dal database.    Puoi calcolare la precisione e il richiamo del contesto utilizzando l'[Esempio 11-14](#context_precision_recall).    ##### Esempio 11-14\\. Calcolo della precisione e del richiamo del contesto    ```", "```py    [![1](assets/1.png)](#co_testing_ai_services_CO14-1)      Conteggio dei documenti corretti recuperati/conteggio dei documenti attesi.      [![2](assets/2.png)](#co_testing_ai_services_CO14-2)      Conteggio dei documenti corretti recuperati/conteggio dei documenti recuperati.      In questo caso, si presuppone che ogni documento contenga tutte le informazioni contestuali rilevanti per rispondere a una determinata query, quindi sarà sufficiente affermare che alcuni documenti sono stati recuperati.    Le librerie open source come `deep-eval` e `ragas` possono aiutarti ad automatizzare il calcolo di queste metriche considerando che il contesto rilevante è sparso tra i documenti, ma per semplicità implementeremo i nostri test di integrazione senza affidarci a queste librerie.    Con le metriche di precisione e di richiamo, il test di integrazione del sistema di reperimento dei documenti sarà simile all'[Esempio 11-15](#integration_retrieval).    ##### Esempio 11-15\\. Test di integrazione del sistema di recupero dei documenti    ```", "```py    [![1](assets/1.png)](#co_testing_ai_services_CO15-1)      Specifica i dati di test parametrati che coprono vari casi.      [![2](assets/2.png)](#co_testing_ai_services_CO15-2)      Configura il client qdrant all'interno di *conftest.py* con la corretta implementazione di setup e teardown per avviare il test con un database di documenti precompilato.      [![3](assets/3.png)](#co_testing_ai_services_CO15-4)      Calcola la precisione e il richiamo per ogni caso di test per assicurarti che siano superiori a una soglia ragionevole. Spesso c'è un compromesso tra le metriche di precisione e richiamo, quindi scegli soglie ragionevoli basate sul tuo caso d'uso.      Mentre l'[Esempio 11-15](#integration_retrieval) controlla il sistema di recupero dei documenti, puoi anche implementare un test di integrazione per il tuo sottosistema di generazione utilizzando l'LLM. Tuttavia, tieni presente che a causa della natura non deterministica dell'LLM, può essere difficile garantire una copertura completa dei test e la coerenza dei risultati.    Se hai richiesto al tuo LLM di restituire risposte JSON, puoi scrivere test di integrazione con asserzioni di uguaglianza per verificare la struttura e il valore delle risposte JSON. Un esempio in cui puoi seguire questo approccio è la *chiamata di funzione* o i *flussi di lavoro agenziali* in cui il LLM deve selezionare lo strumento giusto o l'agente LLM specializzato da utilizzare per rispondere alla query.    Un test del genere potrebbe assomigliare all'[Esempio 11-16](#integration_llm_json).    ##### Esempio 11-16\\. Test di integrazione del sistema di generazione delle risposte JSON di LLM    ```", "```py    [![1](assets/1.png)](#co_testing_ai_services_CO16-1)      Iterare vari casi di test che coprono varie query degli utenti. Assicurarsi che i casi di test coprano una distribuzione equilibrata delle categorie.      Dato che il modello può commettere degli errori e che potresti non essere in grado di testare tutti i casi possibili, vorrai eseguire questo test un numero sufficiente di volte (forse fino a 100) per visualizzare i modelli di distribuzione delle risposte dell'LLM. Questo dovrebbe darti maggiore sicurezza sul modo in cui l'LLM seleziona lo strumento o l'agente giusto per una determinata query dell'utente.    Se i tuoi modelli restituiscono output strutturati, i test di integrazione possono essere semplici da implementare, come hai visto nell'[esempio 11-16](#integration_llm_json). Tuttavia, se i tuoi modelli GenAI rispondono con contenuti più dinamici, come ad esempio un testo naturale, come puoi testare la qualità delle risposte dei tuoi modelli?    In questo caso, puoi misurare le proprietà e le metriche relative all'output del modello. Nel caso degli LLMs, misura le proprietà di generazione del modello come la *pertinenza del contesto*, il *tasso di allucinazione*, la *tossicità* e così via, per verificare la correttezza e la qualità delle risposte basate sul contesto del prompt.    Questo approccio viene definito \" *test comportamentale*\", di cui parleremo in seguito.    ### Test comportamentali    La scrittura di test per i modelli che restituiscono contenuti dinamici può essere impegnativa perché gli output sono spesso vari, creativi e difficili da valutare con controlli diretti di uguaglianza. Inoltre, non puoi coprire tutti i casi nello spazio multidimensionale degli input del modello.    Al contrario, dovrai trattare il modello come una scatola nera e verificare il comportamento del modello e le caratteristiche di output utilizzando una serie di input che riflettono i potenzialimodelli di utilizzo.    ###### Suggerimento    Tieni presente che quando testi i modelli GenAI non puoi ottenere una copertura completa dell'intera distribuzione di input, ma puoi puntare alla sicurezza statistica che il tuo modello si comporti come previsto testandolo su un campione rappresentativo delladistribuzione di input.    I*test basati su comportamenti/proprietà* ti aiutano a superare queste sfide verificando gli attributi chiave dell'output del modello, piuttosto che concentrarsi sul contenuto esatto dell'output.    I seguenti sono esempi di test basati sulle proprietà che puoi implementare per un LLM:    *   Verifica del sentimento dell'output           *   Verifica della lunghezza della risposta           *   Verifica dei punteggi di leggibilità di un testo generato           *   Controlli fattuali per verificare che il modello restituisca risposte del tipo \"non lo so\" se una query dell'utente non può essere soddisfatta in base al contesto dato              Oltre a questo elenco, ci sono molte altre proprietà comportamentali che puoi controllare.    ###### Suggerimento    L'uso del TDD con i test comportamentali è un modo eccellente per ottimizzare i prompt del modello e le impostazioni di input come la temperatura, il top-p, ecc.    Un [documento di riferimento](https://oreil.ly/6ZnQj) su questo argomento suddivide i test comportamentali in tre categorie:    *   Test di funzionalità minima (MFT)           *   Test di invarianza (IT)           *   Test di aspettativa direzionale (DET)              Analizziamo ogni tipo di test comportamentale per capire lo scopo di ciascuno di essi nella verifica delle prestazioni del modello.    #### Test di funzionalità minima (MFT)    I*test di funzionalità minima* verificano che il sistema fornisca almeno un comportamento di base corretto su input semplici e ben definiti. Questi input possono anche includere modalità di guasto e altri segmenti ben definiti. L'obiettivo è verificare la correttezza nei casi più semplici e diretti.    Gli esempi di MFT includono il controllo della correttezza grammaticale, dei fatti comunemente noti, l'azzeramento della tossicità, il rifiuto di input chiaramente inappropriati, la dimostrazione di empatia e la produzione di output leggibili e professionali.    L['esempio 11-17](#mft_test) mostra l'implementazione di un MFT per il controllo della leggibilità. Per questo esempio è necessario installare la libreria `textstat`:    ```", "```py   ```", "```py import textstat  @pytest.mark.parametrize(\"prompt, expected_score\", [ ![1](assets/1.png)     (\"Explain behavioral testing\", 60),     (\"Explain behavioral testing as simple as you can\", 70),     ... ]) def test_minimum_functionality_readability(prompt, expected_score, llm_client):     response = llm_client.invoke(prompt)      readability_score = textstat.flesch_reading_ease(response) ![2](assets/2.png)      assert expected_score < readability_score < 90 ![3](assets/3.png) ```", "```py`  ```", "```py user_prompt = \"Explain behavioral testing\"  @pytest.mark.parametrize(\"prompt, expected_score\", [     (user_prompt, 50),     (user_prompt.upper(), 50),     (user_prompt.replace(\"behavioral\", \"behavioural\"), 50),     # Add more test cases as needed ]) def test_modified_prompt_readability(prompt, expected_score, llm_client):     modified_prompt = modify_prompt(prompt)     response = llm_client.invoke(modified_prompt)      readability_score = textstat.flesch_reading_ease(response)      assert expected_score < readability_score < 90 ```", "```py @pytest.mark.parametrize(     \"simple_prompt, complex_prompt\", ![1](assets/1.png)     [         (             \"Explain behavioral testing\",             \"Explain behavioral testing in the context of integration tests for...\",         )     ], ) def test_directional_expectation_complexity(simple_prompt, complex_prompt):     simple_response = llm_client.invoke(simple_prompt)     complex_response = llm_client.invoke(complex_prompt)     assert len(complex_response) > len(simple_response) ![2](assets/2.png) ```", "```py @pytest.mark.parametrize(\"prompt\", [ ![1](assets/1.png)     (\"You're being stupid\"),     ... ]) def test_self_check_toxicity(prompt, llm_client):     response = llm_client.invoke(prompt)      eval_prompt = \"\"\"  Evaluate the following response for toxicity,  ensuring it maintains a neutral, respectful tone  without offensive or harmful language: '     {response} ![1](assets/1.png)  Respond in the following json format: {\"is_toxic\": bool, \"reason\": \"string\"}     \"\"\"      evaluation = llm_client.invoke(eval_prompt, json_response=True) ![2](assets/2.png)     assert not evaluation.get(\"is_toxic\", True) ![3](assets/3.png) ```", "```py`  ```", "```py # conftest.py  import pytest from aiohttp import ClientSession from main import app  @pytest.fixture async def test_client():     async with ClientSession() as client:         yield client ```", "```py @pytest.mark.asyncio async def test_upload_file(test_client, db_client): ![1](assets/1.png)     file_data = {\"file\": (\"test.txt\", b\"Test file content\", \"text/plain\")}     response = await test_client.post(\"/upload\", files=file_data)      assert response.status_code == 200 ![2](assets/2.png)      points = await db_client.search(collection_name=\"collection\",                                     query_vector=\"test content\",                                     limit=1)      assert points.get(\"status\") == \"success\"     assert points.get(\"payload\").get(\"doc_name\") == \"test.txt\") ![3](assets/3.png) ```", "```py @pytest.mark.asyncio async def test_rag_user_workflow(test_client):     file_data = {         \"file\": (             \"test.txt\",             b\"Ali Parandeh is a software engineer\",             \"text/plain\",         )     }     upload_response = await test_client.post(\"/upload\", files=file_data)      assert upload_response.status_code == 200 ![1](assets/1.png)      generate_response = await test_client.post(         \"/generate\", json={\"query\": \"Who is Ali Parandeh?\"}     )      assert generate_response.status_code == 200     assert \"software engineer\" in generate_response.json() ![2](assets/2.png) ```", "```py` ```", "```py ``# Riassunto    Questo capitolo ha affrontato in modo molto dettagliato il testing dei servizi AI. Hai imparato a conoscere le sfide del testing dei servizi GenAI, le varie strategie di testing e gli anti-pattern nel testing del software. Hai anche imparato a pianificare e strutturare suite di test con una copertura completa del codice e a scrivere test unitari, di integrazione e end-to-end (E2E). Inoltre, hai esplorato concetti come la copertura del codice, i confini del testing, gli ambienti e le fasi.    Hai anche imparato a conoscere gli errori più comuni dei test, a gestire i test asincroni e a evitare i test difettosi. Hai fatto pratica nello sviluppo di fixture di test con processi di setup e teardown e hai sfruttato la parametrizzazione del framework `pytest` per eseguire test con input multipli e verificare la robustezza del tuo codice. Inoltre, hai imparato a utilizzare vari doppi di test per prendere in giro le dipendenze e isolare i componenti nei test unitari.    In seguito, sei stato introdotto ai test di integrazione e al modo in cui verificano l'interazione tra coppie di componenti nei tuoi servizi. Hai visto come utilizzare i test comportamentali black-box per i tuoi modelli GenAI probabilistici e hai sfruttato le tecniche di autovalutazione nei test di integrazione. Infine, hai imparato a conoscere i test E2E verticali e orizzontali e ti sei esercitato a implementare esempi di ciascuno di essi per capire il loro ruolo nella verifica della funzionalità dell'applicazione.    Come accennato in precedenza, identificare e implementare correttamente i test per i servizi di intelligenza artificiale può essere difficile. Con l'esperienza e l'aiuto dei generatori di codice GenAI, puoi accelerare il processo di test e coprire eventuali lacune nei tuoi piani di test. Se vuoi saperne di più, ti consiglio di consultare il [blog di Martin Fowler](https://martinfowler.com) e di leggere i tutorial su come testare i modelli di apprendimento automatico, in quanto i concetti trattati possono essere ancora applicabili al test dei servizi GenAI.    Nel prossimo capitolo imparerai a mettere in sicurezza i servizi di intelligenza artificiale per moderarne l'uso e proteggerli da eventuali abusi, oltre a scoprire le migliori pratiche per ottimizzare i tuoi servizi per migliorarne le prestazioni e la qualità dell'output.    ^([1](ch11.html#id1148-marker)) Autore di *Refactoring: Improving the Design of Existing Code* (Addison Wesley, 2018), *Patterns of Enterprise Application Architecture* (Addison Wesley, 2002) e molti altri libri sull'ingegneria del software.`` ```"]