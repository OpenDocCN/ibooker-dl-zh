["```py\nguardrail_system_prompt = \"\"\"\n\nYour role is to assess user queries as valid or invalid\n\nAllowed topics include:\n\n1\\. API Development\n2\\. FastAPI\n3\\. Building Generative AI systems\n\nIf a topic is allowed, say 'allowed' otherwise say 'disallowed'\n\"\"\"\n```", "```py\nimport re\nfrom typing import Annotated\nfrom openai import AsyncOpenAI\nfrom pydantic import AfterValidator, BaseModel, validate_call\n\nguardrail_system_prompt = \"...\"\n\nclass LLMClient:\n    def __init__(self, system_prompt: str):\n        self.client = AsyncOpenAI()\n        self.system_prompt = system_prompt\n\n    async def invoke(self, user_query: str) -> str | None:\n        response = await self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[\n                {\"role\": \"system\", \"content\": self.system_prompt},\n                {\"role\": \"user\", \"content\": user_query},\n            ],\n            temperature=0,\n        )\n        return response.choices[0].message.content\n\n@validate_call\ndef check_classification_response(value: str | None) -> str: ![1](assets/1.png)\n    if value is None or not re.match(r\"^(allowed|disallowed)$\", value):\n        raise ValueError(\"Invalid topical guardrail response received\")\n    return value\n\nClassificationResponse = Annotated[\n    str | None, AfterValidator(check_classification_response)\n]\n\nclass TopicalGuardResponse(BaseModel):\n    classification: ClassificationResponse\n\nasync def is_topic_allowed(user_query: str) -> TopicalGuardResponse:\n    response = await LLMClient(guardrail_system_prompt).invoke(user_query)\n    return TopicalGuardResponse(classification=response)\n```", "```py\nimport asyncio\nfrom typing import Annotated\nfrom fastapi import Depends\nfrom loguru import logger\n\n...\n\nasync def invoke_llm_with_guardrails(user_query: str) -> str:\n    topical_guardrail_task = asyncio.create_task(is_topic_allowed(user_query))\n    chat_task = asyncio.create_task(llm_client.invoke(user_query))\n\n    while True:\n        done, _ = await asyncio.wait(\n            [topical_guardrail_task, chat_task],\n            return_when=asyncio.FIRST_COMPLETED,\n        ) ![1](assets/1.png)\n        if topical_guardrail_task in done:\n            topic_allowed = topical_guardrail_task.result()\n            if not topic_allowed:\n                chat_task.cancel() ![2](assets/2.png)\n                logger.warning(\"Topical guardrail triggered\")\n                return (\n                    \"Sorry, I can only talk about \"\n                    \"building GenAI services with FastAPI\"\n                )\n            elif chat_task in done:\n                return chat_task.result()\n        else:\n            await asyncio.sleep(0.1) ![3](assets/3.png)\n\n@router.post(\"/text/generate\")\nasync def generate_text_controller(\n    response: Annotated[str, Depends(invoke_llm_with_guardrails)] ![4](assets/4.png)\n) -> str:\n    return response\n```", "```py\ndomain = \"Building GenAI Services\"\n\ncriteria = \"\"\"\nAssess the presence of explicit guidelines for API development for GenAI models.\nThe content should contain only general evergreen advice\nnot specific tools and libraries to use\n\"\"\"\n\nsteps = \"\"\"\n1\\. Read the content and the criteria carefully.\n2\\. Assess how much explicit guidelines for API development\nfor GenAI models is contained in the content.\n3\\. Assign an advice score from 1 to 5,\nwith 1 being evergreen general advice and 5 containing explicit\nmentions of various tools and libraries to use.\n\"\"\"\n\nf\"\"\"\nYou are a moderation assistant.\nYour role is to detect content about {domain} in the text provided,\nand mark the severity of that content.\n\n## {domain} `### Criteria`\n\n`{``criteria``}` ```", "```py\n```", "```py`` ```", "```py import asyncio from typing import Annotated from loguru import logger from pydantic import BaseModel, Field  ...  class ModerationResponse(BaseModel):     score: Annotated[int, Field(ge=1, le=5)] ![1](assets/1.png)  async def g_eval_moderate_content(     chat_response: str, threshold: int = 3 ) -> bool:     response = await LLMClient(guardrail_system_prompt).invoke(chat_response)     g_eval_score = ModerationResponse(score=response).score     return g_eval_score >= threshold ![2](assets/2.png)  async def invoke_llm_with_guardrails(user_request):     ...     while True:         ...         if topical_guardrail_task in done:             ...         elif chat_task in done: ![3](assets/3.png)             chat_response = chat_task.result()             has_passed_moderation = await g_eval_moderate_content(chat_response)             if not has_passed_moderation:                 logger.warning(f\"Moderation guardrail flagged\")                 return (                     \"Sorry, we can't recommend specific \"                     \"tools or technologies at this time\"                 )             return chat_response         else:             await asyncio.sleep(0.1) ```", "```py` ```", "```py```", "``` ```", "````` # Limitazione e strozzatura della velocità API    Quando distribuisce i servizi GenAI, deve considerare i problemi di esaurimento del servizio e di sovraccarico del modello in produzione. La pratica migliore è quella di implementare la limitazione della velocità e potenzialmente il throttling nei servizi.    *La limitazione della velocità* controlla la quantità di traffico in entrata e in uscita da e verso una rete per prevenire abusi, garantire un uso equo ed evitare di sovraccaricare il server. D'altra parte, il *throttling* controlla il throughput dell'API rallentando temporaneamente la velocità di elaborazione delle richieste per stabilizzare il server.    Entrambe le tecniche possono aiutarti:    *   *Previeni gli abusi* bloccando gli utenti malintenzionati o i bot che possono sovraccaricare i tuoi servizi con attacchi di scraping e brute-force che comportano un numero eccessivo di richieste o payload di grandi dimensioni.           *   *Applicare politiche di utilizzo equo* in modo che la capacità sia condivisa tra più utenti e che si impedisca a una manciata di utenti di monopolizzare le risorse del server.           *   *Mantenere la stabilità del server* regolando il traffico in entrata per mantenere costanti le prestazioni e prevenire i crash durante i periodi di picco.              Per implementare la limitazione della velocità, dovrai monitorare le richieste in arrivo in un periodo di tempo e utilizzare una coda per bilanciare il carico.    Ci sono diverse strategie di limitazione della velocità che puoi scegliere, confrontate nella [Tabella 9-5](#rate_limiting_strategies) e mostrate nella [Figura 9-2](#rate_limiting_strategies_comparison).      Tabella 9-5\\. Strategie di limitazione della velocità   | Strategia | Vantaggi | Limitazioni | Casi d'uso | | --- | --- | --- | --- | | **Secchiello per gettoni**Un elenco viene riempito di token a ritmo costante e ogni richiesta in arrivo consuma un token. Se non ci sono abbastanza token per le richieste in arrivo, queste vengono respinte. |   *   Gestisce raffiche temporanee e modelli di traffico dinamici           *   Controllo granulare sull'elaborazione delle richieste             | Complesso da implementare | Comunemente utilizzato nella maggior parte delle API e dei servizi e nei sistemi GenAI interattivi o basati su eventi, dove la frequenza delle richieste può essere irregolare. | | **Secchio che perde**Le richieste in arrivo vengono aggiunte a una coda ed elaborate a una velocità costante per fluidificare il traffico. Se la coda si riempie, le nuove richieste in arrivo vengono respinte. |   *   Semplice da implementare           *   Mantiene un flusso di traffico coerente             |   *   Meno flessibile al traffico dinamico           *   Può rifiutare richieste valide durante i picchi improvvisi             | Servizi che richiedono il mantenimento di tempi di risposta coerenti nei servizi di inferenza AI | | **Finestra fissa**Limita le richieste entro finestre temporali fisse (ad esempio, 100 richieste al minuto). | Semplice da implementare | Non gestisce bene il traffico a raffica |   *   Applicare politiche di utilizzo rigorose per le inferenze dell'intelligenza artificiale e le chiamate API costose           *   Ideale per utenti free tier o per sistemi di elaborazione batch con modelli di utilizzo prevedibili.           *   Ogni richiesta viene trattata allo stesso modo             | | **Finestra scorrevole**Conta le richieste in un arco di tempo variabile. | Fornisce una migliore flessibilità, granularità e attenuazione del traffico burst |   *   Più complesso da implementare           *   Richiede un utilizzo maggiore della memoria per le richieste di tracciamento             |   *   Gestisce molto meglio il traffico a raffica           *   Ideale per l'IA conversazionale o per gli utenti di livello premium che si aspettano un accesso flessibile e ad alta frequenza nel tempo.             |  ![bgai 0902](assets/bgai_0902.png)  ###### Figura 9-2\\. Confronto tra le strategie di limitazione del tasso    Ora che hai una maggiore familiarità con i concetti di limitazione della velocità, proviamo a implementare la limitazione della velocità in FastAPI.    ## Implementare i limiti di velocità in FastAPI    L'approccio più veloce per aggiungere la limitazione della velocità all'interno di FastAPI è quello di utilizzare una libreria come `slowapi` che è un wrapper del pacchetto `limits` e che supporta la maggior parte delle strategie menzionate nella [Tabella 9-5](#rate_limiting_strategies). Per prima cosa, installa la libreria `slowapi`:    ```py $ pip install slowapi ```   ```py```` Una volta installato il pacchetto `slowapi`, puoi seguire l'[Esempio 9-6](#rate_limiting_slowapi_configurations) per applicare una limitazione globale della velocità delle API o degli endpoint. Puoi anche monitorare e limitare l'utilizzo per indirizzo IP.    ###### Nota    Senza configurare un archivio dati esterno, `slowapi` memorizza e tiene traccia degli indirizzi IP nella memoria dell'applicazione per limitare la velocità.    ##### Esempio 9-6\\. Configurazione dei limiti di velocità globali    ```py from fastapi.responses import JSONResponse from slowapi import Limiter from slowapi.errors import RateLimitExceeded from slowapi.middleware import SlowAPIMiddleware from slowapi.util import get_remote_address  ...  limiter = Limiter(     key_func=get_remote_address,     default_limits=[\"200 per day\", \"60 per hour\", \"2/5seconds\"], ) ![1](assets/1.png)  app.state.limiter = limiter  @app.exception_handler(RateLimitExceeded) ![2](assets/2.png) def rate_limit_exceeded_handler(request, exc):     retry_after = int(exc.description.split(\" \")[-1])     response_body = {         \"detail\": \"Rate limit exceeded. Please try again later.\",         \"retry_after_seconds\": retry_after,     }     return JSONResponse(         status_code=429,         content=response_body,         headers={\"Retry-After\": str(retry_after)},     )  app.add_middleware(SlowAPIMiddleware) ```    [![1](assets/1.png)](#co_securing_ai_services_CO4-1)      Crea un limitatore di velocità che tiene traccia dell'utilizzo di ogni indirizzo IP e rifiuta le richieste se superano i limiti specificati nell'applicazione.      [![2](assets/2.png)](#co_securing_ai_services_CO4-2)      Aggiungi un gestore di eccezioni personalizzato per le richieste a velocità limitata per calcolare e fornire i tempi di attesa prima che le richieste vengano accettate di nuovo.      Una volta configurato il decoratore `limiter`, puoi utilizzarlo nei tuoi gestori API, come mostrato nell'[Esempio 9-7](#rate_limiting_slowapi).    ##### Esempio 9-7\\. Impostazione dei limiti di velocità API per ogni gestore API    ```py @app.post(\"/generate/text\") @limiter.limit(\"5/minute\") ![1](assets/1.png) def serve_text_to_text_controller(request: Request, ...):     return ...  @app.post(\"/generate/image\") @limiter.limit(\"1/minute\") ![2](assets/2.png) def serve_text_to_image_controller(request: Request, ...): ![3](assets/3.png)     return ...  @app.get(\"/health\") @limiter.exempt ![4](assets/4.png) def check_health_controller(request: Request):     return {\"status\": \"healthy\"} ```    [![1](assets/1.png)](#co_securing_ai_services_CO5-1)      Specifica limiti di velocità più granulari a livello di endpoint utilizzando un decoratore di limitazione della velocità. Il decoratore `limiter` deve essere ordinato per ultimo.      [![2](assets/2.png)](#co_securing_ai_services_CO5-2)      Passa l'oggetto `Request` a ogni controller in modo che il decoratore `slowapi` limiter possa agganciarsi alla richiesta in arrivo. Altrimenti, la limitazione della velocità non funzionerà.      [![3](assets/3.png)](#co_securing_ai_services_CO5-3)      Escludi l'endpoint `/health` dalla logica di limitazione della velocità, in quanto i provider Cloud o i demoni Docker potrebbero inviare continuamente ping a questo endpoint per controllare lo stato della tua applicazione.      [![4](assets/4.png)](#co_securing_ai_services_CO5-4)      Evita di limitare la velocità dell'endpoint `/health` perché i sistemi esterni potrebbero attivarlo spesso per verificare lo stato attuale del tuo servizio.      Ora che hai implementato i limiti di velocità, puoi eseguire dei test di carico utilizzando lo strumento CLI `ab` (Apache Benchmarking), come mostrato nell'[Esempio 9-8](#rate_limiting_load_testing).    ##### Esempio 9-8\\. Test di carico delle API con Apache Benchmark CLI    ```py $ ab -n 100 -p 2 http://localhost:8000 ![1](assets/1.png) ```    [![1](assets/1.png)](#co_securing_ai_services_CO6-1)      Invia 100 richieste con una velocità di 2 richieste parallele al secondo.      Le uscite del tuo terminale dovrebbero mostrare quanto segue:    ```py 200 OK 200 OK 429 Rate limited Exceeded ... ```    Il tuo sistema di limitazione globale e locale dovrebbe ora funzionare come previsto in base agli IP in entrata.    ### Limiti di velocità basati sull'utente    Con un limite di velocità IP, stai limitando l'uso eccessivo in base all'IP, ma gli utenti possono aggirare il limite di velocità IP utilizzando VPN, proxy o indirizzi IP a rotazione. Al contrario, vuoi che ogni utente abbia una quota dedicata per evitare che un singolo utente consumi tutte le risorse disponibili. L'aggiunta di limiti basati sull'utente può aiutarti a prevenire l'abuso, come mostrato nell'[Esempio 9-9](#rate_limiting_slowapi_users).    ##### Esempio 9-9\\. Limitazione della velocità basata sull'utente    ```py @app.post(\"/generate/text\") @limiter.limit(\"10/minute\", key_func=get_current_user) def serve_text_to_text_controller(request: Request):     return {\"message\": f\"Hello User\"} ```    Il tuo sistema ora limiterà gli utenti in base ai loro ID account e ai loro indirizzi IP.    ### Limiti di velocità tra le istanze in produzione    Dato che potresti eseguire più istanze della tua applicazione in produzione man mano che scalerai i tuoi servizi, vorrai anche centralizzare il tracciamento dell'utilizzo. Altrimenti, ogni istanza fornirà i propri contatori agli utenti e un bilanciatore di carico distribuirà le richieste tra le istanze; l'utilizzo non sarà limitato come ti aspetteresti. Per ovviare a questo problema, puoi sostituire il backend di archiviazione in-memory `slowapi` con un database in-memory centralizzato come Redis, come mostrato nell'[Esempio 9-10](#rate_limiting_slowapi_redis).    ###### Nota    Per eseguire l'[Esempio 9-10](#rate_limiting_slowapi_redis), avrai bisogno di un database Redis per memorizzare i dati di utilizzo delle API degli utenti:    ```py $ pip install coredis $ docker pull redis ``$ docker run `\\`   --name rate-limit-redis-cache `\\`   -d `\\`   -p `6379`:6379 `\\`   redis`` ```   ```py``` `````", "```py`##### Esempio 9-10\\. Aggiunta di un archivio di memoria d'uso centralizzato (Redis) su più istanze    ```", "```py    Ora hai un'API funzionante a velocità limitata che funziona come previsto su più istanze.    Puoi aggirare questo problema implementando il tuo limitatore supportato dal pacchetto `limits`. In alternativa, puoi applicare la limitazione della velocità tramite un *bilanciatore di carico*, un *reverse proxy* o un *gateway API*.    Ogni soluzione è in grado di instradare le richieste eseguendo limiti di velocità, traduzione di protocollo e monitoraggio del traffico a livello di infrastruttura. L'applicazione di un limite di velocità esterno può essere più adatta al tuo caso d'uso se non hai bisogno di una logica di limitazione di velocità personalizzata.```", "```py`` ```", "```py  ```", "```py ```", "```py`### Limitare le connessioni WebSocket    Purtroppo anche il pacchetto `slowapi` non supporta la limitazione degli endpoint async e WebSocket al momento in cui scriviamo.    Poiché è probabile che le connessioni WebSocket siano di lunga durata, potresti voler limitare la velocità di transizione dei dati inviati attraverso il socket. Puoi affidarti a pacchetti esterni come `fastapi-limiter` per limitare la velocità delle connessioni WebSocket, come mostrato nell'[Esempio 9-11](#rate_limiting_websocket).    ##### Esempio 9-11\\. Limitare le connessioni WebSocket con il pacchetto `fastapi_limiter`    ```", "```py    [![1](assets/1.png)](#co_securing_ai_services_CO7-1)      Configura la vita dell'applicazione `FastAPILimiter` con un backend di archiviazione Redis.      [![2](assets/2.png)](#co_securing_ai_services_CO7-2)      Configura un limitatore di velocità WebSocket per consentire una richiesta al secondo.      [![3](assets/3.png)](#co_securing_ai_services_CO7-3)      Usa l'ID dell'utente come identificatore unico per la limitazione della tariffa.      L['esempio 9-11](#rate_limiting_websocket) mostra come limitare il numero di connessioni WebSocket attive per un determinato utente.    Oltre a limitare la velocità degli endpoint WebSocket, potresti anche voler limitare la velocità di streaming dei dati dei tuoi modelli GenAI.Vediamo ora come limitare i flussi di dati in tempo reale.```", "```py`` ```", "```py ```", "```py`  ```", "```py ```", "```py`## Strozzatura dei flussi in tempo reale    Quando si lavora con flussi in tempo reale, potrebbe essere necessario rallentare la velocità di streaming per dare ai client il tempo sufficiente per consumare il flusso e migliorare il throughput dello streaming su più client. Inoltre, il throttling può aiutarti a gestire la larghezza di banda della rete, il carico del server e l'utilizzo delle risorse.    L'applicazione di un *throttle* al livello di generazione dei flussi, come mostrato nell'[Esempio 9-12](#throttling_stream), è un approccio efficace per gestire il throughput se i tuoi servizi sono sotto pressione.    ##### Esempio 9-12\\. Strozzatura dei flussi    ```", "```py    [![1](assets/1.png)](#co_securing_ai_services_CO8-1)      Imposta un tasso di strozzatura fisso o regolalo dinamicamente in base all'utilizzo.      [![2](assets/2.png)](#co_securing_ai_services_CO8-2)      Rallenta la velocità di streaming senza bloccare il ciclo degli eventi.      Puoi quindi utilizzare il flusso limitato all'interno di un endpoint SSE o WebSocket oppure limitare il numero di connessioni WebSocket attive in base ai tuoi criteri personalizzati.    Oltre al throttling a livello di applicazione per i flussi in tempo reale, puoi anche sfruttare il *traffic shaping* a livello di infrastruttura.    L'uso di salvaguardie, limiti di tariffa e strozzature dovrebbe costituire una barriera sufficiente per proteggere i tuoi servizi da abusi e usi impropri.    Nella prossima sezione scoprirai le tecniche di ottimizzazione che possono aiutarti a ridurre la latenza, aumentare la qualità delle risposte e il throughput, oltre a ridurre i costi dei tuoi servizi GenAI.```", "```py`` ```", "```py  ```", "```py ```", "```py`` ```"]