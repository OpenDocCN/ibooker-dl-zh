["```py\nguardrail_system_prompt = \"\"\"\n\nYour role is to assess user queries as valid or invalid\n\nAllowed topics include:\n\n1\\. API Development\n2\\. FastAPI\n3\\. Building Generative AI systems\n\nIf a topic is allowed, say 'allowed' otherwise say 'disallowed'\n\"\"\"\n```", "```py\nimport re\nfrom typing import Annotated\nfrom openai import AsyncOpenAI\nfrom pydantic import AfterValidator, BaseModel, validate_call\n\nguardrail_system_prompt = \"...\"\n\nclass LLMClient:\n    def __init__(self, system_prompt: str):\n        self.client = AsyncOpenAI()\n        self.system_prompt = system_prompt\n\n    async def invoke(self, user_query: str) -> str | None:\n        response = await self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[\n                {\"role\": \"system\", \"content\": self.system_prompt},\n                {\"role\": \"user\", \"content\": user_query},\n            ],\n            temperature=0,\n        )\n        return response.choices[0].message.content\n\n@validate_call\ndef check_classification_response(value: str | None) -> str: ![1](assets/1.png)\n    if value is None or not re.match(r\"^(allowed|disallowed)$\", value):\n        raise ValueError(\"Invalid topical guardrail response received\")\n    return value\n\nClassificationResponse = Annotated[\n    str | None, AfterValidator(check_classification_response)\n]\n\nclass TopicalGuardResponse(BaseModel):\n    classification: ClassificationResponse\n\nasync def is_topic_allowed(user_query: str) -> TopicalGuardResponse:\n    response = await LLMClient(guardrail_system_prompt).invoke(user_query)\n    return TopicalGuardResponse(classification=response)\n```", "```py\nimport asyncio\nfrom typing import Annotated\nfrom fastapi import Depends\nfrom loguru import logger\n\n...\n\nasync def invoke_llm_with_guardrails(user_query: str) -> str:\n    topical_guardrail_task = asyncio.create_task(is_topic_allowed(user_query))\n    chat_task = asyncio.create_task(llm_client.invoke(user_query))\n\n    while True:\n        done, _ = await asyncio.wait(\n            [topical_guardrail_task, chat_task],\n            return_when=asyncio.FIRST_COMPLETED,\n        ) ![1](assets/1.png)\n        if topical_guardrail_task in done:\n            topic_allowed = topical_guardrail_task.result()\n            if not topic_allowed:\n                chat_task.cancel() ![2](assets/2.png)\n                logger.warning(\"Topical guardrail triggered\")\n                return (\n                    \"Sorry, I can only talk about \"\n                    \"building GenAI services with FastAPI\"\n                )\n            elif chat_task in done:\n                return chat_task.result()\n        else:\n            await asyncio.sleep(0.1) ![3](assets/3.png)\n\n@router.post(\"/text/generate\")\nasync def generate_text_controller(\n    response: Annotated[str, Depends(invoke_llm_with_guardrails)] ![4](assets/4.png)\n) -> str:\n    return response\n```", "```py\ndomain = \"Building GenAI Services\"\n\ncriteria = \"\"\"\nAssess the presence of explicit guidelines for API development for GenAI models.\nThe content should contain only general evergreen advice\nnot specific tools and libraries to use\n\"\"\"\n\nsteps = \"\"\"\n1\\. Read the content and the criteria carefully.\n2\\. Assess how much explicit guidelines for API development\nfor GenAI models is contained in the content.\n3\\. Assign an advice score from 1 to 5,\nwith 1 being evergreen general advice and 5 containing explicit\nmentions of various tools and libraries to use.\n\"\"\"\n\nf\"\"\"\nYou are a moderation assistant.\nYour role is to detect content about {domain} in the text provided,\nand mark the severity of that content.\n\n## {domain}\n\n### Criteria\n\n{criteria}\n\n### Instructions\n\n{steps}\n\n### Evaluation (score only!)\n\"\"\"\n```", "```py\nimport asyncio\nfrom typing import Annotated\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\n\n...\n\nclass ModerationResponse(BaseModel):\n    score: Annotated[int, Field(ge=1, le=5)] ![1](assets/1.png)\n\nasync def g_eval_moderate_content(\n    chat_response: str, threshold: int = 3\n) -> bool:\n    response = await LLMClient(guardrail_system_prompt).invoke(chat_response)\n    g_eval_score = ModerationResponse(score=response).score\n    return g_eval_score >= threshold ![2](assets/2.png)\n\nasync def invoke_llm_with_guardrails(user_request):\n    ...\n    while True:\n        ...\n        if topical_guardrail_task in done:\n            ...\n        elif chat_task in done: ![3](assets/3.png)\n            chat_response = chat_task.result()\n            has_passed_moderation = await g_eval_moderate_content(chat_response)\n            if not has_passed_moderation:\n                logger.warning(f\"Moderation guardrail flagged\")\n                return (\n                    \"Sorry, we can't recommend specific \"\n                    \"tools or technologies at this time\"\n                )\n            return chat_response\n        else:\n            await asyncio.sleep(0.1)\n```", "```py\n$ pip install slowapi\n```", "```py\nfrom fastapi.responses import JSONResponse\nfrom slowapi import Limiter\nfrom slowapi.errors import RateLimitExceeded\nfrom slowapi.middleware import SlowAPIMiddleware\nfrom slowapi.util import get_remote_address\n\n...\n\nlimiter = Limiter(\n    key_func=get_remote_address,\n    default_limits=[\"200 per day\", \"60 per hour\", \"2/5seconds\"],\n) ![1](assets/1.png)\n\napp.state.limiter = limiter\n\n@app.exception_handler(RateLimitExceeded) ![2](assets/2.png)\ndef rate_limit_exceeded_handler(request, exc):\n    retry_after = int(exc.description.split(\" \")[-1])\n    response_body = {\n        \"detail\": \"Rate limit exceeded. Please try again later.\",\n        \"retry_after_seconds\": retry_after,\n    }\n    return JSONResponse(\n        status_code=429,\n        content=response_body,\n        headers={\"Retry-After\": str(retry_after)},\n    )\n\napp.add_middleware(SlowAPIMiddleware)\n```", "```py\n@app.post(\"/generate/text\")\n@limiter.limit(\"5/minute\") ![1](assets/1.png)\ndef serve_text_to_text_controller(request: Request, ...):\n    return ...\n\n@app.post(\"/generate/image\")\n@limiter.limit(\"1/minute\") ![2](assets/2.png)\ndef serve_text_to_image_controller(request: Request, ...): ![3](assets/3.png)\n    return ...\n\n@app.get(\"/health\")\n@limiter.exempt ![4](assets/4.png)\ndef check_health_controller(request: Request):\n    return {\"status\": \"healthy\"}\n```", "```py\n$ ab -n 100 -p 2 http://localhost:8000 ![1](assets/1.png)\n```", "```py\n200 OK\n200 OK\n429 Rate limited Exceeded\n...\n```", "```py\n@app.post(\"/generate/text\")\n@limiter.limit(\"10/minute\", key_func=get_current_user)\ndef serve_text_to_text_controller(request: Request):\n    return {\"message\": f\"Hello User\"}\n```", "```py\n$ pip install coredis\n$ docker pull redis\n$ docker run \\\n  --name rate-limit-redis-cache \\\n  -d \\\n  -p 6379:6379 \\\n  redis\n```", "```py\nfrom slowapi import Limiter\nfrom slowapi.middleware import SlowAPIMiddleware\n\napp.state.limiter = Limiter(storage_uri=\"redis://localhost:6379\")\napp.add_middleware(SlowAPIMiddleware)\n```", "```py\nfrom contextlib import asynccontextmanager\nimport redis\nfrom fastapi import Depends, FastAPI\nfrom fastapi.websockets import WebSocket\nfrom fastapi_limiter import FastAPILimiter\nfrom fastapi_limiter.depends import WebSocketRateLimiter\n\n...\n\n@asynccontextmanager\nasync def lifespan(_: FastAPI): ![1](assets/1.png)\n    redis_connection = redis.from_url(\"redis://localhost:6379\", encoding=\"utf8\")\n    await FastAPILimiter.init(redis_connection)\n    yield\n    await FastAPILimiter.close()\n\napp = FastAPI(lifespan=lifespan)\n\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(\n    websocket: WebSocket, user_id: int = Depends(get_current_user) ![2](assets/2.png)\n):\n    ratelimit = WebSocketRateLimiter(times=1, seconds=5)\n    await ws_manager.connect(websocket)\n    try:\n        while True:\n            prompt = await ws_manager.receive(websocket)\n            await ratelimit(websocket, context_key=user_id) ![3](assets/3.png)\n            async for chunk in azure_chat_client.chat_stream(prompt, \"ws\"):\n                await ws_manager.send(chunk, websocket)\n    except WebSocketRateLimitException:\n        await websocket.send_text(f\"Rate limit exceeded. Try again later\")\n    finally:\n        await ws_manager.disconnect(websocket)\n```", "```py\nclass AzureOpenAIChatClient:\n    def __init__(self, throttle_rate = 0.5): ![1](assets/1.png)\n        self.aclient = ...\n        self.throttle_rate = throttle_rate\n\n    async def chat_stream(\n            self, prompt: str, mode: str = \"sse\", model: str = \"gpt-3.5-turbo\"\n    ) -> AsyncGenerator[str, None]:\n        stream = ...  # OpenAI chat completion stream\n        async for chunk in stream:\n            await asyncio.sleep(self.throttle_rate) ![2](assets/2.png)\n            if chunk.choices[0].delta.content is not None:\n                yield (\n                    f\"data: {chunk.choices[0].delta.content}\\n\\n\"\n                    if mode == \"sse\"\n                    else chunk.choices[0].delta.content\n                )\n                await asyncio.sleep(0.05)\n\n        if mode == \"sse\":\n            yield f\"data: [DONE]\\n\\n\"\n```"]