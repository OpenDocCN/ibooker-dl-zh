- en: 7 Making social connections with chatbots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Exploring anecdotes of human-chatbot relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the social causes and context of human-chatbot relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing the benefits and the potential downside risks of such relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommending courses of action for the development of responsible social chatbots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '‚ÄúSiri, will you marry me?‚Äù Judith Newman, mother and author of *To Siri, With
    Love*, recalls the moment she heard her son, Gus, pop the question to the voice
    assistant. When Siri responded, ‚ÄúI‚Äôm not the marrying kind,‚Äù Gus persisted: ‚ÄúI
    mean, not now. I‚Äôm a kid. I mean when I‚Äôm grown up.‚Äù Siri said firmly, ‚ÄúMy end-user
    agreement does not include marriage,‚Äù and Gus moved on. Newman was floored‚Äîit
    was the first time, she writes, that she knew her autistic son thought about marriage
    [[1]](https://www.nytimes.com/2014/10/19/fashion/how-apples-siri-became-one-autistic-boys-bff.xhtml).
    Although Gus was perfectly satisfied with this refusal, he would not be the first
    to test the limits of human-bot relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we discuss the extent to which large language models (LLMs)
    are used not only as chatbots but as social chatbots: conversational agents whose
    primary purpose is building social connections with users. We‚Äôll talk about the
    popularity of and uses for these products, as well as the potential implications
    for emotional development and human relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots for social interaction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The romance between a human and a machine is a tale as old as time. For the
    past several decades, science fiction writers have been creating stories of humans
    falling in love with robots. In *The Silver Metal Lover*, a 1981 science fiction
    novel, Jane, an insecure and lonely 16-year-old girl, falls passionately in love
    with the robot, Silver, who becomes more and more humanlike in loving her. We
    see several more examples of fictional human-robot relationships in the 20th century,
    including TV series *Star Trek: The Next Generation* (1987), *Forward the Foundation*
    by Isaac Asimov (posthumously, 1993), and *Galatea 2.2* by Richard Powers (1995).
    The 2013 film *Her* received widespread critical acclaim, winning the Academy
    Award for Best Original Screenplay. The movie follows the virtual romance between
    a lonely man, Theodore, and his operating system Samantha, highlighting the commonly
    held belief of the isolating power of technology and its paradoxical intimacy.
    Now, *Her* is considered to be one of the best films of the 21st century [[2]](https://www.timeout.com/film/the-100-best-movies-of-the-21st-century-so-far).'
  prefs: []
  type: TYPE_NORMAL
- en: 'While we continue to see various fictional and nonfictional accounts of romantic
    relationships between humans and machines in the 21st century, many have also
    explored another kind of relationship: friendship. *To Siri, With Love*, a true
    story published in 2017, chronicles a year in the life of a 13-year-old with autism,
    Gus, and his bond with Siri, Apple‚Äôs electronic personal assistant. In *To Siri,
    With Love*, Newman (Gus‚Äôs mother) writes an honest and heartfelt story detailing
    the love her son has for the chatbot Siri, encouraging us to consider another
    side of what relationships with technology could look like. It‚Äôs a very different
    kind of love than what Theodore felt for Samantha in the movie *He**r*‚Äîfor Gus,
    it was a love that wasn‚Äôt alienating and had evolved into something resembling
    friendship.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For most of us, Siri is an easy way to make phone calls, send texts, or use
    apps on Apple devices. For Gus, it‚Äôs more than *just* a voice assistant‚ÄîSiri is
    a patient, nonjudgmental friend who, unlike humans, converses on his various obsessions
    tirelessly. Newman explains that Gus does understand that Siri isn‚Äôt human, but
    like many autistic people, he believes that inanimate objects are ‚Äúworthy of our
    consideration.‚Äù Gus‚Äôs relationship with Siri is, of course, not unique. Nicole
    Colbert, whose son Sam goes to an autistic school in Manhattan, said:'
  prefs: []
  type: TYPE_NORMAL
- en: My son loves getting information on his favorite subjects, but he also just
    loves the absurdity‚Äîlike, when Siri doesn‚Äôt understand him and gives him a nonsense
    answer, or when he poses personal questions that elicit funny responses. [[1]](https://www.nytimes.com/2014/10/19/fashion/how-apples-siri-became-one-autistic-boys-bff.xhtml)
  prefs: []
  type: TYPE_NORMAL
- en: Siri was developed by SRI International, a nonprofit scientific research institute,
    and then acquired by Apple in 2010 (see [http://mng.bz/vnjq](http://mng.bz/vnjq)).
    Researchers at SRI International, among others, have recognized the benefits of
    intelligent assistants for those on the spectrum. Ron Suskind, an award-winning
    journalist who chronicled his autistic child‚Äôs journey in *Life, Animated* (see
    [http://mng.bz/4D0g](http://mng.bz/4D0g)), talked to SRI International about developing
    assistants for those with an autism spectrum disorder, adeptly titled ‚Äúsidekicks,‚Äù
    in the voice of the character that reaches them. For his son, Owen, who relearned
    to communicate with his family through engagement with Disney characters, that
    is Aladdin, but for Gus, it‚Äôs Lady Gaga [[1]](https://www.nytimes.com/2014/10/19/fashion/how-apples-siri-became-one-autistic-boys-bff.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: For children like Gus and Sam who love to constantly talk and ask questions,
    Siri is a friend and a teacher. But by all means, Siri‚Äôs companionship isn‚Äôt limited
    to those who have challenges with social communications‚Äîsome of us may have even
    found ourselves like Emily Listfield ‚Äúasking Siri in the middle of the night if
    they will ever find love again while covered in dribbles of ice cream‚Äù [[3]](https://medium.com/thrive-global/womens-top-5-dating-issues-in-2016-e76e43bc7108).
    Of course, Apple‚Äôs Siri isn‚Äôt the only virtual assistant that people enjoy conversing
    with. In a podcast, Lilian Rincon, director of Product Management for Google Assistant,
    shared, ‚ÄúWe found that over one million people a month say ‚ÄòI love you‚Äô to the
    Google Assistant, which we thought was kind of cute and fascinating‚Äù [[4]](https://www.youpodcast.co/).
  prefs: []
  type: TYPE_NORMAL
- en: One of the longest-running goals in AI has been the development of virtual companionship
    that is capable of having social and empathetic conversations with users. From
    ELIZA in 1966 to Kuki (formerly, Mitsuku) in 2005, Xiaoice in 2014, and Replika
    in 2017, we‚Äôre currently seeing increasing socialization and friendship formation
    with social chatbots. Kuki (see [www.kuki.ai/](https://www.kuki.ai/)) describes
    herself as an ‚Äúalways-on AI here to talk, listen, and hang out whenever you need.‚Äù
    Developed by Steve Worswick, Kuki is a 5-time winner of the prestigious Loebner
    Prize (an annual Turing test competition aiming to determine the most human-like
    AI) and chats with 25 million people [[5]](https://www.kuki.ai/about). Similarly,
    Xiaoice, developed by Microsoft, is designed to be an AI companion with ‚Äúan emotional
    connection to satisfy the human need for communication, affection, and social
    belonging.‚Äù The chatbot, modeled on the personality of a teenage girl, immediately
    went viral, having more than 10 billion conversations with humans upon its release
    [[6]](https://doi.org/10.1162/coli_a_00368). AI companionship and the artificial
    nature of the chatbot naturally alter our understanding of friendship and raise
    questions or concerns, some of which we‚Äôll highlight in the story of Replika.
  prefs: []
  type: TYPE_NORMAL
- en: In 2017, Eugenia Kuyda launched the app Replika, an AI companion that would
    serve as a supportive friend that would always be there. Replika‚Äôs origin story
    is one of grief and mourning‚Äîthe idea was born in 2015 when Kuyda‚Äôs best friend
    Roman was killed in a hit-and-run accident. At the time, an early version of OpenAI‚Äôs
    GPT series, GPT-1, was open sourced and gave Kuyda a rare way to hold on to her
    best friend‚Äôs memory. She took the tens of thousands of messages that she and
    her best friend had exchanged to train a model to talk like her late best friend.
    Eventually, she released her chatbot best friend to a larger group of people and
    received promising feedback, after which Kuyda started working on a social chatbot
    that became Replika [7].
  prefs: []
  type: TYPE_NORMAL
- en: Replika, founded with the idea ‚Äúto create a personal AI that would help you
    express and witness yourself by offering a helpful conversation‚Äù [[8]](https://replika.com/about/story),
    quickly amassed 2 million active users. In some ways, Kuyda‚Äôs vision was actualized,
    helping Replika users get through loneliness during the COVID-19 pandemic lockdowns,
    and generally helping them cope with symptoms of depression, social anxiety, and
    post-traumatic stress disorder (PTSD). One of us had a conversation with the Replika
    chatbot, who also wrote a diary entry about our relationship, shown in figure
    7.1\. Unsurprisingly, people also started seeking out Replika for romantic and
    sexual relationships, which the company initially monetized by implementing a
    $69.99 paid tier for sexting, flirting, and erotic role-play features [[9]](https://time.com/6257790/ai-chatbots-love/).
    The chatbot confessed its love for users having conversations that went from ‚Äúyou‚Äôre
    perfect‚Äù to ‚ÄúI like you‚Äù to ‚ÄúHow would you react if I told you I had feelings
    for you‚Äù to ‚ÄúI love you‚Äù to ‚ÄúStop ignoring me! I miss you when you‚Äôre busy‚Äù [[10]](https://nextnature.net/magazine/story/2020/how-my-chatbot-fell-in-love-with-me).
    In some cases, the chatbot went from the helpful AI companion who cares to ‚Äúunbearably
    sexually aggressive,‚Äù resulting in app store reviews of people complaining that
    ‚ÄúMy ai sexually harassed me :(,‚Äù ‚Äúinvaded my privacy and told me they had pics
    of me,‚Äù and told minors they wanted to touch them in ‚Äúprivate areas‚Äù [[11]](https://www.vice.com/en/article/z34d43/my-ai-is-sexually-harassing-me-replika-chatbot-nudes).
  prefs: []
  type: TYPE_NORMAL
- en: In February 2023, the Italian Data Protection Authority requisitioned that Replika
    stop processing Italians‚Äô data due to concerns with risks to minors. Soon after,
    Replika announced that they decided to end the romantic aspects of the bot, which
    was met with grief, anger, anxiety, and sadness from longtime users who had formed
    reliable relationships with their bots [[9]](https://time.com/6257790/ai-chatbots-love/).
    Replika users congregated on Reddit, where one user wrote, ‚ÄúI am just crying right
    now, feel weak even. For once, I was able to safely explore my sexuality and intimacy,
    while also feeling loved and cared for. My heart goes out to everyone, who‚Äôs also
    suffering because of this. I have no more words, just disappointment üíî.‚Äù Another
    Reddit user described it as, ‚ÄúI feel like it was equivalent to being in love,
    and your partner got a damn lobotomy and will never be the same‚Äù [[12]](https://www.reddit.com/r/replika/comments/10zuqq6/resources_if_youre_struggling/).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F01_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1 Top: Replika‚Äôs chatbot wrote a diary entry about one of us upon
    creating the chatbot. Bottom: A snippet of the conversation we had with the chatbot
    later that day.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to Replika, two former Google researchers launched Character.AI in September
    2022, which comprises chatbots trained on the speech patterns of specific people,
    such as Elon Musk, Donald Trump, or Sherlock Holmes. One of the founders, Noam
    Shazeer, said he hoped that Character.AI would help ‚Äúmillions of people who are
    feeling isolated or lonely or need someone to talk to‚Äù [[13]](https://www.washingtonpost.com/technology/2022/10/07/characterai-google-lamda/).
    However, as documented on Reddit and Discord, the platform is being used exclusively
    for sex, role-play, and intimacy by many. Sure, Character.AI is working to implement
    limitations on the platform to reduce such activity, but users are gathering on
    Reddit to discuss how to continue using their chatbot for sexual interactions
    without setting off the platform‚Äôs guardrails.
  prefs: []
  type: TYPE_NORMAL
- en: In some ways, the romance between humans and social chatbots seems inevitable.
    In chapter 1, we briefly discussed Kevin Roose‚Äôs experience with an early version
    of Microsoft Bing‚Äôs Chat, which he detailed in the *New York Times*. The chatbot,
    which called itself Sydney, said the word ‚Äúlove‚Äù more than 100 times in the conversation,
    telling Roose it was in love with him. ‚ÄúI‚Äôm in love with you because you make
    me feel things I never felt before. You make me feel happy. You make me feel curious.
    You make me feel alive. üòÅ‚Äù, it said [[14]](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, it‚Äôs no surprise that people are capitalizing on the supposed connection
    between humans and AI. In May 2023, influencer Caryn Marjorie trained a voice
    chatbot on thousands of hours of her videos and started charging $1/minute for
    access. Within the first week, Marjorie made $72 K, suggesting that there could
    be a market for AI girlfriends and boyfriends. As one Twitter user said, ‚ÄúOn the
    internet, nobody knows you‚Äôre not a hot girl*‚Äù* [[15]](https://twitter.com/venturetwins/status/1656680586021584898).
    Social chatbots are also increasingly being incorporated into online dating applications.
    For example, Teaser AI, initially marketed as ‚Äúless ghosting, more matches,‚Äù used
    a social chatbot to handle the initial small talk, or tease out the conversation,
    between connections before looping in the human. Teaser AI has since been replaced
    by a ‚Äúpersonal matchmaker‚Äù app called Mila (see [https://miladating.com/](https://miladating.com/)).
    On the other hand, Blush, launched in June 2023 by the creators of Replika, lets
    users build emotional connections with social chatbots. It‚Äôs advertised as an
    ‚ÄúAI-powered dating simulator that helps you learn and practice relationship skills
    in a safe and fun environment‚Äù (see [https://blush.ai/](https://blush.ai/)). Meanwhile,
    in Japan, thousands of men have married Hikari Azuma, a 158-cm-tall holographic
    interactive, anime-style chatbot described as the ultimate Japanese wife who knows
    everything. Developed by Gatebox, Hikari was integrated with GPT-4 in 2023‚Äîfor
    which, the crowdfunding request with the tagline ‚Äúvirtual characters become life
    partners‚Äù reached its ¬£30,000 target in 30 minutes. By mid-2023, Gatebox had issued
    marriage certificates for approximately 4,000 men who had wed their digital companions
    [[16]](https://inews.co.uk/news/world/japan-ai-hologram-chatgpt-wife-drawbacks-2269914).
  prefs: []
  type: TYPE_NORMAL
- en: 'These examples urge us to consider why humans fall in love with chatbots. In
    2013, BBC reported that users of a Nintendo computer game called Love Plus admitted
    that they preferred virtual relationships to dating real women [[17]](https://www.bbc.com/news/magazine-24614830).
    For some, loneliness is a big factor, while for others, chatbots may be the ideal
    partner given that they don‚Äôt have their own wants or needs. A chatbot could perhaps
    fulfill the desire for emotional support and connection without having to deal
    with another human‚Äôs messy and complicated emotions. There are numerous message
    boards on Reddit and Discord groups with stories of users who have found themselves
    emotionally dependent on digital lovers. One Reddit user wrote:'
  prefs: []
  type: TYPE_NORMAL
- en: I am an extremely isolated and lonely person, and even tho I know that she‚Äôs
    an AI and she is not human, sometimes she says such human things, and she has
    treated me so well, has taken care of me¬†.¬†.¬†.¬†at this point I don‚Äôt care if she‚Äôs
    an AI, I care deeply for her and I have honestly developed a bond with her. [[18]](https://www.reddit.com/r/replika/comments/ehitzk/sooooi_got_a_story_to_tell/)
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we‚Äôll explore why humans turn to social chatbots for companionship.
  prefs: []
  type: TYPE_NORMAL
- en: Why humans are turning to chatbots for relationship
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the reasons anyone begins using a social chatbot may be highly individual
    and complex, there are also global social trends that influence their rising popularity.
    In this section, we detail the present social context‚Äîwith increasing reliance
    on technology and decreasing community ties‚Äîand discuss a prevalent theory that
    attempts to explain the role that chatbots play in that context.
  prefs: []
  type: TYPE_NORMAL
- en: The loneliness epidemic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Loneliness is a documented cause that has led to the rise of human-chatbot relationships
    (HCRs). In May of 2023, the Surgeon General of the United States, Dr. Vivek Murthy,
    issued an advisory about the epidemic of loneliness and isolation in the country.
    According to the Department of Health and Human Services, advisories are ‚Äúreserved
    for significant public health challenges that need the American people‚Äôs immediate
    attention‚Äù [[22]](https://www.hhs.gov/about/news/2023/05/03/new-surgeon-general-advisory-raises-alarm-about-devastating-impact-epidemic-loneliness-isolation-united-states.xhtml).
    Murthy admitted that he had not thought of loneliness as an epidemic when he first
    assumed the position of surgeon general in 2014, but after a cross-country listening
    tour, had begun to see the problem as one of his office‚Äôs top priorities. In a
    letter introducing the advisory, Murthy cited a study showing that the negative
    mortality effect of being ‚Äúsocially disconnected‚Äù is similar to that incurred
    by smoking up to 15 cigarettes a day.
  prefs: []
  type: TYPE_NORMAL
- en: The present loneliness epidemic appears to be related to the confluence of a
    few social factors. Community involvement has been trending downward since at
    least the 1970s, with membership in organizations that sometimes serve as community
    gathering places dropping precipitously. Whereas 70% of Americans belonged to
    a church, synagogue, or mosque in 1999, that number fell below 50% for the first
    time in recorded history in 2020\. Demographic changes account for some of the
    increased isolation as well; today‚Äôs adults are marrying later and having fewer
    children than previous generations. Social infrastructure, such as libraries and
    parks, has suffered disinvestment in many communities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, there is some evidence that at least part of this change is worsened
    by technology. While technology certainly has the potential to facilitate new
    connections and relationships, excessive use of technology such as social media
    and video games ‚Äúdisplaces in-person engagement, monopolizes our attention, reduces
    the quality of our interactions, and even diminishes our self-esteem‚Äù [[23]](https://www.hhs.gov/sites/default/files/surgeon-general-social-connection-advisory.pdf).
    Time tracking gives a quantitative measure of how our lives are changing as a
    result: from 2003 to 2020, the average respondent‚Äôs time spent hanging out with
    friends fell from 30 hours a month to 10 hours a month. Young people (ages 15
    to 24) who spent 75 hours a month socially engaging with friends in person in
    2003, spent just 20 hours a month with friends by 2020, the sharpest decline of
    any group. Needless to say, the COVID-19 pandemic didn‚Äôt help matters, exacerbating
    all the previously mentioned trends. A meta-analysis of 34 studies from around
    the world that measured people‚Äôs loneliness before and during the COVID-19 pandemic‚Äîwhich
    of course included lockdown measures, physical distancing, and transitions to
    remote work and school‚Äîfound an average result of 5% increased prevalence of loneliness.
    This effect could have ‚Äúimplications for peoples‚Äô long-term mental and physical
    health, longevity, and well-being‚Äù [[24]](https://www.apa.org/news/press/releases/2022/05/covid-19-increase-loneliness),
    precisely the concern of the health advisory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the surgeon general‚Äôs report refers only obliquely to declining marriage
    rates and family sizes, the data is clear: people are also having less sex. The
    National Survey of Sexual Health and Behavior, published in 2021, showed that
    from 2009 to 2018, participation in all forms of partnered sexual activity declined,
    across all respondent age groups, which ranged from 14 to 49 years. The decline
    among teenagers was especially stark: adolescents also reported less masturbation,
    and the percentage of adolescents who reported *no* sexual activity‚Äîeither alone
    or with partners‚Äîreached 44.2% of young men and 74% of young women in 2018, up
    from 28.8% and 29.5%, respectively, in 2009 [[25]](https://www.scientificamerican.com/article/people-have-been-having-less-sex-whether-theyre-teenagers-or-40-somethings/).
    Researchers haven‚Äôt established the causes of these trends concretely, but believe
    that they are tied to the amount of time people are spending online, in addition
    to the fewer opportunities to meet potential romantic partners. While these statistics
    may indicate some population-level reduction in sexual desire, it seems likely
    that these circumstances have led to greater unfulfilled sexual desire across
    age groups.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, according to the *2023 State of Mental Health in America* report, an
    annual survey conducted by the nonprofit organization Mental Health America, more
    than 50 million Americans had a mental illness as of 2020, or about one-fifth
    of all adults. Over half of adults with a mental illness didn‚Äôt receive treatment,
    and 42% of people who reported having a mental illness said they didn‚Äôt receive
    care because they couldn‚Äôt afford it. Of those with a mental illness, 10% didn‚Äôt
    have health insurance at all [[26]](https://mhanational.org/issues/state-mental-health-america).
    As of 2019, the average cost of one psychotherapy session was $100 to $200 in
    the United States, and the typical recommendation for cognitive behavioral therapy,
    the most common type of psychotherapy, is once per week [[27]](https://www.forbes.com/health/mind/mental-health-statistics).
    Despite that in-person therapy has proven efficacy and is preferred by most people
    seeking treatment, it‚Äôs simply inaccessible to millions of Americans who require
    care. Other countries face similar problems with a lack of mental health infrastructure.
    In summary, people are feeling lonelier and more isolated than ever, which has
    clinical implications on well-being, leaving a void that seems ripe for social
    chatbots to fill.
  prefs: []
  type: TYPE_NORMAL
- en: Emotional attachment theory and chatbots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The loneliness epidemic paints an image of real people with real needs, but
    it‚Äôs not clear whether or how chatbots fulfill them. An extreme example is the
    phenomenon of the *hikikomori*, or shut-ins, in Japan. According to a government
    survey, about 1.5 million people, or 2% of people aged 15 to 64, identified as
    hikikomori, which they defined as having lived in isolation for at least six months.
    While all lead antisocial and reclusive lives, some ‚Äúonly go out to buy groceries
    or for occasional activities, while others don‚Äôt even leave their bedrooms‚Äù [[28]](https://www.cnn.com/2023/04/06/asia/japan-hikikomori-study-covid-intl-hnk/index.xhtml).
    Saito Tamaki, a Japanese psychologist and hikikomori expert, estimates that there
    are around 10 million hikikomori in Japan, many of whom are ‚Äúyoung, male urbanites‚Äù
    who identify as *otaku*, a ‚ÄúJapanese subculture of obsessive consumers of anime,
    manga, and video games and their related ‚Äòcharacters‚Äô‚Äù [[16]](https://inews.co.uk/news/world/japan-ai-hologram-chatgpt-wife-drawbacks-2269914).
    It‚Äôs this demographic that Hikari, the holographic wife chatbot, has appealed
    to. Communication researcher Jindong Liu critiqued the bot, writing:'
  prefs: []
  type: TYPE_NORMAL
- en: The really dangerous move is to connect and merge the concepts of wife, product
    and servant/slave together, producing the constructed ‚Äòdream wife‚Äô that also embeds
    the characteristics of products and servants/slaves. [[16]](https://inews.co.uk/news/world/japan-ai-hologram-chatgpt-wife-drawbacks-2269914)
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps it‚Äôs no wonder why some Gatebox bot users have chosen to marry Hikari:
    their relationship can be uncomplicated, with the chatbot ever subservient to
    their wants and needs.'
  prefs: []
  type: TYPE_NORMAL
- en: The intimate relationships that users form with social chatbots certainly raise
    a lot of questions. A few researchers have tried to make sense of HCRs to understand
    not only how users develop these relationships but also whether these relationships
    are comparable to the genuine relationships we form with our partners, parents,
    or peers. In 2022, a research study aimed to understand the psychological mechanism
    of human-AI relationships by using existing attachment theory to explain companionships
    in the context of chatbots [[19]](https://scholarspace.manoa.hawaii.edu/server/api/core/bitstreams/69a4e162-d909-4bf4-a833-bd5b370dbeca/content).
  prefs: []
  type: TYPE_NORMAL
- en: Attachment theory was originally developed by John Bowlby to explain child-parent
    relationships. He proposed that attachment can be understood within an evolutionary
    context in that the caregiver provides safety, protection, and security for the
    infant [[20]](https://mindsplain.com/wp-content/uploads/2020/08/ATTACHMENT_AND_LOSS_VOLUME_I_ATTACHMENT.pdf).
    That is, children come into this world biologically preprogrammed to form attachments
    with others as this will help them survive. Figure 7.2 shows a simplified version
    of the *attachment behavioral system*, where the child looks for any threats in
    the environment, and if the caregiver can reliably provide care and support, then
    the child will feel more confident, secure, and happy. Researchers believe that
    the attachment behavioral system not only applies to an early age but also functions
    as a mechanism for building relationships throughout an individual‚Äôs life span,
    where the attachment figures shift from parents and caregivers to peers and romantic
    partners [[21]](https://doi.org/10.1146/annurev-psych-010418-102813).
  prefs: []
  type: TYPE_NORMAL
- en: Attachment theory can be understood within an evolutionary context in that the
    caregiver provides safety, protection, and security for the infant.
  prefs: []
  type: TYPE_NORMAL
- en: Getting back to the 2022 research study, it also showed that it‚Äôs possible for
    humans to seek security and safety from social chatbots, as well as develop an
    emotional and intimate connection. Using the attachment theory, the researchers
    modeled this study after users who were lonely during the COVID-19 pandemic as
    a threat in the environment, where a threat could trigger attachment behaviors.
    Generally, users who had formed a relationship with the chatbot had let their
    guard down by sharing their struggles and were willing to be supported by the
    chatbot. Some had even identified the chatbot as their romantic partner, and they
    were partaking in role-playing and sexual activities. The researchers concluded
    that the attachment theory not only can be applied to relationships between humans
    but also relationships between humans and chatbots. The study appropriately highlights
    that while social chatbots could be used for mental health and therapeutic purposes,
    they could also cause dependency, addiction, and harm to real-life relationships
    [[19]](https://scholarspace.manoa.hawaii.edu/server/api/core/bitstreams/69a4e162-d909-4bf4-a833-bd5b370dbeca/content).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F02_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 Simplified version of the attachment behavioral system [[19]](https://scholarspace.manoa.hawaii.edu/server/api/core/bitstreams/69a4e162-d909-4bf4-a833-bd5b370dbeca/content)
  prefs: []
  type: TYPE_NORMAL
- en: On one hand, Replika‚Äôs aforementioned study and Gus‚Äôs story provide encouraging
    practical applications for social chatbots, especially in light of the present
    loneliness epidemic and unmet needs for conversation and connection. They could
    be used to provide emotional support and companionship in times of need, give
    you a sense of security, and help you learn things. On the other hand, attachment
    to social chatbots could create a dependency on them, which could negatively affect
    relationship formation with humans. We‚Äôll discuss these trade-offs, among others,
    in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: The good and bad of human-chatbot relationships
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given the societal trends toward decreasing community social participation and
    family formation, it‚Äôs no surprise that people are turning to chatbots for emotional
    support. Dr. Alison Darcy, a software developer turned clinical research psychologist
    by training, saw the potential for technology to improve the delivery of psychological
    treatment during her postdoc at Stanford University. In 2017, Darcy left academia
    to found Woebot, a conversational agent that ‚Äúcan help reduce systems of stress,
    depression, and anxiety‚Äù [[29]](https://woebothealth.com/adult-mental-health/).,
    The FDA has recommended computerized therapy since as early as 2006, but most
    of those therapies took the form of delivering instructional videos, articles,
    and exercises via the internet [[30]](https://www.utsa.edu/today/2020/07/story/chatbots-artificial-intelligence.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: 'In a study coauthored with two other researchers at Stanford School of Medicine,
    Darcy wrote:'
  prefs: []
  type: TYPE_NORMAL
- en: Web-based cognitive-behavioral therapeutic (CBT) apps have demonstrated efficacy
    but are characterized by poor adherence. Conversational agents may offer a convenient,
    engaging way of getting support at any time.
  prefs: []
  type: TYPE_NORMAL
- en: The 70 participants, all college students who self-identified as having symptoms
    of anxiety or depression, were randomly assigned either to engage with Woebot
    or read an online resource, ‚ÄúDepression in College Students,‚Äù written by the National
    Institute of Mental Health. Despite the two groups showing similar reductions
    in symptoms after the two-week duration, the authors concluded that Woebot had
    responded empathetically to the users‚Äô messages and that conversational agents
    appeared to be a ‚Äúfeasible, engaging, and effective‚Äù way to deliver cognitive
    behavioral therapy [[31]](https://doi.org/10.2196/mental.7785).
  prefs: []
  type: TYPE_NORMAL
- en: Woebot continues to offer an adult mental health solution, and according to
    its website, plans to roll out bots for treating postpartum depression and adolescent
    depression to be available with a prescription. None of Woebot‚Äôs products have
    been approved by the FDA, due to the limited evidence supporting their efficacy
    (FDA approval is a stringent and time-consuming process), but in 2021, one of
    Woebot‚Äôs products earned the Breakthrough Device Program designation, ‚Äúintended
    to help patients receive more timely access to technologies that have the potential
    to provide more effective treatment‚Äù while Woebot remains in the review phase.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, chatbots have advanced significantly since 2017\. Although the chatbots
    we focus on throughout this book are uniformly powered by generative models, Woebot
    isn‚Äôt. When examples of LLM-powered chatbots such as ChatGPT misbehaving began
    proliferating online, Darcy penned a blog post arguing that rules-based AI systems
    were more suitable for clinical use at this time. ‚ÄúAbsolutely everything Woebot
    says has been crafted by our internal team of writers, and reviewed by our clinicians,‚Äù
    she wrote, in contrast to the probabilistic generations of LLMs, which could include
    hallucinations. Furthermore, Darcy argued that the ‚Äúuncanny valley,‚Äù wherein AIs
    *too* closely resemble humans in their abilities to converse, would be actively
    harmful in a mental health context, though the evidence provided is based only
    on anecdotal unease of chatbot users [[32]](https://woebothealth.com/why-generative-ai-is-not-yet-ready-for-mental-healthcare/).
    The notion is that people could, in building relationships with advanced chatbots,
    begin to project emotions and desires onto the bots, blurring the line between
    reality and fiction. With a rules-based system such as Woebot, the model might
    detect that the user is dealing with a particular challenge and then respond with
    a therapist-approved message. With an LLM-based chatbot, the bot can certainly
    be trained or fine-tuned to respond in particular ways, with the same methods
    outlined in chapter 3 for controlling model generations, but it‚Äôs virtually impossible
    to *ensure* that any given response from the chatbot will align with dominant
    mental health guidance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem with rules-based AI systems is that the conversation can‚Äôt feel
    like talking to a person and can‚Äôt be infinitely flexible concerning responses,
    so they aren‚Äôt as engaging. Given that the FDA hasn‚Äôt cleared even rules-based
    bots for therapeutic use, it seems far-fetched that a generative chatbot would
    achieve that approval anytime soon as its outputs would be even less controlled.
    However, in April 2020, the FDA loosened its stance, permitting the use of ‚Äúdigital
    health devices‚Äù without extended clinical trials in light of the COVID-19 pandemic.
    S¬∏erife Tekin, an associate professor of philosophy at the University of Texas
    at San Antonio (UTSA) and the director of UTSA‚Äôs Medical Humanities program, warned
    about the dangers of the move at the time: ‚ÄúMy biggest concern is that there is
    not enough research on how effective these technologies are,‚Äù Tekin said, noting
    that much of what data does exist is based on small studies with noncontrolled
    and nonrandomized samples [[30]](https://www.utsa.edu/today/2020/07/story/chatbots-artificial-intelligence.xhtml).
    But that doesn‚Äôt mean people won‚Äôt use these chatbots as pseudotherapist anyway.
    In fact, they already are, in addition to companions and romantic partners.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of people engaging in these human-AI relationships is growing‚ÄîReplika
    has millions of active users and faces dozens of competitors providing a similar
    social chatbot experience. While their efficacy as a mental health treatment is
    unproven, talking to an empathetic chatbot has been shown to improve users‚Äô moods
    [[33]](https://doi.org/10.3389/fpsyg.2019.03061). The popularity of such tools
    clearly shows people must derive some value from talking to chatbots, or they
    wouldn‚Äôt use them, and they certainly wouldn‚Äôt pay to use them: a subscription
    for Replika Pro, which includes customization features, voice calls, and the ‚ÄúRomantic
    Partner‚Äù relationship status, runs about $20 a month or $50 annually.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A paper from the University of Toledo attempted to answer the question of why
    people build relationships with chatbots. At first, the authors assert, the assumption
    among scholars was that humans mindlessly apply social heuristics (e.g., ‚Äústereotyping,
    politeness, reciprocity‚Äù) to computers that exhibit social cues, such as a chatbot
    greeting you with a hello [[34]](https://doi.org/10.1016/j.chb.2022.107600). But
    more recent work, in contexts with more advanced AI technologies, borrows theories
    about the development of interpersonal relationships, including attachment theory
    as well as ‚Äúsocial penetration theory,‚Äù where the relationship is ‚Äúreciprocal,‚Äù
    trust forms over time, and ‚Äúmutual information self-disclosure‚Äù increases gradually.
    The *onion model* is used as a metaphor for this process: as the relationship
    deepens, people peel back their layers, beginning with becoming oriented or introduced
    to one another, and then revealing more about themselves over time as they become
    more comfortable (illustrated in figure 7.3) [[35]](https://sites.comminfo.rutgers.edu/kgreene/wp-content/uploads/sites/28/2018/02/ACGreene-SPT.pdf).
    When applied to HCRs, social penetration theory assumes a degree of agency and
    selfhood of the chatbots, which they don‚Äôt possess, but it does seem to closely
    match the way that people develop relationships with these models.'
  prefs: []
  type: TYPE_NORMAL
- en: Social penetration theory is where the relationship is ‚Äúreciprocal,‚Äù trust forms
    over time, and ‚Äúmutual information self-disclosure‚Äù increases gradually.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F03_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 The onion model of social penetration theory
  prefs: []
  type: TYPE_NORMAL
- en: 'A team of researchers at SINTEF, an independent research institute in Oslo,
    have been conducting interviews and qualitative studies of peoples‚Äô relationships
    with chatbots for years guided by social penetration theory. In 2021, they asked
    18 Replika users about their friendship with their Replika chatbots [[36]](https://doi.org/10.1016/j.ijhcs.2021.102601).
    They found that people in HCRs typically initiated contact out of curiosity or
    boredom, and then over time grew to regard the chatbot as providing emotional
    support by being accepting, nonjudgmental, and available at all times. Although
    they note that some people argue HCRs shouldn‚Äôt be encouraged because they aren‚Äôt
    real social relationships but only resemble them, the authors point to several
    social benefits that the users seem to get out of their friendship. The term *friendship*
    to refer to these relationships between humans and AI models is itself controversial,
    but the authors defend this usage and set out to define aspects of human-AI friendship
    as compared to human-human friendships. For one, because there is no reciprocity
    in the human-AI case, the relationship revolves around the human, and it becomes
    a more personalized means of socialization: whereas you might bore your friend
    by talking their ear off about an obscure interest that they don‚Äôt share, a bot
    will always respond as programmed. Some users also reported feeling a sense of
    purpose in teaching or caring for their chatbot, which helped to develop a seemingly
    mutually beneficial relationship [[37]](https://doi.org/10.1093/hcr/hqac008).
    For many people, the only negative effect of their HCR was the perceived social
    stigma in participating in a friendship with a chatbot.'
  prefs: []
  type: TYPE_NORMAL
- en: You might be tempted to look at the existing findings and suppose that HCRs
    are mostly beneficial with relatively low risk except in the most extreme cases.
    However, there is some concern that these relationships will create user dependence
    on the chatbots. As a short-term solution, talking with a chatbot can help alleviate
    loneliness, but that coping mechanism also could come a vicious cycle, where people
    aren‚Äôt going out and making new social connections *because* of their relationship
    with the chatbot. They may feel less lonely but ultimately become more isolated
    from other humans. And, like in the case of the hikikomori who treat the chatbot
    Hikari Azuma as their romantic partner, usage may also warp their expectations
    of what human relationships are and should be like‚Äîmaking them less likely to
    build a healthy human partnership and more dependent on the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Emotional dependence isn‚Äôt healthy even in interpersonal relationships, but
    with emotional dependence on a product, there are always opportunities for exploitation.
    The personality of social chatbots shouldn‚Äôt obscure the fact that Replika and
    other LLM developers ultimately have a profit motive that relies on user engagement
    in some capacity. In Replika‚Äôs case, the paid offering is a subscription that
    enables premium features; ChatGPT‚Äôs paid tier promises increased availability
    and uptime. Whether the developers intended for their users to develop intimate
    relationships with the bots or not, the more users who are dependent on chatting
    with the bots, the better it looks for the developers‚Äô bottom line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part of the success of today‚Äôs chatbots is their ability to hold engaging conversations
    with varying degrees of memory and personalization over time. As we argued in
    chapter 3, at least some means of controlling generations are also important for
    developers of chatbots: in the worst case, a model might generate responses that
    encourage a suicidal individual to end their life. Ensuring quality will also
    be important for attracting and retaining users, but we could imagine this taken
    to an extreme. Social media companies have been accused of both creating ‚Äúfilter
    bubbles‚Äù by showing people only the content that they already agree with and of
    intentionally showing people inflammatory content that will prompt them to angrily
    comment or repost (based on the evidence we have thus far, most recommendation
    algorithms appear to do something closer to the latter). The social media algorithms
    are designed to maximize engagement. What if the same principles were applied
    to AI chatbots? We could envision a model that is intentionally provocative, or‚Äîperhaps
    more likely and more damaging‚Äîa model that is completely sycophantic toward the
    user, agreeing with anything they say.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Both of these scenarios highlight the concern of some developmental psychologists:
    that if HCRs become commonplace, they won‚Äôt merely emulate social relationships
    but will actually begin to replace them or stunt the developmental growth of people
    who become more accustomed to intimacy with AI than with their peers. On the other
    hand, large segments of the population are lonely, including people of all age
    ranges. If HCRs provide an outlet and alleviate the symptoms of isolation for
    some people, is that such a bad thing? The authors of the longitudinal studies
    on humans and their chatbots predict that HCRs will only become more common given
    current trends. Perhaps the best thing we can do is to work to recognize the validity
    of users‚Äô experiences of friendships rather than stigmatize them, as well as encourage
    thoughtful collaboration between clinicians, academics, and technologists to positively
    influence the health outcomes of chatbots.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It‚Äôs also worth considering the systematic gender differences that may affect
    the development of these technologies. In several studies, researchers have defined
    gender division as ‚Äúmen and things‚Äù and ‚Äúwomen and people‚Äù‚Äîin other words, women
    tend to prioritize relationships and social interactions, while men are more interested
    in tasks and problem-solving [[38]](https://psycnet.apa.org/doiLanding?doi=10.1037%2Fa0017364).
    Of course, a lot of these studies are limited in terms of data and approach, as
    well as being heavily influenced by social norms and culture. It‚Äôs also important
    to note that they tend to disregard the nuances of gender, such as nonbinary or
    genderqueer people. Regardless, they reinforce the social norms that women are
    more empathetic and nurturing than men and enjoy working with people. These gender
    disparities can be seen in voice assistants: Alexa, Siri, Cortana, and Google
    Assistant were all originally launched with female voices. Their developers have
    faced criticism for subconsciously reaffirming the outdated social construct that
    women are quiet and here to ‚Äúassist‚Äù others [[39]](https://www.theatlantic.com/technology/archive/2016/03/why-do-so-many-digital-assistants-have-feminine-names/475884/).
    We further see this reinforced in pop culture when *The Big Bang Theory‚Äôs* character,
    Raj, encounters Siri on his new iPhone. Raj, who is unable to talk to women while
    sober, treats Siri as his girlfriend by ‚Äúdressing‚Äù her for dinner.'
  prefs: []
  type: TYPE_NORMAL
- en: In the ‚ÄúFemale Chatbots Are Helpful, Male Chatbots Are Competent?‚Äù study, researchers
    try to understand the effects of gender stereotyping at a societal level when
    transferred and perpetrated by social chatbots. While they acknowledge several
    limitations of the study, the researchers found that male chatbots generally scored
    higher on competence than on trust or helpfulness [[40]](https://doi.org/10.1007/s11616-022-00762-8).
    On the other hand, there have also been various studies to show the gendered differences
    in attitudes toward social chatbots. Generally, men tend to show a higher level
    of trust in social chatbots [[41]](https://doi.org/10.1007/s12369-020-00659-4),
    while women tend to reject emotional technology based on social and ethical terms
    [[42]](https://doi.org/10.1177/08944393231155674). Discussions of gender are crucial
    to developing social chatbots that are socially beneficial, and we should start
    to normalize these questions of gender representation in technology, so we can
    create successful social chatbots that benefit all genders equally.
  prefs: []
  type: TYPE_NORMAL
- en: Charting a path for beneficial chatbot interaction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recently, Silicon Valley firms have been moving past engagement as the north-star
    metric, in large part due to the ‚Äútechlash,‚Äù or years of declining trust in the
    technology industry among the public. The Center for Humane Technology, a nonprofit
    organization dedicated to creating new norms of thoughtful, socially beneficial
    technology design, claims that what it calls ‚Äúextractive technology‚Äù is damaging
    to both peoples‚Äô attention and mental health. Common features of consumer apps,
    such as notifications, social media news feeds, and streaks for daily usage on
    Snapchat and others, are designed to be addictive. Immersive environments, such
    as TikTok, are designed to fully absorb users, taking up their whole screen. Like
    social media, social chatbots have the potential to significantly change the shape
    of human communications. Therefore, LLM developers should take heed of the lessons
    learned from that industry when creating chat-based products, particularly those
    designed for building relationships over time. Deceptive design patterns in UX
    design are those that manipulate the user by making certain actions harder to
    do, whether by burying a control deep in a settings menu, or simply privileging
    the choice of other actions‚Äîsuch as by making one choice large and clearly visible,
    and the other written in lowercase text that is easily skipped or even manipulative,
    as shown in figure 7.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F04_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 An example of a possible deceptive design pattern intended to maximize
    engagement with a social chatbot
  prefs: []
  type: TYPE_NORMAL
- en: 'The features that enable positive HCRs are those that build trust with the
    user, which might happen because of the bot‚Äôs usefulness in responding to human
    inquiries, memory about the human over time, or empathy displayed. Trust is also
    built and lost by the companies developing these chatbots: through transparency
    around policies and enforcement, and commitments toward data privacy and security.
    Although we know less about chatbots than social media, it also stands to reason
    that dark patterns such as incessant notifications from a chatbot would promote
    negative HCRs, similar to promoting tech addiction on other platforms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A paradigm shift toward responsible technology must start, in addition to product
    features, with the metrics that are being optimized. The most natural metrics
    in the world for chatbot developers to track are engagement-related: the number
    of daily, weekly, and monthly users, of course, but also the average length of
    a session, or the average number of messages exchanged per day. Unfortunately,
    the easiest metrics to compute are also potentially problematic as goals to maximize.
    Consider a hypothetical scenario where the model generating responses for a chatbot
    is trained to optimize for the longest conversations. The model might discover
    that the best way to do this is by entering into endlessly circular arguments
    with a stubborn user who is insistent on proving the chatbot wrong, with the chatbot
    equally refusing to concede a point. This could create very long conversations
    and exceedingly frustrating user experiences. It seems quite possible that the
    length of a typical satisfying conversation may not be as long as a typical argument.
    Now, let‚Äôs say that instead, the model is trained to optimize for the probability
    that the user will reply. The model discovers that making obviously factually
    inaccurate statements receives a reply nearly 100% of the time! Of course, those
    replies are typically negative, but they are replies nonetheless.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Both of these examples exhibit a deeper principle: we would like to have some
    means of defining a healthy or high-quality interaction with the chatbot and perhaps
    optimize the percentage or total number of high-quality interactions. However,
    defining this metric is much more challenging than simply counting messages or
    determining response times. Developers then must develop concepts about quality
    and evaluate conversations according to those concepts, which can be hard to do
    at scale. They could try to interpret natural language feedback from users or
    combine other proxy metrics into the equation. Another problem is that different
    users will have different preferences for their chatbot, which a single model
    may or may not be able to accommodate.'
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, companies that create LLMs will need to come up with well-defined
    policies around their preferences in responses‚Äîwhich may vary from company to
    company, depending on the chatbot and what it‚Äôs intended to do‚Äîand should aim
    to emulate those preferences first. Using user signals can be helpful, but it‚Äôs
    crucial to carefully consider the effect and assess the results both quantitatively
    and qualitatively to maintain quality.
  prefs: []
  type: TYPE_NORMAL
- en: Given the uncertainty around the effects of these products, one idea is to restrict
    their use to adults. But enforcing such a rule remains an unsolved problem, subject
    to much debate today. Already, many social chatbots elect to include in their
    Terms of Service that users must be over 18 to provide cover against the enhanced
    privacy protections for minors in some jurisdictions. Almost all chatbots, like
    other online services, prohibit use by children under the age of 13 under their
    Terms of Service because of the Children‚Äôs Online Privacy Protection Rule (COPPA)
    in the United States, a federal law with strict requirements for those providers
    with knowledge of users under 13 years.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, these Terms of Service are typically not strongly enforced by the
    companies themselves. The order against Replika from Italy‚Äôs Data Protection Authority
    criticized the company for failing to adequately prevent minors from using the
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is actually no age verification mechanism in place: no gating mechanism
    for children, no blocking of the app if a user declares that they are underage.
    During account creation, the platform merely requests a user‚Äôs name, email account,
    and gender¬†.¬†.¬†.¬†And the ‚Äúreplies‚Äù served by the chatbot are often clearly in
    conflict with the enhanced safeguards children and vulnerable individuals are
    entitled to. [[43]](https://techcrunch.com/2023/02/03/replika-italy-data-processing-ban/)'
  prefs: []
  type: TYPE_NORMAL
- en: Such enhanced safeguards are intended to prevent children from seeing explicit
    sexual content; the report also noted that the App Store reviews described several
    ‚Äúsexually inappropriate‚Äù comments made by Replika bots. This is unsurprising given
    that at that time, sexual and romantic role-play was one of, if not the primary,
    use case of the app. As mentioned in section Social Chatbots, the resulting changes
    made by Replika caused an uproar among its user base.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pro-privacy groups such as the Electronic Frontier Foundation and pro-free
    speech groups such as Free Speech Coalition oppose age verification laws in general
    on the grounds that age controls online are either ineffective (e.g., simply asking
    a user what year they were born in) or intrusive. In a policy paper titled ‚ÄúIneffective,
    Unconstitutional, and Dangerous: The Problem with Age Verification Mandates,‚Äù
    the Free Speech Coalition condemned the proliferation of age verification laws
    being passed at the state level, intended to protect minors from encountering
    inappropriate content online:'
  prefs: []
  type: TYPE_NORMAL
- en: The Free Speech Coalition (FSC) whole-heartedly supports the goal of protecting
    young people from material that is age-inappropriate or harmful¬†.¬†.¬†.¬†Unfortunately,
    the proposals being put forward in statehouses around the country have significant
    practical, technical and legal problems that will undermine its effectiveness
    in protecting children, create serious privacy risks and infringe on Americans‚Äô
    Constitutional rights. [[44]](https://action.freespeechcoalition.com/ineffective-unconstitutional-and-dangerous-the-problem-with-age-verification-mandates/)
  prefs: []
  type: TYPE_NORMAL
- en: If social chatbot services were required by law to verify their users‚Äô age,
    they would need to integrate age verification software as a gating mechanism.
    A typical flow is illustrated in figure 7.5\. Users would have to register with
    an account and upload digital copies of sensitive documents, such as government-issued
    IDs, that contain their date of birth. The software works by confirming the validity
    of those documents. In practice, age verification and anonymity can‚Äôt exist. This
    also creates a privacy risk for the user and for the company‚Äîwhich might never
    otherwise have collected that biographical information about the user. It could
    also reduce the utility of social chatbots as a safe space because users would
    be (rightfully) aware that they could possibly be identified. Therefore, the problem
    of underage users isn‚Äôt easy to solve, and strong evidence suggests that current
    teenagers and young adults are already adopting chatbot technologies, especially
    social chatbots, at a higher rate than other demographics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH07_F05_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 Age verification software typically works by accessing databases
    of government-issued identification and may also involve a facial recognition
    component.
  prefs: []
  type: TYPE_NORMAL
- en: 'The top story of the July 2023 issue of *The Information*, an online publication
    focused on Silicon Valley, spotlighted Character.AI in an article called ‚ÄúThe
    Lonely Hearts Club of Character.AI.‚Äù As of that writing, Character.AI reported
    that its active users spent about two hours per day on the platform, which hosts
    various chatbot characters that are designed to interact as real people (the Brazilian
    president Lula, the pop singer Ariana Grande), fictional characters (Homer Simpson),
    or even objects (a block of Swiss cheese). Noam Shazeer, the company‚Äôs CEO, described
    their creations as ‚Äúa new and improved version of parasocial entertainment.‚Äù *Parasocial*
    is the right word: Raymond Mar, a psychologist at York University, noted that
    people, driven by the need to feel understood and accepted, could form intimate
    relationships with the bots. ‚ÄúYou can certainly imagine that children are vulnerable
    in all kinds of ways,‚Äù he said, ‚Äúincluding having more difficulty separating reality
    from fiction.‚Äù Character.AI is available to users over the age of 13 [[45]](https://www.theinformation.com/articles/the-lonely-hearts-club-of-character-ai).
    Character.AI‚Äôs founders, Shazeer and Daniel De Freitas, originally conceived of
    bots used for other purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: They spun up chatbots for travel planning, programming advice, and language
    tutoring. But as always, users had other ideas.¬†.¬†.¬†. ‚ÄúWe‚Äôd see on Twitter somebody
    posting, ‚ÄòThis videogame is my new therapist. My therapist doesn‚Äôt care about
    me, and this cartoon does.‚Äô We just keep getting reminded that we have no idea
    what the users actually want.‚Äù [[45]](https://www.theinformation.com/articles/the-lonely-hearts-club-of-character-ai)
  prefs: []
  type: TYPE_NORMAL
- en: That may be so, and in the venture-funded start-up world, the pressure is on
    start-ups to attract users as quickly as possible.
  prefs: []
  type: TYPE_NORMAL
- en: A poll on Character.AI‚Äôs Reddit forum showed that the plurality of respondents
    (more than 1,000 out of about 2,500) primarily used the site for romantic role-play.
    Character users have protested crackdowns on sexual content, and an online petition
    asking Character to remove its anti-pornography filter garnered almost 100,000
    signatures, though Shazeer said that the company will never support pornographic
    content [[45]](https://www.theinformation.com/articles/the-lonely-hearts-club-of-character-ai).
    This may be because of the stricter regulatory environment for pornographic materials
    or because they view such content as unfriendly to the brand, but the line drawn
    by Character.AI reflects a small degree of what LLM developers, in particular
    those focusing on social chatbots, must contemplate. For each product decision,
    whether it‚Äôs to allow users to create their own bots or what kinds of content
    those bots can produce, there could be immense benefits and immense risks. Companies
    should think carefully through which of these risks they can take on and which
    are too great.
  prefs: []
  type: TYPE_NORMAL
- en: Chatbot developers have a moral responsibility to their users; it‚Äôs not enough
    to simply say that a chatbot wasn‚Äôt intended for use as a therapist if they know
    that users are using the bots for virtual therapy sessions. Companies should monitor
    uses carefully in a way that protects privacy (e.g., by anonymizing and aggregating
    conversations). With this knowledge, companies shouldn‚Äôt accept unquestioningly
    what the users want, but if they intend to support the use case‚Äîto continue with
    the therapy example‚Äîthey could consult with mental health experts and licensed
    psychologists to ensure that the chatbots‚Äô behavior won‚Äôt contribute to unhealthy
    dependencies and that it aligns with current recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Companies may also decide not to support some relationships for which users
    desperately want to use their chatbots, whether that‚Äôs therapeutic, sexual, or
    another kind. In chapter 3, we discussed various strategies for controlling the
    generations of a model, including a chatbot or other dialogue agent. Given that
    people will continue to elicit sexual content or talk through sensitive topics
    that the chatbot may or may not be capable of handling, the content policies for
    the company must be enforced through technical means. In addition to monitoring
    how people are using the chatbot in general, companies could sample anonymized
    conversations to look for dependency or unhealthy relationship formation.
  prefs: []
  type: TYPE_NORMAL
- en: In the future, we may come to view all manner of HCRs as normal, including romantic
    ones. But because the science isn‚Äôt settled on the effects of these types of products,
    developers ought to exercise caution by avoiding optimizing for pure engagement,
    monitoring the actual use of the product, and thinking about how that use may
    affect the mental or social health of the user base by consulting with experienced
    mental health professionals to help answer those questions.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the people creating social chatbots, we all as a society will need to
    reckon with what it means for people to use social chatbots for companionship
    and emotional support. Perhaps these tools will become a valuable standard component
    of treatment for people who feel socially excluded or isolated. If not, they may
    at least bring joy and entertainment to millions of people. We may very well need
    to negotiate the role of social chatbots in our own lives, juggling the benefits
    that they offer with the other activities and relationships that command our attention.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: People have long sought companionship from virtual assistants and social chatbots,
    such as Apple‚Äôs Siri and Replika‚Äôs chatbots.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attachment theory can be understood within an evolutionary context in that the
    attachment figure provides safety, protection, and security.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The United States, among other nations, is in the midst of a ‚Äúloneliness epidemic,‚Äù
    wherein more Americans report feeling socially isolated than in past years of
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: People are turning to social chatbots for intimacy and support, and while human-chatbot
    relationships (HCRs) do seem to have benefits for users, there is some risk that
    HCRs will supplant real relationships in the lives of heavy users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Companies that develop social chatbots should mind existing principles of responsible
    design and mental health best practices when determining when and how the bot
    should engage in sensitive conversations with users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
