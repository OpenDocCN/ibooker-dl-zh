- en: Chapter 2\. Model Development on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will provide an overview of prevailing technologies and
    techniques for developing machine learning models using Kubernetes as a compute
    platform. While we will focus on specific techniques relevant to large language
    models (LLMs) and generative AI, many of the techniques we discuss will apply
    to traditional predictive models and other architectures as well.
  prefs: []
  type: TYPE_NORMAL
- en: Historically, models have required extensive data preparation to curate high-quality,
    labeled datasets that sufficiently capture the problem domain. Creating these
    datasets was very labor-intensive and expensive. More recently, advances in computational
    power, improved algorithms for distributing training across compute resources,
    and widespread open access to training data have all paved the way for extremely
    powerful general-purpose models to be built without heavy data curation.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, foundation LLMs are created via self-supervised learning, a type
    of unsupervised learning, on extremely large, unlabeled datasets. For LLMs, this
    results in a model that understands patterns in human language and can predict
    the most likely output that should follow a given input. These foundational models
    exhibit usefulness across a wide breadth of tasks, but practitioners often need
    to adapt these pretrained base models to some specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: There are several prevailing techniques for adapting these foundational models,
    which differ from each other in their intended use cases, ease of implementation,
    and costs of implementation. Collectively, we will refer to these approaches as
    *model customization techniques*.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of LLM Customization Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The LLM customization space, like much of generative AI, is evolving rapidly
    with new techniques being invented regularly. In general, customization is achieved
    through one or more of the following fundamental techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Customizing an existing model’s output by leaving the model unchanged but carefully
    constructing the input to get a desired result. Examples of this include *prompt
    engineering* and *retrieval-augmented generation* (RAG).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining individual models to achieve an output that is more desirable than
    that from a single model. One example of this is the [*mixture-of-agents* approach](https://oreil.ly/vw3kM).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retraining an existing model using curated data specific to a given task. This
    is *fine-tuning*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Novel model customization techniques are likely to be achieved through new algorithms
    for implementing these fundamental techniques more efficiently or through creatively
    combining these techniques, as in the case of [*retrieval-augmented fine-tuning*
    (RAFT)](https://oreil.ly/feMXE).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the rest of this section, we will provide a primer on two of the most prominent
    approaches (as of this writing) to model customization: RAG and fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval-Augmented Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A fundamental limitation of pretrained foundation models is that they possess
    “knowledge” only of the data that they were trained on. If you ask a model about
    a piece of data that it was not trained on, it will fail to give the desired answer.
    RAG is a technique that extends an existing model’s knowledge by passing relevant
    contextual data as input to the model at query time.
  prefs: []
  type: TYPE_NORMAL
- en: So how does RAG work? Generally, when a user queries a model, a database (typically
    a [vector database](https://oreil.ly/2L8-k)) is queried for information relevant
    to the input query. The RAG system parses the results and uses an algorithm like
    [cosine similarity](https://oreil.ly/7uOIh) to choose the results most relevant
    to the query. Once those are chosen, they are added to the original query as contextual
    information and sent on to the model in a format along the lines of “using information
    found only in this input document, answer this question for me.” [Figure 2-1](#ch02_figure_1_1738498450534664)
    illustrates a hypothetical RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/skia_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. An illustration of a generalized RAG system showing the interactions
    between the user, retrieval system, vector database, and model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The chosen retrieval and ranking algorithm is critically important to the performance
    of the RAG system. If no relevant contextual data is retrieved by the system,
    the model will lack the knowledge needed to give the desired answer to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Even though RAG requires a retrieval system and additional data storage between
    the user and the model, it has a number of benefits. Because RAG supplements the
    model’s knowledge at runtime, it requires less knowledge to be baked into the
    model and opens up the possibility of using a smaller model that is cheaper to
    serve to users while simultaneously allowing users to incorporate rapidly changing
    data like stock prices on the fly. RAG can also reduce the time to achieve value
    with an LLM, because it doesn’t require a lengthy retraining process to work.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the knowledge given to a model via RAG is transient, and
    only exists for a single query. You also have to carefully craft the input prompt
    to get the kind of output you’re interested in. However, this sort of customization
    also has its limits. If you want to make knowledge changes persistent or fully
    customize the format of the model’s output, retraining the foundation model is
    required.
  prefs: []
  type: TYPE_NORMAL
- en: Model Fine-Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training a foundation model is notoriously expensive and time-consuming, which
    isn’t an option for even the largest enterprises. Instead, we can make use of
    a technique called fine-tuning. With fine-tuning, you create a high-quality, labeled
    dataset that is specific to your domain-specific task, knowledge, or desired output
    format. You can then use that dataset to adjust a pretrained model in a fraction
    of the time and with a fraction of the data that would be required for training
    from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: The fine-tuned model will then have your desired knowledge and behavior baked
    in, allowing your production architecture to avoid the complexities required by
    techniques like RAG. However, fine-tuning requires knowing how to train a model,
    the time to curate a training dataset large enough to influence a model, and the
    sometimes high compute cost to perform the training itself.
  prefs: []
  type: TYPE_NORMAL
- en: A number of techniques exist to optimize the compute cost of fine-tuning, such
    as parameter-efficient fine-tuning (PEFT) and low-rank adaptation (LoRA). Both
    of these techniques work by training only a subset of the pretrained model’s weights
    and biases.
  prefs: []
  type: TYPE_NORMAL
- en: While this is complex, there are many tools and entire platforms available to
    help with training and fine-tuning models, such as [InstructLab](https://instructlab.ai)
    and Hugging Face’s [sft_trainer](https://oreil.ly/9f4mP), with many of them available
    within the open source Kubernetes ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes-Native Model Training Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While many training tools and platforms are available, at a fundamental level
    they all provide easy access to the compute power necessary to train and fine-tune
    models. When evaluating a training tool or platform, the following requirements
    should be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: Integration with the training framework(s) (distributed or otherwise) that data
    scientists or data science teams use and are comfortable with (e.g., PyTorch,
    TensorFlow, etc.).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for training/fine-tuning algorithms that your team wants to use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to hardware optimizers, such as accelerators (e.g., GPUs), specialized
    network devices, and specialized storage providers with multi-write-capable storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrations with the development environments data scientists or data science
    teams are already using. A tool that effectively abstracts away Kubernetes so
    that the data scientist or team doesn’t need to manage it is ideal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following subsections, we will explore open source tools that meet these
    requirements and have strong community adoption.
  prefs: []
  type: TYPE_NORMAL
- en: Ray
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Ray](https://oreil.ly/eu-Ll) is a framework that enables users to scale their
    training and fine-tuning processes up from single machines to clusters of machines,
    and can run natively on Kubernetes via the [KubeRay](https://oreil.ly/H7Xgz) operator.
    It seamlessly integrates with PyTorch and other frameworks via [Ray Train](https://oreil.ly/dLTwc)
    and has extensive support for [accelerators](https://oreil.ly/TrRP-). It also
    comes with a dashboard that provides key monitoring information to end users.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An [operator](https://oreil.ly/-lPOg) is an extension to Kubernetes that helps
    to manage Kubernetes applications by using custom resources to automate the application’s
    lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Ray’s biggest strength is its ease of adoption by data scientists who don’t
    know Kubernetes well, but it comes with the downside of increased overhead through
    the management of Ray clusters when compared to options that have a more “raw”
    interface to Kubernetes. It also doesn’t always scale well to extremely large-scale
    training jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow Training Operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Kubeflow](https://www.kubeflow.org) is a community-managed open source ecosystem
    of Kubernetes components that support the full AI lifecycle. A part of that ecosystem,
    the Kubeflow Training Operator (KFTO) is a Kubernetes-native operator that allows
    users to use Kubernetes for distributed training and fine-tuning of large models.
    Its software development kit (SDK) allows for easy integration into existing environments
    and code and has extensive support for common frameworks like PyTorch.'
  prefs: []
  type: TYPE_NORMAL
- en: KFTO accelerator support is tied to the chosen training framework, so it supports
    anything that the training framework and Kubernetes support and can scale to any
    level that the framework and Kubernetes are capable of scaling to. Unlike Ray,
    KFTO is a thin layer on top of the underlying Kubernetes objects, which introduces
    very little compute overhead. The flipside to that, though, is that more of the
    Kubernetes details are exposed to the user, which may be confusing for data scientists
    and developers who do not need to know these details.
  prefs: []
  type: TYPE_NORMAL
- en: Native Training Framework Integration with Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most training frameworks have framework-specific tooling for integrating with
    Kubernetes to provide computational resources. PyTorch, for example, has a universal
    job launcher called TorchX that includes Kubernetes support via its scheduler.
    While this kind of solution is the most lightweight and is the easiest for data
    scientists to adopt, it is less declarative and thus doesn’t lend itself as well
    to administration by MLOps teams.
  prefs: []
  type: TYPE_NORMAL
- en: Another potential downside is that these tools are framework-specific, so usage
    won’t necessarily scale in large organizations with several data science teams
    using different frameworks. These native integrations are best suited for small
    teams of data scientists during experimentation phases.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Typically, once a model is trained or fine-tuned, you will want to evaluate
    its performance. Many existing model evaluation tools that data scientists use
    outside of Kubernetes can also be used when Kubernetes is used as a training platform.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Compute Resources for Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the tools described in the previous section allow you to train and fine-tune
    across many computational resources, this often requires extensive and costly
    hardware resources. Enterprises must pay particular attention to managing the
    cost incurred during training or fine-tuning. A robust management system should
    be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Facilitate the creation of job queues so that requests for compute hardware
    get serviced as soon as the hardware becomes available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign resource quotas to groups of users in order to constrain how many resources
    a given group can consume.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Share resource quotas between groups when individual groups need to burst and
    there are free resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manage request priorities for resources and priority-based job preemption.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide auditability and reporting on resource management at the model, job,
    and team levels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow all of these functions to be centrally managed by IT teams while maintaining
    transparency for users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are currently two major open source projects in this space: [Kueue](https://oreil.ly/JZQ_0)
    and [Volcano](https://oreil.ly/cCQQJ). Both projects are Kubernetes native and
    have strong community adoption. They also have support for managing resources
    of various types, like Ray clusters, KFTO jobs, and PyTorch training jobs.'
  prefs: []
  type: TYPE_NORMAL
- en: While these projects offer similar functionality, they do have some key differences.
    Kueue is an official Kubernetes special interest group project and is thus “blessed”
    by the wider Kubernetes community. It is based on the design principle of delegating
    functionality to existing Kubernetes components when applicable, and because of
    this, Kueue is fairly lightweight.
  prefs: []
  type: TYPE_NORMAL
- en: Volcano, on the other hand, replicates some existing Kubernetes functionality,
    giving it more overhead but allowing it to be a more holistic and better-integrated
    solution. It is also more mature than Kueue and as of this writing offers more
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Once a data science team has a model and training procedure it is ready to send
    to production, it will be necessary to periodically retrain the model while keeping
    track of the datasets that went into each new version of the model. In [Chapter 3](ch03.html#ch03_making_training_repeatable_1738498450655759),
    we will discuss why periodic retraining, model versioning, and dataset versioning
    are necessary along with tools to help with these production workflows.
  prefs: []
  type: TYPE_NORMAL
