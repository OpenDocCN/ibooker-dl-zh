- en: Chapter 2\. Model Development on Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will provide an overview of prevailing technologies and
    techniques for developing machine learning models using Kubernetes as a compute
    platform. While we will focus on specific techniques relevant to large language
    models (LLMs) and generative AI, many of the techniques we discuss will apply
    to traditional predictive models and other architectures as well.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Historically, models have required extensive data preparation to curate high-quality,
    labeled datasets that sufficiently capture the problem domain. Creating these
    datasets was very labor-intensive and expensive. More recently, advances in computational
    power, improved algorithms for distributing training across compute resources,
    and widespread open access to training data have all paved the way for extremely
    powerful general-purpose models to be built without heavy data curation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Generally, foundation LLMs are created via self-supervised learning, a type
    of unsupervised learning, on extremely large, unlabeled datasets. For LLMs, this
    results in a model that understands patterns in human language and can predict
    the most likely output that should follow a given input. These foundational models
    exhibit usefulness across a wide breadth of tasks, but practitioners often need
    to adapt these pretrained base models to some specific use case.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: There are several prevailing techniques for adapting these foundational models,
    which differ from each other in their intended use cases, ease of implementation,
    and costs of implementation. Collectively, we will refer to these approaches as
    *model customization techniques*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Overview of LLM Customization Techniques
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The LLM customization space, like much of generative AI, is evolving rapidly
    with new techniques being invented regularly. In general, customization is achieved
    through one or more of the following fundamental techniques:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Customizing an existing model’s output by leaving the model unchanged but carefully
    constructing the input to get a desired result. Examples of this include *prompt
    engineering* and *retrieval-augmented generation* (RAG).
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining individual models to achieve an output that is more desirable than
    that from a single model. One example of this is the [*mixture-of-agents* approach](https://oreil.ly/vw3kM).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retraining an existing model using curated data specific to a given task. This
    is *fine-tuning*.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Novel model customization techniques are likely to be achieved through new algorithms
    for implementing these fundamental techniques more efficiently or through creatively
    combining these techniques, as in the case of [*retrieval-augmented fine-tuning*
    (RAFT)](https://oreil.ly/feMXE).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'In the rest of this section, we will provide a primer on two of the most prominent
    approaches (as of this writing) to model customization: RAG and fine-tuning.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval-Augmented Generation
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A fundamental limitation of pretrained foundation models is that they possess
    “knowledge” only of the data that they were trained on. If you ask a model about
    a piece of data that it was not trained on, it will fail to give the desired answer.
    RAG is a technique that extends an existing model’s knowledge by passing relevant
    contextual data as input to the model at query time.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: So how does RAG work? Generally, when a user queries a model, a database (typically
    a [vector database](https://oreil.ly/2L8-k)) is queried for information relevant
    to the input query. The RAG system parses the results and uses an algorithm like
    [cosine similarity](https://oreil.ly/7uOIh) to choose the results most relevant
    to the query. Once those are chosen, they are added to the original query as contextual
    information and sent on to the model in a format along the lines of “using information
    found only in this input document, answer this question for me.” [Figure 2-1](#ch02_figure_1_1738498450534664)
    illustrates a hypothetical RAG system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/skia_0201.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. An illustration of a generalized RAG system showing the interactions
    between the user, retrieval system, vector database, and model
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The chosen retrieval and ranking algorithm is critically important to the performance
    of the RAG system. If no relevant contextual data is retrieved by the system,
    the model will lack the knowledge needed to give the desired answer to the user.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Even though RAG requires a retrieval system and additional data storage between
    the user and the model, it has a number of benefits. Because RAG supplements the
    model’s knowledge at runtime, it requires less knowledge to be baked into the
    model and opens up the possibility of using a smaller model that is cheaper to
    serve to users while simultaneously allowing users to incorporate rapidly changing
    data like stock prices on the fly. RAG can also reduce the time to achieve value
    with an LLM, because it doesn’t require a lengthy retraining process to work.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the knowledge given to a model via RAG is transient, and
    only exists for a single query. You also have to carefully craft the input prompt
    to get the kind of output you’re interested in. However, this sort of customization
    also has its limits. If you want to make knowledge changes persistent or fully
    customize the format of the model’s output, retraining the foundation model is
    required.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Model Fine-Tuning
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training a foundation model is notoriously expensive and time-consuming, which
    isn’t an option for even the largest enterprises. Instead, we can make use of
    a technique called fine-tuning. With fine-tuning, you create a high-quality, labeled
    dataset that is specific to your domain-specific task, knowledge, or desired output
    format. You can then use that dataset to adjust a pretrained model in a fraction
    of the time and with a fraction of the data that would be required for training
    from scratch.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: The fine-tuned model will then have your desired knowledge and behavior baked
    in, allowing your production architecture to avoid the complexities required by
    techniques like RAG. However, fine-tuning requires knowing how to train a model,
    the time to curate a training dataset large enough to influence a model, and the
    sometimes high compute cost to perform the training itself.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: A number of techniques exist to optimize the compute cost of fine-tuning, such
    as parameter-efficient fine-tuning (PEFT) and low-rank adaptation (LoRA). Both
    of these techniques work by training only a subset of the pretrained model’s weights
    and biases.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: While this is complex, there are many tools and entire platforms available to
    help with training and fine-tuning models, such as [InstructLab](https://instructlab.ai)
    and Hugging Face’s [sft_trainer](https://oreil.ly/9f4mP), with many of them available
    within the open source Kubernetes ecosystem.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes-Native Model Training Tools
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While many training tools and platforms are available, at a fundamental level
    they all provide easy access to the compute power necessary to train and fine-tune
    models. When evaluating a training tool or platform, the following requirements
    should be considered:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Integration with the training framework(s) (distributed or otherwise) that data
    scientists or data science teams use and are comfortable with (e.g., PyTorch,
    TensorFlow, etc.).
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for training/fine-tuning algorithms that your team wants to use.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to hardware optimizers, such as accelerators (e.g., GPUs), specialized
    network devices, and specialized storage providers with multi-write-capable storage.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrations with the development environments data scientists or data science
    teams are already using. A tool that effectively abstracts away Kubernetes so
    that the data scientist or team doesn’t need to manage it is ideal.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following subsections, we will explore open source tools that meet these
    requirements and have strong community adoption.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Ray
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Ray](https://oreil.ly/eu-Ll) is a framework that enables users to scale their
    training and fine-tuning processes up from single machines to clusters of machines,
    and can run natively on Kubernetes via the [KubeRay](https://oreil.ly/H7Xgz) operator.
    It seamlessly integrates with PyTorch and other frameworks via [Ray Train](https://oreil.ly/dLTwc)
    and has extensive support for [accelerators](https://oreil.ly/TrRP-). It also
    comes with a dashboard that provides key monitoring information to end users.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An [operator](https://oreil.ly/-lPOg) is an extension to Kubernetes that helps
    to manage Kubernetes applications by using custom resources to automate the application’s
    lifecycle.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Ray’s biggest strength is its ease of adoption by data scientists who don’t
    know Kubernetes well, but it comes with the downside of increased overhead through
    the management of Ray clusters when compared to options that have a more “raw”
    interface to Kubernetes. It also doesn’t always scale well to extremely large-scale
    training jobs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow Training Operator
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Kubeflow](https://www.kubeflow.org) is a community-managed open source ecosystem
    of Kubernetes components that support the full AI lifecycle. A part of that ecosystem,
    the Kubeflow Training Operator (KFTO) is a Kubernetes-native operator that allows
    users to use Kubernetes for distributed training and fine-tuning of large models.
    Its software development kit (SDK) allows for easy integration into existing environments
    and code and has extensive support for common frameworks like PyTorch.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: KFTO accelerator support is tied to the chosen training framework, so it supports
    anything that the training framework and Kubernetes support and can scale to any
    level that the framework and Kubernetes are capable of scaling to. Unlike Ray,
    KFTO is a thin layer on top of the underlying Kubernetes objects, which introduces
    very little compute overhead. The flipside to that, though, is that more of the
    Kubernetes details are exposed to the user, which may be confusing for data scientists
    and developers who do not need to know these details.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Native Training Framework Integration with Kubernetes
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most training frameworks have framework-specific tooling for integrating with
    Kubernetes to provide computational resources. PyTorch, for example, has a universal
    job launcher called TorchX that includes Kubernetes support via its scheduler.
    While this kind of solution is the most lightweight and is the easiest for data
    scientists to adopt, it is less declarative and thus doesn’t lend itself as well
    to administration by MLOps teams.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Another potential downside is that these tools are framework-specific, so usage
    won’t necessarily scale in large organizations with several data science teams
    using different frameworks. These native integrations are best suited for small
    teams of data scientists during experimentation phases.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Typically, once a model is trained or fine-tuned, you will want to evaluate
    its performance. Many existing model evaluation tools that data scientists use
    outside of Kubernetes can also be used when Kubernetes is used as a training platform.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Managing Compute Resources for Training
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the tools described in the previous section allow you to train and fine-tune
    across many computational resources, this often requires extensive and costly
    hardware resources. Enterprises must pay particular attention to managing the
    cost incurred during training or fine-tuning. A robust management system should
    be able to do the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Facilitate the creation of job queues so that requests for compute hardware
    get serviced as soon as the hardware becomes available.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign resource quotas to groups of users in order to constrain how many resources
    a given group can consume.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将资源配额分配给用户组，以限制特定组可以消耗的资源数量。
- en: Share resource quotas between groups when individual groups need to burst and
    there are free resources.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当个别组需要爆发并存在空闲资源时，在组之间共享资源配额。
- en: Manage request priorities for resources and priority-based job preemption.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理资源请求的优先级和基于优先级的作业抢占。
- en: Provide auditability and reporting on resource management at the model, job,
    and team levels.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型、作业和团队级别上提供资源管理的可审计性和报告。
- en: Allow all of these functions to be centrally managed by IT teams while maintaining
    transparency for users.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许IT团队集中管理所有这些功能，同时保持对用户透明。
- en: 'There are currently two major open source projects in this space: [Kueue](https://oreil.ly/JZQ_0)
    and [Volcano](https://oreil.ly/cCQQJ). Both projects are Kubernetes native and
    have strong community adoption. They also have support for managing resources
    of various types, like Ray clusters, KFTO jobs, and PyTorch training jobs.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在这个领域有两个主要的开源项目：[Kueue](https://oreil.ly/JZQ_0) 和 [Volcano](https://oreil.ly/cCQQJ)。这两个项目都是Kubernetes原生，并且拥有强大的社区支持。它们还支持管理各种类型的资源，如Ray集群、KFTO作业和PyTorch训练作业。
- en: While these projects offer similar functionality, they do have some key differences.
    Kueue is an official Kubernetes special interest group project and is thus “blessed”
    by the wider Kubernetes community. It is based on the design principle of delegating
    functionality to existing Kubernetes components when applicable, and because of
    this, Kueue is fairly lightweight.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些项目提供类似的功能，但它们确实有一些关键的区别。Kueue是官方Kubernetes特别兴趣小组项目，因此得到了更广泛的Kubernetes社区的认可。它基于将功能委托给现有Kubernetes组件的设计原则，因此Kueue相对较轻量。
- en: Volcano, on the other hand, replicates some existing Kubernetes functionality,
    giving it more overhead but allowing it to be a more holistic and better-integrated
    solution. It is also more mature than Kueue and as of this writing offers more
    capabilities.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Volcano复制了一些现有的Kubernetes功能，这给它带来更多的开销，但允许它成为一个更全面、更好的集成解决方案。它也比Kueue更成熟，截至本文撰写时提供了更多的功能。
- en: Once a data science team has a model and training procedure it is ready to send
    to production, it will be necessary to periodically retrain the model while keeping
    track of the datasets that went into each new version of the model. In [Chapter 3](ch03.html#ch03_making_training_repeatable_1738498450655759),
    we will discuss why periodic retraining, model versioning, and dataset versioning
    are necessary along with tools to help with these production workflows.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据科学团队有了模型和训练流程，准备将其发送到生产环境，就需要定期重新训练模型，同时跟踪进入每个新版本模型的数据集。在[第3章](ch03.html#ch03_making_training_repeatable_1738498450655759)中，我们将讨论定期重新训练、模型版本化和数据集版本化的必要性，以及帮助这些生产流程的工具。
