<html><head></head><body><section data-pdf-bookmark="Chapter 12. Deployment of AI Services" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch12">
<h1><span class="label">Chapter 12. </span>Deployment of AI Services</h1>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1273">
<h1>Chapter Goals</h1>
<p><a data-primary="deployment of AI services" data-type="indexterm" id="ix_ch12-asciidoc0"/>In this chapter, you will learn about:</p>

<ul>
<li>
<p>Deployment options for serving your GenAI services to users</p>
</li>
<li>
<p>Deployment using containers with Docker</p>
</li>
<li>
<p>Container networking in Docker and working with storage volumes</p>
</li>
<li>
<p>Resolving permission issues that arise when working with containers</p>
</li>
<li>
<p>Optimization strategies to minimize the container build time and image size</p>
</li>
</ul>
</div></aside>

<p>In this final chapter, it is time to complete your GenAI solution by deploying it. You’re going to learn several deployment strategies and, as part of deployment, containerize your services with Docker following its best practices.</p>






<section data-pdf-bookmark="Deployment Options" data-type="sect1"><div class="sect1" id="id265">
<h1>Deployment Options</h1>

<p><a data-primary="deployment of AI services" data-secondary="options" data-type="indexterm" id="ix_ch12-asciidoc1"/>You now have a working GenAI service that you want to make accessible to your users.
What are your deployment options? There are a few common deployment strategies you can adapt to make your apps accessible to users:</p>
<ul class="two-col">
<li>
<p>Virtual machines (VMs)</p>
</li>
<li>
<p>Serverless functions</p>
</li>
<li>
<p>Managed application platforms</p>
</li>
<li>
<p>Containerization</p>
</li>
</ul>

<p>Let’s explore each in more detail.</p>








<section data-pdf-bookmark="Deploying to Virtual Machines" data-type="sect2"><div class="sect2" id="id207">
<h2>Deploying to Virtual Machines</h2>

<p><a data-primary="deployment of AI services" data-secondary="options" data-tertiary="deploying to virtual machines" data-type="indexterm" id="ix_ch12-asciidoc2"/>If you plan to use your own on-premises servers or prefer to deploy your services on the same hardware hosting your other applications for high isolation and security, you can deploy your GenAI service to a VM.</p>

<p>A VM is a software emulation of a physical computer running an operating system (OS) and applications.
It’s no different from a physical computer like a laptop, smartphone, or server.</p>

<p>The VM’s <em>host</em> system provides resources such as CPU, memory, and storage, while a software layer called the <em>hypervisor</em> manages the VM and allocates resources from the host to the VM.
The resources that the hypervisor allocates to the VM is the <em>virtual hardware</em> that its OS and applications run on.</p>

<p>The VM could run directly on host’s hardware (bare metal) or on a conventional operating system (i.e., be hosted).
As a result, the OS installed within the VM is then referred to as the <em>guest OS</em>.</p>

<p><a data-type="xref" href="#deployment_vm_architecture">Figure 12-1</a> shows the virtualization technology system architecture.</p>

<figure><div class="figure" id="deployment_vm_architecture">
<img alt="bgai 1201" src="assets/bgai_1201.png"/>
<h6><span class="label">Figure 12-1. </span>Virtualization system architecture</h6>
</div></figure>

<p>Cloud providers or your own data center can consist of several physical servers, each hosting multiple VMs with their own guest OS and hosted applications.
For cost-effective resource sharing, these VMs may share the same mounted physical storage drive even though they’re contained within fully isolated environments, as you can see in <a data-type="xref" href="#deployment_vm_data_center">Figure 12-2</a>.</p>

<figure><div class="figure" id="deployment_vm_data_center">
<img alt="bgai 1202" src="assets/bgai_1202.png"/>
<h6><span class="label">Figure 12-2. </span>Hosted VMs in a data center</h6>
</div></figure>

<p>The benefit of using a VM is that you have direct access to the guest OS, virtual hardware resources, and GPU drivers.
If there are any issues with deployment, you can connect to the VM via the <em>Secure Shell Transfer</em> (SHH) protocol to inspect application logs, set up application environment, and debug production issues.</p>

<p>Deploying your services to VMs will be as straightforward as cloning your code repository to the VM and then installing the required dependencies, packages, and drivers to successfully start up your application.
However, the recommended way to do this is to use a containerization platform such as Docker running on the VM to enable continuous deployments and other benefits.
You should also ensure you size your VM resources appropriately so that your services aren’t starved for CPU/GPU cores, memory, or disk storage.</p>

<p>With on-premises VMs, you can save on-cloud hosting or server rental costs and can fully secure your application environments to a handful of users, isolated from the public internet.
These benefits are also achievable with cloud VMs but require 
<span class="keep-together">additional</span> networking and resource configuration to set up.
In addition, you can have access to GPU hardware and configure drivers for your application requirements.</p>

<p>Bear in mind that using the VM deployment pattern may not be easily scalable and requires significant effort to maintain.
Additionally, VM servers normally run 24/7 incurring constant running costs, unless you automate their startup and shutdown based on your needs.
You’ll be responsible for applying security patches, OS updates, and package upgrades alongside any networking configurations.
With direct access to hardware resources, you’ll also have more decisions to make that can slow you down, leading to decision fatigue.</p>

<p>My advice is to deploy to a VM if you don’t plan to scale your services anytime soon or need to maintain low server costs and a secure isolated application environment for a handful of users.
In addition, make sure you’ve planned sufficient time for deployment, networking, and configuration of your VMs.<a data-startref="ix_ch12-asciidoc2" data-type="indexterm" id="id1274"/></p>
</div></section>








<section data-pdf-bookmark="Deploying to Serverless Functions" data-type="sect2"><div class="sect2" id="id208">
<h2>Deploying to Serverless Functions</h2>

<p><a data-primary="deployment of AI services" data-secondary="options" data-tertiary="deploying to serverless functions" data-type="indexterm" id="ix_ch12-asciidoc3"/><a data-primary="serverless deployments" data-type="indexterm" id="ix_ch12-asciidoc4"/>Aside from VMs, you can also deploy your services on cloud functions that cloud providers supply as <em>serverless</em> systems.
In serverless computing, your code is executed in response to events such as database changes, updates to blobs in a storage, HTTP requests, or messages added to a queue.
This means you pay only for the requests or compute resources your services use, rather than for an entire server as with a continuously running VM.</p>

<p>Serverless deployments are often useful when:</p>

<ul>
<li>
<p>You want to have event-driven systems instead of a running VM, which might be on 24/7</p>
</li>
<li>
<p>You want to deploy your API services using a serverless architecture that’s highly cost-efficient</p>
</li>
<li>
<p>Your services are to perform batch processing jobs</p>
</li>
<li>
<p>You need workflow automation</p>
</li>
</ul>

<p>The term <em>serverless</em> doesn’t mean that cloud functions don’t require hardware resources to execute but rather that the management of these resources is handled by the cloud provider.
This allows you to focus on writing application code without worrying about server and OS-level details.</p>

<p>Cloud providers instantiate compute resources to meet the demand of their customers.
Often, there is a surge in demand, requiring them to create additional resources in advance to handle the demand spike.
However, once the demand drops, excess unallocated compute resources remain that must be either shut down or shared among other customers.</p>

<p>Removing and creating resources is an intensive compute operation to perform. At scale, these operations carry significant costs for cloud providers. Therefore, cloud providers prefer to keep these resources running as much as possible and distribute them among existing customers to maximize billing.</p>

<p>To encourage customers to use these excess compute, they’ve built cloud function services that you can leverage to run your backend services on excess (i.e., serverless) compute.
Luckily, there are packages such as Magnum that allow you to package 
<span class="keep-together">FastAPI</span> services on AWS cloud functions.
You will soon see that FastAPI services can also be deployed as Azure cloud functions.</p>

<p>What you need to bear in mind is that these functions are allocated only a small amount of resources and have a short timeout.
However, you can request longer timeouts and compute resources to be allocated, but it may take longer to receive these allocations, leading to higher latencies for your users.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>If your business logic consumes a lot of resources or requires longer than a handful of minutes to execute, cloud functions may not be a suitable deployment option for you.</p>

<p>However, you can split your FastAPI services across multiple functions, with each function handling a single exposed endpoint.
This way, you can deploy parts of your service as cloud functions, reducing the portion of the FastAPI service that needs to be deployed using other methods.</p>
</div>

<p>The main advantage of using serverless functions for deploying your services is their scalability.
You can scale your applications as needed and pay only a fraction of the cost compared to reserving dedicated VM resources.
Cloud providers typically charge based on the number of function executions and runtime, often with generous monthly quotas.
This means that if your functions run quickly and you have a moderate number of concurrent users, you might be able to host all your services for free.</p>

<p>Furthermore, cloud providers also supply function runtimes that you can install locally for local testing and development so that you can significantly shorten development iterations.</p>

<p>Each cloud provider has their own approach to deploying serverless functions.
Often, you require an entry script such as <em>main.py</em> that can import dependencies from other modules as needed.
Alongside the entry point script, you’ll need to upload a function host JSON configuration file alongside <em>requirements.txt</em> for required dependencies to be installed on deployment on a Python runtime.</p>

<p>You can then deploy functions by uploading all the required files as a zipped directory or using CI/CD pipelines that authenticate with the provider and execute the deployment commands within your cloud project.</p>

<p>As an example, let’s try to deploy a bare-bones FastAPI app that returns LLM responses. The structure of the project will be as follows:</p>

<pre data-type="programlisting">project/
│
├── host.json
├── main.py
├── app.py
└── requirements.txt</pre>

<p>You can then package a FastAPI app as an <a href="https://oreil.ly/ZaOuF">Azure serverless function</a> by following the upcoming code examples.</p>

<p>You will need to install the <code>azure-functions</code> package to run Azure’s serverless function runtime for local development and testing:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install azure-functions<code class="w"/></pre>

<p>Then, create <em>host.json</em> by following <a data-type="xref" href="#deployment_function_azure_host">Example 12-1</a>.</p>
<div data-type="example" id="deployment_function_azure_host">
<h5><span class="label">Example 12-1. </span>Azure Functions host configurations (host.json)</h5>

<pre data-code-language="json" data-type="programlisting"><code class="p">{</code><code class="w"/>
  <code class="nt">"version"</code><code class="p">:</code> <code class="s2">"2.0"</code><code class="p">,</code><code class="w"/>
  <code class="nt">"extensions"</code><code class="p">:</code> <code class="p">{</code><code class="w"/>
    <code class="nt">"http"</code><code class="p">:</code> <code class="p">{</code><code class="w"/>
        <code class="nt">"routePrefix"</code><code class="p">:</code> <code class="s2">""</code><code class="w"/>
    <code class="p">}</code><code class="w"/>
  <code class="p">}</code><code class="w"/>
<code class="p">}</code><code class="w"/></pre></div>

<p>Afterward, implement your GenAI service with the FastAPI service as usual by following <a data-type="xref" href="#deployment_function_azure_app">Example 12-2</a>.</p>
<div data-type="example" id="deployment_function_azure_app">
<h5><span class="label">Example 12-2. </span>Simple FastAPI application serving LLM responses</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># app.py</code>

<code class="kn">import</code> <code class="nn">azure.functions</code> <code class="k">as</code> <code class="nn">func</code>
<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code>

<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">()</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"/generate/text"</code><code class="p">,</code> <code class="n">response_model_exclude_defaults</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">serve_text_to_text_controller</code><code class="p">(</code><code class="n">prompt</code><code class="p">):</code>
<code class="o">...</code></pre></div>

<p>Finally, wrap your FastAPI <code>app</code> within <code>func.AsgiFunctionApp</code> for the Azure serverless function runtime to hook into it, as shown in <a data-type="xref" href="#deployment_function_azure_function">Example 12-3</a>.</p>
<div data-type="example" id="deployment_function_azure_function">
<h5><span class="label">Example 12-3. </span>Deploying a FastAPI service with Azure Functions</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># function.py</code>

<code class="kn">import</code> <code class="nn">azure.functions</code> <code class="k">as</code> <code class="nn">func</code>
<code class="kn">from</code> <code class="nn">main</code> <code class="kn">import</code> <code class="n">app</code> <code class="k">as</code> <code class="n">fastapi_app</code>

<code class="n">app</code> <code class="o">=</code> <code class="n">func</code><code class="o">.</code><code class="n">AsgiFunctionApp</code><code class="p">(</code>
    <code class="n">app</code><code class="o">=</code><code class="n">fastapi_app</code><code class="p">,</code> <code class="n">http_auth_level</code><code class="o">=</code><code class="n">func</code><code class="o">.</code><code class="n">AuthLevel</code><code class="o">.</code><code class="n">ANONYMOUS</code>
<code class="p">)</code></pre></div>

<p>You can then start the function app by running the <code>func start</code> command, which should be available as a CLI command once you install the <code>azure-functions</code> 
<span class="keep-together">package:</span></p>

<pre data-code-language="bash" data-type="programlisting">$ func start<code class="w"/>

&gt;&gt; Found the following functions:<code class="w"/>
&gt;&gt; Functions:<code class="w"/>
&gt;&gt;        http_app_func: <code class="o">[</code>GET,POST,DELETE,HEAD,PATCH,PUT,OPTIONS<code class="o">]</code> <code class="se">\</code>
                          http://localhost:7071//<code class="o">{</code>*route<code class="o">}</code><code class="w"/>

&gt;&gt; Job host started<code class="w"/></pre>

<p>You can then try URLs corresponding to the handlers in the app by sending HTTP requests to both simple and the parameterized paths:</p>

<pre data-type="programlisting">http://localhost:7071/generate/text
http://localhost:7071/&lt;other-paths&gt;</pre>

<p>Once ready, you can then deploy your FastAPI wrapped serverless function to the Azure cloud and then run the following command:</p>

<pre data-type="programlisting">$ func azure functionapp publish &lt;FunctionAppName&gt;</pre>

<p>The <code>publish</code> command will then publish the project files from the project directory to <code>&lt;FunctionAppName&gt;</code> as a ZIP deployment package.</p>

<p>After deployment, you can then test different paths on the deployed URL:</p>

<pre data-type="programlisting">http://&lt;FunctionAppName&gt;.azurewebsites.net/generate/text
http://&lt;FunctionAppName&gt;.azurewebsites.net/&lt;other-paths&gt;</pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Your chosen cloud provider may not support serving a FastAPI server within its function runtime.
If that’s the case, you may want to seek alternative deployment options.
Otherwise, you’ll need to migrate the logic of your endpoints to the supported web framework of the function runtime and create separate functions for each endpoint.</p>
</div>

<p>As you see, deploying your FastAPI service as cloud functions is straightforward and allows you to delegate the management and scaling of your services to cloud 
<span class="keep-together">providers.</span></p>

<p>Bear in mind that if you decide to serve a GenAI model in your service, cloud functions wouldn’t be suitable deployment targets due to their short timeout periods (10 minutes).
Instead, you’d want to use a model provider API in your services so that you have reliable and scalable access to the model without being constrained by execution time limits.<a data-startref="ix_ch12-asciidoc4" data-type="indexterm" id="id1275"/><a data-startref="ix_ch12-asciidoc3" data-type="indexterm" id="id1276"/></p>
</div></section>








<section data-pdf-bookmark="Deploying to Managed App Platforms" data-type="sect2"><div class="sect2" id="id209">
<h2>Deploying to Managed App Platforms</h2>

<p><a data-primary="deployment of AI services" data-secondary="options" data-tertiary="deploying to managed app platforms" data-type="indexterm" id="ix_ch12-asciidoc5"/><a data-primary="managed app platforms, deploying to" data-type="indexterm" id="ix_ch12-asciidoc6"/>In addition to cloud functions or VMs, you can upload your codebase as ZIP files to app platforms managed by cloud providers.
Managed app platforms let you delegate several tasks related to maintenance and management of your services to the cloud provider.
In exchange, you pay only for the compute resources managed by the cloud provider that serve your application.
The cloud provider systems allocate and optimize resources based on your application’s needs.</p>

<p>Examples of such services include Azure App Services, AWS Elastic Beanstalk, Google App Engine, or Digital Ocean app platform.</p>

<p>Third-party platforms such as Heroku, Hugging Face Spaces, railway.app, render.com, or fly.io also exist for deploying your services directly from code in repositories, which abstract away certain decisions from you so that you can deploy faster and easier.
Under the hood, third-party managed app platforms may be using the infrastructure of main cloud providers like Azure, Google, or AWS.</p>

<p>The main benefit of deploying to managed app platforms is the ease and speed of deployment, networking, scaling, and maintaining your services.
Such platforms provide you with tools you need to secure, monitor, scale, and manage your services without having to worry about the underlying resource allocations, security, or software updates.
They can let you configure load balancers, SSL certificates, domain mappings, monitoring, and staging environments so that you can focus more on application development than deployment workload of the project.</p>

<p><a data-primary="platform-as-a-service (PaaS)" data-type="indexterm" id="id1277"/><a data-primary="PaaS (platform-as-a-service)" data-type="indexterm" id="id1278"/>Because these platforms follow the platform-as-a-service (PaaS) payment model, you’ll be billed a higher rate compared to relying on your own infrastructure or using lower-level resources such as bare-bone VMs or serverless compute options.
<a data-primary="infrastructure-as-a-service (IaaS)" data-type="indexterm" id="id1279"/><a data-primary="IaaS (infrastructure-as-a-service)" data-type="indexterm" id="id1280"/>Alternative services may use the infrastructure-as-a-service (IaaS) payment model that often is more cost-effective.</p>

<p>Personally, I find managed app platforms a convenient way to deploy my applications without much hassle.
If I’m working on a prototype and need to get my services available to users as fast as possible, managed app platforms is my first go-to option.
Although, bear in mind that if you need access to GPU hardware for running 
<span class="keep-together">inference</span> services, you’ll have to rely on dedicated VMs, on-premises servers, or specialized AI platform services to serve your models.
The app platforms can only provide CPU, memory, and disk storage for serving backend services or frontend 
<span class="keep-together">applications.</span></p>
<div data-type="tip"><h6>Tip</h6>
<p>A handful of managed cloud provider AI platforms include Azure Machine Learning Studio or Azure AI, Google Cloud Vertex AI Platform, AWS Bedrock and SageMaker, or IBM Watson Studio.</p>

<p>There are also third-party platforms for hosting your models including Hugging Face Inference Endpoints, Weights &amp; Biases Platform, or Replicate.</p>
</div>

<p>Deploying from code repositories will often require you to add certain configuration files to the root of your project depending on which app platform you will be deploying to.
The process also depends on whether the app platform supports the application runtime, libraries, and framework versions you’re using, so a successful deployment isn’t always guaranteed.
It’s also often challenging to migrate to supported runtimes or versions.</p>

<p>Due to these unforeseen issues, many engineers are switching to containerization technologies such as Docker or Podman to package up and deploy their services.
These containerized applications can then be deployed directly to any app platform supporting containers with guarantees that the application will run no matter what the underlying resources, runtime, or dependency versions are.</p>

<p>Deploying services with containers is now one of the most reliable strategies for shipping your applications to production for users to access.<a data-startref="ix_ch12-asciidoc6" data-type="indexterm" id="id1281"/><a data-startref="ix_ch12-asciidoc5" data-type="indexterm" id="id1282"/></p>
</div></section>








<section data-pdf-bookmark="Deploying with Containers" data-type="sect2"><div class="sect2" id="id210">
<h2>Deploying with Containers</h2>

<p><a data-primary="deployment of AI services" data-secondary="options" data-tertiary="deploying with containers" data-type="indexterm" id="ix_ch12-asciidoc7"/><a data-primary="options, deploying with containers" data-type="indexterm" id="ix_ch12-asciidoc8"/>A <em>container</em> is a loosely isolated environment designed for building and running applications.
Containers can run your services quickly and reliably in any computing environment by packaging your code with all the required dependencies.</p>

<p>Under the hood, containers rely on an OS-virtualization method that enables them to run on physical hardware, in the cloud, on VMs, or across multiple operating 
<span class="keep-together">systems.</span></p>
<div data-type="tip"><h6>Tip</h6>
<p>Similar to managed app platforms and serverless functions, you can configure containers to automatically restart and self-heal, if your application exits for any reason.</p>
</div>

<p><a data-primary="containerization" data-secondary="virtualization versus" data-type="indexterm" id="id1283"/><a data-primary="Docker, containerization with" data-secondary="containerization versus virtualization" data-type="indexterm" id="id1284"/><a data-primary="virtualization, containerization versus" data-type="indexterm" id="id1285"/>Unlike VMs whose underlying technologies rely on virtualization, containers rely on containerization.</p>

<p>Containerization packages applications and their dependencies into lightweight, isolated units that share the host OS kernel.
On the other hand, virtualization enables running multiple operating systems on a single physical machine using hypervisors.
Therefore, unlike virtual machines, containers don’t virtualize hardware resources.
Instead, they run on top of a container runtime platform that abstracts the resources, making them lightweight (i.e., as low as a few megabytes to store) and faster than VMs since they don’t require a separate OS per container.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In essence, virtualization is about abstracting hardware resources on the host machine while containerization is about abstracting the operating system kernel and running all application components inside an isolated unit called a <em>container</em>.</p>
</div>

<p><a data-type="xref" href="#deployment_container_architecture">Figure 12-3</a> compares the virtualization and containerization system architectures.</p>

<figure><div class="figure" id="deployment_container_architecture">
<img alt="bgai 1203" src="assets/bgai_1203.png"/>
<h6><span class="label">Figure 12-3. </span>Comparison of containerization and virtualization system architectures</h6>
</div></figure>

<p>The main benefit from using containers is their <em>portability</em>, <em>boot-up speed</em>, <em>compactness</em>, and <em>reliability</em> across various computing environments, as they don’t require a guest OS and a hypervisor software layer.</p>

<p>This makes them perfect for deploying your services with minimal resources, deployment effort and overheads.
They boot up faster than a VM, and scaling them is also more straightforward.
You can add more containers to <em>horizontally scale</em> your 
<span class="keep-together">services.</span></p>

<p>To help with containerizing your applications, you can rely on platforms such as Docker that have been battle-tested across the MLOps and DevOps communities<a data-startref="ix_ch12-asciidoc8" data-type="indexterm" id="id1286"/><a data-startref="ix_ch12-asciidoc7" data-type="indexterm" id="id1287"/>.<a data-startref="ix_ch12-asciidoc1" data-type="indexterm" id="id1288"/></p>
</div></section>
</div></section>






<section data-pdf-bookmark="Containerization with Docker" data-type="sect1"><div class="sect1" id="id211">
<h1>Containerization with Docker</h1>

<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-type="indexterm" id="ix_ch12-asciidoc9"/><a data-primary="Docker, containerization with" data-type="indexterm" id="ix_ch12-asciidoc10"/>Docker is a containerization platform used to build, ship, and run containers.
At the time of writing, Docker has around <a href="https://oreil.ly/A5x63">22% market share</a> in the virtualization platforms market with more than 9 million developers and <a href="https://oreil.ly/8-wx4">11 billion monthly image downloads</a>, making it the most popular containerization platform.
Many server environments and cloud providers support Docker within many variants of Linux and Windows server.</p>

<p>Chances are if you need to deploy your GenAI services, the easiest and most straightforward option will be to use Docker to containerize your application.
However, to get comfortable with Docker, you need to understand its architecture and the underlying subsystems such as storage and networking.</p>








<section data-pdf-bookmark="Docker Architecture" data-type="sect2"><div class="sect2" id="id266">
<h2>Docker Architecture</h2>

<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="Docker architecture" data-type="indexterm" id="id1289"/><a data-primary="Docker, containerization with" data-secondary="Docker architecture" data-type="indexterm" id="id1290"/>The Docker system is composed of an engine, a client, and a server:</p>
<dl>
<dt>Docker engine</dt>
<dd>
<p>The engine consists of several components including a client and a server running on the same host OS.</p>
</dd>
<dt>Docker client</dt>
<dd>
<p>Docker comes with both a <em>CLI tool</em> named <code>docker</code> and a  graphical user interface (GUI) application called <em>Docker Desktop</em>.
Using the client-server implementation, the Docker client can communicate with the local or a remote server instance using a REST API to manage containers by running commands such as running, stopping, and terminating containers.
You can also use the client to pull images from an image registry.</p>
</dd>
<dt>Docker server</dt>
<dd>
<p>The server is a <em>daemon</em> named <code>dockerd</code>.
The Docker daemon responds to the client HTTP requests via the REST API and can interact with other daemons.
It’s also responsible for tracking the lifecycle of containers.</p>
</dd>
</dl>

<p>The Docker platform also allows you to create and configure objects such as <em>networks</em>, <em>storage volumes</em>, <em>plug-ins</em>, and service objects to support your deployments.</p>

<p>Most important, to containerize your applications with Docker, you’ll need to build Docker images.</p>

<p>A <em>Docker image</em> is a portable package containing software and acts as a recipe for creating and running your application containers.
In essence, a container is an in-memory instance of an image.</p>
<div data-type="tip"><h6>Tip</h6>
<p>A container image is <em>immutable</em>, so once you’ve built one, you can’t change it.
You can only add to an image and not subtract.
You’ll have to re-create a new one if you want to apply changes.</p>
</div>

<p>Docker images are the first step toward containerizing your services as you’ll learn in the next section.</p>
</div></section>








<section data-pdf-bookmark="Building Docker Images" data-type="sect2"><div class="sect2" id="id212">
<h2>Building Docker Images</h2>

<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="building Docker images" data-type="indexterm" id="ix_ch12-asciidoc11"/><a data-primary="Docker, containerization with" data-secondary="building Docker images" data-type="indexterm" id="ix_ch12-asciidoc12"/>Let’s imagine you have a small GenAI service using FastAPI, as shown in <a data-type="xref" href="#docker_app">Example 12-4</a>, that you want to containerize.</p>
<div data-type="example" id="docker_app">
<h5><span class="label">Example 12-4. </span>A simple GenAI FastAPI service</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># main.py</code>

<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code>
<code class="kn">from</code> <code class="nn">models</code> <code class="kn">import</code> <code class="n">generate_text</code> <a class="co" href="#callout_deployment_of_ai_services_CO1-1" id="co_deployment_of_ai_services_CO1-1"><img alt="1" src="assets/1.png"/></a>

<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">(</code><code class="p">)</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"</code><code class="s2">/generate</code><code class="s2">"</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">generate_text</code><code class="p">(</code><code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code><code class="p">:</code>
    <code class="k">return</code> <code class="n">generate_text</code><code class="p">(</code><code class="n">prompt</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_deployment_of_ai_services_CO1-1" id="callout_deployment_of_ai_services_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Assume that the <code>generate_text</code> function is calling a model provider API or an external model server.</p></dd>
</dl></div>

<p><a data-primary="Dockerfile" data-type="indexterm" id="ix_ch12-asciidoc13"/>To build this application into a container image, you’ll need to write instructions in a text file called a <em>Dockerfile</em>. Inside this Dockerfile, you can specify the following 
<span class="keep-together">components:</span></p>

<ul>
<li>
<p>The <em>base</em> image to create a new image from, supplying the OS and environment upon which additional application layers are built</p>
</li>
<li>
<p>Commands to update the guest OS and install additional software</p>
</li>
<li>
<p>Build artifacts to include such as your application code</p>
</li>
<li>
<p>Services to expose like storage and networking configuration</p>
</li>
<li>
<p>The command to run when the container starts</p>
</li>
</ul>

<p><a data-type="xref" href="#containers_dockerfile">Example 12-5</a> illustrates how to build an application image in a Dockerfile.</p>
<div data-type="example" id="containers_dockerfile">
<h5><span class="label">Example 12-5. </span>Dockerfile to containerize a FastAPI application</h5>

<pre data-code-language="dockerfile" data-type="programlisting"><code class="k">ARG</code> <code class="nv">PYTHON_VERSION</code><code class="o">=</code><code class="m">3</code><code>.12</code>
<code class="k">FROM</code> <code class="s">python:${PYTHON_VERSION}-slim</code> <code class="k">as</code> <code class="s">base</code> <a class="co" href="#callout_deployment_of_ai_services_CO2-1" id="co_deployment_of_ai_services_CO2-1"><img alt="1" src="assets/1.png"/></a>

<code class="k">WORKDIR</code> <code class="s">/code </code><a class="co" href="#callout_deployment_of_ai_services_CO2-2" id="co_deployment_of_ai_services_CO2-2"><img alt="2" src="assets/2.png"/></a>

<code class="k">COPY</code> <code>requirements.txt</code> <code>.</code> <a class="co" href="#callout_deployment_of_ai_services_CO2-3" id="co_deployment_of_ai_services_CO2-3"><img alt="3" src="assets/3.png"/></a>

<code class="k">RUN</code> <code>pip</code> <code>install</code> <code>--no-cache-dir</code> <code>--upgrade</code> <code>-r</code> <code>requirements.txt</code> <a class="co" href="#callout_deployment_of_ai_services_CO2-4" id="co_deployment_of_ai_services_CO2-4"><img alt="4" src="assets/4.png"/></a>

<code class="k">COPY</code> <code>.</code> <code>.</code> <a class="co" href="#callout_deployment_of_ai_services_CO2-5" id="co_deployment_of_ai_services_CO2-5"><img alt="5" src="assets/5.png"/></a>

<code class="k">EXPOSE</code> <code class="s">8000 </code><a class="co" href="#callout_deployment_of_ai_services_CO2-6" id="co_deployment_of_ai_services_CO2-6"><img alt="6" src="assets/6.png"/></a>

<code class="k">CMD</code> <code class="p">[</code><code class="s2">"uvicorn"</code><code class="p">,</code> <code class="s2">"main:app"</code><code class="p">,</code> <code class="s2">"--host"</code><code class="p">,</code> <code class="s2">"0.0.0.0"</code><code class="p">,</code> <code class="s2">"--port"</code><code class="p">,</code> <code class="s2">"8000"</code><code class="p">]</code> <a class="co" href="#callout_deployment_of_ai_services_CO2-7" id="co_deployment_of_ai_services_CO2-7"><img alt="7" src="assets/7.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_deployment_of_ai_services_CO2-1" id="callout_deployment_of_ai_services_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Use the official Python 3.12 slim image as the <code>base</code> image.<sup><a data-type="noteref" href="ch12.html#id1291" id="id1291-marker">1</a></sup></p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO2-2" id="callout_deployment_of_ai_services_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Set the working directory inside the container to <code>/code</code>.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO2-3" id="callout_deployment_of_ai_services_CO2-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Copy the <code>requirements.txt</code> file from the host to the current directory in the container.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO2-4" id="callout_deployment_of_ai_services_CO2-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Install the Python dependencies listed in <code>requirements.txt</code> without using the cache.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO2-5" id="callout_deployment_of_ai_services_CO2-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>Copy all files from the host’s current directory to the current directory in the container.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO2-6" id="callout_deployment_of_ai_services_CO2-6"><img alt="6" src="assets/6.png"/></a></dt>
<dd><p>Inform Docker daemon that the application inside the container is listening on <code>8000</code> at runtime.
The <code>EXPOSE</code> command doesn’t automatically map or allow access on ports.<sup><a data-type="noteref" href="ch12.html#id1292" id="id1292-marker">2</a></sup></p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO2-7" id="callout_deployment_of_ai_services_CO2-7"><img alt="7" src="assets/7.png"/></a></dt>
<dd><p>Run the <code>uvicorn</code> server with the application module and host/port configuration, when container is launched.</p></dd>
</dl></div>

<p>We won’t be covering the full <a href="https://oreil.ly/8fJ6l">Dockerfile specification</a> in this chapter.
However, notice how each command changes the image structure that enables you to run your full GenAI services within a container.</p>

<p>You can use the <code>docker build</code> command to build the image in <a data-type="xref" href="#containers_dockerfile">Example 12-5</a>:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker build -t genai-service .<code class="w"/></pre>

<p>Notice the steps listed in the output.
When each step executes, a new layer gets added to the image you’re building.</p>

<p>Once you have a container image, you can then use container registries to store, share, and download images<a data-startref="ix_ch12-asciidoc13" data-type="indexterm" id="id1293"/>.<a data-startref="ix_ch12-asciidoc12" data-type="indexterm" id="id1294"/><a data-startref="ix_ch12-asciidoc11" data-type="indexterm" id="id1295"/></p>
</div></section>








<section data-pdf-bookmark="Container Registries" data-type="sect2"><div class="sect2" id="id213">
<h2>Container Registries</h2>

<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="container registries" data-type="indexterm" id="ix_ch12-asciidoc14"/><a data-primary="Docker, containerization with" data-secondary="container registries" data-type="indexterm" id="ix_ch12-asciidoc15"/>To store and distribute images in a version-controlled environment, you can use <em>container registries</em>, which include both the public or private flavors.</p>

<p><a data-primary="Docker Hub" data-type="indexterm" id="ix_ch12-asciidoc16"/><em>Docker Hub</em> is a managed software-as-a-service (SaaS) container registry for storing and distributing images you create.</p>

<p>Docker Hub is public by default.
However, you can also use self-hosted or cloud provider private registries such as Azure Container Registry (ACR), AWS Elastic Container Registry (ECR), or Google Cloud Artifact Registry.</p>

<p>You can view the full Docker platform system architecture in <a data-type="xref" href="#docker_architecture">Figure 12-4</a>.</p>

<figure><div class="figure" id="docker_architecture">
<img alt="bgai 1204" src="assets/bgai_1204.png"/>
<h6><span class="label">Figure 12-4. </span>Docker platform system architecture</h6>
</div></figure>

<p class="less_space pagebreak-before">As you can see in <a data-type="xref" href="#docker_architecture">Figure 12-4</a>, the Docker daemon manages containers and images.
It creates containers from images and communicates with the Docker client, handling commands to build and run images.
The Docker daemon can also pull images from or push them to a registry (e.g., Docker Hub) that contains  images like Ubuntu, Redis, or PostgreSQL.</p>

<p>Using the Docker Hub registry, you can access other contributed images alongside distributing and version controlling your own.
Registries like Docker Hub play a crucial role in scaling your services as container orchestration platforms like Kubernetes need access to registries to pull and run multiple container instances from images.</p>

<p>You can pull public images from Docker Hub using the <code>docker pull</code> command:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker image pull python:3.12-slim<code class="w"/>

bookworm: Pulling from library/python<code class="w"/>
Digest: sha256:3f1d6c17773a45c97bd8f158d665c9709d7b29ed7917ac934086ad96f92e4510<code class="w"/>
Status: Downloaded newer image <code class="k">for</code> python:3.12-slim<code class="w"/>
docker.io/library/python:3.12-slim<code class="w"/></pre>

<p>When you push and pull images, you’ll need to specify a <em>tag</em> using the <code>&lt;name&gt;:&lt;tag&gt;</code> syntax.
If you don’t provide a tag, Docker engine will use the <code>latest</code> tag by default.</p>

<p>Aside from pulling, you can also store your own images in container registries. First, you need to build and tag your image with both a version label and the image repository URL:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker build -t genai-service:latest .<code class="w"/>

$ docker image tag genai-service:latest docker.io/myrepo/genai-service:latest<code class="w"/></pre>

<p>Once your image is built and tagged, you can then push it to the Docker Hub container registry using the <code>docker push</code> command.
You may need to log in first to authenticate with the hub:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker login<code class="w"/>

$ docker image push docker.io/myrepo/genai-service:latest<code class="w"/>

195be5f8be1d: Pushed<code class="w"/></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Be careful that during a push, you don’t overwrite the tag for an image in many repositories.
For instance, an image built and tagged <code>genai:latest</code> in a repository can be overwritten by another image tagged <code>genai:latest</code>.<a data-startref="ix_ch12-asciidoc16" data-type="indexterm" id="id1296"/></p>
</div>

<p>Now that your image is stored in the registry, you can pull it down on another machine<sup><a data-type="noteref" href="ch12.html#id1297" id="id1297-marker">3</a></sup> or at a later time to run the image without the need to rebuild it.<a data-startref="ix_ch12-asciidoc15" data-type="indexterm" id="id1298"/><a data-startref="ix_ch12-asciidoc14" data-type="indexterm" id="id1299"/></p>
</div></section>








<section data-pdf-bookmark="Container Filesystem and Docker Layers" data-type="sect2"><div class="sect2" id="id214">
<h2>Container Filesystem and Docker Layers</h2>

<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="container filesystem and Docker layers" data-type="indexterm" id="ix_ch12-asciidoc17"/><a data-primary="Docker, containerization with" data-secondary="container filesystem and Docker layers" data-type="indexterm" id="ix_ch12-asciidoc18"/><a data-primary="Unionfs" data-type="indexterm" id="ix_ch12-asciidoc19"/>When <a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="Unionfs" data-type="indexterm" id="id1300"/><a data-primary="Docker, containerization with" data-secondary="Unionfs" data-type="indexterm" id="id1301"/>building the image, Docker uses a special filesystem called the <code>Unionfs</code> (stackable unification filesystem) to merge the contents of several directories (i.e., <em>branches</em> or in Docker terminology <em>layers</em>), while keeping their physical content separate.</p>

<p>Using <code>Unionfs</code>, directories of distinct filesystems can be combined and overlaid to form a single coherent virtual filesystem, as shown in <a data-type="xref" href="#docker_unionfs">Figure 12-5</a>.</p>

<figure><div class="figure" id="docker_unionfs">
<img alt="bgai 1205" src="assets/bgai_1205.png"/>
<h6><span class="label">Figure 12-5. </span>Unified virtual filesystem from multiple filesystems</h6>
</div></figure>

<p>Using the <code>Unionfs</code>, Docker can add or remove branches as you build out your container filesystem from an image.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1302">
<h1>Why Do Docker Containers Use Unionfs?</h1>
<p>Imagine you have 10 container instances running from a 1 GB image.</p>

<p>A container is essentially a process. In Linux, new processes are created by forking existing ones.
The fork operation generates a separate address space for the child process, which contains an exact copy of the parent’s memory segments.
To create a new container, all files from the image layers are copied into the container’s namespace.</p>

<p>If your containers use a concrete filesystem, you’ll end up needing 10 GB of physical memory to run them.
So, your disk space usage won’t be optimized.
In addition, since you want your containers to start fast (ideally within a second), having to copy 1 GB of files from image layers to container namespace would significantly increase the cold start time.</p>

<p>Therefore, you’d need a mechanism to share physical memory segments across containers efficiently.
That’s why Unionfs is now used in containers to give a unified view to files and directories of separate filesystems.</p>

<p>In essence, it mounts multiple directories to a single root, so you can think of it as a mounting mechanism rather than a filesystem.</p>
</div></aside>

<p>To illustrate the mechanism of layered architecture in containers, let’s review the image from <a data-type="xref" href="#containers_dockerfile">Example 12-5</a>.</p>

<p>When building the image using <a data-type="xref" href="#containers_dockerfile">Example 12-5</a>, you’re layering a Python 3.12 base image running on a Linux distribution on top of a root filesystem.
Next, you’re adding <em>requirements.txt</em> on top of the Python base image and then installing dependencies on top of <em>requirements.txt</em> layer.
You then add a new layer by coping the content of your project directory into the container, layering it on top of everything else.
Finally, when you start the container with the <code>uvicorn</code> command, you add a final writable layer as part of the container filesystem.
As a result, the ordering of layers becomes important when building Docker images.</p>

<p><a data-type="xref" href="#docker_branches">Figure 12-6</a> shows the layered filesystem architecture.</p>

<figure><div class="figure" id="docker_branches">
<img alt="bgai 1206" src="assets/bgai_1206.png"/>
<h6><span class="label">Figure 12-6. </span>Layered Unionfs filesystem architecture</h6>
</div></figure>

<p>In <a data-type="xref" href="#containers_dockerfile">Example 12-5</a>, each of the command steps is creating a cached image as the build process finalizes the container image.
To run commands, intermediate containers are created and then automatically deleted after.
The underlying cached image is kept on the build host and isn’t removed.
These temporary images are layered over the previous image and combined into a single image once all steps are completed.
This optimization allows future builds to reuse these images to speed up build times.</p>

<p>At the end, the container will comprise one or more image layers and a final ephemeral container layer (i.e., that won’t be persisted) when the container is destroyed.<a data-startref="ix_ch12-asciidoc19" data-type="indexterm" id="id1303"/><a data-startref="ix_ch12-asciidoc18" data-type="indexterm" id="id1304"/><a data-startref="ix_ch12-asciidoc17" data-type="indexterm" id="id1305"/></p>
</div></section>








<section data-pdf-bookmark="Docker Storage" data-type="sect2"><div class="sect2" id="id215">
<h2>Docker Storage</h2>

<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="Docker storage mechanisms" data-type="indexterm" id="ix_ch12-asciidoc20"/><a data-primary="Docker, containerization with" data-secondary="Docker storage mechanisms" data-type="indexterm" id="ix_ch12-asciidoc21"/>In this section, you will learn about various Docker storage mechanisms.
During the development of your services as containers, you can use these tools to manage data persistence, sharing data between containers and maintaining state between container restarts.</p>

<p><a data-primary="ephemeral storage" data-type="indexterm" id="id1306"/>When working with containers, your application may need to write data to the disk, which will persist in an <em>ephemeral</em> storage.
Ephemeral storage is a short-lived, temporary storage deleted once the container is stopped, restarted, or removed.
If you restart your container, you’ll notice that previously persisted data is no longer available.
Under the hood, Docker writes the runtime data to an ephemeral writable container layer in the container’s virtual filesystem.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>You’ll lose all your application generated data and log files you’ve written to disk during a container’s runtime if you rely on the container’s default storage configuration.</p>
</div>

<p>To avoid loss of application runtime data and logs, you have several storage options available that enable you to persist data during a container’s lifetime.
During development, you can use <em>volumes</em> or <em>bind mounts</em> to persist data to the host OS filesystem or rely on local databases for persisting data.</p>

<p><a data-type="xref" href="#docker_storage_options">Table 12-1</a> shows the Docker storage mount options.</p>
<table class="striped" id="docker_storage_options">
<caption><span class="label">Table 12-1. </span>Docker storage mounts</caption>
<thead>
<tr>
<th>Storage</th>
<th>Description</th>
<th>Use cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Volumes</p></td>
<td><p>I/O optimized and preferred storage solution. Managed by Docker and stored in a specific location on the host but decoupled from the host filesystem structure.</p></td>
<td><p>If you need to store and share data across multiple containers.</p><p>If you don’t need to modify files or directories from the host.</p></td>
</tr>
<tr>
<td><p>Bind mounts</p></td>
<td><p>Mount files or directories on host into the container but have limited functionality compared to volumes.</p></td>
<td><p>If you want both containers and host processes to access and modify host’s files and directories. For instance, during local development and testing.</p></td>
</tr>
<tr>
<td><p>Temporary (tmpfs) mounts</p></td>
<td><p>Stores data in the host’s memory (RAM) and never written to the container or host’s filesystem.</p></td>
<td><p>If you need high-performance temporary storage for sensitive or nonstateful data that won’t persist after the container stops.</p></td>
</tr>
</tbody>
</table>

<p><a data-type="xref" href="#docker_storage_mounts">Figure 12-7</a> shows the different types of mounts.</p>

<figure><div class="figure" id="docker_storage_mounts">
<img alt="bgai 1207" src="assets/bgai_1207.png"/>
<h6><span class="label">Figure 12-7. </span>Docker storage mounts</h6>
</div></figure>

<p>We’ll now study each storage option in detail so you can simulate your production environment locally with Docker containers using the appropriate storage.
When deploying containers to production within a cloud environment, you can use a database or cloud storage offering for persisting data instead of Docker volumes or bind mounts to centralize storage across multiple containers.</p>










<section data-pdf-bookmark="Docker volumes" data-type="sect3"><div class="sect3" id="id216">
<h3>Docker volumes</h3>

<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="Docker volumes" data-type="indexterm" id="id1307"/><a data-primary="Docker, containerization with" data-secondary="Docker storage mechanisms" data-tertiary="Docker volumes" data-type="indexterm" id="id1308"/><a data-primary="volumes (Docker)" data-type="indexterm" id="id1309"/>Docker allows you to create isolated <em>volumes</em> for persisting application data between container runtimes. To create a volume, you can run the following command:</p>

<pre data-type="programlisting">$ docker volume create -n data</pre>

<p>Once created, you can use volumes to persist data between container runs.  Volumes also allow you to persist data when you use database and memory store containers.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Restarting a database container with new environment variables may not be enough to reset them with new settings.</p>

<p>Some database systems may require you to re-create the container volume if you need to update settings like administrator user 
<span class="keep-together">credentials.</span></p>
</div>

<p>By default, any volumes you create will be stored on the host machine filesystem until you explicitly remove them via the <code>docker volume remove</code> command.</p>
</div></section>










<section data-pdf-bookmark="Bind mounts" data-type="sect3"><div class="sect3" id="id217">
<h3>Bind mounts</h3>

<p><a data-primary="bind mounts (Docker)" data-type="indexterm" id="id1310"/><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="bind mounts" data-type="indexterm" id="id1311"/><a data-primary="Docker, containerization with" data-secondary="Docker storage mechanisms" data-tertiary="bind mounts" data-type="indexterm" id="id1312"/>In addition to volumes, you can also use filesystem mappings via volume <em>bind mounts</em> that map directories residing on the host filesystem to the container filesystem, as shown in <a data-type="xref" href="#docker_bind_mounts">Figure 12-8</a>.</p>

<figure><div class="figure" id="docker_bind_mounts">
<img alt="bgai 1208" src="assets/bgai_1208.png"/>
<h6><span class="label">Figure 12-8. </span>Bind mounts between host filesystem and a container</h6>
</div></figure>

<p>The mounts happen as you start your container.
With the mounted directories, you can then directly access them from within the 
<span class="keep-together">container.</span>
You can read and persist data to the mounted directories as you run and stop your containers.</p>

<p>To run a container with a volume bind mount, you can use the following command:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker run -v src:/app genai-service<code class="w"/></pre>

<p>Here, the <code>-v</code> flag allows you to map the host directory to a container directory using the <code>&lt;host_dir&gt;:&lt;container_dir&gt;</code> syntax.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>The functionality of the <code>COPY</code> command you use in a Dockerfile is different from directory mounting.</p>

<p>The former makes a separate copy of a host directory into the container during the image build process while the latter allows you to access and update the mapped host directory from within the 
<span class="keep-together">container.</span></p>

<p>This means that if you’re not careful, you can unintentionally modify or delete all your original files on the host machine permanently, from within the container.</p>
</div>

<p>Bind mount volumes can still be useful in a local development environment.
As you change the source code of your services, you’ll be able to observe the real-time impact of modifications on the running application containers.</p>
</div></section>










<section data-pdf-bookmark="Temporary mounts (tmpfs)" data-type="sect3"><div class="sect3" id="id218">
<h3>Temporary mounts (tmpfs)</h3>

<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="temporary mounts" data-type="indexterm" id="id1313"/><a data-primary="Docker, containerization with" data-secondary="Docker storage mechanisms" data-tertiary="temporary mounts" data-type="indexterm" id="id1314"/><a data-primary="temporary (tmpfs) mounts" data-type="indexterm" id="id1315"/><a data-primary="tmpfs (temporary) mounts" data-type="indexterm" id="id1316"/>If you have some nonpersistent data such as model caches or sensitive files that you don’t need to store permanently, you should consider using temporary <em>tmpfs mounts</em>.</p>

<p>This temporary mount will only persist the data to the host memory (RAM) during the container’s runtime and increases the container’s performance by avoiding writes into the container’s writeable layer.</p>

<p>When containerizing GenAI applications, you can use temporary mounts to store cached results, intermediate model computations, temporary files, and session-specific logs that you won’t need once the container stops.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The container’s writeable layer is tightly coupled with the host machine through a storage driver to implement the union filesystem.
Therefore, writing to the container’s writable layer reduces performance due to this additional layer of abstraction.</p>

<p>Instead, you can use data volumes for persistent storage that writes directly to the host filesystem or tmpfs mounts for temporary in-memory storage.</p>
</div>

<p>Unlike bind mounts and volumes, you can’t share the tmpfs mount between containers, and the functionality is available only on Linux systems.
In addition, if you adjust directory permissions on tmpfs mounts, they can reset when the container restarts.</p>

<p class="less_space pagebreak-before">Here are a few other use cases of tmpfs mounts:</p>

<ul>
<li>
<p>Temporarily storing data caches, API responses, logs, test data, configuration files, and AI model artifacts in-memory</p>
</li>
<li>
<p>Avoiding I/O writes to disks while working with library APIs that require file-like objects</p>
</li>
<li>
<p>Simulating high-speed I/O with rapid file access and writes</p>
</li>
<li>
<p>Preventing excessive or unnecessary disk writes if you need temporary 
<span class="keep-together">directories</span></p>
</li>
</ul>

<p>To set a tmpfs mount, you can use the following command:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker run --tmpfs /cache genai-service<code class="w"/></pre>

<p>Here, you are setting a tmpfs mount on the <code>/cache</code> directory for model caches, which will cease to exist once the container stops.</p>
</div></section>










<section data-pdf-bookmark="Handling filesystem permissions" data-type="sect3"><div class="sect3" id="id219">
<h3>Handling filesystem permissions</h3>

<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="handling filesystem permissions" data-type="indexterm" id="ix_ch12-asciidoc22"/><a data-primary="Docker, containerization with" data-secondary="Docker storage mechanisms" data-tertiary="handling filesystem permissions" data-type="indexterm" id="ix_ch12-asciidoc23"/>A big source of frustration and a security consideration for many developers new to Docker is managing directory permissions when using filesystem bind mounts between the host OS and the container.</p>

<p>By default, Docker runs containers as the <code>root</code> user leading to containers having full read/write access to mounted directories on the host OS.
If the <code>root</code> user inside the container creates directories or files, they will be owned by <code>root</code> on the host as well.
You can then face permission issues if you have a nonroot user account on the host when you try to access or modify these directories or files.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Running containers as the default <code>root</code> user is also a great security risk if a malicious actor gets access to the container since they’ll have access to the host system as <code>root</code>.
Additionally, if you run a compromised image, you might risk executing malicious code on your host system with <code>root</code> privileges.</p>
</div>

<p>To mitigate permission issues when running containers with bind mounts, you can use the <code>--user</code> flag to run the container as a nonroot user:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker run --user genai-service<code class="w"/></pre>

<p>Alternatively, you can create and switch to a nonroot user within the final layers of the image build inside the Dockerfile, as shown in <a data-type="xref" href="#docker_permissions">Example 12-6</a>.</p>
<div data-type="example" id="docker_permissions">
<h5><span class="label">Example 12-6. </span>Creating and switching to nonroot user when building container images (Ubuntu/Debian containers only)</h5>

<pre data-code-language="dockerfile" data-type="programlisting"><code class="k">ARG</code> <code class="nv">USERNAME</code><code class="o">=</code><code>fastapi</code> <a class="co" href="#callout_deployment_of_ai_services_CO3-1" id="co_deployment_of_ai_services_CO3-1"><img alt="1" src="assets/1.png"/></a>
<code class="k">ARG</code> <code class="nv">USER_UID</code><code class="o">=</code><code class="m">1001</code>
<code class="k">ARG</code> <code class="nv">USER_GID</code><code class="o">=</code><code class="m">1002</code>

<code class="k">RUN</code> <code>groupadd</code> <code>--gid</code> <code class="nv">$USER_GID</code> <code class="nv">$USERNAME</code> <code class="se">\ </code><a class="co" href="#callout_deployment_of_ai_services_CO3-2" id="co_deployment_of_ai_services_CO3-2"><img alt="2" src="assets/2.png"/></a>
    <code class="o">&amp;&amp;</code> <code>adduser</code> <code class="se">\
</code>    <code>--disabled-password</code> <code class="se">\
</code>    <code>--shell</code> <code class="s2">"/sbin/nologin"</code> <code class="se">\ </code><a class="co" href="#callout_deployment_of_ai_services_CO3-3" id="co_deployment_of_ai_services_CO3-3"><img alt="3" src="assets/3.png"/></a>
    <code>--gecos</code> <code class="s2">""</code> <code class="se">\
</code>    <code>--home</code> <code class="s2">"/nonexistent"</code> <code class="se">\
</code>    <code>--no-create-home</code> <code class="se">\ </code><a class="co" href="#callout_deployment_of_ai_services_CO3-4" id="co_deployment_of_ai_services_CO3-4"><img alt="4" src="assets/4.png"/></a>
    <code>--uid</code> <code class="s2">"</code><code class="si">${</code><code class="nv">UID</code><code class="si">}</code><code class="s2">"</code> <code class="se">\
</code>    <code>--gid</code> <code class="nv">$USER_GID</code>
    <code class="nv">$USERNAME</code> <a class="co" href="#callout_deployment_of_ai_services_CO3-5" id="co_deployment_of_ai_services_CO3-5"><img alt="5" src="assets/5.png"/></a>

<code class="k">USER</code> <code class="s">$USERNAME </code><a class="co" href="#callout_deployment_of_ai_services_CO3-6" id="co_deployment_of_ai_services_CO3-6"><img alt="6" src="assets/6.png"/></a>

<code class="k">CMD</code> <code class="p">[</code><code class="s2">"uvicorn"</code><code class="p">,</code> <code class="s2">"main:app"</code><code class="p">,</code> <code class="s2">"--host"</code><code class="p">,</code> <code class="s2">"0.0.0.0"</code><code class="p">,</code> <code class="s2">"--port"</code><code class="p">,</code> <code class="s2">"8000"</code><code class="p">]</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_deployment_of_ai_services_CO3-1" id="callout_deployment_of_ai_services_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Use build arguments to specify variables during the image build.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO3-2" id="callout_deployment_of_ai_services_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Create a user group with the given <code>USER_GID</code>.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO3-3" id="callout_deployment_of_ai_services_CO3-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Disable user login completely including password-based login.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO3-4" id="callout_deployment_of_ai_services_CO3-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Avoid creating a home directory for the user.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO3-5" id="callout_deployment_of_ai_services_CO3-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>Create a nonroot user account with the given <code>$USER_UID</code> and assign it to the newly created <code>USER_GID</code> group.
Set the name of the user account to <code>fastapi</code>.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO3-6" id="callout_deployment_of_ai_services_CO3-6"><img alt="6" src="assets/6.png"/></a></dt>
<dd><p>Switch to the nonroot <code>fastapi</code> user.</p></dd>
</dl></div>
<div data-type="tip"><h6>Tip</h6>
<p>Often, you’ll need to install packages or add configurations that require privileged disk access or permissions.
You should only switch to a nonroot user at the end of an image build once you’ve completed such installations and configurations.
Avoid switching back and forth between root and nonroot users to prevent any unnecessary complexity and excess image layers.</p>
</div>

<p>If you hit issues with creating new groups or users in <a data-type="xref" href="#docker_permissions">Example 12-6</a>, try changing the <code>USER_UID</code> and <code>USER_GID</code> as those IDs may already be in use by another nonroot user in the image.</p>

<p>Let’s assume that during the image creation, the <code>root</code> user in the container has created the <code>myscripts</code> folder.
You can inspect filesystem permissions using the <code>ls -l</code> command, which returns the following output:</p>

<pre data-code-language="bash" data-type="programlisting">total <code class="m">12</code><code class="w"/>
drw-r--r-- <code class="m">2</code> root root <code class="m">4096</code> Oct  <code class="m">1</code> <code class="m">10</code>:00 myscripts<code class="w"/></pre>

<p>You can read permissions <code>drwxr-xr-x</code> for the <code>myscripts</code> directory using the following breakdown:</p>

<ul>
<li>
<p><code>d</code>: Specifies that <code>myscripts</code> is a directory; otherwise would show a <code>-</code>.</p>
</li>
<li>
<p><code>rwx</code>: Owner <code>root</code> user can (r)read, (w)rite, and e(x)ecute files in this 
<span class="keep-together">directory.</span></p>
</li>
<li>
<p><code>r--</code>: Group <code>root</code> members can perform (r)ead-only operations but can’t write or execute any files.</p>
</li>
<li>
<p><code>r--</code>: Everyone else can read the file but cannot write to or execute it.<sup><a data-type="noteref" href="ch12.html#id1317" id="id1317-marker">4</a></sup></p>
</li>
</ul>

<p>If you want to set ownership or permissions on the <code>myscripts</code> directory, you can use the <code>chmod</code> or <code>chown</code> commands in Linux systems.</p>

<p>Use the <code>chown</code> command to change the directory owner on host so that you can edit the files in your code editor:</p>

<pre data-code-language="bash" data-type="programlisting"><code class="c1"># Set file or directory ownership</code>
$ sudo chown -R username:groupname mydir<code class="w"/></pre>

<p>Alternatively, if you only need to execute the scripts in the <code>myscripts</code> directory, use the <code>chmod</code> command to change the file or directory permissions:</p>

<pre data-code-language="bash" data-type="programlisting"><code class="c1"># Set execute permissions using flags</code>
$ sudo chmod -R +x myscripts<code class="w"/>

<code class="c1"># Set execute permissions in a numeric form</code>
$ sudo chmod -R <code class="m">755</code> myscripts<code class="w"/></pre>
<div data-type="tip"><h6>Tip</h6>
<p>The <code>-R</code> flag will recursively set the ownership or permissions on a nested directory.</p>
</div>

<p>This command will allow <code>root</code> group members and other users to execute files in the <code>myscripts</code> directory.
Others can execute the files only if they use the <code>bash</code> command.
However, only the owner can modify them.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1318">
<h1>Interpreting Linux Filesystem Permissions</h1>
<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="interpreting Linux filesystem permissions" data-type="indexterm" id="id1319"/><a data-primary="Docker, containerization with" data-secondary="Docker storage mechanisms" data-tertiary="interpreting Linux filesystem permissions" data-type="indexterm" id="id1320"/><a data-primary="Linux filesystem permissions" data-type="indexterm" id="id1321"/>The <code>chmod</code> command uses a three-digit octal number to set file permissions.
Each digit represents the permissions for the <em>owner</em>, <em>group</em>, and <em>others</em> (i.e., everyone else), respectively.</p>

<p>For example, <code>chmod 755</code> sets the following:</p>

<ul>
<li>
<p>Owner: <code>rwx</code> (7)</p>
</li>
<li>
<p>Group: <code>r-x</code> (5)</p>
</li>
<li>
<p>Others: <code>r-x</code> (5)</p>
</li>
</ul>

<p><a data-type="xref" href="#docker_linux_permissionss">Table 12-2</a> shows the Linux permissions table to use as a handy reference when working with <code>chmod</code> commands.</p>
<table class="striped" id="docker_linux_permissionss">
<caption><span class="label">Table 12-2. </span>Linux filesystem permissions</caption>
<thead>
<tr>
<th>Numeric value</th>
<th>Symbol</th>
<th>Permissions</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>7</p></td>
<td><p>rwx</p></td>
<td><p>(r)ead, (w)rite, and e(x)ecute</p></td>
</tr>
<tr>
<td><p>6</p></td>
<td><p>rw-</p></td>
<td><p>(r)ead and (w)rite</p></td>
</tr>
<tr>
<td><p>5</p></td>
<td><p>r-x</p></td>
<td><p>(r)ead and e(x)ecute</p></td>
</tr>
<tr>
<td><p>4</p></td>
<td><p>r--</p></td>
<td><p>(r)ead-only</p></td>
</tr>
<tr>
<td><p>3</p></td>
<td><p>-wx</p></td>
<td><p>(w)rite and e(x)ecute</p></td>
</tr>
<tr>
<td><p>2</p></td>
<td><p>-w-</p></td>
<td><p>(w)rite-only</p></td>
</tr>
<tr>
<td><p>1</p></td>
<td><p>--x</p></td>
<td><p>e(x)ecute-only</p></td>
</tr>
<tr>
<td><p>0</p></td>
<td><p>---</p></td>
<td><p>None</p></td>
</tr>
</tbody>
</table>

<p>You can use <a data-type="xref" href="#docker_linux_permissionss">Table 12-2</a> as a reference to troubleshoot any permission-related issues.</p>
</div></aside>

<p>If you inspect the filesystem permissions again using <code>ls -l</code>, you’ll see the following output:</p>

<pre data-code-language="bash" data-type="programlisting">total <code class="m">12</code><code class="w"/>
drwxr-xr-x <code class="m">2</code> root root <code class="m">4096</code> Oct  <code class="m">1</code> <code class="m">10</code>:00 myscripts<code class="w"/></pre>

<ul>
<li>
<p><code>rwx</code>: Owner <code>root</code> user can still (r)read, (w)rite, and e(x)ecute files in this 
<span class="keep-together">directory.</span></p>
</li>
<li>
<p><code>r-x</code>: Group <code>root</code> members can perform (r)ead and e(x)ecute operations but can’t modify any files.</p>
</li>
<li>
<p><code>r-x</code>: Anyone else can’t modify files in <code>myscripts</code> directory but can read and execute them.</p>
</li>
</ul>

<p>You can use <a data-type="xref" href="#docker_permissions_execute">Example 12-7</a> to set permissions when creating directories inside an image.</p>
<div data-type="example" id="docker_permissions_execute">
<h5><span class="label">Example 12-7. </span>Creating scripts folder and allowing files to be executed (Ubuntu/Debian containers only)</h5>

<pre data-code-language="dockerfile" data-type="programlisting"><code class="k">RUN</code> mkdir -p scripts<code class="w"/>

<code class="k">COPY</code> scripts scripts<code class="w"/>

<code class="k">RUN</code> chmod -R +x scripts<code class="w"/></pre></div>

<p>The instructions in <a data-type="xref" href="#docker_permissions_execute">Example 12-7</a> will allow you to configure permissions to execute files in the <code>scripts</code> directory from within the container.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>When using container volumes, be careful with mount bindings as they replace the permissions inside the container with those from the host filesystem.</p>
</div>

<p>The most frustrating issues when working with containers will be related to filesystem permissions.
Therefore, knowing how to set and correct file permissions will save you hours of development when working with containers that produce or modify artifacts on the host machine<a data-startref="ix_ch12-asciidoc23" data-type="indexterm" id="id1322"/><a data-startref="ix_ch12-asciidoc22" data-type="indexterm" id="id1323"/>.<a data-startref="ix_ch12-asciidoc21" data-type="indexterm" id="id1324"/><a data-startref="ix_ch12-asciidoc20" data-type="indexterm" id="id1325"/></p>
</div></section>
</div></section>








<section data-pdf-bookmark="Docker Networking" data-type="sect2"><div class="sect2" id="id220">
<h2>Docker Networking</h2>

<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="Docker networking" data-type="indexterm" id="ix_ch12-asciidoc24"/><a data-primary="Docker, containerization with" data-secondary="Docker networking" data-type="indexterm" id="ix_ch12-asciidoc25"/>Docker networking is one of the hardest concepts to grasp in multicontainer projects.
This section covers how Docker networking works and how to set up local containers to communicate, simulating production environments during development.</p>

<p>Often, when you’re deploying to production environments in the cloud, you configure networking using the cloud provider’s solutions.
However, if you need to connect containers in a development environment for local testing or deploying on on-premises resources, then you’ll benefit from understanding how Docker networking works.</p>

<p>If you’re developing GenAI services that interact with external systems like databases, chances are you’ll be using multiple containers;
one for your application and one for running each of your databases or external systems.</p>

<p>Docker ships with a networking subsystem that allows containers to connect with each other on the same or different hosts.
You can even connect containers via internet-facing hosts.</p>

<p>When you create containers using the <code>docker run</code> command, they’ll have networking enabled by default on a <em>bridge network</em> so that they can make outgoing connections.
However, they won’t expose or publish their ports to the outside world.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p><a data-primary="firewall rules" data-type="indexterm" id="id1326"/>With the default settings, Docker interacts with the OS kernels to configure <em>firewall rules</em> (e.g., <code>iptables</code> and <code>ip6tables</code> rules on Linux) to implement network isolation, port publishing, and 
<span class="keep-together">filtering.</span></p>

<p>Since Docker can override these firewall rules, if you have a port on host like <code>8000</code> closed, Docker can force it open and expose it outside the host machine when you run a container with the <code>-p 8000:8000</code> flag.
To prevent such an exposure, a solution is to run containers using <code>-p 127.0.0.1:8000:8000</code>.</p>
</div>

<p>For the networking subsystem to function, Docker uses <em>networking drivers</em>, as shown in <a data-type="xref" href="#docker_networking_drivers">Table 12-3</a>.</p>
<table class="striped" id="docker_networking_drivers">
<caption><span class="label">Table 12-3. </span>Docker networking drivers</caption>
<thead>
<tr>
<th>Driver</th>
<th>Description</th>
<th>Use case</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Bridge (default)</p></td>
<td><p>Connects containers running on the same Docker daemon host. User-defined networks can leverage an embedded DNS server.</p></td>
<td><p>Control container communication in isolated Docker networks with a simple setup.</p></td>
</tr>
<tr>
<td><p>Host</p></td>
<td><p>Removes the isolation layer between containers and the host system, so any TCP/UDP connections are accessible directly via host network such as the localhost without the need to publish ports.</p></td>
<td><p>Simplify access to container from the host network (e.g., localhost) or when a container needs to handle a large range of ports.</p></td>
</tr>
<tr>
<td><p>None</p></td>
<td><p>Disables all networking services and isolates running containers within the Docker environment.</p></td>
<td><p>Isolate containers from any Docker and non-Docker process for security reasons. Network debugging or simulating outages. Resource isolation and transient containers for short-lived processes.</p></td>
</tr>
<tr>
<td><p>Overlay</p></td>
<td><p>Connects containers across multiple hosts/engines or in a <em>Docker Swarm</em> cluster.</p><p><strong>Note:</strong> Docker engine has <em>swarm</em> mode that enables container orchestration via <em>clusters</em> of Docker daemons/engines.</p></td>
<td><p>Remove the need for OS-level routing when connecting containers across Docker hosts.</p></td>
</tr>
<tr>
<td><p>Macvlan</p></td>
<td><p>Assigns mac addresses to containers as if they’re physical devices.</p><p>Misconfiguration may lead to unintentional degradation of your network due to IP address exhaustion, leading to VLAN spread (large number of mac addresses) or promiscuous mode (overlapping addresses).</p></td>
<td><p>Used in legacy systems or applications that monitor network traffic that expect to be directly connected to a physical network.</p></td>
</tr>
<tr>
<td><p>IPVlan</p></td>
<td><p>Gives you total control over container IPv4 and IPv6 addressing, providing easy access to external services with no need for port mappings.</p></td>
<td><p>Advanced networking setup that bypasses the traditional Linux bridge for isolation, enhanced performance and simplified networking topology.</p></td>
</tr>
</tbody>
</table>

<p>To ensure your containers can communicate together, you may need to specify networking settings and drivers.
You can select a networking driver that matches your use case based on <a data-type="xref" href="#docker_networking_drivers">Table 12-3</a>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Some of these drivers may not be available depending on the platform/host OS you’re running Docker on (Windows, Linux, or macOS host).</p>
</div>

<p>The most commonly used network drivers are bridge, host, and none.
You likely won’t need to use other drivers (e.g., overlay, Macvlan, IPVlan) unless you need more advanced networking configurations.</p>

<p><a data-type="xref" href="#docker_networking_drivers_viz">Figure 12-9</a> visualizes the functionality of the bridge, host, none, overlay, Macvlan, and IPVlan drivers.</p>

<figure><div class="figure" id="docker_networking_drivers_viz">
<img alt="bgai 1209" src="assets/bgai_1209.png"/>
<h6><span class="label">Figure 12-9. </span>Docker networking drivers</h6>
</div></figure>

<p>Let’s explore these networking drivers in more detail.</p>










<section data-pdf-bookmark="Bridge network driver" data-type="sect3"><div class="sect3" id="id275">
<h3>Bridge network driver</h3>

<p><a data-primary="Docker, containerization with" data-secondary="Docker networking" data-tertiary="bridge network driver" data-type="indexterm" id="ix_ch12-asciidoc26"/>The bridge network driver connects containers by creating a default bridge network <code>docker0</code> and associating containers with it and the host’s main network interface, unless otherwise specified.
This will allow your containers to access the host network (and the internet) plus allow you to access the containers.</p>

<p>You can view the networks using the <code>docker network ls</code> command:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker network ls<code class="w"/>
NETWORK ID     NAME      DRIVER    SCOPE<code class="w"/>
72ec0b2e6034   bridge    bridge    <code class="nb">local</code><code class="w"/>
53ec40b3c639   host      host      <code class="nb">local</code><code class="w"/>
64368b7baa5f   none      null      <code class="nb">local</code><code class="w"/></pre>

<p>The <em>network bridge</em> in Docker is a link layer software device running within the host machine’s kernel, allowing linked containers to communicate while isolating non-connected containers.
The bridge driver automatically installs rules in the host machine so that containers on different bridge networks can’t communicate directly.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Bridge networks only apply to containers running on the same Docker engine/daemon host.
To connect containers running on other daemon hosts, you can manage routing at the host OS layer or use an <em>overlay</em> driver.</p>
</div>

<p>In addition to default bridge networks, you can create your own custom networks, which can provide superior isolation and packet routing experience.</p>












<section data-pdf-bookmark="Configure user-defined bridge networks" data-type="sect4"><div class="sect4" id="id221">
<h4>Configure user-defined bridge networks</h4>

<p><a data-primary="Docker, containerization with" data-secondary="bridge network driver" data-tertiary="configuring user-defined bridge networks" data-type="indexterm" id="ix_ch12-asciidoc27"/>If you need more advanced or isolated networking environments for your containers, you can create a separate user-defined network.</p>

<p>User-defined networks are superior to the default bridge networks as they provide better isolation.
In addition, containers can resolve each other by name or alias on user-defined bridge networks unlike the default network where they can only communicate via IP addresses.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>If you run containers without specifying <code>--network</code>, they’ll be attached to the default bridge network.
This can be a security issue as unrelated services are then able to communicate and access each other.</p>
</div>

<p>To create a network, you can use the <code>docker network create</code> command, which will use <code>--driver bridge</code> flag by default:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker network create genai-net<code class="w"/></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>When you create user-defined networks, Docker uses the host OS tools to manage the underlying network infrastructure, such as adding or removing bridge devices and configuring <code>iptables</code> rules on Linux.</p>
</div>

<p>Once the network is created, you can list the networks using the <code>docker network ls</code> command:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker network ls<code class="w"/>
NETWORK ID     NAME         DRIVER    SCOPE<code class="w"/>
72ec0b2e6034   bridge       bridge    <code class="nb">local</code><code class="w"/>
6aa21632e77e   genai-net    bridge    <code class="nb">local</code><code class="w"/></pre>

<p>The network topology will now look like <a data-type="xref" href="#docker_networking_isolated">Figure 12-10</a>.</p>

<figure><div class="figure" id="docker_networking_isolated">
<img alt="bgai 1210" src="assets/bgai_1210.png"/>
<h6><span class="label">Figure 12-10. </span>Isolated bridge networks</h6>
</div></figure>

<p>When you run containers, you can now attach them to the created network using the <code>--network genai-net</code> flag:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker run --network genai-net genai-service<code class="w"/>
$ docker run --network genai-net postgresql<code class="w"/></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>On Linux, there is a limit of 1,000 containers that can connect to a single bridge network due to the Linux kernel restrictions.
Linking more containers to a single bridge network can make it unstable and break inter-container communication.</p>
</div>

<p>Both your containers can now access each other on your better isolated <code>genai-net</code> user-defined network with automatic <em>DNS resolution</em> between containers.<a data-startref="ix_ch12-asciidoc27" data-type="indexterm" id="id1327"/></p>
</div></section>












<section data-pdf-bookmark="Embedded DNS" data-type="sect4"><div class="sect4" id="id276">
<h4>Embedded DNS</h4>

<p><a data-primary="Docker, containerization with" data-secondary="bridge network driver" data-tertiary="embedded DNS" data-type="indexterm" id="id1328"/>Docker leverages an embedded DNS server with user-defined networks, as shown in <a data-type="xref" href="#docker_networking_bridge_dns">Figure 12-11</a>, to map internal IP addresses so that containers can reach one by name.</p>

<figure><div class="figure" id="docker_networking_bridge_dns">
<img alt="bgai 1211" src="assets/bgai_1211.png"/>
<h6><span class="label">Figure 12-11. </span>Embedded DNS</h6>
</div></figure>

<p>For instance, if you name your application container as <code>genai-service</code> and your database container as <code>db</code>, then your <code>genai-service</code> container can communicate with the database by calling the <code>db</code> hostname.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>You can’t access the <code>db</code> container from outside of the Docker bridge network by its name, as the embedded DNS server is not visible to the host machine.</p>

<p>Instead, you can expose a container port <code>5432</code> and access the <code>db</code> container using host’s network (e.g., via <code>localhost:5432</code>).</p>
</div>

<p>Let’s discuss how you can publish container ports to the outside environment such as the host machine next.</p>
</div></section>












<section data-pdf-bookmark="Publishing ports" data-type="sect4"><div class="sect4" id="id222">
<h4>Publishing ports</h4>

<p><a data-primary="Docker, containerization with" data-secondary="bridge network driver" data-tertiary="publishing ports" data-type="indexterm" id="id1329"/>When you run containers in a network, they automatically expose ports to each other.</p>

<p>If you need to access containers from the host machine or non-Docker processes on different networks, you’ll need to expose the container ports by publishing them using the <code>--publish</code> or <code>-p</code> flag:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker run -p <code class="m">127</code>.0.0.1:8000:8000 myimage<code class="w"/></pre>

<p>This command allows you to create a container with exposed port <code>8000</code> mapped to <code>8000</code> port on the host machine (e.g., localhost) using the <code>&lt;host_port&gt;:​&lt;con⁠tainer_port&gt;</code> syntax.</p>

<p>When you don’t specify a container port, Docker will publish and map port <code>80</code> by default.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Always double-check ports you want to expose and avoid publishing container ports that are already in use on your host machine.
Otherwise, there’ll be <em>port conflicts</em> leading to requests being routed to conflicting services, which will also be time-consuming to 
<span class="keep-together">troubleshoot.</span></p>
</div>

<p>If using bridge networks and port mappings are causing you a lot of trouble, you can also use the <em>host</em> networking driver for connecting your containers, albeit without the same isolation and security benefits of bridge networks.<a data-startref="ix_ch12-asciidoc26" data-type="indexterm" id="id1330"/></p>
</div></section>
</div></section>










<section data-pdf-bookmark="Host network driver" data-type="sect3"><div class="sect3" id="id277">
<h3>Host network driver</h3>

<p><a data-primary="Docker, containerization with" data-secondary="Docker networking" data-tertiary="host network driver" data-type="indexterm" id="id1331"/>A <em>host</em> network driver is useful for cases where you want to improve performance, when you want to avoid the container port mapping, or when one of your containers needs to handle a large number of ports.</p>

<p>Running a container with the host driver is as simple as using the <code>--net=host</code> flag with the <code>docker run</code> command:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker run --net<code class="o">=</code>host genai-service<code class="w"/></pre>

<p>In host networking, containers share the host machine’s network namespace, meaning that containers won’t be isolated from the Docker host.
Therefore, containers won’t be allocated their own IP address.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>As soon as you enable the host network driver, previously published ports will be discarded, as containers won’t have their own IP address.</p>
</div>

<p>The host network driver is more performant because it doesn’t need a <em>network address translation</em> (NAT) for mapping IP addresses from one namespace (containers) to another (host machine) and avoids creating a <em>user-land proxy</em> (i.e., port forwarding) for each port.
However, host networking is only supported with Linux—and not Windows—containers.
In addition, containers won’t have access to the network interfaces of the host so can’t bind to host’s IP addresses, leading to added complexity in the network configuration you need.</p>
</div></section>










<section data-pdf-bookmark="None network driver" data-type="sect3"><div class="sect3" id="id223">
<h3>None network driver</h3>

<p><a data-primary="Docker, containerization with" data-secondary="Docker networking" data-tertiary="none network driver" data-type="indexterm" id="id1332"/><a data-primary="none network driver (Docker)" data-type="indexterm" id="id1333"/>If you want to completely isolate the networking stack of a container, you can use the <code>--network none</code> flag when starting the container.
Within the container, only the loopback device is created, a virtual network interface that the container uses to communicate with itself. You can specify the none network driver using the following command:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker run --network none genai-service<code class="w"/></pre>

<p>These are a few cases where isolating containers are useful:</p>

<ul>
<li>
<p>Applications handling highly sensitive data or running critical processes</p>
</li>
<li>
<p>Where there’s a higher risk of network-based attacks or malware</p>
</li>
<li>
<p>Performing network debugging and simulating network outages by eliminating external interference</p>
</li>
<li>
<p>Running stand-alone containers without external dependencies can run 
<span class="keep-together">independently</span></p>
</li>
<li>
<p>Operating transient containers for short-lived processes to minimize network exposure</p>
</li>
</ul>

<p>Generally, use the none network driver if you need to isolate containers from any Docker and non-Docker processes for security reasons.<a data-startref="ix_ch12-asciidoc25" data-type="indexterm" id="id1334"/><a data-startref="ix_ch12-asciidoc24" data-type="indexterm" id="id1335"/></p>
</div></section>
</div></section>








<section data-pdf-bookmark="Enabling GPU Driver" data-type="sect2"><div class="sect2" id="id224">
<h2>Enabling GPU Driver</h2>

<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="enabling GPU driver" data-type="indexterm" id="ix_ch12-asciidoc28"/><a data-primary="GPUs" data-secondary="enabling GPU driver in Docker" data-type="indexterm" id="ix_ch12-asciidoc29"/>If you have an NVIDIA graphics card with the CUDA toolkit and necessary drivers installed, then you can use the <code>--gpus=all</code> flag to enable GPU support for your containers in Docker.<sup><a data-type="noteref" href="ch12.html#id1336" id="id1336-marker">5</a></sup></p>

<p>To test that your system has the necessary drivers and supports GPU in Docker, run the following command to benchmark your GPU:</p>

<pre data-type="programlisting">$ docker run --rm -it \
             --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody \
             -gpu \
             -benchmark

&gt; Windowed mode
&gt; Simulation data stored in video memory
&gt; Single precision floating point simulation
&gt; 1 Devices used for simulation
MapSMtoCores for SM 8.9 is undefined.  Default to use 128 Cores/SM
MapSMtoArchName for SM 8.9 is undefined.  Default to use Ampere
GPU Device 0: "Ampere" with compute capability 8.9

&gt; Compute 8.9 CUDA device: [NVIDIA GeForce RTX 4090]
131072 bodies, total time for 10 iterations: 75.182 ms
= 2285.102 billion interactions per second
= 45702.030 single-precision GFLOP/s at 20 flops per interaction</pre>
<div data-type="tip"><h6>Tip</h6>
<p>You can also use the NVIDIA system management interface <code>nvidia-smi</code> tool to help manage and monitor NVIDIA GPU 
<span class="keep-together">devices.</span></p>
</div>

<p>Deep learning frameworks such as <code>tensorflow</code> or <code>pytorch</code> can automatically detect and use the GPU device when running your applications in a GPU-enabled container.
This includes Hugging Face libraries such as <code>transformers</code> that lets you self-host language models.</p>

<p>If using the <code>transformers</code> package, make sure to also install the <code>accelerate</code> library:</p>

<pre data-type="programlisting">$ pip install accelerate</pre>

<p>You can now move the model to GPU before it’s loaded in CPU by using <code>device_map='cuda'</code>, as shown in <a data-type="xref" href="#docker_gpu">Example 12-8</a>.</p>
<div data-type="example" id="docker_gpu">
<h5><span class="label">Example 12-8. </span>Transferring Hugging Face models to the GPU</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">pipeline</code>

<code class="n">pipe</code> <code class="o">=</code> <code class="n">pipeline</code><code class="p">(</code>
    <code class="s2">"text-generation"</code><code class="p">,</code>
    <code class="n">model</code><code class="o">=</code><code class="s2">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</code><code class="p">,</code>
    <code class="n">device_map</code><code class="o">=</code><code class="s2">"cuda"</code>
<code class="p">)</code></pre></div>

<p>You should be able to run the predictions on the GPU by passing the <code>--gpus=all</code> flag to <code>docker run</code>.<a data-startref="ix_ch12-asciidoc29" data-type="indexterm" id="id1337"/><a data-startref="ix_ch12-asciidoc28" data-type="indexterm" id="id1338"/></p>
</div></section>








<section data-pdf-bookmark="Docker Compose" data-type="sect2"><div class="sect2" id="id225">
<h2>Docker Compose</h2>

<p><a data-primary="Docker Compose tool" data-secondary="basics" data-type="indexterm" id="ix_ch12-asciidoc30"/>In multicontainer environments, you can use the <em>Docker Compose</em> tool for defining and running application containers for a streamlined development and deployment experience.</p>

<p>Using Docker Compose can help you simplify managing several containers, networks, volumes, variables, and secrets with a single <em>YAML configuration file</em>.
This simplifies the complex task of orchestrating and coordinating various containers, making it easier to manage and replicate your services across different application environments using environment variables.
You can also share the YAML file with others so that they can replicate your container environment.
Additionally, it caches configurations to prevent re-creating containers when you restart services.</p>

<p class="less_space pagebreak-before"><a data-type="xref" href="#docker_compose">Example 12-9</a> shows an example YAML configuration file.</p>
<div data-type="example" id="docker_compose">
<h5><span class="label">Example 12-9. </span>Docker Compose YAML configuration file</h5>

<pre data-code-language="yaml" data-type="programlisting"><code class="c1"># compose.yaml</code>

<code class="nt">services</code><code class="p">:</code> <a class="co" href="#callout_deployment_of_ai_services_CO4-1" id="co_deployment_of_ai_services_CO4-1"><img alt="1" src="assets/1.png"/></a>
  <code class="nt">server</code><code class="p">:</code>
    <code class="nt">build</code><code class="p">:</code> <code class="l-Scalar-Plain">.</code> <a class="co" href="#callout_deployment_of_ai_services_CO4-2" id="co_deployment_of_ai_services_CO4-2"><img alt="2" src="assets/2.png"/></a>
    <code class="nt">ports</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="s">"</code><code class="s">8000:8000</code><code class="s">"</code>
    <code class="nt">environment</code><code class="p">:</code>
      <code class="nt">SHOW_DOCS_IN_PRODUCTION</code><code class="p">:</code> <code class="l-Scalar-Plain">$SHOW_DOCS_IN_PRODUCTION</code>
      <code class="nt">ALLOWED_CORS_ORIGINS</code><code class="p">:</code> <code class="l-Scalar-Plain">$ALLOWED_CORS_ORIGINS</code>
    <code class="nt">secrets</code><code class="p">:</code>
       <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">openai_api_token</code> <a class="co" href="#callout_deployment_of_ai_services_CO4-3" id="co_deployment_of_ai_services_CO4-3"><img alt="3" src="assets/3.png"/></a>
    <code class="nt">volumes</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">./src/app:/code/app</code>
    <code class="nt">networks</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">genai-net</code> <a class="co" href="#callout_deployment_of_ai_services_CO4-4" id="co_deployment_of_ai_services_CO4-4"><img alt="4" src="assets/4.png"/></a>

  <code class="nt">db</code><code class="p">:</code>
    <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">postgres:12.2-alpine</code>
    <code class="nt">ports</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="s">"</code><code class="s">5433:5432</code><code class="s">"</code>
    <code class="nt">volumes</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">db-data:/etc/data</code>
    <code class="nt">networks</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">genai-net</code>

<code class="nt">volumes</code><code class="p">:</code>
  <code class="nt">db-data</code><code class="p">:</code>
    <code class="nt">name</code><code class="p">:</code> <code class="s">"</code><code class="s">my-app-data</code><code class="s">"</code>

<code class="nt">networks</code><code class="p">:</code>
  <code class="nt">genai-net</code><code class="p">:</code>
    <code class="nt">name</code><code class="p">:</code> <code class="s">"</code><code class="s">genai-net</code><code class="s">"</code>
    <code class="nt">driver</code><code class="p">:</code> <code class="l-Scalar-Plain">bridge</code>

<code class="nt">secrets</code><code class="p">:</code>
  <code class="nt">openai_api_token</code><code class="p">:</code>
    <code class="nt">environment</code><code class="p">:</code> <code class="l-Scalar-Plain">OPENAI_API_KEY</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_deployment_of_ai_services_CO4-1" id="callout_deployment_of_ai_services_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Create the containers alongside the associated volumes, networks, and secrets.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO4-2" id="callout_deployment_of_ai_services_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Use the Dockerfile located at the same directory as the Compose file to build the <code>server</code> image.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO4-3" id="callout_deployment_of_ai_services_CO4-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Use Docker secrets to mask sensitive data like API keys within the container shell environment.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO4-4" id="callout_deployment_of_ai_services_CO4-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Create a bridge <code>genai-net</code> network and attach both <code>server</code> and <code>db</code> containers 
<span class="keep-together">to it.</span></p></dd>
</dl></div>
<div data-type="tip"><h6>Tip</h6>
<p>If you have Docker objects like volumes and networks that you’re managing yourself, you can tag them with <code>external: true</code> in the compose file so that Docker Compose doesn’t manage them.</p>
</div>

<p>Once you have a <code>compose.yaml</code> file, you can then use simple compose commands to manage your containers:</p>

<pre data-code-language="bash" data-type="programlisting"><code class="c1"># Start services defined in compose.yaml</code>
$ docker compose up<code class="w"/>

<code class="c1"># Stop and remove running services (won't remove created volumes and networks)</code>
$ docker compose down<code class="w"/>

<code class="c1"># Monitor output of running containers</code>
$ docker compose logs<code class="w"/>

<code class="c1"># List all running services with their status</code>
$ docker compose ps<code class="w"/></pre>

<p>You can use these commands to start/stop/restart services and view their logs or container statuses. Additionally, you can edit the Compose file shown in <a data-type="xref" href="#docker_compose">Example 12-9</a> to use <code>watch</code> so that your services are automatically updated as you edit and save your code.</p>

<p><a data-type="xref" href="#docker_compose_watch">Example 12-10</a> shows how to use the <code>watch</code> instruction on a given directory.</p>
<div data-type="example" id="docker_compose_watch">
<h5><span class="label">Example 12-10. </span>Enabling Docker Compose <code>watch</code> on a given directory</h5>

<pre data-code-language="yaml" data-type="programlisting"><code class="nt">services</code><code class="p">:</code><code class="w"/>
  <code class="nt">server</code><code class="p">:</code><code class="w"/>
    <code class="c1"># ...</code><code class="w"/>
    <code class="nt">develop</code><code class="p">:</code><code class="w"/>
      <code class="nt">watch</code><code class="p">:</code><code class="w"/>
        <code class="p-Indicator">-</code> <code class="nt">action</code><code class="p">:</code> <code class="l-Scalar-Plain">sync</code><code class="w"/>
          <code class="nt">path</code><code class="p">:</code> <code class="l-Scalar-Plain">./src</code><code class="w"/>
          <code class="nt">target</code><code class="p">:</code> <code class="l-Scalar-Plain">/code</code><code class="w"/></pre></div>

<p>Whenever a file changes in the <code>./src</code> folder on your host machine, Compose will sync its content to <code>/code</code> and update the running application (server service) without restarting them.</p>

<p class="pagebreak-before less_space">You can then run the <code>watch</code> process using <code>docker compose watch</code>:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker compose watch<code class="w"/>

<code class="o">[</code>+<code class="o">]</code> Running <code class="m">2</code>/2<code class="w"/>
 ✔ Container project-server-1  Created     <code class="m">0</code>.0s<code class="w"/>
 ✔ Container project-db-1      Recreated   <code class="m">0</code>.1s<code class="w"/>
Attaching to db-1, server-1<code class="w"/>
         ⦿ watch enabled<code class="w"/>
...<code class="w"/></pre>

<p>Docker Compose <code>watch</code> allows for greater granularity than is practical with bind mounts, as shown in <a data-type="xref" href="#docker_compose">Example 12-9</a>.
For instance, it lets you ignore specific files or entire directories within the watched tree to avoid I/O performance issues.</p>

<p>Besides using Docker Compose <code>watch</code>, you can merge and override multiple Compose files to create a composite configuration tailored for specific build environments.
Typically, the <code>compose.yml</code> file contains the base configurations, which can be overridden by an optional <code>compose.override.yml</code> file.
For instance, as shown in <a data-type="xref" href="#compose_override">Example 12-11</a>, you can inject local environment settings, mount local volumes, and create new a database service.</p>
<div data-type="example" id="compose_override">
<h5><span class="label">Example 12-11. </span>Merging and overriding Compose files for environment-specific build configurations</h5>

<pre data-code-language="yaml" data-type="programlisting"><code class="c1"># compose.yml</code>

<code class="nt">services</code><code class="p">:</code> <a class="co" href="#callout_deployment_of_ai_services_CO5-1" id="co_deployment_of_ai_services_CO5-1"><img alt="1" src="assets/1.png"/></a>
  <code class="nt">server</code><code class="p">:</code>
      <code class="nt">ports</code><code class="p">:</code>
        <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">8000:8000</code>
      <code class="c1"># ...</code>
      <code class="nt">command</code><code class="p">:</code> <code class="l-Scalar-Plain">uvicorn</code> <code class="l-Scalar-Plain">main:app</code>

<code class="c1"># compose.override.yml</code>

<code class="nt">services</code><code class="p">:</code> <a class="co" href="#callout_deployment_of_ai_services_CO5-2" id="co_deployment_of_ai_services_CO5-2"><img alt="2" src="assets/2.png"/></a>
  <code class="nt">server</code><code class="p">:</code>
    <code class="nt">environment</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">LLM_API_KEY=$LLM_API_KEY</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">DATABASE_URL=$DATABASE_URL</code>
    <code class="nt">volumes</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">./code:/code</code>
    <code class="nt">command</code><code class="p">:</code> <code class="l-Scalar-Plain">uvicorn</code> <code class="l-Scalar-Plain">main:app</code> <code class="l-Scalar-Plain">--reload</code>

  <code class="nt">database</code><code class="p">:</code>
    <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">postgres:latest</code>
    <code class="nt">environment</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">POSTGRES_DB=genaidb</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">POSTGRES_USER=genaiuser</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">POSTGRES_PASSWORD=secretPassword!</code>
    <code class="nt">volumes</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">db_data:/var/lib/postgresql/data</code>

<code class="nt">networks</code><code class="p">:</code>
  <code class="nt">app-network</code><code class="p">:</code>

<code class="nt">volumes</code><code class="p">:</code>
  <code class="nt">db_data</code><code class="p">:</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_deployment_of_ai_services_CO5-1" id="callout_deployment_of_ai_services_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>The base Compose file contains instructions for running the production version of the application.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO5-2" id="callout_deployment_of_ai_services_CO5-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Override base instructions by replacing the container start command, inject local variables, and add volume and networking configurations with a local database service.</p></dd>
</dl></div>

<p>To use these files, run the following command:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker compose up<code class="w"/></pre>

<p>Docker Compose will automatically merge configurations from both Compose files, applying the environment-specific settings from the override Compose file.<a data-startref="ix_ch12-asciidoc30" data-type="indexterm" id="id1339"/></p>
</div></section>








<section data-pdf-bookmark="Enabling GPU Access in Docker Compose" data-type="sect2"><div class="sect2" id="id267">
<h2>Enabling GPU Access in Docker Compose</h2>

<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="enabling GPU access in Docker Compose" data-type="indexterm" id="id1340"/><a data-primary="Docker Compose tool" data-secondary="enabling GPU access in" data-type="indexterm" id="id1341"/><a data-primary="GPUs" data-secondary="enabling GPU access in Docker Compose" data-type="indexterm" id="id1342"/>To access GPU devices with services managed by Docker Compose, you’ll need to add the instructions to the composed file (see <a data-type="xref" href="#DockerComposeapp">Example 12-12</a>).</p>
<div data-type="example" id="DockerComposeapp">
<h5><span class="label">Example 12-12. </span>Adding GPU configurations to the Docker Compose app service</h5>

<pre data-code-language="yaml" data-type="programlisting"><code class="nt">services</code><code class="p">:</code>
  <code class="nt">app</code><code class="p">:</code>
    <code class="c1"># ...</code>
    <code class="nt">deploy</code><code class="p">:</code>
      <code class="nt">resources</code><code class="p">:</code>
        <code class="nt">reservations</code><code class="p">:</code>
          <code class="nt">devices</code><code class="p">:</code>
            <code class="p-Indicator">-</code> <code class="nt">driver</code><code class="p">:</code> <code class="l-Scalar-Plain">nvidia</code>
              <code class="nt">count</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code> <a class="co" href="#callout_deployment_of_ai_services_CO6-1" id="co_deployment_of_ai_services_CO6-1"><img alt="1" src="assets/1.png"/></a>
              <code class="nt">capabilities</code><code class="p">:</code> <code class="p-Indicator">[</code><code class="nv">gpu</code><code class="p-Indicator">]</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_deployment_of_ai_services_CO6-1" id="callout_deployment_of_ai_services_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Limit the number of GPU devices accessible by the app service.</p></dd>
</dl></div>

<p>These instructions will give you more granular control over how your services should use your GPU resources.</p>
</div></section>








<section data-pdf-bookmark="Optimizing Docker Images" data-type="sect2"><div class="sect2" id="id268">
<h2>Optimizing Docker Images</h2>

<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="optimizing Docker images" data-type="indexterm" id="ix_ch12-asciidoc31"/><a data-primary="Docker, containerization with" data-secondary="optimizing Docker images" data-type="indexterm" id="ix_ch12-asciidoc32"/>If your Docker images grow in size, they’ll also be slower to run, build, and test in production.
You’ll also be spending a lot of development time iterating over the development of the image.</p>

<p>In that case, it’s important to understand image optimization strategies, including how to use Docker’s layering mechanism to keep images lightweight and efficient to run, in particular with GenAI workloads.</p>

<p>These are a few ways to reduce image size and speed up the build process:</p>

<ul>
<li>
<p>Using minimal base images</p>
</li>
<li>
<p>Avoiding GPU inference runtimes</p>
</li>
<li>
<p>Externalizing application data</p>
</li>
<li>
<p>Layering ordering and caching</p>
</li>
<li>
<p>Using multi-stage builds</p>
</li>
</ul>

<p>Implementing these optimizations as shown in <a data-type="xref" href="#build_optimization_impact">Table 12-4</a> may reduce typical image sizes from several gigabytes to less than 1 GB.
Similarly, build times can reduce from several minutes on average to less than a minute.</p>
<table class="striped" id="build_optimization_impact">
<caption><span class="label">Table 12-4. </span>Impact of build optimization on a typical image<sup><a data-type="noteref" href="ch12.html#id1343" id="id1343-marker">a</a></sup></caption>
<thead>
<tr>
<th>Optimization step</th>
<th>Build time (seconds)</th>
<th>Image size (GB)</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Initial</p></td>
<td><p>352.9</p></td>
<td><p>1.42</p></td>
</tr>
<tr>
<td><p>Using minimal base images</p></td>
<td><p>38.5</p></td>
<td><p>1.38</p></td>
</tr>
<tr>
<td><p>Use caching</p></td>
<td><p>24.4</p></td>
<td><p>1.38</p></td>
</tr>
<tr>
<td><p>Layer ordering</p></td>
<td><p>17.9</p></td>
<td><p>1.38</p></td>
</tr>
<tr>
<td><p>Multi-stage builds</p></td>
<td><p>10.3</p></td>
<td><p>0.034 (34 MB)</p></td>
</tr>
</tbody>
<tbody><tr class="footnotes"><td colspan="3"><p data-type="footnote" id="id1343"><sup><a href="ch12.html#id1343-marker">a</a></sup> Source: <a class="orm:hideurl" href="https://www.warpbuild.com">warpbuild.com</a></p></td></tr></tbody></table>

<p>Let’s review each in more detail with code examples for clarity.</p>










<section data-pdf-bookmark="Use minimal base image" data-type="sect3"><div class="sect3" id="id278">
<h3>Use minimal base image</h3>

<p><a data-primary="Docker, containerization with" data-secondary="optimizing Docker images" data-tertiary="using minimal base image" data-type="indexterm" id="id1344"/>Base images allow you to start from a preconfigured image so you don’t have to install everything from scratch, including the Python interpreter.
However, some base images available on the Docker Hub may not be suitable for production deployments.
Instead, you’ll want to select the right base image with a minimal OS footprint to work from for faster builds and smaller image sizes, possibly with pre-installed Python dependencies and support for installing its various packages.</p>

<p>Alpine base images use a lightweight Alpine Linux distribution designed to be small and secure, containing only the <em>base minimum</em> essential tools to run your application, but this won’t support installing many Python packages.
On the other hand, slim base images may use other Linux distributions like Debian or CentOS, containing the <em>necessary</em> essential tools for running applications that make them larger than Alpine base images.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Use slim base images if you care about build time and Alpine base images if you care about image size.</p>
</div>

<p>You can use the <code>slim</code> base images such as <code>python:3.12-slim</code> or even Alpine base images like <code>python:3.12-alpine</code> that can be as small as 71.4 MB.
A bare-bones Alpine image can even go down to 12.1 MB. The following command shows a list of base images pulled from the Docker repository:</p>

<pre data-code-language="bash" data-type="programlisting">$ docker image ls<code class="w"/>

REPOSITORY  TAG         IMAGE ID       CREATED         SIZE<code class="w"/>
alpine      <code class="m">3</code>.20        3463e98c969d   <code class="m">4</code> weeks ago     <code class="m">12</code>.1MB<code class="w"/>
python      <code class="m">3</code>.12-alpine c6de2e87f545   <code class="m">6</code> days ago      <code class="m">71</code>.4MB<code class="w"/>
python      <code class="m">3</code>.12-slim   1ba4bc34383e   <code class="m">6</code> days ago      186MB<code class="w"/></pre>
<div data-type="tip"><h6>Tip</h6>
<p>Standard-sized images typically contain a full Linux distribution like Ubuntu or Debian containing a variety of pre-installed packages and dependencies, making them suitable for local development but perhaps not production environments.</p>
</div>
</div></section>










<section data-pdf-bookmark="Avoid GPU inference runtimes" data-type="sect3"><div class="sect3" id="id226">
<h3>Avoid GPU inference runtimes</h3>

<p><a data-primary="Docker, containerization with" data-secondary="optimizing Docker images" data-tertiary="avoiding GPU inference runtimes" data-type="indexterm" id="ix_ch12-asciidoc33"/><a data-primary="GPUs" data-secondary="avoiding GPU inference runtimes in Docker" data-type="indexterm" id="ix_ch12-asciidoc34"/>In AI workloads where you’re serving ML/GenAI models, you may need to install deep learning frameworks, dependencies, and GPU libraries that can suddenly explode the footprint of your images.
For instance, to make inferences on a GPU using the <code>transformers</code> library, you’ll need to install 3 GB of NVIDIA packages for GPU inference, 1.6 GB for the <code>torch</code> to perform the inference.</p>

<p>Unfortunately, you can’t reduce the image size if you need to use a GPU to perform an inference.
<a data-primary="Open Neural Network Exchange (ONNX)" data-type="indexterm" id="id1345"/>However, if you can avoid GPU inference and just rely on CPUs, you may be able to reduce the image size by up to 10 times using the Open Neural Network Exchange (ONNX) runtime with model quantization.</p>

<p>As discussed in <a data-type="xref" href="ch10.html#ch10">Chapter 10</a>, you can use the INT8 quantization with an ONNX model to benefit from model compression without much loss in output quality.</p>

<p>To switch from the GPU inference runtime to the ONNX runtime for Hugging Face transformer models, you can use the <code>transformers[onnx]</code> package:</p>

<pre data-type="programlisting">$ pip install transformers[onnx]</pre>

<p>You can then export any Hugging Face transformer model checkpoint with default configurations to the ONNX format with <code>transformers.onnx</code>:</p>

<pre data-type="programlisting">$ python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/</pre>

<p>This command exports the <code>distilbert/distilbert-base-uncased</code> model checkpoint as an ONNX graph stored in <code>onnx/model.onnx</code>, which can be run with any Hugging Face model accelerator that supports the ONNX standard, as shown in <a data-type="xref" href="#docker_onnx">Example 12-13</a>.</p>
<div data-type="example" id="docker_onnx">
<h5><span class="label">Example 12-13. </span>Model inference using the ONNX runtime with quantization</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">onnxruntime</code> <code class="kn">import</code> <code class="n">InferenceSession</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">AutoTokenizer</code>

<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"</code><code class="s2">distilbert/distilbert-base-uncased</code><code class="s2">"</code><code class="p">)</code>
<code class="n">session</code> <code class="o">=</code> <code class="n">InferenceSession</code><code class="p">(</code><code class="s2">"</code><code class="s2">onnx/model.onnx</code><code class="s2">"</code><code class="p">)</code>

<code class="n">inputs</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="s2">"</code><code class="s2">Using DistilBERT with ONNX Runtime!</code><code class="s2">"</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"</code><code class="s2">np</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_deployment_of_ai_services_CO7-1" id="co_deployment_of_ai_services_CO7-1"><img alt="1" src="assets/1.png"/></a>
<code class="n">output</code> <code class="o">=</code> <code class="n">session</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">output_names</code><code class="o">=</code><code class="p">[</code><code class="s2">"</code><code class="s2">last_hidden_state</code><code class="s2">"</code><code class="p">]</code><code class="p">,</code> <code class="n">input_feed</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_deployment_of_ai_services_CO7-1" id="callout_deployment_of_ai_services_CO7-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>ONNX runtime expects <code>numpy</code> arrays as input.</p></dd>
</dl></div>

<p>Using a technique such as shown in <a data-type="xref" href="#docker_onnx">Example 12-13</a>, you can downsize from image sizes between 5 and 10 GB to around 0.5 GB, which is a massive footprint reduction, significantly more cost-effective and scalable.<a data-startref="ix_ch12-asciidoc34" data-type="indexterm" id="id1346"/><a data-startref="ix_ch12-asciidoc33" data-type="indexterm" id="id1347"/></p>
</div></section>










<section data-pdf-bookmark="Externalize application data" data-type="sect3"><div class="sect3" id="id279">
<h3>Externalize application data</h3>

<p><a data-primary="Docker, containerization with" data-secondary="optimizing Docker images" data-tertiary="externalizing application data" data-type="indexterm" id="id1348"/>A core contributor to image size is copying models and application data into the image during build time.
This approach increases both the build time and image size.</p>

<p>A better approach is to use volumes during local development and external storage solutions for downloading and loading models at application startup in production.
In Kubernetes container orchestration environments, you can also use persistent volumes for model storage.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If your application container takes a long time to download data and model artifacts from an external source, your health checks may fail, and the hosting platform can kill your containers prematurely.
In such cases, configure health check probes to wait longer or as a last resort, bake the model into the image.</p>
</div>
</div></section>










<section data-pdf-bookmark="Layer ordering and caching" data-type="sect3"><div class="sect3" id="id269">
<h3>Layer ordering and caching</h3>

<p><a data-primary="caching" data-secondary="in Docker" data-type="indexterm" id="ix_ch12-asciidoc35"/><a data-primary="Docker, containerization with" data-secondary="optimizing Docker images" data-tertiary="layer ordering and caching" data-type="indexterm" id="ix_ch12-asciidoc36"/>Docker uses a layered filesystem to create layers in an image for each instruction in the Dockerfile.
These layers are like a stack, with each layer adding more content on top of the previous layers.
Whenever a layer changes, that layer (and further layers) will need to be rebuilt for those changes to appear in the image (i.e., build cache must be invalidated).</p>

<p>A layer (i.e., a filesystem snapshot) is created if the instruction is writing or deleting files into the container’s union filesystem.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Dockerfile instructions that modify the filesystem like <code>ENV</code>, <code>COPY</code>, <code>ADD</code>, and <code>RUN</code> will contribute new layers to the build process, effectively increasing the image size.
On the other hand, instructions such as  <code>WORKDIR</code>, <code>ENTRYPOINT</code>, <code>LABEL</code>, and <code>CMD</code> that only update the image metadata don’t create any layers and any build cache.</p>
</div>

<p>After creation, each layer is then cached for reusability across image rebuilds if the instruction and files it depends on haven’t changed since the last build.
Therefore, ideally, you want to write a Dockerfile that allows you to stop, destroy, rebuild, and replace containers with minimal setup and configuration.</p>

<p>There are a few techniques you can use to minimize and optimize these layers as much as possible.</p>












<section data-pdf-bookmark="Layer ordering to avoid frequent cache invalidation" data-type="sect4"><div class="sect4" id="id445">
<h4>Layer ordering to avoid frequent cache invalidation</h4>

<p>Since changes to the earlier layers can invalidate the build cache leading to repeating steps, you should order your Dockerfile from the most stable (e.g., installations) to the most frequently changing or volatile (e.g., application code, configuration files).</p>

<p>Following this ordering, place the most stable yet expensive instructions (e.g., model downloads or heavy dependency installations) at the start of the Dockerfile, and volatile, fast operations (e.g., copying application code) at the bottom.</p>

<p>Imagine your Dockerfile file looks like this:</p>

<pre data-code-language="dockerfile" data-type="programlisting"><code class="k">FROM</code> <code class="s">python:3.12-slim</code> <code class="k">as</code> <code class="s">base</code><code class="w"/>
<code class="c"># Changes to the</code><code class="w"/>
<code class="k">COPY</code> . .<code class="w"/>
<code class="k">RUN</code> pip install requirements.txt<code class="w"/></pre>

<p>Here you’re creating a layer by copying your working directory containing the application code into the image before downloading and installing dependencies.</p>

<p>If any one of source files changes, Docker builder will invalidate the cache causing the dependency installation to be repeated, which is expensive and can take several minutes to complete, if not cached by <code>pip</code>.</p>

<p>To avoid repeating expensive steps, you can logically order your Dockerfile instructions to optimize the layer caching by reordering instructions like these:</p>

<pre data-code-language="dockerfile" data-type="programlisting"><code class="k">FROM</code> <code class="s">python:3.12-slim</code> <code class="k">as</code> <code class="s">base</code><code class="w"/>
<code class="k">COPY</code> requirements.txt requirements.txt<code class="w"/>
<code class="k">RUN</code> pip install requirements.txt<code class="w"/>
<code class="k">COPY</code> . .<code class="w"/></pre>

<p>Now any changes to the source files won’t affect the long dependency installation step, drastically speeding up the build process.</p>
</div></section>












<section data-pdf-bookmark="Minimize layers" data-type="sect4"><div class="sect4" id="id227">
<h4>Minimize layers</h4>

<p>To keep image sizes small, you’ll want to minimize image layers as much as possible.</p>

<p>A simple technique to achieve this is to combine multiple <code>RUN</code> instructions into one.
For instance, instead of writing multiple <code>RUN apt-get</code> installations, you can combine them into a single <code>RUN</code> command with <code>&amp;&amp;</code>:</p>

<pre data-code-language="dockerfile" data-type="programlisting"><code class="k">RUN</code> apt-get update <code class="o">&amp;&amp;</code> apt-get install -y<code class="w"/></pre>

<p><a data-primary="cache busting" data-type="indexterm" id="id1349"/>This will avoid adding unnecessary layers and prevents caching issues with <code>apt-get update</code> using the <em>cache busting</em> technique.</p>

<p>Since the builder may potentially skip updating the package index, causing installations to fail or use outdated packages, using the <code>&amp;&amp;</code> ensures that the latest packages are installed if the package index is updated.</p>
<div data-type="tip"><h6>Tip</h6>
<p>You can also use the <code>--no-cache</code> flag when using <code>docker build</code> to avoid cache hits and ensure fresh downloads of base images and dependencies on every build.</p>
</div>
</div></section>












<section data-pdf-bookmark="Keep build context small" data-type="sect4"><div class="sect4" id="id228">
<h4>Keep build context small</h4>

<p><a data-primary="build context (Docker)" data-type="indexterm" id="id1350"/>The <em>build context</em> is the set of files and directories that’ll be sent to the builder to carry out the Dockerfile instruction.
A smaller build context reduces the amount of data sent to the builder and lowers the chance of cache invalidation, resulting in faster builds.</p>

<p>When you use the <code>COPY . .</code> command in a Dockerfile to copy your working directory into an image, you may also add tool caches, development dependencies, virtual environments, and unused files into the build context.
Not only the image size will be increased, but also the Docker builder will cache these unnecessary files.
Any changes to these files will then invalidate the build, restarting the whole build process.</p>

<p>To prevent the unnecessary cache invalidation, you can add a <em>.dockerignore</em> file next to your Dockerfile, listing all files and directories that your services won’t need in production.
As an example, here are items you can include in a <em>.dockerignore</em> file:</p>

<pre data-type="programlisting">**/.DS_Store
**/__pycache__
**/.mypy_cache
**/.venv
**/.env
**/.git</pre>

<p>Docker builder will then ignore these files even when you run the <code>COPY</code> command across your entire working directory.</p>
</div></section>












<section data-pdf-bookmark="Use cache and bind mounts" data-type="sect4"><div class="sect4" id="id229">
<h4>Use cache and bind mounts</h4>

<p><a data-primary="bind mounts (Docker)" data-type="indexterm" id="id1351"/><a data-primary="cache mounts (Docker)" data-type="indexterm" id="id1352"/>You can use <em>bind mounts</em> to avoid adding unnecessary layers to the image and <em>cache mounts</em> to speed up subsequent builds.</p>

<p>Bind mounts temporarily include files in the build context for a single <code>RUN</code> instruction and won’t persist as image layers after.
Cache mounts specify a persistent cache location that you can read and write data to across multiple builds.</p>

<p>Here is an example where you can download a pretrained model from Hugging Face into a mounted cache to optimize layer caching:</p>

<pre data-code-language="dockerfile" data-type="programlisting"><code class="k">RUN</code> --mount<code class="o">=</code><code class="nv">type</code><code class="o">=</code>cache,target<code class="o">=</code>/root/.cache/huggingface <code class="o">&amp;&amp;</code> <code class="se">\</code>
    pip install transformers <code class="o">&amp;&amp;</code> <code class="se">\</code>
    python -c <code class="s2">"from transformers import AutoModel; \</code>
<code class="s2">    AutoModel.from_pretrained('bert-base-uncased')"</code><code class="w"/></pre>

<p>This <code>RUN</code> instruction creates a cache of the downloaded pretrained model at 
<span class="keep-together"><code>/root/.cache/huggingface</code>,</span> which can be shared across multiple builds.
This helps avoid redundant downloads and optimizes the build process by reusing cached layers.</p>

<p>You can also use the <code>--no-cache-dir</code> flag when using the <code>pip</code> package manager to avoid caching altogether for minimizing image size.
However, you’ll have a significantly slower build process as follow-on builds will need to redownload each time.</p>
</div></section>












<section data-pdf-bookmark="Use external cache" data-type="sect4"><div class="sect4" id="id230">
<h4>Use external cache</h4>

<p>If you’re building and deploying containers using a CI/CD pipeline, you can benefit from an external cache hosted on a remote location.
An external cache can drastically speed up the build process in CI/CD pipelines where builders are often ephemeral and build minutes are precious.</p>

<p>To use an external cache, you can specify the <code>--cache-to</code> and <code>--cache-from</code> options with the <code>docker buildx build</code> command:</p>

<pre data-code-language="dockerfile" data-type="programlisting">docker buildx build --cache-from <code class="nv">type</code><code class="o">=</code>registry,ref<code class="o">=</code>user/app:buildcache .<code class="w"/></pre>

<p>Besides layer ordering and cache optimization, you can use multi-stage builds to significantly shrink your image sizes.<a data-startref="ix_ch12-asciidoc36" data-type="indexterm" id="id1353"/><a data-startref="ix_ch12-asciidoc35" data-type="indexterm" id="id1354"/></p>
</div></section>
</div></section>










<section data-pdf-bookmark="Multi-stage builds" data-type="sect3"><div class="sect3" id="id231">
<h3>Multi-stage builds</h3>

<p><a data-primary="Docker, containerization with" data-secondary="optimizing Docker images" data-tertiary="multi-stage builds" data-type="indexterm" id="ix_ch12-asciidoc37"/>Using <em>multi-stage builds</em>, you can reduce the size of your final image by splitting out the Dockerfile instructions into distinct stages.
Common stages can be reused to include shared components and serve as a starting point for further stages.</p>

<p>You can also selectively copy artifacts from one stage to another, leaving behind everything you don’t want in the final image.
This ensures that only the required outputs are included in the final image from previous stages, avoiding any non-essential artifacts.
Furthermore, you can also execute multiple build stages in parallel to speed up the build process of your images.</p>

<p>A common multi-stage build pattern is when you need a testing/development image and a slimmer production one with both starting from a shared first stage image.
The development or testing image can include additional layers of tooling (i.e., compilers, build systems, and debugging tools) to support the required workflows.</p>

<p>Imagine you need to serve a bert transformer model from Hugging Face in a FastAPI service.
You can write your Dockerfile instructions to use three distinct sequential stages.</p>

<p>The first stage downloads the transformer model into <code>/root/.cache/huggingface</code> and creates a Python virtual environment at <code>/opt/venv</code>:</p>

<pre data-code-language="dockerfile" data-type="programlisting"><code class="c"># Stage 1: Base</code><code class="w"/>
<code class="k">FROM</code> <code class="s">python:3.11.0-slim</code> <code class="k">as</code> <code class="s">base</code><code class="w"/>

<code class="k">RUN</code> python -m venv /opt/venv<code class="w"/>
<code class="k">RUN</code> pip install transformers <code class="o">&amp;&amp;</code> <code class="se">\</code>
    python -c <code class="s2">"from transformers import AutoModel; \</code>
<code class="s2">    AutoModel.from_pretrained('bert-base-uncased')"</code><code class="w"/>
<code class="k">RUN</code> --mount<code class="o">=</code><code class="nv">type</code><code class="o">=</code>cache,target<code class="o">=</code>/root/.cache/pip <code class="se">\</code>
    --mount<code class="o">=</code><code class="nv">type</code><code class="o">=</code>bind,source<code class="o">=</code>requirements.txt,target<code class="o">=</code>requirements.txt <code class="se">\</code>
    python -m pip install -r requirements.txt<code class="w"/></pre>

<p>The second stage then copies the model artifacts and virtual Python environment <code>/opt/ven</code> from the <code>base</code> stage before copying source files over and creating a production version of the FastAPI service:</p>

<pre data-code-language="dockerfile" data-type="programlisting"><code class="c"># Stage 2: Production</code><code class="w"/>
<code class="k">FROM</code> <code class="s">base</code> <code class="k">as</code> <code class="s">production</code><code class="w"/>
<code class="k">RUN</code> apt-get update <code class="o">&amp;&amp;</code> apt-get install -y<code class="w"/>
<code class="k">COPY</code> --from<code class="o">=</code>base /opt/venv /opt/venv<code class="w"/>
<code class="k">COPY</code> --from<code class="o">=</code>base /root/.cache/huggingface /root/.cache/huggingface<code class="w"/>

<code class="k">WORKDIR</code> <code class="s">/code</code><code class="w"/>
<code class="k">COPY</code> . .<code class="w"/>

<code class="k">EXPOSE</code> <code class="s">8000</code><code class="w"/>

<code class="k">ENV</code> <code class="nv">BUILD_ENV</code><code class="o">=</code>PROD<code class="w"/>
<code class="k">CMD</code> <code class="p">[</code><code class="s2">"uvicorn"</code><code class="p">,</code> <code class="s2">"main:app"</code><code class="p">,</code> <code class="s2">"--host"</code><code class="p">,</code> <code class="s2">"0.0.0.0"</code><code class="p">,</code> <code class="s2">"--port"</code><code class="p">,</code> <code class="s2">"8000"</code><code class="p">]</code><code class="w"/></pre>

<p>The last stage copies the production stage virtual Python environment with installed packages and adds several development tools on top.
It then starts the server with hot reload functionality:</p>

<pre data-code-language="dockerfile" data-type="programlisting"><code class="c"># Stage 3: Development</code><code class="w"/>
<code class="k">FROM</code> <code class="s">production</code> <code class="k">as</code> <code class="s">development</code><code class="w"/>

<code class="k">COPY</code> --from<code class="o">=</code>production /opt/venv /opt/venv<code class="w"/>
<code class="k">COPY</code> ./requirements_dev.txt ./<code class="w"/>
<code class="k">RUN</code> pip install --no-cache-dir --upgrade -r requirements_dev.txt<code class="w"/>

<code class="k">ENV</code> <code class="nv">BUILD_ENV</code><code class="o">=</code>DEV<code class="w"/>
<code class="k">CMD</code> <code class="p">[</code><code class="s2">"uvicorn"</code><code class="p">,</code> <code class="s2">"main:app"</code><code class="p">,</code> <code class="s2">"--host"</code><code class="p">,</code> <code class="s2">"0.0.0.0"</code><code class="p">,</code> <code class="s2">"--port"</code><code class="p">,</code> <code class="s2">"8000"</code><code class="p">,</code> <code class="s2">"--reload"</code><code class="p">]</code><code class="w"/></pre>

<p>Using a single Dockerfile, we were able to create three distinct stages and use them as we see fit via the <code>--target development</code> command when needed<a data-startref="ix_ch12-asciidoc37" data-type="indexterm" id="id1355"/>.<a data-startref="ix_ch12-asciidoc32" data-type="indexterm" id="id1356"/><a data-startref="ix_ch12-asciidoc31" data-type="indexterm" id="id1357"/></p>
</div></section>
</div></section>








<section data-pdf-bookmark="docker init" data-type="sect2"><div class="sect2" id="id232">
<h2>docker init</h2>

<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="docker init" data-type="indexterm" id="id1358"/>You now have an in-depth understanding of the containerization process with the Docker platform and the relevant best practices.</p>

<p>If you ever need to add Docker to an existing project, you can use the <code>docker init</code> command, which will guide you through a wizard to create all the necessary Docker deployment files in your current working directory:</p>

<pre data-type="programlisting">$ docker init
&gt;&gt; Answer a few questions in the terminal...

project/
│
├── .dockerignore
├── compose.yaml
├── Dockerfile
└── README.Docker.md
... # other application files</pre>

<p>This will provide a great starting point that you can work from to include additional configuration steps, dependencies, or services as required.</p>
<div data-type="tip"><h6>Tip</h6>
<p>I recommend using <code>docker init</code> when starting out as every generated file will adhere to best practices including leveraging <code>dockerignore</code>, optimizing image layers, using bind and cache mounts for package installation, and switching to nonroot users.</p>
</div>

<p>Once you have an optimized image and a set of working containers, you can choose any cloud provider or self-hosting solution for pushing images to registries and deploying your new GenAI services.<a data-startref="ix_ch12-asciidoc10" data-type="indexterm" id="id1359"/><a data-startref="ix_ch12-asciidoc9" data-type="indexterm" id="id1360"/></p>
</div></section>
</div></section>






<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id233">
<h1>Summary</h1>

<p>In this chapter, we reviewed various strategies for deploying your GenAI services—for instance, on virtual machines, as cloud functions, with managed app service platforms, or via containers.
As part of this, I covered how virtualization differs from containerization and why you may want to deploy your services as containers.</p>

<p>Next, you learned about the Docker containerization platform and how you can use it to build self-contained images of your applications that can run as containers.</p>

<p>We covered the Docker storage and networking mechanisms that allow you to persist data using the union filesystem in containers and how to connect containers with different networking drivers.</p>

<p>Finally, you were introduced to various optimization techniques for reducing the build time and size of your images to deploy your GenAI services as efficiently as possible.</p>

<p>With services containerized, you can push them to container registries to share, distribute, and run them on any cloud or hosting environment of your choice.<a data-startref="ix_ch12-asciidoc0" data-type="indexterm" id="id1361"/></p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id1291"><sup><a href="ch12.html#id1291-marker">1</a></sup> Slim base Python images balance the size and compatibility of the Linux distribution with a wider range of Python packages out of the box compared to Alpine base Python images that minimize size but require extra configurations.</p><p data-type="footnote" id="id1292"><sup><a href="ch12.html#id1292-marker">2</a></sup> You can use the <code>-p</code> or <code>--publish</code> flag when running the container to map and enable container access via a port.</p><p data-type="footnote" id="id1297"><sup><a href="ch12.html#id1297-marker">3</a></sup> Images built on one machine can only run on other machines with the same processor architecture.</p><p data-type="footnote" id="id1317"><sup><a href="ch12.html#id1317-marker">4</a></sup> You can still run executable files with the <code>r</code> permission alone by using the <code>bash script.sh</code> command instead of <code>./script.sh</code>.</p><p data-type="footnote" id="id1336"><sup><a href="ch12.html#id1336-marker">5</a></sup> Refer to the NVIDIA documentation on how to install the latest CUDA toolkit and graphics drivers for your system.</p></div></div></section></body></html>