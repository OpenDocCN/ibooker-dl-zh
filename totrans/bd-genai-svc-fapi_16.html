<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 12. Deployment of AI Services" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch12">
<h1><span class="label">Capitolo 12. </span>Distribuzione dei servizi di intelligenza artificiale</h1><div data-type="note"><p>Questo lavoro è stato tradotto utilizzando l'AI. Siamo lieti di ricevere il tuo feedback e i tuoi commenti: <a href="mailto:translation-feedback@oreilly.com">translation-feedback@oreilly.com</a></p></div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1273">
<h1>Obiettivi del capitolo</h1>
<p><a data-primary="deployment of AI services" data-type="indexterm" id="ix_ch12-asciidoc0"/>In questo capitolo imparerai a conoscere:</p>
<ul>
<li>
<p>Opzioni di distribuzione per servire i tuoi servizi GenAI agli utenti</p>
</li>
<li>
<p>Distribuzione mediante container con Docker</p>
</li>
<li>
<p>Collegamento in rete dei container in Docker e lavoro con i volumi di archiviazione</p>
</li>
<li>
<p>Risolvere i problemi di autorizzazione che si verificano quando si lavora con i container</p>
</li>
<li>
<p>Strategie di ottimizzazione per ridurre al minimo il tempo di creazione del container e le dimensioni dell'immagine</p>
</li>
</ul>
</div></aside>
<p>In questo capitolo finale, è il momento di completare la tua soluzione GenAI distribuendola. Go imparerà diverse strategie di distribuzione e, come parte della distribuzione, conterrà i suoi servizi con Docker seguendo le sue migliori pratiche.</p>
<section data-pdf-bookmark="Deployment Options" data-type="sect1"><div class="sect1" id="id265">
<h1>Opzioni di distribuzione</h1>
<p><a data-primary="deployment of AI services" data-secondary="options" data-type="indexterm" id="ix_ch12-asciidoc1"/>Ora hai un servizio GenAI funzionante che vuoi rendere accessibile ai tuoi utenti. Quali sono le opzioni di distribuzione? Ci sono alcune strategie di distribuzione comuni che puoi adattare per rendere le tue app accessibili agli utenti:</p>
<ul class="two-col">
<li>
<p>Macchine virtuali (VM)</p>
</li>
<li>
<p>Funzioni serverless</p>
</li>
<li>
<p>Piattaforme applicative gestite</p>
</li>
<li>
<p>Containerizzazione</p>
</li>
</ul>
<p>Analizziamo ciascuno di essi in modo più dettagliato.</p>
<section data-pdf-bookmark="Deploying to Virtual Machines" data-type="sect2"><div class="sect2" id="id207">
<h2>Distribuzione su macchine virtuali</h2>
<p><a data-primary="deployment of AI services" data-secondary="options" data-tertiary="deploying to virtual machines" data-type="indexterm" id="ix_ch12-asciidoc2"/>Se intendi utilizzare i tuoi server on-premises o preferisci distribuire i tuoi servizi sullo stesso hardware che ospita le altre applicazioni per ottenere un elevato isolamento e sicurezza, puoi distribuire il tuo servizio GenAI in una VM.</p>
<p>Una macchina virtuale è un'emulazione software di un computer fisico che esegue un sistema operativo (OS) e delle applicazioni. Non è diversa da un computer fisico come un laptop, uno smartphone o un server.</p>
<p>Il sistema <em>host</em> della VM fornisce risorse come CPU, memoria e storage, mentre un livello software chiamato <em>hypervisor</em> gestisce la VM e alloca le risorse dall'host alla VM. Le risorse che l'hypervisor alloca alla VM sono l'<em>hardware virtuale</em> su cui girano il sistema operativo e le applicazioni.</p>
<p>La macchina virtuale può essere eseguita direttamente sull'hardware dell'host (bare metal) o su un sistema operativo convenzionale (cioè essere ospitata). Di conseguenza, il sistema operativo installato all'interno della macchina virtuale viene chiamato <em>sistema operativo guest</em>.</p>
<p>La<a data-type="xref" href="#deployment_vm_architecture">Figura 12-1</a> mostra l'architettura del sistema della tecnologia di virtualizzazione.</p>
<figure><div class="figure" id="deployment_vm_architecture">
<img alt="bgai 1201" src="assets/bgai_1201.png" width="1288" height="706"/>
<h6><span class="label">Figura 12-1. </span>Architettura del sistema di virtualizzazione</h6>
</div></figure>
<p>I provider Cloud o il tuo data center possono essere costituiti da diversi server fisici, ognuno dei quali ospita più macchine virtuali con il proprio sistema operativo guest e le proprie applicazioni ospitate. Per una condivisione delle risorse efficace dal punto di vista dei costi, queste macchine virtuali possono condividere la stessa unità di archiviazione fisica montata, anche se sono contenute in ambienti completamente isolati, come puoi vedere nella <a data-type="xref" href="#deployment_vm_data_center">Figura 12-2</a>.</p>
<figure><div class="figure" id="deployment_vm_data_center">
<img alt="bgai 1202" src="assets/bgai_1202.png" width="1432" height="1271"/>
<h6><span class="label">Figura 12-2. </span>Macchine virtuali ospitate in un data center</h6>
</div></figure>
<p>Il vantaggio di usare una macchina virtuale è che hai accesso diretto al sistema operativo guest, alle risorse hardware virtuali e ai driver della GPU. Se ci sono problemi con l'implementazione, puoi connetterti alla macchina virtuale tramite il protocollo <em>Secure Shell Transfer</em> (SHH) per ispezionare i log dell'applicazione, impostare l'ambiente dell'applicazione e fare il debug dei problemi di produzione.</p>
<p>Per distribuire i tuoi servizi sulle macchine virtuali è sufficiente clonare il repository del codice sulla macchina virtuale e installare le dipendenze, i pacchetti e i driver necessari per avviare con successo l'applicazione. Tuttavia, il metodo consigliato è quello di utilizzare una piattaforma di containerizzazione come Docker in esecuzione sulla macchina virtuale per consentire distribuzioni continue e altri vantaggi. Dovresti anche assicurarti di dimensionare le risorse della macchina virtuale in modo appropriato, in modo che i tuoi servizi non siano affamati di core CPU/GPU, memoria o spazio su disco.</p>
<p>Con le macchine virtuali on-premises puoi risparmiare sui costi di hosting on-cloud o di noleggio dei server e puoi proteggere completamente i tuoi ambienti applicativi per una manciata di utenti, isolati dalla rete internet pubblica. Questi vantaggi sono ottenibili anche con le macchine virtuali in cloud, ma richiedono<span class="keep-together">una</span> configurazione<span class="keep-together">aggiuntiva</span> delle reti e delle risorse da configurare. Inoltre, puoi avere accesso all'hardware delle GPU e configurare i driver per i requisiti delle tue applicazioni.</p>
<p>Tieni presente che l'utilizzo del modello di distribuzione VM potrebbe non essere facilmente scalabile e richiede un notevole sforzo di manutenzione. Inoltre, i server VM funzionano normalmente 24 ore su 24, 7 giorni su 7, comportando costi di gestione costanti, a meno che tu non ne automatizzi l'avvio e lo spegnimento in base alle tue esigenze. Sarai responsabile dell'applicazione delle patch di sicurezza, degli aggiornamenti del sistema operativo e degli aggiornamenti dei pacchetti, oltre che delle configurazioni di rete. Con l'accesso diretto alle risorse hardware, dovrai anche prendere un maggior numero di decisioni che possono rallentare la tua attività, portandoti alla stanchezza decisionale.</p>
<p>Il mio consiglio è quello di implementare una macchina virtuale se non hai intenzione di scalare i tuoi servizi a breve o se hai bisogno di mantenere bassi i costi dei server e un ambiente applicativo isolato e sicuro per una manciata di utenti. Inoltre, assicurati di aver pianificato un tempo sufficiente per l'implementazione, il collegamento in rete e la configurazione delle macchine virtuali.<a data-startref="ix_ch12-asciidoc2" data-type="indexterm" id="id1274"/></p>
</div></section>
<section data-pdf-bookmark="Deploying to Serverless Functions" data-type="sect2"><div class="sect2" id="id208">
<h2>Distribuire le funzioni Serverless</h2>
<p><a data-primary="deployment of AI services" data-secondary="options" data-tertiary="deploying to serverless functions" data-type="indexterm" id="ix_ch12-asciidoc3"/><a data-primary="serverless deployments" data-type="indexterm" id="ix_ch12-asciidoc4"/>Oltre alle macchine virtuali, puoi anche distribuire i tuoi servizi su funzioni cloud che i provider cloud forniscono come sistemi <em>serverless</em>. Nel serverless computing, il tuo codice viene eseguito in risposta a eventi come le modifiche al database, gli aggiornamenti dei blob in uno storage, le richieste HTTP o i messaggi aggiunti a una coda. Questo significa che paghi solo per le richieste o le risorse di calcolo che i tuoi servizi utilizzano, invece che per un intero server come nel caso di una macchina virtuale in esecuzione continua.</p>
<p>Le implementazioni serverless sono spesso utili quando:</p>
<ul>
<li>
<p>Vuoi avere sistemi guidati dagli eventi invece di una macchina virtuale in esecuzione, che potrebbe essere attiva 24 ore su 24, 7 giorni su 7.</p>
</li>
<li>
<p>Vuoi distribuire i tuoi servizi API utilizzando un'architettura serverless che sia altamente efficiente dal punto di vista dei costi.</p>
</li>
<li>
<p>I tuoi servizi devono eseguire lavori di elaborazione batch</p>
</li>
<li>
<p>Hai bisogno dell'automazione del flusso di lavoro</p>
</li>
</ul>
<p>Il termine <em>serverless</em> non significa che le funzioni del cloud non richiedono risorse hardware per essere eseguite, ma piuttosto che la gestione di queste risorse è gestita dal provider del cloud. Questo ti permette di concentrarti sulla scrittura del codice dell'applicazione senza preoccuparti dei dettagli a livello di server e di sistema operativo.</p>
<p>I Cloud provider istanziano le risorse di calcolo per soddisfare la domanda dei loro clienti. Spesso si verifica un'impennata della domanda, che richiede la creazione di risorse aggiuntive in anticipo per gestire il picco di richieste. Tuttavia, una volta che la domanda diminuisce, rimangono risorse di calcolo non allocate in eccesso che devono essere chiuse o condivise tra altri clienti.</p>
<p>La rimozione e la creazione di risorse è un'operazione di calcolo intensiva da eseguire. Su scala, queste operazioni comportano costi significativi per i cloud provider. Per questo motivo, i cloud provider preferiscono mantenere queste risorse attive il più possibile e distribuirle tra i clienti esistenti per massimizzare la fatturazione.</p>
<p>Per incoraggiare i clienti a utilizzare questi calcoli in eccesso, hanno creato dei servizi di funzioni cloud che puoi sfruttare per eseguire i tuoi servizi backend su calcoli in eccesso (cioè serverless). Fortunatamente, esistono pacchetti come Magnum che ti permettono di pacchettizzare i servizi<span class="keep-together">FastAPI</span> su funzioni cloud AWS. Vedrai presto che i servizi FastAPI possono essere distribuiti anche come funzioni cloud Azure.</p>
<p>Devi tenere presente che a queste funzioni viene assegnata solo una piccola quantità di risorse e hanno un timeout breve. Tuttavia, puoi richiedere timeout più lunghi e l'assegnazione di risorse di calcolo, ma potrebbe essere necessario più tempo per ricevere queste allocazioni, con conseguenti latenze più elevate per i tuoi utenti.</p>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>Se la tua logica aziendale consuma molte risorse o richiede più di una manciata di minuti per essere eseguita, le funzioni cloud potrebbero non essere un'opzione di distribuzione adatta a te.</p>
<p>Tuttavia, puoi dividere i tuoi servizi FastAPI in più funzioni, con ogni funzione che gestisce un singolo endpoint esposto. In questo modo, puoi distribuire parti del tuo servizio come funzioni Cloud, riducendo la parte del servizio FastAPI che deve essere distribuita con altri metodi.</p>
</div>
<p>Il vantaggio principale dell'utilizzo di funzioni serverless per l'implementazione dei tuoi servizi è la loro scalabilità: puoi scalare le tue applicazioni in base alle esigenze e pagare solo una frazione del costo rispetto alla prenotazione di risorse VM dedicate. I fornitori di Cloud solitamente applicano tariffe basate sul numero di esecuzioni delle funzioni e sul runtime, spesso con generose quote mensili. Questo significa che se le tue funzioni vengono eseguite rapidamente e hai un numero moderato di utenti contemporanei, potresti essere in grado di ospitare tutti i tuoi servizi gratuitamente.</p>
<p>Inoltre, i fornitori di cloud forniscono anche runtime di funzioni che puoi installare localmente per i test e lo sviluppo locale, in modo da accorciare notevolmente le iterazioni di sviluppo.</p>
<p>Ogni cloud provider ha un proprio approccio alla distribuzione delle funzioni serverless. Spesso è necessario uno script di ingresso, come <em>main.py</em>, che può importare le dipendenze da altri moduli secondo le necessità. Oltre allo script di ingresso, dovrai caricare un file di configurazione JSON dell'host della funzione insieme al <em>file requirements.txt</em> per le dipendenze necessarie da installare al momento della distribuzione su un runtime Python.</p>
<p>Puoi quindi distribuire le funzioni caricando tutti i file necessari in una cartella zippata o utilizzando pipeline CI/CD che si autenticano con il provider ed eseguono i comandi di distribuzione all'interno del tuo progetto cloud.</p>
<p>A titolo di esempio, proviamo a distribuire un'applicazione FastAPI semplice e semplice che restituisca risposte LLM. La struttura del progetto sarà la seguente:</p>
<pre data-type="programlisting" translate="no">project/
│
├── host.json
├── main.py
├── app.py
└── requirements.txt</pre>
<p>Puoi quindi pacchettizzare un'applicazione FastAPI come <a href="https://oreil.ly/ZaOuF">funzione Azure serverless</a> seguendo i prossimi esempi di codice.</p>
<p>Dovrai installare il pacchetto <code translate="no">azure-functions</code> per eseguire il runtime delle funzioni serverless di Azure per lo sviluppo e i test locali:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ pip install azure-functions<code class="w" translate="no"/></pre>
<p>Quindi, crea il <em>file host.json</em> seguendo l'<a data-type="xref" href="#deployment_function_azure_host">Esempio 12-1</a>.</p>
<div data-type="example" id="deployment_function_azure_host">
<h5><span class="label">Esempio 12-1. </span>Configurazioni dell'host di Azure Functions (host.json)</h5>
<pre data-code-language="json" data-type="programlisting" translate="no"><code class="p" translate="no">{</code><code class="w" translate="no"/>
  <code class="nt" translate="no">"version"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"2.0"</code><code class="p" translate="no">,</code><code class="w" translate="no"/>
  <code class="nt" translate="no">"extensions"</code><code class="p" translate="no">:</code> <code class="p" translate="no">{</code><code class="w" translate="no"/>
    <code class="nt" translate="no">"http"</code><code class="p" translate="no">:</code> <code class="p" translate="no">{</code><code class="w" translate="no"/>
        <code class="nt" translate="no">"routePrefix"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">""</code><code class="w" translate="no"/>
    <code class="p" translate="no">}</code><code class="w" translate="no"/>
  <code class="p" translate="no">}</code><code class="w" translate="no"/>
<code class="p" translate="no">}</code><code class="w" translate="no"/></pre></div>
<p>Successivamente, implementa il tuo servizio GenAI con il servizio FastAPI come di consueto seguendo l'<a data-type="xref" href="#deployment_function_azure_app">Esempio 12-2</a>.</p>
<div data-type="example" id="deployment_function_azure_app">
<h5><span class="label">Esempio 12-2. </span>Semplice applicazione FastAPI che serve le risposte LLM</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="c1" translate="no"># app.py</code>

<code class="kn" translate="no">import</code> <code class="nn" translate="no">azure.functions</code> <code class="k" translate="no">as</code> <code class="nn" translate="no">func</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">fastapi</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">FastAPI</code>

<code class="n" translate="no">app</code> <code class="o" translate="no">=</code> <code class="n" translate="no">FastAPI</code><code class="p" translate="no">()</code>

<code class="nd" translate="no">@app</code><code class="o" translate="no">.</code><code class="n" translate="no">post</code><code class="p" translate="no">(</code><code class="s2" translate="no">"/generate/text"</code><code class="p" translate="no">,</code> <code class="n" translate="no">response_model_exclude_defaults</code><code class="o" translate="no">=</code><code class="kc" translate="no">True</code><code class="p" translate="no">)</code>
<code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">serve_text_to_text_controller</code><code class="p" translate="no">(</code><code class="n" translate="no">prompt</code><code class="p" translate="no">):</code>
<code class="o" translate="no">...</code></pre></div>
<p>Infine, avvolgi il tuo FastAPI <code translate="no">app</code> all'interno di <code translate="no">func.AsgiFunctionApp</code> in modo che il runtime delle funzioni serverless di Azure possa agganciarsi ad esso, come mostrato nell'<a data-type="xref" href="#deployment_function_azure_function">Esempio 12-3</a>.</p>
<div data-type="example" id="deployment_function_azure_function">
<h5><span class="label">Esempio 12-3. </span>Distribuzione di un servizio FastAPI con Azure Functions</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="c1" translate="no"># function.py</code>

<code class="kn" translate="no">import</code> <code class="nn" translate="no">azure.functions</code> <code class="k" translate="no">as</code> <code class="nn" translate="no">func</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">main</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">app</code> <code class="k" translate="no">as</code> <code class="n" translate="no">fastapi_app</code>

<code class="n" translate="no">app</code> <code class="o" translate="no">=</code> <code class="n" translate="no">func</code><code class="o" translate="no">.</code><code class="n" translate="no">AsgiFunctionApp</code><code class="p" translate="no">(</code>
    <code class="n" translate="no">app</code><code class="o" translate="no">=</code><code class="n" translate="no">fastapi_app</code><code class="p" translate="no">,</code> <code class="n" translate="no">http_auth_level</code><code class="o" translate="no">=</code><code class="n" translate="no">func</code><code class="o" translate="no">.</code><code class="n" translate="no">AuthLevel</code><code class="o" translate="no">.</code><code class="n" translate="no">ANONYMOUS</code>
<code class="p" translate="no">)</code></pre></div>
<p>Puoi quindi avviare l'applicazione della funzione eseguendo il comando <code translate="no">func start</code>, che dovrebbe essere disponibile come comando CLI una volta installato il<span class="keep-together">pacchetto</span> <code translate="no">azure-functions</code>:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ func start<code class="w" translate="no"/>

&gt;&gt; Found the following functions:<code class="w" translate="no"/>
&gt;&gt; Functions:<code class="w" translate="no"/>
&gt;&gt;        http_app_func: <code class="o" translate="no">[</code>GET,POST,DELETE,HEAD,PATCH,PUT,OPTIONS<code class="o" translate="no">]</code> <code class="se" translate="no">\</code>
                          http://localhost:7071//<code class="o" translate="no">{</code>*route<code class="o" translate="no">}</code><code class="w" translate="no"/>

&gt;&gt; Job host started<code class="w" translate="no"/></pre>
<p>Puoi quindi provare gli URL corrispondenti ai gestori nell'applicazione inviando richieste HTTP sia ai percorsi semplici che a quelli parametrizzati:</p>
<pre data-type="programlisting" translate="no">http://localhost:7071/generate/text
http://localhost:7071/&lt;other-paths&gt;</pre>
<p>Una volta pronta, puoi distribuire la tua funzione serverless FastAPI wrapped nel cloud Azure ed eseguire il seguente comando:</p>
<pre data-type="programlisting" translate="no">$ func azure functionapp publish &lt;FunctionAppName&gt;</pre>
<p>Il comando <code translate="no">publish</code> pubblicherà quindi i file del progetto dalla directory del progetto a <code translate="no">&lt;FunctionAppName&gt;</code> come pacchetto di distribuzione ZIP.</p>
<p>Dopo la distribuzione, puoi testare diversi percorsi sull'URL distribuito:</p>
<pre data-type="programlisting" translate="no">http://&lt;FunctionAppName&gt;.azurewebsites.net/generate/text
http://&lt;FunctionAppName&gt;.azurewebsites.net/&lt;other-paths&gt;</pre>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>Il provider Cloud scelto potrebbe non supportare il servizio di un server FastAPI all'interno del suo runtime di funzione. In questo caso, potresti voler cercare delle opzioni di distribuzione alternative. Altrimenti, dovrai migrare la logica dei tuoi endpoint al framework web supportato del runtime di funzione e creare funzioni separate per ogni endpoint.</p>
</div>
<p>Come vedi, distribuire i tuoi servizi FastAPI come funzioni cloud è semplice e ti permette di delegare la gestione e la scalabilità dei tuoi servizi ai<span class="keep-together">provider</span> cloud<span class="keep-together">.</span></p>
<p>Tieni presente che se decidi di servire un modello GenAI nel tuo servizio, le funzioni Cloud non sono adatte all'implementazione a causa dei loro brevi periodi di timeout (10 minuti). Al contrario, dovresti utilizzare un'API del provider del modello nei tuoi servizi in modo da avere un accesso affidabile e scalabile al modello senza essere vincolato da limiti di tempo di esecuzione.<a data-startref="ix_ch12-asciidoc4" data-type="indexterm" id="id1275"/><a data-startref="ix_ch12-asciidoc3" data-type="indexterm" id="id1276"/></p>
</div></section>
<section data-pdf-bookmark="Deploying to Managed App Platforms" data-type="sect2"><div class="sect2" id="id209">
<h2>Distribuzione su piattaforme di app gestite</h2>
<p><a data-primary="deployment of AI services" data-secondary="options" data-tertiary="deploying to managed app platforms" data-type="indexterm" id="ix_ch12-asciidoc5"/><a data-primary="managed app platforms, deploying to" data-type="indexterm" id="ix_ch12-asciidoc6"/>Oltre alle funzioni o alle macchine virtuali del cloud, puoi caricare la tua base di codice sotto forma di file ZIP sulle piattaforme di app gestite dai provider del cloud. Le piattaforme di app gestite ti permettono di delegare al provider del cloud diversi compiti relativi alla manutenzione e alla gestione dei tuoi servizi. In cambio, paghi solo per le risorse di calcolo gestite dal provider del cloud che servono alla tua applicazione. I sistemi del provider del cloud assegnano e ottimizzano le risorse in base alle esigenze della tua applicazione.</p>
<p>Esempi di tali servizi sono Azure App Services, AWS Elastic Beanstalk, Google App Engine o la piattaforma app Digital Ocean.</p>
<p>Esistono anche piattaforme di terze parti come Heroku, Hugging Face Spaces, railway.app, render.com o fly.io per distribuire i tuoi servizi direttamente dal codice contenuto nei repository, che ti sottraggono alcune decisioni in modo che tu possa distribuire più velocemente e più facilmente. Sotto il cofano, le piattaforme di app gestite da terze parti possono utilizzare l'infrastruttura dei principali provider Cloud come Azure, Google o AWS.</p>
<p>Il vantaggio principale della distribuzione su piattaforme di app gestite è la facilità e la velocità di distribuzione, networking, scalabilità e manutenzione dei tuoi servizi. Queste piattaforme ti forniscono gli strumenti necessari per proteggere, monitorare, scalare e gestire i tuoi servizi senza doverti preoccupare dell'allocazione delle risorse sottostanti, della sicurezza o degli aggiornamenti del software. Possono permetterti di configurare bilanciatori di carico, certificati SSL, mappature dei domini, monitoraggio e ambienti di staging, in modo che tu possa concentrarti maggiormente sullo sviluppo dell'applicazione piuttosto che sul carico di lavoro di distribuzione del progetto.</p>
<p><a data-primary="platform-as-a-service (PaaS)" data-type="indexterm" id="id1277"/><a data-primary="PaaS (platform-as-a-service)" data-type="indexterm" id="id1278"/>Poiché queste piattaforme seguono il modello di pagamento platform-as-a-service (PaaS), ti verrà addebitata una tariffa più alta rispetto all'utilizzo della tua infrastruttura o di risorse di livello inferiore come VM bare-bone o opzioni di calcolo serverless.<a data-primary="infrastructure-as-a-service (IaaS)" data-type="indexterm" id="id1279"/><a data-primary="IaaS (infrastructure-as-a-service)" data-type="indexterm" id="id1280"/>I servizi alternativi possono utilizzare il modello di pagamento infrastructure-as-a-service (IaaS), che spesso è più conveniente.</p>
<p>Personalmente, trovo che le piattaforme app gestite siano un modo conveniente per distribuire le mie applicazioni senza troppi problemi. Se sto lavorando a un prototipo e ho bisogno di rendere disponibili i miei servizi agli utenti il più velocemente possibile, le piattaforme app gestite sono la mia prima opzione. Tuttavia, tieni presente che se hai bisogno di accedere all'hardware GPU per eseguire i servizi di<span class="keep-together">inferenza</span>, dovrai affidarti a macchine virtuali dedicate, a server on-premises o a servizi di piattaforme AI specializzate per servire i tuoi modelli. Le piattaforme app possono solo fornire CPU, memoria e storage su disco per servire i servizi backend o le<span class="keep-together">applicazioni</span> frontend.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Tra le piattaforme AI gestite dai provider cloud ci sono Azure Machine Learning Studio o Azure AI, Google Cloud Vertex AI Platform, AWS Bedrock e SageMaker o IBM Watson Studio.</p>
<p>Esistono anche piattaforme di terze parti per ospitare i tuoi modelli, come Hugging Face Inference Endpoints, Weights &amp; Biases Platform o Replicate.</p>
</div>
<p>Il deploy dai repository di codice richiede spesso l'aggiunta di alcuni file di configurazione alla radice del progetto, a seconda della piattaforma di app su cui verrà effettuato il deploy. Il processo dipende anche dal fatto che la piattaforma di app supporti il runtime dell'applicazione, le librerie e le versioni del framework che stai utilizzando, per cui il successo del deploy non è sempre garantito. Inoltre, è spesso difficile migrare ai runtime o alle versioni supportate.</p>
<p>A causa di questi problemi imprevisti, molti ingegneri stanno passando alle tecnologie di containerizzazione come Docker o Podman per impacchettare e distribuire i loro servizi. Queste applicazioni containerizzate possono poi essere distribuite direttamente su qualsiasi piattaforma di app che supporti i container, con la garanzia che l'applicazione verrà eseguita indipendentemente dalle risorse sottostanti, dal runtime o dalle versioni delle dipendenze.</p>
<p>Il deploy dei servizi con i container è oggi una delle strategie più affidabili per inviare le tue applicazioni in produzione e renderle accessibili agli utenti.<a data-startref="ix_ch12-asciidoc6" data-type="indexterm" id="id1281"/><a data-startref="ix_ch12-asciidoc5" data-type="indexterm" id="id1282"/></p>
</div></section>
<section data-pdf-bookmark="Deploying with Containers" data-type="sect2"><div class="sect2" id="id210">
<h2>Distribuire con i container</h2>
<p><a data-primary="deployment of AI services" data-secondary="options" data-tertiary="deploying with containers" data-type="indexterm" id="ix_ch12-asciidoc7"/><a data-primary="options, deploying with containers" data-type="indexterm" id="ix_ch12-asciidoc8"/>Un <em>container</em> è un ambiente isolato progettato per la creazione e l'esecuzione di applicazioni. I container possono eseguire i tuoi servizi in modo rapido e affidabile in qualsiasi ambiente informatico, confezionando il tuo codice con tutte le dipendenze necessarie.</p>
<p>I container si basano su un metodo di virtualizzazione del sistema operativo che consente loro di essere eseguiti su hardware fisico, nel cloud, su macchine virtuali o su più<span class="keep-together">sistemi</span> operativi.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Analogamente alle piattaforme di app gestite e alle funzioni serverless, puoi configurare i container in modo che si riavviino automaticamente e si auto-riparino, se la tua applicazione esce per qualsiasi motivo.</p>
</div>
<p><a data-primary="containerization" data-secondary="virtualization versus" data-type="indexterm" id="id1283"/><a data-primary="Docker, containerization with" data-secondary="containerization versus virtualization" data-type="indexterm" id="id1284"/><a data-primary="virtualization, containerization versus" data-type="indexterm" id="id1285"/>A differenza delle macchine virtuali, le cui tecnologie sottostanti si basano sulla virtualizzazione, i container si basano sulla containerizzazione.</p>
<p>La containerizzazione impacchetta le applicazioni e le loro dipendenze in unità leggere e isolate che condividono il kernel del sistema operativo host. D'altra parte, la virtualizzazione permette di eseguire più sistemi operativi su una singola macchina fisica grazie agli hypervisor. Pertanto, a differenza delle macchine virtuali, i container non virtualizzano le risorse hardware, ma vengono eseguiti su una piattaforma di runtime per container che astrae le risorse, rendendoli leggeri (cioè con pochi megabyte da memorizzare) e più veloci delle macchine virtuali, poiché non richiedono un sistema operativo separato per container.</p>
<div data-type="note" epub:type="note"><h6>Nota</h6>
<p>In sostanza, la virtualizzazione consiste nell'astrarre le risorse hardware della macchina host, mentre la containerizzazione consiste nell'astrarre il kernel del sistema operativo e nell'eseguire tutti i componenti dell'applicazione all'interno di un'unità isolata chiamata <em>container</em>.</p>
</div>
<p>La<a data-type="xref" href="#deployment_container_architecture">Figura 12-3</a> mette a confronto le architetture dei sistemi di virtualizzazione e di containerizzazione.</p>
<figure><div class="figure" id="deployment_container_architecture">
<img alt="bgai 1203" src="assets/bgai_1203.png" width="1027" height="705"/>
<h6><span class="label">Figura 12-3. </span>Confronto tra le architetture dei sistemi di containerizzazione e virtualizzazione</h6>
</div></figure>
<p>Il vantaggio principale dell'utilizzo dei container è la loro <em>portabilità</em>, <em>velocità di avvio</em>, <em>compattezza</em> e <em>affidabilità</em> in diversi ambienti informatici, in quanto non richiedono un sistema operativo guest e un livello software di hypervisor.</p>
<p>Questo li rende perfetti per distribuire i tuoi servizi con risorse, sforzi di distribuzione e spese generali minime. Si avviano più velocemente di una macchina virtuale e anche la scalabilità è più semplice. Puoi aggiungere altri container per <em>scalare orizzontalmente</em> i tuoi<span class="keep-together">servizi.</span></p>
<p>Per aiutarti a containerizzare le tue applicazioni, puoi affidarti a piattaforme come Docker che sono state testate a fondo dalle comunità MLOps e DevOps<a data-startref="ix_ch12-asciidoc8" data-type="indexterm" id="id1286"/><a data-startref="ix_ch12-asciidoc7" data-type="indexterm" id="id1287"/>.<a data-startref="ix_ch12-asciidoc1" data-type="indexterm" id="id1288"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Containerization with Docker" data-type="sect1"><div class="sect1" id="id211">
<h1>La containerizzazione con Docker</h1>
<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-type="indexterm" id="ix_ch12-asciidoc9"/><a data-primary="Docker, containerization with" data-type="indexterm" id="ix_ch12-asciidoc10"/>Docker è una piattaforma di containerizzazione utilizzata per costruire, spedire ed eseguire container. Al momento in cui scriviamo, Docker detiene circa <a href="https://oreil.ly/A5x63">il 22% della quota di mercato</a> delle piattaforme di virtualizzazione con oltre 9 milioni di sviluppatori e <a href="https://oreil.ly/8-wx4">11 miliardi di download mensili di immagini</a>, il che la rende la piattaforma di containerizzazione più popolare. Molti ambienti server e fornitori di cloud supportano Docker all'interno di molte varianti di server Linux e Windows.</p>
<p>È probabile che se devi distribuire i tuoi servizi GenAI, l'opzione più semplice e diretta sia quella di utilizzare Docker per containerizzare la tua applicazione. Tuttavia, per trovarti a tuo agio con Docker, devi comprenderne l'architettura e i sottosistemi sottostanti, come lo storage e il networking.</p>
<section data-pdf-bookmark="Docker Architecture" data-type="sect2"><div class="sect2" id="id266">
<h2>Architettura di Docker</h2>
<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="Docker architecture" data-type="indexterm" id="id1289"/><a data-primary="Docker, containerization with" data-secondary="Docker architecture" data-type="indexterm" id="id1290"/>Il sistema Docker è composto da un motore, un client e un server:</p>
<dl>
<dt>Motore Docker</dt>
<dd>
<p>Il motore è costituito da diversi componenti, tra cui un client e un server che girano sullo stesso sistema operativo.</p>
</dd>
<dt>Client Docker</dt>
<dd>
<p>Docker viene fornito con uno <em>strumento CLI</em> chiamato <code translate="no">docker</code> e un'applicazione con interfaccia grafica (GUI) chiamata <em>Docker Desktop</em>. Utilizzando l'implementazione client-server, il client Docker può comunicare con l'istanza server locale o remota utilizzando un'API REST per gestire i container eseguendo comandi come l'esecuzione, l'arresto e la terminazione dei container. Puoi anche utilizzare il client per prelevare le immagini da un registro di immagini.</p>
</dd>
<dt>Server Docker</dt>
<dd>
<p>Il server è un <em>demone</em> chiamato <code translate="no">dockerd</code>. Il demone Docker risponde alle richieste HTTP del client tramite l'API REST e può interagire con altri demoni. È anche responsabile del monitoraggio del ciclo di vita dei container.</p>
</dd>
</dl>
<p>La piattaforma Docker ti permette anche di creare e configurare oggetti come <em>reti</em>, <em>volumi di archiviazione</em>, <em>plug-in</em> e oggetti di servizio per supportare le tue implementazioni.</p>
<p>La cosa più importante è che per containerizzare le tue applicazioni con Docker, dovrai creare delle immagini Docker.</p>
<p>Un'<em>immagine Docker</em> è un pacchetto portatile contenente software e funge da ricetta per la creazione e l'esecuzione dei container delle tue applicazioni. In sostanza, un container è un'istanza in memoria di un'immagine.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>L'immagine di un contenitore è <em>immutabile</em>, quindi una volta creata non è più possibile modificarla. Puoi solo aggiungere e non sottrarre un'immagine. Dovrai crearne una nuova se vuoi applicare delle modifiche.</p>
</div>
<p>Le immagini Docker sono il primo passo verso la containerizzazione dei tuoi servizi, come imparerai nella prossima sezione.</p>
</div></section>
<section data-pdf-bookmark="Building Docker Images" data-type="sect2"><div class="sect2" id="id212">
<h2>Creazione di immagini Docker</h2>
<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="building Docker images" data-type="indexterm" id="ix_ch12-asciidoc11"/><a data-primary="Docker, containerization with" data-secondary="building Docker images" data-type="indexterm" id="ix_ch12-asciidoc12"/>Immaginiamo di avere un piccolo servizio GenAI che utilizza FastAPI, come mostrato nell'<a data-type="xref" href="#docker_app">Esempio 12-4</a>, che vogliamo containerizzare.</p>
<div data-type="example" id="docker_app">
<h5><span class="label">Esempio 12-4. </span>Un semplice servizio GenAI FastAPI</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="c1" translate="no"># main.py</code>

<code class="kn" translate="no">from</code> <code class="nn" translate="no">fastapi</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">FastAPI</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">models</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">generate_text</code> <a class="co" href="#callout_deployment_of_ai_services_CO1-1" id="co_deployment_of_ai_services_CO1-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>

<code class="n" translate="no">app</code> <code class="o" translate="no">=</code> <code class="n" translate="no">FastAPI</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code>

<code class="nd" translate="no">@app</code><code class="o" translate="no">.</code><code class="n" translate="no">post</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">/generate</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>
<code class="k" translate="no">def</code> <code class="nf" translate="no">generate_text</code><code class="p" translate="no">(</code><code class="n" translate="no">prompt</code><code class="p" translate="no">:</code> <code class="nb" translate="no">str</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code>
    <code class="k" translate="no">return</code> <code class="n" translate="no">generate_text</code><code class="p" translate="no">(</code><code class="n" translate="no">prompt</code><code class="p" translate="no">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_deployment_of_ai_services_CO1-1" id="callout_deployment_of_ai_services_CO1-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Supponiamo che la funzione <code translate="no">generate_text</code> chiami un fornitore di modelli API o un server di modelli esterno.</p></dd>
</dl></div>
<p><a data-primary="Dockerfile" data-type="indexterm" id="ix_ch12-asciidoc13"/>Per creare questa applicazione in un'immagine contenitore, dovrai scrivere le istruzioni in un file di testo chiamato <em>Dockerfile</em>. All'interno di questo Dockerfile, puoi specificare i seguenti<span class="keep-together">componenti:</span></p>
<ul>
<li>
<p>L'immagine <em>di base</em> da cui creare una nuova immagine, che fornisce il sistema operativo e l'ambiente su cui vengono costruiti i livelli applicativi aggiuntivi.</p>
</li>
<li>
<p>Comandi per aggiornare il sistema operativo guest e installare software aggiuntivo</p>
</li>
<li>
<p>Costruisci gli artefatti da includere, come ad esempio il codice dell'applicazione.</p>
</li>
<li>
<p>Servizi da esporre, come la configurazione dello storage e della rete.</p>
</li>
<li>
<p>Il comando da eseguire all'avvio del contenitore</p>
</li>
</ul>
<p>L<a data-type="xref" href="#containers_dockerfile">'esempio 12-5</a> illustra come costruire l'immagine di un'applicazione in un file Docker.</p>
<div data-type="example" id="containers_dockerfile">
<h5><span class="label">Esempio 12-5. </span>Profilo Docker per containerizzare un'applicazione FastAPI</h5>
<pre data-code-language="dockerfile" data-type="programlisting" translate="no"><code class="k" translate="no">ARG</code> <code class="nv" translate="no">PYTHON_VERSION</code><code class="o" translate="no">=</code><code class="m" translate="no">3</code><code translate="no">.12</code>
<code class="k" translate="no">FROM</code> <code class="s" translate="no">python:${PYTHON_VERSION}-slim</code> <code class="k" translate="no">as</code> <code class="s" translate="no">base</code> <a class="co" href="#callout_deployment_of_ai_services_CO2-1" id="co_deployment_of_ai_services_CO2-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>

<code class="k" translate="no">WORKDIR</code> <code class="s" translate="no">/code </code><a class="co" href="#callout_deployment_of_ai_services_CO2-2" id="co_deployment_of_ai_services_CO2-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a>

<code class="k" translate="no">COPY</code> <code translate="no">requirements.txt</code> <code translate="no">.</code> <a class="co" href="#callout_deployment_of_ai_services_CO2-3" id="co_deployment_of_ai_services_CO2-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a>

<code class="k" translate="no">RUN</code> <code translate="no">pip</code> <code translate="no">install</code> <code translate="no">--no-cache-dir</code> <code translate="no">--upgrade</code> <code translate="no">-r</code> <code translate="no">requirements.txt</code> <a class="co" href="#callout_deployment_of_ai_services_CO2-4" id="co_deployment_of_ai_services_CO2-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a>

<code class="k" translate="no">COPY</code> <code translate="no">.</code> <code translate="no">.</code> <a class="co" href="#callout_deployment_of_ai_services_CO2-5" id="co_deployment_of_ai_services_CO2-5"><img alt="5" src="assets/5.png" width="12" height="12"/></a>

<code class="k" translate="no">EXPOSE</code> <code class="s" translate="no">8000 </code><a class="co" href="#callout_deployment_of_ai_services_CO2-6" id="co_deployment_of_ai_services_CO2-6"><img alt="6" src="assets/6.png" width="12" height="12"/></a>

<code class="k" translate="no">CMD</code> <code class="p" translate="no">[</code><code class="s2" translate="no">"uvicorn"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"main:app"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"--host"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"0.0.0.0"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"--port"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"8000"</code><code class="p" translate="no">]</code> <a class="co" href="#callout_deployment_of_ai_services_CO2-7" id="co_deployment_of_ai_services_CO2-7"><img alt="7" src="assets/7.png" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_deployment_of_ai_services_CO2-1" id="callout_deployment_of_ai_services_CO2-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Usa l'immagine slim ufficiale di Python 3.12 come immagine <code translate="no">base</code>.<sup><a data-type="noteref" href="ch12.html#id1291" id="id1291-marker" translate="no">1</a></sup></p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO2-2" id="callout_deployment_of_ai_services_CO2-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Imposta la directory di lavoro all'interno del contenitore su <code translate="no">/code</code>.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO2-3" id="callout_deployment_of_ai_services_CO2-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Copia il file <code translate="no">requirements.txt</code> dall'host alla directory corrente del contenitore.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO2-4" id="callout_deployment_of_ai_services_CO2-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a></dt>
<dd><p>Installa le dipendenze di Python elencate in <code translate="no">requirements.txt</code> senza utilizzare la cache.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO2-5" id="callout_deployment_of_ai_services_CO2-5"><img alt="5" src="assets/5.png" width="12" height="12"/></a></dt>
<dd><p>Copia tutti i file dalla directory corrente dell'host alla directory corrente del contenitore.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO2-6" id="callout_deployment_of_ai_services_CO2-6"><img alt="6" src="assets/6.png" width="12" height="12"/></a></dt>
<dd><p>Informa il demone Docker che l'applicazione all'interno del contenitore è in ascolto su <code translate="no">8000</code> in fase di runtime. Il comando <code translate="no">EXPOSE</code> non mappa o consente automaticamente l'accesso alle porte.<sup><a data-type="noteref" href="ch12.html#id1292" id="id1292-marker" translate="no">2</a></sup></p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO2-7" id="callout_deployment_of_ai_services_CO2-7"><img alt="7" src="assets/7.png" width="12" height="12"/></a></dt>
<dd><p>Eseguire il server <code translate="no">uvicorn</code> con il modulo applicativo e la configurazione host/porta, quando viene lanciato il container.</p></dd>
</dl></div>
<p>In questo capitolo non tratteremo l'intera <a href="https://oreil.ly/8fJ6l">specifica del Dockerfile</a>, ma notiamo come ogni comando modifica la struttura dell'immagine che ti permette di eseguire tutti i servizi GenAI all'interno di un container.</p>
<p>Puoi utilizzare il comando <code translate="no">docker build</code> per creare l'immagine riportata nell'<a data-type="xref" href="#containers_dockerfile">Esempio 12-5</a>:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker build -t genai-service .<code class="w" translate="no"/></pre>
<p>Nota i passaggi elencati nell'output: quando ogni passaggio viene eseguito, viene aggiunto un nuovo livello all'immagine che stai costruendo.</p>
<p>Una volta ottenuta l'immagine di un container, puoi utilizzare i registri dei container per archiviare, condividere e scaricare le immagini<a data-startref="ix_ch12-asciidoc13" data-type="indexterm" id="id1293"/>.<a data-startref="ix_ch12-asciidoc12" data-type="indexterm" id="id1294"/><a data-startref="ix_ch12-asciidoc11" data-type="indexterm" id="id1295"/></p>
</div></section>
<section data-pdf-bookmark="Container Registries" data-type="sect2"><div class="sect2" id="id213">
<h2>Registri dei contenitori</h2>
<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="container registries" data-type="indexterm" id="ix_ch12-asciidoc14"/><a data-primary="Docker, containerization with" data-secondary="container registries" data-type="indexterm" id="ix_ch12-asciidoc15"/>Per archiviare e distribuire le immagini in un ambiente a controllo di versione, puoi utilizzare i <em>registri dei container</em>, che includono sia la versione pubblica che quella privata.</p>
<p><a data-primary="Docker Hub" data-type="indexterm" id="ix_ch12-asciidoc16"/><em>Docker Hub</em> è un registro di container gestito in modalità software-as-a-service (SaaS) per archiviare e distribuire le immagini create dall'utente.</p>
<p>Docker Hub è pubblico per impostazione predefinita, ma puoi anche utilizzare registri privati autogestiti o di provider cloud come Azure Container Registry (ACR), AWS Elastic Container Registry (ECR) o Google Cloud Artifact Registry.</p>
<p>Puoi vedere l'architettura completa della piattaforma Docker nella <a data-type="xref" href="#docker_architecture">Figura 12-4</a>.</p>
<figure><div class="figure" id="docker_architecture">
<img alt="bgai 1204" src="assets/bgai_1204.png" width="1247" height="545"/>
<h6><span class="label">Figura 12-4. </span>Architettura di sistema della piattaforma Docker</h6>
</div></figure>
<p class="less_space pagebreak-before">Come puoi vedere nella <a data-type="xref" href="#docker_architecture">Figura 12-4</a>, il demone Docker gestisce i contenitori e le immagini. Crea i contenitori dalle immagini e comunica con il client Docker, gestendo i comandi per costruire ed eseguire le immagini. Il demone Docker può anche prelevare le immagini da un registro (ad esempio, Docker Hub) che contiene immagini come Ubuntu, Redis o PostgreSQL.</p>
<p>Utilizzando il registro di Docker Hub, puoi accedere ad altre immagini fornite, oltre a distribuire e controllare la tua versione. I registri come Docker Hub svolgono un ruolo cruciale nella scalabilità dei tuoi servizi, poiché le piattaforme di orchestrazione dei container come Kubernetes hanno bisogno di accedere ai registri per estrarre ed eseguire istanze multiple di container dalle immagini.</p>
<p>Puoi prelevare le immagini pubbliche da Docker Hub utilizzando il comando <code translate="no">docker pull</code>:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker image pull python:3.12-slim<code class="w" translate="no"/>

bookworm: Pulling from library/python<code class="w" translate="no"/>
Digest: sha256:3f1d6c17773a45c97bd8f158d665c9709d7b29ed7917ac934086ad96f92e4510<code class="w" translate="no"/>
Status: Downloaded newer image <code class="k" translate="no">for</code> python:3.12-slim<code class="w" translate="no"/>
docker.io/library/python:3.12-slim<code class="w" translate="no"/></pre>
<p>Quando effettui il push e il pull delle immagini, dovrai specificare un <em>tag</em> utilizzando la sintassi <code translate="no">&lt;name&gt;:&lt;tag&gt;</code>. Se non fornisci un tag, il motore di Docker utilizzerà il tag <code translate="no">latest</code> per impostazione predefinita.</p>
<p>Oltre al pulling, puoi anche memorizzare le tue immagini nei registri dei container. Per prima cosa, devi creare e taggare la tua immagine con un'etichetta di versione e l'URL del repository dell'immagine:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker build -t genai-service:latest .<code class="w" translate="no"/>

$ docker image tag genai-service:latest docker.io/myrepo/genai-service:latest<code class="w" translate="no"/></pre>
<p>Una volta che l'immagine è stata costruita e contrassegnata, puoi inviarla al registro dei container di Docker Hub utilizzando il comando <code translate="no">docker push</code>. Potrebbe essere necessario effettuare il login per autenticarsi con l'hub:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker login<code class="w" translate="no"/>

$ docker image push docker.io/myrepo/genai-service:latest<code class="w" translate="no"/>

195be5f8be1d: Pushed<code class="w" translate="no"/></pre>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>Fai attenzione a non sovrascrivere il tag di un'immagine in molti repository. Ad esempio, un'immagine creata e taggata <code translate="no">genai:latest</code> in un repository può essere sovrascritta da un'altra immagine taggata <code translate="no">genai:latest</code>.<a data-startref="ix_ch12-asciidoc16" data-type="indexterm" id="id1296"/></p>
</div>
<p>Ora che l'immagine è memorizzata nel registro di sistema, puoi richiamarla su un altro computer o in un secondo momento per eseguire l'immagine senza doverla ricostruire.<sup><a data-type="noteref" href="ch12.html#id1297" id="id1297-marker" translate="no">3</a></sup> o in un secondo momento per eseguire l'immagine senza doverla ricostruire.<a data-startref="ix_ch12-asciidoc15" data-type="indexterm" id="id1298"/><a data-startref="ix_ch12-asciidoc14" data-type="indexterm" id="id1299"/></p>
</div></section>
<section data-pdf-bookmark="Container Filesystem and Docker Layers" data-type="sect2"><div class="sect2" id="id214">
<h2>Filesystem del container e livelli di Docker</h2>
<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="container filesystem and Docker layers" data-type="indexterm" id="ix_ch12-asciidoc17"/><a data-primary="Docker, containerization with" data-secondary="container filesystem and Docker layers" data-type="indexterm" id="ix_ch12-asciidoc18"/><a data-primary="Unionfs" data-type="indexterm" id="ix_ch12-asciidoc19"/>Quando <a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="Unionfs" data-type="indexterm" id="id1300"/><a data-primary="Docker, containerization with" data-secondary="Unionfs" data-type="indexterm" id="id1301"/>costruisce l'immagine, Docker utilizza un filesystem speciale chiamato <code translate="no">Unionfs</code> (stackable unification filesystem) per unire il contenuto di diverse directory (cioè <em>rami</em> o, nella terminologia di Docker, <em>livelli</em>), mantenendo il loro contenuto fisico separato.</p>
<p>Utilizzando <code translate="no">Unionfs</code>, le directory di filesystem distinti possono essere combinate e sovrapposte per formare un singolo filesystem virtuale coerente, come mostrato nella <a data-type="xref" href="#docker_unionfs">Figura 12-5</a>.</p>
<figure><div class="figure" id="docker_unionfs">
<img alt="bgai 1205" src="assets/bgai_1205.png" width="1382" height="1269"/>
<h6><span class="label">Figura 12-5. </span>File system virtuale unificato da più file system</h6>
</div></figure>
<p>Utilizzando il sito <code translate="no">Unionfs</code>, Docker può aggiungere o rimuovere rami mentre costruisci il filesystem del tuo container a partire da un'immagine.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1302">
<h1>Perché i contenitori Docker usano Unionfs?</h1>
<p>Immagina di avere 10 istanze di container in esecuzione da un'immagine di 1 GB.</p>
<p>Un contenitore è essenzialmente un processo. In Linux, i nuovi processi vengono creati tramite biforcazione di quelli esistenti. L'operazione di biforcazione genera uno spazio di indirizzi separato per il processo figlio, che contiene una copia esatta dei segmenti di memoria del genitore. Per creare un nuovo contenitore, tutti i file dei livelli dell'immagine vengono copiati nello spazio dei nomi del contenitore.</p>
<p>Se i tuoi container utilizzano un filesystem concreto, avrai bisogno di 10 GB di memoria fisica per farli funzionare, quindi l'utilizzo dello spazio su disco non sarà ottimizzato. Inoltre, dato che vuoi che i tuoi container si avviino velocemente (idealmente entro un secondo), dover copiare 1 GB di file dai livelli di immagine allo spazio dei nomi dei container aumenterebbe significativamente il tempo di avvio a freddo.</p>
<p>Per questo motivo, è necessario un meccanismo che permetta di condividere in modo efficiente i segmenti di memoria fisica tra i vari container. Per questo motivo, Unionfs è ora utilizzato nei container per dare una visione unificata ai file e alle directory di filesystem separati.</p>
<p>In sostanza, monta più directory su un'unica radice, quindi puoi considerarlo un meccanismo di montaggio piuttosto che un filesystem.</p>
</div></aside>
<p>Per illustrare il meccanismo dell'architettura a strati nei container, esaminiamo l'immagine dell'<a data-type="xref" href="#containers_dockerfile">Esempio 12-5</a>.</p>
<p>Quando costruisci l'immagine usando l'<a data-type="xref" href="#containers_dockerfile">Esempio 12-5</a>, stai stratificando un'immagine base di Python 3.12 in esecuzione su una distribuzione Linux in cima al filesystem root. Successivamente, aggiungi <em>requirements.txt</em> in cima all'immagine base di Python e poi installi le dipendenze in cima al livello <em>requirements.txt</em>.
Poi aggiungi un nuovo livello copiando il contenuto della cartella del progetto nel contenitore, sovrapponendolo a tutto il resto. Infine, quando avvii il contenitore con il comando <code translate="no">uvicorn</code>, aggiungi un ultimo livello scrivibile come parte del filesystem del contenitore. Di conseguenza, l'ordine dei livelli diventa importante quando si costruiscono immagini Docker.</p>
<p>La<a data-type="xref" href="#docker_branches">Figura 12-6</a> mostra l'architettura del filesystem a strati.</p>
<figure><div class="figure" id="docker_branches">
<img alt="bgai 1206" src="assets/bgai_1206.png" width="1421" height="918"/>
<h6><span class="label">Figura 12-6. </span>Architettura del filesystem Unionfs a strati</h6>
</div></figure>
<p>Nell'<a data-type="xref" href="#containers_dockerfile">Esempio 12-5</a>, ogni passaggio di comando crea un'immagine nella cache mentre il processo di compilazione finalizza l'immagine del contenitore. Per eseguire i comandi, vengono creati dei contenitori intermedi che vengono poi eliminati automaticamente. L'immagine sottostante nella cache viene mantenuta sull'host di compilazione e non viene rimossa. Queste immagini temporanee vengono sovrapposte all'immagine precedente e combinate in un'unica immagine una volta completati tutti i passaggi. Questa ottimizzazione consente alle future compilazioni di riutilizzare queste immagini per accelerare i tempi di compilazione.</p>
<p>Alla fine, il contenitore comprenderà uno o più livelli di immagine e un livello finale effimero del contenitore (cioè che non sarà persistito) quando il contenitore verrà distrutto.<a data-startref="ix_ch12-asciidoc19" data-type="indexterm" id="id1303"/><a data-startref="ix_ch12-asciidoc18" data-type="indexterm" id="id1304"/><a data-startref="ix_ch12-asciidoc17" data-type="indexterm" id="id1305"/></p>
</div></section>
<section data-pdf-bookmark="Docker Storage" data-type="sect2"><div class="sect2" id="id215">
<h2>Stoccaggio Docker</h2>
<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="Docker storage mechanisms" data-type="indexterm" id="ix_ch12-asciidoc20"/><a data-primary="Docker, containerization with" data-secondary="Docker storage mechanisms" data-type="indexterm" id="ix_ch12-asciidoc21"/>In questa sezione imparerai a conoscere i vari meccanismi di archiviazione di Docker. Durante lo sviluppo dei tuoi servizi come container, puoi utilizzare questi strumenti per gestire la persistenza dei dati, la condivisione dei dati tra i container e il mantenimento dello stato tra i riavvii dei container.</p>
<p><a data-primary="ephemeral storage" data-type="indexterm" id="id1306"/>Quando lavori con i container, la tua applicazione potrebbe aver bisogno di scrivere dati sul disco, che persisteranno in uno storage <em>effimero</em>. Lo storage effimero è uno storage temporaneo di breve durata che viene cancellato una volta che il container viene fermato, riavviato o rimosso. Se riavvii il tuo container, noterai che i dati precedentemente persistenti non sono più disponibili. Sotto il cofano, Docker scrive i dati runtime in un livello container effimero scrivibile nel filesystem virtuale del container.</p>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>Se ti affidi alla configurazione di archiviazione predefinita del contenitore, perderai tutti i dati generati dall'applicazione e i file di log che hai scritto su disco durante il runtime del contenitore.</p>
</div>
<p>Per evitare la perdita dei dati e dei log del runtime dell'applicazione, hai a disposizione diverse opzioni di archiviazione che ti permettono di persistere i dati durante la vita di un container. Durante lo sviluppo, puoi utilizzare <em>volumi</em> o <em>mount bind</em> per persistere i dati nel filesystem del sistema operativo host o affidarti a database locali per la persistenza dei dati.</p>
<p>La<a data-type="xref" href="#docker_storage_options">Tabella 12-1</a> mostra le opzioni di montaggio dello storage di Docker.</p>
<table class="striped" id="docker_storage_options">
<caption><span class="label">Tabella 12-1. </span>Supporti di archiviazione di Docker</caption>
<thead>
<tr>
<th>Immagazzinamento</th>
<th>Descrizione</th>
<th>Casi d'uso</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Volumi</p></td>
<td><p>Soluzione di archiviazione ottimizzata per l'I/O e preferita. Gestita da Docker e memorizzata in una posizione specifica sull'host, ma disaccoppiata dalla struttura del filesystem dell'host.</p></td>
<td><p>Se hai bisogno di archiviare e condividere i dati tra più contenitori.</p><p>Se non hai bisogno di modificare file o directory dell'host.</p></td>
</tr>
<tr>
<td><p>Montaggi vincolati</p></td>
<td><p>Montano i file o le directory dell'host nel contenitore, ma hanno funzionalità limitate rispetto ai volumi.</p></td>
<td><p>Se vuoi che sia i container che i processi host accedano e modifichino i file e le directory dell'host. Ad esempio, durante lo sviluppo e i test locali.</p></td>
</tr>
<tr>
<td><p>Supporti temporanei (tmpfs)</p></td>
<td><p>Memorizza i dati nella memoria dell'host (RAM) e non li scrive mai nel container o nel filesystem dell'host.</p></td>
<td><p>Se hai bisogno di un'archiviazione temporanea ad alte prestazioni per dati sensibili o non statistici che non persistono dopo l'arresto del contenitore.</p></td>
</tr>
</tbody>
</table>
<p><a data-type="xref" href="#docker_storage_mounts">La Figura 12-7</a> mostra i diversi tipi di supporti.</p>
<figure><div class="figure" id="docker_storage_mounts">
<img alt="bgai 1207" src="assets/bgai_1207.png" width="753" height="417"/>
<h6><span class="label">Figura 12-7. </span>Supporti di archiviazione di Docker</h6>
</div></figure>
<p>Ora analizzeremo in dettaglio ogni opzione di archiviazione in modo che tu possa simulare il tuo ambiente di produzione a livello locale con i container Docker utilizzando l'archiviazione appropriata. Quando distribuisci i container in produzione all'interno di un ambiente cloud, puoi utilizzare un database o un'offerta di archiviazione cloud per la persistenza dei dati invece dei volumi Docker o dei mount bind per centralizzare l'archiviazione su più container.</p>
<section data-pdf-bookmark="Docker volumes" data-type="sect3"><div class="sect3" id="id216">
<h3>Volumi di Docker</h3>
<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="Docker volumes" data-type="indexterm" id="id1307"/><a data-primary="Docker, containerization with" data-secondary="Docker storage mechanisms" data-tertiary="Docker volumes" data-type="indexterm" id="id1308"/><a data-primary="volumes (Docker)" data-type="indexterm" id="id1309"/>Docker ti permette di creare <em>volumi</em> isolati per conservare i dati delle applicazioni tra i runtime dei container. Per creare un volume, puoi eseguire il seguente comando:</p>
<pre data-type="programlisting" translate="no">$ docker volume create -n data</pre>
<p>Una volta creati, puoi utilizzare i volumi per conservare i dati tra un'esecuzione e l'altra del contenitore.  I volumi ti permettono anche di persistere i dati quando utilizzi i contenitori di database e di memoria.</p>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>Il riavvio di un contenitore di database con le nuove variabili d'ambiente potrebbe non essere sufficiente per ripristinare le nuove impostazioni.</p>
<p>Alcuni sistemi di database potrebbero richiedere di ricreare il volume del contenitore se devi aggiornare le impostazioni come le<span class="keep-together">credenziali</span> dell'utente amministratore<span class="keep-together">.</span></p>
</div>
<p>Per impostazione predefinita, tutti i volumi creati verranno memorizzati nel filesystem della macchina host fino a quando non li rimuoverai esplicitamente con il comando <code translate="no">docker volume remove</code>.</p>
</div></section>
<section data-pdf-bookmark="Bind mounts" data-type="sect3"><div class="sect3" id="id217">
<h3>Montaggi vincolati</h3>
<p><a data-primary="bind mounts (Docker)" data-type="indexterm" id="id1310"/><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="bind mounts" data-type="indexterm" id="id1311"/><a data-primary="Docker, containerization with" data-secondary="Docker storage mechanisms" data-tertiary="bind mounts" data-type="indexterm" id="id1312"/>Oltre ai volumi, puoi anche utilizzare le mappature del filesystem tramite i <em>mount bind</em> dei volumi che mappano le directory che risiedono sul filesystem dell'host sul filesystem del container, come mostrato nella <a data-type="xref" href="#docker_bind_mounts">Figura 12-8</a>.</p>
<figure><div class="figure" id="docker_bind_mounts">
<img alt="bgai 1208" src="assets/bgai_1208.png" width="1190" height="372"/>
<h6><span class="label">Figura 12-8. </span>Bind mount tra il filesystem host e un container</h6>
</div></figure>
<p>Il montaggio avviene all'avvio del container. Con le directory montate, puoi accedervi direttamente dall'interno del<span class="keep-together">container.</span>Puoi leggere e persistere i dati nelle directory montate durante l'esecuzione e l'arresto del container.</p>
<p>Per eseguire un container con un volume bind mount, puoi utilizzare il seguente comando:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker run -v src:/app genai-service<code class="w" translate="no"/></pre>
<p>In questo caso, il flag <code translate="no">-v</code> ti permette di mappare la directory dell'host in una directory del contenitore utilizzando la sintassi <code translate="no">&lt;host_dir&gt;:&lt;container_dir&gt;</code>.</p>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>La funzionalità del comando <code translate="no">COPY</code> che utilizzi in un file Docker è diversa da quella del montaggio delle directory.</p>
<p>Il primo crea una copia separata di una directory host nel contenitore durante il processo di creazione dell'immagine, mentre il secondo ti permette di accedere e aggiornare la directory host mappata dall'interno del<span class="keep-together">contenitore.</span></p>
<p>Questo significa che se non stai attento, puoi modificare o cancellare involontariamente tutti i file originali sul computer host in modo permanente, dall'interno del contenitore.</p>
</div>
<p>I volumi di montaggio Bind possono essere ancora utili in un ambiente di sviluppo locale: mentre modifichi il codice sorgente dei tuoi servizi, potrai osservare in tempo reale l'impatto delle modifiche sui container applicativi in esecuzione.</p>
</div></section>
<section data-pdf-bookmark="Temporary mounts (tmpfs)" data-type="sect3"><div class="sect3" id="id218">
<h3>Supporti temporanei (tmpfs)</h3>
<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="temporary mounts" data-type="indexterm" id="id1313"/><a data-primary="Docker, containerization with" data-secondary="Docker storage mechanisms" data-tertiary="temporary mounts" data-type="indexterm" id="id1314"/><a data-primary="temporary (tmpfs) mounts" data-type="indexterm" id="id1315"/><a data-primary="tmpfs (temporary) mounts" data-type="indexterm" id="id1316"/>Se hai dei dati non persistenti, come le cache dei modelli o i file sensibili che non hai bisogno di archiviare in modo permanente, dovresti prendere in considerazione l'utilizzo dei <em>montaggi</em> temporanei <em>tmpfs</em>.</p>
<p>Questo montaggio temporaneo persisterà i dati nella memoria host (RAM) solo durante il runtime del contenitore e aumenta le prestazioni del contenitore evitando le scritture nel livello scrivibile del contenitore.</p>
<p>Quando containerizzi le applicazioni GenAI, puoi utilizzare dei mount temporanei per memorizzare i risultati nella cache, i calcoli intermedi del modello, i file temporanei e i log specifici della sessione che non ti serviranno una volta che il container si sarà fermato.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Il livello scrivibile del container è strettamente legato alla macchina host attraverso un driver di archiviazione per implementare il filesystem union. Pertanto, la scrittura sul livello scrivibile del container riduce le prestazioni a causa di questo ulteriore livello di astrazione.</p>
<p>Al contrario, puoi utilizzare i volumi di dati per l'archiviazione persistente che scrive direttamente sul filesystem host o i mount tmpfs per l'archiviazione temporanea in memoria.</p>
</div>
<p>A differenza dei mount e dei volumi bind, non è possibile condividere il mount tmpfs tra i container e la funzionalità è disponibile solo sui sistemi Linux. Inoltre, se modifichi i permessi delle directory sui mount tmpfs, questi possono essere ripristinati al riavvio del container.</p>
<p class="less_space pagebreak-before">Ecco alcuni altri casi di utilizzo dei montaggi tmpfs:</p>
<ul>
<li>
<p>Memorizzare temporaneamente cache di dati, risposte API, log, dati di test, file di configurazione e artefatti del modello AI in memoria</p>
</li>
<li>
<p>Evitare le scritture di I/O sui dischi quando si lavora con le API di libreria che richiedono oggetti di tipo file</p>
</li>
<li>
<p>Simulare l'I/O ad alta velocità con l'accesso e la scrittura rapida dei file</p>
</li>
<li>
<p>Prevenire le scritture su disco eccessive o non necessarie se hai bisogno di<span class="keep-together">directory</span> temporanee.</p>
</li>
</ul>
<p>Per impostare un montaggio tmpfs, puoi utilizzare il seguente comando:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker run --tmpfs /cache genai-service<code class="w" translate="no"/></pre>
<p>In questo caso, stai impostando un mount tmpfs sulla directory <code translate="no">/cache</code> per le cache dei modelli, che cesseranno di esistere una volta che il contenitore si sarà fermato.</p>
</div></section>
<section data-pdf-bookmark="Handling filesystem permissions" data-type="sect3"><div class="sect3" id="id219">
<h3>Gestione dei permessi del filesystem</h3>
<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="handling filesystem permissions" data-type="indexterm" id="ix_ch12-asciidoc22"/><a data-primary="Docker, containerization with" data-secondary="Docker storage mechanisms" data-tertiary="handling filesystem permissions" data-type="indexterm" id="ix_ch12-asciidoc23"/>Una grande fonte di frustrazione e un problema di sicurezza per molti sviluppatori alle prime armi con Docker è la gestione dei permessi delle directory quando si utilizzano i mount bind del filesystem tra il sistema operativo host e il container.</p>
<p>Per impostazione predefinita, Docker esegue i container come utente <code translate="no">root</code> e di conseguenza i container hanno pieno accesso in lettura/scrittura alle directory montate sul sistema operativo host. Se l'utente <code translate="no">root</code> all'interno del container crea directory o file, questi saranno di proprietà di <code translate="no">root</code> anche sull'host. Puoi quindi incorrere in problemi di permessi se hai un account utente non root sull'host quando cerchi di accedere o modificare queste directory o file.</p>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>L'esecuzione dei container come utente predefinito di <code translate="no">root</code> è anche un grande rischio per la sicurezza se un malintenzionato riesce ad accedere al container, poiché avrà accesso al sistema host come <code translate="no">root</code>. Inoltre, se esegui un'immagine compromessa, potresti rischiare di eseguire codice maligno sul sistema host con i privilegi di <code translate="no">root</code>.</p>
</div>
<p>Per attenuare i problemi di permessi durante l'esecuzione di container con mount bind, puoi utilizzare il flag <code translate="no">--user</code> per eseguire il container come utente non root:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker run --user genai-service<code class="w" translate="no"/></pre>
<p>In alternativa, puoi creare e passare a un utente non root nei livelli finali della creazione dell'immagine all'interno del file Docker, come mostrato nell'<a data-type="xref" href="#docker_permissions">Esempio 12-6</a>.</p>
<div data-type="example" id="docker_permissions">
<h5><span class="label">Esempio 12-6. </span>Creazione e passaggio all'utente non root durante la creazione delle immagini del container (solo container Ubuntu/Debian)</h5>
<pre data-code-language="dockerfile" data-type="programlisting" translate="no"><code class="k" translate="no">ARG</code> <code class="nv" translate="no">USERNAME</code><code class="o" translate="no">=</code><code translate="no">fastapi</code> <a class="co" href="#callout_deployment_of_ai_services_CO3-1" id="co_deployment_of_ai_services_CO3-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>
<code class="k" translate="no">ARG</code> <code class="nv" translate="no">USER_UID</code><code class="o" translate="no">=</code><code class="m" translate="no">1001</code>
<code class="k" translate="no">ARG</code> <code class="nv" translate="no">USER_GID</code><code class="o" translate="no">=</code><code class="m" translate="no">1002</code>

<code class="k" translate="no">RUN</code> <code translate="no">groupadd</code> <code translate="no">--gid</code> <code class="nv" translate="no">$USER_GID</code> <code class="nv" translate="no">$USERNAME</code> <code class="se" translate="no">\ </code><a class="co" href="#callout_deployment_of_ai_services_CO3-2" id="co_deployment_of_ai_services_CO3-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a>
    <code class="o" translate="no">&amp;&amp;</code> <code translate="no">adduser</code> <code class="se" translate="no">\
</code>    <code translate="no">--disabled-password</code> <code class="se" translate="no">\
</code>    <code translate="no">--shell</code> <code class="s2" translate="no">"/sbin/nologin"</code> <code class="se" translate="no">\ </code><a class="co" href="#callout_deployment_of_ai_services_CO3-3" id="co_deployment_of_ai_services_CO3-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a>
    <code translate="no">--gecos</code> <code class="s2" translate="no">""</code> <code class="se" translate="no">\
</code>    <code translate="no">--home</code> <code class="s2" translate="no">"/nonexistent"</code> <code class="se" translate="no">\
</code>    <code translate="no">--no-create-home</code> <code class="se" translate="no">\ </code><a class="co" href="#callout_deployment_of_ai_services_CO3-4" id="co_deployment_of_ai_services_CO3-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a>
    <code translate="no">--uid</code> <code class="s2" translate="no">"</code><code class="si" translate="no">${</code><code class="nv" translate="no">UID</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code> <code class="se" translate="no">\
</code>    <code translate="no">--gid</code> <code class="nv" translate="no">$USER_GID</code>
    <code class="nv" translate="no">$USERNAME</code> <a class="co" href="#callout_deployment_of_ai_services_CO3-5" id="co_deployment_of_ai_services_CO3-5"><img alt="5" src="assets/5.png" width="12" height="12"/></a>

<code class="k" translate="no">USER</code> <code class="s" translate="no">$USERNAME </code><a class="co" href="#callout_deployment_of_ai_services_CO3-6" id="co_deployment_of_ai_services_CO3-6"><img alt="6" src="assets/6.png" width="12" height="12"/></a>

<code class="k" translate="no">CMD</code> <code class="p" translate="no">[</code><code class="s2" translate="no">"uvicorn"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"main:app"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"--host"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"0.0.0.0"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"--port"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"8000"</code><code class="p" translate="no">]</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_deployment_of_ai_services_CO3-1" id="callout_deployment_of_ai_services_CO3-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Usa gli argomenti di compilazione per specificare le variabili durante la creazione dell'immagine.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO3-2" id="callout_deployment_of_ai_services_CO3-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Crea un gruppo di utenti con il nome <code translate="no">USER_GID</code>.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO3-3" id="callout_deployment_of_ai_services_CO3-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Disabilita completamente il login degli utenti, compreso quello basato sulla password.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO3-4" id="callout_deployment_of_ai_services_CO3-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a></dt>
<dd><p>Evita di creare una home directory per l'utente.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO3-5" id="callout_deployment_of_ai_services_CO3-5"><img alt="5" src="assets/5.png" width="12" height="12"/></a></dt>
<dd><p>Crea un account utente non root con il nome <code translate="no">$USER_UID</code> e assegnalo al gruppo <code translate="no">USER_GID</code> appena creato. Imposta il nome dell'account utente su <code translate="no">fastapi</code>.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO3-6" id="callout_deployment_of_ai_services_CO3-6"><img alt="6" src="assets/6.png" width="12" height="12"/></a></dt>
<dd><p>Passa all'utente non root <code translate="no">fastapi</code>.</p></dd>
</dl></div>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Spesso dovrai installare dei pacchetti o aggiungere delle configurazioni che richiedono un accesso privilegiato al disco o dei permessi. Dovresti passare a un utente non root solo alla fine della creazione di un'immagine, una volta completate queste installazioni e configurazioni. Evita di passare da un utente root a uno non root per evitare inutili complessità e livelli di immagine in eccesso.</p>
</div>
<p>Se hai problemi con la creazione di nuovi gruppi o utenti nell'<a data-type="xref" href="#docker_permissions">Esempio 12-6</a>, prova a cambiare gli ID <code translate="no">USER_UID</code> e <code translate="no">USER_GID</code> perché potrebbero essere già utilizzati da un altro utente non root nell'immagine.</p>
<p>Supponiamo che durante la creazione dell'immagine, l'utente <code translate="no">root</code> nel container abbia creato la cartella <code translate="no">myscripts</code>. Puoi controllare i permessi del filesystem utilizzando il comando <code translate="no">ls -l</code>, che restituisce il seguente output:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">total <code class="m" translate="no">12</code><code class="w" translate="no"/>
drw-r--r-- <code class="m" translate="no">2</code> root root <code class="m" translate="no">4096</code> Oct  <code class="m" translate="no">1</code> <code class="m" translate="no">10</code>:00 myscripts<code class="w" translate="no"/></pre>
<p>Puoi leggere i permessi <code translate="no">drwxr-xr-x</code> per la directory <code translate="no">myscripts</code> utilizzando la seguente ripartizione:</p>
<ul>
<li>
<p><code translate="no">d</code>: Specifica che <code translate="no">myscripts</code> è una directory; altrimenti mostrerebbe una <code translate="no">-</code>.</p>
</li>
<li>
<p><code translate="no">rwx</code>: Proprietario <code translate="no">root</code> L'utente può (r)leggere, (w)scrivere ed elaborare i file di questa<span class="keep-together">directory.</span></p>
</li>
<li>
<p><code translate="no">r--</code>: I membri del gruppo <code translate="no">root</code> possono eseguire operazioni di sola lettura ma non possono scrivere o eseguire alcun file.</p>
</li>
<li>
<p><code translate="no">r--</code>: Tutti gli altri possono leggere il file ma non possono scriverlo o eseguirlo.<sup><a data-type="noteref" href="ch12.html#id1317" id="id1317-marker" translate="no">4</a></sup></p>
</li>
</ul>
<p>Se vuoi impostare la proprietà o i permessi della directory <code translate="no">myscripts</code>, puoi utilizzare i comandi <code translate="no">chmod</code> o <code translate="no">chown</code> nei sistemi Linux.</p>
<p>Usa il comando <code translate="no">chown</code> per cambiare il proprietario della directory sull'host in modo da poter modificare i file nell'editor di codice:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no"><code class="c1" translate="no"># Set file or directory ownership</code>
$ sudo chown -R username:groupname mydir<code class="w" translate="no"/></pre>
<p>In alternativa, se hai bisogno di eseguire gli script solo nella directory <code translate="no">myscripts</code>, usa il comando <code translate="no">chmod</code> per modificare i permessi dei file o delle directory:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no"><code class="c1" translate="no"># Set execute permissions using flags</code>
$ sudo chmod -R +x myscripts<code class="w" translate="no"/>

<code class="c1" translate="no"># Set execute permissions in a numeric form</code>
$ sudo chmod -R <code class="m" translate="no">755</code> myscripts<code class="w" translate="no"/></pre>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Il flag <code translate="no">-R</code> imposta ricorsivamente la proprietà o i permessi di una directory annidata.</p>
</div>
<p>Questo comando permette ai membri del gruppo <code translate="no">root</code> e ad altri utenti di eseguire i file presenti nella directory <code translate="no">myscripts</code>. Altri utenti possono eseguire i file solo se utilizzano il comando <code translate="no">bash</code>. Tuttavia, solo il proprietario può modificarli.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1318">
<h1>Interpretare i permessi del filesystem di Linux</h1>
<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="interpreting Linux filesystem permissions" data-type="indexterm" id="id1319"/><a data-primary="Docker, containerization with" data-secondary="Docker storage mechanisms" data-tertiary="interpreting Linux filesystem permissions" data-type="indexterm" id="id1320"/><a data-primary="Linux filesystem permissions" data-type="indexterm" id="id1321"/>Il comando <code translate="no">chmod</code> utilizza un numero ottale di tre cifre per impostare i permessi dei file. Ogni cifra rappresenta i permessi per il <em>proprietario</em>, il <em>gruppo</em> e gli <em>altri</em> (cioè tutti gli altri), rispettivamente.</p>
<p>Ad esempio, <code translate="no">chmod 755</code> imposta quanto segue:</p>
<ul>
<li>
<p>Proprietario: <code translate="no">rwx</code> (7)</p>
</li>
<li>
<p>Gruppo: <code translate="no">r-x</code> (5)</p>
</li>
<li>
<p>Altri: <code translate="no">r-x</code> (5)</p>
</li>
</ul>
<p>La<a data-type="xref" href="#docker_linux_permissionss">Tabella 12-2</a> mostra la tabella dei permessi di Linux da utilizzare come riferimento quando si lavora con i comandi di <code translate="no">chmod</code>.</p>
<table class="striped" id="docker_linux_permissionss">
<caption><span class="label">Tabella 12-2. </span>Permessi del filesystem Linux</caption>
<thead>
<tr>
<th>Valore numerico</th>
<th>Simbolo</th>
<th>Permessi</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>7</p></td>
<td><p>rwx</p></td>
<td><p>(r)ead, (w)rite e e(x)ecute</p></td>
</tr>
<tr>
<td><p>6</p></td>
<td><p>rw-</p></td>
<td><p>(r)ead e (w)rite</p></td>
</tr>
<tr>
<td><p>5</p></td>
<td><p>r-x</p></td>
<td><p>(r)ead e e(x)ecute</p></td>
</tr>
<tr>
<td><p>4</p></td>
<td><p>r--</p></td>
<td><p>(r)ead-only</p></td>
</tr>
<tr>
<td><p>3</p></td>
<td><p>-wx</p></td>
<td><p>(w)rite e e(x)ecute</p></td>
</tr>
<tr>
<td><p>2</p></td>
<td><p>-w-</p></td>
<td><p>Solo (w)rite</p></td>
</tr>
<tr>
<td><p>1</p></td>
<td><p>--x</p></td>
<td><p>e(x)solo ecute</p></td>
</tr>
<tr>
<td><p>0</p></td>
<td><p>---</p></td>
<td><p>Nessuno</p></td>
</tr>
</tbody>
</table>
<p>Puoi usare la <a data-type="xref" href="#docker_linux_permissionss">Tabella 12-2</a> come riferimento per risolvere eventuali problemi legati ai permessi.</p>
</div></aside>
<p>Se ispezioni nuovamente i permessi del filesystem utilizzando <code translate="no">ls -l</code>, vedrai il seguente output:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">total <code class="m" translate="no">12</code><code class="w" translate="no"/>
drwxr-xr-x <code class="m" translate="no">2</code> root root <code class="m" translate="no">4096</code> Oct  <code class="m" translate="no">1</code> <code class="m" translate="no">10</code>:00 myscripts<code class="w" translate="no"/></pre>
<ul>
<li>
<p><code translate="no">rwx</code>: Proprietario <code translate="no">root</code> L'utente può ancora (r)leggere, (w)scrivere ed elaborare i file in questa<span class="keep-together">directory.</span></p>
</li>
<li>
<p><code translate="no">r-x</code>: I membri del gruppo <code translate="no">root</code> possono eseguire operazioni di (r)lettura e di (x)calcolo ma non possono modificare alcun file.</p>
</li>
<li>
<p><code translate="no">r-x</code>: Chiunque altro non può modificare i file della directory <code translate="no">myscripts</code> ma può leggerli ed eseguirli.</p>
</li>
</ul>
<p>Puoi utilizzare l'<a data-type="xref" href="#docker_permissions_execute">Esempio 12-7</a> per impostare i permessi quando crei delle directory all'interno di un'immagine.</p>
<div data-type="example" id="docker_permissions_execute">
<h5><span class="label">Esempio 12-7. </span>Creare la cartella degli script e consentire l'esecuzione dei file (solo contenitori Ubuntu/Debian)</h5>
<pre data-code-language="dockerfile" data-type="programlisting" translate="no"><code class="k" translate="no">RUN</code> mkdir -p scripts<code class="w" translate="no"/>

<code class="k" translate="no">COPY</code> scripts scripts<code class="w" translate="no"/>

<code class="k" translate="no">RUN</code> chmod -R +x scripts<code class="w" translate="no"/></pre></div>
<p>Le istruzioni riportate nell'<a data-type="xref" href="#docker_permissions_execute">Esempio 12-7</a> ti permetteranno di configurare i permessi per eseguire i file nella directory <code translate="no">scripts</code> dall'interno del contenitore.</p>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>Quando si utilizzano i volumi dei container, bisogna fare attenzione ai mount bindings perché sostituiscono i permessi all'interno del container con quelli del filesystem host.</p>
</div>
<p>I problemi più frustranti quando si lavora con i container sono legati ai permessi del filesystem. Pertanto, sapere come impostare e correggere i permessi dei file ti farà risparmiare ore di sviluppo quando lavori con container che producono o modificano artefatti sulla macchina host<a data-startref="ix_ch12-asciidoc23" data-type="indexterm" id="id1322"/><a data-startref="ix_ch12-asciidoc22" data-type="indexterm" id="id1323"/>.<a data-startref="ix_ch12-asciidoc21" data-type="indexterm" id="id1324"/><a data-startref="ix_ch12-asciidoc20" data-type="indexterm" id="id1325"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Docker Networking" data-type="sect2"><div class="sect2" id="id220">
<h2>Network+ di Docker</h2>
<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="Docker networking" data-type="indexterm" id="ix_ch12-asciidoc24"/><a data-primary="Docker, containerization with" data-secondary="Docker networking" data-type="indexterm" id="ix_ch12-asciidoc25"/>Il networking di Docker è uno dei concetti più difficili da comprendere nei progetti multicontainer. Questa sezione spiega come funziona il networking di Docker e come impostare i container locali per comunicare, simulando gli ambienti di produzione durante lo sviluppo.</p>
<p>Spesso, quando si esegue il deploy in ambienti di produzione nel cloud, si configura il networking utilizzando le soluzioni del cloud provider. Tuttavia, se hai bisogno di collegare i container in un ambiente di sviluppo per i test locali o per il deploy su risorse on-premises, allora ti sarà utile capire come funziona il networking di Docker.</p>
<p>Se stai sviluppando servizi GenAI che interagiscono con sistemi esterni come i database, è probabile che utilizzerai più container: uno per la tua applicazione e uno per l'esecuzione di ciascun database o sistema esterno.</p>
<p>Docker viene fornito con un sottosistema di rete che permette ai container di connettersi tra loro sullo stesso host o su host diversi. Puoi anche connettere i container tramite host rivolti a internet.</p>
<p>Quando crei dei container con il comando <code translate="no">docker run</code>, per impostazione predefinita avranno il networking abilitato su una <em>rete bridge</em> in modo da poter effettuare connessioni in uscita, ma non esporranno o pubblicheranno le loro porte al mondo esterno.</p>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p><a data-primary="firewall rules" data-type="indexterm" id="id1326"/>Con le impostazioni predefinite, Docker interagisce con i kernel del sistema operativo per configurare le <em>regole del firewall</em> (ad esempio, le regole <code translate="no">iptables</code> e <code translate="no">ip6tables</code> su Linux) per implementare l'isolamento della rete, la pubblicazione delle porte e il<span class="keep-together">filtraggio.</span></p>
<p>Poiché Docker può ignorare queste regole del firewall, se una porta dell'host come <code translate="no">8000</code> è chiusa, Docker può forzarne l'apertura ed esporla all'esterno della macchina host quando si esegue un container con il flag <code translate="no">-p 8000:8000</code>. Per evitare questa esposizione, una soluzione è quella di eseguire i container utilizzando <code translate="no">-p 127.0.0.1:8000:8000</code>.</p>
</div>
<p>Per far funzionare il sottosistema di rete, Docker utilizza i <em>driver di rete</em>, come mostrato nella <a data-type="xref" href="#docker_networking_drivers">Tabella 12-3</a>.</p>
<table class="striped" id="docker_networking_drivers">
<caption><span class="label">Tabella 12-3. </span>Driver di rete di Docker</caption>
<thead>
<tr>
<th>Autista</th>
<th>Descrizione</th>
<th>Caso d'uso</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Ponte (predefinito)</p></td>
<td><p>Collega i container in esecuzione sullo stesso host del demone Docker. Le reti definite dall'utente possono sfruttare un server DNS incorporato.</p></td>
<td><p>Controlla la comunicazione dei container in reti Docker isolate con una semplice configurazione.</p></td>
</tr>
<tr>
<td><p>Ospite</p></td>
<td><p>Rimuove il livello di isolamento tra i container e il sistema host, in modo che tutte le connessioni TCP/UDP siano accessibili direttamente tramite la rete dell'host, ad esempio localhost, senza la necessità di pubblicare le porte.</p></td>
<td><p>Semplificare l'accesso al container dalla rete host (ad esempio, localhost) o quando un container deve gestire un'ampia gamma di porte.</p></td>
</tr>
<tr>
<td><p>Nessuno</p></td>
<td><p>Disattiva tutti i servizi di rete e isola i container in esecuzione all'interno dell'ambiente Docker.</p></td>
<td><p>Isolare i container da qualsiasi processo Docker e non Docker per motivi di sicurezza. Debug della rete o simulazione di interruzioni. Isolamento delle risorse e container transitori per processi di breve durata.</p></td>
</tr>
<tr>
<td><p>Sovrapposizione</p></td>
<td><p>Collega i container tra più host/motore o in un cluster <em>Docker Swarm</em>.</p><p><strong>Nota:</strong> il motore Docker ha una modalità <em>swarm</em> che consente l'orchestrazione dei container tramite <em>cluster</em> di demoni/motore Docker.</p></td>
<td><p>Elimina la necessità di un routing a livello di sistema operativo quando si collegano i container tra gli host Docker.</p></td>
</tr>
<tr>
<td><p>Macvlan</p></td>
<td><p>Assegna gli indirizzi mac ai contenitori come se fossero dispositivi fisici.</p><p>Una configurazione errata può portare a un degrado involontario della rete a causa dell'esaurimento degli indirizzi IP, con conseguente diffusione delle VLAN (numero elevato di indirizzi mac) o modalità promiscua (sovrapposizione di indirizzi).</p></td>
<td><p>Utilizzato in sistemi o applicazioni legacy che monitorano il traffico di rete e che si aspettano di essere collegati direttamente a una rete fisica.</p></td>
</tr>
<tr>
<td><p>IPVlan</p></td>
<td><p>Ti dà il controllo totale sull'indirizzamento dei container IPv4 e IPv6, fornendo un facile accesso ai servizi esterni senza bisogno di mappature delle porte.</p></td>
<td><p>Configurazione di rete avanzata che bypassa il tradizionale bridge di Linux per isolare, migliorare le prestazioni e semplificare la topologia di rete.</p></td>
</tr>
</tbody>
</table>
<p>Per assicurarti che i tuoi container possano comunicare tra loro, potrebbe essere necessario specificare le impostazioni e i driver di rete. Puoi selezionare un driver di rete adatto al tuo caso d'uso in base alla <a data-type="xref" href="#docker_networking_drivers">Tabella 12-3</a>.</p>
<div data-type="note" epub:type="note"><h6>Nota</h6>
<p>Alcuni di questi driver potrebbero non essere disponibili a seconda della piattaforma/host OS su cui stai eseguendo Docker (host Windows, Linux o macOS).</p>
</div>
<p>I driver di rete più comunemente utilizzati sono bridge, host e nessuno. Probabilmente non avrai bisogno di utilizzare altri driver (ad esempio, overlay, Macvlan, IPVlan) a meno che tu non abbia bisogno di configurazioni di rete più avanzate.</p>
<p>La<a data-type="xref" href="#docker_networking_drivers_viz">Figura 12-9</a> visualizza le funzionalità dei driver bridge, host, none, overlay, Macvlan e IPVlan.</p>
<figure><div class="figure" id="docker_networking_drivers_viz">
<img alt="bgai 1209" src="assets/bgai_1209.png" width="1331" height="1122"/>
<h6><span class="label">Figura 12-9. </span>Driver di rete di Docker</h6>
</div></figure>
<p>Analizziamo questi driver di rete in modo più dettagliato.</p>
<section data-pdf-bookmark="Bridge network driver" data-type="sect3"><div class="sect3" id="id275">
<h3>Driver di rete bridge</h3>
<p><a data-primary="Docker, containerization with" data-secondary="Docker networking" data-tertiary="bridge network driver" data-type="indexterm" id="ix_ch12-asciidoc26"/>Il driver di rete bridge collega i container creando una rete bridge predefinita <code translate="no">docker0</code> e associando i container ad essa e all'interfaccia di rete principale dell'host, a meno che non sia specificato diversamente. In questo modo i container potranno accedere alla rete dell'host (e a internet) e tu potrai accedere ai container.</p>
<p>Puoi visualizzare le reti utilizzando il comando <code translate="no">docker network ls</code>:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker network ls<code class="w" translate="no"/>
NETWORK ID     NAME      DRIVER    SCOPE<code class="w" translate="no"/>
72ec0b2e6034   bridge    bridge    <code class="nb" translate="no">local</code><code class="w" translate="no"/>
53ec40b3c639   host      host      <code class="nb" translate="no">local</code><code class="w" translate="no"/>
64368b7baa5f   none      null      <code class="nb" translate="no">local</code><code class="w" translate="no"/></pre>
<p>Il <em>bridge di rete</em> in Docker è un dispositivo software di livello link che gira all'interno del kernel della macchina host e che permette ai container collegati di comunicare isolando i container non collegati. Il driver del bridge installa automaticamente delle regole nella macchina host in modo che i container su reti bridge diverse non possano comunicare direttamente.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Le reti bridge si applicano solo ai container in esecuzione sullo stesso motore Docker/ host demone. Per collegare i container in esecuzione su altri host demone, puoi gestire il routing a livello del sistema operativo host o utilizzare un driver <em>overlay</em>.</p>
</div>
<p>Oltre alle reti bridge predefinite, puoi creare le tue reti personalizzate, che possono offrire un isolamento e un'esperienza di routing dei pacchetti superiori.</p>
<section data-pdf-bookmark="Configure user-defined bridge networks" data-type="sect4"><div class="sect4" id="id221">
<h4>Configurare reti ponte definite dall'utente</h4>
<p><a data-primary="Docker, containerization with" data-secondary="bridge network driver" data-tertiary="configuring user-defined bridge networks" data-type="indexterm" id="ix_ch12-asciidoc27"/>Se hai bisogno di ambienti di rete più avanzati o isolati per i tuoi container, puoi creare una rete separata definita dall'utente.</p>
<p>Le reti definite dall'utente sono superiori alle reti di bridge predefinite in quanto garantiscono un migliore isolamento. Inoltre, i container possono risolversi l'un l'altro tramite nome o alias sulle reti di bridge definite dall'utente, a differenza della rete predefinita in cui possono comunicare solo tramite indirizzi IP.</p>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>Se esegui i container senza specificare <code translate="no">--network</code>, verranno collegati alla rete bridge predefinita, il che può rappresentare un problema di sicurezza in quanto i servizi non correlati possono comunicare e accedere l'uno all'altro.</p>
</div>
<p>Per creare una rete, puoi utilizzare il comando <code translate="no">docker network create</code>, che utilizzerà il flag <code translate="no">--driver bridge</code> per impostazione predefinita:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker network create genai-net<code class="w" translate="no"/></pre>
<div data-type="note" epub:type="note"><h6>Nota</h6>
<p>Quando crei reti definite dall'utente, Docker utilizza gli strumenti del sistema operativo host per gestire l'infrastruttura di rete sottostante, come l'aggiunta o la rimozione di dispositivi bridge e la configurazione delle regole di <code translate="no">iptables</code> su Linux.</p>
</div>
<p>Una volta creata la rete, puoi elencare le reti utilizzando il comando <code translate="no">docker network ls</code>:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker network ls<code class="w" translate="no"/>
NETWORK ID     NAME         DRIVER    SCOPE<code class="w" translate="no"/>
72ec0b2e6034   bridge       bridge    <code class="nb" translate="no">local</code><code class="w" translate="no"/>
6aa21632e77e   genai-net    bridge    <code class="nb" translate="no">local</code><code class="w" translate="no"/></pre>
<p>La topologia della rete avrà ora l'aspetto della <a data-type="xref" href="#docker_networking_isolated">Figura 12-10</a>.</p>
<figure><div class="figure" id="docker_networking_isolated">
<img alt="bgai 1210" src="assets/bgai_1210.png" width="913" height="482"/>
<h6><span class="label">Figura 12-10. </span>Reti a ponte isolate</h6>
</div></figure>
<p>Quando esegui i container, ora puoi collegarli alla rete creata utilizzando il flag <code translate="no">--network genai-net</code>:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker run --network genai-net genai-service<code class="w" translate="no"/>
$ docker run --network genai-net postgresql<code class="w" translate="no"/></pre>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>Su Linux, c'è un limite di 1.000 container che possono connettersi a una singola rete bridge a causa delle restrizioni del kernel Linux. Collegare più container a una singola rete bridge può renderla instabile e interrompere la comunicazione tra container.</p>
</div>
<p>Entrambi i container possono ora accedere l'uno all'altro sulla rete <code translate="no">genai-net</code> meglio isolata e definita dall'utente, con <em>risoluzione DNS</em> automatica tra i container.<a data-startref="ix_ch12-asciidoc27" data-type="indexterm" id="id1327"/></p>
</div></section>
<section data-pdf-bookmark="Embedded DNS" data-type="sect4"><div class="sect4" id="id276">
<h4>DNS integrato</h4>
<p><a data-primary="Docker, containerization with" data-secondary="bridge network driver" data-tertiary="embedded DNS" data-type="indexterm" id="id1328"/>Docker sfrutta un server DNS incorporato con reti definite dall'utente, come mostrato nella <a data-type="xref" href="#docker_networking_bridge_dns">Figura 12-11</a>, per mappare gli indirizzi IP interni in modo che i container possano raggiungerne uno per nome.</p>
<figure><div class="figure" id="docker_networking_bridge_dns">
<img alt="bgai 1211" src="assets/bgai_1211.png" width="1414" height="514"/>
<h6><span class="label">Figura 12-11. </span>DNS incorporato</h6>
</div></figure>
<p>Ad esempio, se chiami il tuo contenitore di applicazioni <code translate="no">genai-service</code> e il tuo contenitore di database <code translate="no">db</code>, allora il tuo contenitore <code translate="no">genai-service</code> può comunicare con il database chiamando il nome host <code translate="no">db</code>.</p>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>Non è possibile accedere al contenitore <code translate="no">db</code> dall'esterno della rete Docker bridge con il suo nome, poiché il server DNS incorporato non è visibile alla macchina host.</p>
<p>Invece, puoi esporre la porta del contenitore <code translate="no">5432</code> e accedere al contenitore <code translate="no">db</code> utilizzando la rete dell'host (ad esempio, tramite <code translate="no">localhost:5432</code>).</p>
</div>
<p>Parliamo ora di come pubblicare le porte dei container nell'ambiente esterno, come ad esempio il computer host.</p>
</div></section>
<section data-pdf-bookmark="Publishing ports" data-type="sect4"><div class="sect4" id="id222">
<h4>Porte di pubblicazione</h4>
<p><a data-primary="Docker, containerization with" data-secondary="bridge network driver" data-tertiary="publishing ports" data-type="indexterm" id="id1329"/>Quando esegui i container in una rete, questi espongono automaticamente le porte l'uno all'altro.</p>
<p>Se hai bisogno di accedere ai container dalla macchina host o da processi non Docker su reti diverse, dovrai esporre le porte dei container pubblicandole con il flag <code translate="no">--publish</code> o <code translate="no">-p</code>:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker run -p <code class="m" translate="no">127</code>.0.0.1:8000:8000 myimage<code class="w" translate="no"/></pre>
<p>Questo comando ti permette di creare un contenitore con la porta esposta <code translate="no">8000</code> mappata sulla porta <code translate="no">8000</code> del computer host (ad esempio, localhost) utilizzando la sintassi <code translate="no">&lt;host_port&gt;:​&lt;con⁠tainer_port&gt;</code>.</p>
<p>Se non specifichi una porta del container, Docker pubblicherà e mapperà la porta <code translate="no">80</code> per impostazione predefinita.</p>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>Controlla sempre due volte le porte che vuoi esporre ed evita di pubblicare porte di container che sono già in uso sul tuo computer host. Altrimenti si creeranno <em>conflitti di porte</em> che porteranno le richieste a servizi in conflitto, il che richiederà molto tempo per<span class="keep-together">la risoluzione dei problemi</span>.</p>
</div>
<p>Se l'utilizzo delle reti bridge e delle mappature delle porte ti crea molti problemi, puoi anche utilizzare il driver di rete <em>host</em> per collegare i tuoi container, anche se senza gli stessi vantaggi di isolamento e sicurezza delle reti bridge.<a data-startref="ix_ch12-asciidoc26" data-type="indexterm" id="id1330"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Host network driver" data-type="sect3"><div class="sect3" id="id277">
<h3>Driver di rete host</h3>
<p><a data-primary="Docker, containerization with" data-secondary="Docker networking" data-tertiary="host network driver" data-type="indexterm" id="id1331"/>Un driver di rete <em>host</em> è utile nei casi in cui vuoi migliorare le prestazioni, quando vuoi evitare la mappatura delle porte dei container o quando uno dei tuoi container deve gestire un gran numero di porte.</p>
<p>L'esecuzione di un container con il driver host è semplice come l'utilizzo del flag <code translate="no">--net=host</code> con il comando <code translate="no">docker run</code>:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker run --net<code class="o" translate="no">=</code>host genai-service<code class="w" translate="no"/></pre>
<p>Nel networking host, i container condividono lo spazio dei nomi di rete della macchina host, il che significa che i container non saranno isolati dall'host Docker. Pertanto, ai container non verrà assegnato un proprio indirizzo IP.</p>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>Non appena abiliti il driver di rete host, le porte pubblicate in precedenza verranno scartate, in quanto i container non avranno un proprio indirizzo IP.</p>
</div>
<p>Il driver di rete host è più performante perché non necessita di una <em>traduzione degli indirizzi di rete</em> (NAT) per mappare gli indirizzi IP da uno spazio dei nomi (container) a un altro (macchina host) ed evita di creare un <em>proxy di terra dell'utente</em> (cioè un port forwarding) per ogni porta. Tuttavia, la rete host è supportata solo dai container Linux e non Windows. Inoltre, i container non avranno accesso alle interfacce di rete dell'host e non potranno quindi effettuare il bind agli indirizzi IP dell'host, il che comporta una maggiore complessità nella configurazione di rete necessaria.</p>
</div></section>
<section data-pdf-bookmark="None network driver" data-type="sect3"><div class="sect3" id="id223">
<h3>Nessun driver di rete</h3>
<p><a data-primary="Docker, containerization with" data-secondary="Docker networking" data-tertiary="none network driver" data-type="indexterm" id="id1332"/><a data-primary="none network driver (Docker)" data-type="indexterm" id="id1333"/>Se vuoi isolare completamente lo stack di rete di un container, puoi utilizzare il flag <code translate="no">--network none</code> all'avvio del container. All'interno del container viene creato solo il dispositivo di loopback, un'interfaccia di rete virtuale che il container utilizza per comunicare con se stesso. Puoi specificare il driver di rete none utilizzando il seguente comando:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker run --network none genai-service<code class="w" translate="no"/></pre>
<p>Questi sono alcuni casi in cui è utile isolare i contenitori:</p>
<ul>
<li>
<p>Applicazioni che gestiscono dati altamente sensibili o processi critici</p>
</li>
<li>
<p>Dove c'è un rischio più elevato di attacchi basati sulla rete o di malware</p>
</li>
<li>
<p>Eseguire il debug della rete e simulare le interruzioni di rete eliminando le interferenze esterne.</p>
</li>
<li>
<p>L'esecuzione di container stand-alone senza dipendenze esterne può essere eseguita<span class="keep-together">in modo indipendente</span></p>
</li>
<li>
<p>Gestione di contenitori transitori per processi di breve durata per ridurre al minimo l'esposizione della rete.</p>
</li>
</ul>
<p>In generale, usa il driver di rete none se hai bisogno di isolare i container da qualsiasi processo Docker e non Docker per motivi di sicurezza.<a data-startref="ix_ch12-asciidoc25" data-type="indexterm" id="id1334"/><a data-startref="ix_ch12-asciidoc24" data-type="indexterm" id="id1335"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Enabling GPU Driver" data-type="sect2"><div class="sect2" id="id224">
<h2>Abilitazione del driver della GPU</h2>
<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="enabling GPU driver" data-type="indexterm" id="ix_ch12-asciidoc28"/><a data-primary="GPUs" data-secondary="enabling GPU driver in Docker" data-type="indexterm" id="ix_ch12-asciidoc29"/>Se hai una scheda grafica NVIDIA con il toolkit CUDA e i driver necessari installati, puoi usare il flag <code translate="no">--gpus=all</code> per abilitare il supporto GPU per i tuoi container in Docker.<sup><a data-type="noteref" href="ch12.html#id1336" id="id1336-marker" translate="no">5</a></sup></p>
<p>Per verificare che il tuo sistema abbia i driver necessari e che supporti la GPU in Docker, esegui il seguente comando per eseguire il benchmark della tua GPU:</p>
<pre data-type="programlisting" translate="no">$ docker run --rm -it \
             --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody \
             -gpu \
             -benchmark

&gt; Windowed mode
&gt; Simulation data stored in video memory
&gt; Single precision floating point simulation
&gt; 1 Devices used for simulation
MapSMtoCores for SM 8.9 is undefined.  Default to use 128 Cores/SM
MapSMtoArchName for SM 8.9 is undefined.  Default to use Ampere
GPU Device 0: "Ampere" with compute capability 8.9

&gt; Compute 8.9 CUDA device: [NVIDIA GeForce RTX 4090]
131072 bodies, total time for 10 iterations: 75.182 ms
= 2285.102 billion interactions per second
= 45702.030 single-precision GFLOP/s at 20 flops per interaction</pre>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Puoi anche usare lo strumento <code translate="no">nvidia-smi</code> dell'interfaccia di gestione del sistema NVIDIA per gestire e monitorare<span class="keep-together">i dispositivi</span> GPU NVIDIA<span class="keep-together">.</span></p>
</div>
<p>I framework di deep learning come <code translate="no">tensorflow</code> o <code translate="no">pytorch</code> possono rilevare e utilizzare automaticamente il dispositivo GPU quando si eseguono le applicazioni in un container abilitato per le GPU. Questo include le librerie Hugging Face come <code translate="no">transformers</code> che consentono di auto-ospitare i modelli linguistici.</p>
<p>Se utilizzi il pacchetto <code translate="no">transformers</code>, assicurati di installare anche la libreria <code translate="no">accelerate</code>:</p>
<pre data-type="programlisting" translate="no">$ pip install accelerate</pre>
<p>Ora puoi spostare il modello sulla GPU prima che venga caricato nella CPU utilizzando <code translate="no">device_map='cuda'</code>, come mostrato nell'<a data-type="xref" href="#docker_gpu">Esempio 12-8</a>.</p>
<div data-type="example" id="docker_gpu">
<h5><span class="label">Esempio 12-8. </span>Trasferimento dei modelli Hugging Face alla GPU</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">from</code> <code class="nn" translate="no">transformers</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">pipeline</code>

<code class="n" translate="no">pipe</code> <code class="o" translate="no">=</code> <code class="n" translate="no">pipeline</code><code class="p" translate="no">(</code>
    <code class="s2" translate="no">"text-generation"</code><code class="p" translate="no">,</code>
    <code class="n" translate="no">model</code><code class="o" translate="no">=</code><code class="s2" translate="no">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</code><code class="p" translate="no">,</code>
    <code class="n" translate="no">device_map</code><code class="o" translate="no">=</code><code class="s2" translate="no">"cuda"</code>
<code class="p" translate="no">)</code></pre></div>
<p>Dovresti essere in grado di eseguire le previsioni sulla GPU passando il flag <code translate="no">--gpus=all</code> a <code translate="no">docker run</code>.<a data-startref="ix_ch12-asciidoc29" data-type="indexterm" id="id1337"/><a data-startref="ix_ch12-asciidoc28" data-type="indexterm" id="id1338"/></p>
</div></section>
<section data-pdf-bookmark="Docker Compose" data-type="sect2"><div class="sect2" id="id225">
<h2>Docker Compose</h2>
<p><a data-primary="Docker Compose tool" data-secondary="basics" data-type="indexterm" id="ix_ch12-asciidoc30"/>In ambienti multicontainer, puoi utilizzare lo strumento <em>Docker Compose</em> per definire ed eseguire i container delle applicazioni per un'esperienza di sviluppo e distribuzione semplificata.</p>
<p>L'uso di Docker Compose può aiutarti a semplificare la gestione di diversi container, reti, volumi, variabili e segreti con un unico <em>file di configurazione YAML</em>. Questo semplifica il complesso compito di orchestrare e coordinare i vari container, rendendo più facile la gestione e la replica dei tuoi servizi in diversi ambienti applicativi utilizzando le variabili d'ambiente. Puoi anche condividere il file YAML con altri utenti in modo che possano replicare il tuo ambiente di container. Inoltre, memorizza le configurazioni per evitare di ricreare i container quando riavvii i servizi.</p>
<p class="less_space pagebreak-before">L<a data-type="xref" href="#docker_compose">'esempio 12-9</a> mostra un esempio di file di configurazione YAML.</p>
<div data-type="example" id="docker_compose">
<h5><span class="label">Esempio 12-9. </span>File di configurazione YAML di Docker Compose</h5>
<pre data-code-language="yaml" data-type="programlisting" translate="no"><code class="c1" translate="no"># compose.yaml</code>

<code class="nt" translate="no">services</code><code class="p" translate="no">:</code> <a class="co" href="#callout_deployment_of_ai_services_CO4-1" id="co_deployment_of_ai_services_CO4-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>
  <code class="nt" translate="no">server</code><code class="p" translate="no">:</code>
    <code class="nt" translate="no">build</code><code class="p" translate="no">:</code> <code class="l-Scalar-Plain" translate="no">.</code> <a class="co" href="#callout_deployment_of_ai_services_CO4-2" id="co_deployment_of_ai_services_CO4-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a>
    <code class="nt" translate="no">ports</code><code class="p" translate="no">:</code>
      <code class="p-Indicator" translate="no">-</code> <code class="s" translate="no">"</code><code class="s" translate="no">8000:8000</code><code class="s" translate="no">"</code>
    <code class="nt" translate="no">environment</code><code class="p" translate="no">:</code>
      <code class="nt" translate="no">SHOW_DOCS_IN_PRODUCTION</code><code class="p" translate="no">:</code> <code class="l-Scalar-Plain" translate="no">$SHOW_DOCS_IN_PRODUCTION</code>
      <code class="nt" translate="no">ALLOWED_CORS_ORIGINS</code><code class="p" translate="no">:</code> <code class="l-Scalar-Plain" translate="no">$ALLOWED_CORS_ORIGINS</code>
    <code class="nt" translate="no">secrets</code><code class="p" translate="no">:</code>
       <code class="p-Indicator" translate="no">-</code> <code class="l-Scalar-Plain" translate="no">openai_api_token</code> <a class="co" href="#callout_deployment_of_ai_services_CO4-3" id="co_deployment_of_ai_services_CO4-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a>
    <code class="nt" translate="no">volumes</code><code class="p" translate="no">:</code>
      <code class="p-Indicator" translate="no">-</code> <code class="l-Scalar-Plain" translate="no">./src/app:/code/app</code>
    <code class="nt" translate="no">networks</code><code class="p" translate="no">:</code>
      <code class="p-Indicator" translate="no">-</code> <code class="l-Scalar-Plain" translate="no">genai-net</code> <a class="co" href="#callout_deployment_of_ai_services_CO4-4" id="co_deployment_of_ai_services_CO4-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a>

  <code class="nt" translate="no">db</code><code class="p" translate="no">:</code>
    <code class="nt" translate="no">image</code><code class="p" translate="no">:</code> <code class="l-Scalar-Plain" translate="no">postgres:12.2-alpine</code>
    <code class="nt" translate="no">ports</code><code class="p" translate="no">:</code>
      <code class="p-Indicator" translate="no">-</code> <code class="s" translate="no">"</code><code class="s" translate="no">5433:5432</code><code class="s" translate="no">"</code>
    <code class="nt" translate="no">volumes</code><code class="p" translate="no">:</code>
      <code class="p-Indicator" translate="no">-</code> <code class="l-Scalar-Plain" translate="no">db-data:/etc/data</code>
    <code class="nt" translate="no">networks</code><code class="p" translate="no">:</code>
      <code class="p-Indicator" translate="no">-</code> <code class="l-Scalar-Plain" translate="no">genai-net</code>

<code class="nt" translate="no">volumes</code><code class="p" translate="no">:</code>
  <code class="nt" translate="no">db-data</code><code class="p" translate="no">:</code>
    <code class="nt" translate="no">name</code><code class="p" translate="no">:</code> <code class="s" translate="no">"</code><code class="s" translate="no">my-app-data</code><code class="s" translate="no">"</code>

<code class="nt" translate="no">networks</code><code class="p" translate="no">:</code>
  <code class="nt" translate="no">genai-net</code><code class="p" translate="no">:</code>
    <code class="nt" translate="no">name</code><code class="p" translate="no">:</code> <code class="s" translate="no">"</code><code class="s" translate="no">genai-net</code><code class="s" translate="no">"</code>
    <code class="nt" translate="no">driver</code><code class="p" translate="no">:</code> <code class="l-Scalar-Plain" translate="no">bridge</code>

<code class="nt" translate="no">secrets</code><code class="p" translate="no">:</code>
  <code class="nt" translate="no">openai_api_token</code><code class="p" translate="no">:</code>
    <code class="nt" translate="no">environment</code><code class="p" translate="no">:</code> <code class="l-Scalar-Plain" translate="no">OPENAI_API_KEY</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_deployment_of_ai_services_CO4-1" id="callout_deployment_of_ai_services_CO4-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Crea i container insieme ai volumi, alle reti e ai segreti associati.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO4-2" id="callout_deployment_of_ai_services_CO4-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Usa il Dockerfile che si trova nella stessa directory del file Compose per creare l'immagine <code translate="no">server</code>.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO4-3" id="callout_deployment_of_ai_services_CO4-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Utilizza i segreti di Docker per mascherare dati sensibili come le chiavi API all'interno dell'ambiente shell del container.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO4-4" id="callout_deployment_of_ai_services_CO4-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a></dt>
<dd><p>Crea una rete bridge <code translate="no">genai-net</code> e collega<span class="keep-together">ad essa</span> i contenitori <code translate="no">server</code> e <code translate="no">db</code> <span class="keep-together">.</span></p></dd>
</dl></div>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Se hai degli oggetti Docker come volumi e reti che gestisci tu stesso, puoi etichettarli con <code translate="no">external: true</code> nel file di composizione in modo che Docker Compose non li gestisca.</p>
</div>
<p>Una volta che hai un file <code translate="no">compose.yaml</code>, puoi utilizzare semplici comandi di composizione per gestire i tuoi contenitori:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no"><code class="c1" translate="no"># Start services defined in compose.yaml</code>
$ docker compose up<code class="w" translate="no"/>

<code class="c1" translate="no"># Stop and remove running services (won't remove created volumes and networks)</code>
$ docker compose down<code class="w" translate="no"/>

<code class="c1" translate="no"># Monitor output of running containers</code>
$ docker compose logs<code class="w" translate="no"/>

<code class="c1" translate="no"># List all running services with their status</code>
$ docker compose ps<code class="w" translate="no"/></pre>
<p>Puoi usare questi comandi per avviare/arrestare i servizi e visualizzare i loro log o lo stato dei container. Inoltre, puoi modificare il file Compose mostrato nell'<a data-type="xref" href="#docker_compose">Esempio 12-9</a> per utilizzare <code translate="no">watch</code> in modo che i servizi vengano aggiornati automaticamente quando modifichi e salvi il codice.</p>
<p>L<a data-type="xref" href="#docker_compose_watch">'esempio 12-10</a> mostra come utilizzare l'istruzione <code translate="no">watch</code> su una determinata directory.</p>
<div data-type="example" id="docker_compose_watch">
<h5><span class="label">Esempio 12-10. </span>Abilitazione di Docker Compose <code translate="no">watch</code> su una determinata directory</h5>
<pre data-code-language="yaml" data-type="programlisting" translate="no"><code class="nt" translate="no">services</code><code class="p" translate="no">:</code><code class="w" translate="no"/>
  <code class="nt" translate="no">server</code><code class="p" translate="no">:</code><code class="w" translate="no"/>
    <code class="c1" translate="no"># ...</code><code class="w" translate="no"/>
    <code class="nt" translate="no">develop</code><code class="p" translate="no">:</code><code class="w" translate="no"/>
      <code class="nt" translate="no">watch</code><code class="p" translate="no">:</code><code class="w" translate="no"/>
        <code class="p-Indicator" translate="no">-</code> <code class="nt" translate="no">action</code><code class="p" translate="no">:</code> <code class="l-Scalar-Plain" translate="no">sync</code><code class="w" translate="no"/>
          <code class="nt" translate="no">path</code><code class="p" translate="no">:</code> <code class="l-Scalar-Plain" translate="no">./src</code><code class="w" translate="no"/>
          <code class="nt" translate="no">target</code><code class="p" translate="no">:</code> <code class="l-Scalar-Plain" translate="no">/code</code><code class="w" translate="no"/></pre></div>
<p>Ogni volta che un file cambia nella cartella <code translate="no">./src</code> sul computer host, Compose sincronizza il suo contenuto con <code translate="no">/code</code> e aggiorna l'applicazione in esecuzione (servizio server) senza riavviarla.</p>
<p class="pagebreak-before less_space">Puoi quindi eseguire il processo <code translate="no">watch</code> utilizzando <code translate="no">docker compose watch</code>:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker compose watch<code class="w" translate="no"/>

<code class="o" translate="no">[</code>+<code class="o" translate="no">]</code> Running <code class="m" translate="no">2</code>/2<code class="w" translate="no"/>
 ✔ Container project-server-1  Created     <code class="m" translate="no">0</code>.0s<code class="w" translate="no"/>
 ✔ Container project-db-1      Recreated   <code class="m" translate="no">0</code>.1s<code class="w" translate="no"/>
Attaching to db-1, server-1<code class="w" translate="no"/>
         ⦿ watch enabled<code class="w" translate="no"/>
...<code class="w" translate="no"/></pre>
<p>Docker Compose <code translate="no">watch</code> consente una granularità maggiore rispetto a quella praticata con i montaggi bind, come mostrato nell'<a data-type="xref" href="#docker_compose">Esempio 12-9</a>. Ad esempio, ti permette di ignorare file specifici o intere directory all'interno dell'albero osservato per evitare problemi di prestazioni I/O.</p>
<p>Oltre a utilizzare Docker Compose <code translate="no">watch</code>, puoi unire e sovrascrivere più file Compose per creare una configurazione composita adatta a specifici ambienti di compilazione. In genere, il file <code translate="no">compose.yml</code> contiene le configurazioni di base, che possono essere sovrascritte da un file opzionale <code translate="no">compose.override.yml</code>. Ad esempio, come mostrato nell'<a data-type="xref" href="#compose_override">Esempio 12-11</a>, puoi iniettare le impostazioni dell'ambiente locale, montare i volumi locali e creare un nuovo servizio di database.</p>
<div data-type="example" id="compose_override">
<h5><span class="label">Esempio 12-11. </span>Unire e sovrascrivere i file Compose per le configurazioni di compilazione specifiche dell'ambiente</h5>
<pre data-code-language="yaml" data-type="programlisting" translate="no"><code class="c1" translate="no"># compose.yml</code>

<code class="nt" translate="no">services</code><code class="p" translate="no">:</code> <a class="co" href="#callout_deployment_of_ai_services_CO5-1" id="co_deployment_of_ai_services_CO5-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>
  <code class="nt" translate="no">server</code><code class="p" translate="no">:</code>
      <code class="nt" translate="no">ports</code><code class="p" translate="no">:</code>
        <code class="p-Indicator" translate="no">-</code> <code class="l-Scalar-Plain" translate="no">8000:8000</code>
      <code class="c1" translate="no"># ...</code>
      <code class="nt" translate="no">command</code><code class="p" translate="no">:</code> <code class="l-Scalar-Plain" translate="no">uvicorn</code> <code class="l-Scalar-Plain" translate="no">main:app</code>

<code class="c1" translate="no"># compose.override.yml</code>

<code class="nt" translate="no">services</code><code class="p" translate="no">:</code> <a class="co" href="#callout_deployment_of_ai_services_CO5-2" id="co_deployment_of_ai_services_CO5-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a>
  <code class="nt" translate="no">server</code><code class="p" translate="no">:</code>
    <code class="nt" translate="no">environment</code><code class="p" translate="no">:</code>
      <code class="p-Indicator" translate="no">-</code> <code class="l-Scalar-Plain" translate="no">LLM_API_KEY=$LLM_API_KEY</code>
      <code class="p-Indicator" translate="no">-</code> <code class="l-Scalar-Plain" translate="no">DATABASE_URL=$DATABASE_URL</code>
    <code class="nt" translate="no">volumes</code><code class="p" translate="no">:</code>
      <code class="p-Indicator" translate="no">-</code> <code class="l-Scalar-Plain" translate="no">./code:/code</code>
    <code class="nt" translate="no">command</code><code class="p" translate="no">:</code> <code class="l-Scalar-Plain" translate="no">uvicorn</code> <code class="l-Scalar-Plain" translate="no">main:app</code> <code class="l-Scalar-Plain" translate="no">--reload</code>

  <code class="nt" translate="no">database</code><code class="p" translate="no">:</code>
    <code class="nt" translate="no">image</code><code class="p" translate="no">:</code> <code class="l-Scalar-Plain" translate="no">postgres:latest</code>
    <code class="nt" translate="no">environment</code><code class="p" translate="no">:</code>
      <code class="p-Indicator" translate="no">-</code> <code class="l-Scalar-Plain" translate="no">POSTGRES_DB=genaidb</code>
      <code class="p-Indicator" translate="no">-</code> <code class="l-Scalar-Plain" translate="no">POSTGRES_USER=genaiuser</code>
      <code class="p-Indicator" translate="no">-</code> <code class="l-Scalar-Plain" translate="no">POSTGRES_PASSWORD=secretPassword!</code>
    <code class="nt" translate="no">volumes</code><code class="p" translate="no">:</code>
      <code class="p-Indicator" translate="no">-</code> <code class="l-Scalar-Plain" translate="no">db_data:/var/lib/postgresql/data</code>

<code class="nt" translate="no">networks</code><code class="p" translate="no">:</code>
  <code class="nt" translate="no">app-network</code><code class="p" translate="no">:</code>

<code class="nt" translate="no">volumes</code><code class="p" translate="no">:</code>
  <code class="nt" translate="no">db_data</code><code class="p" translate="no">:</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_deployment_of_ai_services_CO5-1" id="callout_deployment_of_ai_services_CO5-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Il file Compose di base contiene le istruzioni per eseguire la versione di produzione dell'applicazione.</p></dd>
<dt><a class="co" href="#co_deployment_of_ai_services_CO5-2" id="callout_deployment_of_ai_services_CO5-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Sovrascrive le istruzioni di base sostituendo il comando di avvio del container, inietta variabili locali e aggiunge configurazioni di volume e di rete con un servizio di database locale.</p></dd>
</dl></div>
<p>Per utilizzare questi file, esegui il seguente comando:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker compose up<code class="w" translate="no"/></pre>
<p>Docker Compose unirà automaticamente le configurazioni di entrambi i file Compose, applicando le impostazioni specifiche dell'ambiente dal file Compose sovrascritto.<a data-startref="ix_ch12-asciidoc30" data-type="indexterm" id="id1339"/></p>
</div></section>
<section data-pdf-bookmark="Enabling GPU Access in Docker Compose" data-type="sect2"><div class="sect2" id="id267">
<h2>Abilitazione dell'accesso alla GPU in Docker Compose</h2>
<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="enabling GPU access in Docker Compose" data-type="indexterm" id="id1340"/><a data-primary="Docker Compose tool" data-secondary="enabling GPU access in" data-type="indexterm" id="id1341"/><a data-primary="GPUs" data-secondary="enabling GPU access in Docker Compose" data-type="indexterm" id="id1342"/>Per accedere ai dispositivi GPU con i servizi gestiti da Docker Compose, dovrai aggiungere le istruzioni al file composto (vedi <a data-type="xref" href="#DockerComposeapp">Esempio 12-12</a>).</p>
<div data-type="example" id="DockerComposeapp">
<h5><span class="label">Esempio 12-12. </span>Aggiunta delle configurazioni della GPU al servizio app di Docker Compose</h5>
<pre data-code-language="yaml" data-type="programlisting" translate="no"><code class="nt" translate="no">services</code><code class="p" translate="no">:</code>
  <code class="nt" translate="no">app</code><code class="p" translate="no">:</code>
    <code class="c1" translate="no"># ...</code>
    <code class="nt" translate="no">deploy</code><code class="p" translate="no">:</code>
      <code class="nt" translate="no">resources</code><code class="p" translate="no">:</code>
        <code class="nt" translate="no">reservations</code><code class="p" translate="no">:</code>
          <code class="nt" translate="no">devices</code><code class="p" translate="no">:</code>
            <code class="p-Indicator" translate="no">-</code> <code class="nt" translate="no">driver</code><code class="p" translate="no">:</code> <code class="l-Scalar-Plain" translate="no">nvidia</code>
              <code class="nt" translate="no">count</code><code class="p" translate="no">:</code> <code class="l-Scalar-Plain" translate="no">1</code> <a class="co" href="#callout_deployment_of_ai_services_CO6-1" id="co_deployment_of_ai_services_CO6-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>
              <code class="nt" translate="no">capabilities</code><code class="p" translate="no">:</code> <code class="p-Indicator" translate="no">[</code><code class="nv" translate="no">gpu</code><code class="p-Indicator" translate="no">]</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_deployment_of_ai_services_CO6-1" id="callout_deployment_of_ai_services_CO6-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Limita il numero di dispositivi GPU accessibili dal servizio app.</p></dd>
</dl></div>
<p>Queste istruzioni ti daranno un controllo più granulare su come i tuoi servizi devono utilizzare le risorse della GPU.</p>
</div></section>
<section data-pdf-bookmark="Optimizing Docker Images" data-type="sect2"><div class="sect2" id="id268">
<h2>Ottimizzare le immagini di Docker</h2>
<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="optimizing Docker images" data-type="indexterm" id="ix_ch12-asciidoc31"/><a data-primary="Docker, containerization with" data-secondary="optimizing Docker images" data-type="indexterm" id="ix_ch12-asciidoc32"/>Se le tue immagini Docker crescono di dimensioni, saranno anche più lente da eseguire, costruire e testare in produzione. Inoltre, passerai molto tempo in fase di sviluppo ad iterare lo sviluppo dell'immagine.</p>
<p>In questo caso, è importante comprendere le strategie di ottimizzazione delle immagini, compreso il modo in cui utilizzare il meccanismo di stratificazione di Docker per mantenere le immagini leggere ed efficienti da eseguire, in particolare con i carichi di lavoro GenAI.</p>
<p>Questi sono alcuni modi per ridurre le dimensioni delle immagini e velocizzare il processo di creazione:</p>
<ul>
<li>
<p>Utilizzo di immagini di base minime</p>
</li>
<li>
<p>Evitare i runtime di inferenza su GPU</p>
</li>
<li>
<p>Esternalizzare i dati dell'applicazione</p>
</li>
<li>
<p>Ordinamento a strati e caching</p>
</li>
<li>
<p>Utilizzo di costruzioni in più fasi</p>
</li>
</ul>
<p>L'implementazione di queste ottimizzazioni, come mostrato nella <a data-type="xref" href="#build_optimization_impact">Tabella 12-4</a>, può ridurre le dimensioni tipiche delle immagini da diversi gigabyte a meno di 1 GB. Allo stesso modo, i tempi di creazione possono ridursi da diversi minuti in media a meno di un minuto.</p>
<table class="striped" id="build_optimization_impact">
<caption><span class="label">Tabella 12-4. </span>Impatto dell'ottimizzazione della creazione su un'immagine tipica<sup><a data-type="noteref" href="ch12.html#id1343" id="id1343-marker" translate="no">a</a></sup></caption>
<thead>
<tr>
<th>Fase di ottimizzazione</th>
<th>Tempo di costruzione (secondi)</th>
<th>Dimensione dell'immagine (GB)</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Iniziale</p></td>
<td><p>352.9</p></td>
<td><p>1.42</p></td>
</tr>
<tr>
<td><p>Utilizzo di immagini di base minime</p></td>
<td><p>38.5</p></td>
<td><p>1.38</p></td>
</tr>
<tr>
<td><p>Usa la cache</p></td>
<td><p>24.4</p></td>
<td><p>1.38</p></td>
</tr>
<tr>
<td><p>Ordinamento dei livelli</p></td>
<td><p>17.9</p></td>
<td><p>1.38</p></td>
</tr>
<tr>
<td><p>Costruzioni in più fasi</p></td>
<td><p>10.3</p></td>
<td><p>0.034 (34 MB)</p></td>
</tr>
</tbody>
<tbody><tr class="footnotes"><td colspan="3"><p data-type="footnote" id="id1343"><sup><a href="ch12.html#id1343-marker">a</a></sup> Fonte: <a class="orm:hideurl" href="https://www.warpbuild.com">warpbuild.com</a></p></td></tr></tbody></table>
<p>Esaminiamo ciascuno di essi in modo più dettagliato, con esempi di codice per maggiore chiarezza.</p>
<section data-pdf-bookmark="Use minimal base image" data-type="sect3"><div class="sect3" id="id278">
<h3>Usa un'immagine di base minima</h3>
<p><a data-primary="Docker, containerization with" data-secondary="optimizing Docker images" data-tertiary="using minimal base image" data-type="indexterm" id="id1344"/>Le immagini di base ti permettono di partire da un'immagine preconfigurata in modo da non dover installare tutto da zero, compreso l'interprete Python. Tuttavia, alcune immagini di base disponibili su Docker Hub potrebbero non essere adatte per le distribuzioni di produzione. Al contrario, vorrai selezionare l'immagine di base giusta con un'impronta minima del sistema operativo da cui lavorare per ottenere build più veloci e dimensioni dell'immagine ridotte, possibilmente con dipendenze Python preinstallate e supporto per l'installazione dei vari pacchetti.</p>
<p>Le immagini base Alpine utilizzano una distribuzione Alpine Linux leggera, progettata per essere piccola e sicura, che contiene solo gli strumenti <em>minimi</em> essenziali per l'esecuzione della tua applicazione, ma non supporta l'installazione di molti pacchetti Python. D'altra parte, le immagini base slim possono utilizzare altre distribuzioni Linux come Debian o CentOS, che contengono gli strumenti essenziali <em>necessari</em> per l'esecuzione di applicazioni che le rendono più grandi delle immagini base Alpine.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Usa le immagini base slim se ti interessa il tempo di costruzione e le immagini base alpine se ti interessa la dimensione dell'immagine.</p>
</div>
<p>Puoi utilizzare le immagini di base di <code translate="no">slim</code>, come <code translate="no">python:3.12-slim</code>, o anche le immagini di base Alpine, come <code translate="no">python:3.12-alpine</code>, che possono avere una dimensione di 71,4 MB. Un'immagine Alpine "bare-bones" può addirittura scendere a 12,1 MB. Il comando seguente mostra un elenco di immagini base estratte dal repository di Docker:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$ docker image ls<code class="w" translate="no"/>

REPOSITORY  TAG         IMAGE ID       CREATED         SIZE<code class="w" translate="no"/>
alpine      <code class="m" translate="no">3</code>.20        3463e98c969d   <code class="m" translate="no">4</code> weeks ago     <code class="m" translate="no">12</code>.1MB<code class="w" translate="no"/>
python      <code class="m" translate="no">3</code>.12-alpine c6de2e87f545   <code class="m" translate="no">6</code> days ago      <code class="m" translate="no">71</code>.4MB<code class="w" translate="no"/>
python      <code class="m" translate="no">3</code>.12-slim   1ba4bc34383e   <code class="m" translate="no">6</code> days ago      186MB<code class="w" translate="no"/></pre>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Le immagini di dimensioni standard contengono in genere una distribuzione Linux completa, come Ubuntu o Debian, con una serie di pacchetti e dipendenze preinstallate, che le rendono adatte allo sviluppo locale ma forse non agli ambienti di produzione.</p>
</div>
</div></section>
<section data-pdf-bookmark="Avoid GPU inference runtimes" data-type="sect3"><div class="sect3" id="id226">
<h3>Evita i runtime di inferenza su GPU</h3>
<p><a data-primary="Docker, containerization with" data-secondary="optimizing Docker images" data-tertiary="avoiding GPU inference runtimes" data-type="indexterm" id="ix_ch12-asciidoc33"/><a data-primary="GPUs" data-secondary="avoiding GPU inference runtimes in Docker" data-type="indexterm" id="ix_ch12-asciidoc34"/>Nei carichi di lavoro di intelligenza artificiale in cui stai servendo modelli ML/GenAI, potresti dover installare framework di deep learning, dipendenze e librerie per GPU che possono far esplodere improvvisamente l'ingombro delle tue immagini. Ad esempio, per fare inferenze su una GPU usando la libreria <code translate="no">transformers</code>, dovrai installare 3 GB di pacchetti NVIDIA per l'inferenza su GPU, 1,6 GB per <code translate="no">torch</code> per eseguire l'inferenza.</p>
<p>Purtroppo non è possibile ridurre le dimensioni dell'immagine se devi utilizzare una GPU per eseguire un'inferenza.<a data-primary="Open Neural Network Exchange (ONNX)" data-type="indexterm" id="id1345"/>Tuttavia, se puoi evitare l'inferenza tramite GPU e affidarti solo alla CPU, puoi ridurre le dimensioni dell'immagine fino a 10 volte utilizzando il runtime Open Neural Network Exchange (ONNX) con quantizzazione del modello.</p>
<p>Come discusso nel <a data-type="xref" href="ch10.html#ch10">Capitolo 10</a>, puoi utilizzare la quantizzazione INT8 con un modello ONNX per beneficiare della compressione del modello senza perdere molto in qualità dell'output.</p>
<p>Per passare dal runtime di inferenza GPU al runtime ONNX per i modelli trasformatori Hugging Face, puoi usare il pacchetto <code translate="no">transformers[onnx]</code>:</p>
<pre data-type="programlisting" translate="no">$ pip install transformers[onnx]</pre>
<p>Puoi quindi esportare qualsiasi checkpoint del modello del trasformatore Hugging Face con le configurazioni predefinite nel formato ONNX con <code translate="no">transformers.onnx</code>:</p>
<pre data-type="programlisting" translate="no">$ python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/</pre>
<p>Questo comando esporta il checkpoint del modello <code translate="no">distilbert/distilbert-base-uncased</code> come un grafico ONNX memorizzato in <code translate="no">onnx/model.onnx</code>, che può essere eseguito con qualsiasi acceleratore di modelli Hugging Face che supporti lo standard ONNX, come mostrato nell'<a data-type="xref" href="#docker_onnx">Esempio 12-13</a>.</p>
<div data-type="example" id="docker_onnx">
<h5><span class="label">Esempio 12-13. </span>Inferenza del modello utilizzando il runtime ONNX con quantizzazione</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">from</code> <code class="nn" translate="no">onnxruntime</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">InferenceSession</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">transformers</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">AutoTokenizer</code>

<code class="n" translate="no">tokenizer</code> <code class="o" translate="no">=</code> <code class="n" translate="no">AutoTokenizer</code><code class="o" translate="no">.</code><code class="n" translate="no">from_pretrained</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">distilbert/distilbert-base-uncased</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>
<code class="n" translate="no">session</code> <code class="o" translate="no">=</code> <code class="n" translate="no">InferenceSession</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">onnx/model.onnx</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code>

<code class="n" translate="no">inputs</code> <code class="o" translate="no">=</code> <code class="n" translate="no">tokenizer</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Using DistilBERT with ONNX Runtime!</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code> <code class="n" translate="no">return_tensors</code><code class="o" translate="no">=</code><code class="s2" translate="no">"</code><code class="s2" translate="no">np</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code> <a class="co" href="#callout_deployment_of_ai_services_CO7-1" id="co_deployment_of_ai_services_CO7-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a>
<code class="n" translate="no">output</code> <code class="o" translate="no">=</code> <code class="n" translate="no">session</code><code class="o" translate="no">.</code><code class="n" translate="no">run</code><code class="p" translate="no">(</code><code class="n" translate="no">output_names</code><code class="o" translate="no">=</code><code class="p" translate="no">[</code><code class="s2" translate="no">"</code><code class="s2" translate="no">last_hidden_state</code><code class="s2" translate="no">"</code><code class="p" translate="no">]</code><code class="p" translate="no">,</code> <code class="n" translate="no">input_feed</code><code class="o" translate="no">=</code><code class="nb" translate="no">dict</code><code class="p" translate="no">(</code><code class="n" translate="no">inputs</code><code class="p" translate="no">)</code><code class="p" translate="no">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_deployment_of_ai_services_CO7-1" id="callout_deployment_of_ai_services_CO7-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Il runtime ONNX si aspetta come input gli array <code translate="no">numpy</code>.</p></dd>
</dl></div>
<p>Utilizzando una tecnica come quella illustrata nell'<a data-type="xref" href="#docker_onnx">Esempio 12-13</a>, è possibile ridurre le dimensioni delle immagini da 5 a 10 GB a circa 0,5 GB, il che rappresenta un'enorme riduzione dell'ingombro, significativamente più conveniente e scalabile.<a data-startref="ix_ch12-asciidoc34" data-type="indexterm" id="id1346"/><a data-startref="ix_ch12-asciidoc33" data-type="indexterm" id="id1347"/></p>
</div></section>
<section data-pdf-bookmark="Externalize application data" data-type="sect3"><div class="sect3" id="id279">
<h3>Esternalizzare i dati dell'applicazione</h3>
<p><a data-primary="Docker, containerization with" data-secondary="optimizing Docker images" data-tertiary="externalizing application data" data-type="indexterm" id="id1348"/>Uno dei principali fattori che contribuiscono alle dimensioni dell'immagine è la copia dei modelli e dei dati dell'applicazione nell'immagine durante la creazione. Questo approccio aumenta sia il tempo di creazione che le dimensioni dell'immagine.</p>
<p>Un approccio migliore consiste nell'utilizzare volumi durante lo sviluppo locale e soluzioni di archiviazione esterne per scaricare e caricare i modelli all'avvio dell'applicazione in produzione. Negli ambienti di orchestrazione di container Kubernetes, puoi anche utilizzare volumi persistenti per l'archiviazione dei modelli.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Se il contenitore dell'applicazione impiega molto tempo per scaricare i dati e gli artefatti del modello da un'origine esterna, i controlli sullo stato di salute possono fallire e la piattaforma di hosting può chiudere prematuramente i contenitori. In questi casi, configura le sonde di controllo dello stato di salute in modo che attendano più a lungo o, come ultima risorsa, inserisci il modello nell'immagine.</p>
</div>
</div></section>
<section data-pdf-bookmark="Layer ordering and caching" data-type="sect3"><div class="sect3" id="id269">
<h3>Ordinamento dei livelli e caching</h3>
<p><a data-primary="caching" data-secondary="in Docker" data-type="indexterm" id="ix_ch12-asciidoc35"/><a data-primary="Docker, containerization with" data-secondary="optimizing Docker images" data-tertiary="layer ordering and caching" data-type="indexterm" id="ix_ch12-asciidoc36"/>Docker utilizza un filesystem a livelli per creare livelli in un'immagine per ogni istruzione del file Docker. Questi livelli sono come una pila, con ogni livello che aggiunge altro contenuto in cima ai livelli precedenti. Ogni volta che un livello viene modificato, quel livello (e altri livelli) dovrà essere ricostruito per far sì che le modifiche appaiano nell'immagine (cioè, la cache di compilazione deve essere invalidata).</p>
<p>Viene creato un livello (cioè un'istantanea del filesystem) se l'istruzione sta scrivendo o cancellando dei file nel filesystem union del contenitore.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Le istruzioni di Dockerfile che modificano il filesystem come <code translate="no">ENV</code>, <code translate="no">COPY</code>, <code translate="no">ADD</code> e <code translate="no">RUN</code> contribuiscono a creare nuovi livelli nel processo di compilazione, aumentando di fatto le dimensioni dell'immagine. D'altra parte, istruzioni come <code translate="no">WORKDIR</code>, <code translate="no">ENTRYPOINT</code>, <code translate="no">LABEL</code> e <code translate="no">CMD</code> che aggiornano solo i metadati dell'immagine non creano alcun livello e alcuna cache di compilazione.</p>
</div>
<p>Dopo la creazione, ogni livello viene memorizzato nella cache per essere riutilizzato in tutte le ricostruzioni dell'immagine, se le istruzioni e i file da cui dipende non sono stati modificati dall'ultima compilazione. Per questo motivo, l'ideale è scrivere un file Docker che ti permetta di fermare, distruggere, ricostruire e sostituire i contenitori con una configurazione minima.</p>
<p>Ci sono alcune tecniche che puoi utilizzare per ridurre al minimo e ottimizzare il più possibile questi livelli.</p>
<section data-pdf-bookmark="Layer ordering to avoid frequent cache invalidation" data-type="sect4"><div class="sect4" id="id445">
<h4>Ordinamento dei livelli per evitare l'invalidazione frequente della cache</h4>
<p>Poiché le modifiche ai livelli precedenti possono invalidare la cache di compilazione e quindi ripetere i passaggi, dovresti ordinare i tuoi Dockerfile da quelli più stabili (ad esempio, le installazioni) a quelli che cambiano più frequentemente o sono volatili (ad esempio, il codice dell'applicazione, i file di configurazione).</p>
<p>Seguendo questo ordine, posiziona le istruzioni più stabili ma costose (ad esempio, il download di modelli o l'installazione di dipendenze pesanti) all'inizio del file Docker e le operazioni volatili e veloci (ad esempio, la copia del codice dell'applicazione) in fondo.</p>
<p>Immagina che il tuo file Dockerfile abbia questo aspetto:</p>
<pre data-code-language="dockerfile" data-type="programlisting" translate="no"><code class="k" translate="no">FROM</code> <code class="s" translate="no">python:3.12-slim</code> <code class="k" translate="no">as</code> <code class="s" translate="no">base</code><code class="w" translate="no"/>
<code class="c" translate="no"># Changes to the</code><code class="w" translate="no"/>
<code class="k" translate="no">COPY</code> . .<code class="w" translate="no"/>
<code class="k" translate="no">RUN</code> pip install requirements.txt<code class="w" translate="no"/></pre>
<p>In questo caso stai creando un livello copiando la tua cartella di lavoro contenente il codice dell'applicazione nell'immagine prima di scaricare e installare le dipendenze.</p>
<p>Se uno qualsiasi dei file sorgente cambia, Docker builder invaliderà la cache causando la ripetizione dell'installazione delle dipendenze, che è costosa e può richiedere diversi minuti per essere completata, se non viene memorizzata nella cache da <code translate="no">pip</code>.</p>
<p>Per evitare di ripetere passaggi costosi, puoi ordinare logicamente le istruzioni del tuo file Docker per ottimizzare la cache del livello, riordinando istruzioni come queste:</p>
<pre data-code-language="dockerfile" data-type="programlisting" translate="no"><code class="k" translate="no">FROM</code> <code class="s" translate="no">python:3.12-slim</code> <code class="k" translate="no">as</code> <code class="s" translate="no">base</code><code class="w" translate="no"/>
<code class="k" translate="no">COPY</code> requirements.txt requirements.txt<code class="w" translate="no"/>
<code class="k" translate="no">RUN</code> pip install requirements.txt<code class="w" translate="no"/>
<code class="k" translate="no">COPY</code> . .<code class="w" translate="no"/></pre>
<p>Ora qualsiasi modifica ai file sorgente non influirà sulla lunga fase di installazione delle dipendenze, velocizzando drasticamente il processo di compilazione.</p>
</div></section>
<section data-pdf-bookmark="Minimize layers" data-type="sect4"><div class="sect4" id="id227">
<h4>Riduci al minimo i livelli</h4>
<p>Per mantenere le dimensioni delle immagini ridotte, dovrai ridurre al minimo i livelli di immagine.</p>
<p>Una tecnica semplice per ottenere questo risultato è quella di combinare più istruzioni <code translate="no">RUN</code> in una sola. Ad esempio, invece di scrivere più installazioni <code translate="no">RUN apt-get</code>, puoi combinarle in un unico comando <code translate="no">RUN</code> con <code translate="no">&amp;&amp;</code>:</p>
<pre data-code-language="dockerfile" data-type="programlisting" translate="no"><code class="k" translate="no">RUN</code> apt-get update <code class="o" translate="no">&amp;&amp;</code> apt-get install -y<code class="w" translate="no"/></pre>
<p><a data-primary="cache busting" data-type="indexterm" id="id1349"/>In questo modo si evita di aggiungere livelli non necessari e si prevengono i problemi di cache con <code translate="no">apt-get update</code> che utilizza la tecnica del <em>cache busting</em>.</p>
<p>Poiché il costruttore può potenzialmente saltare l'aggiornamento dell'indice dei pacchetti, causando il fallimento delle installazioni o l'utilizzo di pacchetti obsoleti, l'utilizzo di <code translate="no">&amp;&amp;</code> assicura che vengano installati i pacchetti più recenti se l'indice dei pacchetti viene aggiornato.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Puoi anche usare il flag <code translate="no">--no-cache</code> quando utilizzi <code translate="no">docker build</code> per evitare gli hit della cache e garantire un download fresco delle immagini di base e delle dipendenze a ogni compilazione.</p>
</div>
</div></section>
<section data-pdf-bookmark="Keep build context small" data-type="sect4"><div class="sect4" id="id228">
<h4>Mantenere un contesto di costruzione piccolo</h4>
<p><a data-primary="build context (Docker)" data-type="indexterm" id="id1350"/>Il <em>contesto di compilazione</em> è l'insieme dei file e delle directory che verranno inviati al costruttore per eseguire l'istruzione Dockerfile. Un contesto di compilazione più piccolo riduce la quantità di dati inviati al costruttore e diminuisce la possibilità di invalidare la cache, rendendo le compilazioni più veloci.</p>
<p>Quando usi il comando <code translate="no">COPY . .</code> in un file Docker per copiare la tua directory di lavoro in un'immagine, potresti anche aggiungere cache di strumenti, dipendenze di sviluppo, ambienti virtuali e file inutilizzati nel contesto di compilazione. Non solo le dimensioni dell'immagine aumenteranno, ma anche il costruttore Docker metterà in cache questi file non necessari. Qualsiasi modifica a questi file invaliderà la compilazione, riavviando l'intero processo di compilazione.</p>
<p>Per evitare di invalidare inutilmente la cache, puoi aggiungere un file <em>.dockerignore</em> accanto al tuo file Docker, elencando tutti i file e le directory di cui i tuoi servizi non avranno bisogno in produzione. A titolo di esempio, ecco gli elementi che puoi includere in un file <em>.dockerignore</em>:</p>
<pre data-type="programlisting" translate="no">**/.DS_Store
**/__pycache__
**/.mypy_cache
**/.venv
**/.env
**/.git</pre>
<p>Docker builder ignorerà questi file anche quando eseguirai il comando <code translate="no">COPY</code> su tutta la tua directory di lavoro.</p>
</div></section>
<section data-pdf-bookmark="Use cache and bind mounts" data-type="sect4"><div class="sect4" id="id229">
<h4>Usa la cache e i montaggi bind</h4>
<p><a data-primary="bind mounts (Docker)" data-type="indexterm" id="id1351"/><a data-primary="cache mounts (Docker)" data-type="indexterm" id="id1352"/>Puoi utilizzare i <em>montaggi bind</em> per evitare di aggiungere livelli non necessari all'immagine e i <em>montaggi cache</em> per velocizzare le build successive.</p>
<p>I montaggi Bind includono temporaneamente i file nel contesto di compilazione per una singola istruzione <code translate="no">RUN</code> e non persisteranno come livelli di immagine in seguito. I montaggi Cache specificano una posizione persistente della cache in cui puoi leggere e scrivere dati in più build.</p>
<p>Ecco un esempio in cui puoi scaricare un modello pre-addestrato da Hugging Face in una cache montata per ottimizzare la cache dei livelli:</p>
<pre data-code-language="dockerfile" data-type="programlisting" translate="no"><code class="k" translate="no">RUN</code> --mount<code class="o" translate="no">=</code><code class="nv" translate="no">type</code><code class="o" translate="no">=</code>cache,target<code class="o" translate="no">=</code>/root/.cache/huggingface <code class="o" translate="no">&amp;&amp;</code> <code class="se" translate="no">\</code>
    pip install transformers <code class="o" translate="no">&amp;&amp;</code> <code class="se" translate="no">\</code>
    python -c <code class="s2" translate="no">"from transformers import AutoModel; \</code>
<code class="s2" translate="no">    AutoModel.from_pretrained('bert-base-uncased')"</code><code class="w" translate="no"/></pre>
<p>Questa istruzione <code translate="no">RUN</code> crea una cache del modello preaddestrato scaricato all'indirizzo<span class="keep-together"><code translate="no">/root/.cache/huggingface</code>, che può essere condivisa in più build. Questo aiuta a evitare download ridondanti e ottimizza il processo di creazione riutilizzando i livelli in cache.</span> </p>
<p>Puoi anche utilizzare il flag <code translate="no">--no-cache-dir</code> quando utilizzi il gestore di pacchetti <code translate="no">pip</code> per evitare del tutto la cache e ridurre al minimo le dimensioni dell'immagine. Tuttavia, il processo di compilazione sarà significativamente più lento perché le compilazioni successive dovranno essere riscaricate ogni volta.</p>
</div></section>
<section data-pdf-bookmark="Use external cache" data-type="sect4"><div class="sect4" id="id230">
<h4>Usa una cache esterna</h4>
<p>Se stai costruendo e distribuendo container utilizzando una pipeline CI/CD, puoi trarre vantaggio da una cache esterna ospitata in una posizione remota. Una cache esterna può accelerare drasticamente il processo di compilazione nelle pipeline CI/CD dove i costruttori sono spesso effimeri e i minuti di compilazione sono preziosi.</p>
<p>Per utilizzare una cache esterna, puoi specificare le opzioni <code translate="no">--cache-to</code> e <code translate="no">--cache-from</code> con il comando <code translate="no">docker buildx build</code>:</p>
<pre data-code-language="dockerfile" data-type="programlisting" translate="no">docker buildx build --cache-from <code class="nv" translate="no">type</code><code class="o" translate="no">=</code>registry,ref<code class="o" translate="no">=</code>user/app:buildcache .<code class="w" translate="no"/></pre>
<p>Oltre all'ordinamento dei livelli e all'ottimizzazione della cache, puoi utilizzare le costruzioni multi-stadio per ridurre significativamente le dimensioni delle immagini.<a data-startref="ix_ch12-asciidoc36" data-type="indexterm" id="id1353"/><a data-startref="ix_ch12-asciidoc35" data-type="indexterm" id="id1354"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Multi-stage builds" data-type="sect3"><div class="sect3" id="id231">
<h3>Costruzioni in più fasi</h3>
<p><a data-primary="Docker, containerization with" data-secondary="optimizing Docker images" data-tertiary="multi-stage builds" data-type="indexterm" id="ix_ch12-asciidoc37"/>Utilizzando le <em>build multi-stadio</em>, puoi ridurre le dimensioni dell'immagine finale suddividendo le istruzioni del file Docker in fasi distinte. Le fasi comuni possono essere riutilizzate per includere componenti condivisi e fungere da punto di partenza per le fasi successive.</p>
<p>Puoi anche copiare selettivamente gli artefatti da una fase all'altra, lasciando indietro tutto ciò che non vuoi nell'immagine finale. Questo assicura che solo gli output necessari siano inclusi nell'immagine finale dalle fasi precedenti, evitando gli artefatti non essenziali. Inoltre, puoi anche eseguire più fasi di creazione in parallelo per accelerare il processo di creazione delle tue immagini.</p>
<p>Un modello di compilazione multi-stadio comune è quello che prevede un'immagine di test/sviluppo e una di produzione più snella, che partono entrambe da un'immagine condivisa del primo stadio. L'immagine di sviluppo o di test può includere ulteriori livelli di strumenti (ad esempio, compilatori, sistemi di compilazione e strumenti di debug) per supportare i flussi di lavoro richiesti.</p>
<p>Immagina di dover servire un modello di trasformatore bert da Hugging Face in un servizio FastAPI. Puoi scrivere le istruzioni del tuo Dockerfile in modo da utilizzare tre fasi sequenziali distinte.</p>
<p>La prima fase scarica il modello del trasformatore su <code translate="no">/root/.cache/huggingface</code> e crea un ambiente virtuale Python su <code translate="no">/opt/venv</code>:</p>
<pre data-code-language="dockerfile" data-type="programlisting" translate="no"><code class="c" translate="no"># Stage 1: Base</code><code class="w" translate="no"/>
<code class="k" translate="no">FROM</code> <code class="s" translate="no">python:3.11.0-slim</code> <code class="k" translate="no">as</code> <code class="s" translate="no">base</code><code class="w" translate="no"/>

<code class="k" translate="no">RUN</code> python -m venv /opt/venv<code class="w" translate="no"/>
<code class="k" translate="no">RUN</code> pip install transformers <code class="o" translate="no">&amp;&amp;</code> <code class="se" translate="no">\</code>
    python -c <code class="s2" translate="no">"from transformers import AutoModel; \</code>
<code class="s2" translate="no">    AutoModel.from_pretrained('bert-base-uncased')"</code><code class="w" translate="no"/>
<code class="k" translate="no">RUN</code> --mount<code class="o" translate="no">=</code><code class="nv" translate="no">type</code><code class="o" translate="no">=</code>cache,target<code class="o" translate="no">=</code>/root/.cache/pip <code class="se" translate="no">\</code>
    --mount<code class="o" translate="no">=</code><code class="nv" translate="no">type</code><code class="o" translate="no">=</code>bind,source<code class="o" translate="no">=</code>requirements.txt,target<code class="o" translate="no">=</code>requirements.txt <code class="se" translate="no">\</code>
    python -m pip install -r requirements.txt<code class="w" translate="no"/></pre>
<p>La seconda fase copia gli artefatti del modello e l'ambiente virtuale Python <code translate="no">/opt/ven</code> dalla fase <code translate="no">base</code> prima di copiare i file sorgente e creare una versione di produzione del servizio FastAPI:</p>
<pre data-code-language="dockerfile" data-type="programlisting" translate="no"><code class="c" translate="no"># Stage 2: Production</code><code class="w" translate="no"/>
<code class="k" translate="no">FROM</code> <code class="s" translate="no">base</code> <code class="k" translate="no">as</code> <code class="s" translate="no">production</code><code class="w" translate="no"/>
<code class="k" translate="no">RUN</code> apt-get update <code class="o" translate="no">&amp;&amp;</code> apt-get install -y<code class="w" translate="no"/>
<code class="k" translate="no">COPY</code> --from<code class="o" translate="no">=</code>base /opt/venv /opt/venv<code class="w" translate="no"/>
<code class="k" translate="no">COPY</code> --from<code class="o" translate="no">=</code>base /root/.cache/huggingface /root/.cache/huggingface<code class="w" translate="no"/>

<code class="k" translate="no">WORKDIR</code> <code class="s" translate="no">/code</code><code class="w" translate="no"/>
<code class="k" translate="no">COPY</code> . .<code class="w" translate="no"/>

<code class="k" translate="no">EXPOSE</code> <code class="s" translate="no">8000</code><code class="w" translate="no"/>

<code class="k" translate="no">ENV</code> <code class="nv" translate="no">BUILD_ENV</code><code class="o" translate="no">=</code>PROD<code class="w" translate="no"/>
<code class="k" translate="no">CMD</code> <code class="p" translate="no">[</code><code class="s2" translate="no">"uvicorn"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"main:app"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"--host"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"0.0.0.0"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"--port"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"8000"</code><code class="p" translate="no">]</code><code class="w" translate="no"/></pre>
<p>L'ultima fase copia l'ambiente virtuale Python della fase di produzione con i pacchetti installati, aggiunge diversi strumenti di sviluppo e avvia il server con la funzione di ricarica a caldo:</p>
<pre data-code-language="dockerfile" data-type="programlisting" translate="no"><code class="c" translate="no"># Stage 3: Development</code><code class="w" translate="no"/>
<code class="k" translate="no">FROM</code> <code class="s" translate="no">production</code> <code class="k" translate="no">as</code> <code class="s" translate="no">development</code><code class="w" translate="no"/>

<code class="k" translate="no">COPY</code> --from<code class="o" translate="no">=</code>production /opt/venv /opt/venv<code class="w" translate="no"/>
<code class="k" translate="no">COPY</code> ./requirements_dev.txt ./<code class="w" translate="no"/>
<code class="k" translate="no">RUN</code> pip install --no-cache-dir --upgrade -r requirements_dev.txt<code class="w" translate="no"/>

<code class="k" translate="no">ENV</code> <code class="nv" translate="no">BUILD_ENV</code><code class="o" translate="no">=</code>DEV<code class="w" translate="no"/>
<code class="k" translate="no">CMD</code> <code class="p" translate="no">[</code><code class="s2" translate="no">"uvicorn"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"main:app"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"--host"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"0.0.0.0"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"--port"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"8000"</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"--reload"</code><code class="p" translate="no">]</code><code class="w" translate="no"/></pre>
<p>Utilizzando un unico file Docker, siamo stati in grado di creare tre fasi distinte e di utilizzarle a nostro piacimento tramite il comando <code translate="no">--target development</code> quando necessario<a data-startref="ix_ch12-asciidoc37" data-type="indexterm" id="id1355"/>.<a data-startref="ix_ch12-asciidoc32" data-type="indexterm" id="id1356"/><a data-startref="ix_ch12-asciidoc31" data-type="indexterm" id="id1357"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="docker init" data-type="sect2"><div class="sect2" id="id232">
<h2>Docker init</h2>
<p><a data-primary="deployment of AI services" data-secondary="containerization with Docker" data-tertiary="docker init" data-type="indexterm" id="id1358"/>Ora hai una conoscenza approfondita del processo di containerizzazione con la piattaforma Docker e delle relative best practice.</p>
<p>Se hai bisogno di aggiungere Docker a un progetto esistente, puoi usare il comando <code translate="no">docker init</code>, che ti guiderà attraverso una procedura guidata per creare tutti i file di distribuzione Docker necessari nella tua directory di lavoro corrente:</p>
<pre data-type="programlisting" translate="no">$ docker init
&gt;&gt; Answer a few questions in the terminal...

project/
│
├── .dockerignore
├── compose.yaml
├── Dockerfile
└── README.Docker.md
... # other application files</pre>
<p>Questo ti fornirà un ottimo punto di partenza su cui potrai lavorare per includere ulteriori passaggi di configurazione, dipendenze o servizi, a seconda delle necessità.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Ti consiglio di usare <code translate="no">docker init</code> quando inizi, perché ogni file generato aderirà alle migliori pratiche, tra cui l'uso di <code translate="no">dockerignore</code>, l'ottimizzazione dei livelli di immagine, l'uso di bind e cache mount per l'installazione dei pacchetti e il passaggio a utenti non root.</p>
</div>
<p>Una volta che hai un'immagine ottimizzata e un set di container funzionanti, puoi scegliere qualsiasi provider Cloud o soluzione self-hosting per inviare le immagini ai registri e distribuire i tuoi nuovi servizi GenAI.<a data-startref="ix_ch12-asciidoc10" data-type="indexterm" id="id1359"/><a data-startref="ix_ch12-asciidoc9" data-type="indexterm" id="id1360"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id233">
<h1>Riassunto</h1>
<p>In questo capitolo abbiamo esaminato varie strategie per distribuire i tuoi servizi GenAI, ad esempio su macchine virtuali, come funzioni cloud, con piattaforme di servizi app gestiti o tramite container. In questo capitolo abbiamo spiegato come la virtualizzazione si differenzia dalla containerizzazione e perché potresti voler distribuire i tuoi servizi come container.</p>
<p>Poi hai imparato a conoscere la piattaforma di containerizzazione Docker e come puoi usarla per creare immagini autocontenute delle tue applicazioni che possono essere eseguite come contenitori.</p>
<p>Abbiamo parlato dei meccanismi di archiviazione e di rete di Docker che ti permettono di conservare i dati utilizzando il filesystem union nei container e di come collegare i container con diversi driver di rete.</p>
<p>Infine, ti sono state presentate varie tecniche di ottimizzazione per ridurre i tempi di creazione e le dimensioni delle immagini per distribuire i tuoi servizi GenAI nel modo più efficiente possibile.</p>
<p>Con i servizi containerizzati, puoi inviarli ai registri dei container per condividerli, distribuirli ed eseguirli su qualsiasi cloud o ambiente di hosting di tua scelta.<a data-startref="ix_ch12-asciidoc0" data-type="indexterm" id="id1361"/></p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id1291"><sup><a href="ch12.html#id1291-marker">1</a></sup> Le immagini Python di base Slim bilanciano le dimensioni e la compatibilità della distribuzione Linux con una gamma più ampia di pacchetti Python rispetto alle immagini Python di base Alpine che riducono al minimo le dimensioni ma richiedono configurazioni extra.</p><p data-type="footnote" id="id1292"><sup><a href="ch12.html#id1292-marker">2</a></sup> Puoi utilizzare il flag <code translate="no">-p</code> o <code translate="no">--publish</code> durante l'esecuzione del container per mappare e abilitare l'accesso al container tramite una porta.</p><p data-type="footnote" id="id1297"><sup><a href="ch12.html#id1297-marker">3</a></sup> Le immagini create su una macchina possono essere eseguite solo su altre macchine con la stessa architettura di processore.</p><p data-type="footnote" id="id1317"><sup><a href="ch12.html#id1317-marker">4</a></sup> Puoi comunque eseguire i file eseguibili con il solo permesso <code translate="no">r</code> utilizzando il comando <code translate="no">bash script.sh</code> invece di <code translate="no">./script.sh</code>.</p><p data-type="footnote" id="id1336"><sup><a href="ch12.html#id1336-marker">5</a></sup> Consulta la documentazione NVIDIA su come installare il toolkit CUDA e i driver grafici più recenti per il tuo sistema.</p></div></div></section></div></div></body></html>