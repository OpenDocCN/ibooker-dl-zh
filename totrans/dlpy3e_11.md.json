["```py\n!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n!tar -xf images.tar.gz\n!tar -xf annotations.tar.gz \n```", "```py\nimport pathlib\n\ninput_dir = pathlib.Path(\"images\")\ntarget_dir = pathlib.Path(\"annotations/trimaps\")\n\ninput_img_paths = sorted(input_dir.glob(\"*.jpg\"))\n# Ignores some spurious files in the trimaps directory that start with\n# a \".\"\ntarget_paths = sorted(target_dir.glob(\"[!.]*.png\")) \n```", "```py\nimport matplotlib.pyplot as plt\nfrom keras.utils import load_img, img_to_array, array_to_img\n\nplt.axis(\"off\")\n# Displays input image number 9\nplt.imshow(load_img(input_img_paths[9])) \n```", "```py\ndef display_target(target_array):\n    # The original labels are 1, 2, and 3\\. We subtract 1 so that the\n    # labels range from 0 to 2, and then we multiply by 127 so that the\n    # labels become 0 (black), 127 (gray), 254 (near-white).\n    normalized_array = (target_array.astype(\"uint8\") - 1) * 127\n    plt.axis(\"off\")\n    plt.imshow(normalized_array[:, :, 0])\n\n# We use color_mode='grayscale' so that the image we load is treated as\n# having a single color channel.\nimg = img_to_array(load_img(target_paths[9], color_mode=\"grayscale\"))\ndisplay_target(img) \n```", "```py\nimport numpy as np\nimport random\n\n# We resize everything to 200 x 200 for this example.\nimg_size = (200, 200)\n# Total number of samples in the data\nnum_imgs = len(input_img_paths)\n\n# Shuffles the file paths (they were originally sorted by breed). We\n# use the same seed (1337) in both statements to ensure that the input\n# paths and target paths stay in the same order.\nrandom.Random(1337).shuffle(input_img_paths)\nrandom.Random(1337).shuffle(target_paths)\n\ndef path_to_input_image(path):\n    return img_to_array(load_img(path, target_size=img_size))\n\ndef path_to_target(path):\n    img = img_to_array(\n        load_img(path, target_size=img_size, color_mode=\"grayscale\")\n    )\n    # Subtracts 1 so that our labels become 0, 1, and 2\n    img = img.astype(\"uint8\") - 1\n    return img\n\n# Loads all images in the input_imgs float32 array and their masks in\n# the targets uint8 array (same order). The inputs have three channels\n# (RGB values), and the targets have a single channel (which contains\n# integer labels).\ninput_imgs = np.zeros((num_imgs,) + img_size + (3,), dtype=\"float32\")\ntargets = np.zeros((num_imgs,) + img_size + (1,), dtype=\"uint8\")\nfor i in range(num_imgs):\n    input_imgs[i] = path_to_input_image(input_img_paths[i])\n    targets[i] = path_to_target(target_paths[i]) \n```", "```py\n# Reserves 1,000 samples for validation\nnum_val_samples = 1000\n# Splits the data into a training and a validation set\ntrain_input_imgs = input_imgs[:-num_val_samples]\ntrain_targets = targets[:-num_val_samples]\nval_input_imgs = input_imgs[-num_val_samples:]\nval_targets = targets[-num_val_samples:] \n```", "```py\nimport keras\nfrom keras.layers import Rescaling, Conv2D, Conv2DTranspose\n\ndef get_model(img_size, num_classes):\n    inputs = keras.Input(shape=img_size + (3,))\n    # Don't forget to rescale input images to the [0–1] range.\n    x = Rescaling(1.0 / 255)(inputs)\n\n    # We use padding=\"same\" everywhere to avoid the influence of border\n    # padding on feature map size.\n    x = Conv2D(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n    x = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n    x = Conv2D(128, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n    x = Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n    x = Conv2D(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n    x = Conv2D(256, 3, activation=\"relu\", padding=\"same\")(x)\n\n    x = Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\")(x)\n    x = Conv2DTranspose(256, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n    x = Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\")(x)\n    x = Conv2DTranspose(128, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n    x = Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\")(x)\n    x = Conv2DTranspose(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n\n    # We end the model with a per-pixel three-way softmax to classify\n    # each output pixel into one of our three categories.\n    outputs = Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = get_model(img_size=img_size, num_classes=3) \n```", "```py\nforeground_iou = keras.metrics.IoU(\n    # Specifies the total number of classes\n    num_classes=3,\n    # Specifies the class to compute IoU for (0 = foreground)\n    target_class_ids=(0,),\n    name=\"foreground_iou\",\n    # Our targets are sparse (integer class IDs).\n    sparse_y_true=True,\n    # But our model's predictions are a dense softmax!\n    sparse_y_pred=False,\n) \n```", "```py\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[foreground_iou],\n)\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\n        \"oxford_segmentation.keras\",\n        save_best_only=True,\n    ),\n]\nhistory = model.fit(\n    train_input_imgs,\n    train_targets,\n    epochs=50,\n    callbacks=callbacks,\n    batch_size=64,\n    validation_data=(val_input_imgs, val_targets),\n) \n```", "```py\nepochs = range(1, len(history.history[\"loss\"]) + 1)\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nplt.figure()\nplt.plot(epochs, loss, \"r--\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend() \n```", "```py\nmodel = keras.models.load_model(\"oxford_segmentation.keras\")\n\ni = 4\ntest_image = val_input_imgs[i]\nplt.axis(\"off\")\nplt.imshow(array_to_img(test_image))\n\nmask = model.predict(np.expand_dims(test_image, 0))[0]\n\n# Utility to display a model's prediction\ndef display_mask(pred):\n    mask = np.argmax(pred, axis=-1)\n    mask *= 127\n    plt.axis(\"off\")\n    plt.imshow(mask)\n\ndisplay_mask(mask) \n```", "```py\nimport keras_hub\n\nmodel = keras_hub.models.ImageSegmenter.from_preset(\"sam_huge_sa1b\") \n```", "```py\n>>> model.count_params()\n641090864\n```", "```py\n# Downloads the image and returns the local file path\npath = keras.utils.get_file(\n    origin=\"https://s3.amazonaws.com/keras.io/img/book/fruits.jpg\"\n)\n# Loads the image as a Python Imaging Library (PIL) object\npil_image = keras.utils.load_img(path)\n# Turns the PIL object into a NumPy matrix\nimage_array = keras.utils.img_to_array(pil_image)\n\n# Displays the NumPy matrix\nplt.imshow(image_array.astype(\"uint8\"))\nplt.axis(\"off\")\nplt.show() \n```", "```py\nfrom keras import ops\n\nimage_size = (1024, 1024)\n\ndef resize_and_pad(x):\n    return ops.image.resize(x, image_size, pad_to_aspect_ratio=True)\n\nimage = resize_and_pad(image_array) \n```", "```py\nimport matplotlib.pyplot as plt\nfrom keras import ops\n\ndef show_image(image, ax):\n    ax.imshow(ops.convert_to_numpy(image).astype(\"uint8\"))\n\ndef show_mask(mask, ax):\n    color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n    h, w, _ = mask.shape\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n\ndef show_points(points, ax):\n    x, y = points[:, 0], points[:, 1]\n    ax.scatter(x, y, c=\"green\", marker=\"*\", s=375, ec=\"white\", lw=1.25)\n\ndef show_box(box, ax):\n    box = box.reshape(-1)\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, ec=\"red\", fc=\"none\", lw=2)) \n```", "```py\nimport numpy as np\n\n# Coordinates of our point\ninput_point = np.array([[580, 450]])\n# 1 means foreground, and 0 means background.\ninput_label = np.array([1])\n\nplt.figure(figsize=(10, 10))\n# \"gca\" means \"get current axis\" — the current figure.\nshow_image(image, plt.gca())\nshow_points(input_point, plt.gca())\nplt.show() \n```", "```py\noutputs = model.predict(\n    {\n        \"images\": ops.expand_dims(image, axis=0),\n        \"points\": ops.expand_dims(input_point, axis=0),\n        \"labels\": ops.expand_dims(input_label, axis=0),\n    }\n) \n```", "```py\n>>> outputs[\"masks\"].shape\n(1, 4, 256, 256)\n```", "```py\ndef get_mask(sam_outputs, index=0):\n    mask = sam_outputs[\"masks\"][0][index]\n    mask = np.expand_dims(mask, axis=-1)\n    mask = resize_and_pad(mask)\n    return ops.convert_to_numpy(mask) > 0.0\n\nmask = get_mask(outputs, index=0)\n\nplt.figure(figsize=(10, 10))\nshow_image(image, plt.gca())\nshow_mask(mask, plt.gca())\nshow_points(input_point, plt.gca())\nplt.show() \n```", "```py\ninput_point = np.array([[300, 550]])\ninput_label = np.array([1])\n\noutputs = model.predict(\n    {\n        \"images\": ops.expand_dims(image, axis=0),\n        \"points\": ops.expand_dims(input_point, axis=0),\n        \"labels\": ops.expand_dims(input_label, axis=0),\n    }\n)\nmask = get_mask(outputs, index=0)\n\nplt.figure(figsize=(10, 10))\nshow_image(image, plt.gca())\nshow_mask(mask, plt.gca())\nshow_points(input_point, plt.gca())\nplt.show() \n```", "```py\nfig, axes = plt.subplots(1, 3, figsize=(20, 60))\nmasks = outputs[\"masks\"][0][1:]\nfor i, mask in enumerate(masks):\n    show_image(image, axes[i])\n    show_points(input_point, axes[i])\n    mask = get_mask(outputs, index=i + 1)\n    show_mask(mask, axes[i])\n    axes[i].set_title(f\"Mask {i + 1}\", fontsize=16)\n    axes[i].axis(\"off\")\nplt.show() \n```", "```py\ninput_box = np.array(\n    [\n        # Top-left corner\n        [520, 180],\n        # Bottom-right corner\n        [770, 420],\n    ]\n)\n\nplt.figure(figsize=(10, 10))\nshow_image(image, plt.gca())\nshow_box(input_box, plt.gca())\nplt.show() \n```", "```py\noutputs = model.predict(\n    {\n        \"images\": ops.expand_dims(image, axis=0),\n        \"boxes\": ops.expand_dims(input_box, axis=(0, 1)),\n    }\n)\nmask = get_mask(outputs, 0)\nplt.figure(figsize=(10, 10))\nshow_image(image, plt.gca())\nshow_mask(mask, plt.gca())\nshow_box(input_box, plt.gca())\nplt.show() \n```"]