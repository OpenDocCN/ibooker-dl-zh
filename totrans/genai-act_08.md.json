["```py\ndef split_sentences(text):\n    sentences = re.split('[.!?]', text)   #1\n    sentences = [sentence.strip() for sentence in sentences if sentence]\n    return sentences\n```", "```py\ndef split_sentences_by_textwrap(text):\n    max_chunk_size = 2048            #1\n\n    chunks = textwrap.wrap(text,     #2\n        width=max_chunk_size,\n        break_long_words=False,\n        break_on_hyphens=False)\n\n    return chunks\n```", "```py\n>>> import nltk\n>>> nltk.download()\n```", "```py\ndef split_sentences_by_nltk(text):\n chunks = []\n  for sentence in nltk.sent_tokenize(text):\n    chunks.append(sentence)\n\n  return chunks\n```", "```py\ndef split_sentences_by_nltk(text):\n    for sentence in nltk.sent_tokenize(text):\n        yield sentence\n```", "```py\ndef split_sentences_by_spacy(text, max_tokens, overlap=0, model=\"en_core_web_sm\"):\n    nlp = spacy.load(model)    #1\n\n    doc = nlp(text)            #2\n    sentences = [sent.text for sent in doc.sents]\n\n    tokens_lengths = [count_tokens(sent) for sent in sentences]   #3\n\n    chunks = []\n    start_idx = 0\n\n    while start_idx < len(sentences):\n        current_chunk = []\n        current_token_count = 0\n        for idx in range(start_idx, len(sentences)):\n            if current_token_count + tokens_lengths[idx] > max_tokens:\n                break\n            current_chunk.append(sentences[idx])\n            current_token_count += tokens_lengths[idx]\n\n        chunks.append(\" \".join(current_chunk))\n\n        if overlap >= len(current_chunk):   #4\n            start_idx += 1\n        else:\n            start_idx += len(current_chunk) - overlap\n\n    return chunks\n```", "```py\nimport nltk\nimport spacy\n...\n\nGPT_MODEL = \"gpt-35-turbo\"\n\ndef generate_summaries(chunks):\n    summaries = []                              #1\n    # loop through each chunk\n    for chunk in tqdm(chunks):\n        prompt = f\"Summarize the following text in one\n        ↪sentence:\\n{chunk}\\nSummary:\"\n        response = openai.Completion.create(     #2\n            engine=GPT_MODEL,\n            prompt=prompt,\n            max_tokens=800,\n            temperature=0.7        )\n        summary = response.choices[0].text\n        summaries.append(summary)\n        sleep(1)                                 #3\n\n    # return the list of summaries\n    return summaries\n\ndef process_chunks(sentences):\n    sentence_embeddings = []\n    total_token_count = 0\n\n    for i, sentence in enumerate(tqdm(sentences)):\n        total_token_count += count_tokens( \n              ↪sentence, \"cl100k_base\")  #4\n        embedding = get_embedding(sentence)\n        sentence_embeddings.append([sentence, embedding])\n\n    print(\"\\tNumber of sentence embeddings:\", len(sentence_embeddings))\n    print(\"\\tTotal number of tokens:\", total_token_count)\n\n    return sentence_embeddings\n\nTEXT_FILE = f\"data/women_fifa_worldcup_2023.txt\"      #5\n\nwith open(TEXT_FILE, \"r\") as f:\n    text = f.read()\n\nprint(\"1\\. Simple sentence chunking ...\")\nsentences = split_sentences(text)\n\nprocess_chunks(sentences)\n\nprint(\"=\"*20)\n# ===================================\n\n#Reset variables\nsummaries = []\nsentences = []\nsentence_embeddings = []\ntotal_token_count = 0\nchunks = []\n\nprint(\"2\\. Sentence chunking using textwrap ...\")\nchunks = split_sentences_by_textwrap(text)   #6\nprocess_chunks(chunks)\n\n# ===================================\n\n#Reset variables\n...\n\nprint(\"3\\. Sentence chunking using NLTK ...\")\nchunks = split_sentences_by_nltk(text)        #7\nprocess_chunks(chunks)\n\n# ===================================\n\n#Reset variables\n...\n\nprint(\"4\\. Sentence chunking using spaCy ...\")\nchunks = split_sentences_by_spacy(text, \n               ↪max_tokens=2000, overlap=0)   #8\nprocess_chunks(chunks)\n\n# ===================================\nsummaries = generate_summaries(chunks)          #9\nprint(\"Summaries generated by OpenAI API:\")\nprint(summaries)\n```", "```py\nSummaries generated by OpenAI API:\n[\" The FIFA Women's World Cup is an international association football competition contested by the senior women's national teams of members of FIFA, and has been held every four years since 1991; the most successful team is the United States, with four titles,...\"]\n```", "```py\nimport PyPDF2\ndef extract_text_from_pdf(pdf_path):\n    with open(pdf_path, 'rb') as file:\n        reader = PyPDF2.PdfReader(file)\n        print(\"Number of PDF pages:\", len(reader.pages))\n        text = \"\"\n        for  page in reader.pages:\n            page_text = page.extract_text()\n            text += page_text\n            #print(page_text)\n    return text\n```", "```py\nfrom PIL import Image\nimport tabula\nfrom pdfminer.high_level import extract_pages\nfrom PyPDF2 import PdfReader\n...\n\n# Define the PDF file path\npdf_file = f\"data/test.pdf\"\noutput_folder = f\"data/temp/\"\n\ntext = ''                  #1\n\ndef process_element(element, iw):\n    global text\n    if isinstance(element, LTTextBox):\n        text += element.get_text()\n    elif isinstance(element, LTImage):\n        bmp_file = iw.export_image(element)            #2\n        bmp_file = os.path.join(iw.outdir, bmp_file)\n        img = Image.open(bmp_file)       #3\n        png_file = bmp_file.rsplit('.', 1)[0] + '.png'  #4\n        img.save(png_file)\n    if isinstance(element, LTFigure):\n        for child in element:\n            process_element(child, iw)\n\niw = ImageWriter(output_folder)             #5\n\npage = next(extract_pages(pdf_file))    #6\n\nfor element in page:\n    process_element(element, iw)\n\nwith open(output_folder + 'text.txt', 'w', encoding='utf-8') as f:\n    f.write(text)\n\ntables = tabula.read_pdf(pdf_file, pages='all')  #7\n\nfor i, table in enumerate(tables):              #8\n    table.to_csv(f'{output_folder}table_{i}.csv', index=False)\n```", "```py\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.ai.formrecognizer import DocumentAnalysisClient\n\nendpoint = \"YOUR_FORM_RECOGNIZER_ENDPOINT\"\nkey = \"YOUR_FORM_RECOGNIZER_KEY\"\n\n# sample document\npdf_file = f\"YOUR_PDF_FILE\"\n\ndocument_analysis_client = DocumentAnalysisClient(\n    endpoint=endpoint, credential=AzureKeyCredential(key)\n)\n\npoller = poller = document_analysis_client.begin_analyze_document_from_url(\n    \"prebuilt-layout\", pdf_file)    #1\nresult = poller.result() #2\n\nfor page in result.pages:              #3\n    for line_idx, line in enumerate(page.lines):\n        print(\n         \"...Line # {} has text content '{}'\".format(\n        line_idx,\n        line.content.encode(\"utf-8\")   #4\n        )\n    )\n\n    for selection_mark in page.selection_marks:\n        print(\"...Selection mark is '{}'\".format(\n         selection_mark.state,       #5\n         )\n    )\n\nfor table_idx, table in enumerate(result.tables):\n    print(\n        \"Table # {} has {} rows and {} columns\".format(\n        table_idx, table.row_count, table.column_count\n        )\n    )\n\n    for cell in the table.cells:                  #6\n        print(\n            \"...Cell[{}][{}] has content '{}'\".format(\n            cell.row_index,\n            cell.column_index,\n            cell.content.encode(\"utf-8\"),\n            )\n        )\n```"]