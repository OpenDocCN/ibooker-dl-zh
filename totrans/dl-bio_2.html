<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. Learning the Language of Proteins"><div class="chapter" id="learning-the-language-of-proteins">
 <h1><span class="label">Chapter 2. </span>Learning the Language of Proteins</h1>
 <p>
  <a contenteditable="false" data-primary="proteins, learning the language of" data-type="indexterm" id="ch02_proteins.html0"/>Life as we know it operates on <em>proteins</em>. The human genome holds about 20,000 <em>genes</em>, each made of DNA, that serve as blueprints for building different proteins. Some proteins have simple, well-understood functions—like collagen, which provides structural support and elasticity to tissues, or hemoglobin, which transports oxygen and carbon dioxide between the lungs and the rest of the body. Others have slightly more abstract roles: they act as messengers, modulators, or signal carriers, transmitting information within and between cells. For example, insulin is a protein hormone that signals cells to absorb sugar from the bloodstream.
 </p>
 <p>We’ll dive into how DNA and proteins work in more detail soon. But for now, imagine a protein as a blobby molecular machine bumping around in the crowded cell environment, occasionally making productive collisions. Its shape and movement may seem chaotic, but both have been fine-tuned by millions of years of evolution to carry out very specific molecular functions.</p>
 <p>One key detail for this chapter: a protein can be represented as a sequence of its constituent building blocks, called <em>amino acids</em>. Just as English uses 26 letters to form words, proteins use an alphabet of 20 amino acids to form long chains with specific shapes and jobs. With that in mind, the goal of this chapter is simple: we’ll train a model to predict a protein’s function given its amino acid sequence. For example:
 </p>
 <ul>
  <li>
   <p>Given the sequence of the COL1A1 collagen protein (<code>MFSFVDLR...</code>), we might predict its function is likely <code>structural</code>
    with probability 0.7, <code>enzymatic</code>
    with probability 0.01, and so on.
   </p>
  </li>
  <li>
   <p>Given the sequence of the INS insulin protein (<code>MALWMRLL...</code>), we might predict its function is likely <em>metabolic</em> with probability 0.6, <em>signaling</em> with probability 0.3, and so on.
   </p>
  </li>
 </ul>
 <div data-type="tip"><h6>Tip</h6>
  <p>To get hands-on with the material right away, open the companion Colab notebook and try running the code as you read the chapter. Exploring the examples interactively is one of the best ways to build intuition and make the ideas stick.</p>
 </div>
 <section data-type="sect1" data-pdf-bookmark="Biology Primer"><div class="sect1" id="biology-primer">
  <h1>Biology Primer</h1>
  <p>
   <a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="biology" data-type="indexterm" id="ch02_proteins.html1"/>We already highlighted that proteins are essential units of function within the cell, fulfilling a vast range of biological roles. A protein’s function is very closely tied to its 3D structure, which in turn is determined by its primary amino acid sequence.
  </p>
  <p>
   To recap the flow of information: a gene encodes the primary amino acid sequence of a protein. That sequence determines the protein’s structure, and the structure governs its function.
  </p>
  <section data-type="sect2" data-pdf-bookmark="Protein Structure "><div class="sect2" id="protein-structure">
   <h2>Protein Structure </h2>
   <p>
    <a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="biology" data-tertiary="protein structure" data-type="indexterm" id="ch02_proteins.html2"/>Protein structure is typically described in four hierarchical levels:
   </p>
   <dl>
   <dt>Primary structure</dt>
    <dd>
     <p>The linear sequence of amino acids
     </p>
    </dd>
    <dt>Secondary structure</dt>
    <dd>
     <p>Local folding into structural elements such as alpha helices and beta sheets
     </p>
    </dd>
    <dt>Tertiary structure</dt>
    <dd>
     <p>The overall 3D shape formed by the complete amino acid chain
     </p>
    </dd>
    <dt>Quaternary structure</dt>
    <dd>
     <p>The assembly of multiple protein subunits into a functional complex (not all proteins have this)
     </p>
    </dd>
   </dl>
   <p>
    As an example, <a data-type="xref" href="#id1">Figure 2-1</a> shows the structural organization levels of hemoglobin.
   </p>
      <p>The human genetic code specifies 20 main amino acids. Each has a unique chemical structure, but they can be grouped by shared biochemical properties—such as hydrophobicity (how they interact with water), charge (positive, negative, or neutral), and polarity (how evenly electrical charge is distributed over the molecule).</p> 
    
    <p>Although biochemistry students are often expected to memorize all 20 amino acids, complete with names, structures, and single-letter codes (don’t ask us how we know), it’s more practical here to focus on their functional roles (summarized in <a data-type="xref" href="#amino-acids">Figure 2-2</a>).
   </p>
   <figure><div id="id1" class="figure">
    <img alt="" src="assets/dlfb_0201.png" width="445" height="800"/>
    <h6><span class="label">Figure 2-1. </span>The four levels of protein structure, as illustrated by the hemoglobin protein. Source: <a href="https://oreil.ly/BD2Qa">Wikipedia</a>.
    </h6>
   </div></figure>
   <p>For instance, <code>D</code> (aspartic acid) and <code>E</code> (glutamic acid) are both negatively charged and often interchangeable without drastically altering a protein’s function. But other amino acids play much more specific roles, and even a single substitution can dramatically alter how a protein folds or functions—sometimes with serious effects. In fact, many genetic diseases are caused by such point mutations. One famous example is sickle cell anemia, which is caused by a single-letter change in the gene for hemoglobin that replaces a hydrophilic amino acid (<code>E</code>) with a hydrophobic one (<code>V</code>), which ultimately leads to misshapen red blood cells.</p>
   <figure><div id="amino-acids" class="figure">
    <img alt="" src="assets/dlfb_0202.png" width="600" height="566"/>
    <h6><span class="label">Figure 2-2. </span>Chart showing the chemical structures of the 20 standard amino acids found in living organisms, grouped by biochemical similarity, color-coded by side-chain properties (e.g., acidic, basic, polar, nonpolar), and annotated with their names, one- and three-letter codes, and example DNA codons (the triplet DNA bases that code for that amino acid). Adapted from an infographic by <a href="https://oreil.ly/o7Lyq">Compound Interest</a>.
    </h6>
   </div></figure>
   <p>With that introduction to protein structure, let’s now look at function—what proteins actually do in the cell.<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html2" data-type="indexterm" id="id538"/></p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Protein Function"><div class="sect2" id="protein-function">
   <h2>Protein Function</h2>
   <p>
    <a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="biology" data-tertiary="protein function" data-type="indexterm" id="ch02_proteins.html3"/>Proteins carry out nearly every task required for life: they catalyze chemical reactions, transmit signals, transport molecules, provide structural support, and regulate gene expression. Because of this diversity, systematically cataloging protein functions is a massive undertaking—and one of the most widely used frameworks for doing so is the <em>Gene Ontology</em> (GO) project.</p>
    <p><a contenteditable="false" data-primary="Gene Ontology (GO) system" data-type="indexterm" id="id539"/><a contenteditable="false" data-primary="GO (Gene Ontology) system" data-type="indexterm" id="id540"/>The GO system organizes protein function into three broad categories, each capturing a different aspect of how proteins behave in the cell:</p>
   <dl>
   <dt>Biological process</dt>
    <dd>
     <p>This contributes to—like cell division, response to stress, carbohydrate metabolism, or immune signaling.
     </p>
    </dd>
    <dt>Molecular function</dt>
    <dd>
     <p>This describes the specific biochemical activity of the protein itself—such as binding to DNA or ATP (a molecule that stores and transfers energy in cells), acting as a kinase (an enzyme that attaches a small chemical tag called a phosphate group to other molecules to change their activity), or transporting ions across membranes.
     </p>
    </dd>
    <dt>Cellular component</dt>
    <dd>
     <p>This indicates where in the cell the protein usually resides—such as the nucleus, mitochondria, or extracellular space. Although it’s technically a location label and not a function <em>per se</em>, it often provides important clues about the protein’s role (e.g., proteins in the mitochondria are probably involved in energy production). We’ll return to this theme in <a data-type="xref" href="ch06.html#learning-spatial-organization-patterns-within-cells">Chapter 6</a>.
     </p>
    </dd>
   </dl>
   <p>Each protein can have multiple GO annotations across these categories. For example, a single protein might bind ATP (molecular function), drive muscle contraction (biological process), and localize to muscle fibers (cellular component). Some annotations are derived from direct experimental assays, while others are inferred computationally through similarity to known proteins. In this chapter, we’ll work with a curated subset of high-confidence, experimentally validated GO annotations.
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Predicting Protein Function"><div class="sect2" id="predicting-protein-function">
   <h2>Predicting Protein Function</h2>
   <p>
    Why predict a protein’s function from its sequence? This is actually a fundamental challenge in modern biology. Here are a few of the most common and impactful applications:
   </p>
   <dl>
    <dt>Biotechnology and protein engineering</dt>
    <dd>
     <p>If we can reliably predict function from sequence, we can begin to design new proteins with desired properties. This could be useful for designing enzymes for industrial chemistry, therapeutic proteins for medicine, or synthetic biology components.
     </p>
    </dd>
    <dt>Understanding disease mechanisms</dt>
    <dd>
     <p>Many diseases are caused by specific sequence changes (variants, or mutations) that disrupt protein function. A good predictive model can help identify how specific mutations alter function, offering insights into disease mechanisms and potential therapeutic targets.
     </p>
    </dd>
    <dt>Genome annotation</dt>
    <dd>
     <p>As we continue sequencing the genomes of new species, we’re uncovering vast numbers of proteins whose functions remain unknown. For newly identified proteins—especially those that are distantly evolutionarily related to any known ones—computational prediction is essential for assigning functional hypotheses.
     </p>
    </dd>
    <dt>Metagenomics and microbiome analysis</dt>
    <dd>When sequencing entire microbial communities, such as gut bacteria or ocean microbiota, many protein-coding genes have no close matches in existing databases. Predicting function from sequence helps uncover the roles of these unknown proteins, advancing our understanding of microbial ecosystems and their effects on hosts or the environment.</dd>
   </dl>
   <div data-type="note" epub:type="note"><h6>Note</h6>
    <p>Although the task may sound somewhat straightforward—input a sequence, output a function—accurate protein function prediction is an extremely challenging problem. To succeed, a model must implicitly understand a range of highly complex biological principles: how amino acid sequence determines 3D structure (a Nobel Prize–winning machine learning problem in its own right), how structure enables function, and how these functions operate in the dynamic, crowded environment of the cell.</p>
   </div>
   <p>In this chapter, we won’t aim for state-of-the-art performance. Instead, our goal is to build a simple working model and develop intuition for how protein sequences can be mapped to functional annotations. Along the way, we’ll introduce several useful machine learning techniques—including using pretrained models to extract embeddings, visualizing those embeddings, and training lightweight classifiers on top of them—that will become recurring tools in later chapters<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html3" data-type="indexterm" id="id541"/>.<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html1" data-type="indexterm" id="id542"/></p>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Machine Learning Primer"><div class="sect1" id="machine-learning-primer">
  <h1>Machine Learning Primer</h1>
  <p>
   <a contenteditable="false" data-primary="machine learning" data-type="indexterm" id="ch02_proteins.html4"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning the language of proteins" data-type="indexterm" id="ch02_proteins.html5"/><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="machine learning techniques" data-type="indexterm" id="ch02_proteins.html6"/>We’ve briefly reviewed the biological background of proteins and how their function is encoded. Now, we’ll turn to the machine learning techniques that allow us to learn from protein sequences in practice.
  </p>
  <section data-type="sect2" data-pdf-bookmark="Large Language Models"><div class="sect2" id="large-language-models">
   <h2>Large Language Models</h2>
   <p>
    <a contenteditable="false" data-primary="large language models (LLMs)" data-type="indexterm" id="id543"/><a contenteditable="false" data-primary="LLMs (large language models)" data-type="indexterm" id="id544"/><a contenteditable="false" data-primary="machine learning" data-secondary="large language models" data-type="indexterm" id="id545"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning the language of proteins" data-tertiary="large language models" data-type="indexterm" id="id546"/><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="machine learning techniques" data-tertiary="large language models" data-type="indexterm" id="id547"/>It’s hard these days to go anywhere without bumping into <em>large language models</em> (LLMs). Many recent breakthrough models in AI—such as ChatGPT, Gemini, Claude, and Llama—fall under this category. While these models involve immense engineering, the fundamental idea behind them is surprisingly simple: they’re trained to predict the next token (e.g., a word or character) given the preceding context. There are slight variations—such as masked language models, which hide random tokens during training to encourage contextual reasoning—but the core principle remains the same.
   </p>
   <p>One of the most surprising discoveries in modern AI has been that if you train a large enough model (in terms of the number of parameters) on enough data (in terms of total tokens), remarkable capabilities emerge without explicit supervision. These models can suddenly summarize text, translate between languages, and even generate creative writing like poems and stories—despite never being trained directly to do so.
   </p>
   <p>This holds promise for biology. In many ways, biology is language-like: DNA and proteins are sequences built from discrete alphabets, with complex patterns and context-dependent “grammar.” By training language models on massive corpora of biological sequences—using the same next-token prediction objective—we should be able to learn rich representations of biological information.</p>
<p>These learned representations can then be used for a wide range of downstream tasks, such as predicting a protein’s function, inferring the effects of mutations, or identifying structural properties—all without needing to retrain a new model from scratch.</p> 

<p>Later in this chapter, we will explore one of the most successful protein language models to date: ESM2.
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Embeddings"><div class="sect2" id="embeddings">
   <h2>Embeddings</h2>
   <p><a contenteditable="false" data-primary="embeddings" data-type="indexterm" id="id548"/><a contenteditable="false" data-primary="machine learning" data-secondary="embeddings" data-type="indexterm" id="id549"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning the language of proteins" data-tertiary="embeddings" data-type="indexterm" id="id550"/><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="machine learning techniques" data-tertiary="embeddings" data-type="indexterm" id="id551"/>One of the most powerful and versatile outputs of language models is their ability to generate <em>embeddings</em>. An embedding is a numerical vector—a list of floating-point numbers—that encodes the meaning or structure of an entity like a word, sentence, or protein sequence. For example, a protein might be represented by an embedding such as <code>[0.1, -0.3, 1.3, 0.9, 0.2]</code>, which could capture aspects of its biochemical or structural properties in a compact numerical form.
   </p>
   <p>Embeddings from language models are not just arbitrary numbers—they are structured so that similar inputs result in similar embeddings. Related words like <code>lion</code>, <code>tiger</code>, and <code>panther</code> cluster together in a linguistic “semantic space.” Likewise, protein sequences with similar structure or function—such as collagen I and collagen II—will tend to have embeddings that are close together in what we might call a “protein space.”</p>
<p><a contenteditable="false" data-primary="latent space" data-secondary="defined" data-type="indexterm" id="id552"/>This idea generalizes to the concept of a <em>latent space</em>—a continuous, abstract space where similar entities are positioned close together based on learned patterns. In such spaces, we can perform powerful operations, such as interpolation, clustering, and generative design. For proteins, latent spaces can capture functional relationships that aren’t apparent from sequence alone—for example, two proteins with very different sequences and evolutionary histories may have converged on similar functions and therefore appear close together in the latent space. These representations can also help predict new functions for uncharacterized proteins by comparing them to annotated neighbors in the space.
   </p>
   <div data-type="note" epub:type="note"><h6>Note</h6><p><a contenteditable="false" data-primary="cosine similarity" data-type="indexterm" id="id553"/>To identify proteins with similar structure or function, you can compare their embeddings using <em>cosine similarity</em>—a measure of how aligned two vectors are, regardless of their magnitude. This works even when sequences differ significantly at the amino acid level. By computing cosine similarities between a query protein and a set of known proteins, you can rank the closest matches in embedding space. These top hits often share functional roles, structural features, or evolutionary history.</p></div>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Pretraining and Fine-tuning"><div class="sect2" id="pre-training-and-fine-tuning">
   <h2>Pretraining and Fine-tuning</h2>
   <p><a contenteditable="false" data-primary="fine-tuning" data-type="indexterm" id="id554"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning the language of proteins" data-tertiary="pretraining and fine-tuning" data-type="indexterm" id="id555"/><a contenteditable="false" data-primary="pretraining" data-type="indexterm" id="id556"/><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="machine learning techniques" data-tertiary="pretraining and fine-tuning" data-type="indexterm" id="id557"/>Many machine learning tasks share underlying structure. Whether your goal is detecting hate speech, answering law school entrance questions, or writing poems about capybaras, your model first needs a strong foundation in how language works. Rather than training from scratch for every task, we typically start from a general-purpose model that’s been pretrained on a huge, diverse dataset.
   </p>
   <p><em>Pretraining</em> gives a model broad knowledge and general capabilities. For a specific application, we often follow it with a smaller, focused training step called <span class="keep-together"><em>fine-tuning</em></span>, where the model is trained further on a domain-specific dataset. This two-stage process is now standard in many areas of machine learning, especially as pretrained language models have become increasingly powerful.
   </p>
   <p class="pagebreak-before">In this first technical chapter of the book, we’ll take a slightly different approach. Rather than fine-tuning the entire pretrained model, we’ll treat it as a frozen feature extractor: we’ll use its embeddings as input to a smaller classifier that we’ll train from scratch. This strategy is efficient, requires little data, and still leverages the rich representations learned by the pretrained model. We’ll explore full transfer learning with fine-tuning in later chapters.<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html6" data-type="indexterm" id="id558"/><a contenteditable="false" data-primary="" data-startref="ch02_proteins.html5" data-type="indexterm" id="id559"/><a contenteditable="false" data-primary="" data-startref="ch02_proteins.html4" data-type="indexterm" id="id560"/>
   </p>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Representations of Proteins and Protein LMs"><div class="sect1" id="representations-of-proteins-and-protein-lms">
  <h1>Representations of Proteins and Protein LMs</h1>
  <p><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="representations of proteins and protein LMs" data-type="indexterm" id="ch02_proteins.html7"/>Previously, we discussed what proteins are and how their structure is organized hierarchically—from a linear chain of amino acids, to local folding, to the final 3D form that enables their function. To make this less abstract, let’s load up and visualize an example protein structure using the <code>py3Dmol</code> library:</p>
  
<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">py3Dmol</code>
<code class="kn">import</code> <code class="nn">requests</code>


<code class="k">def</code> <code class="nf">fetch_protein_structure</code><code class="p">(</code><code class="n">pdb_id</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
  <code class="sd">"""Grab a PDB protein structure from the RCSB Protein Data Bank."""</code>
  <code class="n">url</code> <code class="o">=</code> <code class="sa">f</code><code class="s2">"https://files.rcsb.org/download/</code><code class="si">{</code><code class="n">pdb_id</code><code class="si">}</code><code class="s2">.pdb"</code>
  <code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">url</code><code class="p">)</code>
  <code class="k">return</code> <code class="n">response</code><code class="o">.</code><code class="n">text</code>


<code class="c1"># The Protein Data Bank (PDB) is the main database of protein structures.</code>
<code class="c1"># Each structure has a unique 4-character PDB ID. Below are a few examples.</code>
<code class="n">protein_to_pdb</code> <code class="o">=</code> <code class="p">{</code>
  <code class="s2">"insulin"</code><code class="p">:</code> <code class="s2">"3I40"</code><code class="p">,</code>  <code class="c1"># Human insulin – regulates glucose uptake.</code>
  <code class="s2">"collagen"</code><code class="p">:</code> <code class="s2">"1BKV"</code><code class="p">,</code>  <code class="c1"># Human collagen – provides structural support.</code>
  <code class="s2">"proteasome"</code><code class="p">:</code> <code class="s2">"1YAR"</code><code class="p">,</code>  <code class="c1"># Archaebacterial proteasome – degrades proteins.</code>
<code class="p">}</code>

<code class="n">protein</code> <code class="o">=</code> <code class="s2">"collagen"</code>  <code class="c1"># @param ["insulin", "collagen", "proteasome"]</code>
<code class="n">pdb_structure</code> <code class="o">=</code> <code class="n">fetch_protein_structure</code><code class="p">(</code><code class="n">pdb_id</code><code class="o">=</code><code class="n">protein_to_pdb</code><code class="p">[</code><code class="n">protein</code><code class="p">])</code>

<code class="n">pdbview</code> <code class="o">=</code> <code class="n">py3Dmol</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="n">width</code><code class="o">=</code><code class="mi">400</code><code class="p">,</code> <code class="n">height</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code>
<code class="n">pdbview</code><code class="o">.</code><code class="n">addModel</code><code class="p">(</code><code class="n">pdb_structure</code><code class="p">,</code> <code class="s2">"pdb"</code><code class="p">)</code>
<code class="n">pdbview</code><code class="o">.</code><code class="n">setStyle</code><code class="p">({</code><code class="s2">"cartoon"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"color"</code><code class="p">:</code> <code class="s2">"spectrum"</code><code class="p">}})</code>
<code class="n">pdbview</code><code class="o">.</code><code class="n">zoomTo</code><code class="p">()</code>
<code class="n">pdbview</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
</pre>
     
    
   
  
  <p>Running this code in our companion Colab notebook will display an interactive 3D rendering of your chosen protein. A screenshot of the visualization of collagen is shown in <a data-type="xref" href="#collagen-structure">Figure 2-3</a>.
  </p>
  <figure><div id="collagen-structure" class="figure">
   <img alt="" src="assets/dlfb_0203.png" width="600" height="387"/>
   <h6><span class="label">Figure 2-3. </span>A 3D structure of the collagen protein rendered with <code>py3Dmol</code>. Collagen is a structural protein that forms triple-helical fibers, visible here as intertwined ribbonlike strands.
   </h6>
  </div></figure>
  <p>Try viewing the other examples, such as <code>insulin</code> and <code>proteasome</code>, to appreciate the incredible structural diversity of proteins. Their shapes often reflect their specialized roles. For example, the long, springy structure of collagen relates to its function as a flexible, supportive scaffold found throughout many tissues in the body.
  </p>
  <section data-type="sect2" data-pdf-bookmark="Numerical Representation of a Protein"><div class="sect2" id="representing-a-protein-numerically">
   <h2>Numerical Representation of a Protein</h2>
   <p>
    <a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="representations of proteins and protein LMs" data-tertiary="numerical representation" data-type="indexterm" id="id561"/>While 3D visualizations are useful for exploration, machine learning models require numerical input. To analyze or model proteins with machine learning techniques, we typically start from their 1D amino acid sequence.
   </p>
   <p>Protein sequences for most known organisms can be retrieved from public databases such as <a href="https://oreil.ly/9OqAK">Uniprot</a>. For example, here’s the amino acid sequence of human insulin:
   </p>
  
<pre data-type="programlisting" data-code-language="python"><code class="c1"># Precursor insulin protein sequence (processed into two protein chains).</code>
<code class="n">insulin_sequence</code> <code class="o">=</code> <code class="p">(</code>
  <code class="s2">"MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGG"</code>
  <code class="s2">"GPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN"</code>
<code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Length of the insulin protein precursor: </code><code class="si">{</code><code class="nb">len</code><code class="p">(</code><code class="n">insulin_sequence</code><code class="p">)</code><code class="si">}</code><code class="s2">."</code><code class="p">)</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">Length of the insulin protein precursor: 110.
</pre>

   <p>This sequence representation is easy to store and manipulate, but it still needs to be converted to a numerical format before it can be used by machine learning models.
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="One-Hot Encoding of a Protein Sequence"><div class="sect2" id="one-hot-encoding-of-a-protein-sequence">
   <h2>One-Hot Encoding of a Protein Sequence</h2>
   <p>
    <a contenteditable="false" data-primary="one-hot encoding" data-type="indexterm" id="ch02_proteins.html8"/><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="representations of proteins and protein LMs" data-tertiary="one-hot encoding of protein sequence" data-type="indexterm" id="ch02_proteins.html9"/>The simplest way to convert a protein sequence into numerical form is with <em>one-hot encoding</em>. Here is how it works:
   </p>
   <ul class="simple">
    <li>
     <p>
      There are 20 standard amino acids.
     </p>
    </li>
    <li>
     <p>Each amino acid is represented by a binary vector of length 20, where only one position is 1 (indicating the identity of that amino acid) and all other positions are <code>0</code>.
     </p>
    </li>
    <li><p>A protein sequence is then converted into a sequence of these one-hot vectors—one for each amino acid.</p></li>
   </ul>
   <p>
    Let’s walk through a toy example: encoding the short protein <code>MALWN</code> (the first five amino acids of the insulin precursor protein).
   </p>
   <p>
    First, let’s define the mapping between an amino acid letter code to an integer index:
   </p>
  
<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.utils.display</code> <code class="kn">import</code> <code class="n">print_short_dict</code>

<code class="n">amino_acids</code> <code class="o">=</code> <code class="p">[</code>
  <code class="s2">"R"</code><code class="p">,</code> <code class="s2">"H"</code><code class="p">,</code> <code class="s2">"K"</code><code class="p">,</code> <code class="s2">"D"</code><code class="p">,</code> <code class="s2">"E"</code><code class="p">,</code> <code class="s2">"S"</code><code class="p">,</code> <code class="s2">"T"</code><code class="p">,</code> <code class="s2">"N"</code><code class="p">,</code> <code class="s2">"Q"</code><code class="p">,</code> <code class="s2">"G"</code><code class="p">,</code> <code class="s2">"P"</code><code class="p">,</code> <code class="s2">"C"</code><code class="p">,</code> <code class="s2">"A"</code><code class="p">,</code> <code class="s2">"V"</code><code class="p">,</code> <code class="s2">"I"</code><code class="p">,</code>
  <code class="s2">"L"</code><code class="p">,</code> <code class="s2">"M"</code><code class="p">,</code> <code class="s2">"F"</code><code class="p">,</code> <code class="s2">"Y"</code><code class="p">,</code> <code class="s2">"W"</code><code class="p">,</code>
<code class="p">]</code>

<code class="n">amino_acid_to_index</code> <code class="o">=</code> <code class="p">{</code>
  <code class="n">amino_acid</code><code class="p">:</code> <code class="n">index</code> <code class="k">for</code> <code class="n">index</code><code class="p">,</code> <code class="n">amino_acid</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">amino_acids</code><code class="p">)</code>
<code class="p">}</code>

<code class="n">print_short_dict</code><code class="p">(</code><code class="n">amino_acid_to_index</code><code class="p">)</code>
</pre>

<p>Output:</p>
       <pre data-type="programlisting">{'R': 0, 'H': 1, 'K': 2, 'D': 3, 'E': 4, 'S': 5, 'T': 6, 'N': 7, 'Q': 8, 'G': 9}
…(+10 more entries)
</pre>
      
   
   <p>Given a protein sequence, we can convert it to a sequence of integers:</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="c1"># Methionine, alanine, leucine, tryptophan, methionine.</code>
<code class="n">tiny_protein</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"M"</code><code class="p">,</code> <code class="s2">"A"</code><code class="p">,</code> <code class="s2">"L"</code><code class="p">,</code> <code class="s2">"W"</code><code class="p">,</code> <code class="s2">"M"</code><code class="p">]</code>

<code class="n">tiny_protein_indices</code> <code class="o">=</code> <code class="p">[</code>
  <code class="n">amino_acid_to_index</code><code class="p">[</code><code class="n">amino_acid</code><code class="p">]</code> <code class="k">for</code> <code class="n">amino_acid</code> <code class="ow">in</code> <code class="n">tiny_protein</code>
<code class="p">]</code>

<code class="n">tiny_protein_indices</code>
</pre>
      
     
    
    
     
  <p>Output:</p>    
       <pre data-type="programlisting">[16, 12, 15, 19, 16]
</pre>
      
     
    
   
   <p>And given a sequence of integers, we can convert it into a one-hot encoding (see <a data-type="xref" href="#protein-one-hot-encoding">Figure 2-4</a>).
   </p>
   <figure><div id="protein-one-hot-encoding" class="figure">
    <img alt="" src="assets/dlfb_0204.png" width="600" height="206"/>
    <h6><span class="label">Figure 2-4. </span>One-hot encoding converts a protein’s amino acid sequence into a binary matrix where each row corresponds to one amino acid and each column to a possible residue. Most values are zero, with a single “1” indicating the presence of a specific amino acid at each position.
    </h6>
   </div></figure>
   <p>In <a data-type="xref" href="#protein-one-hot-encoding">Figure 2-4</a>, we see that:</p>
   <ul>
    <li><p>The resulting matrix has the shape <code>[5, 20]</code>, where each of the five rows corresponds to one amino acid in the sequence, and each column represents one of the 20 standard amino acids.</p></li>
    <li><p>Each row contains all zeros except for a single 1 in the position corresponding to that amino acid’s identity, preserving its categorical nature without implying any numerical ordering or similarity.</p></li>
   </ul>
   
   <div data-type="note" epub:type="note"><h6>Note</h6>
    <p>Why not just skip the one-hot encoding step and use amino acid indices directly?</p> 
     <p>The issue is that numeric indices (like 3 versus 17) imply an artificial order and relative similarity, even though amino acids are categorical entities without meaningful numerical relationships.</p>
    <p>One-hot encoding avoids this by assigning each amino acid a distinct binary vector—ensuring that the model treats them as equally separate and avoids inferring nonexistent patterns from arbitrary index values.</p>
   </div>
   <p class="pagebreak-before">In code, we can use the handy <code>jax.nn.one_hot</code> utility from the JAX library to get this embedding:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">jax</code>

<code class="n">one_hot_encoded_sequence</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">one_hot</code><code class="p">(</code>
  <code class="n">x</code><code class="o">=</code><code class="n">tiny_protein_indices</code><code class="p">,</code> <code class="n">num_classes</code><code class="o">=</code><code class="nb">len</code><code class="p">(</code><code class="n">amino_acids</code><code class="p">)</code>
<code class="p">)</code>

<code class="nb">print</code><code class="p">(</code><code class="n">one_hot_encoded_sequence</code><code class="p">)</code>
</pre>
      
     
    
    
     
      <p>Output:</p>
       <pre data-type="programlisting">[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]
</pre> 
   
   <p>We can visualize the resulting one-hot encoding matrix as a heatmap as in <a data-type="xref" href="#one-hot-matrix-visualized">Figure 2-5</a> (essentially re-creating the earlier <a data-type="xref" href="#protein-one-hot-encoding">Figure 2-4</a>):
   </p>

       <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">seaborn</code> <code class="k">as</code> <code class="nn">sns</code>

<code class="n">fig</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code>
  <code class="n">one_hot_encoded_sequence</code><code class="p">,</code> <code class="n">square</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">cbar</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"inferno"</code>
<code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">xlabel</code><code class="o">=</code><code class="s2">"Amino Acid Index"</code><code class="p">,</code> <code class="n">ylabel</code><code class="o">=</code><code class="s2">"Protein Sequence"</code><code class="p">);</code>
</pre>
      
     <figure><div id="one-hot-matrix-visualized" class="figure">
      <img alt="" src="assets/dlfb_0205.png" width="600" height="183"/>
      <h6><span class="label">Figure 2-5. </span>One-hot encoded representation of a toy protein sequence (<code>MALWM</code>), visualized with a heatmap. This binary matrix encodes the identity of each residue without implying any similarity between them.</h6>
     </div></figure>
    
   
   <p>Now that we’ve constructed a basic numerical representation of a protein, we’re ready to move beyond this simplistic format and explore <em>learned embeddings</em>—dense vector representations that encode much more biological meaning about each amino acid.<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html9" data-type="indexterm" id="id562"/><a contenteditable="false" data-primary="" data-startref="ch02_proteins.html8" data-type="indexterm" id="id563"/>
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Learned Embeddings of Amino Acids"><div class="sect2" id="learned-embeddings-of-amino-acids">
   <h2>Learned Embeddings of Amino Acids</h2>
   <p>
    <a contenteditable="false" data-primary="amino acids" data-type="indexterm" id="ch02_proteins.html10"/><a contenteditable="false" data-primary="ESM2 protein language model" data-type="indexterm" id="ch02_proteins.html14"/><a contenteditable="false" data-primary="learned embeddings" data-type="indexterm" id="ch02_proteins.html12"/><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="representations of proteins and protein LMs" data-tertiary="learned embeddings of amino acids" data-type="indexterm" id="ch02_proteins.html13"/>In the rest of this chapter, we’ll use a pretrained protein language model called <a href="https://oreil.ly/iZmXA">ESM2</a>, released by Meta in 2023 (ESM stands for <em>evolutionary scale modeling</em>). These models are hosted on the <!--Do not shorten link--><a href="https://huggingface.co">Hugging Face platform</a>. If you haven’t encountered it yet, Hugging Face is a fantastic resource with<!--Do not shorten link--> <a href="https://huggingface.co/models">thousands of pretrained models</a> ready for you to use and explore.
   </p>
   <p>We’ll explore how the ESM2 model works in more detail shortly, but first, let’s examine how it represents individual amino acids. We’ll access the model using the Hugging Face transformers library. ESM2 is based on the <em>transformer</em> neural network architecture <a href="https://oreil.ly/XPvFW">introduced in 2017</a>, which has become the standard for modeling sequences like text and proteins.
   </p>
   <div data-type="note" epub:type="note"><h6>Note</h6><p>Ideally, we’d load the ESM2 model using JAX/Flax, but it’s only officially available in PyTorch at the moment. In practice, being comfortable with multiple deep learning frameworks is useful—so here we’ll use PyTorch to load the model and extract embeddings, which we’ll then process and build on top of using JAX.</p>
<p>The rest of the book will use JAX/Flax exclusively, but this brief mixing of frameworks is a good example of how flexible real-world workflows can be.</p></div>
   
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">AutoTokenizer</code><code class="p">,</code> <code class="n">EsmModel</code>

<code class="c1"># Model checkpoint name taken from this GitHub README:</code>
<code class="c1"># https://github.com/facebookresearch/esm#available-models-and-datasets-</code>
<code class="n">model_checkpoint</code> <code class="o">=</code> <code class="s2">"facebook/esm2_t33_650M_UR50D"</code>
<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_checkpoint</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">EsmModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_checkpoint</code><code class="p">)</code>
</pre>
      
     
    
   
   <p>We can check the model’s token-to-index mapping:</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">vocab_to_index</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="o">.</code><code class="n">get_vocab</code><code class="p">()</code>
<code class="n">print_short_dict</code><code class="p">(</code><code class="n">vocab_to_index</code><code class="p">)</code>
</pre>

<p>Output:</p>
       <pre data-type="programlisting">{'&lt;cls&gt;': 0, '&lt;pad&gt;': 1, '&lt;eos&gt;': 2, '&lt;unk&gt;': 3, 'L': 4, 'A': 5, 'G': 6, 'V': 7,
'S': 8, 'E': 9}
…(+23 more entries)
</pre>
      
   <p>This is similar to the manual amino acid indexing we did earlier, but it includes special tokens like <code>&lt;unk&gt;</code> for unknown residues, <code>&lt;eos&gt;</code> for end-of-sequence, and rare amino acids like <code>U</code> (selenocysteine) and <code>O</code> (pyrrolysine).</p>
   
   <p>Let’s use the ESM2 tokenizer to encode our tiny protein sequence:</p>
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">tokenized_tiny_protein</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="s2">"MALWM"</code><code class="p">)[</code><code class="s2">"input_ids"</code><code class="p">]</code>
<code class="n">tokenized_tiny_protein</code>
</pre>
   
<p>Output:</p>
       <pre data-type="programlisting">[0, 20, 5, 4, 22, 20, 2]
</pre>
      
     
    
   
   <p>If desired, we can drop the special start (<code>&lt;cls&gt;</code>) and end (<code>&lt;eos&gt;</code>) tokens:</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">tokenized_tiny_protein</code><code class="p">[</code><code class="mi">1</code><code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code>
</pre>
      
     
    
    
     
<p>Output:</p>      
       <pre data-type="programlisting">[20, 5, 4, 22, 20]
</pre>
      
     
    
   
   <p>Now we’ll extract the learned token embeddings from the model using <code>model.get_input_embeddings()</code>:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">token_embeddings</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">get_input_embeddings</code><code class="p">()</code><code class="o">.</code><code class="n">weight</code><code class="o">.</code><code class="n">detach</code><code class="p">()</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code>
<code class="n">token_embeddings</code><code class="o">.</code><code class="n">shape</code>
</pre>
      
     
    
    
     
 <p>Output:</p>     
       <pre data-type="programlisting">(33, 1280)
</pre>
      
     
    
   
   <p>Each of the 33 possible tokens is embedded into a 1,280-dimensional space. While humans can’t visualize such high-dimensional spaces directly, we can apply dimensionality reduction techniques like t-SNE or UMAP to project the embeddings down to two dimensions. This allows us to inspect how the model organizes different tokens in a more interpretable form:</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>
<code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="kn">import</code> <code class="n">TSNE</code>

<code class="n">tsne</code> <code class="o">=</code> <code class="n">TSNE</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">embeddings_tsne</code> <code class="o">=</code> <code class="n">tsne</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">token_embeddings</code><code class="p">)</code>
<code class="n">embeddings_tsne_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code>
  <code class="n">embeddings_tsne</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s2">"first_dim"</code><code class="p">,</code> <code class="s2">"second_dim"</code><code class="p">]</code>
<code class="p">)</code>
<code class="n">embeddings_tsne_df</code><code class="o">.</code><code class="n">shape</code>
</pre>
      
     
    
    
     
<p>Output:</p>      
       <pre data-type="programlisting">(33, 2)
</pre>
      
     
    
   
   <p>We can see that the t-SNE–transformed array has shape <code>(33, 2)</code>, meaning that each of the 33 tokens has been projected into a 2D space. <a data-type="xref" href="#tsne-no-chemical-properties">Figure 2-6</a> shows a scatterplot of these points, giving us a visual sense of how the model organizes token embeddings:</p>

       <pre data-type="programlisting" data-code-language="python"><code class="n">fig</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">scatterplot</code><code class="p">(</code>
  <code class="n">data</code><code class="o">=</code><code class="n">embeddings_tsne_df</code><code class="p">,</code> <code class="n">x</code><code class="o">=</code><code class="s2">"first_dim"</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s2">"second_dim"</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code>
<code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s2">"First Dimension"</code><code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s2">"Second Dimension"</code><code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="tsne-no-chemical-properties" class="figure">
      <img alt="" src="assets/dlfb_0206.png" width="600" height="440"/>
      <h6><span class="label">Figure 2-6. </span>A 2D t-SNE projection of the learned token embeddings from the ESM2 model. Even without labels, clusters begin to emerge—hinting that the model has learned to organize tokens in a meaningful way.
      </h6>
     </div></figure>
    
   
   <p>To sanity-check whether similar types of tokens cluster in the 2D embedding space, we can label each token using known amino acid properties (like those shown earlier in the chapter) and replot the t-SNE projection in <a data-type="xref" href="#tsne-with-chemical-properties">Figure 2-7</a>:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">adjustText</code> <code class="kn">import</code> <code class="n">adjust_text</code>

<code class="n">embeddings_tsne_df</code><code class="p">[</code><code class="s2">"token"</code><code class="p">]</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="n">vocab_to_index</code><code class="o">.</code><code class="n">keys</code><code class="p">())</code>

<code class="n">token_annotation</code> <code class="o">=</code> <code class="p">{</code>
  <code class="s2">"hydrophobic"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"A"</code><code class="p">,</code> <code class="s2">"F"</code><code class="p">,</code> <code class="s2">"I"</code><code class="p">,</code> <code class="s2">"L"</code><code class="p">,</code> <code class="s2">"M"</code><code class="p">,</code> <code class="s2">"V"</code><code class="p">,</code> <code class="s2">"W"</code><code class="p">,</code> <code class="s2">"Y"</code><code class="p">],</code>
  <code class="s2">"polar uncharged"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"N"</code><code class="p">,</code> <code class="s2">"Q"</code><code class="p">,</code> <code class="s2">"S"</code><code class="p">,</code> <code class="s2">"T"</code><code class="p">],</code>
  <code class="s2">"negatively charged"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"D"</code><code class="p">,</code> <code class="s2">"E"</code><code class="p">],</code>
  <code class="s2">"positively charged"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"H"</code><code class="p">,</code> <code class="s2">"K"</code><code class="p">,</code> <code class="s2">"R"</code><code class="p">],</code>
  <code class="s2">"special amino acid"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"B"</code><code class="p">,</code> <code class="s2">"C"</code><code class="p">,</code> <code class="s2">"G"</code><code class="p">,</code> <code class="s2">"O"</code><code class="p">,</code> <code class="s2">"P"</code><code class="p">,</code> <code class="s2">"U"</code><code class="p">,</code> <code class="s2">"X"</code><code class="p">,</code> <code class="s2">"Z"</code><code class="p">],</code>
  <code class="s2">"special token"</code><code class="p">:</code> <code class="p">[</code>
    <code class="s2">"-"</code><code class="p">,</code>
    <code class="s2">"."</code><code class="p">,</code>
    <code class="s2">"&lt;cls&gt;"</code><code class="p">,</code>
    <code class="s2">"&lt;eos&gt;"</code><code class="p">,</code>
    <code class="s2">"&lt;mask&gt;"</code><code class="p">,</code>
    <code class="s2">"&lt;null_1&gt;"</code><code class="p">,</code>
    <code class="s2">"&lt;pad&gt;"</code><code class="p">,</code>
    <code class="s2">"&lt;unk&gt;"</code><code class="p">,</code>
  <code class="p">],</code>
<code class="p">}</code>

<code class="n">embeddings_tsne_df</code><code class="p">[</code><code class="s2">"label"</code><code class="p">]</code> <code class="o">=</code> <code class="n">embeddings_tsne_df</code><code class="p">[</code><code class="s2">"token"</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code>
  <code class="p">{</code><code class="n">t</code><code class="p">:</code> <code class="n">label</code> <code class="k">for</code> <code class="n">label</code><code class="p">,</code> <code class="n">tokens</code> <code class="ow">in</code> <code class="n">token_annotation</code><code class="o">.</code><code class="n">items</code><code class="p">()</code> <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">tokens</code><code class="p">}</code>
<code class="p">)</code>

<code class="n">fig</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">scatterplot</code><code class="p">(</code>
  <code class="n">data</code><code class="o">=</code><code class="n">embeddings_tsne_df</code><code class="p">,</code>
  <code class="n">x</code><code class="o">=</code><code class="s2">"first_dim"</code><code class="p">,</code>
  <code class="n">y</code><code class="o">=</code><code class="s2">"second_dim"</code><code class="p">,</code>
  <code class="n">hue</code><code class="o">=</code><code class="s2">"label"</code><code class="p">,</code>
  <code class="n">style</code><code class="o">=</code><code class="s2">"label"</code><code class="p">,</code>
  <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code>
<code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s2">"First Dimension"</code><code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s2">"Second Dimension"</code><code class="p">)</code>
<code class="n">texts</code> <code class="o">=</code> <code class="p">[</code>
  <code class="n">fig</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="n">point</code><code class="p">[</code><code class="s2">"first_dim"</code><code class="p">],</code> <code class="n">point</code><code class="p">[</code><code class="s2">"second_dim"</code><code class="p">],</code> <code class="n">point</code><code class="p">[</code><code class="s2">"token"</code><code class="p">])</code>
  <code class="k">for</code> <code class="n">_</code><code class="p">,</code> <code class="n">point</code> <code class="ow">in</code> <code class="n">embeddings_tsne_df</code><code class="o">.</code><code class="n">iterrows</code><code class="p">()</code>
<code class="p">]</code>
<code class="n">adjust_text</code><code class="p">(</code>
  <code class="n">texts</code><code class="p">,</code> <code class="n">expand</code><code class="o">=</code><code class="p">(</code><code class="mf">1.5</code><code class="p">,</code> <code class="mf">1.5</code><code class="p">),</code> <code class="n">arrowprops</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">arrowstyle</code><code class="o">=</code><code class="s2">"-&gt;"</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s2">"grey"</code><code class="p">)</code>
<code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="tsne-with-chemical-properties" class="figure">
      <img alt="" src="assets/dlfb_0207.png" width="600" height="440"/>
      <h6><span class="label">Figure 2-7. </span>Coloring the t-SNE projection by amino acid properties reveals clear clusters of amino acids with similar biochemical roles that tend to group together in embedding space, reflecting the model’s ability to capture meaningful biological structure. Technical non–amino acid tokens also group together in this latent space.</h6>
     </div></figure>
    
   
   <p>Tokens with similar biochemical properties tend to cluster together. For instance, hydrophobic amino acids like <code>F</code>, <code>Y</code>, and <code>W</code> group in the upper right, while special-purpose tokens such as <code>&lt;cls&gt;</code> and <code>&lt;eos&gt;</code> appear together on the left side of the plot. This structure suggests that the model has learned meaningful distinctions among amino acids based on the roles they play within protein sequences.</p>
<p>Now that we’ve explored what these token embeddings look like, let’s dive into how the ESM2 model actually works—and how it learns such representations in the first place.<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html13" data-type="indexterm" id="id564"/><a contenteditable="false" data-primary="" data-startref="ch02_proteins.html12" data-type="indexterm" id="id565"/><a contenteditable="false" data-primary="" data-startref="ch02_proteins.html10" data-type="indexterm" id="id566"/>
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="The ESM2 Protein Language Model"><div class="sect2" id="exploring-the-esm2-protein-language-model">
   <h2>The ESM2 Protein Language Model</h2>
   <p><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="representations of proteins and protein LMs" data-tertiary="ESM2 protein language model" data-type="indexterm" id="ch02_proteins.html15"/>Now that you’re more familiar with token embeddings, let’s discuss how the ESM2 model actually works. <a contenteditable="false" data-primary="masked language model (MLM)" data-type="indexterm" id="id567"/><a contenteditable="false" data-primary="MLM (masked language model)" data-type="indexterm" id="id568"/>ESM2 is a <em>masked language model</em> (MLM), which means it was trained by repeatedly masking a random subset of amino acids in each protein sequence and asking the model to predict them. In the case of ESM2, a randomly selected 15% of the amino acids in each sequence were masked during training. <a data-type="xref" href="#language-model-training">Figure 2-8</a> illustrates this visually, comparing it to masked language modeling in natural language tasks:</p>
   <figure><div id="language-model-training" class="figure">
    <img alt="" src="assets/dlfb_0208.png" width="600" height="211"/>
    <h6><span class="label">Figure 2-8. </span>A comparison between masked language modeling in natural and protein language models. In natural language, models are trained to predict missing words (or sometimes subwords) from surrounding context. Protein language models use the same principle: randomly masking amino acids in a sequence and training the model to predict them from the surrounding context.
    </h6>
   </div></figure>
   <p>Let’s try masking one amino acid in the insulin protein sequence and see whether the model can predict it:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">insulin_sequence</code> <code class="o">=</code> <code class="p">(</code>
  <code class="s2">"MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGG"</code>
  <code class="s2">"GPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN"</code>
<code class="p">)</code>

<code class="n">masked_insulin_sequence</code> <code class="o">=</code> <code class="p">(</code>
  <code class="c1"># Let's mask the `L` amino acid in the 29th position (0-based indexing):</code>
  <code class="c1">#       ...LALLALWGPDPAAAFVNQH  L   CGSHLVEALYLVCGERGFF...</code>
  <code class="s2">"MALWMRLLPLLALLALWGPDPAAAFVNQH&lt;mask&gt;CGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGG"</code>
  <code class="s2">"GPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN"</code>
<code class="p">)</code>

<code class="c1"># Tokenize the masked insulin sequence.</code>
<code class="n">masked_inputs</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="n">masked_insulin_sequence</code><code class="p">)[</code><code class="s2">"input_ids"</code><code class="p">]</code>

<code class="c1"># Check that we indeed have a &lt;mask&gt; token in the place that we expect it. Note</code>
<code class="c1"># that the tokenizer adds a &lt;cls&gt; token to the start of the sequence, so we in</code>
<code class="c1"># fact expect the &lt;mask&gt; token at position 30 (not 29).</code>
<code class="k">assert</code> <code class="n">masked_inputs</code><code class="p">[</code><code class="mi">30</code><code class="p">]</code> <code class="o">==</code> <code class="n">vocab_to_index</code><code class="p">[</code><code class="s2">"&lt;mask&gt;"</code><code class="p">]</code>
</pre>
      
     
    
   
   <p>The <code>&lt;mask&gt;</code> token tells the model to predict the amino acid at that position. To do this, we load the full language model, <code>EsmForMaskedLM</code>, which includes the language prediction head.</p>

   <div data-type="note" epub:type="note"><h6>Note</h6><p>To accelerate inference, we’ll use a smaller ESM2 model variant (150M parameters with 640-dimensional embeddings) rather than the large, 650M model with 1,280-dimensional embeddings used earlier. This is a good reminder that many models on Hugging Face come in different sizes, and swapping between them is often as simple as changing a model checkpoint.</p>
    <p>Of course, there’s a trade-off—smaller models may capture less information and typically perform worse on complex tasks. Still, they’re great for rapid prototyping and exploring model behavior.</p>
   </div>
  
    
     
      <p>We load up the model:</p>
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">EsmForMaskedLM</code>

<code class="c1"># Model checkpoint name taken from this GitHub README:</code>
<code class="c1"># https://github.com/facebookresearch/esm#available-models-and-datasets-</code>
<code class="n">model_checkpoint</code> <code class="o">=</code> <code class="s2">"facebook/esm2_t30_150M_UR50D"</code>
<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_checkpoint</code><code class="p">)</code>
<code class="n">masked_lm_model</code> <code class="o">=</code> <code class="n">EsmForMaskedLM</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_checkpoint</code><code class="p">)</code>
</pre>
      
     
    
   
   <p>And we’ll run it to get predictions for the masked token. We see that the model correctly predicts the token <code>L</code> (leucine) with very high probability in <a data-type="xref" href="#predict-missing-aa">Figure 2-9</a>.</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>

<code class="n">model_outputs</code> <code class="o">=</code> <code class="n">masked_lm_model</code><code class="p">(</code>
  <code class="o">**</code><code class="n">tokenizer</code><code class="p">(</code><code class="n">text</code><code class="o">=</code><code class="n">masked_insulin_sequence</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">)</code>
<code class="p">)</code>
<code class="n">model_preds</code> <code class="o">=</code> <code class="n">model_outputs</code><code class="o">.</code><code class="n">logits</code>

<code class="c1"># Index into the predictions at the &lt;mask&gt; position.</code>
<code class="n">mask_preds</code> <code class="o">=</code> <code class="n">model_preds</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">30</code><code class="p">]</code><code class="o">.</code><code class="n">detach</code><code class="p">()</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code>

<code class="c1"># Apply softmax to convert the model's predicted logits to probabilities.</code>
<code class="n">mask_probs</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">mask_preds</code><code class="p">)</code>

<code class="c1"># Visualize the predicted probability of each token.</code>
<code class="n">letters</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="n">vocab_to_index</code><code class="o">.</code><code class="n">keys</code><code class="p">())</code>
<code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">bar</code><code class="p">(</code><code class="n">letters</code><code class="p">,</code> <code class="n">mask_probs</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s2">"grey"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xticks</code><code class="p">(</code><code class="n">rotation</code><code class="o">=</code><code class="mi">90</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"Model Probabilities for the Masked Amino Acid"</code><code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="predict-missing-aa" class="figure">
      <img alt="" src="assets/dlfb_0209.png" width="600" height="486"/>
      <h6><span class="label">Figure 2-9. </span>Model prediction for a masked leucine (<code>L</code>) in the insulin sequence. The model confidently predicts the correct amino acid (<code>L</code>) with high probability, showing that it has learned common sequence patterns in proteins.
      </h6>
     </div></figure>
    
   
   <p>Let’s rewrite this code as a more general form as <code>MaskPredictor</code>, with methods that mask a sequence, make a prediction, and plot the predictions:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">MaskPredictor</code><code class="p">:</code>
  <code class="sd">"""Predict masked amino acids using a protein language model."""</code>

  <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">tokenizer</code><code class="p">:</code> <code class="n">PreTrainedTokenizer</code><code class="p">,</code> <code class="n">model</code><code class="p">:</code> <code class="n">PreTrainedModel</code><code class="p">):</code>
    <code class="sd">"""Initialize with a tokenizer and pretrained model."""</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">tokenizer</code> <code class="o">=</code> <code class="n">tokenizer</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">model</code> <code class="o">=</code> <code class="n">model</code>

  <code class="k">def</code> <code class="nf">plot_predictions</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">sequence</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">mask_index</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Figure</code><code class="p">:</code>
    <code class="sd">"""Plot predicted probabilities for the masked amino acid."""</code>
    <code class="n">mask_probs</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">sequence</code><code class="p">,</code> <code class="n">mask_index</code><code class="p">)</code>
    <code class="n">fig</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">bar</code><code class="p">(</code><code class="nb">list</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">tokenizer</code><code class="o">.</code><code class="n">get_vocab</code><code class="p">()</code><code class="o">.</code><code class="n">keys</code><code class="p">()),</code> <code class="n">mask_probs</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s2">"grey"</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">xticks</code><code class="p">(</code><code class="n">rotation</code><code class="o">=</code><code class="mi">90</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code>
      <code class="s2">"Model Probabilities for the Masked Amino Acid</code><code class="se">\n</code><code class="s2">"</code>
      <code class="sa">f</code><code class="s2">"at Index=</code><code class="si">{</code><code class="n">mask_index</code><code class="si">}</code><code class="s2"> (True Amino Acid = </code><code class="si">{</code><code class="n">sequence</code><code class="p">[</code><code class="n">mask_index</code><code class="p">]</code><code class="si">}</code><code class="s2">)."</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">fig</code>

  <code class="k">def</code> <code class="nf">predict</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">sequence</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">mask_index</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Return model probabilities for masked amino acid at a position."""</code>
    <code class="n">masked_sequence</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">mask_sequence</code><code class="p">(</code><code class="n">sequence</code><code class="p">,</code> <code class="n">mask_index</code><code class="p">)</code>
    <code class="n">masked_inputs</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">tokenizer</code><code class="p">(</code><code class="n">masked_sequence</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">)</code>
    <code class="n">model_outputs</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="p">(</code><code class="o">**</code><code class="n">masked_inputs</code><code class="p">)</code>
    <code class="n">mask_preds</code> <code class="o">=</code> <code class="n">model_outputs</code><code class="o">.</code><code class="n">logits</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="n">mask_index</code> <code class="o">+</code> <code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">detach</code><code class="p">()</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code>
    <code class="n">mask_probs</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">mask_preds</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">mask_probs</code>

  <code class="nd">@staticmethod</code>
  <code class="k">def</code> <code class="nf">mask_sequence</code><code class="p">(</code><code class="n">sequence</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">mask_index</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
    <code class="sd">"""Insert mask token at specified index in the input sequence."""</code>
    <code class="k">if</code> <code class="n">mask_index</code> <code class="o">&lt;</code> <code class="mi">0</code> <code class="ow">or</code> <code class="n">mask_index</code> <code class="o">&gt;</code> <code class="nb">len</code><code class="p">(</code><code class="n">sequence</code><code class="p">):</code>
      <code class="k">raise</code> <code class="ne">ValueError</code><code class="p">(</code><code class="s2">"Mask index outside of sequence range."</code><code class="p">)</code>
    <code class="k">return</code> <code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">sequence</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="n">mask_index</code><code class="p">]</code><code class="si">}</code><code class="s2">&lt;mask&gt;</code><code class="si">{</code><code class="n">sequence</code><code class="p">[(</code><code class="n">mask_index</code> <code class="o">+</code> <code class="mi">1</code><code class="p">):]</code><code class="si">}</code><code class="s2">"</code>
</pre>
   
   <p>Let’s try it on a different position—index 26, where the correct amino acid is <code>N</code> (asparagine). The result is shown in <a data-type="xref" href="#predict-missing-aa-less-clear">Figure 2-10</a>:</p>

       <pre data-type="programlisting" data-code-language="python"><code class="n">MaskPredictor</code><code class="p">(</code><code class="n">tokenizer</code><code class="p">,</code> <code class="n">model</code><code class="o">=</code><code class="n">masked_lm_model</code><code class="p">)</code><code class="o">.</code><code class="n">plot_predictions</code><code class="p">(</code>
  <code class="n">sequence</code><code class="o">=</code><code class="n">insulin_sequence</code><code class="p">,</code> <code class="n">mask_index</code><code class="o">=</code><code class="mi">26</code>
<code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="predict-missing-aa-less-clear" class="figure">
      <img alt="" src="assets/dlfb_0210.png" width="600" height="492"/>
      <h6><span class="label">Figure 2-10. </span>Model prediction for a masked asparagine (<code>N</code>) in the insulin sequence. Here, the model is more uncertain—it assigns moderate probability to several possible amino acids, indicating that this position is harder to predict based on surrounding context.
      </h6>
     </div></figure>
    
   
   <p>In this case, the model doesn’t strongly prefer any one amino acid. It assigns moderate probability to several, including <code>A</code>, <code>T</code>, and <code>S</code> (with the model assigning the true amino acid <code>N</code> a fairly low probability). This uncertainty could reflect the biochemical flexibility of that position—some regions of proteins can tolerate different residues due to redundancy, structural flexibility, or lack of strict functional constraints. These are often called “permissive” positions and are common in disordered (unstructured) or surface regions of proteins.</p>

<p>This example illustrates that the model has learned and understands the probabilistic grammar of proteins. The next question is: how can we leverage this understanding to represent an entire protein, and not just one amino acid at a time?<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html15" data-type="indexterm" id="id569"/><a contenteditable="false" data-primary="" data-startref="ch02_proteins.html14" data-type="indexterm" id="id570"/></p>
  </div></section>

  <section data-type="sect2" data-pdf-bookmark="Strategies for Extracting an Embedding for an Entire Protein"><div class="sect2" id="strategies-for-extracting-an-embedding-for-an-entire-protein">
   <h2>Strategies for Extracting an Embedding for an Entire Protein</h2>

   <p><a contenteditable="false" data-primary="embeddings" data-secondary="extracting an embedding for an entire protein" data-type="indexterm" id="ch02_proteins.html16"/><a contenteditable="false" data-primary="embeddings" data-secondary="strategies for extracting an embedding for an entire protein" data-type="indexterm" id="ch02_proteins.html17"/><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="representations of proteins and protein LMs" data-tertiary="strategies for extracting an embedding for an entire protein" data-type="indexterm" id="ch02_proteins.html18"/>So far, we’ve explored how the ESM2 model represents individual amino acids. But many downstream tasks—like predicting protein function—require a fixed-length representation for the entire protein sequence. How can we convert a variable-length sequence of amino acids into a single embedding vector that captures the protein’s overall structure and meaning?</p>

   <p>Several strategies are commonly used:</p>
   <dl>
    <dt>Concatenation of amino acid embeddings</dt>
    <dd>
     <p>One simple approach is to loop through each amino acid in a sequence, extract its embedding, and concatenate them into one long vector. For example, if a protein has length 10 and each amino acid has a 640-dimensional embedding, this yields a protein embedding of length <code>10 × 640 = 6400</code>. While this preserves fine-grained information of each amino acid, it has several drawbacks:</p>
     <dl>
      <dt>Variable length</dt>
      <dd>
       <p>Different proteins will yield different-length embeddings, which complicates model input formatting.
       </p>
      </dd>
      <dt>Scalability</dt>
      <dd>
       <p>Long proteins produce huge embeddings. For example, titin—the longest known human protein at ~34,000 amino acids—would produce an embedding with over <em>43 million</em> values. That’s unwieldy for most models.
       </p>
      </dd>
      <dt>Limited modeling</dt>
      <dd>
       <p>This approach treats amino acids independently, ignoring the contextual relationships that are central to protein function.
       </p>
      </dd>

      </dl>
            </dd>
   <dt>Averaging of amino acid embeddings</dt>
   <dd><p>A more compact approach is to average the token embeddings across the sequence. Using the same example of a length-10 protein with 640-dim <span class="keep-together">embeddings</span>, we take the mean across all 10 embeddings to produce a final 640-dimensional vector.</p>

<ul>
 <li><p>This has the advantage of producing fixed-size vectors, regardless of protein length.</p></li>
 <li><p>It’s efficient and sometimes used, but also crude—averaging discards ordering and interaction information. It’s like summarizing a novel by averaging all its word vectors: some meaning survives, but the nuance is lost.</p></li>
</ul>
</dd>
   <dt>Using the model’s contextual sequence embeddings</dt>
   <dd><p>A more principled approach is to extract the hidden representations for the entire sequence directly from the language model. Since ESM2 is trained to predict masked tokens based on their surrounding context, its internal layers encode rich, contextualized embeddings for every amino acid in the sequence.</p>
   <ul>
    <li><p>Concretely, we can pass a protein sequence through ESM2 and extract the final hidden layer activations, resulting in a tensor of shape (<code>L', D</code>), where <code>L'</code> is the number of output tokens (which may differ from the input length <code>L</code>), and <code>D</code> is the model’s hidden size (e.g., 640).</p></li>
    <li><p>We then apply mean pooling across the sequence length to produce a fixed-length embedding of shape (<code>D,</code>). While averaging may seem simplistic, it often works surprisingly well—because the model has already integrated contextual information into each token’s representation using self-attention, the pooled vector still captures meaningful dependencies across the sequence.<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html18" data-type="indexterm" id="id571"/><a contenteditable="false" data-primary="" data-startref="ch02_proteins.html17" data-type="indexterm" id="id572"/><a contenteditable="false" data-primary="" data-startref="ch02_proteins.html16" data-type="indexterm" id="id573"/></p></li>
   </ul>
   </dd>
  </dl>
  <p>This final approach is the most common and powerful in practice—and it’s the one we’ll explore in the next section.</p>



  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Extracellular Versus Membrane Protein Embeddings"><div class="sect2" id="extracellular-versus-membrane-protein-embeddings">
   <h2>Extracellular Versus Membrane Protein Embeddings</h2>
   <p>
    <a contenteditable="false" data-primary="embeddings" data-secondary="extracellular versus membrane protein embeddings" data-type="indexterm" id="ch02_proteins.html19"/><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="representations of proteins and protein LMs" data-tertiary="extracellular versus membrane protein embeddings" data-type="indexterm" id="ch02_proteins.html20"/>We’ll introduce the GO dataset properly in the next section on protein function prediction. For now, let’s use it to associate each UniProt protein accession and sequence with its known cellular location:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>

<code class="kn">from</code> <code class="nn">dlfb.utils.context</code> <code class="kn">import</code> <code class="n">assets</code>

<code class="n">protein_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="n">assets</code><code class="p">(</code><code class="s2">"proteins/datasets/sequence_df_cco.csv"</code><code class="p">))</code>
<code class="n">protein_df</code> <code class="o">=</code> <code class="n">protein_df</code><code class="p">[</code><code class="o">~</code><code class="n">protein_df</code><code class="p">[</code><code class="s2">"term"</code><code class="p">]</code><code class="o">.</code><code class="n">isin</code><code class="p">([</code><code class="s2">"GO:0005575"</code><code class="p">,</code> <code class="s2">"GO:0110165"</code><code class="p">])]</code>
<code class="n">num_proteins</code> <code class="o">=</code> <code class="n">protein_df</code><code class="p">[</code><code class="s2">"EntryID"</code><code class="p">]</code><code class="o">.</code><code class="n">nunique</code><code class="p">()</code>
<code class="nb">print</code><code class="p">(</code><code class="n">protein_df</code><code class="p">)</code>
</pre>     

<p class="pagebreak-before">Output:</p>
       <pre data-type="programlisting">       EntryID             Sequence  taxonomyID        term aspect  Length
0       O95231  MRLSSSPPRGPQQLSS...        9606  GO:0005622    CCO     258
1       O95231  MRLSSSPPRGPQQLSS...        9606  GO:0031981    CCO     258
2       O95231  MRLSSSPPRGPQQLSS...        9606  GO:0043229    CCO     258
...        ...                  ...         ...         ...    ...     ...
337551  E7ER32  MPPLKSPAAFHEQRRS...        9606  GO:0031974    CCO     798
337552  E7ER32  MPPLKSPAAFHEQRRS...        9606  GO:0005634    CCO     798
337553  E7ER32  MPPLKSPAAFHEQRRS...        9606  GO:0005654    CCO     798

[294731 rows x 6 columns]
</pre> 

<p>For each protein sequence identified by an <code>EntryID</code>, the <code>term</code> column provides its GO annotation for cellular localization.</p>

<p>Let’s focus on two specific locations:</p>
<dl>
  <dt><code>extracellular</code> (GO:0005576)</dt>
  <dd><p>Proteins secreted outside the cell, often involved in signaling, immune response, or structural roles</p></dd>
  <dt><code>membrane</code> (GO:0016020)</dt>
  <dd><p>Proteins embedded in or associated with cell membranes, frequently functioning in transport, signaling, or cell–cell interaction</p></dd>
</dl>
     
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id574">
  <h5>The Connection Between Cell Location and Sequence Features</h5>
  <p><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="representations of proteins and protein LMs" data-tertiary="connection between cell location/sequence features" data-type="indexterm" id="id575"/>We’re filtering proteins based on their cellular location, but the model we’re using is trained purely on sequence. So what’s the connection?</p>

<p>The key point is that certain types of proteins have characteristic sequence features that correlate with where they function in the cell. For example, membrane proteins often contain stretches of amino acids that anchor them into the cell’s outer membrane. These regions tend to be water repellent (hydrophobic), helping them interact with the oily membrane environment.</p>

<p>By contrast, extracellular proteins—those sent outside the cell—typically include short signal sequences that direct their export. They also often form stable structures through chemical bridges called disulfide bonds and may include regions that facilitate binding to other molecules.</p>

<p>These structural features are encoded in the amino acid sequence and should, in theory, be picked up by pretrained language models like ESM2—even though the model was never trained on location labels. In this section, we’re essentially testing whether such structural signals are reflected in the learned embeddings.</p>
</div></aside>
   
   <p>We’ll filter the dataset to proteins annotated with only one of these two locations:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="c1"># Filter protein dataframe to proteins with a single location.</code>
<code class="n">num_locations</code> <code class="o">=</code> <code class="n">protein_df</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s2">"EntryID"</code><code class="p">)[</code><code class="s2">"term"</code><code class="p">]</code><code class="o">.</code><code class="n">nunique</code><code class="p">()</code>
<code class="n">proteins_one_location</code> <code class="o">=</code> <code class="n">num_locations</code><code class="p">[</code><code class="n">num_locations</code> <code class="o">==</code> <code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">index</code>
<code class="n">protein_df</code> <code class="o">=</code> <code class="n">protein_df</code><code class="p">[</code><code class="n">protein_df</code><code class="p">[</code><code class="s2">"EntryID"</code><code class="p">]</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">proteins_one_location</code><code class="p">)]</code>

<code class="n">go_function_examples</code> <code class="o">=</code> <code class="p">{</code>
  <code class="s2">"extracellular"</code><code class="p">:</code> <code class="s2">"GO:0005576"</code><code class="p">,</code>
  <code class="s2">"membrane"</code><code class="p">:</code> <code class="s2">"GO:0016020"</code><code class="p">,</code>
<code class="p">}</code>

<code class="n">sequences_by_function</code> <code class="o">=</code> <code class="p">{}</code>

<code class="n">min_length</code> <code class="o">=</code> <code class="mi">100</code>
<code class="n">max_length</code> <code class="o">=</code> <code class="mi">500</code>  <code class="c1"># Cap sequence length for speed and memory.</code>
<code class="n">num_samples</code> <code class="o">=</code> <code class="mi">20</code>

<code class="k">for</code> <code class="n">function</code><code class="p">,</code> <code class="n">go_term</code> <code class="ow">in</code> <code class="n">go_function_examples</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
  <code class="n">proteins_with_function</code> <code class="o">=</code> <code class="n">protein_df</code><code class="p">[</code>
    <code class="p">(</code><code class="n">protein_df</code><code class="p">[</code><code class="s2">"term"</code><code class="p">]</code> <code class="o">==</code> <code class="n">go_term</code><code class="p">)</code>
    <code class="o">&amp;</code> <code class="p">(</code><code class="n">protein_df</code><code class="p">[</code><code class="s2">"Length"</code><code class="p">]</code> <code class="o">&gt;=</code> <code class="n">min_length</code><code class="p">)</code>
    <code class="o">&amp;</code> <code class="p">(</code><code class="n">protein_df</code><code class="p">[</code><code class="s2">"Length"</code><code class="p">]</code> <code class="o">&lt;=</code> <code class="n">max_length</code><code class="p">)</code>
  <code class="p">]</code>
  <code class="nb">print</code><code class="p">(</code>
    <code class="sa">f</code><code class="s2">"Found </code><code class="si">{</code><code class="nb">len</code><code class="p">(</code><code class="n">proteins_with_function</code><code class="p">)</code><code class="si">}</code><code class="s2"> human proteins</code><code class="se">\n</code><code class="s2">"</code>
    <code class="sa">f</code><code class="s2">"with the molecular function '</code><code class="si">{</code><code class="n">function</code><code class="si">}</code><code class="s2">' (</code><code class="si">{</code><code class="n">go_term</code><code class="si">}</code><code class="s2">),</code><code class="se">\n</code><code class="s2">"</code>
    <code class="sa">f</code><code class="s2">"and </code><code class="si">{</code><code class="n">min_length</code><code class="si">}</code><code class="s2">&lt;=length&lt;=</code><code class="si">{</code><code class="n">max_length</code><code class="si">}</code><code class="s2">.</code><code class="se">\n</code><code class="s2">"</code>
    <code class="sa">f</code><code class="s2">"Sampling </code><code class="si">{</code><code class="n">num_samples</code><code class="si">}</code><code class="s2"> proteins at random.</code><code class="se">\n</code><code class="s2">"</code>
  <code class="p">)</code>
  <code class="n">sequences</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code>
    <code class="n">proteins_with_function</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="n">num_samples</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)[</code><code class="s2">"Sequence"</code><code class="p">]</code>
  <code class="p">)</code>
  <code class="n">sequences_by_function</code><code class="p">[</code><code class="n">function</code><code class="p">]</code> <code class="o">=</code> <code class="n">sequences</code>
</pre>
     
 <p>Output:</p>     
       <pre data-type="programlisting">Found 164 human proteins
with the molecular function 'extracellular' (GO:0005576),
and 100&lt;=length&lt;=500.
Sampling 20 proteins at random.

Found 65 human proteins
with the molecular function 'membrane' (GO:0016020),
and 100&lt;=length&lt;=500.
Sampling 20 proteins at random.
</pre>

   
   <p>We’ll now extract embeddings from these sequences. The function <code>get_mean_embeddings</code> computes the mean hidden state across each sequence, summarizing the model’s representation of protein sequences:</p>
   
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">get_mean_embeddings</code><code class="p">(</code>
  <code class="n">sequences</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">str</code><code class="p">],</code>
  <code class="n">tokenizer</code><code class="p">:</code> <code class="n">PreTrainedTokenizer</code><code class="p">,</code>
  <code class="n">model</code><code class="p">:</code> <code class="n">PreTrainedModel</code><code class="p">,</code>
  <code class="n">device</code><code class="p">:</code> <code class="n">torch</code><code class="o">.</code><code class="n">device</code> <code class="o">|</code> <code class="kc">None</code> <code class="o">=</code> <code class="kc">None</code><code class="p">,</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">:</code>
  <code class="sd">"""Compute mean embedding for each sequence using a protein LM."""</code>
  <code class="k">if</code> <code class="ow">not</code> <code class="n">device</code><code class="p">:</code>
    <code class="n">device</code> <code class="o">=</code> <code class="n">get_device</code><code class="p">()</code>

  <code class="c1"># Tokenize input sequences and pad them to equal length.</code>
  <code class="n">model_inputs</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="n">sequences</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">)</code>

  <code class="c1"># Move tokenized inputs to the target device (CPU or GPU).</code>
  <code class="n">model_inputs</code> <code class="o">=</code> <code class="p">{</code><code class="n">k</code><code class="p">:</code> <code class="n">v</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code> <code class="n">v</code> <code class="ow">in</code> <code class="n">model_inputs</code><code class="o">.</code><code class="n">items</code><code class="p">()}</code>

  <code class="c1"># Move model to the target device and set it to evaluation mode.</code>
  <code class="n">model</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
  <code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>

  <code class="c1"># Forward pass without gradient tracking to obtain embeddings.</code>
  <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="o">**</code><code class="n">model_inputs</code><code class="p">)</code>
    <code class="n">mean_embeddings</code> <code class="o">=</code> <code class="n">outputs</code><code class="o">.</code><code class="n">last_hidden_state</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

  <code class="k">return</code> <code class="n">mean_embeddings</code><code class="o">.</code><code class="n">detach</code><code class="p">()</code><code class="o">.</code><code class="n">cpu</code><code class="p">()</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code>
</pre>
      
     
    
   
  <p>Now we’ll extract embeddings using a smaller ESM2 model, which produces 320-dimensional representations and requires significantly less memory than larger <span class="keep-together">variants</span>:</p>
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">model_checkpoint</code> <code class="o">=</code> <code class="s2">"facebook/esm2_t6_8M_UR50D"</code>
<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_checkpoint</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">EsmModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_checkpoint</code><code class="p">)</code>
</pre>
      
     
    
   
   <p>We then calculate the embeddings:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="c1"># Compute mean protein embeddings for each location.</code>
<code class="n">protein_embeddings</code> <code class="o">=</code> <code class="p">{</code>
  <code class="n">loc</code><code class="p">:</code> <code class="n">get_mean_embeddings</code><code class="p">(</code><code class="n">sequences_by_function</code><code class="p">[</code><code class="n">loc</code><code class="p">],</code> <code class="n">tokenizer</code><code class="p">,</code> <code class="n">model</code><code class="p">)</code>
  <code class="k">for</code> <code class="n">loc</code> <code class="ow">in</code> <code class="p">[</code><code class="s2">"extracellular"</code><code class="p">,</code> <code class="s2">"membrane"</code><code class="p">]</code>
<code class="p">}</code>

<code class="c1"># Reformat data.</code>
<code class="n">labels</code><code class="p">,</code> <code class="n">embeddings</code> <code class="o">=</code> <code class="p">[],</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">location</code><code class="p">,</code> <code class="n">embedding</code> <code class="ow">in</code> <code class="n">protein_embeddings</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
  <code class="n">labels</code><code class="o">.</code><code class="n">extend</code><code class="p">([</code><code class="n">location</code><code class="p">]</code> <code class="o">*</code> <code class="n">embedding</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
  <code class="n">embeddings</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">embedding</code><code class="p">)</code>
  <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">location</code><code class="si">}</code><code class="s2">: </code><code class="si">{</code><code class="n">embedding</code><code class="o">.</code><code class="n">shape</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
</pre>
      
     
    
    
     
 <p>Output:</p>     
       <pre data-type="programlisting">extracellular: (20, 320)
membrane: (20, 320)
</pre>
      
     
    
   
   
    
    <p class="pagebreak-before">Each set of 20 sampled proteins is now represented as a <code>(20, 320)</code> embedding matrix. This means that for each sequence—regardless of its original length—we obtain a fixed-size vector of 320 dimensions. These vectors correspond to the mean of the final hidden layer activations across all tokens in the sequence, and should capture some information about the overall protein structure.</p>
    <p>To visualize how these embeddings might relate to protein localization, we project them into two dimensions using t-SNE, a common method for visualizing high-dimensional data. <a data-type="xref" href="#membrane-protein-embeddings">Figure 2-11</a> shows that the extracellular and membrane proteins tend to form distinct clusters in this space.</p>
 
       <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="k">as</code> <code class="nn">sns</code>
<code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="kn">import</code> <code class="n">TSNE</code>

<code class="n">embeddings_tsne</code> <code class="o">=</code> <code class="n">TSNE</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code>
  <code class="n">np</code><code class="o">.</code><code class="n">vstack</code><code class="p">(</code><code class="n">embeddings</code><code class="p">)</code>
<code class="p">)</code>
<code class="n">embeddings_tsne_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code>
  <code class="p">{</code>
    <code class="s2">"first_dimension"</code><code class="p">:</code> <code class="n">embeddings_tsne</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code>
    <code class="s2">"second_dimension"</code><code class="p">:</code> <code class="n">embeddings_tsne</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code>
    <code class="s2">"location"</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">labels</code><code class="p">),</code>
  <code class="p">}</code>
<code class="p">)</code>

<code class="n">fig</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">scatterplot</code><code class="p">(</code>
  <code class="n">data</code><code class="o">=</code><code class="n">embeddings_tsne_df</code><code class="p">,</code>
  <code class="n">x</code><code class="o">=</code><code class="s2">"first_dimension"</code><code class="p">,</code>
  <code class="n">y</code><code class="o">=</code><code class="s2">"second_dimension"</code><code class="p">,</code>
  <code class="n">hue</code><code class="o">=</code><code class="s2">"location"</code><code class="p">,</code>
  <code class="n">style</code><code class="o">=</code><code class="s2">"location"</code><code class="p">,</code>
  <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code>
  <code class="n">alpha</code><code class="o">=</code><code class="mf">0.7</code><code class="p">,</code>
<code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"tSNE of Protein Embeddings"</code><code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s2">"First Dimension"</code><code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s2">"Second Dimension"</code><code class="p">);</code>
</pre>
      
   
     <figure><div id="membrane-protein-embeddings" class="figure">
      <img alt="" src="assets/dlfb_0211.png" width="600" height="471"/>
      <h6><span class="label">Figure 2-11. </span>Two-dimensional t-SNE projection of the 320-dimensional embeddings from a small ESM2 model. Even with this lightweight model, we observe a tendency for extracellular and membrane proteins to form separate clusters, suggesting that the embeddings contain information relevant to cellular localization.
      </h6>
     </div></figure>
    
   
   <p>While the separation isn’t perfect, there’s a clear trend: extracellular proteins tend to cluster in a different region of embedding space than membrane proteins. It’s quite striking that the model picks up on this purely from sequence. This suggests that the learned embeddings reflect biologically meaningful patterns—even without any explicit supervision for cellular location.<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html20" data-type="indexterm" id="id576"/><a contenteditable="false" data-primary="" data-startref="ch02_proteins.html19" data-type="indexterm" id="id577"/>
   </p>
   <p>With this initial exploration complete, we now turn to the central machine learning task of this chapter: predicting protein function. Let’s begin by preparing the dataset.<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html7" data-type="indexterm" id="id578"/></p>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Preparing the Data"><div class="sect1" id="preparing-the-data">
  <h1>Preparing the Data</h1>
  <p>
   <a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="data preparation for predicting protein function from sequence" data-type="indexterm" id="ch02_proteins.html21"/>Many machine learning books and blog posts jump straight into the exciting <span class="keep-together">parts—training</span> and evaluating models—as soon as possible. But in practice, training is often a small fraction of the overall workflow. A significant portion of time is spent understanding, cleaning, and structuring the data. And when things go wrong with a model, the root cause is often found in the data. So, rather than handing you a polished CSV from the ether, we’ll walk through the data preparation process step-by-step—starting from real-world resources and working through the steps needed to turn them into something a model can use.
  </p>
  <p>Our goal is to fine-tune a model to predict protein function from sequence, which means assembling a dataset of <code>(protein_sequence, protein_function)</code> pairs. Fortunately, biologists have developed systematic frameworks for defining protein functions, and curated datasets already exist. One of the most widely used resources is the <a href="https://oreil.ly/87EN_">CAFA (Critical Assessment of Functional Annotation)</a> challenge, a community-driven competition where teams build models to predict protein function. We’ll use CAFA data as our raw material, but we’ll still need to process and structure it ourselves.
  </p>
  
  <div data-type="note" epub:type="note"><h6>Note</h6><p>If you’re familiar with AlphaFold and protein structure prediction, you may have heard of the similarly named CASP (Critical Assessment of Structure Prediction), which plays a similar role in the protein structure community. Public benchmarks like these have been instrumental in driving progress across a wide range of computational biology problems.</p></div>
  <p>Let’s now explore the CAFA dataset.</p>
  
  <section data-type="sect2" data-pdf-bookmark="Loading the CAFA3 Data"><div class="sect2" id="loading-the-cafa3-data">
   <h2>Loading the CAFA3 Data</h2>
   <p><a contenteditable="false" data-primary="CAFA3 dataset" data-secondary="loading the data" data-type="indexterm" id="ch02_proteins.html22"/><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="data preparation for predicting protein function from sequence" data-tertiary="loading the CAFA3 data" data-type="indexterm" id="ch02_proteins.html23"/>There have been several rounds of CAFA, but the CAFA3 dataset is the most recent publicly available one. We first downloaded the “CAFA3 Targets” and “CAFA3 Training Data” files from the <a href="https://oreil.ly/87EN_">CAFA website</a>. Let’s start by loading the label file, which tells us the functional annotations for each protein:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">labels</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code>
  <code class="n">assets</code><code class="p">(</code><code class="s2">"proteins/datasets/train_terms.tsv.zip"</code><code class="p">),</code> <code class="n">sep</code><code class="o">=</code><code class="s2">"</code><code class="se">\t</code><code class="s2">"</code><code class="p">,</code> <code class="n">compression</code><code class="o">=</code><code class="s2">"infer"</code>
<code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">labels</code><code class="p">)</code>
</pre>
 
<p>Output:</p>
       <pre data-type="programlisting">            EntryID        term aspect
0        A0A009IHW8  GO:0008152    BPO
1        A0A009IHW8  GO:0034655    BPO
2        A0A009IHW8  GO:0072523    BPO
...             ...         ...    ...
5363860      X5M5N0  GO:0005515    MFO
5363861      X5M5N0  GO:0005488    MFO
5363862      X5M5N0  GO:0003674    MFO

[5363863 rows x 3 columns]
</pre>
      
     
    
   
   <p>This dataframe contains three columns:</p>

<dl>
  <dt><code>EntryID</code></dt>
<dd><p>The UniProt ID of the protein</p></dd>

<dt><code>term</code></dt>
<dd><p>A GO accession code describing a specific protein function</p></dd>

<dt><code>aspect</code></dt>
<dd><p>The GO category the function belongs to; one of three types of function described in the introduction: biological process (BPO), molecular function (MFO), and cellular component (CCO)</p></dd>
</dl>

   <p>The <code>term</code> column contains only GO accession codes. To make these more interpretable, we’d ideally like to know their corresponding human-readable descriptions. This information isn’t included directly in the CAFA files, but it is available via the <a href="https://oreil.ly/uNhm2">Gene Ontology downloads page</a>. The ontology is stored in graph format as a <code>.obo</code> file, and we can use the <code>obonet</code> Python library to parse it. Here’s how we retrieve the term descriptions:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">obonet</code>

<code class="k">def</code> <code class="nf">get_go_term_descriptions</code><code class="p">(</code><code class="n">store_path</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">:</code>
  <code class="sd">"""Return GO term to description mapping, downloading if needed."""</code>
  <code class="k">if</code> <code class="ow">not</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">exists</code><code class="p">(</code><code class="n">store_path</code><code class="p">):</code>
    <code class="n">url</code> <code class="o">=</code> <code class="s2">"https://current.geneontology.org/ontology/go-basic.obo"</code>
    <code class="n">graph</code> <code class="o">=</code> <code class="n">obonet</code><code class="o">.</code><code class="n">read_obo</code><code class="p">(</code><code class="n">url</code><code class="p">)</code>

    <code class="c1"># Extract GO term IDs and names from the graph nodes.</code>
    <code class="n">id_to_name</code> <code class="o">=</code> <code class="p">{</code><code class="nb">id</code><code class="p">:</code> <code class="n">data</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"name"</code><code class="p">)</code> <code class="k">for</code> <code class="nb">id</code><code class="p">,</code> <code class="n">data</code> <code class="ow">in</code> <code class="n">graph</code><code class="o">.</code><code class="n">nodes</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="kc">True</code><code class="p">)}</code>
    <code class="n">go_term_descriptions</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code>
      <code class="nb">zip</code><code class="p">(</code><code class="n">id_to_name</code><code class="o">.</code><code class="n">keys</code><code class="p">(),</code> <code class="n">id_to_name</code><code class="o">.</code><code class="n">values</code><code class="p">()),</code>
      <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s2">"term"</code><code class="p">,</code> <code class="s2">"description"</code><code class="p">],</code>
    <code class="p">)</code>
    <code class="n">go_term_descriptions</code><code class="o">.</code><code class="n">to_csv</code><code class="p">(</code><code class="n">store_path</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>

  <code class="k">else</code><code class="p">:</code>
    <code class="n">go_term_descriptions</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="n">store_path</code><code class="p">)</code>
  <code class="k">return</code> <code class="n">go_term_descriptions</code>
</pre>
      
     <p>The function will load the annotations from a local file if it already exists, or download and cache them if not:</p>

     <pre data-type="programlisting" data-code-language="python"><code class="n">go_term_descriptions</code> <code class="o">=</code> <code class="n">get_go_term_descriptions</code><code class="p">(</code>
  <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"proteins/datasets/go_term_descriptions.csv"</code><code class="p">)</code>
<code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">go_term_descriptions</code><code class="p">)</code>
</pre>

<p>Output:</p>
     <pre data-type="programlisting">
             term          description
0      GO:0000001  mitochondrion in...
1      GO:0000002  mitochondrial ge...
2      GO:0000006  high-affinity zi...
...           ...                  ...
40211  GO:2001315  UDP-4-deoxy-4-fo...
40212  GO:2001316  kojic acid metab...
40213  GO:2001317  kojic acid biosy...

[40214 rows x 2 columns]
</pre>
    
 
   <p>We can then merge the human-readable term descriptions back onto the labels dataframe:</p>

       <pre data-type="programlisting" data-code-language="python"><code class="n">labels</code> <code class="o">=</code> <code class="n">labels</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">go_term_descriptions</code><code class="p">,</code> <code class="n">on</code><code class="o">=</code><code class="s2">"term"</code><code class="p">)</code>
<code class="n">labels</code>
</pre>

<p>Output:</p>
       <pre data-type="programlisting">            EntryID        term aspect          description
0        A0A009IHW8  GO:0008152    BPO    metabolic process
1        A0A009IHW8  GO:0034655    BPO  nucleobase-conta...
2        A0A009IHW8  GO:0072523    BPO  purine-containin...
...             ...         ...    ...                  ...
4933955      X5M5N0  GO:0005515    MFO      protein binding
4933956      X5M5N0  GO:0005488    MFO              binding
4933957      X5M5N0  GO:0003674    MFO   molecular_function

[4933958 rows x 4 columns]
</pre>

    
   
   <p>In this chapter, we’ll focus specifically on molecular functions (<code>MFO</code>)—that is, what a protein does at the biochemical level. Later, you may want to extend this chapter’s approach to include the other two GO categories.</p>
   <p>Let’s take a look at which molecular functions are most commonly annotated in the dataset:</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">labels</code> <code class="o">=</code> <code class="n">labels</code><code class="p">[</code><code class="n">labels</code><code class="p">[</code><code class="s2">"aspect"</code><code class="p">]</code> <code class="o">==</code> <code class="s2">"MFO"</code><code class="p">]</code>
<code class="nb">print</code><code class="p">(</code><code class="n">labels</code><code class="p">[</code><code class="s2">"description"</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">())</code>
</pre>
      
     
    
    
     
 <p>Output:</p>     
       <pre data-type="programlisting">description
molecular_function                            78637
binding                                       57380
protein binding                               47987
                                              ...  
kaempferide 7-O-methyltransferase activity        1
protopine 6-monooxygenase activity                1
costunolide 3beta-hydroxylase activity            1
Name: count, Length: 6973, dtype: int64
</pre>
      
     
    
   
   <p>We can already see that the distribution of function annotations is highly skewed. Some terms—like <code>molecular_function</code>, <code>binding</code>, and <code>protein binding</code>—appear tens of thousands of times, while others occur only once. Labels like <code>molecular_function</code> are arguably overly generic and provide little meaningful information, making them unhelpful for machine learning. We’ll filter these out in a later step.
    </p>
    <p>Next, let’s load the protein sequences associated with each protein ID. This information is stored in the file <em>train_sequences.fasta</em>, a standard format for representing biological sequences such as proteins and DNA. We can use BioPython’s <code>SeqIO</code> module to parse the <em>.fasta</em> file into a format we can work with.
   </p>
   <div data-type="note" epub:type="note"><h6>Note</h6>
    <p>A quick aside: no one starts out knowing what BioPython’s <code>SeqIO</code> module is, or how <em>.fasta</em> files work, or what GO annotations mean—and that’s completely normal. Working at the intersection of biology and machine learning means constantly encountering new tools and terminology. Frequent looking up of new terms and tools is not just OK, it’s expected.
    </p>
   </div>
   <p>
    We’ll convert the <em>.fasta</em> sequences into a pandas dataframe to make them easier to manipulate:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">Bio</code> <code class="kn">import</code> <code class="n">SeqIO</code>

<code class="n">sequences_file</code> <code class="o">=</code> <code class="n">assets</code><code class="p">(</code><code class="s2">"proteins/datasets/train_sequences.fasta"</code><code class="p">)</code>
<code class="n">fasta_sequences</code> <code class="o">=</code> <code class="n">SeqIO</code><code class="o">.</code><code class="n">parse</code><code class="p">(</code><code class="nb">open</code><code class="p">(</code><code class="n">sequences_file</code><code class="p">),</code> <code class="s2">"fasta"</code><code class="p">)</code>

<code class="n">data</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">fasta</code> <code class="ow">in</code> <code class="n">fasta_sequences</code><code class="p">:</code>
  <code class="n">data</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>
    <code class="p">{</code>
      <code class="s2">"EntryID"</code><code class="p">:</code> <code class="n">fasta</code><code class="o">.</code><code class="n">id</code><code class="p">,</code>
      <code class="s2">"Sequence"</code><code class="p">:</code> <code class="nb">str</code><code class="p">(</code><code class="n">fasta</code><code class="o">.</code><code class="n">seq</code><code class="p">),</code>
      <code class="s2">"Length"</code><code class="p">:</code> <code class="nb">len</code><code class="p">(</code><code class="n">fasta</code><code class="o">.</code><code class="n">seq</code><code class="p">),</code>
    <code class="p">}</code>
  <code class="p">)</code>
<code class="n">sequence_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">sequence_df</code><code class="p">)</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">           EntryID             Sequence  Length
0           P20536  MNSVTVSHAPYTITYH...     218
1           O73864  MTEYRNFLLLFITSLS...     354
2           O95231  MRLSSSPPRGPQQLSS...     258
...            ...                  ...     ...
142243      Q5RGB0  MADKGPILTSVIIFYL...     448
142244  A0A2R8QMZ5  MGRKKIQITRIMDERN...     459
142245  A0A8I6GHU0  HCISSLKLTAFFKRSF...     138

[142246 rows x 3 columns]
</pre>
   
   <p>We’ve also computed the length of each sequence, since protein lengths can vary widely and this information will be useful later when filtering data.</p>
<p>One important detail: the CAFA dataset includes proteins from many different organisms. To isolate human proteins, we’ll use the associated taxonomy file provided in the download:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">taxonomy_file</code> <code class="o">=</code> <code class="n">assets</code><code class="p">(</code><code class="s2">"proteins/datasets/train_taxonomy.tsv.zip"</code><code class="p">)</code>
<code class="n">taxonomy</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="n">taxonomy_file</code><code class="p">,</code> <code class="n">sep</code><code class="o">=</code><code class="s2">"</code><code class="se">\t</code><code class="s2">"</code><code class="p">,</code> <code class="n">compression</code><code class="o">=</code><code class="s2">"infer"</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">taxonomy</code><code class="p">)</code>
</pre>

<p>Output:</p>
       <pre data-type="programlisting">           EntryID  taxonomyID
0           Q8IXT2        9606
1           Q04418      559292
2           A8DYA3        7227
...            ...         ...
142243  A0A2R8QBB1        7955
142244      P0CT72      284812
142245      Q9NZ43        9606

[142246 rows x 2 columns]
</pre>
      
     
   
   <p>This file contains a taxonomy ID (<code>taxonomyID</code>) for each protein, based on NCBI’s organism classification system. We’ll merge this onto our sequence dataframe and keep only proteins with <code>taxonomyID == 9606</code>, which corresponds to <em>Homo sapiens</em>:
   </p>

          <pre data-type="programlisting" data-code-language="python"><code class="n">sequence_df</code> <code class="o">=</code> <code class="n">sequence_df</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">taxonomy</code><code class="p">,</code> <code class="n">on</code><code class="o">=</code><code class="s2">"EntryID"</code><code class="p">)</code>
<code class="n">sequence_df</code> <code class="o">=</code> <code class="n">sequence_df</code><code class="p">[</code><code class="n">sequence_df</code><code class="p">[</code><code class="s2">"taxonomyID"</code><code class="p">]</code> <code class="o">==</code> <code class="mi">9606</code><code class="p">]</code>
</pre>
  
    <p>Now let’s get an overview of the number of unique proteins and molecular function terms in our filtered dataset:</p>
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">sequence_df</code> <code class="o">=</code> <code class="n">sequence_df</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">labels</code><code class="p">,</code> <code class="n">on</code><code class="o">=</code><code class="s2">"EntryID"</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code>
  <code class="sa">f</code><code class="s1">'Dataset contains </code><code class="si">{</code><code class="n">sequence_df</code><code class="p">[</code><code class="s2">"EntryID"</code><code class="p">]</code><code class="o">.</code><code class="n">nunique</code><code class="p">()</code><code class="si">}</code><code class="s1"> human proteins '</code>
  <code class="sa">f</code><code class="s1">'with </code><code class="si">{</code><code class="n">sequence_df</code><code class="p">[</code><code class="s2">"term"</code><code class="p">]</code><code class="o">.</code><code class="n">nunique</code><code class="p">()</code><code class="si">}</code><code class="s1"> molecular functions.'</code>
<code class="p">)</code>
</pre>
 
<p>Output:</p>
       <pre data-type="programlisting">Dataset contains 16336 human proteins with 4101 molecular functions.
</pre>
      
<p>Let’s also take a look at the resulting <code>sequence_df</code> after merging in the function labels:</p>

<pre data-type="programlisting" data-code-language="python"><code class="nb">print</code><code class="p">(</code><code class="n">sequence_df</code><code class="p">)</code></pre>
 
<p>Output:</p>
<pre data-type="programlisting">       EntryID             Sequence  Length  taxonomyID        term aspect  \
0       O95231  MRLSSSPPRGPQQLSS...     258        9606  GO:0003676    MFO   
1       O95231  MRLSSSPPRGPQQLSS...     258        9606  GO:1990837    MFO   
2       O95231  MRLSSSPPRGPQQLSS...     258        9606  GO:0001216    MFO   
...        ...                  ...     ...         ...         ...    ...   
152523  Q86TI6  MGAAAVRWHLCVLLAL...     347        9606  GO:0005515    MFO   
152524  Q86TI6  MGAAAVRWHLCVLLAL...     347        9606  GO:0005488    MFO   
152525  Q86TI6  MGAAAVRWHLCVLLAL...     347        9606  GO:0003674    MFO   

                description  
0       nucleic acid bin...  
1       sequence-specifi...  
2       DNA-binding tran...  
...                     ...  
152523      protein binding  
152524              binding  
152525   molecular_function  

[152526 rows x 7 columns]
</pre>   
   
   <p>From this table, we can already see that many proteins are associated with multiple molecular functions. To quantify this, we examine the distribution of the number of functions per protein in <a data-type="xref" href="#functions-per-protein">Figure 2-12</a>:</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">sequence_df</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s2">"EntryID"</code><code class="p">)[</code><code class="s2">"term"</code><code class="p">]</code><code class="o">.</code><code class="n">nunique</code><code class="p">()</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code>
  <code class="n">bins</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code> <code class="n">color</code><code class="o">=</code><code class="s2">"grey"</code><code class="p">,</code> <code class="n">log</code><code class="o">=</code><code class="kc">True</code>
<code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Number of Molecular Function Annotations per Protein"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Frequency (log scale)"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"Distribution of Function Counts per Protein"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code>
</pre>
      
     
    
    
     <figure><div id="functions-per-protein" class="figure">
      <img alt="" src="assets/dlfb_0212.png" width="600" height="343"/>
      <h6><span class="label">Figure 2-12. </span>Distribution of the number of molecular functions annotated per protein. The y-axis is shown on a logarithmic scale to make rare cases more visible. While most proteins have fewer than 20 annotated functions, a small number of proteins are associated with more than 50 distinct molecular roles.
      </h6>
     </div></figure>
    
   
   <p>This pattern reflects a complex biological reality: while many proteins carry out a single, well-defined function, others are involved in a wide variety of molecular roles. For example, some proteins act as enzymes, bind to other molecules, and participate in multiple pathways. From a machine learning perspective, this means our model must be able to assign multiple function labels to a single protein and also cope with the fact that some labels are much rarer than others.
   </p>
   <p>Let’s now take a closer look at the most frequent molecular function labels. Some terms are so broad and universally assigned that they offer little meaningful insight. For example, <code>molecular function</code> applies to nearly all proteins, <code>binding</code> covers 93%, and <code>protein binding</code> appears in 89% of cases. These labels will tend to dominate the loss during training and can cause the model to fixate on predicting them at the expense of more meaningful functions. As a dataset preprocessing step, we’ll explicitly remove these overly generic terms:</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">uninteresting_functions</code> <code class="o">=</code> <code class="p">[</code>
  <code class="s2">"GO:0003674"</code><code class="p">,</code>  <code class="c1"># "molecular function". Applies to 100% of proteins.</code>
  <code class="s2">"GO:0005488"</code><code class="p">,</code>  <code class="c1"># "binding". Applies to 93% of proteins.</code>
  <code class="s2">"GO:0005515"</code><code class="p">,</code>  <code class="c1"># "protein binding". Applies to 89% of proteins.</code>
<code class="p">]</code>

<code class="n">sequence_df</code> <code class="o">=</code> <code class="n">sequence_df</code><code class="p">[</code><code class="o">~</code><code class="n">sequence_df</code><code class="p">[</code><code class="s2">"term"</code><code class="p">]</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">uninteresting_functions</code><code class="p">)]</code>
<code class="n">sequence_df</code><code class="o">.</code><code class="n">shape</code>
</pre>
      
     
    
    
     
 <p>Output:</p>     
       <pre data-type="programlisting">(106501, 7)
</pre>
      
     
    
   
   <p>On the opposite end of the spectrum, some molecular functions are extremely rare—for example, <code>GO:0099609</code> (microtubule lateral binding) appears only once. To learn meaningful associations, our model needs enough training examples per function. So we’ll filter out the rarest labels and keep only those that appear in at least 50 proteins:</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">common_functions</code> <code class="o">=</code> <code class="p">(</code>
  <code class="n">sequence_df</code><code class="p">[</code><code class="s2">"term"</code><code class="p">]</code>
  <code class="o">.</code><code class="n">value_counts</code><code class="p">()[</code><code class="n">sequence_df</code><code class="p">[</code><code class="s2">"term"</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code> <code class="o">&gt;=</code> <code class="mi">50</code><code class="p">]</code>
  <code class="o">.</code><code class="n">index</code>
<code class="p">)</code>

<code class="n">sequence_df</code> <code class="o">=</code> <code class="n">sequence_df</code><code class="p">[</code><code class="n">sequence_df</code><code class="p">[</code><code class="s2">"term"</code><code class="p">]</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">common_functions</code><code class="p">)]</code>
<code class="n">sequence_df</code><code class="p">[</code><code class="s2">"term"</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code>
</pre>
      
     
    
    
     
 <p>Output:</p>     
       <pre data-type="programlisting">term
GO:0003824    3875
GO:1901363    2943
GO:0003676    2469
              ... 
GO:0031490      51
GO:0019003      50
GO:0015179      50
Name: count, Length: 303, dtype: int64
</pre>
      
     
    
   
   <p>This gives us a cleaner set of function labels that are more amenable to learning.
   </p>
   <div data-type="note" epub:type="note"><h6>Note</h6><p>Thresholds used during data processing—like how many times a label must appear to be included—are somewhat arbitrary, but they can significantly affect model performance. These decisions are effectively hyperparameters and should be tuned based on the specific task, dataset size, and model capacity.</p></div>
  <p>Now we’ll reshape the dataframe so that each row corresponds to one protein, and each column corresponds to a molecular function label. We’ll use the <code>pivot</code> function in pandas to create this multilabel format:</p>
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">sequence_df</code> <code class="o">=</code> <code class="p">(</code>
  <code class="n">sequence_df</code><code class="p">[[</code><code class="s2">"EntryID"</code><code class="p">,</code> <code class="s2">"Sequence"</code><code class="p">,</code> <code class="s2">"Length"</code><code class="p">,</code> <code class="s2">"term"</code><code class="p">]]</code>
  <code class="o">.</code><code class="n">assign</code><code class="p">(</code><code class="n">value</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
  <code class="o">.</code><code class="n">pivot</code><code class="p">(</code>
    <code class="n">index</code><code class="o">=</code><code class="p">[</code><code class="s2">"EntryID"</code><code class="p">,</code> <code class="s2">"Sequence"</code><code class="p">,</code> <code class="s2">"Length"</code><code class="p">],</code> <code class="n">columns</code><code class="o">=</code><code class="s2">"term"</code><code class="p">,</code> <code class="n">values</code><code class="o">=</code><code class="s2">"value"</code>
  <code class="p">)</code>
  <code class="o">.</code><code class="n">fillna</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
  <code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">int</code><code class="p">)</code>
  <code class="o">.</code><code class="n">reset_index</code><code class="p">()</code>
<code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">sequence_df</code><code class="p">)</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">term      EntryID             Sequence  Length  GO:0000166  GO:0000287  ...  \
0      A0A024R6B2  MIASCLCYLLLPATRL...     670           0           0  ...   
1      A0A087WUI6  MSRKISKESKKVNISS...     698           0           0  ...   
2      A0A087X1C5  MGLEALVPLAMIVAIF...     515           0           0  ...   
...           ...                  ...     ...         ...         ...  ...   
10706      Q9Y6Z7  MNGFASLLRRNQFILL...     277           0           0  ...   
10707      X5D778  MPKGGCPKAPQQEELP...     421           0           0  ...   
10708      X5D7E3  MLDLTSRGQVGTSRRM...     237           0           0  ...   

term   GO:1901702  GO:1901981  GO:1902936  GO:1990782  GO:1990837  
0               0           0           0           0           0  
1               0           0           0           0           0  
2               0           0           0           0           0  
...           ...         ...         ...         ...         ...  
10706           0           0           0           0           0  
10707           0           0           0           0           0  
10708           0           0           0           0           0  

[10709 rows x 306 columns]
</pre>
     
    
   
   <p>Great—this dataset is now in a format that’s almost ready for machine learning. Before we move on, let’s run a few final sanity checks.
   </p>
   <p>First, how many unique proteins do we have?
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">sequence_df</code><code class="p">[</code><code class="s2">"EntryID"</code><code class="p">]</code><code class="o">.</code><code class="n">nunique</code><code class="p">()</code>
</pre>
      
     
    
    
     
<p class="pagebreak-before">Output:</p>      
       <pre data-type="programlisting">10709
</pre>
      
     
    
   
   <p>
    This number is in the right ballpark. There are roughly 21,000 protein-coding genes in the human genome, and since we applied several filtering steps, we expect a somewhat smaller number. It’s always worth keeping rough order-of-magnitude expectations in mind—if we saw 1,000 or 1,000,000 here, we’d suspect something was off.
   </p>
   <p>Next, let’s check whether any protein sequences are duplicated:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">sequence_df</code><code class="p">[</code><code class="s2">"Sequence"</code><code class="p">]</code><code class="o">.</code><code class="n">nunique</code><code class="p">()</code>
</pre>
      
     
    
    
     
  <p>Output:</p>    
       <pre data-type="programlisting">10698
</pre>
      
     
    
   
   <p>It seems that a few protein sequences are repeated. For example, the entries <code>P0DP23</code>, <code>P0DP24</code>, and <code>P0DP25</code> all share the same sequence:</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="nb">print</code><code class="p">(</code><code class="n">sequence_df</code><code class="p">[</code><code class="n">sequence_df</code><code class="p">[</code><code class="s2">"EntryID"</code><code class="p">]</code><code class="o">.</code><code class="n">isin</code><code class="p">([</code><code class="s2">"P0DP23"</code><code class="p">,</code> <code class="s2">"P0DP24"</code><code class="p">,</code> <code class="s2">"P0DP25"</code><code class="p">])])</code>
</pre>
   
<p>Output:</p>
       <pre data-type="programlisting">term EntryID             Sequence  Length  GO:0000166  GO:0000287  ...  \
1945  P0DP23  MADQLTEEQIAEFKEA...     149           0           0  ...   
1946  P0DP24  MADQLTEEQIAEFKEA...     149           0           0  ...   
1947  P0DP25  MADQLTEEQIAEFKEA...     149           0           0  ...   

term  GO:1901702  GO:1901981  GO:1902936  GO:1990782  GO:1990837  
1945           0           0           0           0           0  
1946           0           0           0           0           0  
1947           0           0           0           0           0  

[3 rows x 306 columns]
</pre>
     

   <p>These seem to be legitimate biological duplicates—proteins with different Uniprot identifiers but identical sequences—so we’ll keep them in the dataset.</p>
<p>At this point, we have a final dataset linking 10,709 human proteins to one or more of 303 molecular functions.
   </p>
   <p>Since our simple mean embedding approach can be quite memory intensive, we’ll filter the dataset to include only proteins with a maximum length of 500 amino acids. This helps avoid out-of-memory errors during model inference and training:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="nb">print</code><code class="p">(</code><code class="n">sequence_df</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="n">sequence_df</code> <code class="o">=</code> <code class="n">sequence_df</code><code class="p">[</code><code class="n">sequence_df</code><code class="p">[</code><code class="s2">"Length"</code><code class="p">]</code> <code class="o">&lt;=</code> <code class="mi">500</code><code class="p">]</code>
<code class="nb">print</code><code class="p">(</code><code class="n">sequence_df</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
</pre>
      
     
    
    
     
 <p class="pagebreak-before">Output:</p>     
       <pre data-type="programlisting">(10709, 306)
(5957, 306)
</pre>
      
     
    <p>This roughly halves the dataset, which is perfectly fine for initial prototyping. You can always remove this constraint later if time and memory allow.<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html23" data-type="indexterm" id="id579"/><a contenteditable="false" data-primary="" data-startref="ch02_proteins.html22" data-type="indexterm" id="id580"/></p>
<p>Now that we have a clean and compact dataset, let’s process it further for compatibility with machine learning.</p>
   
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Splitting the Dataset into Subsets"><div class="sect2" id="splitting-the-dataset-into-subsets">
   <h2>Splitting the Dataset into Subsets</h2>
   <p>
    <a contenteditable="false" data-primary="CAFA3 dataset" data-secondary="splitting the dataset into subsets" data-type="indexterm" id="id581"/><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="data preparation for predicting protein function from sequence" data-tertiary="splitting the dataset into subsets" data-type="indexterm" id="id582"/>We will split our dataset into three distinct subsets:
   </p>
   <dl>
    <dt>Training set</dt>
    <dd><p>Used to fit the model. The model sees this data during training and uses it to learn patterns.</p></dd>
    <dt>Validation set</dt>
    <dd><p>Used to evaluate the model’s performance during development. We use this to tune hyperparameters and compare model variants.</p></dd>
    <dt>Test set</dt>
    <dd><p>Used only once, for final evaluation. Crucially, we avoid using this data to guide model design decisions. It serves as our best estimate of how well the model would generalize to completely unseen data.</p></dd>
   </dl>
<p>We’ll split the proteins by their <code>EntryID</code>, ensuring that each protein appears in only one subset:</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="c1"># 60% of the proteins will go into the training set.</code>
<code class="n">train_sequence_ids</code><code class="p">,</code> <code class="n">valid_test_sequence_ids</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
  <code class="nb">list</code><code class="p">(</code><code class="nb">set</code><code class="p">(</code><code class="n">sequence_df</code><code class="p">[</code><code class="s2">"EntryID"</code><code class="p">])),</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.40</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code>
<code class="p">)</code>

<code class="c1"># Split the remaining 40% evenly between validation and test sets.</code>
<code class="n">valid_sequence_ids</code><code class="p">,</code> <code class="n">test_sequence_ids</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
  <code class="n">valid_test_sequence_ids</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.50</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code>
<code class="p">)</code>
</pre>

<p>Now we’ll extract the rows for each split from our dataframe <code>sequence_df</code>:</p>
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">sequence_splits</code> <code class="o">=</code> <code class="p">{</code>
  <code class="s2">"train"</code><code class="p">:</code> <code class="n">sequence_df</code><code class="p">[</code><code class="n">sequence_df</code><code class="p">[</code><code class="s2">"EntryID"</code><code class="p">]</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">train_sequence_ids</code><code class="p">)],</code>
  <code class="s2">"valid"</code><code class="p">:</code> <code class="n">sequence_df</code><code class="p">[</code><code class="n">sequence_df</code><code class="p">[</code><code class="s2">"EntryID"</code><code class="p">]</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">valid_sequence_ids</code><code class="p">)],</code>
  <code class="s2">"test"</code><code class="p">:</code> <code class="n">sequence_df</code><code class="p">[</code><code class="n">sequence_df</code><code class="p">[</code><code class="s2">"EntryID"</code><code class="p">]</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">test_sequence_ids</code><code class="p">)],</code>
<code class="p">}</code>

<code class="k">for</code> <code class="n">split</code><code class="p">,</code> <code class="n">df</code> <code class="ow">in</code> <code class="n">sequence_splits</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
  <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">split</code><code class="si">}</code><code class="s2"> has </code><code class="si">{</code><code class="nb">len</code><code class="p">(</code><code class="n">df</code><code class="p">)</code><code class="si">}</code><code class="s2"> entries."</code><code class="p">)</code>
</pre>

<p>Output:</p>
       <pre data-type="programlisting">train has 3574 entries.
valid has 1191 entries.
test has 1192 entries.
</pre>
      
 <p>This gives us clean, nonoverlapping training, validation, and test sets—each containing a subset of proteins we’ll use throughout model development and evaluation.</p>   
    
   
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Converting Protein Sequences into Their Mean Embeddings"><div class="sect2" id="converting-protein-sequences-into-their-mean-embedding">
   <h2>Converting Protein Sequences into Their Mean Embeddings</h2>
   <p>
    <a contenteditable="false" data-primary="embeddings" data-secondary="converting protein sequences into mean embeddings" data-type="indexterm" id="ch02_proteins.html24"/><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="data preparation for predicting protein function from sequence" data-tertiary="converting protein sequences into their mean embeddings" data-type="indexterm" id="ch02_proteins.html25"/>We will now convert the sequences from each dataset split into their corresponding mean embeddings, just as we did earlier. Since this step can be time-consuming—especially with larger models—it’s worth thinking about how to do it efficiently. Using a GPU can significantly speed up computation, but we can also avoid repeating work by computing the embeddings only once, storing them to disk, and loading them later.
   </p>
   <p>To make this process more convenient, we’ll use a pair of helper functions to store and load sequence embeddings:</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">store_sequence_embeddings</code><code class="p">(</code>
  <code class="n">sequence_df</code><code class="p">:</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">,</code>
  <code class="n">store_prefix</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code>
  <code class="n">tokenizer</code><code class="p">:</code> <code class="n">PreTrainedTokenizer</code><code class="p">,</code>
  <code class="n">model</code><code class="p">:</code> <code class="n">PreTrainedModel</code><code class="p">,</code>
  <code class="n">batch_size</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">64</code><code class="p">,</code>
  <code class="n">force</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">False</code><code class="p">,</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="kc">None</code><code class="p">:</code>
  <code class="sd">"""Extract and store mean embeddings for each protein sequence."""</code>
  <code class="n">model_name</code> <code class="o">=</code> <code class="nb">str</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">name_or_path</code><code class="p">)</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"/"</code><code class="p">,</code> <code class="s2">"_"</code><code class="p">)</code>
  <code class="n">store_file</code> <code class="o">=</code> <code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">store_prefix</code><code class="si">}</code><code class="s2">_</code><code class="si">{</code><code class="n">model_name</code><code class="si">}</code><code class="s2">.feather"</code>

  <code class="k">if</code> <code class="ow">not</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">exists</code><code class="p">(</code><code class="n">store_file</code><code class="p">)</code> <code class="ow">or</code> <code class="n">force</code><code class="p">:</code>
    <code class="n">device</code> <code class="o">=</code> <code class="n">get_device</code><code class="p">()</code>

    <code class="c1"># Iterate through protein dataframe in batches, extracting embeddings.</code>
    <code class="n">n_batches</code> <code class="o">=</code> <code class="n">ceil</code><code class="p">(</code><code class="n">sequence_df</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">/</code> <code class="n">batch_size</code><code class="p">)</code>
    <code class="n">batches</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">]</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_batches</code><code class="p">):</code>
      <code class="n">batch_seqs</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code>
        <code class="n">sequence_df</code><code class="p">[</code><code class="s2">"Sequence"</code><code class="p">][</code><code class="n">i</code> <code class="o">*</code> <code class="n">batch_size</code> <code class="p">:</code> <code class="p">(</code><code class="n">i</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code> <code class="o">*</code> <code class="n">batch_size</code><code class="p">]</code>
      <code class="p">)</code>
      <code class="n">batches</code><code class="o">.</code><code class="n">extend</code><code class="p">(</code><code class="n">get_mean_embeddings</code><code class="p">(</code><code class="n">batch_seqs</code><code class="p">,</code> <code class="n">tokenizer</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">device</code><code class="p">))</code>

    <code class="c1"># Store each of the embedding values in a separate column in the dataframe.</code>
    <code class="n">embeddings</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">vstack</code><code class="p">(</code><code class="n">batches</code><code class="p">))</code>
    <code class="n">embeddings</code><code class="o">.</code><code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="sa">f</code><code class="s2">"ME:</code><code class="si">{</code><code class="nb">int</code><code class="p">(</code><code class="n">i</code><code class="p">)</code><code class="o">+</code><code class="mi">1</code><code class="si">}</code><code class="s2">"</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">embeddings</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])]</code>
    <code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">sequence_df</code><code class="o">.</code><code class="n">reset_index</code><code class="p">(</code><code class="n">drop</code><code class="o">=</code><code class="kc">True</code><code class="p">),</code> <code class="n">embeddings</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
    <code class="n">df</code><code class="o">.</code><code class="n">to_feather</code><code class="p">(</code><code class="n">store_file</code><code class="p">)</code>


<code class="k">def</code> <code class="nf">load_sequence_embeddings</code><code class="p">(</code>
  <code class="n">store_file_prefix</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">model_checkpoint</code><code class="p">:</code> <code class="nb">str</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">:</code>
  <code class="sd">"""Load stored embedding DataFrame from disk."""</code>
  <code class="n">model_name</code> <code class="o">=</code> <code class="n">model_checkpoint</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"/"</code><code class="p">,</code> <code class="s2">"_"</code><code class="p">)</code>
  <code class="n">store_file</code> <code class="o">=</code> <code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">store_file_prefix</code><code class="si">}</code><code class="s2">_</code><code class="si">{</code><code class="n">model_name</code><code class="si">}</code><code class="s2">.feather"</code>
  <code class="k">return</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_feather</code><code class="p">(</code><code class="n">store_file</code><code class="p">)</code>
</pre>
      
     
    
   
   <p>Let’s use the more powerful (but computationally expensive) ESM2 model with 640-dimensional embeddings and store the embeddings for each split using the <code>store_sequence_embeddings</code> function:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">model_checkpoint</code> <code class="o">=</code> <code class="s2">"facebook/esm2_t30_150M_UR50D"</code>
<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_checkpoint</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">EsmModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_checkpoint</code><code class="p">)</code>

<code class="k">for</code> <code class="n">split</code><code class="p">,</code> <code class="n">df</code> <code class="ow">in</code> <code class="n">sequence_splits</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
  <code class="n">store_sequence_embeddings</code><code class="p">(</code>
    <code class="n">sequence_df</code><code class="o">=</code><code class="n">df</code><code class="p">,</code>
    <code class="n">store_prefix</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="sa">f</code><code class="s2">"proteins/datasets/protein_dataset_</code><code class="si">{</code><code class="n">split</code><code class="si">}</code><code class="s2">"</code><code class="p">),</code>
    <code class="n">tokenizer</code><code class="o">=</code><code class="n">tokenizer</code><code class="p">,</code>
    <code class="n">model</code><code class="o">=</code><code class="n">model</code><code class="p">,</code>
  <code class="p">)</code>
</pre>
      
     
    
   
   <p>Once the embeddings are stored, we can load them back into memory whenever needed. Here’s a glimpse of the resulting training dataset that the model will learn from:
   </p>
  
 <pre data-type="programlisting" data-code-language="python"><code class="n">train_df</code> <code class="o">=</code> <code class="n">load_sequence_embeddings</code><code class="p">(</code>
  <code class="n">assets</code><code class="p">(</code><code class="s2">"proteins/datasets/protein_dataset_train"</code><code class="p">),</code>
  <code class="n">model_checkpoint</code><code class="o">=</code><code class="n">model_checkpoint</code><code class="p">,</code>
<code class="p">)</code>

<code class="nb">print</code><code class="p">(</code><code class="n">train_df</code><code class="p">)</code>
</pre>     
     
<p>Output:</p>      
       <pre data-type="programlisting">         EntryID             Sequence  Length  GO:0000166  GO:0000287  ...  \
0     A0A0C4DG62  MAHVGSRKRSRSRSRS...     218           0           0  ...   
1     A0A1B0GTB2  MVITSENDEDRGGQEK...      48           0           0  ...   
2         A0AVI4  MDSPEVTFTLAYLVFA...     362           0           0  ...   
...          ...                  ...     ...         ...         ...  ...   
3571      Q9Y6W5  MPLVTRNIEPRHLCRQ...     498           0           0  ...   
3572      Q9Y6W6  MPPSPLDDRVVVALSR...     482           0           0  ...   
3573      Q9Y6Y9  MLPFLFFSTLFSSIFT...     160           0           0  ...   

        ME:636    ME:637    ME:638    ME:639    ME:640  
0     0.062926  0.040286  0.030008 -0.033614  0.023891  
1     0.129815 -0.044294  0.023842 -0.020635  0.125583  
2     0.153848 -0.075747  0.024440 -0.123321  0.020945  
...        ...       ...       ...       ...       ...  
3571 -0.001535 -0.084161 -0.014317 -0.141801 -0.040719  
3572  0.120192 -0.086032 -0.016481 -0.108710 -0.077937  
3573  0.114847 -0.028570  0.084638  0.038610  0.087047  

[3574 rows x 946 columns]
</pre>   
       
<p>You’ll notice a series of columns labeled <code>ME:1</code> through <code>ME:640</code>. These represent the mean-pooled hidden states from the final layer of the ESM2 model—effectively a fixed-length numerical summary of each protein sequence. These embeddings capture biochemical and structural information learned during pretraining and will serve as the input features for our classifier.</p>

<p>This dataframe becomes the input to a <code>convert_to_tfds</code> function, which we’ve defined to make it easier to prepare the datasets for each split:</p>
      
<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">tensorflow</code> <code class="k">as</code> <code class="nn">tf</code>

<code class="k">def</code> <code class="nf">convert_to_tfds</code><code class="p">(</code>
  <code class="n">df</code><code class="p">:</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">,</code>
  <code class="n">embeddings_prefix</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s2">"ME:"</code><code class="p">,</code>
  <code class="n">target_prefix</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s2">"GO:"</code><code class="p">,</code>
  <code class="n">is_training</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">False</code><code class="p">,</code>
  <code class="n">shuffle_buffer</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">50</code><code class="p">,</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="p">:</code>
  <code class="sd">"""Convert embedding DataFrame into a TensorFlow dataset."""</code>
  <code class="n">dataset</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="o">.</code><code class="n">from_tensor_slices</code><code class="p">(</code>
    <code class="p">{</code>
      <code class="s2">"embedding"</code><code class="p">:</code> <code class="n">df</code><code class="o">.</code><code class="n">filter</code><code class="p">(</code><code class="n">regex</code><code class="o">=</code><code class="sa">f</code><code class="s2">"^</code><code class="si">{</code><code class="n">embeddings_prefix</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code class="n">to_numpy</code><code class="p">(),</code>
      <code class="s2">"target"</code><code class="p">:</code> <code class="n">df</code><code class="o">.</code><code class="n">filter</code><code class="p">(</code><code class="n">regex</code><code class="o">=</code><code class="sa">f</code><code class="s2">"^</code><code class="si">{</code><code class="n">target_prefix</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code class="n">to_numpy</code><code class="p">(),</code>
    <code class="p">}</code>
  <code class="p">)</code>
  <code class="k">if</code> <code class="n">is_training</code><code class="p">:</code>
    <code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="n">shuffle_buffer</code><code class="p">)</code><code class="o">.</code><code class="n">repeat</code><code class="p">()</code>
  <code class="k">return</code> <code class="n">dataset</code>
</pre>
 
<p>Let’s now use our <code>convert_to_tfds</code> function to build a TensorFlow-compatible dataset from the training DataFrame:</p>
  
<pre data-type="programlisting" data-code-language="python"><code class="n">train_ds</code> <code class="o">=</code> <code class="n">convert_to_tfds</code><code class="p">(</code><code class="n">train_df</code><code class="p">,</code> <code class="n">is_training</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>
      
<p>Fetching a batch of data from these datasets is straightforward. We just batch the dataset, convert it to a NumPy iterator, and retrieve a batch by calling <code>next</code>:</p>
  
<pre data-type="programlisting" data-code-language="python"><code class="n">batch_size</code> <code class="o">=</code> <code class="mi">32</code>

<code class="n">batch</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="n">train_ds</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="n">batch_size</code><code class="p">)</code><code class="o">.</code><code class="n">as_numpy_iterator</code><code class="p">())</code>
<code class="n">batch</code><code class="p">[</code><code class="s2">"embedding"</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="p">,</code> <code class="n">batch</code><code class="p">[</code><code class="s2">"target"</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code>
</pre>
 
<p>Output:</p>
<pre data-type="programlisting">((32, 640), (32, 303))</pre>

<p>These shapes confirm that each input is a 640-dimensional embedding vector (from the ESM2 model), and each target is a 303-dimensional binary vector representing the presence or absence of each molecular function label.</p>
      
 <div data-type="tip"><h6>Tip</h6>
  <p>Because the training dataset includes <code>.repeat()</code>, it yields batches indefinitely by looping over the data. This is useful for training, where we want to cycle through the dataset multiple times. In contrast, the validation and test datasets are not repeated—so their batches will eventually be exhausted, which is exactly what we want during evaluation, where each example should be seen only once.</p>
</div>    

<p>To streamline the dataset setup, we’ve wrapped the entire pipeline into a single helper function, <code>build_dataset</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">build_dataset</code><code class="p">(</code>
  <code class="n">store_file_prefix</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">model_checkpoint</code><code class="p">:</code> <code class="nb">str</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="p">]:</code>
  <code class="sd">"""Build train/valid/test TensorFlow datasets from stored embeddings."""</code>
  <code class="n">dataset_splits</code> <code class="o">=</code> <code class="p">{}</code>

  <code class="k">for</code> <code class="n">split</code> <code class="ow">in</code> <code class="p">[</code><code class="s2">"train"</code><code class="p">,</code> <code class="s2">"valid"</code><code class="p">,</code> <code class="s2">"test"</code><code class="p">]:</code>
    <code class="n">dataset_splits</code><code class="p">[</code><code class="n">split</code><code class="p">]</code> <code class="o">=</code> <code class="n">convert_to_tfds</code><code class="p">(</code>
      <code class="n">df</code><code class="o">=</code><code class="n">load_sequence_embeddings</code><code class="p">(</code>
        <code class="n">store_file_prefix</code><code class="o">=</code><code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">store_file_prefix</code><code class="si">}</code><code class="s2">_</code><code class="si">{</code><code class="n">split</code><code class="si">}</code><code class="s2">"</code><code class="p">,</code>
        <code class="n">model_checkpoint</code><code class="o">=</code><code class="n">model_checkpoint</code><code class="p">,</code>
      <code class="p">),</code>
      <code class="n">is_training</code><code class="o">=</code><code class="p">(</code><code class="n">split</code> <code class="o">==</code> <code class="s2">"train"</code><code class="p">),</code>
    <code class="p">)</code>
  <code class="k">return</code> <code class="n">dataset_splits</code>
</pre>

<p>This function loads the saved mean embeddings from disk for all three splits and constructs <code>tf.data.Dataset</code> objects that are ready for training<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html25" data-type="indexterm" id="id583"/><a contenteditable="false" data-primary="" data-startref="ch02_proteins.html24" data-type="indexterm" id="id584"/>:<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html21" data-type="indexterm" id="id585"/></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dataset_splits</code> <code class="o">=</code> <code class="n">build_dataset</code><code class="p">(</code>
  <code class="n">assets</code><code class="p">(</code><code class="s2">"proteins/datasets/protein_dataset"</code><code class="p">),</code> <code class="n">model_checkpoint</code><code class="o">=</code><code class="n">model_checkpoint</code>
<code class="p">)</code>
</pre>

<p>With this, we now have our data fully preprocessed and ready to use in training a model.</p>

  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Training the Model"><div class="sect1" id="training-the-model">
  <h1>Training the Model</h1>

  <p><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="training the model" data-type="indexterm" id="ch02_proteins.html26"/><a contenteditable="false" data-primary="training" data-secondary="for learning language of proteins" data-type="indexterm" id="ch02_proteins.html27"/>We will now train a simple <a href="https://oreil.ly/MjH5C">Flax</a> linear model on top of the mean protein embeddings. Recall that each protein sequence has a variable length, but we’ve already transformed them into fixed-size embeddings. Our goal is to predict which of the 303 possible molecular functions each protein performs. This is a <em>multilabel classification</em> problem, meaning each protein may be associated with several function labels simultaneously.</p>

 <p><a contenteditable="false" data-primary="multilayer perceptrons (MLPs)" data-secondary="training for predicting protein functions" data-type="indexterm" id="id586"/><a contenteditable="false" data-primary="training" data-secondary="for learning language of proteins" data-tertiary="predicting protein functions" data-type="indexterm" id="id587"/>In this setup, we’ll train a lightweight MLP (multilayer perceptron)—a stack of dense layers with nonlinearities. Importantly, we are not fine-tuning the original ESM2 model: it remains frozen, and our model simply learns on top of its embeddings.</p>

<p>Here’s the model code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">flax.linen</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">from</code> <code class="nn">flax.training</code> <code class="kn">import</code> <code class="n">train_state</code>


<code class="k">class</code> <code class="nc">Model</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Simple MLP for protein function prediction."""</code>

  <code class="n">num_targets</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">dim</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">256</code>

  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
    <code class="sd">"""Apply MLP layers to input features."""</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
      <code class="p">[</code>
        <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dim</code> <code class="o">*</code> <code class="mi">2</code><code class="p">),</code>
        <code class="n">jax</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">gelu</code><code class="p">,</code>
        <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dim</code><code class="p">),</code>
        <code class="n">jax</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">gelu</code><code class="p">,</code>
        <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">num_targets</code><code class="p">),</code>
      <code class="p">]</code>
    <code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">x</code>

  <code class="k">def</code> <code class="nf">create_train_state</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">dummy_input</code><code class="p">,</code> <code class="n">tx</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">TrainState</code><code class="p">:</code>
    <code class="sd">"""Initialize model parameters and return a training state."""</code>
    <code class="n">variables</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">init</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="n">dummy_input</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">TrainState</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
      <code class="n">apply_fn</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">apply</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="n">variables</code><code class="p">[</code><code class="s2">"params"</code><code class="p">],</code> <code class="n">tx</code><code class="o">=</code><code class="n">tx</code>
    <code class="p">)</code>
</pre>

  <p>Some notes on this very lightweight model:</p>
  <ul>
   <li>It uses <code>nn.Sequential</code> to stack layers, which keeps the definition clean and readable for this simple model.</li>
   <li>We use a GELU (Gaussian Error Linear Unit) activation function, which is a smooth, nonlinear alternative to ReLU.</li>
   <li>The final layer is an <code>nn.Dense</code> layer projecting to the number of function labels (<code>num_targets</code>). It returns logits, not probabilities—so we’ll apply a suitable activation (like sigmoid) inside the loss function to convert these logits into predicted probabilities.</li>
   <li>This model is frozen on top of the ESM2 embeddings—meaning it does not update the transformer weights. It learns only to map fixed embeddings to functional labels. This is efficient and interpretable, and it reduces memory usage during training.</li>
  </ul>
  <p>You may also have noticed that we attached a convenience function, <span class="keep-together"><code>create_train_state</code></span>, to the model class for creating a training state. This encapsulates model initialization, parameter registration, and optimizer setup into a single <code>TrainState</code> object. It’s particularly useful because it allows us to construct the training state right when everything needed—the model, dummy input for shape inference, and optimizer config—is readily available.</p>

<p>Let’s instantiate the model with the correct number of output targets, based on how many GO term columns we have in the training dataframe:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">targets</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="n">train_df</code><code class="o">.</code><code class="n">columns</code><code class="p">[</code><code class="n">train_df</code><code class="o">.</code><code class="n">columns</code><code class="o">.</code><code class="n">str</code><code class="o">.</code><code class="n">contains</code><code class="p">(</code><code class="s2">"GO:"</code><code class="p">)])</code>

<code class="n">mlp</code> <code class="o">=</code> <code class="n">Model</code><code class="p">(</code><code class="n">num_targets</code><code class="o">=</code><code class="nb">len</code><code class="p">(</code><code class="n">targets</code><code class="p">))</code>
</pre>

<p>This model is now ready to be trained to predict which molecular functions a protein is involved in, using the precomputed embeddings as input.</p>

  <section data-type="sect2" data-pdf-bookmark="Defining the Training Loop"><div class="sect2" id="defining-the-training-loop">
   <h2>Defining the Training Loop</h2>
   <p>
    <a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="training the model" data-tertiary="defining the training loop" data-type="indexterm" id="ch02_proteins.html28"/><a contenteditable="false" data-primary="training loop" data-secondary="for predicting protein functions" data-type="indexterm" id="ch02_proteins.html29"/>With the model and dataset ready, we can now define a function to perform a single training step. This step includes:
   </p>
   <ul>
    <li>A forward pass through the model</li>
    <li>Computing the loss</li>
    <li>Calculating gradients</li>
    <li>Updating the model parameters using those gradients</li>
   </ul>
  <p>Here’s how we implement it:</p>
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="nd">@jax</code><code class="o">.</code><code class="n">jit</code>
<code class="k">def</code> <code class="nf">train_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">batch</code><code class="p">):</code>
  <code class="sd">"""Run a single training step and update model parameters."""</code>

  <code class="k">def</code> <code class="nf">calculate_loss</code><code class="p">(</code><code class="n">params</code><code class="p">):</code>
    <code class="sd">"""Compute sigmoid cross-entropy loss from logits."""</code>
    <code class="n">logits</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_fn</code><code class="p">({</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">params</code><code class="p">},</code> <code class="n">x</code><code class="o">=</code><code class="n">batch</code><code class="p">[</code><code class="s2">"embedding"</code><code class="p">])</code>
    <code class="n">loss</code> <code class="o">=</code> <code class="n">optax</code><code class="o">.</code><code class="n">sigmoid_binary_cross_entropy</code><code class="p">(</code><code class="n">logits</code><code class="p">,</code> <code class="n">batch</code><code class="p">[</code><code class="s2">"target"</code><code class="p">])</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
    <code class="k">return</code> <code class="n">loss</code>

  <code class="n">grad_fn</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">value_and_grad</code><code class="p">(</code><code class="n">calculate_loss</code><code class="p">,</code> <code class="n">has_aux</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
  <code class="n">loss</code><code class="p">,</code> <code class="n">grads</code> <code class="o">=</code> <code class="n">grad_fn</code><code class="p">(</code><code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">)</code>
  <code class="n">state</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_gradients</code><code class="p">(</code><code class="n">grads</code><code class="o">=</code><code class="n">grads</code><code class="p">)</code>
  <code class="k">return</code> <code class="n">state</code><code class="p">,</code> <code class="n">loss</code>
</pre>
      
     
    
   
   <p class="pagebreak-before">In this setup:
   </p>
  <ul>
   <li>We use a sigmoid activation and binary cross-entropy loss, appropriate for multilabel classification. The logits go through a sigmoid activation, not softmax—because we want independent yes/no predictions for each possible protein function. Remember that each protein could have many functions at once.</li>
   <li><code>@jax.jit</code> compiles the training step for better performance.</li>
  </ul>
  <p>Next, let’s implement some metrics to evaluate how well the model is doing beyond the loss alone, using tools from <code>sklearn</code>:</p>
    
<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">sklearn</code>

<code class="k">def</code> <code class="nf">compute_metrics</code><code class="p">(</code>
  <code class="n">targets</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">,</code> <code class="n">probs</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">,</code> <code class="n">thresh</code><code class="o">=</code><code class="mf">0.5</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">float</code><code class="p">]:</code>
  <code class="sd">"""Compute accuracy, recall, precision, auPRC, and auROC."""</code>
  <code class="k">if</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">targets</code><code class="p">)</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
    <code class="k">return</code> <code class="p">{</code>
      <code class="n">m</code><code class="p">:</code> <code class="mf">0.0</code> <code class="k">for</code> <code class="n">m</code> <code class="ow">in</code> <code class="p">[</code><code class="s2">"accuracy"</code><code class="p">,</code> <code class="s2">"recall"</code><code class="p">,</code> <code class="s2">"precision"</code><code class="p">,</code> <code class="s2">"auprc"</code><code class="p">,</code> <code class="s2">"auroc"</code><code class="p">]</code>
    <code class="p">}</code>
  <code class="k">return</code> <code class="p">{</code>
    <code class="s2">"accuracy"</code><code class="p">:</code> <code class="n">metrics</code><code class="o">.</code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">targets</code><code class="p">,</code> <code class="n">probs</code> <code class="o">&gt;=</code> <code class="n">thresh</code><code class="p">),</code>
    <code class="s2">"recall"</code><code class="p">:</code> <code class="n">metrics</code><code class="o">.</code><code class="n">recall_score</code><code class="p">(</code><code class="n">targets</code><code class="p">,</code> <code class="n">probs</code> <code class="o">&gt;=</code> <code class="n">thresh</code><code class="p">)</code><code class="o">.</code><code class="n">item</code><code class="p">(),</code>
    <code class="s2">"precision"</code><code class="p">:</code> <code class="n">metrics</code><code class="o">.</code><code class="n">precision_score</code><code class="p">(</code>
      <code class="n">targets</code><code class="p">,</code>
      <code class="n">probs</code> <code class="o">&gt;=</code> <code class="n">thresh</code><code class="p">,</code>
      <code class="n">zero_division</code><code class="o">=</code><code class="mf">0.0</code><code class="p">,</code>
    <code class="p">)</code><code class="o">.</code><code class="n">item</code><code class="p">(),</code>
    <code class="s2">"auprc"</code><code class="p">:</code> <code class="n">metrics</code><code class="o">.</code><code class="n">average_precision_score</code><code class="p">(</code><code class="n">targets</code><code class="p">,</code> <code class="n">probs</code><code class="p">)</code><code class="o">.</code><code class="n">item</code><code class="p">(),</code>
    <code class="s2">"auroc"</code><code class="p">:</code> <code class="n">metrics</code><code class="o">.</code><code class="n">roc_auc_score</code><code class="p">(</code><code class="n">targets</code><code class="p">,</code> <code class="n">probs</code><code class="p">)</code><code class="o">.</code><code class="n">item</code><code class="p">(),</code>
  <code class="p">}</code>
</pre>
<p>We’ll track the following evaluation metrics for each function label:</p>
<dl>
  <dt>Accuracy</dt>
  <dd><p>The fraction of correct predictions across all labels. In multilabel classification with imbalanced data (like this), accuracy can be misleading—most labels are zero, so a model that always predicts “no function” would appear accurate. Still, it’s an intuitive metric and we’ll include it for now.</p></dd>
    <dt>Recall</dt>
  <dd><p>The proportion of actual function labels the model correctly predicted (i.e., true positives/all actual positives). High recall means the model doesn’t miss many true functions.</p></dd>
    <dt class="less_space pagebreak-before">Precision</dt>
  <dd><p>The proportion of predicted function labels that are correct (i.e., true positives/all predicted positives). High precision means the model avoids false alarms.</p></dd>
    <dt>Area under the precision-recall curve (auPRC)</dt>
  <dd><p>Summarizes the tradeoff between precision and recall at different thresholds. Particularly useful in highly imbalanced settings like this one.</p></dd>
    <dt>Area under the receiver operating characteristic curve (auROC)</dt>
  <dd><p>Measures the model’s ability to distinguish positive from negative examples across all thresholds. While it’s a standard metric of discrimination ability, it can sometimes be misleading in highly imbalanced datasets, as it gives equal weight to both classes.</p>
    </dd>
</dl>
<p>In a multilabel setting, we calculate these metrics for each protein function (i.e., per target/label), then average them to get a holistic view of model performance.</p>
<p>We apply these metrics calculations during the evaluation step <code>eval_step</code>:</p>
<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">eval_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">batch</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">float</code><code class="p">]:</code>
  <code class="sd">"""Run evaluation step and return mean metrics over targets."""</code>
  <code class="n">logits</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_fn</code><code class="p">({</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">},</code> <code class="n">x</code><code class="o">=</code><code class="n">batch</code><code class="p">[</code><code class="s2">"embedding"</code><code class="p">])</code>
  <code class="n">loss</code> <code class="o">=</code> <code class="n">optax</code><code class="o">.</code><code class="n">sigmoid_binary_cross_entropy</code><code class="p">(</code><code class="n">logits</code><code class="p">,</code> <code class="n">batch</code><code class="p">[</code><code class="s2">"target"</code><code class="p">])</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
  <code class="n">target_metrics</code> <code class="o">=</code> <code class="n">calculate_per_target_metrics</code><code class="p">(</code><code class="n">logits</code><code class="p">,</code> <code class="n">batch</code><code class="p">[</code><code class="s2">"target"</code><code class="p">])</code>
  <code class="n">metrics</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"loss"</code><code class="p">:</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">(),</code>
    <code class="o">**</code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">target_metrics</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">to_dict</code><code class="p">(),</code>
  <code class="p">}</code>
  <code class="k">return</code> <code class="n">metrics</code>


<code class="k">def</code> <code class="nf">calculate_per_target_metrics</code><code class="p">(</code><code class="n">logits</code><code class="p">,</code> <code class="n">targets</code><code class="p">):</code>
  <code class="sd">"""Compute metrics for each target in a multi-label batch."""</code>
  <code class="n">probs</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">logits</code><code class="p">)</code>
  <code class="n">target_metrics</code> <code class="o">=</code> <code class="p">[]</code>
  <code class="k">for</code> <code class="n">target</code><code class="p">,</code> <code class="n">prob</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">targets</code><code class="p">,</code> <code class="n">probs</code><code class="p">):</code>
    <code class="n">target_metrics</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">compute_metrics</code><code class="p">(</code><code class="n">target</code><code class="p">,</code> <code class="n">prob</code><code class="p">))</code>
  <code class="k">return</code> <code class="n">target_metrics</code>
</pre>
<p>The evaluation computes metrics per protein in the batch. For each protein, we:</p>
<ul>
  <li><p>Apply sigmoid to its 303 logits to get function probabilities.</p></li>
  <li><p>Threshold those probabilities (e.g., at 0.5) to get binary predictions.</p></li>
  <li><p>Compare these to the true function labels to compute metrics like accuracy, precision, recall, auPRC, and auROC.</p></li>
</ul>
     
 <p class="pagebreak-before">We repeat this for every protein in the batch and then average the resulting metrics across proteins. This tells us how well the model predicts sets of functions per protein. It does not report performance per GO term. If we wanted per-function metrics (e.g., how well the model predicts <code>GO:0003677</code>), we’d need to compute metrics column-wise instead.</p>   
   
   <p>In the next chunk of code, everything comes together into a <code>train</code> function, and variations of this basic setup will be repeated in  every chapter. We have the training loop where we first initialize our model training state and then loop over the dataset in batches to train the model and evaluate it every so often:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">train</code><code class="p">(</code>
  <code class="n">state</code><code class="p">:</code> <code class="n">TrainState</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="p">],</code>
  <code class="n">batch_size</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>
  <code class="n">num_steps</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">300</code><code class="p">,</code>
  <code class="n">eval_every</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">30</code><code class="p">,</code>
<code class="p">):</code>
  <code class="sd">"""Train model using batched TF datasets and track performance metrics."""</code>
  <code class="c1"># Create containers to handle calculated during training and evaluation.</code>
  <code class="n">train_metrics</code><code class="p">,</code> <code class="n">valid_metrics</code> <code class="o">=</code> <code class="p">[],</code> <code class="p">[]</code>

  <code class="c1"># Create batched dataset to pluck batches from for each step.</code>
  <code class="n">train_batches</code> <code class="o">=</code> <code class="p">(</code>
    <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code>
    <code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="n">drop_remainder</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
    <code class="o">.</code><code class="n">as_numpy_iterator</code><code class="p">()</code>
  <code class="p">)</code>

  <code class="n">steps</code> <code class="o">=</code> <code class="n">tqdm</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">num_steps</code><code class="p">))</code>  <code class="c1"># Steps with progress bar.</code>
  <code class="k">for</code> <code class="n">step</code> <code class="ow">in</code> <code class="n">steps</code><code class="p">:</code>
    <code class="n">steps</code><code class="o">.</code><code class="n">set_description</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Step </code><code class="si">{</code><code class="n">step</code> <code class="o">+</code> <code class="mi">1</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>

    <code class="c1"># Get batch of training data, convert into a JAX array, and train.</code>
    <code class="n">state</code><code class="p">,</code> <code class="n">loss</code> <code class="o">=</code> <code class="n">train_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="nb">next</code><code class="p">(</code><code class="n">train_batches</code><code class="p">))</code>
    <code class="n">train_metrics</code><code class="o">.</code><code class="n">append</code><code class="p">({</code><code class="s2">"step"</code><code class="p">:</code> <code class="n">step</code><code class="p">,</code> <code class="s2">"loss"</code><code class="p">:</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">()})</code>

    <code class="k">if</code> <code class="n">step</code> <code class="o">%</code> <code class="n">eval_every</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
      <code class="c1"># For all the evaluation batches, calculate metrics.</code>
      <code class="n">eval_metrics</code> <code class="o">=</code> <code class="p">[]</code>
      <code class="k">for</code> <code class="n">eval_batch</code> <code class="ow">in</code> <code class="p">(</code>
        <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">]</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">)</code><code class="o">.</code><code class="n">as_numpy_iterator</code><code class="p">()</code>
      <code class="p">):</code>
        <code class="n">eval_metrics</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">eval_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">eval_batch</code><code class="p">))</code>
      <code class="n">valid_metrics</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>
        <code class="p">{</code><code class="s2">"step"</code><code class="p">:</code> <code class="n">step</code><code class="p">,</code> <code class="o">**</code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">eval_metrics</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">to_dict</code><code class="p">()}</code>
      <code class="p">)</code>

  <code class="k">return</code> <code class="n">state</code><code class="p">,</code> <code class="p">{</code><code class="s2">"train"</code><code class="p">:</code> <code class="n">train_metrics</code><code class="p">,</code> <code class="s2">"valid"</code><code class="p">:</code> <code class="n">valid_metrics</code><code class="p">}</code>
</pre>
      
     
    
   
   <p class="pagebreak-before">A few notes on this training loop:
   </p>
   <dl>
    <dt>Efficient batch sampling</dt>
    <dd>Training data is streamed via <code>.as_numpy_iterator()</code>, and the <code>.repeat()</code> in the dataset ensures infinite looping over the data.</dd>
    <dt>Regular evaluation</dt>
    <dd>Every <code>eval_every</code> step, the model is evaluated on the full validation set to monitor progress using metrics we defined previously, like auPRC and auROC.</dd>
    <dt>Metric aggregation</dt>
    <dd>Validation metrics are computed batch-wise and then averaged across all batches using <code>pd.DataFrame(...).mean(axis=0)</code>. This gives a stable estimate of performance across the entire validation set.</dd>
   </dl>
   <p>Let’s now train the model. But first, a quick trick: to avoid unnecessarily repeating training from scratch every time you rerun your code cell, we use the <code>@restorable</code> decorator. This lightweight utility checks whether a trained model already exists at a specified path. If it does, it:</p>
   <ul>
    <li>Skips retraining</li>
    <li>Restores the model into a valid <code>TrainState</code></li>
    <li>Returns the model along with any saved metrics</li>
   </ul>
   <p>This makes your workflow much faster and more reproducible, especially during iterative development and debugging. Let’s take a look at how this is used:</p>
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">optax</code>

<code class="kn">from</code> <code class="nn">dlfb.utils.restore</code> <code class="kn">import</code> <code class="n">restorable</code>

<code class="c1"># Initiate training state with dummy data from a single batch.</code>
<code class="n">rng</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">rng</code><code class="p">,</code> <code class="n">rng_init</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">key</code><code class="o">=</code><code class="n">rng</code><code class="p">,</code> <code class="n">num</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>

<code class="n">state</code><code class="p">,</code> <code class="n">metrics</code> <code class="o">=</code> <code class="n">restorable</code><code class="p">(</code><code class="n">train</code><code class="p">)(</code>
  <code class="n">state</code><code class="o">=</code><code class="n">mlp</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code> <code class="n">dummy_input</code><code class="o">=</code><code class="n">batch</code><code class="p">[</code><code class="s2">"embedding"</code><code class="p">],</code> <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adam</code><code class="p">(</code><code class="mf">0.001</code><code class="p">)</code>
  <code class="p">),</code>
  <code class="n">dataset_splits</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">,</code>
  <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code>
  <code class="n">num_steps</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code>
  <code class="n">eval_every</code><code class="o">=</code><code class="mi">30</code><code class="p">,</code>
  <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"proteins/models/mlp"</code><code class="p">),</code>
<code class="p">)</code>
</pre>
      
     
    
   
   <p class="pagebreak-before">Some additional parameters worth mentioning are the optimizer (here, <code>optax.adam</code>) and the total number of training steps (<code>num_steps</code>). Given that we have 2,100 training examples and a batch size of 32, it will take
    about 66 steps for the model to see the entire training set once. Setting <code>num_steps=300</code> means the model will see each training data point several times.
   </p>
   <p>Having trained the model with the previous <code>train</code> call, we can now evaluate its training dynamics and performance on the validation set, as shown in <a data-type="xref" href="#mlp-model-eval">Figure 2-13</a>:
   </p>

       <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="k">as</code> <code class="nn">sns</code>

<code class="kn">from</code> <code class="nn">dlfb.utils.metric_plots</code> <code class="kn">import</code> <code class="n">DEFAULT_SPLIT_COLORS</code>

<code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">nrows</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">ncols</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">9</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>

<code class="c1"># Plot training loss curve.</code>
<code class="n">learning_data</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">(</code>
  <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">metrics</code><code class="p">[</code><code class="n">split</code><code class="p">])</code><code class="o">.</code><code class="n">melt</code><code class="p">(</code><code class="s2">"step"</code><code class="p">)</code><code class="o">.</code><code class="n">assign</code><code class="p">(</code><code class="n">split</code><code class="o">=</code><code class="n">split</code><code class="p">)</code>
  <code class="k">for</code> <code class="n">split</code> <code class="ow">in</code> <code class="p">[</code><code class="s2">"train"</code><code class="p">,</code> <code class="s2">"valid"</code><code class="p">]</code>
<code class="p">)</code>

<code class="n">sns</code><code class="o">.</code><code class="n">lineplot</code><code class="p">(</code>
  <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code>
  <code class="n">x</code><code class="o">=</code><code class="s2">"step"</code><code class="p">,</code>
  <code class="n">y</code><code class="o">=</code><code class="s2">"value"</code><code class="p">,</code>
  <code class="n">hue</code><code class="o">=</code><code class="s2">"split"</code><code class="p">,</code>
  <code class="n">data</code><code class="o">=</code><code class="n">learning_data</code><code class="p">[</code><code class="n">learning_data</code><code class="p">[</code><code class="s2">"variable"</code><code class="p">]</code> <code class="o">==</code> <code class="s2">"loss"</code><code class="p">],</code>
  <code class="n">palette</code><code class="o">=</code><code class="n">DEFAULT_SPLIT_COLORS</code><code class="p">,</code>
<code class="p">)</code>
<code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s2">"Loss over training steps."</code><code class="p">)</code>

<code class="c1"># Plot validation metrics curves.</code>
<code class="n">sns</code><code class="o">.</code><code class="n">lineplot</code><code class="p">(</code>
  <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>
  <code class="n">x</code><code class="o">=</code><code class="s2">"step"</code><code class="p">,</code>
  <code class="n">y</code><code class="o">=</code><code class="s2">"value"</code><code class="p">,</code>
  <code class="n">hue</code><code class="o">=</code><code class="s2">"variable"</code><code class="p">,</code>
  <code class="n">style</code><code class="o">=</code><code class="s2">"variable"</code><code class="p">,</code>
  <code class="n">data</code><code class="o">=</code><code class="n">learning_data</code><code class="p">[</code><code class="n">learning_data</code><code class="p">[</code><code class="s2">"variable"</code><code class="p">]</code> <code class="o">!=</code> <code class="s2">"loss"</code><code class="p">],</code>
  <code class="n">palette</code><code class="o">=</code><code class="s2">"Set2"</code><code class="p">,</code>
<code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="s2">"center left"</code><code class="p">,</code> <code class="n">bbox_to_anchor</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">))</code>
<code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s2">"Validation metrics over training steps."</code><code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="mlp-model-eval" class="figure">
      <img alt="" src="assets/dlfb_0213.png" width="600" height="257"/>
      <h6><span class="label">Figure 2-13. </span>Training and evaluation of the MLP model over 300 steps. On the left, loss curves for the training and validation splits show rapid convergence, with stability reached after ~30 steps. On the right, auPRC, precision, and recall improve gradually. Accuracy and auROC metrics are very high due to class imbalance and are not very informative for this problem.
      </h6>
     </div></figure>
    
    <p>In the left panel, we observe that both training and validation loss drop sharply within the first ~30 steps and then stabilize. This is a typical learning curve, indicating rapid convergence without substantial instability (e.g., no major spikes or divergence). It suggests that the model—a shallow MLP operating on top of frozen pretrained embeddings—quickly captures the low-hanging signal in the data.</p>
    <p>In the right panel, we track several evaluation metrics over time:</p>
    <ul>
     <li>Accuracy and auROC start high and remain flat, but these can be misleading in imbalanced, multilabel settings like this one. Since most function labels are negative (i.e., a protein lacks the majority of all possible functions), a model that mostly predicts zeros can still achieve a high score on these metrics. For that reason, we don’t put much weight on these metrics in this context.</li>
     <li>auPRC steadily improves and does not fully plateau, suggesting the model continues to learn subtle distinctions and could potentially benefit from further training (i.e., by increasing <code>num_steps</code>).</li>
     <li>Precision improves more quickly than recall, indicating the model becomes increasingly confident in its predictions but still fails to capture some true <span class="keep-together">positives</span>.</li>
    </ul>
    <p>Together, these trends indicate that while most of the learning happens early on, there may still be headroom—particularly in recall and auPRC—if training were extended further or if a more powerful architecture were used.</p>
    <div data-type="tip"><h6>Tip</h6>
     <p>It can be slightly tedious to manually log metrics inside every training loop and then hook up custom plotting code to visualize them. To streamline this, later chapters introduce a MetricsLogger (for capturing values) and MetricsPlotter (for rendering them).</p>
<p>Beyond that, many modern machine learning workflows use hosted (or self-hosted) dashboards to automatically collect, store, and display metrics in real time. These tools help monitor experiments, compare training runs, and share results across teams. We encourage you to check them out. Popular options include:</p>
<ul>
 <li><a href="https://oreil.ly/tSPIP">TensorBoard</a></li>
 <li><a href="https://oreil.ly/Loybs">Weights &amp; Biases (W&amp;B)</a></li>
 <li><a href="https://oreil.ly/A1faJ">MLflow</a></li>
</ul>
    </div>
   <p>It’s great to see the model training successfully and loss and metrics curves trending in the right direction—but that’s just the beginning. The real insight comes from analyzing the model’s predictions, understanding where it performs well, and identifying its limitations.<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html29" data-type="indexterm" id="id588"/><a contenteditable="false" data-primary="" data-startref="ch02_proteins.html28" data-type="indexterm" id="id589"/></p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Examining the Model Predictions"><div class="sect2" id="examining-the-model-predictions">
   <h2>Examining the Model Predictions</h2>
   <p><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="training the model" data-tertiary="examining the model predictions" data-type="indexterm" id="ch02_proteins.html30"/><a contenteditable="false" data-primary="training" data-secondary="for learning language of proteins" data-tertiary="examining the model predictions" data-type="indexterm" id="ch02_proteins.html31"/>With a trained model in hand, it’s time to explore its strengths and weaknesses. We’ll start by generating predictions for the entire validation set and storing them in a dataframe for easier inspection:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">valid_df</code> <code class="o">=</code> <code class="n">load_sequence_embeddings</code><code class="p">(</code>
  <code class="n">store_file_prefix</code><code class="o">=</code><code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">assets</code><code class="p">(</code><code class="s1">'proteins/datasets/protein_dataset'</code><code class="p">)</code><code class="si">}</code><code class="s2">_valid"</code><code class="p">,</code>
  <code class="n">model_checkpoint</code><code class="o">=</code><code class="n">model_checkpoint</code><code class="p">,</code>
<code class="p">)</code>

<code class="c1"># Use batch size of 1 to avoid dropping the remainder.</code>
<code class="n">valid_probs</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">valid_batch</code> <code class="ow">in</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">]</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">as_numpy_iterator</code><code class="p">():</code>
  <code class="n">logits</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_fn</code><code class="p">({</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">},</code> <code class="n">x</code><code class="o">=</code><code class="n">valid_batch</code><code class="p">[</code><code class="s2">"embedding"</code><code class="p">])</code>
  <code class="n">valid_probs</code><code class="o">.</code><code class="n">extend</code><code class="p">(</code><code class="n">jax</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">logits</code><code class="p">))</code>

<code class="n">valid_true_df</code> <code class="o">=</code> <code class="n">valid_df</code><code class="p">[[</code><code class="s2">"EntryID"</code><code class="p">]</code> <code class="o">+</code> <code class="n">targets</code><code class="p">]</code><code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s2">"EntryID"</code><code class="p">)</code>
<code class="n">valid_prob_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code>
  <code class="n">np</code><code class="o">.</code><code class="n">stack</code><code class="p">(</code><code class="n">valid_probs</code><code class="p">),</code> <code class="n">columns</code><code class="o">=</code><code class="n">targets</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">valid_true_df</code><code class="o">.</code><code class="n">index</code>
<code class="p">)</code>
</pre>
      
     
    
   
   <p>To get a high-level sense of how the model is performing, we can visualize the full prediction matrix as a heatmap. In <a data-type="xref" href="#predicted-functional-annotation">Figure 2-14</a>, we plot two side-by-side heatmaps: one showing the true protein-function annotations (left) and the other showing the model’s predicted probabilities (right). Each column corresponds to a protein function, and each row to a protein:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">nrows</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">ncols</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">11</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>

<code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code>
  <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code>
  <code class="n">data</code><code class="o">=</code><code class="n">valid_true_df</code><code class="p">,</code>
  <code class="n">yticklabels</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>
  <code class="n">xticklabels</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>
  <code class="n">cmap</code><code class="o">=</code><code class="s2">"flare"</code><code class="p">,</code>
<code class="p">)</code>
<code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s2">"True functional annotations by protein."</code><code class="p">)</code>
<code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s2">"Functional category"</code><code class="p">)</code>

<code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code>
  <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>
  <code class="n">data</code><code class="o">=</code><code class="n">valid_prob_df</code><code class="p">,</code>
  <code class="n">yticklabels</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>
  <code class="n">xticklabels</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>
  <code class="n">cmap</code><code class="o">=</code><code class="s2">"flare"</code><code class="p">,</code>
<code class="p">)</code>
<code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s2">"Predicted functional annotations by protein."</code><code class="p">)</code>
<code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s2">"Functional category"</code><code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="predicted-functional-annotation" class="figure">
      <img alt="" src="assets/dlfb_0214.png" width="600" height="258"/>
      <h6><span class="label">Figure 2-14. </span>Heatmap overview of protein function prediction. The left panel shows the ground truth functional annotations for each protein in the validation set, while the right panel shows the model’s predicted probabilities. Both matrices are sparse, with vertical bands reflecting common function labels.
      </h6>
     </div></figure>
    
   
   <p>
    This visualization is quite zoomed out and high level, but it helps build intuition about overall model behavior:
   </p>
   <ul>
    <li>Some protein functions appear frequently in the dataset (visible as vertical stripes), and the model tends to predict these relatively well.</li>
    <li>Rare functions are harder to capture—the model often misses them entirely, leading to sparse or empty columns in the predicted heatmap.</li>
    <li>A few functions are over-predicted, visible as faint vertical lines across many proteins, suggesting the model is overly confident for those categories.</li>
    <li>Many cells in the predicted matrix show intermediate color tones, which reflect more uncertain probabilities (not a confident near-0 or near-1).</li>
   </ul>
   <p>We’ll now shift from this qualitative view to a quantitative one by evaluating model performance on each protein function individually:</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">metrics_by_function</code> <code class="o">=</code> <code class="p">{}</code>
<code class="k">for</code> <code class="n">function</code> <code class="ow">in</code> <code class="n">targets</code><code class="p">:</code>
  <code class="n">metrics_by_function</code><code class="p">[</code><code class="n">function</code><code class="p">]</code> <code class="o">=</code> <code class="n">compute_metrics</code><code class="p">(</code>
    <code class="n">valid_true_df</code><code class="p">[</code><code class="n">function</code><code class="p">]</code><code class="o">.</code><code class="n">values</code><code class="p">,</code> <code class="n">valid_prob_df</code><code class="p">[</code><code class="n">function</code><code class="p">]</code><code class="o">.</code><code class="n">values</code>
  <code class="p">)</code>

<code class="n">overview_valid</code> <code class="o">=</code> <code class="p">(</code>
  <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">metrics_by_function</code><code class="p">)</code>
  <code class="o">.</code><code class="n">T</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">go_term_descriptions</code><code class="p">,</code> <code class="n">left_index</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">right_on</code><code class="o">=</code><code class="s2">"term"</code><code class="p">)</code>
  <code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s2">"term"</code><code class="p">)</code>
  <code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="s2">"auprc"</code><code class="p">,</code> <code class="n">ascending</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
<code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">overview_valid</code><code class="p">)</code>
</pre>

<p>Output:</p>
 <pre data-type="programlisting">            accuracy    recall  precision     auprc     auroc  \
term                                                            
GO:0004930  0.958858  0.000000   0.000000  0.948591  0.982272   
GO:0004888  0.945424  0.177215   1.000000  0.849885  0.968354   
GO:0003824  0.848027  0.731591   0.819149  0.849362  0.909372   
...              ...       ...        ...       ...       ...   
GO:0003774  0.000000  0.000000   0.000000  0.000000  0.000000   
GO:0051015  0.000000  0.000000   0.000000  0.000000  0.000000   
GO:1902936  0.000000  0.000000   0.000000  0.000000  0.000000   

                    description  
term                             
GO:0004930  G protein-couple...  
GO:0004888  transmembrane si...  
GO:0003824   catalytic activity  
...                         ...  
GO:0003774  cytoskeletal mot...  
GO:0051015  actin filament b...  
GO:1902936  phosphatidylinos...  

[303 rows x 6 columns]
</pre>

<p>This analysis reveals substantial variation in model performance across protein functions. For instance, the model performs well on functions like <code>GO:0004930</code> (G protein–coupled receptor activity), but it struggles with others, such as <code>GO:0003774</code> (cytoskeletal motor activity). However, interpreting these results requires caution: some metrics may be based on very few validation examples, and performance is naturally limited for functions that are underrepresented during training. A high score on a frequent function may simply reflect ample training data, while low scores on rare functions may be expected.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id590">
  <h5>Thresholded and Continuous Evaluation Metrics</h5>
  <p><a contenteditable="false" data-primary="continuous evaluation metrics" data-type="indexterm" id="id591"/><a contenteditable="false" data-primary="evaluation" data-secondary="thresholded and continuous evaluation metrics" data-type="indexterm" id="id592"/><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="training the model" data-tertiary="thresholded and continuous evaluation metrics" data-type="indexterm" id="id593"/><a contenteditable="false" data-primary="thresholded evaluation metrics" data-type="indexterm" id="id594"/><a contenteditable="false" data-primary="training" data-secondary="for learning language of proteins" data-tertiary="thresholded and continuous evaluation metrics" data-type="indexterm" id="id595"/>Our evaluation metrics fall into two categories: thresholded and continuous.</p>
  <ul>
    <li><p><a contenteditable="false" data-primary="precision, as evaluation metric" data-type="indexterm" id="id596"/><a contenteditable="false" data-primary="recall, as evaluation metric" data-type="indexterm" id="id597"/><em>Precision</em> and <em>recall</em> are computed from binary predictions—i.e., after applying a fixed threshold (typically &gt;0.5) to the model’s output probabilities.</p></li>
    <li><p><a contenteditable="false" data-primary="auPRC (area under the precision–recall curve)" data-type="indexterm" id="id598"/><a contenteditable="false" data-primary="auROC (area under the receiver operating characteristic curve)" data-type="indexterm" id="id599"/><em>auPRC</em> (area under the precision–recall curve) and <em>auROC</em> (area under the receiver operating characteristic curve) are <em>threshold independent</em>. They assess how well the model ranks positive examples above negatives across all possible thresholds.</p></li>
  </ul>
  <p>Although a bit counterintuitive, it’s entirely possible for precision and recall to be 0 while auPRC and auROC remain high. This happens when the model assigns higher probabilities to the correct labels, but those probabilities never exceed the decision threshold. In such cases, thresholded metrics show failure, while ranking-based metrics still reflect meaningful signal.</p>
<p>If we wanted to address this issue with the current thresholded metrics, we could lower the decision threshold—for example, to 0.2 or 0.3—to encourage more positive predictions. The threshold can be tuned automatically using metrics like the F1 score (the harmonic mean of precision and recall).</p>
</div></aside>

   <p>Let’s take a closer look at whether there’s a relationship between how often a protein function appears in the training data and how well the model learns to predict it in the validation set:</p>
  
<pre data-type="programlisting" data-code-language="python"><code class="c1"># Compute number of occurrences of each function in the training set.</code>
<code class="n">overview_valid</code> <code class="o">=</code> <code class="n">overview_valid</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code>
  <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">train_df</code><code class="p">[</code><code class="n">targets</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">(),</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s2">"train_n"</code><code class="p">]),</code>
  <code class="n">left_index</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
  <code class="n">right_index</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
<code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">overview_valid</code><code class="p">)</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">            accuracy    recall  precision     auprc     auroc  \
GO:0004930  0.958858  0.000000   0.000000  0.948591  0.982272   
GO:0004888  0.945424  0.177215   1.000000  0.849885  0.968354   
GO:0003824  0.848027  0.731591   0.819149  0.849362  0.909372   
...              ...       ...        ...       ...       ...   
GO:0003774  0.000000  0.000000   0.000000  0.000000  0.000000   
GO:0051015  0.000000  0.000000   0.000000  0.000000  0.000000   
GO:1902936  0.000000  0.000000   0.000000  0.000000  0.000000   

                    description  train_n  
GO:0004930  G protein-couple...      138  
GO:0004888  transmembrane si...      228  
GO:0003824   catalytic activity     1210  
...                         ...      ...  
GO:0003774  cytoskeletal mot...        5  
GO:0051015  actin filament b...       17  
GO:1902936  phosphatidylinos...       18  

[303 rows x 7 columns]
</pre>
      

       <p>At a glance, it seems that functions with higher predictive performance (e.g., higher auPRC) also tend to have more training examples. In <a data-type="xref" href="#auprc-over-train-n">Figure 2-15</a>, we visualize this relationship more clearly with a scatterplot:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">fig</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">scatterplot</code><code class="p">(</code>
  <code class="n">x</code><code class="o">=</code><code class="s2">"train_n"</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s2">"auprc"</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">overview_valid</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s2">"grey"</code>
<code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s2">"# Train instances"</code><code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s2">"Validation auPRC"</code><code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="auprc-over-train-n" class="figure">
      <img alt="" src="assets/dlfb_0215.png" width="600" height="450"/>
      <h6><span class="label">Figure 2-15. </span>Relationship between training frequency and predictive performance (auPRC) across protein functions. Commonly observed functions in the training set tend to be predicted more accurately by the model.
      </h6>
     </div></figure>
    
   
   <p>This plot shows a clear trend: protein functions that occur more frequently in the training set tend to be predicted more accurately by the model on the validation set (as measured by auPRC). This aligns with expectations—machine learning models usually perform better on well-represented classes. It also highlights the challenge of class imbalance: rare functions are often poorly predicted, not necessarily due to biological complexity but because the model has limited data to learn from.
   </p>
   <p>But how do we know whether a specific auPRC score is actually good? An auPRC value of, say, 0.8 for a certain protein function might sound promising—but is that better than chance? Is it meaningful? To interpret these scores, we need something to compare them against.<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html31" data-type="indexterm" id="id600"/><a contenteditable="false" data-primary="" data-startref="ch02_proteins.html30" data-type="indexterm" id="id601"/></p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Evaluating Model Usefulness"><div class="sect2" id="evaluating-model-usefulness">
   <h2>Evaluating Model Usefulness</h2>
   <p>
    <a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="training the model" data-tertiary="evaluating model usefulness" data-type="indexterm" id="ch02_proteins.html32"/><a contenteditable="false" data-primary="training" data-secondary="for learning language of proteins" data-tertiary="evaluating model usefulness" data-type="indexterm" id="ch02_proteins.html33"/>To ground our evaluation, we’ll compare our model against two simple baselines:
   </p>
   <dl>
   <dt>Coin flip</dt>
    <dd>
     <p>For each protein function, randomly predict 0 or 1 with equal probability. This gives us a baseline for total ignorance.
     </p>
    </dd>
    <dt>Proportional guessing</dt>
    <dd>
     <p>Predict 1 for each function with probability equal to its frequency in the training set. This reflects prior class distribution knowledge, but without any learning.
     </p>
    </dd>
   </dl>
  
  <p>These baselines help contextualize the model’s performance. If our trained model doesn’t outperform these simple heuristics, it’s a sign that it may not have learned meaningful structure from the data.</p>
<p>Here are implementations for the baselines:</p>
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">make_coin_flip_predictions</code><code class="p">(</code>
  <code class="n">valid_true_df</code><code class="p">:</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">,</code> <code class="n">targets</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">str</code><code class="p">]</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">:</code>
  <code class="sd">"""Make random coin flip predictions for each protein function."""</code>
  <code class="n">predictions</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">([</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">],</code> <code class="n">size</code><code class="o">=</code><code class="n">valid_true_df</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
  <code class="k">return</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">predictions</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="n">targets</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">valid_true_df</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>


<code class="k">def</code> <code class="nf">make_proportional_predictions</code><code class="p">(</code>
  <code class="n">valid_true_df</code><code class="p">:</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">,</code> <code class="n">train_df</code><code class="p">:</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">,</code> <code class="n">targets</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">str</code><code class="p">]</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">:</code>
  <code class="sd">"""Make random protein function predictions proportional to frequency."""</code>
  <code class="n">percent_1_train</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">(</code><code class="n">train_df</code><code class="p">[</code><code class="n">targets</code><code class="p">]</code><code class="o">.</code><code class="n">mean</code><code class="p">())</code>
  <code class="n">proportional_preds</code> <code class="o">=</code> <code class="p">[]</code>
  <code class="k">for</code> <code class="n">target_column</code> <code class="ow">in</code> <code class="n">targets</code><code class="p">:</code>
    <code class="n">prob_1</code> <code class="o">=</code> <code class="n">percent_1_train</code><code class="p">[</code><code class="n">target_column</code><code class="p">]</code>
    <code class="n">prob_0</code> <code class="o">=</code> <code class="mi">1</code> <code class="o">-</code> <code class="n">prob_1</code>
    <code class="n">proportional_preds</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>
      <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">([</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">],</code> <code class="n">size</code><code class="o">=</code><code class="nb">len</code><code class="p">(</code><code class="n">valid_true_df</code><code class="p">),</code> <code class="n">p</code><code class="o">=</code><code class="p">[</code><code class="n">prob_0</code><code class="p">,</code> <code class="n">prob_1</code><code class="p">])</code>
    <code class="p">)</code>
  <code class="k">return</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code>
    <code class="n">np</code><code class="o">.</code><code class="n">stack</code><code class="p">(</code><code class="n">proportional_preds</code><code class="p">)</code><code class="o">.</code><code class="n">T</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="n">targets</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">valid_true_df</code><code class="o">.</code><code class="n">index</code>
  <code class="p">)</code>
</pre>
      
     
    
   
   <p>These baselines should give us simple but informative reference points. Let’s now apply these prediction methods, alongside our trained model:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">prediction_methods</code> <code class="o">=</code> <code class="p">{</code>
  <code class="s2">"coin_flip_baseline"</code><code class="p">:</code> <code class="n">make_coin_flip_predictions</code><code class="p">(</code><code class="n">valid_true_df</code><code class="p">,</code> <code class="n">targets</code><code class="p">),</code>
  <code class="s2">"proportional_guess_baseline"</code><code class="p">:</code> <code class="n">make_proportional_predictions</code><code class="p">(</code>
    <code class="n">valid_true_df</code><code class="p">,</code> <code class="n">train_df</code><code class="p">,</code> <code class="n">targets</code>
  <code class="p">),</code>
  <code class="s2">"model"</code><code class="p">:</code> <code class="n">valid_prob_df</code><code class="p">,</code>
<code class="p">}</code>
</pre>
      
<p>Now let’s evaluate the baselines in exactly the same way as our model—by computing per-protein metrics and averaging them:</p>     
    
<pre data-type="programlisting" data-code-language="python"><code class="n">metrics_by_method</code> <code class="o">=</code> <code class="p">{}</code>
<code class="k">for</code> <code class="n">method</code><code class="p">,</code> <code class="n">preds_df</code> <code class="ow">in</code> <code class="n">prediction_methods</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
  <code class="n">metrics_by_method</code><code class="p">[</code><code class="n">method</code><code class="p">]</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code>
    <code class="p">[</code>
      <code class="n">compute_metrics</code><code class="p">(</code><code class="n">valid_true_df</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">i</code><code class="p">],</code> <code class="n">preds_df</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">i</code><code class="p">])</code>
      <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">valid_true_df</code><code class="p">))</code>
    <code class="p">]</code>
  <code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>

<code class="nb">print</code><code class="p">(</code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">metrics_by_method</code><code class="p">))</code>
</pre>    

<p>Output:</p>
<pre data-type="programlisting">           coin_flip_baseline  proportional_guess_baseline     model
accuracy             0.500916             0.956447          0.978569
recall               0.499229             0.093555          0.128532
precision            0.023883             0.079994          0.424301
auprc                0.025307             0.039701          0.412350
auroc                0.500027             0.535605          0.882679
</pre>    

   
   <p>Our model clearly outperforms both baselines across all metrics—especially in precision, auPRC, and auROC. This is expected, as the trained model leverages actual sequence features to make more informed predictions. As noted earlier, accuracy is not a reliable metric in this setting, and even simple proportional guessing achieves a deceptively high accuracy due to class imbalance.
   </p>
   <p>Most of the model’s performance gains come from a large increase in precision, while the improvement in recall is more modest. This means the model is good at correctly identifying positive cases when it makes a prediction, but it tends to miss many true positives—it’s cautious and biased toward predicting “no function.”
   </p>
   <p>This highlights a key trade-off: the model is conservative but accurate. Depending on your application, you may want to tune this behavior—for example, by lowering the decision threshold to improve recall, as discussed earlier.</p>
   <p>Next, we’ll break down the model’s strengths and weaknesses by individual protein function and compare performance against both baselines. This allows us to see which specific functions the model predicts well—and where it struggles:</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">auprc_by_function</code> <code class="o">=</code> <code class="p">{}</code>

<code class="k">for</code> <code class="n">method</code><code class="p">,</code> <code class="n">preds_df</code> <code class="ow">in</code> <code class="n">prediction_methods</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
  <code class="n">metrics_by_function</code> <code class="o">=</code> <code class="p">{}</code>

  <code class="k">for</code> <code class="n">function</code> <code class="ow">in</code> <code class="n">targets</code><code class="p">:</code>
    <code class="n">metrics_by_function</code><code class="p">[</code><code class="n">function</code><code class="p">]</code> <code class="o">=</code> <code class="n">compute_metrics</code><code class="p">(</code>
      <code class="n">valid_true_df</code><code class="p">[</code><code class="n">function</code><code class="p">],</code> <code class="n">preds_df</code><code class="p">[</code><code class="n">function</code><code class="p">]</code>
    <code class="p">)</code>

  <code class="n">auprc_by_function</code><code class="p">[</code><code class="n">method</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code>
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">metrics_by_function</code><code class="p">)</code>
    <code class="o">.</code><code class="n">T</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">go_term_descriptions</code><code class="p">,</code> <code class="n">left_index</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">right_on</code><code class="o">=</code><code class="s2">"term"</code><code class="p">)</code>
    <code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s2">"term"</code><code class="p">)</code>
    <code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="s2">"auprc"</code><code class="p">,</code> <code class="n">ascending</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
  <code class="p">)[</code><code class="s2">"auprc"</code><code class="p">]</code><code class="o">.</code><code class="n">to_dict</code><code class="p">()</code>
</pre>
      
     
    
   
   <p>In <a data-type="xref" href="#best-predicted-functions">Figure 2-16</a>, we visualize the function-level auPRC scores as a bar plot to highlight which functional categories the model handles best:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">best_performing</code> <code class="o">=</code> <code class="p">(</code>
  <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">auprc_by_function</code><code class="p">)</code>
  <code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">go_term_descriptions</code><code class="p">,</code> <code class="n">left_index</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">right_on</code><code class="o">=</code><code class="s2">"term"</code><code class="p">)</code>
  <code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s2">"term"</code><code class="p">)</code>
  <code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="s2">"model"</code><code class="p">,</code> <code class="n">ascending</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
  <code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">20</code><code class="p">)</code>
  <code class="o">.</code><code class="n">melt</code><code class="p">(</code><code class="s2">"description"</code><code class="p">)</code>
<code class="p">)</code>

<code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mi">5</code><code class="p">))</code>
<code class="n">sns</code><code class="o">.</code><code class="n">barplot</code><code class="p">(</code>
  <code class="n">x</code><code class="o">=</code><code class="s2">"description"</code><code class="p">,</code>
  <code class="n">y</code><code class="o">=</code><code class="s2">"value"</code><code class="p">,</code>
  <code class="n">hue</code><code class="o">=</code><code class="s2">"variable"</code><code class="p">,</code>
  <code class="n">data</code><code class="o">=</code><code class="n">best_performing</code><code class="p">,</code>
<code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s2">"The model's 20 best performing protein functions"</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s2">"Validation auPRC"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xticks</code><code class="p">(</code><code class="n">rotation</code><code class="o">=</code><code class="mi">90</code><code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="best-predicted-functions" class="figure">
      <img alt="" src="assets/dlfb_0216.png" width="600" height="754"/>
      <h6><span class="label">Figure 2-16. </span>Top 20 protein functions ranked by model auPRC on the validation set. Bars show the auPRC achieved by the model, compared against two simple baselines (i.e., coin flips and proportional guessing).
      </h6>
     </div></figure>
    <p class="pagebreak-before">Many of the top-performing protein functions in the plot are related to membrane or signaling roles (e.g., GPCR activity, kinase activity, transmembrane receptor activity). One possible reason is that these functions often involve well-conserved sequence features—such as transmembrane helices or catalytic domains—that may be easier for models to learn. While speculative, this aligns with the idea that functions tied to strong structural or biochemical motifs may produce clearer sequence-level signals than more context-dependent roles.</p>
    <p>Together, these results suggest that the model is capable of detecting meaningful biological signal for certain classes of protein function—and that it significantly outperforms simple baselines.<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html33" data-type="indexterm" id="id602"/><a contenteditable="false" data-primary="" data-startref="ch02_proteins.html32" data-type="indexterm" id="id603"/></p>
   
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Conducting a Final Check on the Test Set"><div class="sect2" id="final-check-on-the-test-set">
   <h2>Conducting a Final Check on the Test Set</h2>
   <p><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="training the model" data-tertiary="conducting final check on test set" data-type="indexterm" id="id604"/><a contenteditable="false" data-primary="training" data-secondary="for learning language of proteins" data-tertiary="conducting final check on test set" data-type="indexterm" id="id605"/>Take a look at the next section for ideas on how to extend and improve this model. Once you are satisfied with your exploration, we can move on to the final step of this project: making the final predictions on the test set. Remember not to touch the test set until the last stage of your project.
   </p>
   <div data-type="warning" epub:type="warning"><h6>Warning</h6>Be sure not to touch the test set until you’ve fully finalized your model—including all hyperparameters, architectures, and training choices. Evaluating on the test set repeatedly can lead to overly optimistic results and undermine the validity of your findings.</div>
   <p>We’ll make predictions on the test set of proteins in the same way we did for the validation set:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">eval_metrics</code> <code class="o">=</code> <code class="p">[]</code>

<code class="k">for</code> <code class="n">split</code> <code class="ow">in</code> <code class="p">[</code><code class="s2">"valid"</code><code class="p">,</code> <code class="s2">"test"</code><code class="p">]:</code>
  <code class="n">split_metrics</code> <code class="o">=</code> <code class="p">[]</code>

  <code class="k">for</code> <code class="n">eval_batch</code> <code class="ow">in</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="n">split</code><code class="p">]</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="mi">32</code><code class="p">)</code><code class="o">.</code><code class="n">as_numpy_iterator</code><code class="p">():</code>
    <code class="n">split_metrics</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">eval_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">eval_batch</code><code class="p">))</code>

  <code class="n">eval_metrics</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>
    <code class="p">{</code><code class="s2">"split"</code><code class="p">:</code> <code class="n">split</code><code class="p">,</code> <code class="o">**</code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">split_metrics</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">to_dict</code><code class="p">()}</code>
  <code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">eval_metrics</code><code class="p">))</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">   split      loss  accuracy    recall  precision     auprc     auroc
0  valid  0.080156  0.978457  0.126869   0.418515  0.411870  0.880883
1   test  0.080675  0.978032  0.125820   0.435193  0.410439  0.879234
</pre>
      
<p>The test set metrics closely mirror those observed on the validation set, which is good. In many workflows, test performance is slightly lower due to repeated use of the validation set during development—potentially leading to mild overfitting. However, in this case, we haven’t done extensive tuning, so the gap is minimal. Because the test set was held out throughout, its results provide a more reliable estimate of how the model will generalize to truly unseen data. These are the metrics we would report externally.<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html27" data-type="indexterm" id="id606"/><a contenteditable="false" data-primary="" data-startref="ch02_proteins.html26" data-type="indexterm" id="id607"/></p>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Improvements and Extensions"><div class="sect1" id="improvements-and-extensions">
  <h1>Improvements and Extensions</h1>
  <p><a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="improvements and extensions of model" data-type="indexterm" id="ch02_proteins.html34"/>The model we’ve built demonstrates that protein function can be predicted from sequence using pretrained embeddings and a lightweight classifier. However, many directions remain to improve, interpret, and extend this work. We split these ideas into two broad categories: analysis-driven insights and machine learning <span class="keep-together">improvements</span>.
  </p>
  <p>But before diving into technical upgrades, it’s worth stepping back to revisit the bigger picture:</p>
  <dl>
   <dt>Why are you doing this?</dt>
   <dd>Who will use the model, and what do they actually need? Can you share this prototype with users now to gather early feedback?</dd>
   <dt>When are you done?</dt>
   <dd>Is the current model already good enough? What specific improvements would meaningfully increase its utility? What benchmarks exist for this or similar tasks?</dd>
   <dt>What matters most?</dt>
   <dd>Is performance across all functions equally important, or do you care about a specific class (e.g., enzymes versus nonenzymes)? Focusing your optimization accordingly can save time.</dd>
   <dt>Do you need interpretability?</dt>
   <dd>For some applications, understanding why a model makes a prediction may matter more than maximizing performance.</dd>
  </dl>
  <p>Ideally, you’ll have thought about some of these questions before starting the modeling—but revisiting them now can help guide your next steps.
  </p>
  <section data-type="sect2" data-pdf-bookmark="Biological and Analytical Exploration"><div class="sect2" id="biology-extensions">
   <h2>Biological and Analytical Exploration</h2>
   <p>
    <a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="improvements and extensions of model" data-tertiary="biological/analytical exploration" data-type="indexterm" id="id608"/>Even with a fixed model, we can learn a lot more by probing its behavior and comparing it to biological expectations:
   </p>
   <dl>
   <dt>Threshold tuning</dt>
    <dd>
     <p>Our results showed that the model has high auPRC but low recall at a default probability threshold of 0.5. You could optimize this threshold (e.g., per protein function or globally) using a metric like F1 score to find a better trade-off between precision and recall.
     </p>
    </dd>
    <dt>Species generalization</dt>
    <dd>
     <p>The current dataset is human only, but this might be unnecessarily limited. Try including protein-function pairs from other species to see if performance improves.
     </p>
    </dd>
    <dt>Function-specific performance drivers</dt>
    <dd>
     <p>Why does the model do well on some functions (e.g., GPCR activity) but poorly on others (e.g., growth factor activity)? You could investigate whether function prevalence, sequence length, or other properties correlate with performance.
     </p>
    </dd>
    <dt>Examine protein multifunctionality</dt>
    <dd>Does the model struggle more with proteins that have many functions? Group proteins by number of annotated functions and plot performance (e.g., auPRC) to see if there’s a trend.</dd>
    <dt>False positives that might be real</dt>
    <dd>Find proteins where the model confidently predicts a function that isn’t labeled. Could the model be correct and the annotation missing? How might you follow this up?</dd>
   </dl>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Machine Learning Improvements"><div class="sect2" id="machine-learning-extensions">
   <h2>Machine Learning Improvements</h2>
   <p>
    <a contenteditable="false" data-primary="proteins, learning the language of" data-secondary="improvements and extensions of model" data-tertiary="machine learning improvements" data-type="indexterm" id="id609"/>From a machine learning perspective, here are a few directions you could explore:
   </p>
   <dl>
   <dt>Tune the MLP</dt>
    <dd>
     <p>Our model is a small MLP on top of frozen embeddings. Try adding more layers, dropout, or batch normalization to increase capacity while controlling overfitting.
     </p>
    </dd>
    <dt>Alternative input encodings</dt>
    <dd>
     <p>We used the mean-pooled embedding, which loses sequence order information. Try attention pooling or a small 1D CNN or transformer on top of the token-level embeddings.
     </p>
    </dd>
    <dt>Feature engineering</dt>
    <dd>
     <p>You could augment the input to include protein length, species (if you extend beyond human), or even simple statistics like embedding norms. These additional features might help the model distinguish protein types more effectively.
     </p>
    </dd>
    <dt>Train a per-function head:</dt>
    <dd>
     <p>Instead of predicting all functions jointly, try training separate models (or heads) for each function. This can help when tasks are highly imbalanced or unrelated. Alternatively, you could cluster GO functions into a few categories and train one model per cluster.
     </p>
    </dd>
    <dt>Predict function hierarchically</dt>
    <dd>
     <p>Rather than treating each function independently, you could use the GO hierarchy to add structure to predictions—for example, predicting broad function categories first and then refining to more specific ones.
     </p>
    </dd>
    <dt>Try alternative base models</dt>
    <dd>You could plug in other protein language models from Hugging Face or explore combining embeddings from multiple models by concatenating them.</dd>
    <dt>Unfreeze the language model</dt>
    <dd>The ESM2 embeddings are pretrained on a generic task. Fine-tuning the language model directly for protein function classification may boost performance, though it requires more compute and a more involved training setup.</dd>
   </dl>
   <div data-type="note" epub:type="note"><h6>Note</h6>
    <p>While it’s tempting to chase performance gains through increasingly complex models, always align your efforts with the actual goals of your project. Improving interpretability or expanding biological coverage may be more valuable than inching up another point on a leaderboard.<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html34" data-type="indexterm" id="id610"/>
    </p>
   </div>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="summary">
  <h1>Summary</h1>
  <p>In this chapter, we took our first hands-on step into the world of deep learning for biology. Starting with a dataset of human proteins, we explored how to extract meaningful representations using a pretrained protein language model, trained a simple classifier to predict protein function, and evaluated its performance using quantitative metrics.</p>
  <p>Along the way, we encountered practical challenges typical of biological modeling: getting comfortable with a new modeling setup, dealing with imbalanced label distributions, and carefully interpreting evaluation metrics.</p>
  <p>In the next chapter, we’ll build on these foundations by shifting our focus from proteins to DNA. You’ll define convolutional neural networks from scratch in Flax and train them end to end to model regulatory sequences, predict functional elements, and discover motif patterns directly from genomic data.<a contenteditable="false" data-primary="" data-startref="ch02_proteins.html0" data-type="indexterm" id="id611"/></p>
 </div></section>
</div></section></div></div></body></html>