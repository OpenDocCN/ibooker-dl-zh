["```py\n# Load the pipeline\nimage_pipe = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\")\nimage_pipe.to(device);\n\n# Sample an image\nimage_pipe().images[0]\n```", "```py\n# The random starting point for a batch of 4 images\nx = torch.randn(4, 3, 256, 256).to(device)\n\n# Set the number of timesteps lower\nimage_pipe.scheduler.set_timesteps(num_inference_steps=30)\n\n# Loop through the sampling timesteps\nfor i, t in enumerate(image_pipe.scheduler.timesteps):\n\n    # Get the prediction given the current sample x and the timestep t\n    with torch.no_grad():\n        noise_pred = image_pipe.unet(x, t)[\"sample\"]\n\n    # Calculate what the updated sample should look like with the scheduler\n    scheduler_output = image_pipe.scheduler.step(noise_pred, t, x)\n\n    # Update x\n    x = scheduler_output.prev_sample\n\n    # Occasionally display both x and the predicted denoised images\n    if i % 10 == 0 or i == len(image_pipe.scheduler.timesteps) - 1:\n        fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\n        grid = torchvision.utils.make_grid(x, nrow=4).permute(1, 2, 0)\n        axs[0].imshow(grid.cpu().clip(-1, 1) * 0.5 + 0.5)\n        axs[0].set_title(f\"Current x (step {i})\")\n\n        pred_x0 = scheduler_output.pred_original_sample\n        grid = torchvision.utils.make_grid(pred_x0, nrow=4).permute(1, 2, 0)\n        axs[1].imshow(grid.cpu().clip(-1, 1) * 0.5 + 0.5)\n        axs[1].set_title(f\"Predicted denoised images (step {i})\")\n        plt.show()\n```", "```py\ndataset = load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")\n```", "```py\nimage_size = 64\n\n# Define data augmentations\npreprocess = transforms.Compose(\n    [\n        transforms.Resize((image_size, image_size)),  # Resize\n        transforms.RandomHorizontalFlip(),  # Randomly flip (data augmentation)\n        transforms.ToTensor(),  # Convert to tensor (0, 1)\n        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n    ]\n)\n```", "```py\nbatch_size = 32\n\ndef transform(examples):\n    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n    return {\"images\": images}\n\ndataset.set_transform(transform)\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    dataset, batch_size=batch_size, shuffle=True\n)\n```", "```py\nbatch = next(iter(train_dataloader))\nprint('Shape:', batch['images'].shape,\n      '\\nBounds:', batch['images'].min().item(), 'to', batch['images'].max().item())\nshow_images(batch['images'][:8]*0.5 + 0.5) # NB: we map back to (0, 1) for display\n```", "```py\nShape: torch.Size([32, 3, 64, 64])\nBounds: -0.9921568632125854 to 1.0\n```", "```py\nscheduler = DDPMScheduler(num_train_timesteps=1000, beta_start=0.001, beta_end=0.02)\ntimesteps = torch.linspace(0, 999, 8).long()\n\nx = batch['images'][:8]\nnoise = torch.rand_like(x)\nnoised_x = scheduler.add_noise(x, noise, timesteps)\nshow_images((noised_x*0.5 + 0.5).clip(0, 1))\n```", "```py\n# Create a UNet2DModel\nmodel = UNet2DModel(\n    in_channels=3,  # 3 channels for RGB images\n    sample_size=64,  # Specify our input size\n    block_out_channels=(64, 128, 256, 512), # N channels per layer\n    down_block_types=(\"DownBlock2D\", \"DownBlock2D\",\n                      \"AttnDownBlock2D\", \"AttnDownBlock2D\"),\n    up_block_types=(\"AttnUpBlock2D\", \"AttnUpBlock2D\",\n                    \"UpBlock2D\", \"UpBlock2D\"),\n)\n\n# Pass a batch of data through\nwith torch.no_grad():\n    out = model(noised_x, timestep=timesteps).sample\nout.shape\n```", "```py\ntorch.Size([8, 3, 64, 64])\n```", "```py\nnum_epochs = 50 # How many runs through the data should we do?\nlr = 1e-4 # What learning rate should we use\nmodel = model.to(device) # The model we're training (defined in the previous section)\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr) # The optimizer\nlosses = [] # somewhere to store the loss values for later plotting\n\n# Train the model (this takes a while!)\nfor epoch in range(num_epochs):\n    for step, batch in enumerate(train_dataloader):\n\n        # Load the input images\n        clean_images = batch[\"images\"].to(device)\n\n        # Sample noise to add to the images\n        noise = torch.randn(clean_images.shape).to(clean_images.device)\n\n        # Sample a random timestep for each image\n        timesteps = torch.randint(\n            0,\n            scheduler.num_train_timesteps,\n            (clean_images.shape[0],),\n            device=clean_images.device,\n        ).long()\n\n        # Add noise to the clean images according timestep\n        noisy_images = scheduler.add_noise(clean_images, noise, timesteps)\n\n        # Get the model prediction for the noise\n        noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n\n        # Compare the prediction with the actual noise:\n        loss = F.mse_loss(noise_pred, noise)\n\n        # Store the loss for later plotting\n        losses.append(loss.item())\n\n        # Update the model parameters with the optimizer based on this loss\n        loss.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n```", "```py\n# Plot the loss curve:\nplt.plot(losses);\n```", "```py\npipeline = DDPMPipeline(unet=model, scheduler=scheduler)\nims = pipeline(batch_size=4).images\nshow_images(ims, nrows=1)\n```", "```py\n# Random starting point (4 random images):\nsample = torch.randn(4, 3, 64, 64).to(device)\n\nfor i, t in enumerate(scheduler.timesteps):\n\n    # Get model pred\n    with torch.no_grad():\n        noise_pred = model(sample, t).sample\n\n    # Update sample with step\n    sample = scheduler.step(noise_pred, t, sample).prev_sample\n\nshow_images(sample.clip(-1, 1)*0.5 + 0.5, nrows=1)\n```", "```py\nx = next(iter(train_dataloader))['images'][:8]\nnoise = torch.rand_like(x)\n```", "```py\ndef corrupt(x, noise, amount):\n  amount = amount.view(-1, 1, 1, 1) # make sure it's broadcastable\n  return x*(1-amount) + noise*amount\n```", "```py\namount = torch.linspace(0, 1, 8)\nnoised_x = corrupt(x, noise, amount)\nshow_images(noised_x*0.5 + 0.5)\n```", "```py\nclass SimpleScheduler():\n  def __init__(self):\n    self.num_train_timesteps = 1000\n  def add_noise(self, x, noise, timesteps):\n    amount = timesteps / self.num_train_timesteps\n    return corrupt(x, noise, amount)\n\nscheduler = SimpleScheduler()\ntimesteps = torch.linspace(0, 999, 8).long()\nnoised_x = scheduler.add_noise(x, noise, timesteps)\nshow_images(noised_x*0.5 + 0.5)\n```", "```py\nscheduler = DDPMScheduler(beta_end=0.01)\ntimesteps = torch.linspace(0, 999, 8).long()\nnoised_x = scheduler.add_noise(x, noise, timesteps)\nshow_images((noised_x*0.5 + 0.5).clip(0, 1))\n```", "```py\nplot_scheduler(DDPMScheduler()) # The default scheduler\n```", "```py\nplot_scheduler(SimpleScheduler())\n```", "```py\nfig, (ax) = plt.subplots(1, 1, figsize=(8, 5))\nplot_scheduler(DDPMScheduler(beta_schedule=\"linear\"),\n               label = 'default schedule', ax=ax, plot_both=False)\nplot_scheduler(DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\"),\n               label = 'cosine schedule', ax=ax, plot_both=False)\nplot_scheduler(DDPMScheduler(beta_end=0.003, beta_schedule=\"linear\"),\n               label = 'Low beta_end', ax=ax, plot_both=False)\nplot_scheduler(DDPMScheduler(beta_end=0.1, beta_schedule=\"linear\"),\n               label = 'High beta_end', ax=ax, plot_both=False)\n```", "```py\nfrom torch import nn\n\nclass BasicUNet(nn.Module):\n    \"\"\"A minimal UNet implementation.\"\"\"\n    def __init__(self, in_channels=1, out_channels=1):\n        super().__init__()\n        self.down_layers = torch.nn.ModuleList([\n            nn.Conv2d(in_channels, 32, kernel_size=5, padding=2),\n            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n        ])\n        self.up_layers = torch.nn.ModuleList([\n            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n            nn.Conv2d(64, 32, kernel_size=5, padding=2),\n            nn.Conv2d(32, out_channels, kernel_size=5, padding=2),\n        ])\n        self.act = nn.SiLU() # The activation function\n        self.downscale = nn.MaxPool2d(2)\n        self.upscale = nn.Upsample(scale_factor=2)\n\n    def forward(self, x):\n        h = []\n        for i, l in enumerate(self.down_layers):\n            x = self.act(l(x)) # Through the layer and the activation function\n            if i < 2: # For all but the third (final) down layer:\n              h.append(x) # Storing output for skip connection\n              x = self.downscale(x) # Downscale ready for the next layer\n\n        for i, l in enumerate(self.up_layers):\n            if i > 0: # For all except the first up layer\n              x = self.upscale(x) # Upscale\n              x += h.pop() # Fetching stored output (skip connection)\n            x = self.act(l(x)) # Through the layer and the activation function\n\n        return x\n```"]