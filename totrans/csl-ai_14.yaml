- en: 11 Building a causal inference workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building a causal analysis workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating causal effects with DoWhy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating causal effects using machine learning methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal inference with causal latent variable models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 10, I introduced a causal inference workflow, and in this chapter
    we’ll focus on building out this workflow in full. We’ll focus on one type of
    query in particular—causal effects—but the workflow generalizes to all causal
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll focus on causal effect inference, namely estimation of average treatment
    effects (ATEs) and conditional average treatment effects (CATEs) because they
    are the most popular causal queries.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 1, I mentioned “the commodification of inference”—how modern software
    libraries enable us to abstract away the statistical and computational details
    of the inference algorithm. The first thing you’ll see in this chapter is how
    the DoWhy library “commodifies” causal inference, enabling us to focus at a high
    level on the causal assumptions of the algorithms and whether they are appropriate
    for our problem.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll see the phenomenon at play again in an example that uses probabilistic
    machine learning to do causal effect inference on a causal generative model with
    latent variables. Here, we’ll see how deep learning with PyTorch provides another
    way to commodify inference.
  prefs: []
  type: TYPE_NORMAL
- en: '11.1 Step 1: Select the query'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall the causal inference workflow from chapter 10, shown again in figure
    11.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F01_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 A workflow for a causal inference analysis
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s return to our online gaming example and use this workflow to answer a
    simple question:'
  prefs: []
  type: TYPE_NORMAL
- en: How much does side-quest engagement drive in-game purchases?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We’ll call the cause of interest, *Side-Quest Engagement* (*E*), the “treatment”
    variable; *In-Game Purchases* (*I*) will be the “outcome” variable. Our query
    of interest is the average treatment effect (ATE):'
  prefs: []
  type: TYPE_NORMAL
- en: '*E*(*I**[E]*[=“high”] – *I*[*E*][=“low”])'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refresher: Why ATEs and CATEs dominate'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Estimating ATEs and CATEs is the most popular causal effect inference task
    for several reasons, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We can rely on causal effect inference techniques when randomized experiments
    are not feasible, ethical, or possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use causal effect inference techniques to address practical issues with
    real-world experiments (e.g., post-randomization confounding, attrition, spillover,
    missing data, etc.).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In an era when companies can run many different digital experiments in online
    applications and stores, causal effect inference techniques can help prioritize
    experiments, reducing opportunity costs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further, as we investigate our gaming data, we find data from a past experiment
    designed to test the effect of *encouraging* side-quest engagement on in-game
    purchases. In this experiment, all players were randomly assigned either to the
    treatment group or a control group. In the treatment group, the game mechanics
    were modified to tempt players into engaging in more side-quests, while the control
    group played the unmodified version of the game. We’ll define the *Side-Quest
    Group Assignment* (*A*) variable as whether the player was assigned to the treatment
    group in this experiment or the control group.
  prefs: []
  type: TYPE_NORMAL
- en: Why not just go with the estimate of the ATE produced by this experiment? This
    would be an estimate of *E*(*I*[*A*][=“treatment”] – *I*[*A*][=“control”]).
  prefs: []
  type: TYPE_NORMAL
- en: This is the causal effect of the modification of game mechanics on in-game purchases.
    While this drives side-quest engagement, we know side-quest engagement is also
    driven by other potentially confounding factors. So we’ll focus on *E*(*I*[*E*][=“high”]
    – *I*[*E*][=“low”]).
  prefs: []
  type: TYPE_NORMAL
- en: '11.2 Step 2: Build the model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we’ll build our causal model. Since we are targeting an ATE, we can stick
    with a DAG. Let’s suppose we build a more detailed version of our online gaming
    example and produce the causal DAG in figure 11.2\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F02_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 An expanded version of the online gaming DAG. With respect to the
    causal effect of side-quest engagement on in-game purchases, we add two additional
    confounders and two instruments.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The expanded model adds some new variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Side-Quest Group Assignment (A)*—Assigned a value of 1 if a player was exposed
    to the mechanics that encouraged more side-quest engagement in the randomized
    experiment; 0 otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Customization Level (C)*—A score quantifying the player’s customizations of
    their character and the game environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Time Spent Playing (T)*—How much time the player has spent playing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prior Experience (Y)*—How much experience the player had prior to when they
    started playing the game.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Player Skill Level (S)*—A score of how well the player performs in game tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Total Inventory (V)*—The amount of game items the player has accumulated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We are interested in the ATE of *Side-Quest Engagement* on *In-Game Purchases*,
    so we know, based on causal sufficiency (chapter 3), that we need to add common
    causes for these variables. We’ve already seen *Guild Membership* (*G*), but now
    we add additional common causes: *Prior Experience*, *Time Spent Playing*, and
    *Player Skill Level*. We also add *Side-Quest Group Assignment* and *Customization
    Level* because these might be useful *instrumental variables*—variables that are
    causes of the treatment of interest, and where the only path of causality from
    the variable to the outcome is via the treatment. I’ll say more about instrumental
    variables in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll add *Total Inventory*. This is a collider between *In-Game Purchases*
    and *Won Items*. Perhaps it is common for data scientists in our company to use
    this as a predictor of the *In-Game Purchases*. But as you’ll see, we’ll want
    to avoid adding collider bias to causal effect estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your environment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The following code was written with DoWhy 0.11 and EconML 0.15, which expects
    a version of NumPy before version 2.0\. The specific pandas version was 1.5.3\.
    Again, we use Graphviz for visualization, with python PyGraphviz library version
    1.12\. The code should work, save for visualization, if you skip the PyGraphviz
    installation.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s build the DAG and visualize the graph with the PyGraphviz library.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.1 Build the causal DAG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Download PyGraphviz and related libraries.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Optional import for visualizing the DAG in a Jupyter notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Specify the DAG as a DOT language string, and load a PyGraphviz AGraph object
    from the string.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Render the graph to a PNG file.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Display the graph.'
  prefs: []
  type: TYPE_NORMAL
- en: This returns the graph in figure 11.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F03_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 Visualizing our model with the PyGraphviz library
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: At this stage, we can validate our model using the conditional independence
    testing techniques outlined in chapter 4\. But keep in mind that we can also focus
    on the subset of assumptions we rely on for causal effect estimation to work in
    the “refutation” (step 5) part of the workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '11.3 Step 3: Identify the estimand'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we’ll run identification. Our causal query is
  prefs: []
  type: TYPE_NORMAL
- en: '*E*(*I*[*E*][=“high”] – *I*[*E*][=“low”])'
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, let’s recode “high” as 1 and “low” as 0.
  prefs: []
  type: TYPE_NORMAL
- en: '*E*(*I*[*E*][=1] – *I*[*E*][=0])'
  prefs: []
  type: TYPE_NORMAL
- en: This query is on level 2 of the causal hierarchy. We are not running an experiment;
    we only have observational data—samples from a level 1 distribution. Our identification
    task is to use our level 2 query and our causal model and identify a level 1 estimand,
    an operation we can apply to the distribution of the variables in our data.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s download our data and see what variables are in our observational
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.2 Download and display the data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Download an online gaming dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Print the variables.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This prints out the following set of variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Our level 1 observational distribution includes all the variables in the DAG
    except *Prior Experience*. Thus, *Prior Experience* is a latent variable (figure
    11.4).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F04_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 *Prior Experience* is not observed in the data; it is a latent (unobserved)
    variable with respect to our DAG.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We specified the base distribution for the estimand using y0’s domain-specific
    language for probabilistic expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, we’ll use DoWhy. With DoWhy, we specify the observational distribution
    by just passing in the pandas DataFrame, along with the DAG and the causal query,
    to the constructor of the `CausalModel` class.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.3 Instantiate an instance of DoWhy’s `CausalModel`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Install DoWhy and load the CausalModel class.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Instantiate the CausalModel object with the data, which represents the level
    1 observational distribution from which we derive the estimands.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Specify the target causal query we wish to estimate, namely the causal effect
    of the treatment on the outcome.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Provide the causal DAG.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, the `identify_effect` methods will show us possible estimands we can target,
    given our causal model and observed variables.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.4 Run identification in DoWhy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The identify_effect method of the CausalModel class lists identified estimands.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `identified_estimand` object is an object of the class `IdentifiedEstimand`.
    Printing it will list the estimands, if any, and the assumptions they entail.
    In our case, we have three estimands we can target:'
  prefs: []
  type: TYPE_NORMAL
- en: The backdoor adjustment estimand through the adjustment set *Player Skill Level*,
    *Guild Membership*, and *Time Spent Playing*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The front-door adjustment estimand through the mediator *Won Items*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instrumental variable estimands through *Side-Quest Group Assignment* and *Customization
    Level*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graphical identification in DoWhy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: At the time of writing, DoWhy does implement graphical identification algorithms
    like y0, but these are experimental and are not the default identification approach.
    The default approach looks for commonly used estimands (e.g., backdoor, front
    door, instrumental variables) based on the structure of your graph. There may
    be identifiable estimands that the default approach misses, but these would be
    estimands that are not commonly used.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine these estimands more closely.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.1 The backdoor adjustment estimand
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s look at the printed summary for the first estimand, the backdoor adjustment
    estimand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This printout tells us a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: '`EstimandType.NONPARAMETRIC_ATE`—This means the estimand can be identified
    with graphical or “nonparametric” methods, such as the do-calculus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Estimand name: backdoor`—This is the backdoor adjustment estimand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Estimand expression`—The mathematical expression of the estimand. Since we
    want the ATE, we modify the backdoor estimand to target the ATE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Estimand assumption 1`—The causal assumptions underlying the estimand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last item is the most important. For each estimand, DoWhy lists the causal
    assumptions that must hold for valid estimation of the target causal query. In
    this case, the assumption is that there are no hidden (unmeasured) confounders,
    which DoWhy refers to as `U`. Estimation of a backdoor adjustment estimand assumes
    that all confounders are adjusted for.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we do not need to observe *Prior Experience* to obtain a backdoor
    adjustment estimand. We just need to observe an adjustment set of common causes
    that d-separates or “blocks” all backdoor paths.
  prefs: []
  type: TYPE_NORMAL
- en: The next estimand in the printout is an instrumental variable estimand.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.2 The instrumental variable estimand
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The printed summary for the second estimand, the instrumental variable estimand,
    is as follows (note, I shortened the variable names to acronyms so the summary
    fits this page):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7][PRE8]`` `⎥`  `⎣``d[SQGA CL]` `⎝` `d[SQGA  CL]` `⎠` [PRE9] `` `⎦`  Estimand
    assumption 1, As-if-random:      If U→→IGP then ¬(U →→{SQGA,CL}) Estimand assumption
    2, Exclusion:     If we remove {SQGA,CL}→{SQE} then ¬({SQGA,CL}→IGP) `` [PRE10]`
    [PRE11][PRE12]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]`There are two level 2 definitional requirements for a variable to be
    a valid instrument:    1.  *As-if-random*—Any backdoor paths between the instrument
    and the outcome can be blocked. 2.  *Exclusion*—The instrument is a cause of the
    outcome only indirectly through the treatment.    The variables in our model that
    satisfy these constraints are *Side-Quest Group Assignment* and *Customization
    Level*, as shown in figure 11.5.  ![figure](../Images/CH11_F05_Ness.png)  #####
    Figure 11.5 *Side-Quest Group Assignment* and *Customization Level* are valid
    instrumental variables.    The printout of `identified_estimand` shows the two
    constraints:    1.  `Estimand` `assumption` `1,` `As-if-random`—DoWhy assumes
    that none of the other causes of the outcome (*In-Game Purchases*) are also causes
    of either instrument. In other words, there are no backdoor paths between the
    instruments and the outcome. 2.  `Estimand assumption` `2,` `Exclusion`—This says
    that if we remove the causal path from the instruments to the treatment (*Side-quest
    Engagement*), there would be no causal paths from the instruments to the outcome
    (*In-Game Purchases*). In other words, there are no causal paths between the instruments
    and the outcome that are not mediated by the treatment.    Note that DoWhy’s constraints
    are relatively restrictive; DoWhy prohibits the *existence* of backdoor paths
    and non-treatment-mediated causal paths between the instrument and the outcome.
    In practice, it would be possible to block these paths with backdoor adjustment.
    DoWhy is making a trade-off that favors a simpler interface.    ##### Parametric
    assumptions for instrumental variable estimation    The level 2 graphical assumptions
    are not sufficient for instrumental variable identification; additional parametric
    assumptions are needed. DoWhy, by default, makes a linearity assumption. With
    a linear assumption, you can derive the ATE as a simple function of the coefficients
    of linear models of outcome and the treatment given the instrument. DoWhy does
    this by fitting linear regression models.    Next, we’ll look at the third estimand
    identified by DoWhy—the front door estimand.    ### 11.3.3 The front-door adjustment
    estimand    Let’s move on to the assumptions in the third estimand, the front-door
    estimand. DoWhy’s printed summary is as follows (again, I shortened the variable
    names to acronyms in the printout so it fits the page):    [PRE14]    As we saw
    in chapter 10, the front-door estimand requires a mediator on the path from the
    treatment to the outcome—in our DAG, this is *Won Items*. The printout for `identified_estimand`
    lists three key assumptions for the front-door estimand:    1.  `Full-mediation`—The
    mediator (*Won-Items*) intercepts all directed paths from the treatment (*Side-Quest
    Engagement*) to the outcome (*In-Game Purchases*). In other words, conditioning
    on *Won-Items* would d-separate (block) all the paths of causal influence from
    the treatment to the outcome. 2.  `First-stage-unconfoundedness`—There are no
    hidden confounders between the treatment and the mediator. 3.  `Second-stage-unconfoundedness`—There
    are no hidden confounders between the outcome and the mediator.    With our DAG
    and the variables observed in the data, DoWhy has identified three estimands for
    the ATE of *Side-Quest Engagement* on *In-Game Purchases*. Remember, the estimand
    is the thing we estimate, so which estimand should we estimate?    ### 11.3.4
    Choosing estimands and reducing “DAG anxiety”    In step 2 of the causal inference
    workflow, we specified our causal assumptions about the domain as a DAG (or SCM
    or other causal model). The subsequent steps all rely on the assumptions we make
    in step 2\.    Errors in step 2 can lead to errors in the results of the analysis,
    and while we can empirically test these assumptions to some extent (e.g., using
    the methods in chapter 4), we cannot verify all our causal assumptions with observational
    data alone. This dependence on our subjective and unverified causal assumptions
    leads to what I call “DAG anxiety”—a fear that if one gets any part of the causal
    assumptions wrong, then the output of the analysis becomes wrong. Fortunately,
    we don’t need to get all the assumptions right; we only need to rely on the assumptions
    required *to identify our selected estimand*.    This is what makes DoWhy’s `identify_effect`
    method so powerful. By showing us the assumptions required for each estimand it
    lists, we can compare these assumptions and target the estimand where we are most
    confident about those assumptions.    For example, the key assumption behind the
    backdoor adjustment estimand is that we can adjust for all sources of confounding
    from common causes. In our original DAG, we have an edge from *Time Spent Playing*
    to *Player Skill Level*. What if you weren’t sure about the direction of this
    edge, as illustrated in figure 11.6.  ![figure](../Images/CH11_F06_Ness.png)  #####
    Figure 11.6 Uncertainty about the edge between *Time Spent Playing* and *Player
    Skill Level*doesn’t matter with respect to the backdoor adjustment estimand of
    the ATE of interest.    When we initially built the DAG, you might have been thinking
    that playing more causes skill level to increase. But now you may worry that perhaps
    the relationship is the other way around—that being more skilled causes you to
    want to spend more time playing. It doesn’t matter! At least, not with respect
    to the backdoor estimand for the target query—the ATE of *Side-Quest Engagement*
    on *In-Game Purchases*.    Suppose that instead you were worried that the model
    might have omitted edges that reflect direct influence that *Prior Experience*
    has on *Side-Quest Engagement* and *In-Game Purchases*. You worry that players
    might bring their habits in side-quest playing and virtual item purchasing from
    previous games they’ve played to the game environment you are modeling, as in
    figure 11.7.  ![figure](../Images/CH11_F07_Ness.png)  ##### Figure 11.7 Direct
    influence of a latent variable on the treatment and outcome would violate the
    assumption underpinning the backdoor adjustment estimand. If you are not confident
    in an estimand’s assumptions, target another.    If this is true, your backdoor
    adjustment estimand assumption would be violated—you would have a confounder you
    couldn’t adjust for, a backdoor path you couldn’t block. In this case, you’ll
    need to consider whether the backdoor adjustment estimand is the right estimand
    to target.    Fortunately, in this example, we still have two other estimands
    to choose from. Neither the instrumental variable estimand nor the front-door
    adjustment estimand rely on our ability to adjust for all common causes. As long
    as we’re comfortable with the assumptions for either of these estimands, we can
    continue.    ### 11.3.5 When you don’t have identification    The stop sign in
    the causal inference workflow, shown again in figure 11.8, warns against proceeding
    with estimation when you don’t have identification.  ![figure](../Images/CH11_F08_Ness.png)  #####
    Figure 11.8 If you lack identification, do not proceed to estimation. Rather,
    consider how to acquire data that enables identification.    Let’s consider what
    happens if our observational distribution only contains a subset of our initial
    variables, as in figure 11.9.  ![figure](../Images/CH11_F09_Ness.png)  ##### Figure
    11.9 *Player Skill Level*, *Won Items*, *Prior Experience*, *Side-Quest Group
    Assignment*, *and Customization Level become late*nt variables.    In this case,
    we have some problems:    *   If *Player Skill Level* is latent, we can’t adjust
    for confounding from *Player Skill Level* and thus have no backdoor estimand.
    *   If *Won Items* is latent, we can’t identify a front-door estimand. *   If
    the instrumental variables are latent, we can’t target an instrumental variable
    estimand.    When you lack identification, you should not proceed with the next
    step of estimation. Rather, use the results from identification to determine what
    additional variables to collect—consider how you can collect new data with    *   Additional
    confounders that would enable backdoor identification *   A mediator that would
    enable front-door identification *   Variables you can use as instruments    Avoid
    the temptation to change the DAG to get identification with your current data—you
    are modeling the data generating process (DGP), not the data.    However, if you
    do have an identified estimand, you can move on to step 4—estimation.    ## 11.4
    Step 4: Estimate the estimand    In step 4 of the causal inference workflow, we
    select an estimation method for whichever estimand we wish to target. In this
    section, we’ll walk through several estimators for each of our three estimands.
    Note that your results for estimation may vary slightly from those in the text,
    depending on modifications to the dataset and to random elements of the estimator.    In
    DoWhy, we do estimation using a method in the `CausalModel` class called `estimate_effect`,
    as in the following example.    ##### Listing 11.5 Estimating the backdoor estimand
    with linear regression    [PRE15]  #1 The estimate_effect method takes the output
    of the identify_effect method as input. #2 method_name is of the form “[estimand].[estimator]”.
    Here we use the linear regression estimator to estimate the backdoor estimand.
    #3 Return confidence intervals   The first argument is the `identified_estimand`
    object. The second argument `method_name` is a string of the form `"[estimand].[estimator]"`,
    where `"[estimand]"` is the estimand we want to target, and `"[estimator]"` is
    the estimation method we want to use. Thus, `method_name="backdoor.linear_regression"`
    means we want to use linear regression to estimate the backdoor estimand.    In
    this section, we’ll see the benefits of distinguishing identification from estimation.
    In step 3 of the causal inference workflow, we compared identified estimands and
    selected an estimand with assumptions in which we are confident. That step frees
    us to focus on the statistical and computational trade-offs common across data
    science and machine learning when we choose an estimation method in step 4\. We’ll
    walk through these trade-offs in this section. Let’s start by looking at the linear
    regression estimation of the backdoor estimand.    ### 11.4.1 Linear regression
    estimation of the backdoor estimand    In many causal inference texts, particularly
    from econometrics, the default approach to causal inference is regression—specifically,
    regressing the outcome on the treatment and any confounders we wish to adjust
    for or “control for.” What we are doing in this case is using linear regression
    to estimate the backdoor estimand.    Recall that in the case where *Side-Quest
    Engagement* is continuous, the ATE would be  ![figure](../Images/ness-ch11-eqs-4x.png)  This
    is a function of x, not a point value. However, it becomes a point value when
    *E*(*I*[*E*][=][*x*]) is linear—the derivative of a linear function is a constant.    So
    we turn to regression. The backdoor adjustment estimand identifies *Guild Membership*
    (*G*), *Time Spent Playing* (*T*), and *Player Skill Level* (*S*) as the confounders
    we have to adjust for. In general, we have to sum or integrate over these variables
    in the backdoor adjustment estimand. But in the linear regression case, this simplifies
    to simply regressing *I* on the treatment *E* and the confounders *G*, *T*, and
    *S*. The coefficient estimate for *E* is the ATE. In the case of a binary treatment
    like our target ATE,    *E*(*I*[*E*][=1] – *I*[*E*][=][0])    we simply treat
    *E* as a regression dummy variable. The coefficient estimates for the confounders
    are *nuisance parameters—*meaning they are necessary to estimate the ATE, but
    we can discard them once we have it.    To illustrate, let’s print the results
    of our call to `estimate_method`.    ##### Listing 11.6 Print the linear regression
    estimation results    [PRE16]    This prints a bunch of stuff, including the following:    [PRE17]    `Realized
    estimand` shows the regression formula. `Estimate` shows the estimation results,
    the point value, and the 95% confidence interval.    Here we see why linear regression
    is so popular as an estimator:    *   The coefficient estimate of the treatment
    is a point estimate of the ATE. *   We adjust for backdoor confounders by simply
    including them in the regression model (no summation, no integration). *   The
    statistical properties of the estimator (confidence intervals, *p*-values, etc.)
    are well established. *   Many people are familiar with regression and how to
    evaluate a regression fit.    Once we have backdoor identification, the question
    of whether we should use a linear regression estimator in this case involves the
    same considerations of whether a linear regression model is appropriate in non-causal
    explanatory modeling settings (e.g., is the relationship linear?).    ##### Valid
    backdoor adjustment sets: What you can and can’t adjust for    You do not need
    to adjust for *all* confounding from common causes. Any valid backdoor adjustment
    set of common causes will do. As discussed in chapter 10, a valid backdoor adjustment
    set any set that satisfies the backdoor criterion, meaning that it d-separates
    *all* backdoor paths. For example, *Guild Membership*, *Time Spent Playing*, and
    *Player Skill Level* are a valid adjustment set. You don’t need *Prior Experience*
    because *Time Spent Playing* and *Player Skill Level* are sufficient to d-separate
    the backdoor path through *Prior Experience*. This is fortunate for us, since
    *Prior Experience* is unobserved. Though, if it were observed, we could add it
    to the adjustment set—this superset would also be a valid set.    DoWhy selects
    a valid adjustment set when it identifies a backdoor estimand. If you write your
    own estimator, you’ll select your own adjustment set.    Some applied regression
    texts argue that you should try to adjust for or “control for” any covariates
    in your data because they could be potential confounders. This is bad advice.
    Doing so only makes sense if you are sure the covariate is not a mediator or a
    collider between the treatment and outcome variables. Adjusting for a mediator
    will d-separate the causal path you mean to quantify with the ATE. Adjusting for
    a collider will add collider bias. This is a painfully common error in social
    science, one committed even by experts.    ### 11.4.2 Propensity score estimators
    of the backdoor estimand    Propensity score methods are a collection of estimation
    methods for the backdoor estimand that use a quantity called the *propensity score*.
    The traditional definition of a propensity score is the probability of being exposed
    to the treatment conditional on the confounders. In the context of the online
    gaming example, this is the probability that a player has high *Side-Quest Engagement*
    given their *Guild Membership*, *Time Spent Playing*, and *Player Skill Level*,
    i.e., *P*(*E*=1|*T*=*t*, *G*=*g*, *S*=*s*) where *t*, *g*, and *s* are that player’s
    values for *T*, *G*, and *S*. In other words, it quantifies the player’s “propensity”
    of being exposed to the treatment (*E*=1). Typically *P*(*E*=1|*T*=*t*, *G*=*g*,
    *S*=*s*) is fit by logistic regression.    But we can take a more expansive, machine
    learning–friendly view of the propensity score. We can learn a propensity score
    function *λ*(...) of the backdoor adjustment set of confounders that renders those
    confounders conditionally independent of the treatment, as in figure 11.10.  ![figure](../Images/CH11_F10_Ness.png)  #####
    Figure 11.10 The propensity score is a compression of the causal influence of
    the common causes in the backdoor adjustment set.    Here, we learn a function
    *λ*(*T*, *S*, *G*) such that it effectively compresses the explanatory influence
    that *T*, *S*, and *G* have on *E*. The traditional function of *P*(*E*=1|*G*,
    *S*, *T*) compresses this influence into a probability value, but other approaches
    can work as well.    The utility of propensity score modeling is dimensionality
    reduction; now we only need to adjust for the score instead of all the confounders
    in the adjustment set. There are three common propensity score methods:    *   Propensity
    score stratification *   Propensity score matching *   Propensity score weighting    These
    methods make different trade-offs in how they go about backdoor adjustment. Let’s
    examine their use in DoWhy.    #### Propensity score stratification    Propensity
    score stratification tries to break the data up into subsets (“strata”) according
    to propensity scores and then adjust over the strata. Note that this algorithm
    may take some time to run.    ##### Listing 11.7 Propensity score stratification    [PRE18]  #1
    Propensity score stratification   This produces the following results:    [PRE19]    The
    propensity score estimator gives us an estimate and confidence interval that differ
    slightly from that of the regression estimator.    #### Propensity score matching    *Propensity
    score matching* tries to match individuals where treatment = 1 with individuals
    that have a similar propensity score but where treatment = 0 and then compare
    outcomes across matched pairs.    ##### Listing 11.8 Propensity score matching    [PRE20]  #1
    Propensity score matching   This returns the following results:    [PRE21]    Propensity
    score matching, despite also being a propensity score method, returns an estimate
    and confidence interval different from that of propensity score stratification.    ####
    Propensity score weighting    *Propensity score weighting* methods use the propensity
    score to calculate a weight in a class of inference algorithms called *inverse
    probability weighting*. We implement this method in DoWhy as follows.    #####
    Listing 11.9 Propensity score weighting    [PRE22]  #1 Inverse probability weighting
    with the propensity score #2 Parameters used to set the IPS algorithm   This returns
    the following:    [PRE23]    The fact that this estimator’s result differs so
    dramatically from the others suggest that it is relying on statistical assumptions
    that don’t hold in this data.    Next, we’ll move on to a popular class of backdoor
    estimators that implement machine learning.    ### 11.4.3 Backdoor estimation
    with machine learning    Recent developments in causal effect estimation focus
    on leveraging machine learning models, and most of these target the backdoor estimand.
    These approaches to causal effect estimation scale to large datasets and allow
    us to relax parametric assumptions, such as linearity. The following DoWhy code
    uses the sklearn and EconML libraries for these machine learning methods. DoWhy’s
    `estimate_effects` provides a wrapper to the EconML implementation of these methods.    ####
    Double machine learning    Double machine learning (double ML) is a backdoor estimator
    that uses machine learning methods to fit two predictive models: a model of the
    outcome, given the adjustment set of confounders, and a model of the treatment,
    given the adjustment set. The approach then combines these two predictive models
    in a final-stage estimation to create a model of the target causal effect query.    The
    following code performs double ML using a gradient boosting model and regularized
    regression model (`LassoCV`) from sklearn.    ##### Listing 11.10 Double ML with
    DoWhy, EconML, and sklearn    [PRE24]  #1 Select the double ML estimator. #2 Use
    a gradient boosting model to model the outcome given the confounders. #3 Use a
    gradient boosting model to model the treatment given the confounders. #4 Use linear
    regression with L1 regularization (LASSO) as the final model.   This produces
    the following output:    [PRE25]    This gives us an estimate in the ballpark
    of some of the other estimators.    #### Meta learners    Meta learners are another
    ML method for backdoor estimation. Broadly speaking, meta learners train a model
    (or models) of the outcome given the treatment variable and the confounders, and
    then account for the difference in prediction across treatment and control values
    of the treatment variable. They are particularly focused on highlighting heterogeneity
    of treatment effects across the data. The following code shows a meta learner
    example called a T-learner that uses a random forest predictor.    ##### Listing
    11.11 Backdoor estimation with a meta learner    [PRE26]  #1 Meta learner estimation
    of the backdoor estimand. This uses a T-learner with a random forest predictor.   This
    returns the following output:    [PRE27]    The values under “Effect estimates”
    are the estimate of the CATE for each row of the data, conditional on the confounder
    values in the columns of that row.    #### Confidence intervals with machine learning
    methods    DoWhy and EconML provide support for estimating confidence intervals
    for ML methods using a statistical method called nonparametric bootstrap, but
    this is computationally costly for large data. Cheap confidence interval estimation
    is one thing you give up for the flexibility and scalability of using ML methods
    for backdoor estimation.    ### 11.4.4 Front-door estimation    Recall from chapter
    10 that the front-door estimator for our ATE, given our *Won Items* mediator,
    is  ![figure](../Images/ness-ch11-eqs-5x.png)  We can estimate this by fitting
    two statistical models, one that predicts *W* given *E*, and one that predicts
    *I* given *E* and *W*. DoWhy does this with linear regression by default, but
    you also have the option of selecting different predictive models.    ##### Listing
    11.12 Front door estimation with DoWhy    [PRE28]  #1 Select two-stage regression
    for the front-door estimand. #2 Specify estimator hyperparameters.   This produces
    the following output:    [PRE29]    The front-door estimate is similar to some
    of the backdoor estimators, but note that the confidence interval is skewed left.    ###
    11.4.5 Instrumental variable methods    Instrumental variable-based estimation
    of the ATE is straightforward in DoWhy.    ##### Listing 11.13 Instrumental variable
    estimation in DoWhy    [PRE30]  #1 Select instrumental variable estimation. #2
    Select side-quest engagement as the instrument.   This prints the following output:    [PRE31]    Note
    how large the confidence interval is despite the size of the data. This indicates
    that this estimator, with its default assumptions, might have too much variance
    to be useful.    ##### Good instrumental variables should be “strong”    One requirement
    for good instrumental variable estimation is that the instrument is strong, meaning
    it has a strong causal effect on the treatment variable. If you explore this data,
    you’ll find *Side-Quest Group Assignment* is a weak instrument. Weak instruments
    can lead to high variance estimates of the ATE. Keep this in mind when selecting
    an instrument.    #### Regression discontinuity    Regression discontinuity is
    an estimation method popular in econometrics. It uses a continuously valued variable
    related to the treatment variable, and it defines a threshold (a “discontinuity”)
    in the values of that variable that partition the data into “treatment” and “control”
    groups. It then compares observations lying closely on either side of the threshold,
    because those data points tend to have similar values for the confounders.    DoWhy
    treats regression discontinuity as an instrumental variable approach that uses
    continuous instruments. The `rd_variable_name` argument names a continuous instrument
    to use for thresholding, and `rd_threshold_value` is the threshold value. `rd_bandwidth`
    is the distance from the threshold within which confounders can be considered
    the same between treatment and control.    ##### Listing 11.14 Regression discontinuity
    estimation with DoWhy    [PRE32]  #1 DoWhy treats regression discontinuity as
    a special type of IV estimator. #2 Use Customization Level as our instrument.
    #3 The threshold value for the split (“discontinuity”) #4 The distance from the
    threshold within which confounders are considered the same between treatment and
    control values of the treatment variable   This returns the following results:    [PRE33]    Again,
    the variance is too large for us to rely on this estimator. The instrument is
    likely weak, or we need to tune the arguments passed to the estimator.    #####
    Conditional average treatment effect estimation and segmentation    The conditional
    average treatment effect (CATE) is the ATE for a subset of the target population;
    i.e., we condition the ATE on specific values of covariates. DoWhy enables you
    to estimate the CATE as easily as the ATE.    Sometimes the goal of CATE estimation
    is *segmentation*—breaking the population down into segments that have a distinct
    CATE from other segments. A good tool for segmentation is EconML, which enables
    CATE-segmentation using regression trees. EconML can segment data into groups
    that respond similarly to intervention on the treatment variable, and find an
    optimal intervention value for each group in the leaf nodes of the regression
    tree.    ### 11.4.6 Comparing and selecting estimators    In chapter 1, I mentioned
    a phenomenon called *the commodification of inference*. The way DoWhy reduces
    estimation to merely a set of arguments passed to the `estimate_effect` method
    is an example of this phenomenon. You don’t need a detailed understanding of the
    estimator to get going. Once you’ve selected the estimand you wish to target,
    you can switch out different estimators.    ##### Advice: Start with synthetic
    data    One excellent practice is to build your workflow on synthetic data, rather
    than real data. Simulate a synthetic dataset that matches the size and correlation
    structure of your data, as well as your causal and statistical assumptions about
    your data. For example, you can write a causal generative model of your data,
    and use your data to train its parameters. Using this model as ground truth, simulate
    some data and derive a ground truth ATE.    You can then see if DoWhy’s estimates
    get close to the ground truth ATE, and if its confidence intervals contain it.
    You can also see how well the estimators perform under the ideal conditions where
    all your assumptions are true—even in these conditions, the estimates will have
    biases and uncertainty.    Once you debug any problems that arise in these ideal
    conditions, you can switch out the synthetic data for real data. Then, the problems
    that arise are likely due to incorrect assumptions, and you can treat these by
    revisiting your assumptions.    My suggestion is to compare estimators after adding
    the next step, *refutation*, to the workflow. Refutation will help you stress
    test both the causal assumptions in the estimand and the statistical assumptions
    in the estimator. This enables you to make empirical comparisons of different
    estimators. Then, once you know what estimator you want and have seen how it performs
    on your data, you can do a deep dive into the statistical nuts and bolts of your
    chosen estimator.    ## 11.5 Step 5: Refutation    We know that the result of
    our causal inference depends on our initial causal assumptions in step 2, or more
    specifically, the subset of those assumptions we rely on for identification in
    step 3\. In step 4, we select an estimator that makes its own statistical assumptions.
    What if those causal and statistical assumptions are wrong?    We can address
    this in step 5 with *refutation**, where we actively search for evidence that
    our analysis is faulty*. We first saw this concept in chapter 4, when we saw how
    to refute the causal DAG by finding statistical evidence of dependence in the
    data that conflicts with the conditional independence implications of the causal
    DAG. In section 7.6.2, we saw how to refute a model by finding cases where its
    predicted intervention outcomes clash with real-world intervention outcomes. Here,
    we implement refutation as a type of *sensitivity analysis* *that*tries to refute
    the various assumptions underpinning an estimate by simulating violations to those
    assumptions.    The `CausalModel` class in DoWhy has a `refute_estimate` method
    that provides a suite of refuters we can run. Each refuter provides a different
    attack vector for our assumptions. The refuters we run with `refute_estimate`
    perform a simulation-based statistical test; the null hypothesis is that the assumptions
    are not refuted, and the alternative hypothesis is that the assumptions are refuted.
    The tests return a *p*-value. If we take a standard significance threshold of
    .05 and the p-value falls below this threshold, we conclude that our assumptions
    are refuted.    In this section, we’ll investigate a few of DoWhy’s refuters with
    various estimands and estimators.    ### 11.5.1 Data size reduction    One way
    to test the robustness of the analysis is to reduce the size of the data and see
    if we obtain similar results. We are assuming our analysis has more than enough
    data to achieve a stable estimation. We can refute this assumption by slightly
    reducing the size of the data and testing whether we get a similar estimate. Let’s
    try this with the estimator of the front-door estimand.    ##### Listing 11.15
    Refuting the assumption of sufficient data    [PRE34]  #1 Not always necessary,
    but clarifying the estimand targeted by the estimator we want to test can help
    avoid errors. #2 The refute_estimate function takes in the identified estimand
    and the estimator that targets the estimand. #3 Select data_subset_refuter, which
    tests if the causal estimate is different when we run the analysis on a subset
    of the data. #4 Set the size of the subset to 80% the size of the original data.   This
    produces the following output (this is a random process so your results will differ
    slightly):    [PRE35]    The `Estimated effect` is the effect from our original
    analysis. `New` `effect` is the average ATE across the simulations. We want these
    two effects to be similar, because otherwise it would mean that our analysis is
    sensitive to the amount of data we have. The *p*-value here is above the threshold,
    so we failed to refute this assumption.    ### 11.5.2 Adding a dummy confounder    One
    way to test our models is to add dummy common-cause confounders. If a variable
    is not a confounder, it has no bearing on the true ATE, so we assume that our
    causal effect estimation workflow will be unaffected by these variables. In truth,
    additional variables might add statistical noise that throws off our estimator.    The
    following listing attempts to refute the assumption that such noise does not affect
    the double ML estimator of the backdoor estimand.    ##### Listing 11.16 Adding
    a dummy confounder    [PRE36]  #1 Runs 100 simulations of the addition of a dummy
    confounder to the model   This returns output such as the following:    [PRE37]    Again,
    `Estimated effect` is the original causal effect estimate, and `New` `effect`
    is the new causal effect estimate obtained after adding a random common cause
    to the data and re-running the analysis. The dummy variable has no real effect,
    so we expect the ATE to be the same. Again, the *p*-value is above the significance
    threshold, so we failed to refute our assumptions.    ### 11.5.3 Replacing treatment
    with a dummy    We can also experiment with replacing the treatment variable with
    a dummy variable. This is analogous to giving our causal effect inference workflow
    a “placebo,” and seeing how much causality it ascribes to this fake treatment.
    Since this dummy variable will have no effect on the treatment, we expect the
    ATE to be 0\.    Let’s try this with our inverse probability weighting estimator.    #####
    Listing 11.17 Replacing the treatment variable with a dummy variable    [PRE38]  #1
    This refuter replaces the treatment variable with a dummy (placebo) variable.   This
    produces the following output:    [PRE39]    In this case, the *p*-value is calculated
    under the null hypothesis that `New` `effect` is equal to 0\. Again, a low *p*-value
    would refute our assumptions.    In this case, it would seem that our inverse
    probability weighting estimator was thrown off by this refuter. This result indicates
    that there is an issue somewhere in the joint assumptions made by the backdoor
    estimand and this estimator. If we then used this refuter with other backdoor
    estimators and they were not refuted, we would have narrowed down the source of
    the issue to the statistical assumptions made by this estimator.    ### 11.5.4
    Replacing outcome with a dummy outcome    We can substitute the outcome variable
    with a dummy variable. The ATE in this case should be 0, because the treatment
    has no effect on this dummy. We’ll simulate it as a linear function of some of
    the confounders so the outcome still has a meaningful relationship with some of
    the covariates.    Let’s try this with the front door estimator.    ##### Listing
    11.18 Replacing the outcome variable with a dummy variable    [PRE40]  #1 Create
    a function that generates a new dummy outcome variable as a linear function of
    the covariates. #2 Runs refute_estimate with a dummy outcome refuter   Again,
    the *p*-value is calculated under the null hypothesis that `New` `effect` equals
    0, and a low *p*-value refutes our assumptions. In this case, our assumptions
    are not refuted.    Next, we’ll evaluate the sensitivity of the analysis to unobserved
    confounding.    ### 11.5.5 Testing robustness to unmodeled confounders    Our
    backdoor adjustment estimand assumes that the adjustment set blocks all backdoor
    paths. If there were a confounder that we failed to adjust for, that assumption
    is violated, and our estimate would have a confounder bias. That is not necessarily
    the worst thing; if we adjust for all the *major* confounders, bias from unknown
    confounders might be small and not impact our results by much. On the other hand,
    missing a major confounder could lead us to conclude that there is a nonzero ATE
    when one doesn’t exist, or conclude a positive ATE when the true ATE is negative,
    or vice versa. We can therefore test how robust our analysis is to the introduction
    of latent confounders that our model failed to capture. The hope is that the new
    estimate does not change drastically when we introduce some modest influence from
    a newly introduced confounder.    ##### Listing 11.19 Adding an unobserved confounder    [PRE41]  #1
    Setting up a refuter that adds an unobserved common cause   This code does not
    return a *p*-value. It produces the heatmap we see in figure 11.11, showing how
    quickly the estimate changes when the unobserved confounder assumption is violated.
    The horizontal axis shows the various levels of influence the unobserved confounder
    has on the outcome, and the vertical axis shows the various levels of influence
    the confounder can have on the treatment. The color corresponds to the new effect
    estimates that result at different levels of influence.  ![figure](../Images/CH11_F11_Ness.png)  #####
    Figure 11.11 A heatmap illustrating the effects of adding an unobserved confounder
    on the ATE estimate    The code also prints out the following.    [PRE42]    Here,
    we see that the ATE is quite sensitive to the effect the confounder has on the
    treatment. Note that you can change the default parameters of the refuter to experiment
    with different impacts the confounder could have on the treatment and outcome.    Now
    that we’ve run through a full workflow in DoWhy, let’s explore how we’d build
    a similar workflow using the tools of probabilistic machine learning.    ## 11.6
    Causal inference with causal generative models    At the end of chapter 10, we
    calculated an ATE using the `do` intervention operator and a probabilistic inference
    algorithm. This is a powerful universal approach to doing causal inference that
    leverages cutting-edge probabilistic machine learning. But this wasn’t *estimation*.
    Estimation requires data. It would be *estimation* if we estimated the model parameters
    from data *before* running that workflow with the `do` function and probabilistic
    inference.    In this section, we’ll run through a full ATE estimation workflow
    that uses the `do` intervention operator and probabilistic inference. We used
    MCMC for the probabilistic inference step in chapter 10, but here we’ll use variational
    inference with a variational autoencoder to handle latent variables in the data.
    Further, we’ll use a Bayesian estimation approach, meaning we’ll assign prior
    probabilistic distributions to the parameters. The ATE inference step with the
    intervention operator will depend on sampling from the posterior distribution
    on parameters.    The advantage of this approach relative to using DoWhy is being
    able to use modern deep learning tools to work with latent variables as well as
    use Bayesian modeling to address uncertainty. Further, this approach will work
    in cases of causal identification that are not covered by DoWhy (e.g., edge cases
    of graphical identification, identification derived from assignment functions
    or prior distributions, partial identification, etc.).    This approach to ATE
    estimation is a specific case of a general approach to causal inference where
    we train a causal graphical model, *transform* the model in some way that reflects
    the causal query (e.g., with an intervention operator), and then run a probabilistic
    inference algorithm. Let’s review various ways we can transform a model for causal
    inference.    ### 11.6.1 Transformations for causal inference    We have seen
    several ways of modifying a causal model such that it can readily infer a causal
    query. We’ll call these “transformations”: we transform our model into a new model
    that targets a causal inference query. Let’s review the transformations we’ve
    seen so far.    #### Graph Surgery    One of the transformations was basic *graph
    surgery*, illustrated in figure 11.12\. This operation implements an *ideal intervention*,
    setting the intervention target to a constant and severing the causal influence
    from the parents. This operation allows us to use our model to infer *P*(*I*[*E*][=1]),
    the ATE, and similar level 2 queries, and it’s how we have been implementing interventions
    in pgmpy.  ![figure](../Images/CH11_F12_Ness.png)  ##### Figure 11.12 Graph surgery
    is a transformation that implements an ideal intervention by removing incoming
    causal influence on the target node and setting the target node to a constant.    We
    implemented graph surgery in pgmpy by using the `do` method on the `BayesianNetwork`
    class, and then we added a hack that modified the `TabularCPD` object assigned
    to the intervention project so that the intervention value had a probability of
    1\.    PyMC is a probabilistic programming language similar to Pyro. It does implicit
    graph surgery by transforming the logic of the model. For example, PyMC might
    specify *E*, a function of *G,* as `E` `=` `Bernoulli("E",` `p=f(G))`. PyMC uses
    a `do` function to implement the intervention, as in `do(model, {"E":` `1.0})`.
    Under the hood, this function does implicit graph surgery by effectively replacing
    `E` `=` `Bernoulli("E",` `p=f(G))` with `E` `=` `1.0.`    #### Node-splitting    In
    chapter 10, we discussed a slightly nuanced version of graph surgery called a
    node-splitting operation, illustrated in figure 11.13\. Node-splitting converts
    the graph to a *single world intervention graph*, allowing us to infer level 2
    queries just as graph surgery does. It also allows us to infer level 3 queries
    where the factual conditions and hypothetical outcome don’t overlap, such as *P*(*I*[*E*][=][0]|*E*=1)
    (though doing so relies on an additional “single world assumption,” as discussed
    in chapter 10).  ![figure](../Images/CH11_F13_Ness.png)  ##### Figure 11.13 The
    node-splitting transform splits the intervention target into a constant that keeps
    the children and a random variable that keeps the parents.    Pyro’s `do` function
    implements node-splitting (though it behaves just like PyMC’s `do` function if
    you don’t target level 3 queries).    #### Multi-world transformation    We also
    saw how to transform a structural causal model into a parallel world graph. Let’s
    call this a multi-world transformation, illustrated in figure 11.14.  ![figure](../Images/CH11_F14_Ness.png)  #####
    Figure 11.14 Yet another transform converts the model into a parallel-world model.    We
    created parallel-world models by hand in chapter 9 with pgmpy and Pyro. The y0
    library produces parallel world graphs from DAGs. ChiRho, the causal library that
    extends Pyro, has a `TwinWorldCounterfactual` handler that does the multi-world
    transformation.    #### Transformation to a counterfactual graph    Recall that
    we can also transform the causal DAG to a counterfactual graph (which, in the
    case of a level 2 query like *P*(*I*[*E*][=][1]), will simplify to the result
    of graph surgery). Y0 creates a counterfactual graph from your DAG and a given
    query. Future versions of causal probabilistic ML libraries may provide the same
    transformation for a Pyro/PyMC/ChiRho type model.    ### 11.6.2 Steps for inferring
    a causal query with a causal generative model    Given a causal generative model
    and a target causal query, we have two steps to infer the target query: first,
    apply the transformation, and then run probabilistic inference.    We did this
    with the online gaming example at the end of chapter 10\. We targeted *P*(*I*[*E*][=][0])
    and *P*(*I*[*E*][=][0]|*E*=1). For each of these queries, we used the `do` function
    in Pyro to modify the model to represent the intervention *E*=0\. In the case
    of *P*(*I*[*E*][=][0]|*E*=1), we also conditioned on *E*=1\. Then we ran an MCMC
    algorithm to generate samples from these distributions. We also used the probabilistic
    inference with parallel-world graphs to implement level 3 counterfactual inferences
    in chapter 9.    ### 11.6.3 Extending inference to estimation    To extend this
    workflow to estimation, like the DoWhy methods in this chapter, we simply need
    to add a parameter estimation step to our causal graphical inference workflow:    1.  Estimate
    model parameters. 2.  Apply the transformation. 3.  Run probabilistic inference
    on the transformed model.    Let’s look at how to do this with the online game
    data. For simplicity, we’ll work with a reduced model that drops the instruments
    and the collider, since we won’t be using them.    We’ll model the causal Markov
    kernels of each node with some unique parameter vector. We can estimate the parameters
    any way we like, but to stay on brand with probabilistic reasoning, let’s use
    a Bayesian setup, treating each parameter vector as its own random variable with
    its own prior probability distribution. Figure 11.15 illustrates a plate model
    representation of the causal DAG (we discussed plate model visualizations in chapter
    2), drawing these random variables as new nodes, using Greek letters to highlight
    the fact that they are parameters, rather than causal components of the real world
    DGP.  ![figure](../Images/CH11_F15_Ness.png)  ##### Figure 11.15 A plate model
    of the causal DAG with new nodes representing parameters associated with each
    causal Markov kernel. There is a single plate with *N* identical and independent
    observations in the training data. *θ* corresponds to parameters, which are outside
    the plate, because the parameters are the same for each of the *N* data points.    In
    this case, Bayesian estimation will target the posterior distribution:  ![figure](../Images/ness-ch11-eqs-6x.png)  where
    ![equation image](../Images/eq-chapter-11-313-1.png) each represent the *N* examples
    of *E*, *Y*, *T*, *G*, *S*, *W*, *I* in the data.    Estimating the *θ*s in this
    case is easy. For example, in pgmpy we just run `model.fit(data, estimator=` `BayesianEstimator,
    ...)`, where “. . .” contains arguments that specify the type of prior to assign
    the *θ*s. Pgmpy uses the posterior to give us point estimates of the *θ*s. In
    Pyro, we just write sample statements for the *θ*s and use one of Pyro’s various
    inference algorithms to get samples from the posterior.    But the causal effect
    methods in DoWhy highlight the ability to do causal inferences when some causal
    variables are *latent*, such as confounders:    *   Backdoor adjustment with some
    latent confounders is possible (e.g., *Prior Experience*) if you have a valid
    adjustment set (*Time Spent Playing*, *Guild Membership*, and *Player Skill Level*).
    *   If too many confounders are latent, such that you do not have backdoor adjustment,
    you can use other techniques, such as using instrumental variables and front-door
    adjustment.    So for causal generative modeling to compete with DoWhy, it needs
    to accommodate latent variables. Let’s consider the case where the backdoor adjustment
    estimand is not identified. Next, we’ll explore how we can train a latent causal
    generative model and then apply the transformation and probabilistic inference.    In
    this model, we’ll assume that *Guild Membership* is the only observed confounder,
    as in figure 11.16\. In this case, we no longer have backdoor identification.  ![figure](../Images/CH11_F16_Ness.png)  #####
    Figure 11.16 *Guild Membership* is the only observed confounder, so the backdoor
    estimand is not identified.    ##### Setting up your environment    The following
    code is written with torch 2.2, pandas 1.5, and pyro-ppl 1.9\. We’ll use matplotlib
    and seaborn for plotting.    Let’s first reload and modify the data to reflect
    this paucity of observed variables.    ##### Listing 11.20 Load and reduce data
    to a subset of observed variables    [PRE43]  #1 Load the data. #2 Drop everything
    but Guild Membership, Side-Quest Engagement, Won Items, and In-Game Purchases.
    #3 Convert the data to tensors and dynamically set the device for performing tensor
    computations depending on the availability of a CUDA-enabled GPU.   Now we are
    targeting the following posterior:  ![figure](../Images/ness-ch11-eqs-8x.png)  Targeting
    this posterior is harder because, since the observations of ![equation image](../Images/eq-chapter-11-327-1.png)
    are not observed, they are not available to help in inferring *θ*[*Y*], *θ*[*T*],
    and *θ*[*S*]. In fact, in general, *θ*[*Y*], *θ*[*T*], and *θ*[*S*] are *underdetermined*,
    meaning multiple configurations of {*θ*[*Y*], *θ*[*T*], *θ*[*S*]} would be equally
    likely given the data. Further, we’ll have trouble estimating with *θ*[*E*] and
    *θ*[*I*]because it will be hard to disentangle them from the other latent variables.    But
    it doesn’t matter! At least, not in terms of our goal of inferring *P*(*I*[*E*][=][*e*]),
    because we know we have identified the front-door estimand of *P*(*I*[*E*][=][*e*]).
    In other words, the existence of a front-door estimand proves we can infer *P*(*I*[*E*][=][*e*])
    from the observed variables regardless of the lack of identifiability of some
    of the parameters.    ### 11.6.4 A VAE-inspired model for causal inference    We’ll
    make our modeling easier by creating proxy variables ![equation image](../Images/eq-chapter-11-330-1.png)
    and *θ*[*Z*] to stand in for {![equation image](../Images/eq-chapter-11-330-2.png)}
    and {*θ*[*Y*], *θ*[*T*], *θ*[*S*]} respectively. Collapsing the latent confounders
    into these proxies reduces the dimensionality of the estimation problem, and any
    loss of information that occurs from collapsing these variables won’t matter because
    we are ultimately relying on information flowing through the front door. We’ll
    create a causal generative model inspired by the variational autoencoder, where
    ![equation image](../Images/eq-chapter-11-330-3.png) is a latent encoding and
    *θ*[*E*] and *θ*[*I*] become weights in decoders. This is visualized in figure
    11.17.    Now our inference will target the posterior:  ![figure](../Images/ness-ch11-eqs-11x.png)  Our
    model will have two decoders. One decoder maps ![equation image](../Images/eq-chapter-11-333-1.png)
    and *G* to *E*, returning a derived parameter *ρ*`_engagement` that acts as the
    probability that *Side-Quest Engagement* is high. Let’s call this network `Confounders2Engagement`.
    As shown in figure 11.17, ![equation image](../Images/eq-chapter-11-333-2.png)
    is a vector with *K* elements, but we’ll set *K*=1 for simplicity.  ![figure](../Images/CH11_F17_Ness.png)  #####
    Figure 11.17 VAE-inspired model where latent vector *Z* of length *K* proxies
    for the latent confounders in figure 11.16    ##### Listing 11.21 Specify `Confounders2Engagement`
    neural network    [PRE44]  #1 Input is confounder proxy Z concatenated with Guild
    Membership. #2 Choose a hidden dimension of width 5. #3 Linear map from input
    to hidden dimension #4 Linear map from hidden dimension to In-Game Purchases location
    parameter #5 Activation function for hidden layer #6 Activation function for Side-Quest
    Engagement parameter #7 From input to hidden layer #8 From hidden layer to *ρ*_engagement   Next,
    let’s specify another neural net decoder that maps *Z*, *W*, and *G* to a location
    and scale parameter for *I*. Let’s call this `PurchasesNetwork`.    ##### Listing
    11.22 `PurchasesNetwork` neural network    [PRE45]  #1 Input is confounder proxy
    Z concatenated with Guild Membership and Won Items. #2 Choose a hidden dimension
    of width 5. #3 Linear map from input to hidden dimension #4 Linear map from hidden
    dimension to In-Game Purchases location parameter #5 Linear map from hidden dimension
    to In-Game Purchases scale parameter #6 Activation for hidden layer #7 From input
    to hidden layer #8 Mapping from hidden layer to location parameter for purchases
    #9 Mapping from hidden layer scale parameter for purchases. The 1e-6 lets us avoid
    scale values of 0.   Now we use both networks to specify the causal model. The
    model will take a dictionary of parameters called `params` and use them to sample
    the variables in the model. The Bernoulli distributions of *Guild Membership*
    and *Won Items* have parameters passed in a dictionary called `params`, with keys
    *ρ*`_member`*representing**θ*[*G*], and *ρ*`_won_engaged` and *ρ*`_won_not_engaged`
    together representing *θ*[*W*]. *ρ*`_engagement`, which represents the *Side-Quest
    Engagement* parameter **θ*[*E*], is set by the output of `Confounders2Engagement`,
    and `μ_purchases` and `σ_purchases`, which jointly represent the *In-Game Purchases*
    parameter *θ*[*Y*], are the output of `PurchaseNetwork`. The parameter set *θ*[*Z*]
    is a location and scale parameter for a normal distribution. Rather than a learnable
    *θ*[*Z*], I use fixed *θ*[*Z*]= {0, 1} and let the neural nets handle the linear
    transform for *Z*.*   *##### Listing 11.23 Specify the causal model    [PRE46]  #1
    The causal model #2 A latent variable that acts as a proxy for other confounders
    #3 Whether someone is in a guild #4 Use confounders_ 2_engagement to map is_guild_
    member and z to a parameter for Side-Quest Engagement and In-Game Purchases. #5
    Modeling Side-Quest Engagement #6 Modeling amount of Won Items #7 Use purchases_network
    to map is_guild_member, z, and won_items to in_game_purchases. #8 Model in_game_purchases.   This
    model represents a single data point. Now we need to extend the model to every
    example data point in the dataset. We’ll build a `data_model` that loads the neural
    networks, assigns priors to the parameters, and models the data.    ##### Listing
    11.24 Build a data model    [PRE47]  #1 Initialize the neural networks. #2 pyro.module
    lets Pyro know about all the parameters inside the networks. #3 Sample from prior
    distribution for *ρ*_member #4 Sample from prior distribution for *ρ*_won_engaged
    #5 Sample from prior distribution for *ρ*_won_not_engaged #6 The plate context
    manager declares N independent samples (observations) from the causal variables.   `render_model`
    lets us visualize the resulting plate model, producing figure 11.18\. *ρ*`_member`,
    *ρ*`_won_engaged`, *ρ*`_won_not_engaged` are the parameters we wish to estimate,
    alongside the weights in the neural nets.  ![figure](../Images/CH11_F18_Ness.png)  #####
    Figure 11.18 The plate model representation produced by Pyro    Now that we’ve
    specified the model, lets set up inference with SVI.    ### 11.6.5 Setting up
    posterior inference with SVI    We have a data model over an underlying causal
    model, so we can now move on to inference. Using SVI, we need to build a *guide
    function* that represents a distribution that approximates the posterior—the guide
    function will have hyperparameters directly optimized during training, which will
    bring the approximating distribution as close as possible to the posterior.    #####
    Why do inference with SVI and not MCMC?    In chapter 10, we used an MCMC inference
    algorithm to derive *P*(*I**[E]*[=0]) and *P*(*I**[E]*[=0]|*E*=1) from *P*(*G*,
    *W*, *I*, *E*). The *θ* parameters were given. Now the *θ* parameters are unknown,
    and we are using *Bayesian estimation*, meaning we want to infer *I**[E]*[=]*[e]*
    conditional on values of those *θ* parameters sampled from a posterior distribution
    derived from training data. We do this by considering variables {![equation image](../Images/eq-chapter-11-348-1.png),
    ![equation image](../Images/eq-chapter-11-348-2.png), ![equation image](../Images/eq-chapter-11-348-3.png),
    ![equation image](../Images/eq-chapter-11-348-4.png), ![equation image](../Images/eq-chapter-11-348-5.png)}
    where {![equation image](../Images/eq-chapter-11-348-6.png), ![equation image](../Images/eq-chapter-11-348-7.png),
    ![equation image](../Images/eq-chapter-11-348-8.png), ![equation image](../Images/eq-chapter-11-348-9.png)}
    is a set of variables in the training data, ![equation image](../Images/eq-chapter-11-348-10.png)
    is a latent proxy for our latent confounders, and each of these variables are
    vectors of length *N*, the size of our training data.    The challenge is the
    computational complexity of MCMC algorithms generally grows exponentially in the
    dimension of the posterior. When ![equation image](../Images/eq-chapter-11-349-1.png)
    is a latent variable, it gets added to the posterior as another unknown along
    with the *θ*s, so the shape of the posterior increases by ![equation image](../Images/eq-chapter-11-349-2.png)’s
    dimension, which is at least the size of the training data *N*. This poses a challenge
    when *N* is large. We want an inference method that works well with large data
    so we can leverage all that data to do cool things like use deep neural networks
    to help us proxy our latent confounders in a causal generative model. So here
    we use SVI instead of MCMC because SVI shines in high-dimensional large data settings.    The
    main ingredient of the guide function is an encoder that will map *Side-Quest
    Engagement*, *Won Items*, and *In-Game Purchases* to *Z*; i.e., it will *impute*
    the latent values of *Z*.    ##### Listing 11.25 Create an encoder for *Z*    [PRE48]  #1
    Input dimension is 3 because it will combine Side-Quest Engagement, In-Game Purchases,
    and Guild Membership. #2 I use a simple univariate Z, but we could give it higher
    dimension with sufficient data. #3 The width of the hidden layer is 5. #4 Go from
    input to hidden layer. #5 Mapping from hidden layer to location parameter for
    Z #6 Mapping from hidden layer scale parameter to Z   Now, using the encoder,
    we build the overall guide function. In the following guide, we’ll sample the
    parameters *ρ*`_member`, *ρ*`_won_engaged`, and *ρ*`_won_not_engaged` from beta
    distributions parameterized by constants set using `param`. These “hyperparameters”
    are optimized during training, alongside the weights of the neural networks.    #####
    Listing 11.26 Build the guide function (approximating distribution)    [PRE49]  #1
    The guide samples *ρ*_member from a beta distribution where the shape parameters
    are trainable. #2 *ρ*_won_engaged and *ρ*_won_ not_engaged are also sampled from
    beta distributions with trainable parameters. #3 *ρ*_won_engaged and *ρ*_won_not_engaged
    are also sampled from beta distributions with trainable parameters. #4 Z is sampled
    from a normal with parameters returned by the encoder.   Finally, we set up the
    inference algorithm and run the training loop.    ##### Listing 11.27 Run the
    training loop    [PRE50]  #1 Reset parameter values in case we restart the training
    loop. #2 Set up Adam optimizer. A learning rate (“lr”) of 0.001 may work better
    if using CUDA. #3 Condition the data_model on the observed data. #4 Set up SVI.
    #5 Run the training loop.   We’ll now plot the loss curve to see how training
    performed.    ##### Listing 11.28 Plot the losses during training    [PRE51]  #1
    Plot the log of training loss, since loss is initially large.   The losses shown
    in figure 11.19 indicate training has converged.  ![figure](../Images/CH11_F19_Ness.png)  #####
    Figure 11.19 Log of ELBO loss during training    We can print the trained values
    of the hyperparameters (`α_member`, *β*`_member`, `α_won_engaged`, *β*`_won_engaged`,
    `α_won_not_engaged`, and *β*`_won_not_engaged`).    ##### Listing 11.29 Print
    the values of the trained parameters in the guide function    [PRE52]    This
    returned the following:    [PRE53]    We’ll approximate our posterior by sampling
    *ρ*`_member`, *ρ*`_won_engaged`, and *ρ*`_won_not_engaged` from beta distributions
    with these values, sampling *Z* from a normal(0, 1), and then sampling the remaining
    causal variables based on these values.    ### 11.6.6 Posterior predictive inference
    of the ATE    Given a sample of the parameters and a sample vector of *Z* from
    the guide (our proxy for the posterior), we can simulate a new data set. A common
    way of checking how well a Bayesian model fits the data is to compare this simulated
    data with the original data. This comparison is called a *posterior predictive
    check*, and it helps us understand if the trained model is a good fit for the
    data. In the following code, we’ll do a posterior predictive check of *In-Game
    Purchases*; we’ll use the guide to generate samples and use those samples to repeatedly
    simulate *In-Game Purchase* datasets. For each simulated dataset, we’ll create
    a density curve. We’ll then plot these curves, along with the density curve of
    the *In-Game Purchases* in the original data.    ##### Listing 11.30 Posterior
    predictive check of *In-Game Purchases*    [PRE54]  #1 Simulate data from the
    (approximate) posterior predictive distribution. #2 For each batch of simulated
    data, create and plot a density curve of In-Game Purchases. #3 Overlay the empirical
    density distribution of In-Game Purchases so we can compare it with the predictive
    plots.   This produces a plot as in figure 11.20\. The degree to which the simulated
    distribution matches the empirical distribution depends on the model, the size
    of the data, and how well the model is trained.  ![figure](../Images/CH11_F20_Ness.png)  #####
    Figure 11.20 Posterior predictive check of *In-Game Purchases*. Grey lines are
    density curves calculated on simulations from the posterior predictive distribution.
    The black line is the empirical density (density curves calculated on the data
    itself). More overlap indicates the model fits the data well.    Our Bayesian
    estimator of the ATE will be our approach of applying transformation and inference
    to the posterior distribution represented by our model and guide. Since the ATE
    is *E*(*I*[*E*][=][1]) – *E*(*I*[*E*][=][0]), we’ll do posterior predictive sampling
    from *P*(*I*[*E*][=][1]) and *P*(*I*[*E*][=][0]).    First, we’ll use `pyro.do`
    to transform the model to represent the intervention. Then we’ll do forward sampling
    from the model using the `Predictive` class. This will sample 1,000 simulated
    datasets, each equal in length to the original data, and each corresponding to
    a random sample of *ρ*`_member`, *ρ*`_won_engaged`, *ρ*`_won_unengaged`, and a
    data vector of *Z* values. Objects from the `Predictive` class do simple forward
    sampling. If we needed to condition on anything (e.g., conditioning on *E*=1 in
    *P*(*I*[*E*][=][0]|*E*=1)), we’d need to use another inference approach (e.g.,
    importance sampling, MCMC, etc.).    ##### Listing 11.31 Sampling from the posterior
    predictive distributions *P*(*I**[E]*[=][0]) and *P*(*I**[E]*[=][1])    [PRE55]  #1
    Apply pyro.do transformation to implement intervention do(E=0). #2 Sample 1,000
    samples of datasets from P(I *[E]* [=] [0]). #3 Apply pyro.do transformation to
    implement intervention do(E=1). #4 Sample 1,000 samples of datasets from P(I *[E]*
    [=] [1]).   We can plot these two sets of posterior predictive samples as follows:    #####
    Listing 11.32 Plot density curves of predictive datasets sampled from *P*(*I**[E]*[=][1])
    and *P*(*I**[E]*[=][0])    [PRE56]  #1 For each sample, use kdeplot to draw a
    curve. Plot *P*( *I* *[E]* [=] [0]) as dark grey and *P*( *I* *[E]* [=] [1]) as
    light gray. #2 Plot the density curves.   Whereas figure 11.20 plotted a predictive
    distribution on *P*(*I*), figure 11.21 plots predictive density plots of *P*(*I*[*E*][=][0])
    and *P*(*I*[*E*][=][1]). We can see that the distributions differ.  ![figure](../Images/CH11_F21_Ness.png)  #####
    Figure 11.21 Posterior predictive visualization of density curves calculated from
    simulated data from *P*(*I**[E]*[=1]) (light gray) and *P*(*I**[E]*[=0]) (dark
    gray)    Finally, to estimate *E*(*I*[*E*][=][1]) and *E*(*I*[*E*][=][0]), we
    just need take the means of each posterior predictive sample dataset simulated
    from *P*(*I*[*E*][=][1]) and *P*(*I*[*E*][=][1]), respectively. This will yield
    1,000 samples of posterior predictive values of the ATE. Variation between the
    samples reflects posterior uncertainty about the ATE.    ##### Listing 11.33 Estimate
    the ATE    [PRE57]  #1 Estimate *E*( *I* *[E]* [=] [1]). #2 Estimate *E*( *I*
    *[E]* [=] [0]). #3 Estimate the ATE = *E*( *I* *[E]* [=] [1]) – *E*( *I* *[E]*
    [=] [0]). #4 Use a density curve to visualize posterior variation in the ATE values.   This
    prints figure 11.22, a visualization of the posterior predictive distribution
    of the ATE.  ![figure](../Images/CH11_F22_Ness.png)  ##### Figure 11.22 Posterior
    predictive distribution of the ATE    With a Bayesian approach, we get a posterior
    predictive distribution of the ATE. If we want a CATE, we can simply modify the
    posterior predictive inference to condition on other variables. If we want a point
    estimate of the ATE, we can take the mean of these predictive samples. More data
    reduces variance in the ATE distribution (assuming the ATE is identified) as in
    figure 11.23.  ![figure](../Images/CH11_F23_Ness.png)  ##### Figure 11.23 Posterior
    uncertainty declines with more data.    We can construct *credible intervals*
    (the Bayesian analog to confidence intervals) by taking percentiles from this
    distribution.    ### 11.6.7 On the identifiability of the Bayesian causal generative
    inference    We got these results with a causal latent variable model, where *Z*
    was the latent variable. We are no strangers to latent variable models in probabilistic
    machine learning, but are they safe for causal inference? For example, if we could
    do causal inference with this latent variable model, what is to stop us from using
    the model in figure 11.24?    We could train this model, apply the transformations,
    get samples from the posterior predictive distribution of the ATE, and get an
    answer. But we lack graphical identification in this case. Our answer would have
    confounder bias that we couldn’t fix with more data, at least not without some
    strong, non-graphical assumptions (e.g., in the priors or in the functional relationships
    between variables).    Our model has graphical identification. In our case, we
    observed a mediator in *Won Items*, so we know we have a front-door estimand.
    Our causal generative model estimation procedure is just another estimator of
    that estimand.  ![figure](../Images/CH11_F24_Ness.png)  ##### Figure 11.24 The
    causal latent variable model with no mediator *W*, and thus no identification.
    If we had fit this model and used it to infer the ATE, we’d get a result. But
    without identification, we wouldn’t be able to eliminate confounder bias, even
    with more data.    ### 11.6.8 Closing thoughts on causal latent variable models    This
    approach of combining causal generative models with latent variables and deep
    learning is not limited to ATEs—it is general to all causal queries. We only need
    to select the right transformation for the query. This approach “commodifies inference”
    by relying on auto-differentiation tools to do the statistical and computational
    heavy lifting, instead of having to understand and implement different estimators
    like in DoWhy. It also scales to multidimensional causes, outcomes, and other
    variables in a way DoWhy does not. An additional advantage is that tools like
    Pyro and PyMC allow you to put Bayesian priors on the causal models themselves.
    Since the lack of causal identification boils down to model uncertainty, putting
    priors on models gives us an additional way of encoding domain assumptions that
    yield additional identification.    ## Summary    *   DoWhy provides a useful
    workflow for identifying and estimating causal effects. *   In step 1 of the causal
    inference workflow, we specify our target query. In this chapter, we focused on
    causal effects (ATEs and CATEs). *   In step 2 we specify our causal model. We
    specified a DAG in Graphviz DOT format and loaded it into a `CausalModel` in DoWhy.
    *   In step 3 we run identification. DoWhy identified backdoor, front-door, and
    instrumental variable estimands. *   Each estimand relies on a different set of
    causal assumptions. If you are more confident in the causal assumptions of one
    estimand than others, you should target that estimand. *   We targeted the backdoor
    estimand with linear regression, propensity score methods, and machine learning
    (ML) methods. *   The backdoor adjustment set is the set of backdoor variables
    we adjust for in the backdoor adjustment estimand. A valid adjustment set d-separates
    all backdoor paths. There could be more than one valid set. *   In step 4 we estimate
    our selected estimand. DoWhy makes it easy to try different estimators. *   Linear
    regression is a popular estimand because it is simple, familiar, and gives a point
    estimate of the ATE even for continuous causes. *   A propensity score is traditionally
    the probability a subject in the data is exposed to the treatment value of the
    binary cause (treatment) variable, conditional on the confounders in the adjustment
    set. It is often modeled using logistic regression. *   However, a propensity
    score can be any variable you construct that renders the treatment variable conditionally
    independent of the adjustment set. *   Propensity score methods include matching,
    stratification, and inverse probability weighting. *   ML methods targeting the
    backdoor estimand include double ML and meta learners. DoWhy provides a wrapper
    to EconML that implements several ML methods. *   Generally, ML methods are a
    good choice when you have larger datasets. They allow you to rely on fewer statistical
    assumptions. However, calculating confidence intervals on the estimates is computationally
    expensive. *   Instrumental variable estimation and front-door estimation don’t
    rely on having a valid backdoor adjustment set, but they rely on different causal
    assumptions. *   In step 5, we run refutation analysis. Refutation is a sensitivity
    analysis that attempts to refute the causal and statistical assumptions we rely
    on in estimating our target query. *   Causal generative models combine model
    transformations, such as graph mutilation, node-splitting, and multi-world transforms,
    with probabilistic inference to do causal inference. *   This approach becomes
    an estimator of an identified estimand when the model parameters are learned from
    data. *   When there are latent variables, such as latent confounders, you can
    train the causal generative model as a latent variable model. *   The causal inference
    with the latent variable model will work if you have graphical identification.
    If not, you’ll need to rely on other identifying assumptions.*[PRE58]``'
  prefs: []
  type: TYPE_NORMAL
