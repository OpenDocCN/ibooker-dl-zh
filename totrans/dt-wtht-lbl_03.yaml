- en: 3 Dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The curse of dimensionality and its disadvantages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various methods of reducing dimensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular value decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python solutions for both principal component analysis and singular value decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A case study on dimension reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge is a process of piling up facts; wisdom lies in their simplification.—Martin
    H. Fischer
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We face complex situations in life. Life throws multiple options at us, and
    we choose a few viable ones from them. This decision of shortlisting is based
    on the significance, feasibility, utility, and perceived profit from each of the
    options. The ones that fit the bill are then chosen. A perfect example can be
    selecting your vacation destination. Based on the weather, travel time, safety,
    food, budget, and several other options, we choose a few where we would like to
    spend our next vacation. In this chapter, we study precisely the same—how to reduce
    the number of options—albeit in the data science and machine learning world.
  prefs: []
  type: TYPE_NORMAL
- en: In the last chapter, we covered major clustering algorithms. We also went over
    a case study. The datasets we generate and use in such real-world examples have
    a lot of variables. Sometimes, there can be more than 100 variables or *dimensions*
    in the data. But not all of them are important. Having a lot of dimensions in
    the dataset is referred to as the curse of dimensionality. To perform any further
    analysis, we choose a few from the list of all of the dimensions or variables.
    In this chapter, we study the need for dimension reductions, various dimensionality
    techniques, and the respective pros and cons. We will dive deeper into the concepts
    of principal component analysis (PCA) and singular value decomposition (SVD) and
    their mathematical foundations and complement these with Python implementation.
    Also, continuing our structure from the last chapter, we will examine a real-world
    case study in the telecommunication sector. There are other advanced dimensionality
    reduction techniques like t-distributed stochastic neighbor embedding (t-SNE)
    and linear discriminant analysis (LDA), which we will explore in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering and dimensionality reductions are the major categories of unsupervised
    learning. We studied major clustering methods in the last chapter, and we discuss
    dimensionality reduction in this chapter. With these two solutions, we cover a
    lot of ground in the unsupervised learning domain. But there are many more advanced
    topics to be covered, which are part of the latter chapters of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first understand what we mean by the “curse of dimensionality.”
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Technical toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are using the same version of Python as in the last chapters. Jupyter Notebook
    will be used in this chapter too.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the datasets and code files are available at the GitHub repository at ([https://mng.bz/ZlBR](https://mng.bz/ZlBR)).
    You need to install the following Python libraries to execute the code: `numpy`,
    `pandas`, `matplotlib`, `scipy`, and `sklearn`. Since you have used the same packages
    in the last chapter, you don’t need to install them again. CPU is good enough
    for execution, but if you face some computing problems, switch to GPU or Google
    Colab. Refer to the appendix if you face any problems with the installation of
    any of these packages.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 The curse of dimensionality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us continue with the vacation destination example we introduced earlier.
    The choice of destination is dependent on several parameters: safety, availability,
    food, nightlife, weather, budget, health, and so on. Having too many parameters
    is confusing. Let us understand by a real-life example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this: a retailer wishes to launch a new range of shoes in the market,
    and for that, a target group of customers should be chosen. This target group
    will be reached through email, SMS, newsletter, etc. The business objective is
    to entice these customers to buy the newly launched shoes. From the entire customer
    base, the target group of customers can be chosen based on variables like customer
    age, gender, budget, preferred category, average spend, frequency of shopping,
    and so on. These many variables or *dimensions* make it hard to shortlist the
    customers based on a sound data analysis technique. We would be analyzing too
    many parameters simultaneously, examining the effect of each on the shopping probability
    of the customer, and hence it becomes too tedious and confusing of a task. It
    is the curse of dimensionality problem we face in real-world data science projects.
    We can face the curse of dimensionality in one more situation wherein the number
    of observations is fewer than the number of variables. Consider a dataset where
    the number of observations is *X*, while the number of variables is more than
    *X*—in such a case, we face the curse of dimensionality.'
  prefs: []
  type: TYPE_NORMAL
- en: An easy method to understand any dataset is through visualization. Let’s visualize
    a dataset in a vector-space diagram. If we have only one attribute or feature
    in the dataset, we can represent it in one dimension (see the left diagram in
    figure 3.1). For example, we might wish to capture only the height of an object
    using a single dimension. If we have two attributes, we need two dimensions, as
    shown in the middle diagram in figure 3.1, wherein to get the area of an object,
    we will require both length and width. If we have three attributes, for example,
    to calculate the volume, which requires length, width, and height, we require
    a 3D space, as shown in the diagram at right in figure 3.1\. This requirement
    will continue to grow based on the number of attributes.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F01_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 Only one dimension is required to represent the data points—for example,
    to represent the height of an object (left). We need two dimensions to represent
    a data point. Each data point can correspond to the length and width of an object,
    which can be used to calculate the area (middle). Three dimensions are required
    to show a point (right). Here, it can be length, width, and height, which are
    required to get the volume of an object. This process continues based on the number
    of dimensions present in the data.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Consider a dataset where you have an attribute for a data point—for example,
    gender. Then we add age and then education, address, and so on. To represent these
    attributes, the number of dimensions will keep on increasing. Hence, it is quite
    easy for us to conclude that with an increase in the number of dimensions, the
    amount of space required to represent increases by leaps and bounds. This is referred
    to as the c*urse of dimensionality*. The term was introduced by Richard E. Bellman
    and is used to refer to the problem of having too many variables in a dataset—some
    of which are significant while many others may be less important.
  prefs: []
  type: TYPE_NORMAL
- en: There is another well-known theory named the *Hughes phenomenon,* shown in figure
    3.2\. Generally, in data science and machine learning, we wish to have as many
    variables as possible to train our model. The performance of the supervised learning
    classifier algorithm will increase to a certain limit and will peak with the most
    optimal number of variables. But, using the same amount of training data and with
    an increased number of dimensions, there is a decrease in the performance of a
    supervised classification algorithm. In other words, it is not advisable to have
    the variables in a dataset if they are not contributing to the accuracy of the
    solution. We should remove such variables from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F02_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 The Hughes phenomenon shows that the performance of a machine learning
    model will improve initially with an increase in the number of dimensions. But
    a further increase leads to a decrease in the model’s performance.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'An increase in the number of dimensions has the following effects on the machine
    learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: As the model deals with an increased number of variables, the mathematical complexity
    increases. For example, in the case of the k-means clustering method we discussed
    in the last chapter, when we have a greater number of variables, the distance
    calculation between respective points will become complex. Hence the overall model
    becomes more complex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset generated in a larger dimensional space can be much sparser as compared
    to a smaller number of variables. The dataset will be sparser as some of the variables
    will have missing values, NULLs, etc. Therefore, space is much emptier, the dataset
    is less dense, and a smaller number of variables have values associated with them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With increased complexity in the model, the processing time required increases.
    The system feels the pressure to deal with so many dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overall solution becomes more complex to comprehend and execute. Recall
    chapter 1, where we discussed supervised learning algorithms. Due to the high
    number of dimensions, we might face the problem of overfitting in supervised learning
    models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DEFINITION  When a supervised learning model has good accuracy on training data
    but lesser accuracy on unseen data, it is referred to as *overfitting*. Overfitting
    is a nuisance as the very aim of machine learning models is to work well on unseen
    datasets, and overfitting defeats this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Let us relate things to a real-world example. Consider an insurance company
    offering different types of insurance policies like life insurance, vehicle insurance,
    health insurance, home insurance, etc. The company wishes to use data science
    and execute clustering use cases to enhance the customer base and the total number
    of policies sold. They have customer details like age, gender, profession, policy
    amount, historical transactions, number of policies held, annual income, type
    of policy, number of historical defaults, etc. At the same time, let us assume
    that variables like whether the customer is left-handed or right-handed, whether
    they wear black or brown shoes, what shampoo brand they use, the color of their
    hair, and their favorite restaurant are also captured. If we include all the variables
    in the dataset, the total number of variables in the resultant dataset will be
    quite high. The distance calculation will be more complex for a k-means clustering
    algorithm, the processing time will increase, and the overall solution will be
    quite complex.
  prefs: []
  type: TYPE_NORMAL
- en: It is also imperative to note that *not* all the dimensions or variables are
    significant. Hence, it is vital to filter out the important ones from all the
    variables we have. Remember, nature always prefers simpler solutions! In the case
    discussed previously, it is highly likely that variables like hair color and favorite
    restaurant, etc., will not affect the outputs. So it is in our best interest to
    reduce the number of dimensions to ease the complexity and reduce the computation
    time. At the same time, it is also vital to note that dimensionality reduction
    is not always desired. It depends on the type of dataset and the business problem
    we wish to resolve. We will explore this more when we work on the case study in
    subsequent sections of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3.1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: The curse of dimensionality refers to having a lot of data. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Having a high number of variables will always increase the accuracy of a solution.
    True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a large number of variables in a dataset affect the model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have established that having a lot of dimensions is a challenge for us. We
    next examine the various methods to reduce the number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Dimension reduction methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We studied the disadvantages of having really high-dimensional data in the last
    section. A fewer number of dimensions might result in a simpler structure for
    our data, which will be computationally efficient. At the same time, we should
    be careful when reducing the number of variables. The output of the dimension
    reduction method should be complete enough to represent the original data and
    should not lead to any information loss. In other words, if originally we had,
    for example, 500 variables and we reduced it to 120 significant ones, still these
    120 *should* be robust enough to capture *almost* all the information. Let us
    understand using a simple example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this: we wish to predict the amount of rainfall a city will receive
    in the next month. The rainfall prediction for that city might be dependent on
    temperature over a period, wind speed measurements, pressure, distance from the
    sea, elevation above sea level, etc. These variables make sense if we wish to
    predict rainfall. At the same time, variables like the number of cinema halls
    in the city, whether the city is the capital of the country, or the number of
    red cars in the city will not affect the prediction of rainfall. In such a case,
    if we do not use the number of cinema halls in the city to predict the amount
    of rainfall, it will not reduce the capability of the system. The solution, in
    all probability, will still be able to perform quite well. Hence, in such a case,
    no information will be lost by dropping such a variable, and surely we can drop
    it from the dataset. On the other hand, removing variables such as temperature
    or distance from the ocean will very likely negatively affect the prediction accuracy.
    This is a very simple example highlighting the need to reduce the number of variables.'
  prefs: []
  type: TYPE_NORMAL
- en: The dimensions or the number of variables can be reduced by a combination of
    manual and algorithm-based methods. But before studying them in detail, there
    are a few mathematical terms and components we should be aware of, which we will
    discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Mathematical foundation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are quite a few mathematical terms that one must know to develop a thorough
    understanding of dimensionality reduction methods. We are trying to reduce the
    number of dimensions of a dataset. A dataset is nothing but a matrix of values—thus,
    a lot of the concepts are related to matrix manipulation methods, their geometrical
    representation, and performing transformations on such matrices. The mathematical
    concepts are discussed in the appendix. You also need an understanding of eigenvalues
    and eigenvectors. These concepts will be reused throughout the book; they are
    been put in the appendix for quick reference. You are advised to go through them
    before proceeding.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Manual methods of dimensionality reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To tackle the curse of dimensionality, we wish to reduce the number of variables
    in a dataset. The reduction can be done by removing the variables from the dataset.
    Or a very simple solution for dimensionality reduction can be combining the variables
    that can be grouped logically or can be represented using a common mathematical
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: For example, as shown in figure 3.3, the data can be from a retail store where
    different customers have generated different transactions. We will get the sales,
    the number of invoices, and the number of items bought by each customer over a
    period. In the table, customer 1 has generated two invoices, bought five items
    in total, and generated a total sale of 100\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F03_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 In the first table, we have the sales, invoices, and number of items
    as the variables. In the second table, they have been combined to create new variables.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If we wish to reduce the number of variables, we might combine three variables
    into two variables. Here we have introduced variables average transaction value
    (ATV) and average basket size (ABS) wherein ATV = Sales/Invoices and ABS = Number
    Of Items/Invoices.
  prefs: []
  type: TYPE_NORMAL
- en: So, in the second table for customer 1, we have ATV as 50 and ABS as 2.5\. Hence,
    the number of variables has been reduced from three to two. The process here is
    only an example of how we can combine various variables. It does not mean that
    we should replace sales with ATV as a variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process can continue to reduce the number of variables. Similarly, for
    a telecom subscriber, say we have the minutes of mobile calls made during 30 days
    in a month. We can add them to create a single variable—minutes used in a month.
    These examples are very basic ones to start with. Using the manual process, we
    can employ two other commonly used methods: manual selection and using correlation
    coefficient.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Manual feature selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Continuing from the rainfall prediction example we discussed in the last section,
    a data scientist might be able to drop a few variables. This will be based on
    a deep understanding of the business problem at hand and the corresponding dataset
    being used. However, it is an underlying assumption that the dataset is quite
    comprehensible for the data scientist and that they understand the business domain
    well. Most of the time, the business stakeholders will be able to guide on such
    methods. The variables must also be unique, and not much dependency should exist.
    As shown in figure 3.4, we can remove a few of the variables that might not be
    useful for predicting rainfall.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F04_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 In the first table, we have all the variables present in the dataset.
    Using business logic, some of the variables that might not be of much use have
    been discarded in the second table. But this is to be done with due caution; the
    best way is to get guidance from the business stakeholders.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Sometimes, feature selection methods are also referred to as *wrapper methods*.
    Here, a machine learning model is wrapped or fitted with a subset of variables.
    In each iteration, we will get a different set of results. The set that generates
    the best results is selected for the final model.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Correlation coefficient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Correlation between two variables simply means that they have a mutual relationship
    with each other. The change in the value of one variable will affect the value
    of another, which means that data points with similar values in one variable have
    similar values for the other variable. The variables that are highly correlated
    with each other supply similar information, so one of them can be dropped.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Correlation is described in detail in the appendix.
  prefs: []
  type: TYPE_NORMAL
- en: For example, for a retail store, the number of invoices generated in a day will
    be highly correlated with the amount of sales generated, so one of them can be
    dropped. Another example is students who study for a higher number of hours will
    have better grades than the ones who study less (mostly!).
  prefs: []
  type: TYPE_NORMAL
- en: But we should be careful in dropping the variables and not trust correlation
    alone. The business context of a variable should be thoroughly understood before
    making any decision.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  It is a good idea to discuss this with the business stakeholders before
    dropping any variables from the study.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation-based methods are sometimes called *filter methods*. Using correlation
    coefficients, we can filter and choose the variables that are most significant.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3.2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: We can drop a variable simply if we feel it is not required. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If two variables are correlated, always drop one of them. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Manual methods are easier solutions and can be executed quite efficiently. The
    dataset size is reduced, and we can proceed with the analysis. But manual methods
    are sometimes subjective and depend a lot on the business problem at hand. Many
    times, it is also not possible to employ manual methods for dimension reduction.
    In such situations, we have algorithm-based methods, which we study in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3 Algorithm-based methods for reducing dimensions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We examined manual methods in the last section. Continuing from there, we examine
    algorithm-based methods in this section. The algorithm-based techniques are based
    on a more mathematical base and hence prove to be more scientific methods. In
    real-world business problems, we use a combination of both manual and algorithm-based
    techniques. Manual methods are straightforward to execute as compared to algorithm-based
    techniques. Also, we cannot comment on the comparison of both techniques, as they
    are based on different foundations. But at the same time, it is imperative that
    you put due diligence into the implementation of algorithm-based techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'The major techniques used in dimensionality reductions are listed as follows.
    We explore some of them in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalized discriminant analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-negative matrix factorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multidimension scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locally linear embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IsoMaps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: t-SNE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These techniques are utilized for the common end goal: transform the data from
    a high-dimensional space to a low-dimensional one. Some of the data transformations
    are linear in nature, while some are nonlinear.'
  prefs: []
  type: TYPE_NORMAL
- en: We discuss PCA and SVD in detail in this chapter. In the later chapters of the
    book, other major techniques will be explored. PCA is perhaps the most quoted
    dimensionality reduction method, which is explored in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Principal component analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider this: you are working on a dataset that has 250 variables. It is almost
    impossible to visualize such a high-dimensional space. Some of the 250 variables
    might be correlated with each other and some of them might not be, and there is
    a need to reduce the number of variables without losing much information. PCA
    allows us to mathematically select the most important features and leave the rest.
    PCA does reduce the number of dimensions but also preserves the most important
    relationships between the variables and the important structures in the dataset.
    Hence, the number of variables is reduced, but the important information in the
    dataset is kept safe.'
  prefs: []
  type: TYPE_NORMAL
- en: PCA is a projection of high-dimensional data in lower dimensions. In simpler
    terms, we are reducing an *n*-dimensional space into an *m*-dimensional one where
    *n* > *m* while maintaining the nature and the essence of the original dataset.
    In the process, the old variables are reduced to newer ones while maintaining
    the crux of the original dataset. The new variables thus created are called *principal
    components*. The principal components are a linear combination of the raw variables.
    As a result of this transformation, the first principal component captures the
    maximum randomness or the highest variance in the dataset. The second principal
    component created is orthogonal to the first component.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  If two straight lines are orthogonal to each other, it means they are
    at an angle of 90˚ to each other.
  prefs: []
  type: TYPE_NORMAL
- en: The process continues to the third component and so on. Orthogonality allows
    us to maintain that there is no correlation between subsequent principal components.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  PCA utilizes linear transformation of the dataset, and such methods are
    sometimes referred to as feature projections. The resultant dataset or the projection
    is used for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Let us understand this better using an example. In figure 3.5, we have represented
    the total perceived value of a home using some variables. The variables are area
    (sq m), number of bedrooms, number of balconies, distance from the airport, distance
    from the train station, and so on; we have 100+ variables.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F05_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 The variables on which the price of a house can be estimated
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can combine some of the variables mathematically and logically. PCA will
    create a new variable that is a linear combination of some of the variables, as
    shown in the following example. It will get the best *linear* combination of original
    variables so that the new variable is able to capture the maximum variance of
    the dataset. Equation 3.1 is only an example shown for illustration purposes wherein
    we are showing a new variable created by a combination of other variables.
  prefs: []
  type: TYPE_NORMAL
- en: (3.1)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: new_variable = *a**area – *b**bedrooms + *c**distance – *d**schools
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s understand the concept visually. In a vector-space diagram, we can
    represent the dataset, as shown in figure 3.6\. The left figure represents the
    raw data where we can visualize the variables in an x-y diagram. As discussed
    earlier, we wish to create a linear combination of variables. In other words,
    we wish to create a mathematical equation that will be able to explain the relationship
    between x and y.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F06_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 The dataset can be represented in a vector-space diagram (left).
    The straight line can be called the line of best fit having the projections of
    all the data points on it (middle). The differences between the actual value and
    the projections are the error terms (right).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The output of such a process will be a straight line as shown in the middle
    diagram in figure 3.6\. This straight line is sometimes referred to as the *line
    of best fit.* Using this line of best fit, we can predict a value of y for a given
    value of x. These predictions are nothing but the projections of data points on
    a straight line.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between the actual value and the projections is the error, as
    shown in the right diagram in figure 3.6\. The total sum of these errors is called
    the total projection error.
  prefs: []
  type: TYPE_NORMAL
- en: There can be multiple options for this straight line, as shown in figure 3.7\.
    These different straight lines will have different errors and different values
    of variances captured.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F07_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 The dataset can be captured by several lines, but not all the straight
    lines will be able to capture the maximum variance. The equation that gives the
    minimum error will be the one chosen.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The straight line that can capture the maximum variance will be the chosen one.
    In other words, it gives the minimum error. It will be the *first principal component,*
    and the direction of maximum spread will be the *principal axis*.
  prefs: []
  type: TYPE_NORMAL
- en: The second principal component will be derived in a similar fashion. Since we
    know the first principal axis, we can subtract the variance along this principal
    axis from the total variance to get the residual variance. In other words, using
    the first principal component, we would capture some variance in the dataset.
    But there will be a portion of the total variance in the dataset that is still
    unexplained by the first principal component. The portion of the total variance
    unexplained is the residual variance. Using the second principal component, we
    wish to capture as much variance as we can.
  prefs: []
  type: TYPE_NORMAL
- en: Using the same process to capture the direction of maximum variance, we will
    get the second principal component. The second principal component can be at several
    angles with respect to the first one, as shown in figure 3.8\. It is mathematically
    proven that if the second principal component is orthogonal (i.e., 90˚)to the
    first principal component, this allows us to capture the maximum variance using
    the two principal components. In figure 3.8, we can observe that the two principal
    components are at an angle of 90˚ to each other.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F08_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 The first figure on the left is the first principal component. The
    second principal component can be at different angles with respect to the first
    principal component (middle). We should find the second principle that allows
    us to capture the maximum variance. To capture the maximum variance, the second
    principal component should be orthogonal to the first one, and thus the combined
    variance captured is maximized (right).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The process continues for the third and fourth principal components and so on.
    With more principal components, the representation in a vector space becomes difficult
    to visualize. You can think of a vector space diagram with more than three axes.
    Once all the principal components are derived, the dataset is projected onto these
    axes. The columns in this transformed dataset are the *principal components*.
    The principal components created will be fewer than the number of original variables
    and will capture the maximum information present in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we examine the process of PCA in-depth, let’s study its important characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: PCA aims to reduce the number of dimensions in the resultant dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA produces principal components that aim to reduce the noise in the dataset
    by maximizing the feature variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the same time, the principal components reduce the redundancy in the dataset.
    This is achieved by minimizing the covariance between the pairs of features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original variables no longer exist in the newly created dataset. Instead,
    new variables are created using these variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not necessary that the principal components map one-to-one with all the
    variables present in the dataset. They are a new combination of the existing variables.
    Hence, they can be a combination of several different variables in one principal
    component (as shown in equation 3.1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The new features created from the dataset do not share the same column names.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original variables might be correlated with each other, but the newly created
    variables are unrelated to each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of newly created variables is fewer than the original number of variables.
    The process to select the number of principal components has been described in
    section 3.5.2\. After all, that is the whole purpose of dimensionality reduction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If PCA has been used for reducing the number of variables in a training dataset,
    the testing/validation datasets should be reduced by using PCA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA is not synonymous with dimensionality reduction only. It can be put into
    use for a number of other usages beyond dimensionality reduction like feature
    extraction, data visualization, multicollinearity detection, preprocessing, etc.
    Using a PCA only for dimensionality reduction will be a misnomer for sure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will now examine the approach used while implementing PCA, and then we will
    develop a Python solution using PCA. We need not apply all the steps while we
    develop the codes, as the heavy lifting has already been done by the packages
    and libraries. The steps given here are taken care of by the packages, but still,
    it is imperative that you understand these steps to properly appreciate how PCA
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: In PCA, we start with *normalizing our dataset* as a first step. It ensures
    that all our variables have a common representation and become comparable. We
    have methods to perform the normalization in Python, which we will study when
    we develop the code. To explore more about normalizing the dataset, see the appendix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the covariance in the normalized dataset. It allows us to study the relationship
    between the variables. We generally create a covariance matrix, as shown in the
    Python example in the next section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can then calculate the eigenvectors and eigenvalues of the covariance matrix.
    The mathematical concept of eigenvectors is given in the appendix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then sort the eigenvalues in decreasing order of eigenvalues. We choose the
    eigenvectors corresponding to the maximum value of eigenvalues. The components
    chosen will be able to capture the maximum variance in the dataset. There are
    other methods to shortlist the principal components, which we will explore while
    we develop the Python code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exercise 3.3
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: PCA will result in the same number of variables in the dataset. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PCA will be able to capture 100% of the information in the dataset. True or
    False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the logic of selecting principal components in PCA?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, in essence, principal components are the linear combinations of the original
    variables. The weight in this linear combination is the eigenvector satisfying
    the error criteria of the least square method.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1 Eigenvalue decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the context of PCA, the eigenvector will represent the direction of the vector
    and the eigenvalue will be the variance that is captured along that eigenvector.
    See figure 3.9, where we break the original *n* x *n* matrix into components.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F09_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 Using eigenvalue decomposition, the original matrix can be broken
    into an eigenvector matrix, an eigenvalue matrix, and an inverse of an eigenvector
    matrix. We implement PCA using this methodology.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Mathematically, we can show the relation with equation 3.2
  prefs: []
  type: TYPE_NORMAL
- en: (3.2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*A***v* = *λ***v*'
  prefs: []
  type: TYPE_NORMAL
- en: where *A* is a square matrix, *v* is the eigenvector, and *λ* is the eigenvalue.
    Here, it is important to note that the eigenvector matrix is the orthonormal matrix,
    and its columns are eigenvectors. The eigenvalue matrix is the diagonal matrix,
    and its eigenvalues are the diagonal elements. The last component is the inverse
    of the eigenvector matrix. Once we have the eigenvalues and the eigenvectors,
    we can choose the significant eigenvectors for getting the principal components.
  prefs: []
  type: TYPE_NORMAL
- en: We present PCA and SVD as two separate methods in this book. Both methods are
    used to reduce high-dimensional data into lower-dimensional ones and, in the process,
    retain the maximum information in the dataset. The difference between the two
    is SVD exists for any sort of matrix (rectangular or square), whereas eigen decomposition
    is possible only for square matrices. You will understand it better once we have
    covered SVD later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2 Python solution using PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have studied the concepts of PCA and the process using eigenvalue decomposition.
    It is time for us to dive into Python and develop a PCA solution on a dataset.
    I will show you how to create eigenvectors and eigenvalues on the dataset. To
    implement the PCA algorithms, we will use the `sklearn` library. Libraries and
    packages provide a faster solution for implementing algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the Iris dataset for this problem. It is one of the most popular datasets
    used for machine learning problems. The dataset contains data of three iris species
    with 50 samples each and having properties of each flower, like petal length,
    sepal length, etc. The objective of the problem is to predict the species using
    the properties of the flower. The independent variables, hence, are the flower
    properties, whereas the variable “species” is the target variable. The dataset
    and the code are checked in at the GitHub repository. Here we are using the inbuilt
    PCA functions, which reduce the effort required to implement PCA. The steps are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Load all the necessary libraries. We are going to use `numpy`, `pandas`, `seaborn`,
    `matplotlib`, and  `sklearn`. Note that we have imported PCA from `sklearn`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'NOTE  The following are the standard libraries. You will find that almost all
    the machine learning solutions would import these libraries in the solution notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '2\. Load the dataset now. It is a .csv file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 3\. We will now perform a basic check on the dataset, looking at the first five
    rows, the shape of the data, the spread of the variables, etc. We are not performing
    an extensive exploratory data analysis here as the steps are covered in chapter
    2\. The dataset has 150 rows and 6 columns (see figure 3.10).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH03_UN01_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH03_F10_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 Code output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '4\. Here, we should break the dataset into independent variables and a target
    variable. `X_variables` here represent the independent variables, which are in
    columns 2–5 of the dataset while `y_variable` is the target variable, which is
    “species” in this case and is the final column in the dataset. Recall we wish
    to predict the species of a flower using the other properties. Hence, we have
    separated the target variable “species” and other independent variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 5\. Normalize the dataset. The built-in method of `StandardScalar()` does the
    job for us quite easily.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NOTE  The `StandardScalar()` method normalizes the dataset for us. It subtracts
    the mean from the variable and divides it by the standard deviation. For more
    details on normalization, refer to the appendix.
  prefs: []
  type: TYPE_NORMAL
- en: 'We invoke the method and then use it on our dataset to get the transformed
    dataset. Since we are working on independent variables, we are using `X_variables`
    here. First, we invoke the `StandardScalar()` method. Then we use the `fit_transform`
    method. The `fit_transform` method first fits the transformers to *X* and *Y*
    and then returns a transformed version of *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '6\. Calculate the covariance matrix and print it. The output is shown in figure
    3.11\. Getting the covariance matrix is straightforward using `numpy`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH03_F11_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 The covariance matrix
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 7\. Calculate the eigenvalues. Inside the `numpy` library, we have the built-in
    functionality to calculate the eigenvalues. We will then sort the eigenvalues
    in descending order. To shortlist the principal components, we can choose eigenvalues
    greater than 1\. This criterion is called *Kaiser criteria.* We are exploring
    other methods too.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NOTE  The eigenvalue represents how good a component is as a summary of the
    data. If the eigenvalue is 1, it means that the component contains the same amount
    of information as a single variable; hence, we choose the eigenvalue that is greater
    than 1\.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this code, first we get the `eigen_values` and `eigen_vectors`, and then
    we arrange them in descending order (see figure 3.12):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH03_F12_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 Eigenvalues arranged in descending order
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '8\. Invoke the PCA method from the `sklearn` library. The method is used to
    fit the data here. Note we have not yet determined the number of principal components
    we wish to use in this problem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '9\. The principal components are now set. Let’s have a look at the variance
    explained by them. We can observe that the first component captures 72.77% variation,
    the second captures 23.03% variation, and so on (figure 3.13):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH03_F13_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 The degree of variance of the principal components
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '10\. We now plot the components in a bar plot for better visualization (see
    figure 3.14):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH03_F14_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 Bar plot of the principal components
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '11\. Here we draw a scree plot to visualize the cumulative variance being explained
    by the principal components (see figure 3.15):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH03_F15_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 Scree plot of cumulative variance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '12\. In this case study, we choose the top two principal components as the
    final solutions, as these two capture 95.08% of the total variance in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '13\. We will now plot the dataset with respect to two principal components.
    For that, species must be tied back to the actual values of the species variable,
    which are `Iris-setosa`, `Iris-versicolor`, and `Iris-virginica`. Here, `0` is
    mapped to `Iris-setosa`, `1` is `Iris-versicolor`, and `2` is `Iris-virginica`.
    In the following code, first the species variable gets its values replaced by
    using the mapping discussed earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '14\. We will now plot the results with respect to two principal components.
    The plot shows the dataset reduced to two principal components we have just created.
    These principal components can capture 95.08% variance of the dataset. The first
    principal component represents the x-axis in the plot while the second principal
    component represents the y-axis in the plot (see figure 3.16). The color represents
    the various classes of Species. The print version of the book will not show the
    different colors, but the output of the Python code will. The same output is also
    available at the GitHub repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH03_F16_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 The results for two principal components
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This solution has reduced the number of components from four to two and still
    is able to retain most of the information. Here, we have examined three approaches
    to select the principal components based on the Kaiser criteria, the variance
    captured, and the scree plot.
  prefs: []
  type: TYPE_NORMAL
- en: Let us quickly analyze what we have achieved using PCA. Figure 3.17 shows two
    representations of the same dataset. The one on the left is the original dataset
    of X_variables. It has four variables and 150 rows. The right is the output of
    PCA. It has 150 rows but only two variables. Recall we have reduced the number
    of dimensions from four to two. So, the number of observations has remained 150,
    while the number of variables has reduced from four to two.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F17_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 The figure on the left shows the original dataset, which has 150
    rows and four variables. After the implementation of PCA at right, the number
    of variables has been reduced to two. The number of rows remains the same as 150,
    which is shown by the length of pca_2d.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Once we have reduced the number of components, we can continue to implement
    a supervised learning or an unsupervised learning solution. We can implement the
    preceding solution for any of the other real-world problems where we aim to reduce
    the number of dimensions. We explore this more in section 3.8\.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have covered PCA. The GitHub repository contains a very interesting
    PCA decomposition with variables and a corresponding plot.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Singular value decomposition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA transforms the data linearly and generates principal components that are
    not correlated with each other. But the process followed in eigenvalue decomposition
    can only be applied to *square matrices*, whereas SVD can be implemented to any
    *m* × *n* matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Say we have matrix *A*. The shape of *A* is *m* × *n*, or it contains *m* rows
    and *n* columns. The transpose of *A* can be represented as *A*^(*T*).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create two other matrices using *A* and *A*^(*T*) as *A A*^(*T*)and
    *A*^(*T*)*A*. These resultant matrices *A A*^(*T*)and *A*^(*T*)*A* have some special
    properties, which are as follows (the mathematical proof of the properties is
    beyond the scope of the book):'
  prefs: []
  type: TYPE_NORMAL
- en: They are symmetric and square matrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Their eigenvalues are either positive or zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both *A A*^(*T*)and *A*^(*T*)*A* have the same eigenvalue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both *A A*^(*T*)and *A*^(*T*)*A* have the same rank as the original matrix A.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The eigenvectors of *A A*^(*T*)and *A*^(*T*)*A* are referred to as singular
    vectors of A. The square root of their eigenvalues is called singular values.
  prefs: []
  type: TYPE_NORMAL
- en: Since both matrices (*A A*^(*T*)and *A*^(*T*)*A*) are symmetrical, their eigenvectors
    are orthonormal to each other. In other words, because they are symmetrical, the
    eigenvectors are perpendicular to each other and can be of unit length.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, with this mathematical understanding, we can define SVD. As per the SVD
    method, it is possible to factorize any matrix A, as shown in equation 3.3:'
  prefs: []
  type: TYPE_NORMAL
- en: (3.3)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*A* = *U* * *S* * *V*^(*T*)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *A* is the original matrix, *U* and *V* are the orthogonal matrices with
    orthonormal eigenvectors taken from *A A*^(*T*)and *A*^(*T*)*A*, respectively,
    and *S* is the diagonal matrix with *r* elements equal to the singular values.
    In simple terms, SVD can be seen as an enhancement of the PCA methodology using
    eigenvalue decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Singular values are better and numerically more robust than eigenvalues
    decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: PCA was defined as the linear transformation of input variables using principal
    components. All those concepts of linear transformation, such as choosing the
    best components, etc., remain the same. The major process steps also remain similar,
    except in SVD we use a slightly different approach wherein the eigenvalue decomposition
    is replaced by singular vectors and singular values. It is often advisable to
    use SVD when we have a sparse dataset; in the case of a denser dataset, PCA can
    be utilized.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3.4
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: SVD works on the eigenvalue decomposition technique. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PCA is a much more robust methodology than SVD. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are singular values and singular vectors in SVD?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3.6.1 Python solution using SVD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this case study, we are using the *mushrooms* dataset. This dataset contains
    descriptions of 23 species of grilled mushrooms. There are two classes: either
    the mushroom is *e*, which means it is edible, or the mushroom is *p*, meaning
    it is poisonous. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '2\. Import the dataset and check for shape, head, etc. (see figure 3.18):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH03_F18_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 Code output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 3\. As we can observe, the values are categorical in nature in the dataset.
    They should be first encoded into numeric values. This is not the only approach
    for dealing with categorical variables. There are other techniques too, which
    we will explore throughout the book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, invoke the `LabelEncoder` and then apply it to all the columns in the
    dataset. The `LabelEncoder` converts the categorical variables into numeric ones
    using the one-hot encoding method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '4\. Have another look at the dataset. All the categorical values have been
    converted to numeric ones (see figure 3.19):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH03_F19_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 Code output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '5\. The next two steps are the same as the last case study, wherein we break
    the dataset into `X_variables` and `y_label`. Then the dataset is normalized:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '6\. Implement the SVD. There is a method in `numpy` that implements SVD. The
    output is `u`, `s`, and `v`, where `u` and `v` are the singular vectors and `s`
    is the singular value. If you wish, you can analyze their respective shapes and
    dimensions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '7\. We know that singular values allow us to compute variance explained by
    each of the singular vectors. We will now analyze the percentage variance explained
    by each singular vector and plot it (see figure 3.20). The results are shown to
    three decimal places. Then we plot the results as a histogram plot. On the x-axis,
    we have the singular vectors while on the y-axis we have the percent of variance
    explained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH03_F20_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 Code output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '8\. Create a dataframe (see figure 3.21). This new dataframe svd_df contains
    the first two singular vectors and the metadata. We then print the first five
    rows using the head command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH03_F21_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 Dataframe containing the first two singular vectors and the metadata
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '9\. Like the last case study, we replace numeric values with actual class labels;
    `1` is edible while `0` is poisonous:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '10\. We now plot the variance explained by the two components (see figure 3.22).
    Here, we have chosen only the first two components. You are advised to take the
    optimum number of components using the methods described in the last section and
    plot the respective scatter plots. Here, on the x-axis, we have shown the first
    singular vector SV1, and on the y-axis we have shown the second singular vector
    SV2\. The print version of the book does not show the different colors, but the
    output of the Python code does. The same output is available at the GitHub repository
    too:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH03_F22_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.22 Plot of the variance explained by two components
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can observe the distribution of the two classes with respect to the two components.
    The two classes—`Edible` and `Poison`—are color-coded as black and red, respectively.
    As we have noted previously, we have chosen only two components to show the effect
    using a visualization plot. You should choose the optimum number of components
    using the methods described in the last case study and then visualize the results
    using different singular vectors. This solution can be used to reduce dimensions
    in a real-world dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 Pros and cons of dimensionality reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the initial sections of the chapter, we discussed the drawbacks of the curse
    of dimensionality. In the last few sections, we discovered PCA and SVD and implemented
    them using Python. Now we will examine the advantages and challenges of these
    techniques. The major advantages of implementing PCA or SVD are
  prefs: []
  type: TYPE_NORMAL
- en: A reduced number of dimensions leads to less complexity in the dataset. The
    correlated features are removed and transformed. Treating correlated variables
    manually is a tough task, which is quite manual and frustrating. Techniques like
    PCA and SVD do that job for us quite easily. The number of correlated features
    is minimized, and overall dimensions are reduced.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization of the datasetis better if the number of dimensions is fewer.
    It is very difficult to visualize and depict a very high-dimensional dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The accuracyof the machine learning model is improved if the correlated variables
    are removed. These variables do not add anything to the performance of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training time is reduced as the dataset is less complex. Hence, less computation
    power and time are required.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overfittingis a nuisance in supervised machine learning models. It is a condition
    where the model behaves very well on the training dataset but not so well on the
    testing/validation dataset. It means that the model may not be able to perform
    well on real-world unseen datasets. And it defeats the entire purpose of building
    the machine learning model. PCA/SVD helps tackle overfitting by reducing the number
    of variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the same time, there are a few challenges we face with dimensionality reduction
    techniques, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The new components created by PCA/SVD are often less interpretable. They are
    a combination of the independent variables in the dataset and do not actually
    relate to the real world; hence it can be difficult to relate them to real-world
    scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numeric variables are required for PCA/SVD. Hence all the categorical variables
    should be represented in numeric form.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization/standardization of the dataset is required before the solution
    can be implemented.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There might be information losswhen we use PCA or SVD. The principal components
    *cannot* replace the original dataset, and hence there might be some loss of information
    when we implement these methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, despite a few challenges, PCA and SVD are used for reducing dimensions
    in a dataset. They are two of the most popular methods and are quite heavily used.
    Note that these are linear methods; we cover nonlinear methods of dimensionality
    reduction in a later part of the book.
  prefs: []
  type: TYPE_NORMAL
- en: We have now covered the two most important techniques used in dimensionality
    reduction. We will examine more advanced techniques in the later chapters. It
    is time to move on to the case study.
  prefs: []
  type: TYPE_NORMAL
- en: 3.8 Case study for dimension reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s explore a real-world case to relate the use of PCA and SVD in real-world
    business scenarios. Consider this: you are working for a telecommunication service
    provider. You have a subscriber base, and you wish to cluster the consumers over
    several parameters. The challenge is the huge number of dimensions available to
    be analyzed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective will be to reduce the number of attributes using dimension reduction
    algorithms. The consumer dataset might include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Demographic details of the subscriber, which will consist of age, gender, occupation,
    household size, marital status, etc. (see figure 3.23).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F23_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.23 Demographic details of a subscriber like age, gender, marital status,
    household size, city, etc.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Subscription details of the consumer, which might look like figure 3.24.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F24_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.24 Subscription details of a subscriber like tenure, postpaid/prepaid
    connection, etc.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Consumer usage, such as the minutes, call rates, data usages, services, etc.
    (see figure 3.25).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F25_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.25 Usage of a subscriber specifies the number of minutes used, SMS
    sent, data used, days spent in a network, national or international usage, etc.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Payment and transaction details of the subscribers, which could be the various
    transactions made, the mode of payment, frequency of payments, days since last
    payment made, etc. (see figure 3.26).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F26_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.26 Transaction details of a subscriber showing all the details of amount,
    mode, etc.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Many more attributes. So far, we have established that the number of variables
    involved are indeed high. Once we join all these data points, the number of dimensions
    in the final data can be huge (see figure 3.27).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F27_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.27 The final dataset is a combination of all the aforementioned datasets.
    It will be a big, really high-dimensional dataset to be analyzed.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We should reduce the number of attributes before we proceed to any supervised
    or unsupervised solution. In this chapter, we focus on dimensionality reduction
    techniques, and hence the steps cover that aspect of the process. In later chapters,
    we will examine exploratory analysis in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: As a first step, we will perform a sanity check of the dataset and do the data
    cleaning. We will examine the number of data points, number of missing values,
    duplicates, junk values present, etc. This will allow us to delete any variables
    that might be very sparse or contain not much information. For example, if the
    gender is available for only 0.01% of the customer base, it might be a good idea
    to drop the variable. Or if all the customers state their gender is male, the
    variable is not adding any new information to us, and hence it can be discarded.
    Sometimes, using business logic, a variable might be dropped from the dataset.
    An example has been discussed in section 3.4\. In this step, we might combine
    a few variables. For example, we might create a new variable as average transaction
    value by dividing the total amount spent by the total number of transactions.
    In this way, we will be able to reduce a few dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  A Python Jupyter notebook is available in the GitHub repository, where
    we have given a very detailed solution for the data cleaning step.
  prefs: []
  type: TYPE_NORMAL
- en: Once we are done with the basic cleaning of the data, we start with the exploratory
    data analysis. As a part of exploratory analysis, we examine the spread of the
    variable, its distribution, mean/median/mode of numeric variables, and so on.
    This is sometimes referred to as *univariate analysis.* This step allows us to
    measure the spread of the variables, understand the central tendencies, examine
    the distribution of different classes for categorical variables, and look for
    any anomalies in the values. For example, using the dataset mentioned earlier,
    we will be interested in analyzing the maximum/minimum/average data usage or the
    percentage distribution of gender or age. We would want to know the most popular
    method to make a transaction, and we would also be interested to know the maximum/minimum/average
    amount of the transactions. The list goes on.
  prefs: []
  type: TYPE_NORMAL
- en: Then we explore the relationships between variables, which is referred to as
    *bivariate analysis*. Crosstabs, or distribution of data, is a part of bivariate
    analysis. A correlation matrix is created during this step. Variables that are
    highly correlated are examined thoroughly. And based on business logic, one of
    them might be dropped. This step is useful to visualize and understand the behavior
    of one variable in the presence of other variables. We can examine their mutual
    relationships and the respective strength of the relationships. In this case study,
    we would answer questions such as, “Do subscribers who use more data spend more
    time on the network as compared to subscribers who send more SMS?”, “Do the subscribers
    who make a transaction using the online mode generate more revenue than the ones
    using cash?”, or “Is there a relationship between gender/age and the data usage?”
    Many such questions are answered during this phase of the project.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  A Python Jupyter notebook is available in the GitHub repository, which
    provides detailed steps and code for the univariate and bivariate phases. Check
    it out!
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, we have a dataset that has a huge number of dimensions, and we
    want to reduce the number of dimensions. Now is a good time to implement PCA or
    SVD. The techniques will reduce the number of dimensions and will make the dataset
    ready for the next steps in the process, as shown in figure 3.28\. The figure
    is only representative in nature to depict the effect of dimensionality reduction
    methods. Notice how the large number of black lines in the left figure is reduced
    to a smaller number of red lines in the right figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F28_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.28 A very high-dimensional dataset will be reduced to a low-dimensional
    one by using principal components that capture the maximum variance in the dataset.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The output of dimensionality reduction methods will be a dataset with a lower
    number of variables. The dataset can be then used for supervised or unsupervised
    learning. We have already looked at the examples using Python in the earlier sections
    of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our case study on telecom subscribers. The case can be extended
    to any other domain like retail; banking, financial services, and insurance; aviation;
    healthcare; manufacturing; and others.
  prefs: []
  type: TYPE_NORMAL
- en: 3.9 Concluding thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data is everywhere in various forms, levels, and dimensions and with varying
    levels of complexity. It is often mentioned that “the more data, the better.”
    It is indeed true to a certain extent. But with a really high number of dimensions,
    it becomes quite a herculean task to make sense of it. The analysis can become
    biased and very complex to deal with. We explored this curse of dimensionality
    in this chapter. We found PCA and SVD can be helpful to reduce this complexity.
    They make the dataset ready for the next steps.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction is not as straightforward as it looks. It is not an
    easy task, but it is certainly a very rewarding one. And it requires a combination
    of business acumen, logic, and common sense. The resultant dataset might still
    require some additional work. But it is a very good point for building a machine
    learning model.
  prefs: []
  type: TYPE_NORMAL
- en: This marks the end of the third chapter. It also ends the part 1 of the book.
    In this part, we have covered a few core algorithms. We started with the first
    chapter of the book, where we explored the fundamentals and basics of machine
    learning. In the second chapter, we examined three algorithms for clustering.
    In this third chapter, we explored PCA and SVD.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part of the book, we change gears and study more advanced topics.
    We start with association rules in the next chapter. Then we go into advanced
    clustering methods of time-series clustering, fuzzy clustering, Gaussian mixture
    mode clustering, etc. That is followed by a chapter on advanced dimensionality
    reduction algorithms like t-SNE and LDA. To conclude the second part, we examine
    unsupervised learning on text datasets. The third part of the book is even more
    advanced, so still a long way to go. Stay tuned!
  prefs: []
  type: TYPE_NORMAL
- en: 3.10 Practical next steps and suggested readings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following provides suggestions for what to do next and offers some helpful
    reading:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the vehicles dataset used in the last chapter for clustering and implement
    PCA and SVD on it. Compare the performance on clustering before and after implementing
    PCA and SVD.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the datasets from [https://mng.bz/2y9g](https://mng.bz/2y9g). You can find
    many datasets. Compare the performance of PCA and SVD on these datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Go through the following papers on PCA:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://mng.bz/1XKX](https://mng.bz/1XKX)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://mng.bz/Pd0w](https://mng.bz/Pd0w)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://mng.bz/JYeo](https://mng.bz/JYeo)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://mng.bz/wJqO](https://mng.bz/wJqO)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Go through the following research papers on SVD:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://mng.bz/qxqA](https://mng.bz/qxqA)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://mng.bz/7pNm](https://mng.bz/7pNm)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1211.7102.pdf](https://arxiv.org/pdf/1211.7102.pdf)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The “curse of dimensionality” refers to problems arising from high-dimensional
    datasets with too many variables, complicating the analysis and model performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High dimensions can lead to a sparse dataset, increased mathematical complexity,
    longer processing times, and potential overfitting in machine learning models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hughes phenomenon shows that increasing variables only improves model performance
    up to a point, after which it declines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not all dimensions are significant; some may not contribute meaningfully to
    a model’s accuracy and should be removed to reduce complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data visualization can help explain datasets by reducing them to fewer dimensions
    that still capture significant information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manual dimension reduction includes dropping insignificant variables or combining
    them logically to reduce dataset dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithm-based methods for dimension reduction include PCA, SVD, LDA, and t-SNE,
    among others, which transform high-dimensional data into low-dimensional spaces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA reduces dimensions by creating principal components that capture maximum
    variance while minimizing redundancy and noise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVD enhances PCA, handling any matrix shape and decomposing them into singular
    values and vectors to maintain dataset information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each reduction technique requires the normalization of data and converting categorical
    variables to numeric forms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction simplifies datasets, enhancing visualization and model
    accuracy, reducing computation time, and mitigating overfitting risks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges with dimensionality reduction include the loss of interpretability,
    information loss, and the requirement for numerical data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both PCA and SVD are widely used to effectively reduce dimensions, and each
    is suitable for different dataset densities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The techniques can be applied in various industries like retail; banking, financial
    services, and insurance; and healthcare to simplify high-dimensional datasets
    for analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reduction process involves preliminary data cleaning and exploratory data
    analysis and then applying dimension-reduction techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
