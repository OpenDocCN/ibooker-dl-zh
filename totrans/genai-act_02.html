<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">2</span> </span> <span class="chapter-title-text">Introduction to large language models</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">An overview of LLMs</li> 
    <li class="readable-text" id="p3">Key use cases powered by LLMs</li> 
    <li class="readable-text" id="p4">Foundational models and their effect on AI development</li> 
    <li class="readable-text" id="p5">New architecture concepts for LLMs, such as prompts, prompt engineering, embeddings, tokens, model parameters, context window, and emergent behavior</li> 
    <li class="readable-text" id="p6">An overview of small language models</li> 
    <li class="readable-text" id="p7">Comparison of open source and commercial LLMs</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p8"> 
   <p>Large language models (LLMs) are generative AI models that can understand and generate human-like text based on a given input. LLMs are the foundation of many natural language processing (NLP) tasks, such as search, speech-to-text, sentiment analysis, text summarization, and more. In addition, they are general-purpose language models that are pretrained and can be fine-tuned for specific tasks and purposes.</p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>This chapter explores the fascinating world of LLMs and their transformative effect on artificial intelligence (AI). As a significant advancement in AI, LLMs have demonstrated remarkable capabilities in understanding and generating human-like text, thus enabling numerous applications across various industries. Here, we dive into the critical use cases of LLMs, the different types of LLMs, and the concept of foundational models that has revolutionized AI development.</p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>The chapter discusses essential LLM concepts, such as prompts, prompt engineering, embeddings, tokens, model parameters, context windows, transformer architecture, and emergent behavior. Finally, we compare open source and commercial LLMs, highlighting their advantages and disadvantages. By the end of this chapter, you will have a comprehensive understanding of LLMs and their implications for AI applications and research. LLMs are built on foundational models; therefore, we will start by outlining what these models are before discussing LLMs in more depth.</p> 
  </div> 
  <div class="readable-text" id="p11"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_22"><span class="num-string">2.1</span> Overview of foundational models</h2> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p>Introduced by Stanford researchers in 2021, foundational models have substantially transformed the construction of AI systems. They diverge from task-specific models, shifting to broader, more adaptable models trained on large data volumes. These models can excel in diverse natural language tasks, such as machine translation and question answering, as they learn general language representations from extensive text and code datasets. These representations can then be used to perform various tasks, even tasks they were not explicitly trained on, as shown in figure 2.1.</p> 
  </div> 
  <div class="readable-text intended-text" id="p13"> 
   <p>In more technical terms, foundational models utilize established machine learning techniques such as self-supervised learning and transfer learning, enabling them to apply acquired knowledge across various tasks. Developed by means of deep learning, these models employ multilayered artificial neural networks to comprehend complex data patterns; hence, their proficiency with unstructured data such as images, audio, and text. This also extends to 3D signals—data representing 3D attributes that capture spatial dimensions and depth, such as 3D point clouds from LiDAR sensors, 3D medical imaging such as CT scans, or 3D models used in computer graphics and simulations. These can be utilized to make predictions based on 3D data for tasks such as object recognition, scene understanding, and navigation in robotics and autonomous vehicles.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p14"> 
   <p><span class="print-book-callout-head">Note </span> Transfer learning is a machine learning technique in which a model developed for one task is reused as a starting point for a similar task. Instead of starting from scratch, we use the knowledge from the previous task to perform better on the new one. It’s like using knowledge from a previous job to excel at a new but related job.</p> 
  </div> 
  <div class="readable-text" id="p15"> 
   <p>Generative AI and foundational models are closely interlinked. As outlined, foundational models, trained on massive datasets, can be adapted to perform various tasks; this property makes them particularly suitable for generative AI and allows for creating new content. The broad knowledge base of these models allows for effective transfer learning, which can be used to generate new, contextually appropriate content across diverse domains. They represent a unified approach, where a single model can generate various outputs, offering state-of-the-art performance owing to their extensive training. Without foundational models as the backbone, there would be no generative AI models.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p16">  
   <img alt="figure" src="../Images/CH02_F01_Bahree.png" width="925" height="664"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.1</span> Foundational model overview</h5>
  </div> 
  <div class="readable-text" id="p17"> 
   <p>Here are some examples of the common foundation models: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p18"> GPT (Generative Pre-trained Transformer) Family is an NLP family of models developed by OpenAI. It is a large language model trained on a massive dataset of text and code, which makes it capable of generating text, translating languages, writing creative content, and answering your questions informatively. GPT-4, the latest version at the time of this writing, is also a multimodal model—it can manage both language and images. </li> 
   <li class="readable-text" id="p19"> Codex is a large language model trained specifically on code that is used to help with code generation. It supports over a dozen programming languages, including some of the more commonly used, such as C#, Java, Python, JavaScript, SQL, Go, PHP, and Shell, among others.  </li> 
   <li class="readable-text" id="p20"> Claude is an LLM built by a startup called Anthropic. Like OpenAI’s ChatGPT, it predicts the next token in a sequence when given a certain prompt and can generate text, write code, summarize, and reason. </li> 
   <li class="readable-text" id="p21"> BERT (Bidirectional Encoder Representations from Transformers) is an NLP model developed by Google. It is a bidirectional model, meaning it can process text in both directions, from left to right and right to left. This feature makes it better at understanding the context of words and phrases. </li> 
   <li class="readable-text" id="p22"> PaLM (Pathway Language Model) and its successor PaLM2 are large multimodal language models developed by Google. The multimodal model can process text, code, and images simultaneously, making it capable of performing a wider range of tasks across those modalities compared to traditional language models operating only in one modality. </li> 
   <li class="readable-text" id="p23"> Gemini is Google’s latest AI model, capable of understanding text, images, videos, and audio. It’s a multimodal model described as being able to complete complex tasks in math, physics, and other areas, as well as understanding and generating high-quality code in various programming languages. Gemini was built from the ground up to be multimodal, meaning it can generalize and seamlessly understand, operate across, and combine different types of information. It’s also the new umbrella name for all of Google’s AI tools, replacing Google Bard and Duet AI, and is considered a successor to the PaLM model. </li> 
  </ul> 
  <div class="readable-text" id="p24"> 
   <p>Once a foundational model is trained, it can be adapted to a wide range of downstream tasks by fine-tuning its parameters. Fine-tuning involves adjusting the model’s parameters to optimize the model for a specific task. It can be done using a small amount of labeled data. By fine-tuning these models for specific tasks or domains, we use their general understanding of language and supplement it with task-specific knowledge. The benefits of this approach include time and resource efficiency, coupled with remarkable versatility. We can also adapt a model via Prompt engineering, which we’ll discuss later in this chapter. Now that we know more about foundational models, let’s explore LLMs.</p> 
  </div> 
  <div class="readable-text" id="p25"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_23"><span class="num-string">2.2</span> Overview of LLMs</h2> 
  </div> 
  <div class="readable-text" id="p26"> 
   <p>LLMs represent a significant advancement in AI. They are trained on a vast amount of text data, such as books, articles, and websites, to learn patterns in human language. They are also hard to develop and maintain, as they require lots of data, computing, and engineering resources. OpenAI’s ChatGPT is an example of an LLM—it generates human-like text by predicting the probability of a word considering the words already used in the text.</p> 
  </div> 
  <div class="readable-text intended-text" id="p27"> 
   <p>The model learns to generate coherent and contextually relevant sentences by adjusting its internal parameters to minimize the difference between its predictions and the actual outcomes in the training data. When generating text, the model chooses the word with the highest probability as its subsequent output and then repeats the process for the next word.</p> 
  </div> 
  <div class="readable-text intended-text" id="p28"> 
   <p>LLMs are foundational models adapted for natural language processing and language generation tasks. These LLMs are general-purpose and can handle tasks without task-specific training data. As briefly described in the previous chapter, given the right prompt, they can answer questions, write essays, summarize texts, translate languages, and even generate code. LLMs can be applied to many applications across different industries, as outlined in chapter 1—from summarization to classification, Q&amp;A chatbots, content generation, data analysis, entity extraction, and more. Before we get into more details of LLMs, let us look at the Transformer architecture, which makes these foundational models possible.</p> 
  </div> 
  <div class="readable-text" id="p29"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_24"><span class="num-string">2.3</span> Transformer architecture</h2> 
  </div> 
  <div class="readable-text" id="p30"> 
   <p>Transformers are the bedrock of foundational models and are responsible for their remarkable language understanding capabilities. The Transformer model was first introduced in the paper “Attention Is All You Need” by Vaswani et al. in 2017 [1]. Since then, Transformer-based models have become state-of-the-art for many tasks. GPT and BERT are examples of Transformer-based models, and the “T” in GPT stands for Transformers.</p> 
  </div> 
  <div class="readable-text intended-text" id="p31"> 
   <p>At their core, Transformers use a mechanism known as attention (specifically self-attention), which allows the model to consider the entire context of a sentence, considering all words simultaneously rather than processing the sentence word by word. This approach is more efficient and can improve the results of many NLP tasks. </p> 
  </div> 
  <div class="readable-text intended-text" id="p32"> 
   <p>The strength of this approach is that it captures dependencies regardless of their position in the text, which is an essential factor in language understanding. This is key for tasks such as machine translation and text summarization, where the meaning of a sentence can depend on terms that are several words apart. </p> 
  </div> 
  <div class="readable-text intended-text" id="p33"> 
   <p>Transformers can parallelize their computations, which makes them much faster to train than other types of neural networks. This mechanism enables the model to pay attention to the most relevant parts of the task input.</p> 
  </div> 
  <div class="readable-text intended-text" id="p34"> 
   <p>In the context of generative AI, a transformer model would take an input (such as a prompt) and generate an output (such as the next word or the completion of the sentence) by weighing the importance of each part of the input in generating the output. For example, in the sentence “The cat sat on the...,” a Transformer model would likely give much weight to the word “cat” when determining that the likely next word might be “mat.” These models exhibit generative properties by predicting the next item in a sequence—the next word in a sentence or the next note in a melody. We explore this more in the next chapter.</p> 
  </div> 
  <div class="readable-text intended-text" id="p35"> 
   <p>Transformer models are usually very large, requiring significant computational resources to train and use. Using a car analogy, think of Transformer models as supercharged engines that need much power to run but do amazing things. Think of them as the next step after models such as ResNET 50, which is used for recognizing images. While ResNET 50 is like a car with 50 gears, OpenAI’s GPT-3 is like a megatruck with 96 gears and extra features. Because of their advanced capabilities, these models are a top pick for creating intelligent AI outputs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p36"> 
   <p>LLMs use transformers, which are composed of an encoder and a decoder. The encoder processes the input text (i.e., the prompt) and generates a sequence of hidden states that represent the meaning of the input text. The decoder uses these hidden states to generate the output text. These encoders and decoders form one layer, similar to a mini-brain. Multiple layers can be stacked one upon another. As outlined earlier, GPT3 is a decoder-only model with 96 layers.</p> 
  </div> 
  <div class="readable-text" id="p37"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_25"><span class="num-string">2.4</span> Training cutoff</h2> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>In the context of foundational models, the training cutoff refers to the point at which the model’s training ends, that is, the time until the data used to train the model was collected. In the case of AI models developed by OpenAI, such as GPT-3 or GPT-4, the training cutoff is when the model was last trained on new data.</p> 
  </div> 
  <div class="readable-text intended-text" id="p39"> 
   <p>This cutoff is important because after this point, the model is not aware of any events, advancements, new concepts, or changes in language usage. For example, the training data cutoff for the GPT-3.5 Turbo was in September 2021, GPT-4 Turbo in April 2023, and GPT-4o in October 2023, meaning the model does not know about real-world events or advancements in various fields beyond that point.</p> 
  </div> 
  <div class="readable-text intended-text" id="p40"> 
   <p>The key point is that while these models can generate text based on the data they were trained on, they do not learn or update their knowledge after the training cutoff. They cannot access or retrieve real-time information from the internet or any external database. Their responses are generated purely based on patterns they have learned during their training period.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p41"> 
   <p><span class="print-book-callout-head">Note </span> The recent announcement that the premium versions of ChatGPT will have access to the internet via the Bing plugin doesn’t mean that the model has more up-to-date information. This uses a pattern called RAG (retrieval-augmented generation), which will be covered later in chapter 7.</p> 
  </div> 
  <div class="readable-text" id="p42"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_26"><span class="num-string">2.5</span> Types of LLMs</h2> 
  </div> 
  <div class="readable-text" id="p43"> 
   <p>As shown in table 2.1, there are three categories of LLMs. When we talk about LLMs, having the context is crucial, and it might not be evident in some cases. This is of great importance, as the paths we can go down when using the models aren’t interchangeable, and picking the right type depends on the use case one tries to solve. Furthermore, there is also a dependency on how effectively one can adapt the models to specific use cases.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p44"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 2.1</span> Types of LLMs</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         LLM Type 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Base LLM <br/></td> 
      <td>  These are the original models, pretrained on a massive corpus of text data, and they can generate text based on the patterns they learned during this training. Some also call these raw language models or even refer to them as foundational models; they can be used out of the box to generate text. They learn powerful and general representations but lack specific expertise in a particular task. GPT-3’s DaVinci model is an example of a base LLM. <br/></td> 
     </tr> 
     <tr> 
      <td>  Instruction-based LLM <br/></td> 
      <td>  This involves using a base LLM and providing explicit instructions in prompt input. In many examples we saw in the previous chapter, we instructed the model to follow instructions, such as “Translate the following text to French:” or “Summarize this article:” Sometimes, these models are also called instruction-tuned LLMs. <br/></td> 
     </tr> 
     <tr> 
      <td>  Fine-tuned LLM <br/></td> 
      <td>  Fine-tuning involves taking the base LLM and training it further on a task it might perform poorly at, often in a specific domain. An example would be training the model on medical literature if we want it to understand medical topics or training it on customer service interactions if we want it to respond to customer inquiries for a specific industry. Fine- tuning can help make the model more accurate or helpful to particular tasks or domains, but it requires additional data and training time. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p45"> 
   <p>It’s worth noting that all these methods have their pros and cons:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p46"> Base LLMs are versatile and can handle many tasks without additional training. However, they might not be as accurate or reliable as you’d like for specific tasks or domains, especially in an enterprise setting. </li> 
   <li class="readable-text" id="p47"> Instruction-based usage can be very effective for some tasks, but it requires careful prompt crafting and doesn’t fundamentally change the model’s capabilities. This is where many of the prompt engineering techniques and best practices apply. </li> 
   <li class="readable-text" id="p48"> Fine-tuning can yield excellent results for specific tasks or domains. However, it requires additional resources and comes with the risk of overfitting the training data, which could limit the model’s ability to generalize to new examples. </li> 
  </ul> 
  <div class="readable-text" id="p49"> 
   <p>In addition, one can take approaches (zero-shot, few-shot, and transfer learning) to adapt the LLM further for a specific task to make it perform better and be more robust in those specific domains. To some extent, the type of LLM implemented also dictates which approach is better suited for the need. Each approach has its strengths and weaknesses, and the best one depends on the specific task, data available, and resources at hand:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p50"> <em>Zero-shot learning</em><em> </em>—A model can generalize to a task without seeing examples of that task during training. For example, we could ask a model trained only in English to translate from English to German, even if it hasn’t seen any specific examples of the German language during training. It can do this zero-shot translation using semantic similarity, word embeddings, and machine learning. Using these techniques, we can measure how similar two words are even in different languages. </li> 
   <li class="readable-text" id="p51"> <em>Few-shot learning</em><em> </em>—This involves showing the model examples of the task we want to perform and then asking the model to do the same task with a new example. Thus, if we want a model to identify the sentiment of a tweet, we might show it a few examples of tweets and associated sentiment and then ask it to predict the sentiment of a new tweet. </li> 
   <li class="readable-text" id="p52"> <em>Transfer learning</em><em> </em>—This involves training a model on one task and then using what it learned for a different but related task. For example, although LLMs have been trained in language and not specific customer support tickets, they can be used to categorize customer support tickets into different categories, such as billing, technical issues, or general inquiries. This can help streamline the customer support process and ensure a speedy resolution, with higher customer satisfaction. </li> 
  </ul> 
  <div class="readable-text" id="p53"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_27"><span class="num-string">2.6</span> Small language models</h2> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>Small language models (SLMs) are an emerging trend that has excited many enterprises. They are scaled-down versions of larger language models designed to provide many benefits of their larger counterparts while being more resource efficient and accessible. They differ from LLMs (e.g., GPT-4) in several ways, primarily in size and complexity, computational resources, training and operational costs, and performance quality. Techniques such as knowledge distillation and transfer learning empower smaller models to excel in analysis, translation, and summarization with faster training. In some cases, they can also match or outperform the larger LLMs, making them a game changer.</p> 
  </div> 
  <div class="readable-text intended-text" id="p55"> 
   <p>Regarding size and complexity, SLMs are notably smaller and have fewer parameters than large models such as GPT-4. This difference in scale is substantial: a small model may have millions to a few billion parameters, in contrast to the tens or hundreds of billions of parameters in large models. This reduction in size and complexity makes SLMs fundamentally different in how they process and generate language.</p> 
  </div> 
  <div class="readable-text intended-text" id="p56"> 
   <p>From a computational resource perspective, SLMs’ smaller size requires less computational power. This reduced requirement makes them more suitable for applications with limited processing capabilities or situations where real-time responses are crucial. The lesser demand for computational resources also means that SLMs can be deployed in a wider range of environments, including on-edge devices or systems with lower processing capacities.</p> 
  </div> 
  <div class="readable-text intended-text" id="p57"> 
   <p>Regarding training and operational costs, SLMs are generally less expensive to train and operate. This cost-effectiveness stems from their reduced complexity and the smaller amount of data needed to train them. Consequently, SLMs become more accessible to individuals and organizations with limited budgets, democratizing access to advanced language-processing technologies.</p> 
  </div> 
  <div class="readable-text intended-text" id="p58"> 
   <p>However, when it comes to performance and quality, while capable of handling a wide range of language tasks effectively, SLMs generally do not match the performance level of larger models. This is particularly evident in more complex tasks that require a broad understanding of context or specialized knowledge. Large models, with their greater depth and breadth of data and understanding, are better equipped to handle such complexities. In contrast, SLMs might struggle with these challenges due to their inherent limitations in size and training.</p> 
  </div> 
  <div class="readable-text intended-text" id="p59"> 
   <p>Strategic data selection and a new, innovative training approach are the two key reasons why SLMs such as the Phi series have been so successful. Strategic data selection prioritizes high quality over quantity and involves using textbook-quality data, which includes synthetic datasets and meticulously curated web data [2]. Data is selected to provide a robust foundation of common-sense reasoning and general knowledge. This strategic approach to data selection is crucial for the model’s exceptional performance across a spectrum of tasks.</p> 
  </div> 
  <div class="readable-text intended-text" id="p60"> 
   <p>In contrast, the innovative training approach includes scaling up from smaller models such as Phi-1.5 and embedding its knowledge in Phi-2 [3]. This method accelerates training convergence and enhances benchmark scores, challenging conventional scaling laws and demonstrating that remarkable capabilities can be achieved even with smaller-scale language models.</p> 
  </div> 
  <div class="readable-text intended-text" id="p61"> 
   <p>SLMs are still early in the lifecycle but have been increasingly considered for production use in enterprises. However, their readiness largely depends on the specific requirements and application context. Here are some factors to consider:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p62"> <em>Task complexity</em><em> </em>—SLMs are suitable for simpler, more defined tasks. However, a larger model might be necessary for optimal performance if the enterprise application involves complex language understanding or generation. </li> 
   <li class="readable-text" id="p63"> <em>Resource constraints</em><em> </em>—SLMs are an excellent choice for businesses with limited computational resources or those needing to deploy models on edge devices due to their lower resource requirements. </li> 
   <li class="readable-text" id="p64"> <em>Cost efficiency</em><em> </em>—Operating SLMs is generally more cost-effective than operating larger models in terms of computational resources and energy consumption. This can be a significant advantage for enterprises looking to minimize operational costs. </li> 
   <li class="readable-text" id="p65"> <em>Speed and responsiveness</em><em> </em>—SLMs can offer faster response times, which are beneficial for applications where real-time interaction is critical, such as in customer service chatbots. </li> 
   <li class="readable-text" id="p66"> <em>On-premise</em><em> </em>—For applications that cannot be deployed or connected to the cloud for regulatory or policy reasons, SLMs can be an option, as they can be more easily deployed on-premises or in a private cloud. </li> 
  </ul> 
  <div class="readable-text" id="p67"> 
   <p>A few examples of SLMs available today include</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p68"> <em>Phi-3</em><em> </em>—A family of small language models, which is a Phi-2 successor, recently introduced by Microsoft. They are noted for outperforming other models of similar or even larger sizes across various benchmarks and come in three sizes: mini (3.8B), small (7B), and medium (14B). </li> 
   <li class="readable-text" id="p69"> <em>Phi-2</em><em> </em>—A 2.7 billion parameter model from Microsoft that demonstrates state-of-the-art performance on reasoning and language-understanding tasks, which can outperform models 25x its size. </li> 
   <li class="readable-text" id="p70"> <em>Orca 2</em><em> </em>—A 7-billion- or 13-billion-parameter model from Microsoft that learns various reasoning techniques and solution strategies from a more capable teacher model 5. </li> 
   <li class="readable-text" id="p71"> <em>Gemini Nano</em><em> </em>—A 122-million-parameter model from Google, part of the Gemini series, designed for efficient inference and deployment on edge devices. </li> 
   <li class="readable-text" id="p72"> <em>DistilBERT</em><em> </em>—A smaller version of BERT that retains 97% of its language understanding capabilities while being 40% smaller and 60% faster. </li> 
   <li class="readable-text" id="p73"> <em>GPT-Neo</em><em> </em>—A smaller version of the GPT architecture (125M and 1.3B), part of the GPT-Neo series created by EleutherAI. </li> 
  </ul> 
  <div class="readable-text" id="p74"> 
   <p>These SLMs are particularly useful when deploying a large model is infeasible due to cost, speed, or computational requirements. They balance performance and efficiency, making advanced NLP capabilities more accessible.</p> 
  </div> 
  <div class="readable-text intended-text" id="p75"> 
   <p>While SLMs might not be suitable for every enterprise application, especially those requiring deep understanding or complex language generation, they are ready for production in many scenarios, particularly where efficiency, speed, and cost are key considerations. Enterprises should evaluate their specific needs and constraints to determine if an SLM is the right choice for their application.</p> 
  </div> 
  <div class="readable-text" id="p76"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_28"><span class="num-string">2.7</span> Open source vs. commercial LLMs</h2> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>Today’s commercial models provide top-notch performance in terms of AI quality and wide-ranging abilities. However, since the release of ChatGPT, there has been a significant shift toward open source models. Many of these open source initiatives focus on developing smaller foundational models, asserting they can achieve nearly the same quality levels without significant loss. Figure 2.2 [4] illustrates these lineages and how they have exploded quickly.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p78">  
   <img alt="figure" src="../Images/CH02_F02_Bahree.png" width="1015" height="471"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.2</span> Timeline of LLMs with 10+B parameters: A Survey of LLMs </h5>
  </div> 
  <div class="readable-text" id="p79"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_29"><span class="num-string">2.7.1</span> Commercial LLMs</h3> 
  </div> 
  <div class="readable-text" id="p80"> 
   <p>Currently, there are several commercial LLMs, and they will print and have started making traction. Almost all of them follow the OpenAI paradigm and are exposed via an API we use. Although still startups, many have serious funding, and their founders have deep research backgrounds:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p81"> OpenAI is an AI research lab that develops and publishes cutting-edge AI models, such as the GPT series. It certainly has the most mindshare today. It has several foundational models, such as GPT-4, DALL.E, and ChatGPT, and is the most mature in this group, with serious backing and ownership by Microsoft. </li> 
   <li class="readable-text" id="p82"> Azure OpenAI and OpenAI offer access to powerful language models but differ in their nature and implementation. The primary distinction is that Azure OpenAI is a managed service, whereas OpenAI is not. Microsoft handles the underlying infrastructure and maintenance for Azure OpenAI, making it a valuable option for businesses lacking the resources to manage their OpenAI deployment. Furthermore, Azure OpenAI packages the raw OpenAI models into developer-friendly services that developers can seamlessly integrate into their applications. These services run on Azure, ensuring high scalability, reliability, and global availability. </li> 
   <li class="readable-text" id="p83"> Anthropic is a startup founded by ex-OpenAI engineers that has released Claude, an LLM that can generate text and code. Their key differentiator is implementing the LLM using constitutional AI [5]. Constitutional AI uses reinforcement learning (RL) and traditional supervised learning and claims to produce less harmful outputs. As of this publication, Anthropic was backed by both Google and Amazon. Claude 3, the latest family of models, has three versions: Haiku (small-sized), Sonnet (medium-sized), and Opus (large-sized) models. </li> 
   <li class="readable-text" id="p84"> Gemini is Google’s latest GenAI model, available as part of the Google Cloud offering in the recently launched Google AI Studio product. At the time of this writing, Google is opening up API access to the models in private preview. </li> 
   <li class="readable-text" id="p85"> Cohere AI, a startup originating from a Transformer paper (“Attention is all you need”), has an LLM and other products such as Neural Search and Embed. </li> 
  </ul> 
  <div class="readable-text" id="p86"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_30"><span class="num-string">2.7.2</span> Open source LLMs</h3> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>A new crop of OSS LLMs is coming up, and some of these will compete with ChatGPT. As figure 2.1 shows, there are too many to mention, but a few noteworthy ones are listed in table 2.2.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p88"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 2.2</span> Open source LLMs</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Company 
       </div></th> 
      <th> 
       <div>
         Open source LLM 
       </div></th> 
      <th> 
       <div>
         Parameter size 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Meta <br/></td> 
      <td>  Llama LLM is one of the models that has inspired many other OSS models. It comes in many sizes (7B, 13B, 33B, and 65B), and while smaller than GPT-3, it can be matched across many tasks. Meta shared the models with researchers (and they were also leaked separately online), inspiring many others to use that as a jumping-off point.  <br/></td> 
      <td>  Various (7B–65B) <br/></td> 
     </tr> 
     <tr> 
      <td>  Databricks  <br/></td> 
      <td>  Databricks recently released v2 of Dolly, which they label the “world’s first truly open instruction-tuned LLM.” It is released under the CCA 3.0 license, allowing anyone to use, extend, and modify it, including for commercial purposes. <br/></td> 
      <td>  12B <br/></td> 
     </tr> 
     <tr> 
      <td>  Alpaca  <br/></td> 
      <td>  Standford University’s Alpaca, an instruction model based on Llama, claims to match the GPT-3.5 Turbo performance in some tasks. <br/></td> 
      <td>  7B <br/></td> 
     </tr> 
     <tr> 
      <td>  FreedomGPT  <br/></td> 
      <td>  This is an OSS conversational agent based on Alpaca. They claim to offer 100% uncensored and private conversations. <br/></td> 
      <td>  Not disclosed <br/></td> 
     </tr> 
     <tr> 
      <td>  Vicuna  <br/></td> 
      <td>  Academic researchers from multiple institutions (UC Berkeley, CMU, Stanford, UC San Diego, and MBZUAI) released Vicuna, a fine-tuned version of Llama that matches the GPT4 performance across many tasks. <br/></td> 
      <td>  13B <br/></td> 
     </tr> 
     <tr> 
      <td>  Koala  <br/></td> 
      <td>  Berkley AI Research released Koala, a fine-tuned version of Llama using internet dialogues. <br/></td> 
      <td>  13B <br/></td> 
     </tr> 
     <tr> 
      <td>  ChatLLaMa  <br/></td> 
      <td>  Technically, it’s not a model but tooling for models. Nebuly AI released ChatLLaMa, a library that can create ChatGPT-like conversational assistance using your data. <br/></td> 
      <td>  7B <br/></td> 
     </tr> 
     <tr> 
      <td>  ColossalChat  <br/></td> 
      <td>  UC Berkeley’s ColossalAI project released ColossalChat, a ChatGPT-like model that includes complete RLHF pipelines based on Llama. <br/></td> 
      <td>  7B <br/></td> 
     </tr> 
     <tr> 
      <td>  Falcon  <br/></td> 
      <td>  The Technology Innovation Institute (TII) in the United Arab Emirates released a family of LLMs called the Falcon LLM model. At the time, Falcon was the biggest OSS LLM ever released and was at the top of the OSS LLM Leaderboard. More recently, a more powerful 180B parameters model is again at the top of the leaderboard. <br/></td> 
      <td>  Various (1B–180B) <br/></td> 
     </tr> 
     <tr> 
      <td>  Mistral  <br/></td> 
      <td>  Mistral AI, a French startup, has developed a range of models. Some are open source models licensed under Apache 2.0, a permissive license allowing unrestricted use in any context. As mentioned in the previous chapter, they also have commercial models. <br/></td> 
      <td>  Various (7B – 141B) <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p89"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">OpenAI vs. Azure OpenAI</h5> 
   </div> 
   <div class="readable-text" id="p90"> 
    <p>Azure OpenAI and OpenAI are both services that provide access to OpenAI’s powerful language models, but they have some key differences. OpenAI caters more to smalland medium-business individual developers and startups. In contrast, Azure OpenAI is intended for enterprises that need additional security and availability in different parts of the world and that have regulatory needs.</p> 
   </div> 
   <div class="readable-text" id="p91"> 
    <p>Azure OpenAI offers additional enterprise-ready features, such as data privacy, customer-managed keys, encryption at rest, private networking, regional availability, and responsible AI content filtering. These features can be important for businesses that need to comply with specific security or regulatory requirements.</p> 
   </div> 
   <div class="readable-text" id="p92"> 
    <p>Consequently, the APIs between the two are similar but not the same. However, the underlying models are the same, and Azure OpenAI has a deployment that incorporates these additional features that most enterprises require.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p93"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_31"><span class="num-string">2.8</span> Key concepts of LLMs</h2> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>This section describes the architecture of a typical LLM implementation. Figure 2.3 shows the abstract structure of a common LLM implementation at a high level; it follows this process whenever we use an LLM such as OpenAI’s GPT.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p95">  
   <img alt="figure" src="../Images/CH02_F03_Bahree.png" width="765" height="665"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.3</span> Conceptual architecture of an LLM</h5>
  </div> 
  <div class="readable-text" id="p96"> 
   <p>The model starts with the input text—the prompt. It is first converted into a sequence of tokens using tokenization. Each token is then converted into a numerical vector via a process called embedding, which acts as the encoder input.</p> 
  </div> 
  <div class="readable-text intended-text" id="p97"> 
   <p>The encoder processes the input sequence and generates a sequence of hidden states. These hidden states are then fed into the decoder with a start token. The decoder generates the output sequence one token at a time by predicting the next token based on the previous tokens and hidden states. </p> 
  </div> 
  <div class="readable-text intended-text" id="p98"> 
   <p>Once all the layers have processed the information, the model predicts the next token in the learned sequence. This outcome is converted back to the text, and we see the response. This process runs in an iterative loop and occurs for each new token generated, thus creating a coherent text output. The final text that the model generates is an emergent property of this layered, iterative process. The final output sequence is also called a completion.</p> 
  </div> 
  <div class="readable-text intended-text" id="p99"> 
   <p>Let’s examine each of these LLM aspects in more detail.</p> 
  </div> 
  <div class="readable-text" id="p100"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_32"><span class="num-string">2.8.1</span> Prompts</h3> 
  </div> 
  <div class="readable-text" id="p101"> 
   <p>A prompt is how we “talk” to these models. A prompt is just text describing the task we are trying to achieve using natural language. The output of these models is also text. The ability to express our intention in this manner (natural language) instead of conforming to the input restrictions of a machine makes prompts powerful. Crafting or designing the text in the prompt is akin to programming the model and creating a new paradigm called prompt engineering, which will be covered later in the book. </p> 
  </div> 
  <div class="readable-text intended-text" id="p102"> 
   <p>Prompts allow us to capture a broader set of intents and emotions using the same paradigm. In the following two examples (generated by AI), a prompt asks for a Haiku outlining how great LLMs are: one haiku is happy, and the other is sad. Getting this level of emotion and expressiveness on the same task was impossible.</p> 
  </div> 
  <div class="readable-text prompt" id="p103"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Prompt.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Write a happy haiku on how great LLMs are.</p> 
  </div> 
  <div class="readable-text response" id="p104"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><span class="response-head">AI-generated response</span></p> 
  </div> 
  <div class="readable-text response" id="p105"> 
   <p>LLMs are great</p> 
  </div> 
  <div class="readable-text response" id="p106"> 
   <p>They can chat and generate</p> 
  </div> 
  <div class="readable-text response" id="p107"> 
   <p>Creative and smart</p> 
  </div> 
  <div class="readable-text prompt" id="p108"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Prompt.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Write a sad haiku on how great LLMs are.</p> 
  </div> 
  <div class="readable-text response" id="p109"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><span class="response-head">AI-generated response</span></p> 
  </div> 
  <div class="readable-text response" id="p110"> 
   <p>LLMs are great</p> 
  </div> 
  <div class="readable-text response" id="p111"> 
   <p>But they cannot feel or love</p> 
  </div> 
  <div class="readable-text response" id="p112"> 
   <p>Only simulate</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p113"> 
   <p><span class="print-book-callout-head">NOTE</span>  LLMs do not understand text like humans do. They are statistical models that learn patterns in data and use those patterns to generate output. Despite their impressive output, these models don’t comprehend the content generated in the same sense as we do—including beliefs, desires, consciousness, and right and wrong. They are just extremely good at predicting the next word in a sequence of text based on patterns they’ve seen millions of times.</p> 
  </div> 
  <div class="readable-text" id="p114"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_33"><span class="num-string">2.8.2</span> Tokens</h3> 
  </div> 
  <div class="readable-text" id="p115"> 
   <p>Tokens are the basic units of text that an LLM uses to process both the request and the response, that is, to understand and generate text. Tokenization is the process of converting text into a sequence of smaller units called tokens. When using LLMs, we use tokens to converse with these models, which is one of the most fundamental elements of understanding LLMs. </p> 
  </div> 
  <div class="readable-text intended-text" id="p116"> 
   <p>Tokens are the new currency when incorporating LLMs into your application or solutions. They directly correlate with the cost of running models, both in terms of money and of the experience with latency and throughput. The more tokens, the more processing the model must do. This means more computational resources are required for the model, which means lower performance and higher latency.</p> 
  </div> 
  <div class="readable-text intended-text" id="p117"> 
   <p>LLMs convert the text into tokens before processing. Depending on the tokenization algorithm, they can be individual characters, words, sub-words, or even larger linguistic units. A rough rule of thumb is that one token is approximately four characters or 0.75 words for English text. For most LLMs today, the token size that they support includes both the input prompt and the response.</p> 
  </div> 
  <div class="readable-text intended-text" id="p118"> 
   <p>Let’s illustrate this through an example. Figure 2.4 shows how the sentence “I have a white dog named Champ” gets tokenized (using OpenAI’s tokenizer in this case). Each block represents a different token. In this example, we use eight tokens.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p119">  
   <img alt="figure" src="../Images/CH02_F04_Bahree.png" width="774" height="247"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.4</span> Tokenizer example</h5>
  </div> 
  <div class="readable-text" id="p120"> 
   <p>LLMs generate text by predicting the next word or symbol (token) most likely to follow a given sequence of words or symbols (tokens) they use as input, that is, the prompt. We show a visual representation of this in figure 2.5, where the list of tokens on the right shows the highest probability of tokens following the prompt “The dog sat on.” We can influence some of this probability of tokens using a few parameters we will see later in the chapter.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p121">  
   <img alt="figure" src="../Images/CH02_F05_Bahree.png" width="1012" height="450"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.5</span> LLM—next token predictor</h5>
  </div> 
  <div class="readable-text intended-text" id="p122"> 
   <p>Suppose we have a sequence of tokens with a length of n. Utilizing these n tokens as the context, we generate the subsequent token, n + 1. This newly predicted token is then appended to the original sequence of tokens, thereby expanding the context. Consequently, the expanded context window for generating token n + 2 becomes n + (n + 1). This process is repeated in a continuous loop until a predetermined stop condition, such as a specific sequence or a size limit for the tokens, is reached.<span class="aframe-location"/></p> 
  </div> 
  <div class="readable-text intended-text" id="p123"> 
   <p>For example, if we have a sentence, “Hawaiian pizza is my favorite,” the probability distribution of the next word we see is shown in figure 2.6. The most likely word is “type,” finishing the sentence “Hawaiian pizza is my favorite type.”<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p124">  
   <img alt="figure" src="../Images/CH02_F06_Bahree.png" width="858" height="281"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.6</span> Next token probability distribution</h5>
  </div> 
  <div class="readable-text" id="p125"> 
   <p>If you run this example again, you will get a probability different from the one shown here. This is because most AI is nondeterministic, specifically in the case of LLMs. Simultaneously, it might predict one token, and it is probably being looked at across all the possible tokens that the model has learned in the training phase.</p> 
  </div> 
  <div class="readable-text intended-text" id="p126"> 
   <p>We also use two examples that outline how one token changes the distribution dramatically (changing one word from “the” to “a”). Figure 2.7 shows that the most probable next token is “mat” at 41% probability. We also see a list of the other tokens and their probabilistic distributions.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p127">  
   <img alt="figure" src="../Images/CH02_F07_Bahree.png" width="858" height="291"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.7</span> Example 1 </h5>
  </div> 
  <div class="readable-text" id="p128"> 
   <p>However, changing one token from “the” to “a” dramatically changes the next distribution set, with the mat jumping up 30 points to a probability of nearly 75%, as shown in figure 2.8.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p129">  
   <img alt="figure" src="../Images/CH02_F08_Bahree.png" width="858" height="287"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.8</span> Example 2</h5>
  </div> 
  <div class="readable-text" id="p130"> 
   <p>Some settings related to LLMs are important and can change how the model behaves and generates text. These settings are the model configurations and can be changed via an API, GUI, or both. We cover model configurations in more detail later in the chapter.</p> 
  </div> 
  <div class="readable-text" id="p131"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_34"><span class="num-string">2.8.3</span> Counting tokens</h3> 
  </div> 
  <div class="readable-text" id="p132"> 
   <p>Many developers will probably be new to tracking tokens when using LLM, especially in an enterprise setting. However, counting tokens is important for several reasons:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p133"> <em>Memory limitations</em><em> </em>—LLMs can process a maximum number of tokens in a single pass. This is due to the memory limitations of their architecture, often defined by their context window (another concept we discuss later in this chapter). For example, OpenAI’s latest GPT-4o model has a content window of 128K, and Google’s latest Gemini 1.5 Pro has a context window of 1M tokens. GPT3.5-Turbo, another OpenAI model, has two models supporting 8K and 16K token lengths. There is research ongoing to see how to solve this, such as LongNet [6] from Microsoft Research, which shows how to scale to 1B context windows. It is important to point out that this is still an active research area and has not been productized yet. </li> 
   <li class="readable-text" id="p134"> <em>Cost</em><em> </em>—When thinking about cost, there are two dimensions: the computational costs in terms of latency, memory, and the overall experience, and the actual cost in terms of money. For each call, the computational resources required for processing tokens directly correlate to the tokens’ length. As the token length increases, it requires more processing time, leading to more computational requirements (specifically memory and GPUs) and higher latency. This also means increased costs for using the LLMs. </li> 
   <li class="readable-text" id="p135"> <em>AI quality</em><em> </em>—The quality of a model’s output depends on the number of tokens it is asked to generate or process. If the text is too short, the model might not have enough context to provide a good answer. Conversely, if the text is too long, the model might lose coherence in its response. We will touch on the notion of good versus poor as part of prompt engineering later in chapter 6. </li> 
  </ul> 
  <div class="readable-text" id="p136"> 
   <p>For many enterprises, cost and performance are key factors in deciding whether to use tokens. Generally speaking, smaller models are more cost-effective and efficient than bigger ones.</p> 
  </div> 
  <div class="readable-text intended-text" id="p137"> 
   <p>Listing 2.1 shows a simple way to calculate the number of tokens. In this example, we use an open source library called <code>tiktoken</code>, released by OpenAI. This tokenizer library implements a byte-pair encoding (BPE) algorithm. These tokenizers are designed with their respective LLMs, ensuring efficient tokenization and optimal performance during pretraining and fine-tuning processes. If you use one of the OpenAI models, you must use this tokenizer; many other transformer models also use it. If needed, you can install the <code>tiktoken</code> library using <code>pip</code> <code>install</code> <code>tiktoken</code></p> 
  </div> 
  <div class="browsable-container listing-container" id="p138"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.1</span> Counting tokens for GPT</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import tiktoken as tk

def count_tokens(string: str, encoding_name: str) -&gt; int:
    # Get the encoding
    encoding = tk.get_encoding(encoding_name) <span class="aframe-location"/> #1

    # Encode the string
    encoded_string = encoding.encode(string)

    # Count the number of tokens
    num_tokens = len(encoded_string)
    return num_tokens

# Define the input string
prompt = “I have a white dog named Champ”

# Display the number of tokens in the String
print(“Number of tokens:” , count_tokens(prompt, “cl100k_base”))</pre> 
    <div class="code-annotations-overlay-container">
     #1 The encoding specifies how the text is converted into tokens.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p139"> 
   <p>Running this code, as expected, gives us the following output:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p140"> 
   <div class="code-area-container"> 
    <pre class="code-area">$ python countingtokens.py
Number of tokens: 7</pre>  
   </div> 
  </div> 
  <div class="readable-text print-book-callout" id="p141"> 
   <p><span class="print-book-callout-head">Note </span> Byte-pair encoding (BPE) is a compression algorithm widely used in NLP tasks, such as text classification, text generation, and machine translation. One of the BPE advantages is that it is reversible and lossless, so we can get the original text. BPE works on any text that the tokenizer’s training data hasn’t seen, and it compresses the text, resulting in shorter token sequences than the original text. BPE also helps generalize repeating patterns in a language and provides a better understanding of grammar. For example, the gerund -ing form is quite common in English (swimming, running, debugging, etc.). BPE will split it into different tokens, so “swim” and “-ing” in swimming become two tokens and generalize better.</p> 
  </div> 
  <div class="readable-text" id="p142"> 
   <p>If we are not sure of the name of the encoding to use, instead of the function <code>get_ encoding()</code>, we can use the <code>encoding_for_model()</code>function. This takes the name of the model we want to use and utilizes the corresponding encoding, such as <code>encoding =</code> <code>tiktoken.encoding_for_model('gpt-4').</code> For OpenAI, table 2.3 shows different supported encodings.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p143"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 2.3</span> OpenAI encodings</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Encoding 
       </div></th> 
      <th> 
       <div>
         OpenAI model 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  <code>cl100k_base</code> <br/></td> 
      <td>  gpt-4, gpt-3.5-turbo, gpt-35-turbo, text-embedding-ada-002 <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>p50k_base</code> <br/></td> 
      <td>  Codex models, text-davinci-002, text-davinci-003 <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>r50k_base</code> <br/></td> 
      <td>  GPT-3 models (davinci, curie, babage, ada) <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p144"> 
   <p>Listing 2.2 shows how to use different encodings and how to get the original text from the tokens. We should understand this as a basic construct for now, but it is useful for more advanced use cases such as caching and chunking text—aspects that we cover later in the book.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p145"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.2</span> Tokens</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import tiktoken as tk

def get_tokens(string: str, encoding_name: str) -&gt; str:
    # Get the encoding
    encoding = tk.get_encoding(encoding_name)

    # Encode the string
    return encoding.encode(string)

def get_string(tokens: str, encoding_name: str) -&gt; str:
    # Get the encoding
    encoding = tk.get_encoding(encoding_name)

    # Decode the tokens
    return encoding.decode(tokens)

# Define the input string
prompt = “I have a white dog named Champ.”

# Display the tokens
print(“cl100k_base Tokens:” , get_tokens(prompt, “cl100k_base”))
print(“  p50k_base Tokens:” , get_tokens(prompt, “p50k_base”))
print(“  r50k_base Tokens:” , get_tokens(prompt, “r50k_base”))

print(“Original String:” , get_string([40, 617, 264, 4251, 5679, 7086, 56690, 13], “cl100k_base”))

$ python encodings.py
cl100k_base Tokens: [40, 617, 264, 4251, 5679, 7086, 56690, 13]
  p50k_base Tokens: [40, 423, 257, 2330, 3290, 3706, 29260, 13]
  r50k_base Tokens: [40, 423, 257, 2330, 3290, 3706, 29260, 13]
Original String: I have a white dog named Champ.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p146"> 
   <p>In addition to the <code>tiktoken</code> library we have been using in the examples, there are a few other popular tokenizers. Remember that each tokenizer is designed for the corresponding LLM and cannot be interchanged:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p147"> <em>WordPiece</em>—Used by the BERT model from Google, it splits text into smaller units based on the most frequent word pieces, allowing for efficient representation of rare or out-of-vocabulary words. </li> 
   <li class="readable-text" id="p148"> <em>SentencePiece</em>—Meta’s RoBERTa model (Robustly Optimized BERT) uses the model. It combines WordPiece and BPE approaches into a single language-agnostic framework, allowing for more flexibility. </li> 
   <li class="readable-text" id="p149"> <em>T5 tokenizer</em>—Based on SentencePiece, it is used by Google’s T5 model (Text-to-Text Transfer Transformer). </li> 
   <li class="readable-text" id="p150"> <em>XLM tokenizer</em>—This is used in Meta’s XLM (Cross-lingual Language Model) and implements a BPE method with learned embeddings (BPEmb). It is designed to handle multilingual text and support cross-lingual transfer learning. </li> 
  </ul> 
  <div class="readable-text" id="p151"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_35"><span class="num-string">2.8.4</span> Embeddings</h3> 
  </div> 
  <div class="readable-text" id="p152"> 
   <p>Embeddings are powerful machine-learning tools for large inputs representing words. They capture semantic similarities in a vector space (i.e., a collection of vectors, as shown in figure 2.9), allowing us to determine if two text chunks represent the same meaning. By providing a similarity score, embeddings can help us better understand the relationships between different pieces of text.</p> 
  </div> 
  <div class="readable-text intended-text" id="p153"> 
   <p>The idea behind embeddings is that words with similar meanings should have similar vector representations, as measured by their distances. Vectors with smaller distances between them suggest they are highly related, and those with longer distances suggest low relatedness. There are a few ways to measure similarities; we will cover these later in chapter 7.</p> 
  </div> 
  <div class="readable-text intended-text" id="p154"> 
   <p>These vectors are learned during training and are used to capture the meaning of words or phrases. AI algorithms can easily utilize these vectors of floating-point numbers.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p155">  
   <img alt="figure" src="../Images/CH02_F09_Bahree.png" width="852" height="92"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.9</span> Embeddings</h5>
  </div> 
  <div class="readable-text" id="p156"> 
   <p>For example, the word “cat” might be represented by a vector as [0.2, 0.3, -0.1], while the word “dog” might be represented as [0.4, 0.1, 0.2]. These vectors can then be used as input to machine learning models for tasks such as text classification, sentiment analysis, and machine translation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p157"> 
   <p>Embeddings are learned when the model is trained on a large corpus of text data. The idea is to capture the meaning of words or phrases based on their context in the training data.</p> 
  </div> 
  <div class="readable-text intended-text" id="p158"> 
   <p>Depending on the task, there are several algorithms for creating embeddings:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p159"> Similarity embeddings are good at capturing semantic similarity between two or more pieces of text. </li> 
   <li class="readable-text" id="p160"> Text search embeddings measure whether long documents are relevant to a short query. </li> 
   <li class="readable-text" id="p161"> Code search embeddings are useful for embedding code snippets and natural language search queries. </li> 
  </ul> 
  <div class="readable-text print-book-callout" id="p162"> 
   <p><span class="print-book-callout-head">Note </span> Embeddings created by one method cannot be understood by another. In other words, if you create an embedding using OpenAI’s API, embeddings of another provider will not understand the vectors created, and vice versa.</p> 
  </div> 
  <div class="readable-text" id="p163"> 
   <p>Listing 2.3 shows how to get an embedding (from OpenAI in this example). We define a function called <code>get_embedding()</code> that takes a string for which we need to create embeddings as a parameter. The function uses OpenAI’s API to generate an embedding for the input text using the <code>text-embedding-ada-002</code> model. The embedding is returned as a list of floating-point numbers.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p164"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.3</span> Getting an embedding in OpenAI</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import os
from openai import OpenAI

client = OpenAI(api_key=’your-API-key’)

def get_embedding(text):
    response = client.embeddings.create(
        model="text-embedding-ada-002",
        input=text)
    return response.data[0].embedding

embeddings = get_embedding("I have a white dog named Champ.")
print("Embedding Length:", len(embeddings))
print("Embedding:", embeddings[:5])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p165"> 
   <p>The vector space resulting from the embedding isn’t a one-to-one mapping to the tokens but can be a lot more. The output of the previous examples is shown next. For brevity, we only show the first five items in the list:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p166"> 
   <div class="code-area-container"> 
    <pre class="code-area"><code>print("Embedding Length:", len(embeddings))</code>
<code>print("Embedding:", embeddings[:5])</code></pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p167"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_36"><span class="num-string">2.8.5</span> Model configuration</h3> 
  </div> 
  <div class="readable-text" id="p168"> 
   <p>Most LLMs expose some configuration settings to the user, allowing one to tweak how the model operates and its behavior to some extent. While a few parameters would change depending on the model implementation, the three key configurations are temperature, top probability (<code>top_p</code>), and max response. Note that some implementations might have a different name but mean the same thing. The OpenAI implementation of GPT calls the maximum response as max tokens. Let us explore these in a little more detail.</p> 
  </div> 
  <div class="readable-text" id="p169"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Max response</h4> 
  </div> 
  <div class="readable-text" id="p170"> 
   <p>The parameter known as max response essentially defines the upper limit for the text length that the model generates. This means that once the model hits this predetermined length, it halts text generation, regardless of whether it is mid-word or mid-sentence. It’s crucial to grasp this configuration because there is a size limit to the tokens most models can process. Increasing this size corresponds to heightened computational demands, leading to increased latency and cost.</p> 
  </div> 
  <div class="readable-text" id="p171"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Temperature</h4> 
  </div> 
  <div class="readable-text" id="p172"> 
   <p>When generating text, as with any foundational model, inherent randomness yields a different output each time we call the model. Temperature is one of the most important settings for controlling the degree of the model’s randomness. Typically, this is a value from 0 to 1, with 0 representing a more accurate and predictable output. In contrast, setting a 1 makes the output more diverse and random, as shown in figure 2.10.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p173">  
   <img alt="figure" src="../Images/CH02_F10_Bahree.png" width="810" height="326"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.10</span> Temperature settings and their effect on probability</h5>
  </div> 
  <div class="readable-text" id="p174"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Top probability (top_p)</h4> 
  </div> 
  <div class="readable-text" id="p175"> 
   <p>The top probability (<code>top_p</code>) parameter (also known as nucleus sampling) is a setting in language model APIs that steers the randomness of the text-generation process. This parameter allows one to fine-tune the balance between creativity and reliability in the text that the model generates. It defines a threshold probability; only words with probabilities above this threshold are considered when the model generates text. When a language model generates text, it predicts the probability of each word being the next in the sequence. The <code>top_p</code> parameter helps truncate this probability distribution to enhance the quality of the generated text.<span class="aframe-location"/></p> 
  </div> 
  <div class="readable-text intended-text" id="p176"> 
   <p>For example, for output generation, setting <code>top_p</code> to a lower value (e.g., 0.3), the model will only consider the top 30% most probable words for the next word in the sequence, as shown in figure 2.11. This makes the text more predictable and less varied. However, if we set <code>top_p</code> to a higher value (e.g., 0.9), the model will consider a much broader range of words, including those that are less likely. This can lead to a more diverse and potentially interesting generation.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p177">  
   <img alt="figure" src="../Images/CH02_F11_Bahree.png" width="910" height="390"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.11</span> Example showing how top-p works</h5>
  </div> 
  <div class="readable-text" id="p178"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">An example</h4> 
  </div> 
  <div class="readable-text" id="p179"> 
   <p>Let’s show how these settings can be programmatically used. The following code snippet shows an example of how to do these configurations with OpenAI. Most of these settings are used sparingly, are optional, and would fall to default values. Only some options, such as <code>max_tokens</code> and <code>temperature</code>, are used in almost every use case:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p180"> 
   <div class="code-area-container"> 
    <pre class="code-area">client = OpenAI(api_key=’your-API-key’)
response = client.completions.create(
  model="text-davinci-003",
  prompt="...",
  temperature=1,
  max_tokens=256,
  top_p=1,
  frequency_penalty=0,
  presence_penalty=0
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p181"> 
   <p>Given that the API is stateless, these settings can differ between different instances and API calls of the same instance, depending on the business scenario one tries to achieve. Furthermore, there are no globally optimal values for these settings, which depend on the task. In general, if you want to have a balanced output and not have the model hallucinate much, a setting of 0.7 or 0.8 is good for temperature. Table 2.4 outlines configuration settings to control the behavior. </p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p182"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 2.4</span> LLM configuration settings</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Configuration 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Max tokens <br/></td> 
      <td>  This sets a limit on the number of tokens per model response. Depending on the model, a maximum limit is shared between the prompt (including system message, examples, message history, and user query) and the model’s response. <br/></td> 
     </tr> 
     <tr> 
      <td>  Temperature <br/></td> 
      <td>  This controls randomness. Lowering the temperature means the model produces more repetitive and deterministic responses. Increasing the temperature results in more unexpected or creative responses. Try adjusting the temperature or <code>top_p</code> but not both. Typically, as sequences get longer, the model naturally becomes more confident in its predictions, and one can use a much higher temperature for long prompts without going off-topic. Conversely, using a high-temperature setting on short prompts can lead to unstable outputs. <br/></td> 
     </tr> 
     <tr> 
      <td>  Top probability ( <code>top_p</code>) <br/></td> 
      <td>  This is a probability threshold that, like temperature, controls randomness but uses a different method. Lowering <code>top_p</code> narrows the model’s token selection to the most likely tokens and ignores the long tail of less likely tokens. Increasing <code>top_p</code> will allow the model to choose from high- and low-likelihood tokens. Try adjusting either temperature or <code>top_p,</code> but not both. <br/></td> 
     </tr> 
     <tr> 
      <td>  Stop sequences <br/></td> 
      <td>  This makes the model end its response at a desired point. The model response will end before the specified sequence so that it won’t contain the stop-sequence text. <br/></td> 
     </tr> 
     <tr> 
      <td>  Frequency penalty <br/></td> 
      <td>  This reduces the chance of repeating a token proportionally based on how often it has appeared in the text. This decreases the likelihood of repeating the same text in response. <br/></td> 
     </tr> 
     <tr> 
      <td>  Presence penalty <br/></td> 
      <td>  This reduces the chance of repeating any token that has appeared in the text so far. This increases the likelihood of introducing new topics in a response. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p183"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_37"><span class="num-string">2.8.6</span> Context window</h3> 
  </div> 
  <div class="readable-text" id="p184"> 
   <p>A context window is a relatively new and very important concept. It refers to the range of tokens or words surrounding a particular word or token that an LLM considers when making predictions. The context window helps the model understand the dependencies and relationships between the words, enabling it to generate more accurate and coherent predictions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p185"> 
   <p>For example, when predicting the next word in a sentence, the context window might include several words preceding the target word. The context window size may vary depending on the model and its architecture. In LLMs, context windows can be quite large, allowing the model to capture long-range dependencies and intricate semantic relationships among the words. These longer-context windows can help get better output for tasks such as text generation, translation, and summarization.</p> 
  </div> 
  <div class="readable-text intended-text" id="p186"> 
   <p>The current LLM architecture limits the context window size to several thousand tokens. Although some of the newer models support up to a million tokens, the context window is still a critical focal point, mainly because the global nature of the attention mechanism imposes computational costs that are quadratic in context length. In other words, the bigger the context windows, the more the computation cost is proportional to the square of the input data size. While having a larger context window might seem good, it is important to understand that it has both positive and negative implications. With larger context windows, the performance of the model is much slower in terms of both understanding the request and the generation, with a higher latency. While we might feel like a longer context would be better, in general, use a smaller window if that would suffice for the task at hand—it would have a better performance than the larger one. Some of the pros are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p187"> <em>Improved comprehension of context</em><em> </em>—A longer context window allows the model to capture long-range dependencies and intricate semantic relationships among words, resulting in better predictions and more coherent text generation. However, this comes at a considerable cost and should be used judiciously. </li> 
   <li class="readable-text" id="p188"> <em>Better performance at complex tasks</em><em> </em>—With a more extensive context window, language models can perform better at tasks that require a better comprehension of the broader context, such as machine translation, summarization, and sentiment analysis. </li> 
  </ul> 
  <div class="readable-text" id="p189"> 
   <p>Here are the cons:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p190"> <em>Increased computational requirements</em><em> </em>—A longer context window requires more memory and computational power to process and store the additional information, which can result in longer training and inference times and require more powerful hardware or distributed computing solutions. </li> 
   <li class="readable-text" id="p191"> <em>Potential for overfittin</em>g<em> </em>—As the context window increases, the model becomes more complex and prone to overfitting, especially if the training data is limited. Overfitting occurs when the model learns to perform exceptionally well on the training data but struggles to generalize new and unseen data. </li> 
   <li class="readable-text" id="p192"> <em>Difficulty handling very long sequences</em><em> </em>—Although longer context windows can improve performance, they may also introduce challenges when processing extremely long sequences. Some models may struggle to maintain the necessary information over such long distances, leading to a degradation in performance. </li> 
   <li class="readable-text" id="p193"> <em>Diminishing returns</em><em> </em>—While increasing the context window size can improve performance, there may be a point of diminishing returns where further increases yield only marginal improvements. It’s essential to balance the context window size with the computational cost and model complexity to achieve optimal performance. </li> 
  </ul> 
  <div class="readable-text" id="p194"> 
   <p>The context window as a concept is quite important for several reasons:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p195"> <em>Captures dependencies</em><em> </em>—A context window allows the model to understand relationships between words, phrases, or sentences within a text. This helps the model grasp the overall meaning and context of the input. </li> 
   <li class="readable-text" id="p196"> <em>Improved predictions</em><em> </em>—This is probably what most of us see when using LLMs, where the context window enables the model to generate more accurate and coherent suggestions based on the preceding text. </li> 
   <li class="readable-text" id="p197"> <em>Provides context for better understanding</em><em> </em>—By considering the context window, LLMs can better understand the context at play and, as a result, the syntactical structure and semantic relationships in the text; this allows for more accurate language understanding and generation. </li> 
  </ul> 
  <div class="readable-text" id="p198"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_38"><span class="num-string">2.8.7</span> Prompt engineering</h3> 
  </div> 
  <div class="readable-text" id="p199"> 
   <p>Prompt engineering is a relatively new field that involves curating or designing prompts to elicit desired responses or behaviors from a machine learning model, particularly LLMs. It is a powerful technique that can improve the performance of language models on various tasks. Prompt engineering is an emerging field that requires creativity and attention to detail. </p> 
  </div> 
  <div class="readable-text intended-text" id="p200"> 
   <p>Prompt engineering can be seen as both an art and a science. It involves careful selection and phrasing of input commands to help guide the AI in producing the desired output. These input commands can be as simple as selecting the right words, phrases, and formats to guide the model in generating high-quality and relevant texts for a specific task. </p> 
  </div> 
  <div class="readable-text intended-text" id="p201"> 
   <p>For instance, to get a detailed answer, you might use a prompt such as “Explain in detail …,” or to get a quick summary, you might use “Summarize in a few bullets … ” Similarly, to make the AI write in the style of a particular author, you might use a prompt like “Write a passage as if you were P. G. Wodehouse.”</p> 
  </div> 
  <div class="readable-text intended-text" id="p202"> 
   <p>Prompt engineering requires understanding the AI model’s capabilities, underlying training data, and how it responds to different kinds of input. Effective prompt engineering can significantly improve the usefulness of AI models in various tasks. Note that this section is just an introduction to prompt engineering as a fundamental concept; we will cover prompt engineering in more depth later in chapter 6.</p> 
  </div> 
  <div class="readable-text" id="p203"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_39"><span class="num-string">2.8.8</span> Model adaptation</h3> 
  </div> 
  <div class="readable-text" id="p204"> 
   <p>LLMs are pretrained and general-purpose, and sometimes they must be fine-tuned. They are trained on a large corpus of text data and can be used as a starting point for training on a smaller dataset for a specific task. Fine-tuning the base LLM on the smaller dataset can improve its performance for that specific task.</p> 
  </div> 
  <div class="readable-text intended-text" id="p205"> 
   <p>Fine-tuning is taking a pretrained model and training it further on a new task or dataset. The pretrained model is used as a starting point, and the weights of the model are adjusted during training to improve its performance on the new task. Fine-tuning is often used in transfer learning, where a model trained on one task is adapted to another related task.</p> 
  </div> 
  <div class="readable-text intended-text" id="p206"> 
   <p>Some examples of fine-tuning LLMs include the following:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p207"> <em>Text classification</em><em> </em>—Fine-tuning an LLM on a specific text classification task, such as sentiment analysis or spam detection </li> 
   <li class="readable-text" id="p208"> <em>Question answering</em><em> </em>—Fine-tuning an LLM on a specific question-answering task, such as answering questions about a specific topic </li> 
   <li class="readable-text" id="p209"> <em>Language generation</em><em> </em>—Fine-tuning an LLM on a specific language generation task, such as generating summaries or translations </li> 
  </ul> 
  <div class="readable-text" id="p210"> 
   <p>Fine-tuning an LLM such as GPT-3.5 Turbo can be a powerful way to customize the model for specific tasks, but it can also be very expensive and should be one of the last options. In some cases, fine-tuning can also lead to catastrophic forgetting. This occurs when the model is fine-tuned on a new dataset, causing it to forget the knowledge it had learned from its original training data, resulting in the fine-tuned model losing its reasoning skills. We will cover model adaptation in more detail in chapter 9, including any pitfalls of fine-tuning.</p> 
  </div> 
  <div class="readable-text" id="p211"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_40"><span class="num-string">2.8.9</span> Emergent behavior</h3> 
  </div> 
  <div class="readable-text" id="p212"> 
   <p>The concept of emergent behavior defines the significance of foundation models and LLMs. Emergent behavior implies unexpected behaviors exhibited by LLMs when interacting with their environment, specifically when trained with large amounts of data. LLMs are not explicitly trained to have these abilities but learn them by observing natural language. The emergence of the behavior is implicitly induced rather than explicitly constructed; it is both the source of scientific excitement and anxiety about unanticipated consequences. </p> 
  </div> 
  <div class="readable-text intended-text" id="p213"> 
   <p>As figure 2.12 shows, a model’s performance on multiple natural language benchmarks (e.g., question answering) is no better than random chance until they reach a certain scale, measured in this example by training computation in FLOPs. The model’s performance sharply increases at this point, which is an example of emergent abilities. This helps us understand that emergent properties of LLMs are not present in smaller models. Furthermore, these abilities show up only at a scale when the model’s size reaches a certain threshold. Emergent properties cannot be predicted by extrapolating the performance of smaller models.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p214">  
   <img alt="figure" src="../Images/CH02_F12_Bahree.png" width="1100" height="760"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.12</span> Model scaling as measured by training computation requirements in FLOP<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p215"> 
   <p>Let us see an example using code, as the exact logic and nature of what a piece of code is doing cannot be predetermined or programmed for an LLM. It needs to rely on the emergent behavior property of foundational models to understand, analyze, and comprehend the code.</p> 
  </div> 
  <div class="readable-text intended-text" id="p216"> 
   <p>Say we have C code that reverses a string, as shown in the following listing. This uses some of C’s more advanced features, such as pointers, XOR operators, and macros. In our example, we use GPT-4 to convert this C function to Python.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p217"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.4</span> Function written in C to reverse a string</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">#include &lt;String.h&gt; // for strlen()

// reverse the given null-terminated String in place
void inplace_reverse(char * str)
{
  if (str)
  {
    char * end = str + strlen(str) - 1;

    // swap the values in the two given variables
    // XXX: fails when a and b refer to same memory location
#   define XOR_SWAP(a,b) do\
    {\
      a ^= b;\
      b ^= a;\
      a ^= b;\
    } while (0)

    // walk inwards from both ends of the String, 
    // swapping until we get to the middle
    while (str &lt; end)
    {
      XOR_SWAP(*str, *end);
      str++;
      end--;
    }
#   undef XOR_SWAP
  }
}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p218"> 
   <p>The first step would be to ask our LLM to convert this code to Python using a prompt similar to the next one. We are omitting the C function (listing 2.4) again for brevity. The code is part of the prompt.</p> 
  </div> 
  <div class="readable-text prompt" id="p219"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Prompt.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Convert this C function to Python:</p> 
  </div> 
  <div class="browsable-container listing-container prompt" id="p220"> 
   <div class="code-area-container"> 
    <pre class="code-area">#include &lt;string.h&gt; // for strlen()

// reverse the given null-terminated String in place
void inplace_reverse(char * str)
{
...
}
</pre> 
   </div> 
  </div> 
  <div class="readable-text" id="p221"> 
   <p>Listing 2.5 shows the output from our model with the code converted to Python. It does a good job of understanding what the C function is doing and outlines Python’s different constraints and properties in the response. Note that we did not give it any hints as to what the function is doing. It deciphered the <code>while</code> loop itself, showing an example of emergent behavior.</p> 
  </div> 
  <div class="readable-text response" id="p222"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>This function will return the reversed string rather than modify the original string because Python strings are immutable. In other words, once a string is created in Python, it cannot be changed. Hence, the function <code>inplace_reverse</code> is a bit of a misnomer in this Python context. You could consider renaming it to something like <code>reverse_string</code>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p223"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.5</span> Converted Python function for string reversal</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def inplace_reverse(str):
    if str:
        # Convert the String to a list since strings are immutable in Python
        lst = list(str)

        # Initialize two pointers at the start and end of the list
        start = 0
        end = len(lst) - 1

        # Reverse the list in place
        while start &lt; end:
            # XOR_SWAP equivalent in Python
            lst[start], lst[end] = lst[end], lst[start]

            # Move the pointers towards the center of the list
            start += 1
            end -= 1

        # Convert the list back into a string
        return ''.join(lst)
    return str</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p224"> 
   <p>We can simplify this further by asking the model the following:</p> 
  </div> 
  <div class="readable-text prompt" id="p225"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Prompt.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Can you further simplify and optimize the Python code?</p> 
  </div> 
  <div class="readable-text response" id="p226"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Sure! One of the simplest ways to reverse a string in Python uses slicing. Here’s a function that will return the reverse of a string:</p> 
  </div> 
  <div class="browsable-container listing-container prompt" id="p227"> 
   <div class="code-area-container"> 
    <pre class="code-area">def reverse_string(s):
    return s[::-1]
            </pre> 
   </div> 
  </div> 
  <div class="readable-text response" id="p228"> 
   <p>The <code>[::-1]</code> is a slice that starts at the end of the string and moves backward.</p> 
  </div> 
  <div class="readable-text prompt" id="p229"> 
   <p>However, if you want to emulate the same logic as the C program, that is, reversing a string in place using two pointers, here is a more Pythonic way of achieving this:</p> 
  </div> 
  <div class="browsable-container listing-container prompt" id="p230"> 
   <div class="code-area-container"> 
    <pre class="code-area">def reverse_string(s):
    lst = list(s)
    lst.reverse()
    return ''.join(lst)
            </pre> 
   </div> 
  </div> 
  <div class="readable-text" id="p231"> 
   <p>In this version, we convert the string to a list, reverse the list in place using the built-in <code>reverse</code> function, and then join the list back into a string. The <code>reverse</code> function in Python uses a similar two-pointer approach, but it’s all under the hood, so the code looks much simpler.</p> 
  </div> 
  <div class="readable-text intended-text" id="p232"> 
   <p>The point the response calls out on a similar two-pointer approach demonstrates the emergent property’s power. We never explicitly called this out anywhere in the conversation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p233"> 
   <p>In this chapter, we looked at the core concepts that make LLMs work and what we need to be mindful of when implementing these new generative AI techniques in the enterprise. We cover new concepts such as prompts, prompt engineering, embeddings, tokens, model parameters, context windows, transformer architecture, and emergent behavior. Now that we have a broad understanding of LLMs and their implications in the next chapter, let us put this into action and work through an API to generate text, both as completions where we provide instructions via a prompt and as chat where we can have a dialogue, and in that, generate text, in the context of the conversation.</p> 
  </div> 
  <div class="readable-text" id="p234"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_41">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p235"> Large language models (LLMs) represent a major advancement in AI. They are trained on vast amounts of text data to learn patterns in human language. </li> 
   <li class="readable-text" id="p236"> LLMs are general-purpose and can handle tasks without task-specific training data, such as answering questions, writing essays, summarizing texts, translating languages, and generating code. </li> 
   <li class="readable-text" id="p237"> Key LLM use cases include summarization, classification, Q&amp;A/chatbots, content generation, data analysis, translation and localization, process automation, research and development, sentiment analysis, and entity extraction. </li> 
   <li class="readable-text" id="p238"> Types of LLMs include base, instruction-based, and fine-tuned LLM. Each has pros and cons and is powered by foundational models. </li> 
   <li class="readable-text" id="p239"> Foundational models are large AI models trained on vast quantities of data at a massive scale, resulting in models that can be adapted to a wide range of downstream tasks. </li> 
   <li class="readable-text" id="p240"> Some key LLM concepts include prompts, prompt engineering, embeddings, tokens, model parameters, context windows, transformer architecture, and emergent behavior. </li> 
   <li class="readable-text" id="p241"> Open source and commercial LLMs have advantages and disadvantages, with commercial models typically offering state-of-the-art performance and open source models providing more flexibility for customization and integration. </li> 
   <li class="readable-text" id="p242"> Small language models (SLMs) are a new emerging trend of lightweight generative AI models that produce text, summarize documents, translate languages, and answer questions. In some cases, they offer capabilities similar to those of larger models. </li> 
  </ul>
 </div></div></body></html>