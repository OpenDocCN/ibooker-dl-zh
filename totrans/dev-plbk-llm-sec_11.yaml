- en: Chapter 11\. Trust the Process
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 11 章\. 信任过程
- en: If you can’t describe what you are doing as a process, you don’t know what you’re
    doing.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你不能将你所做的事情描述为一个过程，你就不知道你在做什么。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: W. Edwards Deming
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: W. Edwards Deming
- en: We’ve spent most of this book exploring the dangers of applying LLM technology
    in production. While there is great power in technology, there are many risks.
    Security, privacy, financial, legal, and reputational risks seem to be around
    every corner. With that understanding, how can you move forward with confidence?
    It’s time to talk about actionable, durable, repeatable solutions. While we’ve
    discussed practical mitigation strategies for each risk, tackling them individually
    as a patchwork isn’t likely to cut it. You must build security into your development
    process to ensure your success.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这本书的大部分内容中探讨了在生产中应用 LLM 技术的危险。虽然技术具有巨大的力量，但风险也很多。安全、隐私、财务、法律和声誉风险似乎无处不在。有了这种理解，你如何能够有信心地前进？是时候讨论可操作、持久、可重复的解决方案了。虽然我们已经讨论了针对每个风险的实用缓解策略，但将它们作为补丁组合单独处理不太可能奏效。你必须将安全构建到你的开发过程中，以确保你的成功。
- en: This chapter will discuss two process elements that have emerged as key ingredients
    in successful projects. First, we’ll discuss the evolution of the DevSecOps movement
    and how it’s become central to application security for any large software project.
    We will examine how it has evolved to encompass specific challenges with AI/ML
    and LLMs. As part of this discussion, we’ll look at development-time tools to
    scan for security vulnerabilities and runtime tools (known as guardrails) that
    can help protect your LLM in production.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论两个在成功项目中成为关键要素的过程元素。首先，我们将讨论 DevSecOps 运动的演变以及它如何成为任何大型软件项目应用安全的核心。我们将探讨它如何演变以涵盖与
    AI/ML 和 LLM 相关的具体挑战。作为这次讨论的一部分，我们将查看开发时用于扫描安全漏洞的工具以及运行时工具（称为护栏），这些工具可以帮助保护你的 LLM
    在生产中的安全。
- en: We’ll also look at how security testing has evolved and the emerging field of
    AI red teaming. Red teams have been around for a long time in cybersecurity circles,
    but AI red teaming has recently gained more prominence as specific techniques
    have evolved that apply to LLM projects.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨安全测试的演变以及新兴的 AI 红队领域。红队在网络安全领域已经存在很长时间了，但随着特定技术的发展，这些技术适用于 LLM 项目，AI 红队最近获得了更多的关注。
- en: The Evolution of DevSecOps
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DevSecOps 的演变
- en: The origin of *DevOps* can be traced back to the early 2000s when it emerged
    in response to the growing need for better collaboration and integration between
    software development (Dev) and IT operations (Ops) teams. This need arose from
    the limitations observed in traditional software development methodologies, which
    often led to siloed teams, delayed releases, and a need for more alignment between
    development objectives and operational stability. The DevOps movement aimed to
    bridge this gap by promoting a culture of collaboration, automation, continuous
    integration, and continuous delivery (CI/CD), thereby enhancing the speed and
    quality of software deployment.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*DevOps* 的起源可以追溯到 2000 年代初，当时它作为对软件开发（Dev）和 IT 运维（Ops）团队之间日益增长的更好协作和整合需求的回应而出现。这种需求源于对传统软件开发方法论的局限性观察，这些方法论往往导致团队孤立、发布延迟，以及开发目标与运营稳定性之间需要更多一致性的需求。DevOps
    运动旨在通过推广协作、自动化、持续集成和持续交付（CI/CD）的文化，从而提高软件部署的速度和质量。'
- en: As DevOps practices matured and became more widely adopted, the critical need
    to integrate security principles into the development lifecycle became increasingly
    apparent. This realization led to integrating security (Sec) into the DevOps process,
    giving us *DevSecOps*. DevSecOps enriches DevOps practices by embedding security
    at every phase of the software development process, from design to deployment.
    The goal is to ensure that security considerations are not an afterthought but
    are integrated into the workflow, enabling the early discovery and mitigation
    of vulnerabilities, thus building more secure software.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着DevOps实践的成熟和更广泛的采用，将安全原则整合到开发生命周期中的关键需求变得越来越明显。这种认识导致了将安全（Sec）整合到DevOps过程中，形成了*DevSecOps*。DevSecOps通过在软件开发过程的每个阶段嵌入安全，从设计到部署，丰富了DevOps实践。目标是确保安全考虑不是事后想法，而是集成到工作流程中，从而实现早期发现和缓解漏洞，从而构建更安全的软件。
- en: We want to enable this same proactive security stance in the development and
    deployment of applications using LLMs. To do so, the principles of DevOps and
    DevSecOps have further inspired the emergence of *MLOps* and *LLMOps* to address
    the unique challenges and requirements of deploying and managing AI/ML systems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在开发和使用LLM的应用程序部署中实现同样的主动安全立场。为此，DevOps和DevSecOps的原则进一步激发了*MLOps*和*LLMOps*的出现，以解决部署和管理AI/ML系统的独特挑战和需求。
- en: MLOps (machine learning operations) focuses on automating and optimizing the
    machine learning lifecycle (including data preparation, model training, deployment,
    and observability) to ensure consistent and efficient ML model development and
    maintenance. LLMOps (large language model operations) explicitly addresses the
    operational needs of large language models, focusing on aspects such as prompt
    engineering, model fine-tuning, and RAG. These specialized practices demonstrate
    the ongoing expansion of the DevOps philosophy, which has adapted to encompass
    emerging technologies’ operational and security needs, thus ensuring their effective
    integration into the broader software development and deployment ecosystem. Using
    concepts from both MLOps and LLMOps will help you extend your organization’s DevSecOps
    process to account for the specific needs of adding advanced AI technology to
    your stack.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps（机器学习运维）专注于自动化和优化机器学习生命周期（包括数据准备、模型训练、部署和可观察性），以确保机器学习模型开发和维护的一致性和效率。LLMOps（大型语言模型运维）明确针对大型语言模型的运营需求，重点关注提示工程、模型微调和RAG等方面。这些专业实践展示了DevOps哲学的持续扩展，该哲学已经适应了新兴技术的运营和安全需求，从而确保它们能够有效地集成到更广泛的软件开发和部署生态系统中。结合MLOps和LLMOps的概念将有助于您将组织的DevSecOps流程扩展到考虑添加高级AI技术到您的堆栈的具体需求。
- en: MLOps
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLOps
- en: MLOps is a set of best practices that aims to streamline and automate the machine
    learning lifecycle, from data preparation and model development to deployment
    and monitoring. Key elements of MLOps include version control for both models
    and data, ensuring reproducibility and traceability, model training, and validation
    for selecting the best model candidates.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps是一套最佳实践，旨在简化和自动化机器学习生命周期，从数据准备和模型开发到部署和监控。MLOps的关键要素包括模型和数据的版本控制，确保可重复性和可追溯性，以及为选择最佳模型候选者进行的模型训练和验证。
- en: CI/CD pipelines are tailored for ML workflows to automate the testing and deployment
    of models and for monitoring model performance in production to catch and address
    model degradation due to model or data drift over time. Additionally, MLOps emphasizes
    collaboration between data scientists, ML engineers, and operations teams to facilitate
    a more efficient and seamless development process, ensuring accurate, scalable,
    and maintainable ML models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: CI/CD管道针对机器学习工作流程进行定制，以自动化模型的测试和部署，并在生产中监控模型性能，以捕捉和解决由于模型或数据漂移而导致的模型退化。此外，MLOps强调数据科学家、机器学习工程师和运维团队之间的协作，以促进更高效和无缝的开发过程，确保机器学习模型准确、可扩展和可维护。
- en: MLOps infrastructure plays a critical role in the security landscape of machine
    learning systems. By integrating security practices throughout the ML lifecycle,
    MLOps can help identify and mitigate risks early in development. This includes
    ensuring data privacy and compliance with regulations such as GDPR, managing access
    to sensitive datasets, and securing model endpoints against adversarial attacks.
    Automated vulnerability scanning and incorporating security checks into CI/CD
    pipelines help catch security issues before deployment. Moreover, monitoring deployed
    models for anomalous behavior can detect potential security breaches, contributing
    to a more robust security posture for ML applications.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps基础设施在机器学习系统的安全格局中扮演着至关重要的角色。通过在整个机器学习生命周期中整合安全实践，MLOps可以帮助在开发早期识别和缓解风险。这包括确保数据隐私和符合GDPR等法规，管理对敏感数据集的访问，以及保护模型端点免受对抗性攻击。自动漏洞扫描和将安全检查纳入CI/CD管道有助于在部署前发现安全问题。此外，监控已部署模型的不寻常行为可以检测潜在的网络安全漏洞，从而为机器学习应用提供更稳健的安全态势。
- en: LLMOps
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMOps
- en: MLOps, while crucial in establishing practices for any application leveraging
    machine learning, doesn’t address all the unique challenges LLMs pose. LLMs introduce
    specific challenges, such as prompt engineering, robust monitoring to capture
    the nuanced performance, and the potential misuse of generated outputs. This means
    we must take advantage of the best that DevSecOps and MLOps can teach us and then
    add more techniques specific to LLMs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps虽然在为任何利用机器学习的应用程序建立实践方面至关重要，但它并没有解决LLMs提出的所有独特挑战。LLMs引入了特定的挑战，例如提示工程、强大的监控来捕捉细微的性能，以及生成输出的潜在滥用。这意味着我们必须利用DevSecOps和MLOps所能教给我们的最好方法，然后添加更多针对LLMs的特定技术。
- en: LLMOps evolved as a specialized discipline to address these challenges. It encompasses
    practices tailored for deploying, monitoring, and maintaining LLMs in production
    environments. LLMOps deals with aspects such as model versioning and management
    at a much larger scale, advanced deployment strategies to handle the high computational
    load, and specific monitoring techniques for evaluating the qualitative aspects
    of model outputs. Furthermore, LLMOps emphasizes the importance of prompt engineering
    and feedback loops to refine model performance and mitigate risks associated with
    model-generated content. This specialized focus ensures that LLM deployments are
    efficient, ethical, and aligned with user expectations and regulatory requirements.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps作为一门解决这些挑战的专业学科而发展起来。它包括针对在生产环境中部署、监控和维护LLMs的特定实践。LLMOps在模型版本化和管理方面处理更大规模的方面，采用高级部署策略来处理高计算负载，以及针对评估模型输出定性方面的特定监控技术。此外，LLMOps强调提示工程和反馈循环的重要性，以优化模型性能并减轻与模型生成内容相关的风险。这种专业化的关注确保了LLM部署的高效性、道德性以及与用户期望和监管要求的对齐。
- en: Now, let’s examine how best to integrate security practices into LLMOps to ensure
    a repeatable process for delivering more secure applications.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探讨如何将安全实践最佳地整合到LLMOps中，以确保交付更安全应用程序的可重复过程。
- en: Building Security into LLMOps
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将安全性融入LLMOps
- en: 'All this discussion about DevSecOps, MLOps, and LLMOps may sound daunting.
    However, the critical tasks required to secure our process for building secure
    LLM apps can be broken down into five simple steps: foundation model selection,
    data preparation, validation, deployment, and monitoring, as shown in [Table 11-1](#table-11-1).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所有关于DevSecOps、MLOps和LLMOps的讨论可能听起来令人畏惧。然而，为了确保我们构建安全LLM应用的过程安全，所需的关键任务可以分解为五个简单的步骤：基础模型选择、数据准备、验证、部署和监控，如[表11-1](#table-11-1)所示。
- en: Table 11-1\. LLMOps steps
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-1\. LLMOps步骤
- en: '| Task | LLMOps security measures |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | LLMOps安全措施 |'
- en: '| --- | --- |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Foundation model selection | Opt for foundation models with robust security
    features. Assess the security history and vulnerability reports of the model’s
    source. Review the model card provided with the foundation model and the security-specific
    information provided. Review what you can about the datasets used to train the
    foundation model. Implement processes to watch for new versions of the foundation
    model, which may add security or alignment improvements. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 基础模型选择 | 选择具有强大安全功能的基座模型。评估模型的来源的安全历史和漏洞报告。审查与基座模型一起提供的模型卡和安全特定信息。审查有关用于训练基座模型的训练数据集的信息。实施监控基座模型新版本的流程，这些新版本可能添加安全或对齐改进。|'
- en: '| Data preparation | If you plan to use fine-tuning or RAG to enhance the domain-specific
    knowledge available to your application, you must prepare your data. Carefully
    evaluate the sources of your datasets. Ensure data is scrubbed, anonymized, and
    free from illegal or inappropriate content. Evaluate your data for possible bias.
    Implement secure data handling and access controls during fine-tuning or embedding
    generation. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 数据准备 | 如果您计划使用微调或RAG来增强应用程序可用的特定领域知识，您必须准备您的数据。仔细评估您的数据集来源。确保数据被清理、匿名化，且不包含非法或不适当的内容。评估您的数据以确定可能的偏见。在微调或嵌入生成期间实施安全数据处理和访问控制。|'
- en: '| Validation | Extend your security testing to include LLM-specific vulnerability
    scanners and AI red teaming exercises. (We’ll talk more about AI red teams later
    in the chapter.) Extend your validation steps to check for nontraditional security
    threats such as toxicity and bias. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 验证 | 将您的安全测试扩展到包括针对LLM的特定漏洞扫描器和AI红队演习。（我们将在本章后面更多地讨论AI红队。）将您的验证步骤扩展到检查非传统安全威胁，如毒性和偏见。|'
- en: '| Deployment | Ensure you have appropriate runtime guardrails to screen prompts
    entering your model and output. Automate your build process to ensure that your
    ML-BOM is regenerated and stored with every set of changes. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 部署 | 确保你有适当的运行时防护措施来筛选进入模型和输出的提示。自动化你的构建过程，以确保每次更改后都重新生成并存储你的机器学习物料清单（ML-BOM）。
    |'
- en: '| Monitoring | Log all activity and monitor for anomalies that could indicate
    jailbreaks, attempts to deny service, or other compromises of your infrastructure.
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 监控 | 记录所有活动并监控异常，这些异常可能表明越狱、拒绝服务尝试或其他对基础设施的破坏。 |'
- en: Security in the LLM Development Process
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM开发过程中的安全
- en: Now it’s time to move past process abstractions and get into the practical steps
    you must adopt to make your secure development process repeatable. We’ll look
    at topics that range across the entire development lifecycle. We’ll start by looking
    at how to make sure your development environment and pipeline are secure. Then
    we’ll look into LLM-specific security testing tools you can use to check your
    security procedures before deployment. We’ll also review the steps you must take
    to ensure the security of your software supply chain.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候超越流程抽象，采取实际步骤使你的安全开发流程可重复。我们将探讨涵盖整个开发生命周期的主题。我们将首先查看如何确保你的开发环境和管道是安全的。然后，我们将探讨在部署前可以使用哪些针对LLM的特定安全测试工具来检查你的安全程序。我们还将回顾你必须采取的步骤，以确保你的软件供应链的安全性。
- en: Securing Your CI/CD
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保护你的CI/CD
- en: The security of the development pipeline is paramount in preventing your project
    from becoming a weak link in the supply chain. In [Chapter 9](ch09.html#find_the_weakest_link),
    we reviewed the SolarWinds case study, which shows how disastrous it can be for
    you and your downstream customers if your pipeline is compromised. This section
    explores strategies to fortify the pipeline against threats, ensuring that your
    LLM application does not get compromised or inadvertently contribute to the security
    vulnerabilities of downstream users.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 开发管道的安全性对于防止你的项目成为供应链中的薄弱环节至关重要。在[第9章](ch09.html#find_the_weakest_link)中，我们回顾了SolarWinds案例研究，它展示了如果你的管道被破坏，对你和你的下游客户来说可能会多么灾难性。本节探讨了加强管道以抵御威胁的策略，确保你的LLM应用不会受到损害或无意中导致下游用户的网络安全漏洞。
- en: Implementing robust security practices
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实施稳健的安全实践
- en: 'Let’s look at some critical practices you’ll need to implement your security
    program:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看你需要实施安全计划的几个关键实践：
- en: CI/CD security
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: CI/CD安全
- en: Integrate security checks into the CI/CD pipeline to automatically detect vulnerabilities
    or misconfigurations early in the development process.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 将安全检查集成到持续集成/持续部署（CI/CD）管道中，以便在开发早期自动检测漏洞或配置错误。
- en: Dependency management
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖关系管理
- en: Regularly audit and update the dependencies used in your project to mitigate
    vulnerabilities associated with outdated or compromised libraries. ML-specific,
    open source build pipeline components, such as PyTorch, have had severe, zero-day
    security issues reported recently, demonstrating the importance of this step.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 定期审计和更新项目中使用的依赖项，以减轻与过时或受损库相关的漏洞。最近报告了PyTorch等特定于ML的开源构建管道组件的严重零日安全漏洞，这证明了这一步骤的重要性。
- en: Access control and monitoring
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 访问控制和监控
- en: Limit access to the CI/CD environment and monitor activity to promptly detect
    and respond to suspicious behavior. Secure your training data repositories, just
    as you would your source code, to help protect against possible data poisoning
    attacks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 限制对CI/CD环境的访问，并监控活动，以便及时检测和响应可疑行为。像保护源代码一样保护你的训练数据存储库，以帮助防止可能的数据中毒攻击。
- en: Fostering a culture of security awareness
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 培养安全意识文化
- en: 'Training your humans can be just as important as training your LLM in building
    a secure app. Here are some things to think about in how your train and prepare
    your people:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 训练你的团队成员与训练你的大型语言模型（LLM）在构建安全应用中同样重要。以下是一些关于如何培训和准备你的团队成员的事项：
- en: Training and awareness
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 培训和意识
- en: Educate members of the development team on the importance of supply chain security
    and their role in maintaining it. Ensure your team understands the new components,
    such as foundation models and training datasets, that must be managed as part
    of your application’s supply chain.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 教育开发团队成员关于供应链安全的重要性以及他们在维护中的作用。确保你的团队了解必须作为应用程序供应链的一部分管理的新的组件，例如基础模型和训练数据集。
- en: Incident response planning
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 事件响应计划
- en: Develop and regularly update an incident response plan that includes procedures
    for addressing supply chain threats, including zero-day vulnerability disclosures.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 制定并定期更新一个事件响应计划，包括应对供应链威胁的程序，包括零日漏洞披露。
- en: LLM-Specific Security Testing Tools
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM特定安全测试工具
- en: Application security testing tools can come in multiple flavors, such as Static
    Application Security Testing (SAST), Dynamic Application Security Testing (DAST),
    and Interactive Application Security Testing (IAST). All have established themselves
    as indispensable instruments in developing traditional web applications. While
    each has its strengths and weaknesses, they all help automate the identification
    of vulnerabilities and security flaws, facilitating early detection and remediation.
    Their integration into the software development lifecycle enables organizations
    to adopt a proactive stance on security, ensuring that applications are functional
    and secure by design.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 应用安全测试工具可以有多种形式，例如静态应用安全测试（SAST）、动态应用安全测试（DAST）和交互式应用安全测试（IAST）。所有这些都已经确立了自己在开发传统Web应用中的不可或缺的地位。虽然每种方法都有其优势和劣势，但它们都帮助自动化漏洞和安全缺陷的识别，促进早期检测和修复。将它们集成到软件开发生命周期中，使组织能够采取主动的安全立场，确保应用在设计时就具备功能和安全性。
- en: LLMs present unique security challenges that are not fully addressed by traditional
    security testing methodologies. Their complexity, novelty, and susceptibility
    to issues like data bias, hallucination, and adversarial attacks necessitate specialized
    tools tailored to their distinct context. Although the field is relatively nascent,
    new tools aimed at fortifying LLM applications against a spectrum of vulnerabilities
    are beginning to emerge. Let’s look at several examples.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs带来了独特的安全挑战，这些挑战尚未被传统的安全测试方法完全解决。它们的复杂性、新颖性和易受数据偏差、幻觉和对抗性攻击等问题的影响，需要针对其特定环境定制化的工具。尽管该领域相对较新，但旨在加强LLM应用以抵御一系列漏洞的新工具正在开始出现。让我们看看几个例子。
- en: TextAttack
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TextAttack
- en: TextAttack has been around in some form since at least 2020\. It is a sophisticated
    Python framework designed for adversarial testing of NLP models, including LLMs.
    Free and open source, distributed under the MIT license, it facilitates the exploration
    of vulnerabilities in language models and the development of robust defenses against
    adversarial attacks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: TextAttack自2020年以来以某种形式存在。它是一个复杂的Python框架，旨在为NLP模型（包括LLMs）进行对抗性测试。它是免费且开源的，遵循MIT许可证发布，便于探索语言模型中的漏洞并开发针对对抗性攻击的强大防御措施。
- en: TextAttack stands out by offering a modular architecture that allows for the
    customization and testing of attack strategies across various models and datasets.
    It simulates adversarial examples to reveal potential weaknesses in NLP applications,
    thereby guiding improvements in model resilience. The tool provides detailed reports
    on attack methodologies, success rates, and model responses, making it invaluable
    for security assessments. Its adaptability and comprehensive coverage of attack
    techniques make TextAttack a powerful tool for developers and researchers aiming
    to enhance the security and reliability of LLM applications.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: TextAttack脱颖而出，提供了一种模块化架构，允许对各种模型和数据集的攻击策略进行定制和测试。它模拟对抗性示例以揭示NLP应用中的潜在弱点，从而指导模型弹性的改进。该工具提供了关于攻击方法、成功率以及模型响应的详细报告，对于安全评估来说价值连城。其适应性和对攻击技术的全面覆盖使TextAttack成为开发者和研究人员增强LLM应用安全和可靠性的强大工具。
- en: Garak
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Garak
- en: Garak, named after an obscure *Star Trek* character, is an LLM vulnerability
    scanner. Garak was developed by Leon Derczynski, who was a significant contributor
    to developing the first versions of the OWASP Top 10 for LLM Applications. Garak
    is free to use and distributed under a liberal Apache open source license.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个鲜为人知的*星际迷航*角色命名的Garak是一个LLM漏洞扫描器。Garak由Leon Derczynski开发，他是LLM应用OWASP Top
    10第一版的重要贡献者。Garak免费使用，并遵循宽松的Apache开源许可证发布。
- en: Garak adopts a model similar to that of DAST tools, where it probes the application
    at runtime and examines its behavior, looking for vulnerabilities. The tool sends
    various prompts to models, analyzing multiple outputs using detectors to identify
    unwanted content. The results aren’t scientifically validated, but a higher passing
    percentage indicates better performance. It can be customized with plug-ins for
    additional prompts or vulnerabilities. It generates detailed reports that include
    all test parameters, prompts, responses, and scores. There’s potential for expansion
    to different models and vulnerabilities based on user contributions and requests.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Garak采用与DAST工具类似的模式，它在运行时探测应用程序并检查其行为，寻找漏洞。该工具向模型发送各种提示，使用检测器分析多个输出以识别不受欢迎的内容。结果未经科学验证，但更高的通过百分比表明更好的性能。它可以通过插件进行定制，以添加额外的提示或漏洞。它生成包含所有测试参数、提示、响应和分数的详细报告。根据用户贡献和请求，有潜力扩展到不同的模型和漏洞。
- en: Responsible AI Toolbox
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负责任AI工具箱
- en: The [Responsible AI Toolbox](https://oreil.ly/6hpZE), developed by Microsoft,
    is an open source tool suite that enables developers and data scientists to infuse
    ethical principles, fairness, and transparency into their AI systems. This toolbox
    is distributed under the MIT license and offers an integrated environment to assess,
    improve, and monitor models on various dimensions of responsible AI, including
    fairness, interpretability, and privacy.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由微软开发的[负责任AI工具箱](https://oreil.ly/6hpZE)是一个开源工具套件，它使开发人员和数据科学家能够将道德原则、公平性和透明度融入他们的AI系统中。此工具箱遵循MIT许可证分发，并提供一个综合环境来评估、改进和监控负责任AI的各个维度的模型，包括公平性、可解释性和隐私。
- en: Giskard LLM Scan
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Giskard LLM扫描
- en: Giskard LLM Scan is an open source tool used to assess an LLM’s ethical considerations
    and safety. Available under the Apache 2.0 license, this component of the Giskard
    AI suite aims to identify biases, detect instances of toxic content, and promote
    the responsible deployment of LLMs. It employs a variety of metrics and tests
    designed to evaluate LLM behavior in terms of fairness, toxicity, and inclusiveness.
    Through its interface, Giskard LLM Scan offers detailed reports highlighting areas
    of concern, assisting developers and researchers in understanding and potentially
    mitigating ethical risks in their AI models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Giskard LLM扫描是一个开源工具，用于评估LLM的道德考虑和安全。在Apache 2.0许可证下可用，Giskard AI套件的这一部分旨在识别偏见、检测有害内容的实例，并促进LLM的负责任部署。它采用各种指标和测试，旨在评估LLM在公平性、毒性和包容性方面的行为。通过其界面，Giskard
    LLM扫描提供详细报告，突出关注领域，帮助开发人员和研究人员理解和可能减轻其AI模型中的道德风险。
- en: Integrating security tools into DevOps
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将安全工具集成到DevOps中
- en: Integrating automated, LLM-specific security testing tools and traditional AST
    (application security testing) tools into LLMOps processes is not merely beneficial
    but imperative. Embedding these tools within CI/CD pipelines ensures that security
    is not an afterthought but a foundational aspect of application development. This
    approach enables automated, repeatable security checks performed with every build,
    significantly reducing the risk of vulnerabilities in production. Moreover, it
    fosters a culture of security mindfulness among development teams, ensuring that
    security considerations are paramount from the inception of a project through
    to its deployment.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 将自动化的、针对LLM特定的安全测试工具和传统的AST（应用程序安全测试）工具集成到LLMOps流程中不仅有益，而且是强制性的。将这些工具嵌入到CI/CD管道中确保安全不是事后考虑，而是应用开发的基础部分。这种方法使得每次构建时都能进行自动化的、可重复的安全检查，显著降低了生产中的漏洞风险。此外，它还培养了开发团队的安全意识文化，确保从项目的开始到部署，安全考虑始终是首要的。
- en: Managing Your Supply Chain
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理您的供应链
- en: As discussed in [Chapter 9](ch09.html#find_the_weakest_link), the supply chain
    represents more than sourcing components and tools. It involves the meticulous
    generation, storage, and accessibility of development artifacts such as model
    cards and ML-BOMs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第9章](ch09.html#find_the_weakest_link)所述，供应链不仅代表组件和工具的采购，还涉及开发工件（如模型卡和ML-BOM）的细致生成、存储和可访问性。
- en: Model cards are essential documentation for LLMs, providing an overview of a
    model’s purpose, performance, and potential biases. Similarly, ML-BOMs detail
    the components, datasets, and dependencies involved in developing an application
    using machine learning technologies like an LLM. Together, these artifacts form
    a cornerstone of transparency and accountability in LLM development.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 模型卡片是LLMs的必要文档，概述了模型的目的、性能和潜在偏差。同样，ML-BOMs详细说明了使用LLM等机器学习技术开发应用程序所涉及的组件、数据集和依赖项。这些工件共同构成了LLMs开发中透明度和问责制的基石。
- en: To manage them effectively, developers must implement systems for generating,
    storing, and making these artifacts easily searchable. This facilitates regulatory
    compliance and enhances stakeholder collaboration and trust. By integrating these
    practices into a broader SBOM strategy, teams can ensure a holistic view of both
    AI and non-AI components of their applications, reinforcing the security and integrity
    of the supply chain.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地管理它们，开发者必须实施生成、存储和使这些工件易于搜索的系统。这有助于合规性监管并增强利益相关者的协作和信任。通过将这些实践整合到更广泛的SBOM策略中，团队可以确保对其应用程序的AI和非AI组件有一个全面的视角，从而加强供应链的安全性和完整性。
- en: 'You’ll need to focus on three pillars to ensure your artifacts are properly
    tracked, thus helping to ensure you’re in control of your supply chain:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要关注三个支柱，以确保您的工件得到适当跟踪，从而帮助确保您对供应链有控制权：
- en: Automated generation
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成
- en: Implement tools and workflows that automatically generate model cards and ML-BOMs
    at key development milestones.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在关键开发里程碑处实现自动生成模型卡片和ML-BOM的工具和工作流程。
- en: Secure storage
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 安全存储
- en: Store these artifacts in secure, version-controlled repositories to ensure they
    are tamper-proof and retrievable.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些工件存储在安全、版本控制的存储库中，以确保它们防篡改且可检索。
- en: Accessibility
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 可访问性
- en: Make these artifacts accessible to relevant stakeholders, incorporating search
    functionalities to facilitate quick retrieval and review.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使这些工件对相关利益相关者可访问，并整合搜索功能以促进快速检索和审查。
- en: The supply chain in LLM application development is a complex ecosystem that
    requires diligent management to ensure the security and integrity of both development
    artifacts and the development pipeline. By prioritizing the generation and storage
    of key artifacts like model cards and ML-BOMs and by securing the development
    pipeline, organizations can safeguard against supply chain vulnerabilities, fostering
    trust and reliability in their LLM applications.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs应用开发中，供应链是一个复杂的生态系统，需要勤勉的管理以确保开发工件和开发管道的安全性和完整性。通过优先考虑关键工件如模型卡片和ML-BOMs的生成和存储，并通过保护开发管道，组织可以防范供应链漏洞，增强其LLM应用程序的信任和可靠性。
- en: Protect Your App with Guardrails
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用护栏保护您的应用程序
- en: Tools such as web application firewalls (WAFs) and runtime application self-protection
    (RASP) have become fundamental in defending web applications against attacks during
    runtime. Unlike AST tools that analyze code for vulnerabilities at build and test
    time, WAFs and RASP provide continuous protection while an application operates
    in production. They act as vigilant guardians, identifying and mitigating threats
    in real time, thus adding a critical layer of security.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如同网络应用防火墙（WAFs）和运行时应用自我保护（RASP）等工具已成为在运行时防御Web应用程序攻击的基本工具。与在构建和测试时分析代码以查找漏洞的AST工具不同，WAFs和RASP在应用程序运行时提供持续的保护。它们作为警觉的守护者，实时识别和缓解威胁，从而增加了一个关键的安全层。
- en: In the context of LLMs, a parallel can be drawn with the concept of *guardrails*.
    Guardrails help ensure that LLMs operate within defined ethical, legal, and safety
    parameters, preventing misuse and guiding the models toward generating appropriate
    and safe outputs. Initially, guardrail implementations were relatively simplistic,
    often built in house and tailored to specific use cases. In [Chapter 7](ch07.html#trust_no_one),
    we walked through the construction of some simple guardrails to help screen output
    from the LLM for toxicity and PII. This exercise was a great way to understand
    the basics of how some guardrails work.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs的背景下，可以将“护栏”概念与之类比。护栏有助于确保LLMs在定义的道德、法律和安全参数内运行，防止滥用并引导模型生成适当和安全的输出。最初，护栏的实现相对简单，通常是在内部构建并针对特定用例定制。在[第7章](ch07.html#trust_no_one)中，我们探讨了构建一些简单护栏的过程，以帮助筛选LLM的输出，防止毒性和PII。这项练习是理解一些护栏如何工作的基础知识的好方法。
- en: However, the demand for more sophisticated security and safety frameworks has
    increased as LLM-based applications have grown more complex. Today, there is a
    burgeoning ecosystem of tools, both open source and commercial, offering more
    comprehensive guardrail frameworks for LLMs. These tools serve as runtime security
    measures, continuously monitoring and guiding the behavior of LLMs to prevent
    the generation of harmful, biased, or otherwise undesirable content. They are
    akin to WAFs and RASP in the web application space, providing a dynamic shield
    that adapts to emerging threats and challenges.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着基于LLM的应用变得更加复杂，对更高级的安全和安全性框架的需求也在增加。如今，一个蓬勃发展的生态系统正在形成，其中包括开源和商业工具，为LLM提供更全面的护栏框架。这些工具作为运行时安全措施，持续监控和引导LLM的行为，以防止生成有害、有偏见或其他不希望的内容。它们类似于网络应用空间中的WAF和RASP，提供一种动态的防护盾，能够适应新兴的威胁和挑战。
- en: The Role of Guardrails in an LLM Security Strategy
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在LLM安全策略中，护栏的作用
- en: Incorporating advanced guardrail solutions into LLM deployments is not just
    a recommendation; it’s becoming a necessity. As these models become more deeply
    integrated into critical and consumer-facing applications, the potential impact
    of their misuse or malfunction grows exponentially. Guardrails offer a way to
    mitigate these risks. Guardrails frameworks offer a range of functionality, but
    here are some typical functions you’ll want to look for as you evaluate your options.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将高级护栏解决方案纳入LLM部署不仅是一个建议，而正在成为一种必要性。随着这些模型越来越深入地集成到关键和面向消费者的应用中，它们误用或故障的潜在影响呈指数增长。护栏提供了一种减轻这些风险的方法。护栏框架提供了一系列功能，但以下是一些在评估选项时您希望寻找的典型功能。
- en: Input validation
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入验证
- en: 'There are several benefits of implementing guardrails that scan the input into
    your LLM:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 实施扫描输入到您的LLM的护栏有一些好处：
- en: Prompt injection prevention
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 预防提示注入
- en: Monitor for signs of prompt injection, such as unusual phrases, hidden characters,
    and odd encodings, to prevent malicious manipulation of the LLM.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 监控即时注入的迹象，例如不寻常的短语、隐藏的字符和奇特的编码，以防止恶意操纵LLM。
- en: Domain limitation
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 领域限制
- en: Keep the LLM focused on relevant topics by restricting or ignoring irrelevant
    prompts. This enhances security by reducing the risk of generating inappropriate
    or irrelevant content and diminishing the likelihood of hallucinations.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过限制或忽略不相关的提示，使LLM专注于相关主题。这通过减少生成不适当或不相关内容的风险和降低幻觉的可能性来增强安全性。
- en: Anonymization and secret detection
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 匿名化和秘密检测
- en: While interacting with the LLM, users may input confidential data, like email
    addresses, telephone numbers, or API keys. This poses a problem if the data is
    logged, stored, or transferred to a third-party LLM provider or if the data could
    potentially be used for training purposes. It’s crucial to anonymize PII and redact
    sensitive data before the LLM processes it.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在与LLM交互时，用户可能会输入机密数据，如电子邮件地址、电话号码或API密钥。如果数据被记录、存储或传输到第三方LLM提供商，或者如果数据可能被用于训练目的，这就会成为一个问题。在LLM处理之前匿名化PII和删除敏感数据至关重要。
- en: Output validation
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出验证
- en: 'Screening all output from your LLM is a critical part of your zero trust strategy.
    Here are some of the benefits:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对您LLM的所有输出进行筛选是您零信任策略的关键部分。以下是一些好处：
- en: Ethical screening
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 道德审查
- en: Filter outputs for content that could be considered toxic, inappropriate, or
    hateful to ensure the LLM’s interactions align with ethical guidelines. This could
    have saved poor Tay from [Chapter 1](ch01.html#chatbots_breaking_bad) and countless
    other projects from falling victim to vulnerabilities such as unchecked toxicity.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤输出内容，以避免被认为是具有毒性、不适当或仇恨的内容，确保LLM的交互符合道德准则。这本来可以拯救可怜的Tay从[第一章](ch01.html#chatbots_breaking_bad)以及其他无数项目免受未检查的毒性等漏洞的侵害。
- en: Sensitive information protection
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感信息保护
- en: Implement measures to prevent the disclosure of PII or other sensitive data
    through the LLM’s outputs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 实施措施以防止通过LLM的输出泄露PII或其他敏感数据。
- en: Code output
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出
- en: Look for unintended code generation that could lead to downstream attacks such
    as SQL injection, server-side request forgery (SSRF), and XSS.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找可能导致下游攻击的意外代码生成，例如SQL注入、服务器端请求伪造（SSRF）和XSS。
- en: Compliance assurance
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 合规性保证
- en: In sectors with strict regulatory standards, like health care or legal, tailor
    outputs to meet specific compliance requirements and keep the LLM’s responses
    within the scope of its intended use.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有严格监管标准的行业，如医疗保健或法律，调整输出以满足特定的合规要求，并确保 LLM 的响应在其预期用途范围内。
- en: Fact-checking and hallucination detection
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 事实核查和幻觉检测
- en: Verify the accuracy of LLM outputs against trusted sources to ensure the information
    provided is factual and reliable. Identify and mitigate instances where the LLM
    generates fictitious or irrelevant content to ensure outputs remain relevant and
    grounded in reality.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 将 LLM 输出的准确性与可信来源进行验证，以确保提供的信息是事实性和可靠的。识别并减轻 LLM 生成虚构或不相关内容的情况，以确保输出保持相关性并扎根于现实。
- en: Open Source Versus Commercial Guardrail Solutions
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源与商业护栏解决方案的比较
- en: The choice between open source and commercial guardrail solutions depends on
    several factors, including the organization’s specific needs, the level of customization
    required, and budget considerations.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在开源和商业护栏解决方案之间进行选择取决于多个因素，包括组织的具体需求、所需的定制程度以及预算考虑。
- en: Open source tools offer the benefits of flexibility and community support, allowing
    organizations to tailor solutions to their unique requirements. However, they
    may require significant internal expertise and resources to deploy and maintain
    effectively. Some examples of open source guardrails tools you may wish to evaluate
    include NVIDIA NeMo-Guardrails, Meta Llama Guard, Guardrails AI, and Protect AI.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 开源工具提供了灵活性和社区支持的好处，允许组织根据其独特需求定制解决方案。然而，它们可能需要大量的内部专业知识和资源才能有效部署和维护。您可能希望评估的一些开源护栏工具包括
    NVIDIA NeMo-Guardrails、Meta Llama Guard、Guardrails AI 和 Protect AI。
- en: On the other hand, commercial solutions may provide more out-of-the-box functionality
    with the added benefits of professional support, regular updates, and advanced
    features. Some examples of commercial guardrail options include Prompt Security,
    Lakera Guard, WhyLabs LangKit, Lasso Security, PromptArmor, and Cloudflare Firewall
    for AI.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，商业解决方案可能提供更多开箱即用的功能，并附带专业支持、定期更新和高级功能的好处。一些商业护栏选项的例子包括 Prompt Security、Lakera
    Guard、WhyLabs LangKit、Lasso Security、PromptArmor 和 Cloudflare Firewall for AI。
- en: Mixing Custom and Packaged Guardrails
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合自定义和包装的护栏
- en: In [Chapter 7](ch07.html#trust_no_one), we implemented some basic guardrails
    by hand. While the emergence of prebuilt guardrail frameworks can offer a significant
    boost in security, these handcrafted guardrails still have a role. Supplementing
    a guardrail framework with your own custom, domain-, or application-specific guardrails
    can make a lot of sense. These types of defense-in-depth strategies are often
    the most successful in cybersecurity.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 7 章](ch07.html#trust_no_one)中，我们手动实现了一些基本的护栏。虽然预构建的护栏框架的出现可以在安全性方面提供显著的提升，但这些手工制作的护栏仍然有其作用。用您自己的定制、领域或应用程序特定的护栏补充护栏框架是有意义的。这类深度防御策略在网络安全中通常是最成功的。
- en: Monitoring Your App
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控您的应用程序
- en: In the lifecycle of LLM applications, effective monitoring encompasses not only
    the conventional components—such as web servers, middleware, application code,
    and databases—but also the unique elements intrinsic to LLMs, including the model
    itself and associated vector databases used for RAG. This comprehensive approach
    is pivotal for maintaining operational integrity and security throughout the application’s
    lifecycle.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLM 应用的生命周期中，有效的监控不仅包括传统的组件——如网络服务器、中间件、应用程序代码和数据库——还包括 LLM 本身及其用于 RAG 的相关向量数据库的独特元素。这种全面的方法对于在整个应用生命周期中保持操作完整性和安全性至关重要。
- en: Logging Every Prompt and Response
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记录每个提示和响应
- en: 'One of the foundational practices in monitoring LLM applications is to log
    every prompt and response. This detailed logging serves multiple purposes: it
    provides insights into how users interact with the application, enables the identification
    of potential misuse or problematic outputs, and forms a baseline for understanding
    the model’s performance over time. Such granular data collection is critical for
    diagnosing issues, optimizing model behavior, and ensuring compliance with data
    governance standards.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 监控 LLM 应用程序的基础实践之一是记录每个提示和响应。这种详细的记录具有多重目的：它提供了用户如何与应用程序互动的见解，使识别潜在的滥用或问题输出成为可能，并形成了解模型性能随时间变化的基线。这种细粒度的数据收集对于诊断问题、优化模型行为和确保符合数据治理标准至关重要。
- en: Centralized Log and Event Management
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集中式日志和事件管理
- en: Aggregating logs and application events into a *security information and event
    management* (SIEM) system is essential. A SIEM system enables data consolidation
    across the entire application stack, offering a unified view of all activities.
    This allows your organization to easily store a historical record of how your
    application has responded to every user input. These centralized logs can then
    be stored for compliance purposes. Also, SIEM systems offer advanced search tools
    that enable your team to quickly search for patterns across a huge range of prompts
    and responses. This can enable your security operations team to hunt for threats
    while your application is in production.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 将日志和应用程序事件聚合到*安全信息和事件管理*（SIEM）系统中是至关重要的。SIEM系统使整个应用程序堆栈的数据整合成为可能，提供了一个对所有活动的统一视图。这使得你的组织能够轻松存储应用程序对每个用户输入的响应的历史记录。这些集中式日志可以随后用于合规目的存储。此外，SIEM系统提供高级搜索工具，使你的团队能够快速搜索大量提示和响应中的模式。这可以使你的安全运营团队能够在生产中寻找威胁。
- en: User and Entity Behavior Analytics
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户和实体行为分析
- en: To enhance monitoring capabilities further, incorporating *user and entity behavior
    analytics* (UEBA) technology can be layered on top of SIEM. UEBA extends traditional
    monitoring by leveraging machine learning and analytics to understand how users
    and entities typically interact with the application, thereby enabling the detection
    of activities that deviate from the norm. For LLM applications, extending UEBA
    frameworks to encompass model-specific behaviors—such as unusual prompt-response
    patterns or atypical access to the vector database—can provide early warning signs
    of security breaches, data leaks, or the need for model retraining. In addition,
    dramatic changes in usage patterns could help you identify denial-of-service,
    denial-of-wallet, and model cloning attacks, as discussed in [Chapter 8](ch08.html#don_t_lose_your_wallet).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步增强监控能力，可以在SIEM（安全信息和事件管理）系统之上叠加*用户和实体行为分析*（UEBA）技术。UEBA通过利用机器学习和分析来理解用户和实体通常如何与应用程序交互，从而能够检测出偏离常规的活动。对于LLM应用，将UEBA框架扩展到包括模型特定的行为——例如不寻常的提示-响应模式或对向量数据库的非典型访问——可以提供关于安全漏洞、数据泄露或需要模型重新训练的早期预警信号。此外，使用模式的重大变化可以帮助你识别拒绝服务、拒绝钱包和模型克隆攻击，如第8章[第8章](ch08.html#don_t_lose_your_wallet)中所述。
- en: Build Your AI Red Team
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建你的AI红队
- en: So far in this chapter, we’ve looked at how to secure your development pipeline,
    use security testing tools in a repeatable way, and guard and then monitor your
    application in production. These are all critical steps, but they’ve repeatedly
    been shown to be necessary but insufficient in understanding your application’s
    actions in the real world. The emerging field of *AI red teaming* is designed
    to do just this. Let’s look at how an AI red team can become an important part
    of validating the security of your application.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们探讨了如何确保你的开发管道安全，以可重复的方式使用安全测试工具，并在生产中保护和监控你的应用程序。这些都是关键步骤，但它们反复被证明是理解你的应用程序在现实世界中的行为的必要但不充分的步骤。新兴的*AI红队*领域正是为此而设计的。让我们看看AI红队如何成为验证你的应用程序安全性的重要部分。
- en: An AI red team is a group of security professionals who adopt an adversarial
    approach to rigorously challenge the safety and security of applications using
    AI technology, such as an LLM. Their objective is to identify and exploit weaknesses
    in AI systems, much like an external attacker might, but to improve security rather
    than cause harm.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: AI红队是一组采用对抗性方法，使用AI技术（如LLM）严格挑战应用程序安全性和安全性的安全专业人士。他们的目标是识别和利用AI系统中的弱点，就像外部攻击者可能会做的那样，但目的是提高安全性而不是造成损害。
- en: Note
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'AI red teams catapulted to the forefront of the AI and LLM security discussion
    when US President Biden issued his October 2023 [“Executive Order on the Safe,
    Secure, and Trustworthy Development and Use of Artificial Intelligence,”](https://oreil.ly/yGHW8)
    which contains the following language:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当美国总统拜登于2023年10月发布其“关于安全、可靠和值得信赖的人工智能开发和使用行政命令”时，AI红队被推到了AI和LLM安全讨论的前沿，该命令包含以下内容：
- en: '*The term “AI red-teaming” means a structured testing effort to find flaws
    and vulnerabilities in an AI system, often in a controlled environment and in
    collaboration with developers of AI. Artificial Intelligence red-teaming is most
    often performed by dedicated “red teams” that adopt adversarial methods to identify
    flaws and vulnerabilities, such as harmful or discriminatory outputs from an AI
    system, unforeseen or undesirable system behaviors, limitations, or potential
    risks associated with the misuse of the system.*'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*“人工智能红队”一词意味着一种结构化测试工作，旨在发现人工智能系统中的缺陷和漏洞，通常在受控环境中进行，并与人工智能的开发者合作。人工智能红队通常由采用对抗性方法的专用“红队”执行，以识别缺陷和漏洞，例如人工智能系统的有害或歧视性输出、不可预见或不受欢迎的系统行为、局限性或与系统误用相关的潜在风险.*'
- en: As a result of this order, the US Artificial Intelligence Safety Institute,
    part of the National Institute of Standards and Technology (NIST), has created
    a dedicated working group on red teaming best practices.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，美国国家标准与技术研究院（NIST）的人工智能安全研究所，作为国家研究院的一部分，已经创建了一个专门的红队最佳实践工作组。
- en: An AI red team operates under the premise that AI systems have unique vulnerabilities
    that traditional software may not possess, such as adversarial input attacks,
    data poisoning, and model stealing attacks. The AI red team helps organizations
    anticipate and mitigate security breaches by simulating real-world AI-specific
    threats.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能红队基于人工智能系统具有传统软件可能不具备的独特漏洞的前提，例如对抗性输入攻击、数据中毒和模型窃取攻击。人工智能红队通过模拟现实世界的特定人工智能威胁，帮助组织预测和缓解安全漏洞。
- en: 'The critical functions of an AI red team include:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能红队的关键功能包括：
- en: Adversarial attack simulation
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性攻击模拟
- en: Crafting and executing attacks that exploit weaknesses in AI systems, such as
    feeding deceptive input to manipulate outcomes or extract sensitive data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 设计和执行利用人工智能系统弱点的攻击，例如输入欺骗性输入以操纵结果或提取敏感数据。
- en: Vulnerability assessment
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 漏洞评估
- en: Systematically reviewing AI systems to identify vulnerabilities that could be
    exploited by attackers, including those in the underlying infrastructure, training
    data, and model outputs.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 系统性地审查人工智能系统，以识别可能被攻击者利用的漏洞，包括底层基础设施、训练数据和模型输出中的漏洞。
- en: Risk analysis
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 风险分析
- en: Evaluating the potential impact of identified vulnerabilities and providing
    a risk-based assessment to prioritize remediation efforts.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 评估已识别漏洞的潜在影响，并提供基于风险的评估，以优先处理修复工作。
- en: Mitigation strategy development
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解策略开发
- en: Recommending defenses and countermeasures to protect AI systems against identified
    threats and vulnerabilities.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 建议防御和反制措施，以保护人工智能系统免受已识别的威胁和漏洞。
- en: Awareness and training
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 意识和培训
- en: Educating developers, security teams, and stakeholders about AI security threats
    and best practices to foster a culture of security-minded AI development.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 教育开发者、安全团队和利益相关者关于人工智能安全威胁和最佳实践，以培养安全意识的人工智能开发文化。
- en: An AI red team is essential to a robust AI security framework. It ensures that
    AI systems are designed and developed securely, continuously tested, and fortified
    against evolving threats in the wild.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能红队对于强大的人工智能安全框架至关重要。它确保人工智能系统被安全地设计和开发，持续进行测试，并针对野外不断发展的威胁进行加固。
- en: Advantages of AI Red Teaming
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能红队的优势
- en: Traditional security measures, while necessary, are often insufficient to address
    complex LLM-specific vulnerabilities. A red team, with its holistic and adversarial
    approach, becomes crucial in identifying and mitigating these threats, not just
    through technical means but by examining the broader implications of human and
    organizational behaviors.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管传统安全措施是必要的，但它们通常不足以解决复杂的长短期记忆（LLM）特定漏洞。一个采用全面和对抗性方法的红队对于识别和缓解这些威胁至关重要，这不仅通过技术手段，还通过审视人类和组织行为的更广泛影响。
- en: Hallucinations, for example, represent a significant risk. A red team, by simulating
    advanced testing scenarios, can identify potential triggers for such behavior,
    enabling developers to understand and mitigate these risks in ways automated testing
    cannot.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，幻觉代表了一个重大的风险。通过模拟高级测试场景，红队可以识别出可能导致此类行为的潜在触发因素，使开发者能够理解和缓解这些风险，这是自动化测试无法做到的。
- en: Data bias poses a more subtle yet profound threat, as it can lead to unfair
    or unethical outcomes. Red teams can assess the technical aspects of bias and
    systemic issues within data collection and processing practices. The team’s external
    perspective can uncover blind spots in data handling and algorithm training that
    might be overlooked by internal teams focused on functionality.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 数据偏差构成了一种更微妙但更深刻的威胁，因为它可能导致不公平或不道德的结果。红队可以评估数据收集和处理实践中的技术偏差和系统性问题。团队的外部视角可以揭示数据处理和算法训练中的盲点，这些盲点可能被专注于功能性的内部团队忽视。
- en: Excessive agency in LLMs, where the model can act beyond its intended scope,
    requires continuous and creative testing to identify. Red teams can probe the
    limits of LLM behavior to ensure that safeguards against unintended autonomous
    actions are robust and effective.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型（LLM）中，模型可以超出其预期范围进行操作，这需要持续和创造性的测试来识别。红队可以探测LLM行为的极限，以确保针对意外自主行为的保护措施是强大和有效的。
- en: Prompt injection attacks exploit how LLMs process input to produce unintended
    outcomes, highlighting the need for a red team’s innovative thinking. The team
    can simulate sophisticated attack vectors that challenge the LLM’s ability to
    handle adversarial inputs safely.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 提示注入攻击利用LLM处理输入以产生意外结果，突出了红队创新思维的需求。团队可以模拟复杂的攻击向量，挑战LLM安全处理对抗性输入的能力。
- en: Moreover, risks like overreliance on LLMs involve technical, human, and organizational
    factors. Red teams can evaluate the broader impact of LLM integration into decision-making
    processes, highlighting areas where reliance on automation might undermine critical
    thinking or operational security.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，过度依赖LLM等风险涉及技术、人类和组织因素。红队可以评估LLM集成到决策过程中的更广泛影响，突出自动化可能损害批判性思维或操作安全性的领域。
- en: The necessity of a red team in LLM application security is not merely a matter
    of adding another layer of defense; it’s about adopting a comprehensive and proactive
    approach to security that addresses the full spectrum of risks—from the technical
    to the human. This approach ensures that LLM applications are resilient against
    current threats and prepared to evolve in the face of emerging vulnerabilities.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 红队在LLM应用安全中的必要性不仅仅是增加另一层防御；它关乎采用一种全面和主动的安全方法，该方法解决从技术到人的全面风险。这种方法确保LLM应用能够抵御当前威胁，并准备好应对新兴漏洞。
- en: Red Teams Versus Pen Tests
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 红队与渗透测试
- en: Red teams and traditional penetration tests are often discussed in the same
    breath, yet they occupy distinct roles in an organization’s security posture.
    As we tease apart the differences between these two approaches, we must recognize
    that they are not mutually exclusive but complementary in fortifying defenses
    against cyber threats. *Penetration testing* is a point-in-time assessment identifying
    exploitable vulnerabilities. In contrast, red teaming is an ongoing, dynamic process
    that simulates real-world attacks across the entire digital and physical spectrum
    of an organization’s defenses.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 红队和传统的渗透测试经常被放在一起讨论，但它们在组织的网络安全态势中扮演着不同的角色。当我们分析这两种方法之间的差异时，我们必须认识到它们不是相互排斥的，而是在加强防御网络威胁方面是互补的。*渗透测试*是一种在特定时间点进行的评估，用于识别可利用的漏洞。相比之下，红队是一种持续、动态的过程，它模拟了组织整个数字和物理防御范围内的现实世界攻击。
- en: Red teaming is particularly crucial when safeguarding the integrity of LLM applications,
    where the attack surface is vast and qualitatively different from traditional
    applications. A red team, operating with a mindset aligned with that of a potential
    adversary, engages in a broader and more fluid form of security testing. This
    encompasses technical vulnerabilities and the organizational, behavioral, and
    psychological aspects of security. In this way, red teaming can also include checking
    for responsible and ethical outcomes, which is extremely difficult for fully automated
    testing.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在保护LLM应用完整性的情况下，红队尤其重要，因为攻击面广泛，与传统的应用在质量上截然不同。红队以与潜在对手心态一致的方式，进行更广泛和更灵活的安全测试。这包括技术漏洞以及安全性的组织、行为和心理方面。通过这种方式，红队还可以包括检查负责任和道德的结果，这对于完全自动化的测试来说极为困难。
- en: '[Table 11-2](#table-11-2) summarizes the differences between a pen test and
    the red team.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11-2](#table-11-2) 总结了渗透测试和红队之间的差异。'
- en: Table 11-2\. Pen test versus red team
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-2\. 渗透测试与红队对比
- en: '| Aspect | Pen test | Red team |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 方面 | 渗透测试 | 红队 |'
- en: '| --- | --- | --- |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Objective | Identify and exploit specific vulnerabilities | Emulate realistic
    cyberattacks to test response capabilities |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 识别和利用特定的漏洞 | 模拟真实的网络攻击以测试响应能力 |'
- en: '| Scope | Focused on specific systems, networks, or applications | Broad, includes
    a variety of attack vectors like social engineering, physical security, and network
    security |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 范围 | 专注于特定的系统、网络或应用程序 | 广泛的，包括各种攻击向量，如社会工程学、物理安全和网络安全 |'
- en: '| Duration | Short-term, typically a few days to a few weeks | Long-term, can
    span several weeks to months to simulate persistent threats |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 持续时间 | 短期，通常为几天到几周 | 长期，可能持续几周到几个月以模拟持续威胁 |'
- en: '| Frequency | Regular intervals, or as part of compliance assessments | Frequent
    or continuous |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 频率 | 定期间隔，或作为合规性评估的一部分 | 频繁或持续 |'
- en: '| Approach | Tactical, seeking to uncover specific technical vulnerabilities
    | Strategic, aiming to reveal systemic weaknesses and organizational response
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 战术性的，旨在发现特定的技术漏洞 | 战略性的，旨在揭示系统性的弱点和组织响应 |'
- en: '| Reporting | Detailed list of vulnerabilities with remediation steps | Comprehensive
    assessment of security posture and recommendations for holistic improvement |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 报告 | 详细列出漏洞及其修复步骤 | 对安全态势的全面评估以及整体改进的建议 |'
- en: Tools and Approaches
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具和方法
- en: While you can build a red team entirely on your own, there are emerging tools
    and services that can help. This space will evolve quickly, but we’ll review a
    couple of emerging options so that you’ll know what to look for.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以完全独立地组建一个红队，但也有一些新兴的工具和服务可以帮助你。这个领域将迅速发展，但我们将回顾一些新兴的选项，以便你知道该寻找什么。
- en: Red team automation tooling
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 红队自动化工具
- en: Introduced in February 2024, PyRIT (Python Risk Identification Toolkit for generative
    AI) is Microsoft’s open source initiative to augment the capabilities of AI red
    teams. PyRIT, which evolved from earlier internal tools developed by Microsoft,
    is designed to support identifying and analyzing vulnerabilities within generative
    AI systems. The toolkit serves as an augmentation tool for human red teamers,
    not as a replacement, emphasizing the toolkit’s role in enhancing human-led security
    efforts.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: PyRIT（Python生成式AI风险识别工具包）于2024年2月推出，是微软开源计划的一部分，旨在增强AI红队的功能。PyRIT是从微软早期内部开发的一些工具演变而来的，旨在支持识别和分析生成式AI系统中的漏洞。该工具包作为人类红队人员的辅助工具，而不是替代品，强调工具包在增强以人为核心的安全努力中的作用。
- en: PyRIT automates aspects of the red teaming process, allowing security professionals
    to efficiently uncover potential weaknesses that could be exploited in generative
    AI systems. PyRIT enables human red teamers to allocate more time to strategic,
    complex attack simulations and creative vulnerability exploration by streamlining
    the detection of issues such as adversarial attacks and data poisoning. This combination
    of automation and human expertise aims to deepen the security testing of AI systems,
    ensuring they are resilient against a broad spectrum of cyber threats.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: PyRIT 自动化了红队测试过程中的某些方面，使安全专业人士能够高效地发现可能被利用在生成式AI系统中的潜在弱点。PyRIT 通过简化对对抗性攻击和数据中毒等问题检测，使人类红队人员能够将更多时间投入到战略性的复杂攻击模拟和创造性漏洞探索中。这种自动化与人类专业知识的结合旨在深化AI系统的安全测试，确保它们能够抵御广泛的网络威胁。
- en: Red team as a service
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 红队作为服务
- en: HackerOne’s AI safety red teaming service offers a possible solution for organizations
    that lack the time, resources, or expertise to develop and sustain an in-house
    red team dedicated to the security of their AI systems. This service provides
    a flexible, “as-a-service” approach, allowing organizations to access the specialized
    skills and insights necessary for comprehensive AI security assessments without
    significant internal investment.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: HackerOne的AI安全红队服务为那些缺乏时间、资源或专业知识来开发和维持内部红队以保护其AI系统的组织提供了一个可能的解决方案。这项服务提供了一种灵活的“作为服务”的方法，允许组织在不进行重大内部投资的情况下，访问进行全面的AI安全评估所需的专门技能和见解。
- en: By leveraging HackerOne’s network of crowdsourced security professionals, companies
    can benefit from thorough and creative adversarial testing tailored to AI technologies’
    unique vulnerabilities. This external expertise supports identifying and mitigating
    potential threats to enhance the security posture of AI systems with flexibility
    and scalability that aligns with organizational needs and capacities.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用HackerOne的众包安全专业人士网络，公司可以从针对AI技术独特漏洞的彻底和创造性的对抗性测试中受益。这种外部专业知识支持识别和缓解潜在威胁，以灵活性和可扩展性来增强AI系统的安全态势，这与组织的需求和能力相一致。
- en: Continuous Improvement
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持续改进
- en: The secure deployment of LLM applications is not a onetime effort but a continuous
    journey of improvement and adaptation. Insights gleaned from logged prompts and
    responses, UEBA, and AI red team exercises are invaluable assets in this process.
    They provide a rich dataset from which to learn and a roadmap for enhancing your
    LLM applications’ security and functionality. Based on the results you see from
    these sources, there are many activities you can execute continuously to improve
    your overall security and safety posture.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: LLM应用的安全部署不是一个一次性努力，而是一个持续改进和适应的旅程。从记录的提示和响应、用户和实体行为分析（UEBA）以及AI红队练习中获得的经验见解是这个过程中的宝贵资产。它们提供了一个丰富的数据集，从中学习并制定增强你的LLM应用安全和功能性的路线图。根据从这些来源看到的结果，有许多活动你可以持续执行以改善你的整体安全和安全态势。
- en: Establishing and Tuning Guardrails
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立和调整边界
- en: 'Earlier in this chapter, we discussed the importance of guardrails and how
    they can be flexibly implemented. You should make maintaining and updating your
    guardrails part of your DevOps process. Whether you build your own guardrails
    by hand or use one of the frameworks discussed earlier, you’ll still need to update
    and tune them continuously:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期，我们讨论了边界的重要性以及它们如何灵活实施。你应该将维护和更新你的边界作为你的DevOps流程的一部分。无论你是手动构建自己的边界还是使用前面讨论的框架之一，你仍然需要持续更新和调整它们：
- en: Adaptive guardrails
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应边界
- en: Use the insights from your monitoring and testing activities to fine-tune existing
    guardrails around your LLM’s operations. This might involve adjusting thresholds
    for acceptable behavior, refining content filters, or enhancing data privacy measures.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 利用你的监控和测试活动的见解来微调围绕你的LLM操作的现有边界。这可能涉及调整可接受行为的阈值、完善内容过滤器或增强数据隐私措施。
- en: New guardrails
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 新的边界
- en: Beyond tuning, the intelligence gathered can reveal the need for entirely new
    guardrails. These might address emerging threats, new patterns of misuse, or unintended
    model behaviors that were previously unnoticed.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 除了调整之外，收集到的情报可以揭示需要全新的边界。这些可能解决新兴威胁、新的滥用模式或之前未注意到的意外模型行为。
- en: Managing Data Access and Quality
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理数据访问和质量
- en: 'In two previous chapters, we’ve discussed the delicate balance of giving your
    LLM too much or too little data. In [Chapter 5](ch05.html#can_your_llm_know_too_much),
    we discussed the risks of sensitive information disclosure. In [Chapter 6](ch06.html#do_language_models_dream_of_electric_sheep),
    we discussed the risks of hallucination. We can help keep those risks in check
    by incorporating these lessons into our process. This is the time to add new expertise
    to your overall DevSecOps approach. As you include MLOps and LLMOps approaches,
    you’ll want to include data scientists and behavioral analysts in your workflows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两个章节中，我们讨论了在给你的大型语言模型（LLM）提供过多或过少数据之间的微妙平衡。在[第五章](ch05.html#can_your_llm_know_too_much)中，我们讨论了敏感信息泄露的风险。在[第六章](ch06.html#do_language_models_dream_of_electric_sheep)中，我们讨论了幻觉的风险。通过将这些经验教训融入我们的流程中，我们可以帮助控制这些风险。现在是时候将新的专业知识添加到你的整体DevSecOps方法中。当你包括MLOps和LLMOps方法时，你希望将数据科学家和行为分析师纳入你的工作流程中：
- en: Data access
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 数据访问
- en: Regularly review and manage the data your LLM can access. This involves removing
    access to sensitive or irrelevant data and incorporating new datasets to help
    the model avoid hallucinations or biases, thereby improving its reliability and
    the quality of its outputs.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 定期审查和管理你的LLM可以访问的数据。这包括移除对敏感或不相关数据的访问，并纳入新的数据集以帮助模型避免幻觉或偏差，从而提高其可靠性和输出质量。
- en: Quality control
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 质量控制
- en: Ensure that the data fed into your LLM is of high quality and representative.
    This reduces the risk of training the model on misleading or harmful information,
    which can directly impact its security and effectiveness.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 确保输入到您的LLM中的数据质量高且具有代表性。这降低了在误导性或有害信息上训练模型的风险，这会直接影响到其安全性和有效性。
- en: Leveraging RLHF for Alignment and Security
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用RLHF进行对齐和安全
- en: '*Reinforcement learning from human feedback* (RLHF) is a sophisticated machine
    learning technique that significantly enhances the performance and alignment of
    LLMs with human values and expectations. At its core, RLHF involves training LLMs
    using feedback generated by human evaluators rather than relying solely on predefined
    reward functions or datasets. This process starts with humans reviewing the outputs
    produced by a model in response to certain inputs or prompts. Evaluators then
    provide feedback, ranging from rankings and ratings to direct corrections or preferences.
    This human-generated feedback is used to create or refine a reward model, guiding
    the LLM in generating responses that are more closely aligned with human judgment
    and ethical standards. The iterative nature of RLHF allows for continuous improvement
    of the model’s accuracy, relevance, and safety, which makes it a critical tool
    in developing user-centric AI applications.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*从人类反馈中进行强化学习*（RLHF）是一种复杂的机器学习技术，显著提高了LLMs（大型语言模型）的性能和对人类价值观和期望的对齐。其核心在于，RLHF涉及使用由人类评估者生成的反馈来训练LLMs，而不是仅仅依赖于预定义的奖励函数或数据集。这个过程从人类审查模型对特定输入或提示产生的输出开始。评估者随后提供反馈，这包括排名、评分，以及直接的纠正或偏好。这些由人类生成的反馈被用来创建或完善奖励模型，指导LLM生成更接近人类判断和道德标准的响应。RLHF的迭代性质允许模型在准确性、相关性和安全性方面持续改进，这使得它成为开发以用户为中心的AI应用的关键工具。'
- en: RLHF bridges the gap between raw computational output and the nuanced understanding
    of language and context that characterizes human communication by integrating
    human insights into the training process. This method improves the model’s ability
    to generate coherent and contextually appropriate responses and ensures that these
    outputs adhere to ethical guidelines and societal norms. As AI applications become
    increasingly integrated into everyday life, the role of RLHF in ensuring these
    technologies act in beneficial and nonharmful ways to humans becomes ever more
    crucial.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF通过将人类洞察力整合到训练过程中，弥合了原始计算输出与人类交流中特有的语言和上下文细微理解的差距。这种方法提高了模型生成连贯且上下文适当的响应的能力，并确保这些输出符合道德指南和社会规范。随着AI应用越来越多地融入日常生活，RLHF在确保这些技术以有益且无害的方式对人类行为发挥关键作用。
- en: Admittedly, incorporating RLHF into the process is more complex, involved, and
    expensive than straightforward interventions, such as tweaking guardrails, fine-tuning,
    or augmenting RAG data. However, for applications where accuracy, alignment with
    human values, and ethical considerations are paramount, RLHF stands out as one
    of the most powerful tools available. Its capability to iteratively refine and
    align the model’s outputs through direct human feedback makes it an invaluable
    asset for developing LLM applications that are not only technologically advanced
    but also deeply attuned to the nuances of human interaction and expectations.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 诚然，将RLHF纳入流程比简单的干预措施，如调整护栏、微调或增强RAG数据更为复杂、涉及更多且成本更高。然而，对于准确度、与人类价值观的对齐和道德考量至关重要的应用，RLHF脱颖而出，成为最强大的工具之一。它通过直接的人类反馈迭代地完善和调整模型输出，使其成为开发不仅技术先进而且深刻理解人类互动和期望的LLM应用的无价资产。
- en: Warning
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: While RLHF offers significant advantages in aligning LLMs with human values
    and improving their performance, it is crucial to be aware of its limitations
    and potential pitfalls. Firstly, introducing human feedback into the training
    process can inadvertently introduce or amplify biases, reflecting the evaluators’
    subjective perspectives or unconscious prejudices. Additionally, RLHF does not
    inherently protect against adversarial attacks; sophisticated adversaries might
    still find ways to exploit vulnerabilities in the model’s responses. Another concern
    is the potential for *policy overfitting*, where the model becomes overly specialized
    in generating responses that satisfy the feedback, but loses generalizability
    and performance across broader contexts. Developers need to weigh these factors
    carefully and consider implementing complementary strategies to mitigate these
    limitations and ensure the responsible development of AI technologies.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RLHF在使LLM与人类价值观保持一致并提高其性能方面提供了显著优势，但了解其局限性和潜在陷阱至关重要。首先，将人类反馈引入训练过程可能会无意中引入或放大偏见，反映了评估者的主观观点或无意识的偏见。此外，RLHF本身并不能保护免受对抗性攻击；复杂的对手可能仍然找到利用模型响应中漏洞的方法。另一个担忧是潜在的*政策过度拟合*，其中模型在生成满足反馈的响应方面变得过于专业化，但失去了在更广泛背景下的泛化能力和性能。开发者需要仔细权衡这些因素，并考虑实施补充策略来缓解这些限制，并确保AI技术的负责任开发。
- en: Conclusion
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Integrating LLMs into production is complex and demands a sophisticated approach
    to security and operations. The shift toward DevSecOps, MLOps, and LLMOps represents
    a critical evolution in developing, deploying, and securing software, which highlights
    the importance of embedding security deeply within the development lifecycle.
    This foundation is crucial for navigating the risks associated with LLM technologies,
    from privacy and security to ethical and regulatory concerns.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 将LLM集成到生产中是复杂的，需要一种对安全和运营的复杂方法。向DevSecOps、MLOps和LLMOps的转变代表了在开发、部署和保障软件中的关键演变，这突出了在开发生命周期中深入嵌入安全的重要性。这个基础对于应对LLM技术相关的风险至关重要，从隐私和安全到伦理和监管问题。
- en: The role of AI red teaming offers a proactive means to identify and mitigate
    potential vulnerabilities through simulated adversarial attacks. Red teaming,
    alongside continuous monitoring and improvement principles, sets the stage for
    a dynamic and resilient approach to LLM application security. It underscores the
    necessity of a vigilant, adaptive stance toward technology integration, where
    ongoing evaluation and refinement are key to safeguarding against evolving threats.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能红队测试的作用提供了一种通过模拟对抗性攻击来主动识别和缓解潜在漏洞的方法。红队测试，与持续监控和改进原则相结合，为LLM应用安全提供了一个动态和有弹性的方法。它强调了在技术集成中保持警惕、适应性立场的重要性，其中持续评估和改进是防范不断演变威胁的关键。
- en: Securing LLM applications is a journey that emphasizes the importance of a continuous,
    iterative process. By rigorously applying the cycle of development, deployment,
    monitoring, and refining, organizations can create systems of unparalleled robustness
    and security. This commitment to perpetual enhancement, guided by the latest security
    practices and insights from each cycle, ensures that with every iteration, the
    applications become safer, more secure, and more aligned with ethical standards.
    This relentless pursuit of improvement will lead to the most resilient LLM applications,
    ready to meet the challenges of tomorrow with confidence.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 保护LLM应用是一项强调持续、迭代过程重要性的旅程。通过严格应用开发、部署、监控和改进的循环，组织可以创建出无与伦比的稳健和安全系统。这种对持续改进的承诺，由每个循环中最新的安全实践和洞察力引导，确保每次迭代，应用都变得更加安全、更加符合道德标准。这种不懈的追求改进将导致最具有弹性的LLM应用，自信地迎接明天的挑战。
