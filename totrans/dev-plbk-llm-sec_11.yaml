- en: Chapter 11\. Trust the Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you can’t describe what you are doing as a process, you don’t know what you’re
    doing.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: W. Edwards Deming
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We’ve spent most of this book exploring the dangers of applying LLM technology
    in production. While there is great power in technology, there are many risks.
    Security, privacy, financial, legal, and reputational risks seem to be around
    every corner. With that understanding, how can you move forward with confidence?
    It’s time to talk about actionable, durable, repeatable solutions. While we’ve
    discussed practical mitigation strategies for each risk, tackling them individually
    as a patchwork isn’t likely to cut it. You must build security into your development
    process to ensure your success.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will discuss two process elements that have emerged as key ingredients
    in successful projects. First, we’ll discuss the evolution of the DevSecOps movement
    and how it’s become central to application security for any large software project.
    We will examine how it has evolved to encompass specific challenges with AI/ML
    and LLMs. As part of this discussion, we’ll look at development-time tools to
    scan for security vulnerabilities and runtime tools (known as guardrails) that
    can help protect your LLM in production.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll also look at how security testing has evolved and the emerging field of
    AI red teaming. Red teams have been around for a long time in cybersecurity circles,
    but AI red teaming has recently gained more prominence as specific techniques
    have evolved that apply to LLM projects.
  prefs: []
  type: TYPE_NORMAL
- en: The Evolution of DevSecOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The origin of *DevOps* can be traced back to the early 2000s when it emerged
    in response to the growing need for better collaboration and integration between
    software development (Dev) and IT operations (Ops) teams. This need arose from
    the limitations observed in traditional software development methodologies, which
    often led to siloed teams, delayed releases, and a need for more alignment between
    development objectives and operational stability. The DevOps movement aimed to
    bridge this gap by promoting a culture of collaboration, automation, continuous
    integration, and continuous delivery (CI/CD), thereby enhancing the speed and
    quality of software deployment.
  prefs: []
  type: TYPE_NORMAL
- en: As DevOps practices matured and became more widely adopted, the critical need
    to integrate security principles into the development lifecycle became increasingly
    apparent. This realization led to integrating security (Sec) into the DevOps process,
    giving us *DevSecOps*. DevSecOps enriches DevOps practices by embedding security
    at every phase of the software development process, from design to deployment.
    The goal is to ensure that security considerations are not an afterthought but
    are integrated into the workflow, enabling the early discovery and mitigation
    of vulnerabilities, thus building more secure software.
  prefs: []
  type: TYPE_NORMAL
- en: We want to enable this same proactive security stance in the development and
    deployment of applications using LLMs. To do so, the principles of DevOps and
    DevSecOps have further inspired the emergence of *MLOps* and *LLMOps* to address
    the unique challenges and requirements of deploying and managing AI/ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps (machine learning operations) focuses on automating and optimizing the
    machine learning lifecycle (including data preparation, model training, deployment,
    and observability) to ensure consistent and efficient ML model development and
    maintenance. LLMOps (large language model operations) explicitly addresses the
    operational needs of large language models, focusing on aspects such as prompt
    engineering, model fine-tuning, and RAG. These specialized practices demonstrate
    the ongoing expansion of the DevOps philosophy, which has adapted to encompass
    emerging technologies’ operational and security needs, thus ensuring their effective
    integration into the broader software development and deployment ecosystem. Using
    concepts from both MLOps and LLMOps will help you extend your organization’s DevSecOps
    process to account for the specific needs of adding advanced AI technology to
    your stack.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MLOps is a set of best practices that aims to streamline and automate the machine
    learning lifecycle, from data preparation and model development to deployment
    and monitoring. Key elements of MLOps include version control for both models
    and data, ensuring reproducibility and traceability, model training, and validation
    for selecting the best model candidates.
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD pipelines are tailored for ML workflows to automate the testing and deployment
    of models and for monitoring model performance in production to catch and address
    model degradation due to model or data drift over time. Additionally, MLOps emphasizes
    collaboration between data scientists, ML engineers, and operations teams to facilitate
    a more efficient and seamless development process, ensuring accurate, scalable,
    and maintainable ML models.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps infrastructure plays a critical role in the security landscape of machine
    learning systems. By integrating security practices throughout the ML lifecycle,
    MLOps can help identify and mitigate risks early in development. This includes
    ensuring data privacy and compliance with regulations such as GDPR, managing access
    to sensitive datasets, and securing model endpoints against adversarial attacks.
    Automated vulnerability scanning and incorporating security checks into CI/CD
    pipelines help catch security issues before deployment. Moreover, monitoring deployed
    models for anomalous behavior can detect potential security breaches, contributing
    to a more robust security posture for ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: LLMOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MLOps, while crucial in establishing practices for any application leveraging
    machine learning, doesn’t address all the unique challenges LLMs pose. LLMs introduce
    specific challenges, such as prompt engineering, robust monitoring to capture
    the nuanced performance, and the potential misuse of generated outputs. This means
    we must take advantage of the best that DevSecOps and MLOps can teach us and then
    add more techniques specific to LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: LLMOps evolved as a specialized discipline to address these challenges. It encompasses
    practices tailored for deploying, monitoring, and maintaining LLMs in production
    environments. LLMOps deals with aspects such as model versioning and management
    at a much larger scale, advanced deployment strategies to handle the high computational
    load, and specific monitoring techniques for evaluating the qualitative aspects
    of model outputs. Furthermore, LLMOps emphasizes the importance of prompt engineering
    and feedback loops to refine model performance and mitigate risks associated with
    model-generated content. This specialized focus ensures that LLM deployments are
    efficient, ethical, and aligned with user expectations and regulatory requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s examine how best to integrate security practices into LLMOps to ensure
    a repeatable process for delivering more secure applications.
  prefs: []
  type: TYPE_NORMAL
- en: Building Security into LLMOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All this discussion about DevSecOps, MLOps, and LLMOps may sound daunting.
    However, the critical tasks required to secure our process for building secure
    LLM apps can be broken down into five simple steps: foundation model selection,
    data preparation, validation, deployment, and monitoring, as shown in [Table 11-1](#table-11-1).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-1\. LLMOps steps
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | LLMOps security measures |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Foundation model selection | Opt for foundation models with robust security
    features. Assess the security history and vulnerability reports of the model’s
    source. Review the model card provided with the foundation model and the security-specific
    information provided. Review what you can about the datasets used to train the
    foundation model. Implement processes to watch for new versions of the foundation
    model, which may add security or alignment improvements. |'
  prefs: []
  type: TYPE_TB
- en: '| Data preparation | If you plan to use fine-tuning or RAG to enhance the domain-specific
    knowledge available to your application, you must prepare your data. Carefully
    evaluate the sources of your datasets. Ensure data is scrubbed, anonymized, and
    free from illegal or inappropriate content. Evaluate your data for possible bias.
    Implement secure data handling and access controls during fine-tuning or embedding
    generation. |'
  prefs: []
  type: TYPE_TB
- en: '| Validation | Extend your security testing to include LLM-specific vulnerability
    scanners and AI red teaming exercises. (We’ll talk more about AI red teams later
    in the chapter.) Extend your validation steps to check for nontraditional security
    threats such as toxicity and bias. |'
  prefs: []
  type: TYPE_TB
- en: '| Deployment | Ensure you have appropriate runtime guardrails to screen prompts
    entering your model and output. Automate your build process to ensure that your
    ML-BOM is regenerated and stored with every set of changes. |'
  prefs: []
  type: TYPE_TB
- en: '| Monitoring | Log all activity and monitor for anomalies that could indicate
    jailbreaks, attempts to deny service, or other compromises of your infrastructure.
    |'
  prefs: []
  type: TYPE_TB
- en: Security in the LLM Development Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now it’s time to move past process abstractions and get into the practical steps
    you must adopt to make your secure development process repeatable. We’ll look
    at topics that range across the entire development lifecycle. We’ll start by looking
    at how to make sure your development environment and pipeline are secure. Then
    we’ll look into LLM-specific security testing tools you can use to check your
    security procedures before deployment. We’ll also review the steps you must take
    to ensure the security of your software supply chain.
  prefs: []
  type: TYPE_NORMAL
- en: Securing Your CI/CD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The security of the development pipeline is paramount in preventing your project
    from becoming a weak link in the supply chain. In [Chapter 9](ch09.html#find_the_weakest_link),
    we reviewed the SolarWinds case study, which shows how disastrous it can be for
    you and your downstream customers if your pipeline is compromised. This section
    explores strategies to fortify the pipeline against threats, ensuring that your
    LLM application does not get compromised or inadvertently contribute to the security
    vulnerabilities of downstream users.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing robust security practices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s look at some critical practices you’ll need to implement your security
    program:'
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD security
  prefs: []
  type: TYPE_NORMAL
- en: Integrate security checks into the CI/CD pipeline to automatically detect vulnerabilities
    or misconfigurations early in the development process.
  prefs: []
  type: TYPE_NORMAL
- en: Dependency management
  prefs: []
  type: TYPE_NORMAL
- en: Regularly audit and update the dependencies used in your project to mitigate
    vulnerabilities associated with outdated or compromised libraries. ML-specific,
    open source build pipeline components, such as PyTorch, have had severe, zero-day
    security issues reported recently, demonstrating the importance of this step.
  prefs: []
  type: TYPE_NORMAL
- en: Access control and monitoring
  prefs: []
  type: TYPE_NORMAL
- en: Limit access to the CI/CD environment and monitor activity to promptly detect
    and respond to suspicious behavior. Secure your training data repositories, just
    as you would your source code, to help protect against possible data poisoning
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Fostering a culture of security awareness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Training your humans can be just as important as training your LLM in building
    a secure app. Here are some things to think about in how your train and prepare
    your people:'
  prefs: []
  type: TYPE_NORMAL
- en: Training and awareness
  prefs: []
  type: TYPE_NORMAL
- en: Educate members of the development team on the importance of supply chain security
    and their role in maintaining it. Ensure your team understands the new components,
    such as foundation models and training datasets, that must be managed as part
    of your application’s supply chain.
  prefs: []
  type: TYPE_NORMAL
- en: Incident response planning
  prefs: []
  type: TYPE_NORMAL
- en: Develop and regularly update an incident response plan that includes procedures
    for addressing supply chain threats, including zero-day vulnerability disclosures.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-Specific Security Testing Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Application security testing tools can come in multiple flavors, such as Static
    Application Security Testing (SAST), Dynamic Application Security Testing (DAST),
    and Interactive Application Security Testing (IAST). All have established themselves
    as indispensable instruments in developing traditional web applications. While
    each has its strengths and weaknesses, they all help automate the identification
    of vulnerabilities and security flaws, facilitating early detection and remediation.
    Their integration into the software development lifecycle enables organizations
    to adopt a proactive stance on security, ensuring that applications are functional
    and secure by design.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs present unique security challenges that are not fully addressed by traditional
    security testing methodologies. Their complexity, novelty, and susceptibility
    to issues like data bias, hallucination, and adversarial attacks necessitate specialized
    tools tailored to their distinct context. Although the field is relatively nascent,
    new tools aimed at fortifying LLM applications against a spectrum of vulnerabilities
    are beginning to emerge. Let’s look at several examples.
  prefs: []
  type: TYPE_NORMAL
- en: TextAttack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TextAttack has been around in some form since at least 2020\. It is a sophisticated
    Python framework designed for adversarial testing of NLP models, including LLMs.
    Free and open source, distributed under the MIT license, it facilitates the exploration
    of vulnerabilities in language models and the development of robust defenses against
    adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: TextAttack stands out by offering a modular architecture that allows for the
    customization and testing of attack strategies across various models and datasets.
    It simulates adversarial examples to reveal potential weaknesses in NLP applications,
    thereby guiding improvements in model resilience. The tool provides detailed reports
    on attack methodologies, success rates, and model responses, making it invaluable
    for security assessments. Its adaptability and comprehensive coverage of attack
    techniques make TextAttack a powerful tool for developers and researchers aiming
    to enhance the security and reliability of LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: Garak
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Garak, named after an obscure *Star Trek* character, is an LLM vulnerability
    scanner. Garak was developed by Leon Derczynski, who was a significant contributor
    to developing the first versions of the OWASP Top 10 for LLM Applications. Garak
    is free to use and distributed under a liberal Apache open source license.
  prefs: []
  type: TYPE_NORMAL
- en: Garak adopts a model similar to that of DAST tools, where it probes the application
    at runtime and examines its behavior, looking for vulnerabilities. The tool sends
    various prompts to models, analyzing multiple outputs using detectors to identify
    unwanted content. The results aren’t scientifically validated, but a higher passing
    percentage indicates better performance. It can be customized with plug-ins for
    additional prompts or vulnerabilities. It generates detailed reports that include
    all test parameters, prompts, responses, and scores. There’s potential for expansion
    to different models and vulnerabilities based on user contributions and requests.
  prefs: []
  type: TYPE_NORMAL
- en: Responsible AI Toolbox
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The [Responsible AI Toolbox](https://oreil.ly/6hpZE), developed by Microsoft,
    is an open source tool suite that enables developers and data scientists to infuse
    ethical principles, fairness, and transparency into their AI systems. This toolbox
    is distributed under the MIT license and offers an integrated environment to assess,
    improve, and monitor models on various dimensions of responsible AI, including
    fairness, interpretability, and privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Giskard LLM Scan
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Giskard LLM Scan is an open source tool used to assess an LLM’s ethical considerations
    and safety. Available under the Apache 2.0 license, this component of the Giskard
    AI suite aims to identify biases, detect instances of toxic content, and promote
    the responsible deployment of LLMs. It employs a variety of metrics and tests
    designed to evaluate LLM behavior in terms of fairness, toxicity, and inclusiveness.
    Through its interface, Giskard LLM Scan offers detailed reports highlighting areas
    of concern, assisting developers and researchers in understanding and potentially
    mitigating ethical risks in their AI models.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating security tools into DevOps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Integrating automated, LLM-specific security testing tools and traditional AST
    (application security testing) tools into LLMOps processes is not merely beneficial
    but imperative. Embedding these tools within CI/CD pipelines ensures that security
    is not an afterthought but a foundational aspect of application development. This
    approach enables automated, repeatable security checks performed with every build,
    significantly reducing the risk of vulnerabilities in production. Moreover, it
    fosters a culture of security mindfulness among development teams, ensuring that
    security considerations are paramount from the inception of a project through
    to its deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Your Supply Chain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in [Chapter 9](ch09.html#find_the_weakest_link), the supply chain
    represents more than sourcing components and tools. It involves the meticulous
    generation, storage, and accessibility of development artifacts such as model
    cards and ML-BOMs.
  prefs: []
  type: TYPE_NORMAL
- en: Model cards are essential documentation for LLMs, providing an overview of a
    model’s purpose, performance, and potential biases. Similarly, ML-BOMs detail
    the components, datasets, and dependencies involved in developing an application
    using machine learning technologies like an LLM. Together, these artifacts form
    a cornerstone of transparency and accountability in LLM development.
  prefs: []
  type: TYPE_NORMAL
- en: To manage them effectively, developers must implement systems for generating,
    storing, and making these artifacts easily searchable. This facilitates regulatory
    compliance and enhances stakeholder collaboration and trust. By integrating these
    practices into a broader SBOM strategy, teams can ensure a holistic view of both
    AI and non-AI components of their applications, reinforcing the security and integrity
    of the supply chain.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll need to focus on three pillars to ensure your artifacts are properly
    tracked, thus helping to ensure you’re in control of your supply chain:'
  prefs: []
  type: TYPE_NORMAL
- en: Automated generation
  prefs: []
  type: TYPE_NORMAL
- en: Implement tools and workflows that automatically generate model cards and ML-BOMs
    at key development milestones.
  prefs: []
  type: TYPE_NORMAL
- en: Secure storage
  prefs: []
  type: TYPE_NORMAL
- en: Store these artifacts in secure, version-controlled repositories to ensure they
    are tamper-proof and retrievable.
  prefs: []
  type: TYPE_NORMAL
- en: Accessibility
  prefs: []
  type: TYPE_NORMAL
- en: Make these artifacts accessible to relevant stakeholders, incorporating search
    functionalities to facilitate quick retrieval and review.
  prefs: []
  type: TYPE_NORMAL
- en: The supply chain in LLM application development is a complex ecosystem that
    requires diligent management to ensure the security and integrity of both development
    artifacts and the development pipeline. By prioritizing the generation and storage
    of key artifacts like model cards and ML-BOMs and by securing the development
    pipeline, organizations can safeguard against supply chain vulnerabilities, fostering
    trust and reliability in their LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: Protect Your App with Guardrails
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tools such as web application firewalls (WAFs) and runtime application self-protection
    (RASP) have become fundamental in defending web applications against attacks during
    runtime. Unlike AST tools that analyze code for vulnerabilities at build and test
    time, WAFs and RASP provide continuous protection while an application operates
    in production. They act as vigilant guardians, identifying and mitigating threats
    in real time, thus adding a critical layer of security.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of LLMs, a parallel can be drawn with the concept of *guardrails*.
    Guardrails help ensure that LLMs operate within defined ethical, legal, and safety
    parameters, preventing misuse and guiding the models toward generating appropriate
    and safe outputs. Initially, guardrail implementations were relatively simplistic,
    often built in house and tailored to specific use cases. In [Chapter 7](ch07.html#trust_no_one),
    we walked through the construction of some simple guardrails to help screen output
    from the LLM for toxicity and PII. This exercise was a great way to understand
    the basics of how some guardrails work.
  prefs: []
  type: TYPE_NORMAL
- en: However, the demand for more sophisticated security and safety frameworks has
    increased as LLM-based applications have grown more complex. Today, there is a
    burgeoning ecosystem of tools, both open source and commercial, offering more
    comprehensive guardrail frameworks for LLMs. These tools serve as runtime security
    measures, continuously monitoring and guiding the behavior of LLMs to prevent
    the generation of harmful, biased, or otherwise undesirable content. They are
    akin to WAFs and RASP in the web application space, providing a dynamic shield
    that adapts to emerging threats and challenges.
  prefs: []
  type: TYPE_NORMAL
- en: The Role of Guardrails in an LLM Security Strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Incorporating advanced guardrail solutions into LLM deployments is not just
    a recommendation; it’s becoming a necessity. As these models become more deeply
    integrated into critical and consumer-facing applications, the potential impact
    of their misuse or malfunction grows exponentially. Guardrails offer a way to
    mitigate these risks. Guardrails frameworks offer a range of functionality, but
    here are some typical functions you’ll want to look for as you evaluate your options.
  prefs: []
  type: TYPE_NORMAL
- en: Input validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are several benefits of implementing guardrails that scan the input into
    your LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt injection prevention
  prefs: []
  type: TYPE_NORMAL
- en: Monitor for signs of prompt injection, such as unusual phrases, hidden characters,
    and odd encodings, to prevent malicious manipulation of the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Domain limitation
  prefs: []
  type: TYPE_NORMAL
- en: Keep the LLM focused on relevant topics by restricting or ignoring irrelevant
    prompts. This enhances security by reducing the risk of generating inappropriate
    or irrelevant content and diminishing the likelihood of hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: Anonymization and secret detection
  prefs: []
  type: TYPE_NORMAL
- en: While interacting with the LLM, users may input confidential data, like email
    addresses, telephone numbers, or API keys. This poses a problem if the data is
    logged, stored, or transferred to a third-party LLM provider or if the data could
    potentially be used for training purposes. It’s crucial to anonymize PII and redact
    sensitive data before the LLM processes it.
  prefs: []
  type: TYPE_NORMAL
- en: Output validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Screening all output from your LLM is a critical part of your zero trust strategy.
    Here are some of the benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Ethical screening
  prefs: []
  type: TYPE_NORMAL
- en: Filter outputs for content that could be considered toxic, inappropriate, or
    hateful to ensure the LLM’s interactions align with ethical guidelines. This could
    have saved poor Tay from [Chapter 1](ch01.html#chatbots_breaking_bad) and countless
    other projects from falling victim to vulnerabilities such as unchecked toxicity.
  prefs: []
  type: TYPE_NORMAL
- en: Sensitive information protection
  prefs: []
  type: TYPE_NORMAL
- en: Implement measures to prevent the disclosure of PII or other sensitive data
    through the LLM’s outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Code output
  prefs: []
  type: TYPE_NORMAL
- en: Look for unintended code generation that could lead to downstream attacks such
    as SQL injection, server-side request forgery (SSRF), and XSS.
  prefs: []
  type: TYPE_NORMAL
- en: Compliance assurance
  prefs: []
  type: TYPE_NORMAL
- en: In sectors with strict regulatory standards, like health care or legal, tailor
    outputs to meet specific compliance requirements and keep the LLM’s responses
    within the scope of its intended use.
  prefs: []
  type: TYPE_NORMAL
- en: Fact-checking and hallucination detection
  prefs: []
  type: TYPE_NORMAL
- en: Verify the accuracy of LLM outputs against trusted sources to ensure the information
    provided is factual and reliable. Identify and mitigate instances where the LLM
    generates fictitious or irrelevant content to ensure outputs remain relevant and
    grounded in reality.
  prefs: []
  type: TYPE_NORMAL
- en: Open Source Versus Commercial Guardrail Solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The choice between open source and commercial guardrail solutions depends on
    several factors, including the organization’s specific needs, the level of customization
    required, and budget considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Open source tools offer the benefits of flexibility and community support, allowing
    organizations to tailor solutions to their unique requirements. However, they
    may require significant internal expertise and resources to deploy and maintain
    effectively. Some examples of open source guardrails tools you may wish to evaluate
    include NVIDIA NeMo-Guardrails, Meta Llama Guard, Guardrails AI, and Protect AI.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, commercial solutions may provide more out-of-the-box functionality
    with the added benefits of professional support, regular updates, and advanced
    features. Some examples of commercial guardrail options include Prompt Security,
    Lakera Guard, WhyLabs LangKit, Lasso Security, PromptArmor, and Cloudflare Firewall
    for AI.
  prefs: []
  type: TYPE_NORMAL
- en: Mixing Custom and Packaged Guardrails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 7](ch07.html#trust_no_one), we implemented some basic guardrails
    by hand. While the emergence of prebuilt guardrail frameworks can offer a significant
    boost in security, these handcrafted guardrails still have a role. Supplementing
    a guardrail framework with your own custom, domain-, or application-specific guardrails
    can make a lot of sense. These types of defense-in-depth strategies are often
    the most successful in cybersecurity.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Your App
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the lifecycle of LLM applications, effective monitoring encompasses not only
    the conventional components—such as web servers, middleware, application code,
    and databases—but also the unique elements intrinsic to LLMs, including the model
    itself and associated vector databases used for RAG. This comprehensive approach
    is pivotal for maintaining operational integrity and security throughout the application’s
    lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Logging Every Prompt and Response
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the foundational practices in monitoring LLM applications is to log
    every prompt and response. This detailed logging serves multiple purposes: it
    provides insights into how users interact with the application, enables the identification
    of potential misuse or problematic outputs, and forms a baseline for understanding
    the model’s performance over time. Such granular data collection is critical for
    diagnosing issues, optimizing model behavior, and ensuring compliance with data
    governance standards.'
  prefs: []
  type: TYPE_NORMAL
- en: Centralized Log and Event Management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Aggregating logs and application events into a *security information and event
    management* (SIEM) system is essential. A SIEM system enables data consolidation
    across the entire application stack, offering a unified view of all activities.
    This allows your organization to easily store a historical record of how your
    application has responded to every user input. These centralized logs can then
    be stored for compliance purposes. Also, SIEM systems offer advanced search tools
    that enable your team to quickly search for patterns across a huge range of prompts
    and responses. This can enable your security operations team to hunt for threats
    while your application is in production.
  prefs: []
  type: TYPE_NORMAL
- en: User and Entity Behavior Analytics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To enhance monitoring capabilities further, incorporating *user and entity behavior
    analytics* (UEBA) technology can be layered on top of SIEM. UEBA extends traditional
    monitoring by leveraging machine learning and analytics to understand how users
    and entities typically interact with the application, thereby enabling the detection
    of activities that deviate from the norm. For LLM applications, extending UEBA
    frameworks to encompass model-specific behaviors—such as unusual prompt-response
    patterns or atypical access to the vector database—can provide early warning signs
    of security breaches, data leaks, or the need for model retraining. In addition,
    dramatic changes in usage patterns could help you identify denial-of-service,
    denial-of-wallet, and model cloning attacks, as discussed in [Chapter 8](ch08.html#don_t_lose_your_wallet).
  prefs: []
  type: TYPE_NORMAL
- en: Build Your AI Red Team
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this chapter, we’ve looked at how to secure your development pipeline,
    use security testing tools in a repeatable way, and guard and then monitor your
    application in production. These are all critical steps, but they’ve repeatedly
    been shown to be necessary but insufficient in understanding your application’s
    actions in the real world. The emerging field of *AI red teaming* is designed
    to do just this. Let’s look at how an AI red team can become an important part
    of validating the security of your application.
  prefs: []
  type: TYPE_NORMAL
- en: An AI red team is a group of security professionals who adopt an adversarial
    approach to rigorously challenge the safety and security of applications using
    AI technology, such as an LLM. Their objective is to identify and exploit weaknesses
    in AI systems, much like an external attacker might, but to improve security rather
    than cause harm.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'AI red teams catapulted to the forefront of the AI and LLM security discussion
    when US President Biden issued his October 2023 [“Executive Order on the Safe,
    Secure, and Trustworthy Development and Use of Artificial Intelligence,”](https://oreil.ly/yGHW8)
    which contains the following language:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The term “AI red-teaming” means a structured testing effort to find flaws
    and vulnerabilities in an AI system, often in a controlled environment and in
    collaboration with developers of AI. Artificial Intelligence red-teaming is most
    often performed by dedicated “red teams” that adopt adversarial methods to identify
    flaws and vulnerabilities, such as harmful or discriminatory outputs from an AI
    system, unforeseen or undesirable system behaviors, limitations, or potential
    risks associated with the misuse of the system.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a result of this order, the US Artificial Intelligence Safety Institute,
    part of the National Institute of Standards and Technology (NIST), has created
    a dedicated working group on red teaming best practices.
  prefs: []
  type: TYPE_NORMAL
- en: An AI red team operates under the premise that AI systems have unique vulnerabilities
    that traditional software may not possess, such as adversarial input attacks,
    data poisoning, and model stealing attacks. The AI red team helps organizations
    anticipate and mitigate security breaches by simulating real-world AI-specific
    threats.
  prefs: []
  type: TYPE_NORMAL
- en: 'The critical functions of an AI red team include:'
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial attack simulation
  prefs: []
  type: TYPE_NORMAL
- en: Crafting and executing attacks that exploit weaknesses in AI systems, such as
    feeding deceptive input to manipulate outcomes or extract sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: Vulnerability assessment
  prefs: []
  type: TYPE_NORMAL
- en: Systematically reviewing AI systems to identify vulnerabilities that could be
    exploited by attackers, including those in the underlying infrastructure, training
    data, and model outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Risk analysis
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the potential impact of identified vulnerabilities and providing
    a risk-based assessment to prioritize remediation efforts.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigation strategy development
  prefs: []
  type: TYPE_NORMAL
- en: Recommending defenses and countermeasures to protect AI systems against identified
    threats and vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Awareness and training
  prefs: []
  type: TYPE_NORMAL
- en: Educating developers, security teams, and stakeholders about AI security threats
    and best practices to foster a culture of security-minded AI development.
  prefs: []
  type: TYPE_NORMAL
- en: An AI red team is essential to a robust AI security framework. It ensures that
    AI systems are designed and developed securely, continuously tested, and fortified
    against evolving threats in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of AI Red Teaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional security measures, while necessary, are often insufficient to address
    complex LLM-specific vulnerabilities. A red team, with its holistic and adversarial
    approach, becomes crucial in identifying and mitigating these threats, not just
    through technical means but by examining the broader implications of human and
    organizational behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: Hallucinations, for example, represent a significant risk. A red team, by simulating
    advanced testing scenarios, can identify potential triggers for such behavior,
    enabling developers to understand and mitigate these risks in ways automated testing
    cannot.
  prefs: []
  type: TYPE_NORMAL
- en: Data bias poses a more subtle yet profound threat, as it can lead to unfair
    or unethical outcomes. Red teams can assess the technical aspects of bias and
    systemic issues within data collection and processing practices. The team’s external
    perspective can uncover blind spots in data handling and algorithm training that
    might be overlooked by internal teams focused on functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Excessive agency in LLMs, where the model can act beyond its intended scope,
    requires continuous and creative testing to identify. Red teams can probe the
    limits of LLM behavior to ensure that safeguards against unintended autonomous
    actions are robust and effective.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt injection attacks exploit how LLMs process input to produce unintended
    outcomes, highlighting the need for a red team’s innovative thinking. The team
    can simulate sophisticated attack vectors that challenge the LLM’s ability to
    handle adversarial inputs safely.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, risks like overreliance on LLMs involve technical, human, and organizational
    factors. Red teams can evaluate the broader impact of LLM integration into decision-making
    processes, highlighting areas where reliance on automation might undermine critical
    thinking or operational security.
  prefs: []
  type: TYPE_NORMAL
- en: The necessity of a red team in LLM application security is not merely a matter
    of adding another layer of defense; it’s about adopting a comprehensive and proactive
    approach to security that addresses the full spectrum of risks—from the technical
    to the human. This approach ensures that LLM applications are resilient against
    current threats and prepared to evolve in the face of emerging vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Red Teams Versus Pen Tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Red teams and traditional penetration tests are often discussed in the same
    breath, yet they occupy distinct roles in an organization’s security posture.
    As we tease apart the differences between these two approaches, we must recognize
    that they are not mutually exclusive but complementary in fortifying defenses
    against cyber threats. *Penetration testing* is a point-in-time assessment identifying
    exploitable vulnerabilities. In contrast, red teaming is an ongoing, dynamic process
    that simulates real-world attacks across the entire digital and physical spectrum
    of an organization’s defenses.
  prefs: []
  type: TYPE_NORMAL
- en: Red teaming is particularly crucial when safeguarding the integrity of LLM applications,
    where the attack surface is vast and qualitatively different from traditional
    applications. A red team, operating with a mindset aligned with that of a potential
    adversary, engages in a broader and more fluid form of security testing. This
    encompasses technical vulnerabilities and the organizational, behavioral, and
    psychological aspects of security. In this way, red teaming can also include checking
    for responsible and ethical outcomes, which is extremely difficult for fully automated
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 11-2](#table-11-2) summarizes the differences between a pen test and
    the red team.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-2\. Pen test versus red team
  prefs: []
  type: TYPE_NORMAL
- en: '| Aspect | Pen test | Red team |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Objective | Identify and exploit specific vulnerabilities | Emulate realistic
    cyberattacks to test response capabilities |'
  prefs: []
  type: TYPE_TB
- en: '| Scope | Focused on specific systems, networks, or applications | Broad, includes
    a variety of attack vectors like social engineering, physical security, and network
    security |'
  prefs: []
  type: TYPE_TB
- en: '| Duration | Short-term, typically a few days to a few weeks | Long-term, can
    span several weeks to months to simulate persistent threats |'
  prefs: []
  type: TYPE_TB
- en: '| Frequency | Regular intervals, or as part of compliance assessments | Frequent
    or continuous |'
  prefs: []
  type: TYPE_TB
- en: '| Approach | Tactical, seeking to uncover specific technical vulnerabilities
    | Strategic, aiming to reveal systemic weaknesses and organizational response
    |'
  prefs: []
  type: TYPE_TB
- en: '| Reporting | Detailed list of vulnerabilities with remediation steps | Comprehensive
    assessment of security posture and recommendations for holistic improvement |'
  prefs: []
  type: TYPE_TB
- en: Tools and Approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While you can build a red team entirely on your own, there are emerging tools
    and services that can help. This space will evolve quickly, but we’ll review a
    couple of emerging options so that you’ll know what to look for.
  prefs: []
  type: TYPE_NORMAL
- en: Red team automation tooling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Introduced in February 2024, PyRIT (Python Risk Identification Toolkit for generative
    AI) is Microsoft’s open source initiative to augment the capabilities of AI red
    teams. PyRIT, which evolved from earlier internal tools developed by Microsoft,
    is designed to support identifying and analyzing vulnerabilities within generative
    AI systems. The toolkit serves as an augmentation tool for human red teamers,
    not as a replacement, emphasizing the toolkit’s role in enhancing human-led security
    efforts.
  prefs: []
  type: TYPE_NORMAL
- en: PyRIT automates aspects of the red teaming process, allowing security professionals
    to efficiently uncover potential weaknesses that could be exploited in generative
    AI systems. PyRIT enables human red teamers to allocate more time to strategic,
    complex attack simulations and creative vulnerability exploration by streamlining
    the detection of issues such as adversarial attacks and data poisoning. This combination
    of automation and human expertise aims to deepen the security testing of AI systems,
    ensuring they are resilient against a broad spectrum of cyber threats.
  prefs: []
  type: TYPE_NORMAL
- en: Red team as a service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: HackerOne’s AI safety red teaming service offers a possible solution for organizations
    that lack the time, resources, or expertise to develop and sustain an in-house
    red team dedicated to the security of their AI systems. This service provides
    a flexible, “as-a-service” approach, allowing organizations to access the specialized
    skills and insights necessary for comprehensive AI security assessments without
    significant internal investment.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging HackerOne’s network of crowdsourced security professionals, companies
    can benefit from thorough and creative adversarial testing tailored to AI technologies’
    unique vulnerabilities. This external expertise supports identifying and mitigating
    potential threats to enhance the security posture of AI systems with flexibility
    and scalability that aligns with organizational needs and capacities.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Improvement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The secure deployment of LLM applications is not a onetime effort but a continuous
    journey of improvement and adaptation. Insights gleaned from logged prompts and
    responses, UEBA, and AI red team exercises are invaluable assets in this process.
    They provide a rich dataset from which to learn and a roadmap for enhancing your
    LLM applications’ security and functionality. Based on the results you see from
    these sources, there are many activities you can execute continuously to improve
    your overall security and safety posture.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing and Tuning Guardrails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Earlier in this chapter, we discussed the importance of guardrails and how
    they can be flexibly implemented. You should make maintaining and updating your
    guardrails part of your DevOps process. Whether you build your own guardrails
    by hand or use one of the frameworks discussed earlier, you’ll still need to update
    and tune them continuously:'
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive guardrails
  prefs: []
  type: TYPE_NORMAL
- en: Use the insights from your monitoring and testing activities to fine-tune existing
    guardrails around your LLM’s operations. This might involve adjusting thresholds
    for acceptable behavior, refining content filters, or enhancing data privacy measures.
  prefs: []
  type: TYPE_NORMAL
- en: New guardrails
  prefs: []
  type: TYPE_NORMAL
- en: Beyond tuning, the intelligence gathered can reveal the need for entirely new
    guardrails. These might address emerging threats, new patterns of misuse, or unintended
    model behaviors that were previously unnoticed.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Data Access and Quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In two previous chapters, we’ve discussed the delicate balance of giving your
    LLM too much or too little data. In [Chapter 5](ch05.html#can_your_llm_know_too_much),
    we discussed the risks of sensitive information disclosure. In [Chapter 6](ch06.html#do_language_models_dream_of_electric_sheep),
    we discussed the risks of hallucination. We can help keep those risks in check
    by incorporating these lessons into our process. This is the time to add new expertise
    to your overall DevSecOps approach. As you include MLOps and LLMOps approaches,
    you’ll want to include data scientists and behavioral analysts in your workflows:'
  prefs: []
  type: TYPE_NORMAL
- en: Data access
  prefs: []
  type: TYPE_NORMAL
- en: Regularly review and manage the data your LLM can access. This involves removing
    access to sensitive or irrelevant data and incorporating new datasets to help
    the model avoid hallucinations or biases, thereby improving its reliability and
    the quality of its outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Quality control
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that the data fed into your LLM is of high quality and representative.
    This reduces the risk of training the model on misleading or harmful information,
    which can directly impact its security and effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging RLHF for Alignment and Security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Reinforcement learning from human feedback* (RLHF) is a sophisticated machine
    learning technique that significantly enhances the performance and alignment of
    LLMs with human values and expectations. At its core, RLHF involves training LLMs
    using feedback generated by human evaluators rather than relying solely on predefined
    reward functions or datasets. This process starts with humans reviewing the outputs
    produced by a model in response to certain inputs or prompts. Evaluators then
    provide feedback, ranging from rankings and ratings to direct corrections or preferences.
    This human-generated feedback is used to create or refine a reward model, guiding
    the LLM in generating responses that are more closely aligned with human judgment
    and ethical standards. The iterative nature of RLHF allows for continuous improvement
    of the model’s accuracy, relevance, and safety, which makes it a critical tool
    in developing user-centric AI applications.'
  prefs: []
  type: TYPE_NORMAL
- en: RLHF bridges the gap between raw computational output and the nuanced understanding
    of language and context that characterizes human communication by integrating
    human insights into the training process. This method improves the model’s ability
    to generate coherent and contextually appropriate responses and ensures that these
    outputs adhere to ethical guidelines and societal norms. As AI applications become
    increasingly integrated into everyday life, the role of RLHF in ensuring these
    technologies act in beneficial and nonharmful ways to humans becomes ever more
    crucial.
  prefs: []
  type: TYPE_NORMAL
- en: Admittedly, incorporating RLHF into the process is more complex, involved, and
    expensive than straightforward interventions, such as tweaking guardrails, fine-tuning,
    or augmenting RAG data. However, for applications where accuracy, alignment with
    human values, and ethical considerations are paramount, RLHF stands out as one
    of the most powerful tools available. Its capability to iteratively refine and
    align the model’s outputs through direct human feedback makes it an invaluable
    asset for developing LLM applications that are not only technologically advanced
    but also deeply attuned to the nuances of human interaction and expectations.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While RLHF offers significant advantages in aligning LLMs with human values
    and improving their performance, it is crucial to be aware of its limitations
    and potential pitfalls. Firstly, introducing human feedback into the training
    process can inadvertently introduce or amplify biases, reflecting the evaluators’
    subjective perspectives or unconscious prejudices. Additionally, RLHF does not
    inherently protect against adversarial attacks; sophisticated adversaries might
    still find ways to exploit vulnerabilities in the model’s responses. Another concern
    is the potential for *policy overfitting*, where the model becomes overly specialized
    in generating responses that satisfy the feedback, but loses generalizability
    and performance across broader contexts. Developers need to weigh these factors
    carefully and consider implementing complementary strategies to mitigate these
    limitations and ensure the responsible development of AI technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Integrating LLMs into production is complex and demands a sophisticated approach
    to security and operations. The shift toward DevSecOps, MLOps, and LLMOps represents
    a critical evolution in developing, deploying, and securing software, which highlights
    the importance of embedding security deeply within the development lifecycle.
    This foundation is crucial for navigating the risks associated with LLM technologies,
    from privacy and security to ethical and regulatory concerns.
  prefs: []
  type: TYPE_NORMAL
- en: The role of AI red teaming offers a proactive means to identify and mitigate
    potential vulnerabilities through simulated adversarial attacks. Red teaming,
    alongside continuous monitoring and improvement principles, sets the stage for
    a dynamic and resilient approach to LLM application security. It underscores the
    necessity of a vigilant, adaptive stance toward technology integration, where
    ongoing evaluation and refinement are key to safeguarding against evolving threats.
  prefs: []
  type: TYPE_NORMAL
- en: Securing LLM applications is a journey that emphasizes the importance of a continuous,
    iterative process. By rigorously applying the cycle of development, deployment,
    monitoring, and refining, organizations can create systems of unparalleled robustness
    and security. This commitment to perpetual enhancement, guided by the latest security
    practices and insights from each cycle, ensures that with every iteration, the
    applications become safer, more secure, and more aligned with ethical standards.
    This relentless pursuit of improvement will lead to the most resilient LLM applications,
    ready to meet the challenges of tomorrow with confidence.
  prefs: []
  type: TYPE_NORMAL
