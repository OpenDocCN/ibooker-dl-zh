- en: 12 Conversational summarization for smooth handoff
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Defining elements of an effective conversation summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instrumenting your conversational AI to enhance summarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing a chat transcript into prose with LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting structured details from a chat transcript with LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversational AI builders would love it if their systems contained all user
    conversations. But for most use cases, some percentage of users will end their
    interaction with a human and not your bot. Conversational AI is designed to handle
    the easily automated conversations and direct the higher-value or more challenging
    ones to human agents. Users who want to self-service may be frustrated by “failing”
    with the conversational AI, so it’s important to give that human agent the best
    start possible at handling the call after a transfer.
  prefs: []
  type: TYPE_NORMAL
- en: The two simplest handoff methods are also the least satisfactory. We can transfer
    the conversation “blind” to the human agent and have them ask again for all the
    information they need. Or we can pass the agent the full conversational transcript
    and ask them to search it for the information they need (while the user is waiting!).
    It’s better to give the human agent a targeted summary of the conversation to
    date so they can quickly pick up where the conversational AI left off.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1 Intro to summarization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A summary makes handoffs from AI to human go smoothly. First, we’ll review why
    summaries are needed. Then we’ll explore the elements of effective summaries.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.1 Why summarization is needed
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most conversational AI solutions store full transcripts of conversations for
    auditing and data analysis purposes. This is a treasure trove of training data
    when you have time to analyze it. But when you are a call center agent being transferred
    a conversation from the AI, you don’t have time to read a lengthy transcript.
    You need to quickly grasp the essence of the user’s problem so you can start helping
    them. We’ve seen AI chat transcripts that go for multiple pages (hundreds of words).
    An agent needs a few targeted bullet points—any more will take too long to read,
    and any less will not convey enough useful information.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 illustrates how a conversational summary is generated from a full
    conversational transcript when the chatbot hands off the conversation to a human
    agent.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F01_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 An effective summary helps an agent get up to speed quickly, even
    if the user previously had a lengthy conversation with a bot.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: From the previous chapter, we know that some users immediately opt out to an
    agent, but many users go through a series of steps before getting frustrated.
    The following listing shows an example conversation where the user apparently
    accomplishes their goal but still requests an agent.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.1 Example conversation between user and AI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 User appears to accomplish their goal'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 User still opts out'
  prefs: []
  type: TYPE_NORMAL
- en: We’re not sure why the user opted out (maybe the check didn’t arrive?). But
    that is beside the point. In most conversational AI solutions, users can opt out
    anywhere in the conversation. The conversation in listing 12.1 had seven turns
    and probably took about two minutes. It’s just long enough that it takes some
    effort to read and comprehend. Would you want to read that full conversation if
    you were the human agent? What if the conversation was longer?
  prefs: []
  type: TYPE_NORMAL
- en: A smooth handoff should happen quickly. The agent should quickly comprehend
    what has happened so they can be effective. The user should not have to wait for
    agent to get up to speed. An effective summary facilitates all those needs.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.2 Elements of effective summaries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A conversation summary includes just enough information to understand the complete
    conversation. It should include structured metadata and a short text summary;
    the summary’s contents will vary based on your specific use case. Figure 12.2
    shows an example.
  prefs: []
  type: TYPE_NORMAL
- en: Metadata summary elements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Structured summary elements often come from closed-form questions, such as “What’s
    your member ID?” (“Open-form” questions are like “How may I help?”) These summary
    elements can include data that was collected during the conversation or context
    that was supplied from outside of the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are some example elements:'
  prefs: []
  type: TYPE_NORMAL
- en: User ID of the logged-in user (chat) or the caller’s phone number (voice/SMS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifiers collected during the chat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifiers found during the chat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of chat sessions the user has ever had
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis of user utterances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F02_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 An effective summary pulls out key details from the conversation.
    Here it includes a summary of the conversation and the last claim searched. The
    AI portion of the call may have taken two minutes, but the human agent can read
    the summary in seconds.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In figure 12.2, the chat collected five pieces of information, but the human
    agent only needs to know three. Figure 12.3 breaks down the summary.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F03_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 Not every closed-form question needs to be stored in the summary.
    In this medical insurance claim review, the most important information is the
    provider ID, member ID, and claim ID.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this medical insurance claim search, it takes a lot of information to validate
    the caller. We need to verify who is calling, who they are calling about, and
    what they are calling about. It takes three data elements alone to confirm the
    member information: an ID, a date, and a confirmation of the name. The human agent
    receiving the transfer only needs to know that the member was verified, and the
    member ID suffices for that.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, there are many pieces of information about the claim—some given by
    the user (date of service) and some by the AI (status, paid date, paid amount).
    The summary only includes the claim ID, which will be sufficient for the agent
    to retrieve the full claim, including all the details that were not part of the
    conversation.
  prefs: []
  type: TYPE_NORMAL
- en: It’s useful to include summary elements in software used by human agents fielding
    conversations. In contact center software, this is called a *screen pop*—a feature
    that displays contextual information to an agent while they handle a conversation.
    Figure 12.4 shows an example screen pop for our medical insurance agent. They
    receive the structured information as a highlight, and through backend integration,
    they can get even more information. Clicking on the member ID should show information
    about the member or an image of their ID card. Clicking on the claim should show
    the claim itself (for instance, as a PDF).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F04_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 When a summary is integrated with contact center software, the human
    agent can have a wealth of information at their fingertips. When an insurance
    agent clicks on the member ID in their software, they could get additional details
    on that member.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Structured metadata gives key data points from the conversation so far. It prevents
    the human agent from having to re-ask questions that the user has already answered
    for the bot. Users get *very* frustrated when they must answer the same questions
    again! The human agent benefits from having this information at their fingertips,
    but they still need to be aware of the overall context of the conversation. That’s
    where the free-text summary comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Free-text summary elements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A conversation may have included hundreds of words before it was transferred
    to a human agent. (The example transcript we are using is about 100 words.) The
    average adult reads at 200 words per minute when reading for fun and slower for
    complex material. Our user doesn’t want to wait any longer than necessary, so
    our human agent needs to get up to speed quickly. A good summary can reduce that
    time by minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Follow the “keep it simple” philosophy. A summary of one to two sentences conveys
    a lot of information quickly. The free-text summary in our example—“User searched
    for their claim and found it was paid three months after filing”—encapsulates
    the entire search process in the first five words as well as a likely reason for
    transfer in the last eight words.
  prefs: []
  type: TYPE_NORMAL
- en: 'The free-text summary is also not repetitive. It eliminates several redundant
    pieces of information:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The user’s initial intent*—This is not explicitly included, since it is inferred
    from a claim search being done.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The intermediate questions*—It does not say “The bot asked for the tax ID,
    member ID, date of birth, etc.” These are all implied from the user finding the
    claim. The system does not find claims until sufficient information is provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The structured content*—The summary doesn’t need to waste words repeating
    the structured content, which has already been provided in compact form.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The summary also does not identify who initiated the transfer (the user or the
    system). This could be added in a new structured field.
  prefs: []
  type: TYPE_NORMAL
- en: There are trade-offs in the summarization process. Too brief a summary will
    omit information that helps the agent. Too lengthy a summary will not help the
    agent learn quickly, compared to reading the original transcript. Choose a summarization
    methodology that works best for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Later in this chapter, we will show you how to use generative AI to generate
    summaries. First, though, we need the conversational AI to structure data in the
    right format to generate summaries.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Design your ideal summaries for the following sample conversation. You will
    refine these summaries later in the chapter with additional techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Design a purely textual summary of the sample conversation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Design a structured summary of the data elements. Would you extract different
    structured elements if the caller asked for the details of the member’s health
    plan or tried to proactively estimate the cost of a procedure?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a conversation transcript from a chatbot you are working on. Summarize
    the conversation in both text and structured elements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 12.2 Preparing your chatbot for summarization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve taken it as a given that your chatbot keeps track of the conversational
    transcript. Most platforms do, but not all. The transcript is the bare minimum
    element you need to build a summary. In this section, we’ll show you multiple
    ways to collect the data necessary for both textual and structured summaries of
    conversations.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.1 Using out-of-the-box elements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Conversational AI platforms often include built-in elements to help with conversational
    summarization. The most common element is a conversational transcript—a running
    log of messages between the user and the assistant. This is accessible in different
    ways on different platforms. One common mechanism is called a *session variable*.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.5 shows how the transcript can be accessed in our platform (watsonx)
    via a built-in session variable called “session history.”
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F05_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 Accessing the conversation transcript through the "session history"
    variable
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Depending on your chat platform, the conversational transcript will be stored
    in different formats. Our platform provides the summary in a JSON format, as shown
    in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Note  In many conversational AI platforms, the transcript is not available to
    the dialogue session as a variable unless you craft it yourself via webhooks.
    We’ll demonstrate how to build a variable with the transcript later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.2 Conversational transcript via the built-in “session history” variable
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The JSON format is intended for machines to read, but you can run the transcript
    through a transform process. We made the figures in this chapter more human-readable
    by replacing the `"a"` keys with `"Bot"`, the `"u"` keys with `"User"`, and the
    `\n` (newlines) with spaces.
  prefs: []
  type: TYPE_NORMAL
- en: The transcript also includes some metadata that you may choose to ignore, such
    as the “yes” and “no” choices being offered via buttons. Other conversational
    AI platforms may include further metadata like timestamps.
  prefs: []
  type: TYPE_NORMAL
- en: Later in this chapter (in section 12.3), we will demonstrate running this transcript
    format through an LLM for summarization. We will see that LLMs are quite resilient
    to the transcript format. You can summarize transcripts in their native format
    or reformat them so they are easier for humans to read.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.2 Instrumenting your chatbot for transcripts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some conversational AI platforms require you to create and store the conversation
    transcript yourself. Or you may choose to create your own version of the transcript
    in the exact format you prefer. Either way, this is implemented in your *orchestration
    layer*, as shown in figure 12.6\. The orchestration layer is responsible for calling
    external systems via APIs. The specific terminology will vary based on your conversational
    AI platform, but this is often called a *webhook*.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F06_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.6 You can use your chatbot’s orchestration layer to create a conversational
    transcript in whatever format you need.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A webhook is a type of API. Webhooks are generally available before the bot
    processes the user’s response (a “pre”-webhook), after processing the user’s response
    (a “post”-webhook), or at other predefined events. Webhooks can access conversational
    context either natively or as input parameters. The next listing demonstrates
    pseudocode for constructing a transcript. (Consult your conversational AI platform
    documentation for the correct terminology and format.)
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.3 Pseudocode for a webhook that updates a transcript
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The user’s message is usually found in the request object.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The bot’s message is usually found in the response object.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The user’s message may not exist every time. For example, most conversations
    start with a bot greeting.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 The transcript should be stored in a context variable (session variable).
    All user and bot messages are appended to the transcript.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.3 demonstrates a post-webhook, since it has access to the request
    and the response. Every time the bot responds to the user, the webhook updates
    the transcript. This transcript includes the minimum possible elements—just the
    user and bot messages—and a very simple one-message-per-line format, readable
    by humans. You can create your transcript in whatever format you wish, such as
    a single string, string array, JSON object, or custom object.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown earlier, a simple transcript is easiest to read. Conversational AI
    platforms generally have many data elements available per message that you can
    optionally use in your transcripts:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Message timestamp*—You can use a timestamp to show the absolute time a message
    was received or sent (“11:25:53 AM”) or the relative time since the beginning
    of the chat (“00:01:15” for a message 1 minute and 15 seconds after the beginning).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Buttons*—You can indicate when the bot offered options via buttons and when
    the user clicked on a button. This is especially interesting in voice solutions,
    where users may enter dual tone multi-frequency (DTMF, or “touch tone”) input
    through their keypad. For example, you’ll know that the user pressed “0” rather
    than saying “zero.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Original or post-processed input*—Many conversational AI platforms normalize
    certain input types like dates and numbers. You can use the original utterance,
    like “February first two thousand twenty-four” or a post-processed version like
    “02/01/2024”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rich-text and non-text elements*—Your bot may respond with HTML markup or
    even images and links that may not be suitable for a transcript.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pseudocode in listing 12.3 demonstrated how to update a context variable
    that tracked conversational context, but it did not demonstrate how to initialize
    that variable. The simplest option is to initialize an empty string like the following:
    `response.context.transcript = ''''`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Several other data elements related to a conversation are available, and you
    may wish to include them at the beginning of your transcript:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Session timestamp*—When the conversation started.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Session duration*—How long the conversation lasted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*User identifier*—This could include information about a logged-in user accessing
    the chat, such as name, email, or user ID. For a phone solution, this could be
    the caller’s phone number.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Device and channel identifier*—How the user accessed your conversational AI,
    such as device type (e.g., mobile or desktop) or which channel they used (e.g.,
    chat widget, SMS, Facebook Messenger, etc.).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Transfer reason*—Why the bot transferred the caller to an agent, such as “immediate
    opt-out,” “opt-out,” or “bot didn’t understand user.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may instead choose to leave these elements for the structured section of
    the summary (as key-value pairs), rather than including them in a prose summary,
    since they apply to the entire conversation.
  prefs: []
  type: TYPE_NORMAL
- en: All these data elements and more are typically available in your conversational
    AI platform. They are often included in the AI’s system logs, which are another
    data source for building transcripts. These optional data elements are provided
    by conversational AI platforms because they are generic and are applicable to
    any conversation as metadata. They are a great start to any conversational transcript.
  prefs: []
  type: TYPE_NORMAL
- en: We saw earlier in the chapter that a good summary includes more than an unstructured
    transcript. Structured metadata is quite useful in summarizing the important parts
    of a conversation. Some parts of this metadata are not generic—they are specific
    for your exact implementation. In our medical insurance example, member IDs and
    claim IDs were unique.
  prefs: []
  type: TYPE_NORMAL
- en: The conversational AI platform doesn’t give any special meaning to member IDs—they
    are recorded as just another user message. If you want to use specific contextual
    elements from your implementation in a summary, you’ll have to instrument the
    AI yourself to store them so that they can be included in a summary. Let’s see
    how.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.3 Instrumenting your chatbot (for data points)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Conversational AI platforms generally let you store arbitrary values in variables,
    often called *context variables* or *session variables*. You should make use of
    these for any data you collect during the conversation that has significant meaning,
    especially if it helps you find more information later.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data you store will vary based on your specific application. Here are a
    few examples by domain:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Medical insurance*—Member ID, provider ID, claim ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Retail*—Order number, product ID, retail location'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Banking*—Account ID, account type'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any time the bot asks a question with a fixed-format response, like “what’s
    your claim ID,” that response is a good candidate for instrumentation.
  prefs: []
  type: TYPE_NORMAL
- en: Be careful with sensitive data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Many types of data need to be treated carefully. There are rules and regulations
    on how to handle data that could identify a person (PI) or other sensitive data
    points. Your conversational AI may already deal with them, but adding them to
    summaries or logs may need to be reviewed with your legal team. Be minimalist
    in what you collect, what you store, and how long you store it, and confirm your
    choices with your lawyers.
  prefs: []
  type: TYPE_NORMAL
- en: The way you store context will depend on your conversational AI platform. You
    may be able to do this in a user interface, or your platform may require you to
    write code. Figure 12.7 shows the low-code method used in our platform to store
    context variables.
  prefs: []
  type: TYPE_NORMAL
- en: You can access your stored context variables later in the conversation, including
    accessing them to create a structured summary. The following listing shows pseudocode
    for accessing these context variables and storing them in a structured object.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.4 Pseudocode for a webhook that creates a structured summary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 This method called as the conversation is transferred to an agent.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 You can define a custom object to hold your summary.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Set any values required for your custom summary.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F07_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.7 Storing contextually important information into a context variable
    so that it can be retrieved later by a summary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You can also use these variables directly in your assistant to create an unstructured
    summary. Figure 12.8 shows the low-code method used in our platform to combine
    several variables into a larger summary string.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F08_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.8 Using a low-code expression editor to combine multiple data elements
    into a summary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This section demonstrated multiple ways to collect the data necessary for summarization.
    Conversational AI platforms collect a lot of data that you can use in summaries.
    You can instrument your AI assistants to collect the additional data you need,
    and you can control the formatting of that data. The data you collect is useful
    for many purposes, including efficient handoffs to human agents.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Revisit the summaries you created in the section 12.1 exercises. Would you now
    change any of the data elements included in those summaries?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 12.3 Improving summaries with generative AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What if you don’t want to modify your conversational AI at all? Can you still
    collect all the data you need for a great summary and format it the way you need?
    Can generative AI do more work so you have less work? Yes! Let’s look at how.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have two key prerequisites. First, you need a conversational transcript
    in some form: a built-in transcript from your conversational AI platform, one
    you created yourself, or an extract from your platform’s conversational logs.
    Second, you need to know what a good summary looks like for your use case. Armed
    with those two prerequisites, you can work with an LLM to get the summary you
    need.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will use the granite-13b-chat-v2 model with greedy decoding.
    This model is good at the summarization and extraction techniques we require.
    We’ll use greedy decoding so that the model will not be creative and the outputs
    will be repeatable. (We want to generate the same summary and extract the same
    details for a given conversation.)
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.1 Generating a text summary of a transcript with summarizing prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll start our exercise with a simple summarization prompt, shown in the following
    listing. We’ll pass the model the JSON version of our chat transcript.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.5 Generating a summary of a JSON chat transcript
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Task description and hint for interpreting the JSON object'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Instruction to limit the summary size'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 JSON version of the chat transcript'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Cue'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Model output'
  prefs: []
  type: TYPE_NORMAL
- en: The generated summary is the verbatim last utterance from the AI. This may seem
    strange at first, but this is a pretty good summary of the conversation. The bot’s
    last utterance is rich in details that encompass the most important elements of
    the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: By using sampling decoding and a creative temperature setting, we could have
    gotten a differently structured summary, at the risk of introducing hallucinations.
    We can also change the transcript format and see if that helps the LLM. The next
    listing shows a prompt that summarizes the same conversation with a different
    input format—unstructured text instead of JSON.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.6 Generating a summary of a text chat transcript
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Task description with no additional hints'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Same instruction to limit the summary size'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Text version of chat transcript'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Same cue'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Model output'
  prefs: []
  type: TYPE_NORMAL
- en: This generated summary contains the same informational content as the summary
    generated from JSON. It’s still based on the bot’s last message, but this time,
    it’s paraphrased into prose. The summary also includes a third sentence—in contrast
    with the prompt’s instructions—that attempts to make sense of the user’s last
    utterance.
  prefs: []
  type: TYPE_NORMAL
- en: Note  For the remainder of this chapter, we will omit the conversational transcript
    from the book to keep the listings smaller. We will use the human-readable version
    of the conversation transcript. The full listings are available in the book’s
    GitHub repository at [https://github.com/andrewrfreed/EffectiveConversationalAI](https://github.com/andrewrfreed/EffectiveConversationalAI).
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember our advice earlier in this section: you need to know what a good summary
    looks like. The summaries generated by the LLM have been okay so far, but perhaps
    we can do better. One method is to provide better instructions to the model, as
    shown in the following listing. Because we are emphasizing instructions, we’ll
    switch to a more instructible model, in this case granite-13b-instruct-v2\. The
    model is asked to emphasize the dialogue immediately preceding the escalation.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.7 Enhancing the summarization instructions to the LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Original task description'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Original instruction to limit the summary size'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Enhanced instruction on what to emphasize'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Text version of chat transcript'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Same cue'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Model output'
  prefs: []
  type: TYPE_NORMAL
- en: The model follows the instruction. The last message from the bot tells the user
    their claim was paid (“The claim was paid on May 23, 2024 for $201.83”), and the
    user then opts out. The LLM summary succinctly says, “they want to know why their
    claim is denied.” This summary is short but perhaps too speculative, seeing that
    the claim was paid and not actually denied. Maybe the user felt they should have
    been paid more, or maybe they needed an itemized list. The text summary also omits
    the near-verbatim playback of the transcript seen in listing 12.6, leaving that
    information for a separate structured summary. We’re closer to what our human
    agent needs. Let’s improve the prompt to reduce the LLM’s speculation.
  prefs: []
  type: TYPE_NORMAL
- en: A useful method is to invoke the LLM with a one-shot prompt (with one example)
    or a few-shot prompt (with multiple examples). Creating the one-shot example forces
    us to think about what a good summary looks like for a given conversation. Using
    one or more examples is often the fastest way to improve a prompt.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows a one-shot example that also uses the granite-13b-instruct-v2
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.8 A one-shot summarization prompt
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Original task description with updated delineation via “&lt;|instruction|&gt;”'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 One-shot example of conversation transcript with summary'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Text version of chat transcript'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Restructured cue'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Model output'
  prefs: []
  type: TYPE_NORMAL
- en: This generated summary is also quite reasonable when combined with the structured
    metadata. There is some speculation—“they want more information”—but again it
    seems like the user would need more information if they wanted an agent after
    finding the claim is supposedly paid. The summary is also structured after the
    example, with a token-for-token match in the first six words of the summary.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the one-shot summarization example (listing 12.8), we used a slightly
    different format. Instead of using delineation via `Transcript:`, we used specially
    formatted tokens like `<|transcript|>`. Without these special tokens, we were
    unable to generate good summaries. Likely the model had trouble separating the
    prompt sections and the conversation elements because both used colons. Future
    large language models (LLMs) may be more resilient to delineation characters appearing
    multiple places in the prompt. This kind of small change can have a huge effect
    on LLM performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Either emphasizing instructions or examples in your prompts can work. Consider
    the following tradeoffs:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Control of the output*—Most LLMs are trained on summarization by default.
    Many are responsive to instructions and generate good summaries. One-shot and
    few-shot prompts further constrain the output to use the language you desire,
    though you may have to provide several examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost*—Adding examples increases the inference cost due to there being more
    input tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The text summaries we have generated so far include an overview of the conversation
    but mostly do not include the structured metadata that will be helpful for the
    agent. Earlier in the chapter, we demonstrated instrumenting your chatbot using
    a variety of code and low-code methods to gather structured metadata that could
    be passed to the agent. What if we don’t want to instrument our chatbot—could
    an LLM extract the structured metadata?
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.2 Generating a structured summary of a transcript with extractive prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can use LLMs to extract structured data from conversation transcripts—extraction
    is another task that many LLMs are trained on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first task is to decide what the structured output needs to look like.
    There are many possibilities, but one useful format is JSON. This is useful for
    two reasons: JSON is easy for downstream applications to consume, and many LLMs
    are good at generating JSON.'
  prefs: []
  type: TYPE_NORMAL
- en: We will again use an instructible model. This time we will use mistral-7b-instruct-
    v0-2 because it can generate JSON from instructions alone. The following listing
    shows a prompt that generates structured JSON output from the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.9 An extractive summary without examples
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Updated task description with simple instruction about JSON format'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Text version of chat transcript'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Updated cue to generate JSON'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Model output'
  prefs: []
  type: TYPE_NORMAL
- en: This is an excellent first attempt. The model generated JSON and extracted all
    the structured data points collected. But it collected more data than we asked
    for—we wanted only the IDs—and it duplicated one of the data points (the tax ID
    is the provider’s ID).
  prefs: []
  type: TYPE_NORMAL
- en: Note  Many models can generate JSON after seeing a few examples. All the models
    we tested extracted several data points rather than the three expected. The mistral
    model was one of the few to generate valid JSON with no examples in the prompt.
    We expect models to continue improving at generating JSON data. Alternatively,
    you can provide an example schema in your instruction.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s augment this prompt with an example (shown in bold).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.10 An extractive summary with one example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '****#1 Same task description'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 One-shot example'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Text version of chat transcript'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Same cue'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Model output****  ****With one example, we were able to show the model that
    we didn’t need every piece of data in the output. We also got the model to stop
    duplicating the provider’s tax ID. Last, the JSON response is now minifying to
    a single line without line breaks. The extracted data is still accurate, but we
    may require exact key names. If the agent’s application expects to read a field
    called `ClaimID`, then it is not acceptable for the summary to reference `ClaimNumber`.'
  prefs: []
  type: TYPE_NORMAL
- en: We give the model a more detailed example (shown in bold) in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.11 Updated one-shot example for extractive summary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '******#1 Same task description'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Updated one-shot example'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Text version of the chat transcript'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Same cue'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Model output******  ******This worked well. We only get the keys we desired.
    It’s slightly frustrating that the one-shot example needed to be so close to the
    second transcript. This implies that to summarize other conversational flows,
    we may need to provide examples for each one. (What if the conversation includes
    authorization IDs, electronic payment IDs, or other IDs?) And there’s one other
    gotcha: the `TaxID` and `MemberID` were numeric values, but it produced a string
    value for `ClaimID`—even after seeing the example.'
  prefs: []
  type: TYPE_NORMAL
- en: Testing for hallucinations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In our extractive summarization examples, we did not encounter any hallucinations,
    but this doesn’t mean they are impossible. Any summarization prompt should be
    tested on multiple inputs before it is deployed to see if it hallucinates. After
    it is deployed, you can detect hallucinations by verifying that each extracted
    value appeared in the transcript text.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go back to the previous one-shot example and instead add some instructions,
    shown in bold in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.12 An extractive summary with one example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**#1 Augmented task description'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Smaller one-shot example'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Text version of the chat transcript'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Same cue'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Model output**  **Voila! Exactly the output we desired. We were able to
    prompt an LLM to produce the structured summary we wanted using a combination
    of instructions and examples. We did not need to modify our assistant, aside from
    calling the LLM. That call is isolated to the component that handles transferring
    conversations to agents.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Review the ideal summary you created in section 12.1 for the example chat transcript.
    Use your favorite LLM (or your company’s preferred LLM) to generate a prose summary
    of that transcript. How close does the LLM get to your preferred summary? Did
    you use instructions, few-shot examples, or both?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat exercise 1, but also modify the model parameters. In this chapter, we
    used greedy decoding and a repetition penalty of 1.1\. Try sampling decoding,
    or try raising or lowering the penalty. Do you get better performance with different
    parameters? Does this match your expectations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Is it possible to generate the prose summary *and* to extract key details from
    the transcript in the same prompt? Design a prompt that generates this output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Invent a conversation that includes multiple claim searches. Create dialogue
    you imagine a bot and user would have if the user was calling to check on four
    total claims. Some of the claims are paid, some of the claims are still processing,
    and the last claim was denied. This conversation will have approximately two to
    four times as much content as the original sample conversation. What do you want
    this summary to look like? Once you have a target summary in mind, use an LLM,
    and try to generate that summary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '5\. Design an ideal summary for the following sample conversation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now build a prompt for your LLM to generate a similar summary.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Test your new prompt against the original sample conversation in listing
    12.1\. If necessary, refine the prompt so that it generates good summaries for
    both conversations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transfers to human agents are an inevitable part of many conversational AI solutions.
    Agents benefit from receiving brief summaries that extract key highlights from
    the conversation, both in prose and in structured formats.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A summary requires a conversational transcript. Most conversational AI platforms
    generate a transcript for you, but you can configure your conversational AI to
    generate one in your desired format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured summaries can be generated by enhancing your conversational AI to
    store key data points as they are collected, or they can be extracted using LLMs
    when the conversation is completed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to know what a good summary looks like before you ask an LLM to generate
    one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can generate prose summaries and extract key details from transcripts.
    Use clear instructions and examples to generate the summary you desire.************  ******#
    index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A
  prefs: []
  type: TYPE_NORMAL
- en: '[accuracy](../Text/chapter-4.html#p14)'
  prefs: []
  type: TYPE_NORMAL
- en: '[APIs (application programming interfaces)](../Text/chapter-1.html#p41)'
  prefs: []
  type: TYPE_NORMAL
- en: '[using](../Text/chapter-1.html#p59)'
  prefs: []
  type: TYPE_NORMAL
- en: AI (artificial intelligence)
  prefs: []
  type: TYPE_NORMAL
- en: '[traditional (classification-based)](../Text/chapter-4.html#p63), [2nd](../Text/chapter-4.html#p79)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Arize](../Text/chapter-6.html#p258)'
  prefs: []
  type: TYPE_NORMAL
- en: annotated logs
  prefs: []
  type: TYPE_NORMAL
- en: '[for traditional (classification-based) AI](../Text/chapter-4.html#p222)'
  prefs: []
  type: TYPE_NORMAL
- en: '[answer generation](../Text/chapter-6.html#p82)'
  prefs: []
  type: TYPE_NORMAL
- en: '[AOV (average order value)](../Text/chapter-3.html#p64)'
  prefs: []
  type: TYPE_NORMAL
- en: B
  prefs: []
  type: TYPE_NORMAL
- en: '[behavioral patterns](../Text/chapter-9.html#p70)'
  prefs: []
  type: TYPE_NORMAL
- en: '[blind testing](../Text/chapter-4.html#p64)'
  prefs: []
  type: TYPE_NORMAL
- en: C
  prefs: []
  type: TYPE_NORMAL
- en: '[conversation outcomes](../Text/chapter-3.html#p83), [2nd](../Text/chapter-3.html#p92),
    [3rd](../Text/chapter-3.html#p98), [4th](../Text/chapter-3.html#p140), [5th](../Text/chapter-3.html#p151)'
  prefs: []
  type: TYPE_NORMAL
- en: '[comparison check](../Text/chapter-6.html#p84)'
  prefs: []
  type: TYPE_NORMAL
- en: '[contained conversations, defined](../Text/chapter-3.html#p79)'
  prefs: []
  type: TYPE_NORMAL
- en: '[cross-functional teams](../Text/chapter-3.html#p28), [2nd](../Text/chapter-3.html#p41)'
  prefs: []
  type: TYPE_NORMAL
- en: '[complexity](../Text/chapter-8.html#p10), [2nd](../Text/chapter-8.html#p40)'
  prefs: []
  type: TYPE_NORMAL
- en: '[effect on business metrics](../Text/chapter-8.html#p25), [2nd](../Text/chapter-8.html#p35)'
  prefs: []
  type: TYPE_NORMAL
- en: '[effect on end user](../Text/chapter-8.html#p14)'
  prefs: []
  type: TYPE_NORMAL
- en: '[incremental cost and benefit of reducing for user](../Text/chapter-8.html#p37)'
  prefs: []
  type: TYPE_NORMAL
- en: '[continuous improvement](../Text/chapter-1.html#p129)'
  prefs: []
  type: TYPE_NORMAL
- en: context, importance of in virtual assistant performance
  prefs: []
  type: TYPE_NORMAL
- en: '[building trust and loyalty](../Text/chapter-9.html#p43)'
  prefs: []
  type: TYPE_NORMAL
- en: '[contextual information](../Text/chapter-9.html#p46), [2nd](../Text/chapter-9.html#p85)'
  prefs: []
  type: TYPE_NORMAL
- en: '[efficiency in problem solving](../Text/chapter-9.html#p31)'
  prefs: []
  type: TYPE_NORMAL
- en: '[enhanced relevance and accuracy](../Text/chapter-9.html#p21)'
  prefs: []
  type: TYPE_NORMAL
- en: '[influencing user interactions](../Text/chapter-9.html#p19), [2nd](../Text/chapter-9.html#p44)'
  prefs: []
  type: TYPE_NORMAL
- en: '[personalized experience](../Text/chapter-9.html#p26)'
  prefs: []
  type: TYPE_NORMAL
- en: '[proactive support](../Text/chapter-9.html#p40)'
  prefs: []
  type: TYPE_NORMAL
- en: '[CohereEmbeddings](../Text/chapter-6.html#p180)'
  prefs: []
  type: TYPE_NORMAL
- en: '[conversational summarization](../Text/chapter-12.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[call center agents](../Text/chapter-2.html#p102)'
  prefs: []
  type: TYPE_NORMAL
- en: '[classification models](../Text/chapter-4.html#p172)'
  prefs: []
  type: TYPE_NORMAL
- en: '[conversational AI](../Text/chapter-1.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[benefits of](../Text/chapter-1.html#p20)'
  prefs: []
  type: TYPE_NORMAL
- en: '[building](../Text/chapter-1.html#p42), [2nd](../Text/chapter-2.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[continuous improvement](../Text/chapter-1.html#p130), [2nd](../Text/chapter-1.html#p188)'
  prefs: []
  type: TYPE_NORMAL
- en: '[defined](../Text/chapter-1.html#p10)'
  prefs: []
  type: TYPE_NORMAL
- en: '[how it works](../Text/chapter-1.html#p28)'
  prefs: []
  type: TYPE_NORMAL
- en: '[responding to users with generative AI](../Text/chapter-2.html#p131), [2nd](../Text/chapter-2.html#p168)'
  prefs: []
  type: TYPE_NORMAL
- en: '[software platforms](../Text/chapter-1.html#p190)'
  prefs: []
  type: TYPE_NORMAL
- en: '[understanding users](../Text/chapter-4.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[confusion matrix](../Text/chapter-5.html#p42), [2nd](../Text/chapter-5.html#p152)'
  prefs: []
  type: TYPE_NORMAL
- en: '[context variables](../Text/chapter-12.html#p87)'
  prefs: []
  type: TYPE_NORMAL
- en: '[complex flows](../Text/chapter-8.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[commercial cloud platform](../Text/chapter-1.html#p194)'
  prefs: []
  type: TYPE_NORMAL
- en: D
  prefs: []
  type: TYPE_NORMAL
- en: '[DTMF (dual tone multi-frequency)](../Text/chapter-12.html#p72)'
  prefs: []
  type: TYPE_NORMAL
- en: '[device type](../Text/chapter-9.html#p60)'
  prefs: []
  type: TYPE_NORMAL
- en: '[decoder-only architectures](../Text/chapter-4.html#p43)'
  prefs: []
  type: TYPE_NORMAL
- en: '[document transformers](../Text/chapter-6.html#p155), [2nd](../Text/chapter-6.html#p166)'
  prefs: []
  type: TYPE_NORMAL
- en: '[disambiguation feature](../Text/chapter-4.html#p173)'
  prefs: []
  type: TYPE_NORMAL
- en: '[document loaders](../Text/chapter-6.html#p154)'
  prefs: []
  type: TYPE_NORMAL
- en: '[direct question](../Text/chapter-7.html#p103)'
  prefs: []
  type: TYPE_NORMAL
- en: '[decoding_method](../Text/chapter-2.html#p163)'
  prefs: []
  type: TYPE_NORMAL
- en: '[DirectoryLoader](../Text/chapter-6.html#p163)'
  prefs: []
  type: TYPE_NORMAL
- en: E
  prefs: []
  type: TYPE_NORMAL
- en: '[encoder-only architectures](../Text/chapter-4.html#p42)'
  prefs: []
  type: TYPE_NORMAL
- en: '[encoder-decoder model architecture](../Text/chapter-4.html#p44)'
  prefs: []
  type: TYPE_NORMAL
- en: '[embedding generation](../Text/chapter-6.html#p122)'
  prefs: []
  type: TYPE_NORMAL
- en: F
  prefs: []
  type: TYPE_NORMAL
- en: '[free-text summary elements](../Text/chapter-12.html#p37)'
  prefs: []
  type: TYPE_NORMAL
- en: '[FaithfulnessEvaluator](../Text/chapter-6.html#p243)'
  prefs: []
  type: TYPE_NORMAL
- en: '[FAISS (Facebook AI Similarity Search)](../Text/chapter-6.html#p192)'
  prefs: []
  type: TYPE_NORMAL
- en: '[FAQ (frequently asked question) bots](../Text/chapter-1.html#p67), [2nd](../Text/chapter-2.html#p6)'
  prefs: []
  type: TYPE_NORMAL
- en: '[dynamic question and answering](../Text/chapter-2.html#p72)'
  prefs: []
  type: TYPE_NORMAL
- en: '[foundations of](../Text/chapter-2.html#p13)'
  prefs: []
  type: TYPE_NORMAL
- en: '[static question and answering](../Text/chapter-2.html#p32), [2nd](../Text/chapter-2.html#p70)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Flan-ul2 model](../Text/chapter-1.html#p116)'
  prefs: []
  type: TYPE_NORMAL
- en: G
  prefs: []
  type: TYPE_NORMAL
- en: golden test set
  prefs: []
  type: TYPE_NORMAL
- en: '[annotating for generative AI](../Text/chapter-4.html#p209)'
  prefs: []
  type: TYPE_NORMAL
- en: '[annotating for traditional (classifier-based) AI](../Text/chapter-4.html#p197)'
  prefs: []
  type: TYPE_NORMAL
- en: '[grammatical variations, generating new](../Text/chapter-7.html#p86), [2nd](../Text/chapter-7.html#p112)'
  prefs: []
  type: TYPE_NORMAL
- en: '[golden intent](../Text/chapter-5.html#p79)'
  prefs: []
  type: TYPE_NORMAL
- en: '[generation metrics](../Text/chapter-6.html#p238), [2nd](../Text/chapter-6.html#p244)'
  prefs: []
  type: TYPE_NORMAL
- en: '[generative AI (artificial intelligence)](../Text/chapter-1.html#p73), [2nd](../Text/chapter-4.html#p81),
    [3rd](../Text/chapter-10.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[annotated logs for](../Text/chapter-4.html#p229)'
  prefs: []
  type: TYPE_NORMAL
- en: '[augmenting intent data with](../Text/chapter-7.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[defined](../Text/chapter-1.html#p81)'
  prefs: []
  type: TYPE_NORMAL
- en: '[effectively using](../Text/chapter-1.html#p111), [2nd](../Text/chapter-1.html#p125)'
  prefs: []
  type: TYPE_NORMAL
- en: '[guardrails](../Text/chapter-1.html#p90), [2nd](../Text/chapter-1.html#p109)'
  prefs: []
  type: TYPE_NORMAL
- en: '[improving summarization with](../Text/chapter-12.html#p105)'
  prefs: []
  type: TYPE_NORMAL
- en: '[model platform](../Text/chapter-1.html#p192)'
  prefs: []
  type: TYPE_NORMAL
- en: '[generated_text](../Text/chapter-2.html#p166)'
  prefs: []
  type: TYPE_NORMAL
- en: '[granite-13b-instruct-v2 model](../Text/chapter-12.html#p120)'
  prefs: []
  type: TYPE_NORMAL
- en: H
  prefs: []
  type: TYPE_NORMAL
- en: '[human in the loop](../Text/chapter-1.html#p106)'
  prefs: []
  type: TYPE_NORMAL
- en: '[hallucinations](../Text/chapter-1.html#p87), [2nd](../Text/chapter-6.html#p139)'
  prefs: []
  type: TYPE_NORMAL
- en: I
  prefs: []
  type: TYPE_NORMAL
- en: '[IVR (interactive voice response)](../Text/chapter-2.html#p104), [2nd](../Text/chapter-11.html#p15)'
  prefs: []
  type: TYPE_NORMAL
- en: intent data
  prefs: []
  type: TYPE_NORMAL
- en: '[augmenting with generative](../Text/chapter-7.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[improvement planning](../Text/chapter-3.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[developing and delivering fixes](../Text/chapter-3.html#p230), [2nd](../Text/chapter-3.html#p235)'
  prefs: []
  type: TYPE_NORMAL
- en: '[driving to same goal](../Text/chapter-3.html#p46), [2nd](../Text/chapter-3.html#p141)'
  prefs: []
  type: TYPE_NORMAL
- en: '[indexing metrics](../Text/chapter-6.html#p214), [2nd](../Text/chapter-6.html#p221)'
  prefs: []
  type: TYPE_NORMAL
- en: '[intents](../Text/chapter-2.html#p33)'
  prefs: []
  type: TYPE_NORMAL
- en: '[hardening existing intents](../Text/chapter-7.html#p4), [2nd](../Text/chapter-7.html#p149)'
  prefs: []
  type: TYPE_NORMAL
- en: immediate opt-outs
  prefs: []
  type: TYPE_NORMAL
- en: '[allowing user to opt in](../Text/chapter-11.html#p79)'
  prefs: []
  type: TYPE_NORMAL
- en: '[conveying capabilities and setting expectations](../Text/chapter-11.html#p68)'
  prefs: []
  type: TYPE_NORMAL
- en: '[starting with great experience](../Text/chapter-11.html#p56), [2nd](../Text/chapter-11.html#p66)'
  prefs: []
  type: TYPE_NORMAL
- en: improvement
  prefs: []
  type: TYPE_NORMAL
- en: '[identifying and resolving problems](../Text/chapter-3.html#p153), [2nd](../Text/chapter-3.html#p218)'
  prefs: []
  type: TYPE_NORMAL
- en: '[recognizing need for](../Text/chapter-3.html#p13)'
  prefs: []
  type: TYPE_NORMAL
- en: K
  prefs: []
  type: TYPE_NORMAL
- en: '[KPIs (key performance indicators)](../Text/chapter-3.html#p23)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Kanban board](../Text/chapter-3.html#p239)'
  prefs: []
  type: TYPE_NORMAL
- en: '[k-fold](../Text/chapter-5.html#p127)'
  prefs: []
  type: TYPE_NORMAL
- en: '[cross validation](../Text/chapter-4.html#p66)'
  prefs: []
  type: TYPE_NORMAL
- en: '[testing](../Text/chapter-5.html#p60)'
  prefs: []
  type: TYPE_NORMAL
- en: L
  prefs: []
  type: TYPE_NORMAL
- en: logs, obtaining and preparing test data from
  prefs: []
  type: TYPE_NORMAL
- en: '[preparing and scrubbing data for use in iterative improvements](../Text/chapter-4.html#p191)'
  prefs: []
  type: TYPE_NORMAL
- en: '[LLMs (large language models)](../Text/chapter-1.html#p68), [2nd](../Text/chapter-2.html#p125),
    [3rd](../Text/chapter-6.html#p60), [4th](../Text/chapter-7.html#p11), [5th](../Text/chapter-7.html#p51),
    [6th](../Text/chapter-10.html#p9)'
  prefs: []
  type: TYPE_NORMAL
- en: '[augmenting intent data with](../Text/chapter-7.html#p151), [2nd](../Text/chapter-7.html#p159)'
  prefs: []
  type: TYPE_NORMAL
- en: '[integrating with](../Text/chapter-2.html#p136)'
  prefs: []
  type: TYPE_NORMAL
- en: '[pros and cons of](../Text/chapter-7.html#p21)'
  prefs: []
  type: TYPE_NORMAL
- en: '[requirements for](../Text/chapter-7.html#p29)'
  prefs: []
  type: TYPE_NORMAL
- en: '[using augmented data](../Text/chapter-7.html#p39)'
  prefs: []
  type: TYPE_NORMAL
- en: '[load method](../Text/chapter-6.html#p158)'
  prefs: []
  type: TYPE_NORMAL
- en: M
  prefs: []
  type: TYPE_NORMAL
- en: '[modality](../Text/chapter-9.html#p81)'
  prefs: []
  type: TYPE_NORMAL
- en: '[comparing modalities](../Text/chapter-9.html#p95)'
  prefs: []
  type: TYPE_NORMAL
- en: '[examples of how modality affects user experience](../Text/chapter-9.html#p111),
    [2nd](../Text/chapter-9.html#p119)'
  prefs: []
  type: TYPE_NORMAL
- en: '[importance in designing virtual assistant flows](../Text/chapter-9.html#p104)'
  prefs: []
  type: TYPE_NORMAL
- en: messages
  prefs: []
  type: TYPE_NORMAL
- en: '[error messages](../Text/chapter-11.html#p165), [2nd](../Text/chapter-11.html#p182)'
  prefs: []
  type: TYPE_NORMAL
- en: '[greeting messages](../Text/chapter-11.html#p184), [2nd](../Text/chapter-11.html#p209)'
  prefs: []
  type: TYPE_NORMAL
- en: '[metadata, summary elements](../Text/chapter-12.html#p21)'
  prefs: []
  type: TYPE_NORMAL
- en: '[meaning, extracting](../Text/chapter-1.html#p55)'
  prefs: []
  type: TYPE_NORMAL
- en: '[min_tokens](../Text/chapter-2.html#p162)'
  prefs: []
  type: TYPE_NORMAL
- en: '[models, selection](../Text/chapter-1.html#p117)'
  prefs: []
  type: TYPE_NORMAL
- en: N
  prefs: []
  type: TYPE_NORMAL
- en: '[NPS (net promoter score)](../Text/chapter-3.html#p123)'
  prefs: []
  type: TYPE_NORMAL
- en: '[NDCG (Normalized Discounted Cumulative Gain)](../Text/chapter-6.html#p236)'
  prefs: []
  type: TYPE_NORMAL
- en: '[NLU (natural language understanding)](../Text/chapter-6.html#p36)'
  prefs: []
  type: TYPE_NORMAL
- en: '[NLP (natural language processing)](../Text/chapter-9.html#p168)'
  prefs: []
  type: TYPE_NORMAL
- en: O
  prefs: []
  type: TYPE_NORMAL
- en: '[OpenAIEmbeddings class](../Text/chapter-6.html#p178)'
  prefs: []
  type: TYPE_NORMAL
- en: '[opt-outs](../Text/chapter-11.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[drivers of](../Text/chapter-11.html#p11), [2nd](../Text/chapter-11.html#p46)'
  prefs: []
  type: TYPE_NORMAL
- en: '[escalation](../Text/chapter-11.html#p211)'
  prefs: []
  type: TYPE_NORMAL
- en: '[gathering data on opt-out behavior](../Text/chapter-11.html#p37)'
  prefs: []
  type: TYPE_NORMAL
- en: '[immediate](../Text/chapter-11.html#p52), [2nd](../Text/chapter-11.html#p82)'
  prefs: []
  type: TYPE_NORMAL
- en: '[improving dialogue with generative AI](../Text/chapter-11.html#p162)'
  prefs: []
  type: TYPE_NORMAL
- en: '[reducing](../Text/chapter-11.html#p90), [2nd](../Text/chapter-11.html#p121)'
  prefs: []
  type: TYPE_NORMAL
- en: '[retention](../Text/chapter-11.html#p131), [2nd](../Text/chapter-11.html#p156)'
  prefs: []
  type: TYPE_NORMAL
- en: '[orchestration layer](../Text/chapter-12.html#p65)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Ollama](../Text/chapter-7.html#p62)'
  prefs: []
  type: TYPE_NORMAL
- en: '[one-shot prompting](../Text/chapter-4.html#p48)'
  prefs: []
  type: TYPE_NORMAL
- en: P
  prefs: []
  type: TYPE_NORMAL
- en: '[persistent user history](../Text/chapter-9.html#p38)'
  prefs: []
  type: TYPE_NORMAL
- en: '[prompts](../Text/chapter-1.html#p98)'
  prefs: []
  type: TYPE_NORMAL
- en: '[prompt engineering](../Text/chapter-4.html#p47)'
  prefs: []
  type: TYPE_NORMAL
- en: '[parameter tuning](../Text/chapter-4.html#p49)'
  prefs: []
  type: TYPE_NORMAL
- en: '[postfiltering output](../Text/chapter-1.html#p103)'
  prefs: []
  type: TYPE_NORMAL
- en: '[previous interactions](../Text/chapter-9.html#p75)'
  prefs: []
  type: TYPE_NORMAL
- en: '[process-oriented bots](../Text/chapter-2.html#p93)'
  prefs: []
  type: TYPE_NORMAL
- en: '[precision and recall, improving for multiple intents](../Text/chapter-5.html#p118)'
  prefs: []
  type: TYPE_NORMAL
- en: '[precision](../Text/chapter-5.html#p23)'
  prefs: []
  type: TYPE_NORMAL
- en: '[process-oriented or transactional solutions](../Text/chapter-1.html#p12)'
  prefs: []
  type: TYPE_NORMAL
- en: '[passage retrieval](../Text/chapter-6.html#p78)'
  prefs: []
  type: TYPE_NORMAL
- en: '[preprocessing data](../Text/chapter-6.html#p121)'
  prefs: []
  type: TYPE_NORMAL
- en: '[prompt stuffing](../Text/chapter-6.html#p167)'
  prefs: []
  type: TYPE_NORMAL
- en: '[prefiltering input](../Text/chapter-1.html#p96)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PII (personal identifiable information)](../Text/chapter-4.html#p129)'
  prefs: []
  type: TYPE_NORMAL
- en: process flows
  prefs: []
  type: TYPE_NORMAL
- en: '[AI-assisted at build time](../Text/chapter-10.html#p12), [2nd](../Text/chapter-10.html#p53)'
  prefs: []
  type: TYPE_NORMAL
- en: '[AI-assisted at run time](../Text/chapter-10.html#p59)'
  prefs: []
  type: TYPE_NORMAL
- en: '[executing dialogue flows with generative AI](../Text/chapter-10.html#p62),
    [2nd](../Text/chapter-10.html#p80)'
  prefs: []
  type: TYPE_NORMAL
- en: Q
  prefs: []
  type: TYPE_NORMAL
- en: '[qualitative problem exploration](../Text/chapter-3.html#p167)'
  prefs: []
  type: TYPE_NORMAL
- en: '[QPS (queries per second)](../Text/chapter-6.html#p217)'
  prefs: []
  type: TYPE_NORMAL
- en: '[question-answering](../Text/chapter-1.html#p11), [2nd](../Text/chapter-4.html#p51)'
  prefs: []
  type: TYPE_NORMAL
- en: '[quantitative evaluation for issue discovery](../Text/chapter-3.html#p174)'
  prefs: []
  type: TYPE_NORMAL
- en: R
  prefs: []
  type: TYPE_NORMAL
- en: '[routing agents](../Text/chapter-1.html#p13), [2nd](../Text/chapter-2.html#p97)'
  prefs: []
  type: TYPE_NORMAL
- en: '[retrievers](../Text/chapter-6.html#p156)'
  prefs: []
  type: TYPE_NORMAL
- en: '[recall](../Text/chapter-5.html#p15)'
  prefs: []
  type: TYPE_NORMAL
- en: '[RAGAS, defined](../Text/chapter-6.html#p259)'
  prefs: []
  type: TYPE_NORMAL
- en: '[retrieval and matching at runtime](../Text/chapter-6.html#p124)'
  prefs: []
  type: TYPE_NORMAL
- en: '[RAG (retrieval-augmented generation)](../Text/chapter-1.html#p76), [2nd](../Text/chapter-3.html#p136),
    [3rd](../Text/chapter-4.html#p50), [4th](../Text/chapter-5.html#p162), [5th](../Text/chapter-6.html#p1),
    [6th](../Text/chapter-6.html#p19), [7th](../Text/chapter-6.html#p51), [8th](../Text/chapter-6.html#p98)'
  prefs: []
  type: TYPE_NORMAL
- en: '[additional considerations](../Text/chapter-6.html#p134), [2nd](../Text/chapter-6.html#p205)'
  prefs: []
  type: TYPE_NORMAL
- en: '[benefits of](../Text/chapter-6.html#p63), [2nd](../Text/chapter-6.html#p88)'
  prefs: []
  type: TYPE_NORMAL
- en: '[combining with other generative AI use cases](../Text/chapter-6.html#p90)'
  prefs: []
  type: TYPE_NORMAL
- en: '[comparing intents, search, and RAG approaches](../Text/chapter-6.html#p96)'
  prefs: []
  type: TYPE_NORMAL
- en: '[designing adaptive flows with](../Text/chapter-9.html#p137), [2nd](../Text/chapter-9.html#p153)'
  prefs: []
  type: TYPE_NORMAL
- en: '[evaluating and analyzing performance](../Text/chapter-6.html#p207)'
  prefs: []
  type: TYPE_NORMAL
- en: '[implementation of](../Text/chapter-6.html#p103), [2nd](../Text/chapter-6.html#p129)'
  prefs: []
  type: TYPE_NORMAL
- en: '[in conversational AI](../Text/chapter-6.html#p53)'
  prefs: []
  type: TYPE_NORMAL
- en: '[maintaining and updating adaptive flows](../Text/chapter-9.html#p166)'
  prefs: []
  type: TYPE_NORMAL
- en: '[retrieving and generating contextually relevant responses](../Text/chapter-9.html#p155)'
  prefs: []
  type: TYPE_NORMAL
- en: '[routing requests to LLMs](../Text/chapter-2.html#p153)'
  prefs: []
  type: TYPE_NORMAL
- en: '[retrieval metrics](../Text/chapter-6.html#p223)'
  prefs: []
  type: TYPE_NORMAL
- en: S
  prefs: []
  type: TYPE_NORMAL
- en: '[slot filling](../Text/chapter-8.html#p73)'
  prefs: []
  type: TYPE_NORMAL
- en: self-service
  prefs: []
  type: TYPE_NORMAL
- en: '[incentivizing](../Text/chapter-11.html#p73)'
  prefs: []
  type: TYPE_NORMAL
- en: '[storage in vector database](../Text/chapter-6.html#p123)'
  prefs: []
  type: TYPE_NORMAL
- en: '[session history](../Text/chapter-9.html#p37)'
  prefs: []
  type: TYPE_NORMAL
- en: '[session variables](../Text/chapter-12.html#p55)'
  prefs: []
  type: TYPE_NORMAL
- en: '[SSA (Sensibleness and Specificity Average)](../Text/chapter-6.html#p242)'
  prefs: []
  type: TYPE_NORMAL
- en: '[SMEs (subject matter experts)](../Text/chapter-3.html#p35), [2nd](../Text/chapter-5.html#p66),
    [3rd](../Text/chapter-10.html#p52)'
  prefs: []
  type: TYPE_NORMAL
- en: '[search, role of in conversational AI](../Text/chapter-6.html#p12), [2nd](../Text/chapter-6.html#p49)'
  prefs: []
  type: TYPE_NORMAL
- en: '[benefits of traditional search](../Text/chapter-6.html#p31)'
  prefs: []
  type: TYPE_NORMAL
- en: '[drawbacks of traditional search](../Text/chapter-6.html#p39)'
  prefs: []
  type: TYPE_NORMAL
- en: '[using search in conversational AI](../Text/chapter-6.html#p24)'
  prefs: []
  type: TYPE_NORMAL
- en: '[search processes, using LLM for](../Text/chapter-10.html#p82), [2nd](../Text/chapter-10.html#p106)'
  prefs: []
  type: TYPE_NORMAL
- en: '[support resources](../Text/chapter-4.html#p17)'
  prefs: []
  type: TYPE_NORMAL
- en: '[screen pop](../Text/chapter-12.html#p33)'
  prefs: []
  type: TYPE_NORMAL
- en: '[synonyms, generating](../Text/chapter-7.html#p59), [2nd](../Text/chapter-7.html#p84)'
  prefs: []
  type: TYPE_NORMAL
- en: '[sensitive data](../Text/chapter-12.html#p94)'
  prefs: []
  type: TYPE_NORMAL
- en: summarization
  prefs: []
  type: TYPE_NORMAL
- en: '[elements of effective](../Text/chapter-12.html#p19), [2nd](../Text/chapter-12.html#p45)'
  prefs: []
  type: TYPE_NORMAL
- en: '[improving with generative AI](../Text/chapter-12.html#p109), [2nd](../Text/chapter-12.html#p155)'
  prefs: []
  type: TYPE_NORMAL
- en: '[need for](../Text/chapter-12.html#p11)'
  prefs: []
  type: TYPE_NORMAL
- en: '[overview of](../Text/chapter-12.html#p9)'
  prefs: []
  type: TYPE_NORMAL
- en: '[preparing chatbot for](../Text/chapter-12.html#p53), [2nd](../Text/chapter-12.html#p101)'
  prefs: []
  type: TYPE_NORMAL
- en: '[sampling decoding](../Text/chapter-7.html#p74)'
  prefs: []
  type: TYPE_NORMAL
- en: '[solutioning](../Text/chapter-3.html#p200)'
  prefs: []
  type: TYPE_NORMAL
- en: '[sprint planning](../Text/chapter-3.html#p232)'
  prefs: []
  type: TYPE_NORMAL
- en: T
  prefs: []
  type: TYPE_NORMAL
- en: '[traditional search](../Text/chapter-6.html#p18)'
  prefs: []
  type: TYPE_NORMAL
- en: '[transitioning from routing agents to process-oriented bots](../Text/chapter-2.html#p110)'
  prefs: []
  type: TYPE_NORMAL
- en: '[transfer decision](../Text/chapter-6.html#p85)'
  prefs: []
  type: TYPE_NORMAL
- en: traditional AI
  prefs: []
  type: TYPE_NORMAL
- en: '[improving weak understanding for](../Text/chapter-5.html#p1), [2nd](../Text/chapter-5.html#p180)'
  prefs: []
  type: TYPE_NORMAL
- en: '[templates, creating examples with](../Text/chapter-7.html#p135), [2nd](../Text/chapter-7.html#p146)'
  prefs: []
  type: TYPE_NORMAL
- en: '[test data, obtaining and preparing from logs](../Text/chapter-4.html#p127)'
  prefs: []
  type: TYPE_NORMAL
- en: '[annotation process](../Text/chapter-4.html#p195), [2nd](../Text/chapter-4.html#p211)'
  prefs: []
  type: TYPE_NORMAL
- en: '[guidelines for identifying candidate test utterances](../Text/chapter-4.html#p136),
    [2nd](../Text/chapter-4.html#p181)'
  prefs: []
  type: TYPE_NORMAL
- en: '[obtaining production logs](../Text/chapter-4.html#p131)'
  prefs: []
  type: TYPE_NORMAL
- en: '[preparing and scrubbing data for use in iterative improvements](../Text/chapter-4.html#p183),
    [2nd](../Text/chapter-4.html#p193)'
  prefs: []
  type: TYPE_NORMAL
- en: '[triaging issues](../Text/chapter-3.html#p183), [2nd](../Text/chapter-3.html#p198)'
  prefs: []
  type: TYPE_NORMAL
- en: '[test time, AI-assisted flows at](../Text/chapter-10.html#p111)'
  prefs: []
  type: TYPE_NORMAL
- en: '[setting up conversational test](../Text/chapter-10.html#p139), [2nd](../Text/chapter-10.html#p153)'
  prefs: []
  type: TYPE_NORMAL
- en: '[setting up generative AI to be user](../Text/chapter-10.html#p116), [2nd](../Text/chapter-10.html#p137)'
  prefs: []
  type: TYPE_NORMAL
- en: '[time zone](../Text/chapter-9.html#p54)'
  prefs: []
  type: TYPE_NORMAL
- en: '[true negatives](../Text/chapter-5.html#p33)'
  prefs: []
  type: TYPE_NORMAL
- en: '[traditional (classification-based) AI solution, assessing](../Text/chapter-4.html#p103)'
  prefs: []
  type: TYPE_NORMAL
- en: '[training data, selection](../Text/chapter-1.html#p92)'
  prefs: []
  type: TYPE_NORMAL
- en: U
  prefs: []
  type: TYPE_NORMAL
- en: '[user location](../Text/chapter-9.html#p48)'
  prefs: []
  type: TYPE_NORMAL
- en: understanding
  prefs: []
  type: TYPE_NORMAL
- en: '[achieving with generative AI](../Text/chapter-4.html#p38), [2nd](../Text/chapter-4.html#p52)'
  prefs: []
  type: TYPE_NORMAL
- en: '[achieving with traditional conversational AI](../Text/chapter-4.html#p29)'
  prefs: []
  type: TYPE_NORMAL
- en: '[annotated logs](../Text/chapter-4.html#p220), [2nd](../Text/chapter-4.html#p242)'
  prefs: []
  type: TYPE_NORMAL
- en: '[fundamentals of](../Text/chapter-4.html#p37)'
  prefs: []
  type: TYPE_NORMAL
- en: '[iterative improvement](../Text/chapter-4.html#p232)'
  prefs: []
  type: TYPE_NORMAL
- en: '[measuring](../Text/chapter-4.html#p61), [2nd](../Text/chapter-4.html#p95)'
  prefs: []
  type: TYPE_NORMAL
- en: '[weak](../Text/chapter-4.html#p12), [2nd](../Text/chapter-4.html#p20)'
  prefs: []
  type: TYPE_NORMAL
- en: user journeys
  prefs: []
  type: TYPE_NORMAL
- en: '[aligning with user’s mental model](../Text/chapter-8.html#p71)'
  prefs: []
  type: TYPE_NORMAL
- en: '[allowing flexibility in expected user responses](../Text/chapter-8.html#p77),
    [2nd](../Text/chapter-8.html#p89)'
  prefs: []
  type: TYPE_NORMAL
- en: '[spotting complex dialogue flows](../Text/chapter-8.html#p49)'
  prefs: []
  type: TYPE_NORMAL
- en: '[supporting self-service task flows with API/backend processes](../Text/chapter-8.html#p92)'
  prefs: []
  type: TYPE_NORMAL
- en: '[using what is known about user](../Text/chapter-8.html#p61)'
  prefs: []
  type: TYPE_NORMAL
- en: users
  prefs: []
  type: TYPE_NORMAL
- en: '[assessing where you are today](../Text/chapter-4.html#p101), [2nd](../Text/chapter-4.html#p115)'
  prefs: []
  type: TYPE_NORMAL
- en: '[understanding](../Text/chapter-4.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[user preferences](../Text/chapter-9.html#p65)'
  prefs: []
  type: TYPE_NORMAL
- en: V
  prefs: []
  type: TYPE_NORMAL
- en: '[verb phrases](../Text/chapter-7.html#p71)'
  prefs: []
  type: TYPE_NORMAL
- en: virtual assistants
  prefs: []
  type: TYPE_NORMAL
- en: '[enhancing context awareness and improving overall user experience with RAG](../Text/chapter-9.html#p134),
    [2nd](../Text/chapter-9.html#p170)'
  prefs: []
  type: TYPE_NORMAL
- en: '[importance of context in performance](../Text/chapter-9.html#p11)'
  prefs: []
  type: TYPE_NORMAL
- en: '[modelities](../Text/chapter-9.html#p91), [2nd](../Text/chapter-9.html#p129)'
  prefs: []
  type: TYPE_NORMAL
- en: '[voice bots, design considerations](../Text/chapter-9.html#p121)'
  prefs: []
  type: TYPE_NORMAL
- en: '[voice solutions, accommodating](../Text/chapter-11.html#p104)'
  prefs: []
  type: TYPE_NORMAL
- en: W
  prefs: []
  type: TYPE_NORMAL
- en: weak understanding
  prefs: []
  type: TYPE_NORMAL
- en: '[establishing baseline](../Text/chapter-5.html#p58)'
  prefs: []
  type: TYPE_NORMAL
- en: '[identifying biggest problems](../Text/chapter-5.html#p50)'
  prefs: []
  type: TYPE_NORMAL
- en: '[identifying problematic patterns in misunderstood utterances](../Text/chapter-5.html#p13),
    [2nd](../Text/chapter-5.html#p37)'
  prefs: []
  type: TYPE_NORMAL
- en: '[improvement plan](../Text/chapter-5.html#p9)'
  prefs: []
  type: TYPE_NORMAL
- en: '[improving F1 score for one intent](../Text/chapter-5.html#p112)'
  prefs: []
  type: TYPE_NORMAL
- en: '[improving for traditional AI](../Text/chapter-5.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[improving precision for one intent](../Text/chapter-5.html#p94)'
  prefs: []
  type: TYPE_NORMAL
- en: '[improving recall for one intent](../Text/chapter-5.html#p76)'
  prefs: []
  type: TYPE_NORMAL
- en: '[incremental improvements](../Text/chapter-5.html#p48)'
  prefs: []
  type: TYPE_NORMAL
- en: '[solving](../Text/chapter-5.html#p139), [2nd](../Text/chapter-5.html#p142),
    [3rd](../Text/chapter-5.html#p154), [4th](../Text/chapter-5.html#p156)'
  prefs: []
  type: TYPE_NORMAL
- en: '[traditional AI](../Text/chapter-5.html#p168), [2nd](../Text/chapter-5.html#p171),
    [3rd](../Text/chapter-5.html#p176)'
  prefs: []
  type: TYPE_NORMAL
- en: '[validating initial training strategy](../Text/chapter-5.html#p65)'
  prefs: []
  type: TYPE_NORMAL
- en: '[wrong intent matched](../Text/chapter-5.html#p74)'
  prefs: []
  type: TYPE_NORMAL
- en: Z
  prefs: []
  type: TYPE_NORMAL
- en: '[zero-shot prompting](../Text/chapter-7.html#p99)******'
  prefs: []
  type: TYPE_NORMAL
