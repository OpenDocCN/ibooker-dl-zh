- en: Chapter 17\. Speeding Up Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 17 章\. 加速变压器
- en: In Chapters [15](ch15.html#transformer_chapter) and [16](ch16.html#vit_chapter),
    we built all kinds of transformers, from classifiers, translators and chatbots,
    to vision and multimodal transformers. While transformers are incredibly versatile
    and powerful, they are far from perfect. In particular, they can be very slow,
    especially when processing long input sequences.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [15](ch15.html#transformer_chapter) 章和第 [16](ch16.html#vit_chapter) 章中，我们构建了各种变压器，从分类器、翻译器和聊天机器人，到视觉和多模态变压器。虽然变压器非常灵活且强大，但它们远非完美。特别是，它们在处理长输入序列时可能会非常慢。
- en: 'Luckily, many techniques have been developed to speed up transformers of any
    size:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，已经开发出许多技术来加速任何规模的变压器：
- en: To speed up decoding in generative transformers, we will use key/value caching
    and speculative decoding, then we will take of a quick look at several approaches
    to parallelize text generation.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了加速生成型变压器的解码，我们将使用键/值缓存和推测性解码，然后我们将简要介绍几种并行化文本生成的方案。
- en: To accelerate multi-head attention (MHA), which is one of the most computationally
    expensive components of transformers, we will look at sparse attention, approximate
    attention, sharing projections, and FlashAttention.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了加速多头注意力（MHA），这是变压器中最计算密集的组件之一，我们将探讨稀疏注意力、近似注意力、共享投影和FlashAttention。
- en: To speed up gigantic transformers of up to trillions of parameters, we will
    discuss mixture of experts (MoE).
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了加速具有高达万亿参数的巨型变压器，我们将讨论专家混合（MoE）。
- en: To train large transformers efficiently, we will discuss parameter-efficient
    fine-tuning (PEFT) using adapters such as Low-Rank Adaptation (LoRA), activation
    checkpointing, sequence packing, gradient accumulation, and parallelism.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了高效训练大型变压器，我们将讨论使用如低秩适应（LoRA）等适配器的参数高效微调（PEFT），激活检查点、序列打包、梯度累积和并行性。
- en: Tip
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Another way to speed up a transformer is to make it smaller. This can be done
    using reduced precision and quantization, which are discussed in [Appendix B](app02.html#precision_appendix).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 加速变压器的另一种方法是使其变得更小。这可以通过使用降低精度和量化来实现，这些内容在[附录 B](app02.html#precision_appendix)中有讨论。
- en: That’s quite a lot of techniques to cover, and they are fairly advanced, so
    you can safely skip this chapter for now if you are new to transformers, and come
    back later whenever needed. This is why this chapter is online-only, available
    at [*https://homl.info*](https://homl.info), to leave room for the other chapters.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 需要介绍的技术非常多，而且相当高级，所以如果你是初学者，现在可以安全地跳过这一章，并在需要时再回来。这也是为什么这一章仅在线上提供，可在[*https://homl.info*](https://homl.info)找到，为其他章节留出空间。
