- en: Chapter 3\. Getting Up to Speed on Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are few areas in technology with the mystique that surrounds machine learning
    and artificial intelligence (AI). Even if you’re an experienced engineer in another
    domain, machine learning can seem like a dense subject with a mountain of assumed
    knowledge requirements. Many developers feel discouraged when they begin to read
    about ML and encounter explanations that invoke academic papers, obscure Python
    libraries, and advanced mathematics. It can feel daunting to even know where to
    start.
  prefs: []
  type: TYPE_NORMAL
- en: In reality, machine learning can be simple to understand and is accessible to
    anyone with a text editor. After you learn a few key ideas, you can easily use
    it in your own projects. Beneath all the mystique is a handy set of tools for
    solving various types of problems. It might sometimes *feel* like magic, but it’s
    all just code, and you don’t need a PhD to work with it.
  prefs: []
  type: TYPE_NORMAL
- en: This book is about using machine learning with tiny devices. In the rest of
    this chapter, you’ll learn all the ML you need to get started. We’ll cover the
    basic concepts, explore some tools, and train a simple machine learning model.
    Our focus is tiny hardware, so we won’t spend long on the theory behind deep learning,
    or the mathematics that makes it all work. Later chapters will dig deeper into
    the tooling, and how to optimize models for embedded devices. But by the end of
    this chapter, you’ll be familiar with the key terminology, have an understanding
    of the general workflow, and know where to go to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What machine learning actually is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The types of problems it can solve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key terms and ideas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The workflow for solving problems with deep learning, one of the most popular
    approaches to machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are many books and courses that explain the science behind deep learning,
    so we won’t be doing that here. That said, it’s a fascinating topic and we encourage
    you to explore! We list some of our favorite resources in [“Learning Machine Learning”](ch12.xhtml#learning_machine_learning).
    But remember, you don’t need all the theory to start building useful things.
  prefs: []
  type: TYPE_NORMAL
- en: What Machine Learning Actually Is
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you own a machine that manufactures widgets. Sometimes it breaks down,
    and it’s expensive to repair. Perhaps if you collected data about the machine
    during operation, you might be able to predict when it is about to break down
    and halt operation before damage occurs. For instance, you could record its rate
    of production, its temperature, and how much it is vibrating. It might be that
    some combination of these factors indicates an impending problem. But how do you
    figure it out?
  prefs: []
  type: TYPE_NORMAL
- en: This is an example of the sort of problem machine learning is designed to solve.
    Fundamentally, machine learning is a technique for using computers to predict
    things based on past observations. We collect data about our factory machine’s
    performance and then create a computer program that analyzes that data and uses
    it to predict future states.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a machine learning program is different from the usual process of writing
    code. In a traditional piece of software, a programmer designs an algorithm that
    takes an input, applies various rules, and returns an output. The algorithm’s
    internal operations are planned out by the programmer and implemented explicitly
    through lines of code. To predict breakdowns in a factory machine, the programmer
    would need to understand which measurements in the data indicate a problem and
    write code that deliberately checks for them.
  prefs: []
  type: TYPE_NORMAL
- en: This approach works fine for many problems. For example, we know that water
    boils at 100°C at sea level, so it’s easy to write a program that can predict
    whether water is boiling based on its current temperature and altitude. But in
    many cases, it can be difficult to know the exact combination of factors that
    predicts a given state. To continue with our factory machine example, there might
    be various different combinations of production rate, temperature, and vibration
    level that might indicate a problem but are not immediately obvious from looking
    at the data.
  prefs: []
  type: TYPE_NORMAL
- en: To create a machine learning program, a programmer feeds data into a special
    kind of algorithm and lets the algorithm discover the rules. This means that as
    programmers, we can create programs that make predictions based on complex data
    without having to understand all of the complexity ourselves. The machine learning
    algorithm builds a *model* of the system based on the data we provide, through
    a process we call *training*. The model is a type of computer program. We run
    data through this model to make predictions, in a process called *inference*.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different approaches to machine learning. One of the most popular
    is *deep learning*, which is based on a simplified idea of how the human brain
    might work. In deep learning, a *network* of simulated neurons (represented by
    arrays of numbers) is trained to model the relationships between various inputs
    and outputs. Different *architectures*, or arrangements of simulated neurons,
    are useful for different tasks. For instance, some architectures excel at extracting
    meaning from image data, while other architectures work best for predicting the
    next value in a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: The examples in this book focus on deep learning, since it’s a flexible and
    powerful tool for solving the types of problems that are well suited to microcontrollers.
    It might be surprising to discover that deep learning can work even on devices
    with limited memory and processing power. In fact, over the course of this book,
    you’ll learn how to create deep learning models that do some really amazing things
    but that still fit within the constraints of tiny devices.
  prefs: []
  type: TYPE_NORMAL
- en: The next section explains the basic workflow for creating and using a deep learning
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The Deep Learning Workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we outlined a scenario for using deep learning to predict
    when a factory machine is likely to break down. In this section, we introduce
    the work necessary to make this happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process will involve the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Decide on a goal
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect a dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Design a model architecture
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run inference
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate and troubleshoot
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s walk through them, one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Decide on a Goal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you’re designing any kind of algorithm, it’s important to start by establishing
    exactly what you want it to do. It’s no different with machine learning. You need
    to decide what you want to predict so you can decide what data to collect and
    which model architecture to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we want to predict whether our factory machine is about to
    break down. We can express this as a *classification* problem. Classification
    is a machine learning task that takes a set of input data and returns the probability
    that this data fits each of a set of known *classes*. In our example, we might
    have two classes: “normal,” meaning that our machine is operating without issue,
    and “abnormal,” meaning that our machine is showing signs that it might soon break
    down.'
  prefs: []
  type: TYPE_NORMAL
- en: This means that our goal is to create a model that classifies our input data
    as either “normal” or “abnormal.”
  prefs: []
  type: TYPE_NORMAL
- en: Collect a Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our factory is likely to have a lot of available data, ranging from the operating
    temperature of our machine through to the type of food that was served in the
    cafeteria on a given day. Given the goal we’ve just established, we can begin
    to identify what data we need.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning models can learn to ignore noisy or irrelevant data. That said,
    it’s best to train your model only using information that is relevant to solving
    the problem. Since it’s unlikely that today’s cafeteria food has an impact on
    the functioning of our machine, we can probably exclude it from our dataset. Otherwise,
    the model will need to learn to negate that irrelevant input, and it might be
    vulnerable to learning spurious associations—perhaps our machine has, coincidentally,
    always broken down on days that pizza is served.
  prefs: []
  type: TYPE_NORMAL
- en: You should always try to combine your domain expertise with experimentation
    when deciding whether to include data. You can also use statistical techniques
    to try to identify which data is significant. If you’re still unsure about including
    a certain data source, you can always train two models and see which one works
    best!
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we’ve identified our most promising data as *rate of production*,
    *temperature*, and *vibration*. Our next step is to collect some data so that
    we can train a model.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s really important that the data you choose will also be available when you
    want to make predictions. For example, since we have decided to train our model
    with temperature readings, we will need to provide temperature readings from the
    exact same physical locations when we run inference. This is because the model
    learns to understand how its inputs can predict its outputs. If we originally
    trained the model on temperature data from the insides of our machine, running
    the model on the current room temperature is unlikely to work.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It’s difficult to know exactly how much data is required to train an effective
    model. It depends on many factors, such as the complexity of the relationships
    between variables, the amount of noise, and the ease with which classes can be
    distinguished. However, there’s a rule of thumb that is always true: the more
    data, the better!'
  prefs: []
  type: TYPE_NORMAL
- en: You should aim to collect data that represents the full range of conditions
    and events that can occur in the system. If our machine can fail in several different
    ways, we should be sure to capture data around each type of failure. If a variable
    changes naturally over time, it’s important to collect data that represents the
    full range. For example, if the machine’s temperature rises on warm days, you
    should be sure to include data from both winter and summer. This diversity will
    help your model represent every possible scenario, not just a select few.
  prefs: []
  type: TYPE_NORMAL
- en: The data we collect about our factory will likely be logged as a set of *time
    series*, meaning a sequence of readings collected on a periodic basis. For example,
    we might have a record of the temperature every minute, the rate of production
    each hour, and the level of vibration on a second-by-second basis. After we collect
    the data, we’ll need to transform these time series into a form appropriate for
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: Labeling data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to collecting data, we need to determine which data represents “normal”
    and “abnormal” operation. We’ll provide this information during the training process
    so that our model can learn how to classify inputs. The process of associating
    data with classes is called *labeling*, and the “normal” and “abnormal” classes
    are our *labels*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This type of training, in which you instruct the algorithm what the data means
    during training, is called *supervised learning*. The resulting classification
    model will be able to process incoming data and predict to which class it is likely
    to belong.
  prefs: []
  type: TYPE_NORMAL
- en: To label the time-series data we’ve collected, we need a record of which periods
    of time the machine was working and which periods of time it was broken. We might
    assume that the period immediately prior to the machine being broken generally
    represents abnormal operation. However, since we can’t necessarily spot abnormal
    operation from a superficial look at the data, getting this correct might require
    some experimentation!
  prefs: []
  type: TYPE_NORMAL
- en: After we’ve decided how to label the data, we can generate a time series that
    contains the labels and add this to our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Our final dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 3-1](#table31) lists the data sources that we’ve assembled at this point
    in the workflow.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-1\. Data sources
  prefs: []
  type: TYPE_NORMAL
- en: '| Data source | Interval | Sample reading |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Rate of production | Once every 2 minutes | 100 units |'
  prefs: []
  type: TYPE_TB
- en: '| Temperature | Once every minute | 30°C |'
  prefs: []
  type: TYPE_TB
- en: '| Vibration (% of typical) | Once every 10 seconds | 23% |'
  prefs: []
  type: TYPE_TB
- en: '| Label (“normal” or “abnormal”) | Once every 10 seconds | normal |'
  prefs: []
  type: TYPE_TB
- en: The table shows the interval of each data source. For example, the temperature
    is logged once per minute. We’ve also generated a time series that contains the
    labels for the data. The interval for our labels is 1 per 10 seconds, which is
    the same as the smallest interval for the other time series. This means that we
    can easily determine the label for every datapoint in our data.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve collected our data, it’s time to use it to design and train a
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Design a Model Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many types of deep learning model architectures, designed to solve
    a wide range of problems. When training a model, you can choose to design your
    own architecture or base it on an existing architecture developed by researchers.
    For many common problems, you can find pretrained models available online for
    free.
  prefs: []
  type: TYPE_NORMAL
- en: Over the course of this book we’ll introduce you to several different model
    architectures, but there are a huge number of possibilities beyond what is covered
    here. Designing a model is both an art and a science, and model architecture is
    a major area of research. New architectures are invented literally every day.
  prefs: []
  type: TYPE_NORMAL
- en: When deciding on an architecture, you need to think about the type of problem
    you are trying to solve, the type of data you have access to, and the ways you
    can transform that data before feeding it into a model (we discuss transforming
    data shortly). The fact is, because the most effective architecture varies depending
    on the type of data that you are working with, your data and the architecture
    of your model are deeply intertwined. Although we introduce them here under separate
    headings, they’ll always be considered together.
  prefs: []
  type: TYPE_NORMAL
- en: You also need to think about the constraints of the device you will be running
    the model on, since microcontrollers generally have limited memory and slow processors,
    and larger models require more memory and take more time to run—the size of a
    model depends on the number of neurons it contains, and the way those neurons
    are connected. In addition, some devices are equipped with hardware acceleration
    that can speed up the execution of certain types of model architectures, so you
    might want to tailor your model to the strengths of the device you have in mind.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we might start by training a simple model with a few layers of
    neurons and then refining the architecture in an iterative process until we get
    a useful result. You’ll see how to do that later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning models accept input and generate output in the form of *tensors*.
    For the purposes of this book,^([1](ch03.xhtml#idm46473587162472)) a tensor is
    essentially a list that can contain either numbers or other tensors; you can think
    of it as similar to an array. Our hypothetical simple model will take a tensor
    as its input. The following subsection describes how we transform our data into
    this form.
  prefs: []
  type: TYPE_NORMAL
- en: Generating features from data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve established that our model will accept some type of tensor as its input.
    But as we discussed earlier, our data comes in the form of time series. How do
    we transform that time-series data into a tensor that we can pass into the model?
  prefs: []
  type: TYPE_NORMAL
- en: Our task now is to decide how to generate features from our data. In machine
    learning, the term *feature* refers to a particular type of information on which
    a model is trained. Different types of models are trained on different types of
    features. For example, a model might accept a single scalar value as its sole
    input feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'But inputs can be much more complex than this: a model designed to process
    images might accept a multidimensional tensor of image data as its input, and
    a model designed to predict based on multiple features might accept a vector containing
    multiple scalar values, one for each feature.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that we decided that our model should use rate of production, temperature,
    and vibration to make its predictions. In their raw form, as time series with
    different intervals, these will not be suitable to pass into the model. The following
    section explains why.
  prefs: []
  type: TYPE_NORMAL
- en: Windowing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the following diagram, each piece of data in our time series is represented
    by a star. The current label is included in the data, since the label is required
    for training. Our goal is to train a model we can use to predict whether the machine
    is operating normally or abnormally at any given moment based on the current conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'However, since our time series each have different intervals (like once per
    minute, or once per 10 seconds), if we pass in only the data available at a given
    moment, it might not include all of the types of data we have available. For example,
    in the moment highlighted in the following image, only vibration is available.
    This would mean that our model would only have information about vibration when
    attempting to make its prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'One solution to this problem might be to choose a window in time, and combine
    all of the data in this window into a single set of values. For example, we might
    decide on a one-minute window and look at all the values contained within it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If we average all the values in the window for each time series and take the
    most recent value for any that lack a datapoint in the current window, we end
    up with a set of single values. We can decide how to label this snapshot based
    on whether there are any “abnormal” labels present in the window. If there’s any
    “abnormal” present at all, the window should be labeled “abnormal.” If not, it
    should be labeled “normal”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The three non-label values are our features! We can pass them into our model
    as a vector, with one element for each time series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: During training, we can calculate a new window for every 10 seconds of data
    and pass it into our model, using the label to inform the training algorithm of
    our desired output. During inference, whenever we want to use the model to predict
    abnormal behavior, we can just look at our data, calculate the most recent window,
    run it through the model, and receive a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: This is a simplistic approach, and it might not always turn out to work in practice,
    but it’s a good enough starting point. You’ll quickly find that machine learning
    is all about trial and error!
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on to training, let’s go over one last thing about input values.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generally, the data you feed into a neural network will be in the form of tensors
    filled with *floating-point* values, or *floats*. A float is a data type used
    to represent numbers that have decimal points. For the training algorithm to work
    effectively, these floating-point values need to be similar in size to one another.
    In fact, it’s ideal if all values are expressed as numbers in the range of 0 to
    1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take another look at our input tensor from the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'These numbers are each at very different scales: the temperature is more than
    100, whereas the vibration is expressed as a fraction of 1\. To pass these values
    into our network, we need to *normalize* them so that they are all in a similar
    range.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One way of doing this is to calculate the mean of each feature across the dataset
    and subtract it from the values. This has the effect of squashing the numbers
    down so that they are closer to zero. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'One situation in which you’ll frequently encounter normalization, implemented
    in a different way, is when images are fed into a neural network. Computers often
    store images as matrices of 8-bit integers, whose values range from 0 to 255\.
    To normalize these values so that they are all between 0 and 1, each 8-bit value
    is multiplied by `1/255`. Here’s an example with a 3 × 3–pixel grayscale image,
    in which each pixel’s value represents its brightness:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Thinking with ML
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we’ve learned how to start thinking about solving problems with machine
    learning. In the context of our factory scenario, we’ve walked through deciding
    on a suitable goal, collecting and labeling the appropriate data, designing the
    features we are going to pass into our model, and choosing a model architecture.
    No matter what problem we are trying to solve, we’ll use the same approach. It’s
    important to note that this is an iterative process, and we often go back and
    forth through the stages of the ML workflow until we’ve arrived at a model that
    works—or decided that the task is too difficult.
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine that we’re building a model to predict the weather. We’ll
    need to decide on our goal (for instance, to predict whether it’s going to rain
    tomorrow), collect and label a dataset (such as weather reports from the past
    few years), design the features that we’ll feed to our model (perhaps the average
    conditions over the past two days), and choose a model architecture suitable for
    this type of data and the device that we want to run it on. We’ll come up with
    some initial ideas, test them out, and tweak our approach until we get good results.
  prefs: []
  type: TYPE_NORMAL
- en: The next step in our workflow is training, which we explore in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Train the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training is the process by which a model learns to produce the correct output
    for a given set of inputs. It involves feeding training data through a model and
    making small adjustments to it until it makes the most accurate predictions possible.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed earlier, a model is a network of simulated neurons represented
    by arrays of numbers arranged in layers. These numbers are known as *weights*
    and *biases*, or collectively as the network’s *parameters*.
  prefs: []
  type: TYPE_NORMAL
- en: When data is fed into the network, it is transformed by successive mathematical
    operations that involve the weights and biases in each layer. The output of the
    model is the result of running the input through these operations. [Figure 3-1](#network_layers)
    shows a simple network with two layers.
  prefs: []
  type: TYPE_NORMAL
- en: The model’s weights start out with random values, and biases typically start
    with a value of 0\. During training, *batches* of data are fed into the model,
    and the model’s output is compared with the desired output (which in our case
    is the correct label, “normal” or “abnormal”). An algorithm called *backpropagation*
    adjusts the weights and biases incrementally so that over time, the output of
    the model gets closer to matching the desired value. Training, which is measured
    in *epochs* (meaning iterations), continues until we decide to stop.
  prefs: []
  type: TYPE_NORMAL
- en: '![A simple deep learning network](Images/timl_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. A simple deep learning network with two layers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We generally stop training when a model’s performance stops improving. At the
    point that it begins to make accurate predictions, it is said to have *converged*.
    To determine whether a model has converged, we can analyze graphs of its performance
    during training. Two common performance metrics are *loss* and *accuracy*. The
    loss metric gives us a numerical estimate of how far the model is from producing
    the expected answers, and the *accuracy* metric tells us the percentage of the
    time that it chooses the correct prediction. A perfect model would have a loss
    of 0.0 and an accuracy of 100%, but real models are rarely perfect.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-2](#convergence) shows the loss and accuracy during training for
    a deep learning network. You can see how as training progresses, accuracy increases
    and loss is reduced, until we reach a point at which the model no longer improves.'
  prefs: []
  type: TYPE_NORMAL
- en: To attempt to improve the model’s performance, we can change our model architecture,
    and we can adjust various values used to set up the model and moderate the training
    process. These values are collectively known as *hyperparameters*, and they include
    variables such as the number of training epochs to run and the number of neurons
    in each layer. Each time we make a change, we can retrain the model, look at the
    metrics, and decide whether to optimize further. Hopefully, time and iterations
    will result in a model with acceptable accuracy!
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph showing model convergence during training](Images/timl_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. A graph showing model convergence during training
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s important to remember there’s no guarantee that you’ll be able to achieve
    good enough accuracy for the problem you are trying to solve. There isn’t always
    enough information contained within a dataset to make accurate predictions, and
    some problems just can’t be solved, even with state-of-the-art deep learning.
    That said, your model may be useful even if it is not 100% accurate. In the case
    of our factory example, being able to predict abnormal operation even part of
    the time could be a big help.
  prefs: []
  type: TYPE_NORMAL
- en: Underfitting and overfitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The two most common reasons a model fails to converge are *underfitting* and
    *overfitting*.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network learns to *fit* its behavior to the patterns it recognizes
    in data. If a model is correctly fit, it will produce the correct output for a
    given set of inputs. When a model is *underfit*, it has not yet been able to learn
    a strong enough representation of these patterns to be able to make good predictions.
    This can happen for a variety of reasons, most commonly that the architecture
    is too small to capture the complexity of the system it is supposed to model or
    that it has not been trained on enough data.
  prefs: []
  type: TYPE_NORMAL
- en: When a model is *overfit*, it has learned its training data too well. The model
    is able to exactly predict the minutiae of its training data, but it is not able
    to generalize its learning to data it has not previously seen. Often this happens
    because the model has managed to entirely memorize the training data, or it has
    learned to rely on a shortcut present in the training data but not in the real
    world.
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine you are training a model to classify photos as containing
    either dogs or cats. If all the dog photos in your training data are taken outdoors,
    and all the cat photos are taken indoors, your model may learn to cheat and use
    the presence of the sky in each photograph to predict which animal it is. This
    means that it might misclassify future dog selfies if they happen to be taken
    indoors.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to fight overfitting. One possibility is to reduce the size
    of the model so it does not have enough capacity to learn an exact representation
    of its training set. A set of techniques known as *regularization* can be applied
    during training to reduce the degree of overfitting. To make the most of limited
    data, a technique called *data augmentation* can be used to generate new, artificial
    datapoints by slicing and dicing the existing data. But the best way to beat overfitting,
    when possible, is to get your hands on a larger and more varied dataset. More
    data always helps!
  prefs: []
  type: TYPE_NORMAL
- en: Training, validation, and testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To assess the performance of a model, we can look at how it performs on its
    training data. However, this only tells us part of the story. During training,
    a model learns to fit its training data as closely as possible. As we saw earlier,
    in some cases the model will begin to overfit the training data, meaning that
    it will work well on the training data but not in real life.
  prefs: []
  type: TYPE_NORMAL
- en: To understand when this is happening, we need to *validate* the model using
    new data that wasn’t used in training. It’s common to split a dataset into three
    parts—*training*, *validation*, and *test*. A typical split is 60% training data,
    20% validation, and 20% test. This splitting must be done so that each part contains
    the same distribution of information, and in a way that preserves the structure
    of the data. For example, since our data is a time series, we could potentially
    split it into three contiguous chunks of time. If our data were not a time series,
    we could just sample the datapoints randomly.
  prefs: []
  type: TYPE_NORMAL
- en: During training, the *training* dataset is used to train the model. Periodically,
    data from the *validation* dataset is fed through the model, and the loss is calculated.
    Because the model has not seen this data before, its loss score is a more reliable
    measure of how the model is performing. By comparing the training and validation
    loss (and accuracy, or whichever other metrics are available) over time, you can
    see whether the model is overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-3](#overfitting) shows a model that is overfitting. You can see how
    as the training loss has decreased, the validation loss has gone up. This means
    that the model is becoming better at predicting the training data but is losing
    its ability to generalize to new data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph showing model overfitting during training](Images/timl_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. A graph showing model overfitting during training
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we tweak our models and training processes to improve performance and avoid
    overfitting, we will hopefully start to see our validation metrics improve.
  prefs: []
  type: TYPE_NORMAL
- en: However, this process has an unfortunate side effect. By optimizing to improve
    the validation metrics, we might just be nudging the model toward overfitting
    both the training *and* the validation data! Each adjustment we make will fit
    the model to the validation data slightly better, and in the end, we might have
    the same overfitting problem as before.
  prefs: []
  type: TYPE_NORMAL
- en: To verify that this hasn’t happened, our final step when training a model is
    to run it on our *test* data and confirm that it performs as well as during validation.
    If it doesn’t, we have optimized our model to overfit both our training and validation
    data. In this case, we might need to go back to the drawing board and come up
    with a new model architecture, since if we continue to tweak to improve performance
    on our test data, we’ll just overfit to that, too.
  prefs: []
  type: TYPE_NORMAL
- en: After we have a model that performs acceptably well with training, validation,
    and test data, the training part of this process is over. Next, we get our model
    ready to run on-device!
  prefs: []
  type: TYPE_NORMAL
- en: Convert the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this book, we use TensorFlow to build and train models. A TensorFlow
    model is essentially a set of instructions that tell an *interpreter* how to transform
    data in order to produce an output. When we want to use our model, we just load
    it into memory and execute it using the TensorFlow interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: However, TensorFlow’s interpreter is designed to run models on powerful desktop
    computers and servers. Since we’ll be running our models on tiny microcontrollers,
    we need a different interpreter that’s designed for our use case. Fortunately,
    TensorFlow provides an interpreter and accompanying tools to run models on small,
    low-powered devices. This set of tools is called TensorFlow Lite.
  prefs: []
  type: TYPE_NORMAL
- en: Before TensorFlow Lite can run a model, it first must be converted into the
    TensorFlow Lite format and then saved to disk as a file. We do this using a tool
    named the *TensorFlow Lite Converter*. The converter can also apply special optimizations
    aimed at reducing the size of the model and helping it run faster, often without
    sacrificing performance.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 13](ch13.xhtml#chapter_tensorflow_lite_for_microcontrollers), we
    dive into the details of TensorFlow Lite and how it helps us run models on tiny
    devices. For now, all you need to know is that you’ll need to convert your models,
    and that the conversion process is quick and easy.
  prefs: []
  type: TYPE_NORMAL
- en: Run Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the model has been converted, it’s ready to deploy! We’ll now use the
    TensorFlow Lite for Microcontrollers C++ library to load the model and make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is the part where our model meets our application code, we need to
    write some code that takes raw input data from our sensors and transforms it into
    the same form that our model was trained on. We then pass this transformed data
    into our model and run inference.
  prefs: []
  type: TYPE_NORMAL
- en: This will result in output data containing predictions. In the case of our classifier
    model, the output will be a score for each of our classes, “normal” and “abnormal.”
    For models that classify data, typically the scores for all of the classes will
    sum to 1, and the class with the highest score will be the prediction. The higher
    the difference between the scores, the higher the confidence in the prediction.
    [Table 3-2](#example_outputs) lists some example outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-2\. Example outputs
  prefs: []
  type: TYPE_NORMAL
- en: '| Normal score | Abnormal score | Explanation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | 0.9 | High confidence in an abnormal state |'
  prefs: []
  type: TYPE_TB
- en: '| 0.9 | 0.1 | High confidence in a normal state |'
  prefs: []
  type: TYPE_TB
- en: '| 0.7 | 0.3 | Slight confidence in a normal state |'
  prefs: []
  type: TYPE_TB
- en: '| 0.49 | 0.51 | Inconclusive result, since neither state is significantly ahead
    |'
  prefs: []
  type: TYPE_TB
- en: In our factory machine example, each individual inference takes into account
    only a snapshot of the data—it tells us the probability of an abnormal state within
    the last 10 seconds, based on various sensor readings. Since real-world data is
    often messy and machine learning models aren’t perfect, it’s possible that a temporary
    glitch might result in an incorrect classification. For example, we might see
    a spike in a temperature value due to a temporary sensor malfunction. This transient,
    unreliable input might result in an output classification that momentarily doesn’t
    reflect reality.
  prefs: []
  type: TYPE_NORMAL
- en: To prevent these momentary glitches from causing problems, we could potentially
    take the average of all of our model’s outputs across a period of time. For example,
    we could run our model on the current data window every 10 seconds, and take the
    averages of the last 6 outputs to give a smoothed score for each class. This would
    mean that transient issues are ignored, and we only act upon consistent behavior.
    We use this technique to help with wake-word detection in [Chapter 7](ch07.xhtml#chapter_speech_wake_word_example).
  prefs: []
  type: TYPE_NORMAL
- en: After we have a score for each class, it’s up to our application code to decide
    what to do. Perhaps if an abnormal state is detected consistently for one minute,
    our code will send a signal to shut down our machine and alert the maintenance
    team.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate and Troubleshoot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After we’ve deployed our model and have it running on-device, we’ll start to
    see whether its real-world performance approaches what we hoped. Even though we’ve
    already proved that our model makes accurate predictions on its test data, performance
    on the actual problem might be different.
  prefs: []
  type: TYPE_NORMAL
- en: There are many reasons why this might happen. For example, the data used in
    training might not be exactly representative of the data available in real operation.
    Perhaps due to local climate, our machine’s temperature is generally cooler than
    the one from which our dataset was collected. This might affect the predictions
    made by our model, such that they are no longer as accurate as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Another possibility is that our model might have overfit our dataset without
    us realizing. In [“Train the Model”](#ch3_train_the_model), we learned how this
    can happen by accident when the dataset happens to contain additional signals
    that a model can learn to recognize in place of those we expect.
  prefs: []
  type: TYPE_NORMAL
- en: If our model isn’t working in production, we’ll need to do some troubleshooting.
    First, we rule out any hardware problems (like faulty sensors or unexpected noise)
    that might be affecting the data that gets to our model. Second, we capture some
    data from the device where the model is deployed and compare it with our original
    dataset to make sure that it is in the same ballpark. If not, perhaps there’s
    a difference in environmental conditions or sensor characteristics that we weren’t
    expecting. If the data checks out, it might be that overfitting is the problem.
  prefs: []
  type: TYPE_NORMAL
- en: After we’ve ruled out hardware issues, the best fix for overfitting is often
    to train with more data. We can capture additional data from our deployed hardware,
    combine it with our original dataset, and retrain our model. In the process, we
    can apply regularization and data augmentation techniques to help make the most
    of the data we have.
  prefs: []
  type: TYPE_NORMAL
- en: Reaching good real-world performance can sometimes take some iteration on your
    model, your hardware, and the accompanying software. If you run into a problem,
    treat it like any other technology issue. Take a scientific approach to troubleshooting,
    eliminating possible factors, and analyze your data to figure out what is going
    wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you’re familiar with the basic workflow used by machine learning practitioners,
    we’re ready to take the next steps in our TinyML adventure.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.xhtml#chapter_hello_world_training), we’ll build our first
    model and deploy it to some tiny hardware!
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.xhtml#idm46473587162472-marker)) This definition of the word *tensor*
    is different from the mathematical and physics definitions of the word, but it
    has become the norm in data science.
  prefs: []
  type: TYPE_NORMAL
