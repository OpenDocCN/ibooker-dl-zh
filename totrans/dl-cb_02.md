# 第二章。摆脱困境

深度学习模型经常被视为黑匣子；我们在一端输入数据，另一端输出答案，而我们不必太关心网络是如何学习的。虽然深度神经网络确实擅长从复杂输入数据中提取信号，但将这些网络视为黑匣子的反面是，当事情陷入困境时并不总是清楚该怎么办。

我们在这里讨论的技术中的一个共同主题是，我们希望网络*泛化*而不是*记忆*。值得思考的问题是为什么神经网络总体化。本书中描述并用于生产的一些模型包含数百万个参数，这些参数允许网络记忆大量示例的输入。然而，如果一切顺利，它不会这样做，而是会发展出关于其输入的泛化规则。

如果事情不顺利，您可以尝试本章描述的技术。我们将首先看看如何知道我们陷入困境。然后，我们将看看各种方法，我们可以预处理输入数据，使网络更容易处理。

# 2.1 确定您陷入困境

## 问题

当您的网络陷入困境时，您如何知道？

## 解决方案

在网络训练时查看各种指标。

神经网络出现问题的最常见迹象是网络没有学到任何东西，或者学到了错误的东西。当我们设置网络时，我们指定*损失函数*。这决定了网络试图优化的内容。在训练过程中，损失会持续打印。如果这个值在几次迭代后没有下降，我们就有麻烦了。网络没有学到任何东西，根据自己的进展概念来衡量。

另一个方便的指标是*准确率*。这显示网络预测正确答案的输入百分比。随着损失的降低，准确率应该提高。如果准确率没有提高，即使损失在减少，那么我们的网络正在学习某些东西，但不是我们希望的东西。准确率可能需要一段时间才能提高。一个复杂的视觉网络在学习的同时可能需要很长时间才能正确地获取任何标签，因此在过早放弃之前，请考虑这一点。

要寻找的第三个问题，这可能是陷入困境的最常见方式，是*过拟合*。过拟合时，我们看到损失减少，准确率增加，但我们在测试集上看到的准确率并没有跟上。假设我们有一个测试集，并已将其添加到要跟踪的指标中，我们可以在每个时代结束时看到这一点。通常，测试准确率一开始会随着训练集的准确率增加，但随后会出现差距，而且测试准确率甚至有时会开始下降，而训练准确率则继续增加。

这里发生的情况是，我们的网络正在学习输入和预期输出之间的直接映射，而不是学习泛化。只要它看到之前见过的输入，一切都很顺利。但当面对测试集中的样本时，它开始失败。

## 讨论

关注训练过程中显示的指标是跟踪学习过程进展的好方法。我们在这里讨论的三个指标是最重要的，但像 Keras 这样的框架提供了更多选项，也可以自己构建它们。

# 2.2 解决运行时错误

## 问题

当您的网络抱怨不兼容的形状时，您应该怎么办？

## 解决方案

查看网络结构，并尝试不同的数字。

Keras 是对 TensorFlow 或 Theano 等复杂框架的很好的抽象，但是像任何抽象一样，这是有代价的。当一切顺利时，我们清晰定义的模型可以在 TensorFlow 或 Theano 之上愉快地运行。但是，当出现问题时，我们会从底层框架的深处收到错误。如果不了解这些框架的复杂性，这些错误很难理解，而我们使用 Keras 的初衷就是要避免这种情况。

有两件事可以帮助我们，而不需要深入研究。第一件事是打印网络的结构。假设我们有一个简单的模型，接受五个变量并分类为八个类别：

```py
data_in = Input(name='input', shape=(5,))
fc = Dense(12, activation='relu')(data_in)
data_out = Dense(8, activation='sigmoid')(fc)
model = Model(inputs=[data_in], outputs=[data_out])
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
```

现在我们可以使用以下方式检查模型：

```py
model.summary()
```

```py
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           (None, 5)                 0
_________________________________________________________________
dense_5 (Dense)              (None, 12)                72
_________________________________________________________________
dense_6 (Dense)              (None, 8)                 104
=================================================================
Total params: 176
Trainable params: 176
Non-trainable params: 0
```

现在，如果我们遇到一个关于不兼容形状的运行时错误，可能不是我们担心的形式：

```py
InvalidArgumentError: Incompatible shapes: X vs. Y
```

我们知道内部肯定有问题，这不容易通过堆栈跟踪来追踪。不过，还有一些其他尝试的方法。

首先，看看是否有任何形状是`X`或`Y`。如果是这样，那可能就是问题所在。知道这一点已经完成了一半的工作——当然还有另一半。另一件需要注意的事情是层的名称。它们经常出现在错误消息中，有时以扭曲的形式出现。Keras 会自动为匿名层分配名称，因此查看摘要在这方面也很有用。如果需要，我们可以为自己分配名称，就像在这里显示的示例中的输入层一样。

如果我们找不到运行时错误提到的形状或名称，我们可以在深入研究之前尝试其他方法（或在 StackOverflow 上发布）：使用不同的数字。

神经网络包含大量的超参数，如各个层的大小。这些通常是因为它们看起来合理，与做类似事情的其他网络相似。但是它们的实际值有些是任意的。在我们的示例中，隐藏层是否真的需要 12 个单元？11 个会差很多吗，13 会导致过拟合吗？

我们倾向于选择感觉好的数字，通常是 2 的幂。因此，如果您遇到运行时错误，请更改这些数字，看看它对错误消息有何影响。如果错误消息仍然相同，则您更改的变量与此无关。但是一旦开始更改，您就知道已经接近相关内容了。

这可能有点微妙。例如，有些网络要求所有批次具有相同的大小。如果您的数据不能被批次大小整除，那么最后一个批次将太小，您将收到类似以下错误的错误消息：

```py
Incompatible shapes: [X,784] vs. [Y,784]
```

这里`X`将是批量大小，`Y`将是您最后一个不完整批次的大小。您可能会认出`X`作为批量大小，但是很难确定`Y`的位置。但是，如果更改批量大小，`Y`也会更改，这为您提供了一个查找位置的提示。

## 讨论

理解由 Keras 抽象的框架报告的错误基本上是棘手的。抽象被打破，我们突然看到了机器的内部。这个配方中的技术允许您通过发现错误中的形状和名称来推迟查看这些细节，如果失败，则尝试使用数字并查看变化。

# 2.3 检查中间结果

## 问题

您的网络很快达到了一个令人满意的准确度水平，但拒绝超越这一水平。

## 解决方案

检查是否没有陷入明显的局部最大值。

这种情况可能发生在一个标签比其他任何标签都更常见的情况下，您的网络很快学会了总是预测这种结果会得到不错的结果。验证这一点并不难；只需向网络提供一些输入样本并查看输出。如果所有输出都相同，您就陷入了这种情况。

本章中的一些配方提供了如何解决这个问题的建议。或者，你可以调整数据的分布。如果你的例子中有 95%是狗，只有 5%是猫，那么网络可能看不到足够的猫。通过人为改变分布，比如说，65%/35%，可以让网络更容易一些。

当然，这也不是没有风险的。网络现在可能有更多机会了解猫，但它也会学习错误的基础分布，或先验。这意味着在疑惑的情况下，网络现在更有可能选择“猫”作为答案，尽管一切相等，“狗”更有可能。

## 讨论

查看网络输出标签的分布对于一小部分输入样本是了解实际操作的简单方法，但往往被忽视。调整分布是尝试使网络摆脱僵局的一种方式，如果网络只关注顶部答案，你可能应该考虑其他技术。

当网络不快速收敛时，还有其他要注意的输出情况；NaN 的出现表明梯度爆炸，如果网络的输出似乎被截断，无法达到正确的值，那么你可能在最终层上使用了不正确的激活函数。

# 2.4 选择正确的激活函数（用于最终层）

## 问题

当情况不对时，如何为最终层选择正确的激活函数？

## 解决方案

确保激活函数与网络的意图相对应。

开始深度学习的一个好方法是在网上找到一个示例，并逐步修改直到它达到你想要的效果。然而，如果示例网络的意图与你的目标不同，你可能需要更改最终层的激活函数。让我们看一些常见的选择。

Softmax 激活函数确保输出向量的总和恰好为 1。这是一个适合网络的激活函数，它为给定输入输出一个标签（例如，图像分类器）。输出向量将表示概率分布——如果输出向量中“猫”的条目为 0.65，那么网络认为它以 65%的确定性看到了一只猫。Softmax 只在有一个答案时有效。当可能有多个答案时，可以尝试使用 sigmoid 激活。

线性激活函数适用于回归问题，当我们需要根据输入预测数值时。一个例子是根据一系列电影评论预测电影评分。线性激活函数将取前一层的值，并将它们与一组权重相乘，以使其最好地适应预期输出。就像将输入数据归一化为[-1, 1]范围或附近一样是一个好主意，通常也有助于对输出进行相同的处理。因此，如果我们的电影评分在 0 到 5 之间，我们会在创建训练数据时减去 2.5 并除以相同的值。

如果网络输出图像，请确保所使用的激活函数与像素的归一化方式一致。标准的减去平均像素值并除以标准差的归一化会导致值围绕 0 中心，因此它不适用于 sigmoid，而且由于 30%的值将落在范围[-1, 1]之外，tanh 也不是一个好选择。你仍然可以使用这些，但你需要改变应用于输出的归一化。

根据您对输出分布的了解，可能会有更高级的方法。例如，电影评分通常集中在 3.7 左右，因此以此作为中心可能会产生更好的结果。当实际分布偏斜，使得平均值附近的值比异常值更有可能时，使用 tanh 激活函数可能是合适的。这将任何值压缩到[-1, 1]范围内。通过将预期输出映射到相同的范围，考虑预期分布，我们可以模拟输出数据的任何形状。

## 讨论

选择正确的输出激活函数至关重要，但在大多数情况下并不困难。如果您的输出代表具有一个可能结果的概率分布，那么 softmax 适合您；否则，您需要进行实验。

您还需要确保损失函数与最终层的激活函数配合使用。损失函数通过计算给定预期值时预测有多“错误”来引导网络的训练。我们看到当网络进行多标签预测时，softmax 激活函数是正确的选择；在这种情况下，您可能希望选择像 Keras 的`categorical_crossentropy`这样的分类损失函数。

# 2.5 正则化和 Dropout

## 问题

一旦您发现您的网络过拟合了，您可以采取什么措施？

## 解决方案

通过使用正则化和 dropout 来限制网络的功能。

一个具有足够参数的神经网络可以通过记忆来适应任何输入/输出映射。在训练时准确率看起来很好，但当然网络在未见过的数据上表现不佳，因此在测试数据或实际生产中表现不佳。网络过拟合了。

防止网络过拟合的一个明显方法是通过减少参数的数量，可以通过减少层数或使每一层更小来实现。但这当然也会降低网络的表达能力。正则化和 dropout 在这方面为我们提供了一种折中方案，通过限制网络的表达能力，而不会损害学习的能力（太多）。

通过正则化，我们对参数的*极端*值添加惩罚。这里的直觉是，为了适应任意的输入/输出映射，网络需要任意的参数，而学习到的参数往往在一个正常范围内。因此，使得难以达到那些任意参数应该保持网络在学习的道路上，而不是记忆。

在 Keras 中的应用很简单：

```py
dense = Dense(128,
              activation='relu',
              kernel_regularizer=regularizers.l2(0.01))(flatten)
```

正则化器可以应用于核的权重或层的偏置，也可以应用于层的输出。选择哪种方法和使用什么惩罚主要是一个试错的问题。0.01 似乎是一个受欢迎的起始值。

Dropout 是一种类似的技术，但更激进。与保持神经元的权重不同，我们在训练期间随机忽略一定百分比的所有神经元。

与正则化类似，这使得网络更难记住输入/输出对，因为它不能依赖特定的神经元在训练期间工作。这促使网络学习一般的、稳健的特征，而不是一次性的、特定的特征来覆盖一个训练实例。

在 Keras 中，通过使用`Dropout`（伪）层将 dropout 应用于一个层：

```py
    max_pool_1x = MaxPooling1D(window)(conv_1x)
    dropout_1x = Dropout(0.3)(max_pool_1x)
```

这将在最大池化层应用 30%的 dropout，在训练期间忽略 30%的神经元。

在进行推断时，不会应用 dropout。一切相等的情况下，这将使层的输出增加超过 40%，因此框架会自动将这些输出缩小。

## 讨论

随着网络变得更具表现力，它倾向于过拟合或记忆输入而不是学习一般特征的倾向会增加。正则化和 dropout 都可以起到减少这种影响的作用。它们都通过减少网络发展任意特征的自由度来起作用，通过惩罚极端值（正则化）或忽略层中百分比的神经元的贡献（dropout）。

观察具有 dropout 的网络如何工作的一个有趣的替代方式是考虑，如果我们有*N*个神经元，并随机关闭一定百分比的神经元，我们实际上创建了一个可以创建非常多不同但相关网络的生成器。在训练期间，这些不同的网络都学习手头的任务，但在评估时，它们都并行运行，取其平均意见。因此，即使其中一些开始过拟合，很可能在总体投票中被淹没。

# 2.6 网络结构、批量大小和学习率

## 问题

如何找到给定问题的最佳网络结构、批量大小和学习率？

## 解决方案

从小处开始，逐步扩大。

一旦我们确定了解决特定问题所需的网络类型，我们仍然必须做出许多实现决策。其中最重要的决策是关于网络结构、学习率和批量大小的决策。

让我们从网络结构开始。我们将有多少层？每一层的大小将是多少？一个不错的策略是从可能有效的最小尺寸开始。对于“深度学习”感到热情，有一种诱惑是从许多层开始。但通常，如果一个一层或两层的网络根本不起作用，增加更多层也不会真正有所帮助。

继续考虑每个单独层的大小，较大的层可以学到更多，但也需要更长时间，并且有更多隐藏问题的空间。与层数相同，从小处开始，逐步扩大。如果你怀疑较小网络的表现力不足以理解你的数据，考虑简化你的数据；从一个只区分两个最流行标签的小网络开始，然后逐渐增加数据和网络的复杂性。

批量大小是我们在调整权重之前向网络输入的样本数量。批量大小越大，完成一个批次所需的时间就越长，但梯度更准确。为了快速获得结果，建议从一个较小的批量大小开始——32 似乎效果很好。

学习率确定我们将如何根据导数梯度改变网络中的权重。学习率越高，我们在景观中移动得越快。然而，如果学习率太大，我们就有可能跳过好的部分，开始折腾。考虑到较小的批量大小会导致梯度不够准确，我们应该将较小的批量大小与较小的学习率结合起来。因此，建议在开始时再次从小处开始，当事情顺利时，尝试较大的批量率和更高的学习率。

###### 注意

在 GPU 上进行训练会影响这个评估。GPU 可以高效地并行运行步骤，因此没有真正的理由选择一个批量大小太小以至于让 GPU 的一部分空闲。批量大小取决于网络，但只要每个批次的时间不会因增加批量大小而显著增加，你仍然处于正确的一边。在 GPU 上运行时的第二个考虑因素是内存。当一个批次不再适合 GPU 内存时，事情开始失败，你会开始看到内存不足的消息。

## 讨论

网络结构、批量大小和学习率是影响网络性能的重要超参数之一，但与实际策略关系不大。对于所有这些，一个合理的策略是从小开始（但足够大以至于事情仍然有效），逐步扩大，观察网络仍然表现。

随着层数和每层大小的增加，我们会在某个时候开始看到过拟合的症状（例如，训练和测试准确度开始发散）。这可能是一个好时机来考虑正则化和丢弃。
