["```py\nprefix = 'PATH_TO_MATLAB_FILE/'\n\ndata_file = loadmat(prefix +  'YelpChi.mat')   #1\n\nlabels = data_file['label'].flatten()          #2\nfeatures = data_file['features'].todense().A   #2\n\nyelp_homo = data_file['homo']            #3\nsparse_to_adjlist(yelp_homo, prefix +\\\n 'yelp_homo_adjlists.pickle')\n```", "```py\nwith open(prefix + 'yelp_homo_adjlists.pickle', 'rb') as file:\n    homogenous = pickle.load(file)\n```", "```py\nwith open(prefix + 'yelp_homo_adjlists.pickle', 'rb') as file:\nhomogenous = pickle.load(file)\ng = nx.Graph(homogenous)\nprint(f'Number of nodes: {g.number_of_nodes()}')\nprint(f'Number of edges: {g.number_of_edges()}')\nprint(f'Average node degree: {len(g.edges) / len(g.nodes):.2f}')\n```", "```py\nfeatures = data_file['features'].todense().A\n```", "```py\nfrom sklearn.model_selection import train_test_split \nsplit = 0.2\nxtrain, xtest, ytrain, ytest = train_test_split\\\n(features, labels, test_size = \\\nsplit, stratify=labels, random_state = 99)   #1\n\nprint(f'Required shape is {int(len(features)*(1-split))}')   #2\nprint(f'xtrain shape = {xtrain.shape}, \\\nxtest shape = {xtest.shape}')                               \nprint(f'Correct split = {int(len(features)*(1-split))\\\n == xtrain.shape[0]}')\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score, f1_score\n\nclf = LogisticRegression(random_state=0)\\\n.fit(xtrain, ytrain)   #1\nypred = clf.predict_proba(xtest)[:,1]\nacc = roc_auc_score(ytest,ypred)   #2\n\nprint(f\"Model accuracy (logression) = {100*acc:.2f}%\")\n```", "```py\nfrom sklearn.metrics import roc_curve \nfpr, tpr, _ = roc_curve(ytest,ypred)   #1\n\nplt.figure(1)\nplt.plot([0, 1], [0, 1])\nplt.plot(fpr, tpr)\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.show()\n```", "```py\nimport xgboost as xgb\nxgb_classifier = xgb.XGBClassifier()\n\nxgb_classifier.fit(xtrain,ytrain)\nypred2 = xgb_classifier.predict_proba(xtest)[:,1]   #1\nacc = roc_auc_score(ytest,ypred2)\n\nprint(f\"Model accuracy (XGBoost) = {100*acc:.2f}%\")\n\nfpr2, tpr2, _ = roc_curve(ytest,ypred2)   #2\n\nplt.figure(1)\nplt.plot([0, 1], [0, 1])\nplt.plot(fpr, tpr)\nplt.plot(fpr2, tpr2)                     \nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.show()\n```", "```py\nimport torch   #1\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MLP(nn.Module):   #2\n    def __init__(self, in_channels, out_channels, hidden_channels=[128,256]):\n        super(MLP, self).__init__()\n        self.lin1 = nn.Linear(in_channels,hidden_channels[0])\n        self.lin2 = nn.Linear(hidden_channels[0],hidden_channels[1])\n        self.lin3 = nn.Linear(hidden_channels[1],out_channels)\n\n    def forward(self, x):\n        x = self.lin1(x)\n        x = F.relu(x)\n        x = self.lin2(x)\n        x = F.relu(x)\n        x = self.lin3(x)\n        x = torch.sigmoid(x)\n\n        return x\n\nmodel = MLP(in_channels = features.shape[1],\\\n out_channels = 1)   #3\n\nepochs = 100   #4\nlr = 0.001\nwd = 5e-4\nn_classes = 2\nn_samples = len(ytrain)\n\nw= ytrain.sum()/(n_samples - ytrain.sum())   #5\n\noptimizer = torch.optim.Adam(model.parameters()\\\n,lr=lr,weight_decay=wd)   #6\ncriterion = torch.nn.BCELoss()   #7\n\nxtrain = torch.tensor(xtrain).float()   #8\nytrain = torch.tensor(ytrain)\n\nlosses = []\n\nfor epoch in range(epochs):  #9\n    model.train()\n    optimizer.zero_grad()\n    output = model(xtrain)\n    loss = criterion(output, ytrain.reshape(-1,1).float())\n    loss.backward()\n    losses.append(loss.item())\n\n    ypred3 = model(torch.tensor(xtest,dtype=torch.float32))\n\n    acc = roc_auc_score(ytest,ypred3.detach().numpy())\n    print(f'Epoch {epoch} | Loss {loss.item():6.2f}\\\n    | Accuracy = {100*acc:6.3f}% | # True\\ Labels = \\\n    {ypred3.detach().numpy().round().sum()}', end='\\r')\n\n    optimizer.step()\n\nfpr, tpr, _ = roc_curve(ytest,ypred)\nfpr3, tpr3, _ = roc_curve(ytest,ypred3.detach().numpy())   #10\n\nplt.figure(1)   #11\nplt.plot([0, 1], [0, 1])\nplt.plot(fpr, tpr)\nplt.plot(fpr2, tpr2)\nplt.plot(fpr3, tpr3)\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.show()\n```", "```py\nfrom torch_geometric.transforms import NormalizeFeatures\n\nsplit = 0.2                                        #1\nindices = np.arange(len(features))                 #1\nxtrain, xtest, ytrain, ytest, idxtrain, idxtest\\\n = train_test_split(features labels,indices, \\\nstratify=labels, test_size = split, \\\nrandom_state = 99)                                 #2\n\ng = nx.Graph(homogenous)                                            #3\nprint(f'Number of nodes: {g.number_of_nodes()}')\nprint(f'Number of edges: {g.number_of_edges()}')\nprint(f'Average node degree: {len(g.edges) / len(g.nodes):.2f}')\ndata = from_networkx(g)                                            \ndata.x = torch.tensor(features).float()                            \ndata.y = torch.tensor(labels)                                      \ndata.num_node_features = data.x.shape[-1]                          \ndata.num_classes = 1 #binary classification                        \n\nA = set(range(len(labels)))                                  #4\ndata.train_mask = torch.tensor([x in idxtrain for x in A])   #4\ndata.test_mask = torch.tensor([x in idxtest for x in A])     #4\n```", "```py\nclass GCN(torch.nn.Module):       #1\n    def __init__(self, hidden_layers = 64):\n        super().__init__()\n        torch.manual_seed(2022)\n        self.conv1 = GCNConv(data.num_node_features, hidden_layers)\n        self.conv2 = GCNConv(hidden_layers, 1)\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return torch.sigmoid(x)\n\ndevice = torch.device(\"cuda\"\\\n if torch.cuda.is_available() \\\nelse \"cpu\")              #2\nprint(device)\nmodel = GCN()\nmodel.to(device)\ndata.to(device)\n\nlr = 0.01\nepochs = 1000\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\ncriterion = torch.nn.BCELoss()\n\nlosses = []\nfor e in range(epochs):      #3\n    model.train()\n    optimizer.zero_grad()\n    out = model(data)                          #4\n\n    loss = criterion(out[data.train_mask], \\\n    data.y[data.train_mask].\\\n    reshape(-1,1).float())                    \n    loss.backward()\n    losses.append(loss.item())\n\n    optimizer.step()\n\n    ypred = model(data).clone().cpu()\n    pred = data.y[data.test_mask].clone().cpu().detach().numpy()\n    true = ypred[data.test_mask].detach().numpy()\n    acc = roc_auc_score(pred,true)\n\n    print(f'Epoch {e} | Loss {loss:6.2f} \\\n    | Accuracy = {100*acc:6.3f}% \\\n    | # True Labels =\\ {ypred.round().sum()}')\nfpr, tpr, _ = roc_curve(pred,true)      #5\n```", "```py\nfrom torch_geometric.loader import NeighborLoader\n\nbatch_size = 128\nloader = NeighborLoader(\n    data,\n    num_neighbors=[1000]*2,   #1\n    batch_size=batch_size,   #2\n    input_nodes=data.train_mask)\n\nsampled_data = next(iter(loader))\nprint(f'Checking that batch size is \\\n{batch_size}: {batch_size == \\\nsampled_data.batch_size}')\nprint(f'Percentage fraud in batch: \\\n{100*sampled_data.y.sum()/\\\nlen(sampled_data.y):.4f}%')\nsampled_data\n```", "```py\nclass GAT(torch.nn.Module):\n    def __init__(self, hidden_layers=32, heads=1, dropout_p=0.0):\n        super().__init__()\n        torch.manual_seed(2022)\n        self.conv1 = GATConv(data.num_node_features,\\\n hidden_layers, heads, dropout=dropout_p)                            #1\n        self.bn1 = nn.BatchNorm1d(hidden_layers*heads)     #2\n        self.conv2 = GATConv(hidden_layers * heads, \\\n1, dropout=dropout_p)                                               \n\n    def forward(self, data, dropout_p=0.0):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = self.bn1(x)                                   \n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return torch.sigmoid(x)\n```", "```py\nlr = 0.01\nepochs = 1000\n\nmodel = GAT(hidden_layers = 64,heads=2)\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=5e-4)\ncriterion = torch.nn.BCELoss()\n\nlosses = []\nfor e in range(epochs):    \n    epoch_loss = 0.\n    for i, sampled_data in enumerate(loader):  #1\n        sampled_data.to(device)\n        model.train()\n        optimizer.zero_grad()\n        out = model(sampled_data)\n        loss = criterion(out[sampled_data.train_mask],\\ \nsampled_data.y[sampled_data.train_mask].\\\nreshape(-1,1).float())\nloss.backward()\nepoch_loss += loss.item()\n\n        optimizer.step()\n\n        ypred = model(sampled_data).clone().cpu()\n        pred = sampled_data.y[sampled_data.test_mask]\\\n.clone().cpu().detach().numpy()\n        true = ypred[sampled_data.test_mask].detach().numpy()\n        acc = roc_auc_score(pred,true)    \n    losses.append(epoch_loss/batch_size)\n\n    print(f'Epoch {e} | Loss {epoch_loss:6.2f}\\\n    | Accuracy = {100*acc:6.3f}% | \n    # True Labels = {ypred.round().sum()}')\n```", "```py\nclass BalancedNodeSampler(BaseSampler): \n    def __init__(self, data, num_samples=None):\n        super().__init__()\n        self.data = data  \n        self.num_samples = num_samples    #1\n\n    def sample_from_nodes(self, index, **kwargs):\n        majority_indices = torch.\\\nwhere(self.data.y == 0)[0]    #2\n        minority_indices = torch.\\\nwhere(self.data.y == 1)[0]    #3\n\n        if self.num_samples is None:\n            batch_size = min(len(majority_indices),\\\n len(minority_indices))    #4\n        else:\n            batch_size = self.num_samples // 2 \n\n        majority_sample = majority_indices[torch.randperm\\\n(len(majority_indices))[:batch_size]]                        #5\n        minority_sample = minority_indices[torch.randint\\\n(len(minority_indices), (batch_size,))]                     \n        batch_indices = torch.cat\\\n((majority_sample, minority_sample))   #6\n\n        mask = torch.zeros(self.data.num_nodes, dtype=torch.bool)\n        mask[batch_indices] = True    #7\n        row, col = self.data.edge_index \n        mask_edges = mask[row] & mask[col]    #8\n        sub_row = row[mask_edges] \n        sub_col = col[mask_edges] \n\n        new_index = torch.full((self.data.num_nodes,), -1, dtype=torch.long)\n        new_index[batch_indices] = \\\ntorch.arange(batch_indices.size(0))    #9\n        sub_row = new_index[sub_row] \n        sub_col = new_index[sub_col] \n\n        return SamplerOutput(\n            node=batch_indices,\n            row=sub_row,\n            col=sub_col,\n            edge=None,  \n            num_sampled_nodes=[len(batch_indices)],  \n            metadata=(batch_indices, None)\n        )\n```"]