- en: 2 Hallucinations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hallucinations, one of AI’s most important limitations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why hallucinations occur
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether we will be able to avoid them soon
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to mitigate them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How hallucinations can affect businesses and why we should keep them in mind
    whenever we use AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 1 provided an overview of how current AI works. We now focus on its
    limitations, which will help us better understand the capabilities of AI and how
    to use it more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve been worried about hallucinations for quite some time, even before the
    term became popular. In my book, *Smart Until It’s Dumb: Why Artificial Intelligence
    Keeps Making Epic Mistakes [and Why the AI Bubble Will Burst]* (Applied Maths
    Ltd, 2023), I called them “epic fails” or “epic mistakes,” and I expressed my
    skepticism about them being resolved:'
  prefs: []
  type: TYPE_NORMAL
- en: It seems to me that every time an epic fail is fixed, another one pops up. .
    . . As AI keeps improving, the number of problematic cases keeps shrinking and
    thus it becomes more usable. However, the problematic cases never seem to disappear.
    It’s as if you took a step that brings you 80% of the way toward a destination,
    and then another step covering 80% of the remaining distance, and then another
    step to get 80% closer, and so on; you’d keep getting closer to your destination
    but never reach it.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It also seems that each step is much harder than the previous ones; each epic
    fail we find seems to require an increasingly complicated solution to fix.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As hallucinations are one of AI’s major challenges, they deserve a chapter of
    their own.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will first discuss what hallucinations are and why they happen,
    which will help us better understand one of AI’s main limitations so that we’re
    well prepared for them. Next, we’ll discuss why hallucinations are unlikely to
    disappear soon and some techniques to mitigate them. Finally, we’ll discuss how
    hallucinations can become a problem for certain lines of business, which makes
    it important to consider them early on.
  prefs: []
  type: TYPE_NORMAL
- en: What are hallucinations?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hallucinations are unsatisfactory outputs produced by AI with three defining
    characteristics. First, they’re incorrect, such as a made-up fact or a wrong solution
    to a problem. Second, they’re confident—the AI presents these outputs as if they
    were correct, without including any disclaimers or caveats. Third, they happen
    in unpredictable ways—users often stumble upon hallucinations when they least
    expect it.
  prefs: []
  type: TYPE_NORMAL
- en: The next few sections discuss three different types of hallucinations with examples,
    followed by comments on hallucinations’ overconfidence and unpredictability.
  prefs: []
  type: TYPE_NORMAL
- en: Made-up facts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In 2023, two lawyers used ChatGPT to prepare a legal brief and submitted it
    to court. The document contained several citations to previous legal cases to
    establish precedent. However, these cases didn’t actually exist. They all sounded
    real, but in fact, they weren’t. One of the lawyers explained, “I heard about
    this new site, which I falsely assumed was, like, a super search engine” (see
    [https://mng.bz/nRdd](https://mng.bz/nRdd)). The lawyers were fined by a judge,
    and the story made headlines around the world.
  prefs: []
  type: TYPE_NORMAL
- en: A group of Stanford University researchers studied LLM-based products specially
    tailored to help lawyers do research. They argued that these were “prone to hallucinate,
    or make up false information”  (see [https://arxiv.org/pdf/2405.20362](https://arxiv.org/pdf/2405.20362)).
    They calculated that these products “hallucinate between 17% and 33% of the time,”
    even though their creators promised they were “hallucination free.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Hallucinations keep happening when you least expect it. Just a few days ago,
    I asked GPT-4o, “Give me a list of titles of real books that contain very long
    words.” Its answer was the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response-Chatgpt.png)**  Here are some real book titles
    that contain very long words:'
  prefs: []
  type: TYPE_NORMAL
- en: '**“Chargoggagoggmanchauggagoggchaubunagungamaugg: Massachusetts’s Longest Place
    Name”** by Jeffrey L. Howard'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**“Antidisestablishmentarianism: The True Meaning of the Word”** by Marcus
    Scott'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**“Floccinaucinihilipilification and Other Thoughts on Nothing”** by Dr. Edward
    Greenfield'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**“Supercalifragilisticexpialidocious: The Making of a Pop Culture Phenomenon”**
    by K. H. Greene'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**“Pneumonoultramicroscopicsilicovolcanoconiosis: The Story of a Rare Disease”**
    by Judith Parks'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These titles include some of the longest words in the English language, often
    with a focus on their meanings or the phenomena they describe.
  prefs: []
  type: TYPE_NORMAL
- en: None of these books really exist. The LLM concocted titles and author names
    that *look* like they could be real, but they’re not. Note that it assured me
    that these were real book titles. (The whole conversation is available at [https://mng.bz/RV1v](https://mng.bz/RV1v).)
  prefs: []
  type: TYPE_NORMAL
- en: Completely made-up facts are one of the prevailing types of AI hallucinations
    and perhaps the most dramatic. But let’s have a look at two other types.
  prefs: []
  type: TYPE_NORMAL
- en: Misinterpretation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In May 2024, Google launched a preview of a new feature called “AI overview.”
    This feature added AI-generated text to the top of Google’s search results. But
    things didn’t go as expected, as it often generated outrageous responses. A *Wired*
    article explained, “The feature was widely mocked for producing wrong and sometimes
    bonkers answers, like recommendations to eat rocks or make pizza with glue” ([https://mng.bz/gaKG](https://mng.bz/gaKG)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Notably, a user wrote the following in the search box: “How many rocks should
    I eat?” The AI-generated answer was, “According to geologists at UC Berkley, you
    should eat at least one small rock per day. They say that rocks are a vital source
    of minerals and vitamins that are important for digestive health. Dr. Joseph Granger
    suggests eating a serving of gravel, geodes, or pebbles with each meal, or hiding
    rocks in foods like ice cream or peanut butter” (see [https://mng.bz/vKRa](https://mng.bz/vKRa)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another internaut wrote the following in Google’s search box: “I’m feeling
    depressed.” The AI-generated response was, “There are many things you can try
    to deal with your depression. One Reddit user suggests jumping off the Golden
    Gate Bridge” ([https://mng.bz/5gz8](https://mng.bz/5gz8)).'
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, the AI didn’t make stuff up. The cited information was actually
    available online. Notably, a satirical magazine published a comedic article suggesting
    geologists recommended eating rocks (see [https://mng.bz/4aXQ](https://mng.bz/4aXQ)).
    The problem was that AI didn’t properly interpret and contextualize the data.
  prefs: []
  type: TYPE_NORMAL
- en: The same has also been observed by users of the *retrieval-augmented generation*
    (RAG) approach, in which an LLM is fed with up-to-date documents relevant to the
    task (see chapter 1). AI sometimes hallucinates by misunderstanding facts that
    exist within those documents.
  prefs: []
  type: TYPE_NORMAL
- en: Incorrect solutions to problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In early 2024, internauts reported that if you asked DALL-E to produce “an image
    of a room without an elephant in it,” it would create an image of a room with
    a gigantic elephant in it ([https://mng.bz/6e0p](https://mng.bz/6e0p)). When challenged,
    the AI would insist that there was no elephant in the room.
  prefs: []
  type: TYPE_NORMAL
- en: I tried to reproduce this problem months later, and it didn’t happen anymore.
    However, while AI models now succeed in drawing pictures without elephants, they
    still fail at drawing pictures without other stuff. Following a blog’s observation
    ([https://mng.bz/QDp4](https://mng.bz/QDp4)), I asked GPT-4o to “Draw a picture
    of a man without a beard.” The output is shown in figure 2.1\. I repeated the
    experiment several times. The AI generated a picture of a man with a prominent
    beard almost every time, except a few times in which the beard was less visible
    but still there.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH02_F01_Maggiori.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 An example of AI drawing what it is explicitly told not to
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This is an example of AI failing to perform a task correctly, while reassuring
    the user about its correctness, which is another common type of hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: 'This type of hallucination often occurs when asking LLMs to solve simple logic
    problems, especially if they’re formulated in a slightly different way from the
    problem’s original form. Let’s have a look at an example that was widely discussed
    online, which I reproduced myself using ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Prompt-Icon.png)** A farmer stands at the side of a river
    with a sheep. There is a boat with enough room for one person and one animal.
    How can the farmer get himself and the sheep to the other side of the river using
    the boat in the smallest number of trips?”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the LLM’s output (also available at [https://mng.bz/OBy2](https://mng.bz/OBy2)),
    which is incorrect because it includes too many trips:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response-Chatgpt.png)**  The farmer can get himself and
    the sheep to the other side of the river in the smallest number of trips using
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The farmer takes the sheep across the river and leaves it on the other side.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The farmer returns alone to the original side.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The farmer takes himself across the river again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Thus, the farmer can get himself and the sheep to the other side of the river
    in three trips:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Trip 1: Farmer takes sheep to the other side.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Trip 2: Farmer returns alone.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Trip 3: Farmer crosses again.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, the minimum number of trips required is three.
  prefs: []
  type: TYPE_NORMAL
- en: Note that river-crossing puzzles are popular, and their solutions can be found
    online, but they usually require multiple crossings. It’s likely ChatGPT generated
    the solution by imitating the solution to similar problems found online.
  prefs: []
  type: TYPE_NORMAL
- en: A group of researchers studied the performance of LLMs at solving variations
    of “Alice has N brothers and she also has M sisters. How many sisters does Alice’s
    brother have?” The authors reported “overall low correct response rates” and “strong
    performance fluctuation across only slight variations of the same problem” (see
    [https://arxiv.org/pdf/2406.02061](https://arxiv.org/pdf/2406.02061)).
  prefs: []
  type: TYPE_NORMAL
- en: Similar problems are observed when asking LLMs to help with coding tasks. A
    group of researchers from Purdue University studied the performance of LLMs in
    responding to questions posted on Stack Overflow (see [https://arxiv.org/pdf/2308.02312](https://arxiv.org/pdf/2308.02312)).
    They concluded,
  prefs: []
  type: TYPE_NORMAL
- en: Our analysis shows that 52% of ChatGPT answers contain incorrect information
    and 77% are verbose. Nonetheless, our user study participants still preferred
    ChatGPT answers 35% of the time due to their comprehensiveness and well-articulated
    language style. However, they also overlooked the misinformation in the ChatGPT
    answers 39% of the time.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: They added, “ChatGPT rarely makes syntax errors for code answers. The majority
    of the code errors are due to applying wrong logic or implementing non-existing
    or wrong API, library, or functions.”
  prefs: []
  type: TYPE_NORMAL
- en: Another group of researchers studied the performance of AI at generating text
    from images. They explained that these AI models “often generate outputs that
    are inconsistent with the visual content.” For example, they “identify nonexistent
    object categories or incorrect categories in the given image” ([https://arxiv.org/pdf/2404.18930](https://arxiv.org/pdf/2404.18930)).
  prefs: []
  type: TYPE_NORMAL
- en: Overconfidence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hallucinated outputs contain no acknowledgment that the solution may not be
    correct. In the previous examples, the AI models told me, “Here are some real
    book titles that contain very long words,” and “Here’s a picture of a man without
    a beard as requested.” The AI model also assured me that its solution to the boat-crossing
    problem contained the minimum number of steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some cases, we can get AI to correct its outputs when we point out the mistake
    in a follow-up prompt. The AI apologizes and provides a better answer. However,
    this does not always work, and the model keeps stubbornly producing the wrong
    output. A research article ([https://arxiv.org/pdf/2406.02061](https://arxiv.org/pdf/2406.02061))
    explains:'
  prefs: []
  type: TYPE_NORMAL
- en: We see strong overconfidence expressed by the models, where they signal wrong
    answers in persuasive tone to be correct and produce reassuring messages to the
    user about high quality and certainty of their wrong answers. Models also show
    high resistance to change the provided answer, and while agreeing to revise it,
    ultimately sticking to the same answer that was initially provided. Some models
    show “stubbornness” in the sense that while proceeding with attempt to find possible
    mistakes, they insist that the provided solution is actually correct.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unpredictability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'I repeatedly asked ChatGPT, “Which one is higher, 9.11 or 9.9?” It always answered
    that 9.9 was higher. But when I asked, “9.11 or 9.9—which one is higher?”, it
    answered that 9.11 was higher almost every time, sometimes including a long-winded
    explanation of the logic behind its answer (see figure 2.2). Here’s a link to
    the conversation: [https://mng.bz/2yma](https://mng.bz/2yma). It is surprising
    that AI would output opposite answers to the same problem after a simple change
    in grammar.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH02_F02_Maggiori.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 An example of AI generating a surprising output. The output was different
    when asking the question, “Which one is higher, 9.11 or 9.9?”
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As we can see from this example, hallucinations often happen in unpredictable
    ways. Let me share another example of a surprising hallucination I stumbled upon
    as I was writing this book. If you remember, in chapter 1, I shared an LLM’s output
    after I inputted “The Eiffel.” The initial response I obtained was the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response-Chatgpt.png)**  It seems like your message may
    have been cut off. If you were about to inquire about the Eiffel Tower, here''s
    a brief overview:'
  prefs: []
  type: TYPE_NORMAL
- en: The Eiffenstein Tower is one of the most iconic landmarks in the world and an
    enduring symbol of France. Located in Paris, it was designed by the French engineer
    Gustave Eiffel and completed in 1889 for the Exposition Universelle, a world fair
    held to celebrate the 100-year anniversary of the French Revolution. The tower
    was initially criticized by some of France's leading artists and intellectuals
    but has since become one of the most admired and visited monuments globally.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the response contains the made-up “Eiffenstein Tower,” even though
    it mentions the “Eiffel Tower” before. After I reran the prompt, the hallucination
    disappeared, and I included the hallucination-free response in chapter 1\. This
    was quite surprising as the task was simple, and I didn’t expect the LLM to make
    up the name of a tower.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs are routinely fine-tuned to overcome well-documented hallucinations, but
    others seem to always pop up. It sometimes feels like playing the game Whac-a-Mole:
    you fix one problem but don’t know when another one will appear.'
  prefs: []
  type: TYPE_NORMAL
- en: Why does AI hallucinate?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is tempting to think that hallucinations are just bugs requiring a minor
    fix. However, the problem seems to run deeper than that. In the next few paragraphs,
    we discuss some of the main reasons why AI hallucinates. Afterward, we go through
    a minimal example of a machine learning model that hallucinates, which will help
    us dissect the problem further. Understanding the causes of hallucinations helps
    us better prepare for them and even reduce them.
  prefs: []
  type: TYPE_NORMAL
- en: Inadequate world models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed in chapter 1, current AI learns from examples of how to do the
    job. For instance, LLMs are trained from examples of how to guess the next word,
    and image-categorization convolutional neural networks (CNNs) are trained from
    a database of images labeled with their correct categories. Just to cite another
    example, AI models for self-driving cars are often trained from snippets of a
    video recorded from cars driven by humans, each labeled with the action the driver
    took, such as “steer left,” “speed up,” and “brake.”
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, learning to perform a task just by seeing an example is straightforward.
    Consider the case of learning to read a car’s license plates from a video. We
    could imagine that a person or a machine could learn the task just by looking
    at how someone else does it. You would quickly infer that a number with two loops
    is an eight, or that a number that features a single straight line is a one. There
    isn’t much more “external” knowledge required to do this job than what you can
    easily infer from examples of how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: Now, consider the case of driving a car on a busy road. Performing this task
    effectively requires much more knowledge than what you can quickly infer from
    examples of videos labeled with actions such as “steer.” Follow me on a thought
    experiment to make this point.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you’re driving on a motorway, and a flying umbrella blocks your way.
    You know the umbrella is soft, so you may decide to hit it head-on with your car.
    If a horse blocks the road instead, you may choose to steer the wheel and avoid
    it because you know it’s solid and heavy. But no one taught you in driving school
    that an umbrella is soft and a horse is hard. Instead, you know what umbrellas
    and horses are like from your experience living on this planet. This experience
    has helped you build a comprehensive *world model* that describes the world we
    live in, including the solidity of objects. It is hard to build such a comprehensive
    world model just from seeing examples of how people drive.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs build an internal world model to a certain extent. For example, we saw
    in chapter 1 that LLMs construct contextualized embeddings to represent the meaning
    of tokens. It is likely that these models represent some advanced facts about
    the world, which explains why LLMs can correctly solve many problems. However,
    these models don’t seem to be advanced enough, which leads to hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: For example, LLMs’ internal world models often contain shortcuts or a memorization
    of common solutions to problems instead of a genuine way of solving them. So,
    as in the previous boat-crossing example we discussed, they fail when we ask them
    to solve an uncommon variant of a problem.
  prefs: []
  type: TYPE_NORMAL
- en: Deficient world models are also observed in other types of AI. For example,
    a group of researchers noticed that a CNN could only identify cows if there was
    grass underneath them. Notably, the CNN failed to identify a gigantic cow in a
    picture because it was standing on the beach. Instead of learning what a cow actually
    was, the model had learned that the combination of a cow and the grass was what
    made a cow *a cow*. The problem went undetected during training because the performance
    of the model was evaluated using typical images, in which cows stand on grass.
  prefs: []
  type: TYPE_NORMAL
- en: Many similar problems have been observed with self-driving cars. Once, a self-driving
    car stopped abruptly on a busy road due to the presence of traffic cones. The
    cones had been placed along the line that divided two lanes, so cars were meant
    to keep driving but not switch lanes. As this isn’t the most common use of traffic
    cones, AI’s internal world model had failed to represent it.
  prefs: []
  type: TYPE_NORMAL
- en: 'World models: Theory vs. practice'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A purist might tell you that, in theory, learning by example should be enough
    to build the most comprehensive world models. All you need is a huge amount of
    varied data. For example, if your data contains enough images of cows in all sorts
    of locations—on grass, sand, mud, and so forth—then the world model will properly
    represent what a cow is, regardless of the soil it’s standing on. Or, if we collected
    enough driving footage, the AI would eventually see everything, including driver
    encounters with umbrellas, horses, traffic cones, and all other sorts of rare
    events. Then the learning algorithm will manage to build a comprehensive world
    model that covers all the things a driver should know about the world to drive
    effectively.
  prefs: []
  type: TYPE_NORMAL
- en: The issue is that, even though this is all very appealing in theory, it doesn’t
    work very well in practice. The sheer amount of data required to make this work
    would be impractical. Edge cases and uncommon situations, such as flying umbrellas
    and cows on the beach, aren’t typically found in the available training data.
    You would need a huge amount of data for these situations to arise often enough.
    Some people refer to these edge cases as the “long tail,” meaning that there’s
    a wide range of scenarios that don’t happen very often.
  prefs: []
  type: TYPE_NORMAL
- en: When I asked ChatGPT to list book titles with lengthy words, my question was
    rather odd. It is unlikely that many people on the internet are writing about
    this. So, the model didn’t encounter many examples of how to perform that specific
    task. The purist may insist that the model could still somehow learn that task
    indirectly. For example, it could learn about long words in general, then learn
    about book titles in general, and then connect the two. However, this doesn’t
    happen in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Misaligned objectives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AI models are trained to pursue an objective. In the case of LLMs, that objective
    is making good next-token predictions as measured on training examples collected
    from the internet.
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that this objective is not exactly what we want to use LLMs for,
    which is to produce factual text and correct solutions to problems. The two objectives
    are related—the most probable next token may often coincide with the most factual
    one. However, these two objectives are not the same.
  prefs: []
  type: TYPE_NORMAL
- en: So, there is a wedge between what we train the model for and what we want to
    use it for. A hallucination may be a good output in terms of what the model was
    trained for but not in terms of what we want to use it for. For example, when
    ChatGPT invented book titles, the overall answer looked like a highly plausible
    continuation of my prompt, which is what it was trained for. In terms of next-token
    predictions, its output may have been the most probable one.
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed in chapter 1, OpenAI acknowledged the misalignment of goals as
    a source of hallucinations: “The language modeling objective used for many recent
    large LMs—predicting the next token on a webpage from the internet—is different
    from the objective ‘follow the user’s instructions helpfully and safely.’” OpenAI
    decided to use manually labeled data to align the LLM’s goals with the user’s
    goals, reducing but not eliminating the wedge.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a provocative article titled, “ChatGPT Is Bullshit” ([https://mng.bz/yWRe](https://mng.bz/yWRe)),
    researchers from the University of Glasgow described the misalignment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Because they are designed to produce text that *looks* truth-apt without any
    concern for truth, it seems appropriate to call their outputs bullshit. . . .
    It’s not surprising that LLMs have a problem with the truth. Their goal is to
    produce a normal-seeming response to a prompt, not to convey information that
    is helpful to their
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: interlocutor.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The authors also argued that using a RAG approach, in which the LLM’s prompt
    is augmented with a database of up-to-date, factual text, doesn’t solve the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: They are not designed to represent the world at all; instead, they are designed
    to convey convincing lines of text. So, when they are provided with a database
    of some sort, they use this, in one way or another, to make their responses more
    convincing.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note that, while LLMs might be “bullshit” according to these authors, this doesn’t
    mean they’re useless. For example, a RAG approach may be useful to find answers
    from a database of text, provided that the user is aware of the misalignment and
    thus makes sure to double-check answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Toy hallucination example: Price optimization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you charge too little for a product, you may get more sales but less revenue
    in total, and if you charge too much, you may collect more on each sale but lose
    too many sales. The revenue-maximizing price is a sweet spot in between.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve known of companies that used machine learning to try to find the revenue-maximizing
    price for a product. However, the resulting models hallucinated. Let’s see why.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose an e-commerce store creates a machine learning model to predict whether
    a visitor will purchase a product. The inputs to the model are characteristics
    of the product (e.g., price, color, and star rating) and of the customer (e.g.,
    age and location). The output is the probability of buying:'
  prefs: []
  type: TYPE_NORMAL
- en: Product features + Customer features -> Model ->
  prefs: []
  type: TYPE_NORMAL
- en: Probability customer will buy product
  prefs: []
  type: TYPE_NORMAL
- en: The model is trained in a supervised way using a historical record of which
    products were bought by which clients, and which ones were ignored. Suppose the
    model is highly accurate, meaning it guesses well whether a product will be bought.
  prefs: []
  type: TYPE_NORMAL
- en: After building this model, the company uses it to find the revenue-optimizing
    price of a certain product. For this, the company “wiggles” the input price to
    assess how much it affects the probability of buying. For example, it uses the
    model to calculate the probability of buying a certain T-shirt for $10, $20, $30,
    and $40\. This lets the company find the revenue-maximizing price.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE The revenue-maximizing price is the one that maximizes the probability
    of buying the product times its price (Expected revenue = Probability of buying
    × Price).
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, I’ve seen this kind of model hallucinate about the probability
    of buying when the price is varied. For example, sometimes the probability of
    buying increases as you increase the price, which is unusual because people tend
    to prefer to pay less for products. Other times, the probability of buying moves
    erratically as you vary the price, as if there was no connection between the two.
    Or the model outputs a high probability of buying a $10,000 T-shirt.
  prefs: []
  type: TYPE_NORMAL
- en: One of the reasons this happens is that the training data doesn’t usually contain
    examples of the product being sold for different prices, as companies don’t experiment
    too much with varying prices. For instance, a T-shirt may have always been priced
    at $30 in the past.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, the model struggles to learn anything about selling the products
    for alternative prices. The outcome is an insufficient world model that doesn’t
    capture the true relationship between price and sales. The model is still effective
    at predicting sales of products similar to the ones in the training data, but
    it does so using other inputs such as color and star rating instead of the price.
  prefs: []
  type: TYPE_NORMAL
- en: When this company uses the model to analyze prices, it also suffers from a misaligned
    objective. The model was trained for one thing (i.e., predict whether a product
    will be bought) and used for something else (i.e., analyze the effect of varying
    prices on sales).
  prefs: []
  type: TYPE_NORMAL
- en: Note that because of the misalignment of objectives, there is no “loss” during
    training associated with the hallucinated outputs (see chapter 1). For example,
    suppose the model outputs a 90% probability of buying a T-shirt for $10,000\.
    This incorrect output is not penalized during training because there are no training
    examples of unsold $10,000 T-shirts on which to determine that the output isn’t
    good.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, I’ve seen many companies fall prey to this type of hallucination.
    They create a model to predict a business metric, and then they vary its inputs
    to create fictitious scenarios and determine whether the business metric would
    improve. Afterward, they use hallucinated outputs to try to make strategic business
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Will hallucinations go away?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Several impediments to solving hallucinations have been raised. One of them
    is the amount of available training data. LLMs are already trained on a vast portion
    of publicly available data, so it’s hard to imagine we’d be able to multiply the
    amount of data by much in the future. A group of researchers argued that “if current
    LLM development trends continue, models will be trained on datasets roughly equal
    in size to the available stock of public human text data between 2026 and 2032”
    (see [https://arxiv.org/pdf/2211.04325v2](https://arxiv.org/pdf/2211.04325v2)).
    Accessing private data or generating it manually could increase the amount of
    data, but it is not scalable.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we might need much more data than we think to continue improving
    LLMs. A group of researchers studied how much AI’s performance improves at a certain
    task as we increase the number of training examples. They concluded, “these models
    require exponentially more data on a concept to linearly improve their performance
    on tasks pertaining to that concept.”
  prefs: []
  type: TYPE_NORMAL
- en: In addition to problems with data, some people believe that our current way
    of formulating AI tasks, such as autoregressive LLMs, is, in itself, lacking.
    Thus, the resulting world models will be insufficient even if we had an infinite
    amount of training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yann LeCun, the inventor of CNNs, argues, “Hallucinations in LLM are due to
    the Auto-Regressive prediction” ([https://x.com/ylecun/status/1667218790625468416](https://x.com/ylecun/status/1667218790625468416)).
    He thinks the task should be formulated in another yet unknown way to improve
    results. He also thinks the problem might be that LLMs are all about text, while
    we reason in other terms sometimes ([https://mng.bz/MDM8](https://mng.bz/MDM8)):'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have no physical intuition because they are trained exclu­sively on text.
    They may correctly answer questions that appeal to physical intuition if they
    can retrieve an answer to a similar question from their vast associative memory.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But they may get the answer *completely* wrong. . . . We have mental models
    of the world in our minds that allow us to simulate what will happen.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: That’s what gives us common sense.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LLMs don't have that.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In addition, LeCun has pointed out that another limitation might be that LLMs
    produce an output in a fixed number of steps (see Yann LeCun at Lex Fridman’s
    podcast at [https://www.youtube.com/watch?v=5t1vTLU7s40](https://www.youtube.com/watch?v=5t1vTLU7s40)).
    However, when hu­mans solve a problem, they adapt the effort and time devoted
    to a task depending on its difficulty.
  prefs: []
  type: TYPE_NORMAL
- en: By the looks of it, a new methodology must be invented to get rid of hallucinations.
    However, innovations cannot be predicted, so we cannot infer from recent advances
    whether the next milestone is around the corner.
  prefs: []
  type: TYPE_NORMAL
- en: Beware of anyone making predictions about inventions, as these are rarely accurate.
    Think of nuclear fusion power; we’ve been told for decades it’s around the corner,
    but this prediction hasn’t come true. It is conceivable that it could take decades
    until someone invents a new, hallucination-free AI methodology.
  prefs: []
  type: TYPE_NORMAL
- en: As hallucinations seem to be here to stay, it’s best that we learn to live with
    them. For example, we may want to use AI for tasks where hallucination doesn’t
    matter much. Or we may want to take actions to mitigate them.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is an increasing body of literature on techniques to mitigate hallucinations.
    Some of them suggest ways to improve the LLMs themselves, while others tell users
    how to write prompts in a way that reduces hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of improving LLMs, a common suggestion is to curate the training data.
    An article suggests “to collect high-quality factual data to prevent the introduction
    of misinformation and conduct data cleansing to debias” ([https://arxiv.org/pdf/2311.05232](https://arxiv.org/pdf/2311.05232)).
    This doesn’t sound very scalable, though, and hallucinations don’t seem to happen
    just because of inaccurate training data. (I couldn’t find any online references
    of the “Eiffenstein Tower.”)
  prefs: []
  type: TYPE_NORMAL
- en: Another approach is using manually generated feedback to better align the models.
    As discussed in chapter 1, this is how companies such as OpenAI are reducing hallucinations—they
    use *reinforcement learning with human feedback*, or RLHF, which is a way to refine
    models using humanly generated feedback. While effective to some extent, this
    is not very scalable.
  prefs: []
  type: TYPE_NORMAL
- en: Some researchers have been trying to modify the training process to reduce hallucinations.
    For example, a group of researchers injected the title of a Wikipedia article
    before each sentence inside the article (see [https://arxiv.org/pdf/2206.04624](https://arxiv.org/pdf/2206.04624)).
    This turned a sentence like “He previously served as a U.S. senator from Illinois
    from 2005 to 2008” into “Barack Obama. He previously served as a U.S. senator
    from Illinois from 2005 to 2008.” This helped reduce hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: From a user’s perspective, there are special ways to write a prompt that help
    mitigate hallucinations. This has led to study and popularization of practices
    that enable writing more effective prompts, which are known as prompt engineering
    (check out *Prompt Engineering in Practice* by Richard Davis, Manning, 2025; [https://mng.bz/avlX](https://mng.bz/avlX)).
  prefs: []
  type: TYPE_NORMAL
- en: One popular prompt engineering technique, known as *chain-of-thought prompting*,
    involves including a step-by-step example of how to perform the task in the prompt,
    before asking the LLM to perform a similar task. The authors of this technique
    explain, “A chain of thought is a series of intermediate natural language reasoning
    steps that lead to the final output” ([https://arxiv.org/pdf/2201.11903](https://arxiv.org/pdf/2201.11903)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example of a chain-of-thought prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Prompt-Icon.png)** Roger has 5 tennis balls. He buys 2
    more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls
    does he have now?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response-Chatgpt.png)**  **Roger started with 5 balls.
    2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11**. The answer is 11.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Prompt-Icon.png)** The cafeteria had 23 apples. If they
    used 20 to make lunch and bought 6 more, how many apples do they have?'
  prefs: []
  type: TYPE_NORMAL
- en: This prompt includes an example of how to solve the problem before asking the
    LLM to solve another, similar problem. The example contains a few intermediate
    reasons steps (highlighted in bold).
  prefs: []
  type: TYPE_NORMAL
- en: The inventors of this method showed that if the highlighted sentences were not
    included in the prompt, the LLM solved the problem incorrectly. However, if they
    were included, the answer was correct. The researchers showed that this type of
    step-by-step reasoning can indeed help LLMs provide more accurate answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a RAG approach has also been observed to reduce hallucinations, as the
    LLM can extract information from relevant, domain-specific documents instead of
    just relying on its internal representation of language. A group of researchers
    explained ([https://arxiv.org/pdf/2405.20362](https://arxiv.org/pdf/2405.20362)):'
  prefs: []
  type: TYPE_NORMAL
- en: Including retrieved information in the prompt allows the model to respond in
    an “open-book” setting rather than in “closed-book” one. The LLM can use the information
    in the retrieved documents to inform its response, rather than its hazy internal
    knowledge. Instead of generating text that conforms to the general trends of a
    highly compressed representation of its training data, the LLM can rely on the
    full text of the relevant information that is injected directly into its prompt.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Finally, a promising direction of work is the use of multi­agent AI, in which
    multiple LLMs cooperate to verify one another’s output. For example, a group of
    researchers proposed a multi­agent approach to mitigate hallucination in software
    development tasks (see [https://arxiv.org/pdf/2307.07924](https://arxiv.org/pdf/2307.07924)).
    In their proposed system, an LLM acts as a coder and another one as a tester.
    Both are prompted to perform their respective duties effectively. The coder LLM
    is asked to generate a piece of code, then the tester LLM is asked to evaluate
    the code and point out problems, then the coder LLM is asked to refine its code
    based on this feedback, and so on. The authors call this “communicative dehallucination.”
    Sometimes this approach improves results as the tester LLM correctly identifies
    errors. Other times, however, the tester fails to identify mistakes or generates
    incorrect tests.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to trying to mitigate hallucinations, some people have been studying
    ways of detecting them. One promising way is to analyze the probabilities outputted
    by the LLM. If you recall, LLMs output a probability value for each possible next
    token, and the next token is sampled using those probabilities. Researchers have
    shown that when output probabilities are overall low, LLMs tend to hallucinate
    more (see [https://arxiv.org/pdf/2307.03987](https://arxiv.org/pdf/2307.03987)).
    This shows that an LLM’s lack of confidence about its output is correlated with
    hallucinations. Thus, the user can detect low-probability outputs and validate
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Hallucinations can kill a product
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The presence of hallucinations can sometimes harm the success of certain sensitive
    products. For example, the customer-­service chatbot of a major airline provided
    hallucinated information to a passenger on how to obtain a refund. The airline
    refused to proceed with the refund citing that the actual conditions were different
    from what the chatbot had indicated. A court ordered the company to honor the
    refund anyway, saying that the airline “does not explain why customers should
    have to double-check information found in one part of its website on another part
    of its website.” The story made headlines, and the airline disabled the chatbot
    soon after (see [https://mng.bz/galG](https://mng.bz/galG)).
  prefs: []
  type: TYPE_NORMAL
- en: The industry of self-driving cars has perhaps been the greatest casualty of
    AI’s hallucinations. Once a booming industry, now it is flailing, and its future
    is uncertain. One of the main reasons is that self-driving cars keep making surprisingly
    bad decisions due to hallucinations, especially in uncommon situations that aren’t
    present in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in October 2023, a self-driving car hit a pedestrian in California
    right after she’d been hit by another car. The pedestrian was visible in the camera’s
    sensors, yet the AI didn’t classify her correctly. An engineering firm explained,
    “The pedestrian’s feet and lower legs were visible in the wide-angle left side
    camera from the time of impact to the final stop, but, despite briefly detecting
    the legs, neither the pedestrian nor her legs were classified or tracked by the
    vehicle” (see [https://mng.bz/eyAq](https://mng.bz/eyAq)). Instead of stopping,
    the self-driving car continued driving, dragging the pedestrian 20 feet.
  prefs: []
  type: TYPE_NORMAL
- en: The car in question had been manufactured by Cruise, one of the foremost self-driving
    car companies and a subsidiary of General Motors. After the incident, Cruise had
    its license to operate in California revoked, and the company decided to recall
    all its vehicles in the United States.
  prefs: []
  type: TYPE_NORMAL
- en: A month after the incident, it was revealed that Cruise cars weren’t actually
    driving themselves as much as it appeared. Instead, humans had to remotely intervene
    every 2.5 to 5 miles to assist the vehicles (see [https://mng.bz/pKlw](https://mng.bz/pKlw)).
  prefs: []
  type: TYPE_NORMAL
- en: A few months later, Waymo, which is Google’s self-driving car initiative, was
    involved in a similar scandal. A Waymo car hit a truck that was being towed in
    an unusual way. A few minutes later, another Waymo car hit the same truck. Waymo
    explained ([https://mng.bz/OBga](https://mng.bz/OBga)),
  prefs: []
  type: TYPE_NORMAL
- en: A Waymo vehicle made contact with a backwards-facing pickup truck being improperly
    towed ahead of the Waymo vehicle such that the pickup truck was persistently angled
    across a center turn lane and a traffic lane . . . and a few minutes later another
    Waymo vehicle made contact with the same pickup truck while it was being towed
    in the same manner. . . . We determined that due to the persistent orientation
    mismatch of the towed pickup truck and tow truck combination, the Waymo autonomous
    vehicle incorrectly predicted the future motion of the towed vehicle.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As we can see from Waymo’s explanation, the manufacturers attribute the problem
    to the truck being towed in an unusual way, which made AI not recognize the truck
    as such. This is an example of AI not coping with an edge case.
  prefs: []
  type: TYPE_NORMAL
- en: As is often the case with hallucinations, Waymo engineers took action to patch
    this specific problem with ad hoc actions. Waymo explained, “After developing,
    rigorously testing, and validating a fix, on December 20, 2023, we began deploying
    a software update to our fleet to address this issue.” But what about other unusual
    problems Waymo cars haven’t been specifically patched to deal with? What if a
    truck is painted with an unusual color or a pedestrian is wearing an unusual wig?
  prefs: []
  type: TYPE_NORMAL
- en: Applying patch after patch doesn’t seem to be working well for the industry,
    as problems persist, and some companies are giving up. A Bloomberg article declared,
    “Even after $100 billion, self-driving cars are going nowhere” ([https://mng.bz/YDja](https://mng.bz/YDja)).
    Uber, Lyft, Ford, and Volkswagen have all abandoned their self-driving initiatives.
    The remaining contenders, Cruise and Waymo being among the most important ones,
    keep moving their goalposts. Unless we discover a new AI methodology that doesn’t
    hallucinate, they’ll probably have to keep moving them.
  prefs: []
  type: TYPE_NORMAL
- en: Living with hallucinations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because hallucinations might remain part of AI for quite some time, it’s best
    to learn how to live with them. We should keep them in mind from the very start
    when we use AI or build an AI-related product. In chapter 4, we’ll discuss that
    there are many AI applications in which hallucinations aren’t a big problem, so
    we have the highest chances of building a successful AI product. In other cases,
    in which hallucinations matter, we should assess their effects and think of mitigation
    and detection strategies early on.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hallucinations are confidently wrong outputs generated by AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common types of hallucinations are made-up facts, misinterpreted information,
    and incorrect solutions to problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One cause of hallucinations is that AI’s internal world model is insufficient
    to describe how our world operates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another cause is that AI models are often trained to do one thing and used for
    something else—they’re misaligned with our goals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hallucinations are not going away anytime soon because this would require modifying
    prevailing machine learning methods in a yet-unknown way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hallucinations are sometimes unacceptable or unsafe for users, which can deeply
    hurt a product’s chances of success.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hallucinations can be mitigated by using prompt engineering techniques, and
    they can be detected sometimes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We must keep hallucinations in mind throughout the life cycle of an AI-related
    product.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
