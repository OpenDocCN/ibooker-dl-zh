["```py\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True,\n                             download=True, transform=transform)\n\n```", "```py\nfrom torchvision import datasets\n```", "```py\n__len__(self) \n```", "```py\n__getitem__(self, index) \n```", "```py\nfrom torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, data, transforms=None):\n        self.data = data\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        if self.transforms:\n            sample = self.transforms(sample)\n        return sample\n```", "```py\n# Generate synthetic data\ntorch.manual_seed(0)  # For reproducibility\nx = torch.arange(0, 100, dtype=torch.float32)\ny = 2 * x – 1\n```", "```py\nclass CustomDataset(Dataset):\n    def __init__(self, x, y):\n        \"\"\"\n Initialize the dataset with x and y values.\n Arguments:\n x (torch.Tensor): The input features.\n y (torch.Tensor): The output labels.\n \"\"\"\n        self.x = x\n        self.y = y\n\n    def __len__(self):\n        \"\"\"\n Return the total number of samples in the dataset.\n \"\"\"\n        return len(self.x)\n\n    def __getitem__(self, idx):\n        \"\"\"\n Fetch the sample at index `idx` from the dataset.\n Arguments:\n idx (int): The index of the sample to retrieve.\n \"\"\"\n        return self.x[idx], self.y[idx]\n```", "```py\n# Create an instance of CustomDataset\ndataset = CustomDataset(x, y)\n\n# Use DataLoader to handle batching and shuffling\ndata_loader = DataLoader(dataset, batch_size=10, shuffle=True)\n\n# Iterate over the DataLoader\nfor batch_idx, (inputs, labels) in enumerate(data_loader):\n    print(f\"Batch {batch_idx+1}\")\n    print(\"Inputs:\", inputs)\n    print(\"Labels:\", labels)\n    # Break after the first batch for demonstration\n    if batch_idx == 0:\n        break\n```", "```py\n# Create the FashionMNIST dataset\nfashion_mnist_train = datasets.FashionMNIST(root='./data', train=True, \n                                   download=True, transform=transform)\n```", "```py\ncustom_class_to_idx = {'rabbit': 0, 'dog': 1, 'cat': 2}\ndataset = ImageFolder(\n  root='data/animals',\n  target_transform=\n    lambda x: custom_class_to_idx[dataset.classes[x]]\n)\ndataset.class_to_idx = custom_class_to_idx\nprint(dataset.class_to_idx)\n```", "```py\nroot/sarcasm/document1.txt\nroot/sarcasm/document2.txt\nroot/sarcasm/document3.txt\nroot/factual/factdoc1.rtf\nroot/factual/factdoc2.doc\n```", "```py\nimport torch\nfrom torchvision.datasets import FakeData\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n# Define transformations (if needed)\ntransform = transforms.Compose([\n    transforms.ToTensor(),  \n    transforms.Normalize((0.5,), (0.5,))  \n])\n\n# Create FakeData\nfake_dataset = FakeData(size=100, image_size=(3, 224, 224), \n                        num_classes=10, transform=transform)\n\n# DataLoader\ndata_loader = DataLoader(fake_dataset, batch_size=10, shuffle=True)\n```", "```py\n# Load the entire Fashion-MNIST dataset\ndataset = datasets.FashionMNIST(root='./data', \n                                download=True, transform=transform)\n```", "```py\nfrom torch.utils.data import random_split\n\ntotal_count = len(dataset)\ntrain_count = int(0.7 * total_count)\nval_count = int(0.15 * total_count)\n\n# Ensures all data is used\ntest_count = total_count – train_count – val_count  \n\ntrain_dataset, val_dataset, test_dataset = \n     random_split(dataset, [train_count, val_count, test_count])\n```", "```py\n# Define transformations\ntrain_transform = transforms.Compose([\n    transforms.Resize((150,150)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.RandomAffine(\n        degrees=0,  # No rotation\n        translate=(0.2, 0.2),  # Translate up to 20% x and y\n        scale=(0.8, 1.2),  # Zoom in or out by 20%\n        shear=20,  # Shear by up to 20 degrees\n    ),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n])\n\n# Load the datasets\ntrain_dataset = datasets.ImageFolder(root=training_dir, \n                                     transform=train_transform)\nval_dataset = datasets.ImageFolder(root=validation_dir, \n                                   transform=train_transform)\n\n# Data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n```", "```py\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import CIFAR10\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Load CIFAR10 dataset\ndataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n```", "```py\nfrom torch.utils.data import DataLoader\n\n# DataLoader with multiple workers\ndata_loader = DataLoader(dataset, batch_size=64, shuffle=True, \n                         `num_workers``=``4`)\n```", "```py\nimport torch\n\n# Dummy model and optimizer setup\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(3 * 32 * 32, 500),\n    torch.nn.ReLU(),\n    torch.nn.Linear(500, 10)\n)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Training loop\ndef train(model, data_loader):\n    model.train()\n    for batch_idx, (inputs, targets) in enumerate(data_loader):\n        # Reshape inputs to match the model's expected input\n        inputs = inputs.view(inputs.size(0), –1)\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n\n        # Backward pass and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch_idx % 100 == 0:\n            print(f\"Train Epoch: {batch_idx} Loss: {loss.item()}\")\n\ntrain(model, data_loader)\n```"]