["```py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\ndata = pd.read_csv(\"./AB_NYC_2019.csv\")\nexcluding_list = ['price', 'id', 'latitude', 'longitude', 'host_id', \n                  'last_review', 'name', 'host_name']              ①\nlow_card_categorical = ['neighbourhood_group',\n   _    _               'room_type']                               ②\nhigh_card_categorical = ['neighbourhood']                          ③\ncontinuous = ['minimum_nights', 'number_of_reviews', 'reviews_per_month', \n              'calculated_host_listings_count', 'availability_365']\ntarget_mean = (\n    (data[\"price\"] > data[\"price\"].mean())\n    .astype(int))                                                  ④\ntarget_median = (\n    (data[\"price\"] > data[\"price\"].median())\n    .astype(int))                                                  ⑤\ntarget_multiclass = pd.qcut(\n    data[\"price\"], q=5, labels=False)                              ⑥\ntarget_regression = data[\"price\"]                                  ⑦\ncategorical_onehot_encoding = OneHotEncoder(handle_unknown='ignore')\ncategorical_ord_encoding = \nOrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=np.nan)\nnumeric_passthrough = SimpleImputer(strategy=”constant\", fill_value=0)\n\ncolumn_transform = ColumnTransformer(\n    [('low_card_categories', \n      categorical_onehot_encoding, \n      low_card_categorical),\n     ('high_card_categories', \n      categorical_ord_encoding, \n      high_card_categorical),\n     ('numeric', \n      numeric_passthrough, \n      continuous),\n    ],\n    remainder='drop',\n    verbose_feature_names_out=False,\n    sparse_threshold=0.0)                                          ⑧\n```", "```py\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import KFold, cross_validate\n\naccuracy = make_scorer(accuracy_score)\ncv = KFold(5, shuffle=True, random_state=0)\n\ncolumn_transform = ColumnTransformer(\n    [('categories', categorical_onehot_encoding, low_card_categorical),\n     ('numeric', numeric_passthrough, continuous)],              ①\n    remainder='drop',\n    verbose_feature_names_out=False,\n    sparse_threshold=0.0)\n\nmodel = DecisionTreeClassifier(random_state=0)                   ②\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('modeling', model)])                                       ③\n\ncv_scores = cross_validate(estimator=model_pipeline, \n                           X=data, \n                           y=target_median,\n                           scoring=accuracy,\n                           cv=cv, \n                           return_train_score=True,\n                           return_estimator=True)                ④\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nfit_time = np.mean(cv_scores['fit_time'])\nscore_time = np.mean(cv_scores['score_time'])\nprint(f\"{mean_cv:0.3f} ({std_cv:0.3f})\", \n      f\"fit: {fit_time:0.2f}\",\n      f\"secs pred: {score_time:0.2f} secs\")                      ⑤\n```", "```py\n0.761 (0.005) fit: 0.22 secs pred: 0.01 secs\n```", "```py\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\naccuracy = make_scorer(accuracy_score)\ncv = KFold(5, shuffle=True, random_state=0)\nmodel = BaggingClassifier(\n    estimator=DecisionTreeClassifier(),                     ①\n    n_estimators=300, \n    bootstrap=True,                                         ②\n    max_samples=1.0,                                        ③\n    max_features=1.0,                                       ④\n    random_state=0)\n\ncolumn_transform = ColumnTransformer(\n    [('categories', categorical_onehot_encoding, low_card_categorical),\n     ('numeric', numeric_passthrough, continuous)],\n    remainder='drop',\n    verbose_feature_names_out=False,\n    sparse_threshold=0.0)                                   ⑤\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('modeling', model)])                                  ⑥\n\ncv_scores = cross_validate(estimator=model_pipeline, \n                           X=data, \n                           y=target_median,\n                           scoring=accuracy,\n                           cv=cv, \n                           return_train_score=True,\n                           return_estimator=True)           ⑦\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nfit_time = np.mean(cv_scores['fit_time'])\nscore_time = np.mean(cv_scores['score_time'])\nprint(f\"{mean_cv:0.3f} ({std_cv:0.3f})\", \n      f\"fit: {fit_time:0.2f}\", \n      f\"secs pred: {score_time:0.2f} secs\")                 ⑧\n```", "```py\n0.809 (0.004) fit: 37.93 secs pred: 0.83 secs\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\naccuracy = make_scorer(accuracy_score)\ncv = KFold(5, shuffle=True, random_state=0)\nmodel = RandomForestClassifier(n_estimators=300, \n                               min_samples_leaf=3,\n                               random_state=0)              ①\n\ncolumn_transform = ColumnTransformer(\n    [('categories', categorical_onehot_encoding, low_card_categorical),\n     ('numeric', numeric_passthrough, continuous)],\n    remainder='drop',\n    verbose_feature_names_out=False,\n    sparse_threshold=0.0)                                   ②\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('modeling', model)])                                  ③\n\ncv_scores = cross_validate(estimator=model_pipeline, \n                           X=data, \n                           y=target_median,\n                           scoring=accuracy,\n                           cv=cv, \n                           return_train_score=True,\n                           return_estimator=True)           ④\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nfit_time = np.mean(cv_scores['fit_time'])\nscore_time = np.mean(cv_scores['score_time'])\nprint(f\"{mean_cv:0.3f} ({std_cv:0.3f})\", \n      f\"fit: {fit_time:0.2f}\", \n      f\"secs pred: {score_time:0.2f} secs\")                 ⑤\n```", "```py\n0.826 (0.004) fit: 12.29 secs pred: 0.68 secs\n```", "```py\nfrom sklearn.ensemble import ExtraTreesClassifier\n\naccuracy = make_scorer(accuracy_score)\ncv = KFold(5, shuffle=True, random_state=0)\nmodel = ExtraTreesClassifier(n_estimators=300, \n                             min_samples_leaf=3,\n                             random_state=0)                  ①\n\ncolumn_transform = ColumnTransformer(\n    [('categories', categorical_onehot_encoding, low_card_categorical),\n     ('numeric', numeric_passthrough, continuous)],\n    remainder='drop',\n    verbose_feature_names_out=False,\n    sparse_threshold=0.0)                                     ②\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('modeling', model)])                                    ③\n\ncv_scores = cross_validate(estimator=model_pipeline, \n                           X=data, \n                           y=target_median,\n                           scoring=accuracy,\n                           cv=cv, \n                           return_train_score=True,\n                           return_estimator=True)             ④\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nfit_time = np.mean(cv_scores['fit_time'])\nscore_time = np.mean(cv_scores['score_time'])\nprint(f\"{mean_cv:0.3f} ({std_cv:0.3f})\", \n      f\"fit: {fit_time:0.2f}\",\n      f\"secs pred: {score_time:0.2f} secs\")                   ⑤\n```", "```py\n0.823 (0.004) fit: 4.99 secs pred: 0.42 secs\n```", "```py\nfrom sklearn.tree import DecisionTreeRegressor\nimport numpy as np\n\nclass GradientBoosting():\n    def __init__(self, learning_rate=0.1, n_estimators=10, **params):\n        self.learning_rate = learning_rate\n        self.n_estimators = n_estimators\n        self.params = params\n        self.trees = list()\n\n    def sigmoid(self, x):\n        x = np.clip(x, -100, 100)\n        return 1 / (1 + np.exp(-x))                          ①\n\n    def logit(self, x, eps=1e-6):\n        xp = np.clip(x, eps, 1-eps)\n        return np.log(xp / (1 - xp))                         ②\n\n    def gradient(self, y_true, y_pred):\n        gradient =  y_pred - y_true                          ③\n        return gradient\n\n    def fit(self, X, y):\n        self.init = self.logit(np.mean(y))                   ④\n        y_pred = self.init * np.ones((X.shape[0],))\n        for k in range(self.n_estimators):\n            gradient = self.gradient(self.logit(y), y_pred)\n            tree = DecisionTreeRegressor(**self.params)\n            tree.fit(X, -gradient)                           ⑤\n            self.trees.append(tree)\n            y_pred += (\n                self.learning_rate * tree.predict(X)\n            )                                                ⑥\n\n    def predict_proba(self, X):\n        y_pred = self.init * np.ones((X.shape[0],))\n        for tree in self.trees:\n            y_pred += (\n                self.learning_rate * tree.predict(X)\n            )                                                ⑦\n        return self.sigmoid(y_pred)\n\n    def predict(self, X, threshold=0.5):\n        proba = self.predict_proba(X)\n        return np.where(proba >= threshold, 1, 0)\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(range(len(data)), test_size=0.2, \nrandom_state=0)                                              ①\n\ncls = GradientBoosting(n_estimators=300, \n                       learning_rate=0.1,\n                       max_depth=4,\n                       min_samples_leaf=3, \n                       random_state=0)                       ②\n\nX = column_transform.fit_transform(data.iloc[train])         ③\ny = target_median[train]                                     ④\n\ncls.fit(X, y)\n\nXt = column_transform.transform(data.iloc[test])             ⑤\nyt = target_median[test]                                     ⑥\n\npreds = cls.predict(Xt)\nscore = accuracy_score(y_true=yt, y_pred=preds)              ⑦\nprint(f\"Accuracy: {score:0.5f}\")                             ⑧\n```", "```py\nAccuracy: 0.82503\n```", "```py\nimport matplotlib.pyplot as plt\n\nproba = cls.predict_proba(Xt)                              ①\nplt.figure(figsize=(8, 6))\nplt.hist(proba, \n         bins=30,\n         density=True,\n         color='blue',\n         alpha=0.7)                                        ②\nplt.xlabel('Predicted Probabilities')\nplt.ylabel('Density')\nplt.title('Histogram of Predicted Probabilities')\nplt.grid(True)\nplt.show()\n```", "```py\nclass GradientBoostingRegression(GradientBoosting):\n\n    def fit(self, X, y):\n        self.init = np.mean(y)                              ①\n        y_pred = self.init * np.ones((X.shape[0],))\n\n        for k in range(self.n_estimators):\n            gradient = self.gradient(y, y_pred)\n            tree = DecisionTreeRegressor(**self.params)\n            tree.fit(X, -gradient)                          ②\n            self.trees.append(tree)\n            y_pred += (\n                self.learning_rate * tree.predict(X)\n            )                                               ③\n\n    def predict(self, X):\n        y_pred = self.init * np.ones((X.shape[0],))\n        for tree in self.trees:\n            y_pred += (\n                self.learning_rate * tree.predict(X)\n            )                                               ④\n        return y_pred\n\nreg = GradientBoostingRegression(n_estimators=300,\n                                 learning_rate=0.1,\n                                 max_depth=4,\n                                 min_samples_leaf=3, \n                                 random_state=0)\n\nreg.fit(X, y)\n\nproba = reg.predict(Xt)\nplt.figure(figsize=(8, 6))\nplt.hist(proba, \n         bins=10,\n         density=True,\n         color='blue',\n         alpha=0.7)                                         ⑤\nplt.xlabel('Predicted Probabilities')\nplt.ylabel('Density')\nplt.title('Histogram of Predicted Probabilities')\nplt.grid(True)\nplt.show()\n```", "```py\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\naccuracy = make_scorer(accuracy_score)\ncv = KFold(5, shuffle=True, random_state=0)\nmodel = GradientBoostingClassifier(\n    n_estimators=300,\n    learning_rate=0.1,\n    max_depth=4,\n    min_samples_leaf=3, \n    random_state=0\n)                                                            ①\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('modeling', model)])                                   ②\n\ncv_scores = cross_validate(\n    estimator=model_pipeline, \n    X=data, \n    y=target_median,\n    scoring=accuracy,\n    cv=cv, \n    return_train_score=True,\n    return_estimator=True\n)                                                            ③\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nfit_time = np.mean(cv_scores['fit_time'])\nscore_time = np.mean(cv_scores['score_time'])\nprint(f\"{mean_cv:0.3f} ({std_cv:0.3f})\", \n      f\"fit: {fit_time:0.2f}\",\n      f\"secs pred: {score_time:0.2f} secs\")                  ④\n```", "```py\n0.826 (0.004) fit: 16.48 secs pred: 0.07 secs\n```", "```py\nmodel = GradientBoostingClassifier(\n    n_estimators=1000,                                      ①\n    learning_rate=0.1,\n    validation_fraction=0.2,                                ②\n    n_iter_no_change=10,                                    ③\n    max_depth=4,\n    min_samples_leaf=3, \n    random_state=0\n)\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('modeling', model)])\n\ncv_scores = cross_validate(estimator=model_pipeline, \n                           X=data, \n                           y=target_median,\n                           scoring=accuracy,\n                           cv=cv, \n                           return_train_score=True,\n                           return_estimator=True)\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nfit_time = np.mean(cv_scores['fit_time'])\nscore_time = np.mean(cv_scores['score_time'])\nprint(f\"{mean_cv:0.3f} ({std_cv:0.3f})\", \n      f\"fit: {fit_time:0.2f} secs pred: {score_time:0.2f} secs\")\niters = [cv_scores[\"estimator\"][i].named_steps[\"modeling\"].n_estimators_ \n         for i in range(5)]                                  ④\nprint(iters)                                                 ⑤\n```", "```py\n0.826 (0.005) fit: 6.24 secs pred: 0.04 secs\n[145, 109, 115, 163, 159]\n```", "```py\npip install XGBoost\n```", "```py\nconda install -c conda-forge py-XGBoost\n```", "```py\nfrom XGBoost import XGBClassifier\n\naccuracy = make_scorer(accuracy_score)\ncv = KFold(5, shuffle=True, random_state=0)\nxgb = XGBClassifier(booster='gbtree',                        ①\n                    objective='reg:logistic',                ②\n                    n_estimators=300,\n                    max_depth=4,\n                    min_child_weight=3)                      ③\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('XGBoost', xgb)])\n\ncv_scores = cross_validate(estimator=model_pipeline, \n                           X=data, \n                           y=target_median,\n                           scoring=accuracy,\n                           cv=cv, \n                           return_train_score=True,\n                           return_estimator=True)\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nfit_time = np.mean(cv_scores['fit_time'])\nscore_time = np.mean(cv_scores['score_time'])\nprint(f\"{mean_cv:0.3f} ({std_cv:0.3f})\", \n      f\"fit: {fit_time:0.2f}\",\n      f\"secs pred: {score_time:0.2f} secs\")                  ④\n```", "```py\n0.826 (0.004) fit: 0.84 secs pred: 0.05 secs\n```", "```py\nclass NewtonianGradientBoosting(GradientBoosting):              ①\n    \"\"\"the Newton-Raphson method is used to update the predictions\"\"\"\n\n    reg_lambda = 0.25                                           ②\n\n    def hessian(self, y_true, y_pred):\n        hessian = np.ones_like(y_true)                          ③\n        return hessian\n\n    def fit(self, X, y):\n        self.init = self.logit(np.mean(y))\n        y_pred = self.init * np.ones((X.shape[0],))\n\n        for k in range(self.n_estimators):\n            gradient = self.gradient(self.logit(y), y_pred)\n            hessian = self.hessian(self.logit(y), y_pred)\n            tree = DecisionTreeRegressor(**self.params)\n            tree.fit(\n                X, \n                -gradient / (\n                    hessian + self.reg_lambda\n                )\n            )                                                   ④\n            self.trees.append(tree)\n            y_pred += self.learning_rate * tree.predict(X)\n\ncls = NewtonianGradientBoosting(n_estimators=300,\n                                learning_rate=0.1,\n                                max_depth=4,\n                                min_samples_leaf=3, \n                                random_state=0)                 ⑤\n\ncls.fit(X, y)                                                   ⑥\npreds = cls.predict(Xt)\nscore = accuracy_score(y_true=yt, y_pred=preds)\nprint(f\"Accuracy: {score:0.5f}\")                                ⑦\n```", "```py\nAccuracy: 0.82514\n```", "```py\nimport numpy as np\n\ndef gini_impurity(y):\n    _, counts = np.unique(y, return_counts=True)\n    probs = counts / len(y)\n    return 1 - np.sum(probs**2)                              ①\n\ndef histogram_split(x, y, use_histogram, n_bins=256):\n    if use_histogram:\n        hist, thresholds = np.histogram(\n            x, bins=n_bins, density=False\n        )                                                    ②\n    else:\n        thresholds = np.unique(x)                            ③\n    best_score = -1\n    best_threshold = None                                    ④\n    for threshold in thresholds:                             ⑤\n        left_mask = x <= threshold\n        right_mask = x > threshold\n        left_y = y[left_mask]\n        right_y = y[right_mask]                              ⑥\n        score = (\n            gini_impurity(left_y) * len(left_y) \n            + gini_impurity(right_y) * len(right_y)\n        )                                                    ⑦\n        if score > best_score: q                             ⑧\n            best_threshold = threshold\n            best_score = score\n    return best_threshold, best_score                        ⑨\n```", "```py\n%%time\nhistogram_split(x=data.latitude, y=target_median, use_histogram=False)\n\nCPU times: user 46.9 s, sys: 10.1 ms, total: 46.9 s\nWall time: 46.9 s\n(40.91306, 24447.475447387256)\n```", "```py\n%%time\nhistogram_split(\n    x=data.latitude,\n    y=target_median,\n    use_histogram=True,\n    n_bins=256\n)\n\nCPU times: user 563 ms, sys: 0 ns, total: 563 ms\nWall time: 562 ms\n(40.91306, 24447.475447387256)\n```", "```py\ntrain, test = train_test_split(\n    range(len(data)),\n    test_size=0.2,\n    random_state=0\n)                                                          ①\ntrain, validation = train_test_split(\n    train,\n    test_size=0.2,\n    random_state=0\n)                                                          ②\n\nxgb = XGBClassifier(booster='gbtree',\n                    objective='reg:logistic',\n                    n_estimators=1000,\n                    max_depth=4,\n                    min_child_weight=3,\n                    early_stopping_rounds=100,             ③\n                    eval_metric='error')                   ④\n\nX = column_transform.fit_transform(data.iloc[train])\ny = target_median[train]\n\nXv = column_transform.transform(data.iloc[validation])\nyv = target_median[validation]\n\nxgb.fit(X, y, eval_set=[(Xv, yv)], verbose=False)          ⑤\n\nXt = column_transform.transform(data.iloc[test])\nyt = target_median[test]\n\npreds = xgb.predict(Xt)\nscore = accuracy_score(y_true=yt, y_pred=preds)\nprint(f\"Accuracy: {score:0.5f}\")                           ⑥\n```", "```py\nAccuracy: 0.82657\n```", "```py\nfrom lightgbm import LGBMClassifier\n\naccuracy = make_scorer(accuracy_score)\ncv = KFold(5, shuffle=True, random_state=0)\nlgbm = LGBMClassifier(boosting_type='gbdt',            ①\n                      n_estimators=300,\n                      max_depth=-1,\n                      min_child_samples=3,\n                      force_col_wise=True,             ②\n                      verbosity=0) \n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('lightgbm', lgbm)])                              ③\n\ncv_scores = cross_validate(estimator=model_pipeline, \n                           X=data, \n                           y=target_median,\n                           scoring=accuracy,\n                           cv=cv, \n                           return_train_score=True,\n                           return_estimator=True)      ④\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nfit_time = np.mean(cv_scores['fit_time'])\nscore_time = np.mean(cv_scores['score_time'])\nprint(f\"CV Accuracy {mean_cv:0.3f} ({std_cv:0.3f})\", \n      f\"fit: {fit_time:0.2f}\",\n      f\"secs pred: {score_time:0.2f} secs\")            ⑤\n```", "```py\n0.826 (0.004) fit: 1.16 secs pred: 0.16 secs\n```", "```py\nfrom lightgbm import LGBMClassifier, log_evaluation\n\ntrain, test = train_test_split(range(len(data)), test_size=0.2, \nrandom_state=0)                                             ①\ntrain, validation = train_test_split(\n    train, \n    test_size=0.2,\n    random_state=0\n)                                                           ②\n\nlgbm = LGBMClassifier(boosting_type='gbdt', \n                      early_stopping_round=150,\n                      n_estimators=1000, \n                      max_depth=-1,\n                      min_child_samples=3,\n                      force_col_wise=True,\n                      verbosity=0)                          ③\n\nX = column_transform.fit_transform(data.iloc[train])\ny = target_median[train]\n\nXv = column_transform.transform(data.iloc[validation])\nyv = target_median[validation]\n\nlgbm.fit(X, y, eval_set=[(Xv, yv)],                         ④\n         eval_metric='accuracy',                                ⑤\n         callbacks=[log_evaluation(period=0)])                  ⑥\n\nXt = column_transform.transform(data.iloc[test])\nyt = target_median[test]\n\npreds = lgbm.predict(Xt)\nscore = accuracy_score(y_true=yt, y_pred=preds)\nprint(f\"Test accuracy: {score:0.5f}\")                       ⑦\n```", "```py\nAccuracy: 0.82585\n```", "```py\nearly_stopping(\n    stopping_rounds=150,\n    first_metric_only=True,\n    verbose=False,\n    min_delta=0.0\n)\n```", "```py\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\n\naccuracy = make_scorer(accuracy_score)\ncv = KFold(5, shuffle=True, random_state=0)\n\nmodel = HistGradientBoostingClassifier(learning_rate=0.1,\n                                       max_iter=300,\n                                       max_depth=4,\n                                       min_samples_leaf=3,\n                                       random_state=0)      ①\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('modeling', model)])                                  ②\n\ncv_scores = cross_validate(estimator=model_pipeline, \n                           X=data, \n                           y=target_median,\n                           scoring=accuracy,\n                           cv=cv, \n                           return_train_score=True,\n                           return_estimator=True)           ③\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nfit_time = np.mean(cv_scores['fit_time'])\nscore_time = np.mean(cv_scores['score_time'])\nprint(f\"{mean_cv:0.3f} ({std_cv:0.3f})\", \n      f\"fit: {fit_time:0.2f}\",\n      f\"secs pred: {score_time:0.2f} secs\")                 ④\n```", "```py\n0.827 (0.005) fit: 1.71 secs pred: 0.13 secs\n```"]