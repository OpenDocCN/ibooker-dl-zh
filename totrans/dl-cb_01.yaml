- en: Chapter 1\. Tools and Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 工具和技术
- en: In this chapter we’ll take a look at common tools and techniques for deep learning.
    It’s a good chapter to read through once to get an idea of what’s what and to
    come back to when you need it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍深度学习的常见工具和技术。这是一个很好的章节，可以通读一次以了解各种情况，并在需要时回头查看。
- en: We’ll start out with an overview of the different types of neural networks that
    are covered in this book. Most of the recipes later in the book focus on getting
    things done and only briefly discuss how deep neural networks are architected.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先概述本书涵盖的不同类型的神经网络。本书后面的大部分配方都侧重于完成任务，并仅简要讨论深度神经网络的架构。
- en: We’ll then discuss where to get data from. Tech giants like Facebook and Google
    have access to tremendous amounts of data to do their deep learning research,
    but there’s enough data out there for us to do interesting stuff too. The recipes
    in this book take their data from a wide range of sources.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将讨论数据来源。像Facebook和Google这样的科技巨头可以访问大量数据来进行他们的深度学习研究，但我们也有足够的数据可以做一些有趣的事情。本书中的配方从各种来源获取数据。
- en: The next part is about preprocessing of data. This is a very important area
    that is often overlooked. Even if you have the right network setup and you have
    great data, you still need to make sure that the data you have is presented in
    the best way to the network. You want to make it as easy as possible for the network
    to learn the things it needs to learn and not get distracted by other irrelevant
    bits in the data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分是关于数据预处理的。这是一个经常被忽视的非常重要的领域。即使您拥有正确的网络设置和出色的数据，您仍然需要确保您拥有的数据以最佳方式呈现给网络。您希望尽可能地让网络学习它需要学习的东西，并且不要被数据中的其他无关紧要的部分分散注意力。
- en: 1.1 Types of Neural Networks
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1.1 神经网络的类型
- en: Throughout this chapter and indeed the book we will talk about *networks* and
    *models*. Network is short for neural network and refers to a stack of connected
    *layers*. You feed data in on one side and transformed data comes out on the other
    side. Each layer implements a mathematical operation on the data flowing through
    it and has a set of variables that can be modified that determine the exact behavior
    of the layer. *Data* here refers to a *tensor*, a vector with multiple dimensions
    (typically two or three).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和整本书中，我们将讨论*网络*和*模型*。网络是神经网络的简称，指的是一堆连接的*层*。您在一侧输入数据，转换后的数据从另一侧输出。每个层对通过它流动的数据执行数学操作，并具有一组可以修改的变量，这些变量确定了层的确切行为。这里的*数据*指的是*张量*，一个具有多个维度的向量（通常是二维或三维）。
- en: A full discussion of the different types of layers and the math behind their
    operations is beyond the scope of this book. The simplest type of layer, the *fully
    connected* layer, takes its input as a matrix, multiplies that matrix with another
    matrix called the *weights*, and adds a third matrix called the *bias*. Each layer
    is followed by an *activation* function, a mathematical function that maps the
    output of one layer to the input of the next layer. For example, a simple activation
    function called ReLU passes on all positive values, but sets negative values to
    zero.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对不同类型的层及其操作背后的数学进行全面讨论超出了本书的范围。最简单类型的层，即*全连接*层，将其输入作为矩阵，将该矩阵与另一个称为*权重*的矩阵相乘，并添加一个称为*偏置*的第三个矩阵。每个层后面都跟着一个*激活*函数，这是一个数学函数，将一个层的输出映射到下一层的输入。例如，一个简单的激活函数称为ReLU会传递所有正值，但将负值设为零。
- en: Technically the term *network* refers to the architecture, the way in which
    the various layers are connected to each other, while a *model* is a network plus
    all the variables that determine the runtime behavior. Training a model modifies
    those variables to make the predictions fit the expected output better. In practice,
    though, the two terms are often used interchangeably.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，术语*网络*指的是架构，即各个层之间连接的方式，而*模型*是一个网络加上确定运行时行为的所有变量。训练模型会修改这些变量，使预测更好地符合预期输出。然而，在实践中，这两个术语经常可以互换使用。
- en: 'The terms “deep learning” and “neural networks” in reality encompass a wide
    variety of models. Most of these networks will share some elements (for example,
    almost all classification networks will use a particular form of *loss function*).
    While the space of models is diverse, we can group most of them into some broad
    categories. Some models will use pieces from multiple categories: for example,
    many image classification networks have a fully connected section “head” to perform
    the final classification.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，“深度学习”和“神经网络”这些术语涵盖了各种模型。这些网络中的大多数将共享一些元素（例如，几乎所有分类网络都将使用特定形式的*损失函数*）。虽然模型空间多样，但我们可以将大多数模型分为一些广泛的类别。一些模型将使用来自多个类别的部分：例如，许多图像分类网络具有一个全连接部分“头部”来执行最终分类。
- en: Fully Connected Networks
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全连接网络
- en: 'Fully connected networks were the first type of network to be researched, and
    dominated interest until the late 1980s. In a fully connected network, each output
    unit is calculated as a weighted sum of all of the inputs. The term “fully connected”
    arises from this behavior: every output is connected to every input. We can write
    this as a formula:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接网络是第一种被研究的网络类型，并且在1980年代后期之前占据了主导地位。在全连接网络中，每个输出单元都被计算为所有输入的加权和。术语“全连接”源于这种行为：每个输出都连接到每个输入。我们可以将其写成一个公式：
- en: <math><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <munder><mo>∑</mo>
    <mi>j</mi></munder> <msub><mi>W</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <msub><mi>x</mi> <mi>j</mi></msub></mrow></math>
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <munder><mo>∑</mo>
    <mi>j</mi></munder> <msub><mi>W</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <msub><mi>x</mi> <mi>j</mi></msub></mrow></math>
- en: 'For brevity, most papers represent a fully connected network using matrix notation.
    In this case we are multiplying a vector of inputs with a weight matrix *W* to
    get a vector of outputs:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为简洁起见，大多数论文使用矩阵表示全连接网络。在这种情况下，我们将一个输入向量与权重矩阵*W*相乘以获得一个输出向量：
- en: <math><mrow><mi>y</mi> <mo>=</mo> <mi>W</mi> <mi>x</mi></mrow></math>
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'As matrix multiplication is a linear operation, a network that only contained
    matrix multiplies would be limited to learning linear mappings. In order to make
    our networks more expressive, we follow the matrix multiply with a nonlinear activation
    function. This can be any differentiable function, but a few are very common.
    The hyperbolic tangent, or *tanh*, function was until recently the dominant type
    of activation function, and can still be found in some models:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![Tanh activation](assets/dlcb_01in03.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: 'The difficulty with the tanh function is that it is very “flat” when an input
    is far from zero. This results in a small gradient, which means that a network
    can take a very long time to change behavior. Recently, other activation functions
    have become popular. One of the most common is the rectified linear unit, or *ReLU*,
    activation function:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![Relu activation](assets/dlcb_01in04.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: 'Finally, many networks use a *sigmoid* activation function in the last layer
    of the network. This function always outputs a value between 0 and 1\. This allows
    the outputs to be treated as probabilities:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![Sigmoid activation](assets/dlcb_01in05.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: A matrix multiplication followed by the activation function is referred to as
    a *layer* of the network. In some networks the complete network can have over
    100 layers, though fully connected networks tend to be limited to a handful. If
    we are solving a classification problem (“What type of cat is in this picture?”),
    the last layer of the network is called a *classification layer*. It will always
    have the same number of outputs as we have classes to choose from.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'Layers in the middle of the network are called *hidden layers*, and the individual
    outputs from a hidden layer are sometimes referred to as *hidden units*. The term
    “hidden” comes from the fact that these units are not directly visible from the
    outside as inputs or outputs for our model. The number of outputs in these layers
    depends on the model:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![Network layers](assets/dlcb_01in06.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: While there are some rules of thumb about how to choose the number and size
    of hidden layers, there is no general policy for choosing the best setup other
    than trial and error.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Networks
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Early research used fully connected networks to try to solve a wide variety
    of problems. But when our input is images, fully connected networks can be a poor
    choice. Images are very large: a single 256×256-pixel image (a common resolution
    for classification) has 256×256×3 inputs (3 colors for each pixel). If this model
    has a single hidden layer with 1,000 hidden units, then this layer will have almost
    200 million parameters (learnable values)! Since image models require quite a
    few layers to perform well at classification, if we implemented them just using
    fully connected layers we would end up with billions of parameters.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'With so many parameters, it would be almost impossible for us to avoid *overfitting*
    our model (overfitting is described in detail in the next chapter; it refers to
    when a network fails to generalize, but just memorizes outcomes). *Convolutional
    neural networks* (CNNs) provide a way for us to train superhuman image classifiers
    using far fewer parameters. They do this by mimicking how animals and humans see:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN Layers - from Wikipedia](assets/dlcb_01in07.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: The fundamental operation in a CNN is a *convolution*. Instead of applying a
    function to an entire input image, a convolution scans across a small window of
    the image at a time. At each location it applies a *kernel* (typically a matrix
    multiplication followed by an activation function, just like in a fully connected
    network). Individual kernels are often referred to as *filters*. The result of
    applying the kernel to the entire image is a new, possibly smaller image. For
    example, a common filter shape is (3, 3). If we were to apply 32 of these filters
    to our input image, we would need 3 * 3 * 3 (input colors) * 32 = 864 parameters—that’s
    a big savings over a fully connected network!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: CNN中的基本操作是*卷积*。与将函数应用于整个输入图像不同，卷积一次在图像的一个小窗口上扫描。在每个位置，它应用一个*核*（通常是矩阵乘法后跟激活函数，就像在全连接网络中一样）。单个核通常被称为*滤波器*。将核应用于整个图像的结果是一个新的、可能更小的图像。例如，常见的滤波器形状是（3,
    3）。如果我们将32个这些滤波器应用于我们的输入图像，我们将需要3 * 3 * 3（输入颜色）* 32 = 864个参数——这比全连接网络节省了很多！
- en: Subsampling
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 子采样
- en: 'This operation saves on the number of parameters, but now we have a different
    problem. Each layer in the network can only “look” at a 3×3 layer of the image
    at a time: if this is the case, how can we possibly recognize objects that take
    up the entire image? To handle this, a typical convolution network uses *subsampling*
    to reduce the size of the image as it passes through the network. Two common mechanisms
    are used for subsampling:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种操作节省了参数的数量，但现在我们有了一个不同的问题。网络中的每一层只能一次“看”图像的一个3×3层：如果是这种情况，我们如何可能识别占据整个图像的对象？为了处理这个问题，典型的卷积网络在图像通过网络时使用*子采样*来减小图像的大小。用于子采样的两种常见机制是：
- en: Strided convolutions
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 步幅卷积
- en: In a strided convolution, we simply skip one or more pixels while sliding our
    convolution filter across the image. This results in a smaller size image. For
    example, if our input image was 256×256, and we skip every other pixel, then our
    output image will be 128×128 (we are ignoring the issue of padding at the edges
    of the image for simplicity). This type of strided downsampling is commonly found
    in generator networks (see [“Adversarial Networks and Autoencoders”](#adversarial-networks-and-autoencoders)).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在步幅卷积中，我们在滑动卷积滤波器时简单地跳过一个或多个像素。这会导致图像尺寸变小。例如，如果我们的输入图像是256×256，并且我们跳过每隔一个像素，那么我们的输出图像将是128×128（为简单起见，我们忽略了图像边缘的填充问题）。这种步幅下采样通常在生成器网络中找到（参见[“对抗网络和自动编码器”](#adversarial-networks-and-autoencoders)）。
- en: Pooling
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 池化
- en: Instead of skipping over pixels during convolution, many networks use *pooling
    layers* to shrink their inputs. A pooling layer is actually another form of convolution,
    but instead of multiplying our input by a matrix, we apply a pooling operator.
    Typically pooling uses the *max* or *average* operator. Max pooling takes the
    largest value from each *channel* (color) over the region it is scanning. Average
    pooling instead averages all of the values over the region. (It can be thought
    of as a simple type of blurring of the input.)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 许多网络不是在卷积过程中跳过像素，而是使用*池化层*来缩小它们的输入。池化层实际上是另一种形式的卷积，但我们不是将输入乘以矩阵，而是应用池化运算符。通常，池化使用*最大*或*平均*运算符。最大池化从正在扫描的区域中的每个*通道*（颜色）中取最大值。平均池化则对该区域中的所有值进行平均。
    （可以将其视为输入的简单模糊处理。）
- en: One way to think about subsampling is as a way to increase the abstraction level
    of what the network is doing. On the lowest level, our convolutions detect small,
    local features. There are many features that are not very deep. With each pooling
    step, we increase the abstraction level; the number of features is reduced, but
    the depth of each feature increases. This process is continued until we end up
    with very few features with a high level of abstraction that can be used for prediction.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一种思考子采样的方式是将其视为增加网络所做的抽象级别的一种方式。在最低级别上，我们的卷积检测小的局部特征。有许多不太深的特征。通过每个池化步骤，我们增加了抽象级别；特征的数量减少，但每个特征的深度增加。这个过程一直持续到最终得到非常少的具有高度抽象的特征，可以用于预测。
- en: Prediction
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测
- en: After stacking a number of convolutional and pooling layers together, CNNs use
    one or two fully connected layers at the head of the network to output a prediction.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在堆叠了多个卷积和池化层之后，CNN在网络头部使用一个或两个全连接层来输出预测。
- en: Recurrent Networks
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环网络
- en: '*Recurrent neural networks* (RNNs) are similar in concept to CNNs but are structurally
    very different. Recurrent networks are frequently applied when we have a sequential
    input. These inputs are commonly found when working with text or voice processing.
    Instead of processing a single example completely (as we might use a CNN for an
    image), with sequential problems we can process only a portion of the problem
    at a time. For example, let’s consider building a network that writes Shakespearean
    plays for us. Our input would naturally be the existing plays by Shakespeare:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*循环神经网络*（RNNs）在概念上类似于CNNs，但在结构上有很大的不同。当我们有一个顺序输入时，经常应用循环网络。在处理文本或语音时，这些输入通常是常见的。与处理单个示例完全不同（就像我们可能使用CNN处理图像一样），对于顺序问题，我们可以一次只处理问题的一部分。例如，让我们考虑构建一个为我们写莎士比亚剧本的网络。我们的输入自然是莎士比亚的现有剧本：'
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'What we want the network to learn to do is to predict the next word of the
    play for us. To do so, it needs to “remember” the text that it has seen so far.
    Recurrent networks give us a mechanism to do this. They also allow us to build
    models that naturally work across inputs of varying lengths (sentences or chunks
    of speech, for example). The most basic form of an RNN looks like this:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望网络学会为我们预测剧本的下一个单词。为了做到这一点，它需要“记住”到目前为止看到的文本。循环网络为我们提供了这样的机制。它们还允许我们构建自然适用于不同长度输入（例如句子或语音块）的模型。最基本形式的RNN如下所示：
- en: '![RNN Layers - from Wikipedia](assets/dlcb_01in08.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![RNN层-来自维基百科](assets/dlcb_01in08.png)'
- en: 'Conceptually, you can think of this RNN as a very deep fully connected network
    that we have “unrolled.” In this conceptual model, each layer of the network takes
    two inputs instead of the one we are used to:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，你可以将这个RNN看作是一个非常深的全连接网络，我们已经“展开”了。在这个概念模型中，网络的每一层接受两个输入，而不是我们习惯的一个：
- en: '![RNN Layers unrolled](assets/dlcb_01in09.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![RNN层展开](assets/dlcb_01in09.png)'
- en: 'Recall that in our original fully connected network, we had a matrix multiplication
    operation like:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在我们最初的全连接网络中，我们有一个类似的矩阵乘法操作：
- en: <math><mrow><mi>y</mi> <mo>=</mo> <mi>W</mi> <mi>x</mi></mrow></math>
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>y</mi> <mo>=</mo> <mi>W</mi> <mi>x</mi></mrow></math>
- en: 'The simplest way to add our second input to this operation is to just concatenate
    it to our hidden state:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 将第二个输入添加到这个操作的最简单方法是将其简单地连接到我们的隐藏状态中：
- en: <math alttext="dollar-sign h i d d e n Subscript i Baseline equals upper W left-brace
    h i d d e n Subscript i minus 1 Baseline vertical-bar x right-brace dollar-sign"><mrow><mi>h</mi>
    <mi>i</mi> <mi>d</mi> <mi>d</mi> <mi>e</mi> <msub><mi>n</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>W</mi> <mfenced close="}" open="{" separators=""><mi>h</mi> <mi>i</mi>
    <mi>d</mi> <mi>d</mi> <mi>e</mi> <msub><mi>n</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mrow><mo>|</mo> <mi>x</mi></mrow></mfenced></mrow></math>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign h i d d e n Subscript i Baseline equals upper W left-brace
    h i d d e n Subscript i minus 1 Baseline vertical-bar x right-brace dollar-sign"><mrow><mi>h</mi>
    <mi>i</mi> <mi>d</mi> <mi>d</mi> <mi>e</mi> <msub><mi>n</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>W</mi> <mfenced close="}" open="{" separators=""><mi>h</mi> <mi>i</mi>
    <mi>d</mi> <mi>d</mi> <mi>e</mi> <msub><mi>n</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mrow><mo>|</mo> <mi>x</mi></mrow></mfenced></mrow></math>
- en: 'where in this case the “|” stands for concatenate. As with our fully connected
    network, we can apply an activation function to the output of our matrix multiplication
    to obtain our new state:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，“|”代表连接。与全连接网络一样，我们可以对矩阵乘法的输出应用激活函数以获得我们的新状态：
- en: <math alttext="dollar-sign h i d d e n Subscript i Baseline equals f left-parenthesis
    upper W left-brace h i d d e n Subscript i minus 1 Baseline vertical-bar x right-brace
    right-parenthesis dollar-sign"><mrow><mi>h</mi> <mi>i</mi> <mi>d</mi> <mi>d</mi>
    <mi>e</mi> <msub><mi>n</mi> <mi>i</mi></msub> <mo>=</mo> <mi>f</mi> <mfenced close=")"
    open="(" separators=""><mi>W</mi> <mfenced close="}" open="{" separators=""><mi>h</mi>
    <mi>i</mi> <mi>d</mi> <mi>d</mi> <mi>e</mi> <msub><mi>n</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mrow><mo>|</mo> <mi>x</mi></mrow></mfenced></mfenced></mrow></math>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign h i d d e n Subscript i Baseline equals f left-parenthesis
    upper W left-brace h i d d e n Subscript i minus 1 Baseline vertical-bar x right-brace
    right-parenthesis dollar-sign"><mrow><mi>h</mi> <mi>i</mi> <mi>d</mi> <mi>d</mi>
    <mi>e</mi> <msub><mi>n</mi> <mi>i</mi></msub> <mo>=</mo> <mi>f</mi> <mfenced close=")"
    open="(" separators=""><mi>W</mi> <mfenced close="}" open="{" separators=""><mi>h</mi>
    <mi>i</mi> <mi>d</mi> <mi>d</mi> <mi>e</mi> <msub><mi>n</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mrow><mo>|</mo> <mi>x</mi></mrow></mfenced></mfenced></mrow></math>
- en: 'With this interpretation of our RNN, we also can easily understand how it can
    be trained: we simply treat the RNN as we would an unrolled fully connected network
    and train it normally. This is referred to in literature as *backpropagation through
    time* (BPTT). If we have very long inputs, it is common to split them into smaller-sized
    pieces and train each piece independently. While this does not work for every
    problem, it is generally safe and is a widely used technique.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种对我们RNN的解释，我们也可以很容易地理解它如何被训练：我们简单地将RNN视为我们展开的全连接网络，并正常训练它。这在文献中被称为*时间反向传播*（BPTT）。如果我们有非常长的输入，通常将它们分成较小的片段并独立训练每个片段。虽然这并不适用于每个问题，但通常是安全的，并且是一种广泛使用的技术。
- en: Vanishing gradients and LSTMs
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 消失的梯度和LSTM
- en: Our naive RNN unfortunately tends to perform more poorly than we would like
    for long input sequences. This is because its structure makes it likely to encounter
    the “vanishing gradients” problem. Vanishing gradients result from the fact that
    our unrolled network is very deep. Each time we go through an activation function,
    there’s a chance it will result in a small gradient getting passed through (for
    instance, ReLU activation functions have a zero gradient for any input < 0). Once
    this happens for a single unit, no more training can be passed down further through
    the network via that unit. This results in an ever-sparser training signal as
    we go down. The observed result is extremely slow or nonexistent learning of the
    network.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的朴素RNN不幸地在处理长输入序列时表现得比我们希望的要差。这是因为其结构使得它很可能遇到“消失的梯度”问题。消失的梯度是由于我们的展开网络非常深。每次经过激活函数时，都有可能导致一个小梯度通过（例如，ReLU激活函数对于任何输入<0都有零梯度）。一旦这发生在一个单元上，就无法通过该单元进一步传递更多的训练。这导致随着我们向下进行，训练信号变得越来越稀疏。观察到的结果是网络学习非常缓慢或根本没有学习。
- en: 'To combat this, researchers developed an alternative mechanism for building
    RNNs. The basic model of unrolling our state over time is kept, but instead of
    doing a simple matrix multiply followed by the activation function, we have a
    more complex way of passing our state forward (source: [Wikipedia](https://bit.ly/2HJL86P)):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对抗这一点，研究人员开发了一种构建RNN的替代机制。保持时间上展开我们的状态的基本模型，但是不再进行简单的矩阵乘法后跟激活函数，而是通过更复杂的方式将我们的状态向前传递（来源：[维基百科](https://bit.ly/2HJL86P)）：
- en: '![LSTM Architecture](assets/dlcb_01in13.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM架构](assets/dlcb_01in13.png)'
- en: A *long short-term memory network* (LSTM) replaces our single matrix multiplication
    with four, and introduces the idea of *gates* that are multiplied with a vector.
    The key behavior that enables an LSTM to learn more effectively than vanilla RNNs
    is that there is always a path from the final prediction to any layer that preserves
    gradients. The details of how it accomplishes this are beyond the scope of this
    chapter, but several [excellent tutorials](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    exist on the web.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*长短期记忆网络*（LSTM）用四个矩阵乘法替换了我们的单个矩阵乘法，并引入了与向量相乘的*门*的概念。使LSTM比普通RNN更有效地学习的关键行为是，从最终预测到保留梯度的任何层始终存在一条路径。它是如何实现这一点的细节超出了本章的范围，但网上有几篇[优秀的教程](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)。'
- en: Adversarial Networks and Autoencoders
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗网络和自动编码器
- en: 'Adversarial networks and autoencoders do not introduce new structural components,
    like the networks we’ve talked about so far. Instead, they use the structure most
    appropriate to the problem: an adversarial network or autoencoder for images will
    use convolutions, for example. Where they differ is in how they are trained. Most
    normal networks are trained to predict an output (is this a cat?) from an input
    (a picture):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗网络和自动编码器并没有像我们迄今讨论过的网络那样引入新的结构组件。相反，它们使用最适合问题的结构：例如，用于图像的对抗网络或自动编码器将使用卷积。它们的不同之处在于它们的训练方式。大多数普通网络被训练以从输入（一张图片）预测输出（这是一只猫吗？）：
- en: '![Cat detection](assets/dlcb_01in14.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![猫检测](assets/dlcb_01in14.png)'
- en: 'Autoencoders are instead trained to output back the image they are presented:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，自动编码器被训练为输出它们所呈现的图像：
- en: '![Auto encoding cats](assets/dlcb_01in15.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![自动编码猫](assets/dlcb_01in15.png)'
- en: 'Why would we want to do this? If the hidden layers in the middle of our network
    contain a representation of the input image that has (significantly) less information
    than the original image yet from which the original image can be reconstructed,
    then this results in a form of compression: we can take any image and represent
    it just by the values from the hidden layer. One way to think about this is that
    we take the original image and use the network to project it into an abstract
    space. Each point in that space can then be converted back into an image.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们要这样做呢？如果我们网络中间的隐藏层包含比原始图像少（显着）的信息量，但原始图像可以从中重建，那么这将导致一种压缩形式：我们可以通过隐藏层的值来表示任何图像。一种思考方式是，我们将原始图像使用网络投影到一个抽象空间中。该空间中的每个点都可以转换回图像。
- en: Autoencoders have been successfully applied to small images, but the mechanism
    for training them does not scale up to larger problems. The space in the middle
    from which the images are drawn is in practice not “dense” enough, and many of
    the points don’t actually represent coherent images.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器已成功应用于小图像，但训练它们的机制无法扩展到更大的问题。从中间提取图像的空间实际上不够“密集”，许多点实际上并不代表连贯的图像。
- en: We’ll seen an example of an autoencoder network in [Chapter 13](ch13.html#autoencoders).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第13章](ch13.html#autoencoders)中看到一个自动编码器网络的示例。
- en: 'Adversarial networks are a more recent model that can actually generate realistic
    images. They work by splitting the problem into two parts: a generator network
    and a discriminator network. The generator network takes a small random seed and
    produces a picture (or text). The discriminator network tries to determine if
    an input image is “real” or if it came from the generator network.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗网络是一种更近期的模型，可以生成逼真的图像。它们通过将问题分为两部分来工作：生成器网络和鉴别器网络。生成器网络接受一个小的随机种子并生成一幅图片（或文本）。鉴别器网络试图确定输入图像是“真实的”还是来自生成器网络。
- en: 'When we train our adversarial model, we train both of these networks at the
    same time:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练我们的对抗模型时，我们同时训练这两个网络：
- en: '![Adverserial Networks](assets/dlcb_01in16.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: 对抗网络
- en: We sample some images from our generator network and feed them through our discriminator
    network. The generator network is rewarded for producing images that can fool
    the discriminator. The discriminator network also has to correctly recognize real
    images (it can’t just always say an image is a fake). By making the networks compete
    against each other, this procedure can result in a generator network that produces
    high-quality natural images. [Chapter 14](ch14.html#generating_icons) shows how
    we can use generative adversarial networks to generate icons.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从生成器网络中采样一些图像，并通过鉴别器网络进行馈送。生成器网络会受到奖励，因为它生成的图像可以欺骗鉴别器。鉴别器网络还必须正确识别真实图像（不能总是说图像是假的）。通过让网络相互竞争，这个过程可以导致生成器网络生成高质量的自然图像。[第14章](ch14.html#generating_icons)展示了我们如何使用生成对抗网络生成图标。
- en: Conclusion
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: There are a great many ways to architect a network, and the choice obviously
    is mostly driven by the purpose of the network. Designing a new type of network
    is firmly in the research realm, and even reimplementing a type of network described
    in a paper is hard. In practice the easiest thing to do is to find an example
    that does something in the direction of what you want and change it step by step
    until it really does what you want.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 设计网络有许多方法，选择显然主要取决于网络的目的。设计一种新类型的网络属于研究领域，即使重新实现一篇论文中描述的网络类型也很困难。实际上，最容易的方法是找到一个示例，该示例朝着您想要的方向做一些事情，并逐步更改，直到它真正实现您想要的功能。
- en: 1.2 Acquiring Data
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1.2 获取数据
- en: One of the key reasons why deep learning has taken off in recent years is the
    dramatic increase in the availability of data. Twenty years ago networks were
    trained with thousands of images; these days companies like Facebook and Google
    work with billions of images.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来深度学习蓬勃发展的一个关键原因是数据的可用性大幅增加。二十年前，网络是用数千张图像进行训练的；而如今，像Facebook和Google这样的公司使用数十亿张图像。
- en: Having access to all the information from their users no doubt gives these and
    other internet giants a natural advantage in the deep learning field. However,
    there are many data sources easily accessible on the internet that, with a little
    massaging, can fit many training purposes. In this section, we’ll discuss the
    most important ones. For each, we’ll look into how to acquire the data, what popular
    libraries are available to help with parsing, and what typical use cases are.
    I’ll also refer you to any recipes that use this data source.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，这些以及其他互联网巨头对用户信息的全部访问为他们在深度学习领域带来了自然优势。然而，互联网上有许多数据源很容易获取，稍加整理即可适用于许多训练目的。在本节中，我们将讨论最重要的数据源。对于每个数据源，我们将探讨如何获取数据，有哪些流行的库可用于解析，以及典型的用例是什么。我还会引导您查看使用此数据源的任何配方。
- en: Wikipedia
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维基百科
- en: Not only does the English Wikipedia comprise more than 5 million articles, but
    Wikipedia is also available in [hundreds of languages](https://en.wikipedia.org/wiki/List_of_Wikipedias),
    albeit with widely different levels of depth and quality. The basic wiki idea
    only supports links as a way to encode structure, but over time Wikipedia has
    gone beyond this.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 英文维基百科不仅包括500多万篇文章，而且维基百科也以[数百种语言](https://en.wikipedia.org/wiki/List_of_Wikipedias)提供，尽管深度和质量差异很大。基本的维基思想只支持链接作为编码结构的一种方式，但随着时间的推移，维基百科已经超越了这一点。
- en: Category pages link to pages that share a property or a subject, and since Wikipedia
    pages link back to their categories, we can effectively use them as tags. Categories
    can be very simple, like “Cats,” but sometimes encode information in their names
    that effectively assigns (key, value) pairs to a page, like “Mammals described
    in 1758.” The category hierarchy, like much on Wikipedia, is fairly ad hoc, though.
    Moreover, recursive categories can only be traced by walking up the tree.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 类别页面链接到具有相同属性或主题的页面，由于维基百科页面链接回它们的类别，我们可以有效地将它们用作标签。类别可以非常简单，比如“猫”，但有时在它们的名称中编码信息，有效地为页面分配（键，值）对，比如“1758年描述的哺乳动物”。类别层次结构，就像维基百科上的许多内容一样，相当临时。此外，递归类别只能通过沿树向上行走来跟踪。
- en: '*Templates* were originally designed as segments of wiki markup that are meant
    to be copied automatically (“transcluded”) into a page. You add them by putting
    the template’s name in `{{double braces}}`. This made it possible to keep the
    layout of different pages in sync—for example, all city pages have an info box
    with properties like population, location, and flag that are rendered consistently
    across pages.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*模板*最初被设计为维基标记的片段，意味着它们可以自动复制（“转入”）到页面中。通过将模板的名称放在`{{双大括号}}`中添加它们。这使得可以保持不同页面的布局同步，例如，所有城市页面都有一个信息框，其中包含人口、位置和旗帜等属性，这些属性在页面上呈现一致。'
- en: These templates have parameters (like the population) and can be seen as a way
    to embed structured data into a Wikipedia page. In [Chapter 4](ch04.html#movie_recommender)
    we use this to extract a set of movies that we then use to train a movie recommender
    system.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模板具有参数（如人口）并可以被视为将结构化数据嵌入维基百科页面的一种方式。在[第4章](ch04.html#movie_recommender)中，我们使用这个来提取一组电影，然后用来训练电影推荐系统。
- en: Wikidata
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维基数据
- en: '[Wikidata](https://www.wikidata.org/) is Wikipedia’s structured data cousin.
    It is lesser known and also less complete, but even more ambitious. It is intended
    to provide a common source of data that can be used by anyone under a public domain
    license. As such, it makes for an excellent source of freely available data.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[维基数据](https://www.wikidata.org/)是维基百科的结构化数据表兄弟。它不太为人所知，也不太完整，但更加雄心勃勃。它旨在提供一个可以由任何人在公共领域许可下使用的数据共享源。因此，它是一个极好的免费数据来源。'
- en: All Wikidata is stored as triplets of the form (subject, predicate, object).
    All subjects and predicates have their own entries in Wikidata that list all predicates
    that exist for them. Objects can be Wikidata entries or literals such as strings,
    numbers, or dates. This structure takes inspiration from early ideas around the
    semantic web.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 所有维基数据都以（主题，谓词，对象）的形式存储为三元组。所有主题和谓词都有自己的维基数据条目，列出了适用于它们的所有谓词。对象可以是维基数据条目，也可以是字符串、数字或日期等文字。这种结构受到早期围绕语义网络的想法的启发。
- en: 'Wikidata has its own query language that looks like SQL with some interesting
    extensions. For example:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 维基数据有自己的查询语言，看起来像SQL，具有一些有趣的扩展。例如：
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: will select a series of cats and their pictures. Anything that starts with a
    question mark is a variable. `wdt:P31`, or property 31, means “is an instance
    of,” and `wd:Q146` is the class of house cats. So the fourth line stores in `item`
    anything that is an instance of cats. The `OPTIONAL { .. }` clause then tries
    to look up pictures for the item and the last magic line tries to find a label
    for the item using the auto-language feature or, failing that, English.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 将选择一系列猫和它们的图片。任何以问号开头的内容都是一个变量。`wdt:P31`，或属性31，表示“是一个实例”，`wd:Q146`是家猫的类别。因此，第四行将任何猫的实例存储在`item`中。`OPTIONAL
    { .. }`子句然后尝试查找项目的图片，最后一个神奇的行尝试使用自动语言功能或英语找到项目的标签。
- en: In [Chapter 10](ch10.html#image_search) we use a combination of Wikidata and
    Wikipedia to acquire canonical images for categories to use as a basis for a reverse
    image search engine.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](ch10.html#image_search)中，我们使用维基数据和维基百科的组合来获取类别的规范图像，以用作反向图像搜索引擎的基础。
- en: OpenStreetMap
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开放街图
- en: '[OpenStreetMap](https://www.openstreetmap.org/) is like Wikipedia, but for
    maps. Whereas with Wikipedia the idea is that if everybody in the world put down
    everything they knew in a wiki, we’d have the best encyclopedia possible, OpenStreetMap
    (OSM) is based on the idea that if everybody put the roads they knew in a wiki,
    we’d have the best mapping system possible. Remarkably, both of these ideas have
    worked out quite well.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[开放街图](https://www.openstreetmap.org/)就像维基百科，但用于地图。维基百科的理念是，如果世界上每个人都在维基上写下他们所知道的一切，我们将拥有最好的百科全书，而开放街图（OSM）则基于这样一个理念，即如果每个人都在维基上记录他们所知道的道路，我们将拥有最好的地图系统。值得注意的是，这两个理念都取得了相当不错的成效。'
- en: While the coverage of OSM is rather uneven, ranging from areas that are barely
    covered to places where it rivals or exceeds what can be found on Google Maps,
    the sheer amount of data and the fact that it is all freely available makes it
    a great resource for all types of projects that are of a geographical nature.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管OSM的覆盖范围相当不均匀，从几乎没有覆盖的地区到与谷歌地图相媲美或超过的地方，但数据量庞大且全部免费提供，使其成为所有地理性质项目的重要资源。
- en: OSM is downloadable for free in a binary format or a huge XML file. The whole
    world is tens of gigabytes, but there are a number of locations on the internet
    where we can find OSM dumps per country or region if we want to start smaller.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: OSM可以免费下载为二进制格式或一个庞大的XML文件。整个世界有数十吉字节，但在互联网上有许多地方可以找到按国家或地区分类的OSM数据转储，如果我们想要从小处开始。
- en: 'The binary and XML formats both have the same structure: a map is made out
    of a series of *nodes* that each have a *latitude* and a *longitude*, followed
    by a series of *ways* that combine previously defined nodes into larger structures.
    Finally, there are *relations* that combine anything that was seen before (nodes,
    ways, or relations) into superstructures.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制和XML格式都具有相同的结构：地图由一系列具有*纬度*和*经度*的*节点*组成，然后是一系列将先前定义的节点组合成较大结构的*路径*。最后，有*关系*将之前看到的任何内容（节点、路径或关系）组合成超级结构。
- en: Nodes are used to represents points on the maps, including individual features,
    as well as to define the shapes of ways. Ways are used for simple shapes, like
    buildings and road segments. Finally, relations are used for anything that contains
    more than one shape or very big things like coastlines or borders.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 节点用于表示地图上的点，包括单个特征，以及定义路径的形状。路径用于简单的形状，如建筑物和道路段。最后，关系用于包含多个形状或非常大的事物，如海岸线或边界。
- en: Later in the book, we’ll look at a model that takes in satellite images and
    rendered maps and tries to learn to recognize roads automatically. The actual
    data used for those recipes is not specifically from OSM, but it is the sort of
    thing that OSM is used for in deep learning. The [“Images to OSM” project](https://github.com/jremillard/images-to-osm),
    for example, shows how to train a network to learn to extract shapes of sports
    fields from satellite images to improve OSM itself.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 书中的后续部分，我们将研究一个模型，该模型接收卫星图像和渲染地图，并尝试学习自动识别道路。这些配方使用的实际数据并非专门来自OSM，但OSM在深度学习中用于这类事情。例如，["Images
    to OSM"项目](https://github.com/jremillard/images-to-osm)展示了如何训练网络以从卫星图像中提取体育场形状，以改进OSM本身。
- en: Twitter
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Twitter
- en: As a social network Twitter might have trouble competing with the much bigger
    Facebook, but as a source for text to train deep learning models, it is much better.
    Twitter’s API is nicely rounded and allows for all kinds of apps. To the budding
    machine learning hacker though, the streaming API is possibly the most interesting.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个社交网络，Twitter可能无法与更大的Facebook竞争，但作为训练深度学习模型的文本来源，它要好得多。Twitter的API非常全面，可以支持各种应用。对于新手机器学习黑客来说，流式API可能是最有趣的。
- en: The so-called Firehose API offered by Twitter streams all tweets directly to
    a client. As one can imagine, this is a rather large amount of data. On top of
    that, Twitter charges serious money for this. It is less known that the free Twitter
    API offers a sampled version of the Firehose API. This API returns only 1% of
    all tweets, but that is plenty for many text processing applications.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter提供的所谓的Firehose API将所有推文直接流向客户端。可以想象，这是一大量数据。此外，Twitter对此收费。不太为人所知的是，免费的Twitter
    API提供Firehose API的抽样版本。该API仅返回所有推文的1%，但对于许多文本处理应用来说已经足够了。
- en: Tweets are limited in size and come with a set of interesting metainformation
    like the author, a timestamp, sometimes a location, and of course tags, images,
    and URLs. In [Chapter 7](ch07.html#suggest_emojis) we look at using this API to
    build a classifier to predict emojis based on a bit of text. We tap into the streaming
    API and keep only the tweets that contain exactly one emoji. It takes a few hours
    to get a decent training set, but if you have access to a computer with a stable
    internet connection, letting it run for a few days shouldn’t be an issue.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 推文大小有限，并附带一组有趣的元信息，如作者、时间戳、有时位置，当然还有标签、图片和URL。在[第7章](ch07.html#suggest_emojis)中，我们研究使用此API构建分类器来基于文本预测表情符号。我们利用流式API，仅保留包含一个表情符号的推文。获取一个体面的训练集需要几个小时，但如果你有一台带有稳定互联网连接的计算机，让其运行几天不应该是问题。
- en: Twitter is a popular source of data for experiments in sentiment analysis, which
    arguably predicting emojis is a variation of, but models aimed at language detection,
    location disambiguation, and named entity recognition have all been trained successfully
    on Twitter data too.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter是情感分析实验的热门数据来源，可以说预测表情符号是其变体，但针对语言检测、位置消歧和命名实体识别的模型也已成功在Twitter数据上训练。
- en: Project Gutenberg
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 古腾堡计划
- en: Long before Google Books—in fact, long before Google and even the World Wide
    Web, back in 1971, [Project Gutenberg](http://www.gutenberg.org/) launched with
    the aim to digitize all books. It contains the full text of over 50,000 works,
    not just novels, poetry, short stories, and drama, but also cookbooks, reference
    works, and issues of periodicals. Most of the works are in the public domain and
    they can all be freely downloaded from the website.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 早在Google Books之前，事实上，早在Google甚至互联网问世之前，1971年，[古腾堡计划](http://www.gutenberg.org/)启动，旨在数字化所有图书。它包含超过5万部作品的全文，不仅包括小说、诗歌、短篇小说和戏剧，还包括烹饪书、参考书和期刊。大部分作品属于公共领域，可以从网站上免费下载。
- en: 'This is a massive amount of text in a convenient format, and if you don’t mind
    that most of the texts are a little older (since they are no longer in copyright)
    it’s a very good source of data for experiments in text processing. In [Chapter 5](ch05.html#text_generation)
    we use Project Gutenberg to get a copy of Shakespeare’s collected works as a basis
    to generate more Shakespeare-like texts. All it takes is this one-liner if you
    have the Python library available:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个大量文本的便捷格式，如果你不介意大部分文本有点陈旧（因为它们不再受版权保护），那么这是一个非常好的用于文本处理实验的数据来源。在[第5章](ch05.html#text_generation)中，我们使用古腾堡计划获取莎士比亚的作品集作为生成更多类似莎士比亚文本的基础。如果你有Python库可用，只需这一行代码：
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The material available via Project Gutenberg is mostly in English, although
    a small amount of works are available in other languages. The project started
    out as pure ASCII but has since evolved to support a number of character encodings,
    so if you download a non-English text, you need to make sure that you have the
    right encoding—not everything in the world is UTF-8 yet. In [Chapter 8](ch08.html#seq2seq_mapping)
    we extract all dialogue from a set of books retrieved from Project Gutenberg and
    then train a chatbot to mimic those conversations.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通过古腾堡计划提供的材料大多是英文，尽管少量作品提供其他语言版本。该项目最初是纯ASCII，但后来发展到支持多种字符编码，因此如果下载非英文文本，需要确保选择正确的编码方式——世界上并非所有内容都是UTF-8。在[第8章](ch08.html#seq2seq_mapping)中，我们从古腾堡计划检索的一组书籍中提取所有对话，然后训练一个聊天机器人来模仿这些对话。
- en: Flickr
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Flickr
- en: '[Flickr](https://www.flickr.com/) is a photo sharing site that has been in
    operation since 2004\. It originally started as a side project for a massively
    multiplayer online game called *Game Neverending*. When the game failed to become
    a business on its own, the company’s founders realized that the photo sharing
    part of the company was taking off and so they executed what is called a *pivot*,
    completely changing the main focus of the company. Flickr was sold to Yahoo a
    year later.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[Flickr](https://www.flickr.com/)是一个自2004年以来运营的照片分享网站。最初，它是一个名为*Game Neverending*的大型多人在线游戏的副产品。当游戏未能独立成为业务时，公司的创始人意识到公司的照片分享部分正在起飞，因此他们执行了所谓的*转变*，完全改变了公司的主要重点。一年后，Flickr被雅虎收购。'
- en: Among the many, many photo sharing sites out there, Flickr stands out as a useful
    source of images for deep learning experiments for a few reasons.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在众多照片分享网站中，Flickr因为几个原因而成为深度学习实验的有用图像来源。
- en: One is that Flickr has been at this for a long time and has collected a set
    of billions of images. This might pale in comparison to the number of images that
    people upload to Facebook in a single month, but since users upload photos to
    Flickr that they are proud of for public consumption, Flickr images are on average
    of higher quality and of more general interest.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一点是，Flickr已经做了很长时间，并收集了数十亿张图像。这可能与人们在一个月内上传到Facebook的图像数量相比相形见绌，但由于用户将他们为公众消费而感到自豪的照片上传到Flickr，Flickr的图像平均质量更高，更具一般兴趣。
- en: A second reason is licensing. Users on Flickr pick a license for their photos,
    and many pick some form of [Creative Commons licensing](https://creativecommons.org/)
    that allows for reuse of some kind without asking permission. While you typically
    don’t need this if you run a bunch of photos through your latest nifty algorithm
    and are only interested in the end results, it is quite essential if your project
    ultimately needs to republish the original or modified images. Flickr makes this
    possible.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是许可证。Flickr上的用户为他们的照片选择许可证，许多人选择某种形式的[知识共享许可证](https://creativecommons.org/)，允许在不征得许可的情况下重新使用。虽然如果你只是通过最新的巧妙算法运行一堆照片，并且只对最终结果感兴趣，通常不需要这个，但如果你的项目最终需要重新发布原始或修改后的图像，这是非常重要的。Flickr使这成为可能。
- en: The last and possibly most important advantage that Flickr has over most of
    its competitors is the API. Just like Twitter’s, it is a well-thought-out, REST-style
    API that makes it easy to do anything you can do with the site in an automatic
    fashion. And just like with Twitter there are good Python bindings for the API,
    which makes it even easier to start experimenting. All you need is the right library
    and a Flickr API key.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，也可能是最重要的优势是Flickr相对于大多数竞争对手具有API。就像Twitter的API一样，它是一个经过深思熟虑的REST风格API，使得可以轻松地以自动方式执行与网站相同的任何操作。就像Twitter一样，API有很好的Python绑定，这使得开始实验变得更加容易。你只需要正确的库和一个Flickr
    API密钥。
- en: The main features of the API relevant for this book are searching for images
    and fetching of images. The search is quite versatile and mimics most of the search
    options of the main website, although some advanced filters are unfortunately
    missing. Fetching images can be done for a large variety of sizes. It is often
    useful to get started more quickly with smaller versions of the images first and
    scale up later.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本书相关的API的主要特点是搜索图像和获取图像。搜索功能非常灵活，模仿了主要网站的大多数搜索选项，尽管一些高级过滤器遗憾地缺失。获取图像可以为多种尺寸完成。通常最好先用较小版本的图像开始，然后再放大。
- en: In [Chapter 9](ch09.html#transfer_learning) we use the Flickr API to fetch two
    sets of images, one with dogs and one with cats, and train a classifier to learn
    the difference between the two.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.html#transfer_learning)中，我们使用Flickr API获取了两组图像，一组是狗的图像，另一组是猫的图像，并训练了一个分类器来学习两者之间的区别。
- en: The Internet Archive
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 互联网档案馆
- en: The [Internet Archive](https://archive.org/) has a stated mission of providing
    “universal access to all knowledge.” The project is probably most famous for its
    Wayback Machine, a web interface that lets users look at web pages over time.
    It contains over 300 billion captures dating all the way back to 2001 in what
    the project calls a three-dimensional web index.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[互联网档案馆](https://archive.org/)的宣布使命是提供“对所有知识的普遍访问”。该项目可能最著名的是其Wayback Machine，这是一个Web界面，让用户随时间查看网页。它包含超过3000亿个捕获，追溯到2001年，项目称之为三维网络索引。'
- en: But the Internet Archive is far bigger than the Wayback Machine and comprises
    a ragtag assortment of documents, media, and datasets covering everything from
    books out of copyright to NASA images to cover art for CDs to audio and video
    material. These are all really worth browsing through and often inspire new projects
    on the spot.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 但互联网档案馆远远不止Wayback Machine，它包括一系列文档、媒体和数据集，涵盖从过期的书籍到NASA图像再到CD封面艺术品到音频和视频材料的一切。这些都值得浏览，通常会激发新项目的灵感。
- en: One interesting example is a set of all Reddit comments up to 2015 with over
    50 million entries. This started out as a project of a Reddit user who just patiently
    used the Reddit API to download all of them and then announced that on Reddit.
    When the question came up of where to host it, the Internet Archive turned out
    to be a good option (though the same data can be found on Google’s BigQuery for
    even more immediate analysis).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的例子是截至2015年的所有Reddit评论集，共有超过5000万条记录。这起初是一个Reddit用户的项目，他耐心地使用Reddit API下载了所有评论，然后在Reddit上宣布了这一消息。当提出要将其托管在何处时，互联网档案馆成为一个不错的选择（尽管相同的数据也可以在Google的BigQuery上找到，以进行更即时的分析）。
- en: An example we use in this book is the set of [Stack Exchange questions](https://archive.org/details/stackexchange).
    Stack Exchange has always been licensed under a Creative Commons license, so nothing
    would stop us from downloading these sets ourselves, but getting them from the
    Internet Archive is so much easier. In this book we use this dataset to train
    a model to match questions with answers (see [Chapter 6](ch06.html#question_matching)).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中使用的一个示例是[Stack Exchange问题集](https://archive.org/details/stackexchange)。Stack
    Exchange一直以来都是根据知识共享许可证授权的，因此没有什么能阻止我们自己下载这些集合，但是从互联网档案馆获取它们要容易得多。在本书中，我们使用这个数据集来训练一个模型，以匹配问题和答案（参见[第6章](ch06.html#question_matching)）。
- en: Crawling
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 爬取
- en: If you need anything specific for your project, chances are that the data you
    are after is not accessible through a public API. And even if there is a public
    API, it might be rate limited to the point of being useless. Historic results
    for your favorite sports are hard to come by. Your local newspaper might have
    an online archive, but probably no API or data dump. Instagram has a nice API,
    but the recent changes to the terms of service make it hard to use it to acquire
    a large set of training data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要项目中的特定内容，那么您要获取的数据很可能无法通过公共API访问。即使有公共API，它可能会被限制到无法使用的程度。您喜欢的体育比赛的历史结果很难获得。您当地的报纸可能有在线存档，但可能没有API或数据转储。Instagram有一个不错的API，但最近对服务条款的更改使得难以使用它来获取大量的训练数据。
- en: In these cases, you can always resort to scraping, or, if you want to sound
    more respectable, crawling. In the simplest scenario you just want to get a copy
    of a website on your local system and you have no prior knowledge about the structure
    of that website or the format of the URLs. In that case you just start with the
    root of the website, fetch the web content of it, extract all links from that
    web content, and do the same for each of those links until you find no more new
    links. This is how Google does it too, be it at a larger scale. [Scrapy](https://scrapy.org)
    is a useful framework for this sort of thing.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，您总是可以使用爬取，或者，如果您想听起来更加正式，可以使用爬取。在最简单的情况下，您只是想在本地系统上获取网站的副本，并且对该网站的结构或URL格式没有先验知识。在这种情况下，您只需从网站的根开始，获取其网页内容，从该网页内容中提取所有链接，并对每个链接执行相同的操作，直到找不到新链接为止。这也是谷歌的做法，只是规模更大。[Scrapy](https://scrapy.org)是这种类型的框架的有用工具。
- en: Sometimes there is an obvious hierarchy, like a travel website with pages for
    countries, regions in those countries, cities in those regions, and finally attractions
    in those cities. In that case it might be more useful to write a more targeted
    scraper that successively works its way through the various layers of hierarchy
    until it has all the attractions.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 有时存在明显的层次结构，比如一个旅行网站有国家页面，这些国家的地区，这些地区的城市，最后是这些城市的景点。在这种情况下，编写一个更有针对性的爬虫可能更有用，依次通过各层次的层次结构工作，直到获取所有景点。
- en: Other times there is an internal API to take advantage of. Many content-oriented
    websites will load the overall layout and then use a JSON call back to the web
    server to get the actual data and insert this on the fly into the template. This
    makes it easy to support infinite scrolling and search. The JSON returned from
    the server is often easy to make sense of, as are the parameters passed to the
    server. The Chrome extension [Request Maker](http://bit.ly/request-maker) shows
    all requests that a page makes and is a good way to see if anything useful goes
    over the line.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其他时候，有一个内部API可以利用。许多内容导向的网站将加载整体布局，然后使用JSON回调到Web服务器获取实际数据，并将其动态插入到模板中。这样就可以轻松支持无限滚动和搜索。从服务器返回的JSON通常很容易理解，传递给服务器的参数也很容易理解。Chrome扩展程序[Request
    Maker](http://bit.ly/request-maker)显示页面发出的所有请求，是查看是否有任何有用信息传输的好方法。
- en: Then there are the websites that don’t want to be crawled. Google might have
    built an empire on scraping the world, but many of its services very cleverly
    detect signs of scraping and will block you and possibly anybody making requests
    from your IP address until you do a captcha. You can play with rate limiting and
    user agents, but at some point you might have to resort to scraping using a browser.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后有一些不希望被爬取的网站。谷歌可能已经建立了一个基于爬取世界的帝国，但是它的许多服务非常聪明地检测到爬取的迹象，并会阻止您，可能会阻止从您的IP地址发出请求的任何人，直到您完成验证码。您可以尝试限制速率和用户代理，但是在某个时候，您可能不得不使用浏览器进行爬取。
- en: WebDriver, a framework developed for testing websites by instrumenting a browser,
    can be very helpful in these situations. The fetching of the pages is done with
    your choice of browser, so to the web server everything seems as real as it can
    get. You can then “click” on links using your control script to go to the next
    page and inspect the results. Consider sprinkling the code with delays to make
    it seem like a human is exploring the site and you should be good to go.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: WebDriver是一个为测试网站而开发的框架，通过操纵浏览器可以在这些情况下非常有帮助。页面的获取是通过您选择的浏览器完成的，因此对于Web服务器来说，一切都似乎是真实的。然后，您可以使用控制脚本“点击”链接以转到下一页并检查结果。考虑在代码中添加延迟，使其看起来像是一个人在浏览网站，然后您就可以开始了。
- en: The code in [Chapter 10](ch10.html#image_search) uses crawling techniques to
    fetch images from Wikipedia. There is a URL scheme to go from a Wikipedia ID to
    the corresponding image, but it doesn’t always pan out. In that case we fetch
    the page that contains the image and follow the link graph until we get to the
    actual image.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[第10章](ch10.html#image_search)中的代码使用爬取技术从维基百科获取图像。有一个URL方案可以从维基百科ID转到相应的图像，但并不总是奏效。在这种情况下，我们获取包含图像的页面，并跟随链接图形，直到找到实际图像。'
- en: Other Options
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他选项
- en: 'There are many ways to get data. The [ProgrammableWeb](https://www.programmableweb.com/)
    lists more than 18,000 public APIs (though some of those are in a state of disrepair).
    Here are three that are worth highlighting:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多获取数据的方法。[ProgrammableWeb](https://www.programmableweb.com/)列出了超过18,000个公共API（尽管其中一些处于失修状态）。以下是值得强调的三个：
- en: Common Crawl
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Common Crawl
- en: Crawling one site is doable if the site is not very big. But what if you want
    to crawl all of the major pages of the internet? The [Common Crawl](http://commoncrawl.org/)
    runs a monthly crawl fetching around 2 billion web pages each time in an easy-to-process
    format. AWS has this as a public dataset, so if you happen to run on that platform
    that’s an easy way to run jobs on the web at large.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网站规模不是很大，那么爬取一个网站是可行的。但是如果您想爬取互联网的所有主要页面呢？Common Crawl每月进行一次爬取，每次获取大约20亿个网页，格式易于处理。AWS将其作为公共数据集提供，因此如果您恰好在该平台上运行，这是在互联网上运行作业的简便方法。
- en: Facebook
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook
- en: Over the years the Facebook API has shifted subtly from being a really useful
    resource to build applications on top of Facebook’s data to a resource to build
    applications that make Facebook’s data better. While this is understandable from
    Facebook’s perspective, as a data prospector one often wonders about the data
    it could make public. Still, the Facebook API is a useful resource—especially
    the Places API in situations where OSM is just too unevenly edited.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，Facebook API从一个构建在Facebook数据之上的应用程序的非常有用的资源，逐渐转变为一个构建使Facebook数据更好的应用程序的资源。尽管这从Facebook的角度来看是可以理解的，但作为数据勘探者，人们常常想知道它可能公开的数据。尽管如此，Facebook
    API仍然是一个有用的资源——尤其是在OSM编辑不均匀的情况下，Places API是一个有用的资源。
- en: US government
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 美国政府
- en: The US government on all levels publishes a huge amount of data, and all of
    it is freely accessible. For example, the [census data](https://www.census.gov)
    has detailed information about the US population, while [Data.gov](https://www.data.gov/)
    has a portal with many different datasets all over the spectrum. On top of that,
    individual states and cities have their own resources worth looking at.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 美国政府在各个层面发布了大量数据，所有这些数据都是免费可访问的。例如，人口普查数据提供了关于美国人口的详细信息，而Data.gov则提供了一个包含各种不同数据集的门户网站。此外，各个州和城市都有值得关注的资源。
- en: 1.3 Preprocessing Data
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理数据
- en: Deep neural networks are remarkably good at finding patterns in data that can
    help in learning to predict the labels for the data. This also means that we have
    to be careful with the data we give them; any pattern in the data that is not
    relevant for our problem can make the network learn the wrong thing. By preprocessing
    data the right way we can make sure that we make things as easy as possible for
    our networks.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络在数据中找到有助于学习预测数据标签的模式方面表现出色。这也意味着我们必须小心处理给予它们的数据；数据中的任何与我们问题无关的模式都可能导致网络学习错误的内容。通过正确预处理数据，我们可以确保为我们的网络尽可能简化事情。
- en: Getting a Balanced Training Set
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取平衡的训练集
- en: An apocryphal story relates how the US Army once trained a neural network to
    discriminate between camouflaged tanks and plain forest—a useful skill when automatically
    analyzing satellite data. At first sight they did everything right. On one day
    they flew a plane over a forest with camouflaged tanks in it and took pictures,
    and on another day they did the same when there were no tanks, making sure to
    photograph scenes that were similar but not quite the same. They split the data
    up into training and test sets and let the network train.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一个传闻故事讲述了美国军队曾经训练过一个神经网络，用于区分伪装坦克和普通森林——这是在自动分析卫星数据时非常有用的技能。乍一看，他们做了一切正确。有一天，他们在一片有伪装坦克的森林上空飞行，并拍摄了照片，另一天，当没有坦克时，他们做了同样的事情，确保拍摄的场景相似但不完全相同。他们将数据分成训练集和测试集，并让网络进行训练。
- en: The network trained well and started to get good results. However, when the
    researchers sent it out to be tested in the wild, people thought it was a joke.
    The predictions seemed utterly random. After some digging, it turned out that
    the input data had a problem. All the pictures containing tanks had been taken
    on a sunny day, while the pictures with just forest happened to have been taken
    on a cloudy day. So while the researchers thought their network had learned to
    discriminate between tanks and nontanks, they really had trained a network to
    observe the weather.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 网络训练良好，开始获得良好的结果。然而，当研究人员将其送到野外测试时，人们认为这是一个笑话。预测似乎完全随机。经过一番调查，发现输入数据存在问题。所有包含坦克的图片都是在晴天拍摄的，而只有森林的图片恰好是在多云天气下拍摄的。因此，尽管研究人员认为他们的网络已经学会区分坦克和非坦克，但实际上他们训练的是一个观察天气的网络。
- en: Preprocessing data is all about making sure the network picks up on the signals
    we want it to pick up on and is not distracted by things that don’t matter. The
    first step here is to make sure that we actually have the right input data. Ideally
    the data should resemble as closely as possible the real-world situation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理数据的关键在于确保网络捕捉到我们希望捕捉到的信号，并且不会被无关的事物分散注意力。这里的第一步是确保我们实际上拥有正确的输入数据。理想情况下，数据应尽可能接近真实世界的情况。
- en: Making sure that the signal in the data is the signal we are trying to learn
    seems obvious, but it is easy to get this wrong. Getting data is hard, and every
    source has its own peculiarities.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 确保数据中的信号是我们试图学习的信号似乎是显而易见的，但很容易出错。获取数据很困难，每个来源都有其独特之处。
- en: There are a few things we can do when we find our input data is tainted. The
    best thing is, of course, to rebalance the data. So in the tanks versus forest
    example, we would try to get pictures for both scenarios in all types of weather.
    (When you think about it, even if all the original pictures had been taken in
    sunny weather, the training set would still have been suboptimal—a balanced set
    would contain weather conditions of all types.)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们发现输入数据被污染时，我们可以做一些事情。最好的方法当然是重新平衡数据。因此，在坦克与森林的例子中，我们将尝试在所有类型的天气条件下获取两种情况的图片。（当您考虑到，即使所有原始图片都是在晴天拍摄的，训练集仍然会是次优的——平衡的集合应包含所有类型的天气条件。）
- en: A second option is to just throw out some data to make the set more balanced.
    Maybe there were some pictures of tanks taken on cloudy days after all, but not
    enough—so we could throw out some of the sunny pictures. This obviously cuts down
    the size of the training set, however, and might not be an option. (Data augmentation,
    discussed in [“Preprocessing of Images”](#preprocessing-of-images), could help.)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个选择是丢弃一些数据，使数据集更加平衡。也许确实有一些坦克的图片是在多云天气下拍摄的，但数量不够，所以我们可以丢弃一些晴天的图片。然而，这显然会减少训练集的大小，并且可能不是一个选项。（数据增强，在[“图像预处理”](#preprocessing-of-images)中讨论，可能会有所帮助。）
- en: A third option is to try to fix the input data, say by using a photo filter
    that makes the weather conditions appear more similar. This is tricky though,
    and can easily lead to other or even more artifacts that the network might detect.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个选择是尝试修复输入数据，比如使用一个照片滤镜使天气条件看起来更相似。然而，这很棘手，很容易导致网络可能检测到其他或更多的伪影。
- en: Creating Data Batches
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据批次
- en: 'Neural networks consume data in batches (sets of input/output pairs). It is
    important to make sure that these batches are properly randomized. Imagine we
    have a set of pictures, the first half all depicting cats and the second half
    dogs. Without shuffling, it would be impossible for the network to learn anything
    from this dataset: almost all batches would either contain only cats or only dogs.
    If we use Keras and if we have our data entirely in memory, this is easily accomplished
    using the `fit` method since it will do the shuffling for us:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络以批量（输入/输出对的集合）消耗数据。确保这些批次被适当随机化是很重要的。想象一下，我们有一组图片，前一半都是猫，后一半是狗。如果不进行洗牌，网络将无法从这个数据集中学到任何东西：几乎所有的批次要么只包含猫，要么只包含狗。如果我们使用Keras，并且数据完全存储在内存中，这很容易通过使用`fit`方法来实现，因为它会为我们进行洗牌：
- en: '[PRE3]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will randomly create batches with a size of 128 from the `training_data`
    and `training_labels` sets. Keras takes care of the proper randomizing. As long
    as we have our data in memory, this is usually the way to go.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从`training_data`和`training_labels`集合中随机创建大小为128的批次。Keras会负责适当的随机化。只要我们的数据在内存中，这通常是一个不错的选择。
- en: Note
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In some circumstances we might want to call `fit` with one batch at a time,
    in which case we do need to make sure things are properly shuffled. `numpy.random.shuffle`
    will do just fine, though we have to take care to shuffle the data and the labels
    in unison.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可能希望一次使用一个批次调用`fit`，在这种情况下，我们确实需要确保事物被适当洗牌。`numpy.random.shuffle`可以很好地完成这项任务，尽管我们必须小心地同时洗牌数据和标签。
- en: 'We don’t always have all the data in memory, though. Sometimes the data would
    be too big or needs to be processed on the fly and isn’t available in the ideal
    format. In those situations we use `fit_generator`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不总是将所有数据存储在内存中。有时数据量太大，或者需要实时处理，无法以理想格式提供。在这种情况下，我们使用`fit_generator`：
- en: '[PRE4]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, `data_generator` is a generator that yields batches of data. The generator
    has to make sure that the data is properly randomized. If the data is read from
    a file, shuffling is not really an option. If the file comes from an SSD and the
    records are all the same size, we can shuffle by seeking randomly inside of the
    file. If this is not the case and the file has some sort of sorting, we can increase
    randomness by having multiple file handles in the same file, all at different
    locations.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`data_generator`是一个生成器，产生数据批次。生成器必须确保数据被适当随机化。如果数据是从文件中读取的，那么洗牌实际上并不是一个选项。如果文件来自SSD，并且记录都是相同大小，我们可以通过在文件内部随机寻找来进行洗牌。如果不是这种情况，文件有某种排序，我们可以通过在同一个文件中具有多个文件句柄，每个句柄位于不同位置，来增加随机性。
- en: When setting up a generator that produces batches on the fly, we also need to
    pay attention to keep things properly randomized. For example, in [Chapter 4](ch04.html#movie_recommender)
    we build a movie recommender system by training on Wikipedia articles, using as
    the unit of training links from the movie page to some other page. The easiest
    way to generate these (FromPage, ToPage) pairs would be to randomly pick a FromPage
    and then randomly pick a ToPage from all the links found on FromPage.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当设置一个生成器来实时生成批次时，我们还需要注意保持事物适当随机化。例如，在[第四章](ch04.html#movie_recommender)中，我们通过在维基百科文章上进行训练来构建一个电影推荐系统，使用从电影页面到其他页面的链接作为训练单元。生成这些（FromPage，ToPage）对的最简单方法是随机选择一个FromPage，然后从FromPage上找到的所有链接中随机选择一个ToPage。
- en: This works, of course, but it will select links from pages with fewer links
    on them more often than it should. A FromPage with one link on it has the same
    chance of being picked in the first step as a page with a hundred links. In the
    second step, though, that one link is certain to be picked, while any of the links
    from the page with a hundred links has only a small chance of selection.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这种方法有效，但它会更频繁地选择链接较少的页面，而不是应该的频率。一个页面上只有一个链接的FromPage在第一步被选中的机会与一个页面上有一百个链接的页面相同。然而，在第二步中，那一个链接肯定会被选中，而来自有一百个链接的页面的任何链接只有很小的选择机会。
- en: Training, Testing, and Validation Data
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练、测试和验证数据
- en: After we’ve set up our clean, normalized data and before the actual training
    phase, we need to split the data up in a training set, a test set, and possibly
    a validation set. As with many things, the reason we do this has to do with overfitting.
    Networks will almost always memorize a little bit of the training data rather
    than learn generalizations. By separating a small amount of the data into a test
    set that we don’t use for training, we can measure to what extent this is happening;
    after each epoch we measure accuracy over both the training and the test set,
    and as long as the two numbers don’t diverge too much, we’re fine.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们设置好干净、规范化的数据并进入实际训练阶段之前，我们需要将数据分成训练集、测试集和可能的验证集。和许多事情一样，我们这样做的原因与过拟合有关。网络几乎总是会记住一点训练数据，而不是学习泛化。通过将一小部分数据分离出来作为我们不用于训练的测试集，我们可以衡量这种情况发生的程度；每个时代结束后，我们都会在训练集和测试集上测量准确性，只要这两个数字不相差太多，我们就没问题。
- en: 'If we have our data in memory we can use `train_test_split` from `sklearn`
    to neatly split our data into training and test sets:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据在内存中，我们可以使用`sklearn`中的`train_test_split`来将数据整齐地分成训练集和测试集：
- en: '[PRE5]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will create a test set containing 33% of the data. The `random_state` variable
    is used for the random seed, which guarantees that if we run the same program
    twice, we get the same results.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个包含33%数据的测试集。`random_state`变量用于随机种子，这可以保证如果我们两次运行相同的程序，我们会得到相同的结果。
- en: 'When feeding our network using a generator, we need to do the splitting ourselves.
    One general though not very efficient approach would be to use something like:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用生成器向我们的网络提供输入时，我们需要自己进行拆分。一个一般但不是非常高效的方法是使用类似以下的东西：
- en: '[PRE6]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When `train` is `False` this yields every fourth element coming from the generator
    `gen`. When it is `True` it yields the rest.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当`train`为`False`时，这将产生来自生成器`gen`的每四个元素。当它为`True`时，它将产生其余的元素。
- en: Sometimes a third set is split off from the training data, called the *validation
    set*. There is some confusion in the naming here; when there are only two sets
    the test set is sometimes also called the validation set (or holdout set). In
    a scenario where we have training, validation, and test sets, the validation set
    is used to measure performance while tuning the model. The test set is meant to
    be used only when all tuning is done and no more changes are going to be made
    to the code.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 有时会从训练数据中分离出第三组，称为*验证集*。这里的命名有些混淆；当只有两组时，测试集有时也被称为验证集（或留出集）。在有训练、验证和测试集的情况下，验证集用于在调整模型时测量性能。测试集只用于在所有调整完成且不再对代码进行更改时使用。
- en: The reason to keep this third set is to stop us from manually overfitting. A
    complex neural network can have a very large number of tuning options or hyperparameters.
    Finding the right values for these hyperparameters is an optimization problem
    that can also suffer from overfitting. We keep adjusting those parameters until
    the performance on the validation set no longer increases. By having a test set
    that was not used during tuning, we can make sure that we didn’t inadvertently
    optimize our hyper parameters for the validation set.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 保留这第三组的原因是为了防止我们手动过拟合。复杂的神经网络可能有非常多的调整选项或超参数。找到这些超参数的正确值是一个优化问题，也可能会受到过拟合的影响。我们不断调整这些参数，直到验证集上的性能不再增加。通过拥有一个在调整过程中未使用的测试集，我们可以确保我们没有无意中为验证集优化我们的超参数。
- en: Preprocessing of Text
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本预处理
- en: A lot of neural networking problems involve text processing. Preprocessing the
    input texts in these situations involves mapping the input text to a vector or
    matrix that we can feed into a network.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 许多神经网络问题涉及文本处理。在这些情况下，预处理输入文本涉及将输入文本映射到可以馈送到网络中的向量或矩阵。
- en: 'Typically, the first step is to break up the text into units. There are two
    common ways to do this: on a character or a word basis.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，第一步是将文本分成单元。有两种常见的方法可以做到这一点：按字符或按单词。
- en: Breaking up a text into a stream of single characters is straightforward and
    gives us a predictable number of different tokens. If all our text is in one phoneme-based
    script, the number of different tokens is quite restricted.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本分成一系列单个字符是直接的，并且给我们一个可预测的不同标记数量。如果我们所有的文本都是基于音素的脚本，那么不同标记的数量是相当受限的。
- en: Breaking up a text into words is a more complicated tokenizing strategy, especially
    in scripts that don’t indicate the beginning and ending of words. Moreover, there
    is no obvious upper limit to the number of different tokens that we’ll end up
    with. A number of text processing toolkits have a “tokenize” function that usually
    also allows for the removal of accents and optionally converts all tokens to lowercase.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本分成单词是一种更复杂的标记化策略，特别是在不指示单词开始和结束的脚本中。此外，我们最终得到的不同标记的数量没有明显的上限。许多文本处理工具包都有一个“标记化”功能，通常还允许去除重音并可选择将所有标记转换为小写。
- en: A process called *stemming*, where we convert each word to its root form (by
    dropping any grammar-related modifications), can help, especially for languages
    that are more grammar-heavy than English. In [Chapter 8](ch08.html#seq2seq_mapping)
    we’ll encounter a subword tokenizing strategy that breaks up complicated words
    into subtokens thereby guaranteeing a specific upper limit on the number of different
    tokens.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一个称为*词干提取*的过程，其中我们将每个单词转换为其根形式（通过删除任何与语法相关的修改），可以帮助，特别是对于比英语更注重语法的语言。在[第8章](ch08.html#seq2seq_mapping)中，我们将遇到一种子词标记化策略，将复杂的单词分解为子标记，从而保证不同标记的数量有一个特定的上限。
- en: 'Once we have our text split up into tokens, we need to vectorize it. The simplest
    way of doing this is called *one-hot encoding*. Here, we assign to each unique
    token an integer *i* from 0 to the number of tokens and then represent each token
    as a vector containing only 0s, except for the *i*th entry, which contains a 1\.
    In Python code this would be:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将文本分成标记，我们需要对其进行向量化。最简单的方法是称为*一热编码*。在这里，我们为每个唯一的标记分配一个整数*i*，从0到标记数量，然后将每个标记表示为一个只包含0的向量，除了第*i*个条目，其中包含1。在Python代码中，这将是：
- en: '[PRE7]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This should leave us with a large two-dimensional array ready for consumption.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该让我们得到一个准备好供消费的大型二维数组。
- en: One-hot encoding works when we process text at a character level. It also works
    for word-level processing, though for texts with large vocabularies it can get
    unwieldy. There are two popular encoding strategies that work around this.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一热编码适用于在字符级别处理文本时。它也适用于单词级别处理，尽管对于词汇量大的文本，可能会变得难以处理。有两种流行的编码策略可以解决这个问题。
- en: The first one is to treat a document as a “bag of words.” Here, we don’t care
    about the order of the words, just whether a certain word is present. We can then
    represent a document as a vector with an entry for each unique token. In the simplest
    scheme we just put a 1 if the word is present in that document and a 0 if not.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是将文档视为“词袋”。在这里，我们不关心单词的顺序，只关心某个单词是否存在。然后，我们可以将文档表示为一个向量，其中每个唯一标记都有一个条目。在最简单的方案中，如果单词在文档中存在，则我们将放置一个1，如果不存在则放置一个0。
- en: Since the top 100 most frequently occurring words in English make up about half
    of all texts, they are not very useful for text classifying tasks; almost all
    documents will contain them, so having those in our vectors doesn’t really help
    much. A common strategy is to just drop them from our bag of words so the network
    can focus on the words that do make a difference.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 由于英语中出现频率最高的100个单词占所有文本的一半左右，它们对于文本分类任务并不是很有用；几乎所有文档都会包含它们，所以在我们的向量中包含这些词并没有太大帮助。一个常见的策略是从我们的词袋中删除它们，这样网络就可以专注于那些真正有影响的词语。
- en: Term frequency–inverse document frequency, or tf–idf, is a more sophisticated
    version of this. Instead of storing a 1 if a token is present in a document, we
    store the relative frequency of the term in the document compared to how often
    the term occurs throughout the entire corpus of documents. The intuition here
    is that it is more meaningful for a less common token to appear in a document
    than a token that appears all the time. Scikit-learn comes with methods to calculate
    this automatically.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 术语频率-逆文档频率，或tf-idf，是这一概念的一个更复杂的版本。我们不是在文档中存储一个标记是否存在，而是存储该术语在文档中的相对频率，与该术语在整个文档语料库中出现的频率相比。这里的直觉是，一个不太常见的标记在文档中出现比一个一直出现的标记更有意义。Scikit-learn提供了自动计算这一概念的方法。
- en: A second way to handle word-level encoding is by way of embeddings. [Chapter 3](ch03.html#word_embeddings)
    is all about embeddings and offers a good way to understand how they work. With
    embeddings we associate a vector of a certain size—typically with a length of
    50 to 300—with each token. When we feed in a document represented as a sequence
    of token IDs, an embedding layer will automatically look up the corresponding
    embedding vectors and output a two-dimensional array.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 处理单词级别的编码的第二种方法是使用嵌入。[第三章](ch03.html#word_embeddings)讲解了嵌入并提供了一个很好的理解方式。通过嵌入，我们将一个特定大小的向量（通常长度为50到300）与每个标记关联起来。当我们输入一个表示为标记ID序列的文档时，嵌入层将自动查找相应的嵌入向量并输出一个二维数组。
- en: The embedding layer will learn the right weights for each term, just like any
    layer in a neural network. This often takes a lot of learning, both in terms of
    processing and the required amount of data. A nice aspect of embeddings, though,
    is that there are pre-trained sets available for download and we can seed our
    embedding layer with these. [Chapter 7](ch07.html#suggest_emojis) has a good example
    of this approach.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层将学习每个术语的正确权重，就像神经网络中的任何一层一样。这通常需要大量的学习，无论是在处理方面还是所需的数据量方面。不过，嵌入的一个好处是可以下载预训练集，并且我们可以使用这些预训练集来初始化我们的嵌入层。[第七章](ch07.html#suggest_emojis)有一个这种方法的很好例子。
- en: Preprocessing of Images
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像预处理
- en: Deep neural networks have turned out to be very effective when it comes to working
    with images, for anything from detecting cats in videos to applying the style
    of different artists to selfies. As with text, though, it is essential to properly
    preprocess the input images.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络在处理图像方面非常有效，可以用于从视频中检测猫到将不同艺术家的风格应用于自拍等各种任务。然而，与文本一样，正确预处理输入图像是至关重要的。
- en: The first step is normalization. Many networks only operate on a specific size,
    so the first step is to resize/crop the images to that target size. Both center
    cropping and direct resizing are often used, though sometimes a combination works
    better in order to preserve more of the image while keeping resize distortion
    somewhat in check.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是标准化。许多网络只能在特定大小上运行，因此第一步是将图像调整/裁剪到目标大小。通常使用中心裁剪和直接调整大小，但有时结合使用效果更好，以保留更多图像的同时保持调整大小的失真在一定范围内。
- en: To normalize the colors, for each pixel we usually subtract the mean value and
    divide by the standard deviation. This makes sure that all values on average center
    around 0 and that the nearly 70% of all values are within the comfortable [–1,
    1] range. A new development here is the use of *batch normalization*; rather than
    normalizing all data beforehand, this subtracts the mean of the batch and divides
    by the standard deviation. This leads to better results and can just be made part
    of the network.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使颜色标准化，对于每个像素，我们通常减去平均值并除以标准差。这确保所有值的平均值围绕0中心，并且近70%的值在舒适的[-1, 1]范围内。这里的一个新发展是使用*批量标准化*；而不是事先对所有数据进行标准化，这会减去批次的平均值并除以标准差。这会带来更好的结果，并且可以作为网络的一部分。
- en: '*Data augmentation* is a strategy to increase the amount of training data by
    adding variations of our training images. If we add to our training data versions
    of our images flipped horizontally, in a way we double our training data—a mirrored
    cat is still a cat. Looking at this in another way, what we are doing is telling
    our network that flips can be ignored. If all our cat pictures have the cat looking
    in one direction, our network might learn that that is part of catness; adding
    flips undoes that.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据增强*是一种通过添加训练图像的变化来增加训练数据量的策略。如果我们在训练数据中添加水平翻转的图像版本，那么我们就可以将训练数据翻倍——镜像猫仍然是一只猫。从另一个角度来看，我们所做的是告诉网络可以忽略翻转。如果我们所有的猫图片都是朝一个方向看的，我们的网络可能会学习到这是猫的一部分；添加翻转会撤销这一点。'
- en: 'Keras has a handy `ImageDataGenerator` class that you can configure to produce
    all kinds of image variations, including rotations, translations, color adjustments,
    and magnification. You can then use that as a data generator for the `fit_generator`
    method on your model:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Keras有一个方便的`ImageDataGenerator`类，您可以配置它来生成各种图像变化，包括旋转、平移、颜色调整和放大。然后，您可以将其用作模型的`fit_generator`方法的数据生成器：
- en: '[PRE8]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Conclusion
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Preprocessing of data is an important step before training a deep learning model.
    A common thread in all of this is that we want it to be as easy as possible for
    networks to learn the right thing and not be confused by irrelevant features of
    the input. Getting a balanced training set, creating randomized training batches,
    and the various ways to normalize the data are all a big part of this.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度学习模型之前，数据的预处理是一个重要的步骤。所有这些中的一个共同点是，我们希望网络尽可能容易地学习正确的内容，而不会被输入的无关特征所困扰。获取平衡的训练集，创建随机化的训练批次，以及规范化数据的各种方式都是其中的重要部分。
