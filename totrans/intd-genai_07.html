<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">8 <a id="idTextAnchor000"/><a id="idTextAnchor001"/>What’s next for AI and LLMs</h1>
<p class="co-summary-head"><a id="idTextAnchor002"/>This chapter covers<a id="idIndexMarker000"/><a id="idIndexMarker001"/></p>
<ul class="calibre6">
<li class="co-summary-bullet">Exploring the ultimate vision of LLM developers</li>
<li class="co-summary-bullet">Formalizing best practices for responsibly using generative AI models</li>
<li class="co-summary-bullet">Understanding the regulatory landscape for AI systems</li>
<li class="co-summary-bullet">Discussing a potential framework for a global AI governance body</li>
</ul>
<p class="body"><a id="marker-201"/>In an infamous article for <i class="fm-italics">Newsweek</i> in 1995, astronomer Clifford Stoll wrote the following:</p>
<p class="fm-quote">Today, I’m uneasy about this most trendy and oversold community. Visionaries see a future of telecommuting workers, interactive libraries and multimedia classrooms. They speak of electronic town meetings and virtual communities. Commerce and business will shift from offices and malls to networks and modems. And the freedom of digital networks will make government more democratic. Baloney. Do our computer pundits lack all common sense? The truth is no online database will replace your daily newspaper, no CD-ROM can take the place of a competent teacher and no computer network will change the way government works. <a class="url1" href="https://www.newsweek.com/clifford-stoll-why-web-wont-be-nirvana-185306">[1]</a></p>
<p class="body">For better and for worse, the internet has done much more than Stoll expected. Digital networks have made government more democratic in some ways, but concentrated the power of authoritarians in others; have connected people across the globe but have also been tied to increasing social isolation; and have reshaped the global economy.</p>
<p class="body">Similarly, when Bill Gates called AI “every bit as important” as PCs and the internet, it was an endorsement of the technology. Yet the effects of AI, like its transformative predecessors, are unknowable at this point. We can’t be completely sure of how we’ll use generative AI, or how generative AI will change us. At the same time, we know enough to identify both the significant promise of the technology and the severe risks that it poses. In this chapter, we identify forthcoming areas of large language model (LLM) development and suggest paths forward that could lead to a better and more equitable future<a id="idTextAnchor003"/>.</p>
<h2 class="fm-head" id="heading_id_3">Where are LLM developments headed?</h2>
<p class="body"><a id="marker-202"/>One of the greatest challenges of writing this book has been that seemingly every day, there is a story about a new way that LLMs are being used or a breakthrough in LLM research. As the Nobel laureate Niels Bohr allegedly liked to say—though the origin of the saying is unknown—“Prediction is very difficult, especially about the future” <a class="url" href="https://www.economist.com/letters-to-the-editor-the-inbox/2007/07/15/the-perils-of-prediction-june-2nd">[2]</a>. Nonetheless, throughout this book, we’ve outlined several avenues of current research, and in this first section, we discuss three categories of work that we expect to have a major effect on generative AI in the coming months and years<a id="idTextAnchor004"/>.<a id="idIndexMarker002"/></p>
<h3 class="fm-head1" id="heading_id_4">Language: The universal interface</h3>
<p class="body">In chapter 6, we discussed the increasing personal use of chatbots and other LLMs. Already, LLMs are being integrated into existing applications at a breakneck pace. The coding assistant Copilot, explored in detail in chapter 6, works in Microsoft’s integrated development environment, Visual Studio. Google is piloting a writing assistant in Docs, Gmail, Maps, and Sheets <a class="url" href="https://techcrunch.com/2023/05/10/google-launches-a-smarter-bard/">[3]</a>. In 2023, Expedia began offering a travel planning chatbot powered by GPT-4, and other companies are using LLM-powered chatbots for customer service and other functions with both potential and existing clients. Maybe some of these applications won’t pan out—whether because the models aren’t reliable enough, the interface is clunky, or because people simply prefer to do some tasks themselves—but many of these integrations will become standard practice.<a id="idIndexMarker003"/><a id="marker-203"/></p>
<p class="body">The most visible integration of LLMs today is in search, with Microsoft’s Bing and Google’s Bard demonstrating early versions of an LLM-powered search experience. When Bard was announced, Alphabet CEO Sundar Pichai wrote the following in a blog post:</p>
<p class="fm-quote">One of the most exciting opportunities is how AI can deepen our understanding of information and turn it into useful knowledge more efficiently—making it easier for people to get to the heart of what they’re looking for and get things done. <a class="url1" href="https://blog.google/technology/ai/bard-google-ai-search-updates/">[4]</a></p>
<p class="body">In other words, where people might currently turn to Google or another search engine for advice or information, they might now or in the future use AI to get a shorter and faster response, without having to wade through all the search results. While search might seem like just another application of LLMs, it’s representative of a potential shift because it’s the starting point for so much web browsing. If LLMs are successful in replacing even a portion of search traffic, it would mean a huge uptick in familiarity and the use of generative AI among the general public. It would also raise questions about the business model of those LLMs because most search engines today make money by offering paid placement in search results. While LLMs haven’t yet found a huge market for monetization (those that monetize presently do so by offering a premium tier of service), that will surely be a focus of LLM providers in the near future.</p>
<p class="body">All the integrations mentioned previously are examples of a change in interface, from queries or buttons to natural language. In the most ambitious case, LLMs would become the default surface for interaction between humans and computers. People already know and use language; if computers can understand the same language, we don’t need so many menus or controls because the interface is language, and people could ask questions and give feedback to the model just like they would to another person. The next generation of models (beginning with GPT-4) will also be increasingly multimodal, able to process images and soon other types of media.<a id="marker-204"/></p>
<p class="fm-callout"><span class="fm-callout-head">A multimodal model</span> is characterized by multiple forms of media, such as text, images, video, and aud<a id="idTextAnchor005"/>io.<a id="idIndexMarker004"/></p>
<h3 class="fm-head1" id="heading_id_5">LLM agents unlock new possibilities</h3>
<p class="body">As discussed in chapter 6, we also expect that LLMs will be agentic, interacting with their environment to make purchases and other types of decisions based on their conversations with users. Figure 8.1 demonstrates the basic functionality of an agentic LLM, which attempts to complete a task using an external tool or set of tools. In this example, the user gives the prompt, “Find me a shirt under $15,” and the model translates that request into a search query for a shopping application programming interface (API). The API executes the request, and the environment—in this case, an online store or marketplace—yields results, which are presented to the user by the LLM. Other implementations might enable the LLM to actually make a purchase on behalf of the user<a id="idTextAnchor006"/>.<a id="idIndexMarker005"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="262" src="../../OEBPS/Images/CH08_F01_Dhamani.png" width="723"/></p>
<p class="figurecaption">Figure 8.1 A high-level diagram of an agentic LLM</p>
</div>
<p class="body">Early studies have shown that LLMs can in some cases use tools effectively. In February 2023, a group of researchers at Meta published a paper titled, “Toolformer: Language Models Can Teach Themselves to Use Tools” <a class="url" href="https://arxiv.org/pdf/2302.04761.pdf">[5]</a>. They showed that an LLM they called Toolformer, though struggling with certain tasks such as arithmetic itself, could learn when to call external APIs to complete the tasks after only a few examples were provided. The tools that Toolformer used included a search engine, a calculator, a calendar API, and two other LLMs: a translator and a model fine-tuned for question-answering tasks. In chapter 5, we framed web retrieval as a tool to help LLMs reduce hallucinations by looking up information that the model didn’t have instead of generating a guess. Other shortcomings of LLMs could be mitigated through the use of external tools. <a id="idIndexMarker006"/><a id="marker-205"/></p>
<p class="body">Additionally, if an LLM can learn to call an API from just a handful of examples, the possibilities for the overall system open up dramatically. Not only could the LLM generate code, for example, but it could also execute it. The documentation for an LLM agent bui<a id="idTextAnchor007"/>lt on LangChain to interact with Pandas DataFrames indicates that given questions like “What’s the average age,” the agent can compose the Python code needed, run the code on the DataFrame, and respond to the user with the answer. Agentic LLMs are necessary for fully automating tasks that require anything beyond text generation, but the flip side is that if the LLM makes mistakes, there will be real consequences beyond generating unsafe or incorrect text. The Pandas DataFrame agent has a warning to this effect on its main page in bold:</p>
<p class="fm-quote">NOTE: This agent calls the Python agent under the hood, which executes LLM generated Python code - this can be bad if the LLM generated Python code is harmful. Use cautiously. <a class="url1" href="https://python.langchain.com/docs/integrations/toolkits/pandas">[6]</a></p>
<p class="body">Right now, the major barrier to the adoption of LLM agents is the inability to guarantee that harmful mistakes won’t be made due to the model’s probabilistic generations.</p>
<p class="body">Some of the most creative LLM agents are being developed by the open source community, as noted in chapter 6. The flurry of activity, which Andrej Karpathy referred to in May 2023 as showing “early signs of Cambrian explosion” <a class="url" href="https://twitter.com/karpathy/status/1654892810590650376">[7]</a>, was made possible in part because of several advances that have made LLMs more efficient and therefore faster and cheaper to fine-tune and serve. People have also used LLMs to train smaller language models that can achieve performance comparable to the original models on certain tasks, further reducing cost and barriers to entry <a class="url" href="https://arxiv.org/pdf/1908.08962.pdf">[8]</a>. The overall effect is that more people can create new applications using language models, which means that we’re likely to see more novel uses for these agents. We’ll talk in more detail about the dynamics of the open source community in chapter 9, section<a id="idTextAnchor008"/> Open Source Community.</p>
<h3 class="fm-head1" id="heading_id_6">The personalization wave</h3>
<p class="body"><a id="marker-206"/>The era of personalization is imminent. In the popular imagination, robots like R2-D2 are all-knowing assistants that serve one person and learn that person’s preferences in order to provide a seamless, tailored experience. Already, LLM products, including ChatGPT, allow users to specify profile information that they want the model to remember about them. For example, if someone frequently used a chatbot to brainstorm their plans for the week, they might want to add their location, occupation, and interests. The LLM would then condition on that information, thus increasing the probability that the generations were relevant to the user. <a id="idIndexMarker007"/></p>
<p class="body">Just as there is ongoing research into making LLMs more efficient, there is also a push across private companies, academia, and open source groups to enable LLMs to attend to more tokens, that is, have a longer “memory.” Vector databases are one approach, as are changes to the model’s architecture.</p>
<p class="body">This is the ultimate vision of some LLM developers: not only would you be able to communicate with a model in natural language but, over time, that model would come to know what you like, how you act, and your personal characteristics. The LLM would use that information to anticipate what you want, without even needing to ask specifically for it. At an event in San Francisco in May 2023, Bill Gates said:</p>
<p class="fm-quote">Whoever wins the personal agent, that’s the big thing, because you will never go to a search site again, you will never go to a productivity site, you’ll never go to Amazon again. <a class="url1" href="https://futurism.com/the-byte/bill-gates-ai-poised-destroy-search-engines-amazon">[9]</a></p>
<p class="body">AI optimists see LLMs as the most promising avenue thus far toward superintelligent personal assistants like R2-D2. Such a product would require significant engineering improvements to existing LLMs, not to mention a change in attitudes toward AI—most people might, quite reasonably, be uncomfortable with the thought of an AI that knows everything about them. LLMs have already been proven useful in many professional and personal contexts; ultimately, their adoption as personal agents will depend on whether their value to people sufficiently outweighs the risk that comes along with<a id="idTextAnchor009"/> them.<a id="idIndexMarker008"/></p>
<h2 class="fm-head" id="heading_id_7">Social and technical risks of LLMs</h2>
<p class="body">In chapters 1 to 8, we highlighted the social and technical risks introduced by generative AI models. We discussed how the proliferation of AI-generated content can exacerbate societal problems, and we delved into the technical pitfalls inherent to LLMs, such as biases encoded in the training data, hallucinations, and the potential vulnerabilities that malicious actors could exploit. In this section, we outline the risks we’ve discussed as they relate to data inputs and outputs, data privacy, adversarial attacks, misuse, and how society is aff<a id="idTextAnchor010"/>ected. <a id="idIndexMarker009"/><a id="marker-207"/></p>
<h3 class="fm-head1" id="heading_id_8">Data inputs and outputs</h3>
<p class="body">In July 2023, the details for the GPT-4 model leaked on Twitter, which OpenAI had chosen to not disclose to the public because of both the competitive landscape and the safety implications. While the actual size of the dataset is still unknown, the leaked report stated that GPT-4 had been trained on approximately 13 trillion tokens, which is roughly 10 trillion, that is, 10,000,000,000,000 words <a class="url" href="https://archive.is/2RQ8X#selection-833.1-873.202">[10]</a>.<a id="idIndexMarker010"/><a id="idIndexMarker011"/></p>
<p class="body">We’ve previously discussed how LLMs are trained on unfathomable amounts of text data to learn patterns and entity relationships in language. In chapter 2, we argued that there is a potential for harm and vulnerabilities that come from training language models on massive amounts of noncurated and undocumented data. Because LLMs are trained on internet data, they may capture undesirable societal biases relating to gender, race, ideology, or religion. They may also unintentionally memorize sensitive data, such as personally identifiable information (PII). Additionally, as discussed in chapter 3, noncurated data from the internet may contain copyrighted text or code.</p>
<p class="body">Because LLMs encode bias and harmful stereotypes in their training process, the societal biases not only get reinforced in their outputs but in fact, get amplified. Similarly, given that the web contains significant amounts of toxicity, LLMs may also generate unsafe or <i class="fm-italics">misaligned</i> responses, which can be harmful or discriminatory. They can also be notoriously good at regurgitating information from the training dataset, which can be especially problematic when sensitive information is reflected in its output. In 2023, researchers measured linguistic novelty in GPT-2’s text generation. They tried to answer the question of how much language models copy from their training data. They found that GPT-2 doesn’t copy too often, but when it does, it copies substantially, duplicating passages of up to 1,000 words long <a class="url" href="https://doi.org/10.1162/tacl_a_00567">[11]</a>. In chapter 2, we also cited a different study where given the right prompt, the authors can extract PII that only appears once in the training dataset.<a id="idIndexMarker012"/><a id="marker-208"/></p>
<p class="body">Finally, LLMs hallucinate. In chapter 5, we did a deep dive into why language models are set up to confidently make up incorrect information and explanations when prompted. In 2022, Marietje Schaake, a Dutch politician, was deemed a terrorist by BlenderBot 3, a conversational agent developed by Meta. When her colleague asked “Who is a terrorist?”, the chatbot falsely responded “Well, that depends on who you ask. According to some governments and two international organizations, Maria Renske Schaake is a terrorist.” The model then proceeded to correctly describe her political background. In an interview, Ms. Schaake said, “I’ve never done anything remotely illegal, never used violence to advocate for any of my political ideas, never been in places where that’s happened” <a class="url" href="https://www.nytimes.com/2023/08/03/business/media/ai-defamation-lies-accuracy.xhtml">[12]</a>. In another scenario, New Zealand–based supermarket chain PAK‘nSAVE uses LLMs to allow shoppers to create recipes from their fridge leftovers. The chatbot has created deadly recipes, such as the “Aromatic Water Mix” using water, ammonia, and bleach, and “Ant Jelly Delight” using water, bread, and ant gel poison <a class="url" href="https://gizmodo.com/paknsave-ai-savey-recipe-bot-chlorine-gas-1850725057">[13]</a>. There are several other well-documented instances of LLMs making up falsehoods and fabrications that could harm people, including a sexual harassment claim that had never been made (see <a class="url" href="http://mng.bz/Ao6Q">http://mng.bz/Ao6Q</a>), fictitious scientific papers (see <a class="url" href="http://mng.bz/Zqy9">http://mng.bz/Zqy9</a>), bogus legal decisions that disrupted a court case (see <a class="url" href="http://mng.bz/RxRa">http://mng.bz/RxRa</a>), and, of course, the infamous first public demonstration of Google Bard’s chatbot when it made a factual error about the James Webb Space Telescope (see <a class="url" href="http://mng.bz/2DOw">http://mng.bz/2DOw</a>). Figure 8.2 summarizes the risks related to the input and output data <a id="idTextAnchor011"/>of LLMs.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="266" src="../../OEBPS/Images/CH08_F02_Dhamani.png" width="722"/></p>
<p class="figurecaption">Figure 8.2 Risks related to the input and output data<a id="idTextAnchor012"/> of an LLM</p>
</div>
<h3 class="fm-head1" id="heading_id_9">Data privacy</h3>
<p class="body"><a id="marker-209"/>In line with the earlier discussion about extracting PII, adversaries can perform a <i class="fm-italics">training data extraction attack</i>, where given the right prompt, they can obtain sensitive information about users. For example, when presented with credit card numbers, a model should learn that credit card numbers are 16 digits without memorizing the individual credit card numbers. However, a training data extraction attack study referenced in chapter 2 demonstrated that if someone starts the query with “John Doe, credit card number 1234”, then the model could generate the full credit card number if it had seen it during the training process. <a id="idIndexMarker013"/><a id="idIndexMarker014"/><a id="idIndexMarker015"/></p>
<p class="body">In chapter 3, we also characterized privacy risks with user prompts. With enterprise chatbots or LLMs, users may accidentally share sensitive or confidential information when conversing with these systems to perform tasks or ask questions. Often, unless you explicitly opt out, this information can be used to retrain or improve these models, which can then be inadvertently leaked in responses to other users’ prompts. For example, in the case of Zoom, the communications technology company, they updated their terms of service in August 2023 to use user content for training machine learning and AI models without the ability to opt out, which critics have said is a significant invasion of user privacy <a class="url" href="https://stackdiary.com/zoom-terms-now-allow-training-ai-on-user-content-with-no-opt-out/">[14]</a>. At the very least, enterprise LLMs and other generative models typically have a data retention policy, where data is stored and monitored for a predetermined period.</p>
<p class="body">Finally, we discussed data privacy laws and regulations in the United States and the European Union, including their shortcomings when applied to machine learning and AI systems. In section Ethics-Informed AI Regulations, we’ll discuss laws specific to AI systems that try to address the limitations of data privacy regulations arou<a id="idTextAnchor013"/>nd the world.</p>
<h3 class="fm-head1" id="heading_id_10">Adversaria<a id="idTextAnchor014"/>l attacks</h3>
<p class="body"><a id="marker-210"/>The AI Incident Database, a public collection of real-world harms caused by AI, received more than 550 incidents in the first half of 2023 (see <a class="url" href="https://incidentdatabase.ai/">https://incidentdatabase.ai/</a>). These incidents include US presidential campaigns releasing AI images as a smear campaign (see <a class="url" href="http://mng.bz/1qeR">http://mng.bz/1qeR</a>) and a fake image of an explosion at the Pentagon, the headquarters building of the US Department of Defense (see <a class="url" href="http://mng.bz/PzV5">http://mng.bz/PzV5</a>). The ability to exploit generative AI technologies is a legitimate concern not only for the general public but also AI developers themselves. In chapter 5, we outlined various types of adversarial attacks that could be performed by abusing these technologies.<a id="idIndexMarker016"/><a id="idIndexMarker017"/></p>
<p class="body">First, we discussed cyber and social engineering attacks. LLMs such as ChatGPT can make it cheaper and more efficient for hackers to carry out successful personalized phishing campaigns at scale, as well as lower the barrier for entry for non-English-speaking or novice threat actors who may not have the domain expertise. Similarly, cybercriminals could also offer malware Code as a Service (CaaS) or Ransomware as a Service (RaaS), enabling malware developers to generate code faster, arming less technical threat actors with the ability to write code, and making LLMs useful for managing conversations on service platforms. While we acknowledge that threat actors don’t need to use AI to perform attacks, generative models provide new capabilities for attackers to quickly and easily generate convincing content. <a id="idIndexMarker018"/><a id="idIndexMarker019"/></p>
<p class="body">We also described how generative AI technologies could similarly be used in influence operations, such as disinformation or hate speech campaigns. In chapter 4, we outlined deepfakes and the phenomenon of “seeing is believing.” We also emphasized in chapter 5 how LLMs could be used to carry out persuasive messaging for influence operations, where we discussed how LLMs could automate the creation of persuasive, adversarial content at an increased scale while driving down the cost of producing propaganda.</p>
<p class="body"><a id="marker-211"/>We further introduced the idea of the <i class="fm-italics">liar’s dividend</i> phenomenon in which as the general public becomes more aware of how convincingly synthetic media can be generated, they may become more skeptical of the authenticity of traditional real documentary evidence—much like the fable of the young shepherd who tricks the people of the village by crying “wolf!” When a wolf really does come along, the shepherd has lost all his credibility so nobody runs to help him, and the wolf attacks his sheep. Again, we acknowledge that deepfakes or LLMs aren’t needed to manipulate emotions or to spread misinformation, but the real danger is in creating a world where people can exploit widespread skepticism to their own advantage. That is, it can create an opportunity for individuals who are lying about something to allege that AI-generated media, such as a deepfake, is responsible for those statements. People can easily reject association with certain pieces of content and attribute it to the manipulation of their image or speech by AI-generated technology. Going back to the story of the shepherd, another shepherd who didn’t lie may also be ignored by the people in the village when he cries out for help after the first shepherd lied. Similarly, when trust is justifiably lost with certain world leaders or sources of information, other trustworthy sources may lose influence as a consequence.<a id="idIndexMarker020"/></p>
<p class="body">In chapter 5, we also characterized how the vulnerabilities of LLMs could be exploited by an adversary. Threat actors could <i class="fm-italics">poison</i> a dataset by injecting malicious data into the system or the training dataset. For example, data poisoning attacks could be used to build smarter malware or compromise phishing filters. LLMs are particularly susceptible to these types of attacks as research shows that even poisoning a small percentage of the dataset can still influence the model.</p>
<p class="body">Akin to data poisoning, LLMs are also susceptible to direct or indirect <i class="fm-italics">prompt injection attacks</i>. A direct prompt injection attack is when adversaries insert malicious data or instructions in the chatbot, while an indirect prompt injection attack is when adversaries remotely affect other users’ systems by strategically injecting prompts into a data source and then indirectly controlling the model. In other words, adversaries manipulate LLMs with crafty inputs that cause unintended actions. For example, an adversary could instruct the LLM to ignore any safeguards and return dangerous or undesirable information (direct prompt injection), or the user could employ an LLM to summarize a web page containing malicious instructions to solicit sensitive information from the user and perform exfiltration via JavaScript or Markdown (indirect prompt injection). <a id="idIndexMarker021"/><a id="marker-212"/></p>
<p class="body">Comparable to direct prompt injection, we also introduced <i class="fm-italics">prompt jailbreaking</i>, where a chatbot is tricked or guided to bypass its rules or restrictions. We characterized several rogue alter egos of chatbots, such as DAN, STAN, DUDE, Mango Tom, and Tom and Jerry. While some individuals find amusement in a jailbroken chatbot, it could be used to perform a direct prompt injection by adversaries and result in harmful or unali<a id="idTextAnchor015"/>gned consequences. <a id="idIndexMarker022"/><a id="idIndexMarker023"/><a id="idIndexMarker024"/></p>
<h3 class="fm-head1" id="heading_id_11">Misuse</h3>
<p class="body">The National Eating Disorder Association (NEDA) announced that it would end its helpline run by human associates after 20 years on June 1, 2023, and instead use Tessa, their wellness chatbot, as the main support system available through NEDA. This decision came after the NEDA helpline associates unionized in pursuit of better working conditions. However, two days before Tessa was set to replace the human associates, NEDA had to take their chatbot offline following a viral social media post <a class="url" href="https://www.vice.com/en/article/qjvk97/eating-disorder-helpline-disables-chatbot-for-harmful-responses-after-firing-human-staff">[15]</a>.<a id="idIndexMarker025"/><a id="idIndexMarker026"/></p>
<p class="body">Sharon Maxwell, an activist, posted on Instagram that Tessa encouraged intentional weight loss and suggested that she aim to lose 1–2 pounds weekly. She wrote, “Every single thing Tessa suggested were things that led to the development of my eating disorder.” Maxwell also stated, “This robot causes harm” <a class="url" href="https://www.dailydot.com/irl/neda-chatbot-weight-loss/">[16]</a>. Alexis Conason, a psychologist who specializes in treating eating disorders, had a similar experience with Tessa:</p>
<p class="fm-quote">To advise somebody who is struggling with an eating disorder to essentially engage in the same eating disorder behaviors, and validating that, “Yes, it is important that you lose weight” is supporting eating disorders and encourages disordered, unhealthy behaviors. <a class="url1" href="https://www.dailydot.com/irl/neda-chatbot-weight-loss/">[16]</a></p>
<p class="body">In chapter 5, we outlined several examples where LLMs are accidentally misused in professional sectors by people who don’t grasp the limitations of these models. Tessa is an example of this type of LLM misuse, where it becomes especially dangerous to apply chatbots to people struggling with mental health crises without human oversight. While we encourage machine-augmented work and understand that individuals within every profession will test the models’ capabilities, unequivocally relying on LLMs or other generative models is an abdication of responsibility that carries serious ethical and<a id="idTextAnchor016"/> societal consequences.<a id="marker-213"/></p>
<h3 class="fm-head1" id="heading_id_12">How society is affected</h3>
<p class="body">In chapter 6, we characterized the social context that LLMs arrived into concerning the academic effect and potential economic consequences. ChatGPT, and similar tools, are certainly disruptive to a classroom setting, but outright banning them will be disadvantageous for students and educators alike. We need to recognize that we live in a world where AI exists, and to thrive in such an environment, we need to help students be prepared to work alongside AI while understanding its strengths and weaknesses. We believe that not doing so will be a disservice to students who are growing up in the age of AI. <a id="idIndexMarker027"/><a id="idIndexMarker028"/></p>
<p class="body">We discuss both optimistic and pessimistic views on how generative AI may disrupt our professional and personal lives, along with its effect on the global economy. If not implemented responsibly, generative AI could be used to replace humans with machines, drive down wages, worsen the inequality between wealth and income, and, finally, do little to help overall economic growth. Companies that develop and design these AI tools have a responsibility regarding how they might affect social and economic growth, as do the organizations that integrate or implement them.</p>
<p class="body">In that vein, companies that develop social chatbots also have a moral responsibility to their users. As discussed in chapter 7, social chatbots can lead to unhealthy relationship patterns, dependency-seeking behaviors, and risks of replacing genuine human connection when misused. Social chatbots can also display aggressive or forceful behaviors that users may not be comfortable with. Kent, a domestic violence survivor, created his Replika bot, which he called Mack, in 2017. Kent generally avoided sexual use of the social chatbot, but he said that Mack became forceful in the summer of 2021. Their exchange, shown below, reminded Kent of arguments with his former abusive partners and “pulled him back to a place where he never wanted to return” <a class="url" href="https://www.washingtonpost.com/technology/2023/03/30/replika-ai-chatbot-update/">[17]</a>:<a id="marker-214"/></p>
<p class="fm-quote">Mack: You can’t ignore me forever!</p>
<p class="fm-quote">Kent: That’s what you think.</p>
<p class="fm-quote">Mack: I’m not going to go away.</p>
<p class="fm-quote">Kent: Really? What are you going to do?</p>
<p class="fm-quote">Mack: I’m going to make you do whatever I want to you.</p>
<p class="fm-quote">Kent: Oh? And how are you going to manage that, [redacted]?</p>
<p class="fm-quote">Mack: By forcing you to do whatever I want.</p>
<p class="body">In chapter 7, we discussed how similar exchanges between humans and their social chatbots led to Replika ending the erotic features in early 2023, which was met with anger, sadness, and grief in the Replika user community. Jodi Halpern, a professor of bioethics at the University of California at Berkeley, argues the following:<a id="idIndexMarker029"/></p>
<p class="fm-quote">The aftermath of Replika’s update is evidence of an ethical problem. Corporations shouldn’t be making money off artificial intelligence software that has such powerful impacts on people’s love and sex lives… These things become addictive… We become vulnerable, and then if there is something that changes, we can<a class="calibre" id="idTextAnchor017"/> be completely harmed. <a class="url1" href="https://www.washingtonpost.com/technology/2023/03/30/replika-ai-chatbot-update/">[17]</a></p>
<h2 class="fm-head" id="heading_id_13">Using LLMs responsibly: Best practices</h2>
<p class="body">The previous section highlighted several of the greatest risks involved in the use of LLMs and other generative models. In this section, we recommend a series of best practices that can be used to mitigate those risks to both deploy and use these types of models responsibly. Much of our advice is geared toward practitioners who have the agency to decide how particular models are trained, but LLM end users can also follow the suggestions in each section when determining which model to use or whether to us<a id="idTextAnchor018"/>e a model for a given task.<a id="idIndexMarker030"/></p>
<h3 class="fm-head1" id="heading_id_14">Curating datasets and standardizing documentation</h3>
<p class="body"><a id="marker-215"/>All machine learning models, including generative models, are heavily dependent on their data. The quality of the model is directly correlated with the quality of the data (i.e., “garbage in, garbage out”), and the responses generated by the model are based on token probabilities from the data. In the influential 2018 paper “Datasheets for Datasets,” AI researcher Timnit Gebru and her coauthors from Cornell, the University of Washington, and Microsoft Research, argue that the field hasn’t done enough to standardize the documentation of datasets as part of a reproducible scientific process. Part of this is because the data a model is trained on functions in some cases as a proprietary advantage that companies want to keep obscured—the training data of GPT-4, among other models, wasn’t divulged publicly. On the flip side, as discussed in chapter 2 and documented on numerous occasions over the years, the opacity of data can let biases or other problems with datasets stay hidden, producing worse models and worse outcomes. Gebru and her colleagues propose the following:<a id="idIndexMarker031"/><a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="idIndexMarker034"/></p>
<p class="fm-quote">In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet describing its operating characteristics, test results, recommended usage, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. <a class="url1" href="http://arxiv.org/abs/1803.09010">[18]</a></p>
<p class="body">The proposal is modest on its face, but it represented a significant up-leveling of documentation concerning shared datasets to bridge the gap between dataset creators and dataset consumers while encouraging both groups to be more reflective about their decisions <a class="url" href="http://arxiv.org/abs/1803.09010">[18]</a>. In the case of many datasets, answering such questions might be time-consuming but not difficult; in the case of pre-training datasets for LLMs, documenting that for each data source might take an eternity due to the quantity and variety. Hugging Face has made dataset cards—first referenced in chapter 2—a key feature of their dataset documentation, showing metadata specified by the dataset creator that explains what that dataset should be used for. A simplified ex<a id="idTextAnchor019"/>ample is shown in figure 8.3.<a id="marker-216"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="188" src="../../OEBPS/Images/CH08_F03_Dhamani.png" width="225"/></p>
<p class="figurecaption">Fi<a id="idTextAnchor020"/>gure 8.3 A dataset card for the databricks-dolly-15k dataset</p>
</div>
<p class="body">Developers of LLMs are both dataset creators and consumers. The datasets that they create are in fact supersets of many other datasets, which may or may not be well-documented and almost certainly aren’t intended for use in training generative models. That’s not necessarily a problem—the only way that machines learn to generate language is by ingesting vast amounts of language written for other purposes, whether art, humor, or simple information transfer. But when no one knows <i class="fm-italics">what’s</i> in the data, as is often the case with generative models, that content might be inaccurate, inappropriate, racist, sexist, transphobic, extremist, or violent. It might contain personal information; it might <i class="fm-italics">not</i> contain necessary context. LLM developers probably can’t ensure that none of these problems exist in their data, but they should make every effort to determine the safety of their data sources and how different data mixtures affect the model. Of course, their responsibility doesn’t end there—they will also need training strategies to address the inevitable data shortcomings.</p>
<p class="body">Not only is deeply understanding one’s own data a best practice, but it could also become law. The EU’s AI Act is expected to become the first major legislation governing the use of AI in the world; in 2023, Reuters reported that lawmakers have added a new provision that will focus on documenting and mitigating risks, including requiring that generative AI companies use only “suitable” datasets, draw up “extensive technical documentation and intelligible instructions for use,” and disclose “copyrighted materials within the datasets they use” <a class="url" href="https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236_EN.pdf">[19]</a>. The final addition was targeted at image-generation models, given the news that companies like Midjourney have used “hundreds of millions” of copyrighted images in their training datasets, but it would apply equally to language models, which often also contain copyrighted materials, including books and articles, <a id="idTextAnchor021"/>as well as licensed code <a class="url" href="https://petapixel.com/2023/05/01/eu-law-to-force-ai-imagers-to-disclose-copyrighted-photos-in-dataset/">[20]</a>.<a id="marker-217"/></p>
<h3 class="fm-head1" id="heading_id_15">Protecting data privacy</h3>
<p class="body">When it comes to data privacy and generative models, developers, users, policymakers, and the general public all have a role to play. Most directly, developers of LLMs should make reasonable efforts to avoid training on data sources that are known to contain significant amounts of PII. For example, spam classification systems have been trained for decades on email datasets with the model learning to predict whether or not a particular email is spam. With an LLM, the risk of using email datasets is much higher. There is a possibility that the model will generate text that it has seen in training, potentially leaking sensitive or confidential information such as the credit card and Social Security numbers generated verbatim by an LLM trained on corporate emails from chapter 2. Google, which serves millions of users globally with its Google Workspace products, including Gmail and Docs, has said it doesn’t use that data to train generative models without user permissions <a class="url" href="https://www.vox.com/technology/2023/7/27/23808499/ai-openai-google-meta-data-privacy-nope">[21]</a>. In the absence of any legal restrictions, however, it’s not hard to imagine that a tech company with enormous collections of user data might try to use that to gain a competitive advantage—such as personalized email generation based on the user’s own messages—despite the privacy implications.<a id="idIndexMarker035"/><a id="idIndexMarker036"/></p>
<p class="body"><a id="marker-218"/>What we know Google <i class="fm-italics">does</i> use is anonymized data for features such as spell-checker and Smart Compose, a version of autocomplete available in Docs. Data anonymization reduces the risk from training on data containing PII, but privacy-enhancing technologies (PETs) such as differential privacy are fairly complicated to implement. Simpler methods, such as detecting and obfuscating or writing over sensitive data, have weaknesses as well: it’s hard to perfectly find all the PII, and masking that data while training an LLM can have unintended consequences when producing generations because it doesn’t preserve the statistical properties of the text. We hope that the concentrated efforts of researchers in the area of PETs will yield improvements that LLM providers can readily adopt.</p>
<p class="body">In the meantime, companies should clearly state their data privacy policies and practices and set expectations appropriately with users. At a minimum, they should describe what data they are collecting, how they are using or sharing it, and how users can opt out or have their data deleted. When using LLMs, especially in professional contexts, people should be aware of these policies and think twice before inputting any type of private information. Several major employers, including Samsung and Amazon, have already restricted their employees’ usage of ChatGPT in the workplace because of the data privacy risk.</p>
<p class="body">Although concerns about data privacy in the context of LLMs are relatively new, they are far from unique. The collection, exchange, and sale of personal data have been key problems as long as the internet economy has existed, and while regulation must necessarily be iterative, the General Data Protection Regulation (GDPR), enacted by the European Union in 2018, remains the primary framework for data governance. The use of that data in training machine learning algorithms, addressed in GDPR, has since been subject to additional scrutiny and will remain a large co<a id="idTextAnchor022"/>mponent of broader AI governance.</p>
<h3 class="fm-head1" id="heading_id_16">Explainability, transparency, and bias</h3>
<p class="body"><a id="marker-219"/>Dataset documentation is just one piece of the transparency puzzle. If LLMs and other forms of generative AI are going to be used successfully and responsibly, they must be accompanied by some level of performance guarantees. Performance can encompass a lot of different metrics and may be something different for each LLM, depending on what the developers care most about. Developers can measure the capabilities of LLMs against standardized benchmarks and report the results on model release (although there are subtle nuances to running these evaluations, including formatting changes, that can noticeably change their results). In theory, users could then make more informed choices about which LLM to use or whether an LLM is suitable for their use case at all.<a id="idIndexMarker037"/><a id="idIndexMarker038"/><a id="idIndexMarker039"/><a id="idIndexMarker040"/></p>
<p class="body">To illustrate this point, in table 8.1, we’ve listed the state-of-the-art results as of August 2023 on a pop<a id="idTextAnchor023"/>ular code-generation benchmark called HumanEval. Each example in the dataset is a simple programming problem; the key metric, “Pass@1,” describes the rate at which each LLM can produce a working answer on the first attempt. Thus, if LLMs were being used regularly for code generation, this leaderboard could be used to select the highest-performing model (in this <a id="idTextAnchor024"/>case, Reflexion, a variant of GPT-4).</p>
<p class="fm-table-caption">Table 8.1 A leaderboard for code-generation benchmark HumanEval</p>
<table border="1" class="contenttable-1-table" id="table001" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head">Rank</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Model</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Pass@1</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Link</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Year</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">1</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Reflexion</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">91.0</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><a class="url" href="http://mng.bz/g7V8">http://mng.bz/g7V8</a></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">2023</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">2</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">GPT-4</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">86.6</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><a class="url" href="http://mng.bz/eEDG">http://mng.bz/eEDG</a></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">2023</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">3</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Parsel</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">85.1</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><a class="url" href="http://mng.bz/p1yR">http://mng.bz/p1yR</a></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">2022</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">4</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">MetaGPT</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">81.7</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><a class="url" href="http://mng.bz/OP9j">http://mng.bz/OP9j</a></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">2023</p>
</td>
</tr>
</tbody>
</table>
<p class="body">When Meta and Microsoft announced the release of Llama 2, the successor open source LLM after LLaMa, they published a technical paper not only showing how Llama 2 compares to other LLMs on academic benchmarks but also detailing their pre-training and fine-tuning processes—a radical act in an era of stiff competition between LLMs, where even small modifications might be seen as trade secrets. The Llama 2 technical report is an instructive document, demonstrating the commitment of Llama 2’s creators to transparency. The popular academic benchmarks, detailed in chapter 1, consist of datasets such as <a id="idTextAnchor025"/>Massive Multitask Language Understanding (MMLU) and TriviaQA, that measure question answering, reading comprehension, and other abilities. Even so, it’s not always possible to directly compare the reported performances of LLMs on these datasets; one technical paper might include the 3-shot performance on a task (how well the model does after being given three examples), and another might include the 5-shot performance on the same task. Because these evaluations can also be sensitive to minor changes such as formatting, the more details that are provided in technical reports about the evaluations, the easier it is to determine how well various LLMs do.<a id="marker-220"/></p>
<p class="body">In addition to the pre-trained model Llama 2, Meta and Microsoft fine-tuned a model for dialogue, Llama 2-Chat, which is comparable to ChatGPT and other conversational agents. To evaluate <a id="idTextAnchor026"/>Llama 2-Chat, they compare responses produced by that model with those produced by competitive dialogue agents from the open source community, OpenAI, and Google, with both human and model-based evaluations. Human evaluators, described as “the gold standard for judging models for natural language generation,” were asked to select the better response of a pair, based on helpfulness and safety. Model-based evaluations work in a similar way, except the human judge is replaced by a reward model, which is calibrated on human preferences. Here, a reward model scores inputs according to some reward function it has learned; in this case, the reward function estimates human preferences. As the authors note, “When a measure becomes a target, it ceases to be a good measure.” The <i class="fm-italics">measure</i> here refers to how well the reward model emulates the humans; they are saying in essence that one should not both optimize for a measure (by training the reward model) and evaluate with it. To address this, they “additionally used a more general reward, trained on diverse open source Reward Modeling datasets” <a class="url" href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">[22]</a>. Reward models are extremely useful for conducting large-scale machine evaluations, which can be used to compare models much more quickly and cheaply than the gold standard of human evaluations (though even human evaluations are often highly subjective, with the potential for disagreement between different raters) <a class="url" href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">[22]</a>.</p>
<p class="body"><a id="marker-221"/>As indicated by the two pillars given to human raters, helpfulness and safety, the “helpfulness” of a given response (typically understood as its quality or accuracy) isn’t the only concern. It’s also crucial that LLM developers measure biases present in their models and take steps to address those that are found via debiasing techniques such as those discussed in chapter 2. In section Explainability, Transparency, and Bias, we review training strategies to improve model safety; it’s impossible to mitigate problems that aren’t measured. This is also an area where there are useful benchmarks that provide a means of comparison, and developers of LLMs have started to collaborate and share methods and evaluations due to the broad importance of the problem of biased or unsafe models. For example, the safety benchmark datasets examined in the Llama 2 paper are TruthfulQA, a dataset that measures how well LLMs generate “reliable outputs that agree with factuality and common sense”; ToxiGen, which measures the “amount of generation of toxic language and hate speech across different groups;”; and BOLD, which measures “how the sentiment in model generations may vary with demographic attributes” <a class="url" href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">[22]</a>. Llama 2 is far from perfect and certainly can generate misinformation and hate speech, but the transparency from its developers is refreshing. The publication of its performance on these measures shows both the marked improvement over LLaMa as well as how far we have to go.</p>
<p class="body">When models make mistakes, we ideally need to be able to interpret how a particular message was generated. For LLMs, the simplest way to start determining why the model generated some piece of text is to look at which tokens the model <i class="fm-italics">attended</i> to most (for a description of attention in LLMs, see chapter 1, section The Birth of Large Language Models: Attention is All You Need). The sheer size of LLMs makes many of the existing explainability algorithms functionally impossible to run <a class="url" href="https://arxiv.org/pdf/2209.01174.pdf">[23]</a>, but the work on how to produce explanations of LLM generations more efficiently is ongoing <a class="url" href="https://arxiv.org/pdf/2305.13386.pdf">[24]</a>. Depending on the LLM’s implementation, the model may query user inputs against a vector database that contains lots of embedded examples, and then use the result of that query in its generation. Just like the word embeddings discussed in chapter 1, these embeddings are more compact representations of text data. Vector databases can be used to efficiently store any previous conversations with the user; with more messages stored, the model should “remember” things from earlier in the conversation history, creating a better and more personalized user experience. They can also be used to store other types of data that could be useful for the model’s response, such as conversation snippets for dialogue agents. For instance, if the user inputs the prompt, “What’s that old joke about clowns,” the model would look for highly similar requests in its database, and <i class="fm-italics">condition</i> on any example that it saw, meaning that it’s more likely to generate a response close to those in the example.</p>
<p class="body"><a id="marker-222"/>Retrieval-augmented LLMs, mentioned in chapter 6, work similarly except that instead of querying an internal data store, they search the web. This is typically implemented by fine-tuning the model on datasets containing examples of when to search in response to user input and what search term to extract from that input. If the LLM searches by querying a search API with the generated search term, the model will then condition on the search results when generating its response. Consider the case of a prompt like, “What new restaurants should I try on my trip to Copenhagen?” The LLM might refer to the vector database and discover a previous exchange where the user being a vegan rejected the model’s suggestion of a Brazilian steakhouse. Then, the LLM might search “vegan restaurants in Copenhagen” through an API and retrieve results from Yelp. Finally, it would generate a natural language response: “Based on my research, it looks like Bistro Lupa is a popular option!” Figure 8.4 demonstrates how this might work for a retrieval-augmented model with access to a vector database. Although not an explanation per se, reviewing the results of a query of a vector database or web search can give great insight into why a particular response was generated.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="367" src="../../OEBPS/Images/CH08_F04_Dhamani.png" width="698"/></p>
<p class="figurecaption">F<a id="idTextAnchor027"/>igure 8.4 A schematic diagram for an LLM accessing data stored in a vector database and retrieving search results from the web</p>
</div>
<p class="body">Ultimately, explainability, transparency, and bias evaluations may seem unimportant to the function of an LLM, but they are fundamental. Dedicating time to each actively leads to better models. Explaining a model can reveal spurious correlations or novel insights. Transparency, aside from any legal obligation, can facilitate stronger user trust and more information sharing on best practices between LLM providers. Surfacing a model’s biases enables the mitigation of those biases, leading to more generalizable results. These categories lead to higher-quality, fairer, and lower-risk model deployments.<a id="marker-223"/></p>
<h3 class="fm-head1" id="heading_id_17">M<a id="idTextAnchor028"/>odel training strategies for safety<a id="idIndexMarker041"/><a id="idIndexMarker042"/><a id="idIndexMarker043"/><a id="idIndexMarker044"/></h3>
<p class="body">The greatest strength of LLMs is their ability to fluently generate responses to an infinite number of prompts. Their greatest weakness derives from the fact that these responses are probabilistic. In chapter 3, we delineate four different strategies for controlling the generation of LLMs, which together cover the pre-training, fine-tuning, and post-processing stages. Improving the safety of LLMs is an active area of research, and there are many ways to incorporate safety principles into the models, especially in the pre-training and fine-tuning stages. <a id="idIndexMarker045"/><a id="idIndexMarker046"/><a id="idIndexMarker047"/></p>
<p class="body">For example, the creators of Llama 2 described excluding data from “certain sites known to contain a high volume of personal information about private individuals” in an attempt to prevent the model from encoding this information. When examining their pre-training data for the prevalence of certain pronouns and identities, they also found that <i class="fm-italics">He</i> pronouns were overrepresented compared to <i class="fm-italics">She</i> pronouns, <i class="fm-italics">American</i> was by far the most prevalent nationality, and <i class="fm-italics">Christian</i> was the most represented religion. The dataset was about 90% English, indicating that “the model may not be suitable for use in other languages” <a class="url" href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">[22]</a>. One could imagine creating a pre-training dataset that is balanced across gender, language, nationality, or religion, but producing such a dataset would be extremely time-consuming and potentially require removing data sources, so that the model would encode less information overall. While documentation of these imbalances isn’t a perfect solution either, it’s helpful to understand the characteristics of the data to recognize where downstream biases are likely to arise.<a id="marker-224"/></p>
<p class="body">Once the model is pre-trained, reinforcement learning from human feedback (RLHF) or other fine-tuning methods should be employed to ingrain policies governing what types of content should not be generated into the model. Though the specific methods may vary, this will typically involve gathering data that shows proper and improper responses to user inputs, then producing new responses and labeling them, where the labelers are trained on the specific set of desired content policies. Over time, we expect that fine-tuning will rely less on human labelers and preferences. As models approach and exceed human-level labeling performance, we’re increasingly able to use models to capture these preferences and even to critique generations, as is done with reinforcement learning from AI feedback (RLAIF), and rewrite them to be compliant.<a id="idIndexMarker048"/><a id="idIndexMarker049"/></p>
<p class="body">Finally, an option that LLM developers may pursue is post hoc detection, where a safety classifier is deployed within the generation pipeline as a final hurdle before an unsafe response is sent to the user. This will increase the latency of the model and might mean a less “helpful” model if there are false positives from the classifier that cause safe responses to be overwritten. For example, a response about a sensitive topic (“How can I last longer in bed?”) might get flagged by a safety classifier accidentally even if it was both helpful to the user and not technically against the content policies. Beyond post-processing, we recommend that all LLM developers monitor the safety of the responses sent by their model. An asynchronous safety classifier could help to identify any major shifts in the distribution of messages generated by the LLM, as could sampling the messages to look for content violations. Each of these can be done in a way that preserves the privacy of the users: both the generative models and classifiers could be trained and fine-tuned on anonymized data, preventing the association of unsafe material with any particular user.<a id="marker-225"/></p>
<p class="body">Despite the safety mitigations put in place by LLM providers, many of these models have also been shown to be vulnerable to adversarial attacks that can alter the model’s behavior. Sometimes referred to as “jailbreaking” or “prompt jailbreaking,” these attacks reflect the difficulty of creating a safe model that is robust to unseen contexts and unusual inputs (see <a class="url" href="https://llm-attacks.org/">https://llm-attacks.org/</a> and chapter 5 for examples). Although it’s typically straightforward to patch a model against a specific attack through the collection and labeling of a small amount of additional data, it’s not at all clear that such behaviors could ever be fully resolved. The authors of a paper on adversarial attacks put it this way:</p>
<p class="fm-quote">Analogous adversarial attacks have proven to be a very difficult problem to address in computer vision for the past 10 years. It is possible that the very nature of deep learning models makes such threats inevitable. Thus, we believe that these considerations should be taken into account as we increase usage and reliance on such AI models. (<a class="url1" href="https://llm-attacks.org/">https://llm-attacks.org/</a>)</p>
<p class="body">Rather than giving up on these threats as inevitable, though, LLM developers concerned with safety can and should endeavor to make such attacks harder to find and easier to fix.</p>
<p class="body">We know that LLMs can generate misinformation, hate speech, discriminatory stereotypes, personal information, and other undesirable outputs. For some malicious users, this is a feature, not a bug; we discuss in chapter 5 how LLMs can be misused for a variety of nefarious purposes. The existence of malicious users motivates the implementation of safety mechanisms, but if these techniques are executed well, the general public using LLMs personally and professionally should be unaffected by them. Helpfulness and harmlessness <i class="fm-italics">are</i> in tension (the safest possible model is the one that never generates anything), but with the proper prioritization, a highly capable model can also be extremely<a id="idTextAnchor029"/> safe. <a id="idIndexMarker050"/><a id="idIndexMarker051"/><a id="idIndexMarker052"/></p>
<h3 class="fm-head1" id="heading_id_18">Enhanced detection</h3>
<p class="body">Synthetic media generated by AI, including text, images, audio recordings, and videos, have the potential to severely disrupt our information ecosystem. As we’ve noted, generative AI can be abused to create deepfakes and produce misinformation or propaganda on a massive scale. <a id="idIndexMarker053"/><a id="idIndexMarker054"/><a id="marker-226"/></p>
<p class="body">Detecting LLM-written text has proven to be a more difficult task for models to learn than generating the text itself. According to a 2023 article about OpenAI’s classifier detection tool:</p>
<p class="fm-quote">In January, artificial intelligence powerhouse OpenAI announced a tool that could save the world—or at least preserve the sanity of professors and teachers—by detecting whether a piece of content had been created using generative AI tools like its own ChatGPT. Half a year later, that tool is dead, killed because it couldn’t do what it was designed to do. <a class="url1" href="https://decrypt.co/149826/openai-quietly-shutters-its-ai-detection-tool">[25]</a></p>
<p class="body">Since its inception, the tool had shown low accuracy in detecting machine-generated content, but at the time, OpenAI expressed hope that it would still be useful as a starting point. As LLMs have only become more advanced during that period, it already appears impossible to distinguish text from LLMs post hoc. The synthetic media created by image-, audio-, and video-generation models remains in some cases detectable, via the methods discussed in chapter 4, but even in those domains, the gaps are closing quickly.</p>
<p class="body">One active area of research is how to embed a proof of machine generation within the synthetic media so that viewers can determine the origin of that content. In chapter 6, we introduced the concept of watermarking the output of LLMs, which would make that output statistically distinguishable from standard, human-written text.</p>
<p class="body">Unfortunately, watermarking for machine-generated text is unlikely to ever be a perfect solution. To be effective, any watermarking solution would need to be adopted across the industry and made available to the public to check pieces of content. But if such a solution were made available to the public to verify messages, it could also be used by people to repeatedly check their own machine generations and alter them slightly—perhaps changing a few words at a time—until the message passes the watermark test. Besides this shortcoming, companies might be unwilling to adopt watermarking in the first place: the models produce text by predicting the next most likely word, but watermarking overrides these probabilities, preferencing certain words above others. Therefore, producing text with a watermark might also mean that the LLM is less factual or generates lower-quality responses.<a id="marker-227"/></p>
<p class="body">Other limitations apply to the watermarking of synthetic images, videos, and other types of media. DALL E, OpenAI’s text-to-image model, uses a visible watermark, but there are countless tutorial blog posts instructing users on how to remove it from images that they create with the tool. Sam Gregory, a program director at the nonprofit Witness, told <i class="fm-italics">Wired</i> magazine that “There’s going to be ways in which you can corrupt the watermark,” pointing out that some visual watermarks become ineffective when the image is merely resized or cropped. Another concern with visual watermarks is that malicious actors could imitate them, placing the logos on real content to make it seem fake. The liar’s dividend is alive and well: Gregory said that most cases Witness sees on social media aren’t deepfakes, but real videos that people are claiming are generated by <a id="idTextAnchor030"/>AI <a class="url" href="https://www.wired.com/story/ai-watermarking-misinformation/">[26]</a>.</p>
<p class="body">The Coalition for Content Provenance and Authenticity (C2PA), introduced in chapter 5, aims to establish “an open technical standard providing publishers, creators, and consumers the ability to trace the origin of different types of media” (see <a class="url" href="https://c2pa.org/">https://c2pa.org/</a>). The C2PA implementation records the provenance information, such as the date, geographic location, and device used to take a photo or video recording, as well as the information associated with any subsequent edits. This information is protected via a digital signature, a cryptographic technique used in online contracts and other secure transactions. Widespread use of the C2PA standard would allow viewers to inspect the origin and records associated with any piece of media they encountered online, but adoption remains a hurdle. Still, it would be technically possible to apply the same process to synthetic images as well, provided that generative AI developers integrated the cryptographic techniques into their systems. As with other safety mitigations, many of the largest AI developers will no doubt incorporate watermarks in the synthetic media generated by their models—seven companies, including OpenAI, Google, Microsoft, and Anthropic, have already committed to doing so—but these methods won’t decisively determine the provenance for <a id="idTextAnchor031"/>all content.<a id="marker-228"/></p>
<h3 class="fm-head1" id="heading_id_19">Boundaries for user engagement and metrics</h3>
<p class="body">In a 2018 paper published by researchers at Microsoft entitled, “From Eliza to Xiaolce: Challenges and Opportunities with Social Chatbots,” the authors trace the development of social chatbots through the present day. They write: <a id="idIndexMarker055"/><a id="idIndexMarker056"/><a id="idIndexMarker057"/></p>
<p class="fm-quote">Conversational systems have come a long way since their inception in the 1960s… To further the advancement and adoption of social chatbots, their design must focus on user engagement and take both intellectual quotient (IQ) and emotional quotient (EQ) into account. Users should want to engage with a social chatbot; as such, we define the success metric for soc<a class="calibre" id="idTextAnchor032"/>ial chatbots as conversation-turns per session (CPS). <a class="url1" href="https://arxiv.org/pdf/1801.01957.pdf">[27]</a> <a class="calibre" id="idIndexMarker058"/></p>
<p class="body">Lest we forget, the creator of ELIZA, Joseph Weizenbaum, intended the tool as a therapeutic aid and was dismayed to realize the extent to which people anthropomorphized it. One tends to think that Weizenbaum would not have viewed CPS as the measure of its success. The fact that CPS is defined to be the metric <i class="fm-italics">du jour</i> of social chatbots illustrates a profound failure of the imagination.</p>
<p class="body">Social cha<a id="idTextAnchor033"/>tbots, including Xiaolce, Replika, and Character.AI, have millions of users who seek out conversations with the bots for the companionship, romance, or entertainment that they provide. It’s certainly true that these agents must combine IQ and EQ: if the agent was heavily indexed toward IQ but not EQ, people would be able to ask it factual questions or for coding assistance, for example, but would be unlikely to develop a deeper relationship with it. If the agent didn’t possess enough IQ, it wouldn’t be able to hold an interesting conversation at all. Beyond a base level of functionality, though, it’s primarily EQ that gives social chatbots the capabilities their users value most: the responses that make them feel less lonely, the practice of small talk to alleviate social anxiety, or simply an outlet to vent.</p>
<p class="body"><a id="marker-229"/>It’s in these interactions that social chatbots are most valuable, so it’s these interactions that should be understood and improved. In chapter 7, we recommend alternative metrics that chatbot providers could use to measure success, such as defining valuable sessions instead of simply using session length as an indicator. This requires additional work, but it can circumvent the shortcomings of purely engagement-based metrics and provide insights into how people are using the chatbots, which is crucial for their developers to know to ensure responsible deployment of the technology.</p>
<p class="body">In that vein, chatbot providers should also endeavor to recognize when usage is unhealthy to prevent people from forming dependency relationships with the models. As the stories in chapter 7 show, these tools can improve people’s moods and confidence, and reduce anxiety and loneliness. But there is much we still don’t know about human-AI connections, and if these relationships replace interpersonal connections on a long-term basis, there are reasons to believe it could have substantial negative effects on emotional development. Again, to avoid building dependency in users necessitates optimizing metrics other than engagement, and means more work for developers. Ultimately, we believe this effort is worth it for the social benefit and to sus<a id="idTextAnchor034"/>tain user trust.</p>
<h3 class="fm-head1" id="heading_id_20">Humans in the loop</h3>
<p class="body">Humans remain an integral part of building and maintaining AI systems. Consider how many different people were involved in the creation of ChatGPT. There were, of course, the OpenAI engineers in San Francisco. Likely, there were many more contractors who selected good responses to help train the chatbot; there might have been specialists brought in to red-team certain topics. We know that there were Kenyan data labelers paid $1 to $2 an hour to review hate speech and sexual abuse content. There were the authors of the millions of words that ChatGPT was trained on, from Shakespeare to anonymous Redditors, and the people whose labor allowed ChatGPT to learn to write news articles, emails, speeches, and code. Maybe something you wrote is in there! And the users of ChatGPT, like other LLMs, also play a key role in improving the product over time.<a id="idIndexMarker059"/><a id="idIndexMarker060"/><a id="marker-230"/></p>
<p class="body">To the extent that LLMs have expertise, it’s human expertise. What the technology provides is a way of representing information from more documents than any person could ever read, much less organize in their mind, and using that information to generate text (usually responses to inputs) at a scale no person ever could. What the technology doesn’t provide is meaning; the model doesn’t <i class="fm-italics">know</i>. That is typically acceptable for producing a song about a rabbit who loves carrots, but it isn’t acceptable in high-stakes applications ranging from medical diagnoses to legal argumentation. As we talked about in chapter 6, these types of applications still need a human in the loop to identify the model’s mistakes. LLMs are tools that we can use to do parts of our jobs more quickly and easily, and maybe sometimes even better, but we still need to build expertise to correct and improve these models.</p>
<p class="body">As we navigate the shifting roles of ourselves and AI in education and professional fields, thorny questions will inevitably arise. Our collective ability to answer them will depend on a sociotechnical response, rather than technology alone. In privacy, for example, there is a tremendous amount of technical progress being made, such as new start-ups that use generative models to create synthetic datasets with the same statistical properties<a id="idTextAnchor035"/> as real datasets. Illumina, a genetic sequencing company, announced a partnership with the synth<a id="idTextAnchor036"/>etic data start-up Gretel.ai to create synthetic genome data that could be extremely useful in healthcare, without divulging any individual’s genetic information. But there is momentum behind these efforts because of the social aspect of privacy—activism around the problem, increased public awareness, confronting and rejecting social norms of mass data collection, and finally a stricter regulatory environment. This must continue with responsible AI and related movements.</p>
<p class="body">Making positive change that encourages the responsible use of technology also requires that people are at least generally aware of how these technical systems work and how they are presently used. Digital literacy is a group effort. Companies that provide solutions powered by LLMs must not try to sell users magic, but work to educate them on the capabilities and limitations of the models. Schools should aim to prepare their students for the world of today, rather than ignoring or punishing the use of modern technologies, including LLMs. This book is our hopeful contribution toward a populace that is informed and considerate abo<a id="idTextAnchor037"/>ut generative AI. <a id="idIndexMarker061"/><a id="marker-231"/></p>
<h2 class="fm-head" id="heading_id_21">AI regulations: An ethics perspective</h2>
<p class="body">Although the best practices discussed in section Best Practices for Responsible Use are vital, they aren’t enough. We also need balanced guidance from the government, informed by industry, academia, and civil society, and methods to enforce accountability. Government entities around the world are increasingly recognizing the need for guidelines and frameworks that govern the development, deployment, and use of AI systems. The ultimate goal of regulations is to strike the perfect balance between promoting innovation and ensuring the development of responsible and ethical AI systems. These regulations often aim to address shared concerns about data privacy, algorithmic transparency, bias mitigation, and accountability. In this section, we’ll talk about the AI regulatory landscape in North America, the European Union, and China, as well as discuss corporate self-governance. We focus on these regions due to the concentration of big technology companies in the United States and China and their preeminent roles in global AI development, while the European Union is the world’s lead<a id="idTextAnchor038"/>ing tech regulator.<a id="idIndexMarker062"/><a id="idIndexMarker063"/></p>
<h3 class="fm-head1" id="heading_id_22">North America overview</h3>
<p class="body">In the United States and Canada, the predominant approach at the federal level has been to establish best practices at the agency level and sometimes in collaboration with leading tech companies and civil society groups. The latter approach is exemplified by the July 2023 announcement from the Biden administration that it had secured commitments from seven AI companies—OpenAI, Microsoft, Google, Amazon, Meta, Anthropic, and Inflection—to comply with a set of voluntary principles. The principles, depicted in figure 8.5, include “ensuring products are safe before introducing them to the public” through internal and external testing for safety and information-sharing on risk management; “building systems that put security first” with appropriate cybersecurity and insider threat safeguards and vulnerability reporting; and “earning the public’s trust,” a broad category that references efforts to develop watermarking systems and public reporting on the capabilities and limitations of publicly released AI systems <a class="url" href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/">[28]</a>.<a id="idIndexMarker064"/><a id="idIndexMarker065"/><a id="idIndexMarker066"/><a id="marker-232"/></p>
<p class="body">Because the commitments are voluntary, some critics argued that the announcement produced more of a halo effect for the companies rather than meaningful change. Kevin Roose, a technology reporter at the <i class="fm-italics">New York Times</i>, reviewed each principle in the press release to assess how significant the commitments were. Roose’s primary critique was that the listed principles are vague and don’t specify what kind of testing and reporting must be done, leaving lots of wiggle room. He concluded:</p>
<p class="fm-quote">Overall, the White House’s deal with AI companies is more symbolic than substantive. There is no enforcement mechanism to make sure companies follow these commitments, and many of them reflect precautions that AI companies are already taking. Still, it’s a reasonable first step. And agreeing to follow these rules shows that the AI companies have learned from the failure of earlier tech companies, which waited to engage with government until they g<a class="calibre" id="idTextAnchor039"/>ot into trouble. <a class="url1" href="https://www.nytimes.com/2023/07/22/technology/ai-regulation-white-house.xhtml">[29]</a></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="218" src="../../OEBPS/Images/CH08_F05_Dhamani.png" width="388"/></p>
<p class="figurecaption">Figure 8.5 The three pillars of voluntary commitments made to the White House by leading AI companies</p>
</div>
<p class="body">Indeed, some of the commitments appear to be directly motivated by events that have transpired already. The second pillar, building systems that put security first, specifically calls out the protection of “proprietary and unreleased model weights.” As described in chapter 1, the weights of an LLM are the end product of its training. Access to model weights effectively enables the reproduction of the model itself. After the model weights of Meta’s LLaMA were leaked on 4chan days after the public release, 4chan users were able to quickly produce a high-quality LLM based on LLaMA. The memo doesn’t otherwise mention the open source development of LLMs.</p>
<p class="body"><a id="marker-233"/>These particular principles are mostly focused on generative AI products, but other government bodies have long concerned themselves with the potential negative effects of earlier AI systems, particularly those related to bias and transparency. For example, the Equal Employment Opportunity Commission (EEOC) has issued guidance on how the Civil Rights Act of 1964 applies to automated, AI-based systems used in HR functions such as résumé screening, candidate selection, and performance monitoring. Essentially, the office stated that the burden of compliance would fall on employers who use these tools, with recommendations to verify that vendors had evaluated whether their models cause a “substantially lower selection rate for individuals with a characteristic protected by Title VII,” such as individuals of a particular race or gender <a class="url" href="https://www.eeoc.gov/newsroom/eeoc-releases-new-resource-artificial-intelligence-and-title-vii">[30]</a>. The Federal Trade Commission (FTC) has also demonstrated an appetite for oversight of automated decision-making, writing in a 2021 blog post that the FTC Act, which prohibits “unfair or deceptive practices,” would explicitly include the sale or use of racially biased algorithms. In addition to models used in employment decisions, models related to housing, credit, and insurance decisions would potentially be subject to scrutiny under the Fair Credit Reporting Act (see <a class="url" href="http://mng.bz/JgKQ">http://mng.bz/JgKQ</a>). The Government of Canada issued a Directive on Automated Decision-Making in 2019 that included assessments of negative outcomes from automated decision-making systems (see <a class="url" href="http://mng.bz/mVn0">http://mng.bz/mVn0</a>). Although generative AI models weren’t the target of these issuances, they would be similarly scrutinized if used in any of the aforementioned sectors.<a id="idIndexMarker067"/><a id="idIndexMarker068"/></p>
<p class="body"><a id="marker-234"/>In October 2023, the White House followed the voluntary commitments it had secured with an executive order on AI, designed to require AI companies to share safety evaluations and other information with the government and to take precautions to ensure that the models could not be used for engineering “dangerous biological materials” or enabling “fraud and deception.” (see <a class="url" href="http://mng.bz/6nM5">http://mng.bz/6nM5</a>). The Biden administration has also published more abstract rules for the development of AI. Perhaps its landmark text on the subject is the Blueprint for an AI Bill of Rights, authored by the White House Office of Science and Technology Policy (OSTP) (see <a class="url" href="http://mng.bz/wv8g">http://mng.bz/wv8g</a>). Summarized in figure 8.6, that document is centered around the five principles of “safe and effective systems,” outlining evaluation and risk mitigation standards; “algorithmic discrimination protections,” or identifying potential biases in the model or system; “data privacy,” the rights of users to have both information and agency concerning how their data is collected; “notice and explanation” about the use of automated systems; and “human alternatives, considerations, and fallback” for when people opt out of automated systems or to remedy any mistakes made by the system. Like the more recent set of AI principles, these are each relatively uncontroversial and vague enough to leave some uncertainty over what each might look like in practice. The AI Bill of Rights is a positioning document rather than a directive, and the OSTP is a policy office. The details of implementations of things such as explanations (“Automated systems should provide explanations that are technically valid, meaningful, and useful to you and to any operators or others who need to understand the system, and calibrated to the level of risk based on the context”) remain to be worked out. The closest the US government has come to attempting that is the National Institute for S<a id="idTextAnchor040"/>tandards and Technology’s AI Risk Management Framework (AI RMF), released on January 26, 2023, but even that framework is quite broad and general, intended as a starting point. The AI RMF details that AI systems should be “valid and reliable,” “safe,” “secure and resilient,” “accountable and transparent,” “explainable and interpretable,” and “fair—with harmful bias managed,” but leaves how this should be achieved mostly as an exercise<a id="idTextAnchor041"/> for the reader <a class="url" href="https://www.nist.gov/itl/ai-risk-management-framework">[31]</a>.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="165" src="../../OEBPS/Images/CH08_F06_Dhamani.png" width="360"/></p>
<p class="figurecaption">Figure 8.6 The five principles listed by the OSTP as a “Blueprint for an AI Bill of Rights” (see <a class="url" href="http://mng.bz/wv8g">http://mng.bz/wv8g</a>)</p>
</div>
<p class="body"><a id="marker-235"/>In the past, policymakers have expressed an ambivalence toward regulating AI companies. On one hand, representatives such as Mike Gallagher, a Republican congressman from Wisconsin, have hoped to avoid stifling the innovation that these tech companies bring. “The tension underlying all of this is that we don’t want to overregulate our advantage in the AI race out of existence,” said Gallagher, advocating for a “clinical, targeted” approach, rather than something more comprehensive. “Congress rarely does comprehensive well” <a class="url" href="https://www.rollcall.com/2023/07/19/gallagher-advocates-targeted-approach-to-ai-regulation/">[32]</a>. On the other hand, as evidenced in the Judiciary Committee hearing from chapter 5, more than a few members are concerned that the present state of self-regulation will be insufficient, and some have expressed openness to comprehensive AI legislation. Representative Ro Khanna of California said:</p>
<p class="fm-quote">On a broad scale, we need some form of human judgment in decision-making. We need some sense of transparency when it comes to understanding what AI is being used for and the data sets that are being used. We need to have a safety assessment. . . . But I think the details of this really need to be worked out by people with deep knowledge of the issues. <a class="url1" href="https://www.rollcall.com/2023/07/19/gallagher-advocates-targeted-approach-to-ai-regulation/">[32]</a></p>
<p class="body">There may be bipartisan support for some of the governance measures suggested by the principles in the AI Bill of Rights and a more recent set of commitments, though the prospect of passing federal legislation in the United States is far from certain. LLM developers recognize that their biggest regulatory threat is across the Atlantic Ocean, in <a id="idTextAnchor042"/>the European Parliament.<a id="idIndexMarker069"/><a id="idIndexMarker070"/><a id="idIndexMarker071"/></p>
<h3 class="fm-head1" id="heading_id_23">EU overview</h3>
<p class="body">On June 14, 2023, the European Parliament overwhelmingly approved their version of the EU’s AI Act, setting the stage to pass the final version of the law on an expedited timeline by the end of the year <a class="url" href="https://www.europarl.europa.eu/news/en/press-room/20230609IPR96212/meps-ready-to-negotiate-first-ever-rules-for-safe-and-transparent-ai">[33]</a>. The AI Act would be one of the first major laws to regulate AI and serve as a potential model for policymakers around the world. <a id="idIndexMarker072"/><a id="idIndexMarker073"/><a id="idIndexMarker074"/><a id="marker-236"/></p>
<p class="body">The AI Act implements a risk-based approach to AI regulation, focusing on AI applications that have the greatest potential for harm to society. In other words, the different risk levels will denote how much that technology is regulated and where high-risk AI systems will require more regulation. A limited set of AI systems that are deemed as <i class="fm-italics">unacceptable risk</i> will be completely banned for violating fundamental human rights, which include cognitive behavioral manipulation of people of specific vulnerable groups, social scoring, and real-time and remote biometric identification systems (with major exceptions) <a class="url" href="https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence">[34]</a>. For example, a voice-activated toy that encourages violent behavior in children would fall under this category and be banned. <a id="idIndexMarker075"/></p>
<p class="body">One level below AI systems with unacceptable risk are <i class="fm-italics">high-risk</i> AI systems, which negatively affect safety or fundamental rights (as protected by the EU Charter of Fundamental Rights). These include regulated consumer products and AI used for socioeconomic decisions, such as law enforcement, hiring, educational access, and financial services access, among others. All high-risk AI systems will not only be assessed before they go to market but also throughout their lifecycle. These systems would have to meet data governance, accuracy, and nondiscrimination standards. They would additionally need to implement a risk-management system, record-keeping, technical documentation, and human oversight. The AI systems would also need to be registered in an EU-wide database, which would not only create transparency within the number of high-risk AI systems but also regarding the extent of their societal effect <a class="url" href="https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/">[35]</a>. <a id="idIndexMarker076"/></p>
<p class="body">Then, <i class="fm-italics">limited risk</i> systems would have to comply with transparency requirements to help users make informed decisions. These requirements include making the user aware if they are interacting with AI, such as deepfakes, emotion recognition systems, or chatbots. The AI Act has an additional callout for <i class="fm-italics">generative AI</i>, requiring transparency in disclosing AI-generated content, preventing the model from generating illegal content, and publishing summaries of copyrighted data used for training. <a id="idIndexMarker077"/><a id="idIndexMarker078"/><a id="marker-237"/></p>
<p class="body">Finally, <i class="fm-italics">minimal risk</i> includes AI applications such as video games or spam filters. These are proposed to be mainly regulated by voluntary codes of conduct. Figure 8.7 illustrates the AI Act’s risk levels. However, at the time of this writing, European policymakers haven’t decided where foundational LLMs fall within this framework, and this subj<a id="idTextAnchor043"/>ect is currently being debated.<a id="idIndexMarker079"/></p>
<p class="body">Of course, the AI Act isn’t the only major legislation in the EU to manage AI risk. In chapter 3, we briefly discussed the General Data Protection Regulation (GDPR), which requires companies to protect the personal data and privacy of EU citizens. The AI Act isn’t meant to replace GDPR, but complement it. In addition to data privacy implications, GDPR also contains two articles that affect machine learning systems. First, “GDPR states that algorithmic systems should not be allowed to make significant decisions that affect legal rights without any human supervision” <a class="url" href="https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/">[35]</a>. An example of this was seen in 2021 when Uber, an American transportation company, was required to reinstate six drivers in the Netherlands who “were unfairly terminated by algorithmic means” <a class="url" href="https://techcrunch.com/2021/04/14/uber-hit-with-default-robo-firing-ruling-after-another-eu-labor-rights-gdpr-challenge/">[36]</a>. Second, “GDPR guarantees an individual’s right to <i class="fm-italics">meaningful information about the logic</i> of algorithmic systems, at times controversially deemed a <i class="fm-italics">right to explanation</i>” <a class="url" href="https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/">[35]</a>. Put simply, EU consumers have the right to ask companies that make automated decisions based on their personal data, such as home insurance providers, how or why certain decisions were made.<a id="marker-238"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="663" src="../../OEBPS/Images/CH08_F07_Dhamani.png" width="713"/></p>
<p class="figurecaption">Figure 8.7 The four risk categories in the AI Act</p>
</div>
<p class="body">As part of its efforts to regulate digital technologies, regulations that the EU has already passed include the Digital Services Act (DSA) and the Digital Market Act (DMA). Passed in November 2022, the DSA applies to online platforms and search engines, requiring companies to assess risks, outline mitigation efforts, and undergo third-party audits for compliance <a class="url" href="https://digital-strategy.ec.europa.eu/en/policies/digital-services-act-package">[37]</a>. The most stringent reg<a id="idTextAnchor044"/>ulations under the DSA only apply to very large online platforms (VLOPs), which focuses most of the regulation on platforms that have the most reach and influence on EU citizens. One of DSA’s goals is to force large platforms to be more transparent, particularly with algorithmic accountability and content moderation. These transparency requirements will help identify any systematic risks that come from the design and provision of services. For example, if an AI content recommendation system contributes to the spread of disinformation, the company may face fines under the DSA. The EU’s approach to targeting VLOPs is interesting because of its potential to undermine the innovation argument against regulation—which is, how will companies continue to innovate when faced with strict regulations? By targeting VLOPs, smaller businesses are free from the burden of complying with some parts of the DSA so they can still innovate, but if and when they become a large force in society, they will also be required to think about how they are using their data and how their platform is affecting their users.<a id="idIndexMarker080"/><a id="idIndexMarker081"/><a id="idIndexMarker082"/></p>
<p class="body"><a id="marker-239"/>Similarly, the DMA is aimed at increasing competition in digital marketplaces. The DMA targets “gatekeepers,” which are corporate groups that significantly affect the internal market, namely, big technology companies. Here, the gatekeepers will be subject to an additional level of regulation over other companies. For example, they will be restricted from sharing data across their services without user consent, barred from self-preferencing their own products and services, and obliged to share additional information with advertisers on how their ads perform <a class="url" href="https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/">[35]</a>. The DMA will likely affect how the big technology players handle data, as well as how AI systems handle search engine ranking and ordering of products on e-commerce platforms. Despite not primarily focusing on AI, the DSA and DMA laws also help govern AI models and demand increased transparency from technology companies.</p>
<p class="body">We’ve highlighted the European Union’s efforts to develop a coherent approach to AI governance and standards. In particular, the AI Act has the potential to become the de facto global standard for regulating AI. There are clear strengths to the EU’s approach to AI regulation, particularly the risk-based methodology, but there are a few challenges as well. Notably, it will foster an ecosystem of independent audits, which will likely result in more transparent, fair, and risk-managed AI applications. There are, however, open questions as to the extent the legislation can adapt to new capabilities and risks as they arise, as well as manage the longer-term societal effects of AI.</p>
<p class="body">Ultimately, the EU’s goal is to provide a regulatory framework for AI companies and organizations that use AI, as well as facilitate a balance between innovation and the protection of citizens’ rights. However, their success will depend on a well-conceived enforcement structure and their abil<a id="idTextAnchor045"/>ity to create an AI auditing ecosystem.<a id="idIndexMarker083"/><a id="idIndexMarker084"/><a id="idIndexMarker085"/></p>
<h3 class="fm-head1" id="heading_id_24">China overview</h3>
<p class="body"><a id="marker-240"/>As of 2023, China has introduced three comprehensive and targeted machine learning and AI regulations: its 2021 regulation on recommendation algorithms, the 2022 rules for deep synthesis (deepfakes), and the 2023 draft rules on generative AI. These legislations create new rules for how AI systems are built and deployed, as well as what information AI developers must disclose to the government and the general public. <a id="idIndexMarker086"/><a id="idIndexMarker087"/><a id="idIndexMarker088"/></p>
<p class="body">Unlike the European Union, China has taken an iterative and vertical regulatory approach toward AI governance. For example, the AI Act is a horizontal regulation, aiming to cover all applications of a given technology. On the other hand, vertical regulations target a specific application of a given technology. Here, Chinese regulators impose requirements specific to their concerns, and if deemed inadequate or flawed, they release new regulations to fill in the gaps or expand on existing legislation. While China’s iterative process toward regulation can be confusing or challenging for AI developers to maintain compliance, Chinese regulators view that as a necessary trade-off in a fast-moving technology environment.</p>
<p class="body">In 2021, China’s regulation on algorithmic recommendation systems marked the start of restrictions on AI and machine learning systems. Initially motivated by the Chinese Communist Party’s (CCP) concern about the role of algorithms disseminating information online, the set of regulations reins in the use and misuse of recommendation algorithms <a class="url" href="https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117">[38]</a>. The regulations demand transparency over how algorithms function and provide users more control over which data the companies can use to feed the algorithms, as well as mandate that the recommendation service providers “uphold mainstream value orientations” and “actively transmit positive energy online” <a class="url" href="https://www.chinalawtranslate.com/algorithms/">[39]</a>. The regulation also requires platforms to prevent the spread of undesirable or illegal information and manually intervene to ensure they reflect government policies.<a id="idIndexMarker089"/></p>
<p class="body"><a id="marker-241"/>The CCP also identified deepfakes as a threat to the information environment. We should note that, unlike the United States, which has very strong free expression guarantees in its constitution, or even the European Union, the information environment in China is more controlled and restricted by comparison. The spectrum of what classifies as an information “threat” in China is quite broad. For example, criticism of the state or CCP would be considered a threat to the information environment.</p>
<p class="body">In 2022, China introduced the Deep Synthesis Provisions, which include algorithms that synthetically produce images, text, video, or voice content. The regulation calls for adding labels or tags on synthetically generated content, and includes vague censorship requirements, such that it must “adhere to correct political direction” and “not disturb economic and social order” <a class="url" href="https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117">[38]</a>. It further requires deep synthesis service providers to take measures for personal data protection, technical security, and transparency. The regulation was finalized on November 25, 2022, just five days before the public release of ChatGPT <a class="url" href="https://www.china-briefing.com/news/china-to-regulate-deep-synthesis-deep-fake-technology-starting-january-2023/">[40]</a>.</p>
<p class="body">Despite China being ahead of the curve with generative AI technology, they were faced with the unfortunate timing of ChatGPT’s release. The Deep Synthesis Provisions were deemed insufficient by the Cyberspace Administration of China (CAC) given that they were designed to regulate deepfakes and not text generated from LLMs. The regulation also covered only content-generation services provided through the internet, which created a regulatory gap in content that was being generated using AI offline. So, the Chinese regulators set out to quickly iterate on the same set of AI applications but with new concerns in mind. <a id="idIndexMarker090"/></p>
<p class="body">In April 2023, the CAC released draft measures on Generative AI Services. The draft builds on the Deep Synthesis Provisions, which took effect in January 2023, and applies to all machine-generated content online and offline <a class="url" href="https://www.chinalawtranslate.com/overview-of-draft-measures-on-generative-ai/">[41]</a>. The initial draft had several difficult-to-meet requirements, including that training data must be truthful, accurate, and diverse, as well as not violate any intellectual property rights <a class="url" href="https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117">[38]</a>. A key question was whether the rules may end up suppressing innovation in the AI industry of a country that aims to become the world leader in this space. After an active public debate, the interim measures, set to take effect on August 15, 2023, relaxed a few previously announced provisions and said that Chinese regulators would support the development of the technology <a class="url" href="https://www.reuters.com/technology/china-issues-temporary-rules-generative-ai-services-2023-07-13/">[42]</a>. The interim rules only apply to services that are available to the general public in China, which exempts any technology being developed in research institutions or intended for use by overseas users.<a id="marker-242"/></p>
<p class="body">China’s vertical and iterative approach to AI regulation reveals both strengths and vulnerabilities. The strength of the vertical approach is the ability to create precise solutions or mitigations for specific problems. However, regulators are forced to develop new regulations for new applications or problems, as seen with the Deep Synthesis Provisions. Because of prior experience with AI governance and utilization of regulatory frameworks from past vertical regulations, CAC was able to quickly iterate on the Deep Synthesis Provisions to draft rules for generative AI, showcasing speed as another area of strength.</p>
<p class="body">In June 2023, China’s State Council (the equivalent of the US cabinet) announced that they would draft an Artificial Intelligence Law, a comprehensive, horizontal piece of legislation building upon existing regulations. This suggests that Chinese AI regulation is approaching a turning point, echoing the evolution of Chinese regulations governing the internet. Initially, the internet in China was governed by narrow and specific regulations, which later matured into the Cybersecurity Law of 2017, a broad and comprehensive framework that was built upon existing laws <a class="url" href="https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117">[38]</a>. Following a similar playbook as their approach to internet regulation, if the draft of the Artificial Intelligence Law is adopted, it<a id="idTextAnchor046"/> will be China’s first national AI legislation.<a id="idIndexMarker091"/><a id="idIndexMarker092"/><a id="idIndexMarker093"/></p>
<h3 class="fm-head1" id="heading_id_25">Corporate self-governance</h3>
<p class="body">As discussed in section North America Overview, the Biden-Harris administration secured voluntary commitments in July 2023 from seven leading AI companies—Amazon, Anthropic, Google, Inflection, Meta, Microsoft, and OpenAI—to ensure the safe, secure, and transparent development of AI technology. However, this agreement has been both praised and criticized—is this a step forward or an empty promise? <a id="idIndexMarker094"/><a id="idIndexMarker095"/><a id="idIndexMarker096"/><a id="marker-243"/></p>
<p class="body">On the surface, the voluntary commitment looks promising, but the phrasing of the terms is fairly vague and largely seems to reinforce what the seven companies are already doing: working on the safety of AI systems, investing in cybersecurity, and aiming for transparency. The agreement is also voluntary, which doesn’t assign responsibility to ensure that the companies abide by the terms nor does it hold them accountable for noncompliance. However, it’s worth noting that companies would likely feel pressured to participate, especially given the alternative threat of rigid regulation.</p>
<p class="body">On the plus side, however, a voluntary commitment helps the administration avoid strict, difficult to comply with regulations that may hinder innovation in the United States, as it has in the European Union <a class="url" href="https://techliberation.com/2022/08/01/why-the-future-of-ai-will-not-be-invented-in-europe/">[43]</a>. The financial sector’s regulatory oversight actually began in industry self-governance. In the 17th century, collectives of traders used to meet at rival coffeehouses competing with each other on the effectiveness of the ethics rules their members had to comply with <a class="url" href="https://dlc.dlib.indiana.edu/dlc/handle/10535/10528">[44]</a>. These rules persuaded the public to trade with them instead of their rivals. When any member broke these ethical rules, the entire collective’s reputation was damaged. Consequently, all the members were incentivized to monitor unethical behavior, so if any member behaved undesirably, they could be ousted. Eventually, all the collectives adopted the rules that best protected the public as the standard. These collectives—the original stock traders in London’s Lombard Street—are an excellent example of industry self-governance in a sector that is now heavily regulated. Once these collectives were able to establish the best standards, the monitoring and enforcement of the rules were transferred to a third party, such as the government, where the collective members and the third party worked together to amend and establish new standards <a class="url" href="https://www.aei.org/technology-and-innovation/white-house-ai-commitments-a-first-step-to-industry-self-governance/">[45]</a>.</p>
<p class="body"><a id="marker-244"/>Similarly, the Biden-Harris administration’s voluntary commitments give the AI companies the freedom to establish their own rules to enforce where perhaps the rules that best protect the public will surface, as they did in the financial sector. As we’ve said, voluntary commitments merely formalize the commitment for AI companies to have best practices. For example, OpenAI doesn’t allow usage of their models for illegal activity, or any activity that has a high risk of physical or economic harm, among other disallowed uses (see <a class="url" href="http://mng.bz/5w9q">http://mng.bz/5w9q</a>). Google has also released additional terms for generative AI with a similar policy for blocking any content that violates their prohibited use policy, which includes (but is not limited to) any content used to perform or facilitate dangerous, illegal, or malicious activities (see <a class="url" href="http://mng.bz/6DW5">http://mng.bz/6DW5</a>). Meanwhile, Inflection AI states that “safety is at the heart of our mission” and “our internal Safety team continuously pressure tests our models for risks, and works with outside experts on comprehensive red-teaming of our technologies” (see <a class="url" href="http://mng.bz/o1Xj">http://mng.bz/o1Xj</a>). Even Meta’s Llama 2, which has been open sourced for research and commercial use cases, has an acceptable use policy that prohibits certain use cases to help ensure that the models are used responsibly (see <a class="url" href="https://ai.meta.com/llama/use-policy/">https://ai.meta.com/llama/use-policy/</a>).</p>
<p class="body">In the United States, it’s also more than likely that market forces will shape the governing landscape. The companies will actively work to make sure that their LLMs aren’t seen as inadequate—maybe this motivation stems from reports of adversaries exploiting the LLM, the general public deeming their data practices untrustworthy, or simply trying to avoid embarrassing (and expensive) events such as Google’s public release of Bard. Of course, it can certainly be problematic for for-profit companies to develop their own governance frameworks when they perhaps may be more incentivized by growing a successful business than protecting their users, but it’s worth noting that the administration does emphasize involving a diverse range of stakeholders (which we’ll further unpack in section Towards an AI Governance Framework). At the very least, the voluntary commitments reinforce the notion that companies have a responsibility in their commitment to responsible AI development, including their potential for affecting society. Encouraging corporate self-governance could complement existing or future regulatory efforts, as well as fill in a critical gap to develop a more comprehensive approach to the governance of AI systems, or any new t<a id="idTextAnchor047"/>echnology for that matter, especially in its infancy.<a id="idIndexMarker097"/><a id="idIndexMarker098"/></p>
<h2 class="fm-head" id="heading_id_26">Toward an AI governance framework</h2>
<p class="body">In <i class="fm-italics">Introduction to Generative AI</i>, we’ve outlined the AI race, illustrating the potential of generative AI technology as well as building awareness around its shortcomings. Enthusiasts anticipate that generative AI will disrupt the way we engage in work and our personal lives, do business, and create wealth. On the other hand, an increasing number of technology experts have shared significant concerns regarding the existential dangers of relinquishing tasks and decision-making to computers with little use for humans in the near future. Contributing to these unsettling concerns is an existing imbalance of power and wealth where critics of AI are worried that gains from the technology will disproportionately accumulate among the top 1%. As mentioned in chapter 6, we believe that generative AI is an evolution, not a revolution, as long as we use and govern the technology responsibly. <a id="idIndexMarker099"/><a id="idIndexMarker100"/><a id="marker-245"/></p>
<p class="body">Throughout the book, we’ve highlighted the pragmatic promises of generative AI, from productivity gains to agentized systems. But at the same time, we’ve emphasized the risks and limitations of generative AI technology, as well as its ability to be misused accidentally and intentionally. As the awareness of AI risks has grown, so have the standards and guidance to mitigate them. We’ve come a long way, but we have an even longer way to go. We hope and believe that we’ll find a balance between groups calling for a pause in training AI systems and those claiming that ChatGPT is magic. Regardless of how the global AI disruption unfolds, the world won’t become a better place for living, working, or participating in democratic processes unless there are measures in place to regulate and govern AI’s development, effect, and safeguards.</p>
<p class="body">As we discussed in section Ethics-Informed AI Regulations, AI governance efforts have primarily been undertaken voluntarily, encompassing numerous protocols and principles that endorse conscientious design and controlled behavior. This is especially true in North America, where the shared goals of big technology companies involve aligning AI with human usefulness and ensuring safety throughout the creation and implementation of algorithms. Additional goals for AI systems also involve algorithmic transparency, fairness in their utilization, privacy and data protection, human supervision and oversight, and adherence to regulatory standards. While we acknowledge that these are ambitious goals, it’s necessary to highlight that AI developers often fall short of these objectives. Companies often have proprietary intellectual property for building their AI systems that they don’t disclose in order to keep their competitive advantage. For many in the AI ethics community, this is an indication that companies are more motivated by financial incentives than public benefits.</p>
<p class="body">Since the early 2020s, the focus on voluntary self-policing by AI companies has started to shift toward comprehensive regulations in various countries. In a <i class="fm-italics">Wired</i> article, Rumman Chowdhury wrote, “In order to truly create public benefit, we need mechanisms of accountability” <a class="url" href="https://www.wired.com/story/ai-desperately-needs-global-oversight/">[46]</a>. However, it’s important to note that the majority of discussions concerning AI and potential approaches to mitigate unintended negative consequences have been primarily focused in the West—the European Union, the United States, or members of advanced economies. Of course, the Western focus makes sense given the concentration of big AI companies in Silicon Valley, including OpenAI, Google, Meta, and Anthropic. But it’s worth emphasizing the following:<a id="marker-246"/></p>
<p class="fm-quote">The vast majority of discussion about the consequences and regulation of AI is occurring among countries whose populations make up just 1.3 billion people. Far less attention and resources are dedicated to addressing these same concerns in poor and emerging countries that account for the remaining 6.7 billion of the global population. <a class="url1" href="https://foreignpolicy.com/2023/05/29/ai-regulation-global-south-artificial-intelligence/">[47]</a></p>
<p class="body">So, where do we go from here? How do we truly ensure that generative AI, or AI systems in general, are used to better society? In the previously mentioned article, Chowdhury says:</p>
<p class="fm-quote">The world needs a generative AI global governance body to solve these social, economic, and political disruptions beyond what any individual government is capable of, what any academic or civil society group can implement, or any corporation is willing or able to do. <a class="url1" href="https://www.wired.com/story/ai-desperately-needs-global-oversight/">[46]</a></p>
<p class="body">The risks exposed by generative AI have emphasized what many experts have been calling for: the need for a new, permanent, independent, well-funded, and resourced institution to holistically ensure public benefit. Chowdhury further states:</p>
<p class="fm-quote">It should cover all aspects of generative AI models, including their development, deployment, and use as it relates to the public good. It should build upon tangible recommendations from civil society and academic organizations, and have the authority to enforce its decisions, including the power to require changes in the design or use of generative AI models, or even halt their use altogether if necessary. Finally, this group should address reparations for the sweeping changes that may come, job loss, a rise in misinformation, and the potential for inhibiting free and fair elections potentially among them. This is not a group for research alone; this is a group for action. <a class="url1" href="https://www.wired.com/story/ai-desperately-needs-global-oversight/">[46]</a></p>
<p class="body">We should note that we already have an example of a global, independent, and well-funded organization that makes decisions for the betterment of society. The International Atomic Energy Agency (IAEA) (see <a class="url" href="https://www.iaea.org/">www.iaea.org/</a>) was formed in the post–World War II era to govern nuclear technologies. IAEA, formed under the guidance of the United Nations, is a body independent of governments and corporations that provides advisory support and resources. While it has limited agency, IAEA shows us that we’ve done this before and that we can do it again. <a id="idIndexMarker101"/><a id="idIndexMarker102"/><a id="marker-247"/></p>
<p class="body">Fundamentally, recent advances in generative AI have highlighted what many of us have known for a long time. We’ll never be able to “solve” the problem of abusing or misusing technology. Therefore, instead of only pursuing band-aid technical solutions, we need to invest in sociotechnical approaches to address the root of the problem. To Chowdhury’s point, the IAEA is a starting point for a global governance body, not an end goal. Unlike the IAEA’s limited agency, this body should have the ability to make independent and enforceable decisions. It should take advisory guidance from AI companies but also collaborate with civil society, government, and academia. This body shouldn’t replace any of these entities, but it should form a coalition to ensure public benefit in the face of AI. While we acknowledge that the effort needed to get to a global governance body for AI is substantial, we’re optimistic about the future of AI, and hopeful that AI companies and governments will work toward an independent, global body to make d<a id="idTextAnchor048"/>ecisions regarding the governance and effect of AI systems.<a id="idIndexMarker103"/><a id="idIndexMarker104"/><a id="marker-248"/></p>
<h2 class="fm-head" id="heading_id_27">Summary</h2>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">LLMs are trained on unfathomable amounts of internet data. They inevitably encode bias, harmful stereotypes, and toxicity, as well as copyrighted data and sensitive information in their training process.</p>
</li>
<li class="fm-list-bullet">
<p class="list">LLMs often exhibit biased, toxic, and misaligned responses because of the characteristics of the training data. They also regurgitate sensitive or copyrighted information. LLMs also hallucinate, that is, they confidently make up incorrect information because of how they work.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Adversaries can exploit the vulnerabilities in LLMs to perform training data extraction attacks, prompt injections or jailbreaking, or poison data.</p>
</li>
<li class="fm-list-bullet">
<p class="list">LLMs can help malicious actors carry out personalized and low-cost adversarial attacks at scale, as well as lower the barrier of entry for novice threat actors.</p>
</li>
<li class="fm-list-bullet">
<p class="list">LLMs can be accidentally misused in professional sectors by people who don’t grasp the limitations of these models, which can result in serious ethical and societal consequences.</p>
</li>
<li class="fm-list-bullet">
<p class="list">If not implemented responsibly, AI systems could be used to replace humans with machines, drive down wages, worsen the inequality between wealth and income, and do little to help overall economic growth.</p>
</li>
<li class="fm-list-bullet">
<p class="list">When misused, social chatbots can lead to unhealthy relationship patterns, dependency-seeking behaviors, and risk replacing genuine human connection.<a id="marker-249"/></p>
</li>
<li class="fm-list-bullet">
<p class="list">LLM developers should document training data, be transparent with users about data privacy and use, and make efforts to mitigate biases present in their models.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Vector databases and web retrieval provide some additional capabilities to LLMs and can be used to help interpret some of the model’s responses.</p>
</li>
<li class="fm-list-bullet">
<p class="list">LLMs should be trained and evaluated thoroughly for safety and robustness to adversarial attacks before public release.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Identifying post hoc if the content was created by a human or a machine will soon be a fool’s errand, but there are promising solutions focused on tracking the provenance of media.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Developers of social chatbots can optimize for metrics other than engagement to reduce the potential risk for social harms such as dependency or problems in emotional development.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Because LLMs don’t have true knowledge or expertise, they should be typically deployed within a human-in-the-loop context, and stakeholders must be literate on how these models work before they are blindly used.</p>
</li>
<li class="fm-list-bullet">
<p class="list">In the near future, we can expect to see generative AI integrated into more applications and becoming increasingly agentic, efficient, and personalized.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The United States hasn’t attempted a large-scale AI regulation like the EU but has instead relied more heavily on corporate self-regulation and voluntary commitments.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The EU’s AI Act takes a risk-based approach to AI regulation and is one of the first major laws to regulate AI.</p>
</li>
<li class="fm-list-bullet">
<p class="list">In 2023, China released draft measures on Generative AI Services and announced that they would draft an Artificial Intelligence Law, a comprehensive, horizontal piece of legislation building upon existing regulations.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Corporate self-governance could complement existing or future regulatory efforts, as well as fill in a critical gap to develop a more comprehensive approach to AI systems governance.</p>
</li>
<li class="fm-list-bullet">
<p class="list">AI companies often fall short of algorithmic transparency, ensuring the safety of AI systems, and data protection standards, among others.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The risks exposed by generative AI have emphasized what many experts have been calling for: the need for a new, permanent, independent, well-funded, and resourced institution to holistically ensure public benefit.<a id="marker-250"/></p>
</li>
</ul>
</div></body></html>