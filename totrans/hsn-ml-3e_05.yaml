- en: Chapter 4\. Training Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章。训练模型
- en: 'So far we have treated machine learning models and their training algorithms
    mostly like black boxes. If you went through some of the exercises in the previous
    chapters, you may have been surprised by how much you can get done without knowing
    anything about what’s under the hood: you optimized a regression system, you improved
    a digit image classifier, and you even built a spam classifier from scratch, all
    without knowing how they actually work. Indeed, in many situations you don’t really
    need to know the implementation details.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们大多将机器学习模型及其训练算法视为黑匣子。如果您在之前章节的一些练习中有所了解，您可能会对不知道底层原理的情况下能做多少事情感到惊讶：您优化了一个回归系统，改进了一个数字图像分类器，甚至从头开始构建了一个垃圾邮件分类器，所有这些都是在不知道它们实际如何工作的情况下完成的。实际上，在许多情况下，您并不真正需要知道实现细节。
- en: However, having a good understanding of how things work can help you quickly
    home in on the appropriate model, the right training algorithm to use, and a good
    set of hyperparameters for your task. Understanding what’s under the hood will
    also help you debug issues and perform error analysis more efficiently. Lastly,
    most of the topics discussed in this chapter will be essential in understanding,
    building, and training neural networks (discussed in [Part II](part02.html#neural_nets_part)
    of this book).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对事物如何运作有一个良好的理解可以帮助您快速找到适当的模型、正确的训练算法以及适合您任务的一组良好的超参数。了解底层原理还将帮助您更有效地调试问题并执行错误分析。最后，本章讨论的大多数主题将对理解、构建和训练神经网络（本书的[第二部分](part02.html#neural_nets_part)中讨论）至关重要。
- en: 'In this chapter we will start by looking at the linear regression model, one
    of the simplest models there is. We will discuss two very different ways to train
    it:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先看一下线性回归模型，这是最简单的模型之一。我们将讨论两种非常不同的训练方法：
- en: Using a “closed-form” equation⁠^([1](ch04.html#idm45720217568672)) that directly
    computes the model parameters that best fit the model to the training set (i.e.,
    the model parameters that minimize the cost function over the training set).
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个“封闭形式”方程⁠^([1](ch04.html#idm45720217568672))直接计算最适合训练集的模型参数（即最小化训练集上成本函数的模型参数）。
- en: 'Using an iterative optimization approach called gradient descent (GD) that
    gradually tweaks the model parameters to minimize the cost function over the training
    set, eventually converging to the same set of parameters as the first method.
    We will look at a few variants of gradient descent that we will use again and
    again when we study neural networks in [Part II](part02.html#neural_nets_part):
    batch GD, mini-batch GD, and stochastic GD.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一种称为梯度下降（GD）的迭代优化方法，逐渐调整模型参数以最小化训练集上的成本函数，最终收敛到与第一种方法相同的参数集。我们将看一下几种梯度下降的变体，当我们研究神经网络时会一再使用：批量GD、小批量GD和随机GD。
- en: Next we will look at polynomial regression, a more complex model that can fit
    nonlinear datasets. Since this model has more parameters than linear regression,
    it is more prone to overfitting the training data. We will explore how to detect
    whether or not this is the case using learning curves, and then we will look at
    several regularization techniques that can reduce the risk of overfitting the
    training set.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将看一下多项式回归，这是一个可以拟合非线性数据集的更复杂模型。由于这个模型比线性回归有更多的参数，所以更容易过拟合训练数据。我们将探讨如何通过学习曲线检测是否存在这种情况，然后我们将看一下几种正则化技术，可以减少过拟合训练集的风险。
- en: 'Finally, we will examine two more models that are commonly used for classification
    tasks: logistic regression and softmax regression.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将研究另外两种常用于分类任务的模型：逻辑回归和softmax回归。
- en: Warning
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: There will be quite a few math equations in this chapter, using basic notions
    of linear algebra and calculus. To understand these equations, you will need to
    know what vectors and matrices are; how to transpose them, multiply them, and
    inverse them; and what partial derivatives are. If you are unfamiliar with these
    concepts, please go through the linear algebra and calculus introductory tutorials
    available as Jupyter notebooks in the [online supplemental material](https://github.com/ageron/handson-ml3).
    For those who are truly allergic to mathematics, you should still go through this
    chapter and simply skip the equations; hopefully, the text will be sufficient
    to help you understand most of the concepts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将包含相当多的数学方程，使用线性代数和微积分的基本概念。要理解这些方程，您需要知道向量和矩阵是什么；如何转置、相乘和求逆；以及什么是偏导数。如果您对这些概念不熟悉，请查看[在线补充材料](https://github.com/ageron/handson-ml3)中作为Jupyter笔记本提供的线性代数和微积分入门教程。对于那些真正对数学过敏的人，您仍然应该阅读本章，并简单跳过方程；希望文本足以帮助您理解大部分概念。
- en: Linear Regression
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: 'In [Chapter 1](ch01.html#landscape_chapter) we looked at a simple regression
    model of life satisfaction:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第一章](ch01.html#landscape_chapter)中，我们看了一个关于生活满意度的简单回归模型：
- en: '*life_satisfaction* = *θ*[0] + *θ*[1] × *GDP_per_capita*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*life_satisfaction* = *θ*[0] + *θ*[1] × *GDP_per_capita*'
- en: This model is just a linear function of the input feature `GDP_per_capita`.
    *θ*[0] and *θ*[1] are the model’s parameters.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型只是输入特征`GDP_per_capita`的线性函数。*θ*[0]和*θ*[1]是模型的参数。
- en: More generally, a linear model makes a prediction by simply computing a weighted
    sum of the input features, plus a constant called the *bias term* (also called
    the *intercept term*), as shown in [Equation 4-1](#equation_four_one).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，线性模型通过简单地计算输入特征的加权和加上一个称为*偏置项*（也称为*截距项*）的常数来进行预测，如[方程4-1](#equation_four_one)所示。
- en: Equation 4-1\. Linear regression model prediction
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-1。线性回归模型预测
- en: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>θ</mi>
    <mi>n</mi></msub> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math>
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>θ</mi>
    <mi>n</mi></msub> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math>
- en: 'In this equation:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*ŷ* is the predicted value.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ŷ*是预测值。'
- en: '*n* is the number of features.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n*是特征数量。'
- en: '*x*[*i*] is the *i*^(th) feature value.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[*i*]是第*i*个特征值。'
- en: '*θ*[*j*] is the *j*^(th) model parameter, including the bias term *θ*[0] and
    the feature weights *θ*[1], *θ*[2], ⋯, *θ*[*n*].'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*θ*[*j*]是第*j*个模型参数，包括偏置项*θ*[0]和特征权重*θ*[1]、*θ*[2]、⋯、*θ*[*n*]。'
- en: This can be written much more concisely using a vectorized form, as shown in
    [Equation 4-2](#linear_regression_prediction_vectorized_equation).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以用矢量化形式更简洁地表示，如[方程4-2](#linear_regression_prediction_vectorized_equation)所示。
- en: Equation 4-2\. Linear regression model prediction (vectorized form)
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-2\. 线性回归模型预测（矢量化形式）
- en: <math><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msub><mi>h</mi><mi
    mathvariant="bold">θ</mi></msub><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo><mo>=</mo><mi
    mathvariant="bold">θ</mi><mo>·</mo><mi mathvariant="bold">x</mi></math>
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msub><mi>h</mi><mi
    mathvariant="bold">θ</mi></msub><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo><mo>=</mo><mi
    mathvariant="bold">θ</mi><mo>·</mo><mi mathvariant="bold">x</mi></math>
- en: 'In this equation:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*h*[**θ**] is the hypothesis function, using the model parameters **θ**.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*h*[**θ**]是假设函数，使用模型参数**θ**。'
- en: '**θ** is the model’s *parameter vector*, containing the bias term *θ*[0] and
    the feature weights *θ*[1] to *θ*[*n*].'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**θ**是模型的*参数向量*，包括偏置项*θ*[0]和特征权重*θ*[1]到*θ*[*n*]。'
- en: '**x** is the instance’s *feature vector*, containing *x*[0] to *x*[*n*], with
    *x*[0] always equal to 1.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**x**是实例的*特征向量*，包含*x*[0]到*x*[*n*]，其中*x*[0]始终等于1。'
- en: '**θ** · **x** is the dot product of the vectors **θ** and **x**, which is equal
    to *θ*[0]*x*[0] + *θ*[1]*x*[1] + *θ*[2]*x*[2] + ... + *θ*[*n*]*x*[*n*].'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**θ** · **x**是向量**θ**和**x**的点积，等于*θ*[0]*x*[0] + *θ*[1]*x*[1] + *θ*[2]*x*[2]
    + ... + *θ*[*n*]*x*[*n*]。'
- en: Note
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In machine learning, vectors are often represented as *column vectors*, which
    are 2D arrays with a single column. If **θ** and **x** are column vectors, then
    the prediction is <math><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup><mi mathvariant="bold">x</mi></math>,
    where <math><msup><mi mathvariant="bold">θ</mi><mo>⊺</mo></msup></math> is the
    *transpose* of **θ** (a row vector instead of a column vector) and <math><msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup><mi mathvariant="bold">x</mi></math>
    is the matrix multiplication of <math><msup><mi mathvariant="bold">θ</mi><mo>⊺</mo></msup></math>
    and **x**. It is of course the same prediction, except that it is now represented
    as a single-cell matrix rather than a scalar value. In this book I will use this
    notation to avoid switching between dot products and matrix multiplications.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，向量通常表示为*列向量*，这是具有单列的二维数组。如果**θ**和**x**是列向量，那么预测值为<math><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup><mi mathvariant="bold">x</mi></math>，其中<math><msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup></math>是**θ**的*转置*（行向量而不是列向量），<math><msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup><mi mathvariant="bold">x</mi></math>是<math><msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup></math>和**x**的矩阵乘法。当然，这是相同的预测，只是现在表示为单元格矩阵而不是标量值。在本书中，我将使用这种表示法，以避免在点积和矩阵乘法之间切换。
- en: OK, that’s the linear regression model—but how do we train it? Well, recall
    that training a model means setting its parameters so that the model best fits
    the training set. For this purpose, we first need a measure of how well (or poorly)
    the model fits the training data. In [Chapter 2](ch02.html#project_chapter) we
    saw that the most common performance measure of a regression model is the root
    mean square error ([Equation 2-1](ch02.html#rmse_equation)). Therefore, to train
    a linear regression model, we need to find the value of **θ** that minimizes the
    RMSE. In practice, it is simpler to minimize the mean squared error (MSE) than
    the RMSE, and it leads to the same result (because the value that minimizes a
    positive function also minimizes its square root).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这就是线性回归模型，但我们如何训练它呢？嗯，回想一下，训练模型意味着设置其参数，使模型最好地适应训练集。为此，我们首先需要一个衡量模型与训练数据拟合程度的指标。在[第2章](ch02.html#project_chapter)中，我们看到回归模型最常见的性能指标是均方根误差（[方程2-1](ch02.html#rmse_equation)）。因此，要训练线性回归模型，我们需要找到最小化RMSE的**θ**的值。在实践中，最小化均方误差（MSE）比最小化RMSE更简单，并且会导致相同的结果（因为最小化正函数的值也会最小化其平方根）。
- en: Warning
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Learning algorithms will often optimize a different loss function during training
    than the performance measure used to evaluate the final model. This is generally
    because the function is easier to optimize and/or because it has extra terms needed
    during training only (e.g., for regularization). A good performance metric is
    as close as possible to the final business objective. A good training loss is
    easy to optimize and strongly correlated with the metric. For example, classifiers
    are often trained using a cost function such as the log loss (as you will see
    later in this chapter) but evaluated using precision/recall. The log loss is easy
    to minimize, and doing so will usually improve precision/recall.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，学习算法通常会优化不同的损失函数，而不是用于评估最终模型的性能指标。这通常是因为该函数更容易优化和/或因为在训练期间仅需要额外的项（例如，用于正则化）。一个好的性能指标应尽可能接近最终的业务目标。一个好的训练损失易于优化，并且与指标强相关。例如，分类器通常使用成本函数进行训练，如对数损失（稍后在本章中将看到），但使用精度/召回率进行评估。对数损失易于最小化，这样做通常会提高精度/召回率。
- en: The MSE of a linear regression hypothesis *h*[**θ**] on a training set **X**
    is calculated using [Equation 4-3](#mse_cost_function).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归假设*h*[**θ**]在训练集**X**上的MSE是使用[方程4-3](#mse_cost_function)计算的。
- en: Equation 4-3\. MSE cost function for a linear regression model
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-3. 线性回归模型的MSE成本函数
- en: <math display="block"><mrow><mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi>
    <mo>,</mo> <msub><mi>h</mi> <mi mathvariant="bold">θ</mi></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msup><mi>y</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi>
    <mo>,</mo> <msub><mi>h</mi> <mi mathvariant="bold">θ</mi></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msup><mi>y</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: Most of these notations were presented in [Chapter 2](ch02.html#project_chapter)
    (see [“Notations”](ch02.html#notations)). The only difference is that we write
    *h*[**θ**] instead of just *h* to make it clear that the model is parametrized
    by the vector **θ**. To simplify notations, we will just write MSE(**θ**) instead
    of MSE(**X**, *h*[**θ**]).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些符号在[第2章](ch02.html#project_chapter)中已经介绍过（参见[“符号”](ch02.html#notations)）。唯一的区别是我们写*h*[**θ**]而不是只写*h*，以明确模型是由向量**θ**参数化的。为了简化符号，我们将只写MSE(**θ**)而不是MSE(**X**,
    *h*[**θ**])。
- en: The Normal Equation
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正规方程
- en: To find the value of **θ** that minimizes the MSE, there exists a *closed-form
    solution*—in other words, a mathematical equation that gives the result directly.
    This is called the *Normal equation* ([Equation 4-4](#equation_four_four)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到最小化MSE的**θ**的值，存在一个*闭式解*——换句话说，一个直接给出结果的数学方程。这被称为*正规方程*（[方程4-4](#equation_four_four)）。
- en: Equation 4-4\. Normal equation
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-4. 正规方程
- en: <math display="block"><mrow><mover accent="true"><mi mathvariant="bold">θ</mi>
    <mo>^</mo></mover> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">X</mi>
    <mo>⊺</mo></msup> <mi mathvariant="bold">X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>  <msup><mi
    mathvariant="bold">X</mi> <mo>⊺</mo></msup>  <mi mathvariant="bold">y</mi></mrow></math>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mover accent="true"><mi mathvariant="bold">θ</mi>
    <mo>^</mo></mover> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">X</mi>
    <mo>⊺</mo></msup> <mi mathvariant="bold">X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>  <msup><mi
    mathvariant="bold">X</mi> <mo>⊺</mo></msup>  <mi mathvariant="bold">y</mi></mrow></math>
- en: 'In this equation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: <math><mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover></math>
    is the value of **θ** that minimizes the cost function.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math><mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover></math>是最小化成本函数的**θ**的值。
- en: '**y** is the vector of target values containing *y*^((1)) to *y*^((*m*)).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**y**是包含*y*^((1))到*y*^((*m*))的目标值向量。'
- en: 'Let’s generate some linear-looking data to test this equation on ([Figure 4-1](#generated_data_plot)):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一些看起来线性的数据来测试这个方程（[图4-1](#generated_data_plot)）：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![mls3 0401](assets/mls3_0401.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0401](assets/mls3_0401.png)'
- en: Figure 4-1\. A randomly generated linear dataset
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1. 随机生成的线性数据集
- en: 'Now let’s compute <math><mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover></math>
    using the Normal equation. We will use the `inv()` function from NumPy’s linear
    algebra module (`np.linalg`) to compute the inverse of a matrix, and the `dot()`
    method for matrix multiplication:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用正规方程计算<math><mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover></math>。我们将使用NumPy的线性代数模块（`np.linalg`）中的`inv()`函数计算矩阵的逆，以及矩阵乘法的`dot()`方法：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The `@` operator performs matrix multiplication. If `A` and `B` are NumPy arrays,
    then `A @ B` is equivalent to `np.matmul(A, B)`. Many other libraries, like TensorFlow,
    PyTorch, and JAX, support the `@` operator as well. However, you cannot use `@`
    on pure Python arrays (i.e., lists of lists).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`@`运算符执行矩阵乘法。如果`A`和`B`是NumPy数组，则`A @ B`等同于`np.matmul(A, B)`。许多其他库，如TensorFlow、PyTorch和JAX，也支持`@`运算符。但是，不能在纯Python数组（即列表的列表）上使用`@`。'
- en: 'The function that we used to generate the data is *y* = 4 + 3*x*[1] + Gaussian
    noise. Let’s see what the equation found:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来生成数据的函数是*y* = 4 + 3*x*[1] + 高斯噪声。让我们看看方程找到了什么：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We would have hoped for *θ*[0] = 4 and *θ*[1] = 3 instead of *θ*[0] = 4.215
    and *θ*[1] = 2.770\. Close enough, but the noise made it impossible to recover
    the exact parameters of the original function. The smaller and noisier the dataset,
    the harder it gets.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望*θ*[0] = 4和*θ*[1] = 3，而不是*θ*[0] = 4.215和*θ*[1] = 2.770。足够接近，但噪声使得无法恢复原始函数的确切参数。数据集越小且噪声越大，问题就越困难。
- en: 'Now we can make predictions using <math><mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover></math>:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用<math><mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover></math>进行预测：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s plot this model’s predictions ([Figure 4-2](#linear_model_predictions_plot)):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这个模型的预测（[图4-2](#linear_model_predictions_plot)）：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![mls3 0402](assets/mls3_0402.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0402](assets/mls3_0402.png)'
- en: Figure 4-2\. Linear regression model predictions
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2. 线性回归模型预测
- en: 'Performing linear regression using Scikit-Learn is relatively straightforward:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn执行线性回归相对简单：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Notice that Scikit-Learn separates the bias term (`intercept_`) from the feature
    weights (`coef_`). The `LinearRegression` class is based on the `scipy.linalg.lstsq()`
    function (the name stands for “least squares”), which you could call directly:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Scikit-Learn将偏置项（`intercept_`）与特征权重（`coef_`）分开。`LinearRegression`类基于`scipy.linalg.lstsq()`函数（名称代表“最小二乘法”），您可以直接调用该函数：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This function computes <math><mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover><mo>=</mo><msup><mi
    mathvariant="bold">X</mi><mo>+</mo></msup><mi mathvariant="bold">y</mi></math>,
    where <math><msup><mi mathvariant="bold">X</mi><mo>+</mo></msup></math> is the
    *pseudoinverse* of **X** (specifically, the Moore–Penrose inverse). You can use
    `np.linalg.pinv()` to compute the pseudoinverse directly:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数计算<math><mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover><mo>=</mo><msup><mi
    mathvariant="bold">X</mi><mo>+</mo></msup><mi mathvariant="bold">y</mi></math>，其中<math><msup><mi
    mathvariant="bold">X</mi><mo>+</mo></msup></math>是**X**的*伪逆*（具体来说，是Moore-Penrose逆）。您可以使用`np.linalg.pinv()`直接计算伪逆：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The pseudoinverse itself is computed using a standard matrix factorization
    technique called *singular value decomposition* (SVD) that can decompose the training
    set matrix **X** into the matrix multiplication of three matrices **U** **Σ**
    **V**^⊺ (see `numpy.linalg.svd()`). The pseudoinverse is computed as <math><msup><mi
    mathvariant="bold">X</mi><mo>+</mo></msup><mo>=</mo><mi mathvariant="bold">V</mi><msup><mi
    mathvariant="bold">Σ</mi><mo>+</mo></msup><msup><mi mathvariant="bold">U</mi><mo>⊺</mo></msup></math>.
    To compute the matrix <math><msup><mi mathvariant="bold">Σ</mi><mo>+</mo></msup></math>,
    the algorithm takes **Σ** and sets to zero all values smaller than a tiny threshold
    value, then it replaces all the nonzero values with their inverse, and finally
    it transposes the resulting matrix. This approach is more efficient than computing
    the Normal equation, plus it handles edge cases nicely: indeed, the Normal equation
    may not work if the matrix **X**^⊺**X** is not invertible (i.e., singular), such
    as if *m* < *n* or if some features are redundant, but the pseudoinverse is always
    defined.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 伪逆本身是使用称为*奇异值分解*（SVD）的标准矩阵分解技术计算的，可以将训练集矩阵**X**分解为三个矩阵**U** **Σ** **V**^⊺的矩阵乘法（参见`numpy.linalg.svd()`）。伪逆计算为<math><msup><mi
    mathvariant="bold">X</mi><mo>+</mo></msup><mo>=</mo><mi mathvariant="bold">V</mi><msup><mi
    mathvariant="bold">Σ</mi><mo>+</mo></msup><msup><mi mathvariant="bold">U</mi><mo>⊺</mo></msup></math>。为了计算矩阵<math><msup><mi
    mathvariant="bold">Σ</mi><mo>+</mo></msup></math>，算法取**Σ**并将小于一个微小阈值的所有值设为零，然后用它们的倒数替换所有非零值，最后转置结果矩阵。这种方法比计算正规方程更有效，而且可以很好地处理边缘情况：实际上，如果矩阵**X**^⊺**X**不可逆（即奇异），例如如果*m*<*n*或者某些特征是冗余的，那么正规方程可能无法工作，但伪逆总是被定义的。
- en: Computational Complexity
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算复杂度
- en: The Normal equation computes the inverse of **X**^⊺ **X**, which is an (*n*
    + 1) × (*n* + 1) matrix (where *n* is the number of features). The *computational
    complexity* of inverting such a matrix is typically about *O*(*n*^(2.4)) to *O*(*n*³),
    depending on the implementation. In other words, if you double the number of features,
    you multiply the computation time by roughly 2^(2.4) = 5.3 to 2³ = 8.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 正规方程计算**X**^⊺**X**的逆，这是一个（*n*+1）×（*n*+1）矩阵（其中*n*是特征数）。求解这样一个矩阵的*计算复杂度*通常约为*O*(*n*^(2.4))到*O*(*n*³)，取决于实现。换句话说，如果特征数翻倍，计算时间大约会乘以2^(2.4)=5.3到2³=8。
- en: The SVD approach used by Scikit-Learn’s `LinearRegression` class is about *O*(*n*²).
    If you double the number of features, you multiply the computation time by roughly
    4.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn的`LinearRegression`类使用的SVD方法大约是*O*(*n*²)。如果特征数量翻倍，计算时间大约会乘以4。
- en: Warning
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Both the Normal equation and the SVD approach get very slow when the number
    of features grows large (e.g., 100,000). On the positive side, both are linear
    with regard to the number of instances in the training set (they are *O*(*m*)),
    so they handle large training sets efficiently, provided they can fit in memory.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当特征数量增多时（例如100,000），正规方程和SVD方法都变得非常慢。积极的一面是，它们都与训练集中实例数量线性相关（它们是*O*(*m*)），因此它们可以有效地处理大型训练集，只要它们可以放入内存。
- en: 'Also, once you have trained your linear regression model (using the Normal
    equation or any other algorithm), predictions are very fast: the computational
    complexity is linear with regard to both the number of instances you want to make
    predictions on and the number of features. In other words, making predictions
    on twice as many instances (or twice as many features) will take roughly twice
    as much time.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一旦训练好线性回归模型（使用正规方程或任何其他算法），预测速度非常快：计算复杂度与您要进行预测的实例数量和特征数量成正比。换句话说，对两倍实例（或两倍特征）进行预测将花费大约两倍的时间。
- en: Now we will look at a very different way to train a linear regression model,
    which is better suited for cases where there are a large number of features or
    too many training instances to fit in memory.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看一种非常不同的训练线性回归模型的方法，这种方法更适用于特征数量较多或训练实例太多无法放入内存的情况。
- en: Gradient Descent
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: '*Gradient descent* is a generic optimization algorithm capable of finding optimal
    solutions to a wide range of problems. The general idea of gradient descent is
    to tweak parameters iteratively in order to minimize a cost function.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*梯度下降*是一种通用的优化算法，能够找到各种问题的最优解。梯度下降的一般思想是迭代地调整参数，以最小化成本函数。'
- en: 'Suppose you are lost in the mountains in a dense fog, and you can only feel
    the slope of the ground below your feet. A good strategy to get to the bottom
    of the valley quickly is to go downhill in the direction of the steepest slope.
    This is exactly what gradient descent does: it measures the local gradient of
    the error function with regard to the parameter vector **θ**, and it goes in the
    direction of descending gradient. Once the gradient is zero, you have reached
    a minimum!'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在浓雾中的山中迷失了方向，只能感受到脚下的坡度。快速到达山谷底部的一个好策略是沿着最陡的坡度方向下坡。这正是梯度下降所做的：它测量了关于参数向量**θ**的误差函数的局部梯度，并沿着下降梯度的方向前进。一旦梯度为零，你就到达了一个最小值！
- en: In practice, you start by filling **θ** with random values (this is called *random
    initialization*). Then you improve it gradually, taking one baby step at a time,
    each step attempting to decrease the cost function (e.g., the MSE), until the
    algorithm *converges* to a minimum (see [Figure 4-3](#gradient_descent_diagram)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，您首先用随机值填充**θ**（这称为*随机初始化*）。然后逐渐改进它，每次尝试减少成本函数（例如MSE）一点点，直到算法*收敛*到最小值（参见[图4-3](#gradient_descent_diagram)）。
- en: '![mls3 0403](assets/mls3_0403.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0403](assets/mls3_0403.png)'
- en: Figure 4-3\. In this depiction of gradient descent, the model parameters are
    initialized randomly and get tweaked repeatedly to minimize the cost function;
    the learning step size is proportional to the slope of the cost function, so the
    steps gradually get smaller as the cost approaches the minimum
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3。在这个梯度下降的描述中，模型参数被随机初始化，并不断调整以最小化成本函数；学习步长大小与成本函数的斜率成比例，因此随着成本接近最小值，步长逐渐变小
- en: An important parameter in gradient descent is the size of the steps, determined
    by the *learning rate* hyperparameter. If the learning rate is too small, then
    the algorithm will have to go through many iterations to converge, which will
    take a long time (see [Figure 4-4](#small_learning_rate_diagram)).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降中的一个重要参数是步长的大小，由*学习率*超参数确定。如果学习率太小，那么算法将需要经过许多迭代才能收敛，这将花费很长时间（参见[图4-4](#small_learning_rate_diagram)）。
- en: '![mls3 0404](assets/mls3_0404.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0404](assets/mls3_0404.png)'
- en: Figure 4-4\. Learning rate too small
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4。学习率太小
- en: On the other hand, if the learning rate is too high, you might jump across the
    valley and end up on the other side, possibly even higher up than you were before.
    This might make the algorithm diverge, with larger and larger values, failing
    to find a good solution (see [Figure 4-5](#large_learning_rate_diagram)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果学习率太高，您可能会跳过山谷，最终停在另一侧，甚至可能比之前更高。这可能导致算法发散，产生越来越大的值，无法找到一个好的解决方案（参见[图4-5](#large_learning_rate_diagram)）。
- en: '![mls3 0405](assets/mls3_0405.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0405](assets/mls3_0405.png)'
- en: Figure 4-5\. Learning rate too high
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5。学习率太高
- en: Additionally, not all cost functions look like nice, regular bowls. There may
    be holes, ridges, plateaus, and all sorts of irregular terrain, making convergence
    to the minimum difficult. [Figure 4-6](#gradient_descent_pitfalls_diagram) shows
    the two main challenges with gradient descent. If the random initialization starts
    the algorithm on the left, then it will converge to a *local minimum*, which is
    not as good as the *global minimum*. If it starts on the right, then it will take
    a very long time to cross the plateau. And if you stop too early, you will never
    reach the global minimum.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，并非所有成本函数都像漂亮的、规则的碗一样。可能会有洞、脊、高原和各种不规则的地形，使得收敛到最小值变得困难。[图4-6](#gradient_descent_pitfalls_diagram)展示了梯度下降的两个主要挑战。如果随机初始化将算法开始于左侧，则它将收敛到*局部最小值*，这不如*全局最小值*好。如果它从右侧开始，则穿过高原将需要很长时间。如果您停得太早，您将永远无法达到全局最小值。
- en: '![mls3 0406](assets/mls3_0406.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0406](assets/mls3_0406.png)'
- en: Figure 4-6\. Gradient descent pitfalls
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-6。梯度下降的陷阱
- en: 'Fortunately, the MSE cost function for a linear regression model happens to
    be a *convex function*, which means that if you pick any two points on the curve,
    the line segment joining them is never below the curve. This implies that there
    are no local minima, just one global minimum. It is also a continuous function
    with a slope that never changes abruptly.⁠^([2](ch04.html#idm45720216856720))
    These two facts have a great consequence: gradient descent is guaranteed to approach
    arbitrarily closely the global minimum (if you wait long enough and if the learning
    rate is not too high).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，线性回归模型的MSE成本函数恰好是一个*凸函数*，这意味着如果您选择曲线上的任意两点，连接它们的线段永远不会低于曲线。这意味着没有局部最小值，只有一个全局最小值。它还是一个连续函数，斜率永远不会突然改变。这两个事实有一个重要的结果：梯度下降保证可以无限接近全局最小值（如果等待足够长的时间且学习率不太高）。
- en: While the cost function has the shape of a bowl, it can be an elongated bowl
    if the features have very different scales. [Figure 4-7](#elongated_bowl_diagram)
    shows gradient descent on a training set where features 1 and 2 have the same
    scale (on the left), and on a training set where feature 1 has much smaller values
    than feature 2 (on the right).⁠^([3](ch04.html#idm45720216853456))
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然成本函数的形状像一个碗，但如果特征具有非常不同的比例，它可能是一个延长的碗。[图4-7](#elongated_bowl_diagram)展示了在特征1和2具有相同比例的训练集上的梯度下降（左侧），以及在特征1的值远小于特征2的训练集上的梯度下降（右侧）。
- en: '![mls3 0407](assets/mls3_0407.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0407](assets/mls3_0407.png)'
- en: Figure 4-7\. Gradient descent with (left) and without (right) feature scaling
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-7。特征缩放的梯度下降（左）和不缩放的梯度下降（右）
- en: As you can see, on the left the gradient descent algorithm goes straight toward
    the minimum, thereby reaching it quickly, whereas on the right it first goes in
    a direction almost orthogonal to the direction of the global minimum, and it ends
    with a long march down an almost flat valley. It will eventually reach the minimum,
    but it will take a long time.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，左侧的梯度下降算法直接朝向最小值，因此快速到达，而右侧首先朝向几乎与全局最小值方向正交的方向，最终沿着几乎平坦的山谷长途跋涉。它最终会到达最小值，但需要很长时间。
- en: Warning
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When using gradient descent, you should ensure that all features have a similar
    scale (e.g., using Scikit-Learn’s `StandardScaler` class), or else it will take
    much longer to converge.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用梯度下降时，您应确保所有特征具有相似的比例（例如，使用Scikit-Learn的`StandardScaler`类），否则收敛所需的时间将更长。
- en: 'This diagram also illustrates the fact that training a model means searching
    for a combination of model parameters that minimizes a cost function (over the
    training set). It is a search in the model’s *parameter space*. The more parameters
    a model has, the more dimensions this space has, and the harder the search is:
    searching for a needle in a 300-dimensional haystack is much trickier than in
    3 dimensions. Fortunately, since the cost function is convex in the case of linear
    regression, the needle is simply at the bottom of the bowl.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表还说明了训练模型意味着寻找一组模型参数的组合，使得成本函数（在训练集上）最小化。这是在模型的*参数空间*中进行的搜索。模型的参数越多，空间的维度就越多，搜索就越困难：在一个300维的草堆中搜索一根针比在3维空间中要困难得多。幸运的是，由于线性回归的情况下成本函数是凸的，所以这根针就在碗底。
- en: Batch Gradient Descent
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量梯度下降
- en: To implement gradient descent, you need to compute the gradient of the cost
    function with regard to each model parameter *θ*[*j*]. In other words, you need
    to calculate how much the cost function will change if you change *θ*[*j*] just
    a little bit. This is called a *partial derivative*. It is like asking, “What
    is the slope of the mountain under my feet if I face east”? and then asking the
    same question facing north (and so on for all other dimensions, if you can imagine
    a universe with more than three dimensions). [Equation 4-5](#mse_partial_derivatives)
    computes the partial derivative of the MSE with regard to parameter *θ*[*j*],
    noted ∂ MSE(**θ**) / ∂θ[*j*].
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现梯度下降，您需要计算成本函数相对于每个模型参数*θ*[*j*]的梯度。换句话说，您需要计算如果您稍微改变*θ*[*j*]，成本函数将如何变化。这被称为*偏导数*。这就像问，“如果我面向东，脚下的山坡有多陡？”然后面向北问同样的问题（如果您可以想象一个超过三维的宇宙，那么其他维度也是如此）。[方程4-5](#mse_partial_derivatives)计算了关于参数*θ*[*j*]的MSE的偏导数，表示为∂
    MSE(**θ**) / ∂θ[*j*]。
- en: Equation 4-5\. Partial derivatives of the cost function
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-5\. 成本函数的偏导数
- en: <math display="block"><mrow><mstyle scriptlevel="0" displaystyle="true"><mfrac><mi>∂</mi>
    <mrow><mi>∂</mi><msub><mi>θ</mi> <mi>j</mi></msub></mrow></mfrac></mstyle> <mtext>MSE</mtext>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mstyle
    scriptlevel="0" displaystyle="true"><mfrac><mn>2</mn> <mi>m</mi></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <mrow><mo>(</mo> <msup><mi mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo>
    <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow>
    <msubsup><mi>x</mi> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow></math>
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mstyle scriptlevel="0" displaystyle="true"><mfrac><mi>∂</mi>
    <mrow><mi>∂</mi><msub><mi>θ</mi> <mi>j</mi></msub></mrow></mfrac></mstyle> <mtext>MSE</mtext>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mstyle
    scriptlevel="0" displaystyle="true"><mfrac><mn>2</mn> <mi>m</mi></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <mrow><mo>(</mo> <msup><mi mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo>
    <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow>
    <msubsup><mi>x</mi> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow></math>
- en: Instead of computing these partial derivatives individually, you can use [Equation
    4-6](#mse_gradient_vector) to compute them all in one go. The gradient vector,
    noted ∇[**θ**]MSE(**θ**), contains all the partial derivatives of the cost function
    (one for each model parameter).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与单独计算这些偏导数不同，您可以使用[方程4-6](#mse_gradient_vector)一次性计算它们。梯度向量，表示为∇[**θ**]MSE(**θ**)，包含成本函数的所有偏导数（每个模型参数一个）。
- en: Equation 4-6\. Gradient vector of the cost function
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-6\. 成本函数的梯度向量
- en: <math display="block"><mrow><msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub>
    <mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mfrac><mi>∂</mi>
    <mrow><mi>∂</mi><msub><mi>θ</mi> <mn>0</mn></msub></mrow></mfrac> <mtext>MSE</mtext>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd><mrow><mfrac><mi>∂</mi> <mrow><mi>∂</mi><msub><mi>θ</mi> <mn>1</mn></msub></mrow></mfrac>
    <mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mrow><mfrac><mi>∂</mi> <mrow><mi>∂</mi><msub><mi>θ</mi>
    <mi>n</mi></msub></mrow></mfrac> <mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr></mtable></mfenced> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>2</mn> <mi>m</mi></mfrac></mstyle> <msup><mi mathvariant="bold">X</mi>
    <mo>⊺</mo></msup> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi> <mi mathvariant="bold">θ</mi>
    <mo>-</mo> <mi mathvariant="bold">y</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub>
    <mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mfrac><mi>∂</mi>
    <mrow><mi>∂</mi><msub><mi>θ</mi> <mn>0</mn></msub></mrow></mfrac> <mtext>MSE</mtext>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd><mrow><mfrac><mi>∂</mi> <mrow><mi>∂</mi><msub><mi>θ</mi> <mn>1</mn></msub></mrow></mfrac>
    <mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mrow><mfrac><mi>∂</mi> <mrow><mi>∂</mi><msub><mi>θ</mi>
    <mi>n</mi></msub></mrow></mfrac> <mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr></mtable></mfenced> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>2</mn> <mi>m</mi></mfrac></mstyle> <msup><mi mathvariant="bold">X</mi>
    <mo>⊺</mo></msup> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi> <mi mathvariant="bold">θ</mi>
    <mo>-</mo> <mi mathvariant="bold">y</mi> <mo>)</mo></mrow></mrow></math>
- en: Warning
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Notice that this formula involves calculations over the full training set **X**,
    at each gradient descent step! This is why the algorithm is called *batch gradient
    descent*: it uses the whole batch of training data at every step (actually, *full
    gradient descent* would probably be a better name). As a result, it is terribly
    slow on very large training sets (we will look at some much faster gradient descent
    algorithms shortly). However, gradient descent scales well with the number of
    features; training a linear regression model when there are hundreds of thousands
    of features is much faster using gradient descent than using the Normal equation
    or SVD decomposition.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个公式涉及对整个训练集**X**进行计算，每次梯度下降步骤都要进行！这就是为什么该算法被称为*批量梯度下降*：它在每一步使用整个批量的训练数据（实际上，*全梯度下降*可能是一个更好的名称）。因此，在非常大的训练集上，它非常慢（我们很快将看到一些更快的梯度下降算法）。然而，梯度下降随着特征数量的增加而扩展得很好；当特征数量达到数十万时，使用梯度下降训练线性回归模型比使用正规方程或SVD分解要快得多。
- en: Once you have the gradient vector, which points uphill, just go in the opposite
    direction to go downhill. This means subtracting ∇[**θ**]MSE(**θ**) from **θ**.
    This is where the learning rate *η* comes into play:⁠^([4](ch04.html#idm45720216763152))
    multiply the gradient vector by *η* to determine the size of the downhill step
    ([Equation 4-7](#gradient_descent_step)).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦有了指向上坡的梯度向量，只需朝相反方向前进以下坡。这意味着从**θ**中减去∇[**θ**]MSE(**θ**)。这就是学习率*η*发挥作用的地方：⁠^([4](ch04.html#idm45720216763152))将梯度向量乘以*η*来确定下坡步长的大小（[方程4-7](#gradient_descent_step)）。
- en: Equation 4-7\. Gradient descent step
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-7\. 梯度下降步骤
- en: <math><msup><mi mathvariant="bold">θ</mi><mrow><mo>(</mo><mtext>next step</mtext><mo>)</mo></mrow></msup><mo>=</mo><mi
    mathvariant="bold">θ</mi><mo>-</mo><mi>η</mi><msub><mo>∇</mo><mi mathvariant="bold">θ</mi></msub><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></math>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: <math><msup><mi mathvariant="bold">θ</mi><mrow><mo>(</mo><mtext>下一步</mtext><mo>)</mo></mrow></msup><mo>=</mo><mi
    mathvariant="bold">θ</mi><mo>-</mo><mi>η</mi><msub><mo>∇</mo><mi mathvariant="bold">θ</mi></msub><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></math>
- en: 'Let’s look at a quick implementation of this algorithm:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速实现这个算法：
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'That wasn’t too hard! Each iteration over the training set is called an *epoch*.
    Let’s look at the resulting `theta`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不难！每次对训练集的迭代称为*epoch*。让我们看看得到的`theta`：
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Hey, that’s exactly what the Normal equation found! Gradient descent worked
    perfectly. But what if you had used a different learning rate (`eta`)? [Figure 4-8](#gradient_descent_plot)
    shows the first 20 steps of gradient descent using three different learning rates.
    The line at the bottom of each plot represents the random starting point, then
    each epoch is represented by a darker and darker line.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 嘿，这正是正规方程找到的！梯度下降完美地工作了。但是如果您使用了不同的学习率（`eta`）会怎样呢？[图4-8](#gradient_descent_plot)显示了使用三种不同学习率的梯度下降的前20步。每个图中底部的线代表随机起始点，然后每个迭代由越来越深的线表示。
- en: '![mls3 0408](assets/mls3_0408.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0408](assets/mls3_0408.png)'
- en: Figure 4-8\. Gradient descent with various learning rates
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-8\. 不同学习率的梯度下降
- en: 'On the left, the learning rate is too low: the algorithm will eventually reach
    the solution, but it will take a long time. In the middle, the learning rate looks
    pretty good: in just a few epochs, it has already converged to the solution. On
    the right, the learning rate is too high: the algorithm diverges, jumping all
    over the place and actually getting further and further away from the solution
    at every step.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，学习率太低：算法最终会达到解，但需要很长时间。在中间，学习率看起来相当不错：在几个迭代中，它已经收敛到解。在右侧，学习率太高：算法发散，跳来跳去，实际上每一步都离解越来越远。
- en: To find a good learning rate, you can use grid search (see [Chapter 2](ch02.html#project_chapter)).
    However, you may want to limit the number of epochs so that grid search can eliminate
    models that take too long to converge.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到一个好的学习率，可以使用网格搜索（参见[第2章](ch02.html#project_chapter)）。然而，您可能希望限制迭代次数，以便网格搜索可以消除收敛时间过长的模型。
- en: You may wonder how to set the number of epochs. If it is too low, you will still
    be far away from the optimal solution when the algorithm stops; but if it is too
    high, you will waste time while the model parameters do not change anymore. A
    simple solution is to set a very large number of epochs but to interrupt the algorithm
    when the gradient vector becomes tiny—that is, when its norm becomes smaller than
    a tiny number *ϵ* (called the *tolerance*)—because this happens when gradient
    descent has (almost) reached the minimum.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道如何设置迭代次数。如果太低，当算法停止时，您仍然离最优解很远；但如果太高，您将浪费时间，因为模型参数不再改变。一个简单的解决方案是设置一个非常大的迭代次数，但在梯度向量变得微小时中断算法——也就是说，当其范数小于一个微小数*ϵ*（称为*容差*）时——因为这表示梯度下降已经（几乎）达到了最小值。
- en: Stochastic Gradient Descent
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: The main problem with batch gradient descent is the fact that it uses the whole
    training set to compute the gradients at every step, which makes it very slow
    when the training set is large. At the opposite extreme, *stochastic gradient
    descent* picks a random instance in the training set at every step and computes
    the gradients based only on that single instance. Obviously, working on a single
    instance at a time makes the algorithm much faster because it has very little
    data to manipulate at every iteration. It also makes it possible to train on huge
    training sets, since only one instance needs to be in memory at each iteration
    (stochastic GD can be implemented as an out-of-core algorithm; see [Chapter 1](ch01.html#landscape_chapter)).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降的主要问题在于，它在每一步使用整个训练集来计算梯度，这使得在训练集很大时非常缓慢。相反，*随机梯度下降* 在每一步选择训练集中的一个随机实例，并仅基于该单个实例计算梯度。显然，一次只处理一个实例使得算法更快，因为每次迭代时需要操作的数据量很少。这也使得在庞大的训练集上进行训练成为可能，因为每次迭代只需要一个实例在内存中（随机梯度下降可以作为一种离线算法实现；参见[第1章](ch01.html#landscape_chapter)）。
- en: 'On the other hand, due to its stochastic (i.e., random) nature, this algorithm
    is much less regular than batch gradient descent: instead of gently decreasing
    until it reaches the minimum, the cost function will bounce up and down, decreasing
    only on average. Over time it will end up very close to the minimum, but once
    it gets there it will continue to bounce around, never settling down (see [Figure 4-9](#sgd_random_walk_diagram)).
    Once the algorithm stops, the final parameter values will be good, but not optimal.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，由于其随机（即随机）性质，这种算法比批量梯度下降不规则得多：成本函数不会温和地减少直到达到最小值，而是会上下波动，仅平均减少。随着时间的推移，它最终会非常接近最小值，但一旦到达那里，它将继续上下波动，永远不会稳定下来（参见[图4-9](#sgd_random_walk_diagram)）。一旦算法停止，最终的参数值将是不错的，但不是最优的。
- en: '![mls3 0409](assets/mls3_0409.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0409](assets/mls3_0409.png)'
- en: Figure 4-9\. With stochastic gradient descent, each training step is much faster
    but also much more stochastic than when using batch gradient descent
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-9。使用随机梯度下降，每个训练步骤比使用批量梯度下降快得多，但也更不规则。
- en: When the cost function is very irregular (as in [Figure 4-6](#gradient_descent_pitfalls_diagram)),
    this can actually help the algorithm jump out of local minima, so stochastic gradient
    descent has a better chance of finding the global minimum than batch gradient
    descent does.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当成本函数非常不规则时（如[图4-6](#gradient_descent_pitfalls_diagram)中所示），这实际上可以帮助算法跳出局部最小值，因此随机梯度下降比批量梯度下降更有可能找到全局最小值。
- en: Therefore, randomness is good to escape from local optima, but bad because it
    means that the algorithm can never settle at the minimum. One solution to this
    dilemma is to gradually reduce the learning rate. The steps start out large (which
    helps make quick progress and escape local minima), then get smaller and smaller,
    allowing the algorithm to settle at the global minimum. This process is akin to
    *simulated annealing*, an algorithm inspired by the process in metallurgy of annealing,
    where molten metal is slowly cooled down. The function that determines the learning
    rate at each iteration is called the *learning schedule*. If the learning rate
    is reduced too quickly, you may get stuck in a local minimum, or even end up frozen
    halfway to the minimum. If the learning rate is reduced too slowly, you may jump
    around the minimum for a long time and end up with a suboptimal solution if you
    halt training too early.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随机性有助于摆脱局部最优解，但也不好，因为这意味着算法永远无法稳定在最小值处。解决这一困境的一个方法是逐渐降低学习率。步骤开始很大（有助于快速取得进展并摆脱局部最小值），然后变得越来越小，允许算法在全局最小值处稳定下来。这个过程类似于*模拟退火*，这是一种受金属冶炼过程启发的算法，其中熔化的金属被慢慢冷却。确定每次迭代学习率的函数称为*学习计划*。如果学习率降低得太快，您可能会陷入局部最小值，甚至最终冻结在最小值的一半。如果学习率降低得太慢，您可能会在最小值周围跳来跳去很长时间，并且如果您在训练过早停止，最终会得到一个次优解。
- en: 'This code implements stochastic gradient descent using a simple learning schedule:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码使用简单的学习计划实现随机梯度下降：
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'By convention we iterate by rounds of *m* iterations; each round is called
    an *epoch*, as earlier. While the batch gradient descent code iterated 1,000 times
    through the whole training set, this code goes through the training set only 50
    times and reaches a pretty good solution:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 按照惯例，我们按照*m*次迭代的轮次进行迭代；每一轮称为*epoch*，如前所述。虽然批量梯度下降代码通过整个训练集迭代了1,000次，但这段代码只通过训练集迭代了50次，并达到了一个相当不错的解决方案：
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[Figure 4-10](#sgd_plot) shows the first 20 steps of training (notice how irregular
    the steps are).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-10](#sgd_plot)显示了训练的前20步（请注意步骤的不规则性）。'
- en: Note that since instances are picked randomly, some instances may be picked
    several times per epoch, while others may not be picked at all. If you want to
    be sure that the algorithm goes through every instance at each epoch, another
    approach is to shuffle the training set (making sure to shuffle the input features
    and the labels jointly), then go through it instance by instance, then shuffle
    it again, and so on. However, this approach is more complex, and it generally
    does not improve the result.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于实例是随机选择的，一些实例可能在每个epoch中被多次选择，而其他实例可能根本不被选择。如果您想确保算法在每个epoch中通过每个实例，另一种方法是对训练集进行洗牌（确保同时洗牌输入特征和标签），然后逐个实例地进行，然后再次洗牌，依此类推。然而，这种方法更复杂，通常不会改善结果。
- en: '![mls3 0410](assets/mls3_0410.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0410](assets/mls3_0410.png)'
- en: Figure 4-10\. The first 20 steps of stochastic gradient descent
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-10。随机梯度下降的前20步
- en: Warning
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When using stochastic gradient descent, the training instances must be independent
    and identically distributed (IID) to ensure that the parameters get pulled toward
    the global optimum, on average. A simple way to ensure this is to shuffle the
    instances during training (e.g., pick each instance randomly, or shuffle the training
    set at the beginning of each epoch). If you do not shuffle the instances—for example,
    if the instances are sorted by label—then SGD will start by optimizing for one
    label, then the next, and so on, and it will not settle close to the global minimum.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用随机梯度下降时，训练实例必须是独立同分布的（IID），以确保参数平均被拉向全局最优解。确保这一点的一个简单方法是在训练期间对实例进行洗牌（例如，随机选择每个实例，或在每个epoch开始时对训练集进行洗牌）。如果不对实例进行洗牌，例如，如果实例按标签排序，则SGD将从优化一个标签开始，然后是下一个标签，依此类推，并且不会接近全局最小值。
- en: 'To perform linear regression using stochastic GD with Scikit-Learn, you can
    use the `SGDRegressor` class, which defaults to optimizing the MSE cost function.
    The following code runs for maximum 1,000 epochs (`max_iter`) or until the loss
    drops by less than 10^(–5) (`tol`) during 100 epochs (`n_iter_no_change`). It
    starts with a learning rate of 0.01 (`eta0`), using the default learning schedule
    (different from the one we used). Lastly, it does not use any regularization (`penalty=None`;
    more details on this shortly):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Scikit-Learn进行随机梯度下降线性回归，您可以使用`SGDRegressor`类，默认情况下优化MSE成本函数。以下代码最多运行1,000个时代（`max_iter`）或在100个时代内损失下降不到10^(–5)（`tol`）时停止（`n_iter_no_change`）。它以学习率0.01（`eta0`）开始，使用默认学习计划（与我们使用的不同）。最后，它不使用任何正则化（`penalty=None`；稍后会详细介绍）：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once again, you find a solution quite close to the one returned by the Normal
    equation:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，您会发现解决方案与正规方程返回的解非常接近：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Tip
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'All Scikit-Learn estimators can be trained using the `fit()` method, but some
    estimators also have a `partial_fit()` method that you can call to run a single
    round of training on one or more instances (it ignores hyperparameters like `max_iter`
    or `tol`). Repeatedly calling `partial_fit()` will gradually train the model.
    This is useful when you need more control over the training process. Other models
    have a `warm_start` hyperparameter instead (and some have both): if you set `warm_start=True`,
    calling the `fit()` method on a trained model will not reset the model; it will
    just continue training where it left off, respecting hyperparameters like `max_iter`
    and `tol`. Note that `fit()` resets the iteration counter used by the learning
    schedule, while `partial_fit()` does not.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Scikit-Learn估计器都可以使用`fit()`方法进行训练，但有些估计器还有一个`partial_fit()`方法，您可以调用它来对一个或多个实例运行一轮训练（它会忽略`max_iter`或`tol`等超参数）。反复调用`partial_fit()`会逐渐训练模型。当您需要更多控制训练过程时，这是很有用的。其他模型则有一个`warm_start`超参数（有些模型两者都有）：如果您设置`warm_start=True`，在已训练的模型上调用`fit()`方法不会重置模型；它将继续训练在哪里停止，遵守`max_iter`和`tol`等超参数。请注意，`fit()`会重置学习计划使用的迭代计数器，而`partial_fit()`不会。
- en: Mini-Batch Gradient Descent
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小批量梯度下降
- en: 'The last gradient descent algorithm we will look at is called *mini-batch gradient
    descent*. It is straightforward once you know batch and stochastic gradient descent:
    at each step, instead of computing the gradients based on the full training set
    (as in batch GD) or based on just one instance (as in stochastic GD), mini-batch
    GD computes the gradients on small random sets of instances called *mini-batches*.
    The main advantage of mini-batch GD over stochastic GD is that you can get a performance
    boost from hardware optimization of matrix operations, especially when using GPUs.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要看的最后一个梯度下降算法称为*小批量梯度下降*。一旦您了解了批量梯度下降和随机梯度下降，这就很简单了：在每一步中，小批量梯度下降不是基于完整训练集（批量梯度下降）或仅基于一个实例（随机梯度下降）计算梯度，而是在称为*小批量*的小随机实例集上计算梯度。小批量梯度下降相对于随机梯度下降的主要优势在于，您可以通过硬件优化矩阵运算获得性能提升，尤其是在使用GPU时。
- en: The algorithm’s progress in parameter space is less erratic than with stochastic
    GD, especially with fairly large mini-batches. As a result, mini-batch GD will
    end up walking around a bit closer to the minimum than stochastic GD—but it may
    be harder for it to escape from local minima (in the case of problems that suffer
    from local minima, unlike linear regression with the MSE cost function). [Figure 4-11](#gradient_descent_paths_plot)
    shows the paths taken by the three gradient descent algorithms in parameter space
    during training. They all end up near the minimum, but batch GD’s path actually
    stops at the minimum, while both stochastic GD and mini-batch GD continue to walk
    around. However, don’t forget that batch GD takes a lot of time to take each step,
    and stochastic GD and mini-batch GD would also reach the minimum if you used a
    good learning schedule.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法在参数空间中的进展比随机梯度下降更加稳定，尤其是在使用相当大的小批量时。因此，小批量梯度下降最终会比随机梯度下降更接近最小值，但它可能更难逃离局部最小值（在存在局部最小值的问题中，不同于具有MSE成本函数的线性回归）。[图4-11](#gradient_descent_paths_plot)显示了训练过程中三种梯度下降算法在参数空间中的路径。它们最终都接近最小值，但批量梯度下降的路径实际上停在最小值处，而随机梯度下降和小批量梯度下降则继续移动。但是，请不要忘记，批量梯度下降需要很长时间才能完成每一步，如果您使用良好的学习计划，随机梯度下降和小批量梯度下降也会达到最小值。
- en: '![mls3 0411](assets/mls3_0411.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0411](assets/mls3_0411.png)'
- en: Figure 4-11\. Gradient descent paths in parameter space
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-11\. 参数空间中的梯度下降路径
- en: '[Table 4-1](#linear_regression_algorithm_comparison) compares the algorithms
    we’ve discussed so far for linear regression⁠^([5](ch04.html#idm45720216235312))
    (recall that *m* is the number of training instances and *n* is the number of
    features).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[表4-1](#linear_regression_algorithm_comparison)比较了迄今为止我们讨论过的线性回归算法（请回忆*m*是训练实例的数量，*n*是特征的数量）。'
- en: Table 4-1\. Comparison of algorithms for linear regression
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-1\. 线性回归算法比较
- en: '| Algorithm | Large *m* | Out-of-core support | Large *n* | Hyperparams | Scaling
    required | Scikit-Learn |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 大 *m* | 支持离线 | 大 *n* | 超参数 | 需要缩放 | Scikit-Learn |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Normal equation | Fast | No | Slow | 0 | No | N/A |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 正规方程 | 快 | 否 | 慢 | 0 | 否 | N/A |'
- en: '| SVD | Fast | No | Slow | 0 | No | `LinearRegression` |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| SVD | 快 | 否 | 慢 | 0 | 否 | `LinearRegression` |'
- en: '| Batch GD | Slow | No | Fast | 2 | Yes | N/A |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 批量梯度下降 | 慢 | 否 | 快 | 2 | 是 | N/A |'
- en: '| Stochastic GD | Fast | Yes | Fast | ≥2 | Yes | `SGDRegressor` |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 随机梯度下降 | 快 | 是 | 快 | ≥2 | 是 | `SGDRegressor` |'
- en: '| Mini-batch GD | Fast | Yes | Fast | ≥2 | Yes | N/A |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 小批量梯度下降 | 快 | 是 | 快 | ≥2 | 是 | N/A |'
- en: 'There is almost no difference after training: all these algorithms end up with
    very similar models and make predictions in exactly the same way.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后几乎没有区别：所有这些算法最终得到非常相似的模型，并以完全相同的方式进行预测。
- en: Polynomial Regression
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项式回归
- en: What if your data is more complex than a straight line? Surprisingly, you can
    use a linear model to fit nonlinear data. A simple way to do this is to add powers
    of each feature as new features, then train a linear model on this extended set
    of features. This technique is called *polynomial regression*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据比一条直线更复杂怎么办？令人惊讶的是，你可以使用线性模型来拟合非线性数据。一个简单的方法是将每个特征的幂作为新特征添加，然后在这个扩展的特征集上训练线性模型。这种技术称为*多项式回归*。
- en: 'Let’s look at an example. First, we’ll generate some nonlinear data (see [Figure 4-12](#quadratic_data_plot)),
    based on a simple *quadratic equation*—that’s an equation of the form *y* = *ax*²
    + *bx* + *c*—plus some noise:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子。首先，我们将生成一些非线性数据（参见[图4-12](#quadratic_data_plot)），基于一个简单的*二次方程*——即形式为*y*
    = *ax*² + *bx* + *c*的方程——再加上一些噪声：
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![mls3 0412](assets/mls3_0412.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0412](assets/mls3_0412.png)'
- en: Figure 4-12\. Generated nonlinear and noisy dataset
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-12。生成的非线性和嘈杂数据集
- en: 'Clearly, a straight line will never fit this data properly. So let’s use Scikit-Learn’s
    `PolynomialFeatures` class to transform our training data, adding the square (second-degree
    polynomial) of each feature in the training set as a new feature (in this case
    there is just one feature):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，一条直线永远无法正确拟合这些数据。因此，让我们使用Scikit-Learn的`PolynomialFeatures`类来转换我们的训练数据，将训练集中每个特征的平方（二次多项式）作为新特征添加到训练数据中（在这种情况下只有一个特征）：
- en: '[PRE15]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`X_poly` now contains the original feature of `X` plus the square of this feature.
    Now we can fit a `LinearRegression` model to this extended training data ([Figure 4-13](#quadratic_predictions_plot)):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_poly`现在包含了`X`的原始特征以及该特征的平方。现在我们可以将`LinearRegression`模型拟合到这个扩展的训练数据上（[图4-13](#quadratic_predictions_plot)）：'
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![mls3 0413](assets/mls3_0413.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0413](assets/mls3_0413.png)'
- en: Figure 4-13\. Polynomial regression model predictions
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-13。多项式回归模型预测
- en: 'Not bad: the model estimates <math><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <mn>0.56</mn> <msup><mrow><msub><mi>x</mi> <mn>1</mn></msub></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>0.93</mn> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <mn>1.78</mn></mrow></math> when in fact the original function was
    <math><mrow><mi>y</mi> <mo>=</mo> <mn>0.5</mn> <msup><mrow><msub><mi>x</mi> <mn>1</mn></msub></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>1.0</mn> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo>
    <mn>2.0</mn> <mo>+</mo> <mtext>Gaussian noise</mtext></mrow></math> .'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 不错：模型估计<math><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mo>=</mo>
    <mn>0.56</mn> <msup><mrow><msub><mi>x</mi> <mn>1</mn></msub></mrow> <mn>2</mn></msup>
    <mo>+</mo> <mn>0.93</mn> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mn>1.78</mn></mrow></math>，而实际上原始函数是<math><mrow><mi>y</mi>
    <mo>=</mo> <mn>0.5</mn> <msup><mrow><msub><mi>x</mi> <mn>1</mn></msub></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>1.0</mn> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo>
    <mn>2.0</mn> <mo>+</mo> <mtext>高斯噪声</mtext></mrow></math>。
- en: Note that when there are multiple features, polynomial regression is capable
    of finding relationships between features, which is something a plain linear regression
    model cannot do. This is made possible by the fact that `PolynomialFeatures` also
    adds all combinations of features up to the given degree. For example, if there
    were two features *a* and *b*, `PolynomialFeatures` with `degree=3` would not
    only add the features *a*², *a*³, *b*², and *b*³, but also the combinations *ab*,
    *a*²*b*, and *ab*².
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当存在多个特征时，多项式回归能够找到特征之间的关系，这是普通线性回归模型无法做到的。这是因为`PolynomialFeatures`还会添加给定次数的所有特征组合。例如，如果有两个特征*a*和*b*，`PolynomialFeatures`的`degree=3`不仅会添加特征*a*²、*a*³、*b*²和*b*³，还会添加组合*ab*、*a*²*b*和*ab*²。
- en: Warning
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: '`PolynomialFeatures(degree=*d*)` transforms an array containing *n* features
    into an array containing (*n* + *d*)! / *d*!*n*! features, where *n*! is the *factorial*
    of *n*, equal to 1 × 2 × 3 × ⋯ × *n*. Beware of the combinatorial explosion of
    the number of features!'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`PolynomialFeatures(degree=*d*)`将包含*n*个特征的数组转换为包含(*n* + *d*)! / *d*!*n*!个特征的数组，其中*n*!是*n*的*阶乘*，等于1
    × 2 × 3 × ⋯ × *n*。注意特征数量的组合爆炸！'
- en: Learning Curves
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习曲线
- en: If you perform high-degree polynomial regression, you will likely fit the training
    data much better than with plain linear regression. For example, [Figure 4-14](#high_degree_polynomials_plot)
    applies a 300-degree polynomial model to the preceding training data, and compares
    the result with a pure linear model and a quadratic model (second-degree polynomial).
    Notice how the 300-degree polynomial model wiggles around to get as close as possible
    to the training instances.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果进行高次多项式回归，你很可能会比普通线性回归更好地拟合训练数据。例如，[图4-14](#high_degree_polynomials_plot)将一个300次多项式模型应用于前面的训练数据，并将结果与纯线性模型和二次模型（二次多项式）进行比较。请注意，300次多项式模型在训练实例周围摆动以尽可能接近训练实例。
- en: '![mls3 0414](assets/mls3_0414.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0414](assets/mls3_0414.png)'
- en: Figure 4-14\. High-degree polynomial regression
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-14。高次多项式回归
- en: This high-degree polynomial regression model is severely overfitting the training
    data, while the linear model is underfitting it. The model that will generalize
    best in this case is the quadratic model, which makes sense because the data was
    generated using a quadratic model. But in general you won’t know what function
    generated the data, so how can you decide how complex your model should be? How
    can you tell that your model is overfitting or underfitting the data?
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这个高次多项式回归模型严重过拟合了训练数据，而线性模型则欠拟合了。在这种情况下，最能泛化的模型是二次模型，这是有道理的，因为数据是使用二次模型生成的。但通常你不会知道是什么函数生成了数据，那么你如何决定模型应该有多复杂呢？你如何判断你的模型是过拟合还是欠拟合了数据？
- en: In [Chapter 2](ch02.html#project_chapter) you used cross-validation to get an
    estimate of a model’s generalization performance. If a model performs well on
    the training data but generalizes poorly according to the cross-validation metrics,
    then your model is overfitting. If it performs poorly on both, then it is underfitting.
    This is one way to tell when a model is too simple or too complex.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html#project_chapter)中，您使用交叉验证来估计模型的泛化性能。如果模型在训练数据上表现良好，但根据交叉验证指标泛化能力差，那么您的模型是过拟合的。如果两者表现都不好，那么它是拟合不足的。这是判断模型过于简单或过于复杂的一种方法。
- en: 'Another way to tell is to look at the *learning curves*, which are plots of
    the model’s training error and validation error as a function of the training
    iteration: just evaluate the model at regular intervals during training on both
    the training set and the validation set, and plot the results. If the model cannot
    be trained incrementally (i.e., if it does not support `partial_fit()` or `warm_start`),
    then you must train it several times on gradually larger subsets of the training
    set.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是查看*学习曲线*，这是模型的训练误差和验证误差作为训练迭代的函数的图表：只需在训练集和验证集上定期评估模型，并绘制结果。如果模型无法进行增量训练（即，如果它不支持`partial_fit()`或`warm_start`），那么您必须在逐渐扩大的训练集子集上多次训练它。
- en: 'Scikit-Learn has a useful `learning_curve()` function to help with this: it
    trains and evaluates the model using cross-validation. By default it retrains
    the model on growing subsets of the training set, but if the model supports incremental
    learning you can set `exploit_incremental_learning=True` when calling `learning_curve()`
    and it will train the model incrementally instead. The function returns the training
    set sizes at which it evaluated the model, and the training and validation scores
    it measured for each size and for each cross-validation fold. Let’s use this function
    to look at the learning curves of the plain linear regression model (see [Figure 4-15](#underfitting_learning_curves_plot)):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn有一个有用的`learning_curve()`函数来帮助解决这个问题：它使用交叉验证来训练和评估模型。默认情况下，它会在不断增长的训练集子集上重新训练模型，但如果模型支持增量学习，您可以在调用`learning_curve()`时设置`exploit_incremental_learning=True`，它将逐步训练模型。该函数返回评估模型的训练集大小，以及每个大小和每个交叉验证折叠的训练和验证分数。让我们使用这个函数来查看普通线性回归模型的学习曲线（参见[图4-15](#underfitting_learning_curves_plot)）：
- en: '[PRE17]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![mls3 0415](assets/mls3_0415.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0415](assets/mls3_0415.png)'
- en: Figure 4-15\. Learning curves
  id: totrans-186
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-15\. 学习曲线
- en: This model is underfitting. To see why, first let’s look at the training error.
    When there are just one or two instances in the training set, the model can fit
    them perfectly, which is why the curve starts at zero. But as new instances are
    added to the training set, it becomes impossible for the model to fit the training
    data perfectly, both because the data is noisy and because it is not linear at
    all. So the error on the training data goes up until it reaches a plateau, at
    which point adding new instances to the training set doesn’t make the average
    error much better or worse. Now let’s look at the validation error. When the model
    is trained on very few training instances, it is incapable of generalizing properly,
    which is why the validation error is initially quite large. Then, as the model
    is shown more training examples, it learns, and thus the validation error slowly
    goes down. However, once again a straight line cannot do a good job of modeling
    the data, so the error ends up at a plateau, very close to the other curve.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型拟合不足。为了了解原因，首先让我们看看训练误差。当训练集中只有一个或两个实例时，模型可以完美拟合它们，这就是曲线从零开始的原因。但随着新实例被添加到训练集中，模型无法完美拟合训练数据，因为数据存在噪声，而且根本不是线性的。因此，训练数据的误差会上升，直到达到一个平台，在这一点上，向训练集添加新实例不会使平均误差变得更好或更糟。现在让我们看看验证误差。当模型在非常少的训练实例上训练时，它无法正确泛化，这就是为什么验证误差最初相当大的原因。然后，随着模型展示更多的训练示例，它学习，因此验证误差慢慢下降。然而，再次，一条直线无法很好地对数据建模，因此误差最终会达到一个接近另一条曲线的平台。
- en: These learning curves are typical of a model that’s underfitting. Both curves
    have reached a plateau; they are close and fairly high.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这些学习曲线是典型的拟合不足模型。两条曲线都达到了一个平台；它们接近且相当高。
- en: Tip
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If your model is underfitting the training data, adding more training examples
    will not help. You need to use a better model or come up with better features.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型对训练数据拟合不足，增加更多的训练样本将无济于事。您需要使用更好的模型或提出更好的特征。
- en: 'Now let’s look at the learning curves of a 10th-degree polynomial model on
    the same data ([Figure 4-16](#learning_curves_plot)):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看相同数据上10次多项式模型的学习曲线（参见[图4-16](#learning_curves_plot)）：
- en: '[PRE18]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![mls3 0416](assets/mls3_0416.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0416](assets/mls3_0416.png)'
- en: Figure 4-16\. Learning curves for the 10th-degree polynomial model
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-16\. 10次多项式模型的学习曲线
- en: 'These learning curves look a bit like the previous ones, but there are two
    very important differences:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这些学习曲线看起来有点像之前的曲线，但有两个非常重要的区别：
- en: The error on the training data is much lower than before.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据上的误差比以前低得多。
- en: There is a gap between the curves. This means that the model performs significantly
    better on the training data than on the validation data, which is the hallmark
    of an overfitting model. If you used a much larger training set, however, the
    two curves would continue to get closer.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线之间存在差距。这意味着模型在训练数据上的表现明显优于验证数据，这是过拟合模型的标志。然而，如果您使用更大的训练集，这两条曲线将继续接近。
- en: Tip
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: One way to improve an overfitting model is to feed it more training data until
    the validation error reaches the training error.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 改进过拟合模型的一种方法是提供更多的训练数据，直到验证误差达到训练误差。
- en: Regularized Linear Models
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化线性模型
- en: 'As you saw in Chapters [1](ch01.html#landscape_chapter) and [2](ch02.html#project_chapter),
    a good way to reduce overfitting is to regularize the model (i.e., to constrain
    it): the fewer degrees of freedom it has, the harder it will be for it to overfit
    the data. A simple way to regularize a polynomial model is to reduce the number
    of polynomial degrees.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[第1章](ch01.html#landscape_chapter)和[第2章](ch02.html#project_chapter)中看到的，减少过拟合的一个好方法是对模型进行正则化（即，约束它）：它的自由度越少，过拟合数据的难度就越大。对多项式模型进行正则化的一种简单方法是减少多项式次数。
- en: For a linear model, regularization is typically achieved by constraining the
    weights of the model. We will now look at ridge regression, lasso regression,
    and elastic net regression, which implement three different ways to constrain
    the weights.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线性模型，通常通过约束模型的权重来实现正则化。我们现在将看一下岭回归、套索回归和弹性网络回归，它们实现了三种不同的约束权重的方式。
- en: Ridge Regression
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 岭回归
- en: '*Ridge regression* (also called *Tikhonov regularization*) is a regularized
    version of linear regression: a *regularization term* equal to <math><mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><msub><mi>θ</mi><mi>i</mi></msub><mn>2</mn></msup></math>
    is added to the MSE. This forces the learning algorithm to not only fit the data
    but also keep the model weights as small as possible. Note that the regularization
    term should only be added to the cost function during training. Once the model
    is trained, you want to use the unregularized MSE (or the RMSE) to evaluate the
    model’s performance.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*岭回归*（也称为*Tikhonov正则化*）是线性回归的正则化版本：一个等于<math><mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><msub><mi>θ</mi><mi>i</mi></msub><mn>2</mn></msup></math>的*正则化项*被添加到MSE中。这迫使学习算法不仅拟合数据，还要尽量保持模型权重尽可能小。请注意，正则化项应该只在训练期间添加到成本函数中。一旦模型训练完成，您希望使用未经正则化的MSE（或RMSE）来评估模型的性能。'
- en: The hyperparameter *α* controls how much you want to regularize the model. If
    *α* = 0, then ridge regression is just linear regression. If *α* is very large,
    then all weights end up very close to zero and the result is a flat line going
    through the data’s mean. [Equation 4-8](#ridge_cost_function) presents the ridge
    regression cost function.⁠^([7](ch04.html#idm45720215617520))
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数*α*控制着您希望对模型进行多少正则化。如果*α*=0，则岭回归就是线性回归。如果*α*非常大，则所有权重最终都非常接近零，结果是一条通过数据均值的平坦线。[方程4-8](#ridge_cost_function)呈现了岭回归成本函数。⁠^([7](ch04.html#idm45720215617520))
- en: Equation 4-8\. Ridge regression cost function
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-8。岭回归成本函数
- en: <math><mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><msub><mi>θ</mi><mi>i</mi></msub><mn>2</mn></msup></math>
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><msub><mi>θ</mi><mi>i</mi></msub><mn>2</mn></msup></math>
- en: Note that the bias term *θ*[0] is not regularized (the sum starts at *i* = 1,
    not 0). If we define **w** as the vector of feature weights (*θ*[1] to *θ*[*n*]),
    then the regularization term is equal to *α*(∥ **w** ∥[2])² / *m*, where ∥ **w**
    ∥[2] represents the ℓ[2] norm of the weight vector.⁠^([8](ch04.html#idm45720215599696))
    For batch gradient descent, just add 2*α***w** / *m* to the part of the MSE gradient
    vector that corresponds to the feature weights, without adding anything to the
    gradient of the bias term (see [Equation 4-6](#mse_gradient_vector)).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，偏置项*θ*[0]不被正则化（总和从*i*=1开始，而不是0）。如果我们将**w**定义为特征权重的向量（*θ*[1]到*θ*[*n*]），则正则化项等于*α*(∥
    **w** ∥[2])² / *m*，其中∥ **w** ∥[2]表示权重向量的ℓ[2]范数。⁠^([8](ch04.html#idm45720215599696))
    对于批量梯度下降，只需将2*α***w** / *m*添加到对应于特征权重的MSE梯度向量的部分，而不要将任何内容添加到偏置项的梯度（参见[方程4-6](#mse_gradient_vector)）。
- en: Warning
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: It is important to scale the data (e.g., using a `StandardScaler`) before performing
    ridge regression, as it is sensitive to the scale of the input features. This
    is true of most regularized models.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行岭回归之前，重要的是对数据进行缩放（例如，使用`StandardScaler`），因为它对输入特征的规模敏感。这对大多数正则化模型都是正确的。
- en: '[Figure 4-17](#ridge_regression_plot) shows several ridge models that were
    trained on some very noisy linear data using different *α* values. On the left,
    plain ridge models are used, leading to linear predictions. On the right, the
    data is first expanded using `PolynomialFeatures(degree=10)`, then it is scaled
    using a `StandardScaler`, and finally the ridge models are applied to the resulting
    features: this is polynomial regression with ridge regularization. Note how increasing
    *α* leads to flatter (i.e., less extreme, more reasonable) predictions, thus reducing
    the model’s variance but increasing its bias.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-17](#ridge_regression_plot)显示了在一些非常嘈杂的线性数据上使用不同*α*值训练的几个岭模型。在左侧，使用普通的岭模型，导致线性预测。在右侧，首先使用`PolynomialFeatures(degree=10)`扩展数据，然后使用`StandardScaler`进行缩放，最后将岭模型应用于生成的特征：这是带有岭正则化的多项式回归。请注意，增加*α*会导致更平缓（即，更不极端，更合理）的预测，从而减少模型的方差但增加其偏差。'
- en: '![mls3 0417](assets/mls3_0417.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0417](assets/mls3_0417.png)'
- en: Figure 4-17\. Linear (left) and a polynomial (right) models, both with various
    levels of ridge regularization
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-17。线性（左）和多项式（右）模型，都具有不同级别的岭正则化
- en: As with linear regression, we can perform ridge regression either by computing
    a closed-form equation or by performing gradient descent. The pros and cons are
    the same. [Equation 4-9](#ridge_regression_solution) shows the closed-form solution,
    where **A** is the (*n* + 1) × (*n* + 1) *identity matrix*,⁠^([9](ch04.html#idm45720215579520))
    except with a 0 in the top-left cell, corresponding to the bias term.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性回归一样，我们可以通过计算闭式方程或执行梯度下降来执行岭回归。优缺点是相同的。[方程4-9](#ridge_regression_solution)显示了闭式解，其中**A**是(*n*
    + 1) × (*n* + 1) *单位矩阵*，⁠^([9](ch04.html#idm45720215579520))除了左上角的单元格为0，对应于偏置项。
- en: Equation 4-9\. Ridge regression closed-form solution
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-9. 岭回归闭式解
- en: <math display="block"><mrow><mover accent="true"><mi mathvariant="bold">θ</mi>
    <mo>^</mo></mover> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">X</mi>
    <mo>⊺</mo></msup> <mi mathvariant="bold">X</mi><mo>+</mo><mi>α</mi><mi mathvariant="bold">A</mi><mo>)</mo></mrow>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup>  <msup><mi mathvariant="bold">X</mi>
    <mo>⊺</mo></msup>  <mi mathvariant="bold">y</mi></mrow></math>
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mover accent="true"><mi mathvariant="bold">θ</mi>
    <mo>^</mo></mover> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">X</mi>
    <mo>⊺</mo></msup> <mi mathvariant="bold">X</mi><mo>+</mo><mi>α</mi><mi mathvariant="bold">A</mi><mo>)</mo></mrow>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup>  <msup><mi mathvariant="bold">X</mi>
    <mo>⊺</mo></msup>  <mi mathvariant="bold">y</mi></mrow></math>
- en: 'Here is how to perform ridge regression with Scikit-Learn using a closed-form
    solution (a variant of [Equation 4-9](#ridge_regression_solution) that uses a
    matrix factorization technique by André-Louis Cholesky):'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用Scikit-Learn执行岭回归的闭式解（一种[方程4-9](#ridge_regression_solution)的变体，使用André-Louis
    Cholesky的矩阵分解技术）：
- en: '[PRE19]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: And using stochastic gradient descent:⁠^([10](ch04.html#idm45720215546880))
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机梯度下降：⁠^([10](ch04.html#idm45720215546880))
- en: '[PRE20]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `penalty` hyperparameter sets the type of regularization term to use. Specifying
    `"l2"` indicates that you want SGD to add a regularization term to the MSE cost
    function equal to `alpha` times the square of the ℓ[2] norm of the weight vector.
    This is just like ridge regression, except there’s no division by *m* in this
    case; that’s why we passed `alpha=0.1 / m`, to get the same result as `Ridge(alpha=0.1)`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`penalty`超参数设置要使用的正则化项的类型。指定`"l2"`表示您希望SGD将正则化项添加到MSE成本函数中，等于`alpha`乘以权重向量的ℓ[2]范数的平方。这就像岭回归一样，只是在这种情况下没有除以*m*；这就是为什么我们传递`alpha=0.1
    / m`，以获得与`Ridge(alpha=0.1)`相同的结果。'
- en: Tip
  id: totrans-222
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The `RidgeCV` class also performs ridge regression, but it automatically tunes
    hyperparameters using cross-validation. It’s roughly equivalent to using `GridSearchCV`,
    but it’s optimized for ridge regression and runs *much* faster. Several other
    estimators (mostly linear) also have efficient CV variants, such as `LassoCV`
    and `ElasticNetCV`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`RidgeCV`类也执行岭回归，但它会自动使用交叉验证调整超参数。它大致相当于使用`GridSearchCV`，但它针对岭回归进行了优化，并且运行*快得多*。其他几个估计器（主要是线性的）也有高效的CV变体，如`LassoCV`和`ElasticNetCV`。'
- en: Lasso Regression
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lasso回归
- en: '*Least absolute shrinkage and selection operator regression* (usually simply
    called *lasso regression*) is another regularized version of linear regression:
    just like ridge regression, it adds a regularization term to the cost function,
    but it uses the ℓ[1] norm of the weight vector instead of the square of the ℓ[2]
    norm (see [Equation 4-10](#lasso_cost_function)). Notice that the ℓ[1] norm is
    multiplied by 2*α*, whereas the ℓ[2] norm was multiplied by *α* / *m* in ridge
    regression. These factors were chosen to ensure that the optimal *α* value is
    independent from the training set size: different norms lead to different factors
    (see [Scikit-Learn issue #15657](https://github.com/scikit-learn/scikit-learn/issues/15657)
    for more details).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '*最小绝对值收缩和选择算子回归*（通常简称为*Lasso回归*）是线性回归的另一个正则化版本：就像岭回归一样，它向成本函数添加一个正则化项，但是它使用权重向量的ℓ[1]范数，而不是ℓ[2]范数的平方（参见[方程4-10](#lasso_cost_function)）。请注意，ℓ[1]范数乘以2*α*，而ℓ[2]范数在岭回归中乘以*α*
    / *m*。选择这些因子是为了确保最佳*α*值与训练集大小无关：不同的范数导致不同的因子（有关更多细节，请参阅[Scikit-Learn问题＃15657](https://github.com/scikit-learn/scikit-learn/issues/15657)）。'
- en: Equation 4-10\. Lasso regression cost function
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-10. Lasso回归成本函数
- en: <math><mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mrow><mn>2</mn><mi>α</mi><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfenced
    open="|" close="|"><msub><mi>θ</mi><mi>i</mi></msub></mfenced></mrow></math>
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mrow><mn>2</mn><mi>α</mi><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfenced
    open="|" close="|"><msub><mi>θ</mi><mi>i</mi></msub></mfenced></mrow></math>
- en: '[Figure 4-18](#lasso_regression_plot) shows the same thing as [Figure 4-17](#ridge_regression_plot)
    but replaces the ridge models with lasso models and uses different *α* values.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-18](#lasso_regression_plot)显示了与[图4-17](#ridge_regression_plot)相同的内容，但用Lasso模型替换了岭模型，并使用不同的*α*值。'
- en: '![mls3 0418](assets/mls3_0418.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0418](assets/mls3_0418.png)'
- en: Figure 4-18\. Linear (left) and polynomial (right) models, both using various
    levels of lasso regularization
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-18. 线性（左）和多项式（右）模型，都使用不同级别的Lasso正则化
- en: 'An important characteristic of lasso regression is that it tends to eliminate
    the weights of the least important features (i.e., set them to zero). For example,
    the dashed line in the righthand plot in [Figure 4-18](#lasso_regression_plot)
    (with *α* = 0.01) looks roughly cubic: all the weights for the high-degree polynomial
    features are equal to zero. In other words, lasso regression automatically performs
    feature selection and outputs a *sparse model* with few nonzero feature weights.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso回归的一个重要特征是它倾向于消除最不重要特征的权重（即将它们设置为零）。例如，[图4-18](#lasso_regression_plot)中右侧图中的虚线看起来大致是立方形：高次多项式特征的所有权重都等于零。换句话说，Lasso回归自动执行特征选择，并输出具有少量非零特征权重的*稀疏模型*。
- en: 'You can get a sense of why this is the case by looking at [Figure 4-19](#lasso_vs_ridge_plot):
    the axes represent two model parameters, and the background contours represent
    different loss functions. In the top-left plot, the contours represent the ℓ[1]
    loss (|*θ*[1]| + |*θ*[2]|), which drops linearly as you get closer to any axis.
    For example, if you initialize the model parameters to *θ*[1] = 2 and *θ*[2] =
    0.5, running gradient descent will decrement both parameters equally (as represented
    by the dashed yellow line); therefore *θ*[2] will reach 0 first (since it was
    closer to 0 to begin with). After that, gradient descent will roll down the gutter
    until it reaches *θ*[1] = 0 (with a bit of bouncing around, since the gradients
    of ℓ[1] never get close to 0: they are either –1 or 1 for each parameter). In
    the top-right plot, the contours represent lasso regression’s cost function (i.e.,
    an MSE cost function plus an ℓ[1] loss). The small white circles show the path
    that gradient descent takes to optimize some model parameters that were initialized
    around *θ*[1] = 0.25 and *θ*[2] = –1: notice once again how the path quickly reaches
    *θ*[2] = 0, then rolls down the gutter and ends up bouncing around the global
    optimum (represented by the red square). If we increased *α*, the global optimum
    would move left along the dashed yellow line, while if we decreased *α*, the global
    optimum would move right (in this example, the optimal parameters for the unregularized
    MSE are *θ*[1] = 2 and *θ*[2] = 0.5).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过查看[图4-19](#lasso_vs_ridge_plot)来了解这种情况：坐标轴代表两个模型参数，背景轮廓代表不同的损失函数。在左上角的图中，轮廓代表ℓ[1]损失（|θ[1]|
    + |θ[2]|），随着你靠近任何轴，损失会线性下降。例如，如果你将模型参数初始化为θ[1] = 2和θ[2] = 0.5，运行梯度下降将等量减少两个参数（如虚线黄线所示）；因此θ[2]会先达到0（因为它最初更接近0）。之后，梯度下降将沿着槽滚动，直到达到θ[1]
    = 0（稍微反弹一下，因为ℓ[1]的梯度从不接近0：对于每个参数，它们要么是-1要么是1）。在右上角的图中，轮廓代表套索回归的成本函数（即，MSE成本函数加上ℓ[1]损失）。小白色圆圈显示了梯度下降优化某些模型参数的路径，这些参数最初设定为θ[1]
    = 0.25和θ[2] = -1：再次注意路径如何迅速到达θ[2] = 0，然后沿着槽滚动并最终在全局最优解周围反弹（由红色方块表示）。如果增加α，全局最优解将沿着虚线黄线向左移动，而如果减小α，全局最优解将向右移动（在这个例子中，未正则化MSE的最佳参数为θ[1]
    = 2和θ[2] = 0.5）。
- en: '![mls3 0419](assets/mls3_0419.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0419](assets/mls3_0419.png)'
- en: Figure 4-19\. Lasso versus ridge regularization
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-19。套索与岭正则化
- en: The two bottom plots show the same thing but with an ℓ[2] penalty instead. In
    the bottom-left plot, you can see that the ℓ[2] loss decreases as we get closer
    to the origin, so gradient descent just takes a straight path toward that point.
    In the bottom-right plot, the contours represent ridge regression’s cost function
    (i.e., an MSE cost function plus an ℓ[2] loss). As you can see, the gradients
    get smaller as the parameters approach the global optimum, so gradient descent
    naturally slows down. This limits the bouncing around, which helps ridge converge
    faster than lasso regression. Also note that the optimal parameters (represented
    by the red square) get closer and closer to the origin when you increase *α*,
    but they never get eliminated entirely.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 两个底部图表展示了相同的情况，但使用了ℓ[2]惩罚。在左下角的图中，你可以看到随着我们靠近原点，ℓ[2]损失减少，因此梯度下降直接朝着那个点前进。在右下角的图中，轮廓代表岭回归的成本函数（即，MSE成本函数加上ℓ[2]损失）。正如你所看到的，随着参数接近全局最优解，梯度变小，因此梯度下降自然减慢。这限制了反弹，有助于岭回归比套索收敛更快。还要注意，当增加α时，最佳参数（由红色方块表示）越来越接近原点，但它们永远不会完全消失。
- en: Tip
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: To keep gradient descent from bouncing around the optimum at the end when using
    lasso regression, you need to gradually reduce the learning rate during training.
    It will still bounce around the optimum, but the steps will get smaller and smaller,
    so it will converge.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止在使用套索回归时梯度下降在最后反弹到最优解周围，你需要在训练过程中逐渐减小学习率。它仍然会在最优解周围反弹，但步长会变得越来越小，因此会收敛。
- en: The lasso cost function is not differentiable at *θ*[*i*] = 0 (for *i* = 1,
    2, ⋯, *n*), but gradient descent still works if you use a *subgradient vector*
    **g**⁠^([11](ch04.html#idm45720215341648)) instead when any *θ*[*i*] = 0\. [Equation
    4-11](#lasso_subgradient_vector) shows a subgradient vector equation you can use
    for gradient descent with the lasso cost function.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 套索成本函数在θ[i] = 0（对于i = 1, 2, ⋯, n）处不可微，但如果在任何θ[i] = 0时使用*子梯度向量* **g**⁠^([11](ch04.html#idm45720215341648))，梯度下降仍然有效。[方程4-11](#lasso_subgradient_vector)展示了一个你可以用于套索成本函数的梯度下降的子梯度向量方程。
- en: Equation 4-11\. Lasso regression subgradient vector
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-11。套索回归子梯度向量
- en: <math><mrow><mi>g</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>,</mo><mi>J</mi><mo>)</mo></mrow><mo>=</mo><mrow><msub><mo>∇</mo><mi
    mathvariant="bold">θ</mi></msub><mtext>MSE</mtext><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mrow><mn>2</mn><mi>α</mi><mfenced><mtable><mtr><mtd><mo>sign</mo><mo>(</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>)</mo></mtd></mtr><mtr><mtd><mo>sign</mo><mo>(</mo><msub><mi>θ</mi><mn>2</mn></msub><mo>)</mo></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mo>sign</mo><mo>(</mo><msub><mi>θ</mi><mi>n</mi></msub><mo>)</mo></mtd></mtr></mtable></mfenced></mrow><mtext>where </mtext><mrow><mo>sign</mo><mo>(</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>)</mo></mrow><mo>=</mo><mrow><mfenced
    open="{" close=""><mtable><mtr><mtd><mo>-</mo><mn>1</mn></mtd><mtd><mtext>if </mtext><msub><mi>θ</mi><mi>i</mi></msub><mo><</mo><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mtext>if </mtext><msub><mi>θ</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mo>+</mo><mn>1</mn></mtd><mtd><mtext>if </mtext><msub><mi>θ</mi><mi>i</mi></msub><mo>></mo><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>g</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>,</mo><mi>J</mi><mo>)</mo></mrow><mo>=</mo><mrow><msub><mo>∇</mo><mi
    mathvariant="bold">θ</mi></msub><mtext>MSE</mtext><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mrow><mn>2</mn><mi>α</mi><mfenced><mtable><mtr><mtd><mo>sign</mo><mo>(</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>)</mo></mtd></mtr><mtr><mtd><mo>sign</mo><mo>(</mo><msub><mi>θ</mi><mn>2</mn></msub><mo>)</mo></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mo>sign</mo><mo>(</mo><msub><mi>θ</mi><mi>n</mi></msub><mo>)</mo></mtd></mtr></mtable></mfenced></mrow><mtext>where </mtext><mrow><mo>sign</mo><mo>(</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>)</mo></mrow><mo>=</mo><mrow><mfenced
    open="{" close=""><mtable><mtr><mtd><mo>-</mo><mn>1</mn></mtd><mtd><mtext>if </mtext><msub><mi>θ</mi><mi>i</mi></msub><mo><</mo><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mtext>if </mtext><msub><mi>θ</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mo>+</mo><mn>1</mn></mtd><mtd><mtext>if </mtext><msub><mi>θ</mi><mi>i</mi></msub><mo>></mo><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Here is a small Scikit-Learn example using the `Lasso` class:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个使用`Lasso`类的小型Scikit-Learn示例：
- en: '[PRE21]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that you could instead use `SGDRegressor(penalty="l1", alpha=0.1)`.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您也可以使用`SGDRegressor(penalty="l1", alpha=0.1)`。
- en: Elastic Net Regression
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弹性网回归
- en: '*Elastic net regression* is a middle ground between ridge regression and lasso
    regression. The regularization term is a weighted sum of both ridge and lasso’s
    regularization terms, and you can control the mix ratio *r*. When *r* = 0, elastic
    net is equivalent to ridge regression, and when *r* = 1, it is equivalent to lasso
    regression ([Equation 4-12](#elastic_net_cost_function)).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '*弹性网回归*是岭回归和套索回归之间的中间地带。正则化项是岭回归和套索回归正则化项的加权和，您可以控制混合比例*r*。当*r*=0时，弹性网等同于岭回归，当*r*=1时，它等同于套索回归（[方程4-12](#elastic_net_cost_function)）。'
- en: Equation 4-12\. Elastic net cost function
  id: totrans-246
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-12。弹性网成本函数
- en: <math><mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mi>r</mi><mfenced><mrow><mn>2</mn><mi>α</mi><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfenced
    open="|" close="|"><msub><mi>θ</mi><mi>i</mi></msub></mfenced></mrow></mfenced><mo>+</mo><mo>(</mo><mn>1</mn><mo>-</mo><mi>r</mi><mo>)</mo><mfenced><mrow><mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>θ</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mfenced></math>
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mi>r</mi><mfenced><mrow><mn>2</mn><mi>α</mi><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfenced
    open="|" close="|"><msub><mi>θ</mi><mi>i</mi></msub></mfenced></mrow></mfenced><mo>+</mo><mo>(</mo><mn>1</mn><mo>-</mo><mi>r</mi><mo>)</mo><mfenced><mrow><mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>θ</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mfenced></math>
- en: So when should you use elastic net regression, or ridge, lasso, or plain linear
    regression (i.e., without any regularization)? It is almost always preferable
    to have at least a little bit of regularization, so generally you should avoid
    plain linear regression. Ridge is a good default, but if you suspect that only
    a few features are useful, you should prefer lasso or elastic net because they
    tend to reduce the useless features’ weights down to zero, as discussed earlier.
    In general, elastic net is preferred over lasso because lasso may behave erratically
    when the number of features is greater than the number of training instances or
    when several features are strongly correlated.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 那么何时使用弹性网回归，或者岭回归、套索回归，或者普通线性回归（即没有任何正则化）？通常最好至少有一点点正则化，因此通常应避免普通线性回归。岭回归是一个很好的默认选择，但如果您怀疑只有少数特征是有用的，您应该更喜欢套索或弹性网，因为它们倾向于将无用特征的权重降至零，正如前面讨论的那样。总的来说，相对于套索，弹性网更受青睐，因为当特征数量大于训练实例数量或者多个特征强相关时，套索可能表现不稳定。
- en: 'Here is a short example that uses Scikit-Learn’s `ElasticNet` (`l1_ratio` corresponds
    to the mix ratio *r*):'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个使用Scikit-Learn的`ElasticNet`的简短示例（`l1_ratio`对应混合比例*r*）：
- en: '[PRE22]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Early Stopping
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 早停
- en: A very different way to regularize iterative learning algorithms such as gradient
    descent is to stop training as soon as the validation error reaches a minimum.
    This is called *early stopping*. [Figure 4-20](#early_stopping_plot) shows a complex
    model (in this case, a high-degree polynomial regression model) being trained
    with batch gradient descent on the quadratic dataset we used earlier. As the epochs
    go by, the algorithm learns, and its prediction error (RMSE) on the training set
    goes down, along with its prediction error on the validation set. After a while,
    though, the validation error stops decreasing and starts to go back up. This indicates
    that the model has started to overfit the training data. With early stopping you
    just stop training as soon as the validation error reaches the minimum. It is
    such a simple and efficient regularization technique that Geoffrey Hinton called
    it a “beautiful free lunch”.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非常不同的正则化迭代学习算法（如梯度下降）的方法是在验证错误达到最小值时停止训练。这被称为*早停止*。[图4-20](#early_stopping_plot)显示了一个复杂模型（在本例中，是一个高次多项式回归模型）在我们之前使用的二次数据集上使用批量梯度下降进行训练。随着时代的变迁，算法学习，其在训练集上的预测误差（RMSE）下降，以及在验证集上的预测误差也下降。然而，一段时间后，验证错误停止下降并开始上升。这表明模型已经开始过拟合训练数据。通过早停止，您只需在验证错误达到最小值时停止训练。这是一种简单而高效的正则化技术，Geoffrey
    Hinton称之为“美丽的免费午餐”。
- en: '![mls3 0420](assets/mls3_0420.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0420](assets/mls3_0420.png)'
- en: Figure 4-20\. Early stopping regularization
  id: totrans-254
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-20。早停止正则化
- en: Tip
  id: totrans-255
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: With stochastic and mini-batch gradient descent, the curves are not so smooth,
    and it may be hard to know whether you have reached the minimum or not. One solution
    is to stop only after the validation error has been above the minimum for some
    time (when you are confident that the model will not do any better), then roll
    back the model parameters to the point where the validation error was at a minimum.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机梯度下降和小批量梯度下降，曲线不那么平滑，可能很难知道是否已经达到最小值。一个解决方案是只有在验证错误超过最小值一段时间后（当您确信模型不会再有更好的表现时），然后将模型参数回滚到验证错误最小值的点。
- en: 'Here is a basic implementation of early stopping:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这是早停止的基本实现：
- en: '[PRE23]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This code first adds the polynomial features and scales all the input features,
    both for the training set and for the validation set (the code assumes that you
    have split the original training set into a smaller training set and a validation
    set). Then it creates an `SGDRegressor` model with no regularization and a small
    learning rate. In the training loop, it calls `partial_fit()` instead of `fit()`,
    to perform incremental learning. At each epoch, it measures the RMSE on the validation
    set. If it is lower than the lowest RMSE seen so far, it saves a copy of the model
    in the `best_model` variable. This implementation does not actually stop training,
    but it lets you revert to the best model after training. Note that the model is
    copied using `copy.deepcopy()`, because it copies both the model’s hyperparameters
    *and* the learned parameters. In contrast, `sklearn.base.clone()` only copies
    the model’s hyperparameters.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码首先添加多项式特征并缩放所有输入特征，对于训练集和验证集都是如此（代码假定您已将原始训练集分成较小的训练集和验证集）。然后它创建一个没有正则化和较小学习率的`SGDRegressor`模型。在训练循环中，它调用`partial_fit()`而不是`fit()`，以执行增量学习。在每个时代，它测量验证集上的RMSE。如果低于迄今为止看到的最低RMSE，则将模型的副本保存在`best_model`变量中。这个实现实际上并没有停止训练，但它允许您在训练后返回到最佳模型。请注意，使用`copy.deepcopy()`复制模型，因为它同时复制了模型的超参数和学习参数。相比之下，`sklearn.base.clone()`只复制模型的超参数。
- en: Logistic Regression
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: As discussed in [Chapter 1](ch01.html#landscape_chapter), some regression algorithms
    can be used for classification (and vice versa). *Logistic regression* (also called
    *logit regression*) is commonly used to estimate the probability that an instance
    belongs to a particular class (e.g., what is the probability that this email is
    spam?). If the estimated probability is greater than a given threshold (typically
    50%), then the model predicts that the instance belongs to that class (called
    the *positive class*, labeled “1”), and otherwise it predicts that it does not
    (i.e., it belongs to the *negative class*, labeled “0”). This makes it a binary
    classifier.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第1章](ch01.html#landscape_chapter)中讨论的那样，一些回归算法可以用于分类（反之亦然）。*逻辑回归*（也称为*logit回归*）通常用于估计一个实例属于特定类别的概率（例如，这封电子邮件是垃圾邮件的概率是多少？）。如果估计的概率大于给定阈值（通常为50%），则模型预测该实例属于该类别（称为*正类*，标记为“1”），否则预测它不属于该类别（即属于*负类*，标记为“0”）。这使其成为一个二元分类器。
- en: Estimating Probabilities
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 估计概率
- en: So how does logistic regression work? Just like a linear regression model, a
    logistic regression model computes a weighted sum of the input features (plus
    a bias term), but instead of outputting the result directly like the linear regression
    model does, it outputs the *logistic* of this result (see [Equation 4-13](#logisticregression_model_estimated_probability_vectorized_form)).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 那么逻辑回归是如何工作的呢？就像线性回归模型一样，逻辑回归模型计算输入特征的加权和（加上偏置项），但是不像线性回归模型直接输出结果，它输出这个结果的*逻辑*（参见[方程4-13](#logisticregression_model_estimated_probability_vectorized_form)）。
- en: Equation 4-13\. Logistic regression model estimated probability (vectorized
    form)
  id: totrans-264
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-13。逻辑回归模型估计概率（向量化形式）
- en: <math display="block"><mrow><mover accent="true"><mi>p</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mi>h</mi> <mi mathvariant="bold">θ</mi></msub> <mrow><mo>(</mo>
    <mi mathvariant="bold">x</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <msup><mi mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">x</mi>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mover accent="true"><mi>p</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mi>h</mi> <mi mathvariant="bold">θ</mi></msub> <mrow><mo>(</mo>
    <mi mathvariant="bold">x</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <msup><mi mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">x</mi>
    <mo>)</mo></mrow></mrow></math>
- en: The logistic—noted *σ*(·)—is a *sigmoid function* (i.e., *S*-shaped) that outputs
    a number between 0 and 1\. It is defined as shown in [Equation 4-14](#equation_four_fourteen)
    and [Figure 4-21](#logistic_function_plot).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑函数 *σ*(·) 是一个 *S* 形函数，输出介于 0 和 1 之间的数字。它的定义如 [方程式 4-14](#equation_four_fourteen)
    和 [图 4-21](#logistic_function_plot) 所示。
- en: Equation 4-14\. Logistic function
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 4-14\. 逻辑函数
- en: <math display="block"><mrow><mi>σ</mi> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><mo
    form="prefix">exp</mo><mo>(</mo><mo>-</mo><mi>t</mi><mo>)</mo></mrow></mfrac></mstyle></mrow></math>![mls3
    0421](assets/mls3_0421.png)
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>σ</mi> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><mo
    form="prefix">exp</mo><mo>(</mo><mo>-</mo><mi>t</mi><mo>)</mo></mrow></mfrac></mstyle></mrow></math>![mls3
    0421](assets/mls3_0421.png)
- en: Figure 4-21\. Logistic function
  id: totrans-269
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-21\. 逻辑函数
- en: Once the logistic regression model has estimated the probability <math><mover><mi>p</mi><mo>^</mo></mover></math>
    = *h*[**θ**](**x**) that an instance **x** belongs to the positive class, it can
    make its prediction *ŷ* easily (see [Equation 4-15](#equation_four_fifteen)).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型一旦估计出概率 <math><mover><mi>p</mi><mo>^</mo></mover></math> = *h*[**θ**](**x**)，即实例
    **x** 属于正类的概率，它可以轻松地进行预测 *ŷ*（见 [方程式 4-15](#equation_four_fifteen)）。
- en: Equation 4-15\. Logistic regression model prediction using a 50% threshold probability
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 4-15\. 使用 50% 阈值概率的逻辑回归模型预测
- en: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mover accent="true"><mi>p</mi>
    <mo>^</mo></mover> <mo><</mo> <mn>0.5</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mover accent="true"><mi>p</mi>
    <mo>^</mo></mover> <mo>≥</mo> <mn>0.5</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mover accent="true"><mi>p</mi>
    <mo>^</mo></mover> <mo><</mo> <mn>0.5</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mover accent="true"><mi>p</mi>
    <mo>^</mo></mover> <mo>≥</mo> <mn>0.5</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: Notice that *σ*(*t*) < 0.5 when *t* < 0, and *σ*(*t*) ≥ 0.5 when *t* ≥ 0, so
    a logistic regression model using the default threshold of 50% probability predicts
    1 if **θ**^⊺ **x** is positive and 0 if it is negative.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到当 *t* < 0 时，*σ*(*t*) < 0.5，当 *t* ≥ 0 时，*σ*(*t*) ≥ 0.5，因此使用默认的 50% 概率阈值的逻辑回归模型会在
    **θ**^⊺ **x** 为正时预测为 1，为负时预测为 0。
- en: Note
  id: totrans-274
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The score *t* is often called the *logit*. The name comes from the fact that
    the logit function, defined as logit(*p*) = log(*p* / (1 – *p*)), is the inverse
    of the logistic function. Indeed, if you compute the logit of the estimated probability
    *p*, you will find that the result is *t*. The logit is also called the *log-odds*,
    since it is the log of the ratio between the estimated probability for the positive
    class and the estimated probability for the negative class.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 得分 *t* 通常被称为 *对数几率*。这个名字来自于对数几率函数的定义，即 logit(*p*) = log(*p* / (1 – *p*))，它是逻辑函数的反函数。实际上，如果计算估计概率
    *p* 的对数几率，你会发现结果是 *t*。对数几率也被称为 *对数几率比*，因为它是正类估计概率与负类估计概率之间的比值的对数。
- en: Training and Cost Function
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和成本函数
- en: Now you know how a logistic regression model estimates probabilities and makes
    predictions. But how is it trained? The objective of training is to set the parameter
    vector **θ** so that the model estimates high probabilities for positive instances
    (*y* = 1) and low probabilities for negative instances (*y* = 0). This idea is
    captured by the cost function shown in [Equation 4-16](#cost_function_of_a_single_training_instance)
    for a single training instance **x**.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道逻辑回归模型如何估计概率并进行预测了。但是它是如何训练的呢？训练的目标是设置参数向量 **θ**，使模型为正实例（*y* = 1）估计出高概率，为负实例（*y*
    = 0）估计出低概率。这个想法被 [方程式 4-16](#cost_function_of_a_single_training_instance) 中的成本函数所捕捉，针对单个训练实例
    **x**。
- en: Equation 4-16\. Cost function of a single training instance
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 4-16\. 单个训练实例的成本函数
- en: <math><mrow><mi>c</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mfenced
    open="{" close=""><mtable><mtr><mtd><mo>-</mo><mi>log</mi><mo>(</mo><mover accent="true"><mi>p</mi><mo>^</mo></mover><mo>)</mo></mtd><mtd><mtext>if </mtext><mi>y</mi><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd><mo>-</mo><mi>log</mi><mo>(</mo><mn>1</mn><mo>-</mo><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mo>)</mo></mtd><mtd><mtext>if </mtext><mi>y</mi><mo>=</mo><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>c</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mfenced
    open="{" close=""><mtable><mtr><mtd><mo>-</mo><mi>log</mi><mo>(</mo><mover accent="true"><mi>p</mi><mo>^</mo></mover><mo>)</mo></mtd><mtd><mtext>if </mtext><mi>y</mi><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd><mo>-</mo><mi>log</mi><mo>(</mo><mn>1</mn><mo>-</mo><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mo>)</mo></mtd><mtd><mtext>if </mtext><mi>y</mi><mo>=</mo><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: This cost function makes sense because –log(*t*) grows very large when *t* approaches
    0, so the cost will be large if the model estimates a probability close to 0 for
    a positive instance, and it will also be large if the model estimates a probability
    close to 1 for a negative instance. On the other hand, –log(*t*) is close to 0
    when *t* is close to 1, so the cost will be close to 0 if the estimated probability
    is close to 0 for a negative instance or close to 1 for a positive instance, which
    is precisely what we want.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这个成本函数是有意义的，因为当 *t* 接近 0 时，–log(*t*) 会变得非常大，所以如果模型为正实例估计出接近 0 的概率，成本会很大，如果模型为负实例估计出接近
    1 的概率，成本也会很大。另一方面，当 *t* 接近 1 时，–log(*t*) 接近 0，所以如果负实例的估计概率接近 0，或者正实例的估计概率接近 1，成本会接近
    0，这正是我们想要的。
- en: The cost function over the whole training set is the average cost over all training
    instances. It can be written in a single expression called the *log loss*, shown
    in [Equation 4-17](#logistic_regression_cost_function).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-17\. Logistic regression cost function (log loss)
  id: totrans-282
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math><mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mfenced
    open="[" close="]"><mrow><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mi>l</mi><mi>o</mi><mi>g</mi><mfenced><msup><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mfenced><mo>+</mo><mo>(</mo><mn>1</mn><mo>-</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo><mi>l</mi><mi>o</mi><mi>g</mi><mfenced><mrow><mn>1</mn><mo>-</mo><msup><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mfenced></mrow></mfenced></mrow></math>
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-284
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The log loss was not just pulled out of a hat. It can be shown mathematically
    (using Bayesian inference) that minimizing this loss will result in the model
    with the *maximum likelihood* of being optimal, assuming that the instances follow
    a Gaussian distribution around the mean of their class. When you use the log loss,
    this is the implicit assumption you are making. The more wrong this assumption
    is, the more biased the model will be. Similarly, when we used the MSE to train
    linear regression models, we were implicitly assuming that the data was purely
    linear, plus some Gaussian noise. So, if the data is not linear (e.g., if it’s
    quadratic) or if the noise is not Gaussian (e.g., if outliers are not exponentially
    rare), then the model will be biased.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: The bad news is that there is no known closed-form equation to compute the value
    of **θ** that minimizes this cost function (there is no equivalent of the Normal
    equation). But the good news is that this cost function is convex, so gradient
    descent (or any other optimization algorithm) is guaranteed to find the global
    minimum (if the learning rate is not too large and you wait long enough). The
    partial derivatives of the cost function with regard to the *j*^(th) model parameter
    *θ*[*j*] are given by [Equation 4-18](#logistic_cost_function_partial_derivatives).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-18\. Logistic cost function partial derivatives
  id: totrans-287
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mstyle scriptlevel="0" displaystyle="true"><mfrac><mi>∂</mi>
    <mrow><mi>∂</mi><msub><mi>θ</mi> <mi>j</mi></msub></mrow></mfrac></mstyle> <mtext>J</mtext>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mstyle
    scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <mfenced separators="" open="(" close=")"><mi>σ</mi> <mrow><mo>(</mo> <msup><mi
    mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>-</mo>
    <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mfenced>
    <msubsup><mi>x</mi> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow></math>
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'This equation looks very much like [Equation 4-5](#mse_partial_derivatives):
    for each instance it computes the prediction error and multiplies it by the *j*^(th)
    feature value, and then it computes the average over all training instances. Once
    you have the gradient vector containing all the partial derivatives, you can use
    it in the batch gradient descent algorithm. That’s it: you now know how to train
    a logistic regression model. For stochastic GD you would take one instance at
    a time, and for mini-batch GD you would use a mini-batch at a time.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Decision Boundaries
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use the iris dataset to illustrate logistic regression. This is a famous
    dataset that contains the sepal and petal length and width of 150 iris flowers
    of three different species: *Iris setosa*, *Iris versicolor*, and *Iris virginica*
    (see [Figure 4-22](#iris_dataset_diagram)).'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用鸢尾花数据集来说明逻辑回归。这是一个包含150朵三种不同物种鸢尾花（*Iris setosa*、*Iris versicolor*和*Iris
    virginica*）的萼片和花瓣长度和宽度的著名数据集（参见[图4-22](#iris_dataset_diagram)）。
- en: '![mls3 0422](assets/mls3_0422.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0422](assets/mls3_0422.png)'
- en: Figure 4-22\. Flowers of three iris plant species⁠^([12](ch04.html#idm45720214766432))
  id: totrans-293
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-22。三种鸢尾植物物种的花朵⁠^([12](ch04.html#idm45720214766432))
- en: 'Let’s try to build a classifier to detect the *Iris virginica* type based only
    on the petal width feature. The first step is to load the data and take a quick
    peek:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试构建一个基于花瓣宽度特征的分类器来检测*Iris virginica*类型。第一步是加载数据并快速查看：
- en: '[PRE24]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next we’ll split the data and train a logistic regression model on the training
    set:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将拆分数据并在训练集上训练逻辑回归模型：
- en: '[PRE25]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Let’s look at the model’s estimated probabilities for flowers with petal widths
    varying from 0 cm to 3 cm ([Figure 4-23](#logistic_regression_plot)):⁠^([13](ch04.html#idm45720214606864))
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看模型对花朵的估计概率，这些花朵的花瓣宽度从0厘米到3厘米不等（参见[图4-23](#logistic_regression_plot)）：⁠^([13](ch04.html#idm45720214606864))
- en: '[PRE26]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![mls3 0423](assets/mls3_0423.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0423](assets/mls3_0423.png)'
- en: Figure 4-23\. Estimated probabilities and decision boundary
  id: totrans-301
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-23。估计的概率和决策边界
- en: 'The petal width of *Iris virginica* flowers (represented as triangles) ranges
    from 1.4 cm to 2.5 cm, while the other iris flowers (represented by squares) generally
    have a smaller petal width, ranging from 0.1 cm to 1.8 cm. Notice that there is
    a bit of overlap. Above about 2 cm the classifier is highly confident that the
    flower is an *Iris virginica* (it outputs a high probability for that class),
    while below 1 cm it is highly confident that it is not an *Iris virginica* (high
    probability for the “Not Iris virginica” class). In between these extremes, the
    classifier is unsure. However, if you ask it to predict the class (using the `predict()`
    method rather than the `predict_proba()` method), it will return whichever class
    is the most likely. Therefore, there is a *decision boundary* at around 1.6 cm
    where both probabilities are equal to 50%: if the petal width is greater than
    1.6 cm the classifier will predict that the flower is an *Iris virginica*, and
    otherwise it will predict that it is not (even if it is not very confident):'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '*Iris virginica*花朵的花瓣宽度（表示为三角形）范围从1.4厘米到2.5厘米，而其他鸢尾花（用方块表示）通常具有较小的花瓣宽度，范围从0.1厘米到1.8厘米。请注意，存在一些重叠。大约在2厘米以上，分类器非常确信花朵是*Iris
    virginica*（输出该类的高概率），而在1厘米以下，它非常确信它不是*Iris virginica*（“非Iris virginica”类的高概率）。在这两个极端之间，分类器不确定。但是，如果要求它预测类别（使用`predict()`方法而不是`predict_proba()`方法），它将返回最有可能的类别。因此，在大约1.6厘米处有一个*决策边界*，两个概率都等于50%：如果花瓣宽度大于1.6厘米，分类器将预测花朵是*Iris
    virginica*，否则它将预测它不是（即使它不太自信）：'
- en: '[PRE27]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[Figure 4-24](#logistic_regression_contour_plot) shows the same dataset, but
    this time displaying two features: petal width and length. Once trained, the logistic
    regression classifier can, based on these two features, estimate the probability
    that a new flower is an *Iris virginica*. The dashed line represents the points
    where the model estimates a 50% probability: this is the model’s decision boundary.
    Note that it is a linear boundary.⁠^([14](ch04.html#idm45720214369424)) Each parallel
    line represents the points where the model outputs a specific probability, from
    15% (bottom left) to 90% (top right). All the flowers beyond the top-right line
    have over 90% chance of being *Iris virginica*, according to the model.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-24](#logistic_regression_contour_plot)显示了相同的数据集，但这次显示了两个特征：花瓣宽度和长度。一旦训练完成，逻辑回归分类器可以根据这两个特征估计新花朵是*Iris
    virginica*的概率。虚线代表模型估计50%概率的点：这是模型的决策边界。请注意，这是一个线性边界。⁠^([14](ch04.html#idm45720214369424))
    每条平行线代表模型输出特定概率的点，从15%（左下角）到90%（右上角）。所有超过右上线的花朵根据模型有超过90%的概率是*Iris virginica*。'
- en: '![mls3 0424](assets/mls3_0424.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0424](assets/mls3_0424.png)'
- en: Figure 4-24\. Linear decision boundary
  id: totrans-306
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-24。线性决策边界
- en: Note
  id: totrans-307
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The hyperparameter controlling the regularization strength of a Scikit-Learn
    `LogisticRegression` model is not `alpha` (as in other linear models), but its
    inverse: `C`. The higher the value of `C`, the *less* the model is regularized.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 控制Scikit-Learn `LogisticRegression`模型正则化强度的超参数不是`alpha`（像其他线性模型一样），而是它的倒数：`C`。`C`值越高，模型的正则化就越*少*。
- en: Just like the other linear models, logistic regression models can be regularized
    using ℓ[1] or ℓ[2] penalties. Scikit-Learn actually adds an ℓ[2] penalty by default.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他线性模型一样，逻辑回归模型可以使用ℓ[1]或ℓ[2]惩罚进行正则化。Scikit-Learn实际上默认添加了ℓ[2]惩罚。
- en: Softmax Regression
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Softmax回归
- en: The logistic regression model can be generalized to support multiple classes
    directly, without having to train and combine multiple binary classifiers (as
    discussed in [Chapter 3](ch03.html#classification_chapter)). This is called *softmax
    regression*, or *multinomial logistic regression*.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型可以直接泛化为支持多类别，而无需训练和组合多个二元分类器（如[第3章](ch03.html#classification_chapter)中讨论的）。这称为*softmax回归*或*多项式逻辑回归*。
- en: 'The idea is simple: when given an instance **x**, the softmax regression model
    first computes a score *s*[*k*](**x**) for each class *k*, then estimates the
    probability of each class by applying the *softmax function* (also called the
    *normalized exponential*) to the scores. The equation to compute *s*[*k*](**x**)
    should look familiar, as it is just like the equation for linear regression prediction
    (see [Equation 4-19](#softmax_score_for_class_k)).'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法很简单：给定一个实例**x**，Softmax回归模型首先为每个类别*k*计算一个分数*s*[*k*](**x**)，然后通过应用*softmax函数*（也称为*归一化指数函数*）来估计每个类别的概率。计算*s*[*k*](**x**)的方程应该看起来很熟悉，因为它就像线性回归预测的方程（参见[方程4-19](#softmax_score_for_class_k)）。
- en: Equation 4-19\. Softmax score for class k
  id: totrans-313
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-19。类别k的Softmax分数
- en: <math display="block"><mrow><msub><mi>s</mi> <mi>k</mi></msub> <mrow><mo>(</mo>
    <mi mathvariant="bold">x</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi
    mathvariant="bold">θ</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow>
    <mo>⊺</mo></msup> <mi mathvariant="bold">x</mi></mrow></math>
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>s</mi> <mi>k</mi></msub> <mrow><mo>(</mo>
    <mi mathvariant="bold">x</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi
    mathvariant="bold">θ</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow>
    <mo>⊺</mo></msup> <mi mathvariant="bold">x</mi></mrow></math>
- en: Note that each class has its own dedicated parameter vector **θ**^((*k*)). All
    these vectors are typically stored as rows in a *parameter matrix* **Θ**.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 注意每个类别都有自己专用的参数向量**θ**^((*k*))。所有这些向量通常被存储为*参数矩阵* **Θ** 的行。
- en: Once you have computed the score of every class for the instance **x**, you
    can estimate the probability <math><msub><mover><mi>p</mi><mo>^</mo></mover><mi>k</mi></msub></math>
    that the instance belongs to class *k* by running the scores through the softmax
    function ([Equation 4-20](#softmax_function)). The function computes the exponential
    of every score, then normalizes them (dividing by the sum of all the exponentials).
    The scores are generally called logits or log-odds (although they are actually
    unnormalized log-odds).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你计算出每个类别对于实例**x**的得分，你可以通过将得分通过softmax函数（[方程4-20](#softmax_function)）来估计实例属于类别*k*的概率<math><msub><mover><mi>p</mi><mo>^</mo></mover><mi>k</mi></msub></math>。该函数计算每个得分的指数，然后对它们进行归一化（除以所有指数的和）。这些得分通常被称为对数几率或对数几率（尽管它们实际上是未归一化的对数几率）。
- en: Equation 4-20\. Softmax function
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-20\. Softmax函数
- en: <math display="block"><mrow><msub><mover accent="true"><mi>p</mi> <mo>^</mo></mover>
    <mi>k</mi></msub> <mo>=</mo> <mi>σ</mi> <msub><mfenced separators="" open="("
    close=")"><mi mathvariant="bold">s</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mfenced>
    <mi>k</mi></msub> <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mo
    form="prefix">exp</mo><mfenced separators="" open="(" close=")"><msub><mi>s</mi>
    <mi>k</mi></msub> <mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></mfenced></mrow>
    <mrow><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>K</mi></munderover>
    <mrow><mo form="prefix">exp</mo><mfenced separators="" open="(" close=")"><msub><mi>s</mi>
    <mi>j</mi></msub> <mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></mfenced></mrow></mrow></mfrac></mstyle></mrow></math>
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mover accent="true"><mi>p</mi> <mo>^</mo></mover>
    <mi>k</mi></msub> <mo>=</mo> <mi>σ</mi> <msub><mfenced separators="" open="("
    close=")"><mi mathvariant="bold">s</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mfenced>
    <mi>k</mi></msub> <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mo
    form="prefix">exp</mo><mfenced separators="" open="(" close=")"><msub><mi>s</mi>
    <mi>k</mi></msub> <mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></mfenced></mrow>
    <mrow><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>K</mi></munderover>
    <mrow><mo form="prefix">exp</mo><mfenced separators="" open="(" close=")"><msub><mi>s</mi>
    <mi>j</mi></msub> <mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></mfenced></mrow></mrow></mfrac></mstyle></mrow></math>
- en: 'In this equation:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*K* is the number of classes.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*K* 是类别的数量。'
- en: '**s**(**x**) is a vector containing the scores of each class for the instance
    **x**.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**s**(**x**)是包含实例**x**每个类别得分的向量。'
- en: '*σ*(**s**(**x**))[*k*] is the estimated probability that the instance **x**
    belongs to class *k*, given the scores of each class for that instance.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*σ*(**s**(**x**))[*k*]是实例**x**属于类别*k*的估计概率，给定该实例每个类别的得分。'
- en: Just like the logistic regression classifier, by default the softmax regression
    classifier predicts the class with the highest estimated probability (which is
    simply the class with the highest score), as shown in [Equation 4-21](#softmax_regression_classifier_prediction).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 就像逻辑回归分类器一样，默认情况下，softmax回归分类器预测具有最高估计概率的类别（即具有最高得分的类别），如[方程4-21](#softmax_regression_classifier_prediction)所示。
- en: Equation 4-21\. Softmax regression classifier prediction
  id: totrans-324
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-21\. Softmax回归分类器预测
- en: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <munder><mo form="prefix">argmax</mo> <mi>k</mi></munder> <mi>σ</mi>
    <msub><mfenced separators="" open="(" close=")"><mi mathvariant="bold">s</mi><mo>(</mo><mi
    mathvariant="bold">x</mi><mo>)</mo></mfenced> <mi>k</mi></msub> <mo>=</mo> <munder><mo
    form="prefix">argmax</mo> <mi>k</mi></munder> <msub><mi>s</mi> <mi>k</mi></msub>
    <mrow><mo>(</mo> <mi mathvariant="bold">x</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo
    form="prefix">argmax</mo> <mi>k</mi></munder> <mfenced separators="" open="("
    close=")"><msup><mrow><mo>(</mo><msup><mi mathvariant="bold">θ</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>⊺</mo></msup> <mi mathvariant="bold">x</mi></mfenced></mrow></math>
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <munder><mo form="prefix">argmax</mo> <mi>k</mi></munder> <mi>σ</mi>
    <msub><mfenced separators="" open="(" close=")"><mi mathvariant="bold">s</mi><mo>(</mo><mi
    mathvariant="bold">x</mi><mo>)</mo></mfenced> <mi>k</mi></msub> <mo>=</mo> <munder><mo
    form="prefix">argmax</mo> <mi>k</mi></munder> <msub><mi>s</mi> <mi k</mi></msub>
    <mrow><mo>(</mo> <mi mathvariant="bold">x</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo
    form="prefix">argmax</mo> <mi>k</mi></munder> <mfenced separators="" open="("
    close=")"><msup><mrow><mo>(</mo><msup><mi mathvariant="bold">θ</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>⊺</mo></msup> <mi mathvariant="bold">x</mi></mfenced></mrow></math>
- en: The *argmax* operator returns the value of a variable that maximizes a function.
    In this equation, it returns the value of *k* that maximizes the estimated probability
    *σ*(**s**(**x**))[*k*].
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '*argmax*运算符返回最大化函数的变量值。在这个方程中，它返回最大化估计概率*σ*(**s**(**x**))[*k*]的*k*值。'
- en: Tip
  id: totrans-327
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The softmax regression classifier predicts only one class at a time (i.e., it
    is multiclass, not multioutput), so it should be used only with mutually exclusive
    classes, such as different species of plants. You cannot use it to recognize multiple
    people in one picture.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: softmax回归分类器一次只预测一个类别（即它是多类别的，而不是多输出的），因此它只能用于具有互斥类别的情况，例如不同种类的植物。你不能用它来识别一张图片中的多个人。
- en: Now that you know how the model estimates probabilities and makes predictions,
    let’s take a look at training. The objective is to have a model that estimates
    a high probability for the target class (and consequently a low probability for
    the other classes). Minimizing the cost function shown in [Equation 4-22](#cross_entropy_cost_function),
    called the *cross entropy*, should lead to this objective because it penalizes
    the model when it estimates a low probability for a target class. Cross entropy
    is frequently used to measure how well a set of estimated class probabilities
    matches the target classes.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道模型如何估计概率并进行预测了，让我们来看看训练。目标是让模型估计目标类的概率很高（因此其他类的概率很低）。最小化[方程4-22](#cross_entropy_cost_function)中显示的成本函数，称为*交叉熵*，应该能够实现这个目标，因为当模型估计目标类的概率很低时，它会受到惩罚。交叉熵经常用来衡量一组估计的类别概率与目标类别的匹配程度。
- en: Equation 4-22\. Cross entropy cost function
  id: totrans-330
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-22. 交叉熵成本函数
- en: <math><mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">Θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msubsup><mi>y</mi><mi>k</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mi>log</mi><mfenced><msubsup><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mi>k</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mfenced></mrow></math>
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">Θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msubsup><mi>y</mi><mi>k</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mi>log</mi><mfenced><msubsup><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mi>k</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mfenced></mrow></math>
- en: In this equation, <math><msubsup><mi>y</mi><mi>k</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></math>
    is the target probability that the *i*^(th) instance belongs to class *k*. In
    general, it is either equal to 1 or 0, depending on whether the instance belongs
    to the class or not.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，<math><msubsup><mi>y</mi><mi>k</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></math>是第*i*个实例属于第*k*类的目标概率。一般来说，它要么等于1，要么等于0，取决于实例是否属于该类。
- en: Notice that when there are just two classes (*K* = 2), this cost function is
    equivalent to the logistic regression cost function (log loss; see [Equation 4-17](#logistic_regression_cost_function)).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当只有两类（*K* = 2）时，这个成本函数等同于逻辑回归成本函数（对数损失；参见[方程4-17](#logistic_regression_cost_function)）。
- en: The gradient vector of this cost function with regard to **θ**^((*k*)) is given
    by [Equation 4-23](#cross_entropy_gradient_vector_for_class_k).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 这个成本函数关于**θ**^((*k*))的梯度向量由[方程4-23](#cross_entropy_gradient_vector_for_class_k)给出。
- en: Equation 4-23\. Cross entropy gradient vector for class k
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-23. 类别k的交叉熵梯度向量
- en: <math display="block"><mrow><msub><mi>∇</mi> <msup><mi mathvariant="bold">θ</mi>
    <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">Θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <mrow><mfenced
    separators="" open="(" close=")"><msubsup><mover accent="true"><mi>p</mi> <mo>^</mo></mover>
    <mi>k</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup> <mo>-</mo> <msubsup><mi>y</mi>
    <mi>k</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mfenced> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mrow></math>
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>∇</mi> <msup><mi mathvariant="bold">θ</mi>
    <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">Θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <mrow><mfenced
    separators="" open="(" close=")"><msubsup><mover accent="true"><mi>p</mi> <mo>^</mo></mover>
    <mi>k</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup> <mo>-</mo> <msubsup><mi>y</mi>
    <mi>k</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mfenced> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mrow></math>
- en: Now you can compute the gradient vector for every class, then use gradient descent
    (or any other optimization algorithm) to find the parameter matrix **Θ** that
    minimizes the cost function.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以计算每个类别的梯度向量，然后使用梯度下降（或任何其他优化算法）来找到最小化成本函数的参数矩阵**Θ**。
- en: 'Let’s use softmax regression to classify the iris plants into all three classes.
    Scikit-Learn’s `LogisticRegression` classifier uses softmax regression automatically
    when you train it on more than two classes (assuming you use `solver="lbfgs"`,
    which is the default). It also applies ℓ[2] regularization by default, which you
    can control using the hyperparameter `C`, as mentioned earlier:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用softmax回归将鸢尾花分类为所有三类。当你在多于两类上训练Scikit-Learn的`LogisticRegression`分类器时，它会自动使用softmax回归（假设你使用`solver="lbfgs"`，这是默认值）。它还默认应用ℓ[2]正则化，你可以使用之前提到的超参数`C`来控制：
- en: '[PRE28]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'So the next time you find an iris with petals that are 5 cm long and 2 cm wide,
    you can ask your model to tell you what type of iris it is, and it will answer
    *Iris virginica* (class 2) with 96% probability (or *Iris versicolor* with 4%
    probability):'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '所以下次当你发现一朵花瓣长5厘米，宽2厘米的鸢尾花时，你可以让你的模型告诉你它是什么类型的鸢尾花，它会以96%的概率回答*Iris virginica*（第2类）（或以4%的概率回答*Iris
    versicolor*）:'
- en: '[PRE29]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[Figure 4-25](#softmax_regression_contour_plot) shows the resulting decision
    boundaries, represented by the background colors. Notice that the decision boundaries
    between any two classes are linear. The figure also shows the probabilities for
    the *Iris versicolor* class, represented by the curved lines (e.g., the line labeled
    with 0.30 represents the 30% probability boundary). Notice that the model can
    predict a class that has an estimated probability below 50%. For example, at the
    point where all decision boundaries meet, all classes have an equal estimated
    probability of 33%.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-25](#softmax_regression_contour_plot) 显示了由背景颜色表示的决策边界。请注意，任意两个类之间的决策边界是线性的。图中还显示了*Iris
    versicolor*类的概率，由曲线表示（例如，标有 0.30 的线表示 30% 概率边界）。请注意，模型可以预测估计概率低于 50% 的类。例如，在所有决策边界相交的点，所有类的估计概率均为
    33%。'
- en: '![mls3 0425](assets/mls3_0425.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0425](assets/mls3_0425.png)'
- en: Figure 4-25\. Softmax regression decision boundaries
  id: totrans-344
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-25\. Softmax 回归决策边界
- en: In this chapter, you learned various ways to train linear models, both for regression
    and for classification. You used a closed-form equation to solve linear regression,
    as well as gradient descent, and you learned how various penalties can be added
    to the cost function during training to regularize the model. Along the way, you
    also learned how to plot learning curves and analyze them, and how to implement
    early stopping. Finally, you learned how logistic regression and softmax regression
    work. We’ve opened up the first machine learning black boxes! In the next chapters
    we will open many more, starting with support vector machines.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了训练线性模型的各种方法，包括回归和分类。你使用闭式方程解决线性回归问题，以及梯度下降，并学习了在训练过程中如何向成本函数添加各种惩罚以对模型进行正则化。在此过程中，你还学习了如何绘制学习曲线并分析它们，以及如何实现早期停止。最后，你学习了逻辑回归和
    softmax 回归的工作原理。我们已经打开了第一个机器学习黑匣子！在接下来的章节中，我们将打开更多黑匣子，从支持向量机开始。
- en: Exercises
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Which linear regression training algorithm can you use if you have a training
    set with millions of features?
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有一个拥有数百万个特征的训练集，你可以使用哪种线性回归训练算法？
- en: Suppose the features in your training set have very different scales. Which
    algorithms might suffer from this, and how? What can you do about it?
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你的训练集中的特征具有非常不同的尺度。哪些算法可能会受到影响，以及如何受影响？你可以采取什么措施？
- en: Can gradient descent get stuck in a local minimum when training a logistic regression
    model?
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练逻辑回归模型时，梯度下降是否会陷入局部最小值？
- en: Do all gradient descent algorithms lead to the same model, provided you let
    them run long enough?
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果让所有梯度下降算法运行足够长的时间，它们会导致相同的模型吗？
- en: Suppose you use batch gradient descent and you plot the validation error at
    every epoch. If you notice that the validation error consistently goes up, what
    is likely going on? How can you fix this?
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你使用批量梯度下降，并在每个时期绘制验证误差。如果你注意到验证误差持续上升，可能出现了什么问题？如何解决？
- en: Is it a good idea to stop mini-batch gradient descent immediately when the validation
    error goes up?
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当验证误差上升时，立即停止小批量梯度下降是一个好主意吗？
- en: Which gradient descent algorithm (among those we discussed) will reach the vicinity
    of the optimal solution the fastest? Which will actually converge? How can you
    make the others converge as well?
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们讨论的梯度下降算法中，哪种算法会最快接近最优解？哪种实际上会收敛？如何使其他算法也收敛？
- en: Suppose you are using polynomial regression. You plot the learning curves and
    you notice that there is a large gap between the training error and the validation
    error. What is happening? What are three ways to solve this?
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你正在使用多项式回归。你绘制学习曲线并注意到训练误差和验证误差之间存在很大差距。发生了什么？有哪三种方法可以解决这个问题？
- en: Suppose you are using ridge regression and you notice that the training error
    and the validation error are almost equal and fairly high. Would you say that
    the model suffers from high bias or high variance? Should you increase the regularization
    hyperparameter *α* or reduce it?
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你正在使用岭回归，并且注意到训练误差和验证误差几乎相等且相当高。你会说模型存在高偏差还是高方差？你应该增加正则化超参数*α*还是减少它？
- en: 'Why would you want to use:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么要使用：
- en: Ridge regression instead of plain linear regression (i.e., without any regularization)?
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是否可以使用岭回归代替普通线性回归（即，没有任何正则化）？
- en: Lasso instead of ridge regression?
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是否可以使用 Lasso 代替岭回归？
- en: Elastic net instead of lasso regression?
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是否可以使用弹性网络代替 Lasso 回归？
- en: Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.
    Should you implement two logistic regression classifiers or one softmax regression
    classifier?
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你想要将图片分类为室内/室外和白天/黑夜。你应该实现两个逻辑回归分类器还是一个 softmax 回归分类器？
- en: Implement batch gradient descent with early stopping for softmax regression
    without using Scikit-Learn, only NumPy. Use it on a classification task such as
    the iris dataset.
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 NumPy 实现批量梯度下降并进行早期停止以进行 softmax 回归，而不使用 Scikit-Learn。将其应用于鸢尾花数据集等分类任务。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可在本章笔记本的末尾找到，网址为[*https://homl.info/colab3*](https://homl.info/colab3)。
- en: '^([1](ch04.html#idm45720217568672-marker)) A closed-form equation is only composed
    of a finite number of constants, variables, and standard operations: for example,
    *a* = sin(*b* – *c*). No infinite sums, no limits, no integrals, etc.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.html#idm45720217568672-marker)) 闭式方程仅由有限数量的常数、变量和标准操作组成：例如，*a* =
    sin(*b* – *c*)。没有无限求和、极限、积分等。
- en: ^([2](ch04.html#idm45720216856720-marker)) Technically speaking, its derivative
    is *Lipschitz continuous*.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.html#idm45720216856720-marker)) 从技术上讲，它的导数是*Lipschitz连续*的。
- en: ^([3](ch04.html#idm45720216853456-marker)) Since feature 1 is smaller, it takes
    a larger change in *θ*[1] to affect the cost function, which is why the bowl is
    elongated along the *θ*[1] axis.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.html#idm45720216853456-marker)) 由于特征 1 较小，改变*θ*[1]以影响成本函数需要更大的变化，这就是为什么碗沿着*θ*[1]轴被拉长的原因。
- en: ^([4](ch04.html#idm45720216763152-marker)) Eta (*η*) is the seventh letter of
    the Greek alphabet.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.html#idm45720216763152-marker)) Eta（*η*）是希腊字母表的第七个字母。
- en: ^([5](ch04.html#idm45720216235312-marker)) While the Normal equation can only
    perform linear regression, the gradient descent algorithms can be used to train
    many other models, as you’ll see.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch04.html#idm45720216235312-marker)) 而正规方程只能执行线性回归，梯度下降算法可以用来训练许多其他模型，您将会看到。
- en: ^([6](ch04.html#idm45720215648240-marker)) This notion of bias is not to be
    confused with the bias term of linear models.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch04.html#idm45720215648240-marker)) 这种偏差的概念不应与线性模型的偏差项混淆。
- en: ^([7](ch04.html#idm45720215617520-marker)) It is common to use the notation
    *J*(**θ**) for cost functions that don’t have a short name; I’ll often use this
    notation throughout the rest of this book. The context will make it clear which
    cost function is being discussed.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch04.html#idm45720215617520-marker)) 通常使用符号*J*(**θ**)表示没有简短名称的代价函数；在本书的其余部分中，我经常会使用这种符号。上下文将清楚地表明正在讨论哪个代价函数。
- en: ^([8](ch04.html#idm45720215599696-marker)) Norms are discussed in [Chapter 2](ch02.html#project_chapter).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch04.html#idm45720215599696-marker)) 范数在[第2章](ch02.html#project_chapter)中讨论。
- en: ^([9](ch04.html#idm45720215579520-marker)) A square matrix full of 0s except
    for 1s on the main diagonal (top left to bottom right).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch04.html#idm45720215579520-marker)) 一个全是0的方阵，除了主对角线（从左上到右下）上的1。
- en: ^([10](ch04.html#idm45720215546880-marker)) Alternatively, you can use the `Ridge`
    class with the `"sag"` solver. Stochastic average GD is a variant of stochastic
    GD. For more details, see the presentation [“Minimizing Finite Sums with the Stochastic
    Average Gradient Algorithm”](https://homl.info/12) by Mark Schmidt et al. from
    the University of British Columbia.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch04.html#idm45720215546880-marker)) 或者，您可以使用`Ridge`类与`"sag"`求解器。随机平均梯度下降是随机梯度下降的一种变体。有关更多详细信息，请参阅由不列颠哥伦比亚大学的Mark
    Schmidt等人提出的演示[“使用随机平均梯度算法最小化有限和”](https://homl.info/12)。
- en: ^([11](ch04.html#idm45720215341648-marker)) You can think of a subgradient vector
    at a nondifferentiable point as an intermediate vector between the gradient vectors
    around that point.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch04.html#idm45720215341648-marker)) 您可以将非可微点处的次梯度向量视为该点周围梯度向量之间的中间向量。
- en: ^([12](ch04.html#idm45720214766432-marker)) Photos reproduced from the corresponding
    Wikipedia pages. *Iris virginica* photo by Frank Mayfield ([Creative Commons BY-SA
    2.0](https://creativecommons.org/licenses/by-sa/2.0)), *Iris versicolor* photo
    by D. Gordon E. Robertson ([Creative Commons BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0)),
    *Iris setosa* photo public domain.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch04.html#idm45720214766432-marker)) 照片来源于相应的维基百科页面。*Iris virginica*照片由Frank
    Mayfield拍摄（[知识共享署名-相同方式共享 2.0](https://creativecommons.org/licenses/by-sa/2.0)），*Iris
    versicolor*照片由D. Gordon E. Robertson拍摄（[知识共享署名-相同方式共享 3.0](https://creativecommons.org/licenses/by-sa/3.0)），*Iris
    setosa*照片为公共领域。
- en: '^([13](ch04.html#idm45720214606864-marker)) NumPy’s `reshape()` function allows
    one dimension to be –1, which means “automatic”: the value is inferred from the
    length of the array and the remaining dimensions.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch04.html#idm45720214606864-marker)) NumPy的`reshape()`函数允许一个维度为-1，表示“自动”：该值是从数组的长度和剩余维度推断出来的。
- en: ^([14](ch04.html#idm45720214369424-marker)) It is the set of points **x** such
    that *θ*[0] + *θ*[1]*x*[1] + *θ*[2]*x*[2] = 0, which defines a straight line.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch04.html#idm45720214369424-marker)) 它是一组点**x**，使得*θ*[0] + *θ*[1]*x*[1]
    + *θ*[2]*x*[2] = 0，这定义了一条直线。
