- en: Chapter 4\. Training Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far we have treated machine learning models and their training algorithms
    mostly like black boxes. If you went through some of the exercises in the previous
    chapters, you may have been surprised by how much you can get done without knowing
    anything about what’s under the hood: you optimized a regression system, you improved
    a digit image classifier, and you even built a spam classifier from scratch, all
    without knowing how they actually work. Indeed, in many situations you don’t really
    need to know the implementation details.'
  prefs: []
  type: TYPE_NORMAL
- en: However, having a good understanding of how things work can help you quickly
    home in on the appropriate model, the right training algorithm to use, and a good
    set of hyperparameters for your task. Understanding what’s under the hood will
    also help you debug issues and perform error analysis more efficiently. Lastly,
    most of the topics discussed in this chapter will be essential in understanding,
    building, and training neural networks (discussed in [Part II](part02.html#neural_nets_part)
    of this book).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will start by looking at the linear regression model, one
    of the simplest models there is. We will discuss two very different ways to train
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: Using a “closed-form” equation⁠^([1](ch04.html#idm45720217568672)) that directly
    computes the model parameters that best fit the model to the training set (i.e.,
    the model parameters that minimize the cost function over the training set).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using an iterative optimization approach called gradient descent (GD) that
    gradually tweaks the model parameters to minimize the cost function over the training
    set, eventually converging to the same set of parameters as the first method.
    We will look at a few variants of gradient descent that we will use again and
    again when we study neural networks in [Part II](part02.html#neural_nets_part):
    batch GD, mini-batch GD, and stochastic GD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next we will look at polynomial regression, a more complex model that can fit
    nonlinear datasets. Since this model has more parameters than linear regression,
    it is more prone to overfitting the training data. We will explore how to detect
    whether or not this is the case using learning curves, and then we will look at
    several regularization techniques that can reduce the risk of overfitting the
    training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will examine two more models that are commonly used for classification
    tasks: logistic regression and softmax regression.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There will be quite a few math equations in this chapter, using basic notions
    of linear algebra and calculus. To understand these equations, you will need to
    know what vectors and matrices are; how to transpose them, multiply them, and
    inverse them; and what partial derivatives are. If you are unfamiliar with these
    concepts, please go through the linear algebra and calculus introductory tutorials
    available as Jupyter notebooks in the [online supplemental material](https://github.com/ageron/handson-ml3).
    For those who are truly allergic to mathematics, you should still go through this
    chapter and simply skip the equations; hopefully, the text will be sufficient
    to help you understand most of the concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 1](ch01.html#landscape_chapter) we looked at a simple regression
    model of life satisfaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '*life_satisfaction* = *θ*[0] + *θ*[1] × *GDP_per_capita*'
  prefs: []
  type: TYPE_NORMAL
- en: This model is just a linear function of the input feature `GDP_per_capita`.
    *θ*[0] and *θ*[1] are the model’s parameters.
  prefs: []
  type: TYPE_NORMAL
- en: More generally, a linear model makes a prediction by simply computing a weighted
    sum of the input features, plus a constant called the *bias term* (also called
    the *intercept term*), as shown in [Equation 4-1](#equation_four_one).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-1\. Linear regression model prediction
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>θ</mi>
    <mi>n</mi></msub> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ŷ* is the predicted value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n* is the number of features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[*i*] is the *i*^(th) feature value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*θ*[*j*] is the *j*^(th) model parameter, including the bias term *θ*[0] and
    the feature weights *θ*[1], *θ*[2], ⋯, *θ*[*n*].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be written much more concisely using a vectorized form, as shown in
    [Equation 4-2](#linear_regression_prediction_vectorized_equation).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-2\. Linear regression model prediction (vectorized form)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msub><mi>h</mi><mi
    mathvariant="bold">θ</mi></msub><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo><mo>=</mo><mi
    mathvariant="bold">θ</mi><mo>·</mo><mi mathvariant="bold">x</mi></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[**θ**] is the hypothesis function, using the model parameters **θ**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**θ** is the model’s *parameter vector*, containing the bias term *θ*[0] and
    the feature weights *θ*[1] to *θ*[*n*].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**x** is the instance’s *feature vector*, containing *x*[0] to *x*[*n*], with
    *x*[0] always equal to 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**θ** · **x** is the dot product of the vectors **θ** and **x**, which is equal
    to *θ*[0]*x*[0] + *θ*[1]*x*[1] + *θ*[2]*x*[2] + ... + *θ*[*n*]*x*[*n*].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In machine learning, vectors are often represented as *column vectors*, which
    are 2D arrays with a single column. If **θ** and **x** are column vectors, then
    the prediction is <math><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup><mi mathvariant="bold">x</mi></math>,
    where <math><msup><mi mathvariant="bold">θ</mi><mo>⊺</mo></msup></math> is the
    *transpose* of **θ** (a row vector instead of a column vector) and <math><msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup><mi mathvariant="bold">x</mi></math>
    is the matrix multiplication of <math><msup><mi mathvariant="bold">θ</mi><mo>⊺</mo></msup></math>
    and **x**. It is of course the same prediction, except that it is now represented
    as a single-cell matrix rather than a scalar value. In this book I will use this
    notation to avoid switching between dot products and matrix multiplications.
  prefs: []
  type: TYPE_NORMAL
- en: OK, that’s the linear regression model—but how do we train it? Well, recall
    that training a model means setting its parameters so that the model best fits
    the training set. For this purpose, we first need a measure of how well (or poorly)
    the model fits the training data. In [Chapter 2](ch02.html#project_chapter) we
    saw that the most common performance measure of a regression model is the root
    mean square error ([Equation 2-1](ch02.html#rmse_equation)). Therefore, to train
    a linear regression model, we need to find the value of **θ** that minimizes the
    RMSE. In practice, it is simpler to minimize the mean squared error (MSE) than
    the RMSE, and it leads to the same result (because the value that minimizes a
    positive function also minimizes its square root).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Learning algorithms will often optimize a different loss function during training
    than the performance measure used to evaluate the final model. This is generally
    because the function is easier to optimize and/or because it has extra terms needed
    during training only (e.g., for regularization). A good performance metric is
    as close as possible to the final business objective. A good training loss is
    easy to optimize and strongly correlated with the metric. For example, classifiers
    are often trained using a cost function such as the log loss (as you will see
    later in this chapter) but evaluated using precision/recall. The log loss is easy
    to minimize, and doing so will usually improve precision/recall.
  prefs: []
  type: TYPE_NORMAL
- en: The MSE of a linear regression hypothesis *h*[**θ**] on a training set **X**
    is calculated using [Equation 4-3](#mse_cost_function).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-3\. MSE cost function for a linear regression model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi>
    <mo>,</mo> <msub><mi>h</mi> <mi mathvariant="bold">θ</mi></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msup><mi>y</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Most of these notations were presented in [Chapter 2](ch02.html#project_chapter)
    (see [“Notations”](ch02.html#notations)). The only difference is that we write
    *h*[**θ**] instead of just *h* to make it clear that the model is parametrized
    by the vector **θ**. To simplify notations, we will just write MSE(**θ**) instead
    of MSE(**X**, *h*[**θ**]).
  prefs: []
  type: TYPE_NORMAL
- en: The Normal Equation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To find the value of **θ** that minimizes the MSE, there exists a *closed-form
    solution*—in other words, a mathematical equation that gives the result directly.
    This is called the *Normal equation* ([Equation 4-4](#equation_four_four)).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-4\. Normal equation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover accent="true"><mi mathvariant="bold">θ</mi>
    <mo>^</mo></mover> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">X</mi>
    <mo>⊺</mo></msup> <mi mathvariant="bold">X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>  <msup><mi
    mathvariant="bold">X</mi> <mo>⊺</mo></msup>  <mi mathvariant="bold">y</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math><mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover></math>
    is the value of **θ** that minimizes the cost function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**y** is the vector of target values containing *y*^((1)) to *y*^((*m*)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s generate some linear-looking data to test this equation on ([Figure 4-1](#generated_data_plot)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0401](assets/mls3_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. A randomly generated linear dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now let’s compute <math><mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover></math>
    using the Normal equation. We will use the `inv()` function from NumPy’s linear
    algebra module (`np.linalg`) to compute the inverse of a matrix, and the `dot()`
    method for matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `@` operator performs matrix multiplication. If `A` and `B` are NumPy arrays,
    then `A @ B` is equivalent to `np.matmul(A, B)`. Many other libraries, like TensorFlow,
    PyTorch, and JAX, support the `@` operator as well. However, you cannot use `@`
    on pure Python arrays (i.e., lists of lists).
  prefs: []
  type: TYPE_NORMAL
- en: 'The function that we used to generate the data is *y* = 4 + 3*x*[1] + Gaussian
    noise. Let’s see what the equation found:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We would have hoped for *θ*[0] = 4 and *θ*[1] = 3 instead of *θ*[0] = 4.215
    and *θ*[1] = 2.770\. Close enough, but the noise made it impossible to recover
    the exact parameters of the original function. The smaller and noisier the dataset,
    the harder it gets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can make predictions using <math><mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover></math>:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot this model’s predictions ([Figure 4-2](#linear_model_predictions_plot)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0402](assets/mls3_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. Linear regression model predictions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Performing linear regression using Scikit-Learn is relatively straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that Scikit-Learn separates the bias term (`intercept_`) from the feature
    weights (`coef_`). The `LinearRegression` class is based on the `scipy.linalg.lstsq()`
    function (the name stands for “least squares”), which you could call directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This function computes <math><mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover><mo>=</mo><msup><mi
    mathvariant="bold">X</mi><mo>+</mo></msup><mi mathvariant="bold">y</mi></math>,
    where <math><msup><mi mathvariant="bold">X</mi><mo>+</mo></msup></math> is the
    *pseudoinverse* of **X** (specifically, the Moore–Penrose inverse). You can use
    `np.linalg.pinv()` to compute the pseudoinverse directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The pseudoinverse itself is computed using a standard matrix factorization
    technique called *singular value decomposition* (SVD) that can decompose the training
    set matrix **X** into the matrix multiplication of three matrices **U** **Σ**
    **V**^⊺ (see `numpy.linalg.svd()`). The pseudoinverse is computed as <math><msup><mi
    mathvariant="bold">X</mi><mo>+</mo></msup><mo>=</mo><mi mathvariant="bold">V</mi><msup><mi
    mathvariant="bold">Σ</mi><mo>+</mo></msup><msup><mi mathvariant="bold">U</mi><mo>⊺</mo></msup></math>.
    To compute the matrix <math><msup><mi mathvariant="bold">Σ</mi><mo>+</mo></msup></math>,
    the algorithm takes **Σ** and sets to zero all values smaller than a tiny threshold
    value, then it replaces all the nonzero values with their inverse, and finally
    it transposes the resulting matrix. This approach is more efficient than computing
    the Normal equation, plus it handles edge cases nicely: indeed, the Normal equation
    may not work if the matrix **X**^⊺**X** is not invertible (i.e., singular), such
    as if *m* < *n* or if some features are redundant, but the pseudoinverse is always
    defined.'
  prefs: []
  type: TYPE_NORMAL
- en: Computational Complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Normal equation computes the inverse of **X**^⊺ **X**, which is an (*n*
    + 1) × (*n* + 1) matrix (where *n* is the number of features). The *computational
    complexity* of inverting such a matrix is typically about *O*(*n*^(2.4)) to *O*(*n*³),
    depending on the implementation. In other words, if you double the number of features,
    you multiply the computation time by roughly 2^(2.4) = 5.3 to 2³ = 8.
  prefs: []
  type: TYPE_NORMAL
- en: The SVD approach used by Scikit-Learn’s `LinearRegression` class is about *O*(*n*²).
    If you double the number of features, you multiply the computation time by roughly
    4.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Both the Normal equation and the SVD approach get very slow when the number
    of features grows large (e.g., 100,000). On the positive side, both are linear
    with regard to the number of instances in the training set (they are *O*(*m*)),
    so they handle large training sets efficiently, provided they can fit in memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, once you have trained your linear regression model (using the Normal
    equation or any other algorithm), predictions are very fast: the computational
    complexity is linear with regard to both the number of instances you want to make
    predictions on and the number of features. In other words, making predictions
    on twice as many instances (or twice as many features) will take roughly twice
    as much time.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we will look at a very different way to train a linear regression model,
    which is better suited for cases where there are a large number of features or
    too many training instances to fit in memory.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Gradient descent* is a generic optimization algorithm capable of finding optimal
    solutions to a wide range of problems. The general idea of gradient descent is
    to tweak parameters iteratively in order to minimize a cost function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you are lost in the mountains in a dense fog, and you can only feel
    the slope of the ground below your feet. A good strategy to get to the bottom
    of the valley quickly is to go downhill in the direction of the steepest slope.
    This is exactly what gradient descent does: it measures the local gradient of
    the error function with regard to the parameter vector **θ**, and it goes in the
    direction of descending gradient. Once the gradient is zero, you have reached
    a minimum!'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, you start by filling **θ** with random values (this is called *random
    initialization*). Then you improve it gradually, taking one baby step at a time,
    each step attempting to decrease the cost function (e.g., the MSE), until the
    algorithm *converges* to a minimum (see [Figure 4-3](#gradient_descent_diagram)).
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0403](assets/mls3_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. In this depiction of gradient descent, the model parameters are
    initialized randomly and get tweaked repeatedly to minimize the cost function;
    the learning step size is proportional to the slope of the cost function, so the
    steps gradually get smaller as the cost approaches the minimum
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An important parameter in gradient descent is the size of the steps, determined
    by the *learning rate* hyperparameter. If the learning rate is too small, then
    the algorithm will have to go through many iterations to converge, which will
    take a long time (see [Figure 4-4](#small_learning_rate_diagram)).
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0404](assets/mls3_0404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. Learning rate too small
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: On the other hand, if the learning rate is too high, you might jump across the
    valley and end up on the other side, possibly even higher up than you were before.
    This might make the algorithm diverge, with larger and larger values, failing
    to find a good solution (see [Figure 4-5](#large_learning_rate_diagram)).
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0405](assets/mls3_0405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. Learning rate too high
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Additionally, not all cost functions look like nice, regular bowls. There may
    be holes, ridges, plateaus, and all sorts of irregular terrain, making convergence
    to the minimum difficult. [Figure 4-6](#gradient_descent_pitfalls_diagram) shows
    the two main challenges with gradient descent. If the random initialization starts
    the algorithm on the left, then it will converge to a *local minimum*, which is
    not as good as the *global minimum*. If it starts on the right, then it will take
    a very long time to cross the plateau. And if you stop too early, you will never
    reach the global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0406](assets/mls3_0406.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. Gradient descent pitfalls
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Fortunately, the MSE cost function for a linear regression model happens to
    be a *convex function*, which means that if you pick any two points on the curve,
    the line segment joining them is never below the curve. This implies that there
    are no local minima, just one global minimum. It is also a continuous function
    with a slope that never changes abruptly.⁠^([2](ch04.html#idm45720216856720))
    These two facts have a great consequence: gradient descent is guaranteed to approach
    arbitrarily closely the global minimum (if you wait long enough and if the learning
    rate is not too high).'
  prefs: []
  type: TYPE_NORMAL
- en: While the cost function has the shape of a bowl, it can be an elongated bowl
    if the features have very different scales. [Figure 4-7](#elongated_bowl_diagram)
    shows gradient descent on a training set where features 1 and 2 have the same
    scale (on the left), and on a training set where feature 1 has much smaller values
    than feature 2 (on the right).⁠^([3](ch04.html#idm45720216853456))
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0407](assets/mls3_0407.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-7\. Gradient descent with (left) and without (right) feature scaling
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, on the left the gradient descent algorithm goes straight toward
    the minimum, thereby reaching it quickly, whereas on the right it first goes in
    a direction almost orthogonal to the direction of the global minimum, and it ends
    with a long march down an almost flat valley. It will eventually reach the minimum,
    but it will take a long time.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When using gradient descent, you should ensure that all features have a similar
    scale (e.g., using Scikit-Learn’s `StandardScaler` class), or else it will take
    much longer to converge.
  prefs: []
  type: TYPE_NORMAL
- en: 'This diagram also illustrates the fact that training a model means searching
    for a combination of model parameters that minimizes a cost function (over the
    training set). It is a search in the model’s *parameter space*. The more parameters
    a model has, the more dimensions this space has, and the harder the search is:
    searching for a needle in a 300-dimensional haystack is much trickier than in
    3 dimensions. Fortunately, since the cost function is convex in the case of linear
    regression, the needle is simply at the bottom of the bowl.'
  prefs: []
  type: TYPE_NORMAL
- en: Batch Gradient Descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To implement gradient descent, you need to compute the gradient of the cost
    function with regard to each model parameter *θ*[*j*]. In other words, you need
    to calculate how much the cost function will change if you change *θ*[*j*] just
    a little bit. This is called a *partial derivative*. It is like asking, “What
    is the slope of the mountain under my feet if I face east”? and then asking the
    same question facing north (and so on for all other dimensions, if you can imagine
    a universe with more than three dimensions). [Equation 4-5](#mse_partial_derivatives)
    computes the partial derivative of the MSE with regard to parameter *θ*[*j*],
    noted ∂ MSE(**θ**) / ∂θ[*j*].
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-5\. Partial derivatives of the cost function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mstyle scriptlevel="0" displaystyle="true"><mfrac><mi>∂</mi>
    <mrow><mi>∂</mi><msub><mi>θ</mi> <mi>j</mi></msub></mrow></mfrac></mstyle> <mtext>MSE</mtext>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mstyle
    scriptlevel="0" displaystyle="true"><mfrac><mn>2</mn> <mi>m</mi></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <mrow><mo>(</mo> <msup><mi mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo>
    <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow>
    <msubsup><mi>x</mi> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Instead of computing these partial derivatives individually, you can use [Equation
    4-6](#mse_gradient_vector) to compute them all in one go. The gradient vector,
    noted ∇[**θ**]MSE(**θ**), contains all the partial derivatives of the cost function
    (one for each model parameter).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-6\. Gradient vector of the cost function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>∇</mi> <mi mathvariant="bold">θ</mi></msub>
    <mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mfrac><mi>∂</mi>
    <mrow><mi>∂</mi><msub><mi>θ</mi> <mn>0</mn></msub></mrow></mfrac> <mtext>MSE</mtext>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd><mrow><mfrac><mi>∂</mi> <mrow><mi>∂</mi><msub><mi>θ</mi> <mn>1</mn></msub></mrow></mfrac>
    <mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mrow><mfrac><mi>∂</mi> <mrow><mi>∂</mi><msub><mi>θ</mi>
    <mi>n</mi></msub></mrow></mfrac> <mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi>
    <mo>)</mo></mrow></mrow></mtd></mtr></mtable></mfenced> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>2</mn> <mi>m</mi></mfrac></mstyle> <msup><mi mathvariant="bold">X</mi>
    <mo>⊺</mo></msup> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi> <mi mathvariant="bold">θ</mi>
    <mo>-</mo> <mi mathvariant="bold">y</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Notice that this formula involves calculations over the full training set **X**,
    at each gradient descent step! This is why the algorithm is called *batch gradient
    descent*: it uses the whole batch of training data at every step (actually, *full
    gradient descent* would probably be a better name). As a result, it is terribly
    slow on very large training sets (we will look at some much faster gradient descent
    algorithms shortly). However, gradient descent scales well with the number of
    features; training a linear regression model when there are hundreds of thousands
    of features is much faster using gradient descent than using the Normal equation
    or SVD decomposition.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have the gradient vector, which points uphill, just go in the opposite
    direction to go downhill. This means subtracting ∇[**θ**]MSE(**θ**) from **θ**.
    This is where the learning rate *η* comes into play:⁠^([4](ch04.html#idm45720216763152))
    multiply the gradient vector by *η* to determine the size of the downhill step
    ([Equation 4-7](#gradient_descent_step)).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-7\. Gradient descent step
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math><msup><mi mathvariant="bold">θ</mi><mrow><mo>(</mo><mtext>next step</mtext><mo>)</mo></mrow></msup><mo>=</mo><mi
    mathvariant="bold">θ</mi><mo>-</mo><mi>η</mi><msub><mo>∇</mo><mi mathvariant="bold">θ</mi></msub><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a quick implementation of this algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'That wasn’t too hard! Each iteration over the training set is called an *epoch*.
    Let’s look at the resulting `theta`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Hey, that’s exactly what the Normal equation found! Gradient descent worked
    perfectly. But what if you had used a different learning rate (`eta`)? [Figure 4-8](#gradient_descent_plot)
    shows the first 20 steps of gradient descent using three different learning rates.
    The line at the bottom of each plot represents the random starting point, then
    each epoch is represented by a darker and darker line.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0408](assets/mls3_0408.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-8\. Gradient descent with various learning rates
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'On the left, the learning rate is too low: the algorithm will eventually reach
    the solution, but it will take a long time. In the middle, the learning rate looks
    pretty good: in just a few epochs, it has already converged to the solution. On
    the right, the learning rate is too high: the algorithm diverges, jumping all
    over the place and actually getting further and further away from the solution
    at every step.'
  prefs: []
  type: TYPE_NORMAL
- en: To find a good learning rate, you can use grid search (see [Chapter 2](ch02.html#project_chapter)).
    However, you may want to limit the number of epochs so that grid search can eliminate
    models that take too long to converge.
  prefs: []
  type: TYPE_NORMAL
- en: You may wonder how to set the number of epochs. If it is too low, you will still
    be far away from the optimal solution when the algorithm stops; but if it is too
    high, you will waste time while the model parameters do not change anymore. A
    simple solution is to set a very large number of epochs but to interrupt the algorithm
    when the gradient vector becomes tiny—that is, when its norm becomes smaller than
    a tiny number *ϵ* (called the *tolerance*)—because this happens when gradient
    descent has (almost) reached the minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main problem with batch gradient descent is the fact that it uses the whole
    training set to compute the gradients at every step, which makes it very slow
    when the training set is large. At the opposite extreme, *stochastic gradient
    descent* picks a random instance in the training set at every step and computes
    the gradients based only on that single instance. Obviously, working on a single
    instance at a time makes the algorithm much faster because it has very little
    data to manipulate at every iteration. It also makes it possible to train on huge
    training sets, since only one instance needs to be in memory at each iteration
    (stochastic GD can be implemented as an out-of-core algorithm; see [Chapter 1](ch01.html#landscape_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, due to its stochastic (i.e., random) nature, this algorithm
    is much less regular than batch gradient descent: instead of gently decreasing
    until it reaches the minimum, the cost function will bounce up and down, decreasing
    only on average. Over time it will end up very close to the minimum, but once
    it gets there it will continue to bounce around, never settling down (see [Figure 4-9](#sgd_random_walk_diagram)).
    Once the algorithm stops, the final parameter values will be good, but not optimal.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0409](assets/mls3_0409.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-9\. With stochastic gradient descent, each training step is much faster
    but also much more stochastic than when using batch gradient descent
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When the cost function is very irregular (as in [Figure 4-6](#gradient_descent_pitfalls_diagram)),
    this can actually help the algorithm jump out of local minima, so stochastic gradient
    descent has a better chance of finding the global minimum than batch gradient
    descent does.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, randomness is good to escape from local optima, but bad because it
    means that the algorithm can never settle at the minimum. One solution to this
    dilemma is to gradually reduce the learning rate. The steps start out large (which
    helps make quick progress and escape local minima), then get smaller and smaller,
    allowing the algorithm to settle at the global minimum. This process is akin to
    *simulated annealing*, an algorithm inspired by the process in metallurgy of annealing,
    where molten metal is slowly cooled down. The function that determines the learning
    rate at each iteration is called the *learning schedule*. If the learning rate
    is reduced too quickly, you may get stuck in a local minimum, or even end up frozen
    halfway to the minimum. If the learning rate is reduced too slowly, you may jump
    around the minimum for a long time and end up with a suboptimal solution if you
    halt training too early.
  prefs: []
  type: TYPE_NORMAL
- en: 'This code implements stochastic gradient descent using a simple learning schedule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'By convention we iterate by rounds of *m* iterations; each round is called
    an *epoch*, as earlier. While the batch gradient descent code iterated 1,000 times
    through the whole training set, this code goes through the training set only 50
    times and reaches a pretty good solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-10](#sgd_plot) shows the first 20 steps of training (notice how irregular
    the steps are).'
  prefs: []
  type: TYPE_NORMAL
- en: Note that since instances are picked randomly, some instances may be picked
    several times per epoch, while others may not be picked at all. If you want to
    be sure that the algorithm goes through every instance at each epoch, another
    approach is to shuffle the training set (making sure to shuffle the input features
    and the labels jointly), then go through it instance by instance, then shuffle
    it again, and so on. However, this approach is more complex, and it generally
    does not improve the result.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0410](assets/mls3_0410.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-10\. The first 20 steps of stochastic gradient descent
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When using stochastic gradient descent, the training instances must be independent
    and identically distributed (IID) to ensure that the parameters get pulled toward
    the global optimum, on average. A simple way to ensure this is to shuffle the
    instances during training (e.g., pick each instance randomly, or shuffle the training
    set at the beginning of each epoch). If you do not shuffle the instances—for example,
    if the instances are sorted by label—then SGD will start by optimizing for one
    label, then the next, and so on, and it will not settle close to the global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform linear regression using stochastic GD with Scikit-Learn, you can
    use the `SGDRegressor` class, which defaults to optimizing the MSE cost function.
    The following code runs for maximum 1,000 epochs (`max_iter`) or until the loss
    drops by less than 10^(–5) (`tol`) during 100 epochs (`n_iter_no_change`). It
    starts with a learning rate of 0.01 (`eta0`), using the default learning schedule
    (different from the one we used). Lastly, it does not use any regularization (`penalty=None`;
    more details on this shortly):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, you find a solution quite close to the one returned by the Normal
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'All Scikit-Learn estimators can be trained using the `fit()` method, but some
    estimators also have a `partial_fit()` method that you can call to run a single
    round of training on one or more instances (it ignores hyperparameters like `max_iter`
    or `tol`). Repeatedly calling `partial_fit()` will gradually train the model.
    This is useful when you need more control over the training process. Other models
    have a `warm_start` hyperparameter instead (and some have both): if you set `warm_start=True`,
    calling the `fit()` method on a trained model will not reset the model; it will
    just continue training where it left off, respecting hyperparameters like `max_iter`
    and `tol`. Note that `fit()` resets the iteration counter used by the learning
    schedule, while `partial_fit()` does not.'
  prefs: []
  type: TYPE_NORMAL
- en: Mini-Batch Gradient Descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The last gradient descent algorithm we will look at is called *mini-batch gradient
    descent*. It is straightforward once you know batch and stochastic gradient descent:
    at each step, instead of computing the gradients based on the full training set
    (as in batch GD) or based on just one instance (as in stochastic GD), mini-batch
    GD computes the gradients on small random sets of instances called *mini-batches*.
    The main advantage of mini-batch GD over stochastic GD is that you can get a performance
    boost from hardware optimization of matrix operations, especially when using GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm’s progress in parameter space is less erratic than with stochastic
    GD, especially with fairly large mini-batches. As a result, mini-batch GD will
    end up walking around a bit closer to the minimum than stochastic GD—but it may
    be harder for it to escape from local minima (in the case of problems that suffer
    from local minima, unlike linear regression with the MSE cost function). [Figure 4-11](#gradient_descent_paths_plot)
    shows the paths taken by the three gradient descent algorithms in parameter space
    during training. They all end up near the minimum, but batch GD’s path actually
    stops at the minimum, while both stochastic GD and mini-batch GD continue to walk
    around. However, don’t forget that batch GD takes a lot of time to take each step,
    and stochastic GD and mini-batch GD would also reach the minimum if you used a
    good learning schedule.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0411](assets/mls3_0411.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-11\. Gradient descent paths in parameter space
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Table 4-1](#linear_regression_algorithm_comparison) compares the algorithms
    we’ve discussed so far for linear regression⁠^([5](ch04.html#idm45720216235312))
    (recall that *m* is the number of training instances and *n* is the number of
    features).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-1\. Comparison of algorithms for linear regression
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Large *m* | Out-of-core support | Large *n* | Hyperparams | Scaling
    required | Scikit-Learn |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Normal equation | Fast | No | Slow | 0 | No | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| SVD | Fast | No | Slow | 0 | No | `LinearRegression` |'
  prefs: []
  type: TYPE_TB
- en: '| Batch GD | Slow | No | Fast | 2 | Yes | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Stochastic GD | Fast | Yes | Fast | ≥2 | Yes | `SGDRegressor` |'
  prefs: []
  type: TYPE_TB
- en: '| Mini-batch GD | Fast | Yes | Fast | ≥2 | Yes | N/A |'
  prefs: []
  type: TYPE_TB
- en: 'There is almost no difference after training: all these algorithms end up with
    very similar models and make predictions in exactly the same way.'
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What if your data is more complex than a straight line? Surprisingly, you can
    use a linear model to fit nonlinear data. A simple way to do this is to add powers
    of each feature as new features, then train a linear model on this extended set
    of features. This technique is called *polynomial regression*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example. First, we’ll generate some nonlinear data (see [Figure 4-12](#quadratic_data_plot)),
    based on a simple *quadratic equation*—that’s an equation of the form *y* = *ax*²
    + *bx* + *c*—plus some noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0412](assets/mls3_0412.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-12\. Generated nonlinear and noisy dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Clearly, a straight line will never fit this data properly. So let’s use Scikit-Learn’s
    `PolynomialFeatures` class to transform our training data, adding the square (second-degree
    polynomial) of each feature in the training set as a new feature (in this case
    there is just one feature):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`X_poly` now contains the original feature of `X` plus the square of this feature.
    Now we can fit a `LinearRegression` model to this extended training data ([Figure 4-13](#quadratic_predictions_plot)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0413](assets/mls3_0413.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-13\. Polynomial regression model predictions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Not bad: the model estimates <math><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <mn>0.56</mn> <msup><mrow><msub><mi>x</mi> <mn>1</mn></msub></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>0.93</mn> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <mn>1.78</mn></mrow></math> when in fact the original function was
    <math><mrow><mi>y</mi> <mo>=</mo> <mn>0.5</mn> <msup><mrow><msub><mi>x</mi> <mn>1</mn></msub></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>1.0</mn> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo>
    <mn>2.0</mn> <mo>+</mo> <mtext>Gaussian noise</mtext></mrow></math> .'
  prefs: []
  type: TYPE_NORMAL
- en: Note that when there are multiple features, polynomial regression is capable
    of finding relationships between features, which is something a plain linear regression
    model cannot do. This is made possible by the fact that `PolynomialFeatures` also
    adds all combinations of features up to the given degree. For example, if there
    were two features *a* and *b*, `PolynomialFeatures` with `degree=3` would not
    only add the features *a*², *a*³, *b*², and *b*³, but also the combinations *ab*,
    *a*²*b*, and *ab*².
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`PolynomialFeatures(degree=*d*)` transforms an array containing *n* features
    into an array containing (*n* + *d*)! / *d*!*n*! features, where *n*! is the *factorial*
    of *n*, equal to 1 × 2 × 3 × ⋯ × *n*. Beware of the combinatorial explosion of
    the number of features!'
  prefs: []
  type: TYPE_NORMAL
- en: Learning Curves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you perform high-degree polynomial regression, you will likely fit the training
    data much better than with plain linear regression. For example, [Figure 4-14](#high_degree_polynomials_plot)
    applies a 300-degree polynomial model to the preceding training data, and compares
    the result with a pure linear model and a quadratic model (second-degree polynomial).
    Notice how the 300-degree polynomial model wiggles around to get as close as possible
    to the training instances.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0414](assets/mls3_0414.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-14\. High-degree polynomial regression
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This high-degree polynomial regression model is severely overfitting the training
    data, while the linear model is underfitting it. The model that will generalize
    best in this case is the quadratic model, which makes sense because the data was
    generated using a quadratic model. But in general you won’t know what function
    generated the data, so how can you decide how complex your model should be? How
    can you tell that your model is overfitting or underfitting the data?
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html#project_chapter) you used cross-validation to get an
    estimate of a model’s generalization performance. If a model performs well on
    the training data but generalizes poorly according to the cross-validation metrics,
    then your model is overfitting. If it performs poorly on both, then it is underfitting.
    This is one way to tell when a model is too simple or too complex.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to tell is to look at the *learning curves*, which are plots of
    the model’s training error and validation error as a function of the training
    iteration: just evaluate the model at regular intervals during training on both
    the training set and the validation set, and plot the results. If the model cannot
    be trained incrementally (i.e., if it does not support `partial_fit()` or `warm_start`),
    then you must train it several times on gradually larger subsets of the training
    set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-Learn has a useful `learning_curve()` function to help with this: it
    trains and evaluates the model using cross-validation. By default it retrains
    the model on growing subsets of the training set, but if the model supports incremental
    learning you can set `exploit_incremental_learning=True` when calling `learning_curve()`
    and it will train the model incrementally instead. The function returns the training
    set sizes at which it evaluated the model, and the training and validation scores
    it measured for each size and for each cross-validation fold. Let’s use this function
    to look at the learning curves of the plain linear regression model (see [Figure 4-15](#underfitting_learning_curves_plot)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0415](assets/mls3_0415.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-15\. Learning curves
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This model is underfitting. To see why, first let’s look at the training error.
    When there are just one or two instances in the training set, the model can fit
    them perfectly, which is why the curve starts at zero. But as new instances are
    added to the training set, it becomes impossible for the model to fit the training
    data perfectly, both because the data is noisy and because it is not linear at
    all. So the error on the training data goes up until it reaches a plateau, at
    which point adding new instances to the training set doesn’t make the average
    error much better or worse. Now let’s look at the validation error. When the model
    is trained on very few training instances, it is incapable of generalizing properly,
    which is why the validation error is initially quite large. Then, as the model
    is shown more training examples, it learns, and thus the validation error slowly
    goes down. However, once again a straight line cannot do a good job of modeling
    the data, so the error ends up at a plateau, very close to the other curve.
  prefs: []
  type: TYPE_NORMAL
- en: These learning curves are typical of a model that’s underfitting. Both curves
    have reached a plateau; they are close and fairly high.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your model is underfitting the training data, adding more training examples
    will not help. You need to use a better model or come up with better features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s look at the learning curves of a 10th-degree polynomial model on
    the same data ([Figure 4-16](#learning_curves_plot)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0416](assets/mls3_0416.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-16\. Learning curves for the 10th-degree polynomial model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'These learning curves look a bit like the previous ones, but there are two
    very important differences:'
  prefs: []
  type: TYPE_NORMAL
- en: The error on the training data is much lower than before.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a gap between the curves. This means that the model performs significantly
    better on the training data than on the validation data, which is the hallmark
    of an overfitting model. If you used a much larger training set, however, the
    two curves would continue to get closer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One way to improve an overfitting model is to feed it more training data until
    the validation error reaches the training error.
  prefs: []
  type: TYPE_NORMAL
- en: Regularized Linear Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you saw in Chapters [1](ch01.html#landscape_chapter) and [2](ch02.html#project_chapter),
    a good way to reduce overfitting is to regularize the model (i.e., to constrain
    it): the fewer degrees of freedom it has, the harder it will be for it to overfit
    the data. A simple way to regularize a polynomial model is to reduce the number
    of polynomial degrees.'
  prefs: []
  type: TYPE_NORMAL
- en: For a linear model, regularization is typically achieved by constraining the
    weights of the model. We will now look at ridge regression, lasso regression,
    and elastic net regression, which implement three different ways to constrain
    the weights.
  prefs: []
  type: TYPE_NORMAL
- en: Ridge Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Ridge regression* (also called *Tikhonov regularization*) is a regularized
    version of linear regression: a *regularization term* equal to <math><mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><msub><mi>θ</mi><mi>i</mi></msub><mn>2</mn></msup></math>
    is added to the MSE. This forces the learning algorithm to not only fit the data
    but also keep the model weights as small as possible. Note that the regularization
    term should only be added to the cost function during training. Once the model
    is trained, you want to use the unregularized MSE (or the RMSE) to evaluate the
    model’s performance.'
  prefs: []
  type: TYPE_NORMAL
- en: The hyperparameter *α* controls how much you want to regularize the model. If
    *α* = 0, then ridge regression is just linear regression. If *α* is very large,
    then all weights end up very close to zero and the result is a flat line going
    through the data’s mean. [Equation 4-8](#ridge_cost_function) presents the ridge
    regression cost function.⁠^([7](ch04.html#idm45720215617520))
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-8\. Ridge regression cost function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math><mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><msub><mi>θ</mi><mi>i</mi></msub><mn>2</mn></msup></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note that the bias term *θ*[0] is not regularized (the sum starts at *i* = 1,
    not 0). If we define **w** as the vector of feature weights (*θ*[1] to *θ*[*n*]),
    then the regularization term is equal to *α*(∥ **w** ∥[2])² / *m*, where ∥ **w**
    ∥[2] represents the ℓ[2] norm of the weight vector.⁠^([8](ch04.html#idm45720215599696))
    For batch gradient descent, just add 2*α***w** / *m* to the part of the MSE gradient
    vector that corresponds to the feature weights, without adding anything to the
    gradient of the bias term (see [Equation 4-6](#mse_gradient_vector)).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is important to scale the data (e.g., using a `StandardScaler`) before performing
    ridge regression, as it is sensitive to the scale of the input features. This
    is true of most regularized models.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-17](#ridge_regression_plot) shows several ridge models that were
    trained on some very noisy linear data using different *α* values. On the left,
    plain ridge models are used, leading to linear predictions. On the right, the
    data is first expanded using `PolynomialFeatures(degree=10)`, then it is scaled
    using a `StandardScaler`, and finally the ridge models are applied to the resulting
    features: this is polynomial regression with ridge regularization. Note how increasing
    *α* leads to flatter (i.e., less extreme, more reasonable) predictions, thus reducing
    the model’s variance but increasing its bias.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0417](assets/mls3_0417.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-17\. Linear (left) and a polynomial (right) models, both with various
    levels of ridge regularization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As with linear regression, we can perform ridge regression either by computing
    a closed-form equation or by performing gradient descent. The pros and cons are
    the same. [Equation 4-9](#ridge_regression_solution) shows the closed-form solution,
    where **A** is the (*n* + 1) × (*n* + 1) *identity matrix*,⁠^([9](ch04.html#idm45720215579520))
    except with a 0 in the top-left cell, corresponding to the bias term.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-9\. Ridge regression closed-form solution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover accent="true"><mi mathvariant="bold">θ</mi>
    <mo>^</mo></mover> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">X</mi>
    <mo>⊺</mo></msup> <mi mathvariant="bold">X</mi><mo>+</mo><mi>α</mi><mi mathvariant="bold">A</mi><mo>)</mo></mrow>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup>  <msup><mi mathvariant="bold">X</mi>
    <mo>⊺</mo></msup>  <mi mathvariant="bold">y</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how to perform ridge regression with Scikit-Learn using a closed-form
    solution (a variant of [Equation 4-9](#ridge_regression_solution) that uses a
    matrix factorization technique by André-Louis Cholesky):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: And using stochastic gradient descent:⁠^([10](ch04.html#idm45720215546880))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `penalty` hyperparameter sets the type of regularization term to use. Specifying
    `"l2"` indicates that you want SGD to add a regularization term to the MSE cost
    function equal to `alpha` times the square of the ℓ[2] norm of the weight vector.
    This is just like ridge regression, except there’s no division by *m* in this
    case; that’s why we passed `alpha=0.1 / m`, to get the same result as `Ridge(alpha=0.1)`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `RidgeCV` class also performs ridge regression, but it automatically tunes
    hyperparameters using cross-validation. It’s roughly equivalent to using `GridSearchCV`,
    but it’s optimized for ridge regression and runs *much* faster. Several other
    estimators (mostly linear) also have efficient CV variants, such as `LassoCV`
    and `ElasticNetCV`.
  prefs: []
  type: TYPE_NORMAL
- en: Lasso Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Least absolute shrinkage and selection operator regression* (usually simply
    called *lasso regression*) is another regularized version of linear regression:
    just like ridge regression, it adds a regularization term to the cost function,
    but it uses the ℓ[1] norm of the weight vector instead of the square of the ℓ[2]
    norm (see [Equation 4-10](#lasso_cost_function)). Notice that the ℓ[1] norm is
    multiplied by 2*α*, whereas the ℓ[2] norm was multiplied by *α* / *m* in ridge
    regression. These factors were chosen to ensure that the optimal *α* value is
    independent from the training set size: different norms lead to different factors
    (see [Scikit-Learn issue #15657](https://github.com/scikit-learn/scikit-learn/issues/15657)
    for more details).'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-10\. Lasso regression cost function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math><mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mrow><mn>2</mn><mi>α</mi><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfenced
    open="|" close="|"><msub><mi>θ</mi><mi>i</mi></msub></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-18](#lasso_regression_plot) shows the same thing as [Figure 4-17](#ridge_regression_plot)
    but replaces the ridge models with lasso models and uses different *α* values.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0418](assets/mls3_0418.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-18\. Linear (left) and polynomial (right) models, both using various
    levels of lasso regularization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'An important characteristic of lasso regression is that it tends to eliminate
    the weights of the least important features (i.e., set them to zero). For example,
    the dashed line in the righthand plot in [Figure 4-18](#lasso_regression_plot)
    (with *α* = 0.01) looks roughly cubic: all the weights for the high-degree polynomial
    features are equal to zero. In other words, lasso regression automatically performs
    feature selection and outputs a *sparse model* with few nonzero feature weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get a sense of why this is the case by looking at [Figure 4-19](#lasso_vs_ridge_plot):
    the axes represent two model parameters, and the background contours represent
    different loss functions. In the top-left plot, the contours represent the ℓ[1]
    loss (|*θ*[1]| + |*θ*[2]|), which drops linearly as you get closer to any axis.
    For example, if you initialize the model parameters to *θ*[1] = 2 and *θ*[2] =
    0.5, running gradient descent will decrement both parameters equally (as represented
    by the dashed yellow line); therefore *θ*[2] will reach 0 first (since it was
    closer to 0 to begin with). After that, gradient descent will roll down the gutter
    until it reaches *θ*[1] = 0 (with a bit of bouncing around, since the gradients
    of ℓ[1] never get close to 0: they are either –1 or 1 for each parameter). In
    the top-right plot, the contours represent lasso regression’s cost function (i.e.,
    an MSE cost function plus an ℓ[1] loss). The small white circles show the path
    that gradient descent takes to optimize some model parameters that were initialized
    around *θ*[1] = 0.25 and *θ*[2] = –1: notice once again how the path quickly reaches
    *θ*[2] = 0, then rolls down the gutter and ends up bouncing around the global
    optimum (represented by the red square). If we increased *α*, the global optimum
    would move left along the dashed yellow line, while if we decreased *α*, the global
    optimum would move right (in this example, the optimal parameters for the unregularized
    MSE are *θ*[1] = 2 and *θ*[2] = 0.5).'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0419](assets/mls3_0419.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-19\. Lasso versus ridge regularization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The two bottom plots show the same thing but with an ℓ[2] penalty instead. In
    the bottom-left plot, you can see that the ℓ[2] loss decreases as we get closer
    to the origin, so gradient descent just takes a straight path toward that point.
    In the bottom-right plot, the contours represent ridge regression’s cost function
    (i.e., an MSE cost function plus an ℓ[2] loss). As you can see, the gradients
    get smaller as the parameters approach the global optimum, so gradient descent
    naturally slows down. This limits the bouncing around, which helps ridge converge
    faster than lasso regression. Also note that the optimal parameters (represented
    by the red square) get closer and closer to the origin when you increase *α*,
    but they never get eliminated entirely.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To keep gradient descent from bouncing around the optimum at the end when using
    lasso regression, you need to gradually reduce the learning rate during training.
    It will still bounce around the optimum, but the steps will get smaller and smaller,
    so it will converge.
  prefs: []
  type: TYPE_NORMAL
- en: The lasso cost function is not differentiable at *θ*[*i*] = 0 (for *i* = 1,
    2, ⋯, *n*), but gradient descent still works if you use a *subgradient vector*
    **g**⁠^([11](ch04.html#idm45720215341648)) instead when any *θ*[*i*] = 0\. [Equation
    4-11](#lasso_subgradient_vector) shows a subgradient vector equation you can use
    for gradient descent with the lasso cost function.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-11\. Lasso regression subgradient vector
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math><mrow><mi>g</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>,</mo><mi>J</mi><mo>)</mo></mrow><mo>=</mo><mrow><msub><mo>∇</mo><mi
    mathvariant="bold">θ</mi></msub><mtext>MSE</mtext><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mrow><mn>2</mn><mi>α</mi><mfenced><mtable><mtr><mtd><mo>sign</mo><mo>(</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>)</mo></mtd></mtr><mtr><mtd><mo>sign</mo><mo>(</mo><msub><mi>θ</mi><mn>2</mn></msub><mo>)</mo></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mo>sign</mo><mo>(</mo><msub><mi>θ</mi><mi>n</mi></msub><mo>)</mo></mtd></mtr></mtable></mfenced></mrow><mtext>where </mtext><mrow><mo>sign</mo><mo>(</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>)</mo></mrow><mo>=</mo><mrow><mfenced
    open="{" close=""><mtable><mtr><mtd><mo>-</mo><mn>1</mn></mtd><mtd><mtext>if </mtext><msub><mi>θ</mi><mi>i</mi></msub><mo><</mo><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mtext>if </mtext><msub><mi>θ</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mo>+</mo><mn>1</mn></mtd><mtd><mtext>if </mtext><msub><mi>θ</mi><mi>i</mi></msub><mo>></mo><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a small Scikit-Learn example using the `Lasso` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note that you could instead use `SGDRegressor(penalty="l1", alpha=0.1)`.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic Net Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Elastic net regression* is a middle ground between ridge regression and lasso
    regression. The regularization term is a weighted sum of both ridge and lasso’s
    regularization terms, and you can control the mix ratio *r*. When *r* = 0, elastic
    net is equivalent to ridge regression, and when *r* = 1, it is equivalent to lasso
    regression ([Equation 4-12](#elastic_net_cost_function)).'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-12\. Elastic net cost function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math><mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mi>r</mi><mfenced><mrow><mn>2</mn><mi>α</mi><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfenced
    open="|" close="|"><msub><mi>θ</mi><mi>i</mi></msub></mfenced></mrow></mfenced><mo>+</mo><mo>(</mo><mn>1</mn><mo>-</mo><mi>r</mi><mo>)</mo><mfenced><mrow><mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>θ</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mfenced></math>
  prefs: []
  type: TYPE_NORMAL
- en: So when should you use elastic net regression, or ridge, lasso, or plain linear
    regression (i.e., without any regularization)? It is almost always preferable
    to have at least a little bit of regularization, so generally you should avoid
    plain linear regression. Ridge is a good default, but if you suspect that only
    a few features are useful, you should prefer lasso or elastic net because they
    tend to reduce the useless features’ weights down to zero, as discussed earlier.
    In general, elastic net is preferred over lasso because lasso may behave erratically
    when the number of features is greater than the number of training instances or
    when several features are strongly correlated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a short example that uses Scikit-Learn’s `ElasticNet` (`l1_ratio` corresponds
    to the mix ratio *r*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Early Stopping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A very different way to regularize iterative learning algorithms such as gradient
    descent is to stop training as soon as the validation error reaches a minimum.
    This is called *early stopping*. [Figure 4-20](#early_stopping_plot) shows a complex
    model (in this case, a high-degree polynomial regression model) being trained
    with batch gradient descent on the quadratic dataset we used earlier. As the epochs
    go by, the algorithm learns, and its prediction error (RMSE) on the training set
    goes down, along with its prediction error on the validation set. After a while,
    though, the validation error stops decreasing and starts to go back up. This indicates
    that the model has started to overfit the training data. With early stopping you
    just stop training as soon as the validation error reaches the minimum. It is
    such a simple and efficient regularization technique that Geoffrey Hinton called
    it a “beautiful free lunch”.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0420](assets/mls3_0420.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-20\. Early stopping regularization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With stochastic and mini-batch gradient descent, the curves are not so smooth,
    and it may be hard to know whether you have reached the minimum or not. One solution
    is to stop only after the validation error has been above the minimum for some
    time (when you are confident that the model will not do any better), then roll
    back the model parameters to the point where the validation error was at a minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a basic implementation of early stopping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This code first adds the polynomial features and scales all the input features,
    both for the training set and for the validation set (the code assumes that you
    have split the original training set into a smaller training set and a validation
    set). Then it creates an `SGDRegressor` model with no regularization and a small
    learning rate. In the training loop, it calls `partial_fit()` instead of `fit()`,
    to perform incremental learning. At each epoch, it measures the RMSE on the validation
    set. If it is lower than the lowest RMSE seen so far, it saves a copy of the model
    in the `best_model` variable. This implementation does not actually stop training,
    but it lets you revert to the best model after training. Note that the model is
    copied using `copy.deepcopy()`, because it copies both the model’s hyperparameters
    *and* the learned parameters. In contrast, `sklearn.base.clone()` only copies
    the model’s hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in [Chapter 1](ch01.html#landscape_chapter), some regression algorithms
    can be used for classification (and vice versa). *Logistic regression* (also called
    *logit regression*) is commonly used to estimate the probability that an instance
    belongs to a particular class (e.g., what is the probability that this email is
    spam?). If the estimated probability is greater than a given threshold (typically
    50%), then the model predicts that the instance belongs to that class (called
    the *positive class*, labeled “1”), and otherwise it predicts that it does not
    (i.e., it belongs to the *negative class*, labeled “0”). This makes it a binary
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating Probabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So how does logistic regression work? Just like a linear regression model, a
    logistic regression model computes a weighted sum of the input features (plus
    a bias term), but instead of outputting the result directly like the linear regression
    model does, it outputs the *logistic* of this result (see [Equation 4-13](#logisticregression_model_estimated_probability_vectorized_form)).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-13\. Logistic regression model estimated probability (vectorized
    form)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover accent="true"><mi>p</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mi>h</mi> <mi mathvariant="bold">θ</mi></msub> <mrow><mo>(</mo>
    <mi mathvariant="bold">x</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <msup><mi mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">x</mi>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The logistic—noted *σ*(·)—is a *sigmoid function* (i.e., *S*-shaped) that outputs
    a number between 0 and 1\. It is defined as shown in [Equation 4-14](#equation_four_fourteen)
    and [Figure 4-21](#logistic_function_plot).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-14\. Logistic function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>σ</mi> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><mo
    form="prefix">exp</mo><mo>(</mo><mo>-</mo><mi>t</mi><mo>)</mo></mrow></mfrac></mstyle></mrow></math>![mls3
    0421](assets/mls3_0421.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4-21\. Logistic function
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once the logistic regression model has estimated the probability <math><mover><mi>p</mi><mo>^</mo></mover></math>
    = *h*[**θ**](**x**) that an instance **x** belongs to the positive class, it can
    make its prediction *ŷ* easily (see [Equation 4-15](#equation_four_fifteen)).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-15\. Logistic regression model prediction using a 50% threshold probability
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mover accent="true"><mi>p</mi>
    <mo>^</mo></mover> <mo><</mo> <mn>0.5</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mover accent="true"><mi>p</mi>
    <mo>^</mo></mover> <mo>≥</mo> <mn>0.5</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Notice that *σ*(*t*) < 0.5 when *t* < 0, and *σ*(*t*) ≥ 0.5 when *t* ≥ 0, so
    a logistic regression model using the default threshold of 50% probability predicts
    1 if **θ**^⊺ **x** is positive and 0 if it is negative.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The score *t* is often called the *logit*. The name comes from the fact that
    the logit function, defined as logit(*p*) = log(*p* / (1 – *p*)), is the inverse
    of the logistic function. Indeed, if you compute the logit of the estimated probability
    *p*, you will find that the result is *t*. The logit is also called the *log-odds*,
    since it is the log of the ratio between the estimated probability for the positive
    class and the estimated probability for the negative class.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Cost Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now you know how a logistic regression model estimates probabilities and makes
    predictions. But how is it trained? The objective of training is to set the parameter
    vector **θ** so that the model estimates high probabilities for positive instances
    (*y* = 1) and low probabilities for negative instances (*y* = 0). This idea is
    captured by the cost function shown in [Equation 4-16](#cost_function_of_a_single_training_instance)
    for a single training instance **x**.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-16\. Cost function of a single training instance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math><mrow><mi>c</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mfenced
    open="{" close=""><mtable><mtr><mtd><mo>-</mo><mi>log</mi><mo>(</mo><mover accent="true"><mi>p</mi><mo>^</mo></mover><mo>)</mo></mtd><mtd><mtext>if </mtext><mi>y</mi><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd><mo>-</mo><mi>log</mi><mo>(</mo><mn>1</mn><mo>-</mo><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mo>)</mo></mtd><mtd><mtext>if </mtext><mi>y</mi><mo>=</mo><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This cost function makes sense because –log(*t*) grows very large when *t* approaches
    0, so the cost will be large if the model estimates a probability close to 0 for
    a positive instance, and it will also be large if the model estimates a probability
    close to 1 for a negative instance. On the other hand, –log(*t*) is close to 0
    when *t* is close to 1, so the cost will be close to 0 if the estimated probability
    is close to 0 for a negative instance or close to 1 for a positive instance, which
    is precisely what we want.
  prefs: []
  type: TYPE_NORMAL
- en: The cost function over the whole training set is the average cost over all training
    instances. It can be written in a single expression called the *log loss*, shown
    in [Equation 4-17](#logistic_regression_cost_function).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-17\. Logistic regression cost function (log loss)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math><mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mfenced
    open="[" close="]"><mrow><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mi>l</mi><mi>o</mi><mi>g</mi><mfenced><msup><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mfenced><mo>+</mo><mo>(</mo><mn>1</mn><mo>-</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo><mi>l</mi><mi>o</mi><mi>g</mi><mfenced><mrow><mn>1</mn><mo>-</mo><msup><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mfenced></mrow></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The log loss was not just pulled out of a hat. It can be shown mathematically
    (using Bayesian inference) that minimizing this loss will result in the model
    with the *maximum likelihood* of being optimal, assuming that the instances follow
    a Gaussian distribution around the mean of their class. When you use the log loss,
    this is the implicit assumption you are making. The more wrong this assumption
    is, the more biased the model will be. Similarly, when we used the MSE to train
    linear regression models, we were implicitly assuming that the data was purely
    linear, plus some Gaussian noise. So, if the data is not linear (e.g., if it’s
    quadratic) or if the noise is not Gaussian (e.g., if outliers are not exponentially
    rare), then the model will be biased.
  prefs: []
  type: TYPE_NORMAL
- en: The bad news is that there is no known closed-form equation to compute the value
    of **θ** that minimizes this cost function (there is no equivalent of the Normal
    equation). But the good news is that this cost function is convex, so gradient
    descent (or any other optimization algorithm) is guaranteed to find the global
    minimum (if the learning rate is not too large and you wait long enough). The
    partial derivatives of the cost function with regard to the *j*^(th) model parameter
    *θ*[*j*] are given by [Equation 4-18](#logistic_cost_function_partial_derivatives).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-18\. Logistic cost function partial derivatives
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mstyle scriptlevel="0" displaystyle="true"><mfrac><mi>∂</mi>
    <mrow><mi>∂</mi><msub><mi>θ</mi> <mi>j</mi></msub></mrow></mfrac></mstyle> <mtext>J</mtext>
    <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mstyle
    scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <mfenced separators="" open="(" close=")"><mi>σ</mi> <mrow><mo>(</mo> <msup><mi
    mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>-</mo>
    <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mfenced>
    <msubsup><mi>x</mi> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This equation looks very much like [Equation 4-5](#mse_partial_derivatives):
    for each instance it computes the prediction error and multiplies it by the *j*^(th)
    feature value, and then it computes the average over all training instances. Once
    you have the gradient vector containing all the partial derivatives, you can use
    it in the batch gradient descent algorithm. That’s it: you now know how to train
    a logistic regression model. For stochastic GD you would take one instance at
    a time, and for mini-batch GD you would use a mini-batch at a time.'
  prefs: []
  type: TYPE_NORMAL
- en: Decision Boundaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use the iris dataset to illustrate logistic regression. This is a famous
    dataset that contains the sepal and petal length and width of 150 iris flowers
    of three different species: *Iris setosa*, *Iris versicolor*, and *Iris virginica*
    (see [Figure 4-22](#iris_dataset_diagram)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0422](assets/mls3_0422.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-22\. Flowers of three iris plant species⁠^([12](ch04.html#idm45720214766432))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s try to build a classifier to detect the *Iris virginica* type based only
    on the petal width feature. The first step is to load the data and take a quick
    peek:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we’ll split the data and train a logistic regression model on the training
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at the model’s estimated probabilities for flowers with petal widths
    varying from 0 cm to 3 cm ([Figure 4-23](#logistic_regression_plot)):⁠^([13](ch04.html#idm45720214606864))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0423](assets/mls3_0423.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-23\. Estimated probabilities and decision boundary
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The petal width of *Iris virginica* flowers (represented as triangles) ranges
    from 1.4 cm to 2.5 cm, while the other iris flowers (represented by squares) generally
    have a smaller petal width, ranging from 0.1 cm to 1.8 cm. Notice that there is
    a bit of overlap. Above about 2 cm the classifier is highly confident that the
    flower is an *Iris virginica* (it outputs a high probability for that class),
    while below 1 cm it is highly confident that it is not an *Iris virginica* (high
    probability for the “Not Iris virginica” class). In between these extremes, the
    classifier is unsure. However, if you ask it to predict the class (using the `predict()`
    method rather than the `predict_proba()` method), it will return whichever class
    is the most likely. Therefore, there is a *decision boundary* at around 1.6 cm
    where both probabilities are equal to 50%: if the petal width is greater than
    1.6 cm the classifier will predict that the flower is an *Iris virginica*, and
    otherwise it will predict that it is not (even if it is not very confident):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-24](#logistic_regression_contour_plot) shows the same dataset, but
    this time displaying two features: petal width and length. Once trained, the logistic
    regression classifier can, based on these two features, estimate the probability
    that a new flower is an *Iris virginica*. The dashed line represents the points
    where the model estimates a 50% probability: this is the model’s decision boundary.
    Note that it is a linear boundary.⁠^([14](ch04.html#idm45720214369424)) Each parallel
    line represents the points where the model outputs a specific probability, from
    15% (bottom left) to 90% (top right). All the flowers beyond the top-right line
    have over 90% chance of being *Iris virginica*, according to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0424](assets/mls3_0424.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-24\. Linear decision boundary
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The hyperparameter controlling the regularization strength of a Scikit-Learn
    `LogisticRegression` model is not `alpha` (as in other linear models), but its
    inverse: `C`. The higher the value of `C`, the *less* the model is regularized.'
  prefs: []
  type: TYPE_NORMAL
- en: Just like the other linear models, logistic regression models can be regularized
    using ℓ[1] or ℓ[2] penalties. Scikit-Learn actually adds an ℓ[2] penalty by default.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The logistic regression model can be generalized to support multiple classes
    directly, without having to train and combine multiple binary classifiers (as
    discussed in [Chapter 3](ch03.html#classification_chapter)). This is called *softmax
    regression*, or *multinomial logistic regression*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is simple: when given an instance **x**, the softmax regression model
    first computes a score *s*[*k*](**x**) for each class *k*, then estimates the
    probability of each class by applying the *softmax function* (also called the
    *normalized exponential*) to the scores. The equation to compute *s*[*k*](**x**)
    should look familiar, as it is just like the equation for linear regression prediction
    (see [Equation 4-19](#softmax_score_for_class_k)).'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-19\. Softmax score for class k
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>s</mi> <mi>k</mi></msub> <mrow><mo>(</mo>
    <mi mathvariant="bold">x</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi
    mathvariant="bold">θ</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow>
    <mo>⊺</mo></msup> <mi mathvariant="bold">x</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note that each class has its own dedicated parameter vector **θ**^((*k*)). All
    these vectors are typically stored as rows in a *parameter matrix* **Θ**.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have computed the score of every class for the instance **x**, you
    can estimate the probability <math><msub><mover><mi>p</mi><mo>^</mo></mover><mi>k</mi></msub></math>
    that the instance belongs to class *k* by running the scores through the softmax
    function ([Equation 4-20](#softmax_function)). The function computes the exponential
    of every score, then normalizes them (dividing by the sum of all the exponentials).
    The scores are generally called logits or log-odds (although they are actually
    unnormalized log-odds).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-20\. Softmax function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mover accent="true"><mi>p</mi> <mo>^</mo></mover>
    <mi>k</mi></msub> <mo>=</mo> <mi>σ</mi> <msub><mfenced separators="" open="("
    close=")"><mi mathvariant="bold">s</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mfenced>
    <mi>k</mi></msub> <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mo
    form="prefix">exp</mo><mfenced separators="" open="(" close=")"><msub><mi>s</mi>
    <mi>k</mi></msub> <mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></mfenced></mrow>
    <mrow><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>K</mi></munderover>
    <mrow><mo form="prefix">exp</mo><mfenced separators="" open="(" close=")"><msub><mi>s</mi>
    <mi>j</mi></msub> <mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></mfenced></mrow></mrow></mfrac></mstyle></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*K* is the number of classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**s**(**x**) is a vector containing the scores of each class for the instance
    **x**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*σ*(**s**(**x**))[*k*] is the estimated probability that the instance **x**
    belongs to class *k*, given the scores of each class for that instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just like the logistic regression classifier, by default the softmax regression
    classifier predicts the class with the highest estimated probability (which is
    simply the class with the highest score), as shown in [Equation 4-21](#softmax_regression_classifier_prediction).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-21\. Softmax regression classifier prediction
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <munder><mo form="prefix">argmax</mo> <mi>k</mi></munder> <mi>σ</mi>
    <msub><mfenced separators="" open="(" close=")"><mi mathvariant="bold">s</mi><mo>(</mo><mi
    mathvariant="bold">x</mi><mo>)</mo></mfenced> <mi>k</mi></msub> <mo>=</mo> <munder><mo
    form="prefix">argmax</mo> <mi>k</mi></munder> <msub><mi>s</mi> <mi>k</mi></msub>
    <mrow><mo>(</mo> <mi mathvariant="bold">x</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo
    form="prefix">argmax</mo> <mi>k</mi></munder> <mfenced separators="" open="("
    close=")"><msup><mrow><mo>(</mo><msup><mi mathvariant="bold">θ</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>⊺</mo></msup> <mi mathvariant="bold">x</mi></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The *argmax* operator returns the value of a variable that maximizes a function.
    In this equation, it returns the value of *k* that maximizes the estimated probability
    *σ*(**s**(**x**))[*k*].
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The softmax regression classifier predicts only one class at a time (i.e., it
    is multiclass, not multioutput), so it should be used only with mutually exclusive
    classes, such as different species of plants. You cannot use it to recognize multiple
    people in one picture.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how the model estimates probabilities and makes predictions,
    let’s take a look at training. The objective is to have a model that estimates
    a high probability for the target class (and consequently a low probability for
    the other classes). Minimizing the cost function shown in [Equation 4-22](#cross_entropy_cost_function),
    called the *cross entropy*, should lead to this objective because it penalizes
    the model when it estimates a low probability for a target class. Cross entropy
    is frequently used to measure how well a set of estimated class probabilities
    matches the target classes.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-22\. Cross entropy cost function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math><mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">Θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msubsup><mi>y</mi><mi>k</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mi>log</mi><mfenced><msubsup><mover
    accent="true"><mi>p</mi><mo>^</mo></mover><mi>k</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: In this equation, <math><msubsup><mi>y</mi><mi>k</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></math>
    is the target probability that the *i*^(th) instance belongs to class *k*. In
    general, it is either equal to 1 or 0, depending on whether the instance belongs
    to the class or not.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that when there are just two classes (*K* = 2), this cost function is
    equivalent to the logistic regression cost function (log loss; see [Equation 4-17](#logistic_regression_cost_function)).
  prefs: []
  type: TYPE_NORMAL
- en: The gradient vector of this cost function with regard to **θ**^((*k*)) is given
    by [Equation 4-23](#cross_entropy_gradient_vector_for_class_k).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-23\. Cross entropy gradient vector for class k
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>∇</mi> <msup><mi mathvariant="bold">θ</mi>
    <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup></msub> <mi>J</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">Θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <mrow><mfenced
    separators="" open="(" close=")"><msubsup><mover accent="true"><mi>p</mi> <mo>^</mo></mover>
    <mi>k</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup> <mo>-</mo> <msubsup><mi>y</mi>
    <mi>k</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mfenced> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Now you can compute the gradient vector for every class, then use gradient descent
    (or any other optimization algorithm) to find the parameter matrix **Θ** that
    minimizes the cost function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use softmax regression to classify the iris plants into all three classes.
    Scikit-Learn’s `LogisticRegression` classifier uses softmax regression automatically
    when you train it on more than two classes (assuming you use `solver="lbfgs"`,
    which is the default). It also applies ℓ[2] regularization by default, which you
    can control using the hyperparameter `C`, as mentioned earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'So the next time you find an iris with petals that are 5 cm long and 2 cm wide,
    you can ask your model to tell you what type of iris it is, and it will answer
    *Iris virginica* (class 2) with 96% probability (or *Iris versicolor* with 4%
    probability):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-25](#softmax_regression_contour_plot) shows the resulting decision
    boundaries, represented by the background colors. Notice that the decision boundaries
    between any two classes are linear. The figure also shows the probabilities for
    the *Iris versicolor* class, represented by the curved lines (e.g., the line labeled
    with 0.30 represents the 30% probability boundary). Notice that the model can
    predict a class that has an estimated probability below 50%. For example, at the
    point where all decision boundaries meet, all classes have an equal estimated
    probability of 33%.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0425](assets/mls3_0425.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-25\. Softmax regression decision boundaries
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this chapter, you learned various ways to train linear models, both for regression
    and for classification. You used a closed-form equation to solve linear regression,
    as well as gradient descent, and you learned how various penalties can be added
    to the cost function during training to regularize the model. Along the way, you
    also learned how to plot learning curves and analyze them, and how to implement
    early stopping. Finally, you learned how logistic regression and softmax regression
    work. We’ve opened up the first machine learning black boxes! In the next chapters
    we will open many more, starting with support vector machines.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which linear regression training algorithm can you use if you have a training
    set with millions of features?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose the features in your training set have very different scales. Which
    algorithms might suffer from this, and how? What can you do about it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can gradient descent get stuck in a local minimum when training a logistic regression
    model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do all gradient descent algorithms lead to the same model, provided you let
    them run long enough?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose you use batch gradient descent and you plot the validation error at
    every epoch. If you notice that the validation error consistently goes up, what
    is likely going on? How can you fix this?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it a good idea to stop mini-batch gradient descent immediately when the validation
    error goes up?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which gradient descent algorithm (among those we discussed) will reach the vicinity
    of the optimal solution the fastest? Which will actually converge? How can you
    make the others converge as well?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose you are using polynomial regression. You plot the learning curves and
    you notice that there is a large gap between the training error and the validation
    error. What is happening? What are three ways to solve this?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose you are using ridge regression and you notice that the training error
    and the validation error are almost equal and fairly high. Would you say that
    the model suffers from high bias or high variance? Should you increase the regularization
    hyperparameter *α* or reduce it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Why would you want to use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ridge regression instead of plain linear regression (i.e., without any regularization)?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Lasso instead of ridge regression?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Elastic net instead of lasso regression?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.
    Should you implement two logistic regression classifiers or one softmax regression
    classifier?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement batch gradient descent with early stopping for softmax regression
    without using Scikit-Learn, only NumPy. Use it on a classification task such as
    the iris dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch04.html#idm45720217568672-marker)) A closed-form equation is only composed
    of a finite number of constants, variables, and standard operations: for example,
    *a* = sin(*b* – *c*). No infinite sums, no limits, no integrals, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch04.html#idm45720216856720-marker)) Technically speaking, its derivative
    is *Lipschitz continuous*.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch04.html#idm45720216853456-marker)) Since feature 1 is smaller, it takes
    a larger change in *θ*[1] to affect the cost function, which is why the bowl is
    elongated along the *θ*[1] axis.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch04.html#idm45720216763152-marker)) Eta (*η*) is the seventh letter of
    the Greek alphabet.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch04.html#idm45720216235312-marker)) While the Normal equation can only
    perform linear regression, the gradient descent algorithms can be used to train
    many other models, as you’ll see.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch04.html#idm45720215648240-marker)) This notion of bias is not to be
    confused with the bias term of linear models.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch04.html#idm45720215617520-marker)) It is common to use the notation
    *J*(**θ**) for cost functions that don’t have a short name; I’ll often use this
    notation throughout the rest of this book. The context will make it clear which
    cost function is being discussed.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch04.html#idm45720215599696-marker)) Norms are discussed in [Chapter 2](ch02.html#project_chapter).
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch04.html#idm45720215579520-marker)) A square matrix full of 0s except
    for 1s on the main diagonal (top left to bottom right).
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch04.html#idm45720215546880-marker)) Alternatively, you can use the `Ridge`
    class with the `"sag"` solver. Stochastic average GD is a variant of stochastic
    GD. For more details, see the presentation [“Minimizing Finite Sums with the Stochastic
    Average Gradient Algorithm”](https://homl.info/12) by Mark Schmidt et al. from
    the University of British Columbia.
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch04.html#idm45720215341648-marker)) You can think of a subgradient vector
    at a nondifferentiable point as an intermediate vector between the gradient vectors
    around that point.
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch04.html#idm45720214766432-marker)) Photos reproduced from the corresponding
    Wikipedia pages. *Iris virginica* photo by Frank Mayfield ([Creative Commons BY-SA
    2.0](https://creativecommons.org/licenses/by-sa/2.0)), *Iris versicolor* photo
    by D. Gordon E. Robertson ([Creative Commons BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0)),
    *Iris setosa* photo public domain.
  prefs: []
  type: TYPE_NORMAL
- en: '^([13](ch04.html#idm45720214606864-marker)) NumPy’s `reshape()` function allows
    one dimension to be –1, which means “automatic”: the value is inferred from the
    length of the array and the remaining dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch04.html#idm45720214369424-marker)) It is the set of points **x** such
    that *θ*[0] + *θ*[1]*x*[1] + *θ*[2]*x*[2] = 0, which defines a straight line.
  prefs: []
  type: TYPE_NORMAL
