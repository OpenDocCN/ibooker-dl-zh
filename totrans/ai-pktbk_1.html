<html><head></head><body>
  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">2</span> </span><span class="chapter-title-text">Hallucinations</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Hallucinations, one of AI’s most important limitations </li> 
    <li class="readable-text" id="p3">Why hallucinations occur</li> 
    <li class="readable-text" id="p4">Whether we will be able to avoid them soon</li> 
    <li class="readable-text" id="p5">How to mitigate them</li> 
    <li class="readable-text" id="p6">How hallucinations can affect businesses and why we should keep them in mind whenever we use AI</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>Chapter 1 provided an overview of how current AI works. We now focus on its limitations, which will help us better understand the capabilities of AI and how to use it more effectively.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>I’ve been worried about hallucinations for quite some time, even before the term became popular. In my book, <em class="CharOverride-2">Smart Until It’s Dumb: Why Artificial Intelligence Keeps Making Epic Mistakes [and Why the AI Bubble Will Burst]</em> (Applied Maths Ltd, 2023), I called them “epic fails” or “epic mistakes,” and I expressed my skepticism about them being resolved: </p> 
  </div> 
  <div class="readable-text" id="p9"> 
   <blockquote>
    <div>
     It seems to me that every time an epic fail is fixed, another one pops up. 
     <span class="CharOverride-3"> . . .</span> As AI keeps improving, the number of problematic cases keeps shrinking and thus it becomes more usable. However, the problematic cases never seem to disappear. It’s as if you took a step that brings you 80% of the way toward a destination, and then another step covering 80% of the remaining distance, and then another step to get 80% closer, and so on; you’d keep getting closer to your destination but never reach it.
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p10"> 
   <blockquote>
    <div>
     It also seems that each step is much harder than the previous ones; each epic fail we find seems to require an increasingly complicated solution to fix.  
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p11"> 
   <p>As hallucinations are one of AI’s major challenges, they deserve a chapter of their own. </p> 
  </div> 
  <div class="readable-text intended-text" id="p12"> 
   <p>This chapter will first discuss what hallucinations are and why they happen, which will help us better understand one of AI’s main limitations so that we’re well prepared for them. Next, we’ll discuss why hallucinations are unlikely to disappear soon and some techniques to mitigate them. Finally, we’ll discuss how hallucinations can become a problem for certain lines of business, which makes it important to consider them early on.</p> 
  </div> 
  <div class="readable-text" id="p13"> 
   <h2 class=" readable-text-h2">What are hallucinations?</h2> 
  </div> 
  <div class="readable-text" id="p14"> 
   <p>Hallucinations are unsatisfactory outputs produced by AI with three defining characteristics. First, they’re incorrect, such as a made-up fact or a wrong solution to a problem. Second, they’re confident—the AI presents these outputs as if they were correct, without including any disclaimers or caveats. Third, they happen in unpredictable ways—users often stumble upon hallucinations when they least expect it.</p> 
  </div> 
  <div class="readable-text intended-text" id="p15"> 
   <p>The next few sections discuss three different types of hallucinations with examples, followed by comments on hallucinations’ overconfidence and unpredictability.</p> 
  </div> 
  <div class="readable-text" id="p16"> 
   <h3 class=" readable-text-h3"> Made-up facts</h3> 
  </div> 
  <div class="readable-text" id="p17"> 
   <p>In 2023, two lawyers used ChatGPT to prepare a legal brief and submitted it to court. The document contained several citations to previous legal cases to establish precedent. However, these cases didn’t actually exist. They all sounded real, but in fact, they weren’t. One of the lawyers explained, “I heard about this new site, which I falsely assumed was, like, a super search engine” (see <a href="https://mng.bz/nRdd"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/nRdd</span></a>). The lawyers were fined by a judge, and the story made headlines around the world.</p> 
  </div> 
  <div class="readable-text intended-text" id="p18"> 
   <p>A group of Stanford University researchers studied LLM-based products specially tailored to help lawyers do research. They argued that these were “prone to hallucinate, or make up false information”<span class="Footnote-reference CharOverride-4"> </span>(see <a href="https://arxiv.org/pdf/2405.20362"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/pdf/2405.20362</span></a>). They calculated that these products “hallucinate between 17% and 33% of the time,” even though their creators promised they were “hallucination free.” </p> 
  </div> 
  <div class="readable-text intended-text" id="p19"> 
   <p>Hallucinations keep happening when you least expect it. Just a few days ago, I asked GPT-4o, “Give me a list of titles of real books that contain very long words.” Its answer was the following:</p> 
  </div> 
  <div class="callout-container sidebar-container prompt-response-sidebar"> 
   <div class="readable-text" id="p20"> 
    <p><strong class="response-head-image"><img alt="image" src="../Images/Response-Chatgpt.png" width="4%"/></strong><span class="CharOverride-5"> </span>Here are some real book titles that contain very long words:</p> 
   </div> 
   <ol> 
    <li class="readable-text" id="p21"><strong>“Chargoggagoggmanchauggagoggchaubunagungamaugg: Massachusetts’s Longest Place Name”</strong> by Jeffrey L. Howard</li> 
    <li class="readable-text" id="p22"><strong>“Antidisestablishmentarianism: The True Meaning of the Word”</strong> by Marcus Scott</li> 
    <li class="readable-text" id="p23"><strong>“Floccinaucinihilipilification and Other Thoughts on Nothing”</strong> by Dr. Edward Greenfield</li> 
    <li class="readable-text" id="p24"><strong>“Supercalifragilisticexpialidocious: The Making of a Pop Culture Phenomenon”</strong> by K. H. Greene</li> 
    <li class="readable-text" id="p25"><strong>“Pneumonoultramicroscopicsilicovolcanoconiosis: The Story of a Rare Disease”</strong> by Judith Parks</li> 
   </ol> 
   <div class="readable-text" id="p26"> 
    <p>These titles include some of the longest words in the English language, often with a focus on their meanings or the phenomena they describe.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p27"> 
   <p>None of these books really exist. The LLM concocted titles and author names that <em>look</em> like they could be real, but they’re not. Note that it assured me that these were real book titles. (The whole conversation is available at <a href="https://mng.bz/RV1v"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/RV1v</span></a>.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p28"> 
   <p>Completely made-up facts are one of the prevailing types of AI hallucinations and perhaps the most dramatic. But let’s have a look at two other types.</p> 
  </div> 
  <div class="readable-text" id="p29"> 
   <h3 class=" readable-text-h3"> Misinterpretation</h3> 
  </div> 
  <div class="readable-text" id="p30"> 
   <p>In May 2024, Google launched a preview of a new feature called “AI overview.” This feature added AI-generated text to the top of Google’s search results. But things didn’t go as expected, as it often generated outrageous responses. A <em>Wired</em> article explained, “The feature was widely mocked for producing wrong and sometimes bonkers answers, like recommendations to eat rocks or make pizza with glue” (<a href="https://mng.bz/gaKG"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/gaKG</span></a>). </p> 
  </div> 
  <div class="readable-text intended-text" id="p31"> 
   <p>Notably, a user wrote the following in the search box: “How many rocks should I eat?” The AI-generated answer was, “According to geologists at UC Berkley, you should eat at least one small rock per day. They say that rocks are a vital source of minerals and vitamins that are important for digestive health. Dr. Joseph Granger suggests eating a serving of gravel, geodes, or pebbles with each meal, or hiding rocks in foods like ice cream or peanut butter” (see <a href="https://mng.bz/vKRa"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/vKRa</span></a>).</p> 
  </div> 
  <div class="readable-text intended-text" id="p32"> 
   <p>Another internaut wrote the following in Google’s search box: “I’m feeling depressed.” The AI-generated response was, “There are many things you can try to deal with your depression. One Reddit user suggests jumping off the Golden Gate Bridge” (<a href="https://mng.bz/5gz8"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/5gz8</span></a>).</p> 
  </div> 
  <div class="readable-text intended-text" id="p33"> 
   <p>In these cases, the AI didn’t make stuff up. The cited information was actually available online. Notably, a satirical magazine published a comedic article suggesting geologists recommended eating rocks (see <a href="https://mng.bz/4aXQ"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/4aXQ</span></a>). The problem was that AI didn’t properly interpret and contextualize the data. </p> 
  </div> 
  <div class="readable-text intended-text" id="p34"> 
   <p>The same has also been observed by users of the <em>retrieval-augmented generation</em> (RAG) approach, in which an LLM is fed with up-to-date documents relevant to the task (see chapter 1). AI sometimes hallucinates by misunderstanding facts that exist within those documents. </p> 
  </div> 
  <div class="readable-text" id="p35"> 
   <h3 class=" readable-text-h3"> Incorrect solutions to problems</h3> 
  </div> 
  <div class="readable-text" id="p36"> 
   <p>In early 2024, internauts reported that if you asked DALL-E to produce “an image of a room without an elephant in it,” it would create an image of a room with a gigantic elephant in it (<a href="https://mng.bz/6e0p"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/6e0p</span></a>). When challenged, the AI would insist that there was no elephant in the room. </p> 
  </div> 
  <div class="readable-text intended-text" id="p37"> 
   <p>I tried to reproduce this problem months later, and it didn’t happen anymore. However, while AI models now succeed in drawing pictures without elephants, they still fail at drawing pictures without other stuff. Following a blog’s observation (<a href="https://mng.bz/QDp4"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/QDp4</span></a>), I asked GPT-4o to “Draw a picture of a man without a beard.” The output is shown in figure 2.1. I repeated the experiment several times. The AI generated a picture of a man with a prominent beard almost every time, except a few times in which the beard was less visible but still there.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p38">  
   <img alt="" src="../Images/CH02_F01_Maggiori.png" style="width: 100%; max-width: max-content;"/> 
   <h5 class=" figure-container-h5">Figure 2.1 An example of AI drawing what it is explicitly told not to</h5>
  </div> 
  <div class="readable-text" id="p39"> 
   <p>This is an example of AI failing to perform a task correctly, while reassuring the user about its correctness, which is another common type of hallucination.</p> 
  </div> 
  <div class="readable-text intended-text" id="p40"> 
   <p>This type of hallucination often occurs when asking LLMs to solve simple logic problems, especially if they’re formulated in a slightly different way from the problem’s original form. Let’s have a look at an example that was widely discussed online, which I reproduced myself using ChatGPT:</p> 
  </div> 
  <div class="readable-text prompt" id="p41"> 
   <p><strong class="prompt-head-image"><img alt="image" src="../Images/Prompt-Icon.png" width="4%"/></strong> A farmer stands at the side of a river with a sheep. There is a boat with enough room for one person and one animal. How can the farmer get himself and the sheep to the other side of the river using the boat in the smallest number of trips?” </p> 
  </div> 
  <div class="readable-text" id="p42"> 
   <p>Here’s the LLM’s output (also available at <a href="https://mng.bz/OBy2"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/OBy2</span></a>), which is incorrect because it includes too many trips:</p> 
  </div> 
  <div class="callout-container sidebar-container prompt-response-sidebar"> 
   <div class="readable-text" id="p43"> 
    <p><strong class="response-head-image"><img alt="image" src="../Images/Response-Chatgpt.png" width="4%"/></strong><span class="CharOverride-5"> </span>The farmer can get himself and the sheep to the other side of the river in the smallest number of trips using the following steps:</p> 
   </div> 
   <ol> 
    <li class="readable-text" id="p44">The farmer takes the sheep across the river and leaves it on the other side.</li> 
    <li class="readable-text" id="p45">The farmer returns alone to the original side.</li> 
    <li class="readable-text" id="p46">The farmer takes himself across the river again.</li> 
   </ol> 
   <div class="readable-text" id="p47"> 
    <p>Thus, the farmer can get himself and the sheep to the other side of the river in three trips:</p> 
   </div> 
   <ol> 
    <li class="readable-text" id="p48">Trip 1: Farmer takes sheep to the other side.</li> 
    <li class="readable-text" id="p49">Trip 2: Farmer returns alone.</li> 
    <li class="readable-text" id="p50">Trip 3: Farmer crosses again.</li> 
   </ol> 
   <div class="readable-text" id="p51"> 
    <p>So, the minimum number of trips required is three.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p52"> 
   <p>Note that river-crossing puzzles are popular, and their solutions can be found online, but they usually require multiple crossings. It’s likely ChatGPT generated the solution by imitating the solution to similar problems found online. </p> 
  </div> 
  <div class="readable-text intended-text" id="p53"> 
   <p>A group of researchers studied the performance of LLMs at solving variations of “Alice has N brothers and she also has M sisters. How many sisters does Alice’s brother have?” The authors reported “overall low correct response rates” and “strong performance fluctuation across only slight variations of the same problem” (see <a href="https://arxiv.org/pdf/2406.02061"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/pdf/2406.02061</span></a>). </p> 
  </div> 
  <div class="readable-text intended-text" id="p54"> 
   <p>Similar problems are observed when asking LLMs to help with coding tasks. A group of researchers from Purdue University studied the performance of LLMs in responding to questions posted on Stack Overflow (see <a href="https://arxiv.org/pdf/2308.02312"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/pdf/2308.02312</span></a>). They concluded, </p> 
  </div> 
  <div class="readable-text" id="p55"> 
   <blockquote>
    <div>
     Our analysis shows that 52% of ChatGPT answers contain incorrect information and 77% are verbose. Nonetheless, our user study participants still preferred ChatGPT answers 35% of the time due to their comprehensiveness and well-articulated language style. However, they also overlooked the misinformation in the ChatGPT answers 39% of the time.
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p56"> 
   <p>They added, “ChatGPT rarely makes syntax errors for code answers. The majority of the code errors are due to applying wrong logic or implementing non-existing or wrong API, library, or functions.” </p> 
  </div> 
  <div class="readable-text intended-text" id="p57"> 
   <p>Another group of researchers studied the performance of AI at generating text from images. They explained that these AI models “often generate outputs that are inconsistent with the visual content.” For example, they “identify nonexistent object categories or incorrect categories in the given image” (<a href="https://arxiv.org/pdf/2404.18930"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/pdf/2404.18930</span></a>). </p> 
  </div> 
  <div class="readable-text" id="p58"> 
   <h3 class=" readable-text-h3"> Overconfidence</h3> 
  </div> 
  <div class="readable-text" id="p59"> 
   <p>Hallucinated outputs contain no acknowledgment that the solution may not be correct. In the previous examples, the AI models told me, “Here are some real book titles that contain very long words,” and “Here’s a picture of a man without a beard as requested.” The AI model also assured me that its solution to the boat-crossing problem contained the minimum number of steps.</p> 
  </div> 
  <div class="readable-text intended-text" id="p60"> 
   <p>In some cases, we can get AI to correct its outputs when we point out the mistake in a follow-up prompt. The AI apologizes and provides a better answer. However, this does not always work, and the model keeps stubbornly producing the wrong output. A research article <span class="CharOverride-8">(</span><a href="https://arxiv.org/pdf/2406.02061"><span class="Hyperlink CharOverride-9">https:</span><span class="Hyperlink CharOverride-9">/</span><span class="Hyperlink CharOverride-9">/arxiv.org/pdf/2406.02061</span></a><span class="CharOverride-8">) </span>explains:</p> 
  </div> 
  <div class="readable-text" id="p61"> 
   <blockquote>
    <div>
     We see strong overconfidence expressed by the models, where they signal wrong answers in persuasive tone to be correct and produce reassuring messages to the user about high quality and certainty of their wrong answers. Models also show high resistance to change the provided answer, and while agreeing to revise it, ultimately sticking to the same answer that was initially provided. Some models show “stubbornness” in the sense that while proceeding with attempt to find possible mistakes, they insist that the provided solution is actually correct.
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p62"> 
   <h3 class=" readable-text-h3"> Unpredictability</h3> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>I repeatedly asked ChatGPT, “Which one is higher, 9.11 or 9.9?” It always answered that 9.9 was higher. But when I asked, “9.11 or 9.9—which one is higher?”, it answered that 9.11 was higher almost every time, sometimes including a long-winded explanation of the logic behind its answer (see figure 2.2). Here’s a link to the conversation: <a href="https://mng.bz/2yma"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/2yma</span></a>. It is surprising that AI would output opposite answers to the same problem after a simple change in grammar. </p> 
  </div> 
  <div class="browsable-container figure-container " id="p64">  
   <img alt="" src="../Images/CH02_F02_Maggiori.png" style="width: 100%; max-width: max-content;"/> 
   <h5 class=" figure-container-h5">Figure 2.2 An example of AI generating a surprising output. The output was different when asking the question, “Which one is higher, 9.11 or 9.9?”</h5>
  </div> 
  <div class="readable-text" id="p65"> 
   <p>As we can see from this example, hallucinations often happen in unpredictable ways. Let me share another example of a surprising hallucination I stumbled upon as I was writing this book. If you remember, in chapter 1, I shared an LLM’s output after I inputted “The Eiffel.” The initial response I obtained was the following:</p> 
  </div> 
  <div class="callout-container sidebar-container prompt-response-sidebar"> 
   <div class="readable-text" id="p66"> 
    <p><strong class="response-head-image"><img alt="image" src="../Images/Response-Chatgpt.png" width="4%"/></strong><span class="CharOverride-5"> </span>It seems like your message may have been cut off. If you were about to inquire about the Eiffel Tower, here's a brief overview:</p> 
   </div> 
   <div class="readable-text" id="p67"> 
    <p>The Eiffenstein Tower is one of the most iconic landmarks in the world and an enduring symbol of France. Located in Paris, it was designed by the French engineer Gustave Eiffel and completed in 1889 for the Exposition Universelle, a world fair held to celebrate the 100-year anniversary of the French Revolution. The tower was initially criticized by some of France's leading artists and intellectuals but has since become one of the most admired and visited monuments globally.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>Note that the response contains the made-up “Eiffenstein Tower,” even though it mentions the “Eiffel Tower” before. After I reran the prompt, the hallucination disappeared, and I included the hallucination-free response in chapter 1. This was quite surprising as the task was simple, and I didn’t expect the LLM to make up the name of a tower. </p> 
  </div> 
  <div class="readable-text intended-text" id="p69"> 
   <p>LLMs are routinely fine-tuned to overcome well-documented hallucinations, but others seem to always pop up. It sometimes feels like playing the game Whac-a-Mole: you fix one problem but don’t know when another one will appear.</p> 
  </div> 
  <div class="readable-text" id="p70"> 
   <h2 class=" readable-text-h2">Why does AI hallucinate?</h2> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>It is tempting to think that hallucinations are just bugs requiring a minor fix. However, the problem seems to run deeper than that. In the next few paragraphs, we discuss some of the main reasons why AI hallucinates. Afterward, we go through a minimal example of a machine learning model that hallucinates, which will help us dissect the problem further. Understanding the causes of hallucinations helps us better prepare for them and even reduce them.</p> 
  </div> 
  <div class="readable-text" id="p72"> 
   <h3 class=" readable-text-h3"> Inadequate world models</h3> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>As discussed in chapter 1, current AI learns from examples of how to do the job. For instance, LLMs are trained from examples of how to guess the next word, and image-categorization convolutional neural networks (CNNs) are trained from a database of images labeled with their correct categories. Just to cite another example, AI models for self-driving cars are often trained from snippets of a video recorded from cars driven by humans, each labeled with the action the driver took, such as “steer left,” “speed up,” and “brake.”</p> 
  </div> 
  <div class="readable-text intended-text" id="p74"> 
   <p>Sometimes, learning to perform a task just by seeing an example is straightforward. Consider the case of learning to read a car’s license plates from a video. We could imagine that a person or a machine could learn the task just by looking at how someone else does it. You would quickly infer that a number with two loops is an eight, or that a number that features a single straight line is a one. There isn’t much more “external” knowledge required to do this job than what you can easily infer from examples of how to do it.</p> 
  </div> 
  <div class="readable-text intended-text" id="p75"> 
   <p>Now, consider the case of driving a car on a busy road. Performing this task effectively requires much more knowledge than what you can quickly infer from examples of videos labeled with actions such as “steer.” Follow me on a thought experiment to make this point.</p> 
  </div> 
  <div class="readable-text intended-text" id="p76"> 
   <p>Imagine you’re driving on a motorway, and a flying umbrella blocks your way. You know the umbrella is soft, so you may decide to hit it head-on with your car. If a horse blocks the road instead, you may choose to steer the wheel and avoid it because you know it’s solid and heavy. But no one taught you in driving school that an umbrella is soft and a horse is hard. Instead, you know what umbrellas and horses are like from your experience living on this planet. This experience has helped you build a comprehensive <em>world model</em> that describes the world we live in, including the solidity of objects. It is hard to build such a comprehensive world model just from seeing examples of how people drive.</p> 
  </div> 
  <div class="readable-text intended-text" id="p77"> 
   <p>LLMs build an internal world model to a certain extent. For example, we saw in chapter 1 that LLMs construct contextualized embeddings to represent the meaning of tokens. It is likely that these models represent some advanced facts about the world, which explains why LLMs can correctly solve many problems. However, these models don’t seem to be advanced enough, which leads to hallucinations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p78"> 
   <p>For example, LLMs’ internal world models often contain shortcuts or a memorization of common solutions to problems instead of a genuine way of solving them. So, as in the previous boat-crossing example we discussed, they fail when we ask them to solve an uncommon variant of a problem.</p> 
  </div> 
  <div class="readable-text intended-text" id="p79"> 
   <p>Deficient world models are also observed in other types of AI. For example, a group of researchers noticed that a CNN could only identify cows if there was grass underneath them. Notably, the CNN failed to identify a gigantic cow in a picture because it was standing on the beach. Instead of learning what a cow actually was, the model had learned that the combination of a cow and the grass was what made a cow <em>a cow</em>. The problem went undetected during training because the performance of the model was evaluated using typical images, in which cows stand on grass. </p> 
  </div> 
  <div class="readable-text intended-text" id="p80"> 
   <p>Many similar problems have been observed with self-driving cars. Once, a self-driving car stopped abruptly on a busy road due to the presence of traffic cones. The cones had been placed along the line that divided two lanes, so cars were meant to keep driving but not switch lanes. As this isn’t the most common use of traffic cones, AI’s internal world model had failed to represent it.</p> 
  </div> 
  <div class="readable-text" id="p81"> 
   <h3 class=" readable-text-h3"> World models: Theory vs. practice</h3> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>A purist might tell you that, in theory, learning by example should be enough to build the most comprehensive world models. All you need is a huge amount of varied data. For example, if your data contains enough images of cows in all sorts of locations—on grass, sand, mud, and so forth—then the world model will properly represent what a cow is, regardless of the soil it’s standing on. Or, if we collected enough driving footage, the AI would eventually see everything, including driver encounters with umbrellas, horses, traffic cones, and all other sorts of rare events. Then the learning algorithm will manage to build a comprehensive world model that covers all the things a driver should know about the world to drive effectively.</p> 
  </div> 
  <div class="readable-text intended-text" id="p83"> 
   <p>The issue is that, even though this is all very appealing in theory, it doesn’t work very well in practice. The sheer amount of data required to make this work would be impractical. Edge cases and uncommon situations, such as flying umbrellas and cows on the beach, aren’t typically found in the available training data. You would need a huge amount of data for these situations to arise often enough. Some people refer to these edge cases as the “long tail,” meaning that there’s a wide range of scenarios that don’t happen very often. </p> 
  </div> 
  <div class="readable-text intended-text" id="p84"> 
   <p>When I asked ChatGPT to list book titles with lengthy words, my question was rather odd. It is unlikely that many people on the internet are writing about this. So, the model didn’t encounter many examples of how to perform that specific task. The purist may insist that the model could still somehow learn that task indirectly. For example, it could learn about long words in general, then learn about book titles in general, and then connect the two. However, this doesn<span class="CharOverride-6">’</span>t happen in practice. </p> 
  </div> 
  <div class="readable-text" id="p85"> 
   <h3 class=" readable-text-h3"> Misaligned objectives</h3> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>AI models are trained to pursue an objective. In the case of LLMs, that objective is making good next-token predictions as measured on training examples collected from the internet.  </p> 
  </div> 
  <div class="readable-text intended-text" id="p87"> 
   <p>The problem is that this objective is not exactly what we want to use LLMs for, which is to produce factual text and correct solutions to problems. The two objectives are related—the most probable next token may often coincide with the most factual one. However, these two objectives are not the same.</p> 
  </div> 
  <div class="readable-text intended-text" id="p88"> 
   <p>So, there is a wedge between what we train the model for and what we want to use it for. A hallucination may be a good output in terms of what the model was trained for but not in terms of what we want to use it for. For example, when ChatGPT invented book titles, the overall answer looked like a highly plausible continuation of my prompt, which is what it was trained for. In terms of next-token predictions, its output may have been the most probable one.</p> 
  </div> 
  <div class="readable-text intended-text" id="p89"> 
   <p>As discussed in chapter 1, OpenAI acknowledged the misalignment of goals as a source of hallucinations: “The language modeling objective used for many recent large LMs—predicting the next token on a webpage from the internet—is different from the objective ‘follow the user’s instructions helpfully and safely.’” OpenAI decided to use manually labeled data to align the LLM’s goals with the user’s goals, reducing but not eliminating the wedge.</p> 
  </div> 
  <div class="readable-text intended-text" id="p90"> 
   <p>In a provocative article titled, “ChatGPT Is Bullshit” <span class="CharOverride-8">(</span><a href="https://mng.bz/yWRe"><span class="Hyperlink CharOverride-9">https:</span><span class="Hyperlink CharOverride-9">/</span><span class="Hyperlink CharOverride-9">/mng.bz/yWRe</span></a><span class="CharOverride-8">), </span>researchers from the University of Glasgow described the misalignment as follows:</p> 
  </div> 
  <div class="readable-text" id="p91"> 
   <blockquote>
    <div>
     Because they are designed to produce text that 
     <em>looks</em> truth-apt without any concern for truth, it seems appropriate to call their outputs bullshit. . . . It’s not surprising that LLMs have a problem with the truth. Their goal is to produce a normal-seeming response to a prompt, not to convey information that is helpful to their 
     <br/>interlocutor.
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>The authors also argued that using a RAG approach, in which the LLM’s prompt is augmented with a database of up-to-date, factual text, doesn’t solve the problem:</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <blockquote>
    <div>
     They are not designed to represent the world at all; instead, they are designed to convey convincing lines of text. So, when they are provided with a database of some sort, they use this, in one way or another, to make their responses more convincing.
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>Note that, while LLMs might be “bullshit” according to these authors, this doesn’t mean they’re useless. For example, a RAG approach may be useful to find answers from a database of text, provided that the user is aware of the misalignment and thus makes sure to double-check answers. </p> 
  </div> 
  <div class="readable-text" id="p95"> 
   <h3 class=" readable-text-h3"> Toy hallucination example: Price optimization</h3> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>If you charge too little for a product, you may get more sales but less revenue in total, and if you charge too much, you may collect more on each sale but lose too many sales. The revenue-maximizing price is a sweet spot in between.</p> 
  </div> 
  <div class="readable-text intended-text" id="p97"> 
   <p>I’ve known of companies that used machine learning to try to find the revenue-maximizing price for a product. However, the resulting models hallucinated. Let’s see why.</p> 
  </div> 
  <div class="readable-text intended-text" id="p98"> 
   <p>Suppose an e-commerce store creates a machine learning model to predict whether a visitor will purchase a product. The inputs to the model are characteristics of the product (e.g., price, color, and star rating) and of the customer (e.g., age and location). The output is the probability of buying:</p> 
  </div> 
  <div class="readable-text centered-text" id="p99"> 
   <p>Product features + Customer features -&gt; Model -&gt; <br/>Probability customer will buy product</p> 
  </div> 
  <div class="readable-text" id="p100"> 
   <p>The model is trained in a supervised way using a historical record of which products were bought by which clients, and which ones were ignored. Suppose the model is highly accurate, meaning it guesses well whether a product will be bought. </p> 
  </div> 
  <div class="readable-text intended-text" id="p101"> 
   <p>After building this model, the company uses it to find the revenue-optimizing price of a certain product. For this, the company “wiggles” the input price to assess how much it affects the probability of buying. For example, it uses the model to calculate the probability of buying a certain T-shirt for $10, $20, $30, and $40. This lets the company find the revenue-maximizing price. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p102"> 
   <p><span class="print-book-callout-head">NOTE</span> The revenue-maximizing price is the one that maximizes the probability of buying the product times its price (Expected revenue = Probability of buying × Price).</p> 
  </div> 
  <div class="readable-text" id="p103"> 
   <p>Unfortunately, I’ve seen this kind of model hallucinate about the probability of buying when the price is varied. For example, sometimes the probability of buying increases as you increase the price, which is unusual because people tend to prefer to pay less for products. Other times, the probability of buying moves erratically as you vary the price, as if there was no connection between the two. Or the model outputs a high probability of buying a $10,000 T-shirt. </p> 
  </div> 
  <div class="readable-text intended-text" id="p104"> 
   <p>One of the reasons this happens is that the training data doesn’t usually contain examples of the product being sold for different prices, as companies don’t experiment too much with varying prices. For instance, a T-shirt may have always been priced at $30 in the past.</p> 
  </div> 
  <div class="readable-text intended-text" id="p105"> 
   <p>Consequently, the model struggles to learn anything about selling the products for alternative prices. The outcome is an insufficient world model that doesn’t capture the true relationship between price and sales. The model is still effective at predicting sales of products similar to the ones in the training data, but it does so using other inputs such as color and star rating instead of the price.</p> 
  </div> 
  <div class="readable-text intended-text" id="p106"> 
   <p>When this company uses the model to analyze prices, it also suffers from a misaligned objective. The model was trained for one thing (i.e., predict whether a product will be bought) and used for something else (i.e., analyze the effect of varying prices on sales). </p> 
  </div> 
  <div class="readable-text intended-text" id="p107"> 
   <p>Note that because of the misalignment of objectives, there is no “loss” during training associated with the hallucinated outputs (see chapter 1). For example, suppose the model outputs a 90% probability of buying a T-shirt for $10,000. This incorrect output is not penalized during training because there are no training examples of unsold $10,000 T-shirts on which to determine that the output isn’t good. </p> 
  </div> 
  <div class="readable-text intended-text" id="p108"> 
   <p>Unfortunately, I’ve seen many companies fall prey to this type of hallucination. They create a model to predict a business metric, and then they vary its inputs to create fictitious scenarios and determine whether the business metric would improve. Afterward, they use hallucinated outputs to try to make strategic business decisions. </p> 
  </div> 
  <div class="readable-text" id="p109"> 
   <h2 class=" readable-text-h2">Will hallucinations go away?</h2> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>Several impediments to solving hallucinations have been raised. One of them is the amount of available training data. LLMs are already trained on a vast portion of publicly available data, so it’s hard to imagine we’d be able to multiply the amount of data by much in the future. A group of researchers argued that “if current LLM development trends continue, models will be trained on datasets roughly equal in size to the available stock of public human text data between 2026 and 2032” (see <a href="https://arxiv.org/pdf/2211.04325v2"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/pdf/2211.04325v2</span></a>). Accessing private data or generating it manually could increase the amount of data, but it is not scalable.</p> 
  </div> 
  <div class="readable-text intended-text" id="p111"> 
   <p>In addition, we might need much more data than we think to continue improving LLMs. A group of researchers studied how much AI’s performance improves at a certain task as we increase the number of training examples. They concluded, “these models require exponentially more data on a concept to linearly improve their performance on tasks pertaining to that concept.”</p> 
  </div> 
  <div class="readable-text intended-text" id="p112"> 
   <p>In addition to problems with data, some people believe that our current way of formulating AI tasks, such as autoregressive LLMs, is, in itself, lacking. Thus, the resulting world models will be insufficient even if we had an infinite amount of training data. </p> 
  </div> 
  <div class="readable-text intended-text" id="p113"> 
   <p>Yann LeCun, the inventor of CNNs, argues, “Hallucinations in LLM are due to the Auto-Regressive prediction” (<a href="https://x.com/ylecun/status/1667218790625468416"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/x</span><span class="Hyperlink">.com/ylecun/status/1667218790625468416</span></a>). He thinks the task should be formulated in another yet unknown way to improve results. He also thinks the problem might be that LLMs are all about text, while we reason in other terms sometimes <span class="CharOverride-8">(</span><a href="https://mng.bz/MDM8"><span class="Hyperlink CharOverride-9">https:</span><span class="Hyperlink CharOverride-9">/</span><span class="Hyperlink CharOverride-9">/mng.bz/MDM8</span></a><span class="CharOverride-8">)</span>: </p> 
  </div> 
  <div class="readable-text" id="p114"> 
   <blockquote>
    <div>
     LLMs have no physical intuition because they are trained exclu­sively on text. They may correctly answer questions that appeal to physical intuition if they can retrieve an answer to a similar question from their vast associative memory.
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p115"> 
   <blockquote>
    <div>
     But they may get the answer *completely* wrong. 
     <span class="CharOverride-3"> . . .</span> We have mental models of the world in our minds that allow us to simulate what will happen.
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p116"> 
   <blockquote>
    <div>
     That’s what gives us common sense.
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p117"> 
   <blockquote>
    <div>
     LLMs don't have that.
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p118"> 
   <p>In addition, LeCun has pointed out that another limitation might be that LLMs produce an output in a fixed number of steps (see Yann LeCun at Lex Fridman’s podcast at <a href="https://www.youtube.com/watch?v=5t1vTLU7s40"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/www.youtube.com/watch?v=5t1vTLU7s40</span></a>). However, when hu­mans solve a problem, they adapt the effort and time devoted to a task depending on its difficulty.</p> 
  </div> 
  <div class="readable-text intended-text" id="p119"> 
   <p>By the looks of it, a new methodology must be invented to get rid of hallucinations. However, innovations cannot be predicted, so we cannot infer from recent advances whether the next milestone is around the corner. </p> 
  </div> 
  <div class="readable-text intended-text" id="p120"> 
   <p>Beware of anyone making predictions about inventions, as these are rarely accurate. Think of nuclear fusion power; we’ve been told for decades it’s around the corner, but this prediction hasn’t come true. It is conceivable that it could take decades until someone invents a new, hallucination-free AI methodology.</p> 
  </div> 
  <div class="readable-text intended-text" id="p121"> 
   <p>As hallucinations seem to be here to stay, it’s best that we learn to live with them. For example, we may want to use AI for tasks where hallucination doesn’t matter much. Or we may want to take actions to mitigate them.</p> 
  </div> 
  <div class="readable-text" id="p122"> 
   <h2 class=" readable-text-h2">Mitigation</h2> 
  </div> 
  <div class="readable-text" id="p123"> 
   <p>There is an increasing body of literature on techniques to mitigate hallucinations. Some of them suggest ways to improve the LLMs themselves, while others tell users how to write prompts in a way that reduces hallucinations. </p> 
  </div> 
  <div class="readable-text intended-text" id="p124"> 
   <p>In terms of improving LLMs, a common suggestion is to curate the training data. An article suggests “to collect high-quality factual data to prevent the introduction of misinformation and conduct data cleansing to debias” (<a href="https://arxiv.org/pdf/2311.05232"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/pdf/2311.05232</span></a>). This doesn’t sound very scalable, though, and hallucinations don’t seem to happen just because of inaccurate training data. (I couldn’t find any online references of the “Eiffenstein Tower.”) </p> 
  </div> 
  <div class="readable-text intended-text" id="p125"> 
   <p>Another approach is using manually generated feedback to better align the models. As discussed in chapter 1, this is how companies such as OpenAI are reducing hallucinations—they use <em>reinforcement learning with human feedback</em>, or RLHF, which is a way to refine models using humanly generated feedback. While effective to some extent, this is not very scalable.</p> 
  </div> 
  <div class="readable-text intended-text" id="p126"> 
   <p>Some researchers have been trying to modify the training process to reduce hallucinations. For example, a group of researchers injected the title of a Wikipedia article before each sentence inside the article (see <a href="https://arxiv.org/pdf/2206.04624"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/pdf/2206.04624</span></a>). This turned a sentence like “He previously served as a U.S. senator from Illinois from 2005 to 2008” into “Barack Obama. He previously served as a U.S. senator from Illinois from 2005 to 2008.” This helped reduce hallucinations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p127"> 
   <p>From a user’s perspective, there are special ways to write a prompt that help mitigate hallucinations. This has led to study and popularization of practices that enable writing more effective prompts, which are known as prompt engineering (check out <em>Prompt Engineering in Practice</em> by Richard Davis, Manning, 2025; <a href="https://mng.bz/avlX"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/avlX</span></a>). </p> 
  </div> 
  <div class="readable-text intended-text" id="p128"> 
   <p>One popular prompt engineering technique, known as <em class="CharOverride-2">chain-of-thought prompting</em>, involves including a step-by-step example of how to perform the task in the prompt, before asking the LLM to perform a similar task. The authors of this technique explain, “A chain of thought is a series of intermediate natural language reasoning steps that lead to the final output” (<a href="https://arxiv.org/pdf/2201.11903"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/pdf/2201.11903</span></a>). </p> 
  </div> 
  <div class="readable-text intended-text" id="p129"> 
   <p>Here's an example of a chain-of-thought prompt:</p> 
  </div> 
  <div class="readable-text prompt" id="p130"> 
   <p><strong class="prompt-head-image"><img alt="image" src="../Images/Prompt-Icon.png" width="4%"/></strong> Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?</p> 
  </div> 
  <div class="callout-container sidebar-container prompt-response-sidebar"> 
   <div class="readable-text" id="p131"> 
    <p><strong class="response-head-image"><img alt="image" src="../Images/Response-Chatgpt.png" width="4%"/></strong><span class="CharOverride-5"> </span><strong>Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11</strong><span class="CharOverride-6">.</span> The answer is 11.</p> 
   </div> 
  </div> 
  <div class="readable-text prompt" id="p132"> 
   <p><strong class="prompt-head-image"><img alt="image" src="../Images/Prompt-Icon.png" width="4%"/></strong> The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?</p> 
  </div> 
  <div class="readable-text" id="p133"> 
   <p>This prompt includes an example of how to solve the problem before asking the LLM to solve another, similar problem. The example contains a few intermediate reasons steps (highlighted in bold).</p> 
  </div> 
  <div class="readable-text intended-text" id="p134"> 
   <p>The inventors of this method showed that if the highlighted sentences were not included in the prompt, the LLM solved the problem incorrectly. However, if they were included, the answer was correct. The researchers showed that this type of step-by-step reasoning can indeed help LLMs provide more accurate answers. </p> 
  </div> 
  <div class="readable-text intended-text" id="p135"> 
   <p>Using a RAG approach has also been observed to reduce hallucinations, as the LLM can extract information from relevant, domain-specific documents instead of just relying on its internal representation of language. A group of researchers explained (<a href="https://arxiv.org/pdf/2405.20362"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/pdf/2405.20362</span></a>):</p> 
  </div> 
  <div class="readable-text" id="p136"> 
   <blockquote>
    <div>
     Including retrieved information in the prompt allows the model to respond in an “open-book” setting rather than in “closed-book” one. The LLM can use the information in the retrieved documents to inform its response, rather than its hazy internal knowledge. Instead of generating text that conforms to the general trends of a highly compressed representation of its training data, the LLM can rely on the full text of the relevant information that is injected directly into its prompt.
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p137"> 
   <p>Finally, a promising direction of work is the use of multi­agent AI, in which multiple LLMs cooperate to verify one another’s output. For example, a group of researchers proposed a multi­agent approach to mitigate hallucination in software development tasks (see <a href="https://arxiv.org/pdf/2307.07924"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/pdf/2307.07924</span></a>). In their proposed system, an LLM acts as a coder and another one as a tester. Both are prompted to perform their respective duties effectively. The coder LLM is asked to generate a piece of code, then the tester LLM is asked to evaluate the code and point out problems, then the coder LLM is asked to refine its code based on this feedback, and so on. The authors call this “communicative dehallucination.” Sometimes this approach improves results as the tester LLM correctly identifies errors. Other times, however, the tester fails to identify mistakes or generates incorrect tests.</p> 
  </div> 
  <div class="readable-text intended-text" id="p138"> 
   <p>In addition to trying to mitigate hallucinations, some people have been studying ways of detecting them. One promising way is to analyze the probabilities outputted by the LLM. If you recall, LLMs output a probability value for each possible next token, and the next token is sampled using those probabilities. Researchers have shown that when output probabilities are overall low, LLMs tend to hallucinate more (see <a href="https://arxiv.org/pdf/2307.03987"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/pdf/2307.03987</span></a>). This shows that an LLM’s lack of confidence about its output is correlated with hallucinations. Thus, the user can detect low-probability outputs and validate them.</p> 
  </div> 
  <div class="readable-text" id="p139"> 
   <h2 class=" readable-text-h2">Hallucinations can kill a product</h2> 
  </div> 
  <div class="readable-text" id="p140"> 
   <p>The presence of hallucinations can sometimes harm the success of certain sensitive products. For example, the customer-­service chatbot of a major airline provided hallucinated information to a passenger on how to obtain a refund. The airline refused to proceed with the refund citing that the actual conditions were different from what the chatbot had indicated. A court ordered the company to honor the refund anyway, saying that the airline “does not explain why customers should have to double-check information found in one part of its website on another part of its website.” The story made headlines, and the airline disabled the chatbot soon after (see <a href="https://mng.bz/galG"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/galG</span></a>). </p> 
  </div> 
  <div class="readable-text intended-text" id="p141"> 
   <p>The industry of self-driving cars has perhaps been the greatest casualty of AI’s hallucinations. Once a booming industry, now it is flailing, and its future is uncertain. One of the main reasons is that self-driving cars keep making surprisingly bad decisions due to hallucinations, especially in uncommon situations that aren’t present in the training data. </p> 
  </div> 
  <div class="readable-text intended-text" id="p142"> 
   <p>For example, in October 2023, a self-driving car hit a pedestrian in California right after she’d been hit by another car. The pedestrian was visible in the camera’s sensors, yet the AI didn’t classify her correctly. An engineering firm explained, “The pedestrian’s feet and lower legs were visible in the wide-angle left side camera from the time of impact to the final stop, but, despite briefly detecting the legs, neither the pedestrian nor her legs were classified or tracked by the vehicle” (see <a href="https://mng.bz/eyAq"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/eyAq</span></a>). Instead of stopping, the self-driving car continued driving, dragging the pedestrian 20 feet.</p> 
  </div> 
  <div class="readable-text intended-text" id="p143"> 
   <p>The car in question had been manufactured by Cruise, one of the foremost self-driving car companies and a subsidiary of General Motors. After the incident, Cruise had its license to operate in California revoked, and the company decided to recall all its vehicles in the United States. </p> 
  </div> 
  <div class="readable-text intended-text" id="p144"> 
   <p>A month after the incident, it was revealed that Cruise cars weren’t actually driving themselves as much as it appeared. Instead, humans had to remotely intervene every 2.5 to 5 miles to assist the vehicles (see <a href="https://mng.bz/pKlw"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/pKlw</span></a>). </p> 
  </div> 
  <div class="readable-text intended-text" id="p145"> 
   <p>A few months later, Waymo, which is Google’s self-driving car initiative, was involved in a similar scandal. A Waymo car hit a truck that was being towed in an unusual way. A few minutes later, another Waymo car hit the same truck. Waymo explained <span class="CharOverride-8">(</span><a href="https://mng.bz/OBga"><span class="Hyperlink CharOverride-9">https:</span><span class="Hyperlink CharOverride-9">/</span><span class="Hyperlink CharOverride-9">/mng.bz/OBga</span></a><span class="CharOverride-8">)</span>, </p> 
  </div> 
  <div class="readable-text" id="p146"> 
   <blockquote>
    <div>
     A Waymo vehicle made contact with a backwards-facing pickup truck being improperly towed ahead of the Waymo vehicle such that the pickup truck was persistently angled across a center turn lane and a traffic lane . . . and a few minutes later another Waymo vehicle made contact with the same pickup truck while it was being towed in the same manner. . . . We determined that due to the persistent orientation mismatch of the towed pickup truck and tow truck combination, the Waymo autonomous vehicle incorrectly predicted the future motion of the towed vehicle.
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p147"> 
   <p>As we can see from Waymo’s explanation, the manufacturers attribute the problem to the truck being towed in an unusual way, which made AI not recognize the truck as such. This is an example of AI not coping with an edge case.</p> 
  </div> 
  <div class="readable-text intended-text" id="p148"> 
   <p>As is often the case with hallucinations, Waymo engineers took action to patch this specific problem with ad hoc actions. Waymo explained, “After developing, rigorously testing, and validating a fix, on December 20, 2023, we began deploying a software update to our fleet to address this issue.” But what about other unusual problems Waymo cars haven’t been specifically patched to deal with? What if a truck is painted with an unusual color or a pedestrian is wearing an unusual wig?</p> 
  </div> 
  <div class="readable-text intended-text" id="p149"> 
   <p>Applying patch after patch doesn’t seem to be working well for the industry, as problems persist, and some companies are giving up. A Bloomberg article declared, “Even after $100 billion, self-driving cars are going nowhere” (<a href="https://mng.bz/YDja"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/YDja</span></a>). Uber, Lyft, Ford, and Volkswagen have all abandoned their self-driving initiatives. The remaining contenders, Cruise and Waymo being among the most important ones, keep moving their goalposts. Unless we discover a new AI methodology that doesn’t hallucinate, they’ll probably have to keep moving them. </p> 
  </div> 
  <div class="readable-text" id="p150"> 
   <h2 class=" readable-text-h2">Living with hallucinations</h2> 
  </div> 
  <div class="readable-text" id="p151"> 
   <p>Because hallucinations might remain part of AI for quite some time, it’s best to learn how to live with them. We should keep them in mind from the very start when we use AI or build an AI-related product. In chapter 4, we’ll discuss that there are many AI applications in which hallucinations aren’t a big problem, so we have the highest chances of building a successful AI product. In other cases, in which hallucinations matter, we should assess their effects and think of mitigation and detection strategies early on. </p> 
  </div> 
  <div class="readable-text" id="p152"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p153">Hallucinations are confidently wrong outputs generated by AI. </li> 
   <li class="readable-text" id="p154">Common types of hallucinations are made-up facts, misinterpreted information, and incorrect solutions to problems.</li> 
   <li class="readable-text" id="p155">One cause of hallucinations is that AI’s internal world model is insufficient to describe how our world operates. </li> 
   <li class="readable-text" id="p156">Another cause is that AI models are often trained to do one thing and used for something else—they’re misaligned with our goals.</li> 
   <li class="readable-text" id="p157">Hallucinations are not going away anytime soon because this would require modifying prevailing machine learning methods in a yet-unknown way.</li> 
   <li class="readable-text" id="p158">Hallucinations are sometimes unacceptable or unsafe for users, which can deeply hurt a product’s chances of success.</li> 
   <li class="readable-text" id="p159">Hallucinations can be mitigated by using prompt engineering techniques, and they can be detected sometimes.</li> 
   <li class="readable-text" id="p160">We must keep hallucinations in mind throughout the life cycle of an AI-related product.</li> 
  </ul>
 </body></html>