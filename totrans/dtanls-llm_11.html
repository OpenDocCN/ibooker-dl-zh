<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1" id="ch__optimization"> <span class="chapter-title-numbering"><span class="num-string">9</span></span> <span class="title-text"> Optimizing cost and quality</span> </h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li id="p2">Model choice and tuning</li> 
    <li id="p3">Prompt engineering</li> 
    <li id="p4">Fine-tuning models</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>Analyzing data with large language models is a great way to burn money quickly. If you’ve been using GPT-4 (or a similarly large model) for a while, you’ve probably noticed how fees pile up quickly, forcing you to recharge your account regularly. But do we always need to use the largest (and most expensive) model? Can’t we make smaller models perform almost as well? How can we get the most bang for our buck?</p> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>This chapter is about saving money when using language models on large data sets. Fortunately, we have quite a few options for doing so. First, we have lots of choices when it comes to large language models. Selecting a model that is as small (or, rather, as cheap) as possible while still performing well on our analysis task can go a long way toward balancing our budget. Second, models typically have various tuning parameters, allowing us to tune everything from the overall text generation strategy to the way specific tokens are (de-)prioritized. We want to optimize our settings there to turn small models into GPT-4 alternatives for certain tasks. Third, we can use prompt engineering to tweak the way we ask the model our questions, sometimes leading to surprisingly different results!</p> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>And finally, if none of these methods cut it, we can choose to create our own models, highly customized for only the task we care about. Of course, assuming we don’t want to spend millions on pretraining, we won’t start training new models from scratch. Instead, we will typically choose to fine-tune existing models with just a few hundred samples. That’s often enough to get significantly better performance than when using the base model.</p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <p>Of course, what works best depends on the task we’re trying to solve, as well as on data properties. Fortunately, if we want to analyze large amounts of data, we can afford to spend a little money on trying different tuning options on a data sample. Chances are, this upfront investment will pay off once we analyze the entire data set! Throughout this chapter, we will apply all of these tuning options in an example scenario.</p> 
  </div> 
  <div class="readable-text" id="p9"> 
   <h2 class=" readable-text-h2" id="example-scenario"><span class="num-string browsable-reference-id">9.1</span> Example scenario</h2> 
  </div> 
  <div class="readable-text" id="p10"> 
   <p>You’re back at Banana and trying to classify user reviews. Users can leave free-form text reviews about their experiences with Banana products on the Banana website. You want to know whether those reviews are positive (i.e., the user was happy with the product) or negative (i.e., reading them will scare away potential customers!). Of course, you can use language models for that task (you saw that in chapter 4). For instance, you can use GPT-4 (at the time of writing, this is OpenAI’s largest model for text processing). Provide GPT-4 with a review, together with instructions for how to classify it (including a description of possible class labels, such as “positive” and “negative”), and the output should be correct for most reviews.</p> 
  </div> 
  <div class="readable-text" id="p11"> 
   <p>However, analyzing data with GPT-4 costs about 6 cents per 1,000 tokens. That (6 cents) may not sound like much, but Banana receives thousands of product reviews every day! Let’s assume the average review contains about 100 tokens (about 400 characters). Furthermore, let’s assume that Banana receives about 10,000 reviews per day. That means you collect 100 × 10,000 tokens per day: about 1 million tokens per day and 365 million tokens per year. How much does it cost to analyze one year’s worth of comments? About 365,000,000 × (0.06/1000) = 21,900 dollars.</p> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p>That may put a bit of a dent in your budget! Can’t you get it cheaper? For example, at the time of writing, GPT-3.5 Turbo is priced at only around 0.0005 dollars per thousand tokens (tokens are priced differently depending on whether they are read or generated, but we will neglect that for now to simplify the calculations). That means only 365,000,000 × (0.0005/1000) = 182.5 dollars to analyze one year’s worth of comments. Much better! But to get satisfactory output quality, you may have to do a little extra work to ensure that you’re using the model in the best possible way.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p13"> 
   <p> <span class="print-book-callout-head">Tip</span> Instead of GPT-3.5 Turbo, you can also use alternative models such as GPT-4o mini (the model ID is <code>gpt-4o-mini</code>) in the following examples. </p> 
  </div> 
  <div class="readable-text" id="p14"> 
   <p>That’s what we will do in this example. Starting from the most naive implementation of our classifier, we will gradually refine our implementation and try all the various tuning options discussed in the introduction to this chapter!</p> 
  </div> 
  <div class="readable-text" id="p15"> 
   <h2 class=" readable-text-h2" id="untuned-classifier"><span class="num-string browsable-reference-id">9.2</span> Untuned classifier</h2> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>Let’s begin with the base version of our classifier. Again, the goal is to take a review and decide whether it should be classified as positive (<code>pos</code>) or negative (<code>neg</code>). We will use the following prompt template to classify reviews:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p17"> 
   <div class="code-area-container"> 
    <pre class="code-area">[Review]
Is the sentiment positive or negative?
Answer ("pos"/"neg"):    </pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p18"> 
   <p>In this prompt template, <code>[Review]</code> is a placeholder that gets replaced with the actual review text. For example, after substitution, our prompt may look like this (the first two lines correspond to an abbreviated version of the review to classify, apparently a new movie streaming on Banana TV that doesn’t match the reviewer’s taste):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p19"> 
   <div class="code-area-container"> 
    <pre class="code-area">I am willing to tolerate almost anything in a Sci-Fi movie, 
but this was almost intolerable. ...
Is the sentiment positive or negative?
Answer ("pos"/"neg"):    </pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p20"> 
   <p>Ideally, if we send this prompt to a GPT model, we expect either <code>pos</code> or <code>neg</code> as the reply (in this specific case, we expect <code>neg</code>). Listing <a href="#code__untuned">9.1</a> shows the complete Python code; we won’t spend too much time discussing it because it is similar to the classifiers we saw in chapter 4. The <code>create_prompt</code> function (<strong class="cueball">1</strong>) instantiates the prompt template for a specific review (stored in the input parameter <code>text</code>). The result is a prompt that we can send to our language model using the <code>call_llm</code> function (<strong class="cueball">2</strong>). We call GPT-3.5 Turbo here (<strong class="cueball">3</strong>) (saving costs). We also set <code>temperature</code> to <code>0</code>, which means we’re minimizing randomness when generating output. This means you should see the same results when running the code repeatedly. You may also notice that <code>call_llm</code> is a little longer in listing <a href="#code__untuned">9.1</a> than the versions we have seen in previous listings. That’s because we retrieve not only the answer generated by our language model but also the number of tokens used (<strong class="cueball">4</strong>). Counting the number of tokens will allow us to calculate the invocation costs on a data sample.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p21"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__untuned"><span class="num-string">Listing <span class="browsable-reference-id">9.1</span></span> Classifying reviews as positive or negative: base version</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse
import openai
import pandas as pd
import time

client = openai.OpenAI()

def create_prompt(text):                         #1
    """ Create prompt for sentiment classification.
    
    Args:
        text: text to classify.
    
    Returns:
        Prompt for text classification.
    """
    task = 'Is the sentiment positive or negative?'
    answer_format = 'Answer ("pos"/"neg")'
    return f'{text}\n{task}\n{answer_format}:'

def call_llm(prompt):                              #2
    """ Query large language model and return answer.
    
    Args:
        prompt: input prompt for language model.
    
    Returns:
        Answer by language model and total number of tokens.
    """
    for nr_retries in range(1, 4):
        try:
             #3
            response = client.chat.completions.create(
                model='gpt-3.5-turbo',
                messages=[
                    {'role':'user', 'content':prompt}
                    ],
                temperature=0
                )
             #4
            answer = response.choices[0].message.content
            nr_tokens = response.usage.total_tokens
            return answer, nr_tokens
        
        except Exception as e:
            print(f'Exception: {e}')
            time.sleep(nr_retries * 2)
    
    raise Exception('Cannot query OpenAI model!')

if __name__ == '__main__':

    parser = argparse.ArgumentParser()                   #5
    parser.add_argument('file_path', type=str, help='Path to input file')
    args = parser.parse_args()
    
    df = pd.read_csv(args.file_path)
    
    nr_correct = 0
    nr_tokens = 0
    
    for _, row in df.iterrows():  #6
        
        text = row['text']       #7
        prompt = create_prompt(text)
        label, current_tokens = call_llm(prompt)
        
        ground_truth = row['sentiment']      #8
        if label == ground_truth:
            nr_correct += 1
        nr_tokens += current_tokens
        
        print(f'Label: {label}; Ground truth: {ground_truth}')

    print(f'Number of correct labels:\t{nr_correct}')
    print(f'Number of tokens used   :\t{nr_tokens}')</pre> 
    <div class="code-annotations-overlay-container">
     #1 Generates prompts
     <br/>#2 Invokes the LLM
     <br/>#3 Generates an answer
     <br/>#4 Extracts answer and token usage
     <br/>#5 Parses arguments
     <br/>#6 Iterates over reviews
     <br/>#7 Classifies the review
     <br/>#8 Updates counters
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p22"> 
   <p>We will assume that reviews to classify are stored in a .csv file. We expect users to specify the path of that .csv file as a command-line argument (<strong class="cueball">5</strong>). After reading the .csv file, we iterate over the reviews (<strong class="cueball">6</strong>) in the order in which they appear in the input file. For each review, we extract the associated text (<strong class="cueball">7</strong>) (we assume it’s stored in the <code>text</code> column), create a prompt for classification, and call the language model. The result is the answer text generated by the language model (hopefully it’s one of the two class labels, <code>pos</code> or <code>neg</code>), as well as the number of tokens used.</p> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>Our goal is to try different methods of querying a language model and compare the output quality and costs. To judge the output quality, we assume that the input .csv file contains not only the review text but also a ground-truth label. This means we assume that each review has already been associated with the correct class label, stored in the <code>sentiment</code> column (because our two class labels describe the sentiment of the review). After receiving the language model’s output, we compare the output to the ground truth (<strong class="cueball">8</strong>) and update the number of correctly classified reviews (variable <code>nr_correct</code>). At the same time, we sum up the total number of tokens used (because processing fees are proportional to that) and store them in the counter called <code>nr_tokens</code>. After iterating over all reviews, listing <a href="#code__untuned">9.1</a> prints out the final number of correct classifications and the number of tokens used.</p> 
  </div> 
  <div class="readable-text" id="p24"> 
   <h2 class=" readable-text-h2" id="model-tuning"><span class="num-string browsable-reference-id">9.3</span> Model tuning</h2> 
  </div> 
  <div class="readable-text" id="p25"> 
   <p>Let’s try it! You can find listing <a href="#code__untuned">9.1</a> under Untuned Classifier on the book’s website. We reuse the movie reviews from chapter 4; search for the Reviews.csv link in the chapter 4 section. The file contains 10 reviews, along with the corresponding ground truth. Let’s assume that the code for listing <a href="#code__untuned">9.1</a> and the reviews are stored in the same folder on disk. Open your terminal, switch to that folder, and run the following command:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p26"> 
   <div class="code-area-container"> 
    <pre class="code-area">python basic_classifier.py reviews.csv</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p27"> 
   <p>You should see the following output:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p28"> 
   <div class="code-area-container"> 
    <pre class="code-area">Label: neg; Ground truth: neg
Label: neg; Ground truth: neg
Label: neg; Ground truth: neg
Label: neg; Ground truth: neg
Label: pos; Ground truth: pos
Label: pos; Ground truth: neg  #1
Label: pos; Ground truth: neg
Label: negative; Ground truth: neg  #2
Label: negative; Ground truth: pos
Label: neg; Ground truth: neg
Number of correct labels:    6
Number of tokens used   :    2228</pre> 
    <div class="code-annotations-overlay-container">
     #1 Incorrect label
     <br/>#2 Nonexistent label
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p29"> 
   <p>The first 10 lines describe the results for each review. We have the label generated by the language model and then the ground-truth label (taken from the input file). At the end, we have the number of correctly classified reviews and the number of tokens used.</p> 
  </div> 
  <div class="readable-text" id="p30"> 
   <p>Out of 10 reviews, we classified 6 correctly. Well, at least that’s better than 50%, but it’s still not a great result. What went wrong? Looking at output gives us some ideas. There are cases (<strong class="cueball">1</strong>) where the language model simply picks the wrong class label. That’s not unexpected. However, there are also cases (<strong class="cueball">2</strong>) where the language model picks a class label that doesn’t even exist! Granted, it’s not too far off (<code>negative</code> instead of <code>neg</code>), and that seems easy to fix.</p> 
  </div> 
  <div class="readable-text" id="p31"> 
   <p>We focus on the (probably) low-hanging fruit of making the language model generate only one of our two possible class labels. How do we do that? Enter the <code>logit_bias</code> parameter. The <code>logit_bias</code> parameter enables users to change the likelihood that certain tokens are selected (we briefly discussed this and other GPT parameters in chapter 3). In this specific case, we would like to significantly increase the probability of the tokens associated with our two class labels (<code>neg</code> and <code>pos</code>). The <code>logit_bias</code> parameter is specified as a Python dictionary, mapping token IDs to a bias. A positive bias means we want to increase the probability that the language model generates the corresponding token. A negative bias means we decrease the probability of generating the associated token.</p> 
  </div> 
  <div class="readable-text" id="p32"> 
   <p>In this case, we want to increase the chances that GPT-3.5 selects one of the two tokens representing class labels. So we want to select a high bias for those two token IDs. Bias scores range from –100 to +100. We will go with the maximum and assign a bias of +100 to the tokens representing class labels. First we need to find their token IDs. Language models represent text as a sequence of token IDs. To change token bias, we need to reference the IDs of the tokens we care about.</p> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>A <em>tokenizer</em> is the component that transforms text into token IDs. You can find tokenizers for all GPT models at <a href="https://platform.openai.com/tokenizer">https://platform.openai.com/tokenizer</a>. We’re using GPT-3.5, so select the one labeled GPT 3.5 &amp; GPT-4. Figure <a href="#fig__gpttokenizer">9.1</a> shows the tokenizer web interface.</p> 
  </div> 
  <div class="readable-text" id="p34"> 
   <p>We can enter text in the text box and click the Token IDs button to see the token IDs for our input text. Using the tokenizer, we learn that the token <code>pos</code> has ID 981 and the token <code>neg</code> has token ID 29875. Now we’re ready to add a bias to our model invocation as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p35"> 
   <div class="code-area-container"> 
    <pre class="code-area">import openai
client = openai.OpenAI()

response = client.chat.completions.create(
    model='gpt-3.5-turbo',
    messages=[
        {'role':'user', 'content':prompt}
        ],
    logit_bias = {981:100, 29875:100},  #1
    temperature=0
    )</pre> 
    <div class="code-annotations-overlay-container">
     #1 Defines the bias
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="browsable-container figure-container" id="p36">  
   <img alt="figure" src="../Images/CH09_F01_Trummer.png" width="1100" height="789"/> 
   <h5 class=" figure-container-h5" id="fig__gpttokenizer"><span class="num-string">Figure <span class="browsable-reference-id">9.1</span></span> GPT tokenizer at <a href="https://platform.openai.com/tokenizer">https://platform.openai.com/tokenizer</a>: enter text to learn the associated token IDs.</h5>
  </div> 
  <div class="readable-text" id="p37"> 
   <p>Compared to the previous call (in listing <a href="#code__untuned">9.1</a>), we add the logit bias (<strong class="cueball">1</strong>) by mapping the IDs of the two tokens we’re interested in (<code>pos</code> with token ID 981 and <code>neg</code> with token ID 29875) to the highest possible bias value of 100. That should fix the problem of generating tokens that do not correspond to class labels, right?</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p38"> 
   <p> <span class="print-book-callout-head">Warning</span> The code described next causes problems and results in long running times and significant monetary fees. Do not try it without integrating the fix presented at the end of this section! </p> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>Let’s try it to be sure. You can add the logit bias to the code from listing <a href="#code__untuned">9.1</a>. Alternatively, later in this chapter, we will present a tunable version of the classifier that will allow you to try different combinations of tuning parameters (including the logit bias). If you execute the classifier with biases added, you will likely see output similar to the following (actually, as executing the code takes a long time and incurs non-negligible costs, you may just want to trust me on this):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p40"> 
   <div class="code-area-container"> 
    <pre class="code-area"> #1
Label: negnegnegnegnegnegnegnegnegneg ...; Ground truth: neg
Label: negposnegnegnegnegnegnegnegneg ...; Ground truth: neg
Label: negposnegnegnegnegnegnegnegneg ...; Ground truth: neg
Label: negposnegposnegnegnegnegnegneg ...; Ground truth: neg
Label: posnegposnegposnegposnegposneg ...; Ground truth: pos
Label: posnegpospospospospospospospos ...; Ground truth: neg
Label: posnegpospospospospospospospos ...; Ground truth: neg
Label: negposnegposnegposnegposnegpos ...; Ground truth: neg
Label: negposnegposnegposnegposnegpos ...; Ground truth: pos
Label: negposnegposnegnegnegnegnegneg ...; Ground truth: neg
Number of correct labels:    0
Number of tokens used   :    2318  #2</pre> 
    <div class="code-annotations-overlay-container">
     #1 Nonexistent labels for each input:
     <br/>#2 Increased token usage
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>Oh, no—not a single correct classification! What happened? Comparing generated “labels” to the ground truth reveals the problem (<strong class="cueball">1</strong>): we’re only generating the two possible tokens (which is great!) but just way too many of them (which is not so great!). That increases token consumption (<strong class="cueball">2</strong>) (note that the output length was limited for generating the example output; otherwise, token consumption would be much higher), but more importantly, it means our output does not correspond to any class label.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p42"> 
    <h5 class=" callout-container-h5 readable-text-h5">Why does the model generate so many tokens?</h5> 
   </div> 
   <div class="readable-text" id="p43"> 
    <p> We essentially restrict the model to generate text using only two tokens. Those are the two tokens we want to see in our output. However, we forgot to enable the model to generate any tokens that indicate the end of output! That is why the model cannot stop generating.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>There are multiple ways to fix this. We could, of course, add postprocessing to extract only the first token from the output generated by the language model. That would (mostly) fix our problem with the class labels. Look at the output, and you’ll see that using the first token leads to correct output in 7 of 10 cases. However, there is (another) problem with this approach: we’re paying to generate tokens that we don’t ultimately use! That’s clearly not what we want. So let’s tune our model even more by restricting the output length as well. All we need is a single token (this works only because our two possible class labels can be represented by a single token). That’s what the <code>max_tokens</code> parameter does. Let’s use it when calling our language model:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p45"> 
   <div class="code-area-container"> 
    <pre class="code-area">response = client.chat.completions.create(
    model='gpt-3.5-turbo',
    messages=[
        {'role':'user', 'content':prompt}
        ],
    logit_bias = {981:100, 29875:100},  #1
    temperature=0, max_tokens=1
    )</pre> 
    <div class="code-annotations-overlay-container">
     #1 Defines the bias
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p46"> 
   <p>When you try it (which should be fast and not costly), you should see this output:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p47"> 
   <div class="code-area-container"> 
    <pre class="code-area">Label: neg; Ground truth: neg
Label: neg; Ground truth: neg
Label: neg; Ground truth: neg
Label: neg; Ground truth: neg
Label: pos; Ground truth: pos
Label: pos; Ground truth: neg
Label: pos; Ground truth: neg
Label: neg; Ground truth: neg
Label: neg; Ground truth: pos
Label: neg; Ground truth: neg
Number of correct labels:    7  #1
Number of tokens used   :    2228  #2</pre> 
    <div class="code-annotations-overlay-container">
     #1 Improves on the untuned classifier
     <br/>#2 Reduces token usage
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p48"> 
   <p>Much better! We have improved the number of correctly handled cases from six (for the unturned version) to seven (<strong class="cueball">1</strong>). That may not sound like much. However, thinking about the entire data set, it essentially means we have improved precision from 60% to 70%: that is, thousands more reviews will now be classified correctly! There is a caveat, of course. In reality, you should probably use a much larger sample. Due to random variations, the accuracy you observe on a sample may not be representative of the accuracy for the entire data set. To keep things simple (and your cost relatively low when trying it), we restrict ourselves to 10 samples here. As an additional bonus, our token consumption has again been reduced (<strong class="cueball">2</strong>) (actually, the gap in token consumption, compared to a version without any output size bound, is likely to be much, much larger). Note that the two parameters discussed here are only a small subset of the available tuning options. You will find more details on relevant parameters in chapter 3. Whenever you tune a model for a new task, be sure to consider all parameters that may be potentially relevant. Then try a few reasonable settings on a data sample to see which option performs best.</p> 
  </div> 
  <div class="readable-text" id="p49"> 
   <h2 class=" readable-text-h2" id="model-selection"><span class="num-string browsable-reference-id">9.4</span> Model selection</h2> 
  </div> 
  <div class="readable-text" id="p50"> 
   <p>Let’s assume that we have maxed out our ability to get better performance by tuning our current model. What else can we do? We can, of course, select a different model. We saw a few GPT alternatives in the last chapter. If you can select a model specifically trained for the task you’re interested in (e.g., text classification), that’s often worth a look. Other factors that can influence your model choices are whether the data you plan to apply the model to is sensitive and whether sending that data to specific providers of language models is acceptable.</p> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>If you want to learn about the relative performance of different models, have a look at <a href="https://crfm.stanford.edu/helm/lite/latest/">https://crfm.stanford.edu/helm/lite/latest/</a>. This website contains the results of HELM, Stanford’s Holistic Evaluation of Language Models benchmark. The benchmark compares language models on various scenarios and contains results for specific tasks, as well as average performance, aggregated over various scenarios. You may want to check this out to get a sense of which models may be interesting to you. However, as various factors can influence a language model’s performance, it still pays to evaluate different models on the specific task you’re interested in.</p> 
  </div> 
  <div class="readable-text" id="p52"> 
   <p>To keep things simple, let’s only consider GPT-4 as an alternative to GPT-3.5 Turbo (which we used up to this point). Replace the name of the model in the language model invocation:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p53"> 
   <div class="code-area-container"> 
    <pre class="code-area">response = client.chat.completions.create(
    model='gpt-4',
    messages=[
        {'role':'user', 'content':prompt}
        ],
    logit_bias = {981:100, 29875:100},  #1
    temperature=0, max_tokens=1
    )</pre> 
    <div class="code-annotations-overlay-container">
     #1 Defines the bias
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>Running the resulting code should lead to the following output:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p55"> 
   <div class="code-area-container"> 
    <pre class="code-area">Label: neg; Ground truth: neg
Label: neg; Ground truth: neg
Label: neg; Ground truth: neg
Label: neg; Ground truth: neg
Label: pos; Ground truth: pos
Label: pos; Ground truth: neg
Label: pos; Ground truth: neg
Label: neg; Ground truth: neg
Label: pos; Ground truth: pos  #1
Label: neg; Ground truth: neg
Number of correct labels:    8  #2
Number of tokens used   :    2228  #3</pre> 
    <div class="code-annotations-overlay-container">
     #1 Correct classification result
     <br/>#2 Best result so far
     <br/>#3 Same number of tokens
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p56"> 
   <p>Compared to the prior version, GPT-4 manages to solve one more test case accurately (<strong class="cueball">1</strong>)! That brings our accuracy to 80% (<strong class="cueball">2</strong>), while our token consumption remains constant (<strong class="cueball">3</strong>). That, by the way, is not guaranteed to be the case if we change the model. As different models may use different tokenizers, representing the same text may require a different number of tokens for different models. In this specific case, because GPT-4 and GPT-3.5 use the same tokenizer, the number of tokens does not change.</p> 
  </div> 
  <div class="readable-text" id="p57"> 
   <p>Does that mean we’re paying the same amount of money? Not quite. Because GPT-4 incurs much higher fees per token, we’re paying roughly 120 times more than before (the relative difference between the per-token prices of GPT-4 and GPT-3.5 Turbo). That’s why we’re trying to make GPT-3.5 perform as well as possible without resorting to GPT-4.</p> 
  </div> 
  <div class="readable-text" id="p58"> 
   <p>Occasionally, during model selection and model tuning, it makes sense to look at the test data yourself. That gives you a better impression of the sweet spots and limitations of various models and enables you to judge whether the test cases on which your model performs badly are representative. For instance, the following review is solved correctly by GPT-4 but not by GPT-3.5:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p59"> 
   <div class="code-area-container"> 
    <pre class="code-area">If you want to see a film starring Stan Laurel from the Laurel &amp; Hardy 
comedies, this is not the film for you. Stan would not begin to find the 
character and rhythms of those films for another two years. If, however, 
you want a good travesty of the Rudolph Valentino BLOOD AND SAND, which 
had been made the previous year, this is the movie for you. All the 
stops are pulled out, both in physical comedy and on the title cards 
and if the movie is not held together by character, the plot of 
Valentino's movie is used -- well sort of.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>This review contains positive (toward the end) as well as negative (the beginning) aspects. Although the final verdict is positive, we may conclude that spending more money to properly analyze borderline cases like that review is not worth it.</p> 
  </div> 
  <div class="readable-text" id="p61"> 
   <h2 class=" readable-text-h2" id="prompt-engineering"><span class="num-string browsable-reference-id">9.5</span> Prompt engineering</h2> 
  </div> 
  <div class="readable-text" id="p62"> 
   <p>Setting aside options to swap models, what else can we do to improve performance with our model? One area we haven’t looked at yet is the definition of the prompt we use for classification. Changing the prompt template can have a significant effect on result quality. The fact that prompt tuning is often crucial has even led to the introduction of a dedicated term, <em>prompt engineering</em>, describing the process of searching for optimal prompt templates. What’s more, the challenges of prompt engineering have led to the creation of multiple platforms offering prompt templates for a plethora of different tasks. If you’re out of ideas for prompt variants, have a look at <a href="https://promptbase.com/">https://promptbase.com/</a>, <a href="https://prompthero.com/">https://prompthero.com/</a>, and similar platforms. The business model of such platforms is to enable users to buy and sell prompt templates that optimize the performance of specific models for specific tasks.</p> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>Figuring out what prompt works best typically requires some experimentation. Next, we will focus on the basics and explore a classical technique to increase output quality by changing the prompt. We’re talking about few-shot learning here, which means we’re helping the model by giving it a few examples. That’s something we know from everyday life: it is often hard to understand a new task or approach based on a pure description alone. It is much better to see some examples to get the hang of it. For instance, in the previous sections, we could have just discussed the semantics of a few relevant model-tuning parameters. But isn’t it much better to see how they can be tuned in a concrete example scenario?</p> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>Of course it is. Language models “feel” the same way, and adding a few helpful examples can often improve their performance. So how do we show them examples? Easy: we specify those examples as part of the prompt. For instance, in our classification scenario, we want the language models to classify reviews. An example would be a review together with the reference class label.</p> 
  </div> 
  <div class="readable-text" id="p65"> 
   <p>We will use the following prompt template to integrate a single sample into the prompt:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p66"> 
   <div class="code-area-container"> 
    <pre class="code-area">[Sample Review]
Is the sentiment positive or negative?
Answer ("pos"/"neg"):[Sample Solution]
[Review to Classify]
Is the sentiment positive or negative?
Answer ("pos"/"neg"):</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p67"> 
   <p>If we replace the placeholders with the sample review, the sample review solution, and the review we’re interested in classifying, we get, for instance, the following prompt:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p68"> 
   <div class="code-area-container"> 
    <pre class="code-area">Now, I won't deny that when I purchased #1
this off eBay, I had high expectations. ...
Is the sentiment positive or negative?  #2
Answer ("pos"/"neg"):neg          #3
I am willing to tolerate almost anything            #4
in a Sci-Fi movie, but this was almost intolerable. ...
Is the sentiment positive or negative?              #5
Answer ("pos"/"neg"): </pre> 
    <div class="code-annotations-overlay-container">
     #1 Sample review
     <br/>#2 Instructions
     <br/>#3 Sample solution
     <br/>#4 Review to classify
     <br/>#5 Instructions
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p69"> 
   <p>You see a sample review (<strong class="cueball">1</strong>), instructions (<strong class="cueball">2</strong>), and the reference class for the sample review (<strong class="cueball">3</strong>). After that, you find the review we want to classify (<strong class="cueball">4</strong>) and the classification instructions (again) (<strong class="cueball">5</strong>), but no solution yet (of course not—that’s what we want the language model to generate). In this prompt, we provide exactly one example of a correctly solved task to the model. Doing so may help the model better understand what we’re asking it to do.</p> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>Of course, there are many options to provide samples in the prompt. We have chosen what is arguably the most straightforward solution: we use the same prompt structure twice for the two reviews. Because we’re using exactly the same structure, our prompt is slightly redundant: we repeat the task instructions (<strong class="cueball">2</strong> and <strong class="cueball">5</strong>), including the specification of the two possible class labels. Although we won’t do so here, it might be interesting to experiment and see whether you can integrate examples into the prompt in a different way, removing redundancies and reducing the prompt length (thereby reducing the number of tokens processed and, ultimately, processing fees).</p> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>Up to now, we have only considered adding a single example. But sometimes, seeing one example is not enough. That’s why it may make sense to add more than one example for the language model as well. Let’s assume that we have a few samples: reviews with associated class labels, stored in a data frame called <code>samples</code>. We can use the following code to generate prompts that integrate those samples:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p72"> 
   <div class="code-area-container"> 
    <pre class="code-area">def create_single_text_prompt(text, label):   #1
    """ Create prompt for classifying a single text.
    
    Args:
        text: text to classify.
        label: correct class label (empty if unavailable).
    
    Returns:
        Prompt for text classification.
    """
    task = 'Is the sentiment positive or negative?'
    answer_format = 'Answer ("pos"/"neg")'
    return f'{text}\n{task}\n{answer_format}:{label}'

def create_prompt(text, samples):            #2
    """ Generates prompt for sentiment classification.
    
    Args:
        text: classify this text.
        samples: integrate these samples into prompt.
    
    Returns:
        Input for LLM.
    """
    parts = []
    for _, row in samples.iterrows():  #3
        sample_text = row['text']
        sample_label = row['sentiment']
        prompt = create_single_text_prompt(sample_text, sample_label)
        parts += [prompt]

    prompt = create_single_text_prompt(text, ")  #4
    parts += [prompt]
    return '\n'.join(parts)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Creates a prompt for one review
     <br/>#2 Generates a prompt for all reviews
     <br/>#3 Integrates the samples
     <br/>#4 Adds the review to classify
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>The <code>create_single_text_prompt</code> function (<strong class="cueball">1</strong>) instantiates the following template for a single review:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p74"> 
   <div class="code-area-container"> 
    <pre class="code-area">[Review]
Is the sentiment positive or negative?
Answer ("pos"/"neg"):[Label]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>We use the same function to specify sample reviews, as well as to specify the review, along with the classification task that we want the language model to solve for us. If we specify a sample review, the <code>[Label]</code> placeholder will be replaced with the reference class label for the corresponding review. If we specify the task the language model should solve, we do not know the correct class label yet. In that case, we replace the <code>[Label]</code> placeholder with the empty string. It will be up to the language model to complete the prompt with the actual class label.</p> 
  </div> 
  <div class="readable-text" id="p76"> 
   <p>The <code>create_prompt</code> function (<strong class="cueball">2</strong>) generates the complete prompt, considering all sample reviews, as well as the review we want to classify. First (<strong class="cueball">3</strong>), it iterates over the sample reviews. We assume that our <code>samples</code> data frame stores review text in the <code>text</code> column and the associated class labels in the <code>sentiment</code> column. We add a prompt part (<strong class="cueball">4</strong>) for the sample review using the <code>create_single_text_prompt</code> function (discussed earlier). Finally, we add instructions to classify the review we’re interested in.</p> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>Let’s switch back to using GPT-3.5 Turbo. However, this time, we will use our new prompt-generation function. For the moment, we will restrict ourselves to a single example review in the prompt. On the book’s companion website, you can find training reviews with the correct class labels under Reviews Training, leading to the file train_reviews.csv. The reviews in this file do not overlap with those in the reviews.csv file (which we use to test our approach). Adding just the first review from train_reviews.csv as a sample to the prompts, you should now see the following output:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p78"> 
   <div class="code-area-container"> 
    <pre class="code-area">Label: neg; Ground truth: neg
Label: neg; Ground truth: neg
Label: neg; Ground truth: neg
Label: neg; Ground truth: neg
Label: pos; Ground truth: pos
Label: pos; Ground truth: neg
Label: pos; Ground truth: neg
Label: neg; Ground truth: neg
Label: pos; Ground truth: pos
Label: neg; Ground truth: neg
Number of correct labels:    8  #1
Number of tokens used   :    4078  #2</pre> 
    <div class="code-annotations-overlay-container">
     #1 Equivalent to GPT-4 result
     <br/>#2 Token usage roughly doubles
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>Hooray! We have increased precision to 80% (<strong class="cueball">1</strong>). That’s the same accuracy we got when using GPT-4 on the original prompts (without sample reviews). At the same time, our token usage has increased (<strong class="cueball">2</strong>). More precisely, because we’re adding a second review to each prompt (i.e., we have one sample review and the review to classify), our token consumption has roughly doubled compared to the last version. However, compared to using GPT-4 on shorter prompts, our current approach is still about 60 times cheaper (because using GPT-4 is about 120 times more expensive than using GPT-3.5 Turbo).</p> 
  </div> 
  <div class="readable-text" id="p80"> 
   <h2 class=" readable-text-h2" id="tunable-classifier"><span class="num-string browsable-reference-id">9.6</span> Tunable classifier</h2> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>Now that we have seen quite a few tuning options, you may be tempted to try new variations. For instance, do we still need to add bias (essentially restricting the output to the two possible class labels) if we’re adding samples? Can we get even better precision when using a larger model together with multiple samples in the prompt? Changing your code to try a new combination quickly becomes tedious. But no worries, we’ve got you covered! On the book’s website, you can find listing <a href="#code__tunableClassifier">9.2</a> under Tunable Classifier. This implementation lets you try all the tuning variants by setting the right command-line parameters. We will quickly discuss the code, which integrates all the code variants discussed previously.</p> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>Generating prompts (<strong class="cueball">1</strong>) works as described in the last section. The <code>create_prompt</code> function takes the review text to classify and sample reviews as input. The sample reviews are added to the prompt, potentially supporting the language models in classifying the review we’re interested in. Note that we can still see how the language model performs without any samples (by not specifying any samples). Classification without any samples corresponds to a special case.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p83"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__tunableClassifier"><span class="num-string">Listing <span class="browsable-reference-id">9.2</span></span> Tunable version of sentiment classifier</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse
import openai
import pandas as pd
import time

client = openai.OpenAI()

def create_single_text_prompt(text, label):
    """ Create prompt for classifying a single text.
    
    Args:
        text: text to classify.
        label: correct class label (empty if unavailable).
    
    Returns:
        Prompt for text classification.
    """
    task = 'Is the sentiment positive or negative?'
    answer_format = 'Answer ("pos"/"neg")'
    return f'{text}\n{task}\n{answer_format}:{label}'

def create_prompt(text, samples):             #1
    """ Generates prompt for sentiment classification.
    
    Args:
        text: classify this text.
        samples: integrate these samples into prompt.
    
    Returns:
        Input for LLM.
    """
    parts = []
    for _, row in samples.iterrows():
        sample_text = row['text']
        sample_label = row['sentiment']
        prompt = create_single_text_prompt(sample_text, sample_label)
        parts += [prompt]
    
    prompt = create_single_text_prompt(text, ")
    parts += [prompt]
    return '\n'.join(parts)
 #2
def call_llm(prompt, model, max_tokens, out_tokens):
    """ Query large language model and return answer.
    
    Args:
        prompt: input prompt for language model.
        model: name of OpenAI model to choose.
        max_tokens: maximum output length in tokens.
        out_tokens: prioritize these token IDs in output.
    
    Returns:
        Answer by language model and total number of tokens.
    """
    optional_parameters = {}
    if max_tokens:
        optional_parameters['max_tokens'] = max_tokens
    if out_tokens:
        logit_bias = {int(tid):100 for tid in out_tokens.split(',')}
        optional_parameters['logit_bias'] = logit_bias
    
    for nr_retries in range(1, 4):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {'role':'user', 'content':prompt}
                    ],
                **optional_parameters, temperature=0
                )
            
            answer = response.choices[0].message.content
            nr_tokens = response.usage.total_tokens
            return answer, nr_tokens
        
        except Exception as e:
            print(f'Exception: {e}')
            time.sleep(nr_retries * 2)
    
    raise Exception('Cannot query OpenAI model!')

if __name__ == '__main__':

    parser = argparse.ArgumentParser()       #3
    parser.add_argument('file_path', type=str, help='Path to input file')
    parser.add_argument('model', type=str, help='Name of OpenAI model')
    parser.add_argument('max_tokens', type=int, help='Maximum output size')
    parser.add_argument('out_tokens', type=str, help='Tokens to prioritize')
    parser.add_argument('nr_samples', type=int, help='Number of samples')
    parser.add_argument('sample_path', type=str, help='Path to samples')
    args = parser.parse_args()
    
    df = pd.read_csv(args.file_path)

    samples = pd.DataFrame()  #4
    if args.nr_samples:
        samples = pd.read_csv(args.sample_path)
        samples = samples[:args.nr_samples]
    
    nr_correct = 0
    nr_tokens = 0
    
    for _, row in df.iterrows():

        text = row['text']   #5
        prompt = create_prompt(text, samples)
        label, current_tokens = call_llm(
            prompt, args.model, 
            args.max_tokens, 
            args.out_tokens)

        ground_truth = row['sentiment']  #6
        if label == ground_truth:
            nr_correct += 1
        nr_tokens += current_tokens
        
        print(f'Label: {label}; Ground truth: {ground_truth}')

         #7
    print(f'Number of correct labels:\t{nr_correct}')
    print(f'Number of tokens used   :\t{nr_tokens}')</pre> 
    <div class="code-annotations-overlay-container">
     #1 Generates prompts with samples
     <br/>#2 Calls language models with parameters
     <br/>#3 Parses command-line parameters
     <br/>#4 Reads samples from disk
     <br/>#5 Classifies the review
     <br/>#6 Updates the counters
     <br/>#7 Prints out the counters
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>Our <code>call_llm</code> function (<strong class="cueball">2</strong>) integrates all the tuning parameters mentioned earlier. First is the name of the model to call (the <code>model</code> parameter). Second, we can specify the maximum number of output tokens (<code>max_tokens</code>). Finally, we can specify bias: tokens that should be prioritized when generating output. The <code>out_tokens</code> parameter allows users to specify a comma-separated list of token IDs to which we assign a high priority (essentially limiting output to one of these tokens). Although the model name is required, setting a value of <code>0</code> for the <code>max_tokens</code> parameter and the empty string for the <code>out_tokens</code> parameter allows us to avoid changing OpenAI’s default settings.</p> 
  </div> 
  <div class="readable-text" id="p85"> 
   <p>The tunable classifier uses quite a few command-line parameters (<strong class="cueball">3</strong>). Let’s discuss them in the order in which you need to specify them:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p86"><code>file_path</code>—Path to the .csv file containing reviews used to evaluate our language model</li> 
   <li class="readable-text" id="p87"><code>model</code>—Name of the language model we want to use (e.g., <code>gpt-3.5-turbo</code>)</li> 
   <li class="readable-text" id="p88"><code>max_tokens</code>—Maximum number of output tokens to generate per input review</li> 
   <li class="readable-text" id="p89"><code>out_tokens</code>—A comma-separated list of tokens to prioritize when generating output</li> 
   <li class="readable-text" id="p90"><code>nr_samples</code>—Number of review samples with solutions to integrate into each prompt</li> 
   <li class="readable-text" id="p91"><code>sample_path</code>—Path to the .csv file containing reviews with correct class labels to use as samples (this can be empty if the <code>nr_samples</code> parameter is set to <code>0</code>)</li> 
  </ul> 
  <div class="readable-text print-book-callout" id="p92"> 
   <p> <span class="print-book-callout-head">Warning</span> Limiting the number of output tokens is almost always a good idea. In particular, you should do it whenever biasing output toward specific tokens without including any of the “stop” tokens (indicating the end of output). </p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>After parsing input parameters, the classifier reads samples from disk (<strong class="cueball">4</strong>) and classifies reviews (<strong class="cueball">5</strong>) while updating counters (<strong class="cueball">6</strong>) that are ultimately printed (<strong class="cueball">7</strong>).</p> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>Let’s see how we can simulate all the different versions of our classifier that we have discussed so far. Using the following invocation should give us the untuned version of our classifier, assuming that the file reviews.csv is located in the same directory as the code itself:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p95"> 
   <div class="code-area-container"> 
    <pre class="code-area">python tunable_classifier.py reviews.csv
    gpt-3.5-turbo 0 "" 0 ""</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>Note that we don’t specify any tokens to prioritize (we specify the empty string), don’t restrict the output length (setting it to <code>0</code> means no restrictions), and set the number of samples in the prompt to <code>0</code> (which means we can set the path to the file containing samples to the empty string as well).</p> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>The following command, on the other hand, will give us the version that restricts the output length while prioritizing the tokens that correspond to our class labels:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p98"> 
   <div class="code-area-container"> 
    <pre class="code-area">python tunable_classifier.py reviews.csv
    gpt-3.5-turbo 1 "981,29875" 0 ""</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p99"> 
   <p>Finally, we can get the last version we discussed, using one sample per prompt while tuning the model as before, via the following command (assuming the file train_reviews.csv is located in the same repository as the code):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p100"> 
   <div class="code-area-container"> 
    <pre class="code-area">python tunable_classifier.py reviews.csv
    gpt-3.5-turbo 1 "981,29875" 1 "train_reviews.csv"</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p101"> 
   <p>Feel free to try new combinations that we haven’t discussed!</p> 
  </div> 
  <div class="readable-text" id="p102"> 
   <h2 class=" readable-text-h2" id="fine-tuning"><span class="num-string browsable-reference-id">9.7</span> Fine-tuning</h2> 
  </div> 
  <div class="readable-text" id="p103"> 
   <p>So far, we have done everything in our power to squeeze the best performance out of existing models. Those models have been trained for tasks that are, perhaps, similar but not <em>exactly</em> like the one we’re interested in. Wouldn’t it be nice to get a model customized specifically for our task? That is possible when using fine-tuning. Let’s see how to implement fine-tuning with OpenAI’s models in practice.</p> 
  </div> 
  <div class="readable-text" id="p104"> 
   <p>Fine-tuning means we take an existing model, such as OpenAI’s GPT-3.5 Turbo model, and specialize it for a task we’re interested in. Of course, in principle, we could train our model from scratch. But that is typically prohibitively expensive, and in addition, we usually don’t find enough task-specific training data to sustain a large model during training. That’s why it is much better to rely on fine-tuning.</p> 
  </div> 
  <div class="readable-text" id="p105"> 
   <p>Fine-tuning is typically the last thing we try when maximizing performance for a specific task. The reason is that fine-tuning requires a certain upfront investment in terms of time and money. During fine-tuning, we pay OpenAI to create a customized version of one of its base models just for our task. The price is based on the size of the training data and the number of times that training data is read (i.e., the number of <em>epochs</em>). For example, at the time of writing, fine-tuning GPT-3.5 Turbo costs about 0.8 cents per 1,000 tokens of training data and epoch. Also, after fine-tuning, we pay to use the fine-tuned model. The price per token is higher for the fine-tuned model than for the base version. That makes sense as, at least in theory, the fine-tuned model should perform better for our specific task.</p> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>One possible advantage of fine-tuning is that we improve the accuracy of the model output. Another possible advantage is that we may be able to shorten our prompts. When using a generic model, the prompt needs to contain a description of the task to perform (along with all relevant data). On the other hand, our fine-tuned model should be specialized to perform a single task and perform well on it. If the model only needs to do one task, in principle it should be possible to leave the task description out of the prompt because it is implicit. Besides the task description, we can leave out other information that is helpful for a generic model but not required for a specialized one. For instance, it may be necessary to integrate samples into the prompt for the generic model to obtain reasonable output quality, whereas that is unnecessary for the fine-tuned version.</p> 
  </div> 
  <div class="readable-text" id="p107"> 
   <p>In our specific scenario, we want to map reviews to a class label (based on the underlying sentiment of the review author). Previously, we specified the classification task as part of the prompt (and even provided some helpful examples). Now, perhaps, when fine-tuning a model, we can leave out those instructions. More precisely, we may no longer need to use prompts like the following (a prompt containing sample reviews (<strong class="cueball">1</strong>) with instructions (<strong class="cueball">2</strong>) and sample solutions (<strong class="cueball">3</strong>), along with the review to classify (<strong class="cueball">4</strong>) and corresponding instructions (<strong class="cueball">5</strong>)):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p108"> 
   <div class="code-area-container"> 
    <pre class="code-area">Now, I won't deny that when I purchased #1
this off eBay, I had high expectations. ...
Is the sentiment positive or negative?  #2
Answer ("pos"/"neg"):neg          #3
I am willing to tolerate almost anything            #4
in a Sci-Fi movie, but this was almost intolerable. ...
Is the sentiment positive or negative?              #5
Answer ("pos"/"neg"): </pre> 
    <div class="code-annotations-overlay-container">
     #1 Sample review
     <br/>#2 Instructions
     <br/>#3 Sample solution
     <br/>#4 Review to classify
     <br/>#5 Instructions
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p109"> 
   <p>Instead, we can assume that the model implicitly knows that it should classify reviews and which class labels are available. Under that assumption, we can simplify the prompt to this:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p110"> 
   <div class="code-area-container"> 
    <pre class="code-area">I am willing to tolerate almost anything
in a Sci-Fi movie, but this was almost intolerable. ...</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p111"> 
   <p>This prompt merely states the review that we want to classify. We assume that all other task-specific information (such as instructions and samples) is already implicitly known to the model. As you certainly noticed, this prompt is much shorter than the previous version. That means we <em>may</em> save money when using the fine-tuned model instead of the base version. On the other hand, keep in mind that using the fine-tuned model is more expensive per token than using the base version. We postpone the corresponding calculations to later. But first, let’s see whether we can even make such concise prompts work in practice via fine-tuning.</p> 
  </div> 
  <div class="readable-text" id="p112"> 
   <h2 class=" readable-text-h2" id="generating-training-data"><span class="num-string browsable-reference-id">9.8</span> Generating training data</h2> 
  </div> 
  <div class="readable-text" id="p113"> 
   <p>First we have to generate our training data for fine-tuning. We will use the reviews with associated class labels contained in the file train_reviews.csv, available on the companion website under Review Training. OpenAI expects training data for fine-tuning in a very specific format. Before we can fine-tune, we need to transform our .csv data into the required format.</p> 
  </div> 
  <div class="readable-text" id="p114"> 
   <p>Training data for fine-tuning OpenAI’s chat models generally takes the form of successful interactions with the model (i.e., examples where the model produces the output we ideally want it to produce). In the case of OpenAI’s chat models, such interactions are described via message histories. Each message is described by a Python dictionary object. For instance, the following describes a successful completion, given the earlier example review as input:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p115"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">{'messages':[
    {'role':'user', 'content':'I am willing to tolerate almost anything 
    <span class="">↪</span> ...'},
    {'role':'assistant', 'content':'neg'}
]}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p116"> 
   <p>This is a negative review (i.e., the review author does not want to recommend the movie), and therefore, we ideally want the model to generate a message that contains the single token <code>neg</code>. That’s the interaction depicted here.</p> 
  </div> 
  <div class="readable-text" id="p117"> 
   <p>To make fine-tuning worth it, you typically want to use at least 50 samples and up to a few thousand samples. Using more samples for fine-tuning can improve performance but is also more expensive. On the other hand, this is a one-time fee because you can reuse the same fine-tuned model for a potentially large data set (and the usage fees for the fine-tuned model do not depend on the amount of training data used for fine-tuning). The example file (reviews_train.csv) contains 100 samples and is therefore within the range of data sizes where fine-tuning may become useful.</p> 
  </div> 
  <div class="readable-text" id="p118"> 
   <p>OpenAI expects data for fine-tuning in JSON-lines format (such files typically have the suffix .jsonl). Files that comply with this format essentially contain one Python dictionary in each line. In this case, each line describes one successful interaction with the model (using the same format as in the previous example). To handle JSON-lines files more easily from Python, we will use the <code>jsonlines</code> library. As a first step, go to the terminal and install the library using the following command:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p119"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install jsonlines==4.0</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p120"> 
   <p>Now we can use the library to transform our .csv data into the format required by OpenAI. Listing <a href="#code__createTrainingData">9.3</a> uses the <code>get_samples</code> function (<strong class="cueball">1</strong>) to prepare samples in the required format. The input is a pandas DataFrame (<code>df</code> parameter) containing the training samples in the usual format (we assume that the <code>text</code> column contains the reviews and the <code>sentiment</code> column contains the associated class labels). We turn each sample into a successful message exchange with the model. First, we create the message sent by the user (<strong class="cueball">2</strong>), which only includes the review text. Second, we create the desired answer message to generate by the model (associated with the “assistant” role) (<strong class="cueball">3</strong>). The full set of training samples is a list of message exchanges, each prepared in the previously mentioned format.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p121"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__createTrainingData"><span class="num-string">Listing <span class="browsable-reference-id">9.3</span></span> Generating training data for fine-tuning</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse
import jsonlines
import pandas

def get_samples(df):                    #1
    """ Generate samples from a data frame.
    
    Args:
        df: data frame containing samples.
    
    Returns:
        List of samples in OpenAI format for fine-tuning.
    """
    samples = []
    for _, row in df.iterrows():
         #2
        text = row['text']
        user_message = {'role':'user', 'content':text}
         #3
        label = row['sentiment']
        assistant_message = {'role':'assistant', 'content':label}
        
        sample = {'messages':[user_message, assistant_message]}
        samples += [sample]
    
    return samples

if __name__ == '__main__':
    #4
    parser = argparse.ArgumentParser()
    parser.add_argument('in_path', type=str, help='Path to input')
    parser.add_argument('out_path', type=str, help='Path to output')
    args = parser.parse_args()
    
    df = pandas.read_csv(args.in_path)
    samples = get_samples(df)    
     #5
    with jsonlines.open(args.out_path, 'w') as file:
        for sample in samples:
            file.write(sample)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Generates training data
     <br/>#2 Creates a user message
     <br/>#3 Creates an assistant message
     <br/>#4 Parses the command-line arguments
     <br/>#5 Stores the training data in new format
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p122"> 
   <p>Listing <a href="#code__createTrainingData">9.3</a> expects as input a path to the .csv file with training samples, as well as the path to the output file (<strong class="cueball">4</strong>). The output file follows the JSON-lines format, so we ideally assign an output path ending with .jsonl. After transforming the input .csv file into the fine-tuning format, we use the <code>jsonlines</code> library to write the transformed samples into the JSON-lines file (<strong class="cueball">5</strong>).</p> 
  </div> 
  <div class="readable-text" id="p123"> 
   <p>As usual, you don’t need to enter the code for this listing. You can find it on the website under Prepare Fine-Tuning. Run it from the terminal using the following command (we assume that the file train_reviews.csv is located in the same repository as the code):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p124"> 
   <div class="code-area-container"> 
    <pre class="code-area">python prep_fine_tuning.py   train_reviews.csv train_reviews.jsonl</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p125"> 
   <p>You may want to manually inspect the train_reviews.jsonl file that was (hopefully) generated by running this command. You should see one training sample on each line, represented as a Python dictionary.</p> 
  </div> 
  <div class="readable-text" id="p126"> 
   <h2 class=" readable-text-h2" id="starting-a-fine-tuning-job"><span class="num-string browsable-reference-id">9.9</span> Starting a fine-tuning job</h2> 
  </div> 
  <div class="readable-text" id="p127"> 
   <p>Now that we have our training data in the right format, we can create a fine-tuning job on OpenAI’s platform. Of course, because the model is stored only on OpenAI’s platform, we cannot do the fine-tuning ourselves. Instead, we send our training data to OpenAI and request to use that data to create a customized model. To create a customized model, we must first choose a base model. In this case, we will start from the GPT-3.5 Turbo model (which makes it easier to compare with the results we have obtained so far).</p> 
  </div> 
  <div class="readable-text" id="p128"> 
   <p>We can create a fine-tuning job using the following code snippet (assuming that <code>in_path</code> is the path to the file containing training data):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p129"> 
   <div class="code-area-container"> 
    <pre class="code-area">import openai
client = openai.OpenAI()

reply = client.files.create(
    file=open(in_path, 'rb'), purpose='fine-tune')</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p130"> 
   <p>The <code>reply</code> object will contain a Python object with metadata about our fine-tuning job (assuming the job creation succeeds). Most importantly, we get the ID of the job we just created in the <code>reply.id</code> field. Fine-tuning jobs typically take a while (around 15 minutes is typical for the fine-tuning job we describe here). That means we have to wait until our fine-tuned model has been created. The job ID allows us to verify the status of our fine-tuning job and retrieve the ID of the freshly created model once it is available. We can retrieve status information about our fine-tuning job using the following piece of Python code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p131"> 
   <div class="code-area-container"> 
    <pre class="code-area">reply = client.fine_tuning.jobs.retrieve(job_id)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p132"> 
   <p>The <code>reply.status</code> field reports the status of the fine-tuning job, which will eventually reach the value <code>succeeded</code>. After that has happened, we can retrieve the ID of the fine-tuned model in <code>reply.fine_tuned_model</code>.</p> 
  </div> 
  <div class="readable-text" id="p133"> 
   <p>Listing <a href="#code__fineTune">9.4</a> starts the fine-tuning process, waits until the corresponding job finishes, and finally prints out the ID of the generated model. Given a path to a file containing training data, the code first uploads the file containing training data (<strong class="cueball">1</strong>). It retrieves the file ID assigned by OpenAI and uses it to create a fine-tuning job (<strong class="cueball">2</strong>). Then, we iterate until the fine-tuning job completes successfully (<strong class="cueball">3</strong>). In each iteration, we print out a timer (measuring seconds since the start of the fine-tuning job) and check for status updates with regard to the job (<strong class="cueball">4</strong>). Finally, we retrieve the model ID and print it (<strong class="cueball">5</strong>).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p134"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__fineTune"><span class="num-string">Listing <span class="browsable-reference-id">9.4</span></span> Fine-tuning a GPT model using training data</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse
import openai
import time

client = openai.OpenAI()

if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('in_path', type=str, help='Path to input file')
    args = parser.parse_args()

    reply = client.files.create(              #1
        file=open(args.in_path, 'rb'), purpose='fine-tune')
    file_id = reply.id

    reply = client.fine_tuning.jobs.create(       #2
        training_file=file_id, model='gpt-3.5-turbo')
    job_id = reply.id
    print(f'Job ID: {job_id}')
    
    status = None
    start_s = time.time()

    while not (status == 'succeeded'):  #3
        
        time.sleep(5)
        total_s = time.time() - start_s
        print(f'Fine-tuning since {total_s} seconds.')
         #4
        reply = client.fine_tuning.jobs.retrieve(job_id) 
        status = reply.status
        print(f'Status: {status}')
    #5
    print(f'Fine-tuning is finished!')
    model_id = reply.fine_tuned_model
    print(f'Model ID: {model_id}')</pre> 
    <div class="code-annotations-overlay-container">
     #1 Uploads training data to OpenAI
     <br/>#2 Creates a fine-tuning job
     <br/>#3 Iterates until the job completes
     <br/>#4 Gets the job status
     <br/>#5 Retrieves the ID of the fine-tuned model
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p135"> 
   <p>You can find the code on the website under Start Fine-Tuning. Run it using the following command (where train_reviews.jsonl is the previously generated file):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p136"> 
   <div class="code-area-container"> 
    <pre class="code-area">python fine_tune.py train_reviews.jsonl</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p137"> 
   <p>If you run the script to completion, you will see output such as the following (this is, of course, just part of the output; dots represent missing lines):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p138"> 
   <div class="code-area-container"> 
    <pre class="code-area">Job ID: ...
Fine-tuning since 5.00495171546936 seconds.
Status: validating_files
...
Fine-tuning since 46.79299879074097 seconds.
Status: running
...
Fine-tuning since 834.6565797328949 seconds.
Status: succeeded
Fine-tuning is finished!
Model ID: ft:gpt-3.5-turbo-0613...</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p139"> 
   <p>After printing out the job ID, we receive regular updates on the job status, typically proceeding from <code>validating_files</code> to <code>running</code> to (hopefully) <code>succeeded</code>. The problem is that the job may take a while to finish (for the previous example, about 14 minutes). If you don’t want to run the script continuously (e.g., to switch off your computer), you can interrupt the script after the fine-tuning job has started (you will know because the script prints out the job ID at that point). The fine-tuning job will proceed as planned on OpenAI’s servers. Depending on your setup, you may even receive an email notifying you once the job has finished. Otherwise, you can periodically run this script.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p140"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__checkFineTuneStatus"><span class="num-string">Listing <span class="browsable-reference-id">9.5</span></span> Checking for the status of fine-tuning jobs</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse
import openai

client = openai.OpenAI()

if __name__ == '__main__':
    
    parser = argparse.ArgumentParser()
    parser.add_argument('job_id', type=str, help='ID of fine-tuning job')
    args = parser.parse_args()
     #1
    job_info = client.fine_tuning.jobs.retrieve(args.job_id)
    print(job_info)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Retrieves and prints the job metadata
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p141"> 
   <p>Given the job ID (retrieved from the output of listing <a href="#code__fineTune">9.4</a>), the script retrieves and prints the job metadata (<strong class="cueball">1</strong>), including the job status and the ID of the resulting model (after the job has finished successfully).</p> 
  </div> 
  <div class="readable-text" id="p142"> 
   <h2 class=" readable-text-h2" id="using-the-fine-tuned-model"><span class="num-string browsable-reference-id">9.10</span> Using the fine-tuned model</h2> 
  </div> 
  <div class="readable-text" id="p143"> 
   <p>Congratulations! You have created a specialized model, fine-tuned to the task (review classification) you care about. How can you use it? Fortunately, doing so is straightforward using the OpenAI library. Instead of specifying the name of one of the standard models (e.g., <code>gpt-3.5-turbo</code>), we now specify the ID of our fine-tuned model, like so (replace the placeholder <code>[Fine-tuned model ID]</code> with the actual model ID):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p144"> 
   <div class="code-area-container"> 
    <pre class="code-area">import openai
client = openai.OpenAI()

response = client.chat.completions.create(
    model='[Fine-tuned model ID]',
    messages=[
        {'role':'user', 'content':prompt}
        ]
    )</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p145"> 
   <p>As before, we assume that the <code>prompt</code> variable contains the prompt text. The prompts, however, differ for our fine-tuned model. Previously, we described the classification task, along with the review text. Now we have trained our custom model to map the review text alone to an appropriate class. That means our prompt-generation function simplifies to the following (in fact, you might argue that creating a dedicated function is no longer required):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p146"> 
   <div class="code-area-container"> 
    <pre class="code-area">def create_prompt(text):
    """ Create prompt for sentiment classification.
    
    Args:
        text: text to classify.
    
    Returns:
        Prompt for text classification.
    """
    return text</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p147"> 
   <p>Instead of generating multipart prompts, we return the review text to classify. You may want to find out what happens when using the simplified prompt with the original model (<code>gpt-3.5-turbo</code>). You will see output like this:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p148"> 
   <div class="code-area-container"> 
    <pre class="code-area">Label: I understand your concern about smoking in movies, 
especially those intended for children and adolescents. 
Smoking in films can have an influence on young viewers 
and potentially normalize the behavior. However, it is 
important to note that not all instances of smoking in 
movies are the result of intentional product placement 
or sponsorship by tobacco companies.
...</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p149"> 
   <p>Clearly, the model gets confused about our intentions—that is, what we expect it to do with the input reviews. Instead of generating correct class labels, it writes elaborate analyses commenting on the primary points raised in the reviews. This is not unexpected. Imagine if someone handed you a review without any further instructions. How would you know that the person wanted you to classify the review, let alone the correct labels of the possible classes? It would be almost impossible to do so, and the same applies to language models.</p> 
  </div> 
  <div class="readable-text" id="p150"> 
   <p>However, if we switch to our fine-tuned model and provide the same prompts as input, we will get the following output instead:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p151"> 
   <div class="code-area-container"> 
    <pre class="code-area">Label: neg; Ground truth: neg
Label: neg; Ground truth: neg
Label: neg; Ground truth: neg
Label: neg; Ground truth: neg
Label: pos; Ground truth: pos
Label: pos; Ground truth: neg
Label: pos; Ground truth: neg
Label: neg; Ground truth: neg
Label: neg; Ground truth: pos
Label: neg; Ground truth: neg
Number of correct labels:    7  #1
Number of tokens used   :    2085  #2</pre> 
    <div class="code-annotations-overlay-container">
     #1 Improved accuracy
     <br/>#2 Lower token consumption
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p152"> 
   <p>Note that even without setting any tuning parameters (or providing any samples in the prompt), we now get an accuracy of 70% (<strong class="cueball">1</strong>), rather than the 60% in our original version! Also, the number of tokens used is reduced by about 200 compared to the initial version (<strong class="cueball">2</strong>). This is because we omit the instructions (and class labels) in each prompt.</p> 
  </div> 
  <div class="readable-text" id="p153"> 
   <p>Okay! We have seen that we can fine-tune a model to classify reviews accurately while reducing the prompt size. But the question remains: Was it worth it? Let’s do some calculations to find that out. We set aside the cost of generating the fine-tuned model because we only have to do that once (and in our example scenario, we assume that we want to analyze one year’s worth of reviews). Without fine-tuning, we can achieve the same accuracy (70%) with the generic model when exploiting tuning parameters (setting bias and a limit on the number of output tokens). In that case, we use 2,228 tokens for our 10 sample reviews. After fine-tuning, we only use 2,085 tokens for our sample reviews. However, with the generic model, we pay 0.05 cents per 1,000 input tokens. On the other hand, for the fine-tuned model, we pay 0.3 cents per 1,000 tokens. That means our cost per token is six times higher after fine-tuning! The moderate decrease in the number of tokens processed does not amortize the higher fees per token in this specific scenario.</p> 
  </div> 
  <div class="readable-text" id="p154"> 
   <p>In general, fine-tuning can be very helpful in increasing quality and possibly reducing costs. However, be aware that it comes with various overheads. Before using a fine-tuned model in production, evaluate it experimentally, do your calculations, and make sure it is worth it!</p> 
  </div> 
  <div class="readable-text" id="p155"> 
   <h2 class=" readable-text-h2" id="summary">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p156">Tuning parameter settings can influence model performance and cost.</li> 
   <li class="readable-text" id="p157">Consider limiting output length and introducing token logit bias.</li> 
   <li class="readable-text" id="p158">Do not always use the largest available model, as doing so increases cost.</li> 
   <li class="readable-text" id="p159">Identify the best model for your task by evaluating it on samples.</li> 
   <li class="readable-text" id="p160">The design of the prompt can have a significant effect on performance.</li> 
   <li class="readable-text" id="p161">Include samples of correctly solved tasks in the prompt for few-shot learning.</li> 
   <li class="readable-text" id="p162">Fine-tuning allows you to specialize base models to the tasks you care about. It may allow you to reduce prompt size due to specialization.</li> 
   <li class="readable-text" id="p163">Fine-tuning incurs overhead proportional to the amount of data trained on. It also increases the cost per token when you use the resulting model.</li> 
  </ul> 
 </div></div></body></html>