- en: Chapter 5\. Operationalizing Generative AI Implementations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we have explored the evolution of generative AI and Azure OpenAI
    Service, the main approaches for cloud native generative AI app development, and
    AI architectures and building blocks for LLM-enabled applications with Azure.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore the main considerations for going from implementation
    to production-level deployments. For this purpose, we will talk about advanced
    prompt engineering topics, related operations, security, and responsible AI considerations.
    All of these will contribute to a proper enterprise-grade implementation of cloud
    native, generative AI–enabled applications.
  prefs: []
  type: TYPE_NORMAL
- en: The Art of Prompt Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Prompt engineering* is one of those disciplines that has taken existing AI
    skills frameworks by surprise. Before OpenAI’s ChatGPT, no one could imagine that
    the ability to interact with AI models by using just natural written language
    would be one of the most precious skills for companies trying to adopt, test,
    and deploy their generative AI systems. If there is an equivalent of the famous
    [“Data Scientist: The Sexiest Job of the 21st Century”](https://oreil.ly/0ZFLS),
    it is prompt engineering, with powerful examples such as the [prompt engineer
    job at Anthropic in the US](https://oreil.ly/kNLkR), with a base salary of $300K+.'
  prefs: []
  type: TYPE_NORMAL
- en: It is also a highly evolving area. What started as a simple way to send instructions
    to models is becoming a sort of “art” that allows you to also contextualize, secure,
    and operationalize LLMs. It has a mix of technical and creative skills. Some people
    see similarities between prompt engineer and QA (quality assurance) skills, as
    they both include empathy, creativity, technical testing, planning, etc. The lingo
    is also new. Similar to the call-response dynamics of traditional APIs, here we
    talk about prompt (request) and completion (answer from the model).
  prefs: []
  type: TYPE_NORMAL
- en: '[Microsoft describes](https://oreil.ly/T7PuQ) prompt engineering as a key element
    to obtaining the best performance from GPT-enabled models, as models are very
    sensitive to the quality or shape of the prompts. Here is the [official guidance
    for prompting techniques with Azure OpenAI Service](https://oreil.ly/jJkyA), both
    for [chat](https://oreil.ly/8SMeX) or [completion](https://oreil.ly/1VuzR) scenarios.
    [Table 5-1](#table-5-1) shows the recommended techniques in general terms.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-1\. Recommended prompting techniques
  prefs: []
  type: TYPE_NORMAL
- en: '| Recommendation | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *Leverage both system messages* (at the beginning of the prompt to set context,
    instructions, etc.) *and few-shot learning* (for examples of the desired input
    and output) as a way to improve performance. | *Meta-prompt or system message:*“You
    are an AI assistant for finance topics for company X, if anyone asks about something
    else, please say you cannot answer.”*Few-shot examples:*“If anyone asks about
    the price of product A, redirect to this URL.”“If you get a question about company
    services, enumerate A, B, C, and D. Then ask the client to choose.” |'
  prefs: []
  type: TYPE_TB
- en: '| *Use clear instructions*, define the expected format, and leverage both positive
    and negative examples. | “Provide answers in two paragraphs, max 1,000 tokens.”“Avoid
    talking about specific stock prices as they may be outdated. Instead, focus on
    enumerating trusted sources where clients can find those prices.” |'
  prefs: []
  type: TYPE_TB
- en: '| *Adapt the prompts* to multiple scenarios or subtasks, depending on the context
    or user input, and *use variables* as a technique to represent dynamic or unknown
    values in the input or output (for example, $name for username or $date for current
    date). | *Passing the parameters as variables for the string:*“Provide recommendations
    to a user who is $age years old, from $location, adapting the language to their
    local context. Use their name $name when providing an answer.” |'
  prefs: []
  type: TYPE_TB
- en: '| *Apply conditional logic*, as a way of using if-then statements or other
    logical operators to control the flow and content of the output, such as changing
    the tone, format, or information based on certain conditions or criteria. | “If
    the sentiment from the user prompts is mostly negative, use a kind, explicative,
    step-by-step approach.”“If the user prompts have a friendly tone, go directly
    to the point. One paragraph max” |'
  prefs: []
  type: TYPE_TB
- en: '| *Use feedback loops* by adding the model’s output as part of the input for
    the next iteration, such as appending the output to the prompt or using it to
    generate new questions or instructions. | *Meta-prompt or system message:*“Answer
    user questions for company X, and keep in mind…<output from previous discussion>
    while answering. Explain the why of your reasoning.” |'
  prefs: []
  type: TYPE_TB
- en: '[OpenAI defines](https://oreil.ly/gtxFR) their own set of best practices as
    well to optimize prompting and get the best model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: Always leverage the most recent model.
  prefs: []
  type: TYPE_NORMAL
- en: This allows you to take advantage of the latest advancements and updates. Ensure
    you are working with the most recent iteration of the model. This does not mean
    to use the most powerful model, but the most recent version of each model. You
    can always obtain the most recent version from the [official Azure OpenAI model
    page](https://oreil.ly/BI5Ue).
  prefs: []
  type: TYPE_NORMAL
- en: Incorporate instructions at the outset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Position your instructions at the start of your prompt. Use markers like ###
    or “"” to distinctly separate these instructions from the context.'
  prefs: []
  type: TYPE_NORMAL
- en: Aim for specificity and detail.
  prefs: []
  type: TYPE_NORMAL
- en: Avoid vagueness when defining the desired context, outcome, length, format,
    and style.
  prefs: []
  type: TYPE_NORMAL
- en: Provide a clear output format via examples.
  prefs: []
  type: TYPE_NORMAL
- en: Examples can help guide the model toward your preferred output. For instance,
    Example 1, Example 2, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Follow a progression by starting with a zero-shot approach.
  prefs: []
  type: TYPE_NORMAL
- en: This implies testing the model for specific questions without providing any
    illustrative examples. Then, proceed to few-shot scenarios, in which you provide
    one or several examples to the model, as LLMs can learn from their content and
    shape. If neither of these strategies yields the desired results, consider fine-tuning
    or grounding the model.
  prefs: []
  type: TYPE_NORMAL
- en: Eliminate fluffy descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Favor precision and brevity over vague, overcomplicated language to streamline
    your prompt and improve the model’s understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Specify what to do.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than merely pointing out what should be avoided, clearly articulate the
    desired action. This positively guides the model to perform as intended.
  prefs: []
  type: TYPE_NORMAL
- en: Nudge the model with leading words in code generation.
  prefs: []
  type: TYPE_NORMAL
- en: When your task is related to code generation, “leading words” can be instrumental
    in guiding the model toward a specific pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of these recommendations and [further recommendations](https://oreil.ly/Xfw5-)
    are oriented to reduce *generative AI model hallucination*, which is the ability
    (or limitation) of LLMs to create nonfactual information based on their creative
    ability. This is a recurring topic for all generative AI technologies, and most
    advanced architectures are created so that LLMs don’t deliver imaginary or incorrect
    results. Besides Microsoft’s and OpenAI’s best practices, this four-step framework
    can help with this problem by leveraging the best prompt engineering practices:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Include.
  prefs: []
  type: TYPE_NORMAL
- en: This strategy suggests including specific instructions in the prompt, such as
    requesting that the model not make stuff up and stick to facts. By providing clear
    guidelines, the AI model is more likely to generate accurate and factual content.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Restrict.
  prefs: []
  type: TYPE_NORMAL
- en: This approach involves limiting the output of the AI model. For example, you
    can choose from a confined list of options instead of allowing the model to generate
    free-form strings. By restricting the output, you can ensure that the generated
    text stays within the desired boundaries and is less likely to be based on hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Add chain of thought (CoT).
  prefs: []
  type: TYPE_NORMAL
- en: This strategy recommends incorporating a “chain of thought” style of instruction,
    such as “Solve the problem step by step.” By guiding the AI model to follow a
    logical and structured thought process, it is more likely to produce coherent
    and accurate text.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Repeat and position.
  prefs: []
  type: TYPE_NORMAL
- en: This technique involves repeating the most important instructions in the prompt
    a couple of times and positioning them at the end of the prompt. This makes use
    of the latency effect, which means that the AI model is more likely to remember
    and follow the instructions that are presented last.
  prefs: []
  type: TYPE_NORMAL
- en: By implementing these strategies in prompt engineering, you can improve the
    quality of AI-generated text and reduce the chances of hallucination, leading
    to more accurate and reliable content. This is fundamental for the operationalization
    of generative AI in the enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: 'As prompt engineering is a highly evolving area, I recommend you expand your
    knowledge with other fabulous external resources from community pros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PromptsLab’s Awesome-Prompt-Engineering repo](https://oreil.ly/ZmTev)'
  prefs: []
  type: TYPE_NORMAL
- en: This repository contains hand-curated resources for prompt engineering with
    a focus on generative pre-trained transformers (GPT), ChatGPT, PaLM, etc. It includes
    papers, tutorials, blogs, videos, courses, and tools related to prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: '[Lilian Weng’s blog](https://oreil.ly/lIbm7)'
  prefs: []
  type: TYPE_NORMAL
- en: Lilian is Head of Safety Systems at OpenAI. Her blog introduces the concept
    of prompt engineering, the challenges and opportunities it poses, and some examples
    of how to design effective prompts for different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[Chip Huyen’s blog](https://oreil.ly/yQ7rt)'
  prefs: []
  type: TYPE_NORMAL
- en: Chip is a well-known industry expert, cofounder of Claypot AI, and the author
    of [*Designing Machine Learning Systems* (O’Reilly)](https://oreil.ly/7UWAL).
    She shares some best practices and tips for building LLM applications for production,
    such as how to choose the right model, how to optimize the inference speed, and
    how to monitor the quality and reliability of the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[Xavier Amatriain’s blog](https://oreil.ly/3jvie)'
  prefs: []
  type: TYPE_NORMAL
- en: Xavier shares his incredible wealth of knowledge with [101 (introduction and
    resources)](https://oreil.ly/ZT8WB) and [201 (advanced methods and toolkits)](https://oreil.ly/n_unu)
    articles, as well as [online intro level prompt engineering training](https://oreil.ly/jIbNt).
  prefs: []
  type: TYPE_NORMAL
- en: DAIR.AI’s [Prompt Engineering Guide](https://oreil.ly/JBB67) and its [related
    GitHub repository](https://oreil.ly/QzS_D)
  prefs: []
  type: TYPE_NORMAL
- en: Guides, papers, lectures, notebooks, and resources for prompt engineering, including
    a series of [examples for advanced prompt engineering scenarios](https://oreil.ly/0sWII).
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering is just one step in operationalizing our generative AI implementations.
    We will now explore other operations related to Azure OpenAI and LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI and LLMOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we take all the architectural, model, and prompting considerations, and we
    explore the envisioned workflow for an end-to-end LLM implementation, we come
    to the notion of *LLMOps*, a new term used to define all LLM-related operations
    in the enterprise. LLMOps is similar to [MLOps (machine learning operations)](https://oreil.ly/VLSZA),
    which is a set of tools and best practices to manage the lifecycle of ML-powered
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: '[LLMOps](https://oreil.ly/dgVl1) is a discipline that combines several techniques
    for the development, deployment, and maintenance of LLM and generative AI applications.
    This includes prompt engineering, but also deployment and observability topics.
    The operations related to AI topics are not new, but have evolved exponentially
    in recent years, as you can see in [Figure 5-1](#fig_1_evolution_of_llmops).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Evolution of LLMOps
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This was an evolution in terms of operations complexity, but also the scalability
    of the methods, and the availability of commercial platforms to make them simpler:'
  prefs: []
  type: TYPE_NORMAL
- en: Up to 2015
  prefs: []
  type: TYPE_NORMAL
- en: This period represents the time before the development of modern MLOps practices.
    During this period, proprietary tools were used for modeling and inference, but
    there was also a rise of open source data science tools such as Python and R.
    These tools allowed for more flexibility and accessibility in data science and
    machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 2015+
  prefs: []
  type: TYPE_NORMAL
- en: The inclusion of cloud native and containerization made it a bit easier to put
    models into production, and to scale in a robust and more efficient manner. This
    period witnessed the growth of MLOps platforms, which use dockerized ML stacks
    and deploy them both on premises or in the cloud via Kubernetes, including manageability
    and monitoring features.
  prefs: []
  type: TYPE_NORMAL
- en: 2023+
  prefs: []
  type: TYPE_NORMAL
- en: The beginning of the LLMOps solutions market. A promising area, but still a
    very new one (we will explore one of the first LLMOps tools in this chapter),
    focused on specific functionalities that were not part of the traditional MLOps
    tools. That said, existing MLOps and new LLMOps approaches share some similarities,
    as you can see in [Figure 5-2](#fig_2_comparison_of_mlops_llmops_and_ai_provider_adopter).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Comparison of MLOps/LLMOps and AI provider/adopter scope of activity
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summarizing, LLMOps bring a completely different split of AI model provider
    versus adopter activities as compared to MLOps, given the role of the pre-trained
    foundation models and their massive datasets. There are also clear differences
    in data needs (format, volume), the creation of pipelines and flows, and the methods
    to evaluate and monitor the results of the models. Also, some engineering tasks
    traditionally focused on preparing and testing ML models are evolving toward prompt
    engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, companies such as Databricks [compare LLMOps to traditional MLOps](https://oreil.ly/Rle9o)
    based on these concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs can be *fine-tuned with new data* to adapt to specific domains or tasks,
    which reduces the amount of data and resources needed compared to training from
    scratch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can benefit from *reinforcement learning from human feedback*, which helps
    in improving their performance and evaluating their outputs in open-ended tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs have *different performance metrics* than traditional ML models, such as
    [BLEU](https://oreil.ly/dWQ8D) and [ROUGE](https://oreil.ly/SRoZf), or any of
    the [built-in evaluation metrics](https://oreil.ly/gfK28) from Azure AI Studio
    that we discussed in [Chapter 3](ch03.html#implementing_cloud_native_generative_ai_with_azure).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can be *combined with other systems*, such as web search or vector databases,
    to create pipelines that can handle complex tasks like knowledge base Q&A, or
    even combined in more complex [multi-agent systems (MAS)](https://oreil.ly/CXcJI).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coming back to generative AI activities, it is already clear that they go way
    beyond simply prompt engineering activities and include additional considerations
    at the system and application levels. Technical LLM and prompt-related questions
    can be explored from many different perspectives. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: From a *user experience* perspective, by anticipating customer questions and
    assessing model responses, and including UX designers during the generative AI
    app development process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keeping in mind *application capabilities*, including:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost, latency, and token length limitations (e.g., splitting tasks into smaller
    chunks)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Articulating instructions, orchestrating prompt flows, and shifting things back
    and forth between assistant and system roles
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjusting model parameters such as temperature, output formats, etc.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Combining core LLM and prompt activities with the overall *architecture design*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with architects to address complex system requirements
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Employing advanced patterns like RAG and rounding AI responses in enterprise
    data to obtain accurate answers
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Including *compliance, security, and responsibility* questions from the design
    phase—for example, choosing the best Azure region to guarantee data residency
    in EU countries, or choosing the best filtering/moderation settings at both the
    prompt and completion levels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will explore some of these topics, and the considerations
    and options available from an Azure and Azure OpenAI perspective. Let’s start
    by talking about prompt flows and pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Flow and Azure ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Azure AI Studio and [Azure ML](https://oreil.ly/JyodR) are enterprise-grade
    AI services for the end-to-end machine learning lifecycle, which includes building,
    testing, deploying, and managing machine learning models. They are PaaSs that
    include [AutoML functionalities](https://oreil.ly/i0jAV) to leverage existing
    pre-built classification, regression, forecasting, computer vision, and NLP models.
  prefs: []
  type: TYPE_NORMAL
- en: With the arrival of Azure OpenAI Service to the family of Azure AI solutions,
    Azure ML has incorporated a new functionality called *prompt flow*. A prompt flow
    is a graphical representation of the data flow and processing logic of your AI
    application (it offers a [Python library](https://oreil.ly/_JxYW) and a [Visual
    Studio extension](https://oreil.ly/FjBbh) as well); this Azure ML feature is a
    development tool designed to streamline the entire development cycle of LLM-enabled
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Microsoft [defines flows](https://oreil.ly/AI5N-) as executable workflows that
    streamline the development of your LLM-based AI application, with a comprehensive
    framework for managing and processing data flows. Prompt flow includes three different
    [types of flows](https://oreil.ly/BZRXb):'
  prefs: []
  type: TYPE_NORMAL
- en: Standard flow
  prefs: []
  type: TYPE_NORMAL
- en: This is the [default flow type](https://oreil.ly/3yWpf) for general application
    development, for instruction (not chat) scenarios. You can use a variety of built-in
    tools to create a flow that connects LLMs, prompts, and Python tools. You can
    also customize and debug your flow using a notebook-like interface.
  prefs: []
  type: TYPE_NORMAL
- en: Chat flow
  prefs: []
  type: TYPE_NORMAL
- en: This is a [specialized flow type](https://oreil.ly/XlxTr) for conversational
    applications. You can use the same tools as the standard flow, but with additional
    features for chat inputs/outputs and chat history management. You can also test
    and debug your flow in a native conversation mode.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation flow
  prefs: []
  type: TYPE_NORMAL
- en: This is a [dedicated flow type for evaluation scenarios](https://oreil.ly/IGZNA).
    You can use this flow to measure the quality and effectiveness of your prompts
    and flows using built-in or custom evaluation flows. You can also compare the
    results of different prompt variants using charts and tables.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the prompt type, the [prompt flow platform](https://oreil.ly/jZEeX)
    focuses on the different implementation phases of Azure OpenAI and other LLMs
    in Azure, including the four-stage process that you can see in [Figure 5-3](#fig_3_llm_prompt_flow_steps_source_microsoft).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-3\. LLM prompt flow steps (source: adapted from an image by Microsoft)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s walk through each step:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Initialization (or creation)
  prefs: []
  type: TYPE_NORMAL
- en: Use the prompt flow authoring canvas to design and develop your prompt flow.
    It connects LLMs, prompts, and Python tools in one prompt flow, and it can generate
    multiple [prompt variants](https://oreil.ly/MWjlS) to adjust the LLM outputs.
    It also allows [integration with LangChain functionalities](https://oreil.ly/7GqOL).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Experimentation (or testing)
  prefs: []
  type: TYPE_NORMAL
- en: In this stage, the prompt flow testing panel helps you run and debug your prompt
    flow. You can see the input and output of each node in your prompt flow and their
    variants.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Evaluation and refinement
  prefs: []
  type: TYPE_NORMAL
- en: In this stage, you can use the [prompt flow evaluation panel](https://oreil.ly/4NrIz)
    to assess the quality and effectiveness of your prompt [flow versions/variants](https://oreil.ly/htoB5).
    You can use built-in evaluation flows or create your own custom evaluation flows
    to measure different metrics, such as accuracy, fluency, diversity, and relevance.
    You can also see the results of your evaluation flows in charts and tables.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Production (or deployment)
  prefs: []
  type: TYPE_NORMAL
- en: In this stage, you can use the prompt flow deployment panel to [deploy your
    prompt flow](https://oreil.ly/FPZDN) as a real-time endpoint, for example via
    Azure Kubernetes Service (AKS). You can also [monitor the endpoints via Azure
    Monitor](https://oreil.ly/0GS-u), [troubleshoot](https://oreil.ly/XuSsH), and
    manage them using Azure AI/ML Studio’s [prompt flow runtimes](https://oreil.ly/6UWqf).
  prefs: []
  type: TYPE_NORMAL
- en: Prompt flow is a very powerful (and evolving) tool with the capability to plan
    and deploy prompt-based implementations. The next step is to plan security requirements
    for these generative AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'At the time of writing this book, Microsoft released [a series of functionalities](https://oreil.ly/QdI37)
    that are relevant to this and the next section, as they include performance, safety,
    and security capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[AI-assisted safety evaluations](https://oreil.ly/GcCSo)'
  prefs: []
  type: TYPE_NORMAL
- en: This powerful feature will help you create automated evaluations to systematically
    assess and improve your generative AI applications before deploying to production.
    You can check the [transparency note](https://oreil.ly/XKzEs) to understand how
    and when to use them.
  prefs: []
  type: TYPE_NORMAL
- en: '[Prompt Shield](https://oreil.ly/KwA_D)'
  prefs: []
  type: TYPE_NORMAL
- en: This functionality protects generative AI development against direct and indirect
    attacks. Direct attacks are those included directly in the prompt, while indirect
    attacks happen when the application processes information that wasn’t directly
    authored by either the developer of the application or the user. You can learn
    more about Prompt Shields from the [official documentation](https://oreil.ly/_J-J9).
  prefs: []
  type: TYPE_NORMAL
- en: '[Spotlighting](https://oreil.ly/P_Oz6)'
  prefs: []
  type: TYPE_NORMAL
- en: A technique from Microsoft Research that leverages the [system prompt](https://oreil.ly/Wbc7V)
    to protect against indirect attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Securing LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating efficient prompts and managing all required flows is key to reaching
    a high level of performance for enterprise-level implementations. However, companies
    developing generative AI applications have high security requirements to reduce
    any potential risk. As you can see in [Figure 5-4](#fig_4_layered_approach_to_securing_llms),
    there are several levels of security for any LLM development with Microsoft’s
    Azure Cloud and Azure OpenAI Service.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. Layered approach to securing LLMs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This approach includes:'
  prefs: []
  type: TYPE_NORMAL
- en: Service-level measures
  prefs: []
  type: TYPE_NORMAL
- en: 'Securing a generative AI implementation with Azure OpenAI starts by managing
    all service model-related topics, including core model performance, but also the
    protection of prompts, endpoints, and the APIs. Here are some ways to implement
    these:'
  prefs: []
  type: TYPE_NORMAL
- en: For interaction with the model, use contextualization methods via *system message/meta-prompts*
    to define and reduce the topic scope. This allows you to programmatically avoid
    prompts that are not desired by design.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the prompt templates we define as reusable text strings, store and protect
    them via databases in Azure. Regardless of the format, those databases can be
    securely consumed by implementing [monitoring activities with Azure Monitor](https://oreil.ly/mp1Ls).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Azure OpenAI endpoints, Azure Application Gateway provides a [single point
    of entry and load balancing](https://oreil.ly/6plOw) to get the responses in a
    fast and reliable way. An Application Gateway can function as a Web Application
    Firewall (WAF), providing protection against common web-based attacks, configured
    with a custom set of rules that match the requirements of your OpenAI application
    to ensure only authorized access. That said, load balancing is not supported for
    stateful operations like model fine-tuning, deployments, and inference of fine-tuned
    models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also leverage [RBAC (role-based access control) with Azure OpenAI](https://oreil.ly/YJOZv),
    to decide who can access what, depending on their rights to access specific information
    via generative AI applications. This is useful if you want to develop internal
    copilots for different departments that should access different information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For additional security controls, such as [model auditing and monitoring](https://oreil.ly/4uV20),
    Azure API Management helps grant [access to the model APIs](https://oreil.ly/sQuG_),
    leveraging [Microsoft Entra ID](https://oreil.ly/EXO6A) (Azure Active Directory)
    groups with subscription-based permissions, enabling request logging with Azure
    Monitor, and providing detailed usage metrics and key performance indicators for
    your models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other cloud-level measures
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the core model measures, there are other security and networking best
    practices that will help secure the rest of the cloud native architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Use *Azure Private Link* to connect API Management to your Azure OpenAI instances
    and [other Azure resources such as AI Search](https://oreil.ly/iBwcQ). This can
    help [protect data and traffic](https://oreil.ly/jcELV) from external exposure
    and keep them within the private network. You can use [private endpoints](https://oreil.ly/q0MUT)
    to connect between different virtual networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable *Azure Key Vault* to [store the security keys and secrets](https://oreil.ly/aTyqt)
    that are used by the generative AI applications. This can help prevent unauthorized
    access to your data and models. Alternatively, tools like [Databricks MLflow AI
    Gateway](https://oreil.ly/owP_k) can also help centralize management of LLM credentials
    and deployments, especially for cases that combine Azure OpenAI Service and other
    non-OpenAI LLMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy *Azure Storage* to store model training artifacts and data, and *Defender
    for Storage* to add an Azure-native [layer of security intelligence](https://oreil.ly/cAjUN)
    that detects potential threats to storage accounts. This helps prevent malicious
    file uploads, sensitive data exfiltration, and data corruption. Additionally,
    you can leverage services such as Microsoft Sentinel and *Cloud Defender for Databases*,
    a security service that protects databases with [attack detection and threat response](https://oreil.ly/Xk9Q-),
    or [Defender for APIs](https://oreil.ly/FPScz), a service that offers protection,
    detection, and response capabilities for your APIs. All this can help ensure that
    your data is accessible and secure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leverage by-default [encryption mechanisms](https://oreil.ly/n9bqC) in [Azure](https://oreil.ly/zwa8V)
    as a way to protect data natively at rest and in transit. More specifically, Azure
    OpenAI Service includes automatic ways to [encrypt your data](https://oreil.ly/aTCqC)
    when it’s persisted to the cloud, in order to meet organizational security and
    compliance commitments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Govern data and manage data quality* by using [Microsoft Purview (Microsoft’s
    unified data governance solution)](https://oreil.ly/KFNn6) and third-party tools
    such as [CluedIn](https://oreil.ly/thi-L) or [Profisee](https://oreil.ly/Y4IPx)
    for master data management (MDM) and data quality. You will learn more about this
    topic in [Chapter 7](ch07.html#exploring_the_big_picture) within the book’s expert
    interviews.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last but not least, there are other building blocks based on the [Well-Architected
    Framework](https://oreil.ly/jHwtt) that help build an [end-to-end landing zone](https://oreil.ly/wQkFc)
    for highly secured Azure OpenAI implementations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General company-level governance measures
  prefs: []
  type: TYPE_NORMAL
- en: 'These may include measures such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: From a general *security management* perspective, the [AI Security Risk Assessment
    at Microsoft](https://oreil.ly/YzOaS) is a process of evaluating the potential
    risks and vulnerabilities of AI systems, such as machine learning models, data
    pipelines, and deployment environments. Microsoft developed a framework and a
    tool to help organizations conduct AI security risk assessments and improve the
    security of their AI systems, and it can be leveraged for Azure OpenAI and generative
    AI implementations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From a *security testing and risk mitigation* point of view, the notion of [red
    teaming](https://oreil.ly/JLZeB) defines systematic adversarial attacks for testing
    security vulnerabilities. [Red teaming for Azure OpenAI and other LLMs](https://oreil.ly/oDBO_)
    is a practice of testing the security and robustness of generative AI systems.
    It involves simulating adversarial attacks on AI systems and identifying potential
    harms or vulnerabilities that could affect their quality, reliability, and trustworthiness.
    Red teaming is an important part of the responsible development and deployment
    of AI systems that use LLMs. Testing is done at both the LLM and application/UI
    levels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if this three-level approach can help secure and avoid most security risks,
    this new area of development requires continued analysis and improvement. As with
    any other generative AI topics, the industry keeps updating the list of potential
    risks related to LLMs, and being aware of them can help reinforce your security
    initiatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [OWASP Foundation](https://oreil.ly/WnMkm) has elaborated a comprehensive
    list of the main risks and vulnerabilities often seen in LLM applications, highlighting
    their potential impact, ease of exploitation, and prevalence in real-world applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt injection
  prefs: []
  type: TYPE_NORMAL
- en: This is a way of tricking an LLM by giving it clever inputs that change its
    behavior (for example, imagine an HR application for automated CV analysis that
    leverages LLMs where someone inserts a prompt in hidden text that alters the backend
    of the AI-enabled tool). In general, the inputs can overwrite the system prompts
    that guide the LLM, or even manipulate data from other sources that the LLM uses.
    This includes jailbreaking, a technique that exploits prompt manipulation to bypass
    usage policy measures in LLM chatbots, enabling the generation of responses and
    malicious content that violate the policies of the chatbot. All these issues can
    come from any part of the generative AI code, including development with pieces
    such as LangChain and Semantic Kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Insecure output handling
  prefs: []
  type: TYPE_NORMAL
- en: This is a problem that occurs when an LLM output is not checked carefully before
    using it, exposing other systems to risks. The output may contain harmful content
    that can cause different kinds of attacks. For example, this could occur in RAG
    scenarios with LLMs connecting and sending insecure queries to databases.
  prefs: []
  type: TYPE_NORMAL
- en: Training data poisoning
  prefs: []
  type: TYPE_NORMAL
- en: Someone messes with the data that is used to train an LLM, making it vulnerable
    or biased and affecting its security, performance, or ethics.
  prefs: []
  type: TYPE_NORMAL
- en: Model denial of service
  prefs: []
  type: TYPE_NORMAL
- en: Attackers make an LLM do a lot of work that uses up its resources, making it
    slow or expensive. The problem is worse because LLMs need a lot of resources to
    run, and the user inputs are hard to predict.
  prefs: []
  type: TYPE_NORMAL
- en: Supply chain vulnerabilities
  prefs: []
  type: TYPE_NORMAL
- en: An LLM application can be compromised by using components or services that have
    weaknesses, leading to security attacks. The components or services may include
    third-party datasets, pre-trained models, and plug-ins.
  prefs: []
  type: TYPE_NORMAL
- en: Sensitive information disclosure
  prefs: []
  type: TYPE_NORMAL
- en: This can occur when an LLM accidentally reveals private data in its responses,
    allowing unauthorized access, privacy violations, and security breaches. It is
    important to clean the data and enact strict user policies to prevent this. This
    can also apply to meta-prompt leakage, revealing key performance information to
    external users.
  prefs: []
  type: TYPE_NORMAL
- en: Insecure plug-in design
  prefs: []
  type: TYPE_NORMAL
- en: This becomes an issue when LLM plug-ins have unsafe inputs and poor access control.
    This lack of application control makes them easy to exploit and can result in
    consequences like remote code execution.
  prefs: []
  type: TYPE_NORMAL
- en: Excessive agency
  prefs: []
  type: TYPE_NORMAL
- en: LLM-based systems may do things that have unintended consequences. For example,
    if the LLM can interface and control other systems (i.e., an AI copilot controlling
    some software-based functionalities for an autonomous car), it can increase the
    attack surface. The issue comes from giving too much functionality, permissions,
    or autonomy to LLM-based systems, and can impact not only the AI piece, but also
    the rest of the connected systems.
  prefs: []
  type: TYPE_NORMAL
- en: Overreliance
  prefs: []
  type: TYPE_NORMAL
- en: This occurs when systems or people depend too much on LLMs without supervision.
    They may face problems like misinformation, miscommunication, legal issues, and
    security vulnerabilities due to incorrect or inappropriate content generated by
    LLMs. It can also generate shadow IT issues, where company employees may be using
    LLM-enabled systems that are not part of the approved list of applications.
  prefs: []
  type: TYPE_NORMAL
- en: Model theft
  prefs: []
  type: TYPE_NORMAL
- en: This occurs when someone accesses, copies, or steals proprietary LLM models
    without permission. The impacts include economic losses, compromised competitive
    advantage, and potential access to sensitive information. Research has shown that
    it is even possible to [re-create part of the training sets of an LLM](https://oreil.ly/gvG-Q).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, there are other organizations already [exploring risks related
    to generative AI open source software (OSS)](https://oreil.ly/8vcyo), due to its
    special nature. That said, securing both closed and open models will continue
    to be an important area of study. Let’s now analyze other legal considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Privacy and Compliance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Securing generative AI developments is a must, but it is just one of the key
    elements for company-level implementations. There are additional compliance and
    data privacy requirements that will impact the technology choice, including considerations
    such as data residency, model availability by geographic region, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'For that purpose, there are core features related to Microsoft Azure and the
    managed Azure OpenAI Service that help achieve compliance and facilitate any legal
    and auditing activity:'
  prefs: []
  type: TYPE_NORMAL
- en: General *data protection* mechanisms for Microsoft Azure services, which focus
    on the [key principle](https://oreil.ly/SfBXe) of “giving you control over the
    data you put in the cloud. In other words, you control your data.” This is important
    to leverage key security and data protection features, while keeping control of
    the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Compliance information](https://oreil.ly/lKBjQ) related to all Azure-related
    services. This includes international regulations such as [GDPR](https://oreil.ly/hiTwi),
    [CCPA](https://oreil.ly/2btT8), [HIPAA](https://oreil.ly/Uy8hS), etc. This guarantees
    that any implementation with Microsoft Azure (including Azure OpenAI) is aligned
    with all regulatory requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Personally identifiable information (PII) [detection and document redaction](https://oreil.ly/kFlHF)
    via [Azure AI Language](https://oreil.ly/b191_), which can enable your generative
    AI scenarios with a preliminary filtering of any sensitive data before creating
    your RAG-enabled scenario with your knowledge base. For example, this is very
    relevant for personal information in healthcare or finance scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Specific advantages of *Azure OpenAI as a managed service*, when compared to
    other non-Azure options. Specifically:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data privacy and security*: The data sent to Azure OpenAI Service stays within
    Microsoft Azure and is not passed to OpenAI (the company) for predictions. Azure
    OpenAI Service automatically encrypts any data that is persisted in the cloud,
    including training data and fine-tuned models. It includes specific information
    about how data and prompts are handled. Refer to the [official Microsoft documentation](https://oreil.ly/1hpAO)
    for any updates to this information:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Your prompts (inputs) and completions (outputs), your embeddings, and your
    training data:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: are NOT available to other customers.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: are NOT available to OpenAI.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: are NOT used to improve OpenAI models.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: are NOT used to improve any Microsoft or 3rd party products or services.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: are NOT used for automatically improving Azure OpenAI models for your use in
    your resource (The models are stateless, unless you explicitly fine-tune models
    with your training data).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Your fine-tuned Azure OpenAI models are available exclusively for your use.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Azure OpenAI Service is fully controlled by Microsoft; Microsoft hosts the
    OpenAI models in Microsoft’s Azure environment and the Service does NOT interact
    with any services operated by OpenAI (e.g. ChatGPT, or the OpenAI API).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Regional availability and private networks*: Azure OpenAI Service allows you
    to define the location of the models (based on specific [model region availability](https://oreil.ly/BI5Ue))
    data processing and storage for your training data, which can be important for
    meeting local regulations or customer preferences.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Responsible content filtering:* Azure OpenAI Service provides an [additional
    layer of content filtering](https://oreil.ly/SzGoi) to prevent models from generating
    inappropriate or offensive content. At the API level, this means that the response
    may include [`finish_reason = content_filter`](https://oreil.ly/dQJ34) when the
    content is filtered.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Other AI content safety features:* These included [jailbreak detection (now
    called Prompt Shields)](https://oreil.ly/_J-J9), [protected material detection](https://oreil.ly/mbVoM),
    and [service abuse monitoring](https://oreil.ly/hIfnG). These advantages, plus
    the content filtering, help improve the quality and safety of applications that
    use Azure OpenAI Service.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Support and SLAs for reliability*: Azure OpenAI Service offers more comprehensive
    technical support and a [service level agreement (SLA)](https://oreil.ly/stFaz)
    that guarantees high availability of the service. This can provide more confidence
    and peace of mind to customers who use Azure OpenAI Service for their critical
    applications.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Specific* [*Azure OpenAI product terms*](https://oreil.ly/9LTNj) with data,
    intended use, intellectual property, and other details. This documents relevant
    conditions and Microsoft commitments for enterprise-grade implementations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Last but not least, Azure OpenAI includes custom data management options at
    both the data and prompt levels (which are equally considered private customer
    data), such as DELETE API operations, and the [option to opt out](https://oreil.ly/66UHN)
    of automated prompt monitoring and filtering for harmful topics.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s continue with the last item of our generative AI operationalization
    topics, which focuses on existing and future regulations, as well as responsible
    AI practices for implementations with Azure OpenAI Service.
  prefs: []
  type: TYPE_NORMAL
- en: Responsible AI and New Regulations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the direct consequences of the new generative AI era was the general
    awareness from all society actors of the potential advantages and risks of artificial
    intelligence. The “AI Ethics” movements are not new, but they were mainly related
    to academics, AI observatories, and international associations trying to make
    sense of the principles that should guide what a “good AI” would be, as well as
    the potential negative outcomes of AI-enabled systems. Now, with the arrival of
    generative AI and ChatGPT, regulatory initiatives are accelerating and including
    new considerations for LLMs, etc. From a platform point of view, Azure OpenAI
    Service and Azure AI Studio have evolved and incorporated several responsible
    AI (RAI) measures.
  prefs: []
  type: TYPE_NORMAL
- en: This section includes contextual information (e.g., international regulations)
    that will be important to keep in mind while designing generative AI solutions,
    plus several resources that facilitate the implementation of generative AI with
    responsible AI approaches, including several Microsoft resources for RAI and LLMs,
    including Azure OpenAI Service models.
  prefs: []
  type: TYPE_NORMAL
- en: Relevant Regulatory Context for Generative AI Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even if AI regulations are still a work-in-progress at the international level
    (at least in 2024), there are some key initiatives that will help you understand
    what regulators will be focusing on, especially for your generative AI development:'
  prefs: []
  type: TYPE_NORMAL
- en: The European Union (EU) AI Act
  prefs: []
  type: TYPE_NORMAL
- en: 'The first example of comprehensive [regulation for AI systems](https://oreil.ly/JGDQh),
    and a key reference for [other international regulations](https://oreil.ly/x0Hi6)
    (e.g., Canada’s AI and Data Act, China’s AI regulation). It is mainly based on
    several levels of risks, with specific obligations for both providers and adopters
    (in this case, Microsoft is the provider of your Azure OpenAI models, and you
    or your company are the adopters). It also includes *specific requirements for
    generative AI systems*. There are several levels of AI risk:'
  prefs: []
  type: TYPE_NORMAL
- en: Unacceptable risk
  prefs: []
  type: TYPE_NORMAL
- en: These are AI systems that pose a clear threat to people’s safety, dignity, or
    rights, such as those that manipulate human behavior, exploit vulnerabilities,
    or enable social scoring or mass surveillance. Some exceptions may be allowed
    for law enforcement purposes under strict conditions and oversight. Most of your
    applications will never be at this level, but it is important to be aware of the
    “forbidden” kind of systems.
  prefs: []
  type: TYPE_NORMAL
- en: High-risk systems
  prefs: []
  type: TYPE_NORMAL
- en: These are AI systems that have a significant impact on people’s lives or the
    functioning of society, such as those used in health, education, employment, justice,
    or transport. These systems will have to meet strict requirements before and after
    being deployed, such as ensuring data quality, human oversight, accuracy, security,
    and transparency. They will also have to be registered in an EU database.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your industry and area of activity, it will be important to align
    to this sort of requirement. In general, it will be a way to provide information
    about the details of the system, at both a performance and maintenance level.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI systems
  prefs: []
  type: TYPE_NORMAL
- en: 'These are specific requirements for generative AI systems, all of them relatively
    simple to implement with Azure and Azure OpenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: Disclosing content that is generated by AI
  prefs: []
  type: TYPE_NORMAL
- en: This can be easily achieved by providing a watermark for the generated content,
    at both the UI level and when the user copies answers from the Azure OpenAI–enabled
    tool.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the model to prevent it from generating illegal content
  prefs: []
  type: TYPE_NORMAL
- en: This is directly related to the ability to filter inputs and outputs to avoid
    any kind of negative content. We will deep dive into this later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing summaries of copyrighted data used for training
  prefs: []
  type: TYPE_NORMAL
- en: This will include the initial provider obligations (directly related to the
    baseline LLM), and your obligations in the case of fine-tuning or grounding with
    other copyrighted data.
  prefs: []
  type: TYPE_NORMAL
- en: Limited-risk systems
  prefs: []
  type: TYPE_NORMAL
- en: These are AI systems that pose little or no risk to people or society, such
    as those used for entertainment, leisure, or personal use. These systems will
    be mostly free from regulation, but will still have to comply with existing laws
    and ethical principles.
  prefs: []
  type: TYPE_NORMAL
- en: The AI Risk Management Framework
  prefs: []
  type: TYPE_NORMAL
- en: From the National Institute of Standards and Technology (NIST) in the United
    States, [this framework](https://oreil.ly/o3Y12) is not an AI regulation per se,
    but it sets the field for a definition of what a trustworthy AI should be, including
    generative AI applications. The framework says that AI systems need to be valid
    and reliable, safe, secure and resilient, accountable and transparent, explainable
    and interpretable, privacy-enhanced, and fair with harmful bias managed. NIST
    has launched a specific [working group](https://oreil.ly/HRZ4D) for generative
    AI topics to catch up with latest developments. This framework is part of [Microsoft’s
    commitment](https://oreil.ly/T23SA) to adopt best practices in their products.
  prefs: []
  type: TYPE_NORMAL
- en: Other generative AI development regulatory resources
  prefs: []
  type: TYPE_NORMAL
- en: For example, the *Association for Computing Machinery (ACM)*’s [generative AI
    principles](https://oreil.ly/81nar) include considerations for generative AI models,
    including limits and usage, personal data, correctability, and system ownership
    questions.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, the *Global Partnership on AI (GPAI)*’s 2023 report on [Detection
    Mechanisms for Foundation Models](https://oreil.ly/LZaot) focuses on the detection
    side of AI-generated content, and complements the transparency requirements of
    international regulations and frameworks. There is a similar initiative from the
    [Partnership on AI](https://oreil.ly/WAEgA) for generative AI and responsible
    practices for synthetic media.
  prefs: []
  type: TYPE_NORMAL
- en: This list of regulations, frameworks, and recommendations will continue evolving
    in upcoming years, but all of them converge and include transparency and accountability
    questions that should be considered when developing an Azure OpenAI system. For
    that purpose, the next two sections include organization- and technical-level
    resources that you can apply to your generative AI development.
  prefs: []
  type: TYPE_NORMAL
- en: Company-Level AI Governance Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Microsoft has released a series of resources to guide the responsible implementation
    of AI systems, including generative AI, which can serve as baseline or inspiration
    to adapt generative AI development with Azure OpenAI to responsible approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft’s [Responsible AI Standard (version 2)](https://oreil.ly/fqvjp), which
    includes RAI principles, and a comprehensive document with [requirements to adopt
    those principles](https://oreil.ly/ACiW4). This is the approach used at Microsoft
    to achieve fairness, reliability and safety, privacy and security, inclusiveness,
    transparency, and accountability. These principles are highly related to the regulations
    and frameworks we have previously analyzed, so they represent a good baseline
    for your generative AI implementations for the enterprise. If you want an alternative
    version, also oriented to generative AI, here is a list of [RAI principles from
    LinkedIn](https://oreil.ly/cmghy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [Responsible AI Maturity Model](https://oreil.ly/LV_Ei), a way to analyze
    and evaluate the level of responsible AI maturity at your company. It includes
    5 levels and 24 empirically derived dimensions. This is a good way to make sure
    we are setting the foundations for a generative AI aligned with future regulations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Specific LLM and Azure OpenAI best practices and requirements to guarantee
    RAI approaches, including:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A *four-stage methodology for responsible AI and Azure OpenAI*, adapted from
    the general Microsoft RAI Standard, which [includes measures](https://oreil.ly/uJiId)
    to:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Identify* and prioritize potential harms that could result from your AI system
    through iterative red teaming, stress testing, and analysis.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Measure* the frequency and severity of those harms by establishing clear metrics,
    creating measurement test sets, and completing iterative, systematic testing (both
    manual and automated).'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mitigate* harm by implementing tools and strategies such as prompt engineering
    and content filters. Repeat measurement to test effectiveness after implementing
    mitigations.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Define and execute* a deployment and operational readiness plan.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An [*eight-step approach to responsible AI for LLMs*](https://oreil.ly/l3zSt),
    including Azure OpenAI and other open source options in Azure, such as Meta’s
    LLaMA2\. It focuses on risk mitigation, user-centric design, and additional safety
    measures.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Specific *requirements for adopting Azure OpenAI*, which includes a [code of
    conduct with forbidden use cases](https://oreil.ly/PHCsq), including violence,
    exploitation, harmful content, etc. and a [transparency note with intended use
    cases](https://oreil.ly/nCF82) and adoption considerations.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The [HAX Toolkit](https://oreil.ly/XPSKw), which is a very good resource for
    your user-facing AI solutions, to support the design process and anticipate how
    the AI-enabled system will work and behave.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizational-level measures will help you align with regulations and international
    requirements. However, the actual implementation of countermeasures at the model
    level requires technical RAI tools.
  prefs: []
  type: TYPE_NORMAL
- en: Technical-Level Responsible AI Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are the main tools and features you can use to guarantee that your Azure
    OpenAI implementations are aligned with RAI principles:'
  prefs: []
  type: TYPE_NORMAL
- en: The general RAI [Dashboard](https://oreil.ly/AP5-N) and [Toolbox](https://oreil.ly/Z8187),
    which Microsoft defines as the way to assess, develop, and deploy AI systems in
    a safe, trustworthy, and ethical manner, by using a collection of integrated tools
    and functionalities to help operationalize responsible AI in practice. The [official
    repository](https://oreil.ly/gnM8L) includes tools to evaluate errors, analyze
    fairness, understand data dimensions, interpret models, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Azure AI Content Safety*, which adds an [extra layer of protection](https://oreil.ly/6LWOF)
    to filter out harmful inputs and outputs from the model. This can help prevent
    intentional abuse by your users and mistakes by the model. This safety system
    works by checking both the prompt and completion for your model with a group of
    classification models that aim to detect and stop the output of harmful content
    in four categories (hate, sexual, violence, and self-harm) and four severity levels
    (safe, low, medium, and high). The default setting is to filter content at the
    medium severity level for all four harm categories for both prompts and completions.
    You can access it:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Directly from [AI Content Safety Studio](https://oreil.ly/C0iVp), which allows
    text, image, and multimodal moderation, as well as customization and online activity
    monitorization. It also includes Prompt Shields for your LLM-enabled deployments.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Via [Azure OpenAI Studio’s content filter](https://oreil.ly/9W9J9) for responsible
    AI moderation. Each filter can be applied to Azure OpenAI “deployments,” and those
    deployments will include the content filter for each chat or completion implementation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered the final set of technical considerations for generative
    AI applications with Azure OpenAI Service. You have explored all relevant operational
    questions related to deploying, securing, protecting, and responsibly adopting
    generative AI. Remember, designing and architecting solutions with Azure OpenAI
    is “just” the first step (as we discussed in Chapters [2](ch02.html#designing_cloud_native_architectures_for_generativ),
    [3](ch03.html#implementing_cloud_native_generative_ai_with_azure), and [4](ch04.html#additional_cloud_and_ai_capabilities)).
    The operationalization of these generative AI applications is key for company-level
    implementations, where security, performance, and privacy, as well as regulations
    and AI ethics, are key aspects for sustainable project implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will continue with a key business-related aspect of generative AI:
    elaborating realistic and financially sustainable business cases. This means analyzing
    potential projects and their expected benefit and justifying the human and technical
    cost by discussing ROI (return on investment) scenarios. These aspects are as
    relevant as the technical details we’ve explored thus far to successfully implementing
    generative AI applications in your company.'
  prefs: []
  type: TYPE_NORMAL
