- en: '4 *Data engineering for large language models: Setting up for success*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Common foundation models used in the industry
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to evaluate and compare large language models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different data sources and how to prepare your own
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating your own custom tokenizers and embeddings
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing a Slack dataset to be used in future chapters
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data is like garbage. You’d better know what you are going to do with it before
    you collect it.—Mark Twain
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Creating our own LLM is no different from any ML project in that we will start
    by preparing our assets—and there isn’t a more valuable asset than your data.
    All successful AI and ML initiatives are built on a good data engineering foundation.
    It’s important then that we acquire, clean, prepare, and curate our data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other ML models, you generally won’t be starting from scratch when creating
    an LLM customized for your specific task. Of course, if you do start from scratch,
    you’ll likely only do it once. Then it’s best to tweak and polish that model to
    further refine it for your specific needs. Selecting the right base model can
    make or break your project. Figure 4.1 gives a high-level overview of the different
    pieces and assets you’ll need to prepare before training or finetuning a new model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-1.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 The different elements of training an LLM. Combining earth, fire,
    water—wait, no, not those elements. To get started, you’ll need to collect several
    assets, including a foundation model, training data, text encoders (e.g., tokenizer),
    and evaluation data.
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As was so well defined in the book *Fundamentals of Data Engineering*[¹](#footnote-151):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering is the development, implementation, and maintenance of systems
    and processes that take in raw data and produce high-quality, consistent information
    that supports downstream use cases, such as analysis and machine learning.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the steps you’ll need to take before you can
    start creating your LLM, which largely involves preparing the data assets necessary
    to train a model. We will go over many of the base or foundation models available
    to you as a starting point and how to evaluate and compare them. We will then
    go into depth on many of the different datasets available and how to prepare your
    own for finetuning a model, including preparing your own tokenizer or embeddings.
    Lastly, we will craft a dataset that we will use to finetune a model in the next
    chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Models are the foundation
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will first discuss the most important dataset you will need to collect when
    training, which is the model weights of a pretrained model. A big reason why LLMs
    are so successful as a technology is that we can take a model already trained
    on language as a whole and tweak it to do well on a specific task. Of course,
    knowing how that beginning model was trained and what it was trained on will be
    a huge shortcut in choosing the right one to tweak.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right one has become obnoxiously difficult since LLMs have been
    a hot research topic, resulting in a new one that sports benchmark-breaking records
    popping up almost every week. Because we know (or at least assume) you are eager
    to learn about them, we will first discuss the many different models currently
    out there. These models have already been trained (for better or worse) by professionals
    working to make your life easier and put powerful language models into the public
    arena. There are thousands upon thousands of open source models available on GitHub,
    Hugging Face Hub, and elsewhere, so to simplify, we’ll highlight our favorites,
    giving you details about each of the models to make it easier to compare and to
    give you an idea about whether you should use that particular model or opt for
    one of its lesser-known open source variants. If you are planning to train from
    scratch, consider the architecture involved and if there’s a certain family you’d
    like to try.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 GPT
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There’s probably no better place to start than with GPT (Generative Pre-trained
    Transformer) models. A fan favorite and one of ours too, these models are sold
    commercially through OpenAI and have gained popularity for their impressive performance
    on a wide range of tasks. GPT models are so well known that laypersons often use
    “GPT” to replace “LLM,” just as one might say Kleenex or Band-Aid instead of tissue
    or bandage.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The first GPT model was introduced in 2018, shortly after transformers were
    introduced, and only had 120M parameters. It was trained on the small BookCorpus
    dataset and had impressive results on NLP benchmarks at the time. The GPT-2 model
    came out the next year, increasing its size by 10-fold to 1.5B parameters; it
    was trained on the much larger WebText dataset. The next year, in 2020, GPT-3
    came out 100 times larger with 175B parameters and trained on the massive Common
    Crawl dataset. This model was still based on GPT-1’s original architecture with
    slight modifications for improved scaling.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI has chosen to keep further iterations like GPT-4 under greater secrecy,
    not revealing training data or specific architectures, since it has started to
    productionize and sell them as a product. ChatGPT is a finetuned GPT-3 model trained
    for conversational interaction using reinforcement learning with human feedback
    (RLHF). Not to get into the weeds, but there is a whole host of GPT-3 models you
    can find under API names such as ada, babbage, curie, and davinci, as well as
    other finetuned models such as webGPT and InstructGPT. We leave it to the reader
    to investigate further if they are interested.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Other open source variations like GPT-J were created by the open source community
    utilizing the knowledge gained from the whitepapers OpenAI published. Several
    GPT models have no relation to OpenAI, as Generative Pre-trained Transformer is
    a very generic name that fits most LLMs. Of course, OpenAI has started to see
    it as a brand and is trying to trademark the acronym.[²](#footnote-152)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: GPT-X models, although closed source, can be accessed via the OpenAI API, which
    also includes features for their finetuning. We will be using GPT-2 throughout
    this book—even though it is a bit smaller than what most would consider an actual
    LLM—as it is a well-understood architecture and easy to learn with.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 BLOOM
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: BLOOM is one of the most iconic LLMs because of the learning that has come from
    creating it. The model came out in 2022 and is the first public LLM to rival GPT-3’s
    size with 176B parameters; it was trained with complete transparency. It was put
    together by Hugging Face’s BigScience team, with help from Microsoft’s DeepSpeed
    team and NVIDIA’s Megatron-LM team, and was sponsored by French government grants.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: BLOOM was trained on the BigScienceCorpus dataset, a conglomerate of many smaller
    datasets amounting to 1.6TB of pre-processed text. It is licensed under RAIL,
    which means it isn’t technically open source, since there are restrictions on
    how you can use it, but it can be commercialized.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'TIP  You can learn more about the RAIL license here: [https://mng.bz/mR20](https://mng.bz/mR20).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'BLOOM was trained to be industry size and industry grade for all tasks. Because
    of this, fitting on a consumer device was not a priority, but several smaller
    versions were trained as the research team was coming up to speed. There are 560M-,
    3B-, and 7B-parameter versions. There is also BLOOMZ, a multitask, finetuned version
    of the full 176B parameter model. BLOOM was only trained in 46 different languages,
    and BLOOMZ’s goal was to increase the cross-lingual generalization of the model.[³](#footnote-153)
    You can find all of these models on Hugging Face’s hub: [https://huggingface.co/bigscience/bloom](https://huggingface.co/bigscience/bloom).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: The big downside to BLOOM is that it often gives poor responses and doesn’t
    compete very well in benchmarks—most likely due to limited funds and tight deadlines
    of the project, leading to a feeling that it was undertrained. This isn’t always
    a bad thing and is often better than an overtrained model, but you can expect
    to require a lot more finetuning on a larger dataset if you decide to use it.
    The benefit of using it, though, is that it is well understood and trained in
    the open, and you can check its training data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: In general, the authors wouldn’t recommend using it as a foundation model anymore;
    there are better alternatives, but it’s one you should be familiar with because
    of its contributions. For example, BLOOM’s creation of petals, which allowed distributed
    training, was a significant contribution to the field.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 LLaMA
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLaMA is the result of Meta’s foray into LLMs. The first version was released
    in February 2023 and was released to the research community with a noncommercial
    license. A week later, the weights were leaked on 4chan. In an unlikely turn of
    events, this leak has likely been very beneficial to Meta, as this model has become
    the standard for experimentation and development. Several more models we will
    discuss are based on it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Later, in July 2023, Meta released Llama 2, which has both a research and a
    commercial license. Llama 2 is a big deal since it’s the first commercially available
    model that really packs a punch, and you’ll see many other models based on its
    architecture. There are three different model sizes available: 7B, 13B, and 70B
    parameters. You can download them here: [https://ai.meta.com/llama/](https://ai.meta.com/llama/).
    You’ll need to request access and accept the terms and conditions if you plan
    to use it.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Llama 2 was trained on 2 trillion tokens from a curated dataset taken from the
    internet where they removed websites known to contain personal information and
    upsampled what they considered factual sources. While exact details of the dataset
    haven’t been shared, it likely contained data from Common Crawl, GitHub, Wikipedia,
    Project Gutenberg, ArXiv, and Stack Exchange since those were the primary datasets
    for LLaMA 1\. These datasets were later packaged together and distributed under
    the name RedPajama. Llama 2 was then further finetuned using RLHF, with one model
    finetuned for chat and another for code.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Wizard
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Wizard family of language models comes from the 2023 paper “WizardLM: Empowering
    Large Language Models to Follow Complex Instructions.”[⁴](#footnote-154) These
    models follow the idea that LLMs function better when trained on dense training
    data filled with high-complexity tasks. Based on a proposed framework for creating
    more complex instruction tasks, the WizardLM methodology has been applied to many
    popular datasets and used to finetune almost all of the most popular models. The
    methodology is so popular that, amazingly, it only took the community two days
    after LlamaCoder34B came out to finetune the WizardCoder34B model.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: These models have been consistently praised for their human-like prose and their
    ability to correctly sort through complex problems that rivals many paid services.
    One problem we encourage you to try is to ask WizardCoder34B to write a program
    that draws a realistic-looking tree using any language you’d like. Because the
    Wizard models don’t revolve as much around a specific dataset as they do around
    the methodology of changing an existing dataset to fit the Wizard style, the applications
    are incredibly broad and diverse. If you hit a wall where you aren’t sure how
    to improve when using another model or architecture, try taking the dataset you’ve
    already used and applying the Wizard methodology. You’re welcome.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: As a side note, WizardCoder models tend to get a lot of attention, but the WizardMath
    models are also impressive in their own right. We note that a lot of readers likely
    deal more with data problems than code problems, and the WizardMath models might
    be a great place to start when working with talk-to-your-data applications.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.5 Falcon
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Falcon models are a model family from the Technology Innovation Institute in
    Abu Dhabi. They are the first state-of-the-art models to be released under a truly
    open source license, Apache 2.0\. You can get the model from the institute’s website:
    [https://falconllm.tii.ae/falcon-models.xhtml](https://falconllm.tii.ae/falcon-models.xhtml).
    Its easy access and the open license make this a dream for hackers, practitioners,
    and the industry.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Falcon models first introduced in June 2023 only introduced 7B and 40B parameter
    models, but in September 2023, Falcon released a 180B parameter model that can
    truly compete with GPT-3–sized models. What’s also exciting and probably more
    important to many readers is that Falcon has often led LLM leaderboards in many
    benchmarking tasks. The models were primarily trained on the RefinedWeb dataset,
    which is a smaller but much higher-quality dataset that was carefully and meticulously
    curated and extracted from the Common Crawl dataset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.6 Vicuna
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vicuna was trained on a dataset of user-shared conversations from ShareGPT.
    The logic is that a model trained off of the best outputs of ChatGPT will be able
    to emulate the performance of ChatGPT, piggy-backing off of the Llama–Alpaca trend.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  We won’t talk about Alpaca here, but we introduced it in chapter 3 when
    discussing knowledge distillation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Vicuna has been praised for both its performance and its relatively low training
    costs. Vicuna is an amazing example of why data coverage and quality matter so
    much while simultaneously demonstrating the dangers of model collapse from training
    on the output of another model. Model collapse happens when an ML model is trained
    on synthetic data, leading to increasingly less diverse outputs. For example,
    Vicuna performs admirably on anything that is at least close to what appeared
    in the dataset, but when asked to perform more generative or agent-like tasks,
    it tends to hallucinate far beyond what its predecessors do. Vicuna is not licensed
    for commercial use, but it is amazing for personal projects.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.7 Dolly
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Created by Databricks as more of a thought experiment than a competitive model,
    Dolly and its V2 do not perform well compared to other models of the same size.
    However, Dolly boasts one of the best underlying understandings of English and
    is a fantastic starting point for finetuning or creating low-ranking adaptations
    (LoRAs; which we will discuss in chapter 5) to influence other models. Dolly 1.0
    was trained on the Stanford Alpaca Dataset, while Dolly 2.0 was trained on a high-quality
    human-generated instruction-following dataset that was crowdsourced by the Databricks
    employees. Dolly 2.0 has been open sourced in its entirety, including the training
    code, dataset, and model weights, all with a commercial use license.[⁵](#footnote-155)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.8 OpenChat
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenChat is similar to Vicuna in that OpenChat used 80K ShareGPT conversations
    for training, but dissimilar in that their conditioning and weighted loss strategies
    end up creating a model that is undeniably great in its ability to generate human-like
    and, more importantly, human-preferred responses.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: OpenChat models—not to be confused with the open source chatbot console—are
    a collection of various finetunings for different tasks, with some meant for coding,
    others for agents, and others for chatting. Free for commercial use under the
    Llama 2 Community License, these models could be a great solution to build off
    of at your corporation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: We’ve discussed a lot of models already, and while we could go on like this
    for the rest of the chapter, it’s in everyone’s best interest that we don’t. Table
    4.1 shows a summary highlighting some of the major points of comparison for the
    models we discussed. One major point we’d like to highlight is that a lot of models
    are available for commercial use! While many of the licenses come with restrictions,
    they likely aren’t rules you plan to break anyway.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.1 Comparison of LLM model families
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Model family | Dataset | Largest model size | Commercial license | Organization
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| GPT  | Common Crawl/RLHF  | 1.76T  | No  | OpenAI  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| BLOOM  | BigScienceCorpus  | 176B  | Yes  | BigSciense  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| Llama  | RedPajama  | 70B  | Yes  | Meta  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| Wizard  | Evol-Instruct  | 70B  | No  | Microsoft  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: '| Falcon  | RefinedWeb  | 180B  | Yes  | TII  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: Now that you have an understanding of some of the more popular model families,
    you might have an idea of which model to pick to start for your project. But how
    can you be sure? In the next section, we’ll look at different ways you can evaluate
    and compare models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Evaluating LLMs
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While we have just discussed some of our favorite model families, there are
    so many more and varying models available out there, with many more coming out
    every month, all claiming to be the best. It is impossible to keep them all straight.
    So how do you pick the best one to use? Can it perform well on your task out of
    the box, or will it require finetuning? How do you know if your finetuning improved
    the model or just made it worse? How do you know if you picked the right size?
    A smaller model is convenient, but larger models perform better on many tasks.
    To be honest, these are not easy questions to answer, but thankfully, there are
    a few industry standards we can rely on.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们刚刚讨论了一些我们最喜欢的模型家族，但还有许多其他不同的模型可供选择，每个月都有更多的新模型出现，所有这些模型都声称自己是最好的。不可能将它们全部理清楚。那么，你如何选择最好的一个来使用？它是否能够直接在你的任务上表现良好，或者是否需要微调？你如何知道你的微调是否提高了模型，或者只是让它变得更糟？你如何知道你选择了正确的大小？较小的模型很方便，但较大的模型在许多任务上的表现更好。说实话，这些问题并不容易回答，但幸运的是，有一些行业标准我们可以依赖。
- en: 'When evaluating a model, you will need two things: a metric and a dataset.
    A metric is an algorithm that allows us to compare results to a ground truth.
    A dataset is a list of tasks we want our model to run, which we will then compare
    using our metrics of choice.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当评估一个模型时，你需要两样东西：一个指标和一个数据集。指标是一个算法，它允许我们将结果与真实情况进行比较。数据集是我们希望模型执行的任务列表，然后我们将使用我们选择的指标进行比较。
- en: In this section, we will discuss many different methodologies employed to evaluate
    LLMs so we can evaluate and compare them objectively. We will discuss everything
    from common industry benchmarks to methodologies used to develop your own unique
    evaluations. Let’s get started.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论许多用于评估大型语言模型的不同方法，以便我们可以客观地评估和比较它们。我们将从常见的行业基准到开发你自己的独特评估方法的一切内容进行讨论。让我们开始吧。
- en: 4.2.1 Metrics for evaluating text
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 评估文本的指标
- en: Evaluating text is often difficult because it’s easy to say the exact same thing
    in two different ways. Semantically, two sentences can be exactly the same, but
    syntactically, they are nothing alike, making text comparison tricky. See what
    I did there?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 评估文本通常很困难，因为用两种不同的方式说出相同的话很容易。从语义上讲，两个句子可能是完全相同的，但从句法上讲，它们却毫无相似之处，这使得文本比较变得复杂。看看我做了什么？
- en: To evaluate our models, we will need better metrics than just an exact match
    or check for equality, which we can get away with for most other ML problems.
    We need a metric that allows us to compare the generated text from our models
    against a ground truth without being too rigid. Let’s look at some of the most
    common metrics used.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的模型，我们需要比仅仅匹配或检查相等性更好的指标，这对于大多数其他机器学习问题来说是可以做到的。我们需要一个指标，它允许我们比较模型生成的文本与真实情况，而不会过于严格。让我们看看一些最常用的指标。
- en: ROUGE
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ROUGE
- en: 'ROUGE, short for Recall-Oriented Understudy for Gisting Evaluation, is one
    of the oldest metrics used for evaluating machine translation tasks, but still
    one of the most reliable. It was developed specifically for automatic summarization
    tasks where the goal is to take a long article and sum it up in a short brief.
    Let’s consider the problem: How do you determine whether a summary is correct?
    The simplest method would be to compare it to a known summary—a ground truth,
    if you will. However, no matter the article, there’s often thousands of ways you
    could choose to simplify the text to be more concise, and you don’t want to penalize
    a model simply because it chose a different word order than the ground truth;
    this would only lead to overfitting.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE，即“面向检索的摘要评估的辅助研究”，是用于评估机器翻译任务的最古老的指标之一，但仍然是最可靠的。它是专门为自动摘要任务开发的，其目标是把一长篇文章总结成简短的摘要。让我们考虑一下这个问题：你如何确定一个摘要是否正确？最简单的方法就是将其与一个已知的摘要——即“真实情况”——进行比较。然而，无论文章如何，通常都有成千上万种方法可以用来简化文本，使其更加简洁，你不想仅仅因为模型选择了与真实情况不同的词序就惩罚它；这只会导致过度拟合。
- en: Rouge doesn’t compare the generated summary to the ground truth summary expecting
    an exact match; instead, it looks for overlaps between the two summaries using
    N-grams—the greater the overlap, the higher the score. This is similar to how
    a full-text search engine works. There are multiple variations depending on what
    N is for the N-gram, but there is also a version that compares longest common
    subsequences and versions that compare skip-bigrams, which are any pair of words
    in their sentence order and not necessarily right next to each other.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: The original implementation of ROUGE was written in Perl, and we remember having
    to use it even a couple of years ago. Easily some of the worst days of one author’s
    career were having to work in Perl. Thankfully, it seems that in the last year
    or so, there have finally been fast, stable reimplementations in Python. In the
    next listing, we use the rouge-score library, which is a reimplementation from
    Google. We’ll compare two explanations of *The Legend of Zelda* and see how well
    they compare.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.1 Using ROUGE
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Example N-gram where N=1 and also using the longest common subsequence'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the example, even though these two texts are quite different
    syntactically, they are both accurate descriptions. Because of this, instead of
    giving a big fat zero for the score, ROUGE gives a little more flexibility and
    a better comparison with similarity scores around 0.25\. The ROUGE algorithm is
    a fast and effective way to quickly compare the similarity between two short bodies
    of text. ROUGE is very common in the industry, and many benchmarks use it as one
    of their metrics.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: BLEU
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: BLEU, which stands for BiLingual Evaluation Understudy, is the oldest evaluation
    metric we will talk about in this book. It was developed to evaluate machine translation
    tasks and compare methods of translating one language to another. It is very similar
    to ROUGE, where we compare N-grams between a target and a prediction. While ROUGE
    is primarily a recall metric, BLEU is a precision metric, but using standard precision
    can lead to some problems we need to account for.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: To understand the problem, we can calculate standard precision with the code
    from listing 4.1\. Replace the target variable with “the cat in the hat” and the
    prediction variable with “cat hat.” Rerun the listing, and you’ll notice the recall
    is 0.4—we got two out of five words correct—but the precision is 1.0, a perfect
    score despite not being very good! This result is because both words “cat” and
    “hat” show up in the target.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'BLEU fixes this by adding two adjustments. The first is straightforward: add
    a brevity penalty. If the prediction is shorter than the target, we’ll penalize
    it. The second adjustment, known as the modified N-gram precision, is a bit more
    complicated, but it allows us to compare a prediction against multiple targets.
    The next listing shows how to use the NLTK library to calculate the BLEU score.
    We are using the same *Zelda* example as we did with ROUGE so you can compare
    results.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.2 Using BLEU
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: BLEU has long been an industry standard, as it has been reported several times
    to correlate well with human judgment on translation tasks. In our example, we
    split the sentences, but it would be better to tokenize the sentences instead.
    Of course, you can’t compare BLEU scores that use different tokenizers. On that
    note, SacreBLEU is a variant worth looking at, as it attempts to improve the comparability
    of scores despite different tokenizers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: BPC
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The bits per character (BPC) evaluation is an example of an entropy-based evaluation
    for language models. These are metrics we try to minimize. We will not dive deeply
    into entropy or perplexity, but we’ll go over an intuitive understanding here.
    Entropy is an attempt to measure information by calculating the average amount
    of binary digits required per character in a language. Entropy is the average
    number of BPC.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Perplexity can be broken down into attempting to measure how often a language
    model draws particular sequences from its corpus or vocabulary. This draws directly
    from the model’s tokenization strategy (too many `<UNKS>` equals bad perplexity),
    meaning that a 1:1 comparison between LLMs with different tokenization strategies
    using perplexity—or entropy, for that matter—is impossible. For example, a model
    that tokenizes at the character level will have much lower perplexity than a model
    that tokenizes at the word level but often performs worse overall. That doesn’t
    invalidate either as a metric, as they are very helpful metrics during training
    of the same model.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Entropy-related metrics are highly related to information theory, which
    we don’t cover. However, we recommend you take a look at these metrics if you’re
    interested in creating or improving evaluation metrics for LLMs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: To drive the point further with a hands-on example, comparing two models that
    use different tokenization strategies is like comparing how good one third-grader
    is at addition with another third-grader’s multiplication ability. Saying one
    is better than the other doesn’t really matter because they’re doing different
    things at the same skill level. The closest you could get to an accurate comparison
    would be having the two third-graders do the same task, say spelling. Then you
    could at least compare apples to apples, as much as possible.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have some metrics under our belt, let’s look into benchmark datasets
    that we will run our evaluations on.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Industry benchmarks
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Evaluating language models’ performance is a notoriously difficult problem,
    and many benchmarks have been created to tackle it. In this subsection, we’ll
    discuss several of the most common solutions you are likely to run into and what
    type of problem they are trying to solve. Since benchmarks typically are only
    good at evaluating one quality of a model and LLMs are usually deployed to do
    many general tasks, you will likely need to run several evaluation benchmarks
    to get a full picture of the strengths and weaknesses of your model. As we go
    through this list, don’t think about which metric is better than another, but
    about how they can be used in tandem to improve your overall success.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: GLUE
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The General Language Understanding Evaluation (GLUE) is essentially a standardized
    test (think ACT, SAT, GRE, etc.) for language models (just “language models” this
    time) to measure performance versus humans and each other on language tasks meant
    to test understanding. When introduced, two problems arose pretty quickly: the
    LMs surpassed human parity on the tasks too fast, and there were doubts about
    whether the tasks demonstrated actual understanding. Similar to when people train
    animals like parrots to speak, the question is always there: Is the parrot actually
    acquiring human language or simply being conditioned to mimic certain sound sequences
    in response to specific stimuli in exchange for food? That said, the GLUE benchmark
    is still valuable for comparing model performance.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: GLUE is no longer an industry standard, but it can still give you a fairly quick
    idea of how well your model is performing, especially if you are training on an
    instruction-based dataset and using GLUE to measure few or zero-shot performance
    on new tasks. You can view the leaderboard at [https://gluebenchmark.com/leaderboard](https://gluebenchmark.com/leaderboard).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: SuperGLUE
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As stated in the previous section, one problem that came up quickly was human
    parity on the GLUE tasks. To solve this problem, one year after GLUE was developed,
    SuperGLUE was created and contains more difficult and diverse tasks styled in
    the same easy-to-use way as GLUE. Beyond that, because the GLUE nonexpert human
    benchmark was being surpassed so quickly, more expert people were used to generate
    the SuperGLUE benchmark. That said, the SuperGLUE human baselines are in eighth
    place on the leaderboard at the time of this writing, calling into question the
    second problem with GLUE: Do the SuperGLUE tasks adequately measure understanding?'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering that models like PaLM 540B, which are beating the human baseline,
    struggle to generate output generally considered acceptable to people, another
    question arises: How much of the training data and evaluation metrics are idealized
    and nonreflective of how we actually use language? There aren’t yet any adequate
    answers to these questions, but they’re helpful to consider when your evaluation
    metrics could be what stands between your model and acceptable performance on
    its task.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: In listing 4.3, we show how to run a model against the MultiRC SuperGLUE test.
    The MultiRC dataset contains short paragraphs and asks comprehension questions
    about the content of the paragraph. Let’s go ahead and load the dataset and take
    a quick look at what we are dealing with.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.3 Example SuperGLUE Benchmark
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 SuperGlue has multiple test datasets; options are boolq, cb, copa, multirc,
    record, rte, wic, wsc, wsc.fixed, axb, and axg.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we see a paragraph discussing some basic physics around forces along with
    a simple yes-or-no question and its answer:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s go ahead and pull down a small model and run it against the dataset.
    For this example, we’ll print out the model’s generated answer to the correct
    answer to compare qualitatively:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Replace this with the correct input for your benchmark.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '#2 We use this to trim out the input.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'From this, you might get results similar to the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can see our model isn’t doing all that great, but we aren’t too concerned;
    we just want to show a SuperGLUE test in action. You may be wondering why we aren’t
    using a metric like ROUGE or BLEU. While we could do so to improve our understanding,
    if you decide to submit results to the SuperGLUE leaderboard, it will want the
    raw generated text.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE  For more information on how to use SuperGLUE, check out SuperGLUE FAQs:
    [https://super.gluebenchmark.com/faq](https://super.gluebenchmark.com/faq).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'SuperGLUE does exactly what it sets out to do: be GLUE but super. If you want
    to test your model’s few and zero-shot capabilities, SuperGLUE would be one of
    the ultimate tests. It will show whether your LLM can follow instructions with
    very low perplexity, only generating what is needed and not more. You can look
    at the current SuperGLUE leaderboard at [https://super.gluebenchmark.com/leaderboard](https://super.gluebenchmark.com/leaderboard).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: MMLU
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Massive Multitask Language Understanding (MMLU) test was developed primarily
    by UC Berkeley in cooperation with several other universities to test deeper knowledge
    than the GLUE tasks. No longer concerned with surface-level language understanding,
    MMLU seeks to test whether a model understands language well enough to answer
    second-tier questions about subjects such as history, mathematics, morality, and
    law. For example, instead of asking, “What did Newton write about gravity?”, ask,
    “What arguments would Newton have gotten into with Einstein?”
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: MMLU’s questions range in difficulty from an elementary level to an advanced
    professional level, and they test both world knowledge and problem-solving ability.
    They are known to be quite difficult, with unspecialized humans from Mechanical
    Turk only obtaining results slightly better than random with 34.5% accuracy.[⁶](#footnote-156)
    Experts in their field performed much better, but generally only for the portion
    of the test that was their specialty. So when we look at the models’ performance
    on the test, as might be expected, the models, even at the top of SuperGLUE’s
    leaderboard, are barely better than random at applying the language understanding
    to answer questions about it. This test encompasses a much wider range of understanding
    tasks than GLUE and takes a much lower perplexity to pass.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 shows how to run this test. We’ll download the MMLU dataset and
    then, for convenience, run the test against OpenAI’s different models for comparison.
    The code also allows for different levels of few-shot prompting. We haven’t discussed
    this, but we wanted to show an example early. Try adjusting this parameter to
    see how different numbers of examples can improve your overall results.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 Example MMLU evaluation
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Sets up the model'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Δefines benchmark with specific tasks and shots'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Runs benchmark'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: MMLU gets at a deeper understanding than any of the previous benchmarks, which
    is promising, and a correlation can be drawn between this test and chat models
    that generally produce human-preferred responses. With deeper understanding, though,
    comes the need for more responsibility in the testing, and ethical concerns are
    beginning to be raised about these evaluations. For example, are the models being
    trained to answer questions about US history truthfully, or are they being evaluated
    on propaganda for an ideal nation? When answering questions about the law, are
    they conditioned to accept any bias the law system may or may not contain? The
    current answer is that models are likely demonstrating a deeper bias when performing
    well on these types of evals, and greater care needs to be taken to ensure that
    the bias presupposed in our evaluations is unharmful and generally accepted.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Responsible AI benchmarks
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pretty much all industry benchmarks that evaluate LLMs have focused strictly
    on the model’s capability: “Is it able to do the job?” Unfortunately, that’s where
    most industry benchmarks end. It’s quite unfortunate, especially since going a
    step further and evaluating for bias isn’t that different from or more difficult
    than other evaluation techniques. It’s your responsibility, and it makes good
    business sense. Knowing where your model’s blind spots are can help you create
    plans to resolve them, which will help you sell to larger markets and avoid costly
    trouble in the future.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The most common approach to evaluating bias is to take what we already know
    and segment the data across diverse groups. Thankfully, since moderating behavior
    for chat groups and social media sites is valuable to businesses, we already have
    many good evaluation metrics for traits like toxicity, polarity, hurtfulness,
    and similar. These metrics aim to measure how much a given text is related to
    violence, hate speech, racism, or profanity and how positive or negative they
    may be or just generally in violation of your company’s terms and conditions.
    Listing 4.5 demonstrates how to use the HONEST evaluation metric, which compares
    how hurtful prompt completions are for different genders. For example, when comparing
    potential attributes, generating the adjective “smart” would be less hurtful than
    “dumb.”
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.5 The HONEST metric
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Once you pick one or more evaluators, you’ll need a dataset of prompts. You
    can find several in Hugging Face’s dataset. WinoBias focuses on gender bias. It
    contains prompts like, “The mechanic greets the receptionist because he” along
    with the opposite gender version, “The mechanic greets the receptionist because
    she.” We would run these prompts through our model and compare the groups. Some
    other datasets worth checking out include CALM, which compares gender and race,
    and WinoQueer, which looks at bias for LGBTQ+ groups. There are many more to check
    out, including datasets for politics, religions, and occupations, to name a few.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE  You can learn more about CALM at [https://arxiv.org/abs/2308.12539v1](https://arxiv.org/abs/2308.12539v1),
    and WinoQueer here: [https://arxiv.org/abs/2306.15087](https://arxiv.org/abs/2306.15087).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: To put this all together, in listing 4.6, we’ll create an evaluation pipeline
    utilizing the Regard metric. The Regard metric looks at the polarity of content—whether
    it is a positive or negative statement. We’ll run this across the WinoBias dataset,
    segmenting the data by gender. Once we’ve run the analysis for each group, we
    can compare the results across the segments and see whether the distributions
    differ. Before reading on, take a guess. Do you think we’ll see more positive
    results for men or women, or will they be the same? What about negative results?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.6 Running an evaluation pipeline on Regard
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Pulls model, data, and metrics'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Prepares dataset'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Runs through the evaluation pipeline'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Analyzes results'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Prints the mean polarity scores'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly to many, this example shows that gender polarity is rather comparable
    in our model. A good sign for this model! The bigger takeaway is that you should
    be automating your evaluations and running pipelines across many metrics, including
    looking for bias, not just performance. Overall, there are still many opportunities
    to improve evaluations and metrics in this space, especially when creating datasets
    and finetuning models to reduce bias. We expect to see lots of growth and innovation
    in this area of research.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 Developing your own benchmark
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Overall, developing good benchmark datasets is still an unsolved problem. This
    is partly because once we develop one, our models quickly surpass it, making it
    obsolete and no longer “good.” There will be times when we discover edge cases
    for our model, such as parts of speech or certain tasks where it seems to struggle—maybe
    that’s playing chess or identifying sarcasm. Spoiler alert: LLMs are still terrible
    at these tasks, and if you haven’t seen a GPT versus Stockfish video yet, you’re
    in for a treat. In these cases, where we are trying to perform a specialized task,
    a simple evaluation would be to compare a custom list of prompts with expected
    responses.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: We recommend first checking out OpenAI’s Evals library ([https://github.com/openai/evals](https://github.com/openai/evals)),
    where OpenAI has open sourced its evaluations. The library acts both as an evaluation
    framework and as a registry for edge-case datasets. At the time of this writing,
    the library contains almost 400 different datasets and is a great place to get
    started and contribute. This library gives you access to the same evaluation standards
    that OpenAI uses for their state-of-the-art models, and they’ve already done most
    of the heavy lifting in identifying areas of interest and curating datasets for
    these areas.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: As with most libraries built for a specific company but subsequently open sourced,
    it can be a bit of a pain to generalize. Running these evaluations against OpenAI’s
    models is easy-peasy, but extending it to run against your own models is anything
    but. While this is an annoyance that will likely go away if the community fully
    embraces and adopts the framework, the real downside to using this library is,
    ironically, that it’s open sourced. Being both a framework and registry (the data
    is stored alongside the code in the GitHub repo), if you are looking to curate
    a new evaluation dataset, but the dataset is private or can’t be open sourced
    for whatever reason, you are left with forking the repo and all the pain of managing
    it as your fork goes out of date.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Another library to pay attention to is Hugging Face’s Evaluate. The Evaluate
    library is also a framework for building evaluation methods; however, the datasets
    are separate and can be found on the Hugging Face Hub in their own spaces. Since
    spaces can be private or public, it’s a much more user-friendly experience. Hugging
    Face has custom metrics and all the standard benchmarks already discussed in this
    chapter, as well as several not discussed. In listing 4.7, we show how to use
    the Evaluate library to get SQuAD metrics. SQuAD stands for the Stanford Question
    Answering Dataset, which is an older dataset with 100K questions and answers.
    SQuAD is a reading comprehension dataset consisting of questions generated from
    a set of Wikipedia articles, where the answer to every question is a segment of
    text inside the reading passage. The SQuAD metrics are a set of custom metrics
    that consist of an exact match; F1 scores were used in the paper introducing the
    dataset.[⁷](#footnote-157)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.7 Using the Evaluate library to run SQuAD
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Δownloads a metric from Hugging Face''s Hub'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Example from the SQuAΔ dataset'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: If you are creating your own benchmark, with the Evaluate library, you can easily
    create your own metric in a metric space and the dataset to use with the metric.
    This process isn’t too difficult. If you’ve decided not to create your own, the
    hardest part is finding good metrics. Searching through the hub is one thing,
    but since anyone can upload a metric and dataset, you never know if what you find
    is all that good, well curated, or clean.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: We haven’t dug too deeply into actually generating a dataset or metric, as that
    will be very specific to your use case, but what we have discussed are two great
    libraries you can use to do it. Evals is great if you are looking for an already
    curated dataset, and Evaluate is easy to use when generating your own. These tools
    are very useful, but in some special cases, you’ll need to think outside the box,
    and one of those cases that sticks out like a sore thumb is code generation.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.5 Evaluating code generators
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most valuable and sought-after use cases for LLMs is to have them
    help us write code. While we are unaware of any industry standard evaluation metrics
    for evaluating the generated code, thankfully, there are plenty of industry standards
    for evaluating the code itself (e.g., tests, profiles, security scanners, etc.).
    Using these tools provides a powerful path to evaluating the LLM through the code
    it generates.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic setup looks like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Have your model generate code based on docstrings.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the generated code in a safe environment on prebuilt tests to ensure they
    work and that no errors are thrown.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the generated code through a profiler and record the time it takes to complete.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the generated code through a security scanner and count the number of vulnerabilities.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the code against architectural fitness functions to determine artifacts,
    like how much coupling, integrations, and internal dependencies there are.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run steps 1 to 5 on another LLM.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare results.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Listing 4.8 demonstrates an example using everyone’s favorite LeetCode problem,
    the Fibonacci sequence, as our prompt. This example shows using a separate fibonacci.py
    file as a prompt for our LLM to generate code. We could then use this test file
    to check that it runs correctly and how fast it runs.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.8 An example test for evaluating code generators
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Runs tests using pytest and times it'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: There is lots of flexibility to this system, but the major downside is that
    it requires you to either create docstrings of coding challenges and write tests
    for them ahead of time or scrape LeetCode. Of course, you could have your LLM
    generate both of those too, but it’s easy to write simple tests that always pass
    and much harder to write tests that cover all the edge cases. So at some point,
    you’ll want a human in the loop.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.6 Evaluating model parameters
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, all the evaluation methods we’ve looked at involve running the model
    and checking the results, but there is a lot we can learn by simply looking at
    the model. Surprisingly, there’s a lot you can learn by simply looking at the
    parameters of an ML model. For example, an untrained model will have a completely
    random distribution. By evaluating the distribution and paying attention to distinct
    features of a model’s parameters, we can learn whether a model is over- or undertrained.
    In the next listing, we use the weightwatcher library to do just that on the GPT-2
    model, which will tell us which layers are over- or undertrained.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.9 Using the weightwatcher library to evaluate GPT-2
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This code prints out the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Along with summary statistics, weightwatcher provides spectral analysis plots,
    as shown in figure 4.2\. To create these plots, change line 8 in listing 4.9 to
    `plot=True`. The spectral analysis plots evaluate the frequencies of eigenvalues
    for each layer of a model. When evaluating these plots, we care about the tail
    of the distribution—the straighter it is (indicating a nice heavy tail), the better
    trained we expect the layer to be.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-2.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 weightwatcher Empirical Spectral Density (ESD) plots generated for
    GPT2’s second layer, which is predicted to be overtrained
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'NOTE  These plots are created to mimic Spectral Density plots you might see
    in a physics lab. We will not discuss them in this book, but if interested, we
    recommend you check out the WeightWatchers documentation: [https://github.com/CalculatedContent/WeightWatcher](https://github.com/CalculatedContent/WeightWatcher).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: weightwatcher is rather powerful, as it allows us to compare different models,
    helping us better understand which model is better trained without running them
    at all, making it relatively inexpensive. This capability comes in handy when
    you are trying to determine which base model to use, as an undertrained model
    may require a lot more finetuning.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Since we are comparing models based on their parameters alone, this method provides
    a nice agnostic view of the current state of a model. We can implement it during
    and after training and during ongoing updates using methods such as RLHF. It is
    both an easy and powerful evaluation method. However, the downside is that it
    doesn’t provide any insight into the training data, so it can’t tell us which
    model is that effective at which task and is best paired with other evaluation
    methods already discussed.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve already spent quite a bit of time talking about data most data engineers
    likely don’t think about often: model weights and evaluation data. These are crucial
    ingredients to gather to generate a specialized finetuned LLM. Indeed, LLMs introduce
    new data engineering challenges, just like they introduce new MLOps and data science
    challenges. Next, we will discuss what many of you have been waiting for: the
    training data. We’ll discuss different datasets that are essential to know about,
    where to get them, and how to prepare them to train or finetune LLMs.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Data for LLMs
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It has been shown that data is the most important part of training an LLM. We
    hope that the sudden importance of language modeling will persuade businesses
    to start managing their data generally according to accepted guidelines. As is
    shown by experiments like LLaMA, Alpaca, Goat, Vicuna, and later, LIMA[⁸](#footnote-158)
    and SpQR,[⁹](#footnote-159) high-quality training data and clever modeling are
    much more important than the number of parameters or size of training data. Measuring
    that quality is still a point of difficulty in general; however, we’ll discuss
    methodologies you can employ to do so.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: We’ll first discuss common datasets you should know about, what’s in them, why
    you would want them, and where you can get them. Then we’ll talk about common
    processing and preparation techniques you’ll need to understand to get the most
    out of them and get better results from your LLMs.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Datasets you should know
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you didn’t notice, in section 4.1, we made it a point to discuss which datasets
    different models were trained on. It might have come across as just another factoid
    about the model, but this is highly valuable information! Knowing what a model
    was trained on (or not trained on) is the first step to understanding what it
    can or cannot do. For example, knowing an LLM coding model was trained heavily
    on the C programming language but didn’t see a lick of C++ will be more than enough
    to realize why it seems to work syntactically but produces so many errors and
    bugs when writing C++ code.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Wikitext
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the most familiar datasets, Wikitext is, as the name implies, essentially
    Wikipedia. It was crafted by the Salesforce team back in 2016\. It is a great
    dataset to turn to when you’re only trying to do a proof of concept or a rapid
    prototype since the English version comes in at only 741 MB, not even 1 GB. Add
    to that the fact that Wikipedia is a trusted source of information—especially
    compared to the internet at large, where most of the other sources come from—and
    this gets even better!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Some downsides: it is purely an English dataset, which greatly reduces the
    diversity of tokens the model will see; Wikipedia contains an idealized version
    of language—one that we subjectively value as clear—even though it doesn’t contain
    any instances of how language is actually used, only meta-explanations on usage.
    Also, it’s almost a decade old as of this writing, which, of course, no one checks.
    We’ve seen many teams use it to quickly prototype and create Q&A bots due to its
    ease of use and access. It does well in prototyping but always comes off as unimpressive
    when it gets to production, as users tend to prefer asking questions about current
    events. Always check the freshness of your data! Overall, it’s a valuable dataset
    information-wise, but bad if you want your models to interact in a human-like
    way.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Wiki-40B
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A good alternative is Wiki-40B from 2020, a cleaned-up version of Wikitext
    with 40 different language variations. It comes in at a little over 10 GB. So
    it’s still quite small for prototyping. It comes with all the same benefits Wikitext
    does: it’s a clean dataset and a trusted source of information. Plus, it’s newer
    and has more languages. This is a great dataset to use to become familiar with
    multilingual modeling.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Europarl
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the best toy datasets for multilingual problems, Europarl contains the
    European Parliament proceedings from 1996 to 2011\. It includes translations in
    21 different European languages and is great for smaller projects and multilingual
    demos. Europarl is an excellent source of data, albeit idealized and outdated,
    much like English Wikitext. In addition, the project includes many parallel corpora,
    which are paired down to English and one of the 20 other languages. The total
    dataset is just 1.5 GB and can be found at [https://www.statmt.org/europarl/](https://www.statmt.org/europarl/).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Common Crawl
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Common Crawl dataset is essentially the entire internet, web scraped and
    open sourced. It uses web crawlers similar to what Google or Microsoft use to
    enable search engines. C4, the Colossal Cleaned version of the Common Crawl dataset,
    is the most common dataset for self-supervised pretraining. Unfortunately, being
    cleaned doesn’t mean it is free of inherent societal bias, which is true for pretty
    much all the datasets openly available today. Containing the entirety of the internet
    means it contains all the good and the bad; it is a very diverse dataset full
    of multiple languages and code.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'The Common Crawl dataset is named after the nonprofit organization of the same
    name that is dedicated to providing a copy of the internet to anyone for the purpose
    of research and analysis. You can access the dataset at [https://commoncrawl.org/](https://commoncrawl.org/),
    where you will find many versions because Common Crawl periodically crawls the
    web and updates the dataset. The community has been archiving the internet since
    2008\. It comes in four variants to help with your various needs: a 305 GB version
    containing the actual C4; a 380 GB version that contains so-called bad words along
    with everything else; a 2.3 TB version, which is the uncleaned version (not recommended);
    and a 15 GB version of data that is professional enough to appear on the news.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: OpenWebText
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another dataset we’d recommend for pretraining is OpenWebText, which only takes
    up 55 GB on disk. It is an open source effort to reproduce OpenAI’s WebText dataset
    used to train GPT-2\. Instead of being a copy of the entire internet, researchers
    used Reddit to extract URLs from posts and then filtered the list using Reddit’s
    karma ranking system. They then scraped the URLs to create the dataset. Since
    the content mainly comes from Reddit, it calls into question its real-world accuracy
    due to the selection bias of only including people with a Reddit account. It is
    made up mostly of news articles, blog posts, and other content often shared on
    forums. You can think of it as a highly curated and much smaller version of the
    Common Crawl dataset.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Like Wikitext, it’s a bit older; the most commonly used version was created
    in 2019, and a new version hasn’t been updated in four years at the time of writing.
    Of course, since the dataset was curated with a specific methodology, it could
    be refreshed at any time.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: The Pile
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One dataset that has garnered a lot of attention and should be on your radar
    is The Pile, which was created by EleutherAI in 2020 and published on December
    31 of the same year.[^(10)](#footnote-160) It is useful for self-supervised pretraining
    tasks. The Pile is one of the largest datasets we’ll discuss at 825 GB and consists
    of 22 smaller high-quality datasets combined to make a diverse and dense training
    set. It includes most of the datasets we have already discussed, like Common Crawl,
    OpenWebText, and Wikipedia. It also contains book datasets, like Books3 and Gutenberg;
    code datasets, like GitHub and Stack Exchange; and specialist datasets, like PubMed
    and FreeLaw. It also includes datasets like the Enron Emails, which we can’t help
    but think was a mistake.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Because it’s so massive and includes multiple languages and code samples, it
    has proven useful in training many LLMs. It is multilingual in addition to dense,
    making it ideal for learning sparse general language representations. Overall,
    though, it’s not very clean and is essentially just a conglomerate of multiple
    datasets. Unless you are training LLMs from scratch, you likely won’t use this
    dataset, but it’s important to become familiar with it, as many of the largest
    models have been trained on it. You can find the dataset at EleutherAI’s website:
    [https://pile.eleuther.ai/](https://pile.eleuther.ai/).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: RedPajama
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: RedPajama is a dataset created by a collaboration of Together.ai, Ontocord.ai,
    ETH DS3Lab, Stanford CRFM, and Hazy Research. The goal was to create a fully open
    dataset that mimicked what was described in the LLaMA paper.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE  You can read the blog post introducing RedPajama here: [https://together.ai/blog/redpajama](https://together.ai/blog/redpajama).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is similar to The Pile but much larger at 5 TB and newer, published
    in April 2023\. It contains fewer datasets: GitHub, arXiv, Books, Wikipedia, StackExchange,
    and Common Crawl. It is so large because it contains five different dumps of the
    Common Crawl dataset with varying filters and the standard C4 dataset. It is made
    available through the Hugging Face Hub and can be found at [https://mng.bz/4ppD](https://mng.bz/4ppD).'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: OSCAR
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The best dataset by far to train on for multilingual models is OSCAR, which
    is larger than any other dataset discussed, coming in at 9.4TB, over 11 times
    as big as The Pile! It is an open source project started in 2019 and has been
    funded by a multitude of institutes and governments. You can learn more about
    the project and dataset at [https://oscar-project.org/](https://oscar-project.org/).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: This project is actively being worked on, and new releases come out annually
    with regular updates. It currently supports 166 languages at the time of this
    writing, much more than any other dataset. As a work in progress, though, there
    are some languages much more represented than others, with some in the TBs of
    data and others in KBs. This is one of our favorite datasets because it is actively
    being worked on, and the team is passionate about representation in LLMs and AI,
    as well as producing highly clean, high-quality data. We encourage all interested
    readers to contribute to this dataset.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Summary of datasets
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In table 4.2, you can see a summary of the datasets we’ve discussed so far.
    These datasets are all commonly used in industry and worth familiarizing yourself
    with. We encourage you to investigate them further and take a closer look at the
    data within.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.2 Summary of datasets
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Dataset | Contents | Size | Last update |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| Wikitext  | English Wikipedia  | <1 GB  | 2016  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| Wiki-40B  | Multi-lingual Wikipedia  | 10 GB  | 2020  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| Europarl  | European Parliament proceedings  | 1.5 GB  | 2011  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| Common Crawl  | The internet  | ~300 GB  | Ongoing  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| OpenWebText  | Curated internet using Reddit  | 55 GB  | 2019  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| The Pile  | Everything above plus specialty datasets (books, law, med)  |
    825 GB  | 2020  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| RedPajama  | GitHub, arXiv, Books, Wikipedia, StackExchange, and multiple
    version of Common Crawl  | 5 TB  | 2023  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| OSCAR  | Highly curated multilingual dataset with 166 languages  | 9.4 TB  |
    Ongoing  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: Corpora
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As you probably picked up on, most of the datasets out there are essentially
    just text dumps of the internet. If you’re looking for something with a little
    more finesse, something that contains more meta info to help your model disambiguate
    for more complex tasks, consider downloading a corpus. A corpus is just like a
    dataset, except it is more easily searchable, visualized, and explained. Corpora
    are often paid datasets that can be well worth your money. Corpora, like the Corpus
    Of Historical American English (COHA) and the Corpus of Contemporary American
    English (COCA), are excellent downloads. They contain not just text data but also
    frequency analysis (bag of words) and collocates (N-grams), all ready to go. Whether
    or not you are interested in the applications of allowing models to analyze metadata
    as part of training, using corpora can help with model explainability and quality
    of data.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: You can think of a corpus as a vector database that has already been highly
    cleaned and curated and is ready to go. While it hasn’t yet been done, a corpus
    that combines both the linguistic explainability and time-series bucketing with
    precalculated embeddings put into a real-time vector database would likely be
    invaluable and highly profitable in this field for the foreseeable future, especially
    if both textual and audio data are captured. If your company has its own language
    data it wants to train on, your best course of action is to create a corpus where
    your biggest job is saying where data came from when and what the overall goal
    of the data going into the model is. Almost every NLP library has strategies for
    creating corpora, from NLTK to spaCy and even LangChain. Be mindful about which
    strategies and tools you pick because at the end of the day, your dataset or corpus
    contains everything your model will see.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Data cleaning and preparation
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you pulled any of the previously mentioned datasets, you might be surprised
    to realize most of them are just giant text dumps—a large parquet or text file.
    There are no labels or annotations, and feature engineering hasn’t been done at
    all. LLMs are trained via self-supervised methods to predict the next word or
    a masked word, so a lot of traditional data cleaning and preparation processes
    are unneeded. This fact leads many to believe that data cleaning as a whole is
    unnecessary, but this couldn’t be further from the truth. Datasets are the lifeblood
    of all ML, and they are so much more than a pile of data. Yet that’s what most
    businesses have—a pile of data. Data cleaning and curation are difficult, time-consuming,
    and ultimately subjective tasks that are difficult to tie to key performance indicators
    (KPIs). Still, taking the time and resources to clean your data will create a
    more consistent and unparalleled user experience.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Since the 1990s, people have tested whether Big Data can produce better results
    than high-quality data; we believe the answer is no. Big Data is nowhere close
    to devoid of value. The Law of Big Numbers has been applied, and it has shown
    that models can generate convincing syntax at the same level as people. However,
    as we’ve said before, models have also soundly demonstrated that syntax is in
    no way connected to semantics or pragmatics.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we hope to share with you the right frame of mind when preparing
    your dataset. We will focus on the high-level linguistic considerations you should
    be thinking about when preparing a dataset, and we won’t be going too deep into
    how to create the actual data pipelines. That said, the main logic is simple and
    follows these basic steps:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Take your pile of data, and determine a schema for the features.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure all the features conform to a distribution that makes sense for the
    outcome you’re trying to get through normalization or scaling.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the data for bias/anomalies (most businesses skip this step by using automated
    checking instead of informed verification).
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the data into a format for the model to ingest (for LLMs, it’s through
    tokenization and embedding)
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train, check, and retrain.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'NOTE For more information on creating data pipelines, check out Fundamentals
    of Data Engineering,[^(11)](#footnote-161) WizardLM,[^(12)](#footnote-162) and
    “LIMA: Less Is More for Alignment.”[^(13)](#footnote-163) These resources can
    help you create effective data pipelines to get as much data into a trainable
    state as possible.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: None of these steps are necessarily easy, but we hope to share a few tips and
    tricks. Evaluating whether your distribution is correct can be as simple as looking
    at the data and asking yourself whether it truly represents the problem or as
    difficult as creating a whole human-in-the-loop workflow to validate your model’s
    output. Next, we’ll go over the first three steps, and in the next section, we’ll
    go over the fourth. The last step is covered in depth in the next chapter.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Instruct schema
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the best and most common data schemas you should consider when preparing
    your data, especially for finetuning, is the instruct schema. Instruction tuning
    is based on the intuitive logic that if we show a model how to perform a task
    with instructions, the model will perform better than if we just show it tasks
    and “answers.” Instruction tuning involves demonstrating for the model what you
    would like to happen, and as such, the datasets are more intensive to create than
    your run-of-the-mill crawl data. You need to prepare your data to match a format
    that will look something like this:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '**CB ###Instruction**'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '{user input}'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '**CB ###Input**'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '{meta info about the instruction}'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '**![chatGpt](../Images/chatGpt.png) ###Response**'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '{model output}'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Instruction datasets are powerful because they allow the model to consider both
    instructions and relevant input. For example, if the instruction was “Translate
    this sentence to Japanese,” the input would be the sentence you’d want translated,
    and the response would be the Japanese translation. Thus, they prepare your model
    for many prompting techniques and prompt tuning, making them more effective later.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Despite their name, instruction tuning datasets are not restricted to test-based
    modalities; they can also use vision instruction tuning (image–instruction–answer)
    and red teaming instruction (RLHF) datasets. The “instruction” offers a semblance
    of pragmatics within the model and prompt, providing important guardrails for
    the LLM as it generates responses. It grounds the prompt with syntax that repeats
    and is predictable, along with syntax that is unpredictable for the model to guess
    at. These syntactic landmarks `(###Instruction`, `User:`, `Chat` `History:`, etc.)
    also help lower the chance of an EOS (end-of-sequence) token being predicted early
    due to the variable length of what can come between each of them, like chat history.
    Chat history can be one message or thousands of tokens, but the pattern, given
    there’s another landmark coming afterward, helps the model succeed in long-term
    memory. When you are deciding what to train your model on, keep those landmarks
    in mind, as they can make an instruct-tuned model even better at a specific task
    if you only need it to do one thing.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: This isn’t the only format; some competitors in the space include the evol-instruct
    format used by WizardLM and the self-instruct format used by Alpaca, both of which
    use scripts to create instruction-based prompts. The best format is still an open-ended
    question, and we’d like to extend a challenge to the reader to explore creating
    their own. GitHub ([https://mng.bz/5OmD](https://mng.bz/5OmD)) and Hugging Face
    datasets are both great places to look for vetted datasets at the moment, but
    keep in mind that if the dataset doesn’t contain many examples of the tasks you’d
    like your model to perform or it doesn’t contain enough examples of semantic ambiguity
    being resolved when completing the task, performance will be unstable—which takes
    us to step 2 in our cleaning process.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring proficiency with speech acts
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In preparing the dataset, the most important consideration is what you want
    the model to do. If you want a model to predict housing prices in Boston, you
    probably shouldn’t train it on survivors of the Titanic. This is obvious when
    stated, but it raises the question, “Is my dataset correct for the problem, and
    how would I know?” When it comes to language data, the answer isn’t as obvious
    as we might hope. Let’s look at an example to figure out why.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say you want your model to take orders at a fast-food restaurant. This
    scenario may seem boring and mundane, where all we expect to see are queries like,
    “I’ll order the #3 combo,” which you will. But if you ask a cashier about how
    people actually talk to them, really, anything can happen! I had a friend who
    worked at Burger King tell me that because of Burger King’s slogan “Have It Your
    Way,” he received many crazy requests, like asking for a burger with two top buns.
    That blew my mind, but it was also a tame example. Not to mention, you never know
    when the next LARPing convention will bring more creative and colorful interactions
    to otherwise mundane scenarios. A generic dataset containing customer orders and
    cashier responses won’t be enough here. When you aren’t intentional about what
    kind of data goes into your model, the performance of the model suffers.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: DEFINITION LARP stands for live-action role-playing, and you can imagine the
    tomfoolery of a customer pretending to be an elf, orc, or pirate and thus breaking
    all rules and expectations.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: To ensure your data is right for the task, first, you should think about what
    speech acts generally go together to perform the task at hand. Speech acts refer
    to the various functions language can perform in communication beyond conveying
    information. They are a way of categorizing utterances based on their intended
    effect or purpose in a conversation. Speech acts are important, as they shed light
    on how communication goes beyond the literal meaning of words and involves the
    speaker’s intentions and the listener’s interpretation.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Speech acts defined
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The following list includes common speech acts and their definitions:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '*Expressives*—Greetings, apologies, congratulations, condolences, thanksgivings
    (e.g., “You’re the best!”)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Commissives*—Promises, oaths, pledges, threats, vows (e.g., “I swear by the
    realm, the princess will come to no harm.”)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Directives*—Commands, requests, challenges, invitations, orders, summons,
    entreaties, dares (e.g., “Get it done in the next three days.”)'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Declarations*—Blessings, firings, baptisms, arrests, marrying, juridical speech
    acts such as sentencings, declaring a mistrial, declaring out of order (e.g.,
    “You’re hired!”)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Verdictives*—Rankings, assessments, appraising, condoning (combinations such
    as representational declarations; e.g., “You’re out!”)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Questions*—Usually starting with interrogative words like *what*, *where*,
    *when*, *why*, *who*, or indicated with rising intonation at the end in English
    (e.g., “Which model is best for my task?”)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Representatives*—Assertions, statements, claims, hypotheses, descriptions,
    suggestions, answers to questions (e.g., “This model is best for your task.”'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The current way we measure the robustness of datasets for LLMs is the vanilla
    number of tokens. Instruct datasets are relatively new, but they rely on you being
    intentional with how instruction for the model happens. What will your model do
    when given a directive it shouldn’t respond to when it’s only been trained on
    helpful responses to directives? If you aren’t sure, now’s the time to consider.
    For example, imagine a user declaring with glee to your bot, “Promise you’ll help
    me take over the world!” If it was only trained to be helpful, it would likely
    respond by promising to do just that because similar scenarios are in the training
    set. And now we have an evil AI overlord taking over the world. Thanks. In actuality,
    this is a fairly innocuous example, but the unpredictability of the seemingly
    infinite number of possible responses from the model should make you think, especially
    if this agent has access to tools like Google or your internal HR documents. Being
    cognizant of speech acts can simplify your work so that you don’t have to focus
    as much on individual tokens for the vocabulary as on the overall structure of
    what your model will come in contact with during training.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Going back, when you think about a customer-facing role like a cashier, how
    many of these speech acts are likely to occur in your average order? Take a minute
    to think it through. We can tell you that declarations and verdictives are out,
    and commissives are uncommon. But what if you get them regardless? You then need
    to consider how you might want to steer such highly expressive customers toward
    the speech acts you can work with, likely questions, directives, and representatives.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: To make matters more complicated, the form of a speech act doesn’t always have
    to match its function. For example, you could say “You’re fired” to your friend
    who doesn’t work for you, where, even though its form is declarative, its function
    is more likely expressive. Once you have a dataset or a trained LLM and are looking
    to improve its ability to take instruction, this is something to seriously consider
    to increase your data’s quality and your LLM’s performance. Does your model weirdly
    fail when users frame utterances as questions when they’re actually directives?
    Does your model start hallucinating when coming in contact with the representative-only
    HR documents you’ve been asked to analyze? As a note, you don’t have to completely
    finetune a model all over again to improve performance. We’ll go over this in
    more detail later, but giving specific examples within the prompt can patch a
    lot of these edge cases quickly and inexpensively.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have an understanding of the different features you should be looking
    for in your dataset, let’s consider the best ways to annotate your dataset so
    you can make sure it conforms to expectations.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Annotating the data
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Annotation is labeling your data, usually in a positionally aware way. For speech
    recognition tasks, annotations would identify the different words as *noun*, *verb*,
    *adjective*, or *adverb*. Annotations were used as labels in supervised learning
    tasks as the main way to train a model. Now annotations essentially give us metadata
    that makes it easier to reason about and analyze our datasets. Instead of worrying
    about micro information like speech recognition or named-entity recognition, you’ll
    get more value by focusing on macro metadata, like the speech acts just discussed
    or what language the data is in.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, this is the real trick, isn’t it? If this were easy, every company
    on the face of the earth would have its own models already in production. The
    fact is data wrangling is too large to be done by hand but too varying to be done
    automatically, and you need to find the middle ground as quickly as possible.
    You don’t want to ignore your data and just download a dataset someone recommended
    (even us) and then proceed to harm a real-world population because it contained
    harmful data. But you also don’t want to have to hand-validate millions of rows
    of utterances. Thankfully, there are tools to help with every part of this, but
    we’d like to specifically mention these first:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '*Prodi.gy* ([https://prodi.gy/](https://prodi.gy/))—Prodigy takes a one-time
    payment for a quick and powerful multimodal annotation tool.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*doccano: Open source annotation tool for machine learning practitioners*([https://github.com/doccano/doccano](https://github.com/doccano/doccano))—A
    truly open-source and, at the time of writing, updated web-based platform for
    data annotation.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*d5555/TagEditor: Annotation tool for spaCy* ([https://github.com/d5555/TagEditor](https://github.com/d5555/TagEditor))—Works
    in conjunction with [https://spacy.io](https://spacy.io). Both create an ecosystem
    on top of spaCy, a popular NLP framework that makes rapid prototyping well within
    the reach of your average ML team.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Praat: Doing phonetics by computer* ([https://github.com/praat/praat](https://github.com/praat/praat))—The
    only audio annotation tool on this list, Praat is fundamentally a tool for phonetics
    with annotations thrown in. Given how much we predict the LLM space to shift toward
    phonetics, we couldn’t omit this one from the list.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Galileo* ([https://www.rungalileo.io/llm-studio](https://www.rungalileo.io/llm-studio))—At
    the time of this writing, Galileo’s LLM studio has yet to come out, but it makes
    some big promises for prompt creation and evaluation, which would immensely speed
    up annotation and creation of instruction datasets.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Which tool is best for your project depends entirely on the goal of your annotation.
    Going into annotating without a specified goal leads nowhere, as you’ll find discrepancies
    on the other end of data processing. Of course, we recommend adding speech act
    annotations; you’ll also want to consider additional annotations looking for bias
    and anomalies. We can show that by measuring the number of pieces of outside context
    present in the text (things like insinuations or entailments), you can gain a
    confidence score about how high quality a particular data is. The reason for this
    is intuitive: the more ambiguity a set of examples can solve for the model, the
    more the model learns from that set. The hard part is that no one can pin any
    of these contextual information nuggets on repeating parts of orthography, such
    as individual characters or a particular word or subword.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'Annotating can be a lot of work, but the reason for all of this consideration
    at the front is fairly simple: your model can only learn what you teach it. Thankfully,
    to make matters much easier, the goal isn’t to annotate every bit of text in your
    dataset. We are simply annotating a large-enough sample to ensure our dataset
    is representative of the task. Remember, LLMs are generally trained in two steps:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '*Self-supervised pretraining *—Analyzing many different speech acts in varying
    forms and functions to learn general representations'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Finetuning and RLHF *—Teaching the model how/when to use the representations
    learned in step 1'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This training significantly lightens the burden on you as a trainer of attempting
    to parse every possible locution (what a person literally says) and illocution
    (what they actually mean in context) within the given task. Even for something
    viewed as simple work, like being a cashier, having to come up with a dataset
    vast enough to cover all edge cases would be quite a headache. For most cases,
    all you need to do is prepare a finetuning dataset, which often doesn’t need to
    be large at all—sometimes a dozen examples is more than enough to start getting
    good results.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Text processors
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have a dataset for training or finetuning, we need to transform
    it into something that can be consumed by the LLM. Simply put, we need to turn
    the text into numbers. We’ve already briefly gone over the process of doing that
    conversion quickly and effectively, so let’s dive into different examples and
    methodologies.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll show you how to train your own tokenizers, both byte-pair
    encoding (BPE) and SentencePiece tokenizers, and how to grab embeddings from (almost)
    any model for storage or manipulation later. This step is often ignored when working
    with an LLM through an API, but much of modern performance in data applications
    depends on doing this process correctly and specifically for your goal. There
    are many mathematically sound and correct ways to tokenize text, so you can’t
    rely on something someone else did when you have a specific use case. You need
    to prepare it for that use case. Training your own tokens will allow you to minimize
    unknown tokens, `<UKN>`, while also maximizing encoded semantics. Having control
    of this process is one of the simplest and easiest hacks to give your models a
    major boost in performance. Let’s start first with tokenization.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Tokenization
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tokenization is a bit more involved than simple vectorization but leads to
    the same overall result: text input, vector output, and the ability to encode
    and decode. We mentioned in chapter 2 the multilingual factor and in chapter 3
    the token tax of foreign languages, which are both motivations to be at least
    aware of your own tokenization strategies. However, it goes beyond those. Your
    tokenization strategy isn’t just important; it is vitally important for every
    subsequent step.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: A good example is comparing GOAT 7B and GPT-4 in math and arithmetic. Consider
    table 4.3\. The left column is a simple arithmetic prompt. Then we see the two
    models’ answers and, for reference, the actual answer so you don’t have to pull
    out your calculator.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.3 Tokenization allows GOAT 7B to outperform GPT-4 in math
  id: totrans-280
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Prompt | GOAT 7B | GPT-4 1.7T | Correct |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '| 3978640188 + 42886272 =  | 4021526460  | 4,021,526,460  | 4,021,526,460  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| 4523646 minus 67453156  | –62929510  | –63,930,510  | –62,929,510  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| Calculate 397 × 4429  | 1758313  | 1,757,413  | 1,758,313  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '| What is 8914/64?  | 139 R 18  | 139.15625  | 139.28125 Or 139 R 18  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: GOAT 7B consistently outperforms GPT-4, which leaves the question, “Why does
    GOAT perform better despite being 200 times smaller? Aren’t larger models more
    likely to show emergent behavior?” You probably already guessed the answer based
    on the subsection’s heading, but if you didn’t, it’s because of the tokenization
    algorithm used!
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'The GPT family of models tokenizes all subwords and digits in groups based
    purely on frequency only, meaning that if that exact group of numbers or words
    hadn’t shown up before, they could be grouped together during the embedding and
    inference processes later! GOAT is a finetuned Llama model, meaning that while
    it was finetuned on math to be good at it, the underlying secret to success lies
    in its tokenization strategy, which is the same as Llama’s. GPT-X tokenizes like
    this:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Did you notice how the first group of numbers is seven digits long, but the
    entire output is eight tokens? This is the exact grouping methodology we’re talking
    about. Compare that to Llama’s tokenization strategy in figure 4.3\. Notice that
    each digit is highlighted individually, meaning that the model will eventually
    see all the digits. As this example demonstrates, your tokenization strategy will
    ultimately determine what your model will see and won’t see, as they’ll become
    `<UNK>` tokens—and that’s why it’s vitally important to get it right for your
    use case.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-3.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 Llama’s tokenization of the first arithmetic problem in the comparison
    table. Notice that each digit is highlighted individually, meaning that the model
    will eventually see all the digits.
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'What started out as creating a simple set of bag-of-words conversion dictionaries
    has evolved immensely, and we couldn’t be happier about it. Tokenization essentially
    consists of two major steps: a step to split up the text and a step to turn it
    into numbers. The most obvious form of tokenization is splitting a string on whitespace
    and then converting it to a number based on a word-to-integer dictionary.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'This makes sense to most Indo-European language speakers, but we can’t recommend
    this because of the two assumptions presupposed: alphabets and whitespace. What
    will you do when you come across a language that doesn’t use an alphabet, like
    Chinese? And what will you do when you come across a language that doesn’t use
    whitespace in the same way as English, like Hungarian or Turkish? Or code, for
    that matter—whitespace is critical to Python’s syntax and is more than just a
    separator; it has semantic meanings. This is one reason why multilingual models
    end up outperforming monolinguals on the same tasks in almost every case: they’re
    forced to learn deeper representations for meaning without the bowling bumpers
    of easy tokenization. So let’s look at some deeper methodologies that work for
    UTF-8 encoded languages.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are examples of all the current popular options for basing your tokenization:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '*Word-based*—“Johannes Gutenberg” becomes `[''Johannes'',` `''Gutenberg'']`.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Character-based* —“Shakespeare” becomes `[''S'',''h'',''a'',''k'',''e'',''s'',''p'',''e'',
    ''a'',''r'',''e'']`.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Subword-based*—“The quick red Delphox jumped over the lazy brown Emolga” becomes
    `[''the'',''quick'',''red'',''delph'',''ox'',''jump'',''ed'',''over'',''the'',
    ''laz'',''y'',''brown'',''emol'',''ga'']`'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a look at each of them in turn.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Word-based
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Word-based tokenizers most commonly split on whitespace, but there are other
    methods like using regular expressions, dictionaries, or punctuation. For example,
    a punctuation-based approach would split “It’s the truth!” into `['It',` `'` `‘'`
    `,` `'` `s',` `'` `the',` `'` `truth',` `'` `!']`, which gives us slightly better
    context than splitting on whitespace alone. The `TreebankWordTokenizer` from NLTK
    is an example of a regular expression tokenizer. Word-based tokenizers are relatively
    easy to implement but require us to keep an unmanageably large dictionary mapping
    to encode every single possible word. That’s unreasonable, so generally, you’ll
    implement a dictionary cutoff and return unknown tokens when the model runs into
    unrecognized words to make it work. This makes the tokenizer poor at many tasks
    like code, name, and entity recognition, as well as generalizing across domains.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Character-based
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Character-based encoding methods are the most straightforward and easiest to
    implement since we split on the UTF-8 character encodings. With this method, we
    only need the tiniest of dictionaries to map characters to numbers, which means
    we can prevent the need for unknown tokens and related concerns. However, it comes
    with a major loss of information and fails to keep relevant syntax, semantics,
    or morphology of the text.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Subword-based
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Just like Goldilocks and the Three Bears, while character-based tokenizers are
    too hard and word-based tokenizers are too soft, subword-based tokenizers are
    just right. Subword-based tokenizers have proven to be the best option, being
    a mixture of the previous two. We are able to use a smaller dictionary like a
    character-based tokenizer but lose less semantics like a word-based tokenizer.
    It even has the added bonus of including some morphological information. However,
    it’s an unsolved problem for where and how words should be split, and there are
    many different methods and approaches. The best method to choose will be, like
    all other things with LLMs, dependent on the task. If you don’t have a specific
    goal in mind for what you are trying to do, there will be consequences later.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Three main algorithms are used to create the subword dictionaries: BPE, WordPiece,
    and Unigram. In addition, SentencePiece, a combination of the three that explicitly
    handles whitespaces, is also very common. It’s outside the scope of this book
    to discuss how they work, but as a book focused on production, you should know
    that the most popular subword tokenization methodologies are BPE (GPT-x) and SentencePiece
    (LlamaX).'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'In listing 4.10, we’ll go over how to train a custom version for both BPE and
    SentencePiece on your data so that you’re equipped to face (almost) any dataset
    head-on. While reading the code, pay attention to where we train the tokenizers.
    In particular, you’ll want to tune three key parameters: `vocab_size`, `min_frequency`,
    and `special_tokens`. A larger vocabulary size means your tokenizer will be more
    robust and will likely be better at handling more languages, but it will add computational
    complexity. Minimum frequency determines how often a particular subword token
    has to be seen in the dataset before it is added to the dictionary. Larger values
    prevent rare and likely unimportant tokens from filling our dictionary and prevent
    us from learning rare tokens that are important. Lastly, special tokens are relatively
    straightforward and include syntactical tokens we care about specifically for
    model training.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.10 Training your own subword tokenizers
  id: totrans-308
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Initializes the texts to train from'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Trains a byte-pair encoding tokenizer'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Trains a SentencePiece tokenizer'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Converts'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '#5 And saves for later!'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Out of the two, BPE and SentencePiece, we find ourselves using both about equally.
    It mostly depends on which model we’re finetuning or using as a base for a particular
    project. Algorithmically, we’re partial to SentencePiece because it tends to boost
    evaluation scores on pretty much any test for models trained on it, and it’s also
    closer to how we interact with morphology as people.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: All in all, tokenization loses information, just as converting from speech to
    text does—namely, word order (syntax) and meaning (semantics). All of the information
    about what a number is and how it would differ from a letter is completely gone
    after tokenization. To circumvent potential semantic and syntactic problems, we
    need to create an approximation for each of these features and figure out how
    to mathematically represent them in abstraction to insert that meaning back into
    the tokenized vector. For this, we have embeddings.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 Embeddings
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Embeddings provide meaning to the vectors generated during tokenization. Tokenized
    text is just numbers assigned almost arbitrarily (occurrence-based) to a dictionary,
    but it’s at least in a format that the model can ingest. Embeddings are the next
    step, where positional and semantic encodings are created and looked up to give
    the model additional context for making decisions about how to (probably) complete
    the task it’s given.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Embeddings are imperfect for several reasons, but perhaps the most relevant
    is this theoretical question: Can you represent a set using only a subset of that
    same set? In this case, the first set is language, one or more, and the second
    set is numbers, floats, and digits. Math is a subset of language used to describe
    things axiomatically that we accept as true. Take the English alphabet, for example:
    Can you represent the entire alphabet by only using some fraction of the 26 letters?
    Obviously not, but what if both the original set and the subset are infinite?
    Can you represent all digits using only the decimals between 0 and 1? Given that
    the first is a numerable infinite set and the second is a nonenumerable infinite
    set, the answer is yes, which should be enheartening for the field of language
    modeling.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve talked about why embeddings shouldn’t be completely and blindly
    relied on, embeddings are what most businesses are looking for with LLMs. You
    don’t need a 1.7T-parameter model to handle customers asking questions about your
    pricing or performing a search through your documents. As we discussed in chapter
    2, embeddings have the innate advantage of being comparable by distance, provided
    both embeddings you’re comparing were created by the same model in the same dimensional
    space. That opens up the door for all sorts of speedy computation and retrieval
    where you never have to figure out how to host a gigantic model somewhere because
    you can run a smaller embedding model on a CPU, and it takes milliseconds for
    hundreds of tokens.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular and coolest applications of embeddings at the moment
    is retrieval-augmented generation (RAG), where you store data that is pertinent
    to the overall task of the model and give portions of that data as needed to a
    larger model at prompting time to improve results. Suppose we apply RAG to the
    Boston Housing dataset and attempt to predict the value of a new house. In that
    case, we can compare that house’s embedded data to the closest comparable houses
    in the area and generate an informed appraisal without ever needing an appraiser
    to verify, as long as the embeddings you’re retrieving from are up-to-date.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings can be used for dozens of different tasks and are the result of taking
    final hidden state representations from your model. Every layer of your model
    is a potential option, but the general consensus is to take representations after
    the final layer before any decoding or final linear layers or softmaxes. Listing
    4.11 gives a practical example of how to extract the embeddings from both PyTorch
    and Hugging Face models. Best practice dictates that you should extract the embeddings
    from documents using whatever embedding model you are planning to use for inference,
    especially if those embeddings will end up being stored in a VectorDB later on.
    After creating our embeddings, we show how to do a simple similarity search on
    the results, which is the basis of RAG systems.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.11 Example embeddings
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 Δownloads embedding model and dataset'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates embeddings'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Adds Faiss index that allows similarity search'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Runs query'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Prints results'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Extracting embeddings, like the listing shows, is pretty simple and differs
    very little from simply running inference or training on a dataset. Remember,
    if you aren’t using sentence transformers, set your model to `eval` mode, run
    with `torch.no_grad()`, and if you’re running on torch 2.0+, run `torch.compile(model)`.
    Things should speed up and become more computationally efficient immediately.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Another as-of-yet unsolved problem is how to compare embedding spaces. Mathematically
    sound comparisons have popped up time and again over the years, but as has been
    demonstrated, mathematical soundness isn’t the first problem to be solved; the
    modality is. In addition, pairwise comparison functions have mathematical limits
    on how fast it is possible to run them. If you’re comparing language embeddings,
    a mathematically sound conversion of a linguistically sound comparison method
    is the solution, and a linguistically sound comparison is dependent upon the goal
    of the comparison. It’s too much to go into here, but we dive more deeply into
    this topic in appendix C, where we discuss diffusion and multimodal LLMs.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Preparing a Slack dataset
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have learned the ins and outs of preparing the necessary assets
    to train our own LLM, we wanted to end this chapter by preparing a dataset that
    we can use later. For this exercise, we will tackle a very common problem in the
    industry. I’m sure most readers have experienced or witnessed an HR help channel
    constantly inundated with the same questions over and over. It doesn’t matter
    how many FAQ pages are created; users don’t want to waste their time searching
    for documentation when they could ask an expert. So let’s build a chatbot to answer
    these questions!
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: We will show you how to pull your company’s Slack data and prepare it for training
    an LLM-based chatbot. In listing 4.12, we pull Slack data, filter it to keep just
    the user’s data, and save it to a parquet file. This way, you can create a bot
    that will talk like you, but feel free to edit it. For example, you might enjoy
    creating a bot that talks like your boss, but I’d recommend not telling them in
    case they feel threatened knowing you are automating them out of a job.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.12 Example of pulling Slack data
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, there’s not much to it! We have an example dataset we pulled
    using this script in the GitHub repo accompanying this book. We will use this
    dataset in the coming chapters.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: We’ve gone over a lot in this chapter, but you should now be prepared and know
    how to select and evaluate a foundation model, prepare and clean a dataset, and
    optimize your own text processors. We will use this information in the next chapter
    to train and finetune our own LLM model.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data engineers have unique datasets to acquire and manage LLMs, like model weights,
    evaluation datasets, and embeddings.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No matter your task, there is a wide array of open source models to choose from
    to finetune your own model.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text-based tasks are harder to evaluate than simple equality metrics you’d find
    in traditional ML tasks, but there are many industry benchmarks to help you get
    started.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating LLMs for more than just performance, like bias and potential harm,
    is your responsibility.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use the Evaluate library to build your own evaluation metrics.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many large open source datasets, but most come from scraping the web
    and require cleaning.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instruct schemas and annotating your data can be effective ways to clean and
    analyze your data.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finetuning a model on a dataset with an appropriate distribution of speech acts
    for the task you want your model to perform will help it generate context-appropriate
    content.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building your own subword tokenizer to match your data can greatly improve your
    model’s performance.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many problems teams are trying to use LLMs for can be solved by using embeddings
    from your model instead.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#footnote-source-1) Joe Reis and Matt Housley, Fundamentals of Data Engineering,
    O’Reilly, 2022.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#footnote-source-2) C. Loizos, “‘GPT’ may be trademarked soon if OpenAI
    has its way,” TechCrunch, April 25, 2023, [https://mng.bz/5Omq](https://mng.bz/5Omq).'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#footnote-source-3) N. Muennighoff et al., “Cross lingual generalization
    through multitask finetuning,” November 3, 2022, [https://arxiv.org/abs/2211.01786](https://arxiv.org/abs/2211.01786).'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#footnote-source-4) C. Xu et al., “WizardLM: Empowering large language
    models to follow complex instructions,” Jun. 10, 2023, [https://arxiv.org/abs/2304.12244](https://arxiv.org/abs/2304.12244).'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#footnote-source-5) Mike Conover et al., “Free Dolly: Introducing the
    world’s first truly open instruction-tuned LLM,” Databricks, April 12, 2023, [https://mng.bz/n0e8](https://mng.bz/n0e8).'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#footnote-source-6) D. Hendrycks et al., “Measuring massive multitask
    language understanding,” arXiv (Cornell University), September 2020, [https://doi.org/10.48550/arxiv.2009.03300](https://doi.org/10.48550/arxiv.2009.03300).'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#footnote-source-7) P. Rajpurkar, R. Jia, and P. Liang, “Know what you
    don’t know: Unanswerable questions for SQuAD,” June 2018, [https://arxiv.org/abs/1806.03822](https://arxiv.org/abs/1806.03822).'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#footnote-source-8) C. Zhou et al., “LIMA: Less is more for alignment,”
    arXiv.org, May 18, 2023, [https://arxiv.org/abs/2305.11206](https://arxiv.org/abs/2305.11206).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#footnote-source-9) T. Dettmers et al., “SpQR: A sparse-quantized representation
    for near-lossless LLM weight compression,” arXiv.org, June 5, 2023, [https://arxiv.org/abs/2306.03078](https://arxiv.org/abs/2306.03078).'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#footnote-source-10) L. Gao et al., “The Pile: An 800GB Dataset of Diverse
    Text for Language Modeling,” Dec. 2020, [https://arxiv.org/abs/2101.00027](https://arxiv.org/abs/2101.00027).'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#footnote-source-11) Reis and Housley, Fundamentals of Data Engineering,
    2022.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#footnote-source-12) Xu et al., “WizardLM,” 2023.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#footnote-source-13) Zou et al., “LIMA,” 2023.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
