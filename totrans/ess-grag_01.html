<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">2</span> </span> <span class="chapter-title-text">Vector similarity search and hybrid search</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Introduction to embeddings, embedding models, vector space, and vector similarity search</li>
<li class="readable-text" id="p3">How vector similarity fits in RAG applications</li>
<li class="readable-text" id="p4">A practical walkthrough of a RAG application using vector similarity search</li>
<li class="readable-text" id="p5">Adding full-text search to the RAG application to see how enabling a hybrid search approach can improve results</li>
</ul>
</div>
<div class="readable-text" id="p6">
<p>Creating a knowledge graph can be an iterative process where you start with unstructured data and then add structure to it. This is often the case when you have a lot of unstructured data and you want to start using it to answer questions.</p>
</div>
<div class="readable-text intended-text" id="p7">
<p>This chapter will look at how we can use RAG to answer questions using unstructured data. We’ll look at how to use vector similarity search and hybrid search to find relevant information and how to use that information to generate an answer. In later chapters, we’ll look at what techniques we can use to improve the retriever and generator to get better results when there’s some structure to the data.</p>
</div>
<div class="readable-text intended-text" id="p8">
<p>In data science and machine learning, embedding models and vector similarity search are important tools for handling complex data. This chapter looks at how these technologies turn complicated data, like text and images, into uniform formats called embeddings.</p>
</div>
<div class="readable-text intended-text" id="p9">
<p>In this chapter, we will cover the basics of embedding models and vector similarity search, explaining why they are useful, how they are used, and the challenges they help solve in RAG applications. To follow along, you’ll need access to a running, blank Neo4j instance. This can be a local installation or a cloud-hosted instance; just make sure it’s empty. You can follow the implementation directly in the accompanying Jupyter notebook available here: <a href="https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch02.ipynb">https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch02.ipynb</a>.</p>
</div>
<div class="readable-text" id="p10">
<h2 class="readable-text-h2"><span class="num-string">2.1</span> Components of a RAG architecture</h2>
</div>
<div class="readable-text" id="p11">
<p>In a RAG application, there are two main components: a <em>retriever</em> and a <em>generator</em>. The retriever finds relevant information, and the generator uses that information to create a response. Vector similarity search is used in the retriever to find relevant information; this is explained in more detail later. Let’s dig into both these components. </p>
</div>
<div class="readable-text" id="p12">
<h3 class="readable-text-h3"><span class="num-string">2.1.1</span> The retriever</h3>
</div>
<div class="readable-text" id="p13">
<p>The retriever is the first component of a RAG application. Its purpose is to find relevant information and pass that information to the generator. How the retriever finds the relevant information is not implied in the RAG framework, but the most common way is to use vector similarity search. Let’s look at what’s needed to prepare data for the retriever to be successful using vector similarity search. </p>
</div>
<div class="readable-text" id="p14">
<h4 class="readable-text-h4">Vector index</h4>
</div>
<div class="readable-text" id="p15">
<p>While a vector index isn’t strictly required for vector similarity search, it’s highly recommended. A vector index is a data structure (like a map) that stores vectors in a way that makes it easy to search for similar vectors. When using a vector index, the retriever method is often referred to as an <em>approximate nearest neighbor search</em>. This is because the vector index doesn’t find the exact nearest neighbors, but it finds vectors that are very close to the nearest neighbor. This is a tradeoff between speed and accuracy. The vector index is much faster than a brute-force search, but it’s not as accurate. </p>
</div>
<div class="readable-text" id="p16">
<h4 class="readable-text-h4">Vector similarity search function</h4>
</div>
<div class="readable-text" id="p17">
<p>A <em>vector similarity search</em> function is a function that takes a vector as input and returns a list of similar vectors. This function might use a vector index to find similar vectors, or it might use some other (brute-force) method. The important thing is that it returns a list of similar vectors. </p>
</div>
<div class="readable-text intended-text" id="p18">
<p>The two most common vector similarity search functions are cosine similarity and Euclidean distance. <em>Euclidean distance</em> represents the content and intensity of the text, which is not as important in most cases covered in this book. <em>Cosine similarity</em> is a measure of the angle between two vectors. In our text-embedding case, this angle represents how similar two texts are in their meaning. The cosine similarity function takes two vectors as input and returns a number between 0 and 1; 0 means the vectors are completely different, and 1 means they are identical. Cosine similarity is considered the best fit for text chatbots, and it’s the one we’ll use in this book. </p>
</div>
<div class="readable-text" id="p19">
<h4 class="readable-text-h4">Embedding model</h4>
</div>
<div class="readable-text" id="p20">
<p>The result from a semantic classification of text is called an <em>embedding</em>. Any text you want to match using vector similarity search must be converted into an embedding. This is done using an embedding model, and it’s important that the embedding model stays the same throughout the RAG application. If you want to change the embedding model, you must repopulate the vector index. </p>
</div>
<div class="readable-text intended-text" id="p21">
<p>Embeddings are lists of numbers, and the length of the list is called the embedding dimension. The embedding dimension is important because it determines how much information the embedding can hold. The higher the embedding dimension, the more computationally expensive it is to work with the embedding, both when generating the embedding as well as when performing vector similarity search.</p>
</div>
<div class="readable-text intended-text" id="p22">
<p>An <em>embedding</em> is a way to represent complex data as a set of numbers in a simpler, lower-dimensional space. Think of it as translating data into a format that a computer can easily understand and work with. </p>
</div>
<div class="readable-text intended-text" id="p23">
<p><em>Embedding models</em> provide a uniform way to represent different types of data. Input to an embedding model can be any complex data, and the output is a vector. For instance, in dealing with text, an embedding model will take words or sentences and turn them into vectors, which are lists of numbers. The model is trained to ensure that these number lists capture essential aspects of the original words, such as their meaning or context.</p>
</div>
<div class="readable-text" id="p24">
<h4 class="readable-text-h4">Text chunking</h4>
</div>
<div class="readable-text" id="p25">
<p><em>Text chunking</em> is the process of splitting up text into smaller pieces. This is done to improve the accuracy of the retriever. The presence of smaller pieces of text means that the embedding is narrower and more specific; thus the retriever will find more relevant information when searching. </p>
</div>
<div class="readable-text intended-text" id="p26">
<p>Text chunking is very important and not easy to get right. You need to think about how to split up the text: Should it be sentences, paragraphs, semantic meaning, or something else? Should you use a sliding window, or should you use a fixed size? How big should the chunks be?</p>
</div>
<div class="readable-text intended-text" id="p27">
<p>There are no right answers to these questions, and it depends on the use case, data, and domain. But it’s important to think about these questions and try different approaches to find the best solution for your use case. </p>
</div>
<div class="readable-text" id="p28">
<h4 class="readable-text-h4">Retriever pipeline</h4>
</div>
<div class="readable-text" id="p29">
<p>Once all pieces are in place, the retriever pipeline is quite simple. It takes a query as input, converts it into an embedding using the embedding model, and then uses the vector similarity search function to find similar embeddings. In the naive case, the retriever pipeline then just returns the source chunks, which then are passed to the generator. But in most cases, the retriever pipeline needs to do some postprocessing to find the best chunks to pass to the generator. We’ll get to more advanced strategies in the next chapter. </p>
</div>
<div class="readable-text" id="p30">
<h3 class="readable-text-h3"><span class="num-string">2.1.2</span> The generator</h3>
</div>
<div class="readable-text" id="p31">
<p>The <em>generator</em> is the second component of a RAG application. It uses the information found by the retriever to generate a response. The generator is often an LLM, but one benefit of RAG over finetuning or relying on a model’s base knowledge is that the models don’t need to be as large. This is because the retriever finds relevant information, so the generator doesn’t need to know everything. It does need to know how to use the information found by the retriever to create a response. This is a much smaller task than knowing everything. </p>
</div>
<div class="readable-text intended-text" id="p32">
<p>So we’re using the language model for its ability to generate text, not for its knowledge. This means we can use smaller language models, which are faster and cheaper to run. It also means that we can trust that the language model will base its response on the information found by the retriever and therefore make fewer things up and hallucinate less. </p>
</div>
<div class="readable-text" id="p33">
<h2 class="readable-text-h2"><span class="num-string">2.2</span> RAG using vector similarity search</h2>
</div>
<div class="readable-text" id="p34">
<p>There are a few pieces needed to implement a RAG application using vector similarity search. We’ll go through each of them in this chapter. The goal is to show how to implement a RAG application using vector similarity search and how to use the information found by the retriever to generate a response. Figure 2.1 illustrates the data flow for the finished RAG application. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p35">
<img alt="figure" height="399" src="../Images/2-1.png" width="1009"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.1</span> The data flow for this RAG application using vector similarity search</h5>
</div>
<div class="readable-text" id="p36">
<p>We need to separate the application into two stages:</p>
</div>
<ul>
<li class="readable-text" id="p37"> Data setup </li>
<li class="readable-text" id="p38"> Query time </li>
</ul>
<div class="readable-text" id="p39">
<p>We’ll start by looking at the data setup, and then we’ll look at what the application will do at query time.</p>
</div>
<div class="readable-text" id="p40">
<h3 class="readable-text-h3"><span class="num-string">2.2.1</span> Application data setup</h3>
</div>
<div class="readable-text" id="p41">
<p>From earlier sections, we know that we need to process the data a bit to be able to place it in the embedding model vector space to perform vector similarity search at run time. The pieces needed are </p>
</div>
<ul>
<li class="readable-text" id="p42"> A text corpus </li>
<li class="readable-text" id="p43"> Text-chunking function </li>
<li class="readable-text" id="p44"> Embedding model </li>
<li class="readable-text" id="p45"> Database with vector similarity search ability </li>
</ul>
<div class="readable-text" id="p46">
<p>We will go through these pieces one by one and show how they contribute to the application data setup.</p>
</div>
<div class="readable-text intended-text" id="p47">
<p>The data will be stored in text chunks in a database, and the vector index will be populated with the embeddings of the text chunks. Later, at run time, when a user asks a question, the question will be embedded using the same embedding model as the text chunks, and then the vector index will be used to find similar text chunks. Figure 2.2 shows the data flow for the application data setup. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p48">
<img alt="figure" height="175" src="../Images/2-2.png" width="927"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.2</span> The pieces in the pipeline for the application data setup</h5>
</div>
<div class="readable-text" id="p49">
<h3 class="readable-text-h3"><span class="num-string">2.2.2</span> The text corpus</h3>
</div>
<div class="readable-text" id="p50">
<p>The text we will be using in this example is a paper titled “Einstein’s Patents and Inventions” (Caudhuri, 2017). Even though LLMs are well aware of Albert Einstein, we show that RAG works by asking very specific questions and comparing them with the answers we get from the paper versus answers we get from an LLM. </p>
</div>
<div class="readable-text" id="p51">
<h3 class="readable-text-h3"><span class="num-string">2.2.3</span> Text chunking</h3>
</div>
<div class="readable-text" id="p52">
<p>With an LLM having a large enough context window, we can use the whole paper as a single chunk. But to get better results, we’ll split the paper into smaller chunks and use every few hundred characters as a chunk. The chunk size that yields the best results varies on a case-by-case basis, so make sure to experiment with different chunk sizes. </p>
</div>
<div class="readable-text intended-text" id="p53">
<p>In this case, we also want to have some overlap between the chunks. This is because we want to be able to find answers that span multiple chunks. So we’ll use a sliding window with a size of 500 characters and an overlap of 40 characters. This will make the index a bit bigger, but it will also make the retriever more accurate.</p>
</div>
<div class="readable-text intended-text" id="p54">
<p>To help the embedding model better classify the semantics of each chunk, we will only chunk at spaces, so we don’t have broken words at the start and end of each chunk. This function takes a text, chunk size (number of characters), overlap (number of characters), and an optional argument whether to split on any character or on whitespaces only and returns a list of chunks.</p>
</div>
<div class="browsable-container listing-container" id="p55">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.1</span> The text-chunking function</h5>
<div class="code-area-container">
<pre class="code-area">def chunk_text(text, chunk_size, overlap, split_on_whitespace_only=True):  <span class="aframe-location"/> #1
    chunks = []
    index = 0

    while index &lt; len(text):
        if split_on_whitespace_only:
            prev_whitespace = 0
            left_index = index - overlap
            while left_index &gt;= 0:
                if text[left_index] == " ":
                    prev_whitespace = left_index
                    break
                left_index -= 1
            next_whitespace = text.find(" ", index + chunk_size)
            if next_whitespace == -1:
                next_whitespace = len(text)
            chunk = text[prev_whitespace:next_whitespace].strip()
            chunks.append(chunk)
            index = next_whitespace + 1
        else:
            start = max(0, index - overlap + 1)
            end = min(index + chunk_size + overlap, len(text))
            chunk = text[start:end].strip()
            chunks.append(chunk)
            index += chunk_size

    return chunks


chunks = chunk_text(text, 500, 40)  <span class="aframe-location"/> #2


print(len(chunks)) # 89 chunks in total   <span class="aframe-location"/> #3</pre>
<div class="code-annotations-overlay-container">
     #1 Defines the function to chunk text
     <br/>#2 Calls the function and get chunks back
     <br/>#3 Prints the length of the chunks list. The majority of the function is just to make sure that we don’t split individual words but only split on spaces. 
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p56">
<h3 class="readable-text-h3"><span class="num-string">2.2.4</span> Embedding model</h3>
</div>
<div class="readable-text" id="p57">
<p>When choosing an embedding model, it’s important to think about what kind of data you want to match. In this case, we want to match text, so we’ll use a text-embedding model. Throughout this book, we will use both embedding models and LLMs from OpenAI, but there are many alternatives out there. <code>all-MiniLM-L12-v2</code> via Sentence Transformers (<a href="https://mng.bz/nZZ2">https://mng.bz/nZZ2</a>) from Hugging Face is a great alternative to OpenAI’s embedding models, and it’s very easy to use and can run on your local CPU. </p>
</div>
<div class="readable-text intended-text" id="p58">
<p>Once we have decided on a embedding model, we need to make sure that we use the same model throughout the RAG application. This is because the vector index is populated with vectors from the embedding model, so if we change the embedding model, we need to repopulate the vector index. To embed the chunks using OpenAI’s embedding models, we’ll use the following code.</p>
</div>
<div class="browsable-container listing-container" id="p59">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.2</span> Embedding chunks</h5>
<div class="code-area-container">
<pre class="code-area">def embed(texts):   <span class="aframe-location"/> #1
    response = open_ai_client.embeddings.create(
        input=texts,
        model="text-embedding-3-small",
    )
    return list(map(lambda n: n.embedding, response.data))

embeddings = embed(chunks)      <span class="aframe-location"/> #2

print(len(embeddings)) # 89, matching the number of chunks   <span class="aframe-location"/> #3
print(len(embeddings[0])) # 1536 dimensions   <span class="aframe-location"/> #4</pre>
<div class="code-annotations-overlay-container">
     #1 Defines the function to embed chunks
     <br/>#2 Calls the function and get embeddings back
     <br/>#3 Prints the length of the embeddings list 
     <br/>#4 Prints the length of the first embedding
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p60">
<h3 class="readable-text-h3"><span class="num-string">2.2.5</span> Database with vector similarity search function</h3>
</div>
<div class="readable-text" id="p61">
<p>Now that we have the embeddings, we need to store them so we can perform a similarity search later. In this book, we will use Neo4j as our database, since it has a built-in vector index and it’s easy to use; later in the book we will use Neo4j for its graph capabilities. </p>
</div>
<div class="readable-text intended-text" id="p62">
<p>The data model we’ll use at this stage is quite simple. We’ll have a single node type <code>Chunk</code> with two properties: <code>text</code> and <code>embedding</code>. The <code>text</code> property will hold the text of the chunk, and the <code>embedding</code> property will hold the embedding of the chunk. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p63">
<img alt="figure" height="219" src="../Images/2-3.png" width="327"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 2.3</span> The data model</h5>
</div>
<div class="readable-text" id="p64">
<p>Figure 2.3 shows the simplistic data model that will be used to demonstrate how to implement a RAG application using vector similarity search.</p>
</div>
<div class="readable-text intended-text" id="p65">
<p>First, let’s create a vector index. One thing to keep in mind is that when we create the vector index, we need to define the number of dimensions the vectors will have. If you at any point in the future change the embedding model that outputs a different number of dimensions, you need to recreate the vector index.</p>
</div>
<div class="readable-text intended-text" id="p66">
<p>As we saw in the code listing 2.2, the embedding model we used outputs vectors with 1,536 dimensions, so we’ll use that as the number of dimensions when we create the vector index.</p>
</div>
<div class="browsable-container listing-container" id="p67">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.3</span> Creating a vector index in Neo4j</h5>
<div class="code-area-container">
<pre class="code-area">driver.execute_query("""CREATE VECTOR INDEX pdf IF NOT EXISTS
FOR (c:Chunk)
ON c.embedding""")</pre>
</div>
</div>
<div class="readable-text" id="p68">
<p>We will name the vector index <code>pdf</code> and it will be used to index nodes of type <code>Chunk</code> on the property <code>embedding</code> using the cosine similarity search function. </p>
</div>
<div class="readable-text intended-text" id="p69">
<p>Now that we have a vector index, we can populate it with the embeddings. We will do this using Cypher, where we first create a node for each chunk and then set the <code>text</code> and <code>embedding</code> properties on the node using a Cypher loop. We’re also storing an index on each <code>:Chunk</code> node, so we can easily find the chunk later. </p>
</div>
<div class="browsable-container listing-container" id="p70">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.4</span> Storing chunks and populating the vector index in Neo4j</h5>
<div class="code-area-container">
<pre class="code-area">cypher_query = '''
WITH $chunks as chunks, range(0, size($chunks)) AS index
UNWIND index AS i
WITH i, chunks[i] AS chunk, $embeddings[i] AS embedding
MERGE (c:Chunk {index: i})
SET c.text = chunk, c.embedding = embedding
'''

driver.execute_query(cypher_query, chunks=chunks, embeddings=embeddings)</pre>
</div>
</div>
<div class="readable-text" id="p71">
<p>To check what’s in the database, we can run this Cypher query to get the <code>:Chunk</code> node with index 0. </p>
</div>
<div class="browsable-container listing-container" id="p72">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.5</span> Getting data from a chunk node in Neo4j</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">records, _, _ = driver.execute_query(
<span class="">↪</span> "MATCH (c:Chunk) WHERE c.index = 0 RETURN c.embedding, c.text")

print(records[0]["c.text"][0:30])
print(records[0]["c.embedding"][0:3])</pre>
</div>
</div>
<div class="readable-text" id="p73">
<h3 class="readable-text-h3"><span class="num-string">2.2.6</span> Performing vector search</h3>
</div>
<div class="readable-text" id="p74">
<p>Now that we have the vector index populated with the embeddings, we can perform a vector similarity search. First, we need to embed the question that we want to answer. We’ll use the same embedding model as we used for the chunks, and we’ll use the same function as we used to embed the chunks. </p>
</div>
<div class="browsable-container listing-container" id="p75">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.6</span> Embedding user question</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">question = "At what time was Einstein really interested<span class="">↪</span>
<span class="">↪</span> in experimental works?"
question_embedding = embed([question])[0]</pre>
</div>
</div>
<div class="readable-text" id="p76">
<p>Now that we have the question embedded, we can perform a vector similarity search using Cypher.</p>
</div>
<div class="browsable-container listing-container" id="p77">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.7</span> Performing vector search in Neo4j</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">query = '''
CALL db.index.vector.queryNodes('pdf', 2, $question_embedding) YIELD node<span class="">↪</span>
<span class="">↪</span> AS hits, score
RETURN hits.text AS text, score, hits.index AS index
'''
similar_records, _, _ = driver.execute_query(query, question_embedding=question_embedding)</pre>
</div>
</div>
<div class="readable-text" id="p78">
<p>The query returns the top two most similar chunks, and we can print the results to see what we got back. The code will print the following text chunks and their similarity scores.</p>
</div>
<div class="browsable-container listing-container" id="p79">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.8</span> Printing results</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">for record in similar_records:
    print(record["text"])
    print(record["score"], record["index"])
    print("======")

upbringing, his interest in inventions and patents was not unusual.
Being a manufacturer’s son, Einstein grew upon in an environment of<span class="">↪</span>
<span class="">↪</span> machines and instruments.
When his father’s company obtained the contract to illuminate Munich city<span class="">↪</span>
<span class="">↪</span> during beer festival, he
was actively engaged in execution of the contract. In his ETH days<span class="">↪</span>
<span class="">↪</span> Einstein was genuinely interested
in experimental works. He wrote to his friend, “most of the time I worked<span class="">↪</span>
<span class="">↪</span> in the physical laboratory,
fascinated by the direct contact with observation.” Einstein's
0.8185358047485352 42
======
instruments. However, it must also be
emphasized that his main occupation was theoretical physics. The<span class="">↪</span>
<span class="">↪</span> inventions he worked upon were
his diversions. In his unproductive times he used to work upon on solving<span class="">↪</span>
<span class="">↪</span> mathematical problems (not
related to his ongoing theoretical investigations) or took upon some<span class="">↪</span>
<span class="">↪</span> practical problem. As shown in
Table. 2, Einstein was involved in three major inventions; (i)<span class="">↪</span>
<span class="">↪</span> refrigeration system with Leo Szilard, (ii)
Sound reproduction system with Rudolf Goldschmidt and (iii) automatic<span class="">↪</span>
<span class="">↪</span> camera
0.7906564474105835 44
======</pre>
</div>
</div>
<div class="readable-text" id="p80">
<p>From the print, we can see the matched chunks, their similarity score, and their index. The next step is to use the chunks to generate an answer using an LLM. </p>
</div>
<div class="readable-text" id="p81">
<h3 class="readable-text-h3"><span class="num-string">2.2.7</span> Generating an answer using an LLM</h3>
</div>
<div class="readable-text" id="p82">
<p>When communicating with an LLM, we have the ability to pass in what’s called a “system message,” where we can pass in instructions for the LLM to follow. We also pass in a “user message,” which holds the original question and, in our case, the answer to the question. </p>
</div>
<div class="readable-text intended-text" id="p83">
<p>In the user message, we pass in the chunks that we want the LLM to use to generate the answer. We do this by passing in the <code>text</code> property of the similar chunks we found in the similar search in listing 2.8. </p>
</div>
<div class="browsable-container listing-container" id="p84">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.9</span> The LLM context</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">system_message = "You're an Einstein expert, but can only use the provided
<span class="">↪</span> documents to respond to the questions."

user_message = f"""
Use the following documents to answer the question that will follow:
{[doc["text"] for doc in similar_records]}

---

The question to answer using information only from the above documents:
<span class="">↪</span> {question}
"""</pre>
</div>
</div>
<div class="readable-text" id="p85">
<p>Let’s now use the LLM to generate an answer.</p>
</div>
<div class="browsable-container listing-container" id="p86">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.10</span> Generating an answer using an LLM</h5>
<div class="code-area-container">
<pre class="code-area">print("Question:", question)

stream = open_ai_client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message}
    ],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="")</pre>
</div>
</div>
<div class="readable-text" id="p87">
<p>This will stream the result from the LLM as it’s generated, and we can see the result as it’s generated.</p>
</div>
<div class="browsable-container listing-container" id="p88">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.11</span> Answer from LLM</h5>
<div class="code-area-container">
<pre class="code-area">Question: At what time was Einstein really interested in experimental works?
During his ETH days, Einstein was genuinely interested in experimental works.</pre>
</div>
</div>
<div class="readable-text" id="p89">
<p>Wow, look at that! The LLM was able to generate an answer based on the information found by the retriever. </p>
</div>
<div class="readable-text" id="p90">
<h2 class="readable-text-h2"><span class="num-string">2.3</span> Adding full-text search to the RAG application to enable hybrid search</h2>
</div>
<div class="readable-text" id="p91">
<p>In the previous section, we saw how to implement a RAG application using vector similarity search. While pure vector similarity search can take you a long way and is a great improvement over plain full-text search, it’s often not enough to produce high enough quality, accuracy, and performance for production use cases. </p>
</div>
<div class="readable-text intended-text" id="p92">
<p>In this section, we’ll look at how to improve the retriever to get better results. We’ll consider how to add full-text search to the RAG application to enable hybrid search.</p>
</div>
<div class="readable-text" id="p93">
<h3 class="readable-text-h3"><span class="num-string">2.3.1</span> Full-text search index</h3>
</div>
<div class="readable-text" id="p94">
<p><em>Full-text search</em>, a text search method in databases, has existed for a long time. It searches for matches in the data via keywords and not by similarity in a vector space. To find a match in a full-text search, the search term must be an exact match to a word in the data. </p>
</div>
<div class="readable-text intended-text" id="p95">
<p>To enable hybrid search, we need to add a full-text search index to the database. Most databases have some kind of full-text search index, and in this book we’ll use Neo4j’s full-text search index.</p>
</div>
<div class="browsable-container listing-container" id="p96">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.12</span> Creating a full-text index in Neo4j</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">driver.execute_query("CREATE FULLTEXT INDEX PdfChunkFulltext FOR (c:Chunk)
<span class="">↪</span> ON EACH [c.text]")</pre>
</div>
</div>
<div class="readable-text" id="p97">
<p>Here we create a full-text index named <code>PdfChunkFulltext</code> on the <code>text</code> property of the <code>:Chunk</code> nodes. </p>
</div>
<div class="readable-text" id="p98">
<h3 class="readable-text-h3"><span class="num-string">2.3.2</span> Performing hybrid search</h3>
</div>
<div class="readable-text" id="p99">
<p>The idea with the hybrid search is that we perform a vector similarity search and a full-text search and then combine the results. To be able to compare the scores for the two different matches, we need to normalize the scores. We do this by dividing the scores by the highest score for each search. </p>
</div>
<div class="browsable-container listing-container" id="p100">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.13</span> Performing hybrid search in Neo4j</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">hybrid_query = '''
CALL {
    // vector index
    CALL db.index.vector.queryNodes('pdf', $k, $question_embedding)<span class="">↪</span>
<span class="">↪</span> YIELD node, score
    WITH collect({node:node, score:score}) AS nodes, max(score) AS max
    UNWIND nodes AS n
    // Normalize scores
    RETURN n.node AS node, (n.score / max) AS score
    UNION
    // keyword index
    CALL db.index.fulltext.queryNodes('ftPdfChunk', $question, {limit: $k})
    YIELD node, score
    WITH collect({node:node, score:score}) AS nodes, max(score) AS max
    UNWIND nodes AS n
    // Normalize scores
    RETURN n.node AS node, (n.score / max) AS score
}
// deduplicate nodes
WITH node, max(score) AS score ORDER BY score DESC LIMIT $k
RETURN node, score
'''</pre>
</div>
</div>
<div class="readable-text" id="p101">
<p>We write a union Cypher query where we first perform a vector similarity search and then a full-text search. We then deduplicate the results and return the top <code>k</code> results.</p>
</div>
<div class="browsable-container listing-container" id="p102">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.14</span> Calling hybrid search in Neo4j</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">similar_hybrid_records, _, _ = driver.execute_query(hybrid_query,
<span class="">↪</span> question_embedding=question_embedding, question=question, k=4)

for record in similar_hybrid_records:
    print(record["node"]["text"])
    print(record["score"], record["node"]["index"])
    print("======")</pre>
</div>
</div>
<div class="browsable-container listing-container" id="p103">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.15</span> Answer from hybrid search</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">CH-Switzerland
Considering Einstein’s upbringing, his interest in inventions and patents<span class="">↪</span>
<span class="">↪</span> was not unusual.
Being a manufacturer’s son, Einstein grew upon in an environment of<span class="">↪</span>
<span class="">↪</span> machines and instruments.
When his father’s company obtained the contract to illuminate Munich city<span class="">↪</span>
<span class="">↪</span> during beer festival, he
was actively engaged in execution of the contract. In his ETH days<span class="">↪</span>
<span class="">↪</span> Einstein was genuinely interested
in experimental works. He wrote to his friend, “most of the time I worked<span class="">↪</span>
<span class="">↪</span> in the physical laboratory,
fascinated by the direct contact with observation.” Einstein's
1.0 42
======
Einstein
left his job at the Patent office and joined the University of Zurich on<span class="">↪</span>
<span class="">↪</span> October 15, 1909. Thereafter, he
continued to rise in ladder. In 1911, he moved to Prague University as a<span class="">↪</span>
<span class="">↪</span> full professor, a year later, he
was appointed as full professor at ETH, Zurich, his alma-mater. In 1914,<span class="">↪</span>
<span class="">↪</span> he was appointed Director of
the Kaiser Wilhelm Institute for Physics (1914–1932) and a professor at<span class="">↪</span>
<span class="">↪</span> the Humboldt University of
Berlin, with a special clause in his contract that freed him from<span class="">↪</span>
<span class="">↪</span> teaching obligations. In the meantime,
he was working for
0.9835733295862473 31
======</pre>
</div>
</div>
<div class="readable-text" id="p104">
<p>Here we can see that the top result got a score of 1.0 because of the normalization. This means that the top result is the same as the top result from the vector similarity search. But we can also see that the second result is different. This is because the full-text search found a better match than the vector similarity search. </p>
</div>
<div class="readable-text" id="p105">
<h2 class="readable-text-h2"><span class="num-string">2.4</span> Concluding thoughts</h2>
</div>
<div class="readable-text" id="p106">
<p>In this chapter, we looked at what vector similarity search is, what components it consists of, and how it fits into RAG applications. We then added full-text search to improve the performance of the retriever.</p>
</div>
<div class="readable-text intended-text" id="p107">
<p>By using both vector similarity search and full-text search, we can get better results than by using only one of them. While this approach might work well in certain situations, its quality, accuracy, and performance when using hybrid search is still quite limited since we’re using unstructured data to retrieve information. References in the text are not always captured, and the surrounding context is not always enough to understand the meaning of the text for the LLMs to generate good answers.</p>
</div>
<div class="readable-text intended-text" id="p108">
<p>In the next chapter, we’ll look at how to improve the retriever to get better results.</p>
</div>
<div class="readable-text" id="p109">
<h2 class="readable-text-h2">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p110"> A RAG application consists of a retriever and a generator. The retriever finds relevant information, and the generator uses that information to create a response. </li>
<li class="readable-text" id="p111"> Text embeddings capture the meaning of text in a vector space, which allows us to use vector similarity search to find similar text. </li>
<li class="readable-text" id="p112"> By adding full-text search to the RAG application, we can enable hybrid search to improve the performance of the retriever. </li>
<li class="readable-text" id="p113"> Vector similarity search and hybrid search can work well in certain situations, but their quality, accuracy, and performance are still quite limited as the data complexity grows. </li>
</ul>
</div></body></html>