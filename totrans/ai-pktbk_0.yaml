- en: 1 How AI works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The way LLMs process inputs and generate outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The transformer architecture that powers LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How LLMs and other AI models learn from data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How convolutional neural networks are used to process different types of media
    with AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining different types of data (e.g., producing images from text)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter clarifies how AI works, discussing many foundational AI topics.
    Since the latest AI boom, many of these topics (e.g., “embeddings” and “temperature”)
    are now widely discussed, not just by AI practitioners but also by businesspeople
    and the general public. This chapter demystifies them.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of just piling up definitions and writing textbook explanations, this
    chapter is a bit more opinionated. It points out common AI problems, misconceptions,
    and limitations based on my experience working in the field, as well as discussing
    some interesting insights you might not be aware of. For example, we’ll discuss
    why language generation is more expensive in French than in English and how OpenAI
    hires armies of human workers to manually help train ChatGPT. So, even if you
    are already familiar with all the topics covered in this chapter, reading it might
    provide you with a different perspective.
  prefs: []
  type: TYPE_NORMAL
- en: The first part of this chapter is a high-level explanation of how *large language
    models* (LLMs) such as ChatGPT work. Its sections are ordered to roughly mimic
    how LLMs themselves turn inputs into outputs one step at a time.
  prefs: []
  type: TYPE_NORMAL
- en: The middle part of this chapter discusses *machine learning,* which is the technique
    that makes computers learn from data to create LLMs and other types of AI. Note
    that AI and machine learning don’t mean the same. AI is a research field that
    tries to create computer programs to perform tasks in a way similar to humans.
    Machine learning may or may not be used for that goal. However, machine learning
    has been the preferred methodology in AI for at least two decades. So, you might
    hear people use the terms AI and machine learning interchangeably. When I speak
    of AI in this book, I mean current AI methods, and these methods involve the use
    of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: The last third of this chapter discusses how AI works outside language generation.
    Specifically, I give an overview of how AI analyzes and generates images or combinations
    of text and images. We also comment on current developments in AI-based video
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: Enjoy the ride!
  prefs: []
  type: TYPE_NORMAL
- en: How LLMs work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language models are computer programs that try to represent the structure of
    human language. A large language model, or LLM, is a language model on steroids.
    Its sheer size lets the LLM perform complex analyses of sentences and generate
    new text with impressive performance. Examples of LLMs are Open­AI’s GPT-4o, Meta’s
    Llama-3, Anthropic’s Claude 3.5 Sonnet, Google’s Gemini 1.5 Pro, and Mistral AI’s
    Mixtral 8x7b.
  prefs: []
  type: TYPE_NORMAL
- en: Current LLMs are designed to perform one specific task—guess the next word given
    an input sentence. The input sentence is known as the *prompt.* Suppose I asked
    you to predict the word that comes after the incomplete sentence “The Eiffel.”
    You’re very likely to suggest that “Tower” is the most logical choice. This is
    the exact job LLMs are designed to do. So, we can think of LLMs as sophisticated
    autocomplete programs. Officially, we say that LLMs are *autoregressive,* which
    means that they’re designed to produce a single extra piece of content based on
    previous content.
  prefs: []
  type: TYPE_NORMAL
- en: 'The autocomplete task may seem simple at first, but it is far-reaching. Consider
    the following prompt: “How much is 2 + 5? It is. . .” Autocompleting this kind
    of sentence requires knowing how to perform arithmetic operations. So, the task
    of performing arithmetic operations is included in the autocomplete task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, consider the following prompt: “How do you say ‘umbrella’ in French?”
    To accurately autocomplete this kind of sentence, you’d need to be capable of
    translating French to English. So, at least in theory, the autocomplete task encompasses
    all sorts of tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are created using machine learning, a process in which a computer analyzes
    a huge amount of data—pretty much a snapshot of the entire public internet—to
    automatically put the LLM together. The resulting LLM is a self-contained piece
    of software, meaning that it doesn’t access any external information to generate
    its outputs. For example, it doesn’t browse the web to make its next-word predictions.
    In addition, the LLM is static, so it must be periodically updated with new data
    if we want it to speak about recent events.
  prefs: []
  type: TYPE_NORMAL
- en: When we interact with LLMs, we don’t usually do so directly. Instead, we use
    an intermediary piece of software that processes our requests and manages the
    underlying LLM. Let’s call it the *LLM wrapper.* The wrapper uses tricks to provide
    further functionality to the user than just guessing the next word like the bare
    LLM would do. For example, the wrapper generates entire sentences, responds in
    a chatty way, and answers with real-time information, such as the current date.
  prefs: []
  type: TYPE_NORMAL
- en: An example of an LLM wrapper is ChatGPT, which is OpenAI’s customer-facing application.
    This application manages our interactions with the underlying LLM, such as GPT-4
    and GPT-4o. Note that it is common to just use the term LLM to refer to the whole
    AI system, including the wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: The next few sections discuss examples of how LLM wrappers use tricks to enhance
    the capabilities of their underlying, next-word guessing LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Text generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We typically use LLMs to output entire sentences instead of just guessing a
    single word. The LLM wrapper achieves this through a simple trick: it makes the
    LLM eat its own output repeatedly. Suppose we give an LLM the prompt “The Eiffel.”
    The LLM guesses the most likely continuation of the sentence: “Tower.” The LLM
    wrapper then attaches this word to the initial prompt, which leads to the new
    prompt: “The Eiffel Tower.” It then uses the LLM to guess the following word,
    say, “is” and attaches it to the prompt again. The process is repeated (see figure
    1.1) to generate entire sentences such as “The Eiffel Tower is in Paris.”'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH01_F01_Maggiori.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1  To generate full sentences, the LLM wrapper used the LLM to generate
    one word, then attached that word to the initial prompt, then used the LLM again
    to generate one more word, and so on.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: End of text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to outputting regular words, LLMs are designed to output a handful
    of special code words. One of them, often stylized as “<|end of text|>” in the
    literature, is a code word that signals the end of the text. When the LLM is built,
    it is exposed to examples of sentences containing this special code word to indicate
    their end. So, the LLM gains the capability of guessing that the next best word
    is actually an indication of its ending.
  prefs: []
  type: TYPE_NORMAL
- en: When the LLM wrapper encounters this special code word, it stops the process
    of having the LLM eat its own output to generate more text, as explained in the
    previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Chat
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Have a look at what happened when I inputted the prompt “The Eiffel” to ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response-Chatgpt.png)**  It seems that your sentence was
    cut off. Were you asking about the Eiffel Tower?'
  prefs: []
  type: TYPE_NORMAL
- en: The Eiffel Tower is one of the most iconic landmarks in the world, located in
    Paris, France. It was designed by Gustave Eiffel and completed in 1889 for the
    Exposition Universelle, a world’s fair held to celebrate the 100th anniversary
    of the French Revolution. Originally criticized by some of Paris's leading artists
    and intellectuals for its design, the tower has become a global cultural icon
    of France and one of the most recognizable structures in the world.
  prefs: []
  type: TYPE_NORMAL
- en: By the looks of it, the LLM is not just autocompleting my input, or else it
    would have continued with “Tower” instead of telling me that my sentence was cut
    off.
  prefs: []
  type: TYPE_NORMAL
- en: What happened here was that, behind the scenes, the LLM wrapper inserted a special
    code word to indicate the end of my prompt. This code word is often stylized as
    “<|end of prompt|>” in the literature. So, the actual input sent to the LLM by
    the wrapper was “The Eiffel<|end of prompt|>”. During its construction, the underlying
    LLM was fed examples of dialog ending with “<|end of prompt|>”, which is why it
    recognized that in my sentence, the code word was located at an unusual place.
  prefs: []
  type: TYPE_NORMAL
- en: As we continue chatting back and forth with ChatGPT, the wrapper sends the LLM
    a history of the entire conversation each time, filled with “<|end of prompt|>”
    and “<|end of text|>” code words to clearly delimit the beginning and end of each
    interaction. This helps ChatGPT converse effectively.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI lets software developers use a more naked version of the LLM wrapper,
    which doesn’t insert the “<|end of prompt|>” code word after the user’s prompt.
    This is available through the API (an API is a way to connect two pieces of software—in
    this case, your software with OpenAI’s software) or the online platform for software
    developers (see [https://platform.openai.com](https://platform.openai.com)) by
    using the *completions* mode of the API instead of the default *chat* mode.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE OpenAI’s documentation contains a warning sign telling us that the completions
    mode is “now considered legacy,” but it is still available for use as of this
    writing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what happened when I inputted “The Eiffel” to the LLM using the completions
    mode instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response-Chatgpt.png)**  Tower was inaugurated on March
    31st in 1889\. It was originally built as the entrance arch for the 1889 World’s
    Fair, and was only intended to stand for 20 years. However, its unique design
    and popularity among Parisians and tourists alike led to its preservation and
    it has become an iconic symbol of France and one of the most recognizable monuments
    in the world. The idea for the Eiffel Tower came from businessman Gustave Eiffel,
    who wanted to create a structure that would showcase the advancements in engineering
    and design during the Industrial Revolution. He and his team spent two years designing
    and constructing the tower, using over 18,000 individual pieces of wrought iron
    and 2.5 million rivets.'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the API simply autocompleted my input with a sensible continuation,
    which is why it started with “ Tower” (the word *Tower* with a leading space)
    instead of telling me that my prompt was incomplete.
  prefs: []
  type: TYPE_NORMAL
- en: The system prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I asked, “What is today’s date?” The response was
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response-Chatgpt.png)**  Today’s date is May 30, 2024.'
  prefs: []
  type: TYPE_NORMAL
- en: This was the correct response at the time of me asking. This is a bit surprising
    because, as LLMs simply analyze sentences to guess the next word, they don’t have
    access to real-time data.
  prefs: []
  type: TYPE_NORMAL
- en: 'What happened here was that ChatGPT secretly inserted additional text before
    my prompt to provide contextual information to the LLM. This is known as the *system
    prompt.* We don’t know the exact details, but the rumor is that ChatGPT’s system
    prompt is as follows (see [https://mng.bz/RVOv](https://mng.bz/RVOv)):'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Prompt-Icon.png)** You are ChatGPT, a large language model
    trained by Open­AI. Answer as concisely as possible. Knowledge cutoff: [knowledge
    cutoff] Current date: [current date and time]'
  prefs: []
  type: TYPE_NORMAL
- en: This prompt is secretly inserted every time you start a chat with ChatGPT. Because
    the date appears in ChatGPT’s system prompt, the chatbot can answer questions
    about the current date, as in the previous example. Note that the knowledge cutoff
    date is also inserted, which helps ChatGPT inform the user that it cannot answer
    questions about events that took place after a certain date.
  prefs: []
  type: TYPE_NORMAL
- en: Software developers can interact with OpenAI’s LLMs via an API instead of using
    the customer-facing ChatGPT. The API lets you define what the system prompt is,
    which is inserted before your initial interactions with the LLM. Figure 1.2 shows
    a visual interface provided by OpenAI to help developers try out the API. We can
    see a box dedicated to the system prompt.
  prefs: []
  type: TYPE_NORMAL
- en: I asked the GPT-4o LLM about the current date using OpenAI’s API, while leaving
    the system prompt empty. In figure 1.2, we can see that the LLM refused to answer
    about the date.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH01_F02_Maggiori.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2  OpenAI’s API lets users define a system prompt, which is a piece
    of text inserted into the beginning of the user’s prompt.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 1.3 shows that the LLM does answer with the date if it is given as part
    of the system prompt, like ChatGPT would do.
  prefs: []
  type: TYPE_NORMAL
- en: Calling external software functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I asked ChatGPT about the current weather in London. ChatGPT’s user interface
    showed a sign that said, “Searching the web.” A second later, the sign turned
    into “Searching current weather in London.” Afterward, it told me what the weather
    in London was like (see figure 1.4).
  prefs: []
  type: TYPE_NORMAL
- en: The trick here is to describe in the system prompt a list of software functions
    that the LLM can suggest the wrapper to call if it needs to gather external information.
    If the LLM suggests calling one of those functions, it is the job of the LLM wrapper
    to call it and then insert the result into the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH01_F03_Maggiori.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3  When the current date is supplied as part of the system prompt,
    the LLM can answer questions about the current date.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Suppose a developer wants to create a chatbot app that can seamlessly answer
    questions about current events, such as the weather, the value of stocks, and
    trending news topics. The developer could explain in the system prompt that, if
    the current weather in London is required, the LLM should output `"current_weather(London)"`,
    if the value of Apple stock is needed, it should output `"stock_value(Apple)"`,
    and so on. When these special messages are outputted, the developer will call
    software functions to gather the necessary information and add it to the prompt.
    This will give the end user the impression of seamless access to real-time data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH01_F04_Maggiori.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4  ChatGPT called a function to search the web behind the scenes and
    inserted the results into the user’s prompt. This creates the illusion that the
    LLM browses the web.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'OpenAI has created a framework that lets a developer easily define a list of
    functions that the LLM could suggest calling. Here’s an example of how to define
    a `"get_current_weather"` function, as described in the official documentation
    (see [https://mng.bz/2y4a](https://mng.bz/2y4a)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that the code of the weather-fetching function is not part of this declaration.
    Only a description of the function and its inputs is provided. The LLM wrapper
    inserts the description of this function into the system prompt so that the underlying
    LLM can suggest calling it if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the wrapper detects that the LLM suggests calling the function, it notifies
    the user. Here’s an example of the API response object, using OpenAI’s Python
    SDK, that resulted after the user asked about the weather in London:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The software developer must code the "`get_current_weather"` function, run
    it, and then insert the response into the following prompt (“Weather in London,
    United Kingdom: 20 degrees Celsius, rainy”). The LLM can then use this newly added
    information. The app end user gets the impression that the LLM itself was capable
    of answering about the weather in real time. In reality, the LLM is still a self-contained
    program; the enhanced functionality is achieved outside the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval-augmented generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes users want the LLM to analyze documents that aren’t present in the
    training data. For example, a business may want to answer questions about its
    internal documents, or an app may want to analyze the content of up-to-date webpages.
    Retrieval-augmented generation, or RAG, is a popular way of doing that (you can
    learn more in *A Simple Guide to Retrieval Augmented Generation* by Abhinav Kimothi,
    available at [https://mng.bz/yWpe](https://mng.bz/yWpe)). When the user submits
    a prompt, the LLM wrapper first searches for relevant documents in a database.
    For example, it may extract keywords from the prompt and find documents that match
    the keywords. This is known as *retrieval.*
  prefs: []
  type: TYPE_NORMAL
- en: '*Afterward, the LLM wrapper inserts the content of these documents into the
    prompt. So, the prompt is said to be *augmented* with additional, relevant information.'
  prefs: []
  type: TYPE_NORMAL
- en: When the LLM generates text, it has access to these documents as part of the
    prompt, so it can use their content to enhance its predictions. RAG is a popular
    approach to creating an in-house chatbot adapted to a specific business. In addition,
    it is commonly used to create the illusion that an LLM can access up-to-date web
    content in real time. RAG can also help identify specific sources used by the
    LLM to generate its output and thus cite references.
  prefs: []
  type: TYPE_NORMAL
- en: One of the challenges of the RAG approach is finding relevant documents based
    on the prompt. Many algorithms have been used for a long time by search engines
    to index and retrieve content, and researchers are studying specific retrieval
    techniques for RAG (see [https://arxiv.org/abs/2405.06211](https://arxiv.org/abs/2405.06211)).
    Another challenge is that prompts can become quite long with the added documents.
    LLMs only accept a maximum prompt length (more on this in the following), so you
    must make sure that the documents inserted into the prompt fit the maximum allowed
    length. In addition, longer prompts incur higher costs as AI providers charge
    fees that depend on the amount of text inputted and outputted.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve been saying that LLMs guess the next word from an input prompt, but this
    isn’t quite accurate. Let’s now refine our understanding.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs contain a fixed-size internal vocabulary. These are the words that LLMs
    can read and generate. An LLM’s vocabulary typically contains
  prefs: []
  type: TYPE_NORMAL
- en: Common words (e.g., “dog”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common pieces of words (e.g., “ish”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common Latin characters (e.g., “a” and “b”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Special symbols from a text-encoding standard called UTF-8, which are combined
    together to represent non-Latin characters and other symbols (e.g., “á,” “æ,”
    and “你”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Special code words such as “<end of text>” and “<end of prompt>”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each element in the vocabulary is known as a *token.* We can think of a token
    as a common piece of text. Using tokens instead of entire words, lets LLMs read
    and produce words that aren’t in the dictionary (e.g., “hungryish”) by combining
    common pieces of words (“hungry” + “ish”). It also lets LLMs read and produce
    non-Latin text and invent new words.
  prefs: []
  type: TYPE_NORMAL
- en: Current LLMs’ vocabularies contain roughly 100,000 different possible tokens.
    For example, some of OpenAI’s LLMs, including GPT-3.5 and GPT-4, have a vocabulary
    with 100,261 possible tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Note that many tokens represent common words with a leading space attached to
    them. For example, both “dog” and “ dog” are tokens in the vocabulary of OpenAI’s
    LLMs. So, the LLM is often spared from having to use the dedicated whitespace
    token. From now on, whenever I speak of an individual token in this book, such
    as the “dog” token, bear in mind there might be a leading space attached to it.
    (I won’t be writing the space every time, as it’s a bit ugly to read.)
  prefs: []
  type: TYPE_NORMAL
- en: The vocabulary of an LLM is created by running an automated analysis over thousands
    of documents to identify the most common text patterns (the algorithm usually
    used for this is called byte pair encoding. You can find more details and a step-by-step
    example in a blog article I wrote at [https://emaggiori.com/chatgpt-vocabulary/](https://emaggiori.com/chatgpt-vocabulary/)).
    OpenAI stopped disclosing how it creates LLMs’ vocabularies, but we do know how
    they did it with older models. For example, GPT-3’s vocabulary was created by
    automatically following links from popular Reddit discussions, collecting the
    text from the linked webpages, and identifying the most common words and combinations
    of characters in them (Redford et al., “Language Models are Unsupervised Multitask
    Learners,” 2019).
  prefs: []
  type: TYPE_NORMAL
- en: One token at a time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LLMs are designed to read a sequence of valid tokens from their vocabulary.
    So, the LLM wrapper first subdivides the input prompt into valid tokens. For example,
    when using GPT-3.5, the prompt “The dog’s bark was barely” is subdivided as follows
    by the LLM wrapper before passing it to the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The subdivision is performed using an algorithm that roughly tries to split
    the input using the largest possible tokens from the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI provides a webpage where you can input text and see how it’s tokenized
    before being fed into a model. You can find it at [https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer).
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs don’t read raw text. Instead, the LLM wrapper first converts the input
    prompt into a list of integers indicating the ID of each token, which is its position
    in the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Afterward, the wrapper uses the LLM to predict the ID of the most likely next
    token. In the previous example, the LLM outputs that the token with ID 80415 is
    the most likely continuation of the input prompt. This token corresponds to “audible”.The
    LLM wrapper then attaches that token to the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the LLM wrapper feeds this new prompt (as a list of integers, `[791,`
    `5679,` `596,` `54842,` `574,` `20025,` `80415]`) to the LLM to have it “eat its
    own output” and generate one more token. This process is repeated many times to
    generate more tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this example, after a few more paragraphs of mumbo jumbo regarding dogs and
    noise, the LLM decided that the token with ID 100276 was the most likely continuation
    of the prompt. This token is code for “<|end of text|>”. So, the LLM deemed this
    a good place to end the text. Upon stumbling on this token, the LLM wrapper heeded
    the LLM’s recommendation and stopped generating more text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look at how GPT-3.5 explained to me the meaning of the word “hungryish”,
    token by token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can see that, even though the word “hungryish” isn’t part of GPT-3.5’s vocabulary,
    it managed to generate it using a sequence of two tokens, “hungry” and “ish.”
    Note that the words “milder,” “I’m,” “you’re,” and “It’s” were also produced using
    two tokens each.
  prefs: []
  type: TYPE_NORMAL
- en: Billed by the token
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most LLM APIs, which let software developers use LLMs programmatically, bill
    users by the number of tokens inputted and outputted to the LLM. Thus, longer
    prompts and longer responses incur higher costs.
  prefs: []
  type: TYPE_NORMAL
- en: As of today, for example, GPT-4o costs US$5 per million input tokens plus US$15
    per million output tokens. For reference, the entire Shakespearean play *Romeo
    and Juliet* requires 40,000 tokens, so inputting it to GPT-4o would cost $0.20,
    and generating it would cost $0.60\. This doesn’t sound like a lot, but bills
    can easily add up if you use LLMs repeatedly. For example, if you send a long
    prompt to an LLM every time a user visits your website, you could spend thousands
    a month.
  prefs: []
  type: TYPE_NORMAL
- en: Note that when you chat back and forth with an LLM, you must include your entire
    chat history on every interaction with it, or at least you must do so if you want
    the LLM to be able to analyze the previous conversation when generating new outputs.
    So, the prompt becomes increasingly expensive as your chat history becomes longer.
  prefs: []
  type: TYPE_NORMAL
- en: What about languages other than English?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLM’s vocabularies tend to be optimized for the English language. For example,
    they contain a “dog” token but not one to represent the French word for dog. So,
    words not in English tend to be split into many tokens, often covering one or
    two letters at a time, as the vocabulary doesn’t contain as many tokens to represent
    entire words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look at how the preamble of the U.S. Constitution is tokenized before
    being inputted into GPT-4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'And now, have a look at its French translation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The French text takes more than twice the number of tokens than its English
    counterpart. In addition, the subdivision of words in French doesn’t make much
    sense. For example, “États-Unis d’Amérique” (United States of America) is chopped
    up into many meaningless pieces such as “ats” and “-Un.”
  prefs: []
  type: TYPE_NORMAL
- en: 'This problem gets even more serious with non-Latin alphabets. An extreme example,
    widely discussed around the internet, is the word for “woman” in Telugu, one of
    the languages spoken in India: స్త్రీ. This word is made up of a combination of
    six characters arranged horizontally and vertically. GPT-4 requires a whopping
    18 tokens to represent this word using special UTF-8 tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: As LLMs are billed by the token, the higher number of tokens can make them more
    expensive to use in other languages compared to English. In addition, it can be
    more challenging for the LLM to analyze the prompt because individual inputs,
    such as an “é” token, don’t carry much meaning by themselves; the LLM must work
    extra hard to contextualize adjacent tokens and derive meaning from them.
  prefs: []
  type: TYPE_NORMAL
- en: The bias toward a specific language—English in the most popular LLMs—may not
    be easily removed. To better tokenize words in other languages, the vocabulary
    would have to be extended to include words or common pieces of words in, say,
    French, Chinese, Telugu, and so on. This would multiply the vocabulary size, well
    beyond the current 100,000 mark, which could turn LLMs ineffective and slow.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI has been working on improving its LLMs’ internal vocabularies to better
    handle non-English text. The details haven’t been disclosed yet as of this writing,
    but its creators shared a few illustrative cases with the new vocabulary used
    by GPT-4o (see [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)).
    For example, a snippet of text in Telugu requires 3.5× fewer tokens than before,
    but it still requires twice as many as its English counterpart.
  prefs: []
  type: TYPE_NORMAL
- en: Why do LLMs need tokens anyway?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One may wonder why tokens are needed at all; that is, why not have the LLM directly
    read and generate individual characters instead? As we’ll discuss soon, LLMs try
    to internally describe the *meaning* of each individual input. Describing the
    meaning of a token such as “Paris” is quite easy. For instance, we could describe
    it as “capital of France.” However, describing the meaning of a token such as
    “P” is much harder, as we don’t know what the letter refers to unless we analyze
    the context. That’s why it’s much more straightforward to take “Paris” as a single
    token in one go. The same goes for generating text—it’s much more straightforward
    to let the LLM output a token such as “Paris,” which carries a strong meaning
    by itself, instead of having it output the same word one character at a time.
  prefs: []
  type: TYPE_NORMAL
- en: We could take this idea to the extreme and create a huge vocabulary that includes
    all sorts of words and their derivatives, such as “Parisian,” “Parisians,” “Parisian
    weather,” and “Emily in Paris.” But this would go too far—the vocabulary would
    become huge, and it would be wasteful because many tokens would represent closely
    related ideas. The current setup, with tokens representing the most common words
    and pieces of words, is an in-between solution that works well in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Embeddings: A way to represent meaning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the greatest challenges of AI is finding an effective way to represent
    high-level concepts, meaning, and ideas. When designing an LLM, we want the model
    to internally represent the meaning of a token instead of its letters. For example,
    we want the token “dog” to be represented by a description of what a dog is (say,
    a friendly, four-legged animal).
  prefs: []
  type: TYPE_NORMAL
- en: An *embedding* is one of the most common ways of representing meaning. It is
    used by LLMs and other types of AI*.* An embedding is a list (or “vector”) of
    numbers. The number of elements in the vector is known as the embedding’s *dimension.*
  prefs: []
  type: TYPE_NORMAL
- en: '*We can think of each position in this vector as a measure of how much a token
    matches a certain topic. Let’s have a look at an example. Imagine an embedding
    vector of length five represents the following five topics: “Animal,” “Cat,” “Large,”
    “Scary,” and “Four legs.” Suppose we want to represent the meaning of the “dog”
    token using these topics. Figure 1.5 provides an (imagined) solution.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH01_UN01_Maggiori.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 Each token is mapped to a vector of numbers. We can imagine that
    each number in the vector represents a topic. Here’s an imaginary list of topics
    and their respective numbers for the “dog” token.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this illustration, the token was mapped to five numbers, each of them indicating
    how much the meaning of the token matches each topic. We can see that the token
    scores a high value with respect to the “Animal” topic, as a dog is certainly
    an animal. The token scores a negative value with respect to the “Cat” topic,
    as a dog is sometimes seen as the opposite of a cat. It scores a neutral value
    of zero with respect to “Large” because we don’t typically think of a dog as being
    a particularly large or small object. Figure 1.6 shows how we could imagine the
    embedding for the “elephant” token.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH01_UN02_Maggiori.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 An imaginary embedding vector for the “elephant” token
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this case, the embedding vector is neutral with respect to “Cat” and highly
    positive with respect to “Large.”
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are all about embeddings. LLMs go to great lengths to try to find a good,
    contextualized representation of tokens by using embeddings. At the end of many
    layers of processing, the embeddings are very good at representing the true meaning
    of the input tokens, which makes it easy for the LLM to do the job of guessing
    the next token.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs use much longer embedding vectors than in the above example, which lets
    them represent a huge number of topics. For example, GPT-3 uses 12,288-dimensional
    embeddings, so each input token is represented by 12,288 numbers. The smallest
    model in the Llama 3 family, developed by Meta, uses embeddings of 4,096 dimensions,
    and the largest one uses embeddings of 16,384 dimensions ([https://arxiv.org/abs/2407.21783](https://arxiv.org/abs/2407.21783)).
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning and embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Designing long embeddings by hand would be very difficult. Thus, we use *machine
    learning* to do the job instead. This means that we make a computer analyze a
    large amount of data, such as text collected from the internet, to come up with
    useful embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: When AI engineers use, say, 12,288-dimensional embeddings inside an LLM, what
    they do is leave room for 12,288 topics. However, it is up to the machine to select
    and organize the topics to best attain its objectives.
  prefs: []
  type: TYPE_NORMAL
- en: As embeddings are created automatically, it is very hard to know which topics
    are represented by each of their dimensions. In addition, the topics may not be
    as clear-cut as “Large” and “Cat.” So, by using machine learning, we can create
    effective embeddings—the proof being that LLMs work well—but we can’t understand
    exactly how they work. *Explainability* is sacrificed in the name of predictive
    power.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A location on Earth can be determined by its latitude and longitude. We can
    equally think of each number inside an embedding vector as coordinates that help
    us figure out where the token is inside a space of meaning. Figure 1.7 illustrates
    an example of the space of meaning defined by a 2D embedding vector with the topics
    “Scary” and “Large.” Every token is placed inside this space according to its
    “Scary” and “Large” values in the embedding vector.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH01_F05_Maggiori.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 We can think the numbers in an embedding vector as coordinates that
    place the token in a multidimensional “meaning space.”
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You can see that similar objects tend to group together; that’s why the tokens
    “anaconda” and “snake” are close together in this space and so are “beetle” and
    “ladybug,” but “anaconda” and “ladybug” are far apart.
  prefs: []
  type: TYPE_NORMAL
- en: Well-designed, useful embeddings are such that tokens that are closely related
    in terms of meaning are also placed close together within this imaginary embedding
    space. If embedding vectors do a bad job at representing the true meaning of tokens,
    then related tokens will not be close together in this imaginary embedding space.
  prefs: []
  type: TYPE_NORMAL
- en: As embedding vectors are usually very long, the embedding space is high-dimensional.
    We can’t draw it, but we can still imagine that, in this high-dimensional space,
    related tokens are physically clustered together.
  prefs: []
  type: TYPE_NORMAL
- en: Why embeddings are useful
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Embedding vectors are particularly useful because it’s possible to compare them
    or extract information from them very easily, just by performing simple, linear
    calculations. Suppose you want to compare the meaning of two tokens. You can do
    that by calculating their physical distance in the imaginary embedding space.
    One popular way of doing that is calculating the *dot product* between the two
    vectors, which produces a sort of “signed distance” between them. If the result
    is positive, the tokens are close enough in the embedding space and thus their
    meanings are related. If it’s zero, they are unrelated. If it’s negative, their
    meanings are opposed, such as in “large” and “small.”
  prefs: []
  type: TYPE_NORMAL
- en: NOTE The dot product is calculated by multiplying the numbers in one vector
    by their corresponding numbers in the other vector (at the same position) and
    then adding the results.
  prefs: []
  type: TYPE_NORMAL
- en: Now, suppose you want to extract a limited amount of information of interest
    from a much more expressive embedding vector. For example, you may want to extract
    animal-related topics and dump everything else. We can think of this as squashing
    the multidimensional embedding space into a lower-dimensional space, such as flattening
    the 3D space to turn it into a thin plate, thus discarding uninformative dimensions.
    We could imagine, for instance, squashing the entire 12,288-dimensional space
    into, say, a 100-dimentional space that only focuses on animal-related topics
    (e.g., “Barks,” “Mammal,” “Pet”). The mathematical operation to perform such a
    squashing is known as a *projection.*
  prefs: []
  type: TYPE_NORMAL
- en: '*A projection is performed by multiplying a matrix by the embedding vector.
    The matrix represents the direction in which we want to squeeze the embedding
    space. Note that, as we don’t usually understand how embeddings encode meaning,
    we don’t understand how meaning is represented in the squeezed embedding space.
    Just like with the embeddings, the projections into squeezed spaces are also determined
    through machine learning and not designed by hand.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to their use within LLMs and other types of AI, it has become popular
    for engineers to use third-party tools to generate embeddings for all sorts of
    content-retrieval applications. For example, you can use an embeddings API to
    generate embeddings that represent the meaning of text documents, and then you
    compare documents by calculating the dot product of their embeddings. Specifically,
    OpenAI provides an embeddings API that helps generate an embedding for a text
    document.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, some APIs generate embeddings for different input modalities, such
    as text and images. One example is Google Clouds’ embeddings API (see [https://mng.bz/1Xvq](https://mng.bz/1Xvq)).
    The generated embeddings can be directly compared. For example, a piece of text
    speaking about cats and a picture of a cat are mapped to closely related embedding
    vectors. Thus, you can use dot products to find the image that best matches a
    description.
  prefs: []
  type: TYPE_NORMAL
- en: Why LLMs struggle to analyze individual letters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs are notorious for struggling to correctly analyze the individual letters
    in words, such as counting the number of occurrences of a letter. They also struggle
    to follow instructions that require generating text with certain letters in it.
    Figure 1.8 shows an example of this problem using GPT-4o.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH01_F06_Maggiori.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8  LLMs often struggle to analyze individual letters in words.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you remember, LLMs receive tokens as inputs, not letters. So, the exact letters
    of a word are not inputted to the model. In the example of figure 1.8, the token
    “berry” is inputted to the LLM in one go.
  prefs: []
  type: TYPE_NORMAL
- en: Each token is then mapped to an embedding vector to represent its meaning. So,
    any references to individual letters are likely to be completely lost at this
    stage, as it’d be wasteful to devote space in the embedding vector to represent
    topics such as “token with two times the letter a,” when there are much more useful
    topics to represent instead.
  prefs: []
  type: TYPE_NORMAL
- en: As people have been widely mocking LLMs’ terrible performance at analyzing letters,
    it’s likely AI engineers will take ad hoc measures to directly address this problem.
    For example, the LLM wrapper may augment the prompt with words’ spellings if it
    detects that there are questions about individual letters. Maybe some of this
    has already been done, as newer LLMs seem to struggle less to analyze individual
    letters. However, the problem persists in even the most recent LLMs as of this
    writing, so it hasn’t been fully solved yet.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The methodology that powers current LLMs was invented by a group of Google researchers.
    It was described in a famous paper, published in 2017, titled “Attention Is All
    You Need” (available at [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)).
    The paper proposed a new way of designing language models, which became known
    as the *transformer architecture* or just *transformers.*
  prefs: []
  type: TYPE_NORMAL
- en: If you remember, when I asked an LLM to complete the sentence “the dog’s bark
    was barely,” it correctly outputted “audible.” Despite its apparent simplicity,
    this sentence is challenging because the word “bark” has two distinct meanings—the
    noise made by a dog and the coating of a tree. If I asked an LLM to continue the
    sentence “the tree’s bark was barely,” then “audible” would be a poor choice.
    I tried it, and the LLM outputted “visible” instead of “audible.” The LLM managed
    to correctly disambiguate the word “bark” based on whether “dog” or “tree” appeared
    earlier in the sentence. The transformer architecture was especially designed
    to effectively disambiguate tokens based on their context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before the transformer architecture, the most popular language models were
    based on a type of AI model known as LSTM (long short-term memory). LSTMs try
    to predict the next token based on the following two things:'
  prefs: []
  type: TYPE_NORMAL
- en: The last token in the input prompt (“barely” if the input is “the dog’s bark
    was barely”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A single embedding that summarizes the meaning of all the previous tokens (a
    single embedding vector that represents “the dog’s bark was”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These two pieces of information are used to predict the next token (“audible”
    in this case). As the entire context before the last token is squashed into a
    single, fixed-sized embedding vector, LSTMs can process inputs of varying lengths
    without any complications. This is one of the reasons they became so popular.
    But this is also LSTMs’ Achilles’ heel—by squashing such a large context into
    a single vector, they often lose important, fine-grained contextual information
    necessary to properly guess the next word.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer architecture solved this problem by processing the previous
    tokens in a different way, without squashing them all. The process, which follows
    three steps, is depicted in figure 1.9\.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH01_F07_Maggiori.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9  LLM overview. In step 1, the tokens are mapped to embeddings one
    by one. In step 2, each embedding is improved by contextualizing it using the
    previous tokens in the prompt. In step 3,
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: the much-improved embeddings are used to make predictions about
  prefs: []
  type: TYPE_NORMAL
- en: the next token.
  prefs: []
  type: TYPE_NORMAL
- en: First, the model maps each token in the input prompt to an embedding vector
    that seeks to represent its meaning. This is performed on each token separately,
    so no contextual information is used—each token is processed as if the other ones
    didn’t exist. While these embeddings can be okay sometimes, they can’t be too
    good because in many cases, it’s hard to know the true meaning of a token without
    looking at the context. For example, the embedding generated for a token such
    as “bark” will be poor because the model can’t know if it refers to dogs or trees.
  prefs: []
  type: TYPE_NORMAL
- en: In the second step, the LLM improves the embedding of each individual token
    by analyzing its previous tokens—each token is *transformed* by taking its context
    into account. Note that, compared to LSTMs, the transformer architecture does
    not squash embeddings together to summarize the entire prompt.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM uses a fixed number of previous tokens to contextualize each token,
    which is known as the *context window.* For example, suppose an LLM has a context
    window of 10,000 tokens. Each token is contextualized by analyzing its previous
    9,999 tokens. If the user’s prompt is shorter than 10,000 tokens, then the beginning
    of the prompt is padded with dummy values like zeros until it reaches 10,000 tokens.
    If the user’s prompt is longer than 10,000 tokens, then the LLM wrapper rejects
    the user request or drops the beginning of the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: You need to carefully consider the context window before using an LLM. If you
    want to, say, ask an LLM to summarize an entire novel, you need to make sure that
    it fits within the context window, or the LLM won’t be able to summarize the entire
    novel at once. In addition, if you use a RAG approach to insert the content of
    relevant documents into a user’s prompt, you also need to make sure the context
    window can fit them all. Moreover, when you chat back and forth with an LLM-based
    app, the entire history of the conversation is usually included in each prompt,
    making the prompt longer as you converse with the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier LLMs had very limited context windows. For example, GPT-3’s context
    window was 2,048 tokens. Therefore, their capabilities to analyze long inputs
    were limited.
  prefs: []
  type: TYPE_NORMAL
- en: Over time, the context window has grown. As of this writing, OpenAI’s latest
    model, GPT-4o, has a context window of 128,000 tokens. And one of Google’s models,
    Gemini 1.5 Pro, offers a context window of 1 million tokens to its enterprise
    customers. The size of the context window is specified in an LLM’s official documentation.
  prefs: []
  type: TYPE_NORMAL
- en: After the end of this contextualization step, the embeddings associated with
    each input token are much more accurate and thus useful than the initial ones,
    thanks to contextualization. For example, we could imagine that the embedding
    for “bark” becomes more animal-like at the end of step 2 if the word “dog” appears
    before. Conversely, its embedding would become more tree-like if the context contains
    tree references.
  prefs: []
  type: TYPE_NORMAL
- en: The third step in the transformer architecture (see figure 1.9) is to predict
    the next token based on the enhanced, contextualized embeddings generated in step
    2\. This is performed through a very simple mathematical operation because it
    is assumed that step 2 produced really good embeddings that can help guess the
    next word very easily.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we describe each of the three steps in more detail,
    and we explain how machine learning enters the picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Initial embeddings'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The initial embeddings are obtained very easily. The LLM contains an internal
    dictionary that maps each possible token to its corresponding embedding. We could
    imagine it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The initial embeddings are created by looking up each token in the dictionary
    and replacing it with its corresponding embedding. The result is an initial set
    of embeddings, created one by one without context, which concludes step 1 (see
    figure 1.9).
  prefs: []
  type: TYPE_NORMAL
- en: 'The numbers inside the dictionary are not defined by hand. These numbers are
    all *learnable parameters* of the model. This means that the AI engineer leaves
    them as blanks in the code and lets the computer fill in their values later, when
    the learning algorithm runs. We can think of the previous dictionary as follows
    from the point of view of the AI engineer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: When the computer fills in the blanks, which is known as *learning* or *training,*
    it designs its own embedding space. So, the computer is free to organize tokens
    and pick topics as it wishes to attain its goal of effectively guessing the next
    word.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a model whose vocabulary contains 100,000 different tokens and whose
    embeddings contain 10,000 dimensions, as is the case with many LLMs. The dictionary
    would contain 100,000 entries, and each entry would contain 10,000 numbers, which
    are question marks. The total number of learnable parameters (the question marks)
    would be 100,000 × 10,000 = 1 billion. That’s a lot of learnable parameters! And
    it’s just the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Contextualization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the second step, the LLM contextualizes each of the input tokens, one by
    one, by considering its previous tokens (within the context window). Let’s see,
    for example, how the LLM would contextualize the token “bark” in “dog’s bark”.
  prefs: []
  type: TYPE_NORMAL
- en: Contextualization starts by calculating an attention score for each token in
    the context. The attention score indicates how it’s best to divide attention among
    all the tokens in the context window to disambiguate the last one. For example,
    to contextualize “bark”, it’s worth focusing most of your attention on “dog,”
    followed by “bark” itself, and finally by “’s”. Figure 1.10 represents this operation.
  prefs: []
  type: TYPE_NORMAL
- en: The calculation of attention scores, known as the *attention mechanism,* is
    performed through a series of mathematical operations, such as projections on
    the embedding vectors (see section 1.3.3). We won’t cover the details here, so
    let’s just say that these operations are specially designed to let the LLM extract
    meaning from the embeddings and compare them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH01_F08_Maggiori.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10  The attention mechanism calculates the relative relevance of all
    tokens in the context window to contextualize or disambiguate the last token.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The AI engineer determines the type of number of operations but leaves blanks
    that are determined later using machine learning. For example, the numbers inside
    projection matrixes, which configure what projections do, are left as blanks.
    Thus, we can picture a projection matrix as
  prefs: []
  type: TYPE_NORMAL
- en: '[[ ? ? ? ... ?],'
  prefs: []
  type: TYPE_NORMAL
- en: '[ ? ? ? ... ?],'
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: '[ ? ? ? ... ?]]'
  prefs: []
  type: TYPE_NORMAL
- en: So, the AI engineer tells the computer how to disambiguate tokens—by using projections
    to compare embeddings, and so on—but lets the machine fill in the details. The
    machine discovers by itself useful ways of analyzing the embeddings to disambiguate
    problematic tokens like “bark”. Projection matrices are rather large, so this
    step can easily add a few hundred million, if not billions of learnable parameters
    to the model.
  prefs: []
  type: TYPE_NORMAL
- en: Once the LLM has calculated attention scores, it uses the resulting values to
    guide the contextualization of tokens’ embedding vectors. We can think of this
    step as letting information from tokens rub off onto other tokens using the attention
    score for guidance.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a lot of information from “dog” rubs off on “bark”, as its attention
    mechanism determined that the token “dog” was relevant to the meaning of “bark”.
    As a consequence of this step, the embedding for “bark” becomes more animal-like,
    as opposed to tree-like. Conversely, very little information from “’s” rubs off
    on “bark”, as the attention score deems it rather irrelevant. The process of updating
    the embeddings based on the context is known as the *feed-forward* step of the
    transformer.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, the end result of the attention and feed-forward mechanisms
    is an improved version of the embedding for “bark”. The same process is applied
    to contextualize all the tokens in input the prompt, using their previous ones,
    which leads to a new generation of improved embeddings, as illustrated in step
    2 of figure 1.9.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this process, the LLM is in a much better position to make a guess
    about the next token, as it contains an improved, contextualized representation
    of the meaning of the entire input prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer architecture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The contextualization step we just described (step 2) is usually applied multiple
    times. So, the embedding vectors are improved many times. This is known as a *multilayer*
    transformer. Most LLMs contain at least a few tens of layers of transformers applied
    in sequence. Each transformer layer has its own set of learnable parameters, so
    each layer can specialize in different contextualization tasks.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3, for example, has 96 transformer layers. This leads to a whopping total
    of 175 billion learnable parameters inside the model.
  prefs: []
  type: TYPE_NORMAL
- en: Multiheaded attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The attention mechanism is often subdivided into different heads, meaning that
    it analyzes different parts of the embedding vectors separately, one chunk at
    a time. This forces the LLM to design embedding vectors with highly specialized
    segments. For example, we could imagine that one segment is dedicated to all things
    animal related and another one to all things tree related, although we still can’t
    usually understand the embedding vectors. This has been observed to work better
    in practice than having a single head that processes the entire embedding vector
    at once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Predictions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last step, step 3 in figure 1.9, is to make a prediction about the most
    likely next token, which is the LLM’s ultimate job. This is performed through
    projections over the contextualized embeddings generated in step 2.
  prefs: []
  type: TYPE_NORMAL
- en: While we’ve been saying that LLMs predict the most likely next token, that’s
    not quite accurate. In reality, they calculate a probability value for each possible
    token in the vocabulary. So, the LLM’s output is a vector with as many numbers
    as tokens in the vocabulary. Each position refers to one possible token, as shown
    in table 1.1\. In this example, the token “audible” receives a high probability
    of 0.8, meaning that the LLM deems it a highly likely next token.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1.1  In the last step, the LLM assigns a probability value
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: for each possible token in the vocabulary. All the values add to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.01 | 0.0 | 0.05 | … | 0.8 | … |'
  prefs: []
  type: TYPE_TB
- en: '| “a” | “b” | “c” |  | “audible” |  |'
  prefs: []
  type: TYPE_TB
- en: The LLM wrapper picks the next token based on the LLM’s output probabilities.
    One way to do this is to pick the token with the highest probability according
    to the LLM (in the unlikely event that two tokens have the exact same probability,
    either one can be picked at random). However, there are other ways to do this,
    which lets the LLM get more adventurous. We will discuss this next.
  prefs: []
  type: TYPE_NORMAL
- en: Temperature
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed in the previous section, LLMs output a probability for each token
    that describes how likely it is to come right after the input prompt. One way
    to select the next token from the vocabulary is to pick the one with the highest
    probability according to the LLM. However, this encourages the LLM to play it
    a bit too safe—sometimes we want a more adventurous output. So, instead, the next
    token is often selected by randomly *sampling* a token from the vocabulary using
    the LLM’s output probabilities. For example, if the LLM outputs a probability
    of 0.9 for the “audible” token, then the sampler picks that token with 90% probability
    and other ones with 10% probability.
  prefs: []
  type: TYPE_NORMAL
- en: The user can usually regulate how adventurous the output should be by adjusting
    a setting known as the *temperature.* This setting squeezes or smooths out the
    LLM’s output probabilities. A low temperature pushes the highest sampling probabilities
    upward and lowers the others. For example, a probability of 0.9 may be transformed
    into 0.95, while a probability of 0.05 may be transformed into 0.01\. This makes
    it more likely for the LLM wrapper to pick tokens at the top of the ranking. We
    can think of this as making the LLM wrapper more conservative, as it becomes more
    prone to select the most obvious tokens at the top of the ranking and less prone
    to pick alternative ones.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, a high temperature smooths out probabilities. For example, a probability
    of 0.9 may be transformed to 0.8, and a probability of 0.01 may be transformed
    to 0.05\. This makes the output more creative by making lower-ranked tokens more
    likely to be picked. Each LLM wrapper offers its own range of temperature values.
    OpenAI’s API, for example, allows users to set the temperature to a value between
    zero (conservative) and two (creative).
  prefs: []
  type: TYPE_NORMAL
- en: In the following paragraphs, we describe two alternative ways of setting how
    adventurous we want our output to be.
  prefs: []
  type: TYPE_NORMAL
- en: Top-p
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An alternative setting known as *Top-p* is a cutoff level of cumulative probability.
    If we set Top-p to, say, 0.8, then we only sample from the top tokens that cover
    80% of the probability. The tokens covering the bottom 20% of probability are
    ignored.
  prefs: []
  type: TYPE_NORMAL
- en: Top k
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *Top-k* setting imposes a limit on the number of top tokens we can sample
    from. For example, if we set Top-k to 20, the LLM wrapper is only allowed to pick
    a token among the top-20 tokens. If we set Top-k to 1, we force the LLM to pick
    the top token every time.
  prefs: []
  type: TYPE_NORMAL
- en: Note that not all LLM wrappers let users configure all these settings—sometimes
    only one or two of them are available. For example, as of today, OpenAI lets users
    set temperature and Top-p but not Top-k. The available settings are described
    in the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Can you get an LLM to always output the same thing?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is sometimes desirable to generate *reproducible* outputs with an LLM, meaning
    that the LLM generates the exact same output every time it’s given the same input
    prompt. This can be useful to benchmark the performance of LLMs or share examples
    of LLMs’ outputs that others can replicate.
  prefs: []
  type: TYPE_NORMAL
- en: It is theoretically possible to have an LLM generate reproducible outputs. For
    example, this could be achieved by using a top-1 sampling strategy, in which we
    always pick the token with the highest probability, thus making sure that all
    mathematical calculations inside the LLM are performed exactly the same way on
    different runs.
  prefs: []
  type: TYPE_NORMAL
- en: However, while this is theoretically possible, it is not always the case in
    practice. As of today, for example, it’s not possible to guarantee that OpenAI’s
    LLMs will generate the exact same output on different runs. There is official
    guidance on how to configure settings to produce mostly reproducible outputs,
    but they’re not guaranteed to be exactly alike (see [https://mng.bz/PdeR](https://mng.bz/PdeR)).
  prefs: []
  type: TYPE_NORMAL
- en: This probably happens because popular AI and arithmetic libraries divide a calculation
    into multiple threads which can be executed in different orders every time (see
    [https://news.ycombinator.com/item?id=37006224](https://news.ycombinator.com/item?id=37006224)).
    This can cause slight differences in outputs due to round-off errors when adding
    the same numbers in different orders (see [https://mng.bz/JYQZ](https://mng.bz/JYQZ)).
    In the future, if these problems are fixed, it will be possible to generate reproducible
    outputs with popular LLM APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Where to learn more
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we’ve covered the gist of how LLMs work. We haven’t discussed
    the implementation details, such as the exact calculations performed inside the
    LLM, but we did discuss the overall process LLMs follow to make their predictions.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to know the details, I recommend you to directly have a look at
    the publicly available source code of GPT-2 ([https://mng.bz/wJR5](https://mng.bz/wJR5)).
    The file called models.py is the most important one; it defines the entire model
    in a very compact way (just 174 lines). The code is moderately easy to follow
    if you understand some Python coding and the TensorFlow library and start from
    the bottom of the file. I also recommend you read a guide called *The Illustrated
    Transformer* ([https://mng.bz/qxlx](https://mng.bz/qxlx)) to learn the details
    of the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Even if you don’t want to go through all the code, a quick skim through it reveals
    that the LLM is genuinely just a sequence of simple mathematical operations. As
    you can see in the code, each layer (called a “block”) first calculates the attention
    scores (“attn”) and then uses them to update the embeddings (“mlp”). Projections
    (“matmul”) are among the most common operations performed by the model.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now covered how LLMs generate their predictions and mentioned that their
    details are filled in using machine learning. We haven’t, however, described how
    learning unfolds. That’s where we move next.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In traditional software development, the engineer writes every single line of
    code to tell the computer exactly what to do. Machine learning, or ML, is a different
    way of creating programs (these programs are known as *models* in ML jargon).
  prefs: []
  type: TYPE_NORMAL
- en: 'The ML approach comprises two steps. The first step is designing the *architecture*
    of the solution, which in ML means a template of the steps the program will follow
    to accomplish the task. Have a look at a piece of Python code using the popular
    ML library PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the first line, the engineer defines an embedding operation that maps a vocabulary
    of 100,000 tokens to embedding vectors of length 10,000, similar to what LLMs
    do. In the second line, the engineer defines a projection to transform an embedding
    vector of length 10,000 into one of length 2,000\. The third line applies each
    of those operations sequentially, first the embedding and then the projection.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the engineer puts together the building blocks of the model
    manually. However, the model has blanks in it, known as *parameters,* which are
    not defined by hand. In the above example, the embedding block contains 1 billion
    parameters (100,000 × 10,000) which are not defined by hand. The second building
    block, the projection, contains over 20 million parameters (I’ll leave the math
    to you).
  prefs: []
  type: TYPE_NORMAL
- en: Note that the architecture of a machine learning model is designed carefully—the
    building blocks are introduced with a specific intention in mind and in a way
    that is tailor-made to the application. For example, the transformer architecture
    is designed to contextualize words.
  prefs: []
  type: TYPE_NORMAL
- en: The following step in the ML approach is known as *training* or *learning.*
    The choice between these two words is down to grammar—you typically say that a
    person trains a model and that the machine learns.
  prefs: []
  type: TYPE_NORMAL
- en: During training, the engineer runs an algorithm that tries to find the best
    way of setting the model’s parameters (filling in the blanks in the template)
    to accomplish the desired task. The training algorithm uses data for guidance—usually
    lots of it—to find promising ways of adjusting the parameter values to improve
    the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training step is time-consuming and data-hungry, but, if all goes well,
    the resulting model is often seen to perform much better than if we tried to write
    the entire program by hand. This is mainly due to the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The process is data-driven, so we rely on evidence to build the best model instead
    of intuition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model can be millions of times larger than a manually written program.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training process can identify serendipitous ways of solving the problem
    that engineers wouldn’t rely on if writing the program manually.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout this section, we’ll dig a bit deeper into how machine learning works
    and discuss common terminology.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In earlier machine learning, the engineer would first write a dedicated piece
    of software to extract representative *features* from the input. For example,
    the engineer would write a dedicated algorithm to extract keywords from text or
    detect lines in an image. Afterward, a small ML model would be used to make predictions
    from these manually engineered features. This process can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Raw input -> Feature engineering -> Model -> Prediction
  prefs: []
  type: TYPE_NORMAL
- en: 'In deep learning, which is a type of machine learning, the model processes
    much rawer inputs, such as tokens or an unprocessed input image:'
  prefs: []
  type: TYPE_NORMAL
- en: Raw input -> Model -> Prediction
  prefs: []
  type: TYPE_NORMAL
- en: 'In deep learning, the model itself learns a useful way to represent the input—it
    performs its own feature engineering. We saw that in action with LLMs: the machine
    works hard to produce contextualized embeddings to represent the meaning of the
    input tokens. To process rawer inputs, the model usually contains multiple layers
    of processing stacked on top of each other, which is where the name “deep” comes
    from.'
  prefs: []
  type: TYPE_NORMAL
- en: In many applications within text generation and image analysis, deep learning
    is much more accurate than the previous two-step process with manually engineered
    features. This requires, however, devising an effective architecture for the task,
    such as the transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Note that there’s still a place for old-school, “shallow” learning. Whenever
    your input is already abstract and informative—say, patient records with their
    age, blood type, and so on—then all you need is a shallow ML model on top. In
    addition, deep learning models are too large to understand, so it’s hard to know
    exactly how they produce outputs. We need to trust them based on their high performance.
    But sometimes you want to have an explainable model that you can fully understand.
    In that case, a more explainable model over manually engineered features may be
    the right choice.
  prefs: []
  type: TYPE_NORMAL
- en: Types of machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we discuss the four most common ML paradigms. These paradigms
    differ in terms of how they formulate the task and process the training data.
    Afterward, we discuss which of these paradigms is used by LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most ML models learn by example. You supply the computer with a large—or even
    huge—number of examples of how to do the job you want it to do. This is known
    as *supervised learning.* In supervised learning, each example is a pair of an
    input and its corresponding *label,* which is the “true” output we’d like the
    model to learn how to produce.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of LLMs, training examples are sentences labeled with the “correct”
    next-token guess, such as
  prefs: []
  type: TYPE_NORMAL
- en: “Better safe than” / “sorry”
  prefs: []
  type: TYPE_NORMAL
- en: “The Eiffel” / “Tower”
  prefs: []
  type: TYPE_NORMAL
- en: This way, the LLM is shown examples of how to perform the exact task it is expected
    to perform. All the examples supplied to the machine make up its *training data.*
  prefs: []
  type: TYPE_NORMAL
- en: '*To cite another example, in the case of a model for automated image categorization,
    the training data contains thousands of sample images, each of them labeled with
    their right category (“strawberry,” “plane,” “dog,” and so on).'
  prefs: []
  type: TYPE_NORMAL
- en: Gathering labeled data often requires manual work. For example, to create an
    ML model for image categorization, people are often hired to manually label tens
    of thousands of images with their respective categories. Sometimes, there is no
    way to escape this, and data labeling becomes a costly and time-consuming bottleneck.
    In other cases, it’s possible to use tricks to generate labels automatically by
    analyzing existing data, which we will discuss soon.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, the machine will learn a general process to perform the required task.
    So, it will also work well with inputs not exactly present in the training data,
    such as new sentences or new images. When this happens, the model is said to *generalize.*
  prefs: []
  type: TYPE_NORMAL
- en: In some unfortunate cases, the model memorizes specific training examples instead
    of learning a general process to perform the task. So, it doesn’t work well when
    it must do its job on data not seen during training. This is known as *overfitting.*
    In other cases, a model might learn a process that is too simple, so it doesn’t
    work effectively on training data or other data. This is known as *underfitting.*
  prefs: []
  type: TYPE_NORMAL
- en: '*#### A note on simulated data'
  prefs: []
  type: TYPE_NORMAL
- en: As of late, people have been asking me why they can’t just run a computer program
    to generate simulated training samples (also known as synthetic data) instead
    of going through the painstaking process of collecting and manually labeling data.
    Imagine you had a program that could generate training examples for an LLM. That
    program would have to be able to correctly guess the next word given a prompt
    to generate examples such as “Better safe than” / “sorry”. But that program would
    already be an LLM. If you had such a program to effectively generate correctly
    labeled training examples, then you wouldn’t need to build an LLM in the first
    place!
  prefs: []
  type: TYPE_NORMAL
- en: The confusion about simulated data seems to arise from the fact that, in a few
    narrow scenarios, it is indeed possible to create training data by simulation.
    This was the case with AlphaZero, the famous ML model that beat a human player
    at the game of Go. Its creators had a computer play Go against itself to generate
    millions of simulated games and generate training examples. But this was only
    possible because it’s easy to calculate the end result of a game—you can easily
    tell who won. This isn’t the case with most applications outside game-playing.
    For example, you can’t easily tell what the next token is unless you already have
    an LLM, and you can’t easily tell an image’s category unless you already have
    an effective image categorization model.
  prefs: []
  type: TYPE_NORMAL
- en: Some people also suggest augmenting your existing training data by automatically
    creating new training examples from combinations of existing ones. One technique
    called SMOTE (*synthetic minority oversampling technique*), for instance, is sometimes
    used to generate more examples of an underrepresented category. Suppose you’re
    trying to train an ML model to detect whether a credit card transaction is fraudulent.
    The training data may contain very few instances of transactions labeled as fraud
    because (hopefully) fraud doesn’t happen all that often. By using SMOTE, the AI
    engineer creates additional examples of fraudulent transactions by combining existing
    ones. However, this doesn’t add any *new* information to the training data. So,
    the machine cannot learn anything new with this extra data that it couldn’t learn
    before (for a more detailed discussion, see [https://mng.bz/7paQ](https://mng.bz/7paQ)).
    I advise you to be careful if anyone suggests you should concoct fake data to
    improve the performance of your model. In most cases, such fake data is used to
    compensate for a poor formulation of the task and not a necessity.
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervised learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In some applications, it’s possible to generate a huge number of labeled examples
    by automatically extracting information from existing data. This is known as *self-supervised
    learning.*
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that an AI engineer collects a huge amount of text from the internet.
    The engineer then extracts thousands of sentences from it and removes the last
    token from each, turning it into the label. The result is a large number of examples
    of how to guess the next token from the previous ones, which is exactly what LLMs
    need.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose a sentence in the data is “The Eiffel Tower is in Paris.” The engineer
    generates the training examples by using the previous process, as shown in figure
    1.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH01_UN03_Maggiori.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 Training examples are generated by subdividing existing sentences
    and turning the last token in each into the desired autocomplete label.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Afterward, supervised machine learning is used to train the LLM from these examples.
    Machine learning is still supervised, because it’s based on examples of how to
    do the job. But we say it’s “self” supervised to indicate that the labels were
    generated automatically from our original data source as opposed to obtained elsewhere,
    such as through manual labeling.
  prefs: []
  type: TYPE_NORMAL
- en: This trick works only when we can formulate the task as learning to reconstruct
    a corrupted input. In the case of LLMs, we artificially corrupt the input by removing
    the last token and then ask the LLM to reconstruct it by guessing that token.
    The fact that we can use this trick is probably one of the main reasons for LLMs’
    success, as it’s possible to generate a huge number of training examples without
    manual labeling.
  prefs: []
  type: TYPE_NORMAL
- en: This isn’t the case, however, with most ML applications. For example, when building
    a model for image categorization, we cannot use the self-supervised trick. Suppose
    our data contains a picture of a strawberry. The label “strawberry” is not available
    inside the picture, so we can’t remove it and then ask the model to guess it as
    we do with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In an alternative, less common ML paradigm, the computer learns by trial and
    error. The training algorithm picks random actions, tries them out, and learns
    from feedback collected afterward. For example, suppose an advertising platform
    wants to create a model of a user’s interests using machine learning. The advertiser
    first shows random ads every time the user visits a webpage and registers whether
    the user clicked on the ad or not—this is known as *exploration.* Over time, the
    training algorithm identifies the kinds of things the user is interested in based
    on their clicks. Once the advertiser has an idea of the user’s interests, it starts
    showing relevant ads to them—this is known as *exploitation.*
  prefs: []
  type: TYPE_NORMAL
- en: '*The technique of learning by trial and error is known as *reinforcement learning,*
    or *RL**.* One of the major research topics in this field is how to balance exploitation
    and exploration over time. For example, after user preferences are discovered,
    the advertiser may still want to sometimes show random ads to the user to discover
    new preferences.'
  prefs: []
  type: TYPE_NORMAL
- en: While RL has been successful in some applications, its use in a commercial setting
    is rare. This is probably because learning by trial and error is a rather wasteful
    way of learning compared to supervised learning, in which we directly provide
    the machine with examples of how to do the job.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the rise of LLMs, there is a new flavor of RL that has become popular,
    called *reinforcement learning with human feedback,* or RLHF. This technique is
    used to improve an existing LLM. It works as follows: an army of human workers
    are asked to manually create thousands of imaginary LLM prompts and pairs of alternative
    LLM outputs, and they are asked to label the alternative outputs based on preference
    (“best” versus “not best”). Afterward, AI engineers train a supervised ML model
    to guess whether an LLM output is good or bad based on these manually labeled
    examples. The result is an LM model, called the *reward model,* which is especially
    designed to determine whether an LLM’s output is good or bad.'
  prefs: []
  type: TYPE_NORMAL
- en: Afterward, the AI engineers run a reinforcement learning algorithm to refine
    an existing LLM. The algorithm generates random LLM outputs and determines how
    good they are using the reward model. The feedback from the reward model is used
    to slightly improve the LLM. This algorithm progressively refines the LLM by better
    aligning it with what the human labelers considered good outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our final machine learning category is *unsupervised learning.* In this paradigm,
    we do not supply the machine with examples of the “right output.” In fact, there
    is no such notion because the task doesn’t have a single right answer. Unsupervised
    learning is typically used to explore data and find patterns in it.
  prefs: []
  type: TYPE_NORMAL
- en: The most common example of unsupervised learning is *clustering*, in which we
    try to group similar data points together. For example, we may want to group similar
    patients together based on their medical records to create a handful of imaginary
    representative patients and analyze them. There is no notion of the “right group”
    a patient should belong to. We could group them into two, three, or five clusters,
    and there is no conclusive way of determining which number of clusters is the
    right one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because there is no uniquely right model, we cannot measure the success of
    an unsupervised learning algorithm in a clear-cut way. That’s why people often
    suggest a multitude of rules of thumb to use unsupervised learning. Some of them
    are poorly defined. For example, they suggest creating many different models,
    calculating a metric for each, plotting a curve with the results, and finally,
    picking the model at the “knee” or “elbow” of the curve. The popular book *The
    Elements of Statistical Learning* (2nd ed., Penguin, 2009) by Hastie et al. explains
    the conundrum as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: With supervised learning there is a clear measure of success, or lack thereof,
    that can be used to judge adequacy in particular situations and to compare the
    effectiveness of different methods over various situations. . . . In the context
    of unsupervised learning, there is no such direct measure of success. It is difficult
    to ascertain the validity of inferences drawn from the output of most unsupervised
    learning algorithms. One must resort to heuristic arguments not only for motivating
    the algorithms, as is often the case in supervised learning as well, but also
    for judgments as to the quality of the results. This uncomfortable situation has
    led to heavy proliferation of proposed methods, since effectiveness is a matter
    of opinion and cannot be verified directly. (p. 486)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In my experience, many of the people who try to use unsupervised learning need
    supervised learning instead.
  prefs: []
  type: TYPE_NORMAL
- en: For example, I know an engineer from a hospital who was trying to predict the
    severity of a patient’s disease. He used a clustering algorithm to automatically
    group patients together into a handful of representative patients. Afterward,
    when a new patient arrived, he tried to triage them based on their closest cluster.
  prefs: []
  type: TYPE_NORMAL
- en: It didn’t work well, and the engineer was quite frustrated. He’d tried several
    popular approaches to create good clusters. He asked me, “How can I find high-quality
    clusters, so that triage works well?” I explained to him that there is no such
    thing; you cannot evaluate the quality of clusters independently of what you want
    to use them for. What he really needed was supervised learning trained on pairs
    of patient records with their expected triage outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Note that sometimes people use the term “unsupervised learning” to refer to
    supervised learning without manually generated labels, which only adds extra confusion
    to the matter.
  prefs: []
  type: TYPE_NORMAL
- en: How LLMs are trained (and tamed)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first LLMs were built using only self-supervised learning. The AI engineers
    collected a huge amount of text from the internet and generated training examples
    automatically using the process described above (“Better safe than sorry” / “Better
    safe than” / “Sorry”). One popular source of data was Common Crawl, a database
    that contains a huge amount of text gathered from all over the internet. Another
    popular source of text was Books3, a database of 190,000 books. Note that a lot
    of this data was collected without authorization from its authors; we’ll return
    to this controversial topic later.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a 2018 paper, OpenAI researchers revealed that their largest model until
    then, GPT-2, managed to perform impressive tasks just by using self-supervised
    learning (Redford et al., 2019). This promising result made them very ambitious
    about this approach. They speculated that the large amount of data available on
    the internet combined with self-supervised learning could lead to LLMs that learned
    to perform all sorts of tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: The internet contains a vast amount of information that is passively available
    without the need for interactive communication. Our speculation is that a language
    model with sufficient capacity will begin to learn to infer and perform the tasks
    demonstrated in natural language sequences [e.g., asking the LLM to translate
    or summarize text] in order to better predict them [guess the next word], regardless
    of their method of procurement.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In addition, they argued that the task of guessing the next word encompassed
    many other tasks, so it was generally enough to build really powerful LLMs. By
    using jargon from the mathematical optimization field, they explained that the
    “global minimum” (the best solution) to the next-token-prediction task coincided
    with the “global minimum” (the best solution) to perform all sorts of other tasks.
    So, striving to find the best solution to the next-token-prediction task was equivalent
    to striving to find the best solution to other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'But enthusiasm didn’t last long. While LLMs trained using the self-supervised
    approach worked very well in many cases, they also erred badly in others. In addition,
    sometimes they generated inappropriate outputs. Researchers from OpenAI discussed
    the problem in a 2022 paper (available at [https://arxiv.org/pdf/2203.02155](https://arxiv.org/pdf/2203.02155)):'
  prefs: []
  type: TYPE_NORMAL
- en: These models often express unintended behaviors such as making up facts, generating
    biased or toxic text, or simply not following user instructions. This is because
    the language modeling objective used for many recent large LMs—predicting the
    next token on a webpage from the internet—is different from the objective “follow
    the user’s instructions helpfully and safely.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As this quote illustrates, the researchers attributed the problem to a misalignment
    between what we *really* want from LLMs—produce factual, appropriate text—and
    what they’re trained to do—guess the next token according to text collected from
    the internet.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenAI researchers proposed a solution to this problem, called InstructGPT
    ([https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)), which
    trains the LLM in four steps. First, the model is trained the usual way by using
    internet data in a self-supervised way.
  prefs: []
  type: TYPE_NORMAL
- en: Second, human workers are hired to manually write thousands of examples of input
    prompts and their corresponding desired outputs. These manually written examples
    provide extra training data to help improve and “tame” the model, for example,
    by showing it how to perform popular tasks, have two-way conversations, and refuse
    to answer inappropriate questions. According to a *Time* article, “OpenAI used
    Kenyan Workers on less than $2 per hour” for the job of labeling data ([https://mng.bz/mGP8](https://mng.bz/mGP8)).
    This goes to show that training high-performing LLMs is more manual than it seems
    at first sight.
  prefs: []
  type: TYPE_NORMAL
- en: Third, the existing LLM is *fine-tuned* using the manually generated data. This
    means that its parameters are slightly adjusted through a few extra rounds of
    training with the new examples.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth step is to use reinforcement learning with human feedback to refine
    the LLM even further (see the explanation under “*Reinforcement learning”*). In
    this case, humans are asked to manually rank alternative LLM outputs based on
    their quality, which provides feedback to the training algorithm to improve the
    LLM.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT was the first popular model trained using steps 1–4\. This turn of events
    may have caused some serious disappointment among those who believed that the
    highest performing LLMs would be created just from data collected from the internet,
    without any manual labeling.
  prefs: []
  type: TYPE_NORMAL
- en: A note on privacy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As people use LLMs, their conversations may be recorded by the LLM provider.
    The resulting data may be used to improve models, either automatically—by generating
    new training data and fine-tuning the model—or manually—by having employees identify
    recurring problems faced by users and come up with ways of fixing them. Some apps
    such as ChatGPT let users rate answers with a thumbs up or thumbs down, and they
    sometimes ask users to rank alternative answers, which might be later used to
    improve the LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: You should be careful if you include sensitive information within an LLM’s prompt,
    as it might be seen or used by the staff who works on creating and improving LLMs.
    You’ll probably be able to opt out from your prompts being recorded. For example,
    OpenAI’s website explains, “When you use our services for individuals such as
    ChatGPT, we may use your content to train our models. You can opt out of training
    through our privacy portal. . . . We do not use content from our business offerings
    such as ChatGPT Team or ChatGPT Enterprise to train our models.”
  prefs: []
  type: TYPE_NORMAL
- en: Loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s move on to the topic of how ML models learn. The first ingredient is a
    way of assessing the quality of a given model, known as the *loss* or *cost.*
    This is used by the learning algorithm to compare alternative models (with different
    parameter values) and find opportunities for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: The loss calculates how inaccurate the outputs of a model are compared to the
    training examples—the higher the value, the worse the model.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a training example, “The Eiffel” paired with its corresponding label
    “Tower”, which is used to train an LLM. Our goal is to calculate a loss value
    that measures how far off the LLM’s output is when given the input “The Eiffel”.
  prefs: []
  type: TYPE_NORMAL
- en: The loss is calculated by looking at the probability the LLM assigns to the
    right token, such as “Tower” in this case. If the probability is high, the loss
    is low, and vice versa. This is calculated by taking the negative logarithm of
    the probability, which is known as the *cross-entropy loss* or *log loss.* The
    loss is zero if the probability of “ Tower” is 1.0 (−log(1) = 0), and it takes
    an increasingly higher value the lower the probability assigned to “Tower” (e.g.,
    −log(0.2) = 1.6 and −log(0.1) = 2.3).
  prefs: []
  type: TYPE_NORMAL
- en: The loss over the entire dataset is calculated by adding the individual losses
    of each of the training examples. The better the model is at guessing the correct
    next token according to the training data, the higher the probabilities it assigns
    to them, and the lower the loss. Mission accomplished.
  prefs: []
  type: TYPE_NORMAL
- en: Note, however, that the loss measures the performance of the model on *training*
    data. The AI engineer hopes that a lower loss will translate to a higher performance
    on unseen, new data. But this isn’t always the case; if the model suffers from
    overfitting, it memorizes individual instances of the training data, thus achieving
    a low loss, but it doesn’t work well with other data.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, we’ve described the following ML ingredients:'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of a model, which contains learnable parameters (“blanks”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A way to measure the quality of a model (the loss) according to the training
    examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The only remaining ingredient is an algorithm to find the best way to adjust
    the parameters, so that the model yields the lowest loss.
  prefs: []
  type: TYPE_NORMAL
- en: The most common algorithm for this, used to build LLMs and many other ML models,
    is stochastic gradient descent (SGD). It works as follows. First, all the parameters
    inside the model are initialized using random values. So, this first version of
    the model is completely useless at the task at hand—for example, the next-token
    predictions of the LLM are nonsensical.
  prefs: []
  type: TYPE_NORMAL
- en: Afterward, the training algorithm selects a small number of training samples,
    called a *batch* or *minibatch,* to calculate a promising way of slightly modifying
    the model’s parameters to reduce the loss on that batch. In calculus jargon, this
    amounts to computing the *gradient* of the loss. We can think of this as wiggling
    the parameters a little bit to find a promising direction of change. Think of
    an optometrist slightly varying your glasses prescription and asking you if you
    see better than before. Afterward, the training algorithm slightly modifies the
    model’s parameters according to the promising direction it just found, hoping
    this will slightly improve the model.
  prefs: []
  type: TYPE_NORMAL
- en: Note that only a batch of training examples is used for this calculation, instead
    of the entire training data. This is why the algorithm is said to be *stochastic*,
    because you estimate the gradient based on a sample of the data instead of all
    the data. This makes the process much quicker.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to repeat the above operation using a second batch of examples
    extracted from the training data. The parameters are again slightly updated in
    the direction of the gradient calculated on that batch. This process is repeated,
    one batch at a time. At some point, the algorithm makes a full pass over the entire
    training data, which is known as an *epoch.* Usually, training is performed for
    several epochs, so there are multiple passes over the entire training data. We
    don’t know the exact number of epochs used to train popular LLMs, but OpenAI once
    revealed training a model for 100 epochs (see [https://mng.bz/5gy7](https://mng.bz/5gy7)).
  prefs: []
  type: TYPE_NORMAL
- en: The training process is very time-consuming. It can take days to complete and
    multiple GPUs working in unison to do all the number crunching.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient descent helps progressively improve the model, but it doesn’t
    guarantee finding the best possible model of all. This is because making slight
    improvements in the direction of the gradient can get the model stuck in a *local
    minimum.* This means that the model cannot be improved any further by making *small*
    changes to parameter values. There may be a better model, perhaps the globally
    best one, if parameters were changed widely from their current ones, but this
    is like finding a needle in a haystack.
  prefs: []
  type: TYPE_NORMAL
- en: It is kind of crazy that we can create a good LLM following this process, as
    we must find effective values for billions of parameters starting from completely
    random ones. It is wild! The reason it works is that the model’s architecture
    is laser-focused and tailor-made to the task (e.g., it enforces a multi-headed
    attention mechanism with simple, linear projections and dot products). So, the
    model’s parameter values are guided in the right direction thanks to their specialization
    to perform the task in a human-prescribed way.
  prefs: []
  type: TYPE_NORMAL
- en: Note that using an existing model is much faster than training it. All the parameters
    are already defined, so you just need to use the model once to calculate its outputs
    from its inputs. Using an already created model is often described as *inference
    time* to distinguish it from the much lengthier *training time.*
  prefs: []
  type: TYPE_NORMAL
- en: '*So far, we’ve covered AI within the context of LLMs. Understanding the gist
    of how AI works with other inputs, such as images, isn’t a big leap from what
    we’ve already discussed. In the next couple of sections, we’ll briefly comment
    on how AI processes images and combinations of different data types. We start
    with convolutional neural networks, which are a type of architecture that did
    for image analysis what transformers did for text analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI (and are LLMs generative?)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Since the proliferation of LLMs, the term “generative AI” has become a popular
    way of describing any AI model used to generate new content, such as text and
    images. In this sense, LLMs are generative.
  prefs: []
  type: TYPE_NORMAL
- en: However, that’s not what “generative” used to mean in the technical ML literature,
    so you may find conflicting uses of the word.
  prefs: []
  type: TYPE_NORMAL
- en: In ML, a model is said to be *discriminative* when it calculates the probability
    of a label given the input. We can describe this mathematically as the conditional
    probability P(Label | Input). This is exactly what LLMs calculate—the probability
    of the next token given the previous ones—so they’re technically discriminative
    models.
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, a *generative* model in the ML literature is one that calculates
    the probability of stumbling upon a certain piece of data—both input and label.
    For example, if you give the generative model a picture of a cat paired with the
    label “cat,” it tells you how likely you are to ever find such an image paired
    with such a label. So, it also assesses the plausibility of the cat image itself.
    If you give the model a picture of a blue cat paired with the label “cat,” it
    will probably output a low probability, as you’re unlikely to find pictures of
    blue cats. In mathematical terms, a generative model calculates P(Input, Label),
    the joint probability of stumbling upon a specific input/label training example.
    LLMs are not designed to do this, so, strictly speaking, they’re not generative
    models (see discussion at [https://mng.bz/6eMR](https://mng.bz/6eMR)).
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions (images, video, and audio)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now step away from text generation and take a quick look at how AI models
    process other data types like images. In the 2010s, an ML model architecture known
    as *convolutional neural network*, or CNN, became extremely popular for image
    categorization. The input to a CNN is an image—represented as a table of numbers,
    or *pixels*—and the output is a prediction of the image’s category, such as “strawberry.”
  prefs: []
  type: TYPE_NORMAL
- en: 'CNNs were specifically designed to exploit a strong assumption about image
    categorization: objects can be detected by the presence of their parts (e.g.,
    a cat can be identified by the presence of a tail, eyes, whiskers), but we don’t
    care so much about the exact location of the parts (e.g., the direction in which
    a cat’s tail points is irrelevant to recognize that it’s a cat).'
  prefs: []
  type: TYPE_NORMAL
- en: A CNN applies a series of transformations to the input image. The first transformation
    is a *convolution,* which is a simple mathematical operation that filters the
    image and produces a slightly modified version of it. Convolutions can be configured
    to do things such as
  prefs: []
  type: TYPE_NORMAL
- en: Blur the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highlight areas of a specific color
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highlight areas of sharp color changes in given directions (e.g., diagonal lines)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The exact filter applied by a convolution is configured by defining the numbers
    in a small matrix. These values are learnable parameters of the CNN, so the model
    decides which filters to apply at training time instead of the engineer defining
    them beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: The CNN performs multiple convolutions simultaneously and combines the results
    into a new image. Afterward, the image in *downsampled,* meaning that it is spatially
    shrunk. For example, an image of size 1024 × 1024 pixels might be shrunk to a
    size of 512 × 512 pixels by averaging the values of quadruplets of neighboring
    pixels. The effect of downsampling is to make this image more abstract by removing
    objects’ precise locations (as we said above, we assumed precise locations to
    be unimportant in the context of image categorization).
  prefs: []
  type: TYPE_NORMAL
- en: New convolutions are applied to the resulting image, followed by another round
    of downsampling. This is then done again and again. As filters are applied over
    already filtered images, the CNN can detect progressively complicated patterns.
    We could imagine, for example, that at first, the CNN uses convolutions to detect
    simple lines, then it detects pairs of parallel lines, then groups of parallel
    lines, then whiskers from those lines, and finally, it detects cats from their
    whiskers. As the exact filters are determined through machine learning, it’s hard
    to understand the exact strategy used by CNNs to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The end result of this process is an embedding that effectively represents the
    content of the image in an abstract way. This embedding is used to predict the
    probability of the image belonging to each possible category. Mission accomplished.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs are also used to transform images into other images of the same size. This
    is useful, for example, when reconstructing a damaged image or making any picture
    look like a Van Gogh painting. A popular architecture, called U-Net, achieves
    this in two steps. First, a usual CNN performs the above-described transformations
    to shrink the input image into a smaller, more abstract representation of its
    content. Afterward, another CNN-like structure extracts the intermediate images
    produced by the CNN and “stiches” them together to reconstruct a full-size image
    in a different style.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs have also become popular to process audio and video. The principle is the
    same—the input goes through a series of convolutions and downsampling operations
    until it’s transformed into a more abstract representation.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers have become the go-to architecture to process text, and CNNs have
    become the go-to architecture to process images, video, and audio. In the next
    section, we see how transformer and CNNs are combined in multimodal AI.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some AI models, known as *multimodal,* are capable of consuming or producing
    combinations of text, image, and audio. One example is AI that generates images
    from a textual description, such as the popular Midjourney and DALL-E.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal AI models are architected by combining LLMs and CNNs. There are myriad
    ways of combining them, so we’ll only briefly describe two approaches, one to
    generate text from images and one to generate images from text.
  prefs: []
  type: TYPE_NORMAL
- en: A popular image-to-text architecture uses an independently trained CNN to generate
    an embedding for the input image. The embedding is then transformed through a
    linear projection to make it comparable to the LLM’s embedding. For example, the
    embedding generated by the CNN for an image of a cat is turned into the embedding
    the LLM uses for the “cat” token. The new embedding is then injected inside the
    LLM. *Voilà!*
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now turn to a highly popular text-to-image approach, known as a *conditional
    diffusion* model. In this approach, a U-Net type of CNN is trained to reconstruct
    an image from a corrupted version of the image and its textual caption (see figure
    1.11).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH01_F09_Maggiori.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11  A diffusion model is trained to improve a corrupted image paired
    with its caption.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The model takes two inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: A corrupted image (often called a “noisy” image)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An embedding that represents the meaning of the text caption (e.g., generated
    using a language model)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The text embedding is inserted into the model as an additional input. This is
    often done, for example, by using an embedding that matches the image size and
    inserting it as an additional color channel, on top of red, green, and blue.
  prefs: []
  type: TYPE_NORMAL
- en: The CNN is trained to repair the damaged image. This is performed in a supervised
    way. This requires a database with numerous examples of corrupted images, their
    corresponding captions, and their uncorrupted versions. The corrupted image is
    generated automatically by artificially corrupting a higher-quality image, and
    the captions are generated manually.
  prefs: []
  type: TYPE_NORMAL
- en: Once this model is trained, it is capable of slightly improving a bad image
    using the caption for guidance. Let’s see how this model is used to create a brand-new
    image from a description, as we do with Midjourney.
  prefs: []
  type: TYPE_NORMAL
- en: First, the model is fed a totally random image, which resembles the static noise
    in a faulty TV set, together with the caption of the desired image (see figure
    1.12). The model then produces a slightly “improved” version of this image, where
    we see the desired object slightly pop up from the noise.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH01_F10_Maggiori.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12  A diffusion model is used repeatedly to have a desired image emerge
    from Gaussian noise.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The model is then used repeatedly on its own output, which progressively enhances
    the image. After doing this many times, the image becomes nice and sharp. This
    is usually performed a fixed number of times decided in advance through experimentation—the
    number of steps is set to be large enough to guarantee that most images will be
    sharp by the end. Some people are studying techniques to vary the number of steps
    depending on the prompt ([https://arxiv.org/abs/2408.02054](https://arxiv.org/abs/2408.02054)).
    We can think of this process as diffusing away the “noise,” hence the term “diffusion
    model.” This technique powers the most popular text-to-image models. Diffusion
    is also the cornerstone of text-to-video models, which is a hot research topic.
    For example, OpenAI’s video-generating model called Sora uses diffusion ([https://mng.bz/oKlD](https://mng.bz/oKlD)).
    Instead of denoising an image, it is designed to denoise a *patch,* which is a
    representation of a small piece of video over space and time. A patch covers a
    small portion of the screen, such as the top-left corner, across a few contiguous
    frames. Just like with images, the model is used repeatedly to progressively enhance
    patches using the prompt for guidance, starting from random noise. As of this
    writing, the model hasn’t yet been released to the public.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to the end of our (relatively) quick rundown of some of the fundamental
    elements of AI. Let’s draw things to a close with a high-level reflection about
    machine learning before we move on to the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: No free lunch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I’d like to wrap things up with a reflection about machine learning. As we’ve
    seen throughout this chapter, ML requires designing a dedicated architecture to
    each problem. For example, transformers are used to generate text, CNNs are used
    to analyze images, and creative combinations of the two are used in a multimodal
    setting. Each model’s architecture is based on assumptions of how to best solve
    the problem at hand. For example, transformers force the model to calculate attention
    scores, and CNNs impose using convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: Every ML milestone has been attained thanks to the invention of a new type of
    architecture that does a better job than previous ones at the task at hand. For
    example, transformers replaced LSTMs, and there was a boom in AI’s performance
    at text generation. Progress is made when we tailor architectures to specific
    tasks in a creative and useful way. So, current AI is about designing tailored
    solutions to each problem and not about devising a general approach that works
    on everything.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, the No Free Lunch Theorem of machine learning says, in simple terms,
    that there is no universally best architecture that is optimal for all problems
    (see David Wolpert, 1996, “The lack of a priori distinctions between learning
    algorithms,” *Neural Computation* 8.7: 1341–1390). Instead, each problem requires
    a dedicated architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, we get the impression that machines learn by themselves and that
    current AI is a general approach. In reality, we help the machine learn. And we
    help a lot.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are designed to guess the best next word that completes an input prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs subdivide inputs into valid tokens (common words or pieces of words) from
    an internal vocabulary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs calculate the probability that each possible token is the one that comes
    next after the input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A wrapper around the LLM enhances its capabilities. For examples, it makes the
    LLM eat its own output repeatedly to generate full outputs, one token at a time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Current LLMs represent information using embedding vector, which are lists of
    numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Current LLMs follow the transformer architecture, which is a method to progressively
    contextualize input tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are created using machine learning, meaning that data is used to define
    missing parameters inside the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are different types of machine learning, including supervised, self-supervised,
    and unsupervised learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In supervised learning, the computer learns by example—it is fed with examples
    of how to perform the task. In the case of self-supervised learning, these examples
    are generated automatically by scanning data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popular LLMs were first trained in a self-supervised way using publicly available
    data, and then, they were refined using manually generated data to align them
    to the users’ objectives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNNs are a popular architecture to process other types of data, such as images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNNs are combined with transformers to create multimodal AI.*******
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
