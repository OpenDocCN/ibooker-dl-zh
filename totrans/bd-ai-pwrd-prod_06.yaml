- en: Chapter 6\. Setting Goals and Measuring Success
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unpacking success in AI products can be surprisingly challenging. No single
    metric can fully capture an AI product’s impact. Instead, an understanding of
    success emerges from a balanced combination of diverse metrics working together
    to provide a comprehensive view of product health. When determining whether a
    product or feature is ready for launch, it’s crucial to consider these metrics
    strategically. Remember, there’s no one-size-fits-all recipe; the right metrics
    depend on the nature of your product, its users, and the problem it aims to solve.
  prefs: []
  type: TYPE_NORMAL
- en: To truly understand the performance of an AI feature, relying on a single metric
    will almost always fall short. The real insights come from examining multiple
    metrics in tandem. I like to think of this as an AI *product metric blend* (see
    [Figure 6-1](#ch06_figure_1_1736793247813164)). Each metric highlights a specific
    aspect of the product’s characteristics. By blending these metrics and examining
    the results, you can gain a holistic understanding of an AI feature’s performance
    and overall impact.
  prefs: []
  type: TYPE_NORMAL
- en: 'A successful AI product has three core components: product health metrics,
    system health metrics, and AI proxy metrics. This chapter explores each in detail
    to build a solid foundation for evaluating your AI product’s success, then presents
    a framework to help you craft the right OKRs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bapp_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-1\. The product metric blend (source: Dr. Marily Nika)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Product Health Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Product health metrics, such as engagement, retention, and satisfaction, fall
    squarely within your domain of responsibility as an AI PM. These metrics will
    become the cornerstone of how you monitor and optimize your product.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The lists in this and the following two sections are not comprehensive. Rather,
    they highlight the most commonly used product health metrics a PM may come across.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break down the key metrics in this category by exploring an example that
    mirrors what you might encounter in your day-to-day work. Suppose you’re the product
    manager for a (fictional) AI-driven fitness app that we’ll call FitAI, which aims
    to become the go-to solution for fitness enthusiasts. Your mission? Continuously
    monitor and optimize FitAI using a variety of product health metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Engagement
  prefs: []
  type: TYPE_NORMAL
- en: Engagement measures how actively users interact with your AI product. Frequency
    of use, session duration, and the number of interactions per session are strong
    indicators of engagement. For AI products, engagement often focuses on how frequently
    users rely on AI-driven features, such as recommendations or insights.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine your users opening the FitAI app every morning to check out their personalized,
    AI-generated workout plans, track their progress, and share their achievements.
    High engagement here means the product is providing clear value. You can track
    this by measuring usage frequency, consistency, and time spent within the app.
    For example, you might experiment by introducing a new feature that allows users
    to adjust their workout intensity based on daily performance. An increase in engagement
    following the introduction of this feature could indicate it resonates with users,
    whereas a decrease might signal areas for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: User satisfaction
  prefs: []
  type: TYPE_NORMAL
- en: While engagement is essential, it’s usually closely linked with user satisfaction.
    This qualitative metric reflects how happy users are with your AI product. High
    satisfaction often translates into loyalty and advocacy. Surveys, feedback forms,
    and [Net Promoter Scores (NPSs)](https://oreil.ly/E7dFZ) are great ways to measure
    this. For AI products, satisfaction also hinges on whether the AI meets user expectations,
    provides relevant outputs, and enhances the overall experience.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of FitAI, you may run surveys to discover that users are thrilled
    with the app’s personalized workout plans and progress tracking but find the interface
    rigid and unfriendly. This feedback can guide you in making design decisions that
    make the app more intuitive, directly impacting user satisfaction scores.
  prefs: []
  type: TYPE_NORMAL
- en: Adoption
  prefs: []
  type: TYPE_NORMAL
- en: Adoption tracks the rate at which new users start using your AI product. High
    adoption suggests that the product is gaining traction in the market. Monitoring
    sign-up rates and identifying trends can provide insights into what drives user
    adoption, like successful marketing campaigns or word of mouth.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose FitAI experiences a surge in new users after a partnership with a famous
    sports team, like the Boston Celtics. This spike in adoption confirms that the
    product is on the right track, encouraging you to explore additional collaborations
    to sustain growth.
  prefs: []
  type: TYPE_NORMAL
- en: Conversion
  prefs: []
  type: TYPE_NORMAL
- en: Conversion measures to what extent you’ve achieved your end goal. For example,
    sales bots and agents should be measured on their ability to close deals. A donation
    solicitation bot should be measured on the number of donations received after
    engagement.
  prefs: []
  type: TYPE_NORMAL
- en: Retention
  prefs: []
  type: TYPE_NORMAL
- en: Retention rates measure how well your AI product keeps users coming back over
    time. This is the metric that tests your product’s lasting value. For FitAI, retention
    would be counted when users not only download the app but also consistently use
    it to meet their fitness goals.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you notice that users who complete their first AI-recommended workout
    are 50% more likely to remain active. This insight suggests that new features
    should focus on guiding users through their initial interactions, possibly with
    gamified rewards, to boost retention. It is also important to measure churn. *Churn*
    is the flip side of retention. It’s the rate at which users stop using your product.
    Analyzing churn patterns can provide valuable insights into potential issues.
    If you notice a wave of users quitting FitAI after one month because they find
    the AI-generated workouts repetitive, it signals an opportunity to introduce customizable
    workout options. Understanding why users leave is just as crucial as knowing why
    they stay.
  prefs: []
  type: TYPE_NORMAL
- en: Financial metrics
  prefs: []
  type: TYPE_NORMAL
- en: Financial metrics such as revenue and ROI gauge the economic impact of your
    product. For FitAI, assessing the balance between costs and revenue from subscriptions
    and in-app purchases will help you make informed decisions about future investments.
  prefs: []
  type: TYPE_NORMAL
- en: Your job as an AI PM is to monitor these product health metrics continuously,
    analyze them, and implement strategies for optimization. This often means collaborating
    with your development team to address technical issues and adjusting your marketing
    strategies to boost adoption and retention. Now let’s pivot to system health metrics
    to understand how the product performs under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: System Health Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While your main focus as an AI PM might be on product health, it’s essential
    to have a grasp of system health metrics. These metrics reveal how the product
    performs at a technical level, providing insights into scalability, reliability,
    and overall performance. They include factors such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Uptime and latency
  prefs: []
  type: TYPE_NORMAL
- en: Uptime tracks how often the system is available to users, while latency measures
    the system’s response time. High uptime and low latency are vital for maintaining
    user trust. For FitAI, maximizing uptime and minimizing latency ensures a smooth
    experience as users interact with their personalized workout plans.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs: []
  type: TYPE_NORMAL
- en: Scalability becomes critical as your user base grows. By conducting load testing
    and monitoring resource usage during peak traffic, you can ensure that your AI
    system can handle increasing loads effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Error rate
  prefs: []
  type: TYPE_NORMAL
- en: Error rates track the frequency of system errors. High error rates can lead
    to user dissatisfaction and disengagement. If FitAI experiences a spike in errors
    after a new feature release, it’s a prompt to investigate and resolve bugs swiftly.
  prefs: []
  type: TYPE_NORMAL
- en: To maintain system health, routine monitoring and audits are essential. Implement
    automated alerts for key metrics and conduct stress tests regularly. Developing
    an incident response plan will help you act swiftly if issues arise.
  prefs: []
  type: TYPE_NORMAL
- en: With system health under control, let’s now focus on the AI component—the proxy
    metrics that directly impact model performance.
  prefs: []
  type: TYPE_NORMAL
- en: AI Proxy Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similarly to system health metrics, you may not have direct control over AI
    proxy metrics as an AI PM. Still, you must recognize when these metrics indicate
    a change in how the user base interacts with a product. AI proxy metrics play
    a pivotal role in assessing trade-offs and making strategic decisions, and they
    serve as a yardstick for evaluating the effectiveness of the underlying model.
    Proxy metrics focus on the integrity of the underlying models used in a product.
  prefs: []
  type: TYPE_NORMAL
- en: In ML, proxy metrics play a significant role in gauging model accuracy. The
    name *proxy* reflects that they measure the model’s performance but are different
    from the ultimate goal of the product or feature. Let me give you a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: Model Quality Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Model quality*refers to the effectiveness of a trained model in making predictions
    or decisions based on new, unseen data. It is typically assessed through various
    performance metrics that indicate how well the model’s predictions align with
    actual outcomes.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model quality is crucial for AI PMs because it directly impacts
    the reliability, efficiency, and impact of an AI product. For instance, in a healthcare
    AI application used to diagnose diseases, high precision ensures that the diagnoses
    provided by the model are correct. In contrast, high recall ensures that the model
    identifies as many true cases of the disease as possible. Poor model quality could
    lead to incorrect diagnoses, potentially harming patients and damaging the credibility
    of the healthcare provider.
  prefs: []
  type: TYPE_NORMAL
- en: As an AI PM, you must critically evaluate model performance using these metrics
    to manage AI products effectively. This involves understanding what each metric
    tells you about the model’s behavior and how to improve these metrics through
    various optimization strategies.
  prefs: []
  type: TYPE_NORMAL
- en: The AIPDL, which we’ve explored throughout this book, is inherently iterative.
    After deployment, you’ll frequently revisit earlier phases, particularly model
    training and validation, as new user data becomes available. This iterative cycle
    includes a series of experiments where you run evaluations (evals) to compare
    the performance of the live model against offline versions. These evaluations
    help you determine if a new model offers a significant improvement, guiding your
    decision on whether it’s time to launch the updated version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s cover a few of the most frequently used model quality metrics, using
    the example of a system that classifies incoming emails by whether or not they
    are spam:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs: []
  type: TYPE_NORMAL
- en: We measure the accuracy of this email classification feature by the percentage
    of correct classifications made by the system. Think of it as a test; out of all
    the emails classified, how many did the algorithm accurately identify as spam
    or nonspam?
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs: []
  type: TYPE_NORMAL
- en: Precision is the ratio of true positive results to all positive results predicted
    by the model. It determines the accuracy of positive predictions. In our spam
    example, out of all the emails the algorithm marked as spam, we want to know the
    number of emails that are truly spam. High precision indicates that the model
    has few false positives; in other words, fewer type I errors.
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity measures how good the model is at finding all the positive cases.
    Let’s use the spam example to provide some context. The sensitivity metric evaluates
    the algorithm’s success in identifying every spam email in our inbox. High sensitivity
    means you have few false negatives—in other words, fewer type II errors.
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs: []
  type: TYPE_NORMAL
- en: Recall measures the model’s ability to correctly identify all relevant positive
    cases. In the spam example, it tells us how many of the actual spam emails were
    successfully flagged by the algorithm. High recall indicates that the model misses
    very few spam emails, meaning it is effective at capturing positive cases with
    minimal false negatives. Suppose there are 100 actual spam emails in the inbox,
    and the algorithm correctly identifies 80 of them as spam; however, it misses
    20 spam emails by classifying them as nonspam. If the algorithm successfully identified
    80% of all spam emails, you would say that it had 80% recall.
  prefs: []
  type: TYPE_NORMAL
- en: Receiver operating characteristic (ROC) curve
  prefs: []
  type: TYPE_NORMAL
- en: The ROC curve is a graphical representation that illustrates the trade-off between
    the true positive rate and the false positive rate at various threshold settings.
    In the context of our spam example, the ROC curve helps evaluate how well the
    model can distinguish between spam and nonspam emails across different decision
    thresholds. A model with a curve closer to the top-left corner has better discriminatory
    power.
  prefs: []
  type: TYPE_NORMAL
- en: Objective Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Objective functions* are proxy metrics that evaluate an ML model’s performance
    during training. They measure how well the model’s predictions match the actual
    outcomes, guiding the learning process.'
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used objective functions are *loss functions*, which calculate
    the difference between the predicted and actual values for each prediction. The
    goal is to minimize this loss to improve the model’s accuracy. *Mean square error*
    (MSE), for example, is a loss function used for tasks that use regression models,
    called *regression tasks*. An example of a regression task is predicting the demand
    a store may experience for a given product. Depending on a set of features such
    as price or utility, a regression model can help predict the demand for the product.
  prefs: []
  type: TYPE_NORMAL
- en: MSE calculates the average of the squared differences between predicted and
    actual values. Imagine you are tasked with predicting the weekly sales of a new
    AI product in a popular retail store. Accurate sales predictions help you manage
    inventory effectively, cutting the losses from overstocking and stockouts. Suppose
    you predicted sales for Week 1 to be 100 units, but it was 50\. The error for
    Week 1 is 50 units. Each week represents a “loss.” The bigger the error, the higher
    the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine this happening over several weeks. The error differs each week,
    and you want to penalize weeks with a more significant error. To do this, you
    square the error. To get a meaningful metric across time, you then calculate the
    average of the squared errors and the MSE. Loss functions show how accurate a
    model’s predictions are. Workshopping an algorithm to minimize loss functions
    will enable systems to work as intended.
  prefs: []
  type: TYPE_NORMAL
- en: When you’re launching a feature or product, you aim to address specific user
    pain points. Just relying on an ML model is not sufficient. As a PM, you must
    ensure that your ML model is seamlessly integrated into a product experience so
    that users can benefit from the technology. Consider, for instance, a recommendation
    widget on Spotify that suggests new songs to enrich playlists, or a smart-matching
    feature on Tinder that connects individuals based on shared hobbies. Fusing the
    model with the user experience is the key to unlocking the true potential of this
    AI technology. By considering all of the metrics, you will get a complete picture
    of your AI feature’s success.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion Matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Confusion matrices are handy evaluation tools highlighting many critical model
    performance metrics. A *confusion matrix* is a table of actual and predicted binary
    features used to evaluate a classification algorithm’s performance. It provides
    a summary of prediction results on a classification problem. The matrix in [Table 6-1](#ch06_table_1_1736793247815966)
    compares the actual values with the values predicted by the model.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-1\. Describing the four elements of the confusion matrix for the email
    classification example
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| True positive | When a prediction is correctly classified as spam |'
  prefs: []
  type: TYPE_TB
- en: '| True negative | When a prediction is correctly classified as nonspam |'
  prefs: []
  type: TYPE_TB
- en: '| False positive | When a prediction is incorrectly classified as spam |'
  prefs: []
  type: TYPE_TB
- en: '| False negative | When a prediction is incorrectly classified as nonspam |'
  prefs: []
  type: TYPE_TB
- en: This can be applied to a feature that distinguishes between spam and nonspam
    emails in your inbox, as shown in [Figure 6-2](#ch06_figure_2_1736793247813200).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bapp_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Visual representation of four possible outcomes when AI classifies
    emails as spam or nonspam
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 6-3](#ch06_figure_3_1736793247813223) provides a visual representation
    of how the actual values and the values predicted by the model translate to the
    user experience.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bapp_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-3\. AI proxy metrics inbox (source: Dr. Marily Nika)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: OKRs for AI Products
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we’ve explored various metrics, it’s time to discuss how these metrics
    can transform into actionable OKRs. The key is to balance all three categories:
    product health, system health, and AI proxy metrics. A well-rounded OKR framework
    should include a North Star metric that captures the core value your product delivers,
    supported by other metrics that track specific aspects of performance.'
  prefs: []
  type: TYPE_NORMAL
- en: In the rest of this chapter, we will discuss how to craft effective OKRs for
    your AI product, ensuring that your team focuses on what truly matters to drive
    success. We’ll also explore examples of setting ambitious, user-focused objectives
    and defining measurable key results to monitor progress.
  prefs: []
  type: TYPE_NORMAL
- en: Tying Metrics to Goals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having a well-defined set of goals is crucial for the success of any AI product.
    By understanding and leveraging the AI product metric blend, you can craft a holistic
    view of your product’s performance and align it with user needs, technical excellence,
    and business goals. With this comprehensive framework, you’ll be well prepared
    to measure success accurately and drive impactful product development. Let’s now
    discuss building actionable OKRs that will help your team stay focused and aligned
    in this dynamic, AI-driven landscape.
  prefs: []
  type: TYPE_NORMAL
- en: 'OKRs represent your primary goal: the overarching objective you aim to achieve.
    It should be ambitious, inspirational, and aligned with the strategic vision of
    your AI product. You can have several OKRs, each addressing aspects of your AI
    product’s development and performance. Each OKR consists of multiple KPIs or quantifiable
    metrics that communicate various dimensions of how well the product is performing
    relative to your OKRs. Your KPIs should be specific, measurable, achievable, relevant,
    and time bound (SMART). These indicators help track progress and determine whether
    you are on the right path to achieving your goal. Each objective should be comprehensive
    and aligned with the company’s broader goals.'
  prefs: []
  type: TYPE_NORMAL
- en: In [“A Framework for Crafting AI Product OKRs”](#ch06_a_framework_for_crafting_ai_product_okrs_1736793247822325),
    I’ll share the framework I use for crafting OKRs for AI products. I like this
    framework because it provides a structured approach to setting and measuring goals.
    It emphasizes clarity, focus, and alignment with the product’s core values.
  prefs: []
  type: TYPE_NORMAL
- en: The North Star metric represents the important KPI. It captures and represents
    the overall value your AI product is delivering. Each OKR can have multiple key
    metrics, depending on the complexity and scope of the objective. While the North
    Star metric represents the primary goal, other supporting KPIs can help measure
    progress in specific dimensions. Despite having multiple KPIs, each OKR should
    have only one North Star metric as a precise, focused measure of success that
    encapsulates the core value the product creates. It is the primary indicator of
    success for any OKR, reflecting the ultimate impact and progress.
  prefs: []
  type: TYPE_NORMAL
- en: A Framework for Crafting AI Product OKRs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This framework incorporates a mix of metrics covered earlier in this chapter
    to provide a balanced and holistic view of progress and success. When crafting
    OKRs for AI products, you should include at least one metric from each metric
    bucket in the AI product metric cocktail: product health metrics, system health
    metrics, and AI proxy metrics. This framework helps maintain a structured and
    strategic approach to goal setting and performance measurement in AI product management.
    We always want to prioritize delivering impact and measurable value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Having a reliable OKR framework can act as a foundation. For each AI product
    or feature, you’d fill in the framework with specifics relevant to your product.
    The examples provided are just illustrative. Adjust the framework as necessary
    based on the nuances of the AI product and your organization’s strategic priorities.
    By integrating diverse metrics, you can craft well-rounded OKRs that drive your
    AI product’s development and ensure its alignment with user needs, technical excellence,
    and business goals:'
  prefs: []
  type: TYPE_NORMAL
- en: OKRs
  prefs: []
  type: TYPE_NORMAL
- en: Hereyou state the main goal for the next quarter. This should be user focused
    and explicitly stated (who is it for?) and needs to state the desired outcome
    (i.e., enhance the user experience by providing more personalized music recommendations).
  prefs: []
  type: TYPE_NORMAL
- en: Specific features
  prefs: []
  type: TYPE_NORMAL
- en: What features or changes will you introduce to achieve the objective?
  prefs: []
  type: TYPE_NORMAL
- en: North Star (KPI)
  prefs: []
  type: TYPE_NORMAL
- en: What is the primary metric showcasing product success? While this is typically
    a singular, focused metric, teams may use supporting metrics to provide additional
    context.
  prefs: []
  type: TYPE_NORMAL
- en: Product health metrics (KPIs)
  prefs: []
  type: TYPE_NORMAL
- en: Which metrics will measure user satisfaction or the product’s health? Consider
    multiple relevant metrics such as retention rate, user satisfaction surveys, or
    feature adoption rates.
  prefs: []
  type: TYPE_NORMAL
- en: Guardrail metrics (KPIs)
  prefs: []
  type: TYPE_NORMAL
- en: What potential adverse side effects or risks do you want to monitor and minimize?
    Use a combination of metrics to track these risks effectively, such as error rates,
    response times, or user complaints.
  prefs: []
  type: TYPE_NORMAL
- en: System health metrics (KPIs)
  prefs: []
  type: TYPE_NORMAL
- en: How will you ensure that the tool or feature remains reliable and performant?
    Include multiple metrics to measure aspects such as system uptime, latency, or
    resource utilization.
  prefs: []
  type: TYPE_NORMAL
- en: AI proxy metrics (KPIs)
  prefs: []
  type: TYPE_NORMAL
- en: Which AI-specific metrics will you track to assess algorithm performance? Consider
    metrics such as model accuracy, precision, recall, or user engagement with AI-driven
    features.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 6-2](#ch06_table_2_1736793247816002) uses this framework to provide
    a detailed hypothetical OKR example for a streaming music service’s ​recommendation
    system.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-2\. Example OKRs for a streaming music service’s recommendation system
  prefs: []
  type: TYPE_NORMAL
- en: '| Component | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Objective | Enhance the user experience by providing more personalized music
    recommendations. |'
  prefs: []
  type: TYPE_TB
- en: '| Specific feature | Introduce three new personalization algorithms based on
    user behavior, mood, and music trends. |'
  prefs: []
  type: TYPE_TB
- en: '| North Star metric (KPI) | Increase user engagement with recommended playlists
    by 25%. |'
  prefs: []
  type: TYPE_TB
- en: '| Product health metric (KPI) | Reduce the number of users skipping songs within
    AI-generated playlists by 20%. |'
  prefs: []
  type: TYPE_TB
- en: '| Guardrail metric (KPI) | Ensure that the overall time spent listening to
    the music does not decrease by more than 5%. |'
  prefs: []
  type: TYPE_TB
- en: '| System health metric (KPI) | Maintain 99% system uptime, and reduce playlist
    loading times to under one second. |'
  prefs: []
  type: TYPE_TB
- en: '| AI proxy metric (KPI) | Increase the precision of the recommendation algorithm
    by 15%. |'
  prefs: []
  type: TYPE_TB
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having a well-defined set of goals is essential for the success of any AI product.
    I hope the frameworks in this chapter for evaluating product success provide a
    structured approach to achieving your and your team’s goals. There are always
    nuanced metrics that apply to specific situations, so it may take time to find
    the ones that work best for you and your team. That said, the framework for setting
    actionable goals and defining success will be similar across different projects.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the relevant metrics to add to your AI product metrics blend will
    help your teams capture a holistic view of the product’s success. Defining specific
    objectives and OKRs can help AI teams focus on what truly matters and track their
    progress with precision. Identifying the right metrics and concentrating on a
    small set of crucial objectives will help teams maintain clarity and drive toward
    the most critical outcomes. Setting well-defined goals and measuring success accurately
    through a robust framework will drive progress and help align AI product development
    with broader business objectives. Using these frameworks will allow teams to understand
    and address the multifaceted nature of AI product performance.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 7](ch07.html#ch07_ai_tools_for_product_managers_1736793248032450),
    I will cover the most popular product management tools I’ve come across as an
    AI product manager.
  prefs: []
  type: TYPE_NORMAL
