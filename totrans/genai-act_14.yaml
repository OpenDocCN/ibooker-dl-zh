- en: 12 Evaluations and benchmarks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 评估和基准
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding the significance of benchmarking and evaluating LLMs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基准测试和评估LLMs的重要性
- en: Learning different evaluation metrics
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习不同的评估指标
- en: Benchmarking model performance
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准测试模型性能
- en: Implementing comprehensive evaluation strategies
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施全面的评估策略
- en: Best practices for evaluation benchmarks and key evaluation criteria to consider
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估基准的最佳实践和需要考虑的关键评估标准
- en: Taking into account the recent surge of interest in GenAI and specifically in
    large language models (LLMs), it’s crucial to approach these novel and uncertain
    features cautiously and responsibly. Many leaderboards and studies have shown
    that LLMs can match human performance in various tasks, such as taking standardized
    tests or creating art, sparking enthusiasm and attention. However, their novelty
    and uncertainties necessitate careful handling.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到近期对通用人工智能（GenAI）以及大型语言模型（LLMs）的兴趣激增，谨慎和负责任地处理这些新颖且不确定的特性至关重要。许多排行榜和研究表明，LLMs在各种任务中可以匹配人类的表现，例如参加标准化测试或创作艺术，引发了热情和关注。然而，它们的创新性和不确定性需要谨慎处理。
- en: The role of benchmarking LLMs in production deployment cannot be overstated.
    It involves evaluating performance, comparing models, guiding improvements, accelerating
    technological advancement, managing costs and latency, and ensuring efficient
    task flow for real-world applications. While evaluations are part of LLMOps, their
    criticality in ensuring LLMs meet the demands of various applications warrants
    a separate discussion in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产部署中基准测试大型语言模型（LLMs）的作用不容小觑。它涉及评估性能、比较模型、指导改进、加速技术进步、管理成本和延迟，并确保现实应用中任务流的效率。虽然评估是LLMOps的一部分，但它们在确保LLMs满足各种应用需求中的关键性，使得本章需要单独讨论。
- en: Evaluating LLMs is not a simple task but a complex and multifaceted process
    that demands quantitative and qualitative approaches. When evaluating LLMs, comprehensive
    assessment methods covering various aspects of model performance and effect must
    be employed. Stanford University’s Human-Centered Artificial Intelligence (HAI)
    publishes an annual AI Index report [1] that aims to collate and track different
    data points related to AI. One of the most significant challenges we face is the
    lack of standardized evaluations, which makes a systematic comparison between
    different models incredibly difficult when it comes to capabilities and potential
    risks and harms. This means we don’t have an objective measure of how good or
    smart any of these specific models are, which underscores the complexity and importance
    of the evaluation process.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 评估LLMs并非一项简单任务，而是一个复杂且多方面的过程，需要定量和定性的方法。在评估LLMs时，必须采用涵盖模型性能和效果的各个方面进行全面评估的方法。斯坦福大学的人本人工智能（HAI）每年发布一份AI指数报告[1]，旨在收集和跟踪与AI相关的不同数据点。我们面临的最具挑战性的问题之一是缺乏标准化的评估，这使得在能力和潜在风险及危害方面对不同模型进行系统比较变得极其困难。这意味着我们没有客观的衡量标准来衡量这些特定模型的好坏或智能程度，这突显了评估过程的复杂性和重要性。
- en: When we discuss GenAI evaluations in this initial stage, most discussions concern
    accuracy and performance evaluations that assess how well a language model can
    comprehend and produce text that resembles human language. This aspect is very
    important for applications that rely on the quality and relevance of the content
    they generate, such as chatbots, content creation, and summarization tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在这一初始阶段讨论通用人工智能（GenAI）评估时，大多数讨论都集中在准确性和性能评估上，这些评估旨在衡量语言模型理解并产生类似人类语言文本的能力。这一方面对于依赖于生成内容质量和相关性的应用非常重要，例如聊天机器人、内容创作和摘要任务。
- en: 'There are three general types of evaluations that can measure accuracy and
    performance: traditional evaluation metrics that judge language quality, LLM task-specific
    benchmarks for assessing specific tasks, and human evaluations. Let’s start by
    understanding what LLM evaluations are and learn about some of the best practices
    associated with evaluations.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种一般类型的评估可以衡量准确性和性能：判断语言质量的传统评估指标、评估特定任务的LLM特定基准，以及人工评估。让我们首先了解LLM评估是什么，并了解一些与评估相关的最佳实践。
- en: 12.1 LLM evaluations
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 LLM评估
- en: 'It is essential to evaluate LLMs to ensure they are reliable and appropriate
    for real-world applications. A strong evaluation strategy covers performance metrics
    such as accuracy, fluency, coherence, and relevance. These metrics help us to
    understand the model’s advantages and disadvantages across different contexts.
    I summarize here a few areas as best practices to consider when evaluating LLMs:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 评估LLM至关重要，以确保它们可靠且适用于实际应用。一个强大的评估策略涵盖了性能指标，如准确性、流畅性、连贯性和相关性。这些指标帮助我们了解模型在不同情境下的优缺点。以下是我总结的几个作为最佳实践考虑的领域：
- en: To evaluate the LLM meaningfully, it must be tested on the use cases it is designed
    for, meaning using the model on various natural language processing (NLP) tasks,
    such as summarization, question-answering, and translation. The evaluation process
    should use standard metrics such as ROUGE (Recall-Oriented Understudy for Gisting
    Evaluation) for summarization to maintain reliability and comparability.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了对LLM进行有意义的评估，必须在其设计的使用案例上进行测试，这意味着在多种自然语言处理（NLP）任务上使用模型，例如摘要、问答和翻译。评估过程应使用标准指标，如ROUGE（Recall-Oriented
    Understudy for Gisting Evaluation）进行摘要，以保持可靠性和可比性。
- en: Another important aspect of LLM evaluation is the creation of prompts. Prompts
    must be unambiguous and fair, providing a valid assessment of the model’s abilities.
    This ensures that the evaluation outcomes reflect the model’s actual performance.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM评估的另一个重要方面是提示（prompts）的创建。提示必须明确且公平，以提供对模型能力的有效评估。这确保了评估结果反映了模型的实际性能。
- en: Benchmarking is a crucial practice that enables evaluating an LLM’s performance
    based on existing criteria and other models. This not only tracks progress but
    also identifies areas requiring improvement. A continuous evaluation process,
    combined with constant development practices, allows for periodic assessment and
    refinement of the LLM.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准测试（benchmarking）是一项关键实践，它使人们能够根据现有标准和其它模型评估LLM的性能。这不仅跟踪了进展，还确定了需要改进的领域。结合持续的开发实践，持续的评估过程允许定期评估和改进LLM。
- en: The evaluation of LLMs must involve ethical considerations at every step. The
    process must check the model for biases, fairness, and ethical problems, looking
    at the training data and the outputs. Moreover, the user experience should be
    a key part of the evaluation, ensuring that the model’s outputs match user needs
    and expectations.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的评估必须在每个步骤都涉及伦理考量。该过程必须检查模型是否存在偏见、公平性和伦理问题，并关注训练数据和输出。此外，用户体验应该是评估的关键部分，确保模型输出符合用户需求和期望。
- en: The evaluation must be transparent at every stage. Recording the criteria, methods,
    and results allows for independent verification and increases confidence in the
    LLM’s abilities. Finally, the evaluation outcomes should inform a continuous improvement
    cycle, improving the model, training data, and the evaluation process based on
    performance measures and feedback.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估必须在每个阶段都是透明的。记录标准、方法和结果允许独立验证，并增加对LLM能力的信心。最后，评估结果应指导持续改进周期，根据性能指标和反馈改进模型、训练数据和评估过程。
- en: These practices underscore the importance of a rigorous and systematic approach
    to evaluating LLMs, ensuring that they are accurate but also fair, ethical, and
    suitable for various applications.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实践强调了以严格和系统的方法评估LLM的重要性，确保它们准确但同时也公平、道德且适用于各种应用。
- en: By following these practices, enterprises can conduct reliable and effective
    evaluations, developing trustworthy and helpful LLMs for different uses. Now that
    we know what evaluations are, let’s take a look at some metrics we should use.
    They can be categorized into traditional and newer LLM-specific evaluation metrics.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这些实践，企业可以开展可靠有效的评估，开发出值得信赖且有助于不同用途的LLM。既然我们已经了解了评估是什么，让我们来看看我们应该使用的某些指标。它们可以分为传统和较新的LLM特定评估指标。
- en: 12.2 Traditional evaluation metrics
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 传统评估指标
- en: BLEU (Bilingual Evaluation Understudy), ROUGE (Recall-Oriented Understudy for
    Gisting Evaluation), and BERTScore (BERT Similarity Score) are some of the more
    standardized metrics. These metrics help quantify the linguistic quality of model
    outputs against reference texts and are used to evaluate text quality in tasks
    such as machine translation or text summarization. Still, they differ in their
    approaches and focus on different aspects of the text. Table 12.1 shows a detailed
    explanation of what each of the three scores indicates. We will show how to compute
    these in the next section.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU（双语评估助手）、ROUGE（基于回忆的摘要评估助手）和BERTScore（BERT相似度得分）是一些较为标准化的度量标准。这些度量标准有助于量化模型输出与参考文本之间的语言质量，并用于评估机器翻译或文本摘要等任务中的文本质量。尽管如此，它们在方法和关注的文本方面有所不同。表12.1展示了这三个分数的详细解释。我们将在下一节中展示如何计算这些度量。
- en: Table 12.1 Traditional evaluation metrics
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表12.1 传统评估度量
- en: '| Metric | Focus | Method | Limitations |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 度量标准 | 重点关注 | 方法 | 局限性 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| BLEU  | It primarily measures precision, the percentage of words in the machine-
    generated text that appear in the reference text.  | It compares n-grams (word
    sequences) of the candidate translation with the reference translation and counts
    the matches.  | It can miss the mark on semantic meaning because it doesn’t account
    for synonyms or the context of words. It also doesn’t handle word reordering well.  |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| BLEU  | 它主要衡量精确度，即机器生成文本中出现在参考文本中的单词百分比。  | 它将候选翻译的n-gram（单词序列）与参考翻译进行比较，并计算匹配项。  |
    由于它没有考虑同义词或单词的上下文，因此它可能会错过语义意义。它还处理不好单词重排。  |'
- en: '| ROUGE  | It is more recall oriented, focusing on the percentage of words
    from the reference text that appear in the generated text.  | It has several variants,
    such as ROUGE-N, which compares n-grams, and ROUGE-L, which looks at the longest
    common subsequence.  | Like BLEU, ROUGE can overlook semantic similarities and
    paraphrasing because it’s based on exact word matches.  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE  | 它更侧重于回忆，关注参考文本中出现在生成文本中的单词百分比。  | 它有几个变体，如ROUGE-N，它比较n-gram，以及ROUGE-L，它查看最长公共子序列。  |
    与BLEU一样，ROUGE可能会忽略语义相似性和释义，因为它基于精确的词匹配。  |'
- en: '| BERTScore  | It evaluates semantic similarity rather than relying on exact
    word matches.  | It uses contextual embeddings from models such as BERT to represent
    the text and calculates the cosine similarity between these embeddings.  | It
    can capture paraphrasing and semantic meaning better than BLEU and ROUGE because
    it considers each word’s context.  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| BERTScore  | 它评估语义相似性，而不是依赖于精确的词匹配。  | 它使用BERT等模型生成的上下文嵌入来表示文本，并计算这些嵌入之间的余弦相似度。  |
    由于它考虑了每个词的上下文，因此它比BLEU和ROUGE更好地捕捉到释义和语义意义。  |'
- en: Metrics such as ROUGE, BLEU, and BERTScore compare the similarities between
    text generated by an LLM and reference text written by humans. They are commonly
    used for evaluating tasks such as summarization and machine translation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 比如ROUGE、BLEU和BERTScore等度量标准，它们比较由LLM生成的文本和人类编写的参考文本之间的相似性。它们通常用于评估摘要和机器翻译等任务。
- en: 12.2.1 BLEU
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.1 BLEU
- en: BLEU (Bilingual Evaluation Understudy) [2] is an algorithm used to evaluate
    the quality of machine-translated text from one natural language to another. Its
    central idea is to measure the correspondence between a machine’s output and that
    of a human translator. In other words, according to BLEU, the closer a machine
    translation is to a professional human translation, the better it is. BLEU does
    not consider intelligibility or grammatical correctness; it focuses on content
    overlap.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU（双语评估助手）[2]是一种用于评估从一种自然语言到另一种自然语言的机器翻译质量的算法。其核心思想是衡量机器输出与人类翻译之间的对应关系。换句话说，根据BLEU，机器翻译越接近专业的人类翻译，质量就越好。BLEU不考虑可理解性或语法正确性；它关注内容重叠。
- en: 12.2.2 ROUGE
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.2 ROUGE
- en: 'ROUGE (Recall-Oriented Understudy for Gisting Evaluation) [3] is a set of measures
    used in NLP to assess how well automatic text summarization and machine translation
    perform. Its main goal is to contrast summaries or translations produced by machines
    with human reference summaries. It evaluates the following aspects:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE（基于回忆的摘要评估助手）[3]是一组用于NLP中评估自动文本摘要和机器翻译性能的度量标准。其主要目标是对比机器生成的摘要或翻译与人类参考摘要。它评估以下方面：
- en: '*Recall*—ROUGE measures how much of the reference summary the system summary
    captures. It evaluates how well the system recovers or captures content from the
    reference.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*召回率*——ROUGE衡量系统摘要捕获了多少参考摘要的内容。它评估系统恢复或捕获参考内容的效果。'
- en: '*Precision*—It also assesses how much of the system summary is relevant, needed,
    or useful.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*精确度*——它还评估系统摘要中有多少是相关、必要或有用的。'
- en: '*F-measure*—It combines precision and recall to provide a balanced view of
    system performance.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*F度量*——它结合精确度和召回率，提供一个平衡的系统性能视图。'
- en: ROUGE has different versions, such as ROUGE-N (which uses n-grams) and ROUGE-L
    (based on the Longest Common Subsequence algorithm). By looking at single words
    and sequences, ROUGE helps us measure the effectiveness of NLP algorithms in summarization
    and translation tasks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE有不同的版本，如ROUGE-N（使用n-gram）和ROUGE-L（基于最长公共子序列算法）。通过观察单个单词和序列，ROUGE帮助我们衡量NLP算法在摘要和翻译任务中的有效性。
- en: However, ROUGE has limitations. It relies solely on surface-level overlap and
    doesn’t account for semantic meaning or fluency. Sensitivity to stop words, stemming,
    and word order can affect scores. While ROUGE provides valuable insights, it’s
    essential to consider other evaluation metrics and human judgment to assess summary
    quality comprehensively. Researchers often use a combination of metrics to evaluate
    summarization models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，ROUGE存在局限性。它仅依赖于表面层的重叠，不考虑语义意义或流畅性。对停用词、词干提取和词序的敏感性会影响分数。虽然ROUGE提供了有价值的见解，但考虑其他评估指标和人类判断来全面评估摘要质量是至关重要的。研究人员通常使用多种指标来评估摘要模型。
- en: 12.2.3 BERTScore
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.3 BERTScore
- en: 'BERTScore [4] is a measure of how good text generation is. It uses pretrained
    BERT model embeddings to compare candidate and reference sentences. The idea is
    to find similar words in the candidate and reference sentences based on cosine
    similarity. This metric agrees with human opinion in sentence- and system-level
    evaluations. It has the following elements:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: BERTScore [4] 是衡量文本生成质量的一个指标。它使用预训练的BERT模型嵌入来比较候选句和参考句。其思路是基于余弦相似性，在候选句和参考句中找到相似单词。这个指标在句子级和系统级评估中与人类意见一致。它包含以下要素：
- en: '*Contextual embeddings*—BERTScore represents both the candidate and reference
    sentences with embeddings that consider each word’s context.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*上下文嵌入*——BERTScore使用考虑每个单词上下文的嵌入来表示候选句和参考句。'
- en: '*Cosine similarity*—It calculates the cosine similarity between the embeddings
    of the candidate and reference texts.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*余弦相似度*——它计算候选文本和参考文本嵌入之间的余弦相似度。'
- en: '*Token matching*—To compute precision and recall scores, each token in the
    candidate text matches the most similar token in the reference text.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标记匹配*——为了计算精确度和召回率分数，候选文本中的每个标记都与参考文本中最相似的标记进行匹配。'
- en: '*F1 score*—The precision and recall are combined to calculate the F1 score,
    providing a single quality measure.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*F1分数*——将精确度和召回率结合起来计算F1分数，提供一个单一的质量度量。'
- en: The key advantage of BERTScore over traditional metrics such as BLEU is its
    ability to capture semantic similarity. This means it can recognize when different
    words have similar meanings and when the same words are used in different contexts.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: BERTScore相对于传统的指标如BLEU的关键优势在于其捕捉语义相似性的能力。这意味着它能够识别不同单词具有相似含义的情况，以及相同单词在不同语境中的使用。
- en: 12.2.4 An example of traditional metric evaluation
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.4 传统指标评估的示例
- en: Let’s bring it all together and make it real through a simple example. Here
    we have two information summaries and can evaluate which one might be better.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子将所有这些内容结合起来，使其变得真实。这里我们有两个信息摘要，可以评估哪一个可能更好。
- en: For this example, we take the AI development principles of the Bill and Melinda
    Gates Foundation as the article we want to analyze and understand. This article
    is available at [https://mng.bz/vJe4](https://mng.bz/vJe4). From the article,
    we create two summaries that we’ll compare. In this case, one is created by NLTK
    and the other by another LLM (GPT-3.5). This could also be two different human-written
    versions or any other combination. We use the `newspaper3K` and `bert_score` packages
    to download the article and the Hugging Face Evaluate package for the evaluations.
    These can be installed in conda using `conda` `install` `-c` `conda-forge` `newspaper3k`
    `evaluate` `bert_score`. In pip, use `pip install evaluate newspaper3k bert_score`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: We use `newspaper3k` to download and parse the article first. Then we apply
    the `nlp()` function to process the article and get the summary from the summary
    property. We must ensure the article is downloaded and parsed before using NLP;
    note that this only works for Western languages. We use the summary created by
    NLP as our reference summary and the `Evaluate` library to calculate the specific
    metrics. The listing shows the code to implement this.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.1 Automated evaluation metrics
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Sets up the OpenAI details'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Function to download and parse the article; returns both the article text
    and a summary'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Summarizes the article using OpenAI'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Function to calculate metrics (BLEU, ROUGE, etc.)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Configures newspaper3k to allow downloading articles'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the output we can observe when executing the code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we have seen, the BLEU score is composed of several components that collectively
    assess the quality of a machine-generated translation against a set of reference
    translations. Let’s examine each component and see what it means, starting with
    the BLEU score outlined in table 12.2\. Tables 12.3 and 12.4 show the results
    for the ROUGE score and the BERT score, respectively.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Table 12.2 BLEU score
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Component value | Meaning |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
- en: '| BLEU: 0.047 (4.7%)  | This is the overall BLEU score, which is quite low.
    BLEU scores range from 0 to 1 (or 0% to 100%), with higher scores indicating better
    translation quality. A score below 10% is generally considered poor.  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
- en: '| Precisions  | These are the n-gram precision scores for 1-gram, 2-gram, 3-gram,
    and 4-gram matches. Our scores indicate a decent number of 1-gram matches but
    few longer matches, suggesting that the translation has some correct words but
    lacks coherent phrases and sentences.  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
- en: '| Brevity penalty: 1.0  | This means there was no penalty for brevity; the
    translation length was appropriate compared to the reference length.  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '| Length ratio: 1.27  | The translation is 27% longer than the reference, which
    might suggest some verbosity.  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| Translation length: 140  | The length of the machine-translated text  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '| Reference length: 110  | The length of the reference text  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: Table 12.3 ROUGE score
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Component value | Meaning |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| ROUGE-1: 0.3463 (34.63%)  | It measures the overlap of 1-gram between the
    system output and the reference summary. A moderate score indicates a fair amount
    of overlap.  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| ROUGE-2 : 0.0961 (9.61%)  | It measures the overlap of bigrams and is a stricter
    metric than ROUGE-1\. A low score suggests that the system struggles to form accurate
    phrases.  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| ROUGE-L: 0.1645 (16.45%)  | It measures the longest common subsequence, indicating
    the fluency and order of the words. The score suggests limited fluency.  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| ROUGE-Lsum: 0.2684 (26.84%)  | It is similar to ROUGE-L but considers the
    sum of the longest common subsequences, indicating a slightly better grasp of
    the content structure.  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: Table 12.4 BERT Score
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Component value | Meaning |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: '| Precision: 0.8524 (85.24%)  | It measures how many words in the candidate
    text are relevant or needed.  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: '| Recall: 0.8710 (87.10%)  | It measures how much of the candidate text captures
    the reference content.  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: '| F1 Score: 0.8616 (86.16%)  | This harmonic mean of precision and recall provides
    a single score that balances both. An F1 score closer to 1 indicates better performance.  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: The BLEU and ROUGE scores suggest that the translation or summary has room for
    improvement, particularly in forming coherent phrases and sentences. However,
    the BERT score is quite high, indicating that the candidate text is semantically
    similar to the reference text and captures most of its content. Thus, while the
    translation may not match the reference text word for word, it does convey the
    same overall meaning quite well.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Even though metrics such as BERTScore, ROUGE, and BLEU help compare similar
    text, they primarily focus on surface-level similarity. They may not capture semantic
    equivalence or the overall quality of the generated text. These more traditional
    metrics often penalize LLMs, which can produce coherent and fluent generations.
    For these, we need LLM task-specific benchmarks.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 LLM task-specific benchmarks
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Measuring the performance of LLMs across various NLP tasks requires task-specific
    benchmarks. They are created to test how well the models can understand, reason,
    and generate natural language for specific domains or tasks, providing a clear
    way to compare different models. These benchmarks can reveal a model’s abilities
    and limitations, enabling focused improvements.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Task-specific benchmarks assess LLMs on specific NLP tasks such as text classification,
    sentiment analysis, question answering, summarization, and more. These benchmarks
    usually consist of datasets with predefined inputs and expected outputs, allowing
    for quantitative assessment of model performance through metrics such as accuracy,
    F1 score, or BLEU score, depending on the task. Some key LLM benchmarks are groundedness,
    relevance, coherence, fluency, and GPT similarity; these evaluation metrics are
    outlined in table 12.5.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Table 12.5 LLM evaluation metrics
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Metric | Focus | Method | When to use? |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
- en: '| Groundedness  | It evaluates how well the answers the model produces match
    the information in the source data (context that the user provides). This metric
    ensures that the context backs up the answers generated by AI.  | It evaluates
    how well the statements in an AI-generated answer match the source context, ensuring
    that the context supports these statements. It is rated from 1 (bad) to 5 (good).  |
    It is used when we want to check that the AI responses match and are confirmed
    by the given context. It is also used when being factually correct and contextually
    precise is important, such as when finding information, answering questions, and
    summarizing content.  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 基础性 | 它评估模型生成的答案与源数据（用户提供的上下文）中的信息匹配得有多好。这个指标确保 AI 生成的答案有上下文支持。 | 它评估 AI
    生成的答案中的陈述与源上下文匹配得有多好，确保上下文支持这些陈述。评分从 1（差）到 5（好）。 | 当我们想要检查 AI 响应与给定上下文匹配并得到确认时使用它。当在查找信息、回答问题和总结内容时，确保事实正确和上下文精确很重要时也使用它。
    |'
- en: '| Coherence  | It evaluates the model’s ability to generate coherent, natural
    output similar to human language.  | It evaluates how well the generation is structured
    and connected. This is rated from 1 (bad) to 5 (good).  | Use it when evaluating
    how easy and user-friendly your model’s generated responses are in real-world
    situations.  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 连贯性 | 它评估模型生成与人类语言相似、连贯的自然输出的能力。 | 它评估生成的结构化和连接性如何。这个评分从 1（差）到 5（好）。 | 在评估模型在实际场景中生成的响应的易用性和用户友好性时使用它。
    |'
- en: '| Fluency  | It measures the grammar proficiency and readability of a model’s
    generated response.  | The fluency measure evaluates how well the generated text
    follows grammatical rules, syntactic structures, and suitable word choices. It
    is scored from 1 (bad) to 5 (good).  | This tool assesses the linguistic accuracy
    of the generated text, ensuring that it follows appropriate grammar rules, syntax
    structures, and word choices.  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 流畅性 | 它衡量模型生成响应的语法熟练度和可读性。 | 流畅性衡量评估生成的文本如何遵循语法规则、句法结构和合适的词汇选择。评分从 1（差）到
    5（好）。 | 该工具评估生成文本的语言准确性，确保其遵循适当的语法规则、句法结构和词汇选择。 |'
- en: '| GPT similarity  | It compares how similar a source data (ground truth) sentence
    is to the output from an AI model.  | This assessment involves creating sentence-level
    embeddings for both the ground truth and the model’s prediction, which are high-dimensional
    vector representations that encode the semantic meaning and context of the sentences.  |
    Use it to get an unbiased measure of a model’s performance, especially in text
    generation tasks where we have the correct responses available. This lets us check
    how closely the generated text matches the intended content, which helps us evaluate
    the model’s quality and accuracy.  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| GPT 相似度 | 它比较源数据（真实情况）句子与 AI 模型输出句子的相似度。 | 这种评估涉及为真实情况和模型预测创建句子级嵌入，这些是高维向量表示，编码了句子的语义意义和上下文。
    | 使用它来获得模型性能的无偏测量，特别是在我们有正确响应的文本生成任务中。这让我们可以检查生成的文本与预期内容匹配的紧密程度，有助于我们评估模型的质量和准确性。
    |'
- en: To illustrate how this works, we will apply a reference-free evaluation method
    based on the G-Eval method. Reference-free means that we do not depend on comparing
    a generated summary to a preexisting reference summary. Let’s start with understanding
    G-Eval.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明其工作原理，我们将应用基于 G-Eval 方法的无参考评估方法。无参考意味着我们不需要将生成的摘要与现有的参考摘要进行比较。让我们先了解 G-Eval。
- en: '12.3.1 G-Eval: A measuring approach for NLG evaluation'
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.1 G-Eval：NLG 评估的测量方法
- en: G-Eval [5] introduces a new framework for measuring the quality of text produced
    by NLG systems. Using LLMs, G-Eval combines a chain-of-thought–prompting method
    with a form-filling technique to examine different aspects of the NLG output,
    such as coherence, consistency, and relevance. G-Eval judges the quality of the
    generated content based on the input prompt and text alone, without any reference
    texts, and is thus considered reference free.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: G-Eval [5] 提出了一种用于衡量自然语言生成（NLG）系统生成文本质量的新框架。利用大型语言模型（LLMs），G-Eval 结合了思维链提示方法与表格填写技术，以检查
    NLG 输出的不同方面，例如连贯性、一致性和相关性。G-Eval 仅根据输入提示和文本本身来判断生成内容的品质，无需任何参考文本，因此被认为是无参考的。
- en: 'The method is particularly useful for novel datasets and tasks with few human
    references available. This flexibility makes G-Eval suitable for various innovative
    applications, especially in fields where data is continuously evolving or is highly
    specific. Here are a few scenarios where G-Eval would be beneficial:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法特别适用于新颖的数据集和人类参考较少的任务。这种灵活性使 G-Eval 适用于各种创新应用，尤其是在数据持续演变或高度具体的领域。以下是 G-Eval
    将有益的几个场景：
- en: '*Medical report generation*—In the medical domain, where automated systems
    produce customized reports from various patient data, G-Eval can evaluate the
    reports for correctness, consistency, and medical relevance. As patient scenarios
    differ a lot, conventional reference-based metrics might not always work, making
    G-Eval a more adaptable and appropriate option that guarantees the quality and
    dependability of medical reports.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*医疗报告生成*—在医疗领域，自动化的系统从各种患者数据中生成定制报告时，G-Eval 可以评估报告的正确性、一致性和医学相关性。由于患者情况差异很大，基于参考的常规指标可能并不总是适用，这使得
    G-Eval 成为一个更适应性和合适的选项，保证了医疗报告的质量和可靠性。'
- en: '*Legal document writing*—When AI creates legal documents that suit particular
    cases, G-Eval assesses how well the documents meet legal requirements, how clear
    and coherent they are, and how well they follow the rules. This is important in
    legal situations where having precise reference texts for every situation is not
    feasible, but accuracy and conformity to legal standards are vital.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*法律文件撰写*—当AI创建适合特定案例的法律文件时，G-Eval 评估这些文件在满足法律要求、清晰性和连贯性以及遵循规则方面的表现。在无法为每种情况都提供精确参考文本的法律情况下，这一点尤为重要，但准确性和符合法律标准至关重要。'
- en: '*Creative content evaluation*—Novelty is essential in fields that require creativity,
    such as advertising or video game storytelling. G-Eval helps assess the novelty,
    appeal, and target audience suitability of such content, providing a way to gauge
    the quality of creativity that is more than just word or phrase similarity.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*创意内容评估*—在需要创造力的领域，如广告或视频游戏叙事，新颖性至关重要。G-Eval 帮助评估此类内容的创新性、吸引力和目标受众的适用性，提供了一种衡量创造力的质量的方法，这不仅仅是词或短语相似性的衡量。'
- en: '*AI-based content moderation*—G-Eval can help verify that moderation actions
    are suitable and successful, even when there is no reliable reference data, by
    using AI systems to moderate changing online content. This is especially important
    in online settings where context and sensitivity matter.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于AI的内容审核*—G-Eval 可以通过使用AI系统来审核不断变化的在线内容，即使在没有可靠参考数据的情况下，也能帮助验证审核措施是否合适且成功。这在需要考虑上下文和敏感性的在线环境中尤为重要。'
- en: 'These examples show how G-Eval can assess the quality of AI-generated text
    with human-like standards and flexibility to meet different needs. This is important
    for GenAI applications where conventional metrics are insufficient. G-Eval has
    many advantages for businesses that want to develop and use effective NLG solutions:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子展示了 G-Eval 如何以类似人类的标准和灵活性评估人工智能生成文本的质量，这对于传统指标不足的 GenAI 应用来说非常重要。G-Eval
    对于希望开发和使用有效自然语言生成解决方案的企业具有许多优势：
- en: G-Eval shows much better agreement with human evaluation than conventional metrics
    such as BLEU and ROUGE. This is especially clear in open-ended and creative NLG
    tasks, where conventional metrics often fail. By giving a more precise measurement
    of NLG system quality, G-EVAL helps enterprises make smart choices about their
    development and deployment.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: G-Eval 与人类评估的协议度比传统的指标如 BLEU 和 ROUGE 要好得多。这在开放式和创造性的自然语言生成任务中尤为明显，因为传统的指标往往失效。通过更精确地衡量自然语言生成系统的质量，G-EVAL
    帮助企业就其开发和部署做出明智的选择。
- en: G-Eval uses the probabilities of output tokens from LLMs to produce fine-grained
    continuous scores. This enables the capture of slight differences between generated
    texts, giving more detailed feedback than traditional metrics that often depend
    on discrete scoring. Such precise feedback can be very helpful for enterprises
    as they adjust their NLG systems for the best performance.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: G-Eval 使用来自大型语言模型（LLM）的输出标记的概率来生成细粒度的连续分数。这能够捕捉到生成文本之间的细微差异，比传统指标提供更详细的反馈，而传统指标往往依赖于离散评分。这种精确的反馈对于企业在调整其自然语言生成系统以获得最佳性能时非常有帮助。
- en: An interesting feature of G-Eval is that it can be customized to evaluate different
    NLG tasks by changing the prompt and evaluation criteria. This flexibility removes
    the need for task-specific evaluators, making the evaluation process easier for
    enterprises working with various NLG applications.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, a possible problem with LLM-based evaluators is that they may prefer
    text generated by LLMs. This problem needs more research and solutions to ensure
    reliable and correct evaluation, especially when using LLM-based metrics to enhance
    NLG systems.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: G-Eval provides a potential method for NLG evaluation in enterprises, which
    can help create and use more efficient and dependable NLG systems for different
    purposes. Let’s see how we can use this.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'We can demonstrate how G-Eval can be very helpful with a simple example. Imagine
    an enterprise that wants to evaluate customer service chatbots. These chatbots
    are usually trained to deal with many kinds of customer questions and problems,
    and their performance is essential for maintaining customer satisfaction and loyalty.
    For example, let’s think about a customer complaint about a service. Suppose a
    customer writes the following complaint in an email:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: I am extremely disappointed with the delay in service. I was promised a two-day
    delivery, and it’s already been a week without any updates. This is unacceptable.
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Now imagine two different automated responses generated by customer service
    bots:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '*Response A (more literal and generic)*—“We apologize for any inconvenience
    caused. Your complaint has been registered. We will update you shortly.”'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Response B (more empathetic and specific)*—“We’re really sorry to hear about
    this delay and completely understand your frustration. It’s not the experience
    we want to provide. Our team is looking into this as a priority, and we’ll reach
    out with an update on your delivery by tomorrow morning.”'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conventional metrics such as BLEU and ROUGE would likely assess these responses
    based on how closely certain words or phrases match a set of predefined correct
    responses. Response A might score reasonably well if the reference responses favor
    generic acknowledgments. However, these metrics might miss nuances in tone and
    specificity crucial for customer satisfaction. When evaluating with G-Eval, it
    would be more likely to assess the content, tone, empathy, and relevance of the
    response to the specific complaint. It would consider how effectively the response
    addresses the customer’s emotional state and the problem raised. In our example,
    response B would likely score higher on G-Eval because it acknowledges the customer’s
    feelings, provides a specific promise, and sets clear expectations—all of which
    are important to human judges (i.e., customers) in evaluating the quality of customer
    service.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: For enterprises, particularly in areas such as customer service, the effectiveness
    of automated responses can significantly affect customer satisfaction and loyalty.
    G-Eval aligns better with human evaluation because it captures the qualitative
    aspects of communication that are important in real-life interactions—such as
    empathy, specificity, and reassurance—but that are often overlooked by traditional
    metrics such as BLEU and ROUGE.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.2 An example of LLM-based evaluation metrics
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this example, we implement a G-Eval approach using Azure OpenAI’s GPT-4
    model to measure how good text summaries are. It uses the following four criteria:
    relevance, coherence, consistency, and fluency. We have an article and two summaries
    that are based on it. In addition, we use the code to score each summary on the
    four criteria and show which is better. As an example, we use the AI principles
    of the Bill and Melinda Gates Foundation, which are listed as “The first principles
    guiding our work with AI” and can be accessed online at [https://mng.bz/vJe4](https://mng.bz/vJe4).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: We have two summaries made from this source article that we want to compare
    with the article. The NLP library makes one summary, and another is made by LLM
    (Google’s Gemini Pro 1.5). We have saved all these locally for easy access and
    are reading them from there. The full code where we download the article and create
    the summaries is shown in the book’s GitHub repository.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Listings 12.2 and 12.3 show the key areas of this example with the full code.
    ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)). We define the evaluation
    metrics and their criteria and steps using prompt engineering and RAG. Each describes
    the scoring criteria and the steps to follow when evaluating a summary. Note that
    we don’t show all the code for brevity reasons.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.2 LLM-based evaluation metrics
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Evaluation prompt template based on G-Eval'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Defines the relevance metric as outlined by G-Eval'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Outlines the rules of the relevance metrics and how to measure'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Defines the coherence metric as outlined by G-Eval'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Outlines the rules of the coherence metric and how to measure'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Defines the consistency metric as outlined by G-Eval'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Outlines the rules of the consistency metrics and how to measure'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Defines the fluency metric as outlined by G-Eval'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Outlines the rules of the fluency metrics and how to measure'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already explained the prompts that define the metrics and the rules
    for computing them. Now look at the rest of the code in listing 12.3\. This is
    simple, and we do the following:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Use the `get_article()` function to get the article and summaries.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `get_geval_score()` function, loop over the evaluation metrics and summaries,
    generate a G-Eval score for each combination, and store the results in a dictionary.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, convert the dictionary to a data frame so we can pivot it and print
    it to the console.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note  The parameters for the Azure OpenAI are quite strict, with `max_ tokens`
    set to 5, `temperature` set to 0, and `top_p` set to 1.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.3 LLM-based evaluation metrics
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Function to load the files from disk'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Function to calculate various evaluation metrics'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sets up the prompt for G-Eval'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Completion API to run the evaluation'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Dictionary to store the evaluation results'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Loops over the evaluation metrics and summaries'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Checks if result is not empty and if it is a number'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Evaluation result stored in a dictionary'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Converts the dictionary to a Pandas DataFrame'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Pivots the DataFrame to allow easy visualization'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of this is as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s see how we interpret these results and what these scores mean:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Coherence measures how logically and smoothly ideas transition from one sentence
    to another. A score of 5 for the LLM summary indicates it presents information
    logically, is well-organized, and is easy for readers to follow. The traditional
    NLP summary, with a score of 1, likely struggles with disjointed ideas or lacks
    logical flow, which makes it difficult for readers to understand the sequence
    or connection of thoughts.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistency relates to the absence of contradictions within the text and maintaining
    the same standards throughout the summary. The LLM’s high score of 5 suggests
    it maintains a uniform tone, style, and factual accuracy. While good, the traditional
    NLP’s score of 4 indicates minor problems with maintaining these elements uniformly.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fluency assesses the text’s smoothness and the language’s naturalness. A score
    of 3 for the LLM indicates moderate fluency; the language is generally clear but
    might have some awkward phrasing or complexity that could impede readability.
    The traditional NLP, scoring lower, might exhibit more significant problems such
    as grammatical errors or unnatural sentence structures.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relevance measures how well the summary addresses the main points and purpose
    of the original content. The LLM’s score of 5 suggests that it effectively captures
    and focuses on the key elements of the original text, providing a summary that
    meets the informational needs of the reader. The traditional NLP, with a score
    of 2, likely includes some relevant information but misses important details or
    includes irrelevant content.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we have seen, assessing LLMs requires more than traditional tasks and includes
    more difficult benchmarks that measure higher-level understanding, logic, and
    adaptation skills. Some of these benchmarks, such as HELM, HEIM, HellaSWAG, and
    MMLU (Massive Multitask Language Understanding), are notable for their difficulty
    and scope.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.3 HELM
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: HELM (Holistic Evaluation of Language Models) [6] is a holistic framework for
    evaluating foundational models introduced by Stanford University. The framework
    aims to comprehensively assess language models, focusing on their abilities, limitations,
    and associated risks. Developed to enhance model transparency, HELM offers a more
    detailed understanding of model performance across diverse scenarios. It categorizes
    the extensive range of potential scenarios and metrics relevant to language models.
    A subset of these scenarios and metrics is then evaluated based on their coverage
    and practicality, ensuring that HELM is a practical and useful tool for enterprises.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The HELM approach uses a multimetric evaluation, assessing various factors such
    as accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency
    for each chosen scenario. It consists of large-scale evaluations of different
    language models performed under standardized conditions to guarantee comparability.
    Moreover, HELM promotes openness by sharing all model prompts and completions
    with the public for further analysis. This is supplemented by a modular toolkit
    that facilitates continuous benchmarking within the community.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: For enterprises considering the use of GenAI language models, HELM provides
    valuable information to make informed choices. It allows for the comparison of
    different models on various metrics, aiding in the selection of the most suitable
    model. Moreover, HELM helps mitigate risks by assessing potential harms such as
    bias and toxicity, enabling enterprises to identify and address problems before
    real-world application. The transparency and trust promoted by HELM through access
    to raw model predictions and evaluation data further enhance understanding and
    confidence in using LMs within an organization.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.4 HEIM
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Stanford University introduced a benchmark called Holistic Evaluation of Text-To-Image
    Models (HEIM) [7] to provide a complete quantitative analysis of the strengths
    and weaknesses of text-to-image models. Unlike other evaluations measuring text–image
    alignment and image quality, HEIM examines 12 important aspects of using models
    in the real world. Some of these aspects are aesthetics, originality, reasoning,
    knowledge, bias, toxicity, and efficiency.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: HEIM takes a holistic approach to evaluating the text-to-image models by curating
    62 scenarios. This holistic approach reveals that no single model excels in all
    areas, highlighting different strengths and weaknesses among various models.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'HEIM should be a key evaluation criterion for enterprises, as it provides a
    transparent and standardized way to assess text-to-image models. By understanding
    the strengths and limitations of these models, enterprises can make informed decisions
    about which models to use for specific tasks or services. Moreover, the evaluation
    helps identify potential risks such as bias or toxicity, which could have legal
    and reputational implications for businesses. Consequently, for enterprises building
    and deploying GenAI applications to production, the HEIM benchmark offers valuable
    insights across the following four dimensions:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '*Model selection*—HEIM highlights that no single model excels in all aspects.
    Enterprises must carefully evaluate and select models based on their application’s
    specific requirements. For example, applications focused on artistic creation
    might prioritize aesthetics and originality, while those requiring factual accuracy
    might focus on alignment and knowledge.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Risk mitigation*—HEIM emphasizes evaluating bias, toxicity, and fairness.
    Enterprises must ensure their applications are ethically sound and avoid perpetuating
    harmful stereotypes or generating inappropriate content. This necessitates careful
    model selection, fine-tuning, and implementation of safety measures.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Performance optimization*—Evaluating reasoning, robustness, and multilinguality
    is crucial for ensuring application reliability and user satisfaction. Enterprises
    must select models that perform well across diverse scenarios and user inputs.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Efficiency considerations*—Image generation efficiency affects user experience
    and operational costs. Enterprises should consider the tradeoffs between model
    size, speed, and resource requirements when selecting and deploying models.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will show an example and explain how we can apply HEIM to think about models.
    As mentioned before, HEIM assesses text-to-image models by creating scenarios
    that cover the 12 aspects it measures. If we wanted to test one of those 12 aspects—the
    text–image alignment aspect—HEIM might use a scenario where the model gets a complicated
    textual prompt and then checks how well the image it generates matches the prompt
    details and context. Here’s a possible evaluation scenario:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt*—“A futuristic cityscape at dusk with flying cars and neon signs reflecting
    in the water below.”'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model generation*—The model generates an image based on the prompt.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Evaluation*—Human evaluators or automated metrics assess the image on various
    factors:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the image accurately depict a cityscape at dusk?
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there flying cars and neon signs as described?
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a reflection in the water, and how realistic does it look?
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, how well does the image align with the prompt?
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model’s performance in this scenario would contribute to its overall score
    in text-image alignment. Similar scenarios would be created for other aspects,
    such as image quality, originality, reasoning, and so forth. The comprehensive
    evaluation across all 12 aspects provides insights into the model’s capabilities
    and limitations.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: HEIM’s approach ensures that models are evaluated on their ability to generate
    visually appealing images and their understanding of the text, creativity, and
    potential biases or ethical concerns. This holistic evaluation is crucial for
    enterprises, as it helps them choose models that align with their values and needs,
    while being aware of the risks involved.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: More details on HEIM, including the leaderboard, dataset, and other dependencies
    such as model access unified APIs, can be found at [https://crfm.stanford.edu/helm/heim/latest](https://crfm.stanford.edu/helm/heim/latest).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.5 HellaSWAG
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: HellaSWAG [8] is a challenging benchmark that tests AI models’ common-sense
    reasoning skills. It improves on its previous version, SWAG, by adding a more
    diverse and complex set of multiple-choice questions that require the models to
    do more than just language processing.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'HellaSWAG is a task in which each question presents a scenario with four possible
    endings, and the LLM must choose the most fitting ending among the options. For
    instance, the following question is from HellaSWAG’s dataset. The question consists
    of the context given to the LLM and the four options, which are the possible endings
    for that context. Only one of these options makes sense with common-sense reasoning.
    In this example, option C is highlighted:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: A woman is outside with a bucket and a dog. The dog is running around trying
    to avoid a bath. She…
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: A. rinses the bucket off with soap and blow-dries the dog’s head.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: B. uses a hose to keep it from getting soapy.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: C. gets the dog wet, then it runs away again.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: D. gets into a bathtub with the dog.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: The model’s selections are compared to the correct answers to measure its performance.
    This testing method evaluates the LLM’s knowledge of language nuances and its
    deeper comprehension of common-sense logic and the complexities of real-world
    situations. Doing well in HellaSWAG means that a model has a nuanced understanding
    and reasoning ability, which is essential for applications that need a sophisticated
    grasp of context and logic.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: If interested, the HellaSWAG’s dataset is available online via HuggingFace at
    [https://huggingface.co/datasets/Rowan/hellaswag](https://huggingface.co/datasets/Rowan/hellaswag).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.6 Massive Multitask Language Understanding
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Massive Multitask Language Understanding (MMLU) [9] benchmark assesses the
    breadth and depth of an LLM’s knowledge on various topics and domains. MMLU stands
    out by covering hundreds of tasks linked to different knowledge areas, from science
    and literature to history and social sciences.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: MMLU was developed in response to the observation that while many language models
    excelled at NLP tasks, they struggled with natural language understanding (NLU).
    Previous benchmarks such as GLUE and SuperGLUE were quickly mastered by LLMs,
    indicating a need for more rigorous testing. MMLU aimed to fill this gap by testing
    language understanding and problem-solving abilities using knowledge encountered
    during training.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: The benchmark includes questions from various subjects, including humanities,
    social sciences, hard sciences, and other specialized areas, varying from elementary
    to advanced professional levels. This approach was unique because most NLU benchmarks
    at the time focused on elementary knowledge. MMLU sought to push the boundaries
    by testing specialized knowledge, a new challenge for LLMs2.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Like HellaSWAG, MMLU often uses a multiple-choice format where the model must
    identify the right answer from a set of options. The overall accuracy across these
    diverse tasks shows the model’s general performance, comprehensively measuring
    its language understanding and knowledge application across domains. High performance
    on MMLU means that a model has a large amount of information and is skilled at
    using this knowledge to answer questions and problems correctly. This wide range
    of understanding is essential for developing LLMs that can handle the complexities
    of human knowledge and language subtly and informatively.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: While comprehensive, the MMLU faces several limitations. First, the performance
    of language models on this test can be constrained by the diversity and quality
    of their training data. If certain topics are underrepresented, models may underperform
    in those areas. Additionally, MMLU primarily assesses whether models can generate
    correct answers but does not evaluate how they arrive at these conclusions. It
    is crucial in applications where the reasoning process is as important as the
    outcome.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Another significant concern is the potential for bias within the test. Because
    MMLU is constructed from various sources, it may inadvertently include biases
    from these materials, affecting the fairness of model assessments, especially
    on sensitive topics. Furthermore, there is a risk that models could be overfitting
    to the specific MMLU format and style, optimizing for test performance rather
    than genuine understanding and applicability in real-world scenarios.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the logistical demands of running such a comprehensive test are substantial,
    requiring significant computational resources that might not be available to all
    researchers. This limitation can restrict the range of insights gleaned from the
    test. Finally, the scalability of knowledge poses a challenge; as fields evolve,
    the test must be updated regularly to stay relevant, necessitating ongoing resource
    investment. These factors highlight the complexities of using MMLU as a benchmark
    and underscore the need for continuous refinement to maintain its efficacy and
    relevance.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.7 Using Azure AI Studio for evaluations
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As an Azure customer, you can easily use these evaluations, as they are already
    integrated into Azure AI Studio. It includes tools for recording, seeing, and
    exploring detailed evaluation metrics, with the option to use custom evaluation
    flows and batch runs without evaluation. With AI Studio, we can make an evaluation
    run from a test dataset or flow with ready-made evaluation metrics. For more adaptability,
    we can make our evaluation flow.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you can use AI-assisted metrics to evaluate your model, make sure you
    have these things prepared:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: A test dataset in either CSV or JSONL format. If you don’t have a dataset ready,
    you can also enter data by hand from the UI.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of these models deployed: GPT-3.5 models, GPT-4 models, or Davinci models.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A compute instance runtime to run the evaluation.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 12.1 shows an example of how to set up the various tests. More details
    can be found at [https://mng.bz/4pMj](https://mng.bz/4pMj).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: If you are not using Azure, other similar options exist, such as DeepEval (see
    the next section). This open source LLM evaluation framework allows running multiple
    LLM metrics and makes this process quite easy.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F01_Bahree.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 Azure AI Studio evaluations
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '12.3.8 DeepEval: An LLM evaluation framework'
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DeepEval is a free LLM evaluation framework that works like `pytest` (a popular
    testing framework for Python) but focuses on unit-testing LLM outputs. DeepEval
    uses the newest research to assess LLM outputs based on metrics such as hallucination,
    answer relevancy, and RAGAS (Retrieval Augmented Generation Assessment). These
    metrics rely on LLMs and other NLP models that run on your computer locally for
    evaluation.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: DeepEval supports many useful features for enterprise applications. It can evaluate
    whole datasets simultaneously, create custom metrics, compare any LLM to popular
    benchmarks, and evaluate in real-time in production. In addition, it works with
    tools such as LlamaIndex and Hugging Face and is automatically connected to Confident
    AI for ongoing evaluation of your LLM application throughout its lifetime.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'The framework also offers a platform for recording test outcomes, measuring
    metrics’ passes/fails, selecting and comparing the best hyperparameters, organizing
    evaluation test cases/datasets, and monitoring live LLM responses in production.
    This book does not cover DeepEval in detail; more details are available at their
    GitHub repository: [https://github.com/confident-ai/deepeval](https://github.com/confident-ai/deepeval).
    Figure 12.2 shows a simple example of a test metric.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F02_Bahree.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 DeepEval test session example
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 12.4 New evaluation benchmarks
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the last 12 to 18 months, we have seen that AI models have reached performance
    saturation on established industry benchmarks such as ImageNet, SQuAD, and SuperGLUE,
    to name a few. This has spurred the industry to develop more challenging benchmarks.
    Some of the newer ones are SWE-bench for coding, MMMU for general reasoning, MoCa
    for moral reasoning, and HaluEval for hallucinations.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.1 SWE-bench
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To measure the progress of GenAI systems that can code, we need more difficult
    tasks to evaluate them. SWE-bench [10] is a dataset containing nearly hundreds
    of software engineering problems from real-world GitHub and Python repositories.
    It poses a harder challenge for AI coding skills, requiring that systems make
    changes across multiple functions, deal with different execution environments,
    and perform complex reasoning.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: The SWE-bench dataset evaluates systems’ abilities to solve GitHub problems
    automatically. It collects 2,294 problem-pull request pairs from 12 popular Python
    repositories. The evaluation is performed by verifying the proposed solutions
    using unit tests and comparing them to the post-PR behavior as the reference solution.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: The primary evaluation metric for SWE-bench is the *percentage of resolved task
    instances.* In other words, it measures how effectively a model can address the
    given problems—the higher the percentage of resolved instances, the better the
    model’s performance. More details on SWE-bench can be found at [https://www.swebench.com](https://www.swebench.com).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.2 MMMU
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MMMU (Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark)
    [9] is a new benchmark designed to evaluate multimodal models’ capabilities on
    tasks requiring college-level subject knowledge and expert-level reasoning across
    multiple disciplines. It includes 11.5K multimodal questions from college exams,
    quizzes, and textbooks, covering six core disciplines: art and design, business,
    science, health and medicine, humanities and social science, and tech and engineering.
    These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous
    image types, such as charts, diagrams, maps, tables, music sheets, and chemical
    structures.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'MMMU is unique because it focuses on advanced perception and reasoning with
    domain-specific knowledge and challenging models to perform tasks that experts
    face. The benchmark has been used to evaluate several open source LLMs and proprietary
    models such as GPT-4V, highlighting the substantial challenges MMMU poses. Even
    the advanced models only achieve accuracies between 56% and 59%, indicating significant
    room for improvement. It operates by assessing LLMs’ ability to perceive, understand,
    and reason across different disciplines and subfields using various image types.
    The benchmark focuses on three essential skills in LLMs: perception, knowledge,
    and reasoning.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Note that it might seem that the MMLU discussed earlier is the same as MMMU;
    however, they are different. MMLU evaluates language models on a wide range of
    text-based tasks across various domains, focusing solely on language understanding.
    In contrast, MMMU assesses multimodal models, requiring both visual and textual
    comprehension across specialized disciplines, thus challenging models with complex,
    domain-specific multimodal content.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: The MMMU benchmark presents several key challenges for multimodal models, which
    include
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '*Comprehensiveness*—Since the benchmark includes a wide array of 11.5K college-level
    problems across broad disciplines, the models must have a broad knowledge base
    and understanding across multiple fields.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Highly heterogeneous image types*—The questions involve 30 different types
    of images, such as charts, diagrams, maps, tables, music sheets, and chemical
    structures. This means the models must be able to interpret and understand various
    visual information.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Interleaved text and images*—Many questions feature a mix of text and images,
    requiring models to process and integrate information from both modalities to
    arrive at the correct answer.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Expert-level perception and reasoning*—The tasks demand deep subject knowledge
    and expert-level reasoning, akin to the challenges faced by human experts in their
    respective fields.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These challenges aim to stretch the limits of existing multimodal models, testing
    their capacity to do sophisticated perception, analytical thinking, and domain-specific
    reasoning. The questions demand a profound understanding of the topic and the
    ability to use knowledge in intricate scenarios, even for human experts.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.3 MoCa
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The MoCa (Measuring Human-Language Model Alignment on Causal and Moral Judgment
    Tasks) [11] framework evaluates how well LLMs align with human participants in
    making causal and moral judgments about text-based scenarios. AI models can perform
    well in language and vision tasks, but their ability to make moral decisions,
    especially those that match human opinions, is unclear. To investigate this topic,
    a group of Stanford researchers created a new dataset (MoCa) of human stories
    with moral aspects. Here, we will look at the details of each:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '*Causal judgments*—Humans intuitively understand events, people, and the world
    around them by organizing their understanding into intuitive theories. These theories
    help us reason about how objects and agents interact with one another, including
    concepts related to causality. The MoCa framework collects a dataset of stories
    from cognitive science papers and annotates each story with the factors they investigate.
    It then tests whether LLMs make causal judgments about text scenarios that align
    with those of humans. On an aggregate level, alignment has improved with more
    recent LLMs. However, statistical analyses reveal that LLMs weigh various factors
    in a different way than human participants.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Moral judgments*—Tasks evaluate agents in narrative-like text for moral reasoning.
    These tasks and datasets vary in structure, ranging from free-form anecdotes to
    more structured inputs. The MoCa framework assesses how well LLMs align with human
    moral intuitions in these scenarios.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main metric used in MoCa is the Area under the Receiver Operating Characteristic
    (AuROC) curve, which measures the alignment between LLMs and human judgments.
    Furthermore, accuracy serves as a secondary metric for comparison between models.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: A higher score indicates closer alignment with human moral judgment. The study
    yielded intriguing results. No model perfectly matches human moral systems. However,
    newer, larger models such as GPT-4 and Claude show greater alignment with human
    moral sentiments than smaller models such as GPT-3, suggesting that as AI models
    scale, they are gradually becoming more morally aligned with humans.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: In summary, MoCa provides insights into how LLMs handle causal and moral reasoning,
    shedding light on their implicit tendencies and alignment (or lack thereof) with
    human intuitions. We can get more details on MoCa at [https://moca-llm.github.io](https://moca-llm.github.io).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.4 HaluEval
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: HaluEval [12] benchmark is a large-scale evaluation framework designed to assess
    LLMs’ performance in recognizing hallucinations. In this context, hallucinations
    refer to LLM-generated content that conflicts with the source or cannot be verified
    by factual knowledge. The benchmark includes a collection of generated and human-annotated
    hallucinated samples.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: A two-step framework involving sampling-then-filtering is used to create these
    samples, often based on responses from models such as ChatGPT. Human labelers
    also contribute by annotating hallucinations in the responses. The empirical results
    from HaluEval suggest that LLMs, including ChatGPT, can generate hallucinated
    content, particularly on specific topics, by fabricating unverifiable information.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: The study also explores how good current LLMs are at finding hallucinations.
    It can lead LLMs to spot hallucinations in tasks such as question-answering knowledge-grounded
    dialogue and text summarization. The results show that many LLMs have difficulties
    with these tasks, emphasizing that hallucination is a serious, persistent problem.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'The HaluEval benchmark includes 5,000 general user queries with ChatGPT responses
    and 30,000 task-specific examples from three tasks: question answering, knowledge-grounded
    dialogue, and text summarization. It''s a significant step toward understanding
    and improving the reliability of LLMs in generating accurate and verifiable content.
    HaluEval’s GitHub repository at [https://github.com/RUCAIBox/HaluEval](https://github.com/RUCAIBox/HaluEval)
    provides more details.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 12.5 Human evaluation
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Human evaluation plays a crucial role in understanding the quality of LLMs,
    as it captures nuances, context, and potential biases that automated metrics might
    overlook. For enterprises to conduct effective human evaluations, they need to
    start by defining clear criteria to guide the assessment of LLM outputs. These
    criteria should cover aspects such as accuracy, fluency, relevance, and the presence
    of any biases. To ensure consistency and objectivity, enterprises should develop
    comprehensive guidelines and rubrics for evaluators to follow.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: When choosing the right evaluation methods, enterprises have several options.
    They can either engage domain experts or trained annotators for detailed assessments
    or opt for crowdsourcing platforms such as Amazon Mechanical Turk ([https://www.mturk.com](https://www.mturk.com))
    to access a wider pool of evaluators. The next step involves data collection and
    annotation, which requires user-friendly interfaces and clear instructions to
    ensure quality and consistency. It’s also important to collect enough data to
    yield statistically significant results.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: After collecting data, a thorough analysis is necessary. Enterprises should
    employ statistical methods to measure interrater agreement and confirm the reliability
    of the evaluations. The insights derived from this process should then be used
    to make iterative improvements to LLMs, including adjustments to the training
    data, model architecture, and prompt engineering. Regular human evaluations are
    essential for monitoring progress and pinpointing areas that need further improvement.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: While human evaluation is invaluable, it does come with its challenges. It can
    be expensive and time-consuming, especially when dealing with large datasets.
    There’s also the risk of subjectivity and bias in human judgments. However, these
    problems can be mitigated by providing clear guidelines, adequate training for
    evaluators, and employing proper aggregation methods.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Several tools and platforms are available to help streamline the human evaluation
    process. Crowdsourcing platforms provide access to a diverse workforce, while
    annotation tools offer efficient data-labeling features. Evaluation frameworks
    are also available, including libraries of metrics and scripts designed specifically
    for LLM evaluation and to support human evaluation.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Some examples of tools that assist with annotations are Label Studio ([https://labelstud.io](https://labelstud.io)),
    which offers both open source and enterprise offerings. Prodigy ([https://prodi.gy](https://prodi.gy))
    is another annotation tool that supports text, images, videos, and audio. Text-only
    annotation tools also exist, such as Labellerr ([https://www.labellerr.com](https://www.labellerr.com)).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Some companies specialize in LLM evaluation, offering robust testing frameworks
    and various resources to assist the evaluation process. For enterprises that are
    not comfortable implementing their evaluation frameworks using tools such as PromptFlow,
    which we saw earlier in the previous chapter, Weights and Biases ([https://wandb.ai](https://wandb.ai)),
    and so forth, there are new companies that specialize in LLM evaluations, such
    as Giskard ([https://www.giskard.ai](https://www.giskard.ai)).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: By following these steps and utilizing the available resources, enterprises
    can implement a structured and effective human evaluation process for LLMs. It’s
    important to remember that this is an evolving field, and staying up to date on
    the latest developments and training is crucial for maintaining the quality of
    evaluations.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Benchmarking systems are essential for verifying the performance of GenAI and
    LLMs, directing enhancements, and confirming real-world suitability. They assist
    us in evaluating the efficiency and preparedness of generative AI and LLMs for
    deployment in production environments.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The correlation between evaluations and LLMs is a new and emerging area. We
    should use traditional metrics, LLM task-specific benchmarks, and human evaluation
    to assess LLM performance and ensure its suitability for real-world applications.
    G-Eval is a reference-free evaluation method using LLMs to assess the generated
    text’s coherence, consistency, and relevance.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conventional metrics such as BLEU, ROUGE, and BERTScore help measure text generation
    quality and evaluate text numerically based on n-gram matching or semantic similarity.
    They do face some challenges in fully representing contextual meaning and paraphrasing.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM-specific benchmarks measure how well LLMs perform tasks such as text classification,
    sentiment analysis, and question answering. They introduce new metrics such as
    groundedness, coherence, fluency, and GPT similarity that help assess the quality
    of LLM outputs and how close they are to human-like standards.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective evaluation methods for meaningful LLM evaluations include testing
    in relevant settings, creating fair prompts, conducting ethical reviews, and assessing
    the user experience. These include advanced benchmarks such as HELM, HEIM, HellaSWAG,
    and MMLU, which test LLMs against various scenarios and capabilities.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools such as Azure AI Studio and the DeepEval framework enable effective LLM
    evaluations in an enterprise context. These tools allow the development of customized
    evaluation workflows, batch executions, and the incorporation of real-time evaluations
    into production settings.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
