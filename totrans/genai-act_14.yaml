- en: 12 Evaluations and benchmarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Understanding the significance of benchmarking and evaluating LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning different evaluation metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmarking model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing comprehensive evaluation strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for evaluation benchmarks and key evaluation criteria to consider
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking into account the recent surge of interest in GenAI and specifically in
    large language models (LLMs), it’s crucial to approach these novel and uncertain
    features cautiously and responsibly. Many leaderboards and studies have shown
    that LLMs can match human performance in various tasks, such as taking standardized
    tests or creating art, sparking enthusiasm and attention. However, their novelty
    and uncertainties necessitate careful handling.
  prefs: []
  type: TYPE_NORMAL
- en: The role of benchmarking LLMs in production deployment cannot be overstated.
    It involves evaluating performance, comparing models, guiding improvements, accelerating
    technological advancement, managing costs and latency, and ensuring efficient
    task flow for real-world applications. While evaluations are part of LLMOps, their
    criticality in ensuring LLMs meet the demands of various applications warrants
    a separate discussion in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating LLMs is not a simple task but a complex and multifaceted process
    that demands quantitative and qualitative approaches. When evaluating LLMs, comprehensive
    assessment methods covering various aspects of model performance and effect must
    be employed. Stanford University’s Human-Centered Artificial Intelligence (HAI)
    publishes an annual AI Index report [1] that aims to collate and track different
    data points related to AI. One of the most significant challenges we face is the
    lack of standardized evaluations, which makes a systematic comparison between
    different models incredibly difficult when it comes to capabilities and potential
    risks and harms. This means we don’t have an objective measure of how good or
    smart any of these specific models are, which underscores the complexity and importance
    of the evaluation process.
  prefs: []
  type: TYPE_NORMAL
- en: When we discuss GenAI evaluations in this initial stage, most discussions concern
    accuracy and performance evaluations that assess how well a language model can
    comprehend and produce text that resembles human language. This aspect is very
    important for applications that rely on the quality and relevance of the content
    they generate, such as chatbots, content creation, and summarization tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three general types of evaluations that can measure accuracy and
    performance: traditional evaluation metrics that judge language quality, LLM task-specific
    benchmarks for assessing specific tasks, and human evaluations. Let’s start by
    understanding what LLM evaluations are and learn about some of the best practices
    associated with evaluations.'
  prefs: []
  type: TYPE_NORMAL
- en: 12.1 LLM evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is essential to evaluate LLMs to ensure they are reliable and appropriate
    for real-world applications. A strong evaluation strategy covers performance metrics
    such as accuracy, fluency, coherence, and relevance. These metrics help us to
    understand the model’s advantages and disadvantages across different contexts.
    I summarize here a few areas as best practices to consider when evaluating LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the LLM meaningfully, it must be tested on the use cases it is designed
    for, meaning using the model on various natural language processing (NLP) tasks,
    such as summarization, question-answering, and translation. The evaluation process
    should use standard metrics such as ROUGE (Recall-Oriented Understudy for Gisting
    Evaluation) for summarization to maintain reliability and comparability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another important aspect of LLM evaluation is the creation of prompts. Prompts
    must be unambiguous and fair, providing a valid assessment of the model’s abilities.
    This ensures that the evaluation outcomes reflect the model’s actual performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmarking is a crucial practice that enables evaluating an LLM’s performance
    based on existing criteria and other models. This not only tracks progress but
    also identifies areas requiring improvement. A continuous evaluation process,
    combined with constant development practices, allows for periodic assessment and
    refinement of the LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evaluation of LLMs must involve ethical considerations at every step. The
    process must check the model for biases, fairness, and ethical problems, looking
    at the training data and the outputs. Moreover, the user experience should be
    a key part of the evaluation, ensuring that the model’s outputs match user needs
    and expectations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evaluation must be transparent at every stage. Recording the criteria, methods,
    and results allows for independent verification and increases confidence in the
    LLM’s abilities. Finally, the evaluation outcomes should inform a continuous improvement
    cycle, improving the model, training data, and the evaluation process based on
    performance measures and feedback.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These practices underscore the importance of a rigorous and systematic approach
    to evaluating LLMs, ensuring that they are accurate but also fair, ethical, and
    suitable for various applications.
  prefs: []
  type: TYPE_NORMAL
- en: By following these practices, enterprises can conduct reliable and effective
    evaluations, developing trustworthy and helpful LLMs for different uses. Now that
    we know what evaluations are, let’s take a look at some metrics we should use.
    They can be categorized into traditional and newer LLM-specific evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 Traditional evaluation metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BLEU (Bilingual Evaluation Understudy), ROUGE (Recall-Oriented Understudy for
    Gisting Evaluation), and BERTScore (BERT Similarity Score) are some of the more
    standardized metrics. These metrics help quantify the linguistic quality of model
    outputs against reference texts and are used to evaluate text quality in tasks
    such as machine translation or text summarization. Still, they differ in their
    approaches and focus on different aspects of the text. Table 12.1 shows a detailed
    explanation of what each of the three scores indicates. We will show how to compute
    these in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Table 12.1 Traditional evaluation metrics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Metric | Focus | Method | Limitations |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BLEU  | It primarily measures precision, the percentage of words in the machine-
    generated text that appear in the reference text.  | It compares n-grams (word
    sequences) of the candidate translation with the reference translation and counts
    the matches.  | It can miss the mark on semantic meaning because it doesn’t account
    for synonyms or the context of words. It also doesn’t handle word reordering well.  |'
  prefs: []
  type: TYPE_TB
- en: '| ROUGE  | It is more recall oriented, focusing on the percentage of words
    from the reference text that appear in the generated text.  | It has several variants,
    such as ROUGE-N, which compares n-grams, and ROUGE-L, which looks at the longest
    common subsequence.  | Like BLEU, ROUGE can overlook semantic similarities and
    paraphrasing because it’s based on exact word matches.  |'
  prefs: []
  type: TYPE_TB
- en: '| BERTScore  | It evaluates semantic similarity rather than relying on exact
    word matches.  | It uses contextual embeddings from models such as BERT to represent
    the text and calculates the cosine similarity between these embeddings.  | It
    can capture paraphrasing and semantic meaning better than BLEU and ROUGE because
    it considers each word’s context.  |'
  prefs: []
  type: TYPE_TB
- en: Metrics such as ROUGE, BLEU, and BERTScore compare the similarities between
    text generated by an LLM and reference text written by humans. They are commonly
    used for evaluating tasks such as summarization and machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.1 BLEU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: BLEU (Bilingual Evaluation Understudy) [2] is an algorithm used to evaluate
    the quality of machine-translated text from one natural language to another. Its
    central idea is to measure the correspondence between a machine’s output and that
    of a human translator. In other words, according to BLEU, the closer a machine
    translation is to a professional human translation, the better it is. BLEU does
    not consider intelligibility or grammatical correctness; it focuses on content
    overlap.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.2 ROUGE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'ROUGE (Recall-Oriented Understudy for Gisting Evaluation) [3] is a set of measures
    used in NLP to assess how well automatic text summarization and machine translation
    perform. Its main goal is to contrast summaries or translations produced by machines
    with human reference summaries. It evaluates the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Recall*—ROUGE measures how much of the reference summary the system summary
    captures. It evaluates how well the system recovers or captures content from the
    reference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Precision*—It also assesses how much of the system summary is relevant, needed,
    or useful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*F-measure*—It combines precision and recall to provide a balanced view of
    system performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROUGE has different versions, such as ROUGE-N (which uses n-grams) and ROUGE-L
    (based on the Longest Common Subsequence algorithm). By looking at single words
    and sequences, ROUGE helps us measure the effectiveness of NLP algorithms in summarization
    and translation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: However, ROUGE has limitations. It relies solely on surface-level overlap and
    doesn’t account for semantic meaning or fluency. Sensitivity to stop words, stemming,
    and word order can affect scores. While ROUGE provides valuable insights, it’s
    essential to consider other evaluation metrics and human judgment to assess summary
    quality comprehensively. Researchers often use a combination of metrics to evaluate
    summarization models.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.3 BERTScore
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'BERTScore [4] is a measure of how good text generation is. It uses pretrained
    BERT model embeddings to compare candidate and reference sentences. The idea is
    to find similar words in the candidate and reference sentences based on cosine
    similarity. This metric agrees with human opinion in sentence- and system-level
    evaluations. It has the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Contextual embeddings*—BERTScore represents both the candidate and reference
    sentences with embeddings that consider each word’s context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cosine similarity*—It calculates the cosine similarity between the embeddings
    of the candidate and reference texts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Token matching*—To compute precision and recall scores, each token in the
    candidate text matches the most similar token in the reference text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*F1 score*—The precision and recall are combined to calculate the F1 score,
    providing a single quality measure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key advantage of BERTScore over traditional metrics such as BLEU is its
    ability to capture semantic similarity. This means it can recognize when different
    words have similar meanings and when the same words are used in different contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.4 An example of traditional metric evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s bring it all together and make it real through a simple example. Here
    we have two information summaries and can evaluate which one might be better.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we take the AI development principles of the Bill and Melinda
    Gates Foundation as the article we want to analyze and understand. This article
    is available at [https://mng.bz/vJe4](https://mng.bz/vJe4). From the article,
    we create two summaries that we’ll compare. In this case, one is created by NLTK
    and the other by another LLM (GPT-3.5). This could also be two different human-written
    versions or any other combination. We use the `newspaper3K` and `bert_score` packages
    to download the article and the Hugging Face Evaluate package for the evaluations.
    These can be installed in conda using `conda` `install` `-c` `conda-forge` `newspaper3k`
    `evaluate` `bert_score`. In pip, use `pip install evaluate newspaper3k bert_score`.
  prefs: []
  type: TYPE_NORMAL
- en: We use `newspaper3k` to download and parse the article first. Then we apply
    the `nlp()` function to process the article and get the summary from the summary
    property. We must ensure the article is downloaded and parsed before using NLP;
    note that this only works for Western languages. We use the summary created by
    NLP as our reference summary and the `Evaluate` library to calculate the specific
    metrics. The listing shows the code to implement this.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.1 Automated evaluation metrics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Sets up the OpenAI details'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Function to download and parse the article; returns both the article text
    and a summary'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Summarizes the article using OpenAI'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Function to calculate metrics (BLEU, ROUGE, etc.)'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Configures newspaper3k to allow downloading articles'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the output we can observe when executing the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As we have seen, the BLEU score is composed of several components that collectively
    assess the quality of a machine-generated translation against a set of reference
    translations. Let’s examine each component and see what it means, starting with
    the BLEU score outlined in table 12.2\. Tables 12.3 and 12.4 show the results
    for the ROUGE score and the BERT score, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Table 12.2 BLEU score
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Component value | Meaning |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BLEU: 0.047 (4.7%)  | This is the overall BLEU score, which is quite low.
    BLEU scores range from 0 to 1 (or 0% to 100%), with higher scores indicating better
    translation quality. A score below 10% is generally considered poor.  |'
  prefs: []
  type: TYPE_TB
- en: '| Precisions  | These are the n-gram precision scores for 1-gram, 2-gram, 3-gram,
    and 4-gram matches. Our scores indicate a decent number of 1-gram matches but
    few longer matches, suggesting that the translation has some correct words but
    lacks coherent phrases and sentences.  |'
  prefs: []
  type: TYPE_TB
- en: '| Brevity penalty: 1.0  | This means there was no penalty for brevity; the
    translation length was appropriate compared to the reference length.  |'
  prefs: []
  type: TYPE_TB
- en: '| Length ratio: 1.27  | The translation is 27% longer than the reference, which
    might suggest some verbosity.  |'
  prefs: []
  type: TYPE_TB
- en: '| Translation length: 140  | The length of the machine-translated text  |'
  prefs: []
  type: TYPE_TB
- en: '| Reference length: 110  | The length of the reference text  |'
  prefs: []
  type: TYPE_TB
- en: Table 12.3 ROUGE score
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Component value | Meaning |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ROUGE-1: 0.3463 (34.63%)  | It measures the overlap of 1-gram between the
    system output and the reference summary. A moderate score indicates a fair amount
    of overlap.  |'
  prefs: []
  type: TYPE_TB
- en: '| ROUGE-2 : 0.0961 (9.61%)  | It measures the overlap of bigrams and is a stricter
    metric than ROUGE-1\. A low score suggests that the system struggles to form accurate
    phrases.  |'
  prefs: []
  type: TYPE_TB
- en: '| ROUGE-L: 0.1645 (16.45%)  | It measures the longest common subsequence, indicating
    the fluency and order of the words. The score suggests limited fluency.  |'
  prefs: []
  type: TYPE_TB
- en: '| ROUGE-Lsum: 0.2684 (26.84%)  | It is similar to ROUGE-L but considers the
    sum of the longest common subsequences, indicating a slightly better grasp of
    the content structure.  |'
  prefs: []
  type: TYPE_TB
- en: Table 12.4 BERT Score
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Component value | Meaning |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Precision: 0.8524 (85.24%)  | It measures how many words in the candidate
    text are relevant or needed.  |'
  prefs: []
  type: TYPE_TB
- en: '| Recall: 0.8710 (87.10%)  | It measures how much of the candidate text captures
    the reference content.  |'
  prefs: []
  type: TYPE_TB
- en: '| F1 Score: 0.8616 (86.16%)  | This harmonic mean of precision and recall provides
    a single score that balances both. An F1 score closer to 1 indicates better performance.  |'
  prefs: []
  type: TYPE_TB
- en: The BLEU and ROUGE scores suggest that the translation or summary has room for
    improvement, particularly in forming coherent phrases and sentences. However,
    the BERT score is quite high, indicating that the candidate text is semantically
    similar to the reference text and captures most of its content. Thus, while the
    translation may not match the reference text word for word, it does convey the
    same overall meaning quite well.
  prefs: []
  type: TYPE_NORMAL
- en: Even though metrics such as BERTScore, ROUGE, and BLEU help compare similar
    text, they primarily focus on surface-level similarity. They may not capture semantic
    equivalence or the overall quality of the generated text. These more traditional
    metrics often penalize LLMs, which can produce coherent and fluent generations.
    For these, we need LLM task-specific benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 LLM task-specific benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Measuring the performance of LLMs across various NLP tasks requires task-specific
    benchmarks. They are created to test how well the models can understand, reason,
    and generate natural language for specific domains or tasks, providing a clear
    way to compare different models. These benchmarks can reveal a model’s abilities
    and limitations, enabling focused improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Task-specific benchmarks assess LLMs on specific NLP tasks such as text classification,
    sentiment analysis, question answering, summarization, and more. These benchmarks
    usually consist of datasets with predefined inputs and expected outputs, allowing
    for quantitative assessment of model performance through metrics such as accuracy,
    F1 score, or BLEU score, depending on the task. Some key LLM benchmarks are groundedness,
    relevance, coherence, fluency, and GPT similarity; these evaluation metrics are
    outlined in table 12.5.
  prefs: []
  type: TYPE_NORMAL
- en: Table 12.5 LLM evaluation metrics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Metric | Focus | Method | When to use? |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Groundedness  | It evaluates how well the answers the model produces match
    the information in the source data (context that the user provides). This metric
    ensures that the context backs up the answers generated by AI.  | It evaluates
    how well the statements in an AI-generated answer match the source context, ensuring
    that the context supports these statements. It is rated from 1 (bad) to 5 (good).  |
    It is used when we want to check that the AI responses match and are confirmed
    by the given context. It is also used when being factually correct and contextually
    precise is important, such as when finding information, answering questions, and
    summarizing content.  |'
  prefs: []
  type: TYPE_TB
- en: '| Coherence  | It evaluates the model’s ability to generate coherent, natural
    output similar to human language.  | It evaluates how well the generation is structured
    and connected. This is rated from 1 (bad) to 5 (good).  | Use it when evaluating
    how easy and user-friendly your model’s generated responses are in real-world
    situations.  |'
  prefs: []
  type: TYPE_TB
- en: '| Fluency  | It measures the grammar proficiency and readability of a model’s
    generated response.  | The fluency measure evaluates how well the generated text
    follows grammatical rules, syntactic structures, and suitable word choices. It
    is scored from 1 (bad) to 5 (good).  | This tool assesses the linguistic accuracy
    of the generated text, ensuring that it follows appropriate grammar rules, syntax
    structures, and word choices.  |'
  prefs: []
  type: TYPE_TB
- en: '| GPT similarity  | It compares how similar a source data (ground truth) sentence
    is to the output from an AI model.  | This assessment involves creating sentence-level
    embeddings for both the ground truth and the model’s prediction, which are high-dimensional
    vector representations that encode the semantic meaning and context of the sentences.  |
    Use it to get an unbiased measure of a model’s performance, especially in text
    generation tasks where we have the correct responses available. This lets us check
    how closely the generated text matches the intended content, which helps us evaluate
    the model’s quality and accuracy.  |'
  prefs: []
  type: TYPE_TB
- en: To illustrate how this works, we will apply a reference-free evaluation method
    based on the G-Eval method. Reference-free means that we do not depend on comparing
    a generated summary to a preexisting reference summary. Let’s start with understanding
    G-Eval.
  prefs: []
  type: TYPE_NORMAL
- en: '12.3.1 G-Eval: A measuring approach for NLG evaluation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: G-Eval [5] introduces a new framework for measuring the quality of text produced
    by NLG systems. Using LLMs, G-Eval combines a chain-of-thought–prompting method
    with a form-filling technique to examine different aspects of the NLG output,
    such as coherence, consistency, and relevance. G-Eval judges the quality of the
    generated content based on the input prompt and text alone, without any reference
    texts, and is thus considered reference free.
  prefs: []
  type: TYPE_NORMAL
- en: 'The method is particularly useful for novel datasets and tasks with few human
    references available. This flexibility makes G-Eval suitable for various innovative
    applications, especially in fields where data is continuously evolving or is highly
    specific. Here are a few scenarios where G-Eval would be beneficial:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Medical report generation*—In the medical domain, where automated systems
    produce customized reports from various patient data, G-Eval can evaluate the
    reports for correctness, consistency, and medical relevance. As patient scenarios
    differ a lot, conventional reference-based metrics might not always work, making
    G-Eval a more adaptable and appropriate option that guarantees the quality and
    dependability of medical reports.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Legal document writing*—When AI creates legal documents that suit particular
    cases, G-Eval assesses how well the documents meet legal requirements, how clear
    and coherent they are, and how well they follow the rules. This is important in
    legal situations where having precise reference texts for every situation is not
    feasible, but accuracy and conformity to legal standards are vital.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Creative content evaluation*—Novelty is essential in fields that require creativity,
    such as advertising or video game storytelling. G-Eval helps assess the novelty,
    appeal, and target audience suitability of such content, providing a way to gauge
    the quality of creativity that is more than just word or phrase similarity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AI-based content moderation*—G-Eval can help verify that moderation actions
    are suitable and successful, even when there is no reliable reference data, by
    using AI systems to moderate changing online content. This is especially important
    in online settings where context and sensitivity matter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These examples show how G-Eval can assess the quality of AI-generated text
    with human-like standards and flexibility to meet different needs. This is important
    for GenAI applications where conventional metrics are insufficient. G-Eval has
    many advantages for businesses that want to develop and use effective NLG solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: G-Eval shows much better agreement with human evaluation than conventional metrics
    such as BLEU and ROUGE. This is especially clear in open-ended and creative NLG
    tasks, where conventional metrics often fail. By giving a more precise measurement
    of NLG system quality, G-EVAL helps enterprises make smart choices about their
    development and deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: G-Eval uses the probabilities of output tokens from LLMs to produce fine-grained
    continuous scores. This enables the capture of slight differences between generated
    texts, giving more detailed feedback than traditional metrics that often depend
    on discrete scoring. Such precise feedback can be very helpful for enterprises
    as they adjust their NLG systems for the best performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An interesting feature of G-Eval is that it can be customized to evaluate different
    NLG tasks by changing the prompt and evaluation criteria. This flexibility removes
    the need for task-specific evaluators, making the evaluation process easier for
    enterprises working with various NLG applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, a possible problem with LLM-based evaluators is that they may prefer
    text generated by LLMs. This problem needs more research and solutions to ensure
    reliable and correct evaluation, especially when using LLM-based metrics to enhance
    NLG systems.
  prefs: []
  type: TYPE_NORMAL
- en: G-Eval provides a potential method for NLG evaluation in enterprises, which
    can help create and use more efficient and dependable NLG systems for different
    purposes. Let’s see how we can use this.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can demonstrate how G-Eval can be very helpful with a simple example. Imagine
    an enterprise that wants to evaluate customer service chatbots. These chatbots
    are usually trained to deal with many kinds of customer questions and problems,
    and their performance is essential for maintaining customer satisfaction and loyalty.
    For example, let’s think about a customer complaint about a service. Suppose a
    customer writes the following complaint in an email:'
  prefs: []
  type: TYPE_NORMAL
- en: I am extremely disappointed with the delay in service. I was promised a two-day
    delivery, and it’s already been a week without any updates. This is unacceptable.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Now imagine two different automated responses generated by customer service
    bots:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Response A (more literal and generic)*—“We apologize for any inconvenience
    caused. Your complaint has been registered. We will update you shortly.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Response B (more empathetic and specific)*—“We’re really sorry to hear about
    this delay and completely understand your frustration. It’s not the experience
    we want to provide. Our team is looking into this as a priority, and we’ll reach
    out with an update on your delivery by tomorrow morning.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conventional metrics such as BLEU and ROUGE would likely assess these responses
    based on how closely certain words or phrases match a set of predefined correct
    responses. Response A might score reasonably well if the reference responses favor
    generic acknowledgments. However, these metrics might miss nuances in tone and
    specificity crucial for customer satisfaction. When evaluating with G-Eval, it
    would be more likely to assess the content, tone, empathy, and relevance of the
    response to the specific complaint. It would consider how effectively the response
    addresses the customer’s emotional state and the problem raised. In our example,
    response B would likely score higher on G-Eval because it acknowledges the customer’s
    feelings, provides a specific promise, and sets clear expectations—all of which
    are important to human judges (i.e., customers) in evaluating the quality of customer
    service.
  prefs: []
  type: TYPE_NORMAL
- en: For enterprises, particularly in areas such as customer service, the effectiveness
    of automated responses can significantly affect customer satisfaction and loyalty.
    G-Eval aligns better with human evaluation because it captures the qualitative
    aspects of communication that are important in real-life interactions—such as
    empathy, specificity, and reassurance—but that are often overlooked by traditional
    metrics such as BLEU and ROUGE.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.2 An example of LLM-based evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this example, we implement a G-Eval approach using Azure OpenAI’s GPT-4
    model to measure how good text summaries are. It uses the following four criteria:
    relevance, coherence, consistency, and fluency. We have an article and two summaries
    that are based on it. In addition, we use the code to score each summary on the
    four criteria and show which is better. As an example, we use the AI principles
    of the Bill and Melinda Gates Foundation, which are listed as “The first principles
    guiding our work with AI” and can be accessed online at [https://mng.bz/vJe4](https://mng.bz/vJe4).'
  prefs: []
  type: TYPE_NORMAL
- en: We have two summaries made from this source article that we want to compare
    with the article. The NLP library makes one summary, and another is made by LLM
    (Google’s Gemini Pro 1.5). We have saved all these locally for easy access and
    are reading them from there. The full code where we download the article and create
    the summaries is shown in the book’s GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Listings 12.2 and 12.3 show the key areas of this example with the full code.
    ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)). We define the evaluation
    metrics and their criteria and steps using prompt engineering and RAG. Each describes
    the scoring criteria and the steps to follow when evaluating a summary. Note that
    we don’t show all the code for brevity reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.2 LLM-based evaluation metrics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Evaluation prompt template based on G-Eval'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Defines the relevance metric as outlined by G-Eval'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Outlines the rules of the relevance metrics and how to measure'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Defines the coherence metric as outlined by G-Eval'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Outlines the rules of the coherence metric and how to measure'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Defines the consistency metric as outlined by G-Eval'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Outlines the rules of the consistency metrics and how to measure'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Defines the fluency metric as outlined by G-Eval'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Outlines the rules of the fluency metrics and how to measure'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already explained the prompts that define the metrics and the rules
    for computing them. Now look at the rest of the code in listing 12.3\. This is
    simple, and we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the `get_article()` function to get the article and summaries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `get_geval_score()` function, loop over the evaluation metrics and summaries,
    generate a G-Eval score for each combination, and store the results in a dictionary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, convert the dictionary to a data frame so we can pivot it and print
    it to the console.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note  The parameters for the Azure OpenAI are quite strict, with `max_ tokens`
    set to 5, `temperature` set to 0, and `top_p` set to 1.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.3 LLM-based evaluation metrics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Function to load the files from disk'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Function to calculate various evaluation metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sets up the prompt for G-Eval'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Completion API to run the evaluation'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Dictionary to store the evaluation results'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Loops over the evaluation metrics and summaries'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Checks if result is not empty and if it is a number'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Evaluation result stored in a dictionary'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Converts the dictionary to a Pandas DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Pivots the DataFrame to allow easy visualization'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see how we interpret these results and what these scores mean:'
  prefs: []
  type: TYPE_NORMAL
- en: Coherence measures how logically and smoothly ideas transition from one sentence
    to another. A score of 5 for the LLM summary indicates it presents information
    logically, is well-organized, and is easy for readers to follow. The traditional
    NLP summary, with a score of 1, likely struggles with disjointed ideas or lacks
    logical flow, which makes it difficult for readers to understand the sequence
    or connection of thoughts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistency relates to the absence of contradictions within the text and maintaining
    the same standards throughout the summary. The LLM’s high score of 5 suggests
    it maintains a uniform tone, style, and factual accuracy. While good, the traditional
    NLP’s score of 4 indicates minor problems with maintaining these elements uniformly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fluency assesses the text’s smoothness and the language’s naturalness. A score
    of 3 for the LLM indicates moderate fluency; the language is generally clear but
    might have some awkward phrasing or complexity that could impede readability.
    The traditional NLP, scoring lower, might exhibit more significant problems such
    as grammatical errors or unnatural sentence structures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relevance measures how well the summary addresses the main points and purpose
    of the original content. The LLM’s score of 5 suggests that it effectively captures
    and focuses on the key elements of the original text, providing a summary that
    meets the informational needs of the reader. The traditional NLP, with a score
    of 2, likely includes some relevant information but misses important details or
    includes irrelevant content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we have seen, assessing LLMs requires more than traditional tasks and includes
    more difficult benchmarks that measure higher-level understanding, logic, and
    adaptation skills. Some of these benchmarks, such as HELM, HEIM, HellaSWAG, and
    MMLU (Massive Multitask Language Understanding), are notable for their difficulty
    and scope.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.3 HELM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: HELM (Holistic Evaluation of Language Models) [6] is a holistic framework for
    evaluating foundational models introduced by Stanford University. The framework
    aims to comprehensively assess language models, focusing on their abilities, limitations,
    and associated risks. Developed to enhance model transparency, HELM offers a more
    detailed understanding of model performance across diverse scenarios. It categorizes
    the extensive range of potential scenarios and metrics relevant to language models.
    A subset of these scenarios and metrics is then evaluated based on their coverage
    and practicality, ensuring that HELM is a practical and useful tool for enterprises.
  prefs: []
  type: TYPE_NORMAL
- en: The HELM approach uses a multimetric evaluation, assessing various factors such
    as accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency
    for each chosen scenario. It consists of large-scale evaluations of different
    language models performed under standardized conditions to guarantee comparability.
    Moreover, HELM promotes openness by sharing all model prompts and completions
    with the public for further analysis. This is supplemented by a modular toolkit
    that facilitates continuous benchmarking within the community.
  prefs: []
  type: TYPE_NORMAL
- en: For enterprises considering the use of GenAI language models, HELM provides
    valuable information to make informed choices. It allows for the comparison of
    different models on various metrics, aiding in the selection of the most suitable
    model. Moreover, HELM helps mitigate risks by assessing potential harms such as
    bias and toxicity, enabling enterprises to identify and address problems before
    real-world application. The transparency and trust promoted by HELM through access
    to raw model predictions and evaluation data further enhance understanding and
    confidence in using LMs within an organization.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.4 HEIM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Stanford University introduced a benchmark called Holistic Evaluation of Text-To-Image
    Models (HEIM) [7] to provide a complete quantitative analysis of the strengths
    and weaknesses of text-to-image models. Unlike other evaluations measuring text–image
    alignment and image quality, HEIM examines 12 important aspects of using models
    in the real world. Some of these aspects are aesthetics, originality, reasoning,
    knowledge, bias, toxicity, and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: HEIM takes a holistic approach to evaluating the text-to-image models by curating
    62 scenarios. This holistic approach reveals that no single model excels in all
    areas, highlighting different strengths and weaknesses among various models.
  prefs: []
  type: TYPE_NORMAL
- en: 'HEIM should be a key evaluation criterion for enterprises, as it provides a
    transparent and standardized way to assess text-to-image models. By understanding
    the strengths and limitations of these models, enterprises can make informed decisions
    about which models to use for specific tasks or services. Moreover, the evaluation
    helps identify potential risks such as bias or toxicity, which could have legal
    and reputational implications for businesses. Consequently, for enterprises building
    and deploying GenAI applications to production, the HEIM benchmark offers valuable
    insights across the following four dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Model selection*—HEIM highlights that no single model excels in all aspects.
    Enterprises must carefully evaluate and select models based on their application’s
    specific requirements. For example, applications focused on artistic creation
    might prioritize aesthetics and originality, while those requiring factual accuracy
    might focus on alignment and knowledge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Risk mitigation*—HEIM emphasizes evaluating bias, toxicity, and fairness.
    Enterprises must ensure their applications are ethically sound and avoid perpetuating
    harmful stereotypes or generating inappropriate content. This necessitates careful
    model selection, fine-tuning, and implementation of safety measures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Performance optimization*—Evaluating reasoning, robustness, and multilinguality
    is crucial for ensuring application reliability and user satisfaction. Enterprises
    must select models that perform well across diverse scenarios and user inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Efficiency considerations*—Image generation efficiency affects user experience
    and operational costs. Enterprises should consider the tradeoffs between model
    size, speed, and resource requirements when selecting and deploying models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will show an example and explain how we can apply HEIM to think about models.
    As mentioned before, HEIM assesses text-to-image models by creating scenarios
    that cover the 12 aspects it measures. If we wanted to test one of those 12 aspects—the
    text–image alignment aspect—HEIM might use a scenario where the model gets a complicated
    textual prompt and then checks how well the image it generates matches the prompt
    details and context. Here’s a possible evaluation scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt*—“A futuristic cityscape at dusk with flying cars and neon signs reflecting
    in the water below.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model generation*—The model generates an image based on the prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Evaluation*—Human evaluators or automated metrics assess the image on various
    factors:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the image accurately depict a cityscape at dusk?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there flying cars and neon signs as described?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a reflection in the water, and how realistic does it look?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, how well does the image align with the prompt?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model’s performance in this scenario would contribute to its overall score
    in text-image alignment. Similar scenarios would be created for other aspects,
    such as image quality, originality, reasoning, and so forth. The comprehensive
    evaluation across all 12 aspects provides insights into the model’s capabilities
    and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: HEIM’s approach ensures that models are evaluated on their ability to generate
    visually appealing images and their understanding of the text, creativity, and
    potential biases or ethical concerns. This holistic evaluation is crucial for
    enterprises, as it helps them choose models that align with their values and needs,
    while being aware of the risks involved.
  prefs: []
  type: TYPE_NORMAL
- en: More details on HEIM, including the leaderboard, dataset, and other dependencies
    such as model access unified APIs, can be found at [https://crfm.stanford.edu/helm/heim/latest](https://crfm.stanford.edu/helm/heim/latest).
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.5 HellaSWAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: HellaSWAG [8] is a challenging benchmark that tests AI models’ common-sense
    reasoning skills. It improves on its previous version, SWAG, by adding a more
    diverse and complex set of multiple-choice questions that require the models to
    do more than just language processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'HellaSWAG is a task in which each question presents a scenario with four possible
    endings, and the LLM must choose the most fitting ending among the options. For
    instance, the following question is from HellaSWAG’s dataset. The question consists
    of the context given to the LLM and the four options, which are the possible endings
    for that context. Only one of these options makes sense with common-sense reasoning.
    In this example, option C is highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: A woman is outside with a bucket and a dog. The dog is running around trying
    to avoid a bath. She…
  prefs: []
  type: TYPE_NORMAL
- en: A. rinses the bucket off with soap and blow-dries the dog’s head.
  prefs: []
  type: TYPE_NORMAL
- en: B. uses a hose to keep it from getting soapy.
  prefs: []
  type: TYPE_NORMAL
- en: C. gets the dog wet, then it runs away again.
  prefs: []
  type: TYPE_NORMAL
- en: D. gets into a bathtub with the dog.
  prefs: []
  type: TYPE_NORMAL
- en: The model’s selections are compared to the correct answers to measure its performance.
    This testing method evaluates the LLM’s knowledge of language nuances and its
    deeper comprehension of common-sense logic and the complexities of real-world
    situations. Doing well in HellaSWAG means that a model has a nuanced understanding
    and reasoning ability, which is essential for applications that need a sophisticated
    grasp of context and logic.
  prefs: []
  type: TYPE_NORMAL
- en: If interested, the HellaSWAG’s dataset is available online via HuggingFace at
    [https://huggingface.co/datasets/Rowan/hellaswag](https://huggingface.co/datasets/Rowan/hellaswag).
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.6 Massive Multitask Language Understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Massive Multitask Language Understanding (MMLU) [9] benchmark assesses the
    breadth and depth of an LLM’s knowledge on various topics and domains. MMLU stands
    out by covering hundreds of tasks linked to different knowledge areas, from science
    and literature to history and social sciences.
  prefs: []
  type: TYPE_NORMAL
- en: MMLU was developed in response to the observation that while many language models
    excelled at NLP tasks, they struggled with natural language understanding (NLU).
    Previous benchmarks such as GLUE and SuperGLUE were quickly mastered by LLMs,
    indicating a need for more rigorous testing. MMLU aimed to fill this gap by testing
    language understanding and problem-solving abilities using knowledge encountered
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: The benchmark includes questions from various subjects, including humanities,
    social sciences, hard sciences, and other specialized areas, varying from elementary
    to advanced professional levels. This approach was unique because most NLU benchmarks
    at the time focused on elementary knowledge. MMLU sought to push the boundaries
    by testing specialized knowledge, a new challenge for LLMs2.
  prefs: []
  type: TYPE_NORMAL
- en: Like HellaSWAG, MMLU often uses a multiple-choice format where the model must
    identify the right answer from a set of options. The overall accuracy across these
    diverse tasks shows the model’s general performance, comprehensively measuring
    its language understanding and knowledge application across domains. High performance
    on MMLU means that a model has a large amount of information and is skilled at
    using this knowledge to answer questions and problems correctly. This wide range
    of understanding is essential for developing LLMs that can handle the complexities
    of human knowledge and language subtly and informatively.
  prefs: []
  type: TYPE_NORMAL
- en: While comprehensive, the MMLU faces several limitations. First, the performance
    of language models on this test can be constrained by the diversity and quality
    of their training data. If certain topics are underrepresented, models may underperform
    in those areas. Additionally, MMLU primarily assesses whether models can generate
    correct answers but does not evaluate how they arrive at these conclusions. It
    is crucial in applications where the reasoning process is as important as the
    outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Another significant concern is the potential for bias within the test. Because
    MMLU is constructed from various sources, it may inadvertently include biases
    from these materials, affecting the fairness of model assessments, especially
    on sensitive topics. Furthermore, there is a risk that models could be overfitting
    to the specific MMLU format and style, optimizing for test performance rather
    than genuine understanding and applicability in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the logistical demands of running such a comprehensive test are substantial,
    requiring significant computational resources that might not be available to all
    researchers. This limitation can restrict the range of insights gleaned from the
    test. Finally, the scalability of knowledge poses a challenge; as fields evolve,
    the test must be updated regularly to stay relevant, necessitating ongoing resource
    investment. These factors highlight the complexities of using MMLU as a benchmark
    and underscore the need for continuous refinement to maintain its efficacy and
    relevance.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.7 Using Azure AI Studio for evaluations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As an Azure customer, you can easily use these evaluations, as they are already
    integrated into Azure AI Studio. It includes tools for recording, seeing, and
    exploring detailed evaluation metrics, with the option to use custom evaluation
    flows and batch runs without evaluation. With AI Studio, we can make an evaluation
    run from a test dataset or flow with ready-made evaluation metrics. For more adaptability,
    we can make our evaluation flow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you can use AI-assisted metrics to evaluate your model, make sure you
    have these things prepared:'
  prefs: []
  type: TYPE_NORMAL
- en: A test dataset in either CSV or JSONL format. If you don’t have a dataset ready,
    you can also enter data by hand from the UI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of these models deployed: GPT-3.5 models, GPT-4 models, or Davinci models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A compute instance runtime to run the evaluation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 12.1 shows an example of how to set up the various tests. More details
    can be found at [https://mng.bz/4pMj](https://mng.bz/4pMj).
  prefs: []
  type: TYPE_NORMAL
- en: If you are not using Azure, other similar options exist, such as DeepEval (see
    the next section). This open source LLM evaluation framework allows running multiple
    LLM metrics and makes this process quite easy.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F01_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 Azure AI Studio evaluations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '12.3.8 DeepEval: An LLM evaluation framework'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DeepEval is a free LLM evaluation framework that works like `pytest` (a popular
    testing framework for Python) but focuses on unit-testing LLM outputs. DeepEval
    uses the newest research to assess LLM outputs based on metrics such as hallucination,
    answer relevancy, and RAGAS (Retrieval Augmented Generation Assessment). These
    metrics rely on LLMs and other NLP models that run on your computer locally for
    evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: DeepEval supports many useful features for enterprise applications. It can evaluate
    whole datasets simultaneously, create custom metrics, compare any LLM to popular
    benchmarks, and evaluate in real-time in production. In addition, it works with
    tools such as LlamaIndex and Hugging Face and is automatically connected to Confident
    AI for ongoing evaluation of your LLM application throughout its lifetime.
  prefs: []
  type: TYPE_NORMAL
- en: 'The framework also offers a platform for recording test outcomes, measuring
    metrics’ passes/fails, selecting and comparing the best hyperparameters, organizing
    evaluation test cases/datasets, and monitoring live LLM responses in production.
    This book does not cover DeepEval in detail; more details are available at their
    GitHub repository: [https://github.com/confident-ai/deepeval](https://github.com/confident-ai/deepeval).
    Figure 12.2 shows a simple example of a test metric.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH12_F02_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 DeepEval test session example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 12.4 New evaluation benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the last 12 to 18 months, we have seen that AI models have reached performance
    saturation on established industry benchmarks such as ImageNet, SQuAD, and SuperGLUE,
    to name a few. This has spurred the industry to develop more challenging benchmarks.
    Some of the newer ones are SWE-bench for coding, MMMU for general reasoning, MoCa
    for moral reasoning, and HaluEval for hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.1 SWE-bench
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To measure the progress of GenAI systems that can code, we need more difficult
    tasks to evaluate them. SWE-bench [10] is a dataset containing nearly hundreds
    of software engineering problems from real-world GitHub and Python repositories.
    It poses a harder challenge for AI coding skills, requiring that systems make
    changes across multiple functions, deal with different execution environments,
    and perform complex reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: The SWE-bench dataset evaluates systems’ abilities to solve GitHub problems
    automatically. It collects 2,294 problem-pull request pairs from 12 popular Python
    repositories. The evaluation is performed by verifying the proposed solutions
    using unit tests and comparing them to the post-PR behavior as the reference solution.
  prefs: []
  type: TYPE_NORMAL
- en: The primary evaluation metric for SWE-bench is the *percentage of resolved task
    instances.* In other words, it measures how effectively a model can address the
    given problems—the higher the percentage of resolved instances, the better the
    model’s performance. More details on SWE-bench can be found at [https://www.swebench.com](https://www.swebench.com).
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.2 MMMU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MMMU (Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark)
    [9] is a new benchmark designed to evaluate multimodal models’ capabilities on
    tasks requiring college-level subject knowledge and expert-level reasoning across
    multiple disciplines. It includes 11.5K multimodal questions from college exams,
    quizzes, and textbooks, covering six core disciplines: art and design, business,
    science, health and medicine, humanities and social science, and tech and engineering.
    These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous
    image types, such as charts, diagrams, maps, tables, music sheets, and chemical
    structures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'MMMU is unique because it focuses on advanced perception and reasoning with
    domain-specific knowledge and challenging models to perform tasks that experts
    face. The benchmark has been used to evaluate several open source LLMs and proprietary
    models such as GPT-4V, highlighting the substantial challenges MMMU poses. Even
    the advanced models only achieve accuracies between 56% and 59%, indicating significant
    room for improvement. It operates by assessing LLMs’ ability to perceive, understand,
    and reason across different disciplines and subfields using various image types.
    The benchmark focuses on three essential skills in LLMs: perception, knowledge,
    and reasoning.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that it might seem that the MMLU discussed earlier is the same as MMMU;
    however, they are different. MMLU evaluates language models on a wide range of
    text-based tasks across various domains, focusing solely on language understanding.
    In contrast, MMMU assesses multimodal models, requiring both visual and textual
    comprehension across specialized disciplines, thus challenging models with complex,
    domain-specific multimodal content.
  prefs: []
  type: TYPE_NORMAL
- en: The MMMU benchmark presents several key challenges for multimodal models, which
    include
  prefs: []
  type: TYPE_NORMAL
- en: '*Comprehensiveness*—Since the benchmark includes a wide array of 11.5K college-level
    problems across broad disciplines, the models must have a broad knowledge base
    and understanding across multiple fields.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Highly heterogeneous image types*—The questions involve 30 different types
    of images, such as charts, diagrams, maps, tables, music sheets, and chemical
    structures. This means the models must be able to interpret and understand various
    visual information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Interleaved text and images*—Many questions feature a mix of text and images,
    requiring models to process and integrate information from both modalities to
    arrive at the correct answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Expert-level perception and reasoning*—The tasks demand deep subject knowledge
    and expert-level reasoning, akin to the challenges faced by human experts in their
    respective fields.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These challenges aim to stretch the limits of existing multimodal models, testing
    their capacity to do sophisticated perception, analytical thinking, and domain-specific
    reasoning. The questions demand a profound understanding of the topic and the
    ability to use knowledge in intricate scenarios, even for human experts.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.3 MoCa
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The MoCa (Measuring Human-Language Model Alignment on Causal and Moral Judgment
    Tasks) [11] framework evaluates how well LLMs align with human participants in
    making causal and moral judgments about text-based scenarios. AI models can perform
    well in language and vision tasks, but their ability to make moral decisions,
    especially those that match human opinions, is unclear. To investigate this topic,
    a group of Stanford researchers created a new dataset (MoCa) of human stories
    with moral aspects. Here, we will look at the details of each:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Causal judgments*—Humans intuitively understand events, people, and the world
    around them by organizing their understanding into intuitive theories. These theories
    help us reason about how objects and agents interact with one another, including
    concepts related to causality. The MoCa framework collects a dataset of stories
    from cognitive science papers and annotates each story with the factors they investigate.
    It then tests whether LLMs make causal judgments about text scenarios that align
    with those of humans. On an aggregate level, alignment has improved with more
    recent LLMs. However, statistical analyses reveal that LLMs weigh various factors
    in a different way than human participants.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Moral judgments*—Tasks evaluate agents in narrative-like text for moral reasoning.
    These tasks and datasets vary in structure, ranging from free-form anecdotes to
    more structured inputs. The MoCa framework assesses how well LLMs align with human
    moral intuitions in these scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main metric used in MoCa is the Area under the Receiver Operating Characteristic
    (AuROC) curve, which measures the alignment between LLMs and human judgments.
    Furthermore, accuracy serves as a secondary metric for comparison between models.
  prefs: []
  type: TYPE_NORMAL
- en: A higher score indicates closer alignment with human moral judgment. The study
    yielded intriguing results. No model perfectly matches human moral systems. However,
    newer, larger models such as GPT-4 and Claude show greater alignment with human
    moral sentiments than smaller models such as GPT-3, suggesting that as AI models
    scale, they are gradually becoming more morally aligned with humans.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, MoCa provides insights into how LLMs handle causal and moral reasoning,
    shedding light on their implicit tendencies and alignment (or lack thereof) with
    human intuitions. We can get more details on MoCa at [https://moca-llm.github.io](https://moca-llm.github.io).
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.4 HaluEval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: HaluEval [12] benchmark is a large-scale evaluation framework designed to assess
    LLMs’ performance in recognizing hallucinations. In this context, hallucinations
    refer to LLM-generated content that conflicts with the source or cannot be verified
    by factual knowledge. The benchmark includes a collection of generated and human-annotated
    hallucinated samples.
  prefs: []
  type: TYPE_NORMAL
- en: A two-step framework involving sampling-then-filtering is used to create these
    samples, often based on responses from models such as ChatGPT. Human labelers
    also contribute by annotating hallucinations in the responses. The empirical results
    from HaluEval suggest that LLMs, including ChatGPT, can generate hallucinated
    content, particularly on specific topics, by fabricating unverifiable information.
  prefs: []
  type: TYPE_NORMAL
- en: The study also explores how good current LLMs are at finding hallucinations.
    It can lead LLMs to spot hallucinations in tasks such as question-answering knowledge-grounded
    dialogue and text summarization. The results show that many LLMs have difficulties
    with these tasks, emphasizing that hallucination is a serious, persistent problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The HaluEval benchmark includes 5,000 general user queries with ChatGPT responses
    and 30,000 task-specific examples from three tasks: question answering, knowledge-grounded
    dialogue, and text summarization. It''s a significant step toward understanding
    and improving the reliability of LLMs in generating accurate and verifiable content.
    HaluEval’s GitHub repository at [https://github.com/RUCAIBox/HaluEval](https://github.com/RUCAIBox/HaluEval)
    provides more details.'
  prefs: []
  type: TYPE_NORMAL
- en: 12.5 Human evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Human evaluation plays a crucial role in understanding the quality of LLMs,
    as it captures nuances, context, and potential biases that automated metrics might
    overlook. For enterprises to conduct effective human evaluations, they need to
    start by defining clear criteria to guide the assessment of LLM outputs. These
    criteria should cover aspects such as accuracy, fluency, relevance, and the presence
    of any biases. To ensure consistency and objectivity, enterprises should develop
    comprehensive guidelines and rubrics for evaluators to follow.
  prefs: []
  type: TYPE_NORMAL
- en: When choosing the right evaluation methods, enterprises have several options.
    They can either engage domain experts or trained annotators for detailed assessments
    or opt for crowdsourcing platforms such as Amazon Mechanical Turk ([https://www.mturk.com](https://www.mturk.com))
    to access a wider pool of evaluators. The next step involves data collection and
    annotation, which requires user-friendly interfaces and clear instructions to
    ensure quality and consistency. It’s also important to collect enough data to
    yield statistically significant results.
  prefs: []
  type: TYPE_NORMAL
- en: After collecting data, a thorough analysis is necessary. Enterprises should
    employ statistical methods to measure interrater agreement and confirm the reliability
    of the evaluations. The insights derived from this process should then be used
    to make iterative improvements to LLMs, including adjustments to the training
    data, model architecture, and prompt engineering. Regular human evaluations are
    essential for monitoring progress and pinpointing areas that need further improvement.
  prefs: []
  type: TYPE_NORMAL
- en: While human evaluation is invaluable, it does come with its challenges. It can
    be expensive and time-consuming, especially when dealing with large datasets.
    There’s also the risk of subjectivity and bias in human judgments. However, these
    problems can be mitigated by providing clear guidelines, adequate training for
    evaluators, and employing proper aggregation methods.
  prefs: []
  type: TYPE_NORMAL
- en: Several tools and platforms are available to help streamline the human evaluation
    process. Crowdsourcing platforms provide access to a diverse workforce, while
    annotation tools offer efficient data-labeling features. Evaluation frameworks
    are also available, including libraries of metrics and scripts designed specifically
    for LLM evaluation and to support human evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Some examples of tools that assist with annotations are Label Studio ([https://labelstud.io](https://labelstud.io)),
    which offers both open source and enterprise offerings. Prodigy ([https://prodi.gy](https://prodi.gy))
    is another annotation tool that supports text, images, videos, and audio. Text-only
    annotation tools also exist, such as Labellerr ([https://www.labellerr.com](https://www.labellerr.com)).
  prefs: []
  type: TYPE_NORMAL
- en: Some companies specialize in LLM evaluation, offering robust testing frameworks
    and various resources to assist the evaluation process. For enterprises that are
    not comfortable implementing their evaluation frameworks using tools such as PromptFlow,
    which we saw earlier in the previous chapter, Weights and Biases ([https://wandb.ai](https://wandb.ai)),
    and so forth, there are new companies that specialize in LLM evaluations, such
    as Giskard ([https://www.giskard.ai](https://www.giskard.ai)).
  prefs: []
  type: TYPE_NORMAL
- en: By following these steps and utilizing the available resources, enterprises
    can implement a structured and effective human evaluation process for LLMs. It’s
    important to remember that this is an evolving field, and staying up to date on
    the latest developments and training is crucial for maintaining the quality of
    evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Benchmarking systems are essential for verifying the performance of GenAI and
    LLMs, directing enhancements, and confirming real-world suitability. They assist
    us in evaluating the efficiency and preparedness of generative AI and LLMs for
    deployment in production environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The correlation between evaluations and LLMs is a new and emerging area. We
    should use traditional metrics, LLM task-specific benchmarks, and human evaluation
    to assess LLM performance and ensure its suitability for real-world applications.
    G-Eval is a reference-free evaluation method using LLMs to assess the generated
    text’s coherence, consistency, and relevance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conventional metrics such as BLEU, ROUGE, and BERTScore help measure text generation
    quality and evaluate text numerically based on n-gram matching or semantic similarity.
    They do face some challenges in fully representing contextual meaning and paraphrasing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM-specific benchmarks measure how well LLMs perform tasks such as text classification,
    sentiment analysis, and question answering. They introduce new metrics such as
    groundedness, coherence, fluency, and GPT similarity that help assess the quality
    of LLM outputs and how close they are to human-like standards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective evaluation methods for meaningful LLM evaluations include testing
    in relevant settings, creating fair prompts, conducting ethical reviews, and assessing
    the user experience. These include advanced benchmarks such as HELM, HEIM, HellaSWAG,
    and MMLU, which test LLMs against various scenarios and capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools such as Azure AI Studio and the DeepEval framework enable effective LLM
    evaluations in an enterprise context. These tools allow the development of customized
    evaluation workflows, batch executions, and the incorporation of real-time evaluations
    into production settings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
