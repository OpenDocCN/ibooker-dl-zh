<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">4</span> </span> <span class="chapter-title-text">Understanding what your users really want</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Recognizing indicators of weak understanding</li> 
    <li class="readable-text" id="p3">Measuring chatbot understanding</li> 
    <li class="readable-text" id="p4">Assessing your chatbot’s current state</li> 
    <li class="readable-text" id="p5">Collecting and preparing log data to measure chatbot understanding</li> 
    <li class="readable-text" id="p6">Interpreting initial log data</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>A good chatbot experience is generally associated with the chatbot identifying (understanding) what the user wants. This is one of the key metrics you will use to measure performance. Sometimes a chatbot is deployed and has great initial understanding (or at least “good enough” for a pilot program). Over time, though, you may notice that it is returning wrong answers. Maybe your users are complaining more, either directly to the chatbot (“That doesn’t answer my question!”) or in the form of survey responses. Engagement could be trending downward while abandonment trends upward. You may start hearing from the call center about escalations that should have been handled in the virtual assistant. These are all indications that your conversational solution might be suffering from weak understanding.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>In theory, chatbots should get better over time, but it is not uncommon to see a decline in understanding. We want to help you recognize when and why this could be happening in your solution. We will explain how to avoid some of the pitfalls and plan for common eventualities in the lifecycle of your solution. In this chapter, we will explore what it means for your conversational AI to have “good performance” in terms of its ability to correctly identify or classify a user’s goal (i.e., to understand the user). We will also offer techniques for preparing data for use in measuring a classifier’s performance or assessing generated responses. </p> 
  </div> 
  <div class="readable-text" id="p9"> 
   <h2 class=" readable-text-h2"><span class="num-string">4.1</span> Fundamentals of understanding</h2> 
  </div> 
  <div class="readable-text" id="p10"> 
   <p>Being understood is a fundamental aspect of human communication. In a conversational AI, we use natural language processing techniques to try to understand what our user wants or needs. Because the scope of things a user could want is nearly infinite, and the way they might combine words to express those wants or needs is also infinite, this is a very difficult problem to solve. </p> 
  </div> 
  <div class="readable-text" id="p11"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.1.1</span> The impact of weak understanding</h3> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p>Not being understood by a chatbot is probably the biggest source of frustration for a user. They came to your chatbot to get answers, and they may get an answer, but it may have nothing to do with their question. Perhaps the chatbot instructed them to rephrase their question, so they come up with different words to express the same goal. Sometimes this works, and other times they get a response asking them to rephrase (again!). Oftentimes, as in figure 4.1, your users will end up asking for an agent after one or two failures. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p13">  
   <img alt="figure" src="../Images/CH04_F01_Freed2.png" width="1029" height="393"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.1</span> Accuracy or coverage problems frustrate the user because it takes more time—and sometimes multiple contacts—to achieve their goal. It also causes the user to lose confidence in the virtual agent.</h5>
  </div> 
  <div class="readable-text" id="p14"> 
   <p>If this is happening to your users, your chatbot most likely has a problem with <em>accuracy</em> (the chatbot’s ability to match what it heard against what it knows), <em>coverage</em> (the range of topics that your solution is expected to know about), or both. From the outside, it is impossible to tell which is the underlying root cause. For that, you are going to need to collect data. Without that information, it is difficult to know what to fix—and fixing the wrong thing can obfuscate or compound existing problems. Before you know it, your conversational solution becomes costly and difficult to maintain. Worse still, it is not delivering the value it promised (by failing to reduce, or perhaps even increasing, the need for human intervention).</p> 
  </div> 
  <div class="readable-text intended-text" id="p15"> 
   <p>One of the biggest success factors for a chat solution is how an organization approaches the ongoing maintenance of the solution. Ideally, the project sponsor and support team will have set the expectation that the solution needs iterative improvements—especially in the beginning—as it is exposed to more data from real-world users. Despite advances in autolearning, large language models, and generative AI, chatbots don’t tend to magically get better over time. </p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p16"> 
    <h5 class=" callout-container-h5 readable-text-h5">Expect to commit support resources throughout the bot’s lifecycle</h5> 
   </div> 
   <div class="readable-text" id="p17"> 
    <p>Does the organization feel that a chatbot should be a “set it and forget it” solution? Is there a lack of commitment to the ongoing care and feeding of the virtual assistant? These are the red flags of neglect, and they pretty much guarantee eventual failure. </p> 
   </div> 
   <div class="readable-text" id="p18"> 
    <p>A chatbot is essentially a digital employee. Much like a human resource, it requires initial training plus occasional retraining, reinforcement, and the opportunity to acquire new skills. </p> 
   </div> 
  </div> 
  <div class="readable-text" id="p19"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.1.2</span> What causes weak understanding?</h3> 
  </div> 
  <div class="readable-text" id="p20"> 
   <p>These are the most common reasons a chatbot will exhibit a decline in understanding: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p21"> Manufactured training data (trained examples that do not reflect a representative user’s vocabulary) </li> 
   <li class="readable-text" id="p22"> Insufficient scope or gaps in topic coverage </li> 
   <li class="readable-text" id="p23"> New information in the world that is not passed on to the virtual assistant </li> 
   <li class="readable-text" id="p24"> Lack of a vetting or gatekeeping process when adding new intents, updating existing intents, or changing model inference parameters </li> 
  </ul> 
  <div class="readable-text" id="p25"> 
   <p>That last point—lack of a gatekeeping process—results in the types of weak understanding problems that are the most difficult to resolve. Without oversight by a knowledgeable owner or a dedicated model-training team, unvetted changes can quickly compound the problem of weak understanding. In traditional classifiers, model updates made by someone who is not familiar with the entire training set often introduce duplications, intent training conflicts, and unjustified disparities in the volume of training examples across intents. Untested model parameter or prompt changes will cause unexpected behavior in a generative model.</p> 
  </div> 
  <div class="readable-text intended-text" id="p26"> 
   <p>In fact, we saw this happen with a client who had been making changes to their classifier training set, growing the total intents from 21 to 53 over the course of nine production deployments. The business did not see an effect right away; rather, over time the result of these untested changes manifested as poor survey results, incomplete journeys, unnecessary escalations, and lots of negative feedback. Subject matter experts reported that the bot was giving wrong answers for questions that it used to get right. These are classic symptoms of weak understanding, but they could not pinpoint exactly when it all started. A series of retroactive experiments against their prior versions told the story, which is shown in figure 4.2.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p27">  
   <img alt="figure" src="../Images/CH04_F02_Freed2.png" width="1013" height="873"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.2</span> A retroactive assessment of classifier performance shows a hard-won lesson on the effect of untested changes over time. Had each version been tested as part of a predeployment process, the team would have postponed any version updates until the classifier problems were resolved. It took several weeks to get the classifier back into good working order. </h5>
  </div> 
  <div class="readable-text" id="p28"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.1.3</span> How do we achieve understanding with traditional conversational AI?</h3> 
  </div> 
  <div class="readable-text" id="p29"> 
   <p>Traditional (non-generative) conversational AI systems are taught by ingesting examples of user requests grouped by <em>intents</em>, sometimes referred to as<em> classifications </em>or<em> clusters</em>. Intents contain a variety of paraphrases that all express the same goal. Some systems also incorporate <em>entities</em>, which are like keywords that further refine the meaning or specifications of a request. </p> 
  </div> 
  <div class="readable-text intended-text" id="p30"> 
   <p>The conversational logic is configured to identify an intent (or a combination of intent + entity) and take an action based on that identification. This action could be as simple as answering a question, or it could initiate a complex transactional exchange. Table 4.1 shows examples of intents, entities, and potential next steps in a conversational exchange.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p31"> 
   <h5 class=" browsable-container-h5"><span class="num-string">Table 4.1</span> Example utterances may be handled differently based on the presence or absence of entities.</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Utterance 
       </div></th> 
      <th> 
       <div>
         Intent 
       </div></th> 
      <th> 
       <div>
         Entity 
       </div></th> 
      <th> 
       <div>
         Possible next step 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  “How many bags can I check?” <br/></td> 
      <td>  <code>Bag_Allowance</code> <br/></td> 
      <td/> 
      <td>  Display bag check policy <br/></td> 
     </tr> 
     <tr> 
      <td>  “I want to book a flight” <br/></td> 
      <td>  <code>Book_Flight</code> <br/></td> 
      <td/> 
      <td>  Collect destination <br/></td> 
     </tr> 
     <tr> 
      <td>  “I need a one-way ticket to Costa Rica” <br/></td> 
      <td>  <code>Book_Flight</code> <br/></td> 
      <td>  Costa Rica <br/></td> 
      <td>  Collect departure details <br/></td> 
     </tr> 
     <tr> 
      <td>  “I’d like to upgrade my seat to first class” <br/></td> 
      <td>  <code>Flight_Upgrade</code> <br/></td> 
      <td>  first class <br/></td> 
      <td>  Initiate upgrade process <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p32"> 
   <p>The types of bots that use traditional classification technology tend to be topic routing agents, question/answer (FAQ) bots, and, to some extent, process-oriented (self-service) assistants. Keep in mind that classification-based bots rely on a predefined set of question topics (intents). You need to know in advance what questions you expect your bot to encounter. </p> 
  </div> 
  <div class="readable-text intended-text" id="p33"> 
   <p>As a matter of practicality, the range of topics or intents that you teach your system will be specific to your domain and your solution’s use case or purpose. As solution owners, one of our primary and ongoing tasks is to tune our system to correctly understand the greatest volume of user demands. Finding the ideal balance between topic breadth and topic depth can be difficult and often involves tradeoffs. For example, it is not cost effective to train a classifier to understand every possible topic. Furthermore, attempting to do so can weaken its understanding of topics that are salient to your users. </p> 
  </div> 
  <div class="readable-text intended-text" id="p34"> 
   <p>When an organization tries to train a classifier to detect every possible topic, the classifier’s ability to see clear distinctions across all intents can be diminished. If the intents trained in your system aren’t representative of user demand (meaning you have a large number of low-volume topics), they tend to cause problems with accuracy and confidence. Figure 4.3 illustrates a “long tail” chart; the greatest business value for a classifier-based chatbot is typically realized by focusing on the high-to-moderate volume requests. Low-volume requests are typically handled by some sort of fallback mechanism, such as escalation, search, or generative AI.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p35">  
   <img alt="figure" src="../Images/CH04_F03_Freed2.png" width="754" height="499"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.3</span> As request volume tapers off to the right, the chart has the appearance of a long tail. Each use case must define the optimum tradeoff of depth and breadth as it relates to topic coverage. The cutoff point for business value is usually somewhere in the moderate-volume range. This is not to say that all low-volume requests should be excluded, but there may be diminishing returns associated with extending your classifier’s coverage for these topics. </h5>
  </div> 
  <div class="readable-text" id="p36"> 
   <p>Prior to initial launch, you need to make some predictions about which topics will be most important for your bot to understand. These predictions are often based on logs from human interactions, call center metrics, focus groups, surveys, or other research or information-gathering activities. Your focus should be on training your model to be good at recognizing these requests, as well as any other ancillary conversational maintenance intents (such as greetings, chitchat, repeat, and escalate). Once your solution is in production, you’ll need to validate those predictions by collecting and analyzing data about your conversational interactions. </p> 
  </div> 
  <div class="readable-text" id="p37"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.1.4</span> How do we achieve understanding with generative AI?</h3> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>How does a generative AI model achieve understanding? This is a trick question, because generative AI does not so much understand the meaning of an utterance as it creates new data that looks like the data it was trained on, using the utterance as a reference point. This is a nuanced distinction, but with generative AI, we try to simulate understanding by instructing a model to assess the input from a certain viewpoint and then generate a specific type of output. </p> 
  </div> 
  <div class="readable-text intended-text" id="p39"> 
   <p>Particularly for conversational AI, our goal is to generate output that reflects or addresses the user’s request with specificity and/or personalization (not just a high-level categorization, such as topic classification or entity extraction). Figure 4.4 demonstrates the fundamental difference between classification model outputs and generative model outputs. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p40">  
   <img alt="figure" src="../Images/CH04_F04_Freed2.png" width="1014" height="374"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.4</span> Traditional classification models use supervised learning to predict one of several predefined classifications. They look for the intent, or meaning, of a user input. Generative models use decoding transformers to create a text completion. They predict the next sequence of tokens (loosely, words or characters) that are most likely to occur after the user input. </h5>
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p41"> 
    <h5 class=" callout-container-h5 readable-text-h5">A quick note on LLM foundation architectures</h5> 
   </div> 
   <div class="readable-text" id="p42"> 
    <p><em>Encoder-only</em> architectures are best for non-generative use cases, such as training predictive models based on text embeddings. They focus on extracting meaningful context from inputs and require labeled data for fine tuning.</p> 
   </div> 
   <div class="readable-text" id="p43"> 
    <p><em>Decoder-only</em> architectures are designed explicitly for generative AI use cases. They are “trained” in an unsupervised fashion by ingesting large amounts of data. They focus on predicting the next token in a sequence and can be instructed to perform specific tasks, including classification, question answering, and summarization. </p> 
   </div> 
   <div class="readable-text" id="p44"> 
    <p>Some LLM model architectures are <em>encoder–decoder</em>, which means they can support both generative and non-generative uses cases. These are typically used in scenarios where the input is large, but the output is relatively small, such as translation or summarization. </p> 
   </div> 
  </div> 
  <div class="readable-text" id="p45"> 
   <p>Unlike traditional classifier-based AI, there is no predefined list of intents that are “in-scope” for the generative model. But like traditional AI, you still need to have good command of the domain and of the range of problems your users are likely to bring to your bot. This will inform the strategies you employ that nudge your LLM to produce responses demonstrating that the user’s input was understood. There are several effective tools at your disposal to accomplish this: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p46"> <em>Selecting the right model for the job</em><em> </em>—Some models are more optimized for conversational output (as opposed to generating code or writing an essay or news article). </li> 
   <li class="readable-text" id="p47"> <em>Prompt engineering</em><em> </em>—This technique supplies a model with inputs in order to produce optimal outputs. These inputs might include instructions, context, input data, and output indicators. Prompt engineering can often achieve a good simulation of understanding, and it can instruct the model to produce output in a conversational tone.  </li> 
   <li class="readable-text" id="p48"> <em>One-shot or few-shot prompting</em><em> </em>—You can enhance your prompt with one or more examples of the output and format you want the model to generate. </li> 
   <li class="readable-text" id="p49"> <em>Parameter tuning</em>—Parameters such as temperature, top-<em>p</em>, and top-<em>k</em> influence the randomness and diversity of the generated text. Increasing these values tends to increase the “creativity” in a generated response.  </li> 
   <li class="readable-text" id="p50"> <em>Retrieval-augmented generation (RAG)</em><em> </em>—RAG can enhance the perception that the bot understands while keeping the generated answer grounded in your domain. Many businesses employ RAG in their conversational solutions to ensure that the generated responses are based on external, verifiable facts and the latest information.  </li> 
  </ul> 
  <div class="readable-text" id="p51"> 
   <p>At the time of writing, enterprise conversational solutions most often employ generative AI as question-answering (Q&amp;A) bots. Most business-oriented chatbots that use this technology are not fully generative—they often employ a hybrid approach of classification (with predefined response pairs), task-oriented flows, and generated responses. Generated responses may be incorporated into the dialogue design, invoked as a fallback option (e.g., when classification fails to predict an intent with sufficient confidence), or both. </p> 
  </div> 
  <div class="readable-text intended-text" id="p52"> 
   <p>Generative AI can also be used to enhance classification response outputs by inserting a personalized greeting or problem summary before delivering a “canned” (preconfigured) dialogue response or launching a task flow. Done well, this can engage a chatbot user on a deeper level, exhibiting “understanding” with empathy by acknowledging the user’s specific situation.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p53"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercises</h5> 
   </div> 
   <div class="readable-text" id="p54"> 
    <p>Reflect on the solution you are currently building or supporting. Ask yourself these questions:</p> 
   </div> 
   <ol> 
    <li class="readable-text buletless-item" id="p55"> Is my solution exhibiting any symptoms of weak understanding, such as 
     <ul> 
      <li> Giving wrong answers, especially answers that are not relevant or are completely unrelated to the input topic </li> 
      <li> Taking the fallback/anything else/escalation routes more often than you would expect </li> 
      <li> Disambiguating, or clarifying the topic, more often than you would expect on a seemingly straightforward request (for solutions that employ a disambiguation feature) </li> 
      <li> Producing outdated or incorrect information </li> 
      <li> Receiving negative feedback or poor NPS scores </li> 
     </ul></li> 
    <li class="readable-text" id="p57"> How was my solution originally trained and tested? If it was deployed, was a baseline measurement taken? </li> 
    <li class="readable-text" id="p58"> Has the solution been updated to recognize new topics and produce answers that are accurate and current? </li> 
    <li class="readable-text" id="p59"> Who is allowed to make changes to the solution? Are these changes documented? Is the solution monitored after a change to ensure the change produces the intended effect? </li> 
   </ol> 
  </div> 
  <div class="readable-text" id="p60"> 
   <h2 class=" readable-text-h2"><span class="num-string">4.2</span> How is understanding measured?</h2> 
  </div> 
  <div class="readable-text" id="p61"> 
   <p>Understanding, for a chat solution, is typically measured in terms of accuracy. For a classifier, that means an ability to accurately predict the intent. For generative models, it is the ability to create correct and useful output. There are multiple methodologies and tools for measuring how well a solution understands user inputs. The approach you take will depend on which technologies your solution uses (traditional, generative, or both) and what phase you are currently in (predeployment or post).</p> 
  </div> 
  <div class="readable-text" id="p62"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.2.1</span> Measuring understanding for traditional (classification-based) AI</h3> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>Classifier performance is measured in terms of accuracy, precision, and recall. <em>Accuracy</em> is the percentage of correct predictions that were made. <em>Recall</em> refers to the classifier’s ability to identify the correct intent, while <em>precision</em> is the classifier’s ability to refrain from giving a wrong intent. Higher accuracy usually correlates to a perception of “good understanding.” A chatbot can’t deliver a predefined response or invoke the correct process-oriented flow if it does not understand the user’s intent. </p> 
  </div> 
  <div class="readable-text intended-text" id="p64"> 
   <p>You can assess your classifier’s performance using some data science techniques, such as <em>k</em>-fold cross validation or blind testing. <em>Blind testing</em> refers to the fact that a given test utterance does not already exist in the training set; i.e., the classifier has not “seen” the utterance before. Your test set may be manufactured, such as with AI-generated data, or representative (constructed from actual user utterances pulled from logs). <em>K</em>-fold and blind tests can provide information about your model’s overall accuracy, as well as report on its recall and precision. The metrics produced by such tests help identify where the model is performing well and where it might be confused. Chapter 5 contains detailed instructions for improving classifier understanding, so we will just give an overview of the testing approaches here.</p> 
  </div> 
  <div class="readable-text" id="p65"> 
   <h4 class=" readable-text-h4">Measuring understanding with k-fold cross validation</h4> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>If your chatbot has not yet been deployed, a <em>k</em>-fold cross validation test is the easiest and most accessible method for measuring accuracy because it does not require additional annotated data. It uses only your existing training set. This method essentially measures the internal consistency of your data labeling—a high accuracy score mainly indicates that your training examples were grouped with other similar examples. The process involves pulling a percentage of data out of training, creating a temporary blind test set. The remaining data is used to create a temporary classifier. Next, each blind example is run against the classifier, and the predictions are scored. Finally, the temporary blind set is folded back into the training set. This process is iterated <em>k </em>times so that every example is used as a training example and as a test utterance, but never both at the same time. </p> 
  </div> 
  <div class="readable-text intended-text" id="p67"> 
   <p>A <em>k</em>-fold test will give you a prediction of the accuracy of your classifier, assuming the data you used to train your model is representative of the inputs your model will encounter when it is deployed to production. However, this can lead to a false sense of security, especially if your training data is highly manufactured or does not quite resemble actual user utterances. Another caveat is that small datasets can produce unreliable measurements if there isn’t enough data to withhold examples for testing while still training each intent with minimally sufficient examples. For these reasons, <em>k</em>-fold testing is not the preferred testing method once your solution is in production. </p> 
  </div> 
  <div class="readable-text" id="p68"> 
   <h4 class=" readable-text-h4">Measuring understanding with AI-generated blind test data</h4> 
  </div> 
  <div class="readable-text" id="p69"> 
   <p>Obtaining test data through a generative process is done through the same means as obtaining generated training data: you prompt a model to generate variations of examples and use them as a “blind” test set. This method is best suited for predeployment but may also be appropriate in the early go-live phase to supplement gaps in your production logs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p70"> 
   <p>Like <em>k</em>-fold testing, the validity of your accuracy measurements is wholly dependent on whether the test data closely mirrors the inputs your model receives at production runtime. This approach can be vulnerable to bias and over-fitting. As such, we advise caution and suggest you validate your generated data against production logs as soon as they are available. </p> 
  </div> 
  <div class="readable-text" id="p71"> 
   <h4 class=" readable-text-h4">Measuring understanding with representative blind test data</h4> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>If your chatbot has already been deployed, the production logs are one of your key tools for assessing your chatbot’s accuracy. These logs contain truly representative data about what your users ask for and how they phrase these requests. By “representative,” we mean both a realistic volume distribution of the intents triggered in your system as well as utterances that capture the user’s goal—in whatever combination of words comes naturally to them. </p> 
  </div> 
  <div class="readable-text intended-text" id="p73"> 
   <p>Using production logs will produce the least biased testing data, but it also requires a degree of upfront, manual effort. That effort does pay off, however, as you will have created a reusable asset for taking measurements of future changes. You’ll need to obtain a sample of these logs and review the customer inputs (utterances) against the intents returned by your system. This data will need to be annotated by a human who can identify the definitive correct (aka “golden”) intent that the utterance belongs to. Your initial annotations will give you a baseline accuracy. This data will then be used to build your <em>representative blind test set</em>, which is essentially a list of test questions and the answer key all in one file. </p> 
  </div> 
  <div class="readable-text" id="p74"> 
   <h4 class=" readable-text-h4">Selecting the best method for your situation</h4> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>The cost and effort tradeoffs for each method are entirely dependent on the size and current phase of your solution (predeployment or post): </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p76"> <em>K</em>-fold cross validation may be seen as “cheap and easy” because it does not require human annotation beyond the task of the initial annotation done for training purposes. However, there may be an API cost to running your experiment <em>k</em> times. This cost is usually negligible for smaller systems but could result in thousands or tens of thousands of API calls per experiment for larger systems. </li> 
   <li class="readable-text" id="p77"> Generated test datasets incur the cost of generating the data in addition to the API cost of running an experiment. </li> 
   <li class="readable-text" id="p78"> A representative blind test set may have a lower API cost for running an experiment (compared to <em>k</em>-fold, assuming your test set is smaller than your training set), but the cost of human annotation can be significant. This also requires that the solution is in production, interacting with real users. The benefit is that the experiment results are going to be more meaningful than <em>k</em>-fold and generated test set results. </li> 
  </ul> 
  <div class="readable-text" id="p79"> 
   <p>In summary, there are three primary methods to measure your classifier’s ability to understand users. The method you choose should align with your current stage of development or deployment, as outlined in figure 4.5.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p80">  
   <img alt="figure" src="../Images/CH04_F05_Freed2.png" width="781" height="218"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.5</span> <em>K</em>-fold cross validation and generated test data are suitable for situations where representative data is not available. Once a solution is deployed to production, representative blind test data will produce the most reliable measurement of your classifier’s ability to understand.</h5>
  </div> 
  <div class="readable-text" id="p81"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.2.2</span> Measuring understanding for generative AI</h3> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>Measuring whether a generated answer has demonstrated “good understanding” is an onerous task, and automated test approaches are still emerging. Our challenge is the nature of generative AI: every generated response is possible or likely to be unique to each user input. </p> 
  </div> 
  <div class="readable-text intended-text" id="p83"> 
   <p>Before you deploy a solution with generative AI, you should define what it looks like for your bot to demonstrate good understanding. For generative conversational AI, we suggest you define “good understanding” by the following dimensions:</p> 
  </div> 
  <ul> 
   <li class="readable-text buletless-item" id="p84"> The generated answer matches any specified output format or style, including 
    <ul> 
     <li> Positioning of the bot (the purpose and viewpoint of the bot’s persona) </li> 
     <li> The designated tone and personality of the bot’s persona </li> 
    </ul></li> 
   <li class="readable-text" id="p85"> The generated answer is appropriate for the user’s input in terms of content length and structure (for example, does the nature of the user input require a response that is a short answer, step-by-step instructions, or a detailed essay?). </li> 
   <li class="readable-text" id="p86"> The generated answer is free from false information (hallucinations). </li> 
   <li class="readable-text" id="p87"> The generated answer is free from hate, abuse, profanity, bias, and discrimination. </li> 
   <li class="readable-text" id="p88"> The generated answer is free from damaging information—even if true—such that a company would be legally liable or incur damage to their reputation (for example, negative commentary about a competitor or leaking sensitive data). </li> 
   <li class="readable-text" id="p89"> The generated answer is resilient to prompt-injection attempts. </li> 
   <li class="readable-text" id="p90"> The generated answer is correct and complete and either successfully terminates a flow or progresses the flow to a next step or the next best action. </li> 
  </ul> 
  <div class="readable-text" id="p91"> 
   <p>If your solution has already been deployed, obtain a representative sample of your logs. Perform a manual review to assess your bot’s level of understanding. Each generated answer will be judged as correct, sufficient, or appropriate against the dimensions you have defined for the solution.</p> 
  </div> 
  <div class="readable-text intended-text" id="p92"> 
   <p>This is, of course, time-consuming, but the effort will pay off. Your annotated set can be used as a golden test set for future improvements. This test set will give you a baseline for tracking the effect of changes to your model parameters (such as temperature, top <em>p</em>, top <em>k</em>) and other LLM configuration settings. These samples can also inform any few-shot examples (sample inputs paired with desired outputs) you include in your prompt engineering or fine-tuning.</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.2.3</span> Measuring understanding with direct user feedback</h3> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>One way to measure good understanding at scale is to incorporate an answer feedback mechanism directly in the user experience, such as a thumbs up/down reply option. This method can be used for both traditional and generative solutions. </p> 
  </div> 
  <div class="readable-text intended-text" id="p95"> 
   <p>Be mindful of how often you solicit feedback, and know what purpose your feedback serves. Which aspect of the experience is the rating meant to reflect: satisfaction or dissatisfaction with a particular answer (for a question/answer use case), the self-serve process and its outcome (for a process-oriented bot), or the conversational experience as a whole?</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p96"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercises</h5> 
   </div> 
   <ol> 
    <li class="readable-text buletless-item" id="p97"> Explore and document your solution (or review and update it as needed), with emphasis on the components most responsible for demonstrating understanding: 
     <ul> 
      <li> For classifiers, this means auditing the training data. </li> 
      <li> For solutions that include search and retrieval, audit the source documents or URLs, any supplemental document enrichments, and the ingestion schedule to ensure that your knowledge base contains the most relevant and up-to-date information. </li> 
      <li> For generative AI solutions, audit the dialogue flows that invoke generated responses, and map the prompts, parameters, and LLM settings to their intended outcomes. </li> 
     </ul></li> 
    <li class="readable-text" id="p98"> Reflect on your current test methodologies, if any. Do you have any historical test metrics that can be correlated to current symptoms of weak understanding? </li> 
    <li class="readable-text" id="p99"> Think about the test methodologies presented in this section. Which approach is optimal for the current phase of your solution lifecycle? </li> 
   </ol> 
  </div> 
  <div class="readable-text" id="p100"> 
   <h2 class=" readable-text-h2"><span class="num-string">4.3</span> Assessing where you are today</h2> 
  </div> 
  <div class="readable-text" id="p101"> 
   <p>Before you start making plans for improvements, you will want to perform an assessment of where the solution stands in terms of its ability to accurately identify the users’ goals and needs. The nature of your assessment will depend on which technology your solution uses. Classification and generative models perform very different functions and therefore have different aspects to be assessed. </p> 
  </div> 
  <div class="readable-text" id="p102"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.3.1</span> Assessing your traditional (classification-based) AI solution</h3> 
  </div> 
  <div class="readable-text" id="p103"> 
   <p>For traditional AI, start by reviewing the training set to orient yourself to the domain and current scope:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p104"> How many classifiers are used in your solution? </li> 
   <li class="readable-text" id="p105"> How many different intents does the system (or each classifier) handle? </li> 
   <li class="readable-text" id="p106"> How unique is each intent? </li> 
   <li class="readable-text" id="p107"> Do the training examples in any intent seem to overlap with other intents? </li> 
   <li class="readable-text" id="p108"> Does the range of topics (intents) align with your impression of the chatbot’s purpose? </li> 
   <li class="readable-text" id="p109"> How does the solution handle input it does not understand? </li> 
   <li class="readable-text" id="p110"> What is the complexity of the dialogue? Are there complex flows, backend integrations, or search integrations? </li> 
  </ul> 
  <div class="readable-text" id="p111"> 
   <p>It can be helpful to visualize your classifier training data volume in chart form. Figure 4.6 shows an example training set. There isn’t a lot of information to be gleaned just yet, but this will give us a basis for comparison once we assemble our test data.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p112">  
   <img alt="figure" src="../Images/CH04_F06_Freed2.png" width="1016" height="378"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.6</span> This classifier has 13 intents. The training example counts range from 7 to 30.<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p113"> 
   <p>In general, we expect our intents with higher counts of training examples to be more popular. We want our most popular topics to be understood a majority of the time. Higher-volume intents may also represent topics that handle a greater variety of phrases. For the most part, we don’t like to see a huge disparity in volumes across the training set. For instance, a training set that has some intents trained with hundreds of examples while others have just a handful might exhibit performance problems such as over-selection (frequently selecting a wrong intent due to the bias of training volume). </p> 
  </div> 
  <div class="readable-text" id="p114"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.3.2</span> Assessing your generative AI solution</h3> 
  </div> 
  <div class="readable-text" id="p115"> 
   <p>With generative AI, as in traditional AI, you need to understand the domain and scope your bot operates within. However, instead of concerning yourself with classifications of input, you need to appraise the data sources that your model will draw its answers from when it produces an output. Is generative AI used to produce answers or responses in your solution? If so, familiarize yourself with the circumstances: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p116"> Are answers generated for every user input? </li> 
   <li class="readable-text" id="p117"> Do you call for generated answers as a fallback option for your classifier? </li> 
   <li class="readable-text" id="p118"> Do you call for generated text to supplement a classification-based “canned” answer in the dialogue? </li> 
   <li class="readable-text" id="p119"> Does your solution make use of more than one LLM, such as different models for different types of responses, multiple language support, etc.? </li> 
   <li class="readable-text" id="p120"> Does your solution make use of prompt engineering, prompt tuning, fine tuning, or other customized settings? Is this documented anywhere, along with the outcome goals for which each setting was originally implemented? </li> 
   <li class="readable-text" id="p121"> Does your solution make use of RAG? If so, what is that data source? How often is it updated? Does it contain additional data enrichments? </li> 
  </ul> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p122"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercises</h5> 
   </div> 
   <ol> 
    <li class="readable-text" id="p123"> Assess your solution using the criteria we described in this section (according to the type of AI you use). </li> 
    <li class="readable-text" id="p124"> Once you have performed your initial solution assessment, be sure to document its current state—this will be your baseline system configuration. </li> 
    <li class="readable-text" id="p125"> As you follow along with the improvement recommendations and examples given throughout this book, be prepared to record your changes in a way that will help you correlate any updates you make to the subsequent performance measurements. </li> 
   </ol> 
  </div> 
  <div class="readable-text" id="p126"> 
   <h2 class=" readable-text-h2"><span class="num-string">4.4</span> Obtaining and preparing test data from logs</h2> 
  </div> 
  <div class="readable-text" id="p127"> 
   <p>For the rest of this chapter, we’ll assume that you do have a production system and access to the logs. We will show you how to obtain and prepare that data to create an asset you can use to measure the current state (and to validate future changes). </p> 
  </div> 
  <div class="readable-text intended-text" id="p128"> 
   <p>There’s a bit of initial work involved to build a test set from production logs. Figure 4.7 shows the major tasks involved in preparing data for testing (or training).<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p129">  
   <img alt="figure" src="../Images/CH04_F07_Freed2.png" width="928" height="82"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.7</span> Once you obtain some data, each utterance should first be sorted into buckets to identify potential candidates; this will separate the good, usable test data from the bad or irrelevant user inputs. The data may also need to be scrubbed to fix problems like personal identifiable information (PII). After that, the data will need to be annotated (for classifiers, it will need to be labeled with the correct intent; for generative AI, it will need to be associated with an ideal output response). Finally, the data will need to be converted into one or more sets that can be consumed by your testing tool.</h5>
  </div> 
  <div class="readable-text" id="p130"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.4.1</span> Obtaining production logs</h3> 
  </div> 
  <div class="readable-text" id="p131"> 
   <p>Ideally, you will have access to production logs that span a full year or more. This will help ensure that your test set will have a true representative sample of the range of topics your bot encounters for the various seasons and events that influence your industry. Collect log samples from various weeks or months throughout the year. If your solution is newer, expect to refresh your test sets more frequently during your solution’s first 12 to 18 months.</p> 
  </div> 
  <div class="readable-text intended-text" id="p132"> 
   <p>Once you have obtained some production logs, you may find it easiest to convert this data into a CSV or Excel file (if it hasn’t already been updated). We find it most useful to transform the data into one row per conversational exchange (a user input and a bot output), grouped by conversation ID. Depending on the timeframe you select, the volume of users, and the complexity and purpose of your solution, your file may have just a few hundred rows of data, or it could have 100,000 rows or more of conversational exchanges. </p> 
  </div> 
  <div class="readable-text intended-text" id="p133"> 
   <p>One simple shortcut for reducing the volume to a manageable set is to select the first user utterance in each conversation. This may not work in all cases, but figure 4.8 shows that it is often a reliable way to harvest useful data from your logs. In a natural language-driven exchange, users tend to express their most important need in the initial input. If your average conversation lasts ten turns, a conversation log with 100,000 rows of raw data could be reduced to about 10,000 rows of data to review. Deduplication can often further reduce this by a few thousand. This is a very workable volume and will usually contain rich and diverse examples that you can use for testing your solution. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p134">  
   <img alt="figure" src="../Images/CH04_F08_Freed2.png" width="1014" height="394"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.8</span> Raw chat logs show that a user’s primary goal is often captured in the first turn of a conversation, but sometimes it occurs as an additional request later in the conversation or after exchanging a pleasantry. It might even follow an opt-out request. Selecting the first row will usually yield enough usable data while reducing the amount of time your annotators spend sorting through the utterances that aren’t useful to the classifier, such as button clicks, common responses, and PII or other user-specific information. (The structure of your logs may vary by tool.)</h5>
  </div> 
  <div class="readable-text" id="p135"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.4.2</span> Guidelines for identifying candidate test utterances</h3> 
  </div> 
  <div class="readable-text" id="p136"> 
   <p>Whatever you do to obtain and preprocess your logs, your next task is to identify potential blind test candidates. We treat this as a “first pass” exercise: just determine if an utterance is <em>potentially</em> usable. Additionally, we counsel the reviewers not to over-analyze what they see; if you cannot make a determination about any given utterance within a minute or so, discard the utterance and move on. (If you’re feeling really conflicted or sense a pattern, mark it for later review and move on.) We use the following criteria to identify potential test candidates from production logs, along with any special handling instructions:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p137"> Is the utterance unintelligible? </li> 
   <li class="readable-text" id="p138"> Is the utterance completely unrelated to the domain? </li> 
   <li class="readable-text" id="p139"> Is the utterance ambiguous? </li> 
   <li class="readable-text" id="p140"> Does the utterance contain multiple intents? </li> 
   <li class="readable-text" id="p141"> Is the utterance related to the domain but out of scope? </li> 
   <li class="readable-text" id="p142"> Does the utterance express a goal that is in domain and in scope? </li> 
  </ul> 
  <div class="readable-text" id="p143"> 
   <p>Let’s look at each of these in turn.</p> 
  </div> 
  <div class="readable-text" id="p144"> 
   <h4 class=" readable-text-h4">Is the utterance unintelligible? </h4> 
  </div> 
  <div class="readable-text" id="p145"> 
   <p>Maybe a cat walked across the keyboard, or the user just mashed the keys in a fit of frustration. Perhaps the speech-to-text technology mistranscribed the caller’s question into an unintelligible mess. Speech solutions can also pick up background noise and conversations, especially if they are not properly tuned for the environment. Your file may contain a number of user inputs that just don’t make any sense. </p> 
  </div> 
  <div class="readable-text intended-text" id="p146"> 
   <p>These are examples of unintelligible or unrelated utterances:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p147"> “does it school” (Incoherent—if this came from a voice solution, it was potentially a speech mistranscription.) </li> 
   <li class="readable-text" id="p148"> “she didn’t she said there are four and only gave us one yes you can do that I’m about to catch my flight and I’ll check on it when I get to the office” (Potentially a speech transcription of a background conversation on the caller side.) </li> 
   <li class="readable-text" id="p149"> “klewtkhaccalifornia liense” (Likely typos, severe enough to render the utterance unintelligible.) </li> 
  </ul> 
  <div class="readable-text" id="p150"> 
   <p>These lines can be excluded from your blind test set. Any recognizable patterns, such as possible speech transcription problems, should be set aside for further evaluation or forwarded to the appropriate team.</p> 
  </div> 
  <div class="readable-text" id="p151"> 
   <h4 class=" readable-text-h4">Is the utterance completely unrelated to the domain? </h4> 
  </div> 
  <div class="readable-text" id="p152"> 
   <p>You may occasionally come across questions that are intelligible but entirely off-topic for the domain or the bot’s intended purpose. For example, if your solution is designed to help electric utility customers manage their account and services, you can exclude questions about pop culture trivia if they happen to appear in the logs. </p> 
  </div> 
  <div class="readable-text intended-text" id="p153"> 
   <p>Though you could configure a solution to send unrecognized topics to an LLM, these utterances do not belong in your classifier test set because a golden intent cannot be assigned. Such utterances could be used in negative testing, which will help you understand if your solution is appropriately identifying when it should <em>not</em> attempt to answer. </p> 
  </div> 
  <div class="readable-text" id="p154"> 
   <h4 class=" readable-text-h4">Is the utterance ambiguous? </h4> 
  </div> 
  <div class="readable-text" id="p155"> 
   <p>Perhaps you’ll find a single word or a short phrase that is related to the domain but doesn’t express a clear goal. For example, if a user of a banking chatbot simply says, “account,” what do they want? Do they want to open an account? Close an account? Check an account balance? Who knows? </p> 
  </div> 
  <div class="readable-text intended-text" id="p156"> 
   <p>A subset of ambiguous utterances may include responses generated by a button click or as part of an information-gathering flow. (If you selected the first natural language utterance of every conversation, you might not see these.) These are generally not useful for the classifier’s performance testing unless they align with an intent that is used within a flow. Include such utterances only when appropriate.</p> 
  </div> 
  <div class="readable-text intended-text" id="p157"> 
   <p>These are examples of ambiguous utterances:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p158"> “driver license” (Perhaps relevant to the domain, but no clear goal is expressed.) </li> 
   <li class="readable-text" id="p159"> “that one” (An anaphor referring to contextual information that appears to have been provided in an earlier statement but may have lost its meaning as an individual utterance.) </li> 
   <li class="readable-text" id="p160"> “2” (Could refer to a button choice or phone channel selection, or to an amount or quantity provided as a response to the previous question.) </li> 
  </ul> 
  <div class="readable-text" id="p161"> 
   <p>In most cases, these utterances should not be included in your classifier accuracy test because they likely will not align with any single intent, but rather multiple intents. They are not meaningless, however. Set these aside to understand how often your users communicate in this way. Determine whether your other chatbot features, such as disambiguation or clarifying questions, are handling them appropriately.</p> 
  </div> 
  <div class="readable-text" id="p162"> 
   <h4 class=" readable-text-h4">Does the utterance contain multiple intents? </h4> 
  </div> 
  <div class="readable-text" id="p163"> 
   <p>Most classifier-based chatbots perform best when they are given one goal at a time. Utterances that express multiple valid, distinct goals should be excluded from your classifier accuracy test set because you cannot definitively assign a “correct” intent. </p> 
  </div> 
  <div class="readable-text intended-text" id="p164"> 
   <p>The exception to this rule would be if your solution has a disambiguation mechanism. Disambiguation is a way to clarify the user’s primary goal by presenting the top <em>n </em>intents identified by a classifier. For these solutions, you may want to run your multi-intent utterances against your classifier to verify that all intents listed would be presented with the appropriate disambiguation choices. </p> 
  </div> 
  <div class="readable-text intended-text" id="p165"> 
   <p>These are some examples of utterances with multiple intents:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p166"> “Do you have the COVID booster? How can I make an appointment?” (Two goals expressed: 1) availability of vaccine booster, 2) make an appointment.) </li> 
   <li class="readable-text" id="p167"> “I want to update the address on my driver’s license and find out what is required to get a commercial driver’s license.” (Two goals expressed: 1) update address, 2) get information for obtaining a CDL.) </li> 
   <li class="readable-text" id="p168"> “I currently have 95,000 loyalty points. Do they expire? How many more points do I need to reach Platinum status? Can I purchase points for this?” (Three goals expressed: 1) find out if reward points expire, 2) find out the delta between current point balance and next level reward status, 3) get information about purchasing points to reach a higher status.) </li> 
   <li class="readable-text buletless-item" id="p169"> “I want to talk to an agent about reporting a stolen vehicle.” This is very common. A user will often pair a request for an agent along with their true goal. If both intents exist in your classifier training set, you can handle such utterances in one of two ways: 
    <ul> 
     <li> Exclude these as candidates if it is impossible to label a single “correct” intent. </li> 
     <li> Include these candidates, but label them according to the “preferred” intent. (A preferred intent might be the self-service option if containment is a priority and the competing intent would escalate.) </li> 
    </ul></li> 
  </ul> 
  <div class="readable-text" id="p170"> 
   <p>As with ambiguous utterances, these should be set aside and evaluated separately to better understand your users. You may want to devise additional strategies to handle these situations if they are occurring often. If users tend to ask related questions, or they pair common requests in a single utterance, your output responses in these intents could be updated to anticipate or meet all of the needs. For the first example we gave—“Do you have the COVID booster? How can I make an appointment?”—your answer regarding booster availability may include a link to make an appointment. </p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p171"> 
    <h5 class=" callout-container-h5 readable-text-h5">A word about handling multiple intents with classification models</h5> 
   </div> 
   <div class="readable-text" id="p172"> 
    <p>We have seen extensive and heroic attempts to handle multiple intents programmatically in conversational AI solutions. This usually involves logic to collect the top <em>n</em> intents and store them in context, and then more logic to present the additional topics after the first one is answered. In most cases, the result is an over-engineered solution that is brittle, difficult to scale, or simply wasted effort. This approach also has a major flaw: such logic cannot reliably distinguish between an utterance that truly contains multiple goals and an utterance that contains a single goal that may have triggered multiple intents. </p> 
   </div> 
   <div class="readable-text" id="p173"> 
    <p>Many modern chatbot frameworks provide automated topic disambiguation (for example, “Did you mean: [Intent 1] [Intent 2] [Intent 3]”). Our general recommendation is to allow the disambiguation feature to do its job. Sometimes, this means that the user must ask their questions or state their goals one at a time. The frequency and importance of such scenarios is usually not worth the effort required to build and maintain custom logic for handling multiple intents in a classification-based chatbot.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p174"> 
   <p>Generative AI is typically much better at handling multiple intents than classification-based solutions, so you can include these utterances as candidates in a test set if your solution has this capability.</p> 
  </div> 
  <div class="readable-text" id="p175"> 
   <h4 class=" readable-text-h4">Is the utterance related to the domain but out of scope? </h4> 
  </div> 
  <div class="readable-text" id="p176"> 
   <p>You are likely to come across utterances that express a single, clear goal that is relevant to the domain, but the current solution is not equipped to handle them. For example, a banking chatbot may allow users to check an account balance but may not be trained to recognize requests about interest rates. An airline chatbot may be versed on airline policy but not be grounded in facts about airport security. </p> 
  </div> 
  <div class="readable-text intended-text" id="p177"> 
   <p>Such questions may be very reasonable from the user’s perspective, and gaps in topic coverage often lead to frustration for your users. This is especially true if you don’t have a generative AI or search fallback. If your bot responds, “I’m sorry, I don’t understand. Please rephrase your question,” no amount of rephrasing will get the user to a satisfactory answer. How should these be handled?</p> 
  </div> 
  <div class="readable-text intended-text" id="p178"> 
   <p>If your classifier does not have any trained intents to handle such requests, these should set aside. On further review, they may be grouped into topics or categories, but they will be excluded from your test set for now because a golden intent cannot be assigned. Monitor these topics for volume and add them to your improvement backlog as appropriate.</p> 
  </div> 
  <div class="readable-text intended-text" id="p179"> 
   <p>Similarly, if your generative solution is not prepared to answer such questions (for example, the document repository in a RAG solution does not have content to address the topic), set these aside for the time being, but monitor the volume. </p> 
  </div> 
  <div class="readable-text" id="p180"> 
   <h4 class=" readable-text-h4">Does the utterance express a goal that is in domain and in scope? </h4> 
  </div> 
  <div class="readable-text" id="p181"> 
   <p>Score! Questions or requests that are in scope for your solution and domain belong in your golden test set. </p> 
  </div> 
  <div class="readable-text" id="p182"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.4.3</span> Preparing and scrubbing data for use in iterative improvements</h3> 
  </div> 
  <div class="readable-text" id="p183"> 
   <p>If you’ve never seen production logs for a chatbot, you will be surprised at how messy they are. You are going to see a lot of bad or informal grammar, misspelled words or typos (on a text-based channel), speech mistranscriptions (on a voice channel), and potentially various forms of personal identifiable information (PII). Here’s how we recommend handling these.</p> 
  </div> 
  <div class="readable-text" id="p184"> 
   <h4 class=" readable-text-h4">Bad or informal grammar</h4> 
  </div> 
  <div class="readable-text" id="p185"> 
   <p>For the most part, leave it be! There is a lot of diversity in how humans express themselves. The user may not know exactly how to communicate what they need—especially to a machine. If a goal can be identified, it is a representative example and should be generally preserved as is. </p> 
  </div> 
  <div class="readable-text" id="p186"> 
   <h4 class=" readable-text-h4">Typos and misspelled words</h4> 
  </div> 
  <div class="readable-text" id="p187"> 
   <p>Unless a typo or misspelled word significantly changes the meaning of the overall phrase, leave it as is. Commonly misspelled words are representative of how your users communicate. Your classifier should be able to give a good answer whether the user asks, “What’s the difference between loan balance and principal?” or “whats teh diffrence between loan balance and principle?” </p> 
  </div> 
  <div class="readable-text intended-text" id="p188"> 
   <p>Proper case and punctuation are generally ignored by a classifier, but you may need to verify this with your technology platform.</p> 
  </div> 
  <div class="readable-text" id="p189"> 
   <h4 class=" readable-text-h4">Speech mistranscriptions</h4> 
  </div> 
  <div class="readable-text" id="p190"> 
   <p>If your solution uses speech-to-text (aka automated speech recognition), you won’t encounter typos, but you probably will see unexpected words that are most likely the result of a speech mistranscription. The first line of attack is to train your speech models, if possible. The underlying technology of a chatbot classifier is text-driven, so it is best to have the most faithful representation of the user’s utterance before it hits the text classifier. </p> 
  </div> 
  <div class="readable-text intended-text" id="p191"> 
   <p>If you find that the speech models are still consistently mistranscribing words that are significant within your domain, include these in your test set (and ultimately, you will probably end up supplementing your training data). For example, for an electric utility company, we consistently saw an important domain term, “residential,” mistranscribed as “presidential.” As speech model updates can take longer to implement, and this was causing loss of call containment, an immediate fix was to add “presidential” as a synonym to our chat solution. Another example was the mistranscription of “VIN” as “BIN” for a use case that needed to understand “vehicle identification number.” For this, we made sure that the training data contained both variations. We also preserved the mistranscriptions for our testing purposes.</p> 
  </div> 
  <div class="readable-text" id="p192"> 
   <h4 class=" readable-text-h4">Personal identifiable information</h4> 
  </div> 
  <div class="readable-text" id="p193"> 
   <p>You may also find various forms of PII, such as names, phone numbers, physical or email addresses, social security numbers, account numbers, etc. These do not belong in your training or test data. Ideally, this information would be masked in your logs, but even this technology is not perfect. If your solution has a PII masking function, you should replace any real data with the same type of masking characters (e.g., ###-###-#### for a ten-digit phone number). If not, either remove the PII entirely, or replace it with an obviously fictionalized representation, such as “username@email.com.”</p> 
  </div> 
  <div class="readable-text" id="p194"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.4.4</span> The annotation process</h3> 
  </div> 
  <div class="readable-text" id="p195"> 
   <p>After you have narrowed down your data to utterances that express a clear goal that belongs in your domain (and have scrubbed them where appropriate), they need to be properly annotated for the task at hand. </p> 
  </div> 
  <div class="readable-text" id="p196"> 
   <h4 class=" readable-text-h4">Annotating a golden test set for traditional (classifier-based) AI</h4> 
  </div> 
  <div class="readable-text" id="p197"> 
   <p>Annotating a test set for a classifier involves labeling each utterance with the appropriate intent. This task is a little easier said than done, and it’s where you will spend the most time building your test set. </p> 
  </div> 
  <div class="readable-text intended-text" id="p198"> 
   <p>It’s fairly easy to identify and discard an unintelligible or ambiguous user input. However, once you know an utterance belongs in your domain, it takes a bit more time to label it with the correct intent. The person or team tasked with annotating (labeling each utterance with the correct intent) will need to be familiar with the current training data. This process will definitely expose problems with overlap in your intents, as your human annotators will be stuck with the question of how to label an utterance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p199"> 
   <p>A team might take several approaches to complete the work of labeling data for testing or training. Sometimes a single person is tasked with the job. Sometimes a whole team will try to take this on. When that happens, they often think that a “divide and conquer” approach is most efficient. In our experience, this can lead to problems that take longer to resolve. </p> 
  </div> 
  <div class="readable-text intended-text" id="p200"> 
   <p>In an ideal world, everyone would sit in the same room and judge each utterance together. This approach facilitates discussion regarding the purpose of each intent. All annotators need to understand the criteria used to differentiate intents that share a lot of the same key words but have different goals. Another equally valid approach is to have multiple annotators judge the same data separately (or at least a percentage of overlapping data) and compare any differences to reach a resolution. </p> 
  </div> 
  <div class="readable-text intended-text" id="p201"> 
   <p>There is one shortcut we wouldn’t hesitate to take if your logs include the intent that was predicted at runtime for each utterance: make a first pass and judge whether the predicted intent was correct. Then you need only judge and label the remaining incorrect utterances with the correct intent. </p> 
  </div> 
  <div class="readable-text intended-text" id="p202"> 
   <p>This exercise may take anywhere from a few hours to several days, and it can be taxing on both your vision and your cognitive load. As a first run, instruct your annotators to make their best judgment and move on. If it takes more than sixty seconds to judge an utterance, skip it and come back later. It is also important to take breaks every hour or two. It helps to walk away and come back after a period of refresh. </p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p203"> 
    <h5 class=" callout-container-h5 readable-text-h5">Could I just use an LLM to do all that work?</h5> 
   </div> 
   <div class="readable-text" id="p204"> 
    <p>If you are building your first classifier, you could certainly run utterances against an LLM as a first pass at labeling or classifying your data. However, if you already have production logs, there will be no added value to running the utterances against a separate classification LLM because you still need a human judge to review the classifications produced by this exercise. </p> 
   </div> 
  </div> 
  <div class="readable-text" id="p205"> 
   <p>Once you have annotated the test set, you will have a golden set of human-judged, labeled data. Depending on your use case, this could include a few hundred to a few thousand utterances. This asset will give you some immediate information about your classifier’s current accuracy. It will also be used to help tune your system.</p> 
  </div> 
  <div class="readable-text intended-text" id="p206"> 
   <p>The last thing you need to do is convert your data into a file that can be consumed by your testing tool. This will produce an asset that can be used to measure the effect of future updates. The format may vary by tool, but it will typically be a text or CSV file that contains a row for each test utterance in one column and the golden intent in the other column. Table 4.2 shows how a test set might look.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p207"> 
   <h5 class=" browsable-container-h5"><span class="num-string">Table 4.2</span> Sample test set with one utterance/intent pair per row</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Utterance 
       </div></th> 
      <th> 
       <div>
         Golden intent 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  I want to speak with a real person <br/></td> 
      <td>  <code>Request_Agent</code> <br/></td> 
     </tr> 
     <tr> 
      <td>  Can I talk to a manager <br/></td> 
      <td>  <code>Request_Agent</code> <br/></td> 
     </tr> 
     <tr> 
      <td>  Get me customer service <br/></td> 
      <td>  <code>Request_Agent</code> <br/></td> 
     </tr> 
     <tr> 
      <td>  Are you open on Sundays <br/></td> 
      <td>  <code>Office_Hours</code> <br/></td> 
     </tr> 
     <tr> 
      <td>  What time do you open <br/></td> 
      <td>  <code>Office_Hours</code> <br/></td> 
     </tr> 
     <tr> 
      <td>  When does your office close <br/></td> 
      <td>  <code>Office_Hours</code> <br/></td> 
     </tr> 
     <tr> 
      <td>  What are your weekend hours <br/></td> 
      <td>  <code>Office_Hours</code> <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p208"> 
   <h4 class=" readable-text-h4">Annotating a golden test set for generative AI</h4> 
  </div> 
  <div class="readable-text" id="p209"> 
   <p>Creating a test set to measure generative AI involves judging the quality of the answer produced by your solution (if you are working with production logs) and updating or replacing it with the ideal answer, according to the dimensions you previously defined for your solution. Subject matter experts will need to review each example output to ensure that it is factual and complete, represents the brand, and reflects the purpose and positioning of the virtual agent persona.</p> 
  </div> 
  <div class="readable-text intended-text" id="p210"> 
   <p>Once you have reviewed the output, you will have a set of utterances paired with a golden answer or response. This asset will give you some immediate information about the quality of your generated responses. It will also be used to tune your prompts and LLM configurations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p211"> 
   <p>The last thing you need to do is convert your data into a file that can be consumed by your testing tool. The format may vary by tool, but it will typically be a text or CSV file that contains a row for each test utterance in one column and the golden response in the other column. Table 4.3 shows a sample test set.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p212"> 
   <h5 class=" browsable-container-h5"><span class="num-string">Table 4.3</span> Sample test set with one utterance/answer pair per row</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Utterance 
       </div></th> 
      <th> 
       <div>
         Golden response 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Can I bring a snowboard on my flight as checked baggage? <br/></td> 
      <td>  You can bring one set of snowboard equipment as a checked bag. The set must be in one bag and can include up to two snowboards and one snow boot bag. If the set weighs more than 50 pounds (23 kg), you’ll have to pay overweight bag fees. <br/></td> 
     </tr> 
     <tr> 
      <td>  How long do I have to wait to get my refund? <br/></td> 
      <td>  Credit card refunds will be processed within five business days of the request. All other refunds will be processed within 20 business days of the request. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p213"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercises</h5> 
   </div> 
   <ol> 
    <li class="readable-text" id="p214"> Obtain data from your own logs, and identify candidate test utterances. </li> 
    <li class="readable-text" id="p215"> Scrub the data as needed to remove PII. </li> 
    <li class="readable-text" id="p216"> Assess the classification predictions or generated answer content. Record these outcomes as baseline performance measurements. </li> 
    <li class="readable-text" id="p217"> Assign a golden intent or ideal response. </li> 
    <li class="readable-text" id="p218"> Save the file in a format that can be consumed by your testing tool. </li> 
   </ol> 
  </div> 
  <div class="readable-text" id="p219"> 
   <h2 class=" readable-text-h2"><span class="num-string">4.5</span> What does the data tell us?</h2> 
  </div> 
  <div class="readable-text" id="p220"> 
   <p>If your logs included the original intent prediction or generated answer, you now have what is needed to calculate a baseline measurement of your solution’s current accuracy rate for understanding. (Divide the number of correct predictions or answers by the total candidates judged.) Your annotated utterances will show you the range and frequency of topics your users present to the chatbot. </p> 
  </div> 
  <div class="readable-text" id="p221"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.5.1</span> Interpreting annotated logs for traditional (classification-based) AI</h3> 
  </div> 
  <div class="readable-text" id="p222"> 
   <p>For classifier-based systems, you might be interested in looking at the volume distribution across your intents. How does this compare to your training example volumes for each intent? Figure 4.9 shows an idealized, fairly balanced distribution of training examples compared to occurrences seen in the logs.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p223">  
   <img alt="figure" src="../Images/CH04_F09_Freed2.png" width="1015" height="415"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.9</span> The dark bars represent the number of training examples in a system. The light bars represent the number of annotated utterances for each intent. If your chart follows a similar pattern, your training priorities are probably in good alignment with the demands of your solution. </h5>
  </div> 
  <div class="readable-text" id="p224"> 
   <p>A stark disparity between trained examples and actual occurrences in the logs is not indicative of problems in and of itself, but it can inform your priorities if your accuracy is low. Figure 4.10 shows an example of annotated utterances that are wildly out of alignment with how the system was trained. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p225">  
   <img alt="figure" src="../Images/CH04_F10_Freed2.png" width="1015" height="427"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 4.10</span> The training example counts (dark bars) show a large disparity across many intents, as compared to the annotated log data (light bars). Without accuracy numbers for each intent, we cannot immediately tell if this disparity is having a negative effect. However, we can make some observations, such as 1) the first five intents are not nearly as important to our users as we thought they might be, and 2) the intents with the highest volume in our logs (the light bars for intents 6, 10, 11, and 12) may be a lot more important to our users than we predicted. </h5>
  </div> 
  <div class="readable-text" id="p226"> 
   <p>You should also review the volume of utterances that were judged to be in domain but out of the current scope. (These would have been identified and set aside as part of the preparation tasks described in section 4.1.4.) Does there appear to be a demand for topics that the classifier is not currently trained on? A misalignment between what your users expect to be able to ask and what your classifier is trained to recognize contributes to a perception of weak understanding.</p> 
  </div> 
  <div class="readable-text intended-text" id="p227"> 
   <p>Your overall accuracy provided a big-picture view of the solution’s ability to understand. The next step is to drill down into the specific intents. You might start by looking at the poorest performers that are also high-volume/high-value in your solution. In chapter 5, we will explore in depth the process for improving classifier understanding.</p> 
  </div> 
  <div class="readable-text" id="p228"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.5.2</span> Interpreting annotated logs for generative AI</h3> 
  </div> 
  <div class="readable-text" id="p229"> 
   <p>Your annotated logs for a generative AI solution will give you a picture of the range of questions and requests that users are providing. Throughout the annotation process, you may have discovered gaps in coverage about the domain. You may also have gained a better grasp of how prompt engineering or fine-tuning improvements could make your generated answers better. If your solution employs RAG, you might start correlating the quality of your answers to the documents in your repository. </p> 
  </div> 
  <div class="readable-text intended-text" id="p230"> 
   <p>Your overall accuracy provided a big-picture view of the solution’s ability to understand. In chapter 6, we will explore in depth the process for improving your generative AI so that it conveys good understanding.</p> 
  </div> 
  <div class="readable-text" id="p231"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.5.3</span> The case for iterative improvement</h3> 
  </div> 
  <div class="readable-text" id="p232"> 
   <p>At this point, you should be armed with the data you need to begin planning improvement cycles. Your performance findings will serve as a roadmap for improvements. Keep in mind that this is an iterative process. You will make changes. Then you will take measurements to determine whether your change had a positive, neutral, or negative effect on understanding.</p> 
  </div> 
  <div class="readable-text intended-text" id="p233"> 
   <p>It is also important to note that your blind or golden test set will need to be refreshed throughout the lifecycle of your solution. Recall that one of the reasons a chatbot can become inaccurate is due to new information in the world. These are some examples we have seen:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p234"> The global COVID-19 pandemic, which changed the way nearly everyone worked, navigated public spaces, and supported their families. </li> 
   <li class="readable-text" id="p235"> New legislation passed, resulting in government organizations getting related questions. </li> 
   <li class="readable-text" id="p236"> New products on the market or product recalls. </li> 
   <li class="readable-text" id="p237"> A company experienced a data breach, and once the news broke, the chatbot was bombarded with questions like, “Is my data safe?” and “I want to know more about the hack.” </li> 
  </ul> 
  <div class="readable-text" id="p238"> 
   <p>Plan to review your logs on a regular basis. Depending on the volume of your solution, that might start out daily right after launch, then weekly, monthly, and quarterly. Don’t forget to update your test sets according to the changes you make: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p239"> If new intents are added to your system, new utterances need to be added to your test set. </li> 
   <li class="readable-text" id="p240"> If intents were merged or split as part of your improvement efforts, the affected intents will need to be updated in your test set. </li> 
   <li class="readable-text" id="p241"> If new areas of coverage are added to the knowledge base your generative solution references, your test set should include validations for this. </li> 
   <li class="readable-text" id="p242"> If your solution adds new LLM scenarios or prompt customizations, these should be reflected in the test set. </li> 
  </ul> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p243"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercises</h5> 
   </div> 
   <ol> 
    <li class="readable-text buletless-item" id="p244"> Review your annotated data and reflect on the findings. Are there areas that show poor understanding? 
     <ul> 
      <li> If so, what would you hypothesize is the root cause? </li> 
      <li> Is there more than one root cause? </li> 
     </ul></li> 
    <li class="readable-text" id="p245"> How would you prioritize the improvements needed to achieve better understanding? </li> 
   </ol> 
  </div> 
  <div class="readable-text" id="p246"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p247"> Chatbots demonstrate good understanding when they identify what a user wants and they provide a satisfactory answer or progress the user toward their goal. </li> 
   <li class="readable-text" id="p248"> For traditional AI, understanding relies on at least two mechanisms: correct classification of an intent and an ability to deliver an output based on that classification. (Additional mechanisms, such as entity detection or context, may modify or personalize outputs.) </li> 
   <li class="readable-text" id="p249"> For generative AI, understanding relies on the utterance and any accompanying prompt to create a response meant to address a user’s question or goal. </li> 
   <li class="readable-text" id="p250"> Weak understanding is detrimental to business value and is often exhibited by a chatbot returning wrong answers or no answers at all. </li> 
   <li class="readable-text" id="p251"> You can’t assess the performance of your chatbot without first collecting some data. </li> 
   <li class="readable-text" id="p252"> Chatbot understanding is usually measured in terms of accuracy or the rate at which the solution delivers a correct answer or takes the correct action. </li> 
   <li class="readable-text" id="p253"> There are multiple tools and methods for measuring understanding. Some are dependent on the type of AI and/or the current phase, whether predeployment or post. </li> 
   <li class="readable-text" id="p254"> A representative golden test set, curated from real user utterances (production logs), can be used to measure the bot’s baseline performance and can be converted into a reusable asset to measure the effect of future changes. </li> 
   <li class="readable-text" id="p255"> You should plan to monitor and retrain your solution throughout the life of the bot. </li> 
   <li class="readable-text" id="p256"> Updates to training may require corresponding updates to the blind test set. </li> 
  </ul>
 </div></div></body></html>