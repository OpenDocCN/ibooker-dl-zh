["```py\nwith open(\"files/anna.txt\",\"r\") as f:\n    text=f.read()    \nwords=text.split(\" \")    \nprint(words[:20]) \n```", "```py\n['Chapter', '1\\n\\n\\nHappy', 'families', 'are', 'all', 'alike;', 'every',\n 'unhappy', 'family', 'is', 'unhappy', 'in', 'its',\n'own\\nway.\\n\\nEverything', 'was', 'in', 'confusion', 'in', 'the',\n\"Oblonskys'\"]\n```", "```py\nclean_text=text.lower().replace(\"\\n\", \" \")               ①\nclean_text=clean_text.replace(\"-\", \" \")                  ②\nfor x in \",.:;?!$()/_&%*@'`\":\n    clean_text=clean_text.replace(f\"{x}\", f\" {x} \")\nclean_text=clean_text.replace('\"', ' \" ')                ③\ntext=clean_text.split()\n```", "```py\nfrom collections import Counter   \nword_counts = Counter(text)    \nwords=sorted(word_counts, key=word_counts.get,\n                      reverse=True)\nprint(words[:10])\n```", "```py\n[',', '.', 'the', '\"', 'and', 'to', 'of', 'he', \"'\", 'a']\n```", "```py\ntext_length=len(text)                                      ①\nnum_unique_words=len(words)                                ②\nprint(f\"the text contains {text_length} words\")\nprint(f\"there are {num_unique_words} unique tokens\")\nword_to_int={v:k for k,v in enumerate(words)}              ③\nint_to_word={k:v for k,v in enumerate(words)}              ④\nprint({k:v for k,v in word_to_int.items() if k in words[:10]})\nprint({k:v for k,v in int_to_word.items() if v in words[:10]})\n```", "```py\nthe text contains 437098 words\nthere are 12778 unique tokens\n{',': 0, '.': 1, 'the': 2, '\"': 3, 'and': 4, 'to': 5, 'of': 6, 'he': 7,\n\"'\": 8, 'a': 9}\n{0: ',', 1: '.', 2: 'the', 3: '\"', 4: 'and', 5: 'to', 6: 'of', 7: 'he',\n 8: \"'\", 9: 'a'}\n```", "```py\nprint(text[0:20])\nwordidx=[word_to_int[w] for w in text]  \nprint([word_to_int[w] for w in text[0:20]])  \n```", "```py\n['chapter', '1', 'happy', 'families', 'are', 'all', 'alike', ';', 'every',\n 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '.',\n 'everything', 'was']\n[208, 670, 283, 3024, 82, 31, 2461, 35, 202, 690, 365, 38, 690, 10, 234,\n 147, 166, 1, 149, 12]\n```", "```py\nimport torch\nseq_len=100                                             ①\nxys=[]\nfor n in range(0, len(wordidx)-seq_len-1):              ②\n    x = wordidx[n:n+seq_len]                            ③\n    y = wordidx[n+1:n+seq_len+1]                        ④\n    xys.append((torch.tensor(x),(torch.tensor(y))))\n```", "```py\nfrom torch.utils.data import DataLoader\n\ntorch.manual_seed(42)\nbatch_size=32\nloader = DataLoader(xys, batch_size=batch_size, shuffle=True)\n```", "```py\nfrom torch import nn\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\nclass WordLSTM(nn.Module):\n    def __init__(self, input_size=128, n_embed=128,\n             n_layers=3, drop_prob=0.2):\n        super().__init__()\n        self.input_size=input_size\n        self.drop_prob = drop_prob\n        self.n_layers = n_layers\n        self.n_embed = n_embed\n        vocab_size=len(word_to_int)\n        self.embedding=nn.Embedding(vocab_size,n_embed)    ①\n        self.lstm = nn.LSTM(input_size=self.input_size,\n            hidden_size=self.n_embed,\n            num_layers=self.n_layers,\n            dropout=self.drop_prob,batch_first=True)       ②\n        self.fc = nn.Linear(input_size, vocab_size)    \n\n    def forward(self, x, hc):\n        embed=self.embedding(x)\n        x, hc = self.lstm(embed, hc)                       ③\n        x = self.fc(x)\n        return x, hc      \n\n    def init_hidden(self, n_seqs):                         ④\n        weight = next(self.parameters()).data\n        return (weight.new(self.n_layers,\n                           n_seqs, self.n_embed).zero_(),\n                weight.new(self.n_layers,\n                           n_seqs, self.n_embed).zero_())  \n```", "```py\nmodel=WordLSTM().to(device)\n```", "```py\nlr=0.0001\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nloss_func = nn.CrossEntropyLoss()\n```", "```py\nmodel.train()\n\nfor epoch in range(50):\n    tloss=0\n    sh,sc = model.init_hidden(batch_size)\n    for i, (x,y) in enumerate(loader):                      ①\n        if x.shape[0]==batch_size:\n            inputs, targets = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            output, (sh,sc) = model(inputs, (sh,sc))        ②\n            loss = loss_func(output.transpose(1,2),targets) ③ \n            sh,sc=sh.detach(),sc.detach()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 5)\n            optimizer.step()                                ④\n            tloss+=loss.item()\n        if (i+1)%1000==0:\n            print(f\"at epoch {epoch} iteration {i+1}\\\n            average loss = {tloss/(i+1)}\")\n```", "```py\nimport pickle\n\ntorch.save(model.state_dict(),\"files/wordLSTM.pth\")\nwith open(\"files/word_to_int.p\",\"wb\") as fb:\n    pickle.dump(word_to_int, fb)\n```", "```py\nmodel.load_state_dict(torch.load(\"files/wordLSTM.pth\",\n                                    map_location=device))\nwith open(\"files/word_to_int.p\",\"rb\") as fb:\n    word_to_int = pickle.load(fb)\nint_to_word={v:k for k,v in word_to_int.items()}\n```", "```py\nimport numpy as np\ndef sample(model, prompt, length=200):\n    model.eval()\n    text = prompt.lower().split(' ')\n    hc = model.init_hidden(1)\n    length = length - len(text)                              ①\n    for i in range(0, length):\n        if len(text)<= seq_len:\n            x = torch.tensor([[word_to_int[w] for w in text]])\n        else:\n            x = torch.tensor([[word_to_int[w] for w \\\nin text[-seq_len:]]])                                        ②\n        inputs = x.to(device)\n        output, hc = model(inputs, hc)                       ③\n        logits = output[0][-1]\n        p = nn.functional.softmax(logits, dim=0).detach().cpu().numpy()\n        idx = np.random.choice(len(logits), p=p)             ④\n        text.append(int_to_word[idx])                        ⑤\n    text=\" \".join(text)\n    for m in \",.:;?!$()/_&%*@'`\":\n        text=text.replace(f\" {m}\", f\"{m} \")\n    text=text.replace('\"  ', '\"')\n    text=text.replace(\"'  \", \"'\")\n    text=text.replace('\" ', '\"')\n    text=text.replace(\"' \", \"'\")\n    return text  \n```", "```py\ntorch.manual_seed(42)\nnp.random.seed(42)\nprint(sample(model, prompt='Anna and the prince'))  \n```", "```py\nanna and the prince did not forget what he had not spoken. when the softening barrier was not so long as he had talked to his brother,  all the hopelessness of the impression. \"official tail,  a man who had tried him,  though he had been able to get across his charge and locked close,  and the light round the snow was in the light of the altar villa. the article in law levin was first more precious than it was to him so that if it was most easy as it would be as the same. this was now perfectly interested. when he had got up close out into the sledge,  but it was locked in the light window with their one grass,  and in the band of the leaves of his projects,  and all the same stupid woman,  and really,  and i swung his arms round that thinking of bed. a little box with the two boys were with the point of a gleam of filling the boy,  noiselessly signed the bottom of his mouth,  and answering them took the red\n```", "```py\ndef generate(model, prompt , top_k=None, \n             length=200, temperature=1):\n    model.eval()\n    text = prompt.lower().split(' ')\n    hc = model.init_hidden(1)\n    length = length - len(text)    \n    for i in range(0, length):\n        if len(text)<= seq_len:\n            x = torch.tensor([[word_to_int[w] for w in text]])\n        else:\n            x = torch.tensor([[word_to_int[w] for w in text[-seq_len:]]])\n        inputs = x.to(device)\n        output, hc = model(inputs, hc)\n        logits = output[0][-1]\n        logits = logits/temperature                             ①\n        p = nn.functional.softmax(logits, dim=0).detach().cpu()    \n        if top_k is None:\n            idx = np.random.choice(len(logits), p=p.numpy())\n        else:\n            ps, tops = p.topk(top_k)                            ②\n            ps=ps/ps.sum()\n            idx = np.random.choice(tops, p=ps.numpy())          ③\n        text.append(int_to_word[idx])\n\n    text=\" \".join(text)\n    for m in \",.:;?!$()/_&%*@'`\":\n        text=text.replace(f\" {m}\", f\"{m} \")\n    text=text.replace('\"  ', '\"')   \n    text=text.replace(\"'  \", \"'\")  \n    text=text.replace('\" ', '\"')   \n    text=text.replace(\"' \", \"'\")     \n    return text  \n```", "```py\nprompt=\"I ' m not going to see\"\ntorch.manual_seed(42)\nnp.random.seed(42)\nfor _ in range(10):\n    print(generate(model, prompt, top_k=None, \n         length=len(prompt.split(\" \"))+1, temperature=1))\n```", "```py\ni'm not going to see you\ni'm not going to see those\ni'm not going to see me\ni'm not going to see you\ni'm not going to see her\ni'm not going to see her\ni'm not going to see the\ni'm not going to see my\ni'm not going to see you\ni'm not going to see me\n```", "```py\nprompt=\"I ' m not going to see\"\ntorch.manual_seed(42)\nnp.random.seed(42)\nfor _ in range(10):\n    print(generate(model, prompt, top_k=3, \n         length=len(prompt.split(\" \"))+1, temperature=0.5))\n```", "```py\ni'm not going to see you\ni'm not going to see the\ni'm not going to see her\ni'm not going to see you\ni'm not going to see you\ni'm not going to see you\ni'm not going to see you\ni'm not going to see her\ni'm not going to see you\ni'm not going to see her\n```", "```py\ntorch.manual_seed(42)\nnp.random.seed(42)\nprint(generate(model, prompt='Anna and the prince',\n               top_k=3,\n               temperature=0.5))\n```", "```py\nanna and the prince had no milk. but,  \"answered levin,  and he stopped. \"i've been skating to look at you all the harrows,  and i'm glad. . .  \"\"no,  i'm going to the country. \"\"no,  it's not a nice fellow. \"\"yes,  sir. \"\"well,  what do you think about it? \"\"why,  what's the matter? \"\"yes,  yes,  \"answered levin,  smiling,  and he went into the hall. \"yes,  i'll come for him and go away,  \"he said,  looking at the crumpled front of his shirt. \"i have not come to see him,  \"she said,  and she went out. \"i'm very glad,  \"she said,  with a slight bow to the ambassador's hand. \"i'll go to the door. \"she looked at her watch,  and she did not know what to say \n```", "```py\nprompt=\"I ' m not going to see\"\ntorch.manual_seed(42)\nnp.random.seed(42)\nfor _ in range(10):\n    print(generate(model, prompt, top_k=None, \n         length=len(prompt.split(\" \"))+1, temperature=2))\n```", "```py\ni'm not going to see them\ni'm not going to see scarlatina\ni'm not going to see behind\ni'm not going to see us\ni'm not going to see it\ni'm not going to see it\ni'm not going to see a\ni'm not going to see misery\ni'm not going to see another\ni'm not going to see seryozha\n```", "```py\ntorch.manual_seed(42)\nnp.random.seed(42)\nprint(generate(model, prompt='Anna and the prince',\n               top_k=None,\n               temperature=2))\n```", "```py\nanna and the prince took sheaves covered suddenly people. \"pyotr marya borissovna,  propped mihail though her son will seen how much evening her husband;  if tomorrow she liked great time too. \"adopted heavens details for it women from this terrible,  admitting this touching all everything ill with flirtation shame consolation altogether:  ivan only all the circle with her honorable carriage in its house dress,  beethoven ashamed had the conversations raised mihailov stay of close i taste work? \"on new farming show ivan nothing. hat yesterday if interested understand every hundred of two with six thousand roubles according to women living over a thousand:  snetkov possibly try disagreeable schools with stake old glory mysterious one have people some moral conclusion,  got down and then their wreath. darya alexandrovna thought inwardly peaceful with varenka out of the listen from and understand presented she was impossible anguish. simply satisfied with staying after presence came where he pushed up his hand as marya her pretty hands into their quarters. waltz was about the rider gathered;  sviazhsky further alone have an hand paused riding towards an exquisite\n```"]