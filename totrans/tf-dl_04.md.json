["```py\nimport deepchem as dc\n\n_, (train, valid, test), _ = dc.molnet.load_tox21()\ntrain_X, train_y, train_w = train.X, train.y, train.w\nvalid_X, valid_y, valid_w = valid.X, valid.y, valid.w\ntest_X, test_y, test_w = test.X, test.y, test.w\n```", "```py\n# Remove extra tasks\ntrain_y = train_y[:, 0]\nvalid_y = valid_y[:, 0]\ntest_y = test_y[:, 0]\ntrain_w = train_w[:, 0]\nvalid_w = valid_w[:, 0]\ntest_w = test_w[:, 0]\n```", "```py\nd = 1024\nwith tf.name_scope(\"placeholders\"):\n  x = tf.placeholder(tf.float32, (None, d))\n  y = tf.placeholder(tf.float32, (None,))\n```", "```py\nwith tf.name_scope(\"hidden-layer\"):\n  W = tf.Variable(tf.random_normal((d, n_hidden)))\n  b = tf.Variable(tf.random_normal((n_hidden,)))\n  x_hidden = tf.nn.relu(tf.matmul(x, W) + b)\n```", "```py\nwith tf.name_scope(\"placeholders\"):\n  x = tf.placeholder(tf.float32, (None, d))\n  y = tf.placeholder(tf.float32, (None,))\nwith tf.name_scope(\"hidden-layer\"):\n  W = tf.Variable(tf.random_normal((d, n_hidden)))\n  b = tf.Variable(tf.random_normal((n_hidden,)))\n  x_hidden = tf.nn.relu(tf.matmul(x, W) + b)\nwith tf.name_scope(\"output\"):\n  W = tf.Variable(tf.random_normal((n_hidden, 1)))\n  b = tf.Variable(tf.random_normal((1,)))\n  y_logit = tf.matmul(x_hidden, W) + b\n  # the sigmoid gives the class probability of 1\n  y_one_prob = tf.sigmoid(y_logit)\n  # Rounding P(y=1) will give the correct prediction.\n  y_pred = tf.round(y_one_prob)\nwith tf.name_scope(\"loss\"):\n  # Compute the cross-entropy term for each datapoint\n  y_expand = tf.expand_dims(y, 1)\n  entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n  # Sum all contributions\n  l = tf.reduce_sum(entropy)\n\nwith tf.name_scope(\"optim\"):\n  train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n\nwith tf.name_scope(\"summaries\"):\n  tf.summary.scalar(\"loss\", l)\n  merged = tf.summary.merge_all()\n```", "```py\nkeep_prob = tf.placeholder(tf.float32)\n```", "```py\nwith tf.name_scope(\"hidden-layer\"):\n  W = tf.Variable(tf.random_normal((d, n_hidden)))\n  b = tf.Variable(tf.random_normal((n_hidden,)))\n  x_hidden = tf.nn.relu(tf.matmul(x, W) + b)\n  # Apply dropout\n  x_hidden = tf.nn.dropout(x_hidden, keep_prob)\n```", "```py\nstep = 0\nfor epoch in range(n_epochs):\n  pos = 0\n  while pos < N:\n    batch_X = train_X[pos:pos+batch_size]\n    batch_y = train_y[pos:pos+batch_size]\n    feed_dict = {x: batch_X, y: batch_y, keep_prob: dropout_prob}\n    _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n    print(\"epoch %d, step %d, loss: %f\" % (epoch, step, loss))\n    train_writer.add_summary(summary, step)\n\n    step += 1\n    pos += batch_size\n```", "```py\ntrain_weighted_score = accuracy_score(train_y, train_y_pred, sample_weight=train_w)\nprint(\"Train Weighted Classification Accuracy: %f\" % train_weighted_score)\nvalid_weighted_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\nprint(\"Valid Weighted Classification Accuracy: %f\" % valid_weighted_score)\n```", "```py\nTrain Weighted Classification Accuracy: 0.742045\nValid Weighted Classification Accuracy: 0.648828\n```"]