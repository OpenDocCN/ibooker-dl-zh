- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since their introduction in 2017, transformers have become the de facto standard
    for tackling a wide range of natural language processing (NLP) tasks in both academia
    and industry. Without noticing it, you probably interacted with a transformer
    today: Google now uses BERT to enhance its search engine by better understanding
    users’ search queries. Similarly, the GPT family of models from OpenAI have repeatedly
    made headlines in mainstream media for their ability to generate human-like text
    and images.^([1](preface01.xhtml#idm46238735099696)) These transformers now power
    applications like [GitHub’s Copilot](https://copilot.github.com), which, as shown
    in [Figure P-1](#copilot), can convert a comment into source code that automatically
    creates a neural network for you!'
  prefs: []
  type: TYPE_NORMAL
- en: So what is it about transformers that changed the field almost overnight? Like
    many great scientific breakthroughs, it was the synthesis of several ideas, like
    *attention*, *transfer learning*, and *scaling up neural networks*, that were
    percolating in the research community at the time.
  prefs: []
  type: TYPE_NORMAL
- en: But however useful it is, to gain traction in industry any fancy new method
    needs tools to make it accessible. The ![nlpt_pin01](Images/nlpt_pin01.png) [Transformers
    library](https://oreil.ly/Z79jF) and its surrounding ecosystem answered that call
    by making it easy for practitioners to use, train, and share models. This greatly
    accelerated the adoption of transformers, and the library is now used by over
    five thousand organizations. Throughout this book we’ll guide you on how to train
    and optimize these models for practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![copilot](Images/nlpt_p001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure P-1\. An example from GitHub Copilot where, given a brief description
    of the task, the application provides a suggestion for the entire class (everything
    following `class` is autogenerated)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Who Is This Book For?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is written for data scientists and machine learning engineers who
    may have heard about the recent breakthroughs involving transformers, but are
    lacking an in-depth guide to help them adapt these models to their own use cases.
    The book is not meant to be an introduction to machine learning, and we assume
    you are comfortable programming in Python and has a basic understanding of deep
    learning frameworks like [PyTorch](https://pytorch.org) and [TensorFlow](https://www.tensorflow.org).
    We also assume you have some practical experience with training models on GPUs.
    Although the book focuses on the PyTorch API of ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers, [Chapter 2](ch02.xhtml#chapter_classification) shows you how to
    translate all the examples to TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following resources provide a good foundation for the topics covered in
    this book. We assume your technical knowledge is roughly at their level:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Hands-On Machine Learning with Scikit-Learn and TensorFlow*, by Aurélien Géron
    (O’Reilly)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Learning for Coders with fastai and PyTorch*, by Jeremy Howard and Sylvain
    Gugger (O’Reilly)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Natural Language Processing with PyTorch*, by Delip Rao and Brian McMahan
    (O’Reilly)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*The Hugging Face Course*](https://oreil.ly/n3MaR), by the open source team
    at Hugging Face'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What You Will Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of this book is to enable you to build your own language applications.
    To that end, it focuses on practical use cases, and delves into theory only where
    necessary. The style of the book is hands-on, and we highly recommend you experiment
    by running the code examples yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'The book covers all the major applications of transformers in NLP by having
    each chapter (with a few exceptions) dedicated to one task, combined with a realistic
    use case and dataset. Each chapter also introduces some additional concepts. Here’s
    a high-level overview of the tasks and topics we’ll cover:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 1](ch01.xhtml#chapter_introduction), *Hello Transformers*, introduces
    transformers and puts them into context. It also provides an introduction to the
    Hugging Face ecosystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 2](ch02.xhtml#chapter_classification), *Text Classification*, focuses
    on the task of sentiment analysis (a common text classification problem) and introduces
    the `Trainer` API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 3](ch03.xhtml#chapter_anatomy), *Transformer Anatomy*, dives into
    the Transformer architecture in more depth, to prepare you for the chapters that
    follow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 4](ch04.xhtml#chapter_ner), *Multilingual Named Entity Recognition*,
    focuses on the task of identifying entities in texts in multiple languages (a
    token classification problem).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 5](ch05.xhtml#chapter_generation), *Text Generation*, explores the
    ability of transformer models to generate text, and introduces decoding strategies
    and metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 6](ch06.xhtml#chapter_summarization), *Summarization*, digs into the
    complex sequence-to-sequence task of text summarization and explores the metrics
    used for this task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 7](ch07.xhtml#chapter_qa), *Question Answering*, focuses on building
    a review-based question answering system and introduces retrieval with Haystack.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 8](ch08.xhtml#chapter_compression), *Making Transformers Efficient
    in Production*, focuses on model performance. We’ll look at the task of intent
    detection (a type of sequence classification problem) and explore techniques such
    a knowledge distillation, quantization, and pruning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 9](ch09.xhtml#chapter_fewlabels), *Dealing with Few to No Labels*,
    looks at ways to improve model performance in the absence of large amounts of
    labeled data. We’ll build a GitHub issues tagger and explore techniques such as
    zero-shot classification and data augmentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 10](ch10.xhtml#chapter_fromscratch), *Training Transformers from Scratch*,
    shows you how to build and train a model for autocompleting Python source code
    from scratch. We’ll look at dataset streaming and large-scale training, and build
    our own tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 11](ch11.xhtml#chapter_future), *Future Directions*, explores the
    challenges transformers face and some of the exciting new directions that research
    in this area is going into.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![nlpt_pin01](Images/nlpt_pin01.png) Transformers offers several layers of
    abstraction for using and training transformer models. We’ll start with the easy-to-use
    pipelines that allow us to pass text examples through the models and investigate
    the predictions in just a few lines of code. Then we’ll move on to tokenizers,
    model classes, and the `Trainer` API, which allow us to train models for our own
    use cases. Later, we’ll show you how to replace the `Trainer` with the ![nlpt_pin01](Images/nlpt_pin01.png)
    Accelerate library, which gives us full control over the training loop and allows
    us to train large-scale transformers entirely from scratch! Although each chapter
    is mostly self-contained, the difficulty of the tasks increases in the later chapters.
    For this reason, we recommend starting with Chapters [1](ch01.xhtml#chapter_introduction)
    and [2](ch02.xhtml#chapter_classification), before branching off into the topic
    of most interest.'
  prefs: []
  type: TYPE_NORMAL
- en: Besides ![nlpt_pin01](Images/nlpt_pin01.png) Transformers and ![nlpt_pin01](Images/nlpt_pin01.png)
    Accelerate, we will also make extensive use of ![nlpt_pin01](Images/nlpt_pin01.png)⁠
    ⁠Datasets, which seamlessly integrates with other libraries. ![nlpt_pin01](Images/nlpt_pin01.png)⁠
    ⁠Datasets offers similar functionality for data processing as Pandas but is designed
    from the ground up for tackling large datasets and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: With these tools, you have everything you need to tackle almost any NLP challenge!
  prefs: []
  type: TYPE_NORMAL
- en: Software and Hardware Requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Due to the hands-on approach of this book, we highly recommend that you run
    the code examples while you read each chapter. Since we’re dealing with transformers,
    you’ll need access to a computer with an NVIDIA GPU to train these models. Fortunately,
    there are several free online options that you can use, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Google Colaboratory](https://oreil.ly/jyXgA)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kaggle Notebooks](https://oreil.ly/RnMP3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Paperspace Gradient Notebooks](https://oreil.ly/mZEKy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To run the examples, you’ll need to follow the installation guide that we provide
    in the book’s GitHub repository. You can find this guide and the code examples
    at [*https://github.com/nlp-with-transformers/notebooks*](https://github.com/nlp-with-transformers/notebooks).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We developed most of the chapters using NVIDIA Tesla P100 GPUs, which have 16GB
    of memory. Some of the free platforms provide GPUs with less memory, so you may
    need to reduce the batch size when training the models.
  prefs: []
  type: TYPE_NORMAL
- en: Conventions Used in This Book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following typographical conventions are used in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Italic*'
  prefs: []
  type: TYPE_NORMAL
- en: Indicates new terms, URLs, email addresses, filenames, and file extensions.
  prefs: []
  type: TYPE_NORMAL
- en: '`Constant width`'
  prefs: []
  type: TYPE_NORMAL
- en: Used for program listings, as well as within paragraphs to refer to program
    elements such as variable or function names, databases, data types, environment
    variables, statements, and keywords.
  prefs: []
  type: TYPE_NORMAL
- en: '**`Constant width bold`**'
  prefs: []
  type: TYPE_NORMAL
- en: Shows commands or other text that should be typed literally by the user.
  prefs: []
  type: TYPE_NORMAL
- en: '*`Constant width italic`*'
  prefs: []
  type: TYPE_NORMAL
- en: Shows text that should be replaced with user-supplied values or by values determined
    by context.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This element signifies a tip or suggestion.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This element signifies a general note.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This element indicates a warning or caution.
  prefs: []
  type: TYPE_NORMAL
- en: Using Code Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Supplemental material (code examples, exercises, etc.) is available for download
    at [*https://github.com/nlp-with-transformers/notebooks*](https://github.com/nlp-with-transformers/notebooks).
  prefs: []
  type: TYPE_NORMAL
- en: If you have a technical question or a problem using the code examples, please
    send email to [*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com).
  prefs: []
  type: TYPE_NORMAL
- en: This book is here to help you get your job done. In general, if example code
    is offered with this book, you may use it in your programs and documentation.
    You do not need to contact us for permission unless you’re reproducing a significant
    portion of the code. For example, writing a program that uses several chunks of
    code from this book does not require permission. Selling or distributing examples
    from O’Reilly books does require permission. Answering a question by citing this
    book and quoting example code does not require permission. Incorporating a significant
    amount of example code from this book into your product’s documentation does require
    permission.
  prefs: []
  type: TYPE_NORMAL
- en: 'We appreciate, but generally do not require, attribution. An attribution usually
    includes the title, author, publisher, and ISBN. For example: “*Natural Language
    Processing with Transformers* by Lewis Tunstall, Leandro von Werra, and Thomas
    Wolf (O’Reilly). Copyright 2022 Lewis Tunstall, Leandro von Werra, and Thomas
    Wolf, 978-1-098-10324-8.”'
  prefs: []
  type: TYPE_NORMAL
- en: If you feel your use of code examples falls outside fair use or the permission
    given above, feel free to contact us at [*permissions@oreilly.com*](mailto:permissions@oreilly.com).
  prefs: []
  type: TYPE_NORMAL
- en: O’Reilly Online Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For more than 40 years, [*O’Reilly Media*](http://oreilly.com) has provided
    technology and business training, knowledge, and insight to help companies succeed.
  prefs: []
  type: TYPE_NORMAL
- en: Our unique network of experts and innovators share their knowledge and expertise
    through books, articles, and our online learning platform. O’Reilly’s online learning
    platform gives you on-demand access to live training courses, in-depth learning
    paths, interactive coding environments, and a vast collection of text and video
    from O’Reilly and 200+ other publishers. For more information, visit [*http://oreilly.com*](http://oreilly.com).
  prefs: []
  type: TYPE_NORMAL
- en: How to Contact Us
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please address comments and questions concerning this book to the publisher:'
  prefs: []
  type: TYPE_NORMAL
- en: O’Reilly Media, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1005 Gravenstein Highway North
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sebastopol, CA 95472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 800-998-9938 (in the United States or Canada)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 707-829-0515 (international or local)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 707-829-0104 (fax)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a web page for this book, where we list errata, examples, and any additional
    information. You can access this page at [*https://oreil.ly/nlp-with-transformers*](https://oreil.ly/nlp-with-transformers).
  prefs: []
  type: TYPE_NORMAL
- en: Email [*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com) to comment
    or ask technical questions about this book.
  prefs: []
  type: TYPE_NORMAL
- en: For news and information about our books and courses, visit [*http://oreilly.com*](http://oreilly.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'Find us on Facebook: [*http://facebook.com/oreilly*](http://facebook.com/oreilly)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow us on Twitter: [*http://twitter.com/oreillymedia*](http://twitter.com/oreillymedia)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Watch us on YouTube: [*http://youtube.com/oreillymedia*](http://youtube.com/oreillymedia)'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Writing a book about one of the fastest-moving fields in machine learning would
    not have been possible without the help of many people. We thank the wonderful
    O’Reilly team, and especially Melissa Potter, Rebecca Novack, and Katherine Tozer
    for their support and advice. The book has also benefited from amazing reviewers
    who spent countless hours to provide us with invaluable feedback. We are especially
    grateful to Luca Perozzi, Hamel Husain, Shabie Iqbal, Umberto Lupo, Malte Pietsch,
    Timo Möller, and Aurélien Géron for their detailed reviews. We thank Branden Chan
    at [deepset](https://www.deepset.ai) for his help with extending the Haystack
    library to support the use case in [Chapter 7](ch07.xhtml#chapter_qa). The beautiful
    illustrations in this book are due to the amazing [Christa Lanz](https://christalanz.ch)—thank
    you for making this book extra special. We were also fortunate enough to have
    the support of the whole Hugging Face team. Many thanks to Quentin Lhoest for
    answering countless questions on ![nlpt_pin01](Images/nlpt_pin01.png) Datasets,
    to Lysandre Debut for help on everything related to the Hugging Face Hub, Sylvain
    Gugger for his help with ![nlpt_pin01](Images/nlpt_pin01.png) Accelerate, and
    Joe Davison for his inspiration for [Chapter 9](ch09.xhtml#chapter_fewlabels)
    with regard to zero-shot learning. We also thank Sidd Karamcheti and the whole
    [Mistral team](https://oreil.ly/aOYLt) for adding stability tweaks for GPT-2 to
    make [Chapter 10](ch10.xhtml#chapter_fromscratch) possible. This book was written
    entirely in Jupyter Notebooks, and we thank Jeremy Howard and Sylvain Gugger for
    creating delightful tools like [fastdoc](https://oreil.ly/yVCfT) that made this
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: Lewis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To Sofia, thank you for being a constant source of support and encouragement—without
    both, this book would not exist. After a long stretch of writing, we can finally
    enjoy our weekends again!
  prefs: []
  type: TYPE_NORMAL
- en: Leandro
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thank you Janine, for your patience and encouraging support during this long
    year with many late nights and busy weekends.
  prefs: []
  type: TYPE_NORMAL
- en: Thomas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I would like to thank first and foremost Lewis and Leandro for coming up with
    the idea of this book and pushing strongly to produce it in such a beautiful and
    accessible format. I would also like to thank all the Hugging Face team for believing
    in the mission of AI as a community effort, and the whole NLP/AI community for
    building and using the libraries and research we describe in this book together
    with us.
  prefs: []
  type: TYPE_NORMAL
- en: More than what we build, the journey we take is what really matters, and we
    have the privilege to travel this path with thousands of community members and
    readers like you today. Thank you all from the bottom of our hearts.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](preface01.xhtml#idm46238735099696-marker)) NLP researchers tend to name
    their creations after characters in *Sesame Street*. We’ll explain what all these
    acronyms mean in [Chapter 1](ch01.xhtml#chapter_introduction).
  prefs: []
  type: TYPE_NORMAL
