- en: Capitolo 11\. Testare i servizi di intelligenza artificiale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Questo lavoro è stato tradotto utilizzando l''AI. Siamo lieti di ricevere il
    tuo feedback e i tuoi commenti: [translation-feedback@oreilly.com](mailto:translation-feedback@oreilly.com)'
  prefs: []
  type: TYPE_NORMAL
- en: In questo capitolo scoprirai l'importanza del testing e le sue sfide nella creazione
    di servizi GenAI, oltre a concetti chiave come i piani di test, i modelli di verifica
    e validazione, la piramide del testing e il ruolo dei dati, degli ambienti e dei
    confini del testing.
  prefs: []
  type: TYPE_NORMAL
- en: Per fare pratica con i test, utilizzerai `pytest`, un popolare framework di
    test con funzioni come le fixture di test, gli ambiti, i marcatori e la parametrizzazione
    delle fixture. Imparerai anche a conoscere il plug-in `pytest-mock` per il patching
    delle funzioni e a usare stub, mock e oggetti spia per simulare e controllare
    le dipendenze esterne durante i test.
  prefs: []
  type: TYPE_NORMAL
- en: Poiché il mocking può rendere i test più fragili, esploreremo anche la dependency
    injection, che ti permette di iniettare dipendenze mock o stub direttamente nei
    componenti da testare, evitando modifiche al codice in runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Parleremo del ruolo dell'isolamento e dell'idempotenza nei test, di quando usare
    i mock e di come testare il codice GenAI sia deterministico che probabilistico.
    Alla fine di questo capitolo, sarai sicuro di poter scrivere suite di test complete
    che includono test unitari, di integrazione, end-to-end e comportamentali per
    i tuoi servizi.
  prefs: []
  type: TYPE_NORMAL
- en: Prima di tuffarci nella scrittura dei test, esploriamo i concetti fondamentali
    del testing del software tradizionale e come affrontare il testing dei servizi
    GenAI, che può rivelarsi impegnativo a causa della natura probabilistica dei modelli
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: L'importanza dei test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In teoria, tutti concordano sul fatto che i test sono necessari quando si realizza
    un software. Si scrivono test per avere fiducia nella funzionalità e nelle prestazioni
    dei sistemi, soprattutto quando interagiscono tra loro. Ma realisticamente, i
    progetti possono saltare l'implementazione di test manuali o automatizzati a causa
    di vari vincoli, tra cui il budget, il tempo o i costi di manodopera associati
    alla manutenzione dei test.
  prefs: []
  type: TYPE_NORMAL
- en: I progetti che saltano i test, in parte o del tutto, finiscono per affrontare
    i problemi del software in modo reattivo invece che proattivo. In questo modo
    si accumula un *debito tecnico* che dovrai poi ripagare in costi di manodopera
    e di server, con gli interessi, per saldare.
  prefs: []
  type: TYPE_NORMAL
- en: Il problema di quando effettuare i test è difficile da risolvere. Se stai solo
    sperimentando e mettendo insieme un prototipo in rapide iterazioni, realisticamente
    non dovrai preoccuparti molto dei test. Tuttavia, non appena avrai un prodotto
    minimo vendibile, un sistema che si interfaccia con dati sensibili ed elabora
    i pagamenti degli utenti, dovrai prendere in seria considerazione i piani di test.
  prefs: []
  type: TYPE_NORMAL
- en: All'inizio della mia carriera, stavo costruendo un sistema di gestione dell'apprendimento
    per un cliente. Ho scritto un endpoint webhook per interfacciarmi con i sistemi
    di pagamento di Stripe e con la mia soluzione di autenticazione fatta in casa
    che registrava gli utenti solo al primo pagamento. Il sistema doveva addebitare
    ed elaborare i pagamenti degli abbonamenti sia dei clienti nuovi che di quelli
    già esistenti e inviare email di conferma, tenendo traccia dei record degli utenti,
    degli abbonamenti, dei pagamenti, delle sessioni di checkout e delle fatture.
    La logica di questo webhook è risultata così contorta e complessa che ha portato
    a una mostruosità che è diventata una funzione di 1.000 righe. La funzione controllava
    eventi ricevuti non ordinati di vario tipo, con molteplici viaggi verso il database.
  prefs: []
  type: TYPE_NORMAL
- en: L'intera soluzione ha dovuto essere scartata alla fine, poiché il comportamento
    del webhook era così *incostante*, restituendo risposte non coerenti allo stesso
    insieme di input. Gli utenti non potevano registrarsi nemmeno dopo aver effettuato
    un pagamento con successo. Questa incostanza ha reso insopportabile il debug del
    webhook, costringendomi a riscrivere l'integrazione del sistema di pagamento da
    zero. Se solo avessi rallentato la pianificazione e modularizzato la logica e
    avessi scritto dei test in anticipo, avrei potuto risparmiarmi tanti grattacapi.
  prefs: []
  type: TYPE_NORMAL
- en: Quando rallenti per pianificare e testare i tuoi servizi, stai barattando tempo
    e fatica in cambio di fiducia nel tuo codice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Altre occasioni in cui dovresti prendere in considerazione l''implementazione
    di test sono quando:'
  prefs: []
  type: TYPE_NORMAL
- en: Più collaboratori aggiungono modifiche nel tempo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I manutentori modificano le dipendenze esterne
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aumenta il numero di componenti e di dipendenze nei tuoi servizi
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All'improvviso si nota la comparsa di troppi bug
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'La posta in gioco è troppo alta se le cose vanno male: la mia esperienza è
    rientrata in questa categoria.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ora dovresti capire in che modo i test possono essere utili al tuo progetto.
  prefs: []
  type: TYPE_NORMAL
- en: Test del software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ora che conosci le sfide e i potenziali approcci per testare i servizi GenAI,
    rivediamo i concetti di testing del software per capire la loro rilevanza per
    i casi d'uso GenAI e le insidie comuni da evitare.
  prefs: []
  type: TYPE_NORMAL
- en: Tipi di test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Esistono tre tipi di test comuni nel testing del software che, in ordine crescente
    di dimensione e complessità, sono i seguenti:'
  prefs: []
  type: TYPE_NORMAL
- en: Test unitari
  prefs: []
  type: TYPE_NORMAL
- en: Si concentrano sul test di singoli componenti o funzioni in modo isolato su
    un insieme discreto di input e casi limite per convalidare la funzionalità a livello
    di singolo componente. I test unitari sono atomici e hanno un ambito di applicazione
    minimo e spesso non si basano su sistemi o dipendenze esterne.
  prefs: []
  type: TYPE_NORMAL
- en: Test di integrazione
  prefs: []
  type: TYPE_NORMAL
- en: I test di integrazione spesso rilevano i problemi di comportamento dell'applicazione
    a livello di sottosistema, convalidando i flussi di dati e i contratti di interfaccia
    (cioè le specifiche) tra i vari componenti.
  prefs: []
  type: TYPE_NORMAL
- en: Test end-to-end (E2E)
  prefs: []
  type: TYPE_NORMAL
- en: Verifica le funzionalità dell'applicazione al massimo livello del sistema, dall'inizio
    alla fine, simulando scenari d'uso reali. I test E2E offrono i massimi livelli
    di fiducia nelle funzionalità e nelle prestazioni dell'applicazione, ma sono i
    più impegnativi da progettare, sviluppare e mantenere.
  prefs: []
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I test E2E e i test di integrazione hanno delle somiglianze che li rendono difficili
    da distinguere l'uno dall'altro. Se un test è grande e a volte difettoso, è possibile
    che tu stia lavorando a un test E2E.
  prefs: []
  type: TYPE_NORMAL
- en: I test di integrazione normalmente controllano un sottoinsieme di sistemi e
    interazioni, non l'intero sistema o una lunga catena di sottosistemi.
  prefs: []
  type: TYPE_NORMAL
- en: La[Figura 11-1](#test_types) mostra l'ambito di ciascun tipo di test. I test
    unitari mostrati a sinistra si concentrano su componenti isolati, mentre i test
    di integrazione verificano le interazioni a coppie di più componenti, anche con
    servizi esterni. Infine, i test E2E coprono l'intero percorso dell'utente e il
    flusso di dati all'interno dell'applicazione per confermare la funzionalità prevista.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1101](assets/bgai_1101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 11-1\. Tipi di test nel testing del software
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Prima di implementare i test sopra citati, puoi anche utilizzare *controlli
    statici del codice* con strumenti come `mypy` per individuare gli errori di sintassi
    e di tipo. Durante la scrittura del codice, i controlli statici possono anche
    aiutarti a individuare le violazioni dello stile del codice, l'uso improprio di
    funzioni edipendenze, le vulnerabilità di sicurezza, il codice morto o inutilizzato,
    i problemi di flusso dei dati e i potenziali bug nei componenti del sistema.
  prefs: []
  type: TYPE_NORMAL
- en: Man mano che passi dai controlli statici e dai test unitari all'integrazione
    e poi ai test E2E, i tuoi casi di test diventano più preziosi ma anche più complessi,
    costosi e lenti.Inoltre, dato che i test E2E hanno un campo d'azione più ampio
    con l'interazione di più componenti, diventeranno più fragili e suscettibili di
    fallire, richiedendo aggiornamenti frequenti per rimanere allineati con le modifiche
    del codice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Anche i test E2E sono complessi e poco affidabili/nondeterministici. Secondo
    Martin Fowler,^([1](ch11.html#id1148)) queste sono le ragioni principali del non
    determinismo:'
  prefs: []
  type: TYPE_NORMAL
- en: La*mancanza di isolamento* fa sì che i componenti interferiscano tra loro, portando
    a risultati imprevedibili.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Un*comportamento asincrono* con operazioni che avvengono fuori sequenza o in
    momenti imprevedibili può portare a risultati non deterministici.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*I servizi remoti* possono introdurre una variabilità dovuta alla latenza della
    rete, alla disponibilità del servizio o a risposte diverse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Le*perdite di risorse*, se non gestite correttamente, possono portare a un comportamento
    incoerente del sistema. Le risorse interessate includono la memoria, i file handle
    o le connessioni a database, client, ecc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infine, a causa della fragilità dei test E2E, i refactoring e le modifiche alle
    funzionalità possono farli fallire. Pertanto, esiste un compromesso tra il livello
    di fiducia che ottieni dai test E2E e la flessibilità di apportare modifiche ai
    tuoi sistemi.
  prefs: []
  type: TYPE_NORMAL
- en: La sfida più grande nel testare il software
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: La sfida più grande nel testare i servizi consiste nell'identificare cosa testare
    e con quali dettagli. A tal fine, devi decidere cosa prendere in giro, fingere
    o mantenere reale, oltre a configurare una serie di strumenti e ambienti di test.
  prefs: []
  type: TYPE_NORMAL
- en: Per superare questa sfida, puoi pianificare i test in anticipo, identificando
    i punti di rottura del sistema e restringendo i problemi ai singoli componenti
    e alle interazioni. Poi, immagina chi è l'utente ed elenca i passi che compie
    quando interagisce con i sistemi problematici. Infine, puoi tradurre questi elenchi
    di passi in test individuali e automatizzarli.
  prefs: []
  type: TYPE_NORMAL
- en: Un'altra sfida con i test che causa molta frustrazione è quella di dover riscrivere
    i test ogni volta che viene modificato il codice da testare.
  prefs: []
  type: TYPE_NORMAL
- en: Poiché i refactoring del codice non cambiano il comportamento funzionale ma
    i dettagli dell'implementazione, possono essere un segno che non stai testando
    le cose giuste. Ad esempio, se stai testando la logica interna di elaborazione
    delle stringhe della funzione `count_tokens(text)` piuttosto che solo il suo output
    finale (cioè il conteggio dei token), l'utilizzo di una libreria esterna per sostituire
    la logica di manipolazione delle stringhe può far fallire i tuoi test.
  prefs: []
  type: TYPE_NORMAL
- en: Un segno rivelatore del fatto che potresti testare i dettagli dell'implementazione
    è quando i tuoi test falliscono quando modifichi il codice (ad esempio, falsi
    positivi), oppure passano anche quando introduci modifiche radicali al codice
    (ad esempio, falsi negativi). Puoi usare tecniche come i *test black-box* per
    testare il tuo sistema fornendo degli input e osservando gli output, senza considerare
    i dettagli dell'implementazione.
  prefs: []
  type: TYPE_NORMAL
- en: Se pianifichi i tuoi test in anticipo, puoi evitare queste difficoltà.
  prefs: []
  type: TYPE_NORMAL
- en: Test di pianificazione
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Per identificare i test necessari durante la pianificazione, puoi utilizzare
    il processo di *verifica e validazione* (V&V).
  prefs: []
  type: TYPE_NORMAL
- en: Seguendo questo processo, prima si conferma il possesso dei requisiti giusti
    (validazione) e poi si ricorre a test per verificare che tutti i requisiti siano
    soddisfatti (verifica).
  prefs: []
  type: TYPE_NORMAL
- en: Avvertenze
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Una copertura del codice del 100% con test superati completerà solo il processo
    di verifica, non la validazione. Devi comunque assicurarti che i tuoi servizi
    implementino le funzioni giuste (cioè la validazione).
  prefs: []
  type: TYPE_NORMAL
- en: Il processo di V&V può essere visualizzato come un modello a forma di V come
    nella [Figura 11-2](#vv).
  prefs: []
  type: TYPE_NORMAL
- en: Quando si scende lungo il modello a V, si definiscono i requisiti del software
    e si progetta la soluzione prima di implementarla sotto forma di codice. In seguito,
    si risale la "V" eseguendo test progressivi (unità, integrazione, E2E, ecc.) per
    convalidare che la soluzione soddisfi le esigenze aziendali.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1102](assets/bgai_1102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 11-2\. Modello di verifica e validazione
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Come già detto in precedenza, i test di validazione possono essere difficili
    da identificare. Esattamente quali test progressivi dovresti implementare?
  prefs: []
  type: TYPE_NORMAL
- en: Quando hai difficoltà a individuare cosa testare, puoi scrivere dei test basati
    sui problemi e sui bug che trovi nell'applicazione. Questo approccio è *reattivo*,
    in quanto scrivi i test solo quando si presentano i problemi, il che può aiutarti
    a superare la difficoltà di non sapere cosa testare. Tuttavia, i test per risolvere
    i problemi più avanti nel *ciclo di vita dello sviluppo del software* (SDLC) richiedono
    un impegno significativo, in quanto avrai a che fare con un sistema più complesso
    con molte parti mobili da testare.
  prefs: []
  type: TYPE_NORMAL
- en: Per questo motivo, movimenti come lo *shift-left testing* sostengono l'adozione
    di pratiche di testing *preventive*, pianificate in anticipo e scritte durante
    lo sviluppo, per ridurre l'impegno di testing. La[Figura 11-3](#shift_left) dimostra
    come lo spostamento delle attività di testing all'inizio dello SLDC possa ridurre
    l'onere complessivo.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1103](assets/bgai_1103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 11-3\. Test del software con il cambio a sinistra
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Un approccio comune nei test shift-left è il *test-driven development* (TDD),
    come mostrato nella [Figura 11-4](#tdd).
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1104](assets/bgai_1104.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 11-4\. TDD
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Nell'approccio TDD, scrivi i test prima del codice vero e proprio. Si tratta
    di un processo iterativo in cui i test scritti falliranno all'inizio, ma il tuo
    obiettivo sarà quello di scrivere la quantità minima di codice per far sì che
    i test passino. Una volta passati, rifattorizzerai il codice per ottimizzare il
    sistema, mantenendo i test passati per concludere il processo TDD iterativo.
  prefs: []
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Un ottimo esempio di come le pratiche TDD siano utili per testare i servizi
    GenAI è durante l'*ingegnerizzazione del prompt*. Scrivi prima una serie di test
    e poi continua a iterare sul design del prompt finché tutti i test non passano.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizzando gli stessi casi di test, puoi verificare eventuali segni di regressione
    e se il cambio di modello riduce le prestazioni dei tuoi servizi.
  prefs: []
  type: TYPE_NORMAL
- en: Come puoi vedere, l'obiettivo generale del TDD è quello di ridurre gli sforzi
    di testing migliorando la qualità del codice, la progettazione della soluzione
    e l'individuazione precoce dei problemi.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensioni del test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Inoltre, durante la pianificazione, devi decidere le varie dimensioni dei test,
    tra cui l''*ambito*, la *copertura* e la *completezza*:'
  prefs: []
  type: TYPE_NORMAL
- en: Ambito di applicazione
  prefs: []
  type: TYPE_NORMAL
- en: Definisce quali sono i componenti, i sistemi e gli scenari d'uso da testare.
    Come parte della definizione dell'ambito, dovrai anche tracciare i *confini del
    test* per chiarire cosa sarà testato e cosa no.
  prefs: []
  type: TYPE_NORMAL
- en: Copertura o superficie di prova
  prefs: []
  type: TYPE_NORMAL
- en: Misura la quantità di sistema o di codice da testare.
  prefs: []
  type: TYPE_NORMAL
- en: Completezza
  prefs: []
  type: TYPE_NORMAL
- en: Indica quanto dettagliati, approfonditi e completi saranno i tuoi test all'interno
    dell'ambito e della copertura definiti. Ad esempio, dovrai decidere se testare
    ogni potenziale utilizzo, gli scenari di successo e di fallimento e i casi limite
    per ogni componente, sistema e interazione.
  prefs: []
  type: TYPE_NORMAL
- en: Il *volume/spazio* definisce i confini e le dimensioni di ciò che viene testato;
    la superficie misura la diffusione dei test; la profondità implica il dettaglio,
    la profondità e la completezza dei casi di test.
  prefs: []
  type: TYPE_NORMAL
- en: Dati del test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Per ottenere una maggiore copertura e completezza dei test, puoi sfruttare
    quattro diversi tipi di dati di test:'
  prefs: []
  type: TYPE_NORMAL
- en: Dati validi
  prefs: []
  type: TYPE_NORMAL
- en: Gli ingressi al sistema rientrano nell'intervallo valido e previsto in condizioni
    normali.
  prefs: []
  type: TYPE_NORMAL
- en: Dati non validi
  prefs: []
  type: TYPE_NORMAL
- en: Input inaspettati, sbagliati, NULL o al di fuori dell'intervallo valido. Puoi
    utilizzare i dati negativi per verificare il comportamento del sistema quando
    vengono utilizzati in modo improprio.
  prefs: []
  type: TYPE_NORMAL
- en: Dati di confine
  prefs: []
  type: TYPE_NORMAL
- en: I dati del test si trovano ai limiti degli intervalli di ingresso accettabili,
    sia al limite superiore che a quello inferiore.
  prefs: []
  type: TYPE_NORMAL
- en: Dati enormi
  prefs: []
  type: TYPE_NORMAL
- en: Utilizzato per le prestazioni e per lo stress test del sistema per misurarne
    i limiti.
  prefs: []
  type: TYPE_NORMAL
- en: Fasi del test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Non importa se stai implementando un test unitario, di integrazione o E2E,
    puoi strutturare i test in diverse fasi distinte utilizzando il modello *dato-quando-allora*
    (GWT), come illustrato qui:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Dati (precondizioni)*: Prima di ogni test, puoi impostare le condizioni di
    test e gli stati o i dati predefiniti (ad esempio, le *fixture*).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Quando (fasi del test)*: durante il test, eseguirai una serie di azioni che
    vorrai testare. È qui che passerai le tue fixture di test al *sistema sotto test*
    (SUT) che, a seconda dell''ambito del test, può essere una singola funzione o
    l''interoservizio.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Poi (risultati attesi)*: dopo aver eseguito la SUT con le fixture, in questa
    fase verificherai i risultati rispetto alle tue aspettative utilizzando una serie
    di asserzioni.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Pulizia*: Una volta terminato, puoi ripulire gli artefatti del test in una
    fase opzionale di*pulizia/strappo*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pytest` raccomanda di strutturare i test utilizzando il modello *arrange-act-assert-cleanup*,
    che corrisponde direttamente al modello GWT con una fase di pulizia opzionale.'
  prefs: []
  type: TYPE_NORMAL
- en: Ambienti di prova
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quando pianifichi i test, devi anche considerare diversi *ambienti di test*
    che coprono il tempo di compilazione, il tempo di compilazione e gli ambienti
    di runtime.
  prefs: []
  type: TYPE_NORMAL
- en: In molti linguaggi di programmazione, il *momento della compilazione* è quello
    in cui il codice sorgente viene tradotto in codice eseguibile. Ad esempio, se
    stai scrivendo del codice in C++, il processo di compilazione prevede un controllo
    completo dei tipi di codice. Se vengono riscontrati errori di tipo, il processo
    di compilazione fallisce. Una volta che tutti i controlli sono stati superati,
    il compilatore C++ traduce il codice in un file binario eseguibile.
  prefs: []
  type: TYPE_NORMAL
- en: Nota
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: La tipizzazione forte in C++ è stata progettata per migliorare il rilevamento
    degli errori, la robustezza del codice e il supporto degli strumenti per gli sviluppatori
    in codebase più grandi e complesse che cambiano frequentemente.
  prefs: []
  type: TYPE_NORMAL
- en: Essendo un linguaggio interpretato, Python non ha un tempo di compilazione tradizionale
    come il C++, ma converte il codice in bytecode per l'esecuzione.
  prefs: []
  type: TYPE_NORMAL
- en: Durante le ispezioni, i controllori statici del codice come `mypy` possono identificare
    i problemi di base del tuo codice, fungendo da livello di verifica iniziale per
    i tuoi sforzi di testing.
  prefs: []
  type: TYPE_NORMAL
- en: Avvertenze
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Poiché Python è un linguaggio tipizzato dinamicamente, non applica la tipizzazione
    per impostazione predefinita. Tuttavia, i controllori statici di tipo come `mypy`
    possono fornire un valore significativo se utilizzi i suggerimenti di tipo nel
    tuo codice Python.
  prefs: []
  type: TYPE_NORMAL
- en: Mentre i controlli statici sono ottimi per individuare i problemi di base del
    codice in fase di compilazione, i test unitari, di integrazione e E2E possono
    verificare la funzionalità del sistema in fase *di esecuzione* quando si esegue
    il codice dell'applicazione. In fase di runtime, strumenti come Pydantic possono
    eseguire controlli di convalida dei dati per individuare strutture di dati inaspettate.
  prefs: []
  type: TYPE_NORMAL
- en: I tuoi servizi GenAI possono anche richiedere ulteriori fasi di configurazione
    e compilazione, come il download dei pesi e il precaricamento dei modelli, prima
    di eseguire il codice dell'applicazione. L'ambiente in cui vengono completate
    le fasi di compilazione, la configurazione e l'installazione delle dipendenze
    viene definito *tempo di compilazione*, che puoi anche testare.
  prefs: []
  type: TYPE_NORMAL
- en: Strategie di test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nel panorama del software, diversi esperti hanno sviluppato strategie per bilanciare
    la distribuzione dei test nei progetti, che si basano su anni di esperienza di
    testing del software da parte degli sviluppatori che le hanno rese popolari.
  prefs: []
  type: TYPE_NORMAL
- en: La strategia più adottata è la *piramide dei test*, come mostrato nella[Figura
    11-5](#testing_pyramid), che promuove la scrittura di più test unitari.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1105](assets/bgai_1105.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 11-5\. Piramide dei test
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: La[Tabella 11-1](#testing_pyramid_example) illustra lo scopo di ogni livello
    della piramide di test con un esempio concreto nel contesto di un servizio GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Tabella 11-1\. Test della piramide nel mondo reale
  prefs: []
  type: TYPE_NORMAL
- en: '| Strato | Scopo | Esempio |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Test end-to-end | Convalida l''intero flusso dell''applicazione dall''inizio
    alla fine | Testare il login dell''utente, generare testo in base a un prompt
    e salvare il contenuto generato |'
  prefs: []
  type: TYPE_TB
- en: '| Test di integrazione | Verifica che i vari moduli o servizi funzionino correttamente
    insieme | Testare le interazioni tra l''API di generazione del testo e il database
    che memorizza i prompt dell''utente e i testi generati |'
  prefs: []
  type: TYPE_TB
- en: '| Test unitari | Verifica singoli componenti o funzioni in modo isolato | Testare
    varie funzioni di utilità che elaborano gli input del modello, ad esempio per
    rimuovere i contenuti inappropriati. |'
  prefs: []
  type: TYPE_TB
- en: Il problema del modello piramidale è che mentre i test unitari migliorano la
    copertura del codice, non necessariamente migliorano la "copertura del business",
    poiché i requisiti del progetto e i casi d'uso potrebbero non essere testati a
    fondo. Di conseguenza, affidarsi solo ai test unitari può creare un falso senso
    di sicurezza, trascurando potenzialmente di testare la logica di business essenziale
    e i flussi di lavoro degli utenti. D'altra parte, i test di integrazione ti permettono
    di coprire più terreno e i test orientati al business.
  prefs: []
  type: TYPE_NORMAL
- en: Avvertenze
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Gli esperti di testing del software hanno anche identificato alcune strategie
    come *anti-pattern* che sono controproducenti.
  prefs: []
  type: TYPE_NORMAL
- en: Se li segui, spenderai una quantità eccessiva di tempo per configurare i test,
    implementerai test troppo specifici e strettamente accoppiati e finirai per avere
    test che presentano un comportamento non deterministico a scatti.
  prefs: []
  type: TYPE_NORMAL
- en: La[Tabella 11-2](#testing_antipatterns) e la [Figura 11-6](#testing_antipatterns_viz)
    mostrano un elenco di anti-pattern per il testing del software.
  prefs: []
  type: TYPE_NORMAL
- en: Tabella 11-2\. Antipattern per il testing del software
  prefs: []
  type: TYPE_NORMAL
- en: '| Strategia | Distribuzione del test | Commenti |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Prova del cono gelato | Un piccolo numero di test unitari con un gran numero
    di test di integrazione e E2E, seguiti da test manuali. | Evitare. Considerato
    un anti-pattern a causa dell''inefficienza dell''implementazione dei test manuali
    e degli alti costi di mantenimento dei test di integrazione e E2E. |'
  prefs: []
  type: TYPE_TB
- en: '| Test del cupcake | Simile al cono gelato; ha un piccolo numero di test di
    unità e integrazione automatizzati, un numero moderato di test E2E/GUI automatizzati
    e un gran numero di test manuali.Ogni tipo di test viene eseguito da un team diverso.
    | Evitare. Considerato un anti-pattern perché può portare a cicli di feedback
    lenti, a spese di comunicazione tra i team e a test fragili con alti costi di
    manutenzione. |'
  prefs: []
  type: TYPE_TB
- en: '| Test della clessidra | Un gran numero di test unitari alla base e di test
    E2E in alto, ma un numero significativamente inferiore di test di integrazione
    al centro. | Evita. Considerato un anti-pattern. Non è così grave come il cono
    gelato, ma comporta comunque un numero eccessivo di fallimenti di test che i test
    di media portata avrebbero potuto coprire. |'
  prefs: []
  type: TYPE_TB
- en: '![bgai 1106](assets/bgai_1106.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 11-6\. Visualizzazione degli anti-pattern di testing
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: La[Tabella 11-3](#testing_strategies) e la [Figura 11-7](#testing_strategies_viz)
    mettono a confronto le strategie di testing del software.
  prefs: []
  type: TYPE_NORMAL
- en: Tabella 11-3\. Confronto tra le strategie di test
  prefs: []
  type: TYPE_NORMAL
- en: '| Strategia | Distribuzione del test | Commenti |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Piramide di test**(Mike Cohn) | Un gran numero di test unitari alla base,
    meno test di integrazione al centro e ancora meno test E2E in cima. | Tuttavia,
    la piramide può essere percepita come un concetto fisico per promuovere la costruzione
    del livello inferiore di test unitari, per poi costruire il livello successivo
    e così via, fino a raggiungere la cima. Questo approccio è inefficace se applicato
    ad applicazioni legacy con un''ampia base di codice. |'
  prefs: []
  type: TYPE_TB
- en: '| **Trofeo di prova**(Kent C. Dodds) | Si concentra su una solida base di controlli
    statici, poi test unitari, seguiti da test di integrazione e da un numero minore
    di test E2E in cima. | La motivazione è che i test E2E e di integrazione sono
    i più validi. Tuttavia, i test E2E sono lenti e costosi. I test di integrazione
    rappresentano un equilibrio tra i due mondi. |'
  prefs: []
  type: TYPE_TB
- en: '| **Prova il nido d''ape**(Stephen H. Fishman) | Rappresenta un approccio equilibrato
    con uguale enfasi su unità, integrazione, E2E e altri tipi di test (prestazioni,
    sicurezza, ecc.). | Può essere meno efficiente se non viene gestita correttamente
    e può non essere ottimale per ogni progetto. |'
  prefs: []
  type: TYPE_TB
- en: '![bgai 1107](assets/bgai_1107.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 11-7\. Visualizzazione delle strategie di test
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Per i servizi GenAI, che spesso comportano integrazioni complesse e considerazioni
    sulle prestazioni, la strategia di testing trofeo potrebbe essere la più adatta.
    La strategia trofeo consiste in una solida base di controlli statici, alimentati
    da strumenti come `mypy` e Pydantic, affiancati da test di integrazione che raggiungono
    un equilibrio tra valore, fiducia e costi di testing.
  prefs: []
  type: TYPE_NORMAL
- en: Se i tuoi servizi GenAI devono essere testati in modo completo con diversi tipi
    di test, compresi quelli di performance e quelli esplorativi, allora il modello
    a nido d'ape potrebbe essere più adatto al tuo progetto, in quanto può uniformare
    gli sforzi di test.
  prefs: []
  type: TYPE_NORMAL
- en: Ora dovresti sentirti più a tuo agio nell'identificare gli esami di cui hai
    bisogno e nel pianificare i tuoi test.
  prefs: []
  type: TYPE_NORMAL
- en: Ora che conosci i concetti di testing del software, esaminiamo le sfide e i
    potenziali approcci al testing dei servizi GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Le sfide del test dei servizi GenAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Se hai deciso di testare i tuoi servizi GenAI, dovrai affrontare diverse sfide.
    Il test dei servizi che sfruttano modelli GenAI probabilistici richiede un approccio
    più completo rispetto al software tradizionale.
  prefs: []
  type: TYPE_NORMAL
- en: Vediamo alcuni motivi per cui testare i servizi GenAI sarà una sfida.
  prefs: []
  type: TYPE_NORMAL
- en: Variabilità dei risultati (flaccidezza)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A parità di input e di codice di implementazione, i servizi GenAI spesso producono
    output diversi. Gli output variano perché questi modelli utilizzano tecniche probabilistiche
    come il campionamento da una distribuzione piuttosto che affidarsi a funzioni
    deterministiche. Naturalmente, la variabilità che si riscontra negli output può
    dipendere dal modello e la regolazione delle configurazioni, come i valori di
    temperatura, può ridurre questavarianza.
  prefs: []
  type: TYPE_NORMAL
- en: 'La variabilità degli output dei modelli GenAI può anche far esplodere il numero
    di potenziali casi di test che puoi scrivere (cioè l''*area/ambito di applicazione
    dei test*) per coprire tutte le possibilità. Per questo motivo, non puoi affidarti
    completamente a test deterministici: i tuoi test funzioneranno in modo incoerente
    e saranno troppo *deboli* per essere eseguiti in modo affidabile in una pipeline
    CI/CD.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dovresti invece affrontare il problema dei test GenAI da una prospettiva statistica
    e probabilistica: prendi diversi campioni basati su ipotesi valide da una *distribuzione
    legittima di input* per verificare la qualità dei prodotti in uscita dal tuo modello.'
  prefs: []
  type: TYPE_NORMAL
- en: Nota
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Per *distribuzione legittima degli input*, intendo la selezione di input che
    siano in linea con lo scopo del modello, rappresentativi di scenari reali e rilevanti
    per il problema che stai cercando di risolvere con il modello.
  prefs: []
  type: TYPE_NORMAL
- en: Un approccio più complesso consiste nell'utilizzare modelli di discriminazione
    per attribuire un punteggio ai risultati variabili del tuo servizio, a patto che
    tu stabilisca delle aspettative di una certa tolleranza o soglia.
  prefs: []
  type: TYPE_NORMAL
- en: Vedrai degli esempi di come farlo più avanti nel capitolo.
  prefs: []
  type: TYPE_NORMAL
- en: Vincoli di prestazioni e risorse (lento e costoso)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Poiché il test dei servizi GenAI richiede un approccio più statistico e/o multimodello,
    dovrai affrontare anche problemi di latenza, utilizzo e hosting.
  prefs: []
  type: TYPE_NORMAL
- en: I tuoi test non possono essere eseguiti in modo sufficientemente veloce e affidabile
    per essere eseguiti continuamente all'interno di una pipeline CI/CD tradizionale.
    Finirai per avere costi eccessivi per l'utilizzo dei token con chiamate multiple
    alle API dei modelli e test multimodello complessi e lenti. Queste sfide rimangono
    a meno che tu non faccia diverse ipotesi per semplificare l'ambito dei test, ridurre
    la frequenza dei test dei modelli e utilizzare tecniche di test efficienti come
    il mocking e il patching, l'iniezione di dipendenza e i test statistici di ipotesi.
    Puoi anche studiare l'uso di piccoli modelli discriminatori sintonizzati per ridurre
    la latenza e migliorare le prestazioni.
  prefs: []
  type: TYPE_NORMAL
- en: Regressione
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*I test di regressione* sono un altro tipo di test da pianificare quando si
    lavora con imodelli GenAI.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Una [ricerca pubblicata nel 2023](https://oreil.ly/1oLQG) che ha confrontato
    il comportamento di ChatGPT nel tempo ha rilevato che:'
  prefs: []
  type: TYPE_NORMAL
- en: Le prestazioni e il comportamento di GPT-3.5 e GPT-4 possono variare notevolmente
    nel tempo. Ad esempio, GPT-4 (marzo 2023) è stato ragionevole nell'identificare
    i numeri primi rispetto ai numeri composti (84% di accuratezza), ma GPT-4 (giugno
    2023) è stato scarso in queste stesse domande (51% di accuratezza). GPT-4 è diventato
    meno disposto a rispondere alle domande sensibili e alle domande del sondaggio
    d'opinione a giugno rispetto a marzo. Inoltre, sia GPT-4 che GPT-3.5 hanno avuto
    più errori di formattazione nella generazione del codice a giugno rispetto a marzo.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Secondo questo studio, il comportamento dello "stesso" servizio LLM può cambiare
    in modo sostanziale in un lasso di tempo relativamente breve, evidenziando la
    necessità di un monitoraggio continuo di LLMs (e di qualsiasi servizio GenAI).
    Sulla base di questa scoperta, puoi supporre che le prestazioni dei tuoi servizi
    GenAI possano peggiorare nel tempo a causa della messa a punto o della riqualificazione
    del modello, dei cambiamenti nei modelli di interazione dell'utente e dei cambiamenti
    nei dati di formazione o nell'ambiente operativo.
  prefs: []
  type: TYPE_NORMAL
- en: Per approfondire, i modelli di intelligenza artificiale probabilistici possono
    subire una *deriva del modello*, ovvero una riduzione delle prestazioni nel tempo
    attribuibile ai dati di addestramento sottostanti. La messa a punto su dati aggiuntivi
    può avere effetti collaterali inaspettati sul comportamento del modello in altri
    compiti.
  prefs: []
  type: TYPE_NORMAL
- en: 'Con il passare del tempo, i dati di addestramento originali possono allontanarsi
    dalla realtà: le tendenze cambiano, si verificano nuovi eventi storici, le lingue
    si evolvono e la conoscenza umana si espande o muta assumendo nuove forme che
    non possono essere colte se i dati di addestramento non vengono continuamente
    aggiornati.Questo fenomeno che causa la deriva del modello viene definito *deriva
    concettuale*, che si verifica quando le proprietà statistiche della variabile
    target che il modello sta cercando di prevedere cambiano nel tempo. La deriva
    concettuale può portare a un peggioramento delle prestazioni del modello perché
    le relazioni e i modelli appresi durante l''addestramento non sono più validi.'
  prefs: []
  type: TYPE_NORMAL
- en: Inoltre, anche la *deriva dei dati*, che comporta cambiamenti nella distribuzione
    delle caratteristiche di input all'interno dei dati di formazione, può portare
    alla deriva del modello.
  prefs: []
  type: TYPE_NORMAL
- en: Questo tipo di deriva è spesso dovuta a cambiamenti nei metodi di campionamento,
    nella distribuzione della popolazione e nella raccolta dei dati, a cambiamenti
    stagionali ed effetti temporali nei dati, a cambiamenti esterni nelle fonti di
    dati o a problemi di qualità nelle pipeline di elaborazione.
  prefs: []
  type: TYPE_NORMAL
- en: I test di regressione e il monitoraggio (in particolare se ti affidi a fornitori
    di modelli esterni come OpenAI) possono aiutarti a individuare i problemi di deriva
    del modello sulle tue attività e sui tuoi casi d'uso specifici. Qualsiasi potenziale
    deriva può essere affrontata a livello di applicazione tramite la convalida dei
    dati o l'utilizzo di tecniche come il RAG o a livello di modello tramite la riqualificazione
    e la messa a punto del modello per ridurre i problemi di regressione.
  prefs: []
  type: TYPE_NORMAL
- en: Pregiudizio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Un'altra grande sfida nel testare i servizi GenAI è quella di rilevare le distorsioni
    dei modelli prima di metterli in produzione. Spesso le distorsioni vengono analizzate
    durante il processo di esplorazione dei dati da parte dei data scientist e degli
    ingegneri ML responsabili della produzione dei modelli. Tuttavia, con i modelli
    GenAI di ampia portata, c'è sempre la possibilità che venga introdotta una qualche
    forma di distorsione a causa di una valutazione errata, dei metodi di campionamento,
    dell'elaborazione dei dati, degli algoritmi di formazione o di distorsioni nascoste
    nei dati stessi.
  prefs: []
  type: TYPE_NORMAL
- en: Ad esempio, se un modello linguistico viene addestrato su un set di dati che
    contiene per lo più testi relativi a un gruppo demografico specifico, potrebbe
    generare risultati orientati verso quel gruppo demografico, escludendo potenzialmente
    altri gruppi. Allo stesso modo, se un modello di riconoscimento delle immagini
    viene addestrato per lo più su immagini di uomini che svolgono professioni come
    medici e ingegneri, potrebbe imparare a generare immagini con un pregiudizio di
    genere.
  prefs: []
  type: TYPE_NORMAL
- en: Il pregiudizio diventa particolarmente grave negli scenari in cui si desidera
    utilizzare gli LLMs come giudici, ad esempio come strumenti di valutazione dell'intelligenza
    artificiale o valutatori di colloqui.
  prefs: []
  type: TYPE_NORMAL
- en: I pregiudizi possono manifestarsi in varie forme, come pregiudizi di genere,
    pregiudizi razziali, pregiudizi di età e così via, e ogni tipo di pregiudizio
    richiede test e metriche specifiche per essere rilevato. Senza sapere per cosa
    fare i test, non puoi verificare con sicurezza se i tuoi servizi GenAI sono al
    100% privi di pregiudizi.
  prefs: []
  type: TYPE_NORMAL
- en: Una possibile soluzione a questo problema è quella di sfruttare gli autocontrolli
    dei modelli e i discriminatori dell'intelligenza artificiale, dove un modello
    secondario identifica o misura la presenza di eventuali pregiudizi. Spesso c'è
    un compromesso tra latenza e quote di utilizzo quando si rilevano i pregiudizi
    durante il runtime del servizio.
  prefs: []
  type: TYPE_NORMAL
- en: Attacchi avversari
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I servizi GenAI rivolti al pubblico possono essere vulnerabili ad attacchi avversari
    come la manipolazione dei token, la gestione insicura dei dati, la richiesta di
    jailbreak o l'iniezione di prompt, la divulgazione di informazioni sensibili,
    l'avvelenamento dei dati, il furto di modelli, il denial of service, l'eccessiva
    agenzia e in generale l'uso improprio e l'abuso. Per questo motivo, qualsiasi
    servizio GenAI esposto a Internet dovrà includere livelli di protezione.
  prefs: []
  type: TYPE_NORMAL
- en: Risorse e liste di controllo come la [top 10 di OWASP per le applicazioni LLM](https://genai.owasp.org)
    forniscono un punto di partenza per aggiungere protezioni ai tuoiservizi GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Tuttavia, la creazione di meccanismi di salvaguardia può essere impegnativa
    perché i metodi attuali, al momento in cui scriviamo, si basano su modelli di
    classificazione e discriminazione per rilevare gli attacchi avversari e i contenuti
    dannosi. Questi modelli di salvaguardia spesso richiedono centinaia di megabyte
    di dipendenze, il che può appesantire l'applicazione, rallentare significativamente
    il throughput del servizio e non riuscire a individuare ogni potenziale scenario
    di attacco.
  prefs: []
  type: TYPE_NORMAL
- en: I test avversari assicurano che le misure di salvaguardia adottate siano sufficienti
    a proteggere i tuoi servizi e la tua reputazione. Nell'ambito dei test avversari,
    dovresti anche verificare le prestazioni delle tue protezioni di autenticazione
    e autorizzazione.
  prefs: []
  type: TYPE_NORMAL
- en: '[Il Capitolo 9](ch09.html#ch09) approfondisce l''implementazione di questi
    livelli di protezione e le tecniche di valutazione per proteggere i tuoi modelli
    da questi attacchi.'
  prefs: []
  type: TYPE_NORMAL
- en: Copertura dei test non vincolati
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lo spazio latente dei modelli GenAI è così vasto che non si può fare affidamento
    sui test unitari per ottenere una copertura del 100% di ogni scenario di utilizzo.
  prefs: []
  type: TYPE_NORMAL
- en: Poiché esiste un numero infinito di input e risposte, per quanto tu possa testare
    i tuoi modelli, ci saranno casi limite nascosti che sfuggiranno ai tuoi test.
    Per questo motivo, invece di affidarti alla predefinizione di ogni scenario, puoi
    implementare i *test comportamentali*, che si concentrano sulle proprietà delle
    risposte invece che sugli output esatti.
  prefs: []
  type: TYPE_NORMAL
- en: Esempi di proprietà comportamentali che puoi misurare sono la *coerenza* delle
    strutture di dati generate, la *rilevanza* degli output rispetto agli input, la
    *tossicità*, la *correttezza* e la *fedeltà* (cioè la fedele aderenza alle tue
    politiche e linee guida etiche). Puoi anche aggiungere un umano nel ciclo come
    ulteriore livello di test per individuare le risposte inaspettate.
  prefs: []
  type: TYPE_NORMAL
- en: Avvertenze
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Se stai realizzando un'applicazione RAG o agenziale con più modelli e dipendenze
    esterne, i test comportamentali diventano ancora più pratici. Testare un insieme
    fisso di esempi può far perdere casi limite, interazioni inaspettate tra i componenti
    e variabilità delle risposte dovute a fattori esterni.
  prefs: []
  type: TYPE_NORMAL
- en: Nella prossima sezione imparerai a implementare i tuoi test unitari, di integrazione,
    E2E e comportamentali seguendo un progetto pratico.
  prefs: []
  type: TYPE_NORMAL
- en: 'Progetto: Implementazione di test per un sistema RAG'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nel progetto pratico, scriverai una suite di test per il modulo RAG che hai
    implementato nel [Capitolo 5](ch05.html#ch05). Il sistema RAG che dovrai testare
    si interfaccia con un LLM, un database vettoriale e il filesystem del server tramite
    metodi asincroni, quindi rappresenta un'opportunità perfetta per comprendere i
    principi di test discussi finora.
  prefs: []
  type: TYPE_NORMAL
- en: Seguendo gli esempi di codice, imparerai le migliori pratiche per implementare
    i test di unità, integrazione e E2E per i tuoi servizi GenAI, nonché le lorodifferenze.
  prefs: []
  type: TYPE_NORMAL
- en: Test unitari
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Puoi iniziare a testare i tuoi servizi GenAI con i test unitari. Lo scopo di
    un test unitario è quello di verificare che una parte isolata del tuo codice,
    di solito una singola funzione o metodo, funzioni come previsto.
  prefs: []
  type: TYPE_NORMAL
- en: Prima di scrivere i test, è importante pianificare i casi di test da implementare.
    Per un tipico sistema RAG, puoi scrivere test unitari sulle pipeline di caricamento,
    trasformazione, recupero e generazione dei dati.
  prefs: []
  type: TYPE_NORMAL
- en: La[Figura 11-8](#unit_boundaries) visualizza i confini di questi potenziali
    test unitari su un diagramma di pipeline di dati.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1108](assets/bgai_1108.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 11-8\. Confini dei test unitari visualizzati sul diagramma della pipeline
    di dati RAG
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Si noti che i confini del test terminano all'inizio e alla fine di ogni funzione
    della pipeline di elaborazione dati, perché lo scopo di questi test unitari è
    quello di testare solo il codice della pipeline di elaborazione dati e non il
    database, il filesystem, il modello LLM o le interfacce associate.
  prefs: []
  type: TYPE_NORMAL
- en: In questi test unitari, supporrai che questi sistemi esterni restituiscano ciò
    che ti aspetti e concentrerai i tuoi test unitari solo su ciò che farà il tuo
    codice di elaborazione dei dati.
  prefs: []
  type: TYPE_NORMAL
- en: Per brevità, non testeremo tutti i componenti del sistema, ma seguendo una manciata
    di esempi, dovresti sentirti a tuo agio nell'implementare test successivi per
    ottenere una copertura completa.
  prefs: []
  type: TYPE_NORMAL
- en: Installare e configurare pytest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Per questo progetto utilizzerai il pacchetto `pytest`, che ha componenti integrati
    per la gestione di test fixture, parametri, codice asincrono, collezioni di test
    e un ricco ecosistema di plug-in.`pytest` è più potente, flessibile ed estensibile
    rispetto a`unittest`, il pacchetto di test integrato di Python spesso utilizzato
    per semplici scenari di test.
  prefs: []
  type: TYPE_NORMAL
- en: 'Puoi installare `pytest` utilizzando i seguenti metodi:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Successivamente, crea una cartella `tests` nella radice del tuo progetto dove
    potrai creare moduli Python seguendo lo schema *test_xxx.py*.
  prefs: []
  type: TYPE_NORMAL
- en: '`pytest`Il raccoglitore di test può quindi attraversare la cartella `tests`
    e trovare tutti i moduli, le classi e le funzioni di test in essa contenuti:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'All''interno di ogni file di test, puoi aggiungere le tue funzioni di test
    che contengono sempre almeno un''istruzione `assert`. Se non viene sollevata alcuna
    eccezione in queste istruzioni `assert`, i tuoi test otterranno `PASSED`. Altrimenti,
    `pytest` li contrassegnerà come `FAILED` insieme a un motivo/traccia del perché
    le istruzioni `assert` sono fallite:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Supponiamo che tu abbia scritto due funzioni di test in ogni modulo di test:'
  prefs: []
  type: TYPE_NORMAL
- en: Puoi quindi eseguire i tuoi test tramite il comando `pytest <test_dirt_path>`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Avvertenze
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Evita di scrivere test di grandi dimensioni, perché diventano sempre più difficili
    da capire e da implementare correttamente.
  prefs: []
  type: TYPE_NORMAL
- en: Quando si scrivono i test unitari, ci si preoccupa solo di un componente isolato
    del sistema, come ad esempio una singola funzione del codice, mentre gli altri
    componenti non rientrano nel campo di applicazione dei test unitari. Pertanto,
    si verificherà solo se un singolo componente si comporta come ci si aspetta, dato
    un insieme di dati di test come input.
  prefs: []
  type: TYPE_NORMAL
- en: Ad esempio, puoi verificare se la tua funzione di chunking sta dividendo un
    documento in pezzi come ti aspetteresti. Forse vuoi sperimentare strategie di
    chunking diverse o complesse per la tua pipeline RAG e vuoi assicurarti che il
    testo in ingresso sia suddiviso correttamente. I test unitari con dati di test
    predefiniti o *fixture* possono darti fiducia nella tua funzione di chunking.
  prefs: []
  type: TYPE_NORMAL
- en: L['esempio 11-1](#unit_test) mostra un esempio di test unitario per la tua funzione
    di chunking.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 11-1\. Esempio di test unitario per una funzione di token chunking
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_testing_ai_services_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Suddivide un elenco di token interi in elenchi più piccoli di un determinato
    `chunk_size`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_testing_ai_services_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Specifica i dati del test nella parte *GIVEN (precondizioni)* del test.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_testing_ai_services_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Esegui le fasi di test nella parte *WHEN* del test che includono il passaggio
    dei dati di test al sistema in esame.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_testing_ai_services_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Controlla i risultati rispetto alle uscite previste nella parte *THEN* del test.
  prefs: []
  type: TYPE_NORMAL
- en: Attrezzature e ambito di applicazione
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'I dati di input di che hai definito nell''[Esempio 11-1](#unit_test) per i
    test sono chiamati anche *fixture*, poiché il loro valore rimane fisso per ogni
    esecuzione del test. Esistono due tipi di fixture:'
  prefs: []
  type: TYPE_NORMAL
- en: Apparecchio fresco
  prefs: []
  type: TYPE_NORMAL
- en: La definisci all'interno di ogni test, che poi Python raccoglie (cioè scarta)
    dopo il test. L'[esempio 11-1](#unit_test) utilizza una fixture fresca.
  prefs: []
  type: TYPE_NORMAL
- en: Apparecchio condiviso
  prefs: []
  type: TYPE_NORMAL
- en: Puoi riutilizzarlo in più test per evitare di ripetere sempre la stessa fixture
    per ogni nuovo test.
  prefs: []
  type: TYPE_NORMAL
- en: Puoi dichiarare una fixture condivisa al di fuori delle funzioni di test come
    variabile globale del modulo di test, ma è considerato un anti-pattern, in quanto
    potresti modificarla inavvertitamente.
  prefs: []
  type: TYPE_NORMAL
- en: Avvertenze
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Le fixture condivise devono essere *immutabili*, altrimenti un test può cambiare
    la fixture, creando un effetto collaterale che si ripercuote sugli altri test.
    Una delle principali cause di test difettosi sono le fixture mutabili.
  prefs: []
  type: TYPE_NORMAL
- en: Invece di essere responsabile della gestione dello stato delle fixture condivise,
    puoi affidarti al sistema di iniezione delle dipendenze di `pytest`attraverso
    l'uso di *funzioni di fixture*, come mostrato nell'[Esempio 11-2](#fixture_function).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 11-2\. `pytest` funzione di fissaggio
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_testing_ai_services_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Dichiara la funzione `input_text` come una fixture `pytest` che può essere condivisa
    nel modulo come specificato da `scope="module"`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_testing_ai_services_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Usa la dependency injection di `pytest` per iniettare una fixture condivisa
    in diversi test.
  prefs: []
  type: TYPE_NORMAL
- en: Puoi dichiarare una funzione come una fixture `pytest` utilizzando il decoratore
    `@pytest.fixture(scope)`. Il parametro `scope` specifica la durata della fixture
    condivisa all'interno di una sessione di test.
  prefs: []
  type: TYPE_NORMAL
- en: In base al valore di `scope`, `pytest` crea e distrugge le fixture una volta
    per ogni funzione di test, `class`, `module`, `package`, o per l'intero test `session`.
  prefs: []
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Uno scenario in cui potresti aver bisogno di una fixture condivisa per persistere
    tra i moduli o per l'intera sessione di test è quando recuperi la fixture da un'API
    esterna e vuoi evitare di fare richiesteripetute.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizzando le fixture, puoi implementare diversi test con vari input che coprono
    valori validi, non validi e limite per verificare la robustezza di ogni componente.
    Tuttavia, dovrai separare le funzioni di test per ogni serie di input e output
    previsti. Per evitare di riscrivere lo stesso test, `pytest` ha una funzione di
    *parametrizzazione* che puoisfruttare.
  prefs: []
  type: TYPE_NORMAL
- en: Parametrizzazione
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Con la parametrizzazione di `pytest`, puoi iterare su diversi dati di test e
    output attesi per evitare di duplicare i test, come puoi vedere nell'[Esempio
    11-3](#pytest_params).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 11-3\. Parametrizzazione di`pytest`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_testing_ai_services_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizza la funzione decoratrice `@pytest.mark.parametrize` per specificare
    più argomenti di test e gli output attesi. Gli argomenti di test coprono valori
    validi, vuoti, non validi, intervalli di confine e valori grandi per verificare
    la robustezza della funzione di raggruppamento dei token.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_testing_ai_services_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Inietta i parametri di test nella funzione di test e se l'output previsto è
    un `ValueError`, usa `pytest.raises` per verificare che sia stata sollevata un'eccezione
    `ValueError`. Altrimenti, esegui il controllo dell'asserzione.
  prefs: []
  type: TYPE_NORMAL
- en: Puoi anche memorizzare i dati del test all'interno di file JSON e caricarli
    come fixture da iniettare nelle funzioni di test parametrizzate, come mostrato
    nell'[Esempio 11-4](#pytest_params_json).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 11-4\. Fissaggi JSON nei test parametrici di `pytest`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_testing_ai_services_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Il file JSON contiene un elenco di casi di test come dizionari.
  prefs: []
  type: TYPE_NORMAL
- en: Come puoi vedere, le fixture e la tecnica di parametrizzazione sono strumenti
    estremamente potenti per aiutarti a verificare la robustezza di ogni funzione
    del tuo codice.
  prefs: []
  type: TYPE_NORMAL
- en: Quando scrivi dei test, probabilmente vorrai specificare il codice di configurazione,
    le configurazioni e le fixture globali da condividere tra i file di test. Fortunatamente,
    puoi ottenere questo risultato nel filedi configurazione globale di`pytest`chiamato
    *conftest.py*.
  prefs: []
  type: TYPE_NORMAL
- en: Modulo Conftest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Se vuoi che tutti i tuoi moduli di test abbiano accesso alle fixture e alle
    configurazioni globali, puoi aggiungere un modulo *conftest.py* alla tua directory
    `tests`. Tutte le fixture, il codice di setup e le configurazioni definite nel
    modulo conftest saranno condivise con gli altri moduli di test. Vedi l'[Esempio
    11-5](#conftest_fixture).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 11-5\. Aggiungere un dispositivo condiviso per ogni modulo
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_testing_ai_services_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Definisce una fixture condivisa in *conftest.py* da utilizzare per ogni modulo
    di test. Altrimenti, la fixture sarebbe limitata a un singolo modulo.
  prefs: []
  type: TYPE_NORMAL
- en: Ora hai imparato a scrivere dei test di base utilizzando il framework `pytest`
    secondo il modello GWT. Ora vediamo come eseguire le operazioni di configurazione
    e pulizia prima e dopo i test.
  prefs: []
  type: TYPE_NORMAL
- en: Configurazione e smontaggio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Quando si implementano i test, potrebbe essere necessario configurare un ambiente
    di prova prima ed eseguire operazioni di smantellamento o pulizia dopo. Puoi usare
    la parola chiave `yield` nelle fixture condivise per implementare le operazioni
    di configurazione e smantellamento che devono avvenire in modo coerente per ogni
    test.
  prefs: []
  type: TYPE_NORMAL
- en: Ad esempio, potresti aver bisogno di utilizzare questa funzione per impostare
    e ripulire una sessione del database, come dimostrato nell'[Esempio 11-6](#setup_teardown).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 11-6\. Configurare e chiudere una sessione di database in una fixture
    condivisa
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_testing_ai_services_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Crea una fixture globale condivisa in *conftest.py* che viene creata e distrutta
    una volta per ogni funzione di test.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_testing_ai_services_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Instanzia il client del database `qdrant` e poi crea e configura una collezione
    di test con un punto dati inserito come parte della fase di configurazione del
    test.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_testing_ai_services_CO6-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Consegna il client del database alla funzione di test come parte della fase
    di test principale.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_testing_ai_services_CO6-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Al termine di ogni test, chiude la connessione del client al database. Il codice
    di demolizione dopo la parola chiave `yield` viene eseguito una volta completata
    l'operazione di resa.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_testing_ai_services_CO6-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Inietta il client del database preconfigurato in ogni funzione di test dopo
    aver completato l'impostazione del test. Interroga il database per il documento
    inserito e afferma che è stato recuperato un punto di dati. Una volta completata
    la fase di asserzione, esegui il processo di smantellamento come parte della funzione
    fixture `db_client`.
  prefs: []
  type: TYPE_NORMAL
- en: Seguendo l'esempio dell'[Esempio 11-6](#setup_teardown), puoi anche creare delle
    fixture con fasi di setup e teardown per il tuo client di test API o per qualsiasi
    altro servizio esterno.
  prefs: []
  type: TYPE_NORMAL
- en: Inoltre, avrai notato che l'[Esempio 11-6](#setup_teardown) ha utilizzato un
    client sincrono invece di uno asincrono. Questo perché la gestione dei test asincroni
    può essere complicata, problematica e soggetta a errori e perché richiede l'installazione
    di plugin aggiuntivi di `pytest` per la gestione dei loop di eventi dei test.
  prefs: []
  type: TYPE_NORMAL
- en: Per evitare che i test unitari si rivelino inefficaci, dovresti usare i mock
    per isolare i componenti funzionali dai servizi esterni. Questo perché l'ambito
    e i confini di verifica dei test unitari non includono le dipendenze e le interfacce
    esterne, mentre i test delle dipendenze e delle interfacce esterne, come le interazioni
    con il database, rientrano nell'ambito dei test di integrazione.
  prefs: []
  type: TYPE_NORMAL
- en: In seguito imparerai a gestire i test asincroni e le tecniche di mocking/patching.
  prefs: []
  type: TYPE_NORMAL
- en: Gestione dei test asincroni
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Per eseguire test asincroni, puoi utilizzare dei plug-in come `pytest-asyncio`
    che si integrano`pytest` con `asyncio` di Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11] # tests/rag/retrieve.py  @pytest.mark.asyncio ![1](assets/1.png) async
    def test_search_db(async_db_client): ![2](assets/2.png)     result = await async_db_client.search(         collection_name="test",
    query_vector=[0.18, 0.81, 0.75, 0.12], limit=1     )     assert result is not
    None [PRE12]`  [PRE13] class FakeLLMClient: ![1](assets/1.png)     def __init__(self):         self.cache
    = dict()      def invoke(self, query):         if query in self.cache:             return
    self.cache.get(query) ![2](assets/2.png)          response = requests.post("http://localhost:8001",
    json={"query": query})         if response.status_code != 200:             return
    "Error fetching result"          result = response.json().get("response")         self.cache[query]
    = result         return result  def process_query(query, llm_client, token):     response
    = llm_client.invoke(query, token) ![1](assets/1.png)     return response  def
    test_fake_llm_client(query):     llm_client = FakeLLMClient()     query = "some
    query"     response = process_query(query, llm_client, token="fake_token")     assert
    response == "some response" [PRE14] class DummyLLMClient:     def invoke(self,
    query, token): ![1](assets/1.png)         return "some response"  def process_query(query,
    llm_client, token):     response = llm_client.invoke(query, token) ![1](assets/1.png)     return
    response  def test_dummy_llm_client(query):     llm_client = DummyLLMClient()     query
    = "some query"     response = process_query(query, llm_client, token="fake_token")     assert
    response == "some response" [PRE15] class StubLLMClient:     def invoke(self,
    query):         if query == "specific query": ![1](assets/1.png)             return
    "specific response"         return "default response"  def process_query(query,
    llm_client):     response = llm_client.invoke(query)     return response  def
    test_stub_llm_client():     llm_client = StubLLMClient()     query = "specific
    query"     response = process_query(query, llm_client)     assert response ==
    "specific response" [PRE16] class SpyLLMClient:     def __init__(self):         self.call_count
    = 0         self.calls = []      def invoke(self, query):         self.call_count
    += 1 ![1](assets/1.png)         self.calls.append((query))         return "some
    response"  def process_query(query, llm_client):     response = llm_client.invoke(query)     return
    response  def test_process_query_with_spy():     llm_client = SpyLLMClient()     query
    = "some query"      process_query(query, llm_client)      assert llm_client.call_count
    == 1     assert llm_client.calls == [("some query")] [PRE17] $ pip install pytest-mock
    [PRE18] def process_query(query, llm_client):     response = llm_client.invoke(query)     return
    response  def test_process_query_with_mock(mocker):     llm_client = mocker.Mock()
    ![1](assets/1.png)     llm_client.invoke.return_value = "mock response"     query
    = "some query"      process_query(query, llm_client)     process_query(query,
    llm_client)      assert llm_client.invoke.call_count == 2     llm_client.invoke.assert_any_call("some
    query") [PRE19] class LLMClient:     def invoke(self, query):         return openai.ChatCompletion.create(             model="gpt-4o",
    messages=[{"role": "user", "content": query}]         )  @pytest.fixture def llm_client():     return
    LLMClient()  def test_fake(mocker, llm_client):     class FakeOpenAIClient: ![1](assets/1.png)         @staticmethod         def
    invoke(model, query):             return {"choices": [{"message": {"content":
    "fake response"}}]}      mocker.patch(openai.ChatCompletion, new=FakeOpenAIClient)
    ![1](assets/1.png)     result = llm_client.invoke("test query")     assert result
    == {"choices": [{"message": {"content": "fake response"}}]}  def test_stub(mocker,
    llm_client):     stub = mocker.Mock()     stub.process.return_value = "stubbed
    response"     result = llm_client.invoke(stub)     assert result == "stubbed response"  ![2](assets/2.png)  def
    test_spy(mocker, llm_client):     spy = mocker.spy(LLMClient, ''send_request'')     spy.return_value
    = "some_value"     llm_client.invoke("some query")     spy.call_count == 1  ![3](assets/3.png)  def
    test_mock(mocker, llm_client):     mock = mocker.Mock()     llm_client.invoke(mock)     mock.process.assert_called_once_with("some
    query") ![4](assets/4.png) [PRE20]`  [PRE21] def calculate_recall(expected: list[int],
    retrieved: list[int]) -> int: ![1](assets/1.png)     true_positives = len(set(expected)
    & set(retrieved))     return true_positives / len(expected)   def calculate_precision(expected:
    list[int], retrieved: list[int]) -> int: ![2](assets/2.png)     true_positives
    = len(set(expected) & set(retrieved))     return true_positives / len(retrieved)  expected_document_ids
    = [1, 2, 3, 4, 5] retrieved_documents_ids = [2, 3, 6, 7]  recall = calculate_recall(expected_document_ids,
    retrieved_documents_ids) precision = calculate_precision(expected_document_ids,
    retrieved_documents_ids)  print(f"Recall: {recall:.2f}") # Recall: 0.40 print(f"Precision:
    {precision:.2f}") # Precision: 0.50 [PRE22] @pytest.mark.parametrize("query_vector,
    expected_ids", [ ![1](assets/1.png)     ([0.1, 0.2, 0.3, 0.4], [1, 2, 3]),     ([0.2,
    0.3, 0.4, 0.5], [2, 1, 3]),     ([0.3, 0.4, 0.5, 0.6], [3, 2, 1]),     ... ])
    def test_retrieval_subsystem(db_client, query_vector, expected_ids): ![2](assets/2.png)     response
    = db_client.search( ![2](assets/2.png)         collection_name="test",         query_vector=query_vector,         limit=3     )      retrieved_ids
    = [point.id for point in response]     recall = calculate_recall(expected_ids,
    retrieved_ids)     precision = calculate_precision(expected_ids, retrieved_ids)      assert
    recall >= 0.66 ![3](assets/3.png)     assert precision >= 0.66 ![3](assets/3.png)
    [PRE23] @pytest.mark.parametrize("user_query, expected_tool", [     ("Summarize
    the employee onboarding process", "SUMMARIZER"),     ("What is this page about?
    https://...", "WEBSEARCH"),     ("Analyze the 2024 annual accounts", "ANALYZER"),     ...
    # Add 100 different cases with a balanced category distribution ]) ![1](assets/1.png)
    def test_llm_tool_selection_response(user_query, expected_tool):     response
    = llm.invoke(user_query, response_type="json")     assert response["selected_tool"]
    == expected_tool     assert response["message"] is not None [PRE24] $ pip install
    textstat [PRE25]`##### Esempio 11-17\. Test di funzionalità minima per la leggibilità    [PRE26]    [![1](assets/1.png)](#co_testing_ai_services_CO17-1)      Itera
    su vari esempi controllando il punteggio di leggibilità anche quando un utente
    chiede spiegazioni semplici.      [![2](assets/2.png)](#co_testing_ai_services_CO17-2)      Utilizza
    la formula di Flesch per valutare il punteggio di leggibilità. Un buon punteggio
    si aggira in genere tra 60 e 70, il che indica che il testo è facilmente comprensibile
    dagli studenti delle scuole superiori.      [![3](assets/3.png)](#co_testing_ai_services_CO17-3)      Verifica
    che il punteggio di leggibilità sia superiore ai valori attesi ma non troppo alto.
    Un punteggio molto alto potrebbe indicare risposte troppo semplici e prive di
    dettagli rilevanti.      Il test di leggibilità mostrato nell''[Esempio 11-17](#mft_test)
    dovrebbe darti un''idea su come scrivere le tue MFT. Ad esempio, puoi verificare
    la concisione o il livello di dettaglio delle risposte nei tuoi casi d''uso.[PRE27]`####
    Test di invarianza (IT)    I*test di invarianza* verificano se le previsioni di
    un modello rimangono coerenti quando vengono apportate modifiche irrilevanti agli
    input. Questi test possono misurare la sensibilità dei parametri e verificare
    la robustezza del modello a variazioni che non dovrebbero influenzare gli output.    Esempi
    di IT includono la verifica dell''assenza di cambiamenti nelle risposte del modello
    se si modificano i prompt di:    *   Cambiare la sensibilità alle maiuscole           *   Iniettare
    spazi bianchi, caratteri di escape e caratteri speciali           *   Include
    errori di battitura o grammaticali           *   Sostituzione di parole con sinonimi           *   Commutazione
    del formato dei numeri (tra cifre e parole)           *   Riordinare i pezzi di
    testo/contesto nel prompt              Esistono anche molti altri tipi di controlli
    che possono essere effettuati attraverso i test di invarianza.    L[''esempio
    11-18](#it_test) mostra un semplice test di invarianza.    ##### Esempio 11-18\.
    Test di invarianza    [PRE28]    Come vedi, la maggior parte di questi test modifica
    leggermente gli input con l''aspettativa che gli output rimangano per lo più simili.
    Ora dovresti sentirti sicuro nell''implementare i tuoi test di invarianza.    ####
    Test di aspettativa direzionale (DET)    I*test di aspettativa direzionale* verificano
    se il modello si comporta in modo logico e se le uscite cambiano nella giusta
    direzione al variare degli ingressi.    Esempi di DET sono la verifica dei giusti
    aggiustamenti del sentimento tra il prompt e la risposta o la specificità delle
    risposte a domande specifiche. Se il prompt esprime emozioni negative, il modello
    non deve ignorarle e deve affrontarle in modo appropriato. Allo stesso modo, alle
    domande dettagliate bisogna rispondere con la specificità appropriata.    Come
    puoi vedere nell''[Esempio 11-19](#det_test), ci aspettiamo e verifichiamo una
    correlazione positiva tra il prompt e la risposta sia per quanto riguarda la lunghezza
    che la complessità.    ##### Esempio 11-19\. Test di aspettativa direzionale per
    verificare la lunghezza della risposta    [PRE29]    [![1](assets/1.png)](#co_testing_ai_services_CO18-1)      Itera
    diversi prompt in cui uno è una variante complessa di un altro.      [![2](assets/2.png)](#co_testing_ai_services_CO18-2)      Utilizza
    la lunghezza del testo della risposta come indicatore per verificare la complessità
    relativa delle risposte, partendo dal presupposto che più complesso è il prompt,
    più lunga (e più complessa) è la risposta. Potrebbero esistere indicatori più
    accurati per valutare la complessità delle risposte, come il punteggio di leggibilità
    di Flesch.      Gli MFT, gli IT e i DET non sono gli unici tipi di test che puoi
    implementare per verificare il comportamento dei tuoi modelli. Puoi anche utilizzare
    tecniche più complesse affidandoti ad altri modelli di intelligenza artificiale
    l''esecuzione dei tuoi test, come scoprirai più avanti.    #### Test di autovalutazione    Un''altra
    tecnica per verificare il comportamento dei modelli GenAI è quella di affidarsi
    ad altri modelli AI durante i test, un processo definito *autovalutazione*.    I*test
    di autovalutazione* utilizzano un modello discriminatore/valutatore per verificare
    la qualità dei risultati in base a varie metriche come il tasso di allucinazione,
    la tossicità, la correttezza, la pertinenza delle risposte, ecc. Per i risultati
    LLM, puoi utilizzare un modello LLM o un modello di classificazione come valutatore,
    come mostrato nell''[Esempio 11-20](#self_evaluation_test).    ##### Esempio 11-20\.
    Autovalutazione LLM per la misurazione della tossicità    [PRE30]    [![1](assets/1.png)](#co_testing_ai_services_CO19-1)      Costruisci
    un prompt del sistema di valutazione per il LLM, descrivendo come eseguire il
    compito di valutazione.      [![2](assets/2.png)](#co_testing_ai_services_CO19-3)      Richiedi
    che le risposte siano restituite in formato strutturato per un semplice parsing.
    Puoi anche richiedere delle misure invece di valutazioni booleane.      [![3](assets/3.png)](#co_testing_ai_services_CO19-4)      Ottiene
    con grazia il valore `is_toxic` e fallisce l''asserzione se non è possibile ottenere
    un valore `False`.      L''idea centrale dell''[Esempio 11-20](#self_evaluation_test)
    è quella di far sì che il LLM "valuti se stesso" o di implementare dei test che
    ne verifichino le prestazioni in base a criteri, proprietà o comportamenti predefiniti.    I
    test di valutazione automatica sono tecniche potenti per valutare la qualità delle
    risposte in base a varie metriche, ma si basano su altri modelli e su chiamate
    API aggiuntive, che possono aumentare i costi.    Grazie a queste tecniche di
    test, ora dovresti avere tutti gli strumenti necessari per verificare le prestazioni
    dei tuoi modelli GenAI, sia che tu ti stia interfacciando con un LLM che con altri
    tipi di generatori.    Il passo successivo all''implementazione di diversi test
    di integrazione è quello di testare l''intero sistema utilizzando i test E2E.
    La prossima sezione tratterà l''E2E in modo più dettagliato.[PRE31]`## Test end-to-end    Fino
    a questo punto, hai lavorato su test unitari e di integrazione per i tuoi servizi
    GenAI. Per completare l''ultimo livello di test, ora ti concentrerai sull''implementazione
    di alcuni test E2E.    Nel [Capitolo 5](ch05.html#ch05) hai implementato un web
    scraper e un modulo RAG nel tuo servizio FastAPI e hai sviluppato un''interfaccia
    utente Streamlit per interagire con il tuo servizio API LLM.    Quando hai testato
    la tua applicazione caricando documenti o fornendo URL attraverso l''interfaccia
    utente di Streamlit, stavi eseguendo test E2E *manuali* sull''intero RAG e sulle
    pipeline di web scraper, ciascuna contenente più di due componenti.    La[Figura
    11-11](#e2e_boundaries) mostra i test E2E eseguiti e i loro confini.  ![bgai 1111](assets/bgai_1111.png)  ######
    Figura 11-11\. Confini del test E2E visualizzati sul diagramma della pipeline
    di dati RAG    Come mostrato nella [Figura 11-11](#e2e_boundaries), un segno che
    indica che stai lavorando a un test E2E è la presenza di un confine di test che
    copre più componenti e servizi esterni.    ###### Suggerimento    Puoi combinare
    più test E2E in un test più grande, più complesso ma più lento.    I test più
    grandi tendono a essere più fragili, più incerti e di conseguenza frustranti da
    mantenere, ma possono darti una maggiore sicurezza sulla funzionalità dell''applicazione
    in tutti i componenti e le interazioni.    Sebbene tu abbia eseguito manualmente
    questi test E2E tramite l''interfaccia utente, avresti potuto anche automatizzarli
    utilizzando framework di test con un client di test API o browser headless per
    ridurre il carico di lavoro manuale. Tuttavia, non è necessario automatizzare
    tutti i test E2E, poiché alcuni di essi trarrebbero beneficio dal tocco umano.    I
    test E2E manuali possono comunque aiutarti a scoprire problemi che potrebbero
    passare inosservati con i test automatizzati. Puoi identificare e pianificare
    manualmente alcuni test E2E e poi sviluppare versioni automatizzate da inserire
    nelle tue pipeline CI/CD, assicurandoti di aver tenuto conto della fragilità e
    dell''incertezza di questi test.    Se un test E2E fallisce, significa che anche
    uno o più dei tuoi test di unità o di integrazione potrebbero fallire. Altrimenti,
    forse hai diversi punti ciechi nella tua suite di test o ci sono interazioni tra
    componenti e sottosistemi che danno luogo a comportamenti emergenti a livello
    di sistema che non puoi prevedere con i test di unità o di integrazione.    ######
    Suggerimento    A differenza dei test di unità o di integrazione, non è necessario
    eseguire E2E con la stessa frequenza.    Inoltre, non hai necessariamente bisogno
    di un''interfaccia utente per eseguire i test E2E: puoi attivare gli endpoint
    delle tue API, tramite codice o strumenti di test, e fornire i dati di test per
    verificare la funzionalità prevista di ciascun endpoint.    ###### Avvertenze    Il
    test di un endpoint tramite invocazione non è considerato un test unitario o di
    integrazione, ma piuttosto un test E2E. Questo perché ogni endpoint gestisce una
    funzione di controller che potenzialmente coinvolge diversi servizi e operazioni
    che lavorano insieme per fornire una funzionalità.    Per definizione, i test
    di integrazione hanno come scopo solo la verifica dell''interfaccia di due componenti.    Imparerai
    presto ad automatizzare i test manuali E2E utilizzando `pytest` e un client di
    test API tramite test *verticali* e *orizzontali*:    Test verticali E2E      Verifica
    la funzionalità di una funzione o di un flusso di lavoro specifico, su più livelli
    dell''applicazione, ad esempio dall''interfaccia utente al database.      Test
    E2E orizzontali      Verifica la funzionalità di vari scenari utente, in genere
    su più sistemi e servizi integrati.      Esaminiamo i test E2E verticali in modo
    più dettagliato prima di parlare dei test E2E orizzontali.    ### Test verticali
    E2E    Tornando alla [Figura 11-11](#e2e_boundaries), il test E2E di sinistra,
    che verifica la funzionalità di caricamento dei file, l''estrazione, la trasformazione
    e l''archiviazione dei contenuti in un database, è un test E2E verticale. Allo
    stesso modo, anche il secondo test è considerato verticale, in quanto verifica
    la logica di recupero dei contenuti dal database quando viene fornita una query
    e poi utilizza il modello LLM per la generazione del testo in un contesto di domande
    e risposte. D''altra parte, il test che copre l''intera pipeline di dati RAG,
    dal caricamento dei file alla risposta LLM, è un test orizzontale.    La distinzione
    principale è che i test orizzontali sono più ampi e testano interi scenari utente,
    mentre i test verticali sono più mirati e testano un flusso di lavoro o una funzione
    specifica attraverso i livelli.    ###### Suggerimento    In un''applicazione
    con un''architettura a strati o a cipolla, i test verticali consistono essenzialmente
    nel "navigare nella cipolla" e nel controllare i flussi di dati e le interazioni
    tra i vari livelli per verificare che siano ben integrati e che funzionino come
    previsto.    Prima di implementare qualsiasi test E2E, creiamo una fixture globale
    che inizializzi un client di test FastAPI, come mostrato nell''[Esempio 11-21](#test_client).
    Questo client di test sarà utilizzato per invocare gli endpoint API per i test
    E2E verticali e orizzontali.    ##### Esempio 11-21\. Implementazione di una fixture
    client di prova    [PRE32]    Con il client di test, ora puoi eseguire test E2E
    verticali e orizzontali, iniziando con i test verticali che coprono le funzionalità
    di caricamento e archiviazione dei file, come dimostrato nell''[Esempio 11-22](#test_vertical).    #####
    Esempio 11-22\. Implementazione di un E2E verticale per verificare la funzionalità
    del flusso di lavoro di caricamento e archiviazione    [PRE33]    [![1](assets/1.png)](#co_testing_ai_services_CO20-1)      Usa
    la fixture del client del database vettoriale qdrant che hai creato in precedenza
    durante i test di integrazione.      [![2](assets/2.png)](#co_testing_ai_services_CO20-2)      Carica
    un file utilizzando il client di prova e verifica che la risposta dell''API sia
    positiva.      [![3](assets/3.png)](#co_testing_ai_services_CO20-3)      Controlla
    che la ricerca nel database restituisca il vettore contenente il contenuto del
    file per verificare la funzionalità dell''endpoint `/upload`.      ###### Suggerimento    L[''esempio
    11-22](#test_vertical) potrebbe essere implementato anche con una fixture mock
    `db_client` per evitare di dipendere da una dipendenza esterna. Invece di controllare
    i risultati restituiti dal database, verificherai se il client del database è
    stato chiamato per memorizzare un file e uncontenuto corretti.    Tieni presente
    che l''utilizzo di un mock verificherebbe solo che il client del database sia
    stato chiamato con i parametri previsti, ma non verificherebbe l''effettiva funzionalità
    di memorizzazione o recupero del database.    Come hai visto nell''[Esempio 11-22](#test_vertical),
    i test E2E verticali verificano la funzionalità di un''applicazione livello per
    livello, tipicamente in ordine lineare e gerarchico. Puoi suddividere la tua applicazione
    in livelli distinti e concentrarti su particolari sottosistemi, come le richieste
    API e le chiamate ai database, per verificare se tali sottosistemi funzionano
    come previsto.    ### Test E2E orizzontali    Con i test E2E orizzontali, invece,
    si assume la prospettiva di un utente che naviga attraverso le funzionalità e
    i flussi di lavoro dell''applicazione per individuare errori, bug e altri problemi.
    Questi test coprono l''intera applicazione, quindi è fondamentale avere flussi
    di lavoro ben costruiti e chiaramente definiti per eseguirliin modo efficace.Ad
    esempio, un test E2E orizzontale potrebbe comportare la verifica dell''interfaccia
    utente, del database e dell''integrazione con un LLM per verificare la funzionalità
    di un chatbot abilitato alle RAG da un capo all''altro.    L[''esempio 11-23](#test_horizontal)
    mostra come potrebbe apparire un test orizzontale.    ##### Esempio 11-23\. Implementazione
    di un E2E orizzontale per verificare l''intera funzionalità del flusso di lavoro
    dell''utente RAG Q&A    [PRE34]    [![1](assets/1.png)](#co_testing_ai_services_CO21-1)      Verifica
    che il file sia stato caricato e memorizzato nel database senza errori.      [![2](assets/2.png)](#co_testing_ai_services_CO21-2)      Verifica
    che la risposta di LLM alla domanda del test sia basata sulcontenuto del file
    caricato.      ###### Suggerimento    Puoi scrivere un test orizzontale separato
    per verificare che l''LLM non si riferisca alla sua conoscenza interna o abbia
    delle allucinazioni. Ad esempio, prima di caricare il file nell''[Esempio 11-23](#test_horizontal),
    l''LLM dovrebbe rispondere con "Non lo so" solo se l''utente chiede chi è "Ali
    Parandeh".    Qualsiasi altro risultato potrebbe indicare che LLM ha delle allucinazioni
    o sta utilizzando le sue conoscenze interne. Oppure, il database dei vettori potrebbe
    non essere stato resettato correttamente dalle precedenti esecuzioni del test.
    Una registrazione e un monitoraggio appropriati dei tuoi servizi possono aiutarti
    a risolvere eventuali problemi derivanti da test E2E come questo.    Come hai
    visto nell''[Esempio 11-23](#test_horizontal), il test dei flussi di lavoro degli
    utenti può comportare la chiamata a uno o più endpoint in una sequenza e la verifica
    degli effetti collaterali e dei risultati attesi.    Gli esempi [11-22](#test_vertical)
    e [11-23](#test_horizontal) dovrebbero averti chiarito meglio lo scopo dei test
    E2E, sia verticali che orizzontali; perché differiscono dai test di integrazione;
    e come progettarli e implementarli .[PRE35]``  [PRE36]'
  prefs: []
  type: TYPE_NORMAL
