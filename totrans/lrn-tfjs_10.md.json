["```py\n// Final layer softmax True/False\nmodel.add(\n  tf.layers.dense({\n    units: 2,\n    activation: \"softmax\"\n  })\n);\n```", "```py\n// Final layer sigmoid True/False\nmodel.add(\n  tf.layers.dense({\n    units: 1,\n    activation: \"sigmoid\",\n  })\n);\n```", "```py\n$ npm i danfojs-node\n```", "```py\nconst dfd = require(\"danfojs-node\");\n```", "```py\nconstdf=awaitdfd.read_csv(\"file://../../extra/titanic data/train.csv\");![1](assets/1.png)df.head().print();![2](assets/2.png)\n```", "```py\n// Print the describe data\ndf.describe().print();\n```", "```py\n// Count of empty spots\nempty_spots = df.isna().sum();\nempty_spots.print();\n// Find the average\nempty_rate = empty_spots.div(df.isna().count());\nempty_rate.print();\n```", "```py\n// Load the training CSV constdf=awaitdfd.read_csv(\"file://../../extra/titanic data/train.csv\");console.log(\"Train Size\",df.shape[0])![1](assets/1.png)// Load the test CSV constdft=awaitdfd.read_csv(\"file://../../extra/titanic data/test.csv\");console.log(\"Test Size\",dft.shape[0])![2](assets/2.png)constmega=dfd.concat({df_list:[df,dft],axis: 0})mega.describe().print()![3](assets/3.png)\n```", "```py\n// Remove feature columns that seem less useful\nconst clean = mega.drop({\n  columns: [\"Name\", \"PassengerId\", \"Ticket\", \"Cabin\"],\n});\n```", "```py\n// Remove all rows that have empty spots\nconst onlyFull = clean.dropna();\nconsole.log(`After mega-clean the row-count is now ${onlyFull.shape[0]}`);\n```", "```py\n// Handle embarked characters - convert to numbers constencode=newdfd.LabelEncoder();![1](assets/1.png)encode.fit(onlyFull[\"Embarked\"]);![2](assets/2.png)onlyFull[\"Embarked\"]=encode.transform(onlyFull[\"Embarked\"].values);![3](assets/3.png)onlyFull.head().print();![4](assets/4.png)\n```", "```py\n// 800 random to training\nconst newTrain = onlyFull.sample(800)\nconsole.log(`newTrain row count: ${newTrain.shape[0]}`)\n// The rest to testing (drop via row index)\nconst newTest = onlyFull.drop({index: newTrain.index, axis: 0})\nconsole.log(`newTest row count: ${newTest.shape[0]}`)\n\n// Write the CSV files\nawait newTrain.to_csv('../../extra/cleaned/newTrain.csv')\nawait newTest.to_csv('../../extra/cleaned/newTest.csv')\nconsole.log('Files written!')\n```", "```py\n// Get cleaned data\nconst df = await dfd.read_csv(\"file://../../extra/cleaned/newTrain.csv\");\nconsole.log(\"Train Size\", df.shape[0]);\nconst dft = await dfd.read_csv(\"file://../../extra/cleaned/newTest.csv\");\nconsole.log(\"Test Size\", dft.shape[0]);\n\n// Split train into X/Y\nconst trainX = df.iloc({ columns: [`1:`] }).tensor;\nconst trainY = df[\"Survived\"].tensor;\n\n// Split test into X/Y\nconst testX = dft.iloc({ columns: [`1:`] }).tensor;\nconst testY = dft[\"Survived\"].tensor;\n```", "```py\nmodel.add(tf.layers.dense({inputShape,units: 120,activation:\"relu\",![1](assets/1.png)kernelInitializer:\"heNormal\",![2](assets/2.png)}));model.add(tf.layers.dense({units: 64,activation:\"relu\"}));model.add(tf.layers.dense({units: 32,activation:\"relu\"}));model.add(tf.layers.dense({units: 1,activation:\"sigmoid\",![3](assets/3.png)}));model.compile({optimizer:\"adam\",loss:\"binaryCrossentropy\",![4](assets/4.png)metrics:[\"accuracy\"],![5](assets/5.png)});\n```", "```py\nawaitmodel.fit(trainX,trainY,{batchSize: 32,epochs: 100,validationData:[testX,testY]![1](assets/1.png)})\n```", "```py\n$ npm install -g dnotebook\n```", "```py\ngrp = mega_df.groupby(['Sex'])\ntable(grp.col(['Survived']).mean())\n```", "```py\nsurvival_count = grp.col(['Survived']).count()\ntable(survival_count)\n```", "```py\nsurvivors = mega_df.query({column: \"Survived\", is: \"==\", to: 1 })\n```", "```py\nviz(`agehist`, x => survivors[\"Age\"].plot(x).hist())\n```", "```py\n// Handle person sex - convert to one-hot constsexOneHot=dfd.get_dummies(mega['Sex'])![1](assets/1.png)sexOneHot.head().print()// Swap one column for two mega.drop({columns:['Sex'],axis: 1,inplace: true})![2](assets/2.png)mega.addColumn({column:'male',value: sexOneHot['0']})![3](assets/3.png)mega.addColumn({column:'female',value: sexOneHot['1']})\n```", "```py\n// Group children, young, and over 40yrs\nfunction ageToBucket(x) {\n  if (x < 10) {\n    return 0\n  } else if (x < 40) {\n    return 1\n  } else {\n    return 2\n  }\n}\n```", "```py\n// Create Age buckets\nageBuckets = mega['Age'].apply(ageToBucket)\nmega.addColumn({ column: 'Age_bucket', value: ageBuckets })\n```", "```py\nconst scaler = new dfd.MinMaxScaler()\nscaledData = scaler.fit(featuredData)\nscaledData.head().print()\n```", "```py\nawait model.fit(trainX, trainY, {\n  batchSize: 32,\n  epochs: 100,\n  // Keep random 20% for validation on the fly.\n  // The 20% is selected at the beginning of the training session.\n  validationSplit: 0.2,\n})\n```"]