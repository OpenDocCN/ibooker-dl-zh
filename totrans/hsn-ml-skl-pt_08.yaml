- en: Chapter 7\. Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many machine learning problems involve thousands or even millions of features
    for each training instance. Not only do all these features make training extremely
    slow, but they can also make it much harder to find a good solution, as you will
    see. This problem is often referred to as the *curse of dimensionality*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, in real-world problems, it is often possible to reduce the number
    of features considerably, turning an intractable problem into a tractable one.
    For example, consider the MNIST images (introduced in [Chapter 3](ch03.html#classification_chapter)):
    the pixels on the image borders are almost always white, so you could completely
    drop these pixels from the training set without losing much information. As we
    saw in the previous chapter, [Figure 6-6](ch06.html#mnist_feature_importance_plot)
    confirms that these pixels are utterly unimportant for the classification task.
    Additionally, two neighboring pixels are often highly correlated: if you merge
    them into a single pixel (e.g., by taking the mean of the two pixel intensities),
    you will not lose much information, removing redundancy and sometimes even noise.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Reducing dimensionality can also drop some useful information, just like compressing
    an image to JPEG can degrade its quality: it can make your system perform slightly
    worse, especially if you reduce dimensionality too much. Moreover, some models—such
    as neural networks—can handle high-dimensional data efficiently and learn to reduce
    its dimensionality while preserving the useful information for the task at hand.
    In short, adding an extra preprocessing step for dimensionality reduction will
    not always help.'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from speeding up training and possibly improving your model’s performance,
    dimensionality reduction is also extremely useful for data visualization. Reducing
    the number of dimensions down to two (or three) makes it possible to plot a condensed
    view of a high-dimensional training set on a graph and often gain some important
    insights by visually detecting patterns, such as clusters. Moreover, data visualization
    is essential to communicate your conclusions to people who are not data scientists—in
    particular, decision makers who will use your results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will first discuss the curse of dimensionality and get a
    sense of what goes on in high-dimensional space. Then we will consider the two
    main approaches to dimensionality reduction (projection and manifold learning),
    and we will go through three of the most popular dimensionality reduction techniques:
    PCA, random projection, and locally linear embedding (LLE).'
  prefs: []
  type: TYPE_NORMAL
- en: The Curse of Dimensionality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are so used to living in three dimensions⁠^([1](ch07.html#id1866)) that our
    intuition fails us when we try to imagine a high-dimensional space. Even a basic
    4D hypercube is incredibly hard to picture in our minds (see [Figure 7-1](#hypercube_wikipedia)),
    let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the progression from a point to a tesseract, demonstrating
    the concept of hypercubes from 0D to 4D.](assets/hmls_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)⁠^([2](ch07.html#id1867))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It turns out that many things behave very differently in high-dimensional space.
    For example, if you pick a random point in a unit square (a 1 × 1 square), it
    will have only about a 0.4% chance of being located less than 0.001 from a border
    (in other words, it is very unlikely that a random point will be “extreme” along
    any dimension). But in a 10,000-dimensional unit hypercube, this probability is
    greater than 99.999999%. Most points in a high-dimensional hypercube are very
    close to the border.⁠^([3](ch07.html#id1868))
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a more troublesome difference: if you pick two points randomly in a
    unit square, the distance between these two points will be, on average, roughly
    0.52\. If you pick two random points in a 3D unit cube, the average distance will
    be roughly 0.66\. But what about two points picked randomly in a 1,000,000-dimensional
    unit hypercube? The average distance, believe it or not, will be about 408.25
    (roughly $StartRoot StartFraction 1 comma 000 comma 000 Over 6 EndFraction EndRoot$
    )! This is counterintuitive: how can two points be so far apart when they both
    lie within the same unit hypercube? Well, there’s just plenty of space in high
    dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result, high-dimensional datasets are often very sparse: most training
    instances are likely to be far away from each other, so training methods based
    on distance or similarity (such as *k*-nearest neighbors) will be much less effective.
    And some types of models will not be usable at all because they scale poorly with
    the dataset’s dimensionality (e.g., SVMs or dense neural networks). And new instances
    will likely be far away from any training instance, making predictions much less
    reliable than in lower dimensions since they will be based on much larger extrapolations.
    Since patterns in the data will become harder to identify, models will tend to
    fit the noise more frequently than in lower dimensions; regularization will become
    all the more important. Lastly, models will become even harder to interpret.'
  prefs: []
  type: TYPE_NORMAL
- en: In theory, some of these issues can be resolved by increasing the size of the
    training set to reach a sufficient density of training instances. Unfortunately,
    in practice, the number of training instances required to reach a given density
    grows exponentially with the number of dimensions. With just 100 features—significantly
    fewer than in the MNIST problem—all ranging from 0 to 1, you would need more training
    instances than atoms in the observable universe in order for training instances
    to be within 0.1 of each other on average, assuming they were spread out uniformly
    across all dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Main Approaches for Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before diving into specific dimensionality reduction algorithms, let’s look
    at the two main approaches to reducing dimensionality: projection and manifold
    learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Projection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In most real-world problems, training instances are *not* spread out uniformly
    across all dimensions. Many features are almost constant, while others are highly
    correlated (as discussed earlier for MNIST). As a result, all training instances
    lie within (or close to) a much lower-dimensional *subspace* of the high-dimensional
    space. This sounds abstract, so let’s look at an example. In [Figure 7-2](#dataset_3d_plot),
    a 3D dataset is represented by small spheres (I will refer to this figure several
    times in the following sections).
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that all training instances lie close to a plane: this is a lower-dimensional
    (2D) subspace of the higher-dimensional (3D) space. If we project every training
    instance perpendicularly onto this subspace (as represented by the short dashed
    lines connecting the instances to the plane), we get the new 2D dataset shown
    in [Figure 7-3](#dataset_2d_plot). Ta-da! We have just reduced the dataset’s dimensionality
    from 3D to 2D. Note that the axes correspond to new features *z*[1] and *z*[2]:
    they are the coordinates of the projections on the plane.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A 3D scatter plot showing data points clustered near a 2D plane, illustrating
    a lower-dimensional subspace in higher-dimensional space.](assets/hmls_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. A 3D dataset lying close to a 2D subspace
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Scatter plot showing the 2D dataset with axes labeled as new features z1
    and z2, illustrating dimensionality reduction from 3D to 2D.](assets/hmls_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. The new 2D dataset after projection
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Manifold Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although projection is fast and often works well, it’s not always the best
    approach to dimensionality reduction. In many cases the subspace may twist and
    turn, such as in the Swiss roll dataset represented in [Figure 7-4](#swiss_roll_plot):
    this is a toy dataset containing 3D points in the shape of a Swiss roll.'
  prefs: []
  type: TYPE_NORMAL
- en: '![3D scatter plot of the Swiss roll dataset, illustrating points arranged in
    a spiral shape, used to demonstrate challenges in dimensionality reduction.](assets/hmls_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Swiss roll dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Simply projecting onto a plane (e.g., by dropping *x*[3]) would squash different
    layers of the Swiss roll together, as shown on the left side of [Figure 7-5](#squished_swiss_roll_plot).
    What you probably want instead is to unroll the Swiss roll to obtain the 2D dataset
    on the righthand side of [Figure 7-5](#squished_swiss_roll_plot).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing a squashed Swiss roll dataset on the left, where layers overlap,
    versus an unrolled version on the right, where the data is spread out in two dimensions.](assets/hmls_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. Squashing by projecting onto a plane (left) versus unrolling the
    Swiss roll (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The Swiss roll is an example of a 2D *manifold*. Put simply, a 2D manifold
    is a 2D shape that can be bent and twisted in a higher-dimensional space. More
    generally, a *d*-dimensional manifold is a part of an *n*-dimensional space (where
    *d* < *n*) that locally resembles a *d*-dimensional hyperplane. In the case of
    the Swiss roll, *d* = 2 and *n* = 3: it locally resembles a 2D plane, but it is
    rolled in the third dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: Many dimensionality reduction algorithms (e.g., LLE, Isomap, t-SNE, or UMAP),
    work by modeling the manifold on which the training instances lie; this is called
    *manifold learning*. It relies on the *manifold assumption*, also called the *manifold
    hypothesis*, which holds that most real-world high-dimensional datasets lie close
    to a much lower-dimensional manifold. This assumption is very often empirically
    observed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, think about the MNIST dataset: all handwritten digit images have
    some similarities. They are made of connected lines, the borders are white, and
    they are more or less centered. If you randomly generated images, only a ridiculously
    tiny fraction of them would look like handwritten digits. In other words, the
    degrees of freedom available to you if you try to create a digit image are dramatically
    lower than the degrees of freedom you have if you are allowed to generate any
    image you want. These constraints tend to squeeze the dataset into a lower-dimensional
    manifold.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The manifold assumption is often accompanied by another implicit assumption:
    that the task at hand (e.g., classification or regression) will be simpler if
    expressed in the lower-dimensional space of the manifold. For example, in the
    top row of [Figure 7-6](#manifold_decision_boundary_plot) the Swiss roll is split
    into two classes: in the 3D space (on the left) the decision boundary would be
    fairly complex, but in the 2D unrolled manifold space (on the right) the decision
    boundary is a straight line.'
  prefs: []
  type: TYPE_NORMAL
- en: However, this implicit assumption does not always hold. For example, in the
    bottom row of [Figure 7-6](#manifold_decision_boundary_plot), the decision boundary
    is located at *x*[1] = 5\. This decision boundary looks very simple in the original
    3D space (a vertical plane), but it looks more complex in the unrolled manifold
    (a collection of four independent line segments).
  prefs: []
  type: TYPE_NORMAL
- en: In short, reducing the dimensionality of your training set before training a
    model will usually speed up training, but it may not always lead to a better or
    simpler solution; it all depends on the dataset. Dimensionality reduction is typically
    more effective when the dataset is small relative to the number of features, especially
    if it’s noisy, or many features are highly correlated to one another (i.e., redundant).
    And if you have domain knowledge about the process that generated the data, and
    you know it’s simple, then the manifold assumption certainly holds, and dimensionality
    reduction is likely to help.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, you now have a good sense of what the curse of dimensionality is
    and how dimensionality reduction algorithms can fight it, especially when the
    manifold assumption holds. The rest of this chapter will go through some of the
    most popular algorithms for dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagrams illustrating how dimensionality reduction affects decision boundaries,
    showing a complex spiral structure on the left and its simplified lower-dimensional
    projections on the right.](assets/hmls_0706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. The decision boundary may not always be simpler with lower dimensions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Principal component analysis* (PCA) is by far the most popular dimensionality
    reduction algorithm. First it identifies the hyperplane that lies closest to the
    data, and then it projects the data onto it, as shown back in [Figure 7-2](#dataset_3d_plot).'
  prefs: []
  type: TYPE_NORMAL
- en: Preserving the Variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you can project the training set onto a lower-dimensional hyperplane,
    you first need to choose the right hyperplane. For example, a simple 2D dataset
    is represented on the left in [Figure 7-7](#pca_best_projection_plot), along with
    three different axes (i.e., 1D hyperplanes). On the right is the result of the
    projection of the dataset onto each of these axes. As you can see, the projection
    onto the solid line preserves the maximum variance (top), while the projection
    onto the dotted line preserves very little variance (bottom), and the projection
    onto the dashed line preserves an intermediate amount of variance (middle).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of PCA showing a 2D dataset on the left projected onto three different
    1D axes. The solid line preserves the most variance, the dashed line an intermediate
    amount, and the dotted line the least, as depicted by the spread of points on
    the right.](assets/hmls_0707.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. Selecting the subspace on which to project
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It seems reasonable to select the axis that preserves the maximum amount of
    variance, as it will most likely lose less information than the other projections.
    Consider your shadow on the ground when the sun is directly overhead: it’s a small
    blob that doesn’t look anything like you. But your shadow on a wall at sunrise
    is much larger and it *does* look like you. Another way to justify choosing the
    axis that maximizes the variance is that it is also the axis that minimizes the
    mean squared distance between the original dataset and its projection onto that
    axis. This is the rather simple idea behind PCA, introduced way back [in 1901](https://homl.info/pca)!⁠^([4](ch07.html#id1877))'
  prefs: []
  type: TYPE_NORMAL
- en: Principal Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PCA identifies the axis that accounts for the largest amount of variance in
    the training set. In [Figure 7-7](#pca_best_projection_plot), it is the solid
    line. It also finds a second axis, orthogonal to the first one, that accounts
    for the largest amount of the remaining variance. In this 2D example there is
    no choice: it is the dotted line. If it were a higher-dimensional dataset, PCA
    would also find a third axis, orthogonal to both previous axes, and a fourth,
    a fifth, and so on—as many axes as the number of dimensions in the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: The *i*^(th) axis is called the *i*^(th) *principal component* (PC) of the data.
    In [Figure 7-7](#pca_best_projection_plot), the first PC is the axis on which
    vector **c**[**1**] lies, and the second PC is the axis on which vector **c**[**2**]
    lies. In [Figure 7-2](#dataset_3d_plot), the first two PCs are on the projection
    plane, and the third PC is the axis orthogonal to that plane. After the projection,
    back in [Figure 7-3](#dataset_2d_plot), the first PC corresponds to the *z*[1]
    axis, and the second PC corresponds to the *z*[2] axis.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For each principal component, PCA finds a zero-centered unit vector pointing
    along the direction of the PC. Unfortunately, its direction is not guaranteed:
    if you perturb the training set slightly and run PCA again, the unit vector may
    point in the opposite direction. In fact, a pair of unit vectors may even rotate
    or swap if the variances along these two axes are very close. So if you use PCA
    as a preprocessing step before a model, make sure you always retrain the model
    entirely every time you update the PCA transformer: if you don’t and if the PCA’s
    output doesn’t align with the previous version, the model will be very confused.'
  prefs: []
  type: TYPE_NORMAL
- en: So how can you find the principal components of a training set? Luckily, there
    is a standard matrix factorization technique called *singular value decomposition*
    (SVD) that can decompose the training set matrix **X** into the product of three
    matrices **U** **Σ** **V**^⊺, where **V** contains the unit vectors that define
    all the principal components that you are looking for, in the correct order, as
    shown in [Equation 7-1](#principal_components_matrix).⁠^([5](ch07.html#id1880))
  prefs: []
  type: TYPE_NORMAL
- en: Equation 7-1\. Principal components matrix
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $bold upper V equals Start 3 By 4 Matrix 1st Row 1st Column bar 2nd Column bar
    3rd Column Blank 4th Column bar 2nd Row 1st Column bold c 1 2nd Column bold c
    2 3rd Column midline-horizontal-ellipsis 4th Column bold c Subscript n Baseline
    3rd Row 1st Column bar 2nd Column bar 3rd Column Blank 4th Column bar EndMatrix$
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Python code uses NumPy’s `svd()` function to obtain all the principal
    components of the 3D training set represented back in [Figure 7-2](#dataset_3d_plot),
    then it extracts the two unit vectors that define the first two PCs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PCA assumes that the dataset is centered around the origin. As you will see,
    Scikit-Learn’s PCA classes take care of centering the data for you. If you implement
    PCA yourself (as in the preceding example), or if you use other libraries, don’t
    forget to center the data first.
  prefs: []
  type: TYPE_NORMAL
- en: Projecting Down to d Dimensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you have identified all the principal components, you can reduce the dimensionality
    of the dataset down to *d* dimensions by projecting it onto the hyperplane defined
    by the first *d* principal components (we will discuss how to choose the number
    of dimensions *d* shortly). Selecting this hyperplane ensures that the projection
    will preserve as much variance as possible. For example, in [Figure 7-2](#dataset_3d_plot)
    the 3D dataset is projected down to the 2D plane defined by the first two principal
    components, preserving a large part of the dataset’s variance. As a result, the
    2D projection looks very much like the original 3D dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To project the training set onto the hyperplane and obtain a reduced dataset
    **X**[*d*-proj] of dimensionality *d*, compute the matrix multiplication of the
    training set matrix **X** by the matrix **W**[*d*], defined as the matrix containing
    the first *d* columns of **V**, as shown in [Equation 7-2](#pca_projection).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 7-2\. Projecting the training set down to *d* dimensions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $bold upper X Subscript d hyphen proj Baseline equals bold upper X bold upper
    W Subscript d$
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Python code projects the training set onto the plane defined
    by the first two principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: There you have it! You now know how to reduce the dimensionality of any dataset
    by projecting it down to any number of dimensions, while preserving as much variance
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Using Scikit-Learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scikit-Learn’s `PCA` class uses SVD to implement PCA, just like we did earlier
    in this chapter. The following code applies PCA to reduce the dimensionality of
    the dataset down to two dimensions (note that it automatically takes care of centering
    the data):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After fitting the `PCA` transformer to the dataset, its `components_` attribute
    holds the transpose of **W**[*d*]: it contains one row for each of the first *d*
    principal components.'
  prefs: []
  type: TYPE_NORMAL
- en: Explained Variance Ratio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another useful piece of information is the *explained variance ratio* of each
    principal component, available via the `explained_variance_ratio_` variable. The
    ratio indicates the proportion of the dataset’s variance that lies along each
    principal component. For example, let’s look at the explained variance ratios
    of the first two components of the 3D dataset represented in [Figure 7-2](#dataset_3d_plot):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`This output tells us that about 82% of the dataset’s variance lies along the
    first PC, and about 11% lies along the second PC. This leaves about 7% for the
    third PC, so it is reasonable to assume that the third PC probably carries little
    information.`  [PRE4] ## Choosing the Right Number of Dimensions    Instead of
    arbitrarily choosing the number of dimensions to reduce down to, it is simpler
    to choose the number of dimensions that add up to a sufficiently large portion
    of the variance—say, 95%. (An exception to this rule, of course, is if you are
    reducing dimensionality for data visualization, in which case you will want to
    reduce the dimensionality down to 2 or 3.)    The following code loads and splits
    the MNIST dataset (introduced in [Chapter 3](ch03.html#classification_chapter))
    and performs PCA without reducing dimensionality, then computes the minimum number
    of dimensions required to preserve 95% of the training set’s variance:    [PRE5]py    You
    could then set `n_components=d` and run PCA again, but there’s a better option.
    Instead of specifying the number of principal components you want to preserve,
    you can set `n_components` to be a float between 0.0 and 1.0, indicating the ratio
    of variance you wish to preserve:    [PRE6]py    The actual number of components
    is determined during training, and it is stored in the `n_components_` attribute:    [PRE7]py   [PRE8]`
    [PRE9] X_recovered = pca.inverse_transform(X_reduced) [PRE10] rnd_pca = PCA(n_components=154,
    svd_solver="randomized", random_state=42) X_reduced = rnd_pca.fit_transform(X_train)
    [PRE11] from sklearn.decomposition import IncrementalPCA  n_batches = 100 inc_pca
    = IncrementalPCA(n_components=154) for X_batch in np.array_split(X_train, n_batches):     inc_pca.partial_fit(X_batch)  X_reduced
    = inc_pca.transform(X_train) [PRE12] filename = "my_mnist.mmap" X_mmap = np.memmap(filename,
    dtype=''float32'', mode=''write'', shape=X_train.shape) X_mmap[:] = X_train  #
    could be a loop instead, saving the data chunk by chunk X_mmap.flush() [PRE13]
    X_mmap = np.memmap(filename, dtype="float32", mode="readonly").reshape(-1, 784)
    batch_size = X_mmap.shape[0] // n_batches inc_pca = IncrementalPCA(n_components=154,
    batch_size=batch_size) inc_pca.fit(X_mmap) [PRE14]` [PRE15][PRE16][PRE17] [PRE18]py`
    # Random Projection    As its name suggests, the random projection algorithm projects
    the data to a lower-dimensional space using a random linear projection. This may
    sound crazy, but it turns out that such a random projection is actually very likely
    to preserve distances fairly well, as was demonstrated mathematically by William
    B. Johnson and Joram Lindenstrauss in a famous lemma. So, two similar instances
    will remain similar after the projection, and two very different instances will
    remain very different.    Obviously, the more dimensions you drop, the more information
    is lost, and the more distances get distorted. So how can you choose the optimal
    number of dimensions? Well, Johnson and Lindenstrauss came up with an equation
    that determines the minimum number of dimensions to preserve in order to ensure—with
    high probability—that distances won’t change by more than a given tolerance. For
    example, if you have a dataset containing *m* = 5,000 instances with *n* = 20,000
    features each, and you don’t want the squared distance between any two instances
    to change by more than *ε* = 10%,^([7](ch07.html#id1908)) then you should project
    the data down to *d* dimensions, with *d* ≥ 4 log(*m*) / (½ *ε*² - ⅓ *ε*³), which
    is 7,300 dimensions. That’s quite a significant dimensionality reduction! Notice
    that the equation does not use *n*, it only relies on *m* and *ε*. This equation
    is implemented by the `johnson_lindenstrauss_min_dim()` function:    [PRE19]py`
    `>>>` `d` `=` `johnson_lindenstrauss_min_dim``(``m``,` `eps``=``ε``)` [PRE20]py
    [PRE21]``py [PRE22]`py  [PRE23]py [PRE24]py`` [PRE25]'
  prefs: []
  type: TYPE_NORMAL
