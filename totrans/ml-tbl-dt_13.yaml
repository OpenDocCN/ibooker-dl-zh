- en: 11 Building a machine learning pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: An overview of machine learning pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prerequisites for running a machine learning pipeline in Vertex AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model training and deployment: local implementation vs. machine learning pipeline
    implementation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a machine learning pipeline to train and deploy a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating the model training code to work with a machine learning pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using generative AI to help create the machine learning pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 10, we went through the steps to deploy a deep learning model trained
    on tabular data. We deployed the model in a web application, first with the model
    running entirely on our local system and then having the model deployed to a Vertex
    AI endpoint. In this chapter, we will go through the further steps to automate
    the training and deployment process by using a machine learning (ML) pipeline
    in Vertex AI. We will start by going over the setup steps necessary for a ML pipeline,
    including defining a Vertex AI dataset. Next, we will contrast the local model
    training and deployment we have seen from chapter 10 with model training and deployment
    using an ML pipeline. We will proceed to review the code specifically for the
    ML pipeline itself, along with the updates to the existing code required for the
    model training code to work in the context of an ML pipeline. Finally, we will
    examine some of the ways that we can apply generative AI and get useful help from
    its outputs in the workflow for creating a ML pipeline. The code described in
    this chapter is available at [https://mng.bz/DM4n](https://mng.bz/DM4n).
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 Introduction to ML pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider the steps that we have covered so far in this book to prepare a deep
    learning model trained on tabular data:'
  prefs: []
  type: TYPE_NORMAL
- en: Process the data to deal with problems such as missing values, columns containing
    two distinct kinds of data, and numeric data expressed as strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the model using the processed data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy the trained model so that it can be used by an application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose we needed to go through this process repeatedly for the Kuala Lumpur
    real estate problem. This is a reasonable expectation because the real estate
    market will keep changing as prices develop, interest rates change, and macroeconomic
    factors affect the demand for real estate. Rather than running the various notebooks
    and deployment steps manually for each end-to-end cycle from raw data to deployed
    model, it would be better to have a coded solution that we could run as a unit
    repeatedly and consistently. An ML pipeline gives us this exactly, and in this
    section, we will go through an example illustrating how to set up a simple, end-to-end
    pipeline for the Kuala Lumpur real estate problem.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1.1 Three kinds of pipelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before getting into the details of an ML pipeline, it is worth noting that
    the term *pipeline* has been overloaded with different meanings over time. At
    the moment, there are at least three distinct meanings for the term *pipeline*
    that are predominant in the world of ML/data science:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Training/inference pipeline*—This pipeline ensures that data transformations,
    such as assigning text to tokens or assigning values in a categorical column to
    numeric identifiers, are done consistently in the training and inference steps.
    The preprocessing Keras layers in the Kuala Lumpur model constitute this kind
    of pipeline because they ensure, for example, that the transformations done on
    the processed data prior to training exactly match the transformations done on
    the data points entered in `home.html` in the web deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data pipeline*—This pipeline deals with anomalies in the input training data,
    such as missing values or schema problems. It can overlap the pipeline described
    in the previous point, but it performs a distinct task. In the context of Google
    Cloud, Dataflow and Cloud Data Fusion are examples of products that can perform
    data pipeline tasks. You don’t need to know about Dataflow or Cloud Data Fusion
    for the purposes of this chapter, but if you are curious, you can check out the
    documentation: [https://cloud.google.com/dataflow/docs](https://cloud.google.com/dataflow/docs)
    and [https://cloud.google.com/data-fusion/docs](https://cloud.google.com/data-fusion/docs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ML pipeline*—This is a pipeline that automates various steps such as training,
    deploying, and monitoring the model. TFX and KubeFlow are the two approaches that
    are available in Vertex AI for implementing ML pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 11.1 shows how each of these three kinds of pipelines fits into the end-to-end
    ML workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F01_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 Three kinds of pipelines and how they relate
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.1 illustrates the following characteristics of the pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: The ML pipeline can encompass the entire workflow, from raw data to monitoring
    the deployed model. The rationale for this is that the ML pipeline is intended
    to automate the complete process when the model needs to be retrained and redeployed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distinction between a data pipeline and a training/inference pipeline is
    that the training/inference pipeline handles transformations that need to be applied
    to new data points to which we want to apply the trained model to get predictions,
    such as replacing categorical values with numeric identifiers. The same transformations
    need to be applied to the prepared data prior to training the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we saw in the Keras custom layers solution to the Airbnb NYC price prediction
    problem in chapter 3, the training/inference pipeline can be distinct from the
    model training process. In the Keras customer layers solution, the training/inference
    pipeline was implemented using Scikit-learn pipeline structures and custom classes,
    both of which need to be applied to data prior to model training and prior to
    applying new data points to the trained model to get predictions. In chapter 9,
    on the other hand, we saw how the same processing could be incorporated directly
    into the Keras model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data pipelines can exist outside the context of the ML workflow. The same data
    pipeline tools, such as Dataflow and Cloud Data Fusion, that can be used in ML
    workflows in Google Cloud can be part of applications that don’t include ML.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have described three different kinds of pipelines, in the next section,
    we will start to explore how to create an ML pipeline for the Kuala Lumpur real
    estate price prediction problem in Google Cloud using Kubeflow.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1.2 Overview of Vertex AI ML pipelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In chapter 10, we went through the process of deploying the Kuala Lumpur real
    estate price prediction model to a Vertex AI endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create an ML pipeline for the Kuala Lumpur price prediction model, we are
    going to start with the steps described in the Vertex AI documentation: [https://mng.bz/lYW6](https://mng.bz/lYW6).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an overview of the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up a *service account*. A service account is an account used by an application
    to take actions in Google Cloud. When we imported the Keras model into Google
    Cloud and deployed it to an endpoint, we used our own ID to perform these actions.
    Since the ML pipeline will be an automated script, we need a service account to
    allow the script to perform actions without depending directly on manual intervention
    from any individual. See the Google Cloud documentation for more details on service
    accounts: [https://mng.bz/BXA0](https://mng.bz/BXA0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get a service account key for the service account and provide the service account
    with the required access to run the ML pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a pipeline script to invoke the Vertex AI SDK.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapt the model training notebook to be a standalone Python script that can
    be run in a prebuilt Vertex AI container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the pipeline script to run the training script inside a container and generate
    a trained model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the subsequent sections, we will go through these steps to create an ML pipeline
    for the Kuala Lumpur real estate prediction model.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 ML pipeline preparation steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we can run the ML pipeline to train and deploy a model, we need to set
    up the Google Cloud objects the pipeline needs. In this section, we will set up
    a service account and introduce the Cloud Shell, an instance that is available
    directly in Google Cloud that we can use to enter commands. We will also upload
    our dataset to Google Cloud Storage and use the uploaded dataset to create a Vertex
    AI dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.1 Creating a service account for the ML pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since we want to be able to run the ML pipeline automatically without manual
    intervention, we need to set up a service account to run the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a service account, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.  Select IAM & Admin -> Service Accounts from the overall Google Cloud Console
    menu, as shown in figure 11.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F02_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 Selecting Service Accounts in the Google Cloud Console
  prefs: []
  type: TYPE_NORMAL
- en: 2.  In the Service Accounts page, select Create Service Account, as shown in
    figure 11.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F03_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 Creating a service account
  prefs: []
  type: TYPE_NORMAL
- en: '3.  In the Create service account page, enter a name for the service account
    and click Create and Continue, as shown in figure 11.4\. Note that the service
    account ID gets filled in automatically and that an email ID for the service account
    is shown in the form `service-account-id@project-id.iam.gserviceaccount.com`—in
    this case: `ml-tabular-pipeline@first-project-ml-tabular.iam.gserviceaccount.com`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F04_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 Setting a service account name
  prefs: []
  type: TYPE_NORMAL
- en: 4.  Select Vertex AI User in the Role field and click Done, as shown in figure
    11.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F05_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 Giving the service account Vertex AI user role
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have created a service account and given it access to Vertex AI,
    in the next section we can create a service account key.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.2 Creating a service account key
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ML pipeline uses a service account key to authenticate the service account
    used to run the ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a service account key, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.  In the Service accounts page, click on the email address for the service
    account that you just created, as shown in figure 11.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F06_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 Selecting the service account
  prefs: []
  type: TYPE_NORMAL
- en: 2.  Select the Keys tab and click Add key -> Create new key, as shown in figure
    11.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F07_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 Creating a service account key
  prefs: []
  type: TYPE_NORMAL
- en: 3.  Select JSON and click Create, as shown in figure 11.8.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F08_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 Downloading the service account key
  prefs: []
  type: TYPE_NORMAL
- en: 'A JSON file containing the service account key is created and downloaded to
    your local system with a name that looks like: `first-project-ml-tabular-039ff1f820a8.json`.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.3 Granting the service account access to the Compute Engine default service
    account
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you set up your project in Google Cloud, a Compute Engine default service
    account was created. This account has an email address like `PROJECT_NUMBER-compute@developer.gserviceaccount.com`.
    For more details on the Compute Engine default service account, see the documentation
    ([https://mng.bz/dXdN](https://mng.bz/dXdN)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to give the service account that we set up in the preceding sections
    access to the Compute Engine default service account to run the ML pipeline. Follow
    these steps to set up this access to the Compute Engine default service account:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.  In the Service accounts page, click the copy icon beside the email address
    for the service account you just created (you will need this in the next step)
    and then click the email address of the Compute Engine default service account,
    as shown in figure 11.9.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F09_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 Compute Engine default service account
  prefs: []
  type: TYPE_NORMAL
- en: 2.  Click the Permissions tab and click Grant Access, as shown in figure 11.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F10_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 Granting access to the Compute Engine default service account
  prefs: []
  type: TYPE_NORMAL
- en: 3.  In the Grant access page, paste the email ID of the service account that
    you created in the New Principals field, select Service Account User in the Role
    field, and click Save, as shown in figure 11.11.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F11_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 Specifying access to the Compute Engine default service account
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have completed the steps to set up the service account for the ML
    pipeline, we can continue with the setup of the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.4 Introduction to Cloud Shell
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, all the actions that we have taken in Google Cloud have been in the
    Console UI. Google Cloud also includes the Cloud Shell, which is a self-contained
    instance that lets you run command line commands to interact with Google Cloud.
    In addition to the command line interface, you can use the Cloud Shell Editor
    to edit files in the Cloud Shell filesystem. With Cloud Shell, you get the function
    of a local Linux instance combined with the convenience of a web-based environment
    that is integrated with Google Cloud resources. Cloud Shell is particularly well-suited
    for prototyping and working through tutorials. We will use the Cloud Shell in
    the next few steps of setting up the ML pipeline. For additional details about
    Cloud Shell, see the documentation: [https://cloud.google.com/shell](https://cloud.google.com/shell).'
  prefs: []
  type: TYPE_NORMAL
- en: To start the Cloud Shell, click on the Activate Cloud Shell icon at the top
    of the Google Cloud Console, as shown in figure 11.12.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F12_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 Activating Cloud Shell icon
  prefs: []
  type: TYPE_NORMAL
- en: When you click on the Activate Cloud Shell icon, the Cloud Shell terminal opens
    at the bottom of the console with your home directory as the current directory,
    as shown in figure 11.13.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F13_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 Cloud Console with Cloud Shell activated
  prefs: []
  type: TYPE_NORMAL
- en: You can run commands directly in the Cloud Shell Terminal, including standard
    Linux commands and Google Cloud-specific commands. You can click Open Editor to
    edit files in the Cloud Shell file system, as shown in figure 11.14\. To get back
    to the Cloud Shell Terminal, click on Open Terminal.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F14_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 Cloud Shell Editor
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have taken a brief tour of the Cloud Shell, we can continue with
    the next step of setting up the ML pipeline: making the service account key available
    to the pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.5 Uploading the service account key
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will use the Cloud Shell to upload the service account
    key JSON file and then set an environment variable to point to the location of
    the service account key:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.  In Cloud Shell, set your home directory as the current directory, create
    a new directory called `ml_pipeline`, and then set that new directory as your
    current directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 2.  To upload the service account key, select the three dots in the Cloud Shell
    toolbar and select Upload, as shown in figure 11.15.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F15_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 Uploading a file in Cloud Shell
  prefs: []
  type: TYPE_NORMAL
- en: 3.  In the Upload page, update Destination Directory to be the `ml_pipeline`
    directory in your home directory, click Choose Files, and select the service account
    key JSON file that you downloaded in section 11.2.2 and click Upload, as shown
    in figure 11.16.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F16_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16 Setting upload parameters
  prefs: []
  type: TYPE_NORMAL
- en: '4.  Validate the upload by making `~/ml_pipeline` your current directory and
    using the `ls` command to ensure that the JSON service account key is now in this
    directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '5.  Set the environment variable `GOOGLE_APPLICATION_CREDENTIALS` to the fully
    qualified filename of the service account key JSON file. In the following example,
    replace the fully qualified filename with that for your own service account key
    JSON file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '6.  Confirm the value of the `GOOGLE_APPLICATION_CREDENTIALS` environment variable
    with the following command and validate that it is set to the fully qualified
    path of your service account key file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have uploaded the service account key and set the environment variable
    to point to the location of the service account key, we are ready to get into
    the key step of defining the ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.6 Uploading the cleaned-up dataset to a Google Cloud Storage bucket
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To simplify the pipeline, we will upload the processed dataset generated by
    the data preparation notebook ([https://mng.bz/rKjB](https://mng.bz/rKjB)) to
    a Cloud Storage bucket so that it is accessible to the rest of the ML pipeline.
    In a real-world application, we would incorporate the data cleanup steps into
    the ML pipeline, but for the sake of simplicity, we will start the pipeline with
    the data already cleaned up. Follow the steps in this section to upload the cleaned-up
    dataset to Google Cloud Storage:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.  Upload the CSV version of the cleaned-up dataset to the same bucket that
    you created to upload the model.
  prefs: []
  type: TYPE_NORMAL
- en: 2.  From the Google Cloud Console main menu, select Cloud Storage -> Buckets,
    as shown in figure 11.17.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F17_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.17 Setting upload parameters
  prefs: []
  type: TYPE_NORMAL
- en: 3.  In the Buckets page, select the bucket you created in chapter 10 to contain
    the trained model. In the Bucket details page, select Create Folder, as shown
    in figure 11.18.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F18_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 Creating a folder
  prefs: []
  type: TYPE_NORMAL
- en: 4.  Enter `processed_dataset` in the name field and click Create.
  prefs: []
  type: TYPE_NORMAL
- en: 5.  Select the new folder that you just created, as shown in figure 11.19.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F19_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.19 Selecting the folder
  prefs: []
  type: TYPE_NORMAL
- en: 6.  Click Upload Files and select the CSV file containing the processed version
    of the Kuala Lumpur dataset (output of the data preparation notebook).
  prefs: []
  type: TYPE_NORMAL
- en: 7.  You will see the file in the Bucket details page when the upload is complete.
    Click the three dots, then Copy gsutil URI, as shown in figure 11.20.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F20_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.20 Copying gsutil URI
  prefs: []
  type: TYPE_NORMAL
- en: 'The gsutil Uniform Resource Identifer (URI) value will look like this: `gs://first-project-ml-tabular-bucket/processed_dataset/kl_real_estate_output.csv`'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have uploaded the cleaned-up dataset to a Google Cloud Storage bucket,
    we can use it to create a Vertex AI dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.7 Creating a Vertex AI managed dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The ML pipeline invokes the Vertex AI SDK to train the model; it identifies
    the dataset used to train the model as a Vertex AI managed dataset. To learn more
    about Vertex AI managed datasets, see the documentation: [https://mng.bz/VVRP](https://mng.bz/VVRP).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Vertex AI SDK automatically does the following to make the managed dataset
    available to the training script:'
  prefs: []
  type: TYPE_NORMAL
- en: Copies the content of the dataset to Cloud Storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Divides the dataset into training, validation, and testing subsets. The proportion
    of the dataset for each subset is set in the pipeline config file `pipeline_config.yml`,
    as shown in figure 11.2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F21_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.21 Proportions for train, validation, and test in the pipeline configuration
  prefs: []
  type: TYPE_NORMAL
- en: Divides each of the subsets into multiple CSV files. Figure 11.22 shows an example
    of what the CSV files for the dataset look like in Cloud Storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F22_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.22 Processed dataset in Google Cloud Storage
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen how the dataset gets set up in Cloud Storage, let’s go
    through the steps to create a Vertex AI dataset for the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 1.  In Vertex AI, select Datasets. In the Datasets page, click Create as shown
    in figure 11.23.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F23_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.23 Creating a dataset
  prefs: []
  type: TYPE_NORMAL
- en: 2.  In the Create dataset page, set `kuala-lumpur-real-estate` as the dataset
    name, select the Tabular tab, select Regression/Classification, and click Create,
    as shown in figure 11.24.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F24_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.24 Specifying dataset details
  prefs: []
  type: TYPE_NORMAL
- en: 3.  In the Source tab, select Select CSV file from Cloud Storage. In Import
    file path, click Browse, select the Cloud Storage bucket location where you uploaded
    the processed training file in the previous section, and click Continue, as shown
    in figure 11.25.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F25_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.25 Specifying the source for the dataset
  prefs: []
  type: TYPE_NORMAL
- en: 4.  Note the ID value of the dataset that you just created, as shown in figure
    11.26.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F26_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.26 Dataset ID in Google Cloud Console
  prefs: []
  type: TYPE_NORMAL
- en: This is the value that needs to be set for `dataset_id` in the pipeline config
    file `pipeline_config.yml`, as shown in figure 11.27.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F27_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.27 `dataset_id` in the pipeline config file
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have set up a Vertex AI managed dataset for the dataset
    that the model training portion of the ML pipeline will use to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3 Defining the ML pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far in this chapter, we have completed the following preparation steps for
    the ML pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.  Created a service account and a service account key
  prefs: []
  type: TYPE_NORMAL
- en: 2.  Uploaded the service account key to the directory where we will run the
    pipeline script
  prefs: []
  type: TYPE_NORMAL
- en: 3.  Uploaded the cleaned-up dataset to Cloud Storage
  prefs: []
  type: TYPE_NORMAL
- en: 4.  Created a Vertex AI-managed dataset from the cleaned-up dataset
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will take the elements we prepared in the preceding section
    and use them to create an ML pipeline that takes in a preprocessed dataset at
    one end and produces a trained model deployed with a Vertex AI endpoint at the
    other end.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.1 Local implementation vs. ML pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we continue with defining the ML pipeline, let’s contrast the ML pipeline
    with the local setup to train the Kuala Lumpur real estate price prediction model
    that we implemented in chapter 10\. Figure 11.28 shows this contrast and highlights
    some of the differences between the two implementations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F28_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.28 Training on a local system vs. training with an ML pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.28 contrasts the structure of the training process for an entirely
    local implementation compared to the training process using an ML pipeline. The
    key ways that the ML pipeline implementation differs from the local system implementation
    of the solution are
  prefs: []
  type: TYPE_NORMAL
- en: The data cleanup process is identical. In a real-world production pipeline,
    we would move this data processing step into the Vertex AI environment and make
    it part of the ML pipeline, but to make the ML pipeline as simple as possible,
    we skip that step for our ML pipeline implementation and start the pipeline with
    the cleaned-up dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the local implementation, the data cleanup process output is a pickle file.
    To avoid compatibility problems, we switched to a CSV file for the ML pipeline.
    The ML pipeline takes the contents of this CSV file and splits them into train,
    validation, and test subsets, each of which is segmented into multiple CSV files
    in Cloud Storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training code in the ML pipeline implementation is in a Python `.py` file
    (the *model training script*) rather than a notebook. Significant updates to the
    training code to make it work in a container environment are described in the
    following section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the ML pipeline implementation, the model training config file is in Cloud
    Storage so that its location can be shared by the pipeline script as a parameter
    for the model training script.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pipeline script is a new component in the ML pipeline. This script sets
    up the input necessary for the model training script, uses the Vertex AI SDK to
    create a container for the model training script, and invokes the script to do
    the model training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pipeline config file is a new component in the ML pipeline. This config
    file contains parameters for the pipeline script, including the built-in Vertex
    AI containers to use for the ML pipeline; the proportion of the cleaned-up dataset
    for each of the training, validation, and testing subsets; the dataset ID; and
    the location of the code for the training script.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The trained model is automatically put in the model registry in the ML pipeline
    implementation and deployed to a Vertex AI endpoint. In the local system implementation,
    we manually uploaded the model to Cloud Storage and then deployed it to an endpoint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The endpoint that is the result of both the local system implementation and
    the ML pipeline implementation can be plugged into our web deployment simply by
    updating the `endpoint_id` parameter in the Flask server config file, as shown
    in figure 11.29.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F29_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.29 Web deployment with endpoint from local or ML pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details on the workflow for training a custom model on Vertex AI,
    see the documentation: [https://mng.bz/xKjW](https://mng.bz/xKjW).'
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.2 Introduction to containers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One key point of an ML pipeline in Vertex AI is using containers to make the
    model training process easy to automate and flexible. In this section, we will
    briefly introduce containers and their benefits to the ML pipeline. If you are
    already familiar with the concept of containers and Docker, you can skip this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'A container is a software construct that allows you to package an application
    with its dependencies so that you can run the application predictably and efficiently
    across a range of environments. Google Cloud uses Docker containers. A detailed
    description of containers is beyond the scope of this book, but we need to spend
    some time looking at them to understand why they are used for ML pipelines and
    what constraints they place on our code. For more details on containers, see the
    Docker site: [https://www.docker.com/resources/what-container/](https://www.docker.com/resources/what-container/).'
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.3 Benefits of using containers in an ML pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using containers to package the training code means that we don’t have to worry
    about the Python libraries that are required for the training because the container
    comes with all the required Python libraries already set up. Also, the code is
    easy to reproduce anywhere. Vertex AI provides a range of prebuilt container images
    for the most popular machine learning frameworks, including PyTorch, TensorFlow,
    and XGBoost. We use TensorFlow prebuilt containers for our ML pipeline. See the
    Vertex AI documentation for details on prebuilt containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prebuilt containers for training custom models*—[https://mng.bz/AQ8z](https://mng.bz/AQ8z)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prebuilt containers for prediction*—[https://mng.bz/ZlRP](https://mng.bz/ZlRP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If our training ends up becoming more demanding (either in terms of how quickly
    a training cycle needs to be completed or the compute resources needed to complete
    a training cycle of a given duration), we can take advantage of the containerized
    nature of the training to distribute training across multiple compute engines.
    For a simple problem like the Kuala Lumpur real estate price prediction problem,
    a single node is more than sufficient to do the training, but bigger applications
    can really benefit from distributed training. A detailed explanation of all the
    options that are available for distributed training with Vertex AI is beyond the
    scope of this book. Check out the documentation if you are interested in more
    details: [https://mng.bz/RVmK](https://mng.bz/RVmK).'
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.4 Introduction to adapting code to run in a container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have reviewed some of the benefits of using containers for the training
    process, we can look at changes that are required to run the training code in
    a container. To understand the difference between running code in a nonvirtualized
    environment and in a container, it helps to think of the container as its own
    self-contained machine where the code runs. In particular, code running in a container
    will not, by default, have access to the file system of the environment from which
    the container is managed. Figure 11.30 shows how the model training notebook interacts
    with files in the filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F30_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.30 Training code interactions with external files
  prefs: []
  type: TYPE_NORMAL
- en: When the training code runs in a container, it can’t get access to files on
    an external local filesystem. Instead, the artifacts that the model training script
    uses are stored in Cloud Storage, and the locations for these artifacts in Cloud
    Storage are passed to the model training script as URIs. Figure 11.31 gives an
    example of how to interpret a Google Cloud Storage URI.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F31_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.31 Interpreting a Google Cloud Storage URI
  prefs: []
  type: TYPE_NORMAL
- en: 'In the ML pipeline, we use two methods to pass URIs to the training script
    running in a container: via the environment variables set in the container by
    the Vertex AI SDK and via the argument list of the `job.run` call in the pipeline
    script, as shown in figure 11.32.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F32_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.32 Training code interactions with content in Cloud Storage
  prefs: []
  type: TYPE_NORMAL
- en: The location of the training data (split into training, validation, and test
    subsets) is automatically assigned to environment variables that get set in the
    container when it is set up by the pipeline script. This is standard for all Vertex
    AI containers; see the documentation at [https://mng.bz/2y70](https://mng.bz/2y70).
  prefs: []
  type: TYPE_NORMAL
- en: 'The way that the URI for the config file is passed to the model training script
    is not the default for Vertex AI. If we had a training script that had a small
    number of arguments, we could create an `argparser` list that contains the argument
    values and pass that list to the model training script. The config file for our
    application is too complex for this to be efficient, so instead of passing each
    argument individually, we pass a single argument: the URI for the Cloud Storage
    location where we have saved a copy of the config file. With that, all the model
    training script needs to do is get the Cloud Storage location from the argument
    list and ingest the YAML file from there. Once the arguments have been pulled
    into the config dictionary in the model training script, the rest of the code
    that uses them can work unchanged. This is a major benefit.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.5 Updating the training code to work in a container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will review how we changed the model training notebook ([https://mng.bz/1XJj](https://mng.bz/1XJj))
    that we ran in Colab in chapter 9 to get a model to predict Kuala Lumpur property
    prices. By making these changes, we convert the model training notebook into a
    training script that can run in a Vertex AI built-in container.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the key changes we made to the training notebook to create
    the training script:'
  prefs: []
  type: TYPE_NORMAL
- en: Removed extraneous library imports and associated code. For example, we don’t
    need to generate a diagram of the model when we run the training script, so we
    removed the code associated with `plot_model`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removed code that splits the dataset into training, validation, and testing
    subsets. In the ML pipeline, the Vertex AI SDK takes care of splitting the dataset
    prior to the testing script being started.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added code to interpret the `job.run` argument list, as shown in the following
    listing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 11.1 Loading the saved Keras model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines an argparser object for the arguments passed by the Vertex AI SDK
  prefs: []
  type: TYPE_NORMAL
- en: ② Adds the config_bucket argument to the argparser object
  prefs: []
  type: TYPE_NORMAL
- en: ③ Ingests the arguments passed by the Vertex AI SDK as a dictionary
  prefs: []
  type: TYPE_NORMAL
- en: ④ Gets the config file URI from the argument dictionary
  prefs: []
  type: TYPE_NORMAL
- en: Updated the code that ingests the training config file so that it ingests the
    contents of the config file from the Cloud Storage URI passed by the pipeline
    script (`config_bucket` from listing 11.1) rather than from the local file system.
    As shown in the following listing, the URI for the config file in Cloud Storage
    (`config_bucket`) is used to copy the config file from Cloud Storage to a file
    in the container, and then the contents of that file are copied into the dictionary
    `config`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 11.2 Ingesting the training config file via the URI argument
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ① Gets the bucket prefix of config_bucket
  prefs: []
  type: TYPE_NORMAL
- en: ② Gets the file path suffix of config_buckett
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines a storage.Client object
  prefs: []
  type: TYPE_NORMAL
- en: ④ Creates a storage object for the bucket
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Creates a storage object for the file
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Sets the name of the config file copy in the container
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Downloads the file from Cloud Storage to the container
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Reads the contents of the container version of the config file into a dictionary
  prefs: []
  type: TYPE_NORMAL
- en: Copied the values of the AIP environment variables that the Vertex AI SDK sets
    in the container. These environment variables contain URI patterns for the CSV
    files that the SDK creates in Google Storage that contain the train, validation,
    and test subsets of the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 11.3 Copying AIP environment variable values
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ① Gets the URI for the location to save the trained model
  prefs: []
  type: TYPE_NORMAL
- en: ② Gets the URI for the training dataset CSVs
  prefs: []
  type: TYPE_NORMAL
- en: ③ Gets the URI for the validation dataset CSVs
  prefs: []
  type: TYPE_NORMAL
- en: ④ Gets the URI for the testing dataset CSVs
  prefs: []
  type: TYPE_NORMAL
- en: Created dataframes for each of the patterns from the AIP environment variables.
    For each of these environment variables, we parsed the URI, got the list of matching
    files CSV blobs in Cloud Storage, and reassembled them into a single dataframe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 11.4 Creating a dataframe for subsets of the dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ① For each file pattern, gets the bucket prefix
  prefs: []
  type: TYPE_NORMAL
- en: ② Gets the CSV file pattern
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines a storage.Client object
  prefs: []
  type: TYPE_NORMAL
- en: ④ Gets the list of CSVs in the bucket that match the pattern
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Gets a list of the fully qualified URIs for the CSVs that match the pattern
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Creates a dataframe containing the contents of all the CSVs that match the
    pattern
  prefs: []
  type: TYPE_NORMAL
- en: 'Saved the trained model to a location specified by `OUTPUT_MODEL_DIR`, the
    URI set by the Vertex AI SDK as the location for saving the model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: With these changes, the rest of the training code works running in a container.
    Now that we have gone through the updates required to create the training script,
    in the next section we will go through the key parts of the pipeline script that
    sets up the container that the training script runs in.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.6 The pipeline script
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have gone through the training script, we can examine the code that
    makes up the pipeline script. You can see the complete pipeline script code at
    [https://mng.bz/PdRn](https://mng.bz/PdRn).
  prefs: []
  type: TYPE_NORMAL
- en: The key parts of the pipeline script are
  prefs: []
  type: TYPE_NORMAL
- en: 'Ingest the pipeline config file: [https://mng.bz/JYdV](https://mng.bz/JYdV).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Set the arguments for the training script:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Create a `CustomTrainingJob` object that specifies the location of the training
    script `script_path`, the prebuilt image to use for training `container_uri`,
    and any additional Python libraries that need to be installed in the training
    container requirements, as shown in the following listing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 11.5 Creating a `CustomTrainingJob` object
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ① Sets the prebuilt Vertex AI container image to run the training script
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines the list of any additional requirements to be installed in the container
  prefs: []
  type: TYPE_NORMAL
- en: ③ Sets the prebuilt Vertex AI container image to use for prediction
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the path for the managed dataset used for training (using the dataset
    ID for the managed dataset that you created in section 11.2.7) and create a `TabularDataset`
    object using that path:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Run the job defined previously, specifying the dataset created here; the proportion
    of the dataset to use for training, validation, and test; and the `machine_type`
    to use for the training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 11.6 Running the job
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ① Associates the job with the managed dataset
  prefs: []
  type: TYPE_NORMAL
- en: ② Sets proportions of the dataset to use for training, validation, and testing
  prefs: []
  type: TYPE_NORMAL
- en: ③ Sets the argument list (which contains the URI for the testing script config
    file)
  prefs: []
  type: TYPE_NORMAL
- en: Create an endpoint and deploy the model trained in the training script to that
    endpoint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 11.7 Deploying the trained model to an endpoint
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ① Sets characteristics of the endpoint
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates the endpoint
  prefs: []
  type: TYPE_NORMAL
- en: ③ Deploys the model to the endpoint
  prefs: []
  type: TYPE_NORMAL
- en: The following listing is the main function of the pipeline script that invokes
    the functions to run the pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 11.8 Main function of the pipeline script
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: ① Sets characteristics of the endpoint
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates the endpoint
  prefs: []
  type: TYPE_NORMAL
- en: ③ Deploys the model to the endpoint
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the pipeline script, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Clone [https://github.com/lmassaron/ml_on_tabular_data](https://github.com/lmassaron/ml_on_tabular_data)
    in a new directory in Cloud Shell and make `chapter_11` the current directory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the pipeline config file to ensure that `project_id` and `region` match
    the settings for your project, `dataset_id` matches the ID for your managed dataset,
    `staging_path` matches your staging path, and `config_bucket_path` matches the
    location in Cloud Storage, where you copied the training script config file, as
    shown in figure 11.33.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F33_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.33 Training code interactions with content in Cloud Storage
  prefs: []
  type: TYPE_NORMAL
- en: 'In the root directory where you cloned the repo, enter the following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note that running the entire pipeline script can take 10 minutes or more. If
    the script fails, you will get a message that includes a link to the log file
    containing diagnostic messages about the training run. If the script succeeds,
    the output will end with the pipeline completed and the time taken to run it.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.7 Testing the model trained in the pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once you have run the pipeline script to run the ML pipeline to train and deploy
    a model, you can use the resulting Vertex AI endpoint to exercise the model in
    the same web deployment framework that we used in chapter 10\. Note that testing
    the endpoint using this simple web deployment does not match what you would do
    in a production environment. However, using the same web deployment that we used
    in chapter 10 simplifies the testing process for this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: The steps to test the model trained in the pipeline are
  prefs: []
  type: TYPE_NORMAL
- en: 1.  In the Google Cloud Console, go to the Vertex AI Endpoints. Copy the ID
    for the deployment that was created by the ML pipeline, as shown in figure 11.34.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F34_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.34 Endpoint ID for the model generated by the pipeline
  prefs: []
  type: TYPE_NORMAL
- en: '2.  In the same local system where you tested the initial endpoint deployment
    with Flask in chapter 10, paste the endpoint ID that you just copied into the
    value of the `endpoint_id parameter` in the `flask_web_deploy_config.yml` config
    file and save the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '3.  On your local system, start the Flask server module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 4.  Once the Flask server module is running, go to `localhost:5000` in a browser.
    `home.html` will be rendered as shown in figure 11.35\. When you click on Get
    prediction, the model trained and deployed at a Vertex AI endpoint by the ML pipeline
    will be invoked (see figure 11.35).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F35_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.35 Home.html
  prefs: []
  type: TYPE_NORMAL
- en: Note that the TensorFlow level used in the prebuilt container that you used
    for the model training has to match the TensorFlow level in the environment where
    you run the web application to test the endpoint. For example, if we want to exercise
    the endpoint deployment in an environment that has TensorFlow 2.9, then in the
    pipeline config file, we need to specify a value for `train_image` (the prebuilt
    training container) that is consistent with that level of TensorFlow level, such
    as `us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-9:latest`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you get a protobuf error when you run the pipeline script in Cloud Shell,
    try running the following command to specify the protobuf level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: If you want to experiment with different training configurations, you can update
    the training config file, upload it to Cloud Storage (ensuring that the value
    of `config_bucket_path` in the pipeline config file matches the URI for the training
    config file), and rerun the pipeline script. You can use the web application to
    exercise the new model by updating the value of `endpoint_id` in the pipeline
    config file to match the endpoint ID of the new endpoint and repeating the steps
    in this section. By encapsulating multiple steps in the ML workflow in an ML pipeline,
    we make it easy to get repeatable results and experiment with new settings.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4 Using generative AI to help create the ML pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far in this chapter, we have seen how we use a combination of actions in
    Google Cloud and manual scripting to set up a basic ML pipeline to train and deploy
    a model trained on tabular data. In this section, we’ll explore how we can use
    the generative AI capabilities in Gemini for Google Cloud, introduced in chapter
    10, to simplify or automate some of these actions. As we saw in chapter 10, there
    are four ways that Gemini for Google Cloud can help us:'
  prefs: []
  type: TYPE_NORMAL
- en: Answer questions about Google Cloud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate code from text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpret code. That is, given a piece of code, generate text that explains
    what the code does. We can use this capability to help us understand the code
    that we are adapting from other places. We can use this capability to document
    the code that we are writing ourselves.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarize log entries to help debug problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 11.4.1 Using Gemini for Google Cloud to answer questions about the ML pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we saw in chapter 10, we can use the generative AI capabilities in Gemini
    for Google Cloud to get answers to questions about Google Cloud. The following
    are some examples of questions about creating an ML pipeline that Gemini for Google
    Cloud could help us to answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'What is an ML pipeline? While Gemini for Google Cloud is trained specifically
    for Google Cloud, it is able to answer broad questions about technology, such
    as this one. Note that the answer shown in figure 11.36 is generally applicable
    and not limited to just Google Cloud. The citations come from a variety of credible
    sources, including the documentation for TensorFlow and Scikit-learn:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is an ML pipeline? ([https://mng.bz/wJjP](https://mng.bz/wJjP))
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a data pipeline ([https://cs230.stanford.edu/blog/datapipeline/](https://cs230.stanford.edu/blog/datapipeline/))
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ML pipelines with Scikit-learn ([https://mng.bz/qxjr](https://mng.bz/qxjr))
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F36_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.36 Gemini for Google Cloud answers the question “what is an ML pipeline?”
  prefs: []
  type: TYPE_NORMAL
- en: What is a Vertex AI pipeline? When we take the same question and qualify it,
    as shown in figure 11.37, Gemini for Google Cloud gives us an answer that is specific
    to the ML pipeline implementation in Google Cloud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F37_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.37 Gemini for Google Cloud answers the question “what is a Vertex
    AI pipeline?”
  prefs: []
  type: TYPE_NORMAL
- en: What are Vertex AI prebuilt containers for training custom models? Finally,
    let’s try asking a question related to a specific task we tackled in this chapter.
    As you can see in figure 11.38, the answer provided by Gemini for Google Cloud
    describes both what prebuilt containers for training custom models are as well
    as the point of using them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F38_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.38 Gemini for Google Cloud answers the question “what are Vertex AI
    prebuilt containers for training custom models?”
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have seen how we can use Gemini for Google Cloud to answer
    questions, both general and specific, about building an ML pipeline. In the next
    section, we’ll look at how we use Gemini for Google Cloud to generate the code
    required for the ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.2 Using Gemini for Google Cloud to generate code for the ML pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have seen how Gemini for Google Cloud can answer questions about
    creating an ML pipeline, let’s explore how the generative AI capabilities in Gemini
    for Google Cloud can help us to create the code related to the ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gemini for Google Cloud is enabled in several IDEs supported by Google Cloud,
    including VS Code, Cloud Workstations, and Cloud Shell Editor. In this section,
    we will use Gemini for Google Cloud in the context of Cloud Shell Editor. If you
    need a refresher on Cloud Shell Editor, see the overview documentation: [https://mng.bz/7pvv](https://mng.bz/7pvv).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will see how Gemini for Google Cloud is able to generate the code for functions
    in the pipeline script: [https://mng.bz/PdRn](https://mng.bz/PdRn) Using the function
    signatures and introductory comments from this script, we will see what Gemini
    for Google Cloud generates.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, if you have not done so already, follow the documentation to
    enable Gemini Code Assist in Cloud Shell Editor: [https://mng.bz/mGja](https://mng.bz/mGja).'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have enabled Gemini Code Assist in Cloud Shell Editor, open a new Python
    file in Cloud Shell Editor and enter the signature and introductory comment for
    the `get_pipeline_config` function, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.9 Signature for `get_pipeline_config`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note that this code snippet does not include the logic of the function.
  prefs: []
  type: TYPE_NORMAL
- en: To get Gemini for Google Cloud to generate code to complete this function, simply
    press Enter. Gemini for Google Cloud generates provisional code in italics, as
    shown in the figure 11.39.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F39_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.39 First set of provisional code generated by Gemini for Google Cloud
  prefs: []
  type: TYPE_NORMAL
- en: Press Tab to accept this provisional code and then press Enter again to get
    the next set of code generated, as shown in figure 11.40.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F40_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.40 Second set of provisional code generated by Gemini for Google Cloud
  prefs: []
  type: TYPE_NORMAL
- en: Press Tab again to accept this second set of provisional code. The resulting
    function is shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.10 `get_pipeline_config` function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: ① First set of code generated by Gemini for Google Cloud
  prefs: []
  type: TYPE_NORMAL
- en: ② Second set of code generated by Gemini for Google Cloud
  prefs: []
  type: TYPE_NORMAL
- en: The code in listing 11.10 is not identical to the hand-written code for the
    `get_pipeline_config` function, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.11 get_`pipeline`_`config` function: handwritten version'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: ① Handwritten code includes exception handling for the file opening operation
  prefs: []
  type: TYPE_NORMAL
- en: ② Handwritten code includes 'r' parameter with the file open
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing the code generated by Gemini for Google Cloud in listing 11.10 with
    the hand-written code in listing 11.11, we can see two differences:'
  prefs: []
  type: TYPE_NORMAL
- en: The handwritten code includes exception handling to deal with problems opening
    the config file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The handwritten code includes the `'r'` parameter in the file open operation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `get_pipeline_config` function is trivial, but, nevertheless, Gemini for
    Google Cloud was able to generate working code for the function.
  prefs: []
  type: TYPE_NORMAL
- en: Some additional considerations for Gemini for Google Cloud code generations
    are
  prefs: []
  type: TYPE_NORMAL
- en: You don’t have to accept the provisional code generations from Gemini for Google
    Cloud all at once. To accept the provisional code token-by-token, press CTRL +
    Right Arrow to accept a single token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To reject the entire provisional code generation and start again, press ESC,
    and the entire set of provisional code will be erased.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you ask Gemini for Google Cloud to generate code multiple times with the
    exact same input, you aren’t guaranteed to get identical code generated. For instance,
    in the `get_pipeline_config` example, sometimes Gemini for Google Cloud generated
    the function in two steps, as shown in this section, and sometimes it generated
    the entire function, including the `return` statement, in a single step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have used generative AI to generate code, we’ll see how we can use
    it to explain code in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.3 Using Gemini for Google Cloud to explain code for the ML pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have seen an example of how Gemini for Google Cloud can generate
    code, let’s exercise its ability to interpret code.
  prefs: []
  type: TYPE_NORMAL
- en: To get Gemini for Google Cloud to interpret a code snippet, copy the code in
    the following listing (the `main` function from the pipeline script) into a new
    file in Cloud Shell Editor.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.12 `get_pipeline_config` function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Once you have pasted the code in listing 11.13 into a file, select it and then
    select the Gemini for Google Cloud Smart Actions icon from the Cloud Shell Editor
    toolbar (see figure 11.41).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F41_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.41 Gemini for Google Cloud Smart Actions icon
  prefs: []
  type: TYPE_NORMAL
- en: In the menu that appears, select Explain, as shown in figure 11.42.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F42_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.42 Gemini for Google Cloud Smart Actions icon
  prefs: []
  type: TYPE_NORMAL
- en: When you select this, the explanation for the code appears in the left pane
    of Cloud Shell Editor, as shown in figure 11.43.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F43_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.43 Code explanation
  prefs: []
  type: TYPE_NORMAL
- en: The code explanation capacity of Gemini for Google Cloud can be applied to a
    wide variety of code, including Python, Java, and JavaScript code. You can use
    the code explanations to understand code that you aren’t familiar with and to
    recommend documentation for your own code.
  prefs: []
  type: TYPE_NORMAL
- en: So far in this section, we have seen how we can use the generative AI capabilities
    in Gemini for Google Cloud to answer questions, generate code, and explain code.
    In the next subsection, we’ll see how we can use Gemini for Google Cloud to help
    to summarize log entries.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.4 Using Gemini for Google Cloud to summarize log entries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google Cloud includes a log that you can use to track the behavior of your environment
    and debug problems. Sometimes, however, the log entries can be hard to interpret.
    Gemini for Google Cloud can help you understand the point of a log entry by summarizing
    it. In this subsection, we’ll go through how you can use Gemini for Google Cloud
    to get the most out of Google Cloud logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To exercise this capability of Gemini for Google Cloud, we will try to use
    the foundation model tuning in Vertex AI. Foundation model tuning lets us take
    a pretrained model and tune it with a dataset in the JSONL (JSON Lines: [https://jsonlines.org/](https://jsonlines.org/))
    dataset. See the documentation for more details on tuning text models in Vertex
    AI: [https://mng.bz/5goO](https://mng.bz/5goO).'
  prefs: []
  type: TYPE_NORMAL
- en: To prepare for the example in this section, create a new folder called `staging`
    in the Cloud Storage bucket that you created in chapter 10.
  prefs: []
  type: TYPE_NORMAL
- en: In Vertex AI in the Google Cloud Console, select Vertex AI Studio -> Language.
    In the Language page, select Tune and Distill and then select Create Tuned Model,
    as shown in figure 11.44.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F44_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.44 Vertex AI Studio language page
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Tuning Method pane of the Create Tuned Model page:'
  prefs: []
  type: TYPE_NORMAL
- en: Specify a name for your model in the Tuned model name field.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify the URI for the staging folder that you created at the beginning of
    this section in the Output Directory field.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click Continue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See figure 11.45.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F45_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.45 Tuning method pane of the Create a tuned model page
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Tuning dataset pane of the Create Tuned Model page, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Select Existing File on Cloud Storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Enter the URI for this sample JSONL file `cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`
    in the Cloud storage file path field. See the documentation for details about
    JSONL samples: [https://mng.bz/6ene](https://mng.bz/6ene).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click Start Tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See figure 11.46.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F46_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.46 Tuning dataset pane of the Create a tuned model page
  prefs: []
  type: TYPE_NORMAL
- en: Once you click Start tuning, you will see a list of tuned models with the status
    of your model showing as Running, as shown in figure 11.47.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F47_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.47 Tuning job status
  prefs: []
  type: TYPE_NORMAL
- en: When the tuning job is complete, the status will change to Succeeded, as shown
    in figure 11.48.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F48_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.48 Tuning job status showing a completed tuning job
  prefs: []
  type: TYPE_NORMAL
- en: If the tuning job does not succeed, that’s fine. The goal of this particular
    exercise is to examine an error, so it’s okay if the operation fails for some
    reason.
  prefs: []
  type: TYPE_NORMAL
- en: Once the tuning job is complete, put “logs explorer” in the search field at
    the top of the Console to get to the Logs Explorer page. This page provides many
    options for inspecting the logs generated by Google Cloud. For now, we just want
    to look at one of the errors. To view the errors, select Error in the bottom left
    of the Logs Explorer page, as shown in figure 11.49.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F49_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.49 Tuning job status showing a completed tuning job
  prefs: []
  type: TYPE_NORMAL
- en: The Query Results pane at the bottom of the page shows the errors, as shown
    in figure 11.50.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F50_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.50 Query results pane showing errors
  prefs: []
  type: TYPE_NORMAL
- en: Select one of these error entries to expand it and click on Explain This Log
    Entry as shown in figure 11.51.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F51_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.51 Expanded error entry
  prefs: []
  type: TYPE_NORMAL
- en: On the right, Gemini for Google Cloud shows an explanation of the error, as
    shown in figure 11.52.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F52_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.52 Error explanation
  prefs: []
  type: TYPE_NORMAL
- en: The explanation provided by Gemini for Google Cloud summarizes the nested entries
    in the log and makes it easier to read and interpret. Note that the explanation
    that you will see will depend on the error that you select from the log.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.5 Tuning a foundation model in Vertex AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous subsection, we saw how to use the generative AI capabilities
    in Gemini for Google Cloud to interpret error logs. It’s worth taking a closer
    look at the action that we triggered to generate logs that we could examine with
    Gemini for Google Cloud. Here is a summary of what we did:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We started with one of the foundation models available in Vertex AI, `text-bison`.
    This model is designed for various natural language tasks like content creation
    and classification. See the documentation for more details on `text-bison`: [https://mng.bz/oKrZ](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We applied supervised tuning to adapt the `text-bison` foundation model to
    a particular use case—our case classifying medical transcripts. To learn more
    about supervised tuning of foundation models in Vertex AI, see the documentation:
    [https://mng.bz/nR15](https://mng.bz/nR15).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset that we used for tuning contained medical diagnosis transcripts
    paired with the classification for the transcript, as shown in the following listing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 11.13 Example record from tuning dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: ① Medical transcript
  prefs: []
  type: TYPE_NORMAL
- en: ② Classification
  prefs: []
  type: TYPE_NORMAL
- en: The URI for this dataset is `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the tuning process is complete, you can exercise the tuned model in Vertex
    AI Studio by selecting Language -> Tune and Distill and then selecting Test in
    the row for the model you tuned in the previous subsection, as shown in figure
    11.53.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F53_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.53 Selecting the tuned model in Vertex AI Studio
  prefs: []
  type: TYPE_NORMAL
- en: The prompt editor opens with your tuned model selected as the model, as shown
    in figure 11.54.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F54_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.54 Prompt editor with the tuned model selected
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise the tuned model by entering the following text in the Prompt field
    and clicking Submit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note the response, as shown in figure 11.55.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F55_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.55 Sleep medicine response
  prefs: []
  type: TYPE_NORMAL
- en: Now change the model back to the foundation model `text-bison@001`, as shown
    in figure 11.56, and click Submit again.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F56_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.56 Changing the model back to text-bison@001
  prefs: []
  type: TYPE_NORMAL
- en: What’s the difference between the response you get from the prompt with the
    tuned model and the untuned foundation model? With the tuned model, you get all
    the capability of the foundation model along with appropriate responses for the
    medical transcript classification use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you examine the dataset that we used to tune the foundation model (with
    the URI `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`)
    you will notice that it is, in fact, a tabular dataset with two columns: one containing
    medical transcription notes and the other containing a category for the notes,
    such as “cardiovascular / pulmonary,” “chiropractic,” or “pain management.” So
    far in this book, we have examined how generative AI can be applied to the workflow
    for machine learning on tabular data. The example we used in the log interpretation
    exercise demonstrates a different kind of relationship between tabular data and
    generative AI: tabular data being part of the workflow for generative AI. A detailed
    examination of this subject is beyond the scope of this book, but we think that
    the role of tabular data in generative AI workflows is an underresearched area
    that could yield significant benefits in getting the most out of generative AI.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have seen how we can use Gemini for Google Cloud’s generative
    AI capabilities to answer questions about ML pipelines, generate some of the code
    required to create one, explain the code that makes up an ML pipeline, and explain
    log messages.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Several set-up tasks need to be completed before setting up an ML pipeline in
    Vertex AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A service account needs to be created, and the service account key needs to
    be uploaded to the directory where you run the pipeline script.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset that will be used to train the model in the pipeline needs to be
    uploaded to a Google Cloud Storage bucket. This bucket location then needs to
    be used to define a Vertex AI dataset that will be used as an argument to the
    Vertex AI SDK in the pipeline script.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training script running in a Vertex AI prebuilt container does not have
    access to the file system outside of the container, so the training dataset and
    the training config file for the ML pipeline implementation are in Cloud Storage,
    and their locations are passed to the training script as URIs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training code from the training notebook that we ran in Colab in chapter
    9 needs to be adapted to run in a container. For example, the training script
    needs to be updated to use the Cloud Storage locations for the config file, the
    training data, and the location where the trained model should be saved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pipeline script invokes a series of functions from the Vertex AI SDK to
    create the container the training script runs in, to run the training script,
    and to deploy the trained model to a Vertex AI endpoint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use the same web application that you used to exercise the local deployment
    in chapter 10 to exercise the endpoint deployment generated by the ML pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use Gemini for Google Cloud (the generative AI toolkit incorporated
    in Google Cloud) at various steps of the ML pipeline creation process to answer
    questions, generate code from text, interpret code, and explain log messages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
