["```py\nfor step in range(num_steps):\n  batch = next(training_data)\n  state, loss = update_step(state, batch)\n\n```", "```py\nnum_debugging_steps = 10\nbatch = next(training_data)\n\nfor step in range(num_debugging_steps):\n  state, loss = update_step(state, batch)\n  print(f\"Loss at step {step}: {loss}\")\n\n```", "```py\n@jax.jit\ndef compute_gradients_l2_norm(grads):\n  \"\"\"Compute L2 norm of gradients.\"\"\"\n  grads_flat = jax.tree_util.tree_leaves(grads)  # Flatten.\n  return jnp.sqrt(sum([jnp.sum(jnp.square(g)) for g in grads_flat]))\n\n# Example usage inside a training step:\nloss, grads = jax.value_and_grad(loss_fn)(state.params)\ngrad_norm = compute_gradients_l2_norm(grads)\n\n```", "```py\ndef clip_gradients(grads, threshold):\n  \"\"\"Clip gradients.\"\"\"\n  return jax.tree_map(lambda g: jnp.clip(g, -threshold, threshold), grads)\n\n```", "```py\ntx = optax.chain(optax.clip(threshold), optax.adam(learning_rate))\n\n```"]