- en: 5 Storing our ACH files
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 存储我们的ACH文件
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Creating tables within our PostgreSQL database
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的PostgreSQL数据库中创建表
- en: Designing a relational database capable of storing ACH files
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计一个能够存储ACH文件的数据库
- en: Using Python and Pydantic to validate ACH records and store them in our database
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python和Pydantic验证ACH记录并将它们存储在我们的数据库中
- en: Ensuring that our records are parsed and stored correctly by implementing unit
    testing with pytest
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保通过实现使用pytest的单元测试来正确解析和存储我们的记录
- en: In this sprint, we use another research spike to explore how to define our database.
    Databases store and persist our data across application instances, while providing
    a way to query and ensure the integrity of that data. Here, we examine how to
    store our ACH files in a database. After initial analysis, we expand our APIs
    to store an ACH file in the database. Continuing along that track, we expand our
    ACH parser to store the individual fields as well. Finally, we wrap up the chapter
    by examining how storing ACH data affects our unit and load tests.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个冲刺中，我们使用另一个研究尖峰来探索如何定义我们的数据库。数据库在应用程序实例之间存储和持久化我们的数据，同时提供一种查询和确保数据完整性的方式。在这里，我们检查如何在数据库中存储我们的ACH文件。经过初步分析后，我们扩展了我们的API以在数据库中存储ACH文件。沿着这个方向继续，我们还扩展了我们的ACH解析器以存储单个字段。最后，我们通过检查存储ACH数据如何影响我们的单元和负载测试来结束本章。
- en: The introduction of a database is necessary in our project because an ACH file
    is a flat file. The current ACH system that Futuristic FinTech uses relies on
    flat files, and they can be challenging in many areas, including performance,
    querying, and data integrity. For instance, if a customer questions when a transaction
    was loaded, all the ACH files must be loaded and parsed to perform the search,
    which is time-consuming. Furthermore, keeping the parsed ACH files in memory becomes
    unfeasible given the number of records being dealt with.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的项目中引入数据库是必要的，因为ACH文件是一个平面文件。Futuristic FinTech目前使用的ACH系统依赖于平面文件，它们在许多领域都存在挑战，包括性能、查询和数据完整性。例如，如果客户质疑交易何时被加载，必须加载并解析所有ACH文件以执行搜索，这非常耗时。此外，考虑到处理的记录数量，将解析后的ACH文件保留在内存中变得不可行。
- en: 5.1 Designing our database
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 设计我们的数据库
- en: When a user uploads files to our ACH dashboard, we obviously need the ability
    to save them, or our system will not be very useful. The current ACH dashboard
    for Futuristic FinTech does not use a relational database. Instead, once uploaded,
    the files are parsed and stored in flat files (i.e., unstructured text files),
    which makes more sophisticated functionality a chore to implement. The ACH dashboard
    we are replacing uses only the filesystem to store the files. To provide more
    advanced processing, we want our ACH dashboard to be backed by a relational database,
    and we work through the initial review and implementation of various database
    designs and concepts to support our dashboard. Often, we need to have these types
    of research stories included in our sprints to examine different ways we may go
    about when implementing a desired feature.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户将文件上传到我们的ACH仪表板时，我们显然需要保存它们的能力，否则我们的系统将不会很有用。Futuristic FinTech当前的ACH仪表板不使用关系型数据库。相反，一旦上传，文件就会被解析并存储在平面文件中（即非结构化文本文件），这使得实现更复杂的功能变得繁琐。我们正在替换的ACH仪表板仅使用文件系统来存储文件。为了提供更高级的处理，我们希望我们的ACH仪表板由关系型数据库支持，并且我们通过初步审查和实施各种数据库设计和概念来支持我们的仪表板。通常，我们需要在我们的冲刺中包含这些类型的研究故事，以检查我们在实现所需功能时可能采取的不同方式。
- en: There are at least a dozen different relational databases, and FinTech uses
    many of them. Our choice of database is often already determined by the database
    our company uses. We have seen Oracle, MySQL/MariaDB, and PostgreSQL used by FinTech—to
    name just a few. In our case, we have already set up an environment that enables
    Postgre­SQL to run in a Docker container, and we have seen how to build/initialize
    tables during startup and view our data through CloudBeaver. We can now start
    expanding our database to accommodate storing ACH files.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 至少有十几种不同的关系型数据库，FinTech使用了许多种。我们选择的数据库通常已经由我们公司使用的数据库决定。我们见过FinTech使用Oracle、MySQL/MariaDB和PostgreSQL——仅举几个例子。在我们的案例中，我们已经设置了一个环境，使得PostgreSQL可以在Docker容器中运行，我们也看到了如何在启动时构建/初始化表以及如何通过CloudBeaver查看我们的数据。现在我们可以开始扩展我们的数据库以容纳存储ACH文件。
- en: Databases do much more than just storing our data—they can help ensure the reliability
    and consistency of the data and relationships, a concept known as referential
    integrity. Referential integrity in our database is a fancy way to say that we
    will ensure our tables are appropriately related and the fields are correctly
    defined. For instance, recall that ACH is a fixed-length format, meaning the individual
    fields are fixed length. We may store the file ID modifier from the file header
    record as a `VARCHAR(1)` since it can only be a single uppercase character (or
    0 through 9). Similarly, we may want to store the total debit entry amount in
    the file control (i.e., file trailer) record as `MONEY` or `NUMERIC(12,2)`. The
    `NUMERIC(12,2)` defines a field with a precision of 12 significant digits and
    a scale of 2, which is the number of decimal digits. Whether you’ll use `MONEY`
    or `NUMERIC` is up to you, but we favor the `NUMERIC(12,2)` representation as
    it closely resembles the field definition.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库不仅仅只是存储我们的数据——它们可以帮助确保数据的可靠性和一致性，这个概念被称为引用完整性。在我们的数据库中，引用完整性是一个复杂的说法，意味着我们将确保我们的表适当地相关联，字段定义正确。例如，回想一下ACH是一个固定长度格式，这意味着单个字段是固定长度的。我们可能将文件头记录中的文件ID修改者存储为`VARCHAR(1)`，因为它只能是一个单个大写字母（或0到9）。同样，我们可能希望将文件控制（即文件尾记录）中的总借方条目金额存储为`MONEY`或`NUMERIC(12,2)`。`NUMERIC(12,2)`定义了一个精度为12位有效数字和2位小数位的字段，这是小数位数。你将使用`MONEY`还是`NUMERIC`取决于你，但我们更喜欢`NUMERIC(12,2)`表示，因为它与字段定义非常相似。
- en: Another aspect of referential integrity is preventing orphan records. Remember
    that there is a hierarchy of records in an ACH file, that is, file control → batch
    header → entry records → etc. For example, an orphan record may occur if we have
    not defined our database carefully and we were to delete a batch header record.
    Once we delete a batch header, all the entry and addenda records (as well as the
    batch control record) belonging to that batch are no longer valid, and we should
    delete them. Likewise, deleting the file control record should delete all records
    associated with that file. In our relational database, we can achieve this by
    creating a `FOREIGN` `KEY`—which references the other table entries—and using
    `ON DELETE` `CASCADE`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个引用完整性的方面是防止孤立记录。记住，在ACH文件中存在记录的层次结构，即文件控制 → 批次头 → 条目记录 → 等。例如，如果我们没有仔细定义我们的数据库，删除一个批次头记录可能会导致孤立记录。一旦我们删除了批次头，属于该批次的全部条目和附加记录（以及批次控制记录）就不再有效，我们应该删除它们。同样，删除文件控制记录应该删除与该文件关联的所有记录。在我们的关系数据库中，我们可以通过创建一个`FOREIGN`
    `KEY`（它引用其他表条目）并使用`ON DELETE` `CASCADE`来实现这一点。
- en: 'Our initial database will use the inherent benefits of a relational database
    by defining the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最初的数据库将利用关系数据库的固有优势，通过定义以下内容：
- en: '*Primary key**s*—Unique identifiers for each record in a table'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主键**s*—表中每条记录的唯一标识符'
- en: '*Foreign key**s*—A link between two tables where a field (or fields) in one
    table refers to unique data (such as the primary key) in another table'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*外键**s*—两个表之间的链接，其中一个表中的字段（或字段）引用另一个表中的唯一数据（如主键）'
- en: '*Constraints on the field**s*—For example, `NOT` `NULL` (to ensure data is
    present), `UNIQUE` (to ensure data is unique across all rows), and `DEFAULT` (to
    assign a default value if none was provided)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*字段**s**的约束—例如，`NOT` `NULL`（确保数据存在），`UNIQUE`（确保数据在所有行中是唯一的），以及`DEFAULT`（如果没有提供，则分配默认值）'
- en: '*Data integrit**y*—Attained by defining appropriate data types and sizes of
    fields'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据完整性*—通过定义适当的数据类型和字段大小来获得'
- en: We’ll first look at storing ACH files with only Prearranged Payment and Deposit
    (PPD) data to keep things simpler. The PPD code is typically used for direct deposits
    and recurring payments such as payroll and pensions, so it is a widely used code
    that may frequently affect you (without your knowledge). To get an overview of
    what our database will look like, we again rely on PlantUML for a rendering of
    a proposed database structure.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将查看仅存储预安排支付和存款（PPD）数据的ACH文件，以使事情更简单。PPD代码通常用于直接存款和定期支付，如工资和养老金，因此它是一个广泛使用的代码，可能会经常影响你（而你却不知道）。为了了解我们的数据库将看起来像什么，我们再次依赖PlantUML来渲染一个建议的数据库结构。
- en: Listing 5.1  PlantUML definition of our database
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.1  我们数据库的PlantUML定义
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Begins a PlantUML definition'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 开始一个PlantUML定义'
- en: '#2 Defines a table for our diagram'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 定义我们的图中的表'
- en: '#3 Defines fields within the table for our diagram'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 定义表中的字段以供我们的图使用'
- en: '#4 Shows the relationships between keys in the table'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 显示表中键之间的关系'
- en: '#5 Ends the PlantUML definiton'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 结束 PlantUML 定义'
- en: The preceding definition is presented in figure 5.1, which shows how we may
    define our fields and the relationships between our tables. This is not an exhaustive
    list of the fields in tables, but it gives us an idea of how our tables will be
    related. The arrows represent the foreign key constraints present in the table.
    For instance, we can see how `ach_files_id` in the `ach_files` table field is
    defined as a Universally Unique Identifier (UUID) and references the `ach_files_id`
    from `ach_file_uploads`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的定义在图 5.1 中展示，它显示了我们可以如何定义我们的字段以及我们表格之间的关系。这不是表格中字段的详尽列表，但它给了我们一个关于我们的表格将如何关联的想法。箭头表示表中存在的外键约束。例如，我们可以看到
    `ach_files` 表中的 `ach_files_id` 字段被定义为通用唯一标识符（UUID），并引用了 `ach_file_uploads` 中的
    `ach_files_id`。
- en: '![A diagram of a computer  Description automatically generated](../Images/CH05_F01_Kardell.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![计算机图示 自动生成描述](../Images/CH05_F01_Kardell.png)'
- en: Figure 5.1  Diagram showing relationships between our tables
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.1 显示了我们表格之间关系的图
- en: 'Figure 5.1 also conveys our desire to accomplish the following goals:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 也传达了我们希望实现以下目标：
- en: Maintain the ordering of the records in our file
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护我们文件中记录的顺序
- en: Assume records will be unparsable and accommodate for that with unparsed records
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设记录将无法解析，并为此准备无法解析的记录
- en: Maintain referential integrity by having parsed records refer to the unparsed
    ones
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使解析记录引用未解析的记录来维护引用完整性
- en: While the database structure seemingly meets those goals and the diagram gives
    us a visual guide to get started, there is always room for improvement. Regardless
    of whether this structure was provided to us by a subject matter expert (SME)
    or the interpretation of a database analyst (DBA), there may be opportunities
    to refine our work as we move through the project. With our diagram in hand, we
    should have an idea of how we want the database to look and can start working
    on it. However, we need to follow the general pattern of defining a test, building
    the table/fields, and finally building the API. When working with an SQL database,
    it is important to understand that companies will certainly have different approaches
    to managing their data. Some companies may extract the SQL portion away from the
    developer, either by using an object-relational mapper (ORM) such as SQLAlchemy
    or by rolling their own. An ORM helps simplify code by abstracting the database
    and providing benefits such as database agnostic, optimizations, and improved
    productivity.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据库结构看似满足了这些目标，且图表为我们提供了开始工作的视觉指南，但总有改进的空间。无论这个结构是由主题专家（SME）提供还是由数据库分析师（DBA）的解释，我们在项目进行过程中都可能有机会改进我们的工作。手握我们的图表，我们应该有一个关于数据库外观的想法，并可以开始着手工作。然而，我们需要遵循定义测试、构建表/字段，最后构建
    API 的一般模式。当与 SQL 数据库一起工作时，重要的是要理解公司肯定会有不同的方法来管理他们的数据。一些公司可能会将 SQL 部分从开发者那里提取出来，无论是通过使用
    SQLAlchemy 这样的对象关系映射器（ORM）还是通过自己开发。ORM 通过抽象数据库并提供如数据库无关性、优化和提升生产力等好处来简化代码。
- en: Other companies may require you to write the SQL yourself because they like
    the level of control afforded by direct SQL. ORMs may make complex queries difficult
    or inefficient. Furthermore, troubleshooting queries and performance problems
    may also be more difficult to track down. First, we show here how to use straight
    SQL commands as we have been doing and then move to SQLAlchemy so that you can
    get acquainted with both approaches. There are always several factors that will
    be added, regardless of whether we use one approach over the other or combine
    them. Usually, the existing software will dictate our approach, so be careful
    not to stray too far from existing software standards initially as this could
    create a maintenance nightmare.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其他公司可能要求你自己编写 SQL，因为他们喜欢直接 SQL 提供的控制级别。ORM 可能会使复杂查询变得困难或低效。此外，调试查询和性能问题也可能更难追踪。首先，我们在这里展示如何使用我们一直在使用的直接
    SQL 命令，然后转向 SQLAlchemy，这样你就可以熟悉这两种方法。无论我们使用哪种方法，或者将它们结合起来，总有几个因素会被添加。通常，现有软件将决定我们的方法，所以请注意不要一开始就偏离现有软件标准太远，因为这可能会造成维护噩梦。
- en: 5.2 Using SQL directly
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 直接使用 SQL
- en: 'Chapter 3 explored parsing an ACH file and creating accompanying unit tests.
    We also created a simple API that accessed a database and returned results. So,
    we have all the pieces we need to store an ACH file in a database. Now we only
    have to put them together. In our first approach, we update our parser to store
    the ACH file in the database. We make the following assumptions:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章探讨了解析ACH文件并创建相应的单元测试。我们还创建了一个简单的API，该API访问数据库并返回结果。因此，我们拥有了将ACH文件存储在数据库中的所有必要组件。现在我们只需要将它们组合起来。在我们的第一个方法中，我们更新了解析器以将ACH文件存储在数据库中。我们做出以下假设：
- en: The database is always up and running.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库始终处于运行状态。
- en: We do not have any data that we want to preserve.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们没有想要保留的数据。
- en: We are running our code from an IDE and not inside Docker.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在从IDE中运行我们的代码，而不是在Docker内部。
- en: In other words, we are just working on the process of being able to parse and
    store the records in the database. We build out the previous `AchFileProcessor`
    to allow it to store ACH files. The next listing shows adding the necessary code
    to obtain a connection to the database. Since we are in a research sprint, we
    have hardcoded the database connection parameters such as username and password.
    Later, when we are certain that this is the right approach, we can start to abstract
    some of these hardcoded values so we have a more flexible configuration by using
    `environment` variables or secret management tools such as AWS Secrets Manager
    ([https://aws.amazon.com/secrets-manager/](https://aws.amazon.com/secrets-manager/))
    or HashiCorp Vault ([https://www.hashicorp.com/products/vault](https://www.hashicorp.com/products/vault)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们只是在处理能够解析和存储数据库中记录的过程。我们扩展了之前的`AchFileProcessor`以允许它存储ACH文件。下面的列表显示了添加必要的代码以获取数据库连接。由于我们处于研究冲刺阶段，我们硬编码了数据库连接参数，如用户名和密码。稍后，当我们确定这是正确的方法时，我们可以开始抽象一些这些硬编码的值，以便通过使用`环境变量`或秘密管理工具（如AWS
    Secrets Manager [https://aws.amazon.com/secrets-manager/](https://aws.amazon.com/secrets-manager/)）或HashiCorp
    Vault ([https://www.hashicorp.com/products/vault](https://www.hashicorp.com/products/vault)）来获得更灵活的配置。
- en: Listing 5.2  Adding database fields
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.2 添加数据库字段
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Temporarily hardcodes our username and password'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 暂时硬编码用户名和密码'
- en: '#2 A hardcoded host and port, but we should consider parametrizing those as
    well.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用硬编码的主机和端口，但我们也应该考虑将这些参数参数化。'
- en: '#3 A new function to return a connection to the database'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 一个新函数，用于返回数据库连接'
- en: That should provide us with the ability to connect to the database; however,
    we will need to use this code. Before we begin parsing the ACH file, we want to
    simply store the unparsed records in the database in what we may want to consider
    an extract
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该为我们提供连接到数据库的能力；然而，我们将需要使用此代码。在我们开始解析ACH文件之前，我们只想将未解析的记录存储在数据库中，这可能是一个我们可能想要考虑的提取
- en: –load–transform (ELT) approach rather than directly parsing the records through
    an extract–transform–load (ETL) approach.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: –加载-转换（ELT）方法而不是直接通过提取-转换-加载（ETL）方法解析记录。
- en: ELT vs. ETL
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ELT与ETL
- en: There are two approaches to handling data when processing it. They are usually
    discussed with regard to data warehousing and business intelligence, but ACH processing
    has unique challenges. ETL (extract–transform–load) is a traditional approach
    and may cross our mind first when processing data. For instance, we know that
    we want to parse each of these ACH records into their respective fields and store
    them in a database. However, dealing with data that may not be formatted correctly
    all the time is one of the challenges in ACH processing. With an ETL approach,
    this invalid data may cause processing to halt completely.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据时，有两种处理数据的方法。它们通常与数据仓库和商业智能一起讨论，但ACH处理具有独特的挑战。ETL（提取-转换-加载）是一种传统方法，在处理数据时可能会首先出现在我们的脑海中。例如，我们知道我们想要将每个ACH记录解析到它们各自的字段中，并将它们存储在数据库中。然而，处理可能不是始终正确格式化的数据是ACH处理中的一个挑战。使用ETL方法，这些无效数据可能会完全阻止处理。
- en: In an ELT (extract–load–transform) approach, the data is loaded and then transformed
    once it is going to be used. Typically, we see ELT when dealing with very large
    data and using data warehouses such as Snowflake or Redshift, where there is enough
    processing power to perform transformations on request.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在ELT（提取-加载-转换）方法中，数据被加载然后在使用前进行转换。通常，我们在处理非常大的数据和使用如Snowflake或Redshift这样的数据仓库时看到ELT，这些数据仓库有足够的处理能力来按请求执行转换。
- en: So, why do we care about these approaches? Often, financial institutions will
    allow some leeway with the data and will not always reject a file if an error
    is considered recoverable. These conditions can vary from one financial institution
    to another and with their customers. For example, a count of records on a control
    file may be updated if incorrect rather than just rejecting the file or batch.
    Or, invalid data in a field may just be updated to spaces before loading it. While
    we can rely on logging and exceptions to keep track of changes (and we should
    be logging any such changes), we still want to keep a record of the original file
    so that a banker has the opportunity to review the data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们为什么关心这些方法呢？通常，金融机构会对数据进行一些宽容，并且如果错误被认为是可恢复的，它们不会总是拒绝一个文件。这些条件可能因金融机构而异，以及它们的客户。例如，如果控制文件上的记录计数不正确，可能会更新而不是简单地拒绝文件或批次。或者，字段中的无效数据在加载之前可能只是更新为空格。虽然我们可以依赖日志和异常来跟踪更改（我们应该记录任何此类更改），但我们仍然想保留原始文件的记录，以便银行家有审查数据的机会。
- en: 5.2.1 Adding records to the ach_files table
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 将记录添加到 ach_files 表
- en: We will take the approach of loading our ACH file unaltered into the database
    before parsing out any records. Figure 5.2 illustrates a basic flow for this section.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在解析任何记录之前，将未修改的 ACH 文件加载到数据库中。图 5.2 阐述了本节的基本流程。
- en: '![](../Images/CH05_F02_Kardell.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F02_Kardell.png)'
- en: Figure 5.2  Flowchart for section 5.2
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.2 5.2 节的流程图
- en: Listing 5.3 shows an `ach_files` table. So, before parsing any records, let’s
    just add all our records to that database table.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.3 显示了一个 `ach_files` 表。因此，在解析任何记录之前，我们只需将所有记录添加到该数据库表中。
- en: Listing 5.3 Simple  `ach_files` table
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.3 简单的 `ach_files` 表
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Allows PostgreSQL to create UUIDs'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 允许 PostgreSQL 创建 UUIDs'
- en: '#2 Primary UUID key'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 主要 UUID 键'
- en: '#3 The unparsed ACH record stored as is'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 以原样存储未解析的 ACH 记录'
- en: '#4 A sequence number used to maintain ordering when retrieving data'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 用于在检索数据时保持排序顺序的序列号'
- en: With this code, running `docker-compose` `down`, `docker-compose` `build`, and
    `docker-compose` `up` should allow our table to be created, and now we just need
    to update the code to write out some records!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此代码，运行 `docker-compose` `down`，`docker-compose` `build` 和 `docker-compose`
    `up` 应该允许我们的表被创建，现在我们只需更新代码以写入一些记录！
- en: Before we write out any records, we must ensure that the basics are working.
    So, we simply add the following code at the beginning of the existing parse routine.
    This code merely gets a connection to the database, obtains a cursor that can
    be used to execute SQL statements, and then closes both the cursor and connection.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们写出任何记录之前，我们必须确保基本功能正常。因此，我们只需在现有的解析例程开头添加以下代码。此代码仅获取数据库连接，获取一个可以用来执行 SQL
    语句的光标，然后关闭光标和连接。
- en: Listing 5.4  Testing the connection
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.4  测试连接
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Calls our new get_db() function to connect to the database'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 调用我们的新函数 get_db() 连接到数据库'
- en: '#2 Creates a cursor used to execute SQL commands'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建一个用于执行 SQL 命令的光标'
- en: '#3 Closes the cursor'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 关闭光标'
- en: '#4 Closes the connection'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 关闭连接'
- en: If we were to run our code at this point, we may encounter several problems.
    First, our hardcoded `DATABASE_URL` `string` has a host of `postgres` and a port
    of `5432`. We are working within an IDE and not within Docker, so `postgres` is
    not the correct name to use. Indeed, we should see an error `psycopg.OperationalError:`
    `connection` `is bad:` `Unknown` `host` if we were to run the program. Instead,
    we want to use `localhost` because we are calling from the system hosting Docker.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在运行代码，我们可能会遇到几个问题。首先，我们硬编码的 `DATABASE_URL` `字符串` 有一个 `postgres` 主机和一个 `5432`
    端口。我们正在 IDE 中工作，而不是在 Docker 中，所以 `postgres` 不是应该使用的正确名称。实际上，如果我们运行程序，我们应该看到一个错误
    `psycopg.OperationalError:` `connection` `is bad:` `Unknown` `host`。相反，我们想使用 `localhost`，因为我们是从托管
    Docker 的系统调用。
- en: In addition, we need to expose the port for our container. Our docker-compose.yml
    should look like the one in listing 5.5\. Without exposing the port, we would
    see an error similar to `psycopg.OperationalError:` `connection` `failed:` `:1),`
    `port` `5432` `failed:` `could` `not` `receive` `data` `from` `server:` `Socket`
    `is` `not` `connected`, which would hopefully tip us off that there is a problem
    with our port. Remember that these types of problems will always be popping up
    in our development, and we just need to retrace our steps to see what we missed.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要公开我们容器的端口。我们的 docker-compose.yml 应该看起来像列表 5.5 中的那样。如果不公开端口，我们会看到一个类似于
    `psycopg.OperationalError:` `connection` `failed:` `:1),` `port` `5432` `failed:`
    `could` `not` `receive` `data` `from` `server:` `Socket` `is` `not` `connected`
    的错误，这可能会让我们意识到我们的端口有问题。记住，这些类型的问题总是会出现在我们的开发中，我们只需要回溯我们的步骤，看看我们错过了什么。
- en: Listing 5.5  Updated docker-compose.yml file
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.5  更新的 docker-compose.yml 文件
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Exposes the standard PostgreSQL port'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 揭示了标准的 PostgreSQL 端口'
- en: Once we have a basic connection working, we can start writing out the records.
    We could use a method similar to the one in listing 5.4 where we handle opening
    and closing the connections manually; however, having to remember to close the
    connections can be error-prone as we may forget to close them (we have seen files
    not being closed in a production environment for years only to spring up when
    moving to a new version of software or another vendor). We will use Python’s `with`
    statement as it automatically handles closing of various resources when a particular
    block of code is exited. In fact, we have already used it when reading our file,
    so we can simply expand on that.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有一个基本的连接工作，我们就可以开始编写记录。我们可以使用类似于列表 5.4 中的方法来手动处理打开和关闭连接；然而，记住要关闭连接可能会出错，因为我们可能会忘记关闭它们（我们已经在生产环境中看到文件多年未关闭，直到迁移到新版本的软件或另一个供应商时才出现）。我们将使用
    Python 的 `with` 语句，因为它在退出特定代码块时会自动处理各种资源的关闭。实际上，我们已经在使用它来读取我们的文件了，所以我们可以简单地在此基础上扩展。
- en: Listing 5.6  Updated parse function
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.6  更新的解析函数
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Creates a connection as part of the existing with statement'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 作为现有 with 语句的一部分创建连接'
- en: '#2 Initializes our sequence_number to zero'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将我们的 sequence_number 初始化为零'
- en: '#3 Increments the sequence_number for each record'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 为每条记录递增 sequence_number'
- en: '#4 Creates a cursor using the with statement, inserts, and commits the record'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 使用 with 语句创建游标，插入并提交记录'
- en: 'We can rerun our unit test `test_parsing_ach_file.py`, which will run our sample
    file through our code, and then check CloudBeaver to verify the records have been
    added. This is a good start: we can store the records in our database, and it
    should not be much of a stretch to use a similar approach to go into our individual
    parsing functions and store the data there.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重新运行我们的单元测试 `test_parsing_ach_file.py`，这将运行我们的样本文件通过我们的代码，然后检查 CloudBeaver
    以验证记录是否已添加。这是一个好的开始：我们可以将记录存储在我们的数据库中，并且使用类似的方法进入我们的单个解析函数并存储数据应该不会太困难。
- en: One thing that we need to do is update our unit test to pull the data from the
    database instead of relying on data that is being returned, because our goal is
    to store all the data in the database and not return anything other than a status.
    For now, let’s take a look at updating the `pytest` to get the record count from
    the database.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的一件事是更新我们的单元测试，从数据库中获取数据而不是依赖于返回的数据，因为我们的目标是存储所有数据在数据库中，而不是返回任何其他状态。现在，让我们看看如何更新
    `pytest` 来从数据库中获取记录数。
- en: Listing 5.7  Updated `pytest`
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.7  更新的 `pytest`
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 We pull the code from the AchFileProcessor that connects to the database.
    This is a temporary solution and we will eventually need to refactor the code
    again when more tests need to connect to the database.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们从连接到数据库的 AchFileProcessor 中提取代码。这是一个临时解决方案，当需要更多测试连接到数据库时，我们最终需要再次重构代码。'
- en: '#2 Simple query to fetch the count of records stored in ach_files'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 简单查询以获取存储在 ach_files 中的记录数'
- en: This test should work when Docker is first brought up, but subsequent tests
    will fail because the records are being added. So, our second iteration fails
    with
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个测试在 Docker 首次启动时应能正常工作，但随后的测试将失败，因为记录正在被添加。所以，我们的第二次迭代失败了
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Our the third iteration fails with
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第三次迭代失败了
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We need a way to clear the database before each test. Ideally, we want to have
    a database that only exists for the length of the test, but first, let’s see how
    we may clear the table after each test. We can use a `pytest.fixture` that will
    perform any setup/teardown for our individual tests.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在每次测试之前清除数据库。理想情况下，我们希望有一个仅在测试长度内存在的数据库，但首先，让我们看看我们如何在每次测试后清除表。我们可以使用一个
    `pytest.fixture`，它将为我们的单个测试执行任何设置/拆卸操作。
- en: Listing 5.8  `pytest.fixture` to setup and teardown our unit test
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.8  `pytest.fixture` 用于设置和拆卸我们的单元测试
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Anything preceding yield will execute before the test.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 在 yield 之前执行任何内容。'
- en: '#2 yield to allow the test to execute'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 yield 允许测试执行'
- en: '#3 Anything following yield will execute after the test.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 在 yield 之后执行任何内容。'
- en: '#4 Gets a connection and cursor, then trunctuates the table to clear it'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 获取连接和游标，然后截断表以清除它'
- en: '#5 Includes our fixture in the unit test'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 在单元测试中包含我们的 fixture'
- en: With this code, our test will pass repeatedly because the table is cleared after
    each run. We have worked through our problem of having the database retain records
    between runs. Of course, we need to be careful. If our unit tests point to a database
    that is not a development server, we run the risk of wiping out needed data. For
    this reason, we may want to look at other options such as an in-memory database,
    mocking, or `pytest-postgresql`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这段代码，我们的测试将反复通过，因为每次运行后都会清除表。我们已经解决了数据库在运行之间保留记录的问题。当然，我们需要小心。如果我们的单元测试指向的不是开发服务器上的数据库，我们就有可能清除所需的数据。因此，我们可能想要考虑其他选项，例如内存数据库、模拟或
    `pytest-postgresql`。
- en: Dealing with databases
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 处理数据库
- en: A common way to work with databases for unit testing is to use an in-memory
    database such as SQLite or H2\. This allows us to run a database that exists entirely
    in memory and therefore can be isolated to our unit testing. The benefits are
    usually quick execution and the isolation of data. The drawback is that many times
    there may be functionality that does not exist between our production database
    and these in-memory databases, which can lead to problems when trying to create
    a unit test. For instance, SQLite has five data types to define data, while PostgreSQL
    has over 40 types. This is not to say that one is inherently better than the other—we
    just highlight the challenges we may face if our tables use the unsupported data
    types. We may end up fighting unnecessary battles to get our unit tests to run.
    That is why we should have additional tools and techniques we can use.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在单元测试中与数据库交互的一种常见方式是使用内存数据库，例如 SQLite 或 H2。这使我们能够运行一个完全存在于内存中的数据库，因此可以将其隔离到我们的单元测试中。通常，这些好处包括快速执行和数据隔离。缺点是，很多时候，我们的生产数据库和这些内存数据库之间可能存在不存在的功能，这可能导致在尝试创建单元测试时出现问题。例如，SQLite
    有五种数据类型来定义数据，而 PostgreSQL 有超过 40 种类型。这并不是说一个本质上比另一个更好——我们只是强调，如果我们的表使用不受支持的数据类型，我们可能会面临哪些挑战。我们可能最终会陷入不必要的战斗，以使我们的单元测试运行。这就是为什么我们应该有额外的工具和技术可以使用。
- en: Mocking with a tool such as pgmock can also remove the need for a database in
    your test. In our scenario, we are actually testing whether the data has made
    it to the database, so mocking does not really provide a viable solution but something
    to look into later.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像 pgmock 这样的工具进行模拟也可以消除在测试中需要数据库的需求。在我们的场景中，我们实际上是在测试数据是否已到达数据库，所以模拟并不真正提供可行的解决方案，但可以留待以后考虑。
- en: '`Pytest-postgresql` is a package that helps us manage `postgresql` connections
    for a `pytest`, which offers the best of both worlds by allowing us to connect
    to a test database or create/clear/modify tables as part of the test.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pytest-postgresql` 是一个帮助我们管理 `pytest` 中 `postgresql` 连接的包，它通过允许我们连接到测试数据库或创建/清除/修改测试中的表，提供了两全其美的解决方案。'
- en: As our project progresses, we will find that managing our data for tests and
    keeping the tests isolated becomes increasingly harder. In chapter 10, we eventually
    start incorporating the Testcontainers package to isolate our tests. This approach
    will also be beneficial as the infrastructure for the project matures and we begin
    running our tests as part of a build pipeline.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的项目进展，我们会发现管理测试数据并保持测试隔离变得越来越困难。在第 10 章中，我们最终开始引入 Testcontainers 包来隔离我们的测试。当项目基础设施成熟，我们开始将测试作为构建管道的一部分运行时，这种方法也将是有益的。
- en: The ability to store data is necessary in most applications, but as we have
    seen, it also adds more complexity to our design and coding. In this section,
    we started small by ensuring that we could connect to the database and by storing
    our unparsed records, which helped minimize the number of code changes required.
    As we move forward, the complexity of our application will gradually increase.
    As we work through parsing and storing our ACH files, we should also keep in mind
    that we will be retrieving and aggregating data from the database for our ACH
    dashboard. A way to easily store our ACH files may lend itself to one database
    structure, while the ACH dashboard may be better served by an alternative structure.
    Our job is to strike an acceptable balance between those two goals.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数应用程序中，存储数据是必要的，但正如我们所看到的，它也增加了我们设计和编码的复杂性。在本节中，我们从小处着手，确保我们能够连接到数据库，并通过存储未解析的记录来最小化所需的代码更改。随着我们继续前进，我们应用程序的复杂性将逐渐增加。在我们处理解析和存储我们的ACH文件时，我们还应该记住，我们将从数据库中检索和汇总数据以用于我们的ACH仪表板。一种轻松存储我们的ACH文件的方法可能适合一种数据库结构，而ACH仪表板可能更适合另一种结构。我们的任务是在这两个目标之间找到一个可接受的平衡。
- en: 5.3 Storing the file header record
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 存储文件头记录
- en: The ACH file header record should be the first ACH record we encounter in an
    ACH file. So, it makes sense that this is the first record we should explore by
    adding it to the database. We first show how we may approach it using ChatGPT
    and then how we go through a complete example with GitHub Copilot installed in
    our IDE.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ACH文件头记录应该是我们在ACH文件中遇到的第一个ACH记录。因此，将这个记录作为第一个探索的记录添加到数据库中是有意义的。我们首先展示如何使用ChatGPT来处理这个问题，然后展示如何通过在我们的IDE中安装GitHub
    Copilot来完成一个完整的示例。
- en: 5.3.1 Using generative AI
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 使用生成式AI
- en: 'Generative AI can help with a lot of boilerplate code that can get repetitive
    after a while. Depending on your experience level, that boilerplate code may be
    new to you, and it may be beneficial to go through the process a few times. Once
    it becomes tedious, that may be a great sign we can start to lean on generative
    AI tools. For instance, we may prompt ChatGPT with the following general prompt:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI可以帮助处理很多样板代码，这些代码在一段时间后可能会变得重复。根据你的经验水平，这些样板代码可能对你来说是新的，并且多次进行这个过程可能是有益的。一旦变得乏味，这可能是一个很好的迹象，表明我们可以开始依赖生成式AI工具。例如，我们可以用以下通用提示来提示ChatGPT：
- en: '**![image](../Images/Prompt-Icon.png)** Please create a Postgres table to store
    a parsed Nacha File Header Record.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Prompt-Icon.png)** 请创建一个Postgres表来存储解析后的Nacha文件头记录。'
- en: We then received the `CREATE` `TABLE` statement from ChatGPT that does a great
    job incorporating `CHAR`, `VARCHAR`, and `NOT` `NULL`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从ChatGPT那里收到了一个`CREATE TABLE`语句，它很好地结合了`CHAR`、`VARCHAR`和`NOT NULL`。
- en: Listing 5.9  File header generated by ChatGPT 40
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.9  ChatGPT生成的文件头 40
- en: '[PRE10]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 The file_creation_time is an optional field, so it could be NULL.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 `file_creation_time`是一个可选字段，因此它可能是NULL。'
- en: '#2 These fields are also optional in the Nacha standard. Notice how ChatGPT
    used VARCHAR instead of CHAR since these fields may be padded with spaces.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 这些字段在Nacha标准中也是可选的。注意ChatGPT如何使用`VARCHAR`而不是`CHAR`，因为这些字段可能会用空格填充。'
- en: From personal experience, we prefer to use `VARCHAR` for most fields to avoid
    unnecessary padding. We have not encountered any meaningful performance effects
    from using `CHAR` versus `VARCHAR`. Storing ACH records may be one of the areas
    where using a `CHAR` could make sense since the fixed-length fields will not have
    any unnecessary space. `CHAR` can often be misused when declared too large, and
    any unused space is padded.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从个人经验来看，我们更喜欢在大多数字段中使用`VARCHAR`以避免不必要的填充。我们没有在使用`CHAR`与`VARCHAR`之间遇到任何有意义的性能影响。存储ACH记录可能是使用`CHAR`有意义的领域之一，因为固定长度字段不会有任何不必要的空间。`CHAR`在声明过大时往往会被误用，任何未使用的空间都会被填充。
- en: Out of curiosity, we asked ChatGPT if `CHAR` or `VARCHAR` was more performant
    in a `Postgres` database. After making a nice comparison between the two, it updated
    its example (without us asking it to) to use `VARCHAR` instead of `CHAR`! We were
    okay with this since our preference is to use `VARCHAR`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 出于好奇，我们询问了ChatGPT在Postgres数据库中`CHAR`或`VARCHAR`哪个性能更好。在对比了两者之后，它（未经我们要求）更新了示例，使用`VARCHAR`而不是`CHAR`！我们对此表示赞同，因为我们的偏好是使用`VARCHAR`。
- en: 5.3.2 Full example
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 完整示例
- en: ChatGPT can be of great help if we have a good idea of what we want to accomplish
    or if we don’t mind spending time configuring the prompt. Otherwise, we may want
    to work through the process aided by Copilot. Figure 5.3 shows the flow we will
    be using in this section. Here, we update the unit test at the end of our process
    because this is a relatively short development cycle. If we do not spend too much
    time coding before testing, we should be fine with working on the unit test after
    we do a little coding.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个明确的目标或者不介意花时间配置提示，ChatGPT可以非常有帮助。否则，我们可能想通过Copilot的帮助完成这个过程。图5.3显示了本节我们将使用的流程。在这里，我们更新过程末尾的单元测试，因为这是一个相对较短的开发周期。如果我们测试前不花太多时间编码，那么我们在稍微编码后进行单元测试应该没问题。
- en: '![](../Images/CH05_F03_Kardell.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F03_Kardell.png)'
- en: Figure 5.3  Flow and associated code listings for this section
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3  本节流程和相关的代码列表
- en: Using our knowledge of storing the records, we should be able to store a parsed
    record in the database, and once that is complete, the other record formats should
    fall into place. Recall that we returned the parsed record as a dictionary, as
    shown in the following listing.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 利用我们对存储记录的了解，我们应该能够将解析后的记录存储在数据库中，一旦完成，其他记录格式应该就会就位。回想一下，我们像以下列表所示那样将解析后的记录作为字典返回。
- en: Listing 5.10  A dictionary with a parsed file header record
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.10  包含解析文件头部记录的字典
- en: '[PRE11]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 We use hardcoded values for the offsets because the offsets will not be
    changing, and we want the ability to quickly refer to fields in question when
    problems arise.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们使用硬编码的值作为偏移量，因为偏移量不会改变，并且我们希望在出现问题时能够快速引用相关字段。'
- en: '#2 We strip off extra spaces from fields as necessary.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 当需要时，我们从字段中移除额外的空格。'
- en: Now we want to take that record and store it in a database instead of simply
    returning it. We could keep this method of parsing and call it from another routine
    that was also interested in storing the data in a database. Another approach we
    may choose is to create dedicated parser classes or utility methods to handle
    parsing. Any of these approaches would help keep the code clean and reusable.
    For the sake of simplicity, we are going to take this routine and simply store
    the data in a database.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想将那条记录存储在数据库中，而不仅仅是简单地返回它。我们可以保持这种解析方法，并从另一个也感兴趣将数据存储在数据库中的例程中调用它。我们可能选择的其他方法之一是创建专门的解析器类或实用方法来处理解析。任何这些方法都可以帮助保持代码的整洁和可重用性。为了简化，我们将采取这个例程，并将其数据简单地存储在数据库中。
- en: First, we want to create a table for storing ACH file headers in the database,
    and the next listing shows the sample table. At this point, we are going to keep
    it simple and only supply the fields we need to store the data without referencing
    any foreign keys or other basic constraints such as `field` `length` and `NOT`
    `NULL`. With Copilot installed, many of these field names were automatically populated,
    which saved us some time and effort.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们想在数据库中创建一个用于存储ACH文件头部的表格，接下来的列表显示了示例表格。在这个阶段，我们将保持简单，只提供存储数据所需的字段，而不引用任何外键或其他基本约束，如`字段`
    `长度` 和 `NOT` `NULL`。由于安装了Copilot，许多这些字段名称被自动填充，这为我们节省了一些时间和精力。
- en: Listing 5.11  Table to store ACH file headers
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.11  存储ACH文件头部的表格
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Once we have the table, we can move on to updating our code. We chose to pass
    the connection object (previously created in listing 5.4) into our routine. We
    could have also stored the connection object as part of our class, but passing
    it in as a parameter will make it easier to unit test our routine. Different situations
    may require alternative approaches, so by no means is this the only way to accomplish
    our task.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了表格，我们就可以继续更新我们的代码。我们选择将连接对象（如列表5.4中创建的那样）传递到我们的例程中。我们也可以将连接对象作为我们类的一部分存储，但通过作为参数传递将使单元测试我们的例程更容易。不同的情况可能需要不同的方法，所以这绝对不是完成我们任务的唯一方式。
- en: Listing 5.12  Passing the connection object
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.12  传递连接对象
- en: '[PRE13]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that we have the connection object, we can update the parsing routine to
    store the data.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了连接对象，我们可以更新解析例程以存储数据。
- en: Listing 5.13  Updating the `_parse_file_header` routine
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.13  更新 `_parse_file_header` 例程
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 We add a connection parameter.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们添加了一个连接参数。'
- en: '#2 The return statement becomes a variable named file_header.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 返回语句变成了名为 file_header 的变量。'
- en: '#3 We can execute directly on the connection to insert records.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 我们可以直接在连接上执行以插入记录。'
- en: '#4 We used named variables as placeholders.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 我们使用命名变量作为占位符。'
- en: '#5 The file_header variable passes our values in.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 文件头变量传递了我们的值。'
- en: We should now be able to insert our file header records into the database. Next,
    we can go back and update the unit test.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在应该能够将文件头记录插入数据库。接下来，我们可以回过头来更新单元测试。
- en: Listing 5.14  Updated unit test example
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.14 更新的单元测试示例
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 Creates another get_db function that takes a row_factory parameter'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 创建另一个带有row_factory参数的get_db函数'
- en: '#2 Uses the new parameter in the connect method'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在connect方法中使用新参数'
- en: '#3 Expected result is the same as before.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 预期结果与之前相同。'
- en: '#4 Leaves the current parse result in place'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 保持当前的解析结果不变'
- en: '#5 Gets another connection specifying the result should be returned as a dictionary'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 获取另一个连接，指定结果应以字典形式返回'
- en: '#6 Returns the result; actual_result will be a dictionary.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 返回结果；actual_result将是一个字典。'
- en: '#7 Removes ach_file_headers_id from the returned result'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 从返回结果中删除ach_file_headers_id'
- en: '#8 Compares the two results'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 比较两个结果'
- en: Here, we duplicated some of our code, such as the `get_db` method with a new
    parameter. It is important that as we move through the code, we keep an eye on
    this type of duplication and consider pulling out these methods into utility or
    helper classes whenever possible. The IDEs by JetBrains (and others) will often
    point to duplicated code and provide automated options to extract code into functions.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们重复了一些代码，例如带有新参数的`get_db`方法。随着我们通过代码，我们必须关注这种重复，并在可能的情况下考虑将这些方法提取到实用程序或辅助类中。JetBrains的IDE（以及其他）通常会指出重复的代码，并提供将代码提取到函数中的自动化选项。
- en: We also left the original result comparison in place because the method is still
    returning the records. As we continue to improve the project with additional functionality,
    we will likely remove this in favor of other return information (such as whether
    the record was parsed). We should now understand how the parsed ACH records can
    be stored in the database. For the ACH dashboard, to provide a meaningful interface
    with aggregated ACH data, we need to be able to store all the parsed ACH information
    in the database. In the next sections, we will explore how the code evolves as
    we work with it, not only expanding the features, but also addressing some nonfunctional
    requirements such as maintainability and testability.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也保留了原始的结果比较，因为该方法仍然在返回记录。随着我们继续通过添加更多功能来改进项目，我们可能会移除它，转而使用其他返回信息（例如，记录是否被解析）。我们现在应该了解解析后的ACH记录如何存储在数据库中。对于ACH仪表板，为了提供一个具有聚合ACH数据的有意义界面，我们需要能够在数据库中存储所有解析的ACH信息。在接下来的章节中，我们将探讨代码如何随着我们的工作而演变，不仅扩展功能，还解决一些非功能性要求，如可维护性和可测试性。
- en: 5.4 Storing the rest of our ACH records
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 存储剩余的ACH记录
- en: 'We have now created a database and stored data in two separate tables, so we
    should have enough of a framework to store the data for the remaining records
    in the database. The process is the same for all the remaining tables:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经创建了一个数据库并在两个单独的表中存储了数据，因此我们应该有足够的框架来在数据库中存储剩余记录的数据。对于所有剩余的表，过程都是相同的：
- en: Create the table and restart Docker.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建表并重启Docker。
- en: Update the parse routine.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新解析例程。
- en: Update the unit test and verify your results.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新单元测试并验证你的结果。
- en: If embracing the test-driven development approach, we can swap the updating
    of the parse routine and the unit test. Either way, we should be working in short
    feedback cycles, allowing any errors to shake out early in the process and not
    after we have implemented a dozen tables. We also find opportunities for cleaning
    up the code and making it better overall.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果采用测试驱动开发方法，我们可以交换更新解析例程和单元测试。无论如何，我们都应该工作在短反馈周期中，让任何错误在过程早期而不是在实现了一打表之后被发现。我们还发现了一些清理代码并使其更好的机会。
- en: Given the presumed structure of our database, this might be the perfect place
    to add a few tables. Check whether the database structure makes sense—is there
    anything that should be changed?
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们数据库的预期结构，这可能是一个添加几个表的好地方。检查数据库结构是否合理——是否有什么需要改变的地方？
- en: The following section will go over some of the lessons learned and insights
    gained from working through the rest of the tables that had to be added.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分将介绍从处理必须添加的其他表中学习的一些经验和见解。
- en: '5.4.1 Storing ACH files challenge: Lessons learned'
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 存储ACH文件挑战：经验教训
- en: While adding the additional tables, we should have made some observations about
    the code and taken opportunities to make improvements to our code. Were there
    any that stood out? It is important to note and resolve possible pain points such
    as duplicated code, inefficient processing, confusing logic, and so forth. Sometimes,
    we can really get a sense of accomplishment by cleaning up code to be more straightforward
    and maintainable. We will go through some of the problems we encountered when
    adding additional database tables.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加额外的表格时，我们应该对代码进行一些观察，并抓住机会改进我们的代码。有没有哪些特别突出的？重要的是要注意并解决可能的痛点，例如重复的代码、低效的处理、混乱的逻辑等等。有时候，通过清理代码使其更加直接和易于维护，我们真的能感受到成就感。我们将回顾我们在添加额外数据库表格时遇到的一些问题。
- en: With our use of `psycopg3` (which is confusingly defined as `psycopg`) instead
    of `psycopg2`, we found that our generative AI tools tended not to take advantage
    of some newer enhancements. For instance, GitHub Copilot insisted on declaring
    `cursor` methods at first but seemed to learn our preference for using the connection,
    and after a while, it stopped offering them. This makes sense as Copilot is supposed
    to learn and adapt to our style and coding preferences. In recent releases of
    these tools, we have also seen the inclusion of retrieval-augmented generation
    (RAG), which helps large language models (LLMs) such as ChatGPT stay current with
    up-to-date information. Of course, we will have to see how they perform over the
    long term since a lot of the training data would not have used the newer features.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是 `psycopg3`（它被混淆地定义为 `psycopg`）而不是 `psycopg2`，我们发现我们的生成式 AI 工具往往没有充分利用一些新的增强功能。例如，GitHub
    Copilot 最初坚持声明 `cursor` 方法，但似乎学会了我们的偏好，即使用连接，过了一段时间后，它就不再提供这些方法了。这是有道理的，因为 Copilot
    应该学习和适应我们的风格和编码偏好。在这些工具的最新版本中，我们也看到了检索增强生成（RAG）的加入，这有助于大型语言模型（LLMs）如 ChatGPT 保持与最新信息的同步。当然，我们还将长期观察它们的性能，因为大量的训练数据并没有使用这些新功能。
- en: When creating tables, GitHub Copilot did a good job naming the columns, and
    they matched up with the documented field names in most instances, with only a
    few minor exceptions. This was helpful since we had already created the field
    names in our Python code to match the NACHA standards, and because we were able
    to use named parameters in our SQL queries, it was a breeze to transition from
    returning records to writing out to the database. Another feature that we were
    continually impressed with (although it did not always work) was the ability of
    Copilot to assist in writing our unit tests. We had created unit tests for each
    record that needed to be parsed, and when we broke it down into a dictionary for
    comparison purposes, Copilot lent a helpful hand and populated the individual
    fields with our test data. Even though it was off by a character or two, in some
    cases, it was definitely helpful overall.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建表格时，GitHub Copilot 在命名列方面做得很好，大多数情况下它们与文档中的字段名称相匹配，只有少数小的例外。这很有帮助，因为我们已经在我们的
    Python 代码中创建了字段名称以匹配 NACHA 标准，并且因为我们能够在 SQL 查询中使用命名参数，所以从返回记录到写入数据库的过渡变得非常容易。另一个我们一直很欣赏的功能（尽管它并不总是起作用）是
    Copilot 在编写单元测试方面的协助能力。我们已经为需要解析的每个记录创建了单元测试，当我们将其分解为字典以进行比较时，Copilot 提供了帮助，并用我们的测试数据填充了各个字段。尽管在某些情况下它可能错了一个或两个字符，但总体上它确实很有帮助。
- en: As we went through and built our unit tests, we did eventually create a `SqlUtils`
    class to handle a few of the repetitive tasks. We started by moving the database
    parameters such as the user, password, and the URL to the routine. Then later,
    we expanded this to handle taking a parameter of a row factory so that we could
    return a dictionary of the values. We also created a routine to truncate the table
    so that repetitive tests had clean tables to work with. Therefore, our assertions
    would not fail when we expected a single record but retrieved multiple.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建单元测试的过程中，我们最终创建了一个 `SqlUtils` 类来处理一些重复的任务。我们首先将数据库参数，如用户、密码和 URL 移动到例程中。然后后来，我们扩展了这个例程以处理行工厂参数的传递，这样我们就可以返回一个值的字典。我们还创建了一个例程来截断表，以便重复测试有干净的表可以工作。因此，当我们期望检索单个记录但检索到多个记录时，我们的断言不会失败。
- en: Similar to removing duplication of code with the `SqlUtils` class, we also removed
    hard coding of the table names throughout the unit tests by creating a variable
    to hold the table name and using Python f-strings to create the SQL commands where
    necessary (but not for parameter passing). It is important to note that we still
    used parameterized queries whenever possible, even though we consider this internal
    tooling and would not expect malicious code to be entered. We did, however, consider
    the possibility of checking the passed table names against the database information
    schema to ensure they were valid. However, that seemed a bit overkill for internal
    tooling.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用 `SqlUtils` 类去除代码重复类似，我们也通过创建一个变量来保存表名，并在必要时使用 Python f-strings 创建 SQL 命令（但不是用于参数传递），从而在整个单元测试中移除了对表名的硬编码。需要注意的是，尽管我们将其视为内部工具，并且不期望输入恶意代码，但我们仍然尽可能使用参数化查询。然而，我们也考虑了检查传递的表名是否与数据库信息模式中的信息匹配的可能性，以确保它们是有效的。然而，对于内部工具来说，这似乎有点过度了。
- en: We were bitten several times by neglecting to put the setup/teardown `pytest.fixture`
    into the unit test methods as we were coding. This often led to errors when repeatedly
    running our tests since the database was not always in a clean state. It happened
    often enough that we considered creating a class hierarchy that would incorporate
    the clearing of the table so that we would save ourselves from ourselves. However,
    it felt too early in our process to add that, so we stayed away from it for the
    moment.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在编码时多次因为忘记将设置/拆卸 `pytest.fixture` 放入单元测试方法中而受到咬伤。这通常会在我们反复运行测试时导致错误，因为数据库并不总是处于干净的状态。这种情况发生得足够频繁，以至于我们考虑创建一个类层次结构，以便包含清理表的操作，这样我们就可以避免自己给自己找麻烦。然而，在我们目前的流程中，添加这个功能似乎还为时过早，所以我们暂时回避了这个问题。
- en: 5.5 Storing exceptions
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 存储异常
- en: We should now have the basics of working with Python and PostgreSQL. Having
    successfully stored our sample.ach file in the database should boost our confidence.
    However, we should have also noted that our exceptions are not being stored in
    the database yet. We will want to keep track of those as well. It is common for
    files to be rejected for various reasons, and ACH processors need to be able to
    determine whether the file can be fixed manually or a new one should be requested
    from the originating party. Figure 5.4 shows the flow for this section.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该已经掌握了使用 Python 和 PostgreSQL 的工作基础。成功将我们的 sample.ach 文件存储到数据库中应该会增强我们的信心。然而，我们也应该注意到我们的异常还没有被存储在数据库中。我们希望也能跟踪这些异常。文件因各种原因被拒绝是很常见的，ACH
    处理器需要能够确定文件是否可以手动修复，或者是否需要从原始方请求一个新的文件。图 5.4 展示了这一部分的流程。
- en: '![](../Images/CH05_F04_Kardell.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F04_Kardell.png)'
- en: Figure 5.4  Flow and associated code listings for this section
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.4  本节流程和相关的代码列表
- en: The first order of business is to create the table. We start with a straightforward
    record that only contains the error description. As our project expands, we may
    find that we need to break exceptions out by record types, add references to specific
    records (to help maintain the integrity of the database should records be removed
    or updated), or implement several other improvements that will become more obvious
    as we enhance the project. However, those concerns are beyond the current scope
    of what we need to accomplish, so we will cross that bridge when we come to it
    in chapter 8.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项任务是创建表。我们从一个简单的记录开始，该记录只包含错误描述。随着我们项目的扩展，我们可能会发现我们需要按记录类型分解异常，添加对特定记录的引用（以帮助维护数据库的完整性，以防记录被删除或更新），或者实施其他一些随着项目增强而变得更加明显的改进。然而，这些关注点超出了我们当前需要完成的工作范围，所以我们将这个问题留到第8章再解决。
- en: Listing 5.15  Simple exceptions table
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.15  简单的异常表
- en: '[PRE16]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: With this table created, we can restart Docker and add a method to our AchFile­Processor.py
    to insert the record.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了这个表之后，我们可以重新启动 Docker，并在 AchFile­Processor.py 中添加一个方法来插入记录。
- en: Listing 5.16  Simple method to write the table
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.16  写入表的简单方法
- en: '[PRE17]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 Simple method passing a connection and an exception string'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 简单方法传递一个连接和异常字符串'
- en: '#2 Inserts the string into the table'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将字符串插入到表中'
- en: With this task completed, we can replace the array we used to hold our exceptions
    with a single call. We also need to update the various test cases that covered
    these exceptions, such as an incorrect addenda indicator, invalid record lengths,
    and ensuring the trace numbers are ascending. Updating these unit tests provides
    additional opportunities for creating maintainable code.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这项任务后，我们可以用一个调用替换我们用来保存异常的数组。我们还需要更新覆盖这些异常的各种测试用例，例如错误的附加指示符、无效的记录长度以及确保跟踪号是递增的。更新这些单元测试提供了创建可维护代码的额外机会。
- en: We removed the passing of our exceptions back to the calling routine, so code
    such as `records,` `exceptions` `=` `parser.parse(file_path)` becomes simply `records`
    `=` `parser.parse(file_path)`. However, this change made us immediately retrieve
    the exceptions because our unit tests were validating the number of exceptions
    and exception message text. We chose to add another method to `SqlUtils` to handle
    this.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们移除了将异常返回给调用例程的操作，因此像`records,` `exceptions` `=` `parser.parse(file_path)`这样的代码现在变为`records`
    `=` `parser.parse(file_path)`。然而，这个变化使我们立即检索异常，因为我们的单元测试正在验证异常的数量和异常消息文本。我们选择向`SqlUtils`添加另一个方法来处理这个问题。
- en: Listing 5.17  Method to retrieve ACH exceptions
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.17  获取ACH异常的方法
- en: '[PRE18]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '#1 Used with keyword to get a database connection'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 与关键字一起使用以获取数据库连接'
- en: '#2 Grabs all exceptions in a single execute command'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在单个执行命令中获取所有异常'
- en: '#3 Returns a list of exceptions instead of tuples'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 返回异常列表而不是元组'
- en: With the helper function in place, we can now return the exceptions with `exceptions
    =` `SqlUtils.get_exceptions()`, and the existing logic for the unit test should
    work without any modifications.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在放置了辅助函数之后，我们现在可以用`exceptions =` `SqlUtils.get_exceptions()`返回异常，并且现有的单元测试逻辑应该无需任何修改即可工作。
- en: As a result of storing our exceptions in a table, we now have multiple tables
    that need to be truncated in our unit tests. We could continue to call the `SqlUtils.truncate()`
    method with the new table. At this point, each test has a maximum of only two
    tables. However, we would prefer a way to clear all our tables because that will
    ensure our database is empty for each test. Listing 5.17 showed a truncate-all
    method that clears our tables. Obviously, this approach should be used with care
    as we are now truncating all the tables in the database. We could also have dropped
    and recreated the database; however, this gives us more control since each table
    is accessed individually. We have worked with projects that have expanded on this
    type of approach to examine tables for unexpected data, such as determining whether
    data has unexpectedly been written out to other tables. Of course, our needs will
    vary from one project to another, and it may be that you do not need or want to
    truncate the data using this method.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将异常存储在表中，现在我们的单元测试中需要截断多个表。我们可以继续使用新的表调用`SqlUtils.truncate()`方法。在这个阶段，每个测试最多只有两个表。然而，我们更希望有一种方法来清除所有表，因为这将确保每个测试的数据库都是空的。列表5.17展示了截断所有表的方法。显然，这种方法应该谨慎使用，因为我们现在正在截断数据库中的所有表。我们也可以删除并重新创建数据库；然而，这样做我们可以对每个表进行单独访问，从而获得更多的控制。我们曾与采用这种类型方法的项目合作，以检查表中是否存在意外数据，例如确定数据是否意外写入其他表。当然，我们的需求会因项目而异，可能你不需要或希望使用这种方法截断数据。
- en: Listing 5.18  SQL to truncate all tables
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.18  截断所有表的SQL
- en: '[PRE19]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#1 Creates an anonymous block of code'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 创建一个匿名代码块'
- en: '#2 Declares a variable of type RECORD, a placeholder for a row that has no
    predefined structure'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 声明一个类型为RECORD的变量，作为没有预定义结构的行的占位符'
- en: '#3 Signifies the start of a new transaction'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 表示新事务的开始'
- en: '#4 Disables any triggers on the table'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 禁用表上的任何触发器'
- en: '#5 Gathers a list of tables and truncates them'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 收集表列表并截断它们'
- en: '#6 Enables the triggers again'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 重新启用触发器'
- en: '#7 Commits the current transaction'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 提交当前事务'
- en: At this point, we should have arrived at a similar point to where we were with
    our running code from chapter 3\. Recall we are able to perform some basic parsing
    on some simple ACH files, with the major change being that we are now storing
    the results of our hard work into a database. We can take a moment to congratulate
    ourselves, but only for a moment, because while we have the ability to parse the
    file, we do not have a way for a user to load a file. The next sections look at
    expanding our API to upload an ACH file and see the cascading effect it has on
    our database.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们应该已经到达了与第3章中我们运行代码相似的地方。回想一下，我们能够对一些简单的ACH文件进行一些基本的解析，主要的变化是我们现在将我们辛勤工作的结果存储到数据库中。我们可以花点时间自我祝贺，但只能短暂地，因为虽然我们能够解析文件，但我们没有让用户加载文件的方法。接下来的几节将探讨扩展我们的API以上传ACH文件，并查看它对我们数据库产生的级联效应。
- en: 5.6 Uploading an ACH file
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 上传ACH文件
- en: We may consider this our most important change yet. Of course, being able to
    parse a file is necessary, but it is often allowing interaction with the user
    that feels the most rewarding for many developers. This is possibly true because
    being able to work and interact with our project just feels like an accomplishment.
    And we feel that unit testing gives us much the same rewarding experience, which
    is why we like testing so much! Figure 5.5 shows the flow for this section.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能认为这是我们迄今为止最重要的改变。当然，能够解析文件是必要的，但对于许多开发者来说，最令人感到有成就感的是能够与用户进行交互。这可能是因为能够与我们的项目一起工作和交互本身就感觉像是一种成就。我们还认为单元测试给我们带来了几乎相同的成就感，这就是为什么我们如此喜欢测试！图5.5显示了本节的流程。
- en: '![](../Images/CH05_F05_Kardell.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F05_Kardell.png)'
- en: Figure 5.5  Flow and associated code listings for this section
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5 本节的流程和相关的代码列表
- en: We have previously built a basic API using hardcoded values. We will be taking
    that code and adding the ability to upload an ACH file. From there, we’ll be expanding
    the APIs to retrieve the data from our database instead of the hardcoded values.
    In the original ACH dashboard, one of the problems was a lack of control in processing
    ACH files.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前使用硬编码的值构建了一个基本的API。我们将使用那段代码并添加上传ACH文件的功能。从那里，我们将扩展API以从数据库中检索数据而不是硬编码的值。在原始的ACH仪表板中，一个问题是我们处理ACH文件时缺乏控制。
- en: Before creating a new API, let’s just ensure we can unit-test one of our existing
    endpoints with a hardcoded value. The next listing shows what that looks like.
    Copilot was able to produce the majority of this code as we typed, so it is just
    a matter of making sure it does what we intended.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建新的API之前，让我们确保我们可以使用硬编码的值对现有的端点进行单元测试。接下来的列表显示了它的样子。Copilot能够在我们输入时生成大部分代码，所以我们只需要确保它按照我们的意图执行。
- en: Listing 5.19  A unit test for FastAPI
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.19 FastAPI的单元测试
- en: '[PRE20]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#1 Imports the TestClient needed for unit testing'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 导入单元测试所需的TestClient'
- en: '#2 Imports our application'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 导入我们的应用程序'
- en: '#3 Defines a client'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 定义客户端'
- en: '#4 Defines a method for testing'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 定义了测试方法'
- en: '#5 Calls our endpoint and saves the response'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 调用我们的端点并保存响应'
- en: '#6 Assert statements validate the response'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 断言语句验证响应'
- en: So, that was pretty easy. Now, we would like to work on being able to `POST`
    an ACH file to the backend, get a response that it has been uploaded, and then
    begin parsing the file. Why not parse the file and then return a response to the
    user? The processing of our ACH file may take some time, especially when we consider
    that it will eventually need to interact with other systems to execute tasks such
    as verifying accounts/balances, OFAC validation, fraud analysis, and any other
    tasks an ACH processor may want to perform. Rather than have a user wait, we can
    verify the file is uploaded and start a task to perform the parsing.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这相当简单。现在，我们希望能够将一个ACH文件`POST`到后端，获取一个上传成功的响应，然后开始解析文件。为什么不在解析文件后再向用户返回响应呢？我们的ACH文件处理可能需要一些时间，特别是当我们考虑到它最终需要与其他系统交互以执行诸如验证账户/余额、OFAC验证、欺诈分析以及ACH处理器可能想要执行的其他任务时。为了避免让用户等待，我们可以验证文件已上传，并启动一个任务来执行解析。
- en: We already have an endpoint for posting a file, so we can add a unit test and
    then upload the file. Since the file is going to be uploaded using an `HTTP` `POST`
    request, it will be encoded using `multipart-formdata`. This means that we need
    the package `python-multipart` because this is a requirement for parsing requests
    encoded with that format. If we do not have it installed, we will receive a friendly
    reminder (in the form of an error).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有一个用于上传文件的端点，因此我们可以添加一个单元测试然后上传文件。由于文件将通过 `HTTP` `POST` 请求上传，它将使用 `multipart-formdata`
    编码。这意味着我们需要 `python-multipart` 包，因为这是解析使用该格式的请求的要求。如果我们没有安装它，我们将收到一个友好的提醒（以错误的形式）。
- en: Listing 5.20  `python-multipart` not installed
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.20  `python-multipart` 未安装
- en: '[PRE21]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Creating a test for uploading should look something like the following listing.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 创建上传测试应类似于以下列表。
- en: Listing 5.21  Unit test for uploading a file
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.21  上传文件的单元测试
- en: '[PRE22]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '#1 Opens the sample file'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 打开样本文件'
- en: '#2 Uses the client to POST the file'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用客户端发送文件'
- en: '#3 Ensures we receive a 20 1 status'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 确保我们收到 20 1 状态'
- en: We will also need to update the endpoint to receive the file. There is no need
    to return anything as we are just interested in the status code.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要更新端点以接收文件。由于我们只对状态码感兴趣，因此不需要返回任何内容。
- en: Listing 5.22  Updated endpoint to receive the file
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.22  更新后的端点以接收文件
- en: '[PRE23]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '#1 Additional imports are needed.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 需要额外的导入。'
- en: '#2 Defines the file as UploadFile'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 定义文件为 UploadFile'
- en: '#3 Returns nothing; only the status code as necessary'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 不返回任何内容；仅返回必要的状态码'
- en: Now we should have a successful unit test, and the real work can begin. Uploading
    a file manually or through some automatic process should be the driving force
    behind all our ACH tables. We should store some file information such as filename,
    time uploaded, a file hash (for tracking of duplicate files), and whatever other
    information we may decide is needed. The UUID of this record should then be included
    in any child tables (all the previous tables we have just created). If we had
    more experience or worked through the problem in a different order (perhaps uploading
    a file first), we may have avoided having more rework, but we would have also
    had to introduce more database concepts to start with. A benefit of this approach
    is the actual rework required to incorporate the changes. Often, developers become
    paralyzed because of the fear of change. Large complex systems sometimes seem
    like a house of cards where one wrong move can cause everything to come tumbling
    down. Having good unit test coverage and confidence in the tests can go a long
    way to alleviate these fears. The fear of changing and improving working software
    eventually leads to code rot.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该有一个成功的单元测试，然后真正的任务可以开始。手动上传文件或通过某些自动过程应该是我们所有 ACH 表的驱动力。我们应该存储一些文件信息，如文件名、上传时间、文件哈希（用于跟踪重复文件），以及我们可能决定需要的其他信息。然后，该记录的
    UUID 应该包含在任何子表中（我们刚刚创建的所有之前的表）。如果我们有更多的经验或以不同的顺序处理问题（例如，首先上传文件），我们可能会避免更多的返工，但我们也可能需要从开始引入更多的数据库概念。这种方法的好处是实际需要进行的返工以合并更改。通常，开发者会因为害怕改变而变得瘫痪。大型复杂系统有时就像一座纸牌屋，一次错误的移动可能导致一切崩溃。拥有良好的单元测试覆盖率和对测试的信心可以大大减轻这些恐惧。害怕改变和改进工作软件最终会导致代码腐烂。
- en: We will repurpose our `ach_files` table to be our main table that contains the
    upload information and rename `ach_files` to `ach_records` so that its only job
    is to store the unparsed ach records. The updated table definitions are in the
    following listing.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重新利用 `ach_files` 表作为主表，包含上传信息，并将 `ach_files` 重命名为 `ach_records`，使其唯一的工作是存储未解析的
    ach 记录。更新的表定义如下所示。
- en: Listing 5.23  Updated table listings for `ach_files` and `ach_records`
  id: totrans-235
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.23  `ach_files` 和 `ach_records` 的更新后的表列表
- en: '[PRE24]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '#1 The ach_files table is duplicated and repurposed to store the upload details.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 ach_files 表被复制并重新用于存储上传详情。'
- en: '#2 The ach_files table is duplicated and repurposed to store the upload details.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 ach_files 表被复制并重新用于存储上传详情。'
- en: '#3 The former ach_files will become ach_records.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 之前的 ach_files 将成为 ach_records。'
- en: '#4 Creates a foreign key named ach_files_id that references the ach_files_id
    from ach_files'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 创建一个名为 ach_files_id 的外键，它引用 ach_files 表中的 ach_files_id'
- en: '#5 Removes the file_name as it is now stored in the ach_files table'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 移除 file_name，因为它现在存储在 ach_files 表中'
- en: The new table `ach_records` uses the keywords `REFERENCES`, `ON` `DELETE`, and
    `ON` `UPDATE` to create a foreign key to the `ach_files` table. This feature allows
    the database to maintain its referential integrity. For instance, when we delete
    an ACH file out of `ach_files`, we do not want to go into every table and delete
    associated data. Instead, we want to define a relationship between the tables
    where all associated data is removed if we were to delete the `ach_file`. Once
    we have completed updating our tables, we can see this in action. This will also
    affect our tests. Once we have implemented referential integrity, we will need
    to ensure the `FOREIGN` `KEY` constraints are maintained.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 新的表 `ach_records` 使用 `REFERENCES`、`ON DELETE` 和 `ON UPDATE` 关键字创建到 `ach_files`
    表的外键。这个特性允许数据库保持其引用完整性。例如，当我们从 `ach_files` 中删除一个 ACH 文件时，我们不希望进入每个表并删除相关数据。相反，我们希望在表之间定义一个关系，如果我们删除了
    `ach_file`，则所有相关数据都会被删除。一旦我们完成了更新我们的表，我们就可以看到这个功能的效果。这也会影响我们的测试。一旦我们实现了引用完整性，我们需要确保
    `FOREIGN KEY` 约束得到维护。
- en: For example, if we want to have a unit test write out records to the `ach_records`
    table. we need a valid `ach_files_id` (it must exist in the `ach_files` table).
    For this reason, we will likely be looking at expanding the `SqlUtils` class we
    had developed earlier to set up some generic records and make this easier. Maintaining
    referential integrity can mean extra work on our side when setting up for testing,
    but it is worth implementing. We have worked in systems with the benefit of referential
    integrity and have seen programs crash or loop because of incomplete relationships.
    Usually, a lack of referential integrity is apparent in a system where developers
    have written various utility programs to scan and fix the data (which we have
    had to do ourselves on more than one occasion).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想让单元测试将记录写入 `ach_records` 表，我们需要一个有效的 `ach_files_id`（它必须在 `ach_files`
    表中存在）。因此，我们可能会考虑扩展我们之前开发的 `SqlUtils` 类，设置一些通用的记录并使这更容易。维护引用完整性在我们设置测试时可能意味着额外的工作，但它值得实现。我们曾在具有引用完整性的系统中工作，并看到程序因不完整的关系而崩溃或循环。通常，缺乏引用完整性在开发人员编写了各种实用程序程序来扫描和修复数据的系统中是明显的（我们不得不在多个场合自己这样做）。
- en: Additionally, we want to store the MD5 (or whichever algorithm you prefer) hash
    of the ACH file so we to identify duplicate files. First, we can get the hash
    from the command line using `Get-FileHash` `-Path` `".\sample.ach"` `-Algorithm`
    `MD5` `|` `Select-Object` `-ExpandProperty`, which in our case prints out `18F3C08227C3A60D418B3772213A94E3`.
    We’ll keep this info handy because we’ll be storing the hash as computed with
    Python in the database and expect it to be the same.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还想存储 ACH 文件的 MD5（或您喜欢的任何算法）哈希值，以便识别重复文件。首先，我们可以使用 `Get-FileHash -Path ".\sample.ach"
    -Algorithm MD5 | Select-Object -ExpandProperty` 从命令行获取哈希值，在我们的例子中打印出 `18F3C08227C3A60D418B3772213A94E3`。我们将保留这些信息，因为我们将在数据库中以
    Python 计算的哈希值存储，并期望它相同。
- en: Interestingly enough, when coding the function, Copilot prompted us to use string
    interpolation for the SQL query, `f"INSERT` `INTO` `ach_files` `(file_name,` `md5_hash)
    VALUES` `('{file.filename}',` `'{md5_hash}'),` but we continue with our standard
    parameterized query because we want to maintain secure coding practices. Queries
    that use string interpolation are ripe for SQL injection ([https://mng.bz/QDNj](https://mng.bz/QDNj)).
    Our code for the `/api/v1/files` endpoint is shown in the next listing.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在编写函数时，Copilot 提醒我们使用字符串插值来编写 SQL 查询，`f"INSERT INTO ach_files (file_name,
    md5_hash) VALUES ('{file.filename}', '{md5_hash}')`，但我们继续使用我们的标准参数化查询，因为我们想保持安全的编码实践。使用字符串插值的查询容易受到
    SQL 注入 ([https://mng.bz/QDNj](https://mng.bz/QDNj)) 的攻击。我们的 `/api/v1/files` 端点的代码在下一个列表中显示。
- en: Listing 5.24  Updated endpoint to upload ACH files
  id: totrans-246
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.24  更新端点以上传 ACH 文件
- en: '[PRE25]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#1 Reads the uploaded file'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 读取上传的文件'
- en: '#2 Uses hashlib to create a hash of the file'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用 hashlib 创建文件的哈希值'
- en: '#3 Uses our newly created DbUtils to return a connection'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用我们新创建的 DbUtils 返回连接'
- en: '#4 Simple SQL to INSERT a record into ach_files'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 简单 SQL 将记录插入到 ach_files'
- en: We created a simple `DbUtils` class to return a database connection. We imagine
    it will be used in multiple places in our project and thus extracted it into its
    own file.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个简单的 `DbUtils` 类来返回数据库连接。我们想象它将在我们的项目中多个地方使用，因此将其提取到自己的文件中。
- en: Listing 5.25  `DbUtils` for creating a database connection
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.25  用于创建数据库连接的 `DbUtils`
- en: '[PRE26]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '#1 Hardcoded values to be removed later'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 硬编码的值将在以后被移除'
- en: '#2 Creates and returns a connection'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建并返回一个连接'
- en: With these changes, we should be able to run the unit test and then head over
    to our CloudBeaver UI and see the newly added record and manually compare the
    record hash. However, manually checking the record is no fun, so as a final piece,
    we should update our unit test to incorporate the same `SqlUtils`, as we have
    seen previously, and ensure that we have at least one record in our table. The
    following listing shows the updated unit test.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些更改，我们应该能够运行单元测试，然后转到我们的CloudBeaver UI，查看新添加的记录并手动比较记录哈希。然而，手动检查记录并不有趣，所以作为最后一部分，我们应该更新我们的单元测试以包含相同的`SqlUtils`，就像我们之前看到的那样，并确保我们的表中至少有一条记录。以下列表显示了更新的单元测试。
- en: Listing 5.26  Updated unit test for our upload file
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.26  更新后的上传文件单元测试
- en: '[PRE27]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '#1 Required imports'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 必要的导入'
- en: '#2 Defines our fixture to ensure all tables are truncated'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 定义我们的固定装置以确保所有表都被截断'
- en: '#3 Includes the fixture in our unit test'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将固定装置包含在我们的单元测试中'
- en: '#4 Ensures there is one row in the ach_files table'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 确保ach_files表中有一行'
- en: Being able to upload a file successfully may not seem like a big deal, but we
    must remember that all other functionality of our ACH dashboard is based on this
    one piece. Functionality such as error checking and recovery, crediting/debiting
    accounts, fraud analysis, and much more is available to us now that we can upload
    ACH files. After completing this task, we can expand on the database structure.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 能够成功上传文件可能看起来不是什么大事，但我们必须记住，我们ACH仪表板的所有其他功能都基于这一部分。现在我们可以上传ACH文件，我们可以使用诸如错误检查和恢复、账户贷记/借记、欺诈分析等功能。完成这项任务后，我们可以扩展数据库结构。
- en: 5.7 Storing records with integrity
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.7 以完整性存储记录
- en: As mentioned before, we want our database to have referential integrity, which
    can simplify navigation in database tools, and more importantly, have the ability
    to prevent dangling records. We have seen that databases without referential integrity
    require maintenance programs that run periodically to clean up those dangling
    records, as well as infinite loops and program crashes resulting from incomplete
    data. A great benefit of using relational databases is their support for data
    integrity. We highly recommend keeping it in mind when designing databases.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们希望我们的数据库具有引用完整性，这可以简化数据库工具中的导航，更重要的是，具有防止悬挂记录的能力。我们已经看到，没有引用完整性的数据库需要定期运行的维护程序来清理那些悬挂记录，以及由不完整数据引起的无限循环和程序崩溃。使用关系数据库的一个巨大好处是它们对数据完整性的支持。我们强烈建议在设计数据库时牢记这一点。
- en: In our examples so far, we have defined a `PRIMARY` `KEY` and how to update
    the `ach_records` table. In listing 5.23, we added our first `FOREIGN` `KEY` by
    using the `REFERENCES` keyword, which starts us on the path of maintaining referential
    integrity. We continue to update the remaining tables and code with foreign keys
    and review the effects of using them on our tables, code, and unit tests.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的例子中，我们已经定义了`PRIMARY` `KEY`以及如何更新`ach_records`表。在列表 5.23 中，我们通过使用`REFERENCES`关键字添加了第一个`FOREIGN`
    `KEY`，这使我们开始了维护引用完整性的道路。我们继续使用外键更新剩余的表和代码，并审查使用它们对我们的表、代码和单元测试的影响。
- en: If we try running our unit tests at this point, we will see errors referring
    to an `UndefinedColumn` since we updated our table definitions in listing 5.23\.
    This is not a bad thing. The fact that we have built unit tests allows us to refactor
    our code with confidence. We start with diving into our `test_record_count` unit
    test (from listing 5.7) to resolve our error of `psycopg.errors.UndefinedColumn:`
    `column` `"unparsed_record" of` `relation` `"ach_files"` `does` `not` `exist`.
    The offending code is shown in the following listing.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在尝试运行我们的单元测试，我们会看到引用`UndefinedColumn`的错误，因为我们更新了列表 5.23 中的表定义。这不是坏事。我们构建了单元测试，这使我们能够有信心重构我们的代码。我们从深入我们的`test_record_count`单元测试（来自列表
    5.7）开始，解决我们的`psycopg.errors.UndefinedColumn:` `column` `"unparsed_record"` of
    `relation` `"ach_files"` `does` `not` `exist`的错误。有问题的代码如下所示。
- en: Listing 5.27  Invalid column names
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.27  无效的列名
- en: '[PRE28]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Depending on your IDE and setup, this statement may already be flagged. In PyCharm,
    we have defined a connection to the database, so the fields `unparsed_record`
    and `sequence_number` are flagged with an `Unable` `to` `resolve` `column` error.
    If your IDE does not have that capability, then the stack trace also shows the
    line number. Given this information, we notice that the `INSERT` statement is
    pointing to the wrong table as we had renamed `ach_files` to `ach_records`. Changing
    that and rerunning the test gives a new error `psycopg.errors.NotNullViolation:`
    `null` `value` `in` `column "ach_files_id"` `of` `relation` `"ach_records"` `violates`
    `not-null` `constraint`. We defined `ach_files_id` as a foreign key, and it cannot
    be null—in fact, it must point to a valid record in the `ach_files` table. Recall
    that our API will create this record when a file is uploaded. So, it is plausible
    that the `AchFileProcessor` will now need to be called with both the filename
    being parsed and a valid `ach_files_id`. We can update the unit test so that the
    parse routine is called with this ID. The `parser.parse(file_path)` will need
    to become `parser.parse(ach_files_id,` `file_path)`, but it is not enough to define
    `ach_files_id`. We need a way to create a valid `ach_files_id` because we have
    to maintain database integrity. We could drop the constraints during testing,
    which may be an option if our unit tests were primarily concerned with functionality.
    However, in this case, we would like to maintain the constraint. So, we will need
    to create a function to insert the record, then take a step back and consider
    our path forward as there may be a few different routes we want to take.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '根据你的 IDE 和设置，这个语句可能已经被标记。在 PyCharm 中，我们已经定义了一个数据库连接，因此字段 `unparsed_record`
    和 `sequence_number` 被标记为 `Unable to resolve column` 错误。如果你的 IDE 没有这个功能，那么堆栈跟踪也会显示行号。根据这些信息，我们注意到
    `INSERT` 语句指向了错误的表，因为我们已经将 `ach_files` 重命名为 `ach_records`。更改这一点并重新运行测试会得到一个新的错误
    `psycopg.errors.NotNullViolation: null value in column "ach_files_id" of relation
    "ach_records" violates not-null constraint`。我们定义了 `ach_files_id` 为外键，它不能为空——实际上，它必须指向
    `ach_files` 表中的一个有效记录。回想一下，我们的 API 会在文件上传时创建这个记录。因此，现在 `AchFileProcessor` 可能需要同时调用正在解析的文件名和有效的
    `ach_files_id`。我们可以更新单元测试，使解析例程使用这个 ID 被调用。`parser.parse(file_path)` 将需要变成 `parser.parse(ach_files_id,
    file_path)`，但定义 `ach_files_id` 并不足以解决问题。我们需要一种方法来创建一个有效的 `ach_files_id`，因为我们必须保持数据库的完整性。我们可以在测试期间删除约束，这可能是一个选项，如果我们的单元测试主要关注功能的话。然而，在这种情况下，我们希望保持这个约束。因此，我们需要创建一个插入记录的函数，然后退后一步考虑我们的前进路径，因为我们可能想要采取几条不同的路线。'
- en: Remember that we already coded an `INSERT` statement to insert the `ach_files`
    record into our API. We could duplicate that query in our unit test and then grab
    that inserted value. That would work, but we will have duplicated code, and we
    want to try to avoid that whenever possible. We could add a routine to `SqlUtils`
    to handle this task for us, which other methods would have access to because other
    unit tests may also need the functionality. However, `SqlUtils` is meant to assist
    us in our tests only, and we have already seen that we will need this functionality
    elsewhere. Perhaps we should create a utility class for the `AchFileProcessor`
    to insert our desired record, which would also allow us to refactor the existing
    SQL queries out of the parsing routines. There is no definitive answer. Depending
    on the project, our needs, programming style, and experience, we may see other
    alternatives or have preferences on how to approach this problem. In this case,
    we believe that using Pydantic is the best way to move forward.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们已经在代码中编写了一个 `INSERT` 语句来将 `ach_files` 记录插入到我们的 API 中。我们可以在单元测试中重复这个查询并获取插入的值。这会有效，但我们会重复代码，而我们希望尽可能避免这种情况。我们可以在
    `SqlUtils` 中添加一个例程来为我们处理这个任务，这样其他方法也可以访问它，因为其他单元测试可能也需要这个功能。然而，`SqlUtils` 的目的是仅在我们测试时帮助我们，我们已经看到我们还需要在其他地方使用这个功能。也许我们应该为
    `AchFileProcessor` 创建一个工具类来插入我们想要的记录，这将允许我们重构现有的 SQL 查询，使其从解析例程中分离出来。没有明确的答案。根据项目、需求、编程风格和经验，我们可能会看到其他替代方案或对如何处理这个问题有偏好。在这种情况下，我们认为使用
    Pydantic 是前进的最佳方式。
- en: 5.8 Using Pydantic
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.8 使用 Pydantic
- en: In the previous section, we ran into an problem of how to handle inserting a
    record to make our unit test work. While there are several approaches we could
    take, Pydantic will help further refactoring and expansion of our project. We
    had introduced Pydantic previously to help document our APIs in chapter 4, so
    we know that it already has some benefits. Another benefit is that we will be
    able to define our tables and fields in a way that allows developers not to have
    to remember which fields are present. In other words, we start abstracting the
    SQL from the parsing logic. Let’s start applying that with our `ach_files` table
    so that we can create a record and return a valid record ID, which is all we really
    wanted to do to get around our first unit test problem.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们遇到了一个问题，即如何处理插入记录以使我们的单元测试工作。虽然我们可以采取几种方法，但Pydantic将有助于进一步重构和扩展我们的项目。我们之前在第四章中介绍了Pydantic来帮助我们文档化API，因此我们知道它已经有一些好处。另一个好处是，我们将能够以允许开发者不必记住哪些字段存在的方式来定义我们的表和字段。换句话说，我们开始从解析逻辑中抽象SQL。让我们从我们的`ach_files`表开始应用这一点，这样我们就可以创建一个记录并返回一个有效的记录ID，这正是我们真正想要做的，以解决我们的第一个单元测试问题。
- en: Why is there no object-relational model framework?
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么没有对象关系模型框架？
- en: An object-relational model (ORM) has many benefits and is widely used in many
    different industries. We are holding off on incorporating an ORM such as SQLAlchemy
    for now because we want to ensure the reader is exposed directly to SQL in case
    those skills need brushing up on.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 对象关系模型（ORM）有许多好处，并且在许多不同的行业中都得到了广泛的应用。我们现在推迟将如SQLAlchemy之类的ORM纳入其中，因为我们想确保读者能够直接接触到SQL，以防那些技能需要复习。
- en: Later, we will show how to include an ORM into the project, so hang in there.
    Of course, if you are familiar with ORMs, you can dive right in and start using
    them.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将展示如何将ORM包含到项目中，所以请耐心等待。当然，如果你熟悉ORM，你可以直接跳进去开始使用它们。
- en: We first create a Pydantic model for our table, as shown in the following listing.
    This simple model provides a basic layout for modeling the data that’ll be written
    to our `ach_files` table.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先为我们的表创建一个Pydantic模型，如下所示。这个简单的模型为将要写入我们的`ach_files`表的建模提供了一个基本布局。
- en: Listing 5.28  Our Pydantic model for `ach_files`
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.28  我们为`ach_files`的Pydantic模型
- en: '[PRE29]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '#1 Necessary import statements'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 必要的导入语句'
- en: '#2 The class extends the Pydantic BaseModel.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 该类扩展了Pydantic BaseModel。'
- en: '#3 Our field definitions; note the Optional keyword for fields (such as ID)
    that our database will supply.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 我们的字段定义；注意Optional关键字用于数据库将提供的字段（如ID）。'
- en: Next, we can go ahead and define a class that will provide some basic Create,
    Read, Update, and Delete (CRUD) methods to handle working with the database table.
    The following listing shows the `AchFileSql` class we will create to wrap the
    logic for dealing with our `ach_files` database table.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以定义一个类，它将提供一些基本的创建、读取、更新和删除（CRUD）方法来处理与数据库表的工作。以下列表显示了我们将创建的`AchFileSql`类，以封装处理我们的`ach_files`数据库表的逻辑。
- en: Listing 5.29  `AchFileSql` class
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.29  `AchFileSql`类
- en: '[PRE30]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '#1 Required imports for the class'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 类所需的导入'
- en: '#2 Creates a function to insert the record and return the UUID'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建一个函数来插入记录并返回UUID'
- en: '#3 Uses the RETURNING keyword to return the ID of the newly inserted record'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用RETURNING关键字返回新插入记录的ID'
- en: '#4 Uses model_dump to create a dictionary that references the fields'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 使用model_dump创建一个引用字段的字典'
- en: '#5 Creates a function to return a specified record'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 创建一个函数来返回一个指定的记录'
- en: '#6 By using the row_factory of class_row, we can return the record directly.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 通过使用class_row的row_factory，我们可以直接返回记录。'
- en: '#7 If nothing is found, raises an error'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 如果没有找到任何内容，将引发错误'
- en: Finally, we create a unit test to verify our newly created classes. We create
    this unit test as a class just to show another way to help organize our tests.
    We also introduce the `autouse` option on our `pytest` fixtures so we do not have
    to include them in every method.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建一个单元测试来验证我们新创建的类。我们创建这个单元测试作为一个类，只是为了展示另一种帮助组织测试的方法。我们还引入了`pytest`固定件的`autouse`选项，这样我们就不必在每一个方法中都包含它们。
- en: Listing 5.30  Unit testing our new classes
  id: totrans-295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.30  测试我们的新类
- en: '[PRE31]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '#1 Required imports'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 必要的导入'
- en: '#2 Our fixture now uses autouse.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 我们现在使用autouse。'
- en: '#3 Truncates the tables when finished'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 完成后截断表'
- en: '#4 Inserts the record'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 插入记录'
- en: '#5 Immediately returns the record'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 立即返回记录'
- en: '#6 Assert statements validate our results'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 断言语句验证我们的结果'
- en: This is a great start to taking our project to the next level. You may be wondering
    why we did not write our test first in a test-driven development manner. Sometimes,
    it is easier to take a test-later approach, especially when demonstrating new
    concepts. Again, it is not so about writing the test but about working in a short
    cycle. So, as soon as we got something to test, we started testing it.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个将我们的项目提升到下一个层次的良好开端。你可能想知道为什么我们没有首先以测试驱动开发的方式编写我们的测试。有时，采取“先测试后”的方法更容易，尤其是在演示新概念时。再次强调，这并不是关于编写测试，而是关于在一个短周期内工作。因此，一旦我们有了可以测试的内容，我们就开始测试它。
- en: Recall that with Pydantic, we get field validation and the ability to document
    our APIs. We will be looking at that later, but for now, we should continue refactoring
    our code to take advantage of Pydantic. Once we have the code refactored using
    Pydantic and our unit tests passing, we can move along to handling APIs.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，使用 Pydantic，我们可以获得字段验证和记录我们的 API 的能力。我们稍后会查看这一点，但现在，我们应该继续重构我们的代码以利用 Pydantic。一旦我们使用
    Pydantic 重构了代码并且单元测试通过，我们就可以继续处理 API。
- en: 5.9 Lessons learned
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.9 经验教训
- en: The process of including Pydantic and separating our code required us to refactor
    not only the ach_file_processor.py but our unit tests as well. Refactoring allowed
    us to improve our code in both areas and obtain code that is cleaner and easier
    to test. Unfortunately, we also ran into some problems when we finished creating
    a database structure that followed the original specification from figure 5.1\.
    Did you notice any problems with the structure before refactoring the code?
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Pydantic 包含到我们的代码中并对其进行分离的过程要求我们不仅重构了 ach_file_processor.py，还重构了我们的单元测试。重构使我们能够在两个领域都改进我们的代码，并获得更干净、更容易测试的代码。不幸的是，当我们完成了根据图
    5.1 中的原始规范创建的数据库结构后，我们也遇到了一些问题。你在重构代码之前注意到任何结构上的问题了吗？
- en: Inserting foreign keys into that database structure revealed a problem with
    maintaining data integrity in the `ach_files` table. While deleting a record from
    those child tables would behave correctly, the `ach_files` table did not get rid
    of all the records we desired. For instance, if we were to delete a batch, we
    would expect the associated type 6–8 records to be removed as well, and this cannot
    happen in the current structure.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 将外键插入到该数据库结构中揭示了在 `ach_files` 表中维护数据完整性的问题。虽然从那些子表中删除记录的行为是正确的，但 `ach_files`
    表并没有删除我们想要的全部记录。例如，如果我们删除一个批次，我们期望相关的类型 6-8 记录也会被删除，但在当前的结构中这是不可能发生的。
- en: This situation is not uncommon. Often, when given specs for a project—depending
    on the experience of the project stakeholders and the time allowed for researching
    areas of the project—it may not be possible to flush out all the design details.
    In a scaled Agile context, we may consider this project an enabler—more specifically,
    an exploration enabler. Exploration enablers help explore prospective solutions
    and include activities such as research and prototyping. If these are not completed
    sufficiently before we got a project, we may run into problems we had here. There
    are several alternatives to handle the situation, and the right answer will likely
    depend on the reevaluation of project requirements.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况并不少见。通常，当给定一个项目的规范时——根据项目利益相关者的经验以及用于研究项目领域的允许时间——可能无法完全梳理出所有设计细节。在扩展敏捷的背景下，我们可能会将这个项目视为一种启用器——更具体地说，是一种探索启用器。探索启用器有助于探索潜在的解决方案，包括研究和原型制作等活动。如果我们没有在项目开始之前充分完成这些活动，我们可能会遇到我们在这里遇到的问题。处理这种情况有几种替代方案，正确的答案可能取决于对项目要求的重新评估。
- en: To recap the project requirements, we wanted to find a solution that
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回顾项目要求，我们希望找到一个解决方案，
- en: Provided both ETL and ELT options when processing a database.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理数据库时提供了 ETL 和 ELT 选项。
- en: Provided data integrity over the entire file. As the ACH file is a hierarchy,
    there are multiple scenarios that require additional records to be cleaned up
    when a file, batch, or entry are deleted.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在整个文件中提供了数据完整性。由于 ACH 文件是一个层次结构，因此存在多种场景，当文件、批次或条目被删除时，需要清理额外的记录。
- en: Let’s propose a few different options we may need to present to the business
    stakeholders.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们提出一些可能需要向业务利益相关者展示的不同选项。
- en: First, we could ditch parsing the individual records into the database and just
    store the unparsed records. This would certainly simplify the database design
    as we only have one table to deal with. Of course, this approach limits the usefulness
    of having a relational database and would require additional application code
    to handle the data integrity we mentioned earlier. For instance, if the user wanted
    to delete a batch, we would have to ensure that all the records were deleted by
    the application code, which would expand and complicate our application code.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以放弃将单个记录解析到数据库中，而只存储未解析的记录。这无疑会简化数据库设计，因为我们只需处理一个表。当然，这种方法限制了关系数据库的实用性，并且需要额外的应用程序代码来处理我们之前提到过的数据完整性。例如，如果用户想要删除一批记录，我们必须确保应用程序代码删除了所有记录，这将扩大并复杂化我们的应用程序代码。
- en: Second, we could ditch storing unparsed records in the table. This could potentially
    be a good solution if we determine we do not need the unparsed records. Of course,
    it also means that our file needs to conform to the database constraints, and
    the business has already required the unparsed records to be retained in the event
    of invalid data (such as non-numeric data in a numeric field) that may cause a
    parsed record to be rejected from the database. This seems to be a hard requirement
    to get around.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们可以放弃在表中存储未解析的记录。如果我们确定我们不需要未解析的记录，这可能是一个潜在的解决方案。当然，这也意味着我们的文件需要符合数据库约束，而且业务已经要求在无效数据（如数值字段中的非数值数据）可能导致解析记录被数据库拒绝的情况下保留未解析的记录。这似乎是一个难以绕过的硬性要求。
- en: Third, we could investigate setting up a database trigger to delete the records.
    Database triggers are code that can be automatically executed when certain events
    occur in the database. We may be able to create triggers in the parsed record
    tables that execute when a record is deleted to also eliminate the associated
    unparsed record. That does not sound like much fun though.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们可以考虑设置数据库触发器来删除记录。数据库触发器是一段代码，可以在数据库中发生某些事件时自动执行。我们可能在解析记录表中创建触发器，当记录被删除时，也会删除相关的未解析记录。但这听起来并不有趣。
- en: We opted for another route to address this problem—reorganizing the table into
    the structure where we used an unparsed record table for each type. This required
    substantial refactoring of the tables and associated application code, but for
    us, it made the most sense if we wanted to maintain the requirements for keeping
    unparsed and parsed records. The updated database diagram is shown in figure 5.6,
    along with a reference to the original layout from figure 5.1.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了另一种途径来解决这个问题——将表重新组织成我们为每种类型使用一个未解析记录表的结构。这需要对表及其相关应用程序代码进行大量重构，但对我们来说，如果我们想要保持保留未解析和解析记录的要求，这是最有意义的。更新的数据库结构图如图5.6所示，其中还包括了来自图5.1的原布局引用。
- en: This structure gave us the data integrity we were originally looking for, but
    we now have to find a way to view all the unparsed records. To accomplish this,
    we create a database view. A database view is a virtual table that is the result
    of a stored query. By using a view, we can save ourselves and others who need
    to use our database the tedious task of tying the unparsed data together. The
    next listing shows the created database view.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构为我们提供了最初所寻找的数据完整性，但现在我们必须找到一种方法来查看所有未解析的记录。为了实现这一点，我们创建了一个数据库视图。数据库视图是一个虚拟表，是存储查询的结果。通过使用视图，我们可以避免自己和其他需要使用我们数据库的人进行繁琐的将未解析数据关联起来的任务。接下来的列表显示了创建的数据库视图。
- en: Listing 5.31  Creating a database view
  id: totrans-318
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.31 创建数据库视图
- en: '[PRE32]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '#1 UNION ALL will combine the results from the successive SELECT statements.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 `UNION ALL`将合并连续的`SELECT`语句的结果。'
- en: '#2 The USING statements allow for a more concise syntax instead of ON table.field
    =.table.field. It can be used in PostgreSQL when the joining fields share the
    same name among tables.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用`USING`语句可以提供更简洁的语法，而不是使用`ON table.field = table.field`。在PostgreSQL中，当连接字段在表中具有相同的名称时，可以使用此功能。'
- en: '![](../Images/CH05_F06_Kardell.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F06_Kardell.png)'
- en: Figure 5.6  Updated table diagram
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6 更新后的表结构图
- en: This code allows us to view the entire ACH file as it was uploaded to the system,
    without any manipulation from our processing. Having a way to view the raw file
    allows us to present users with options to view the entire ACH file contents,
    as well as exception records to help them troubleshoot any problems.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码允许我们查看整个 ACH 文件，就像它被上传到系统中一样，没有任何我们的处理操作。有一种查看原始文件的方法，这允许我们向用户提供查看整个 ACH
    文件内容以及异常记录的选项，以帮助他们调试任何问题。
- en: 5.10 Coding changes
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.10 编码更改
- en: Now that we have settled on this database structure, it was a matter of writing
    any additional unit tests and getting the existing ones updated to run correctly.
    Let’s look at some of the changes made to support this structure.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经确定了数据库结构，那么编写任何额外的单元测试以及更新现有的测试以正确运行就是一个问题了。让我们看看为了支持这个结构所做的更改。
- en: 5.10.1 Creating Pydantic schema for the unparsed ACH records
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.10.1 为未解析的 ACH 记录创建 Pydantic 架构
- en: All the unparsed records share the `unparsed_record` and sequence number fields.
    Therefore, this is a good opportunity to create a class structure that will inherit
    those fields so that we do not have to type them every time. We create an `AchRecordBaseSchema`.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 所有未解析的记录都共享 `unparsed_record` 和序列号字段。因此，这是一个创建一个将继承这些字段以避免每次都输入它们的类结构的好机会。我们创建了一个
    `AchRecordBaseSchema`。
- en: Listing 5.32  Base schema for our ACH records
  id: totrans-329
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.32  我们 ACH 记录的基架构
- en: '[PRE33]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '#1 Importing ABC allows us to create an abstract class.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 导入 ABC 允许我们创建一个抽象类。'
- en: '#2 The BaseModel is required for Pydantic.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 `BaseModel` 是 Pydantic 所必需的。'
- en: '#3 Our class inherits from both ABC and BaseModel.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 我们的这个类从 ABC 和 `BaseModel` 继承。'
- en: '#4 Fields that will be present in all ACH record classes'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 所有 ACH 记录类中都将存在的字段'
- en: With that schema, we can define each record type, as shown in the following
    listing.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该架构，我们可以定义每个记录类型，如下所示。
- en: Listing 5.33  ACH record schema
  id: totrans-336
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.33  ACH 记录架构
- en: '[PRE34]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '#1 Required imports'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 必须导入'
- en: '#2 This is a Pydantic class by virtue of being a subclass of AchRecordBaseSchema,
    which was a subclass of BaseModel.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 由于它是 `AchRecordBaseSchema` 的子类，而 `AchRecordBaseSchema` 是 `BaseModel` 的子类，因此这是一个
    Pydantic 类。'
- en: '#3 We have an ID field that is marked as optional since it will be assigned
    by the database.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 我们有一个标记为可选的 ID 字段，因为它将由数据库分配。'
- en: '#4 The ach_files_id field is passed in and is not optional as it is a foreign
    key to the file that was uploaded.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 `ach_files_id` 字段是必须的，因为它是一个外键，指向已上传的文件。'
- en: 5.10.2 Creating Pydantic schema for the parsed ACH records
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.10.2 为解析的 ACH 记录创建 Pydantic 架构
- en: The Pydantic definitions for each of the parsed records are less interesting
    at this point as we just inherit from the Pydantic `BaseModel` record and define
    the necessary fields. We will expand on the fields for constraints, validation,
    and documentation later. For now, we just keep them simple.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，对于每个解析记录的 Pydantic 定义并不那么有趣，因为我们只是继承了 Pydantic 的 `BaseModel` 记录并定义了必要的字段。我们将在稍后扩展字段以用于约束、验证和文档。目前，我们只需保持它们简单。
- en: Listing 5.34  Pydantic schema for an ACH batch header record
  id: totrans-344
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.34  ACH 批次头记录的 Pydantic 架构
- en: '[PRE35]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 5.10.3 Unit test changes
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.10.3 单元测试更改
- en: As part of code refactoring, we ensured that our tests were cleaned up as well
    (listing 5.35). First, we updated our `setup_teardown_method` to set `autouse`
    to `true` and then ensure the `SqlUtils.truncate_all` method was executed first.
    We may have previously chosen to clear the tables after the tests were run, which
    is a good practice that cleans up any data from our test. However, it also has
    the unfortunate side effect of cleaning up the data when tests fail, which is
    not very helpful when we want to examine the database after a test. To make debugging
    and troubleshooting easier, we decided to clear the data before the test. This
    also ensures the database is ready as we do not have to rely on a previous test
    to clean up after itself. Adding the `autouse` parameters means we no longer need
    to pass the fixture to our tests. We also used `truncate_all` instead of a specific
    table since we now have multiple tables being used.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 作为代码重构的一部分，我们确保我们的测试也得到了清理（列表 5.35）。首先，我们更新了 `setup_teardown_method` 以将 `autouse`
    设置为 `true`，并确保首先执行 `SqlUtils.truncate_all` 方法。我们可能之前选择在测试运行后清除表，这是一个很好的实践，可以清理测试中的任何数据。然而，这也带来了一个不幸的副作用，即在测试失败时也会清理数据，这在我们想要在测试后检查数据库时并不是很有帮助。为了使调试和故障排除更容易，我们决定在测试之前清除数据。这也确保了数据库已准备好，因为我们不需要依赖之前的测试来自动清理。添加
    `autouse` 参数意味着我们不再需要将 fixture 传递给我们的测试。我们还使用了 `truncate_all` 而不是特定的表，因为我们现在使用了多个表。
- en: Listing 5.35  Updated `pytest` fixture
  id: totrans-348
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.35 更新的 `pytest` 固定装置
- en: '[PRE36]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 5.11 Design and different approaches
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.11 设计和不同方法
- en: It was a fair amount of rework to add foreign keys to the database, so we may
    be thinking about ways to minimize that work or why we did not just add them in
    the beginning. As you saw, we dove into parsing the ACH file, which we accomplished,
    and then moved on to supporting some additional functionality that required a
    lot of rework. This was partly because we wanted to show some ACH basics before
    getting into overall functionality and additional database knowledge.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 向数据库添加外键需要进行相当多的重工作，所以我们可能会考虑如何最小化这项工作，或者为什么我们没有一开始就添加它们。正如你所见，我们深入解析了ACH文件，并完成了这项工作，然后转向支持一些需要大量重工作的附加功能。这主要是因为我们想在深入研究整体功能和额外的数据库知识之前，先展示一些ACH基础知识。
- en: However, let’s consider whether we approached the problem from a functionality
    standpoint and whether the user was going to need these files uploaded. Had we
    then started with the `ach_files` table and associated endpoint, we could have
    incorporated foreign keys from the beginning. Assuming you had knowledge and experience
    working with ACH files and APIs, it would certainly have been a valid approach.
    Then we could have proceeded in much the same fashion, with the exception that
    we may have had a better-designed database from the start.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，让我们考虑一下我们是否从功能角度来处理这个问题，以及用户是否需要上传这些文件。如果我们当时从`ach_files`表及其相关端点开始，我们就可以从一开始就包含外键。假设你具备处理ACH文件和API的知识和经验，这肯定是一个有效的方法。然后我们可以以同样的方式继续进行，唯一的区别是我们可能从一开始就有一个设计得更好的数据库。
- en: This just goes to show that initial design can be important as well as the approach
    to the problem you take. We spoke earlier about enabler stories, or what is also
    known as research spikes. They reduce risk and help us understand requirements
    and otherwise gain a better understanding of the tasks that will need to be performed.
    We cannot stress the importance of these types of stories, especially when working
    with large complex systems. We can always expect some rework to be required when
    we get into a project, either due to unidentified requirements or changes in scope
    that could not be accounted for. Hopefully, research spikes help minimize those
    instances, but it can often be difficult for a business to devote time to something
    they do not immediately see a benefit from. This will obviously become a problem
    because problems that are flushed out by enabler stories will not be identified
    and will go unnoticed in PI planning. The approach we may end up taking may require
    rework that could have been identified earlier, and therefore, our story points
    could have been allocated correctly.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是说明初始设计同样重要，以及你解决问题的方法。我们之前讨论过启用故事，或者也称为研究尖峰。它们可以降低风险，帮助我们理解需求，并更好地理解需要执行的任务。我们无法强调这些类型故事的重要性，尤其是在处理大型复杂系统时。我们总是可以预期在项目进行过程中需要一些重工作，无论是由于未识别的需求还是无法预见的范围变化。希望研究尖峰可以帮助最小化这些情况，但企业往往难以将时间投入到他们看不到即时收益的事情上。这显然会成为一个问题，因为通过启用故事暴露的问题将不会被识别，在PI规划中也不会被发现。我们最终可能采取的方法可能需要重工作，而这些重工作本可以在早期识别出来，因此我们的故事点可以正确分配。
- en: We have seen numerous examples where a project is seemingly complete, only to
    have a subject matter expert point out something that was missed in a system demo
    and requires rework. It is important to remember that we will have to deal with
    these types of situations at times as it may be often tempting to take an easier
    way out. In such situations, we will need to judge the effects of the change on
    the project timeline, amount of rework, risk involved, technical debt incurred,
    and so forth. For instance, the business may have decided that we did not need
    referential integrity of the database and that it would be faster to write a program
    that could be manually run to check for dangling records/missing relationships.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了许多例子，一个项目似乎已经完成，但最终某个主题专家指出在系统演示中遗漏了某些内容，需要重工作。重要的是要记住，我们有时必须处理这些类型的状况，因为这可能会经常诱使我们选择一条更简单的出路。在这种情况下，我们需要评估变化对项目时间表、重工作量、风险、技术债务等因素的影响。例如，企业可能决定我们不需要数据库的引用完整性，并且编写一个可以手动运行的程序来检查悬空记录/缺失关系会更快。
- en: Summary
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter shows how to create a database that could store our unaltered records,
    as well as a parsed version of the record that will be beneficial when necessary
    to provide details on the loaded ACH files.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章展示了如何创建一个数据库，该数据库可以存储我们的未更改记录，以及一个解析后的记录版本，这在需要提供加载的ACH文件详细信息时是有益的。
- en: We saw the implications of adding referential integrity to our database late
    in the process and the need to rework both our code and unit tests. When a feature
    requires rework to be done correctly, it can often be put on the back burner.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在后期过程中看到了向数据库添加引用完整性的影响，以及需要重新编写我们的代码和单元测试的需求。当一个功能需要重新工作才能正确实现时，它通常可以被放在次要位置。
- en: Despite the extra work to implement these types of features, it is important
    to champion them both to the team members and management so they are not left
    by the wayside.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管实现这些类型的功能需要额外的工作，但向团队成员和管理层倡导它们同样重要，以确保它们不会被忽视。
- en: Defining databases is crucial for data storage, querying, and integrity in applications.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义数据库对于应用程序中的数据存储、查询和完整性至关重要。
- en: ACH files face challenges with performance, querying, and data integrity when
    handled as flat files.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当作为平面文件处理时，ACH文件在性能、查询和数据完整性方面面临挑战。
- en: Relational databases offer advantages, such as primary and foreign keys, constraints,
    and data integrity.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系数据库提供优势，例如主键和外键、约束和数据完整性。
- en: Implementing referential integrity prevents orphan records and ensures database
    consistency.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现引用完整性可以防止孤立记录并确保数据库一致性。
- en: A research spike (enabler story) is beneficial when evaluating database design
    and various implementation approaches.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评估数据库设计和各种实现方法时，研究激增（使能故事）是有益的。
- en: ELT and ETL offer different benefits for processing ACH files and handling errors.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ELT和ETL在处理ACH文件和处理错误方面提供不同的好处。
- en: Pydantic helps in modeling database tables, abstracting SQL, and enhancing documentation
    and validation.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pydantic帮助建模数据库表，抽象SQL，并增强文档和验证。
- en: Uploading files and integrating APIs are foundational for expanding functionality
    in an ACH system.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上传文件和集成API是扩展ACH系统功能的基础。
- en: Data and referential integrity are critical for relational databases to prevent
    errors and improve reliability.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据和引用完整性对于关系数据库至关重要，可以防止错误并提高可靠性。
- en: Continuous testing, refactoring, and revisiting initial design choices help
    maintain and improve database performance and structure.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续测试、重构和重新审视初始设计选择有助于维护和改进数据库性能和结构。
