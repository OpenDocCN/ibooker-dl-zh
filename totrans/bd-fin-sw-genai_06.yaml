- en: 5 Storing our ACH files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creating tables within our PostgreSQL database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing a relational database capable of storing ACH files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Python and Pydantic to validate ACH records and store them in our database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring that our records are parsed and stored correctly by implementing unit
    testing with pytest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this sprint, we use another research spike to explore how to define our database.
    Databases store and persist our data across application instances, while providing
    a way to query and ensure the integrity of that data. Here, we examine how to
    store our ACH files in a database. After initial analysis, we expand our APIs
    to store an ACH file in the database. Continuing along that track, we expand our
    ACH parser to store the individual fields as well. Finally, we wrap up the chapter
    by examining how storing ACH data affects our unit and load tests.
  prefs: []
  type: TYPE_NORMAL
- en: The introduction of a database is necessary in our project because an ACH file
    is a flat file. The current ACH system that Futuristic FinTech uses relies on
    flat files, and they can be challenging in many areas, including performance,
    querying, and data integrity. For instance, if a customer questions when a transaction
    was loaded, all the ACH files must be loaded and parsed to perform the search,
    which is time-consuming. Furthermore, keeping the parsed ACH files in memory becomes
    unfeasible given the number of records being dealt with.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Designing our database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a user uploads files to our ACH dashboard, we obviously need the ability
    to save them, or our system will not be very useful. The current ACH dashboard
    for Futuristic FinTech does not use a relational database. Instead, once uploaded,
    the files are parsed and stored in flat files (i.e., unstructured text files),
    which makes more sophisticated functionality a chore to implement. The ACH dashboard
    we are replacing uses only the filesystem to store the files. To provide more
    advanced processing, we want our ACH dashboard to be backed by a relational database,
    and we work through the initial review and implementation of various database
    designs and concepts to support our dashboard. Often, we need to have these types
    of research stories included in our sprints to examine different ways we may go
    about when implementing a desired feature.
  prefs: []
  type: TYPE_NORMAL
- en: There are at least a dozen different relational databases, and FinTech uses
    many of them. Our choice of database is often already determined by the database
    our company uses. We have seen Oracle, MySQL/MariaDB, and PostgreSQL used by FinTech—to
    name just a few. In our case, we have already set up an environment that enables
    Postgre­SQL to run in a Docker container, and we have seen how to build/initialize
    tables during startup and view our data through CloudBeaver. We can now start
    expanding our database to accommodate storing ACH files.
  prefs: []
  type: TYPE_NORMAL
- en: Databases do much more than just storing our data—they can help ensure the reliability
    and consistency of the data and relationships, a concept known as referential
    integrity. Referential integrity in our database is a fancy way to say that we
    will ensure our tables are appropriately related and the fields are correctly
    defined. For instance, recall that ACH is a fixed-length format, meaning the individual
    fields are fixed length. We may store the file ID modifier from the file header
    record as a `VARCHAR(1)` since it can only be a single uppercase character (or
    0 through 9). Similarly, we may want to store the total debit entry amount in
    the file control (i.e., file trailer) record as `MONEY` or `NUMERIC(12,2)`. The
    `NUMERIC(12,2)` defines a field with a precision of 12 significant digits and
    a scale of 2, which is the number of decimal digits. Whether you’ll use `MONEY`
    or `NUMERIC` is up to you, but we favor the `NUMERIC(12,2)` representation as
    it closely resembles the field definition.
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect of referential integrity is preventing orphan records. Remember
    that there is a hierarchy of records in an ACH file, that is, file control → batch
    header → entry records → etc. For example, an orphan record may occur if we have
    not defined our database carefully and we were to delete a batch header record.
    Once we delete a batch header, all the entry and addenda records (as well as the
    batch control record) belonging to that batch are no longer valid, and we should
    delete them. Likewise, deleting the file control record should delete all records
    associated with that file. In our relational database, we can achieve this by
    creating a `FOREIGN` `KEY`—which references the other table entries—and using
    `ON DELETE` `CASCADE`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our initial database will use the inherent benefits of a relational database
    by defining the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Primary key**s*—Unique identifiers for each record in a table'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Foreign key**s*—A link between two tables where a field (or fields) in one
    table refers to unique data (such as the primary key) in another table'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Constraints on the field**s*—For example, `NOT` `NULL` (to ensure data is
    present), `UNIQUE` (to ensure data is unique across all rows), and `DEFAULT` (to
    assign a default value if none was provided)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data integrit**y*—Attained by defining appropriate data types and sizes of
    fields'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll first look at storing ACH files with only Prearranged Payment and Deposit
    (PPD) data to keep things simpler. The PPD code is typically used for direct deposits
    and recurring payments such as payroll and pensions, so it is a widely used code
    that may frequently affect you (without your knowledge). To get an overview of
    what our database will look like, we again rely on PlantUML for a rendering of
    a proposed database structure.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1  PlantUML definition of our database
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Begins a PlantUML definition'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Defines a table for our diagram'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Defines fields within the table for our diagram'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Shows the relationships between keys in the table'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Ends the PlantUML definiton'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding definition is presented in figure 5.1, which shows how we may
    define our fields and the relationships between our tables. This is not an exhaustive
    list of the fields in tables, but it gives us an idea of how our tables will be
    related. The arrows represent the foreign key constraints present in the table.
    For instance, we can see how `ach_files_id` in the `ach_files` table field is
    defined as a Universally Unique Identifier (UUID) and references the `ach_files_id`
    from `ach_file_uploads`.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a computer  Description automatically generated](../Images/CH05_F01_Kardell.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1  Diagram showing relationships between our tables
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Figure 5.1 also conveys our desire to accomplish the following goals:'
  prefs: []
  type: TYPE_NORMAL
- en: Maintain the ordering of the records in our file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assume records will be unparsable and accommodate for that with unparsed records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain referential integrity by having parsed records refer to the unparsed
    ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the database structure seemingly meets those goals and the diagram gives
    us a visual guide to get started, there is always room for improvement. Regardless
    of whether this structure was provided to us by a subject matter expert (SME)
    or the interpretation of a database analyst (DBA), there may be opportunities
    to refine our work as we move through the project. With our diagram in hand, we
    should have an idea of how we want the database to look and can start working
    on it. However, we need to follow the general pattern of defining a test, building
    the table/fields, and finally building the API. When working with an SQL database,
    it is important to understand that companies will certainly have different approaches
    to managing their data. Some companies may extract the SQL portion away from the
    developer, either by using an object-relational mapper (ORM) such as SQLAlchemy
    or by rolling their own. An ORM helps simplify code by abstracting the database
    and providing benefits such as database agnostic, optimizations, and improved
    productivity.
  prefs: []
  type: TYPE_NORMAL
- en: Other companies may require you to write the SQL yourself because they like
    the level of control afforded by direct SQL. ORMs may make complex queries difficult
    or inefficient. Furthermore, troubleshooting queries and performance problems
    may also be more difficult to track down. First, we show here how to use straight
    SQL commands as we have been doing and then move to SQLAlchemy so that you can
    get acquainted with both approaches. There are always several factors that will
    be added, regardless of whether we use one approach over the other or combine
    them. Usually, the existing software will dictate our approach, so be careful
    not to stray too far from existing software standards initially as this could
    create a maintenance nightmare.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Using SQL directly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Chapter 3 explored parsing an ACH file and creating accompanying unit tests.
    We also created a simple API that accessed a database and returned results. So,
    we have all the pieces we need to store an ACH file in a database. Now we only
    have to put them together. In our first approach, we update our parser to store
    the ACH file in the database. We make the following assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: The database is always up and running.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We do not have any data that we want to preserve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are running our code from an IDE and not inside Docker.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, we are just working on the process of being able to parse and
    store the records in the database. We build out the previous `AchFileProcessor`
    to allow it to store ACH files. The next listing shows adding the necessary code
    to obtain a connection to the database. Since we are in a research sprint, we
    have hardcoded the database connection parameters such as username and password.
    Later, when we are certain that this is the right approach, we can start to abstract
    some of these hardcoded values so we have a more flexible configuration by using
    `environment` variables or secret management tools such as AWS Secrets Manager
    ([https://aws.amazon.com/secrets-manager/](https://aws.amazon.com/secrets-manager/))
    or HashiCorp Vault ([https://www.hashicorp.com/products/vault](https://www.hashicorp.com/products/vault)).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.2  Adding database fields
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Temporarily hardcodes our username and password'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 A hardcoded host and port, but we should consider parametrizing those as
    well.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 A new function to return a connection to the database'
  prefs: []
  type: TYPE_NORMAL
- en: That should provide us with the ability to connect to the database; however,
    we will need to use this code. Before we begin parsing the ACH file, we want to
    simply store the unparsed records in the database in what we may want to consider
    an extract
  prefs: []
  type: TYPE_NORMAL
- en: –load–transform (ELT) approach rather than directly parsing the records through
    an extract–transform–load (ETL) approach.
  prefs: []
  type: TYPE_NORMAL
- en: ELT vs. ETL
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are two approaches to handling data when processing it. They are usually
    discussed with regard to data warehousing and business intelligence, but ACH processing
    has unique challenges. ETL (extract–transform–load) is a traditional approach
    and may cross our mind first when processing data. For instance, we know that
    we want to parse each of these ACH records into their respective fields and store
    them in a database. However, dealing with data that may not be formatted correctly
    all the time is one of the challenges in ACH processing. With an ETL approach,
    this invalid data may cause processing to halt completely.
  prefs: []
  type: TYPE_NORMAL
- en: In an ELT (extract–load–transform) approach, the data is loaded and then transformed
    once it is going to be used. Typically, we see ELT when dealing with very large
    data and using data warehouses such as Snowflake or Redshift, where there is enough
    processing power to perform transformations on request.
  prefs: []
  type: TYPE_NORMAL
- en: So, why do we care about these approaches? Often, financial institutions will
    allow some leeway with the data and will not always reject a file if an error
    is considered recoverable. These conditions can vary from one financial institution
    to another and with their customers. For example, a count of records on a control
    file may be updated if incorrect rather than just rejecting the file or batch.
    Or, invalid data in a field may just be updated to spaces before loading it. While
    we can rely on logging and exceptions to keep track of changes (and we should
    be logging any such changes), we still want to keep a record of the original file
    so that a banker has the opportunity to review the data.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Adding records to the ach_files table
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will take the approach of loading our ACH file unaltered into the database
    before parsing out any records. Figure 5.2 illustrates a basic flow for this section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F02_Kardell.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2  Flowchart for section 5.2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Listing 5.3 shows an `ach_files` table. So, before parsing any records, let’s
    just add all our records to that database table.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.3 Simple  `ach_files` table
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Allows PostgreSQL to create UUIDs'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Primary UUID key'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The unparsed ACH record stored as is'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 A sequence number used to maintain ordering when retrieving data'
  prefs: []
  type: TYPE_NORMAL
- en: With this code, running `docker-compose` `down`, `docker-compose` `build`, and
    `docker-compose` `up` should allow our table to be created, and now we just need
    to update the code to write out some records!
  prefs: []
  type: TYPE_NORMAL
- en: Before we write out any records, we must ensure that the basics are working.
    So, we simply add the following code at the beginning of the existing parse routine.
    This code merely gets a connection to the database, obtains a cursor that can
    be used to execute SQL statements, and then closes both the cursor and connection.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.4  Testing the connection
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Calls our new get_db() function to connect to the database'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates a cursor used to execute SQL commands'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Closes the cursor'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Closes the connection'
  prefs: []
  type: TYPE_NORMAL
- en: If we were to run our code at this point, we may encounter several problems.
    First, our hardcoded `DATABASE_URL` `string` has a host of `postgres` and a port
    of `5432`. We are working within an IDE and not within Docker, so `postgres` is
    not the correct name to use. Indeed, we should see an error `psycopg.OperationalError:`
    `connection` `is bad:` `Unknown` `host` if we were to run the program. Instead,
    we want to use `localhost` because we are calling from the system hosting Docker.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we need to expose the port for our container. Our docker-compose.yml
    should look like the one in listing 5.5\. Without exposing the port, we would
    see an error similar to `psycopg.OperationalError:` `connection` `failed:` `:1),`
    `port` `5432` `failed:` `could` `not` `receive` `data` `from` `server:` `Socket`
    `is` `not` `connected`, which would hopefully tip us off that there is a problem
    with our port. Remember that these types of problems will always be popping up
    in our development, and we just need to retrace our steps to see what we missed.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.5  Updated docker-compose.yml file
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Exposes the standard PostgreSQL port'
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a basic connection working, we can start writing out the records.
    We could use a method similar to the one in listing 5.4 where we handle opening
    and closing the connections manually; however, having to remember to close the
    connections can be error-prone as we may forget to close them (we have seen files
    not being closed in a production environment for years only to spring up when
    moving to a new version of software or another vendor). We will use Python’s `with`
    statement as it automatically handles closing of various resources when a particular
    block of code is exited. In fact, we have already used it when reading our file,
    so we can simply expand on that.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.6  Updated parse function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates a connection as part of the existing with statement'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Initializes our sequence_number to zero'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Increments the sequence_number for each record'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Creates a cursor using the with statement, inserts, and commits the record'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rerun our unit test `test_parsing_ach_file.py`, which will run our sample
    file through our code, and then check CloudBeaver to verify the records have been
    added. This is a good start: we can store the records in our database, and it
    should not be much of a stretch to use a similar approach to go into our individual
    parsing functions and store the data there.'
  prefs: []
  type: TYPE_NORMAL
- en: One thing that we need to do is update our unit test to pull the data from the
    database instead of relying on data that is being returned, because our goal is
    to store all the data in the database and not return anything other than a status.
    For now, let’s take a look at updating the `pytest` to get the record count from
    the database.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.7  Updated `pytest`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 We pull the code from the AchFileProcessor that connects to the database.
    This is a temporary solution and we will eventually need to refactor the code
    again when more tests need to connect to the database.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Simple query to fetch the count of records stored in ach_files'
  prefs: []
  type: TYPE_NORMAL
- en: This test should work when Docker is first brought up, but subsequent tests
    will fail because the records are being added. So, our second iteration fails
    with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Our the third iteration fails with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We need a way to clear the database before each test. Ideally, we want to have
    a database that only exists for the length of the test, but first, let’s see how
    we may clear the table after each test. We can use a `pytest.fixture` that will
    perform any setup/teardown for our individual tests.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.8  `pytest.fixture` to setup and teardown our unit test
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Anything preceding yield will execute before the test.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 yield to allow the test to execute'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Anything following yield will execute after the test.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Gets a connection and cursor, then trunctuates the table to clear it'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Includes our fixture in the unit test'
  prefs: []
  type: TYPE_NORMAL
- en: With this code, our test will pass repeatedly because the table is cleared after
    each run. We have worked through our problem of having the database retain records
    between runs. Of course, we need to be careful. If our unit tests point to a database
    that is not a development server, we run the risk of wiping out needed data. For
    this reason, we may want to look at other options such as an in-memory database,
    mocking, or `pytest-postgresql`.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with databases
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A common way to work with databases for unit testing is to use an in-memory
    database such as SQLite or H2\. This allows us to run a database that exists entirely
    in memory and therefore can be isolated to our unit testing. The benefits are
    usually quick execution and the isolation of data. The drawback is that many times
    there may be functionality that does not exist between our production database
    and these in-memory databases, which can lead to problems when trying to create
    a unit test. For instance, SQLite has five data types to define data, while PostgreSQL
    has over 40 types. This is not to say that one is inherently better than the other—we
    just highlight the challenges we may face if our tables use the unsupported data
    types. We may end up fighting unnecessary battles to get our unit tests to run.
    That is why we should have additional tools and techniques we can use.
  prefs: []
  type: TYPE_NORMAL
- en: Mocking with a tool such as pgmock can also remove the need for a database in
    your test. In our scenario, we are actually testing whether the data has made
    it to the database, so mocking does not really provide a viable solution but something
    to look into later.
  prefs: []
  type: TYPE_NORMAL
- en: '`Pytest-postgresql` is a package that helps us manage `postgresql` connections
    for a `pytest`, which offers the best of both worlds by allowing us to connect
    to a test database or create/clear/modify tables as part of the test.'
  prefs: []
  type: TYPE_NORMAL
- en: As our project progresses, we will find that managing our data for tests and
    keeping the tests isolated becomes increasingly harder. In chapter 10, we eventually
    start incorporating the Testcontainers package to isolate our tests. This approach
    will also be beneficial as the infrastructure for the project matures and we begin
    running our tests as part of a build pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to store data is necessary in most applications, but as we have
    seen, it also adds more complexity to our design and coding. In this section,
    we started small by ensuring that we could connect to the database and by storing
    our unparsed records, which helped minimize the number of code changes required.
    As we move forward, the complexity of our application will gradually increase.
    As we work through parsing and storing our ACH files, we should also keep in mind
    that we will be retrieving and aggregating data from the database for our ACH
    dashboard. A way to easily store our ACH files may lend itself to one database
    structure, while the ACH dashboard may be better served by an alternative structure.
    Our job is to strike an acceptable balance between those two goals.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Storing the file header record
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ACH file header record should be the first ACH record we encounter in an
    ACH file. So, it makes sense that this is the first record we should explore by
    adding it to the database. We first show how we may approach it using ChatGPT
    and then how we go through a complete example with GitHub Copilot installed in
    our IDE.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Using generative AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generative AI can help with a lot of boilerplate code that can get repetitive
    after a while. Depending on your experience level, that boilerplate code may be
    new to you, and it may be beneficial to go through the process a few times. Once
    it becomes tedious, that may be a great sign we can start to lean on generative
    AI tools. For instance, we may prompt ChatGPT with the following general prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Prompt-Icon.png)** Please create a Postgres table to store
    a parsed Nacha File Header Record.'
  prefs: []
  type: TYPE_NORMAL
- en: We then received the `CREATE` `TABLE` statement from ChatGPT that does a great
    job incorporating `CHAR`, `VARCHAR`, and `NOT` `NULL`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.9  File header generated by ChatGPT 40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The file_creation_time is an optional field, so it could be NULL.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 These fields are also optional in the Nacha standard. Notice how ChatGPT
    used VARCHAR instead of CHAR since these fields may be padded with spaces.'
  prefs: []
  type: TYPE_NORMAL
- en: From personal experience, we prefer to use `VARCHAR` for most fields to avoid
    unnecessary padding. We have not encountered any meaningful performance effects
    from using `CHAR` versus `VARCHAR`. Storing ACH records may be one of the areas
    where using a `CHAR` could make sense since the fixed-length fields will not have
    any unnecessary space. `CHAR` can often be misused when declared too large, and
    any unused space is padded.
  prefs: []
  type: TYPE_NORMAL
- en: Out of curiosity, we asked ChatGPT if `CHAR` or `VARCHAR` was more performant
    in a `Postgres` database. After making a nice comparison between the two, it updated
    its example (without us asking it to) to use `VARCHAR` instead of `CHAR`! We were
    okay with this since our preference is to use `VARCHAR`.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Full example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ChatGPT can be of great help if we have a good idea of what we want to accomplish
    or if we don’t mind spending time configuring the prompt. Otherwise, we may want
    to work through the process aided by Copilot. Figure 5.3 shows the flow we will
    be using in this section. Here, we update the unit test at the end of our process
    because this is a relatively short development cycle. If we do not spend too much
    time coding before testing, we should be fine with working on the unit test after
    we do a little coding.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F03_Kardell.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3  Flow and associated code listings for this section
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Using our knowledge of storing the records, we should be able to store a parsed
    record in the database, and once that is complete, the other record formats should
    fall into place. Recall that we returned the parsed record as a dictionary, as
    shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.10  A dictionary with a parsed file header record
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 We use hardcoded values for the offsets because the offsets will not be
    changing, and we want the ability to quickly refer to fields in question when
    problems arise.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 We strip off extra spaces from fields as necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we want to take that record and store it in a database instead of simply
    returning it. We could keep this method of parsing and call it from another routine
    that was also interested in storing the data in a database. Another approach we
    may choose is to create dedicated parser classes or utility methods to handle
    parsing. Any of these approaches would help keep the code clean and reusable.
    For the sake of simplicity, we are going to take this routine and simply store
    the data in a database.
  prefs: []
  type: TYPE_NORMAL
- en: First, we want to create a table for storing ACH file headers in the database,
    and the next listing shows the sample table. At this point, we are going to keep
    it simple and only supply the fields we need to store the data without referencing
    any foreign keys or other basic constraints such as `field` `length` and `NOT`
    `NULL`. With Copilot installed, many of these field names were automatically populated,
    which saved us some time and effort.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.11  Table to store ACH file headers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Once we have the table, we can move on to updating our code. We chose to pass
    the connection object (previously created in listing 5.4) into our routine. We
    could have also stored the connection object as part of our class, but passing
    it in as a parameter will make it easier to unit test our routine. Different situations
    may require alternative approaches, so by no means is this the only way to accomplish
    our task.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.12  Passing the connection object
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the connection object, we can update the parsing routine to
    store the data.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.13  Updating the `_parse_file_header` routine
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 We add a connection parameter.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The return statement becomes a variable named file_header.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 We can execute directly on the connection to insert records.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 We used named variables as placeholders.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 The file_header variable passes our values in.'
  prefs: []
  type: TYPE_NORMAL
- en: We should now be able to insert our file header records into the database. Next,
    we can go back and update the unit test.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.14  Updated unit test example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates another get_db function that takes a row_factory parameter'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Uses the new parameter in the connect method'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Expected result is the same as before.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Leaves the current parse result in place'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Gets another connection specifying the result should be returned as a dictionary'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Returns the result; actual_result will be a dictionary.'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Removes ach_file_headers_id from the returned result'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Compares the two results'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we duplicated some of our code, such as the `get_db` method with a new
    parameter. It is important that as we move through the code, we keep an eye on
    this type of duplication and consider pulling out these methods into utility or
    helper classes whenever possible. The IDEs by JetBrains (and others) will often
    point to duplicated code and provide automated options to extract code into functions.
  prefs: []
  type: TYPE_NORMAL
- en: We also left the original result comparison in place because the method is still
    returning the records. As we continue to improve the project with additional functionality,
    we will likely remove this in favor of other return information (such as whether
    the record was parsed). We should now understand how the parsed ACH records can
    be stored in the database. For the ACH dashboard, to provide a meaningful interface
    with aggregated ACH data, we need to be able to store all the parsed ACH information
    in the database. In the next sections, we will explore how the code evolves as
    we work with it, not only expanding the features, but also addressing some nonfunctional
    requirements such as maintainability and testability.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Storing the rest of our ACH records
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have now created a database and stored data in two separate tables, so we
    should have enough of a framework to store the data for the remaining records
    in the database. The process is the same for all the remaining tables:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the table and restart Docker.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the parse routine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the unit test and verify your results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If embracing the test-driven development approach, we can swap the updating
    of the parse routine and the unit test. Either way, we should be working in short
    feedback cycles, allowing any errors to shake out early in the process and not
    after we have implemented a dozen tables. We also find opportunities for cleaning
    up the code and making it better overall.
  prefs: []
  type: TYPE_NORMAL
- en: Given the presumed structure of our database, this might be the perfect place
    to add a few tables. Check whether the database structure makes sense—is there
    anything that should be changed?
  prefs: []
  type: TYPE_NORMAL
- en: The following section will go over some of the lessons learned and insights
    gained from working through the rest of the tables that had to be added.
  prefs: []
  type: TYPE_NORMAL
- en: '5.4.1 Storing ACH files challenge: Lessons learned'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While adding the additional tables, we should have made some observations about
    the code and taken opportunities to make improvements to our code. Were there
    any that stood out? It is important to note and resolve possible pain points such
    as duplicated code, inefficient processing, confusing logic, and so forth. Sometimes,
    we can really get a sense of accomplishment by cleaning up code to be more straightforward
    and maintainable. We will go through some of the problems we encountered when
    adding additional database tables.
  prefs: []
  type: TYPE_NORMAL
- en: With our use of `psycopg3` (which is confusingly defined as `psycopg`) instead
    of `psycopg2`, we found that our generative AI tools tended not to take advantage
    of some newer enhancements. For instance, GitHub Copilot insisted on declaring
    `cursor` methods at first but seemed to learn our preference for using the connection,
    and after a while, it stopped offering them. This makes sense as Copilot is supposed
    to learn and adapt to our style and coding preferences. In recent releases of
    these tools, we have also seen the inclusion of retrieval-augmented generation
    (RAG), which helps large language models (LLMs) such as ChatGPT stay current with
    up-to-date information. Of course, we will have to see how they perform over the
    long term since a lot of the training data would not have used the newer features.
  prefs: []
  type: TYPE_NORMAL
- en: When creating tables, GitHub Copilot did a good job naming the columns, and
    they matched up with the documented field names in most instances, with only a
    few minor exceptions. This was helpful since we had already created the field
    names in our Python code to match the NACHA standards, and because we were able
    to use named parameters in our SQL queries, it was a breeze to transition from
    returning records to writing out to the database. Another feature that we were
    continually impressed with (although it did not always work) was the ability of
    Copilot to assist in writing our unit tests. We had created unit tests for each
    record that needed to be parsed, and when we broke it down into a dictionary for
    comparison purposes, Copilot lent a helpful hand and populated the individual
    fields with our test data. Even though it was off by a character or two, in some
    cases, it was definitely helpful overall.
  prefs: []
  type: TYPE_NORMAL
- en: As we went through and built our unit tests, we did eventually create a `SqlUtils`
    class to handle a few of the repetitive tasks. We started by moving the database
    parameters such as the user, password, and the URL to the routine. Then later,
    we expanded this to handle taking a parameter of a row factory so that we could
    return a dictionary of the values. We also created a routine to truncate the table
    so that repetitive tests had clean tables to work with. Therefore, our assertions
    would not fail when we expected a single record but retrieved multiple.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to removing duplication of code with the `SqlUtils` class, we also removed
    hard coding of the table names throughout the unit tests by creating a variable
    to hold the table name and using Python f-strings to create the SQL commands where
    necessary (but not for parameter passing). It is important to note that we still
    used parameterized queries whenever possible, even though we consider this internal
    tooling and would not expect malicious code to be entered. We did, however, consider
    the possibility of checking the passed table names against the database information
    schema to ensure they were valid. However, that seemed a bit overkill for internal
    tooling.
  prefs: []
  type: TYPE_NORMAL
- en: We were bitten several times by neglecting to put the setup/teardown `pytest.fixture`
    into the unit test methods as we were coding. This often led to errors when repeatedly
    running our tests since the database was not always in a clean state. It happened
    often enough that we considered creating a class hierarchy that would incorporate
    the clearing of the table so that we would save ourselves from ourselves. However,
    it felt too early in our process to add that, so we stayed away from it for the
    moment.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Storing exceptions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We should now have the basics of working with Python and PostgreSQL. Having
    successfully stored our sample.ach file in the database should boost our confidence.
    However, we should have also noted that our exceptions are not being stored in
    the database yet. We will want to keep track of those as well. It is common for
    files to be rejected for various reasons, and ACH processors need to be able to
    determine whether the file can be fixed manually or a new one should be requested
    from the originating party. Figure 5.4 shows the flow for this section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F04_Kardell.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4  Flow and associated code listings for this section
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The first order of business is to create the table. We start with a straightforward
    record that only contains the error description. As our project expands, we may
    find that we need to break exceptions out by record types, add references to specific
    records (to help maintain the integrity of the database should records be removed
    or updated), or implement several other improvements that will become more obvious
    as we enhance the project. However, those concerns are beyond the current scope
    of what we need to accomplish, so we will cross that bridge when we come to it
    in chapter 8.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.15  Simple exceptions table
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: With this table created, we can restart Docker and add a method to our AchFile­Processor.py
    to insert the record.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.16  Simple method to write the table
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Simple method passing a connection and an exception string'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Inserts the string into the table'
  prefs: []
  type: TYPE_NORMAL
- en: With this task completed, we can replace the array we used to hold our exceptions
    with a single call. We also need to update the various test cases that covered
    these exceptions, such as an incorrect addenda indicator, invalid record lengths,
    and ensuring the trace numbers are ascending. Updating these unit tests provides
    additional opportunities for creating maintainable code.
  prefs: []
  type: TYPE_NORMAL
- en: We removed the passing of our exceptions back to the calling routine, so code
    such as `records,` `exceptions` `=` `parser.parse(file_path)` becomes simply `records`
    `=` `parser.parse(file_path)`. However, this change made us immediately retrieve
    the exceptions because our unit tests were validating the number of exceptions
    and exception message text. We chose to add another method to `SqlUtils` to handle
    this.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.17  Method to retrieve ACH exceptions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Used with keyword to get a database connection'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Grabs all exceptions in a single execute command'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Returns a list of exceptions instead of tuples'
  prefs: []
  type: TYPE_NORMAL
- en: With the helper function in place, we can now return the exceptions with `exceptions
    =` `SqlUtils.get_exceptions()`, and the existing logic for the unit test should
    work without any modifications.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of storing our exceptions in a table, we now have multiple tables
    that need to be truncated in our unit tests. We could continue to call the `SqlUtils.truncate()`
    method with the new table. At this point, each test has a maximum of only two
    tables. However, we would prefer a way to clear all our tables because that will
    ensure our database is empty for each test. Listing 5.17 showed a truncate-all
    method that clears our tables. Obviously, this approach should be used with care
    as we are now truncating all the tables in the database. We could also have dropped
    and recreated the database; however, this gives us more control since each table
    is accessed individually. We have worked with projects that have expanded on this
    type of approach to examine tables for unexpected data, such as determining whether
    data has unexpectedly been written out to other tables. Of course, our needs will
    vary from one project to another, and it may be that you do not need or want to
    truncate the data using this method.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.18  SQL to truncate all tables
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates an anonymous block of code'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Declares a variable of type RECORD, a placeholder for a row that has no
    predefined structure'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Signifies the start of a new transaction'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Disables any triggers on the table'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Gathers a list of tables and truncates them'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Enables the triggers again'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Commits the current transaction'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we should have arrived at a similar point to where we were with
    our running code from chapter 3\. Recall we are able to perform some basic parsing
    on some simple ACH files, with the major change being that we are now storing
    the results of our hard work into a database. We can take a moment to congratulate
    ourselves, but only for a moment, because while we have the ability to parse the
    file, we do not have a way for a user to load a file. The next sections look at
    expanding our API to upload an ACH file and see the cascading effect it has on
    our database.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Uploading an ACH file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We may consider this our most important change yet. Of course, being able to
    parse a file is necessary, but it is often allowing interaction with the user
    that feels the most rewarding for many developers. This is possibly true because
    being able to work and interact with our project just feels like an accomplishment.
    And we feel that unit testing gives us much the same rewarding experience, which
    is why we like testing so much! Figure 5.5 shows the flow for this section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F05_Kardell.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5  Flow and associated code listings for this section
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We have previously built a basic API using hardcoded values. We will be taking
    that code and adding the ability to upload an ACH file. From there, we’ll be expanding
    the APIs to retrieve the data from our database instead of the hardcoded values.
    In the original ACH dashboard, one of the problems was a lack of control in processing
    ACH files.
  prefs: []
  type: TYPE_NORMAL
- en: Before creating a new API, let’s just ensure we can unit-test one of our existing
    endpoints with a hardcoded value. The next listing shows what that looks like.
    Copilot was able to produce the majority of this code as we typed, so it is just
    a matter of making sure it does what we intended.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.19  A unit test for FastAPI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Imports the TestClient needed for unit testing'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Imports our application'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Defines a client'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Defines a method for testing'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Calls our endpoint and saves the response'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Assert statements validate the response'
  prefs: []
  type: TYPE_NORMAL
- en: So, that was pretty easy. Now, we would like to work on being able to `POST`
    an ACH file to the backend, get a response that it has been uploaded, and then
    begin parsing the file. Why not parse the file and then return a response to the
    user? The processing of our ACH file may take some time, especially when we consider
    that it will eventually need to interact with other systems to execute tasks such
    as verifying accounts/balances, OFAC validation, fraud analysis, and any other
    tasks an ACH processor may want to perform. Rather than have a user wait, we can
    verify the file is uploaded and start a task to perform the parsing.
  prefs: []
  type: TYPE_NORMAL
- en: We already have an endpoint for posting a file, so we can add a unit test and
    then upload the file. Since the file is going to be uploaded using an `HTTP` `POST`
    request, it will be encoded using `multipart-formdata`. This means that we need
    the package `python-multipart` because this is a requirement for parsing requests
    encoded with that format. If we do not have it installed, we will receive a friendly
    reminder (in the form of an error).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.20  `python-multipart` not installed
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Creating a test for uploading should look something like the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.21  Unit test for uploading a file
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Opens the sample file'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Uses the client to POST the file'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Ensures we receive a 20 1 status'
  prefs: []
  type: TYPE_NORMAL
- en: We will also need to update the endpoint to receive the file. There is no need
    to return anything as we are just interested in the status code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.22  Updated endpoint to receive the file
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Additional imports are needed.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Defines the file as UploadFile'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Returns nothing; only the status code as necessary'
  prefs: []
  type: TYPE_NORMAL
- en: Now we should have a successful unit test, and the real work can begin. Uploading
    a file manually or through some automatic process should be the driving force
    behind all our ACH tables. We should store some file information such as filename,
    time uploaded, a file hash (for tracking of duplicate files), and whatever other
    information we may decide is needed. The UUID of this record should then be included
    in any child tables (all the previous tables we have just created). If we had
    more experience or worked through the problem in a different order (perhaps uploading
    a file first), we may have avoided having more rework, but we would have also
    had to introduce more database concepts to start with. A benefit of this approach
    is the actual rework required to incorporate the changes. Often, developers become
    paralyzed because of the fear of change. Large complex systems sometimes seem
    like a house of cards where one wrong move can cause everything to come tumbling
    down. Having good unit test coverage and confidence in the tests can go a long
    way to alleviate these fears. The fear of changing and improving working software
    eventually leads to code rot.
  prefs: []
  type: TYPE_NORMAL
- en: We will repurpose our `ach_files` table to be our main table that contains the
    upload information and rename `ach_files` to `ach_records` so that its only job
    is to store the unparsed ach records. The updated table definitions are in the
    following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.23  Updated table listings for `ach_files` and `ach_records`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The ach_files table is duplicated and repurposed to store the upload details.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The ach_files table is duplicated and repurposed to store the upload details.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The former ach_files will become ach_records.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Creates a foreign key named ach_files_id that references the ach_files_id
    from ach_files'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Removes the file_name as it is now stored in the ach_files table'
  prefs: []
  type: TYPE_NORMAL
- en: The new table `ach_records` uses the keywords `REFERENCES`, `ON` `DELETE`, and
    `ON` `UPDATE` to create a foreign key to the `ach_files` table. This feature allows
    the database to maintain its referential integrity. For instance, when we delete
    an ACH file out of `ach_files`, we do not want to go into every table and delete
    associated data. Instead, we want to define a relationship between the tables
    where all associated data is removed if we were to delete the `ach_file`. Once
    we have completed updating our tables, we can see this in action. This will also
    affect our tests. Once we have implemented referential integrity, we will need
    to ensure the `FOREIGN` `KEY` constraints are maintained.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we want to have a unit test write out records to the `ach_records`
    table. we need a valid `ach_files_id` (it must exist in the `ach_files` table).
    For this reason, we will likely be looking at expanding the `SqlUtils` class we
    had developed earlier to set up some generic records and make this easier. Maintaining
    referential integrity can mean extra work on our side when setting up for testing,
    but it is worth implementing. We have worked in systems with the benefit of referential
    integrity and have seen programs crash or loop because of incomplete relationships.
    Usually, a lack of referential integrity is apparent in a system where developers
    have written various utility programs to scan and fix the data (which we have
    had to do ourselves on more than one occasion).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we want to store the MD5 (or whichever algorithm you prefer) hash
    of the ACH file so we to identify duplicate files. First, we can get the hash
    from the command line using `Get-FileHash` `-Path` `".\sample.ach"` `-Algorithm`
    `MD5` `|` `Select-Object` `-ExpandProperty`, which in our case prints out `18F3C08227C3A60D418B3772213A94E3`.
    We’ll keep this info handy because we’ll be storing the hash as computed with
    Python in the database and expect it to be the same.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly enough, when coding the function, Copilot prompted us to use string
    interpolation for the SQL query, `f"INSERT` `INTO` `ach_files` `(file_name,` `md5_hash)
    VALUES` `('{file.filename}',` `'{md5_hash}'),` but we continue with our standard
    parameterized query because we want to maintain secure coding practices. Queries
    that use string interpolation are ripe for SQL injection ([https://mng.bz/QDNj](https://mng.bz/QDNj)).
    Our code for the `/api/v1/files` endpoint is shown in the next listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.24  Updated endpoint to upload ACH files
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Reads the uploaded file'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Uses hashlib to create a hash of the file'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Uses our newly created DbUtils to return a connection'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Simple SQL to INSERT a record into ach_files'
  prefs: []
  type: TYPE_NORMAL
- en: We created a simple `DbUtils` class to return a database connection. We imagine
    it will be used in multiple places in our project and thus extracted it into its
    own file.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.25  `DbUtils` for creating a database connection
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Hardcoded values to be removed later'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates and returns a connection'
  prefs: []
  type: TYPE_NORMAL
- en: With these changes, we should be able to run the unit test and then head over
    to our CloudBeaver UI and see the newly added record and manually compare the
    record hash. However, manually checking the record is no fun, so as a final piece,
    we should update our unit test to incorporate the same `SqlUtils`, as we have
    seen previously, and ensure that we have at least one record in our table. The
    following listing shows the updated unit test.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.26  Updated unit test for our upload file
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Required imports'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Defines our fixture to ensure all tables are truncated'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Includes the fixture in our unit test'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Ensures there is one row in the ach_files table'
  prefs: []
  type: TYPE_NORMAL
- en: Being able to upload a file successfully may not seem like a big deal, but we
    must remember that all other functionality of our ACH dashboard is based on this
    one piece. Functionality such as error checking and recovery, crediting/debiting
    accounts, fraud analysis, and much more is available to us now that we can upload
    ACH files. After completing this task, we can expand on the database structure.
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Storing records with integrity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned before, we want our database to have referential integrity, which
    can simplify navigation in database tools, and more importantly, have the ability
    to prevent dangling records. We have seen that databases without referential integrity
    require maintenance programs that run periodically to clean up those dangling
    records, as well as infinite loops and program crashes resulting from incomplete
    data. A great benefit of using relational databases is their support for data
    integrity. We highly recommend keeping it in mind when designing databases.
  prefs: []
  type: TYPE_NORMAL
- en: In our examples so far, we have defined a `PRIMARY` `KEY` and how to update
    the `ach_records` table. In listing 5.23, we added our first `FOREIGN` `KEY` by
    using the `REFERENCES` keyword, which starts us on the path of maintaining referential
    integrity. We continue to update the remaining tables and code with foreign keys
    and review the effects of using them on our tables, code, and unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: If we try running our unit tests at this point, we will see errors referring
    to an `UndefinedColumn` since we updated our table definitions in listing 5.23\.
    This is not a bad thing. The fact that we have built unit tests allows us to refactor
    our code with confidence. We start with diving into our `test_record_count` unit
    test (from listing 5.7) to resolve our error of `psycopg.errors.UndefinedColumn:`
    `column` `"unparsed_record" of` `relation` `"ach_files"` `does` `not` `exist`.
    The offending code is shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.27  Invalid column names
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Depending on your IDE and setup, this statement may already be flagged. In PyCharm,
    we have defined a connection to the database, so the fields `unparsed_record`
    and `sequence_number` are flagged with an `Unable` `to` `resolve` `column` error.
    If your IDE does not have that capability, then the stack trace also shows the
    line number. Given this information, we notice that the `INSERT` statement is
    pointing to the wrong table as we had renamed `ach_files` to `ach_records`. Changing
    that and rerunning the test gives a new error `psycopg.errors.NotNullViolation:`
    `null` `value` `in` `column "ach_files_id"` `of` `relation` `"ach_records"` `violates`
    `not-null` `constraint`. We defined `ach_files_id` as a foreign key, and it cannot
    be null—in fact, it must point to a valid record in the `ach_files` table. Recall
    that our API will create this record when a file is uploaded. So, it is plausible
    that the `AchFileProcessor` will now need to be called with both the filename
    being parsed and a valid `ach_files_id`. We can update the unit test so that the
    parse routine is called with this ID. The `parser.parse(file_path)` will need
    to become `parser.parse(ach_files_id,` `file_path)`, but it is not enough to define
    `ach_files_id`. We need a way to create a valid `ach_files_id` because we have
    to maintain database integrity. We could drop the constraints during testing,
    which may be an option if our unit tests were primarily concerned with functionality.
    However, in this case, we would like to maintain the constraint. So, we will need
    to create a function to insert the record, then take a step back and consider
    our path forward as there may be a few different routes we want to take.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that we already coded an `INSERT` statement to insert the `ach_files`
    record into our API. We could duplicate that query in our unit test and then grab
    that inserted value. That would work, but we will have duplicated code, and we
    want to try to avoid that whenever possible. We could add a routine to `SqlUtils`
    to handle this task for us, which other methods would have access to because other
    unit tests may also need the functionality. However, `SqlUtils` is meant to assist
    us in our tests only, and we have already seen that we will need this functionality
    elsewhere. Perhaps we should create a utility class for the `AchFileProcessor`
    to insert our desired record, which would also allow us to refactor the existing
    SQL queries out of the parsing routines. There is no definitive answer. Depending
    on the project, our needs, programming style, and experience, we may see other
    alternatives or have preferences on how to approach this problem. In this case,
    we believe that using Pydantic is the best way to move forward.
  prefs: []
  type: TYPE_NORMAL
- en: 5.8 Using Pydantic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we ran into an problem of how to handle inserting a
    record to make our unit test work. While there are several approaches we could
    take, Pydantic will help further refactoring and expansion of our project. We
    had introduced Pydantic previously to help document our APIs in chapter 4, so
    we know that it already has some benefits. Another benefit is that we will be
    able to define our tables and fields in a way that allows developers not to have
    to remember which fields are present. In other words, we start abstracting the
    SQL from the parsing logic. Let’s start applying that with our `ach_files` table
    so that we can create a record and return a valid record ID, which is all we really
    wanted to do to get around our first unit test problem.
  prefs: []
  type: TYPE_NORMAL
- en: Why is there no object-relational model framework?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An object-relational model (ORM) has many benefits and is widely used in many
    different industries. We are holding off on incorporating an ORM such as SQLAlchemy
    for now because we want to ensure the reader is exposed directly to SQL in case
    those skills need brushing up on.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we will show how to include an ORM into the project, so hang in there.
    Of course, if you are familiar with ORMs, you can dive right in and start using
    them.
  prefs: []
  type: TYPE_NORMAL
- en: We first create a Pydantic model for our table, as shown in the following listing.
    This simple model provides a basic layout for modeling the data that’ll be written
    to our `ach_files` table.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.28  Our Pydantic model for `ach_files`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Necessary import statements'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The class extends the Pydantic BaseModel.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Our field definitions; note the Optional keyword for fields (such as ID)
    that our database will supply.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can go ahead and define a class that will provide some basic Create,
    Read, Update, and Delete (CRUD) methods to handle working with the database table.
    The following listing shows the `AchFileSql` class we will create to wrap the
    logic for dealing with our `ach_files` database table.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.29  `AchFileSql` class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Required imports for the class'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates a function to insert the record and return the UUID'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Uses the RETURNING keyword to return the ID of the newly inserted record'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Uses model_dump to create a dictionary that references the fields'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Creates a function to return a specified record'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 By using the row_factory of class_row, we can return the record directly.'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 If nothing is found, raises an error'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we create a unit test to verify our newly created classes. We create
    this unit test as a class just to show another way to help organize our tests.
    We also introduce the `autouse` option on our `pytest` fixtures so we do not have
    to include them in every method.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.30  Unit testing our new classes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Required imports'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Our fixture now uses autouse.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Truncates the tables when finished'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Inserts the record'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Immediately returns the record'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Assert statements validate our results'
  prefs: []
  type: TYPE_NORMAL
- en: This is a great start to taking our project to the next level. You may be wondering
    why we did not write our test first in a test-driven development manner. Sometimes,
    it is easier to take a test-later approach, especially when demonstrating new
    concepts. Again, it is not so about writing the test but about working in a short
    cycle. So, as soon as we got something to test, we started testing it.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that with Pydantic, we get field validation and the ability to document
    our APIs. We will be looking at that later, but for now, we should continue refactoring
    our code to take advantage of Pydantic. Once we have the code refactored using
    Pydantic and our unit tests passing, we can move along to handling APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 5.9 Lessons learned
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process of including Pydantic and separating our code required us to refactor
    not only the ach_file_processor.py but our unit tests as well. Refactoring allowed
    us to improve our code in both areas and obtain code that is cleaner and easier
    to test. Unfortunately, we also ran into some problems when we finished creating
    a database structure that followed the original specification from figure 5.1\.
    Did you notice any problems with the structure before refactoring the code?
  prefs: []
  type: TYPE_NORMAL
- en: Inserting foreign keys into that database structure revealed a problem with
    maintaining data integrity in the `ach_files` table. While deleting a record from
    those child tables would behave correctly, the `ach_files` table did not get rid
    of all the records we desired. For instance, if we were to delete a batch, we
    would expect the associated type 6–8 records to be removed as well, and this cannot
    happen in the current structure.
  prefs: []
  type: TYPE_NORMAL
- en: This situation is not uncommon. Often, when given specs for a project—depending
    on the experience of the project stakeholders and the time allowed for researching
    areas of the project—it may not be possible to flush out all the design details.
    In a scaled Agile context, we may consider this project an enabler—more specifically,
    an exploration enabler. Exploration enablers help explore prospective solutions
    and include activities such as research and prototyping. If these are not completed
    sufficiently before we got a project, we may run into problems we had here. There
    are several alternatives to handle the situation, and the right answer will likely
    depend on the reevaluation of project requirements.
  prefs: []
  type: TYPE_NORMAL
- en: To recap the project requirements, we wanted to find a solution that
  prefs: []
  type: TYPE_NORMAL
- en: Provided both ETL and ELT options when processing a database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provided data integrity over the entire file. As the ACH file is a hierarchy,
    there are multiple scenarios that require additional records to be cleaned up
    when a file, batch, or entry are deleted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s propose a few different options we may need to present to the business
    stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: First, we could ditch parsing the individual records into the database and just
    store the unparsed records. This would certainly simplify the database design
    as we only have one table to deal with. Of course, this approach limits the usefulness
    of having a relational database and would require additional application code
    to handle the data integrity we mentioned earlier. For instance, if the user wanted
    to delete a batch, we would have to ensure that all the records were deleted by
    the application code, which would expand and complicate our application code.
  prefs: []
  type: TYPE_NORMAL
- en: Second, we could ditch storing unparsed records in the table. This could potentially
    be a good solution if we determine we do not need the unparsed records. Of course,
    it also means that our file needs to conform to the database constraints, and
    the business has already required the unparsed records to be retained in the event
    of invalid data (such as non-numeric data in a numeric field) that may cause a
    parsed record to be rejected from the database. This seems to be a hard requirement
    to get around.
  prefs: []
  type: TYPE_NORMAL
- en: Third, we could investigate setting up a database trigger to delete the records.
    Database triggers are code that can be automatically executed when certain events
    occur in the database. We may be able to create triggers in the parsed record
    tables that execute when a record is deleted to also eliminate the associated
    unparsed record. That does not sound like much fun though.
  prefs: []
  type: TYPE_NORMAL
- en: We opted for another route to address this problem—reorganizing the table into
    the structure where we used an unparsed record table for each type. This required
    substantial refactoring of the tables and associated application code, but for
    us, it made the most sense if we wanted to maintain the requirements for keeping
    unparsed and parsed records. The updated database diagram is shown in figure 5.6,
    along with a reference to the original layout from figure 5.1.
  prefs: []
  type: TYPE_NORMAL
- en: This structure gave us the data integrity we were originally looking for, but
    we now have to find a way to view all the unparsed records. To accomplish this,
    we create a database view. A database view is a virtual table that is the result
    of a stored query. By using a view, we can save ourselves and others who need
    to use our database the tedious task of tying the unparsed data together. The
    next listing shows the created database view.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.31  Creating a database view
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '#1 UNION ALL will combine the results from the successive SELECT statements.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The USING statements allow for a more concise syntax instead of ON table.field
    =.table.field. It can be used in PostgreSQL when the joining fields share the
    same name among tables.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F06_Kardell.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6  Updated table diagram
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This code allows us to view the entire ACH file as it was uploaded to the system,
    without any manipulation from our processing. Having a way to view the raw file
    allows us to present users with options to view the entire ACH file contents,
    as well as exception records to help them troubleshoot any problems.
  prefs: []
  type: TYPE_NORMAL
- en: 5.10 Coding changes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have settled on this database structure, it was a matter of writing
    any additional unit tests and getting the existing ones updated to run correctly.
    Let’s look at some of the changes made to support this structure.
  prefs: []
  type: TYPE_NORMAL
- en: 5.10.1 Creating Pydantic schema for the unparsed ACH records
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All the unparsed records share the `unparsed_record` and sequence number fields.
    Therefore, this is a good opportunity to create a class structure that will inherit
    those fields so that we do not have to type them every time. We create an `AchRecordBaseSchema`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.32  Base schema for our ACH records
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Importing ABC allows us to create an abstract class.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The BaseModel is required for Pydantic.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Our class inherits from both ABC and BaseModel.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Fields that will be present in all ACH record classes'
  prefs: []
  type: TYPE_NORMAL
- en: With that schema, we can define each record type, as shown in the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.33  ACH record schema
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Required imports'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 This is a Pydantic class by virtue of being a subclass of AchRecordBaseSchema,
    which was a subclass of BaseModel.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 We have an ID field that is marked as optional since it will be assigned
    by the database.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 The ach_files_id field is passed in and is not optional as it is a foreign
    key to the file that was uploaded.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.10.2 Creating Pydantic schema for the parsed ACH records
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Pydantic definitions for each of the parsed records are less interesting
    at this point as we just inherit from the Pydantic `BaseModel` record and define
    the necessary fields. We will expand on the fields for constraints, validation,
    and documentation later. For now, we just keep them simple.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.34  Pydantic schema for an ACH batch header record
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 5.10.3 Unit test changes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As part of code refactoring, we ensured that our tests were cleaned up as well
    (listing 5.35). First, we updated our `setup_teardown_method` to set `autouse`
    to `true` and then ensure the `SqlUtils.truncate_all` method was executed first.
    We may have previously chosen to clear the tables after the tests were run, which
    is a good practice that cleans up any data from our test. However, it also has
    the unfortunate side effect of cleaning up the data when tests fail, which is
    not very helpful when we want to examine the database after a test. To make debugging
    and troubleshooting easier, we decided to clear the data before the test. This
    also ensures the database is ready as we do not have to rely on a previous test
    to clean up after itself. Adding the `autouse` parameters means we no longer need
    to pass the fixture to our tests. We also used `truncate_all` instead of a specific
    table since we now have multiple tables being used.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.35  Updated `pytest` fixture
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 5.11 Design and different approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It was a fair amount of rework to add foreign keys to the database, so we may
    be thinking about ways to minimize that work or why we did not just add them in
    the beginning. As you saw, we dove into parsing the ACH file, which we accomplished,
    and then moved on to supporting some additional functionality that required a
    lot of rework. This was partly because we wanted to show some ACH basics before
    getting into overall functionality and additional database knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: However, let’s consider whether we approached the problem from a functionality
    standpoint and whether the user was going to need these files uploaded. Had we
    then started with the `ach_files` table and associated endpoint, we could have
    incorporated foreign keys from the beginning. Assuming you had knowledge and experience
    working with ACH files and APIs, it would certainly have been a valid approach.
    Then we could have proceeded in much the same fashion, with the exception that
    we may have had a better-designed database from the start.
  prefs: []
  type: TYPE_NORMAL
- en: This just goes to show that initial design can be important as well as the approach
    to the problem you take. We spoke earlier about enabler stories, or what is also
    known as research spikes. They reduce risk and help us understand requirements
    and otherwise gain a better understanding of the tasks that will need to be performed.
    We cannot stress the importance of these types of stories, especially when working
    with large complex systems. We can always expect some rework to be required when
    we get into a project, either due to unidentified requirements or changes in scope
    that could not be accounted for. Hopefully, research spikes help minimize those
    instances, but it can often be difficult for a business to devote time to something
    they do not immediately see a benefit from. This will obviously become a problem
    because problems that are flushed out by enabler stories will not be identified
    and will go unnoticed in PI planning. The approach we may end up taking may require
    rework that could have been identified earlier, and therefore, our story points
    could have been allocated correctly.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen numerous examples where a project is seemingly complete, only to
    have a subject matter expert point out something that was missed in a system demo
    and requires rework. It is important to remember that we will have to deal with
    these types of situations at times as it may be often tempting to take an easier
    way out. In such situations, we will need to judge the effects of the change on
    the project timeline, amount of rework, risk involved, technical debt incurred,
    and so forth. For instance, the business may have decided that we did not need
    referential integrity of the database and that it would be faster to write a program
    that could be manually run to check for dangling records/missing relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter shows how to create a database that could store our unaltered records,
    as well as a parsed version of the record that will be beneficial when necessary
    to provide details on the loaded ACH files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We saw the implications of adding referential integrity to our database late
    in the process and the need to rework both our code and unit tests. When a feature
    requires rework to be done correctly, it can often be put on the back burner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite the extra work to implement these types of features, it is important
    to champion them both to the team members and management so they are not left
    by the wayside.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining databases is crucial for data storage, querying, and integrity in applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ACH files face challenges with performance, querying, and data integrity when
    handled as flat files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relational databases offer advantages, such as primary and foreign keys, constraints,
    and data integrity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing referential integrity prevents orphan records and ensures database
    consistency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A research spike (enabler story) is beneficial when evaluating database design
    and various implementation approaches.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ELT and ETL offer different benefits for processing ACH files and handling errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pydantic helps in modeling database tables, abstracting SQL, and enhancing documentation
    and validation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uploading files and integrating APIs are foundational for expanding functionality
    in an ACH system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data and referential integrity are critical for relational databases to prevent
    errors and improve reliability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous testing, refactoring, and revisiting initial design choices help
    maintain and improve database performance and structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
