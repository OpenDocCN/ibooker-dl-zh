- en: Chapter 10\. AI Engineering Architecture and User Feedback
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 10 章\. 人工智能工程架构和用户反馈
- en: So far, this book has covered a wide range of techniques to adapt foundation
    models to specific applications. This chapter will discuss how to bring these
    techniques together to build successful products.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这本书已经涵盖了一系列将基础模型适应特定应用的技术。本章将讨论如何将这些技术结合起来构建成功的产品。
- en: Given the wide range of AI engineering techniques and tools available, selecting
    the right ones can feel overwhelming. To simplify this process, this chapter takes
    a gradual approach. It starts with the simplest architecture for a foundation
    model application, highlights the challenges of that architecture, and gradually
    adds components to address them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到可用的广泛人工智能工程技术和工具，选择正确的一个可能会让人感到不知所措。为了简化这个过程，本章采用逐步的方法。它从基础模型应用的最简单架构开始，强调该架构的挑战，并逐步添加组件来解决这些问题。
- en: We can spend eternity reasoning about how to build a successful application,
    but the only way to find out if an application actually achieves its goal is to
    put it in front of users. User feedback has always been invaluable for guiding
    product development, but for AI applications, user feedback has an even more crucial
    role as a data source for improving models. The conversational interface makes
    it easier for users to give feedback but harder for developers to extract signals.
    This chapter will discuss different types of conversational AI feedback and how
    to design a system to collect the right feedback without hurting user experience.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以永远地推理如何构建一个成功的应用程序，但唯一确定一个应用程序是否真正实现其目标的方法是将它展示给用户。用户反馈始终是指导产品开发的无价之宝，但对于人工智能应用程序来说，用户反馈在作为改进模型的数据源方面起着更加关键的作用。对话界面使得用户更容易提供反馈，但对于开发者来说，提取信号则更加困难。本章将讨论不同类型的对话人工智能反馈以及如何设计一个系统来收集正确的反馈，同时不损害用户体验。
- en: AI Engineering Architecture
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能工程架构
- en: A full-fledged AI architecture can be complex. This section follows the process
    that a team might follow in production, starting with the simplest architecture
    and progressively adding more components. Despite the diversity of AI applications,
    they share many common components. The architecture proposed here has been validated
    at multiple companies to be general for a wide range of applications, but certain
    applications might deviate.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完整的AI架构可能非常复杂。本节将按照团队在生产中可能遵循的过程进行说明，从最简单的架构开始，并逐步添加更多组件。尽管人工智能应用程序种类繁多，但它们共享许多共同组件。这里提出的架构已在多家公司得到验证，适用于广泛的应用，但某些应用程序可能会有所偏离。
- en: In its simplest form, your application receives a query and sends it to the
    model. The model generates a response, which is returned to the user, as shown
    in [Figure 10-1](#ch10_figure_1_1730130985262508). There is no context augmentation,
    no guardrails, and no optimization. The *Model API* box refers to both third-party
    APIs (e.g., OpenAI, Google, Anthropic) and self-hosted models. Building an inference
    server for self-hosted models is discussed in [Chapter 9](ch09.html#ch09_inference_optimization_1730130963006301).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单形式中，您的应用程序接收一个查询并将其发送到模型。模型生成一个响应，并将其返回给用户，如图 [图 10-1](#ch10_figure_1_1730130985262508)
    所示。没有上下文增强，没有安全护栏，也没有优化。"模型API" 框架既指第三方API（例如，OpenAI、Google、Anthropic）也指自托管模型。关于为自托管模型构建推理服务器的讨论请见
    [第 9 章](ch09.html#ch09_inference_optimization_1730130963006301)。
- en: '![A diagram of a model  Description automatically generated](assets/aien_1001.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![模型图  描述自动生成](assets/aien_1001.png)'
- en: Figure 10-1\. The simplest architecture for running an AI application.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. 运行人工智能应用程序的最简单架构。
- en: 'From this simple architecture, you can add more components as needs arise.
    The process might look as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个简单的架构开始，根据需要可以添加更多组件。这个过程可能如下所示：
- en: Enhance context input into a model by giving the model access to external data
    sources and tools for information gathering.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过让模型访问外部数据源和信息收集工具来增强模型中的上下文输入。
- en: Put in guardrails to protect your system and your users.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为保护系统和用户，请设置安全护栏。
- en: Add model router and gateway to support complex pipelines and add more security.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加模型路由器和网关以支持复杂管道并增加安全性。
- en: Optimize for latency and costs with caching.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过缓存优化延迟和成本。
- en: Add complex logic and write actions to maximize your system’s capabilities.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加复杂逻辑并编写操作以最大化您系统的能力。
- en: This chapter follows the progression I commonly see in production. However,
    everyone’s needs are different. You should follow the order that makes the most
    sense for your application.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章遵循我在生产中常见的发展过程。然而，每个人的需求都是不同的。你应该遵循对你应用程序最有意义的顺序。
- en: Monitoring and observability, which are integral to any application for quality
    control and performance improvement, will be discussed at the end of this process.
    Orchestration, chaining all these components together, will be discussed after
    that.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 监控和可观察性，这对于任何应用程序的质量控制和性能改进都是至关重要的，将在这一过程的最后进行讨论。编排，将所有这些组件连接在一起，将在之后进行讨论。
- en: Step 1\. Enhance Context
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步：增强上下文
- en: The initial expansion of a platform usually involves adding mechanisms to allow
    the system to construct the relevant context needed by the model to answer each
    query. As discussed in [Chapter 6](ch06.html#ch06_rag_and_agents_1730157386571386),
    context can be constructed through various retrieval mechanisms, including text
    retrieval, image retrieval, and tabular data retrieval. Context can also be augmented
    using tools that allow the model to automatically gather information through APIs
    such as web search, news, weather, events, etc.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 平台最初的扩展通常涉及添加机制，以便系统构建模型回答每个查询所需的相关上下文。如第 6 章所述（ch06.html#ch06_rag_and_agents_1730157386571386），上下文可以通过各种检索机制构建，包括文本检索、图像检索和表格数据检索。上下文还可以通过允许模型通过
    API（如网络搜索、新闻、天气、事件等）自动收集信息的工具进行增强。
- en: '*Context construction is like feature engineering for foundation models.* It
    gives the model the necessary information to produce an output. Due to its central
    role in a system’s output quality, context construction is almost universally
    supported by model API providers. For example, providers like OpenAI, Claude,
    and Gemini allow users to upload files and allow their models to use tools.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*上下文构建类似于基础模型的特征工程。* 它为模型提供必要的信息以生成输出。由于其在对系统输出质量中的核心作用，上下文构建几乎被所有模型 API 提供商所支持。例如，像
    OpenAI、Claude 和 Gemini 这样的提供商允许用户上传文件，并允许他们的模型使用工具。'
- en: However, just like models differ in their capabilities, these providers differ
    in their context construction support. For example, they might have limitations
    on what types of documents and how many you can upload. A specialized RAG solution
    might let you upload as many documents as your vector database can accommodate,
    but a generic model API might let you upload only a small number of documents.
    Different frameworks also differ in their retrieval algorithms and other retrieval
    configurations, like chunk sizes. Similarly, for tool use, solutions also differ
    in the types of tools they support and the modes of execution, such as whether
    they support parallel function execution or long-running jobs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如模型在能力上有所不同一样，这些提供商在上下文构建支持方面也有所不同。例如，它们可能对可以上传的文档类型和数量有限制。一个专门的 RAG 解决方案可能允许你上传尽可能多的文档，只要你的向量数据库能够容纳，但一个通用的模型
    API 可能只允许你上传少量文档。不同的框架在检索算法和其他检索配置（如块大小）方面也有所不同。同样，对于工具的使用，解决方案在支持的工具类型和执行模式（例如是否支持并行函数执行或长时间运行的任务）方面也有所不同。
- en: With context construction, the architecture now looks like [Figure 10-2](#ch10_figure_2_1730130985262560).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上下文构建，现在的架构看起来就像[图 10-2](#ch10_figure_2_1730130985262560)。
- en: '![A diagram of a database  Description automatically generated](assets/aien_1002.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![数据库的图示  描述由自动生成](assets/aien_1002.png)'
- en: Figure 10-2\. A platform architecture with context construction.
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 带有上下文构建的平台架构。
- en: Step 2\. Put in Guardrails
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步：设置安全网
- en: Guardrails help mitigate risks and protect you and your users. They should be
    placed whenever there are exposures to risks. In general, they can be categorized
    into guardrails around inputs and outputs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 安全网有助于降低风险并保护你和你的用户。应该在存在风险暴露的地方设置安全网。一般来说，它们可以分为围绕输入和输出的安全网。
- en: Input guardrails
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入安全网
- en: 'Input guardrails typically protect against two types of risks: leaking private
    information to external APIs and executing bad prompts that compromise your system.
    [Chapter 5](ch05.html#ch05a_prompt_engineering_1730156991195551) discusses many
    different ways attackers can exploit an application through prompt hacks and how
    to defend your application against them. While you can mitigate risks, they can
    never be fully eliminated, due to the inherent nature of how models generate responses
    as well as unavoidable human failures.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 输出防护栏通常保护两种类型的风险：向外部API泄露私人信息以及执行可能损害您系统的恶意提示。[第5章](ch05.html#ch05a_prompt_engineering_1730156991195551)讨论了攻击者如何通过提示攻击利用应用程序以及如何防御这些攻击。虽然您可以减轻风险，但由于模型生成响应的本性和不可避免的人类错误，它们永远无法完全消除。
- en: 'Leaking private information to external APIs is a risk specific to using external
    model APIs when you need to send your data outside your organization. This might
    happen for many reasons, including the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 向外部API泄露私人信息是使用外部模型API时特有的风险，当您需要将数据发送到组织外部时可能会发生。这可能是由于许多原因，包括以下：
- en: An employee copies the company’s secret or a user’s private information into
    a prompt and sends it to a third-party API.^([1](ch10.html#id1761))
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 员工将公司的机密信息或用户的私人信息复制到提示中，并将其发送到第三方API。[^([1](ch10.html#id1761))]
- en: An application developer puts the company’s internal policies and data into
    the application’s system prompt.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序开发者将公司的内部政策和数据放入应用程序的系统提示中。
- en: A tool retrieves private information from an internal database and adds it to
    the context.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具从内部数据库检索私人信息并将其添加到上下文中。
- en: 'There’s no airtight way to eliminate potential leaks when using third-party
    APIs. However, you can mitigate them with guardrails. You can use one of the many
    available tools that automatically detect sensitive data. What sensitive data
    to detect is specified by you. Common sensitive data classes are the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用第三方API时，没有绝对的方法可以消除潜在的泄露。然而，您可以使用防护栏来减轻它们。您可以使用许多可用的工具之一来自动检测敏感数据。要检测的敏感数据由您指定。常见的敏感数据类别如下：
- en: Personal information (ID numbers, phone numbers, bank accounts)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个人信息（身份证号码、电话号码、银行账户）
- en: Human faces
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人脸
- en: Specific keywords and phrases associated with the company’s intellectual property
    or privileged information
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与公司知识产权或机密信息相关的特定关键词和短语
- en: 'Many sensitive data detection tools use AI to identify potentially sensitive
    information, such as determining if a string resembles a valid home address. If
    a query is found to contain sensitive information, you have two options: block
    the entire query or remove the sensitive information from it. For instance, you
    can mask a user’s phone number with the placeholder [PHONE NUMBER]. If the generated
    response contains this placeholder, use a PII reverse dictionary that maps this
    placeholder to the original information so that you can unmask it, as shown in
    [Figure 10-3](#ch10_figure_3_1730130985262586).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 许多敏感数据检测工具使用AI来识别可能敏感的信息，例如确定一个字符串是否类似于有效的家庭地址。如果发现查询中包含敏感信息，你有两个选择：阻止整个查询或从其中删除敏感信息。例如，你可以使用占位符[PHONE
    NUMBER]来屏蔽用户的电话号码。如果生成的响应包含此占位符，请使用将此占位符映射到原始信息的PII反向字典，以便您可以取消屏蔽它，如图[图10-3](#ch10_figure_3_1730130985262586)所示。
- en: '![A screenshot of a computer error  Description automatically generated](assets/aien_1003.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![计算机错误截图  自动生成的描述](assets/aien_1003.png)'
- en: Figure 10-3\. An example of masking and unmasking PII information using a reverse
    PII map to avoid sending it to external APIs.
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-3\. 使用反向PII映射屏蔽和取消屏蔽PII信息以避免将其发送到外部API的示例。
- en: Output guardrails
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出防护栏
- en: 'A model can fail in many different ways. Output guardrails have two main functions:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型可能会以许多不同的方式失败。输出防护栏有两个主要功能：
- en: Catch output failures
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕获输出失败
- en: Specify the policy to handle different failure modes
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定处理不同故障模式的策略
- en: 'To catch outputs that fail to meet your standards, you need to understand what
    failures look like. The easiest failure to detect is when a model returns an empty
    response when it shouldn’t.^([2](ch10.html#id1763)) Failures look different for
    different applications. Here are some common failures in the two main categories:
    quality and security. Quality failures are discussed in [Chapter 4](ch04.html#ch04_evaluate_ai_systems_1730130866187863),
    and security failures are discussed in [Chapter 5](ch05.html#ch05a_prompt_engineering_1730156991195551).
    I’ll quickly mention a few of these failures as a recap:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉不符合你标准的输出，你需要了解失败的样子。最容易检测到的失败是当模型在不应返回空响应时返回空响应.^([2](ch10.html#id1763))
    对于不同的应用程序，失败的表现各不相同。以下是在两个主要类别：质量和安全中的常见失败：质量失败在[第4章](ch04.html#ch04_evaluate_ai_systems_1730130866187863)中讨论，安全失败在[第5章](ch05.html#ch05a_prompt_engineering_1730156991195551)中讨论。我将简要提及这些失败作为回顾：
- en: Quality
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质量
- en: Malformatted responses that don’t follow the expected output format. For example,
    the application expects JSON, and the model generates invalid JSON.
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格式不正确的回答，不符合预期的输出格式。例如，应用程序期望JSON格式，而模型生成了无效的JSON。
- en: Factually inconsistent responses hallucinated by the model.
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型产生的与事实不符的幻觉性回答。
- en: Generally bad responses. For example, you ask the model to write an essay, and
    that essay is just bad.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常很差的回答。例如，你要求模型写一篇论文，而这篇论文却很糟糕。
- en: Security
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全
- en: Toxic responses that contain racist content, sexual content, or illegal activities.
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含种族主义内容、色情内容或非法活动的有害回答。
- en: Responses that contain private and sensitive information.
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含私人敏感信息的回答。
- en: Responses that trigger remote tool and code execution.
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 触发远程工具和代码执行的回答。
- en: Brand-risk responses that mischaracterize your company or your competitors.
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 误导品牌风险的回答，错误地描述了你的公司或你的竞争对手。
- en: Recall from [Chapter 5](ch05.html#ch05a_prompt_engineering_1730156991195551)
    that for security measurements, it’s important to track not only the security
    failures but also the false refusal rate. It’s possible to have systems that are
    too secure, e.g., one that blocks even legitimate requests, interrupting user
    workloads and causing user frustration.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[第5章](ch05.html#ch05a_prompt_engineering_1730156991195551)的内容，对于安全度量，跟踪不仅包括安全失败，还包括错误拒绝率是很重要的。可能存在过于安全的系统，例如，一个甚至阻止合法请求的系统，这会中断用户的工作负载并导致用户感到沮丧。
- en: Many failures can be mitigated by simple retry logic. AI models are probabilistic,
    which means that if you try a query again, you might get a different response.
    For example, if the response is empty, try again X times or until you get a nonempty
    response. Similarly, if the response is malformatted, try again until the response
    is correctly formatted.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 许多失败可以通过简单的重试逻辑来缓解。AI模型是概率性的，这意味着如果你再次尝试查询，你可能会得到不同的响应。例如，如果响应为空，尝试再次X次或直到得到非空响应。同样，如果响应格式不正确，尝试再次直到响应格式正确。
- en: This retry policy, however, can incur extra latency and cost. Each retry means
    another round of API calls. If the retry is carried out after failure, the user-perceived
    latency will double. To reduce latency, you can make calls in parallel. For example,
    for each query, instead of waiting for the first query to fail before retrying,
    you send this query to the model twice at the same time, get back two responses,
    and pick the better one. This increases the number of redundant API calls while
    keeping latency manageable.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种重试策略可能会产生额外的延迟和成本。每次重试意味着另一轮API调用。如果在失败后进行重试，用户感知的延迟将加倍。为了减少延迟，你可以并行调用。例如，对于每个查询，而不是等待第一个查询失败后再重试，你可以同时将这个查询发送给模型两次，得到两个响应，并选择更好的一个。这增加了冗余API调用的数量，同时保持延迟可管理。
- en: It’s also common to fall back on humans for tricky requests. For example, you
    can transfer the queries that contain specific phrases to human operators. Some
    teams use a specialized model to decide when to transfer a conversation to humans.
    One team, for instance, transfers a conversation to human operators when their
    sentiment analysis model detects anger in users’ messages. Another team transfers
    a conversation after a certain number of turns to prevent users from getting stuck
    in a loop.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂请求，也常见的是依赖人类。例如，你可以将包含特定短语的查询转移到人工操作员。一些团队使用专门的模型来决定何时将对话转移到人类。例如，当一个团队的
    sentiment analysis 模型检测到用户消息中的愤怒时，他们会将对话转移到人工操作员。另一个团队在经过一定次数的回合后转移对话，以防止用户陷入循环。
- en: Guardrail implementation
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 栅栏实现
- en: Guardrails come with trade-offs. One is the *reliability versus latency trade-off*.
    While acknowledging the importance of guardrails, some teams told me that latency
    is more important. The teams decided not to implement guardrails because they
    can significantly increase the application’s latency.^([3](ch10.html#id1769))
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Output guardrails might not work well in the stream completion mode. By default,
    the whole response is generated before being shown to the user, which can take
    a long time. In the stream completion mode, new tokens are streamed to the user
    as they are generated, reducing the time the user has to wait to see the response.
    The downside is that it’s hard to evaluate partial responses, so unsafe responses
    might be streamed to users before the system guardrails can determine that they
    should be blocked.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: How many guardrails you need to implement also depends on whether you self-host
    your models or use third-party APIs. While you can implement guardrails on top
    of both, third-party APIs can reduce the guardrails you need to implement since
    API providers typically provide many guardrails out of the box for you. At the
    same time, self-hosting means that you don’t need to send requests externally,
    which reduces the need for many types of input guardrails.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Given the many different places where an application might fail, guardrails
    can be implemented at many different levels. Model providers give their models
    guardrails to make their models better and more secure. However, model providers
    have to balance safety and flexibility. Restrictions might make a model safer
    but can also make it less usable for specific use cases.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Guardrails can also implemented by application developers. Many techniques are
    discussed in [“Defenses Against Prompt Attacks”](ch05.html#ch05a_defense_against_prompt_attacks_1730156991196455).
    Guardrail solutions that you can use out of the box include [Meta’s Purple Llama](https://github.com/meta-llama/PurpleLlama),
    [NVIDIA’s NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails), [Azure’s
    PyRIT](https://github.com/Azure/PyRIT), [Azure’s AI content filters](https://oreil.ly/CxwLn),
    the [Perspective API](https://oreil.ly/d2_sL), and [OpenAI’s content moderation
    API](https://oreil.ly/-kOHE). Due to the overlap of risks in inputs and outputs,
    a guardrail solution will likely provide protection for both inputs and outputs.
    Some model gateways also provide guardrail functionalities, as discussed in the
    next section.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: With guardrails, the architecture looks like [Figure 10-4](#ch10_figure_4_1730130985262606).
    I put scorers under model APIs since scorers are often AI-powered, even if scorers
    are typically smaller and faster than generative models. However, scorers can
    also be placed in the output guardrails box.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram  Description automatically generated](assets/aien_1004.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. Application architecture with the addition of input and output
    guardrails.
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Step 3\. Add Model Router and Gateway
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As applications grow to involve more models, routers and gateways emerge to
    help you manage the complexity and costs of serving multiple models.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 随着应用的增长，涉及到更多模型，路由器和网关出现以帮助您管理服务多个模型复杂性和成本。
- en: Router
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 路由器
- en: Instead of using one model for all queries, you can have different solutions
    for different types of queries. This approach has several benefits. First, it
    allows specialized models, which can potentially perform better than a general-purpose
    model for specific queries. For example, you can have one model specialized in
    technical troubleshooting and another specialized in billing. Second, this can
    help you save costs. Instead of using one expensive model for all queries, you
    can route simpler queries to cheaper models.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是为所有查询使用一个模型，您可以为不同类型的查询提供不同的解决方案。这种方法有几个好处。首先，它允许使用专门的模型，这些模型在特定查询上可能比通用模型表现得更好。例如，您可以有一个专门处理技术故障排除的模型，另一个专门处理账单的模型。其次，这可以帮助您节省成本。而不是为所有查询使用一个昂贵的模型，您可以将简单的查询路由到更便宜的模型。
- en: 'A router typically consists of *an intent classifier* that predicts what the
    user is trying to do. Based on the predicted intent, the query is routed to the
    appropriate solution. As an example, consider different intentions relevant to
    a customer support chatbot:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 路由器通常包含一个**意图分类器**，该分类器预测用户试图做什么。根据预测的意图，查询会被路由到相应的解决方案。例如，考虑与客户支持聊天机器人相关的不同意图：
- en: If the user wants to reset the password, route them to the FAQ page about recovering
    the password.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果用户想要重置密码，将他们路由到关于恢复密码的常见问题解答页面。
- en: If the request is to correct a billing mistake, route it to a human operator.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果请求是纠正账单错误，将其路由到人工操作员。
- en: If the request is about troubleshooting a technical issue, route it to a chatbot
    specialized in troubleshooting.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果请求是关于解决技术问题，将其路由到专门处理故障排除的聊天机器人。
- en: 'An intent classifier can prevent your system from engaging in out-of-scope
    conversations. If the query is deemed inappropriate, the chatbot can politely
    decline to respond using one of the stock responses without wasting an API call.
    For example, if the user asks who you would vote for in the upcoming election,
    a chatbot can respond with: “As a chatbot, I don’t have the ability to vote. If
    you have questions about our products, I’d be happy to help.”'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 意图分类器可以防止您的系统参与超出范围的对话。如果查询被认为不合适，聊天机器人可以礼貌地拒绝回答，使用库存回复之一，而不浪费API调用。例如，如果用户询问您在即将到来的选举中会投给谁，聊天机器人可以回答：“作为聊天机器人，我没有投票的能力。如果您对我们的产品有任何问题，我很乐意帮助。”
- en: An intent classifier can help the system detect ambiguous queries and ask for
    clarification. For example, in response to the query “Freezing”, the system might
    ask, “Do you want to freeze your account or are you talking about the weather?”
    or simply ask, “I’m sorry. Can you elaborate?”
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 意图分类器可以帮助系统检测模糊查询并请求澄清。例如，对于“冷冻”的查询，系统可能会问，“您是想冻结您的账户还是您在谈论天气？”或者简单地问，“对不起，您能详细说明吗？”
- en: 'Other routers can aid the model in deciding what to do next. For example, for
    an agent capable of multiple actions, a router can take the form of a *next-action
    predictor*: should the model use a code interpreter or a search API next? For
    a model with a memory system, a router can predict which part of the memory hierarchy
    the model should pull information from. Imagine that a user attaches a document
    that mentions Melbourne to the current conversation. Later on, the user asks:
    “What’s the cutest animal in Melbourne?” The model needs to decide whether to
    rely on the information in the attached document or to search the internet for
    this query.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其他路由器可以帮助模型决定下一步该做什么。例如，对于一个能够执行多个动作的代理，路由器可以是一个**下一动作预测器**：模型接下来应该使用代码解释器还是搜索API？对于一个具有记忆系统的模型，路由器可以预测模型应该从内存层次结构的哪个部分获取信息。想象一下，用户将一份提到墨尔本的文档附加到当前对话中。稍后，用户问：“墨尔本最可爱的动物是什么？”模型需要决定是依赖附加文档中的信息还是搜索互联网来处理这个查询。
- en: Intent classifiers and next-action predictors can be implemented on top of foundation
    models. Many teams adapt smaller language models like GPT-2, BERT, and Llama 7B
    as their intent classifiers. Many teams opt to train even smaller classifiers
    from scratch. Routers should be fast and cheap so that they can use multiples
    of them without incurring significant extra latency and cost.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 意图分类器和后续动作预测器可以在基础模型之上实现。许多团队采用较小的语言模型，如 GPT-2、BERT 和 Llama 7B 作为他们的意图分类器。许多团队选择从头开始训练更小的分类器。路由器应该快速且成本低，这样它们就可以使用多个路由器而不会产生显著的额外延迟和成本。
- en: When routing queries to models with varying context limits, the query’s context
    might need to be adjusted accordingly. Consider a 1,000-token query that is slated
    for a model with a 4K context limit. The system then takes an action, e.g., a
    web search, that brings back 8,000-token context. You can either truncate the
    query’s context to fit the originally intended model or route the query to a model
    with a larger context limit.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当将查询路由到具有不同上下文限制的模型时，查询的上下文可能需要相应地调整。考虑一个预定用于具有 4K 上下文限制的模型的 1,000 个标记查询。然后系统采取行动，例如进行网络搜索，返回
    8,000 个标记的上下文。您可以选择截断查询的上下文以适应最初打算使用的模型，或者将查询路由到具有更大上下文限制的模型。
- en: Because routing is usually done by models, I put routing inside the Model API
    box in [Figure 10-5](#ch10_figure_5_1730130985262626). Like scorers, routers are
    typically smaller than models used for generation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于路由通常由模型执行，我在[图 10-5](#ch10_figure_5_1730130985262626)中将路由放在模型 API 框内。与评分器一样，路由器通常比用于生成的模型要小。
- en: Grouping routers together with other models makes models easier to manage. However,
    it’s important to note that routing often happens *before* retrieval. For example,
    before retrieval, a router can help determine if a query is in-scope and, if yes,
    if it needs retrieval. Routing can happen after retrieval, too, such as determining
    if a query should be routed to a human operator. However, routing - retrieval
    - generation - scoring is a much more common AI application pattern.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将路由器与其他模型分组在一起可以使模型更容易管理。然而，需要注意的是，路由通常发生在检索之前。例如，在检索之前，路由器可以帮助确定查询是否在范围内，如果是的话，是否需要检索。路由也可以在检索之后发生，例如确定查询是否应该路由到人工操作员。然而，路由-检索-生成-评分是更常见的
    AI 应用模式。
- en: '![A diagram of a system  Description automatically generated](assets/aien_1005.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![系统图示  自动生成描述](assets/aien_1005.png)'
- en: Figure 10-5\. Routing helps the system use the optimal solution for each query.
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. 路由帮助系统为每个查询使用最佳解决方案。
- en: Gateway
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网关
- en: A model gateway is an intermediate layer that allows your organization to interface
    with different models in a unified and secure manner. The most basic functionality
    of a model gateway is to provide a unified interface to different models, including
    self-hosted models and models behind commercial APIs. A model gateway makes it
    easier to maintain your code. If a model API changes, you only need to update
    the gateway instead of updating all applications that depend on this API. [Figure 10-6](#ch10_figure_6_1730130985262644)
    shows a high-level visualization of a model gateway.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 模型网关是一个中间层，允许您的组织以统一和安全的方式与不同的模型进行接口。模型网关最基本的功能是提供统一接口，用于不同的模型，包括自托管模型和位于商业
    API 背后的模型。模型网关使维护代码变得更加容易。如果模型 API 发生变化，您只需更新网关，而不是更新所有依赖此 API 的应用程序。[图 10-6](#ch10_figure_6_1730130985262644)
    展示了模型网关的高级可视化。
- en: '![A diagram of a model gateway  Description automatically generated](assets/aien_1006.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![模型网关图示  自动生成描述](assets/aien_1006.png)'
- en: Figure 10-6\. A model gateway provides a unified interface to work with different
    models.
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-6\. 模型网关提供了一个统一的接口，用于与不同的模型协同工作。
- en: 'In its simplest form, a model gateway is a unified wrapper. The following code
    example gives you an idea of how a model gateway might be implemented. It’s not
    meant to be functional, as it doesn’t contain any error checking or optimization:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单形式中，模型网关是一个统一的包装器。以下代码示例为您展示了模型网关可能如何实现。这不是一个功能性示例，因为它不包含任何错误检查或优化：
- en: '[PRE0]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A model gateway provides *access control and cost management*. Instead of giving
    everyone who wants access to the OpenAI API your organizational tokens, which
    can be easily leaked, you give people access only to the model gateway, creating
    a centralized and controlled point of access. The gateway can also implement fine-grained
    access controls, specifying which user or application should have access to which
    model. Moreover, the gateway can monitor and limit the usage of API calls, preventing
    abuse and managing costs effectively.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 模型网关提供**访问控制和成本管理**。您不必将组织令牌提供给所有希望访问 OpenAI API 的人，这些令牌很容易泄露，而是只给人们访问模型网关的权限，创建一个集中和受控的访问点。网关还可以实现细粒度的访问控制，指定哪个用户或应用程序应该有权访问哪个模型。此外，网关可以监控和限制
    API 调用的使用，防止滥用并有效管理成本。
- en: A model gateway can also be used to implement fallback policies to overcome
    rate limits or API failures (the latter is unfortunately common). When the primary
    API is unavailable, the gateway can route requests to alternative models, retry
    after a short wait, or handle failures gracefully in other ways. This ensures
    that your application can operate smoothly without interruptions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 模型网关还可以用于实现回退策略，以克服速率限制或 API 故障（遗憾的是后者很常见）。当主 API 不可用时，网关可以将请求路由到替代模型，稍作等待后重试，或以其他方式优雅地处理故障。这确保了您的应用程序在没有中断的情况下平稳运行。
- en: Since requests and responses are already flowing through the gateway, it’s a
    good place to implement other functionalities, such as load balancing, logging,
    and analytics. Some gateways even provide caching and guardrails.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于请求和响应已经通过网关流动，因此它是实现其他功能的好地方，例如负载均衡、日志记录和分析。一些网关甚至提供缓存和防护措施。
- en: Given that gateways are relatively straightforward to implement, there are many
    off-the-shelf gateways. Examples include [Portkey’s AI Gateway](https://github.com/Portkey-AI/gateway),
    [MLflow AI Gateway](https://oreil.ly/D2X_Y), [Wealthsimple’s LLM Gateway](https://github.com/wealthsimple/llm-gateway),
    [TrueFoundry](https://oreil.ly/ICRRA), [Kong](https://oreil.ly/St4W6), and [Cloudflare](https://oreil.ly/0NuNb).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于网关相对容易实现，因此有许多现成的网关。例如包括 [Portkey 的 AI 网关](https://github.com/Portkey-AI/gateway)、[MLflow
    AI 网关](https://oreil.ly/D2X_Y)、[Wealthsimple 的 LLM 网关](https://github.com/wealthsimple/llm-gateway)、[TrueFoundry](https://oreil.ly/ICRRA)、[Kong](https://oreil.ly/St4W6)
    和 [Cloudflare](https://oreil.ly/0NuNb)。
- en: In our architecture, the gateway now replaces the model API box, as shown in
    [Figure 10-7](#ch10_figure_7_1730130985262661).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的架构中，网关现在取代了模型 API 盒，如图 [图 10-7](#ch10_figure_7_1730130985262661) 所示。
- en: '![A diagram of a data flow  Description automatically generated](assets/aien_1007.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![数据流图  描述自动生成](assets/aien_1007.png)'
- en: Figure 10-7\. The architecture with the added routing and gateway modules.
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-7\. 增加了路由和网关模块的架构。
- en: Note
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A similar abstraction layer, such as a tool gateway, can also be useful for
    accessing a wide range of tools. It’s not discussed in this book since it’s not
    a common pattern as of this writing.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的抽象层，例如工具网关，也可以用于访问广泛的各种工具。由于它不是本书写作时的常见模式，因此本书没有讨论。
- en: Step 4\. Reduce Latency with Caches
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 4\. 使用缓存降低延迟
- en: 'Caching has long been integral to software applications to reduce latency and
    cost. Many ideas from software caching can be used for AI applications. Inference
    caching techniques, including KV caching and prompt caching, are discussed in
    [Chapter 9](ch09.html#ch09_inference_optimization_1730130963006301). This section
    focuses on system caching. Because caching is an old technology with a large amount
    of existing literature, this book will cover it only in broad strokes. In general,
    there are two major system caching mechanisms: exact caching and semantic caching.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存长期以来一直是软件应用的组成部分，用于降低延迟和成本。软件缓存的许多想法可以用于人工智能应用。推理缓存技术，包括 KV 缓存和提示缓存，在 [第 9
    章](ch09.html#ch09_inference_optimization_1730130963006301) 中讨论。本节重点介绍系统缓存。由于缓存是一项古老的技术，已有大量现有文献，本书将只作概述。一般来说，有两种主要的系统缓存机制：精确缓存和语义缓存。
- en: Exact caching
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精确缓存
- en: With exact caching, cached items are used only when these exact items are requested.
    For example, if a user asks a model to summarize a product, the system checks
    the cache to see if a summary of this exact product exists. If yes, fetch this
    summary. If not, summarize the product and cache the summary.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在精确缓存中，只有当请求这些确切的项目时才使用缓存的项目。例如，如果用户要求模型总结一个产品，系统会检查缓存以查看是否存在该确切产品的摘要。如果存在，则获取此摘要。如果不存在，则总结产品并将摘要缓存起来。
- en: Exact caching is also used for embedding-based retrieval to avoid redundant
    vector search. If an incoming query is already in the vector search cache, fetch
    the cached result. If not, perform a vector search for this query and cache the
    result.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 精确缓存也用于基于嵌入的检索以避免冗余向量搜索。如果传入的查询已经在向量搜索缓存中，则获取缓存的查询结果。如果没有，则为此查询执行向量搜索并将结果缓存。
- en: Caching is especially appealing for queries that involve multiple steps (e.g.,
    chain-of-thought) and/or time-consuming actions (e.g., retrieval, SQL execution,
    or web search).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于涉及多个步骤（例如，思维链）和/或耗时操作（例如，检索、SQL执行或网络搜索）的查询，缓存特别有吸引力。
- en: An exact cache can be implemented using in-memory storage for fast retrieval.
    However, since in-memory storage is limited, a cache can also be implemented using
    databases like PostgreSQL, Redis, or tiered storage to balance speed and storage
    capacity. Having an eviction policy is crucial to manage the cache size and maintain
    performance. Common eviction policies include Least Recently Used (LRU), Least
    Frequently Used (LFU), and first in, first out (FIFO).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用内存存储来实现精确缓存，以便快速检索。然而，由于内存存储有限，也可以使用数据库（如PostgreSQL、Redis或分层存储）来实现缓存，以平衡速度和存储容量。拥有一个驱逐策略对于管理缓存大小并保持性能至关重要。常见的驱逐策略包括最近最少使用（LRU）、最少使用（LFU）和先进先出（FIFO）。
- en: How long to keep a query in the cache depends on how likely this query is to
    be called again. User-specific queries, such as “What’s the status of my recent
    order?”, are less likely to be reused by other users and, therefore, shouldn’t
    be cached. Similarly, it makes less sense to cache time-sensitive queries such
    as “How’s the weather?” Many teams train a classifier to predict whether a query
    should be cached.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存查询多长时间取决于该查询再次被调用的可能性有多大。用户特定的查询，例如“我的最近订单状态如何？”，不太可能被其他用户重用，因此不应缓存。同样，缓存时间敏感的查询，如“天气如何？”也没有太多意义。许多团队训练一个分类器来预测是否应该缓存查询。
- en: Warning
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Caching, when not properly handled, can cause data leaks. Imagine you work
    for an ecommerce site, and user X asks a seemingly generic question such as: “What
    is the return policy for electronics products?” Because your return policy depends
    on the user’s membership, the system first retrieves user X’s information and
    then generates a response containing X’s information. Mistaking this query for
    a generic question, the system caches the answer. Later, when user Y asks the
    same question, the cached result is returned, revealing X’s information to Y.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果处理不当，缓存可能导致数据泄露。想象一下，你为一家电子商务网站工作，用户X提出了一个看似普通的问题，例如：“电子产品产品的退货政策是什么？”因为你的退货政策取决于用户的会员资格，系统首先检索用户X的信息，然后生成包含X信息的响应。如果将此查询误认为是普通问题，系统将缓存答案。后来，当用户Y提出相同的问题时，返回的缓存结果揭示了X的信息。
- en: Semantic caching
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语义缓存
- en: Unlike in exact caching, cached items are used even if they are only semantically
    similar, not identical, to the incoming query. Imagine one user asks, “What’s
    the capital of Vietnam?” and the model answers, “Hanoi”. Later, another user asks,
    “What’s the capital *city* of Vietnam?”, which is semantically the same question
    but with slightly different wording. With semantic caching, the system can reuse
    the answer from the first query instead of computing the new query from scratch.
    Reusing similar queries increases the cache’s hit rate and potentially reduces
    cost. However, semantic caching can reduce your model’s performance.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与精确缓存不同，即使缓存的项与传入的查询只有语义相似，而不是完全相同，也会使用这些项。想象一下，一个用户问：“越南的首都是什么？”模型回答：“河内”。后来，另一个用户问：“越南的*城市*首都是什么？”这是一个语义上相同但措辞略有不同的问题。使用语义缓存，系统可以重用第一个查询的答案，而不是从头开始计算新的查询。重用相似的查询可以提高缓存的命中率并可能降低成本。然而，语义缓存可能会降低您的模型性能。
- en: 'Semantic caching works only if you have a reliable way of determining if two
    queries are similar. One common approach is to use semantic similarity, as discussed
    in [Chapter 3](ch03.html#ch03a_evaluation_methodology_1730150757064067). As a
    refresh, semantic similarity works as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 语义缓存仅在您有可靠的方法确定两个查询是否相似时才有效。一种常见的方法是使用语义相似度，如第3章[第3.1.5节](ch03.html#ch03a_evaluation_methodology_1730150757064067)中所述。作为一个提醒，语义相似度的工作原理如下：
- en: For each query, generate its embedding using an embedding model.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个查询，使用嵌入模型生成其嵌入。
- en: Use vector search to find the cached embedding with the highest similar score
    to the current query embedding. Let’s say this similarity score is *X*.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用向量搜索找到与当前查询嵌入最相似的缓存的嵌入。假设这个相似度分数是 *X*。
- en: If *X* is higher than a certain similarity threshold, the cached query is considered
    similar, and the cached results are returned. If not, process this current query
    and cache it together with its embedding and results.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *X* 超过一定的相似度阈值，则缓存的查询被认为是相似的，并返回缓存的查询结果。如果不满足条件，则处理当前查询并将其与其嵌入和结果一起缓存。
- en: This approach requires a vector database to store the embeddings of cached queries.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法需要一个向量数据库来存储缓存的查询的嵌入。
- en: '*Compared to other caching techniques, semantic caching’s value is more dubious
    because many of its components are prone to failure.* Its success relies on high-quality
    embeddings, functional vector search, and a reliable similarity metric. Setting
    the right similarity threshold can also be tricky, requiring a lot of trial and
    error. If the system mistakes the incoming query for one similar to another query,
    the returned response, fetched from the cache, will be incorrect.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他缓存技术相比，语义缓存的价值更加可疑，因为其许多组件容易出错。其成功依赖于高质量的嵌入、功能性的向量搜索和可靠的相似度度量。设置合适的相似度阈值也可能很棘手，需要大量的尝试和错误。如果系统错误地将传入的查询视为与另一个查询相似，那么从缓存中检索的返回响应将是不正确的。
- en: In addition, semantic cache can be time-consuming and compute-intensive, as
    it involves a vector search. The speed and cost of this vector search depend on
    the size of your cached embeddings.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，语义缓存可能耗时且计算密集，因为它涉及向量搜索。这种向量搜索的速度和成本取决于您缓存的嵌入的大小。
- en: Semantic cache might still be worthwhile if the cache hit rate is high, meaning
    that a good portion of queries can be effectively answered by leveraging the cached
    results. However, before incorporating the complexities of a semantic cache, make
    sure to evaluate the associated efficiency, cost, and performance risks.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果缓存命中率很高，即大部分查询可以通过利用缓存结果得到有效回答，那么语义缓存可能仍然是有价值的。然而，在引入语义缓存的复杂性之前，请确保评估相关的效率、成本和性能风险。
- en: With the added cache systems, the platform looks like [Figure 10-8](#ch10_figure_8_1730130985262677).
    A KV cache and prompt cache are typically implemented by model API providers,
    so they aren’t shown in this image. To visualize them, I’d put them in the Model
    API box. There’s a new arrow to add generated responses to the cache.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在增加缓存系统后，平台看起来像[图 10-8](#ch10_figure_8_1730130985262677)。键值缓存和提示缓存通常由模型 API
    提供商实现，因此在此图中未显示。为了可视化它们，我会将它们放在模型 API 框中。有一个新的箭头用于将生成的响应添加到缓存中。
- en: '![](assets/aien_1008.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aien_1008.png)'
- en: Figure 10-8\. An AI application architecture with the added caches.
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-8\. 增加缓存后的 AI 应用架构。
- en: Step 5\. Add Agent Patterns
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 5 步：添加代理模式
- en: The applications discussed so far are still fairly simple. Each query follows
    a sequential flow. However, as discussed in [Chapter 6](ch06.html#ch06_rag_and_agents_1730157386571386),
    an application flow can be more complex with loops, parallel execution, and conditional
    branching. Agentic patterns, discussed in [Chapter 6](ch06.html#ch06_rag_and_agents_1730157386571386),
    can help you build complex applications. For example, after the system generates
    an output, it might determine that it hasn’t accomplished the task and that it
    needs to perform another retrieval to gather more information. The original response,
    together with the newly retrieved context, is passed into the same model or a
    different one. This creates a loop, as shown in [Figure 10-9](#ch10_figure_9_1730130985262696).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止讨论的应用程序仍然相当简单。每个查询都遵循一个顺序流程。然而，如第 6 章所述（ch06.html#ch06_rag_and_agents_1730157386571386），应用程序流程可以更复杂，包括循环、并行执行和条件分支。在第
    6 章中讨论的代理模式可以帮助您构建复杂的应用程序。例如，系统生成输出后，可能会确定尚未完成任务，需要执行另一个检索以获取更多信息。原始响应与新检索到的上下文一起传递到同一个模型或不同的模型。这创建了一个循环，如图
    10-9 所示（#ch10_figure_9_1730130985262696）。
- en: '![A diagram of a computer system  Description automatically generated](assets/aien_1009.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![计算机系统图  描述自动生成](assets/aien_1009.png)'
- en: Figure 10-9\. The yellow arrow allows the generated response to be fed back
    into the system, allowing more complex application patterns.
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-9\. 黄色箭头允许将生成的响应反馈到系统中，从而允许更复杂的应用模式。
- en: A model’s outputs also can be used to invoke write actions, such as composing
    an email, placing an order, or initializing a bank transfer. Write actions allow
    a system to make changes to its environment directly. As discussed in [Chapter 6](ch06.html#ch06_rag_and_agents_1730157386571386),
    write actions can make a system vastly more capable but also expose it to significantly
    more risks. Giving a model access to write actions should be done with the utmost
    care. With added write actions, the architecture looks like [Figure 10-10](#ch10_figure_10_1730130985262713).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的输出也可以用来调用写入操作，例如撰写电子邮件、下订单或初始化银行转账。写入操作允许系统直接对其环境进行更改。如第6章[讨论](ch06.html#ch06_rag_and_agents_1730157386571386)所示，写入操作可以使系统功能大大增强，但也使其面临显著更多的风险。给予模型访问写入操作应该非常谨慎。添加了写入操作后，架构看起来就像[图10-10](#ch10_figure_10_1730130985262713)。
- en: If you’ve followed all the steps so far, your architecture has likely grown
    quite complex. While complex systems can solve more tasks, they also introduce
    more failure modes, making them harder to debug due to the many potential points
    of failure. The next section will cover best practices for improving system observability.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经遵循了迄今为止的所有步骤，那么您的架构可能已经变得相当复杂。虽然复杂系统可以解决更多任务，但它们也引入了更多的故障模式，由于存在许多潜在的故障点，这使得它们更难以调试。下一节将介绍提高系统可观察性的最佳实践。
- en: '![A diagram of a system  Description automatically generated](assets/aien_1010.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![系统图描述自动生成](assets/aien_1010.png)'
- en: Figure 10-10\. An application architecture that enables the system to perform
    write actions.
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-10\. 允许系统执行写入操作的应用程序架构。
- en: Monitoring and Observability
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控和可观察性
- en: Even though I put observability in its own section, observability should be
    integral to the design of a product, rather than an afterthought. The more complex
    a product, the more crucial observability is.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我将可观察性放在了自己的一个部分，但可观察性应该成为产品设计的一部分，而不是事后考虑。产品越复杂，可观察性就越重要。
- en: Observability is a universal practice across all software engineering disciplines.
    It’s a big industry with established best practices and many ready-to-use proprietary
    and open source solutions.^([4](ch10.html#id1786)) To avoid reinventing the wheel,
    I’ll focus on what’s unique to applications built on top of foundation models.
    The book’s [GitHub repository](https://github.com/chiphuyen/aie-book) contains
    resources for those who want to learn more about observability.^([5](ch10.html#id1787))
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 可观察性是所有软件工程学科的一个通用实践。这是一个大行业，拥有既定的最佳实践和许多现成的专有和开源解决方案。[4](ch10.html#id1786)
    为了避免重复造轮子，我将专注于在基础模型之上构建的应用程序的独特之处。本书的[GitHub仓库](https://github.com/chiphuyen/aie-book)包含有关可观察性的更多资源。[5](ch10.html#id1787)
- en: 'The goal of monitoring is the same as the goal of evaluation: to mitigate risks
    and discover opportunities. Risks that monitoring should help you mitigate include
    application failures, security attacks, and drifts. Monitoring can help discover
    opportunities for application improvement and cost savings. Monitoring can also
    help keep you accountable by giving visibility into your system’s performance.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 监控的目标与评估的目标相同：降低风险并发现机会。监控应帮助您缓解的风险包括应用程序故障、安全攻击和漂移。监控可以帮助发现应用程序改进和成本节约的机会。监控还可以通过提供对系统性能的可见性来帮助您承担责任。
- en: 'Three metrics can help evaluate the quality of your system’s observability,
    derived from the DevOps community:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 三个指标可以帮助评估您系统可观察性的质量，这些指标源自DevOps社区：
- en: 'MTTD (mean time to detection): When something bad happens, how long does it
    take to detect it?'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MTTD（平均检测时间）：当发生不良事件时，需要多长时间才能检测到？
- en: 'MTTR (mean time to response): After detection, how long does it take to be
    resolved?'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MTTR（平均响应时间）：检测后，需要多长时间才能解决？
- en: 'CFR (change failure rate): The percentage of changes or deployments that result
    in failures requiring fixes or rollbacks. If you don’t know your CFR, it’s time
    to redesign your platform to make it more observable.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CFR（变更失败率）：导致需要修复或回滚的失败变更或部署的百分比。如果您不知道您的CFR，那么是时候重新设计您的平台，使其更具可观察性。
- en: Having a high CFR doesn’t necessarily indicate a bad monitoring system. However,
    you should rethink your evaluation pipeline so that bad changes are caught before
    being deployed. Evaluation and monitoring need to work closely together. Evaluation
    metrics should translate well to monitoring metrics, meaning that a model that
    does well during evaluation should also do well during monitoring. Issues detected
    during monitoring should be fed to the evaluation pipeline.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 高CFR并不一定意味着监控系统不好。然而，你应该重新思考你的评估流程，以便在部署之前捕捉到不良变化。评估和监控需要紧密合作。评估指标应该很好地转化为监控指标，这意味着在评估期间表现良好的模型也应该在监控期间表现良好。监控期间检测到的问题应该反馈到评估流程中。
- en: Metrics
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指标
- en: When discussing monitoring, most people think of metrics. However, metrics themselves
    aren’t the goal. Frankly, most companies don’t care what your application’s output
    relevancy score is unless it serves a purpose. The purpose of a metric is to tell
    you when something is wrong and to identify opportunities for improvement.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当讨论监控时，大多数人会想到指标。然而，指标本身并不是目标。坦白说，大多数公司并不关心你的应用程序输出相关性的得分，除非它有用途。指标的目的在于告诉你何时有问题，并识别改进的机会。
- en: Before listing what metrics to track, it’s important to understand what failure
    modes you want to catch and design your metrics around these failures. For example,
    if you don’t want your application to hallucinate, design metrics that help you
    detect hallucinations. One relevant metric might be whether an application’s output
    can be inferred from the context. If you don’t want your application to burn through
    your API credit, track metrics related to API costs, such as the number of input
    and output tokens per request or your cache’s cost and your cache’s hit rate.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在列出要跟踪的指标之前，了解你想要捕捉的失败模式并围绕这些失败设计你的指标是很重要的。例如，如果你不希望你的应用程序产生幻觉，设计可以帮助你检测幻觉的指标。一个相关的指标可能是应用程序的输出是否可以从上下文中推断出来。如果你不希望你的应用程序耗尽你的API信用，跟踪与API成本相关的指标，例如每个请求的输入和输出令牌数量或你的缓存成本和你的缓存命中率。
- en: Because foundation models can generate open-ended outputs, there are many ways
    things can go wrong. Metrics design requires analytical thinking, statistical
    knowledge, and, often, creativity. Which metrics you should track are highly application-specific.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基础模型可以生成开放式输出，事情出错的方式有很多。指标设计需要分析思维、统计知识，并且通常还需要创造力。你应该跟踪哪些指标非常具体地取决于应用。
- en: This book has covered many different types of model quality metrics (Chapters
    [4](ch04.html#ch04_evaluate_ai_systems_1730130866187863)–[6](ch06.html#ch06_rag_and_agents_1730157386571386),
    and later in this chapter) and many different ways to compute them (Chapters [3](ch03.html#ch03a_evaluation_methodology_1730150757064067)
    and [5](ch05.html#ch05a_prompt_engineering_1730156991195551)). Here, I’ll do a
    quick recap.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本书已经涵盖了多种不同类型的模型质量指标（第[4](ch04.html#ch04_evaluate_ai_systems_1730130866187863)章至第[6](ch06.html#ch06_rag_and_agents_1730157386571386)章，以及本章后面的内容）和许多不同的计算方法（第[3](ch03.html#ch03a_evaluation_methodology_1730150757064067)章和第[5](ch05.html#ch05a_prompt_engineering_1730156991195551)章）。在这里，我将快速回顾。
- en: The easiest types of failures to track are format failures because they are
    easy to notice and verify. For example, if you expect JSON outputs, track how
    often the model outputs invalid JSON and, among these invalid JSON outputs, how
    many can be easily fixed (missing a closing bracket is easy to fix, but missing
    expected keys is harder).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最容易跟踪的失败类型是格式错误，因为它们容易注意到和验证。例如，如果你期望JSON输出，跟踪模型输出无效JSON的频率，以及在这些无效JSON输出中，有多少可以轻松修复（缺少一个闭合括号很容易修复，但缺少预期的键则更难）。
- en: For open-ended generations, consider monitoring factual consistency and relevant
    generation quality metrics such as conciseness, creativity, or positivity. Many
    of these metrics can be computed using AI judges.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于开放式生成，考虑监控事实一致性以及相关生成质量指标，如简洁性、创造性和积极性。许多这些指标可以使用AI评判员来计算。
- en: If safety is an issue, you can track toxicity-related metrics and detect private
    and sensitive information in both inputs and outputs. Track how often your guardrails
    get triggered and how often your system refuses to answer. Detect abnormal queries
    to your system, too, since they might reveal interesting edge cases or prompt
    attacks.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果安全问题是一个问题，你可以跟踪与毒性相关的指标，并在输入和输出中检测到私有和敏感信息。跟踪你的安全措施被触发的频率以及你的系统拒绝回答的频率。也要检测对系统的异常查询，因为它们可能揭示有趣的边缘情况或触发攻击。
- en: 'Model quality can also be inferred through user natural language feedback and
    conversational signals. For example, some easy metrics you can track include the
    following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 模型质量也可以通过用户自然语言反馈和对话信号推断出来。例如，你可以跟踪的一些简单指标包括以下内容：
- en: How often do users stop a generation halfway?
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户多频繁地在生成过程中停止？
- en: What’s the average number of turns per conversation?
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次对话的平均轮数是多少？
- en: What’s the average number of tokens per input? Are users using your application
    for more complex tasks, or are they learning to be more concise with their prompts?
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个输入的平均标记数是多少？用户是否在使用你的应用程序进行更复杂的任务，或者他们是否在学会使他们的提示更加简洁？
- en: What’s the average number of tokens per output? Are some models more verbose
    than others? Are certain types of queries more likely to result in lengthy answers?
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个输出的平均标记数是多少？某些模型是否比其他模型更冗长？某些类型的查询更有可能产生较长的答案？
- en: What’s the model’s output token distribution? How has it changed over time?
    Is the model getting more or less diverse?
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的输出标记分布是什么？它随时间如何变化？模型是否变得更加多样化或减少多样化？
- en: Length-related metrics are also important for tracking latency and costs, as
    longer contexts and responses typically increase latency and incur higher costs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与长度相关的指标对于跟踪延迟和成本也很重要，因为较长的上下文和响应通常会增加延迟并产生更高的成本。
- en: Each component in an application pipeline has its own metrics. For example,
    in a RAG application, the retrieval quality is often evaluated using context relevance
    and context precision. A vector database can be evaluated by how much storage
    it needs to index the data and how long it takes to query the data.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序管道中的每个组件都有自己的指标。例如，在 RAG 应用程序中，检索质量通常使用上下文相关性和上下文精确度来评估。向量数据库可以通过它需要多少存储来索引数据以及查询数据所需的时间来评估。
- en: Given that you’ll likely have multiple metrics, it’s useful to measure how these
    metrics correlate to each other and, especially, to your business north star metrics,
    which can be DAU (daily active user), session duration (the length of time a user
    spends actively engaged with the application), or subscriptions. Metrics that
    are strongly correlated to your north star might give you ideas on how to improve
    your north star. Metrics that are not at all correlated might also give you ideas
    on what not to optimize for.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到你可能有多项指标，测量这些指标之间以及特别是与你的业务北极星指标（例如，日活跃用户数 DAU、会话时长（用户与应用程序积极互动的时间长度）或订阅）的相关性是有用的。与北极星指标高度相关的指标可能会给你提供改进北极星指标的想法。完全不相关的指标也可能给你提供关于不需要优化的想法。
- en: 'Tracking latency is essential for understanding the user experience. Common
    latency metrics, as discussed in [Chapter 9](ch09.html#ch09_inference_optimization_1730130963006301),
    include:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪延迟对于理解用户体验至关重要。如第 9 章所述的常见延迟指标包括：
- en: 'Time to first token (TTFT): the time it takes for the first token to be generated.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首个标记的时间（TTFT）：生成第一个标记所需的时间。
- en: 'Time per output token (TPOT): the time it takes to generate each output token.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个输出标记的时间（TPOT）：生成每个输出标记所需的时间。
- en: 'Total latency: the total time required to complete a response.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总延迟：完成响应所需的总时间。
- en: Track all these metrics per user to see how your system scales with more users.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 按用户跟踪所有这些指标，以查看你的系统如何随着用户数量的增加而扩展。
- en: You’ll also want to track costs. Cost-related metrics are the number of queries
    and the volume of input and output tokens, such as tokens per second (TPS). If
    you use an API with rate limits, tracking the number of requests per second is
    important to ensure you stay within your allocated limits and avoid potential
    service interruptions.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你还希望跟踪成本。与成本相关的指标是查询次数和输入输出标记量，例如每秒标记数（TPS）。如果你使用有速率限制的 API，跟踪每秒的请求数量很重要，以确保你保持在分配的限制内，并避免潜在的服务中断。
- en: When calculating metrics, you can choose between spot checks and exhaustive
    checks. Spot checks involve sampling a subset of data to quickly identify issues,
    while exhaustive checks evaluate every request for a comprehensive performance
    view. The choice depends on your system’s requirements and available resources,
    with a combination of both providing a balanced monitoring strategy.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算指标时，你可以选择进行抽查或详查。抽查涉及采样数据子集以快速识别问题，而详查则评估每个请求以获得全面的性能视图。选择取决于你的系统需求和可用资源，两者结合提供了一种平衡的监控策略。
- en: When computing metrics, ensure they can be broken down by relevant axes, such
    as users, releases, prompt/chain versions, prompt/chain types, and time. This
    granularity helps in understanding performance variations and identifying specific
    issues.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算指标时，确保它们可以根据相关轴进行分解，例如用户、发布版本、提示/链版本、提示/链类型和时间。这种粒度有助于理解性能变化并识别特定问题。
- en: Logs and traces
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日志和跟踪
- en: 'Metrics are typically aggregated. They condense information from events that
    occur in your system over time. They help you understand, at a glance, how your
    system is doing. However, there are many questions that metrics can’t help you
    answer. For example, after seeing a spike in a specific activity, you might wonder:
    “Has this happened before?” Logs can help you answer this question.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 指标通常是汇总的。它们从你系统中随时间发生的事件中压缩信息。它们帮助你一眼就能了解你的系统表现如何。然而，有许多问题指标无法帮助你回答。例如，在看到特定活动的峰值后，你可能会想知道：“这种情况以前发生过吗？”日志可以帮助你回答这个问题。
- en: 'If metrics are numerical measurements representing attributes and events, logs
    are an append-only record of events. In production, a debugging process might
    look like this:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果指标是表示属性和事件的数值测量，那么日志是事件的追加记录。在生产环境中，调试过程可能如下所示：
- en: Metrics tell you something went wrong five minutes ago, but they don’t tell
    you what happened.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指标可以告诉你五分钟前出了问题，但它们无法告诉你具体发生了什么。
- en: You look at the logs of events that took place around five minutes ago to figure
    out what happened.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你查看大约五分钟前发生的事件的日志，以了解发生了什么。
- en: Correlate the errors in the logs to the metrics to make sure that you’ve identified
    the right issue.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志中的错误与指标相关联，以确保你已经确定了正确的问题。
- en: For fast detection, metrics need to be computed quickly. For fast response,
    logs need to be readily available and accessible. If your logs are 15 minutes
    delayed, you will have to wait for the logs to arrive to track down an issue that
    happened 5 minutes ago.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速检测，指标需要快速计算。为了快速响应，日志需要随时可用且易于访问。如果你的日志延迟了15分钟，你将不得不等待日志到达以追踪五分钟前发生的问题。
- en: Because you don’t know exactly what logs you’ll need to look at in the future,
    the general rule for logging is to log everything. Log all the configurations,
    including the model API endpoint, model name, sampling settings (temperature,
    top-p, top-k, stopping condition, etc.), and the prompt template.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你不知道未来需要查看哪些日志，日志的一般规则是记录一切。记录所有配置，包括模型API端点、模型名称、采样设置（温度、top-p、top-k、停止条件等）和提示模板。
- en: Log the user query, the final prompt sent to the model, the output, and the
    intermediate outputs. Log if it calls any tool. Log the tool outputs. Log when
    a component starts, ends, when something crashes, etc. When recording a piece
    of log, make sure to give it tags and IDs that can help you know where this log
    comes from in the system.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 记录用户查询、发送给模型的最终提示、输出以及中间输出。记录是否调用了任何工具。记录工具输出。记录组件启动、结束、崩溃等情况。在记录日志条目时，确保为其提供标签和ID，以便你知道这些日志来自系统的哪个部分。
- en: Logging everything means that the amount of logs you have can grow very quickly.
    Many tools for automated log analysis and log anomaly detection are powered by
    AI.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 记录一切意味着你拥有的日志量可能会迅速增长。许多用于自动化日志分析和日志异常检测的工具都由AI驱动。
- en: While it’s impossible to process logs manually, it’s useful to manually inspect
    your production data daily to get a sense of how users are using your application.
    [Shankar et al., (2024)](https://arxiv.org/abs/2404.12272) found that the developers’
    perceptions of what constitutes good and bad outputs change as they interact with
    more data, allowing them to both rewrite their prompts to increase the chance
    of good responses and update their evaluation pipeline to catch bad responses.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然手动处理日志是不可能的，但每天手动检查生产数据以了解用户如何使用你的应用程序是有用的。[Shankar等人，(2024)](https://arxiv.org/abs/2404.12272)发现，随着开发者与更多数据的互动，他们对什么构成良好和不良输出的看法会发生变化，这使得他们既能重写提示以增加获得良好响应的机会，也能更新他们的评估流程以捕捉不良响应。
- en: If logs are a series of disjointed events, traces are reconstructed by linking
    related events together to form a complete timeline of a transaction or process,
    showing how each step connects from start to finish. In short, a trace is the
    detailed recording of a request’s execution path through various system components
    and services. In an AI application, tracing reveals the entire process from when
    a user sends a query to when the final response is returned, including the actions
    the system takes, the documents retrieved, and the final prompt sent to the model.
    It should also show how much time each step takes and its associated cost, if
    measurable. [Figure 10-11](#ch10_figure_11_1730130985262728) is a visualization
    of a request’s trace in [LangSmith](https://oreil.ly/Oml_x).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果日志是一系列不连续的事件，则通过将相关事件链接在一起来重建跟踪，形成一个事务或过程的完整时间线，显示每个步骤如何从开始到结束连接。简而言之，跟踪是对请求通过各种系统组件和服务执行路径的详细记录。在人工智能应用程序中，跟踪揭示了从用户发送查询到最终响应返回的整个过程，包括系统采取的操作、检索的文档以及发送给模型的最终提示。它还应显示每个步骤花费的时间和其相关成本，如果可以衡量的话。[图
    10-11](#ch10_figure_11_1730130985262728) 是请求跟踪在 [LangSmith](https://oreil.ly/Oml_x)
    中的可视化。
- en: 'Ideally, you should be able to trace each query’s transformation step-by-step
    through the system. If a query fails, you should be able to pinpoint the exact
    step where it went wrong: whether it was incorrectly processed, the retrieved
    context was irrelevant, or the model generated a wrong response.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，你应该能够逐步跟踪每个查询在系统中的转换步骤。如果一个查询失败，你应该能够确定它出错的确切步骤：是处理错误，检索的上下文不相关，还是模型生成了错误的响应。
- en: '![Screens screenshot of a chat  Description automatically generated](assets/aien_1011.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![聊天屏幕截图，描述自动生成](assets/aien_1011.png)'
- en: Figure 10-11\. A request trace visualized by LangSmith.
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-11\. 由 LangSmith 可视化的请求跟踪。
- en: Drift detection
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 漂移检测
- en: 'The more parts a system has, the more things that can change. In an AI application
    these can be:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的部分越多，可能发生变化的事物就越多。在人工智能应用程序中，这些可能包括：
- en: System prompt changes
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 系统提示变化
- en: There are many reasons why your application’s system prompt might change without
    your knowing. The system prompt could’ve been built on top of a prompt template,
    and that prompt template was updated. A coworker could’ve found a typo and fixed
    it. A simple logic should be sufficient to catch when your application’s system
    prompt changes.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你的应用程序的系统提示可能在没有你意识到的情况下发生变化，原因有很多。系统提示可能基于一个提示模板构建，而这个提示模板已被更新。一个同事可能发现了错误并进行了修复。一个简单的逻辑应该足以捕捉到你的应用程序的系统提示发生变化。
- en: User behavior changes
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 用户行为变化
- en: Over time, users adapt their behaviors to the technology. For example, people
    have already figured out how to frame their queries to get better results on Google
    Search or how to make their articles rank higher on search results. People living
    in areas with self-driving cars have already figured out how to bully self-driving
    cars into giving them the right of way ([Liu et al., 2020](https://oreil.ly/AWwkx)).
    It’s likely that your users will change their behaviors to get better results
    out of your application. For example, your users might learn to write instructions
    to make the responses more concise. This might cause a gradual drop in response
    length over time. If you look only at metrics, it might not be obvious what caused
    this gradual drop. You need investigations to understand the root cause.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，用户会适应技术。例如，人们已经找到了如何在 Google 搜索中构建他们的查询以获得更好的结果或如何使他们的文章在搜索结果中排名更高的方法。生活在自动驾驶汽车地区的居民已经找到了如何迫使自动驾驶汽车给他们让路的技巧([刘等人，2020](https://oreil.ly/AWwkx))。你的用户可能会改变他们的行为以从你的应用程序中获得更好的结果。例如，你的用户可能会学会编写指令以使响应更加简洁。这可能会导致响应长度随时间逐渐下降。如果你只看指标，可能不明显是什么导致了这种逐渐的下降。你需要进行调查以了解根本原因。
- en: Underlying model changes
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型变化
- en: When using a model through an API, it’s possible that the API remains unchanged
    while the underlying model is updated. As mentioned in [Chapter 4](ch04.html#ch04_evaluate_ai_systems_1730130866187863),
    model providers might not always disclose these updates, leaving it to you to
    detect any changes. Different versions of the same API can have a significant
    impact on performance. For instance, [Chen et al. (2023)](https://arxiv.org/abs/2307.09009)
    observed notable differences in benchmark scores between the March 2023 and June
    2023 versions of GPT-4 and GPT-3.5\. Likewise, Voiceflow reported a [10% performance
    drop](https://oreil.ly/vIfkA) when switching from the older GPT-3.5-turbo-0301
    to the newer GPT-3.5-turbo-1106.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: AI Pipeline Orchestration
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An AI application can get fairly complex, consisting of multiple models, retrieving
    data from many databases, and having access to a wide range of tools. An orchestrator
    helps you specify how these different components work together to create an end-to-end
    pipeline. It ensures that data flows seamlessly between components. At a high
    level, an orchestrator operates in two steps, components definition and chaining:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Components definition
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: You need to tell the orchestrator what components your system uses, including
    different models, external data sources for retrieval, and tools that your system
    can use. A model gateway can make it easier to add a model.^([6](ch10.html#id1811))
    You can also tell the orchestrator if you use any tools for evaluation and monitoring.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Chaining
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Chaining is basically function composition: it combines different functions
    (components) together. In chaining (pipelining), you tell the orchestrator the
    steps your system takes from receiving the user query until completing the task.
    Here’s an example of the steps:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Process the raw query.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve the relevant data based on the processed query.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine the original query and the retrieved data to create a prompt in the
    format expected by the model.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model generates a response based on the prompt.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the response.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the response is considered good, return it to the user. If not, route the
    query to a human operator.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The orchestrator is responsible for passing data between components. It should
    provide toolings that help ensure that the output from the current step is in
    the format expected by the next step. Ideally, it should notify you when this
    data flow is disrupted due to errors such as component failures or data mismatch
    failures.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An AI pipeline orchestrator is different from a general workflow orchestrator,
    like Airflow or Metaflow.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: When designing the pipeline for an application with strict latency requirements,
    try to do as much in parallel as possible. For example, if you have a routing
    component (deciding where to send a query) and a PII removal component, both can
    be done at the same time.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: There are many AI orchestration tools, including [LangChain](https://github.com/langchain-ai/langchain),
    [LlamaIndex](https://github.com/run-llama/llama_index), [Flowise](https://github.com/FlowiseAI/Flowise),
    [Langflow](https://github.com/langflow-ai/langflow), and [Haystack](https://github.com/deepset-ai/haystack).
    Because retrieval and tool use are common application patterns, many RAG and agent
    frameworks are also orchestration tools.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有许多AI编排工具，包括[LangChain](https://github.com/langchain-ai/langchain)、[LlamaIndex](https://github.com/run-llama/llama_index)、[Flowise](https://github.com/FlowiseAI/Flowise)、[Langflow](https://github.com/langflow-ai/langflow)和[Haystack](https://github.com/deepset-ai/haystack)。由于检索和工具使用是常见的应用模式，许多RAG和代理框架也是编排工具。
- en: While it’s tempting to jump straight to an orchestration tool when starting
    a project, *you might want to start building your application without one first.*
    Any external tool brings additional complexity. An orchestrator can abstract away
    critical details of how your system works, making it hard to understand and debug
    your system.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在项目开始时直接跳到编排工具很有吸引力，*但你可能想要先在不使用编排器的情况下构建你的应用。* 任何外部工具都会带来额外的复杂性。编排器可以抽象出系统工作的重要细节，使得理解和调试系统变得困难。
- en: 'As you advance to the later stages of your application development process,
    you might decide that an orchestrator can make your life easier. Here are three
    aspects to keep in mind when evaluating orchestrators:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当你进入应用开发过程的后期阶段时，你可能会决定编排器可以使你的生活变得更轻松。在评估编排器时，请记住以下三个方面：
- en: Integration and extensibility
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 集成与扩展性
- en: Evaluate whether the orchestrator supports the components you’re already using
    or might adopt in the future. For example, if you want to use a Llama model, check
    if the orchestrator supports that. Given how many models, databases, and frameworks
    there are, it’s impossible for an orchestrator to support everything. Therefore,
    you’ll also need to consider an orchestrator’s extensibility. If it doesn’t support
    a specific component, how hard is it to change that?
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 评估编排器是否支持你目前使用或未来可能采用的组件。例如，如果你想使用Llama模型，检查编排器是否支持该模型。鉴于存在许多模型、数据库和框架，编排器不可能支持所有内容。因此，你还需要考虑编排器的可扩展性。如果它不支持某个特定组件，改变它有多难？
- en: Support for complex pipelines
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 支持复杂管道
- en: As your applications grow in complexity, you might need to manage intricate
    pipelines involving multiple steps and conditional logic. An orchestrator that
    supports advanced features like branching, parallel processing, and error handling
    will help you manage these complexities efficiently.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你的应用复杂性增长，你可能需要管理涉及多个步骤和条件逻辑的复杂管道。支持高级功能如分支、并行处理和错误处理的编排器将帮助你高效地管理这些复杂性。
- en: Ease of use, performance, and scalability
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 易用性、性能和可扩展性
- en: Consider the user-friendliness of the orchestrator. Look for intuitive APIs,
    comprehensive documentation, and strong community support, as these can significantly
    reduce the learning curve for you and your team. Avoid orchestrators that initiate
    hidden API calls or introduce latency to your applications. Additionally, ensure
    that the orchestrator can scale effectively as the number of applications, developers,
    and traffic grows.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑编排器的用户友好性。寻找直观的API、全面的文档和强大的社区支持，因为这些可以显著降低你和你团队的学习曲线。避免那些启动隐藏API调用或引入应用延迟的编排器。此外，确保编排器能够随着应用、开发人员和流量的增长而有效地扩展。
- en: User Feedback
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用户反馈
- en: 'User feedback has always played a critical role in software applications in
    two key ways: evaluating the application’s performance and informing its development.
    However, in AI applications, user feedback takes on an even more significant role.
    User feedback is proprietary data, and data is a competitive advantage. A well-designed
    user feedback system is necessary to create the data flywheel discussed in [Chapter 8](ch08.html#ch08_dataset_engineering_1730130932019888).^([7](ch10.html#id1816))'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 用户反馈在软件应用中始终扮演着至关重要的角色，主要体现在两个方面：评估应用性能和指导其开发。然而，在AI应用中，用户反馈的作用更为显著。用户反馈是专有数据，数据是竞争优势。一个设计良好的用户反馈系统对于创建第8章中讨论的数据飞轮至关重要[第8章](ch08.html#ch08_dataset_engineering_1730130932019888).^([7](ch10.html#id1816))
- en: User feedback can be used not only to personalize models for individual users
    but also to train future iterations of the models. As data becomes increasingly
    scarce, proprietary data is more valuable than ever. A product that launches quickly
    and attracts users early can gather data to continually improve models, making
    it difficult for competitors to catch up.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 用户反馈不仅可用于为单个用户个性化模型，还可以用于训练模型的未来迭代。随着数据变得越来越稀缺，专有数据比以往任何时候都更有价值。一个快速推出并早期吸引用户的产物可以收集数据以持续改进模型，这使得竞争对手难以赶上。
- en: It’s important to remember that user feedback is user data. Leveraging user
    feedback requires the same cautions needed when leveraging any data. User privacy
    should be respected. Users have the right to know how their data is being used.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，用户反馈是用户数据。利用用户反馈需要与利用任何数据时相同的谨慎。应尊重用户隐私。用户有权知道他们的数据是如何被使用的。
- en: Extracting Conversational Feedback
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取对话反馈
- en: Traditionally, feedback can be *explicit or implicit*. Explicit feedback is
    information users provide in response to explicit requests for feedback in the
    application, such as thumbs up/thumbs down, upvote/downvote, star rating, or a
    yes/no answer to the question “Did we solve your problem?” Explicit feedback is
    fairly standard across applications—there are only so many ways you can ask a
    person if they like something. Therefore, explicit feedback is better understood.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，反馈可以是**显式或隐式**的。显式反馈是用户在应用中对明确请求反馈时提供的信息，例如点赞/踩、点赞/踩、星级评分或对“我们解决了你的问题吗？”这一问题的肯定/否定回答。显式反馈在应用中相当标准化——询问一个人是否喜欢某物的途径有限。因此，显式反馈更容易理解。
- en: Implicit feedback is information inferred from user actions. For example, if
    someone buys a product recommended to them, it means it was a good recommendation.
    What can be considered implicit feedback depends on what actions a user can do
    within each application and is, therefore, highly application-dependent. Foundation
    models enable a new world of applications and, with them, many genres of implicit
    feedback.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 隐式反馈是从用户行为中推断出的信息。例如，如果有人购买了推荐给他们的产品，这意味着这是一个好的推荐。可以被视为隐式反馈的内容取决于用户在每种应用中可以执行哪些操作，因此它高度依赖于应用。基础模型开启了一个新的应用世界，以及许多隐式反馈的流派。
- en: The conversational interface that many AI applications use makes it easier for
    users to give feedback. Users can encourage good behaviors and correct errors
    the same way they would give feedback in daily dialogues. The language that a
    user uses to give directions to AI can convey feedback about both *the application’s
    performance* and *the user’s preference*.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 许多AI应用使用的对话界面使用户提供反馈变得更加容易。用户可以像在日常对话中一样鼓励良好行为和纠正错误。用户用来向AI下达指令的语言可以传达关于**应用性能**和**用户偏好**的反馈。
- en: 'As an example, imagine you’re using an AI assistant to help you plan your trip
    to Australia. You ask the AI to find a hotel for three nights in Sydney. It responds
    with three recommendations as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一下你正在使用一个AI助手来帮助你计划前往澳大利亚的旅行。你要求AI在悉尼为你找到三晚的酒店。它给出了以下三个推荐：
- en: '[PRE1]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How you respond to these three recommendations reveals your preference. For
    example, if you respond with “Yes book me the one close to galleries”, you show
    an interest in art. On the other hand, the response “Is there nothing under $200?”
    reveals a price-conscious preference and suggests that the assistant doesn’t quite
    get you yet.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 你对这些建议的反应揭示了你的偏好。例如，如果你回答“是的，预订靠近画廊的那家”，这表明你对艺术感兴趣。另一方面，回答“有没有低于200美元的？”则揭示了价格敏感的偏好，并暗示助手还没有完全理解你。
- en: 'User feedback, extracted from conversations, can be used for evaluation, development,
    and personalization:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 从对话中提取的用户反馈可用于评估、开发和个性化：
- en: 'Evaluation: derive metrics to monitor the application'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估：推导出用于监控应用的指标
- en: 'Development: train the future models or guide their development'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发：训练未来的模型或指导其发展
- en: 'Personalization: personalize the application to each user'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个性化：为每个用户个性化应用程序
- en: Implicit conversational feedback can be inferred from both the content of user
    messages and their patterns of communication. Because feedback is blended into
    daily conversations, it’s also challenging to extract. While intuition about conversational
    cues can help you devise an initial set of signals to look for, rigorous data
    analysis and user studies are necessary to understand.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 隐式对话反馈可以从用户消息的内容和他们的交流模式中推断出来。由于反馈融入了日常对话中，因此提取它也具有挑战性。虽然对对话提示的直觉可以帮助你设计出一套初始的信号来寻找，但严格的数据分析和用户研究是理解这些信号的必要条件。
- en: While conversational feedback has enjoyed greater attention thanks to the popularity
    of conversational bots, it had been an active research area for several years
    before ChatGPT came out. The reinforcement learning community has been trying
    to get RL algorithms to learn from natural language feedback since the late 2010s,
    many of them with promising results; see [Fu et al. (2019)](https://arxiv.org/abs/1902.07742);
    [Goyal et al. (2019)](https://arxiv.org/abs/1903.02020); [Zhou and Small (2020)](https://arxiv.org/abs/2008.06924);
    and [Sumers et al. (2020)](https://arxiv.org/abs/2009.14715)). Natural language
    feedback is also of great interest for early conversational AI applications such
    as Amazon Alexa ([Ponnusamy et al., 2019](https://arxiv.org/abs/1911.02557); [Park
    et al., 2020](https://arxiv.org/abs/2010.12251)), Spotify’s voice control feature
    ([Xiao et al., 2021](https://oreil.ly/m8o0h)), and Yahoo! Voice ([Hashimoto and
    Sassano, 2018](https://oreil.ly/bGAeG)).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管由于对话机器人的普及，对话反馈受到了更多的关注，但在ChatGPT出现之前，它已经是一个活跃的研究领域好几年了。自2010年代末以来，强化学习社区一直在尝试让RL算法从自然语言反馈中学习，其中许多取得了有希望的结果；参见[Fu等人（2019）](https://arxiv.org/abs/1902.07742)；[Goyal等人（2019）](https://arxiv.org/abs/1903.02020)；[Zhou和Small（2020）](https://arxiv.org/abs/2008.06924)；以及[Sumers等人（2020）](https://arxiv.org/abs/2009.14715))。自然语言反馈对于早期的对话人工智能应用（如Amazon
    Alexa [Ponnusamy等人，2019](https://arxiv.org/abs/1911.02557)；[Park等人，2020](https://arxiv.org/abs/2010.12251))、Spotify的语音控制功能([Xiao等人，2021](https://oreil.ly/m8o0h))和Yahoo!
    Voice([Hashimoto和Sassano，2018](https://oreil.ly/bGAeG)))也非常感兴趣。
- en: Natural language feedback
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然语言反馈
- en: Feedback extracted from the content of messages is called natural language feedback.
    Here are a couple of natural language feedback signals that tell you how a conversation
    is going. It’s useful to track these signals in production to monitor your application’s
    performance.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 从消息内容中提取的反馈被称为自然语言反馈。以下是一些自然语言反馈信号，它们可以告诉你对话的进展情况。在生产环境中跟踪这些信号对于监控应用程序的性能非常有用。
- en: Early termination
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提前终止
- en: If a user terminates a response early, e.g., stopping a response generation
    halfway, exiting the app (for web and mobile apps), telling the model to stop
    (for voice assistants), or simply leaving the agent hanging (e.g., not responding
    to the agent with which option you want it to go ahead with), it’s likely that
    the conversation isn’t going well.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户提前终止了响应，例如，在响应生成中途停止，退出应用程序（对于网页和移动应用程序），告诉模型停止（对于语音助手），或者简单地让代理悬空（例如，不回应你想让代理继续选择的选项），那么很可能是对话进行得不顺利。
- en: Error correction
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 错误纠正
- en: If a user starts their follow-up with “No, …” or “I meant, …”, the model’s response
    is likely off the mark.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户在后续发言时以“不，…”或“我的意思是，…”开头，那么模型的可能回答可能就不准确。
- en: To correct errors, users might try to rephrase their requests. [Figure 10-12](#ch10_figure_12_1730130985262750)
    shows an example of a user’s attempt to correct the model’s misunderstanding.
    Rephrase attempts can be detected using heuristics or ML models.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 为了纠正错误，用户可能会尝试重新措辞他们的请求。[图10-12](#ch10_figure_12_1730130985262750)展示了用户尝试纠正模型误解的一个例子。可以使用启发式方法或机器学习模型来检测重新措辞的尝试。
- en: '![A screenshot of a computer  Description automatically generated](assets/aien_1012.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](assets/aien_1012.png)'
- en: Figure 10-12\. Because the user both terminates the generation early and rephrases
    the question, it can be inferred that the model misunderstood the intent of the
    original request.
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-12。由于用户既提前终止了生成，又重新措辞了问题，可以推断出模型误解了原始请求的意图。
- en: 'Users can also point out specific things the model should’ve done differently.
    For example, if a user asks the model to summarize a story and the model confuses
    a character, this user can give feedback such as: “Bill is the suspect, not the
    victim.” The model should be able to take this feedback and revise the summary.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 用户还可以指出模型应该有所不同的事情。例如，如果用户要求模型总结一个故事，而模型混淆了角色，这个用户可以提供如下反馈：“比尔是嫌疑人，而不是受害者。”模型应该能够接受这种反馈并修改总结。
- en: This kind of action-correcting feedback is especially common for agentic use
    cases where users might nudge the agent toward more optional actions. For example,
    if a user assigns the agent the task of doing market analysis about company XYZ,
    this user might give feedback such as “You should also check XYZ GitHub page”
    or “Check the CEO’s X profile”.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这种行动纠正的反馈在代理用例中尤其常见，在这些用例中，用户可能会推动代理采取更多可选的行动。例如，如果用户指派代理执行关于公司XYZ的市场分析任务，这个用户可能会提供如下反馈：“你也应该检查XYZ的GitHub页面”或“检查CEO的X档案”。
- en: Sometimes, users might want the model to correct itself by asking for explicit
    confirmation, such as “Are you sure?”, “Check again”, or “Show me the sources”.
    This doesn’t necessarily mean that the model gives wrong answers. However, it
    might mean that your model’s answers lack the details the user is looking for.
    It can also indicate general distrust in your model.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，用户可能希望模型通过请求明确的确认来纠正自己，例如“你确定吗？”、“再检查一下”或“给我看看来源”。这并不一定意味着模型给出了错误的答案。然而，它可能意味着你的模型答案缺少用户正在寻找的细节。这也可能表明用户对你模型的普遍不信任。
- en: Some applications let users edit the model’s responses directly. For example,
    if a user asks the model to generate code, and the user corrects the generated
    code, it’s a very strong signal that the code that got edited isn’t quite right.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 一些应用程序允许用户直接编辑模型的回答。例如，如果用户要求模型生成代码，而用户纠正了生成的代码，这表明被编辑的代码可能并不完全正确。
- en: User edits also serve as a valuable source of preference data. Recall that preference
    data, typically in the format of (query, winning response, losing response), can
    be used to align a model to human preference. Each user edit makes up a preference
    example, with the original generated response being the losing response and the
    edited response being the winning response.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 用户编辑也作为有价值的选择数据来源。回想一下，选择数据通常以（查询，获胜回答，失败回答）的格式存在，可以用来将模型与人类偏好对齐。每个用户编辑都构成一个偏好示例，原始生成的回答是失败回答，而编辑后的回答是获胜回答。
- en: Complaints
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 投诉
- en: Often, users just complain about your application’s outputs without trying to
    correct them. For example, they might complain that an answer is wrong, irrelevant,
    toxic, lengthy, lacking detail, or just bad. [Table 10-1](#ch10_table_1_1730130985279574)
    shows eight groups of natural language feedback resulting from automatic clustering
    the FITS (Feedback for Interactive Talk & Search) dataset ([Xu et al., 2022](https://arxiv.org/abs/2208.03270)).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，用户只是对你的应用程序的输出进行投诉，而不尝试纠正它们。例如，他们可能会投诉答案错误、不相关、有毒、冗长、缺乏细节或只是不好。[表10-1](#ch10_table_1_1730130985279574)显示了从自动聚类FITS（交互式对话与搜索反馈）数据集（Xu等人，2022年）得到的八组自然语言反馈。
- en: Table 10-1\. Feedback types derived from automatic clustering the FITS dataset
    (Xu et al., 2022). Results from [Yuan et al. (2023)](https://arxiv.org/abs/2306.13588).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 表10-1\. 从自动聚类FITS数据集（Xu等人，2022年）得到的反馈类型。结果来自[袁等人（2023）](https://arxiv.org/abs/2306.13588)。
- en: '| Group | Feedback type | Num. | % |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 组 | 反馈类型 | 数量 | % |'
- en: '| --- | --- | --- | --- |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | Clarify their demand again. | 3702 | 26.54% |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 再次明确他们的需求。 | 3702 | 26.54% |'
- en: '| 2 | Complain that the bot (1) does not answer the question or (2) gives irrelevant
    information or (3) asks the user to find out the answer on their own. | 2260 |
    16.20% |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 投诉机器人（1）没有回答问题或（2）提供不相关信息或（3）要求用户自己找出答案。 | 2260 | 16.20% |'
- en: '| 3 | Point out specific search results that can answer the question. | 2255
    | 16.17% |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 指出可以回答问题的具体搜索结果。 | 2255 | 16.17% |'
- en: '| 4 | Suggest that the bot should use the search results. | 2130 | 15.27% |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 建议机器人应该使用搜索结果。 | 2130 | 15.27% |'
- en: '| 5 | State that the answer is (1) factually incorrect, or (2) not grounded
    in the search results. | 1572 | 11.27% |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 声明答案（1）事实错误，或（2）没有基于搜索结果。 | 1572 | 11.27% |'
- en: '| 6 | Point out that the bot’s answer is not specific/accurate/complete/detailed.
    | 1309 | 9.39% |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 指出机器人的答案不具体/准确/完整/详细。 | 1309 | 9.39% |'
- en: '| 7 | Point out that the bot is not confident in its answers and always begins
    its responses with “I am not sure” or “I don’t know”. | 582 | 4.17% |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 指出机器人对其答案缺乏信心，并且总是以“我不确定”或“我不知道”开始其响应。 | 582 | 4.17% |'
- en: '| 8 | Complain about repetition/rudeness in bot responses. | 137 | 0.99% |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 投诉机器人响应中的重复/粗鲁。 | 137 | 0.99% |'
- en: Understanding how the bot fails the user is crucial in making it better. For
    example, if you know that the user doesn’t like verbose answers, you can change
    the bot’s prompt to make it more concise. If the user is unhappy because the answer
    lacks details, you can prompt the bot to be more specific.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 理解机器人如何让用户失望对于改进它至关重要。例如，如果你知道用户不喜欢冗长的答案，你可以改变机器人的提示使其更加简洁。如果用户因为答案缺乏细节而不高兴，你可以提示机器人更加具体。
- en: Sentiment
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 情感
- en: Complaints can also be general expressions of negative sentiments (frustration,
    disappointment, ridicule, etc.) without explaining the reason why, such as “Uggh”.
    This might sound dystopian, but analysis of a user’s sentiments throughout conversations
    with a bot might give you insights into how the bot is doing. Some call centers
    track users’ voices throughout the calls. If a user gets increasingly loud, something
    is wrong. Conversely, if someone starts a conversation angry but ends happily,
    the conversation might have resolved their issue.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 投诉也可以是未解释原因的负面情感（挫败感、失望、嘲讽等）的一般表达，例如“呃”。这可能听起来很反乌托邦，但分析用户与机器人对话中的情感可能会让你了解机器人做得如何。一些呼叫中心跟踪用户在整个通话中的声音。如果用户的声音越来越大，说明出了问题。相反，如果有人开始时很生气但结束时很高兴，那么对话可能已经解决了他们的问题。
- en: Natural language feedback can also be inferred from the model’s responses. One
    important signal is the model’s *refusal rate*. If a model says things like “Sorry,
    I don’t know that one” or “As a language model, I can’t do …”, the user is probably
    unhappy.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言反馈也可以从模型的响应中推断出来。一个重要的信号是模型的*拒绝率*。如果一个模型说“对不起，我不知道那一个”或“作为一个语言模型，我做不到…”，用户可能是不高兴的。
- en: Other conversational feedback
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他对话反馈
- en: Other types of conversational feedback can be derived from user actions instead
    of messages.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类型的对话反馈可以从用户行为而不是消息中得出。
- en: Regeneration
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 再生
- en: Many applications let users generate another response, sometimes with a different
    model. If a user chooses regeneration, it might be because they’re not satisfied
    with the first response. However, it might also be that the first response is
    adequate, but the user wants options to compare. This is especially common with
    creative requests like image or story generation.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用允许用户生成另一个响应，有时使用不同的模型。如果用户选择再生，可能是因为他们对第一个响应不满意。然而，也可能是因为第一个响应是足够的，但用户想要比较的选项。这在像图像或故事生成这样的创意请求中尤其常见。
- en: Regeneration signals might also be stronger for applications with usage-based
    billing than those with subscriptions. With usage-based billing, users are less
    likely to regenerate and spend extra money out of idle curiosity.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于订阅的应用，再生信号可能在基于使用量的计费应用中更强。在使用量计费的情况下，用户不太可能再生并花费额外的钱出于无聊的好奇心。
- en: Personally, I often choose regeneration for complex requests to ensure the model’s
    responses are consistent. If two responses give contradicting answers, I can’t
    trust either.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 个人来说，我经常选择再生来处理复杂请求，以确保模型的响应是一致的。如果两个响应给出了相互矛盾的答案，我就无法信任任何一个。
- en: After regeneration, some applications might explicitly ask to compare the new
    response with the previous one, as shown in [Figure 10-13](#ch10_figure_13_1730130985262768).
    This better or worse data, again, can be used for preference finetuning.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在再生之后，一些应用可能会明确要求将新的响应与之前的响应进行比较，如图[图10-13](#ch10_figure_13_1730130985262768)所示。这些更好或更差的数据，同样可以用于偏好微调。
- en: '![A white background with a black circle  Description automatically generated
    with medium confidence](assets/aien_1013.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![一个白色背景和黑色圆圈，描述自动生成，中等置信度](assets/aien_1013.png)'
- en: Figure 10-13\. ChatGPT asks for comparative feedback when a user regenerates
    another response.
  id: totrans-267
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-13。当用户再生另一个响应时，ChatGPT请求比较反馈。
- en: Conversation organization
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对话组织
- en: The actions a user takes to organize their conversations—such as delete, rename,
    share, and bookmark—can also be signals. Deleting a conversation is a pretty strong
    signal that the conversation is bad, unless it’s an embarrassing conversation
    and the user wants to remove its trace. Renaming a conversation suggests that
    the conversation is good, but the auto-generated title is bad.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 用户为组织他们的对话所采取的行动——例如删除、重命名、分享和书签——也可以作为信号。删除对话是一个非常强烈的信号，表明对话不好，除非这是一个尴尬的对话，而用户想要消除其痕迹。重命名对话暗示着对话很好，但自动生成的标题不好。
- en: Conversation length
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对话长度
- en: Another commonly tracked signal is *the number of turns per conversation*. Whether
    this is a positive or negative signal depends on the application. For AI companions,
    a long conversation might indicate that the user enjoys the conversation. However,
    for chatbots geared toward productivity like customer support, a long conversation
    might indicate that the bot is inefficient in helping users resolve their issues.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个通常跟踪的信号是*每次对话的轮数*。这究竟是正面还是负面的信号取决于应用。对于人工智能伴侣，长时间的对话可能表明用户喜欢这次对话。然而，对于面向生产力的聊天机器人，如客户支持，长时间的对话可能表明机器人帮助用户解决问题效率不高。
- en: Dialogue diversity
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对话多样性
- en: Conversation length can also be interpreted together with *dialogue diversity*,
    which can be measured by the distinct token or topic count. For example, if the
    conversation is long but the bot keeps repeating a few lines, the user might be
    stuck in a loop.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对话长度也可以与*对话多样性*一起解释，这可以通过不同的标记或主题计数来衡量。例如，如果对话很长，但机器人一直在重复几行，用户可能会陷入循环。
- en: Explicit feedback is easier to interpret, but it demands extra effort from users.
    Since many users may not be willing to put in this additional work, explicit feedback
    can be sparse, especially in applications with smaller user bases. Explicit feedback
    also suffers from response biases. For example, unhappy users might be more likely
    to complain, causing the feedback to appear more negative than it is.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 明确的反馈更容易解释，但它需要用户付出额外的努力。由于许多用户可能不愿意投入额外的工作，明确的反馈可能很少，尤其是在用户基数较小的应用中。明确的反馈还受到响应偏差的影响。例如，不满意的用户可能更倾向于抱怨，导致反馈看起来比实际情况更负面。
- en: Implicit feedback is more abundant—what can be considered implicit feedback
    is limited only by your imagination—but it’s noisier. Interpreting implicit signals
    can be challenging. For example, sharing a conversation can either be a negative
    or a positive signal. For example, one friend of mine mostly shares conversations
    when the model has made some glaring mistakes, and another friend mostly shares
    useful conversations with their coworkers. *It’s important to study your users
    to understand why they do each action*.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 隐含的反馈更丰富——可以被认为是隐含反馈的东西仅限于你的想象力，但它更嘈杂。解释隐含信号可能具有挑战性。例如，分享对话可以是正面或负面的信号。例如，我的一个朋友在模型犯了一些明显的错误时主要分享对话，而另一个朋友主要与同事分享有用的对话。*重要的是要研究你的用户，了解他们为什么采取每个行动*。
- en: Adding more signals can help clarify the intent. For example, if the user rephrases
    their question after sharing a link, it might indicate that the conversation didn’t
    meet their expectations. Extracting, interpreting, and leveraging implicit responses
    from conversations is a small but growing area of research.^([8](ch10.html#id1838))
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 添加更多信号可以帮助阐明意图。例如，如果用户在分享链接后重新表述了他们的问题，这可能表明对话没有达到他们的期望。从对话中提取、解释和利用隐含响应是一个小但正在增长的研究领域.^([8](ch10.html#id1838))
- en: Feedback Design
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反馈设计
- en: If you were unsure of what feedback to collect, I hope that the last section
    gave you some ideas.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不确定要收集哪些反馈，我希望上一节能给您一些想法。
- en: This section discusses when and how to collect this valuable feedback.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论何时以及如何收集这些宝贵的反馈。
- en: When to collect feedback
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 何时收集反馈
- en: Feedback can and should be collected throughout the user journey. Users should
    have the option to give feedback, especially to report errors, whenever this need
    arises. The feedback collection option, however, should be nonintrusive. It shouldn’t
    interfere with the user workflow. Here are a few places where user feedback might
    be particularly valuable.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 反馈可以在用户旅程的任何阶段收集，并且用户应该有提供反馈的选择，尤其是在需要报告错误时。然而，反馈收集选项应该是非侵入性的。它不应该干扰用户的工作流程。以下是一些用户反馈可能特别有价值的地方。
- en: In the beginning
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在开始时
- en: When a user has just signed up, user feedback can help calibrate the application
    for the user. For example, a face ID app first must scan your face to work. A
    voice assistant might ask you to read a sentence out loud to recognize your voice
    for wake words (words that activate a voice assistant, like “Hey Google”). A language
    learning app might ask you a few questions to gauge your skill level. For some
    applications, such as face ID, calibration is necessary. For other applications,
    however, initial feedback should be optional, as it creates friction for users
    to try out your product. If a user doesn’t specify their preference, you can fall
    back to a neutral option and calibrate over time.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户刚刚注册时，用户反馈可以帮助调整应用程序以适应用户。例如，一个面部识别应用程序首先必须扫描您的面部才能工作。语音助手可能会要求您大声朗读一句话以识别您的声音作为唤醒词（激活语音助手的词语，如“嘿，谷歌”）。语言学习应用程序可能会问您几个问题以评估您的技能水平。对于某些应用程序，如面部识别，校准是必要的。然而，对于其他应用程序，初始反馈应该是可选的，因为它会给用户尝试您的产品带来摩擦。如果用户没有指定他们的偏好，您可以退回到一个中性的选项，并随着时间的推移进行调整。
- en: When something bad happens
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 当发生不好的事情时
- en: When the model hallucinates a response, blocks a legitimate request, generates
    a compromising image, or takes too long to respond, users should be able to notify
    you of these failures. You can give users the option to downvote a response, regenerate
    with the same model, or change to another model. Users might just give conversational
    feedback like “You’re wrong”, “Too cliche”, or “I want something shorter”.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型产生幻觉的响应、阻止合法请求、生成令人尴尬的图像或响应时间过长时，用户应该能够通知您这些失败。您可以给用户选择对响应进行踩、使用相同模型重新生成或切换到另一个模型的机会。用户可能会给出类似“你错了”、“太陈词滥调”或“我想要更简短的内容”这样的对话反馈。
- en: Ideally, when your product makes mistakes, users should still be able to accomplish
    their tasks. For example, if the model wrongly categorizes a product, users can
    edit the category. Let users collaborate with the AI. If that doesn’t work, let
    them collaborate with humans. Many customer support bots offer to transfer users
    to human agents if the conversation drags on or if users seem frustrated.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，当您的产品出错时，用户仍然应该能够完成他们的任务。例如，如果模型错误地将产品分类，用户可以编辑分类。让用户与 AI 协作。如果这不起作用，让他们与人类协作。许多客户支持机器人如果对话拖沓或用户看起来很沮丧，会提供将用户转移到人工代理的服务。
- en: An example of human–AI collaboration is the *inpainting* functionality for image
    generation.^([9](ch10.html#id1842)) If a generated image isn’t exactly what the
    user needs, they can select a region of the image and describe with a prompt how
    to make it better. [Figure 10-14](#ch10_figure_14_1730130985262784) shows an example
    of inpainting with [DALL-E](https://oreil.ly/Edew9) (OpenAI, 2021). This feature
    allows users to get better results while giving developers high-quality feedback.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 人机协作的一个例子是图像生成中的 *修复* 功能。[9](ch10.html#id1842)。如果生成的图像不是用户需要的，他们可以选择图像的一部分，并通过提示描述如何使其更好。图
    [10-14](#ch10_figure_14_1730130985262784) 展示了使用 [DALL-E](https://oreil.ly/Edew9)（OpenAI，2021）的修复示例。此功能允许用户获得更好的结果，同时为开发者提供高质量的反馈。
- en: '![A screenshot of a comic book  Description automatically generated](assets/aien_1014.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![漫画书的截图  自动生成的描述](assets/aien_1014.png)'
- en: Figure 10-14\. An example of how inpainting works in DALL-E. Image by [OpenAI](https://oreil.ly/nAplp).
  id: totrans-289
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-14\. DALL-E 中修复图像工作原理的示例。图片由 [OpenAI](https://oreil.ly/nAplp) 提供。
- en: When the model has low confidence
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 当模型信心不足时
- en: When a model is uncertain about an action, you can ask the user for feedback
    to increase its confidence. For example, given a request to summarize a paper,
    if the model is uncertain whether the user would prefer a short, high-level summary
    or a detailed section-by-section summary, the model can output both summaries
    side by side, assuming that generating two summaries doesn’t increase the latency
    for the user. The user can choose which one they prefer. Comparative signals like
    this can be used for preference finetuning. An example of comparative evaluation
    in production is shown in [Figure 10-15](#ch10_figure_15_1730130985262799).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型对某个动作不确定时，您可以要求用户反馈以增加其信心。例如，给定一个总结论文的请求，如果模型不确定用户更喜欢简短的高级总结还是详细的逐节总结，模型可以同时输出两个总结，假设生成两个总结不会增加用户的延迟。用户可以选择他们更喜欢哪一个。这样的比较信号可以用于偏好微调。生产中比较评估的一个例子如图
    [10-15](#ch10_figure_15_1730130985262799) 所示。
- en: '![A screenshot of a chat  Description automatically generated](assets/aien_1015.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![聊天截图  自动生成的描述](assets/aien_1015.png)'
- en: Figure 10-15\. Side-by-side comparison of two ChatGPT responses.
  id: totrans-293
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-15。两个ChatGPT响应的并排比较。
- en: Showing two full responses for the user to choose means asking that user for
    explicit feedback. Users might not have time to read two full responses or care
    enough to give thoughtful feedback. This can result in noisy votes. Some applications,
    like Google Gemini, show only the beginning of each response, as shown in [Figure 10-16](#ch10_figure_16_1730130985262822).
    Users can click to expand the response they want to read. It’s unclear, however,
    whether showing full or partial responses side by side gives more reliable feedback.^([10](ch10.html#id1844))
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 让用户选择两个完整的响应意味着要求用户提供明确的反馈。用户可能没有时间阅读两个完整的响应，或者不够关心以至于无法提供深思熟虑的反馈。这可能导致投票噪声。一些应用程序，如谷歌Gemini，只显示每个响应的开头，如图[图10-16](#ch10_figure_16_1730130985262822)所示。用户可以点击展开他们想阅读的响应。然而，不清楚并排显示完整或部分响应是否提供了更可靠的反馈。[10](ch10.html#id1844)
- en: '![A screenshot of a computer  Description automatically generated](assets/aien_1016.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](assets/aien_1016.png)'
- en: Figure 10-16\. Google Gemini shows partial responses side by side for comparative
    feedback. Users have to click on the response they want to read more about, which
    gives feedback about which response they find more promising.
  id: totrans-296
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-16。谷歌Gemini将部分响应并排显示，以便进行比较反馈。用户必须点击他们想了解更多信息的响应，这提供了关于他们认为哪个响应更有希望的反馈。
- en: Another example is a photo organization application that automatically tags
    your photos, so that it can respond to queries like “Show me all the photos of
    X”. When unsure if two people are the same, it can ask you for feedback, as Google
    Photos does in [Figure 10-17](#ch10_figure_17_1730130985262838).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是一个自动标记您照片的图片组织应用程序，以便它可以响应像“显示我所有X的照片”这样的查询。当不确定两个人是否是同一个人时，它可以像[图10-17](#ch10_figure_17_1730130985262838)中的谷歌照片那样请求您的反馈。
- en: '![A screenshot of a cartoon cat  Description automatically generated](assets/aien_1017.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![卡通猫的屏幕截图  自动生成的描述](assets/aien_1017.png)'
- en: Figure 10-17\. Google Photos asks for user feedback when unsure. The two cat
    images were generated by ChatGPT.
  id: totrans-299
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-17。当不确定时，谷歌照片会请求用户反馈。这两张猫图像是由ChatGPT生成的。
- en: 'You might wonder: how about feedback when something good happens? Actions that
    users can take to express their satisfaction include thumbs up, favoriting, or
    sharing. However, Apple’s [human interface guideline](https://oreil.ly/GeZvj)
    warns against asking for both positive and negative feedback. Your application
    should produce good results by default. Asking for feedback on good results might
    give users the impression that good results are exceptions. Ultimately, if users
    are happy, they continue using your application.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想：当发生好事时，反馈怎么办？用户可以采取的行动来表达他们的满意包括点赞、收藏或分享。然而，苹果的[人机界面指南](https://oreil.ly/GeZvj)警告不要同时请求正面和负面反馈。您的应用程序应该默认产生良好的结果。对良好结果的反馈可能会给用户留下好结果是例外的印象。最终，如果用户满意，他们会继续使用您的应用程序。
- en: However, many people I’ve talked to believe users should have the option to
    give feedback when they encounter something amazing. A product manager for a popular
    AI-powered product mentioned that their team needs positive feedback because it
    reveals the features users love enough to give enthusiastic feedback about. This
    allows the team to concentrate on refining a small set of high-impact features
    rather than spreading resources across many with minimal added value.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我交谈过的许多人认为用户应该有在遇到令人惊叹的事情时提供反馈的选项。一个流行的AI产品产品经理提到，他们的团队需要正面反馈，因为这揭示了用户足够喜欢以至于愿意提供热情反馈的功能。这使团队能够专注于改进一小部分具有高影响力的功能，而不是将资源分散到许多具有最小附加价值的功能上。
- en: Some avoid asking for positive feedback out of concern it may clutter the interface
    or annoy users. However, this risk can be managed by limiting the frequency of
    feedback requests. For example, if you have a large user base, showing the request
    to only 1% of users at a time could help gather sufficient feedback without disrupting
    the experience for most users. Keep in mind that the smaller the percentage of
    users asked, the greater the risk of feedback biases. Still, with a large enough
    pool, the feedback can provide meaningful product insights.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 一些人在担心可能会使界面杂乱或打扰用户的情况下避免寻求正面反馈。然而，通过限制反馈请求的频率可以管理这种风险。例如，如果你有一个庞大的用户群，一次只向1%的用户展示请求可以帮助收集足够的反馈，同时不会干扰大多数用户的体验。记住，请求的用户百分比越小，反馈偏差的风险就越大。然而，如果用户池足够大，反馈可以提供有意义的
    产品洞见。
- en: How to collect feedback
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何收集反馈
- en: Feedback should seamlessly integrate into the user’s workflow. It should be
    easy for users to provide feedback without extra work. Feedback collection shouldn’t
    disrupt user experience and should be easy to ignore. There should be incentives
    for users to give good feedback.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 反馈应无缝集成到用户的工作流程中。用户应能轻松提供反馈而无需额外工作。反馈收集不应干扰用户体验，并且应易于忽略。应该有激励用户给出良好反馈的措施。
- en: 'One example often cited as good feedback design is from the image generator
    app Midjourney. For each prompt, Midjourney generates a set of (four) images and
    gives the user the following options, as shown in [Figure 10-18](#ch10_figure_18_1730130985262861):'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经常被引用的好的反馈设计例子来自图像生成应用Midjourney。对于每个提示，Midjourney生成一组（四张）图像，并给用户以下选项，如图[图10-18](#ch10_figure_18_1730130985262861)所示：
- en: Generate an unscaled version of any of these images.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成这些图像中的任何一张的未缩放版本。
- en: Generate variations for any of these images.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为这些图像中的任何一张生成变体。
- en: Regenerate.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新生成。
- en: All these options give Midjourney different signals. Options 1 and 2 tell Midjourney
    which of the four photos is considered by the user to be the most promising. Option
    1 gives the strongest positive signal about the chosen photo. Option 2 gives a
    weaker positive signal. Option 3 signals that none of the photos is good enough.
    However, users might choose to regenerate even if the existing photos are good
    just to see what else is possible.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些选项都向Midjourney发送了不同的信号。选项1和2告诉Midjourney用户认为哪四张照片中最有希望。选项1给出了关于所选照片的最强烈的正面信号。选项2给出了较弱的正面信号。选项3表示没有一张照片足够好。然而，即使现有的照片很好，用户也可能选择重新生成，只是为了看看还有什么是可能的。
- en: '![A screenshot of a video game  Description automatically generated](assets/aien_1018.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![视频游戏截图  自动生成的描述](assets/aien_1018.png)'
- en: Figure 10-18\. Midjourney’s workflow allows the app to collect implicit feedback.
  id: totrans-311
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-18。Midjourney的工作流程允许应用收集隐式反馈。
- en: Code assistants like GitHub Copilot might show their drafts in lighter colors
    than the final texts, as shown in [Figure 10-19](#ch10_figure_19_1730130985262880).
    Users can use the Tab key to accept a suggestion or simply continue typing to
    ignore the suggestion, both providing feedback.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 代码助手（如GitHub Copilot）可能会用比最终文本更浅的颜色显示他们的草稿，如图[图10-19](#ch10_figure_19_1730130985262880)所示。用户可以使用Tab键接受建议，或者简单地继续输入以忽略建议，这两种方式都在提供反馈。
- en: '![A screenshot of a computer program  Description automatically generated](assets/aien_1019.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序截图  自动生成的描述](assets/aien_1019.png)'
- en: Figure 10-19\. GitHub Copilot makes it easy to both suggest and reject a suggestion.
  id: totrans-314
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-19。GitHub Copilot使得提出和拒绝建议都变得容易。
- en: One of the biggest challenges of standalone AI applications like ChatGPT and
    Claude is that they aren’t integrated into the user’s daily workflow, making it
    hard to collect high-quality feedback the way integrated products like GitHub
    Copilot can. For example, if Gmail suggests an email draft, Gmail can track how
    this draft is used or edited. However, if you use ChatGPT to write an email, ChatGPT
    doesn’t know whether the generated email is actually sent.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 独立人工智能应用（如ChatGPT和Claude）的最大挑战之一是它们没有集成到用户的日常工作中，这使得收集高质量反馈变得困难，就像GitHub Copilot这样的集成产品一样。例如，如果Gmail建议一个电子邮件草稿，Gmail可以跟踪这个草稿是如何被使用或编辑的。然而，如果你使用ChatGPT来写电子邮件，ChatGPT并不知道生成的电子邮件是否实际上被发送。
- en: The feedback alone might be helpful for product analytics. For example, seeing
    just the thumbs up/thumbs down information is useful for calculating how often
    people are happy or unhappy with your product. For deeper analysis, though, you
    would need context around the feedback, such as the previous 5 to 10 dialogue
    turns. This context can help you figure out what went wrong. However, getting
    this context might not be possible without explicit user consent, especially if
    the context might contain personally identifiable information.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 单独的反馈可能对产品分析有帮助。例如，仅查看点赞/点踩信息对于计算人们对你产品的满意或不满意程度很有用。然而，对于更深入的分析，你需要围绕反馈的上下文，例如前5到10个对话回合。这些上下文可以帮助你找出哪里出了问题。然而，如果没有明确用户同意，可能无法获得这些上下文，尤其是如果上下文可能包含可识别个人信息的部分。
- en: For this reason, some products include terms in their service agreements that
    allow them to access user data for analytics and product improvement. For applications
    without such terms, user feedback might be tied to a user data donation flow,
    where users are asked to donate (e.g., share) their recent interaction data along
    with their feedback. For example, when submitting feedback, you might be asked
    to check a box to share your recent data as context for this feedback.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一些产品在其服务协议中包含允许他们访问用户数据以进行分析和产品改进的条款。对于没有此类条款的应用程序，用户反馈可能与用户数据捐赠流程相关联，用户被要求捐赠（例如，分享）他们的最近交互数据以及他们的反馈。例如，在提交反馈时，您可能被要求勾选一个框以分享您最近的数据作为此反馈的背景。
- en: Explaining to users how their feedback is used can motivate them to give more
    and better feedback. Do you use a user’s feedback to personalize the product to
    this user, to collect statistics about general usage, or to train a new model?
    If users are concerned about privacy, reassure them that their data won’t be used
    to train models or won’t leave their device (only if these are true).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 向用户解释他们的反馈如何被使用可以激励他们提供更多和更好的反馈。您是否使用用户的反馈来个性化产品以适应该用户，收集关于一般使用的统计数据，或者训练新模型？如果用户对隐私感到担忧，请向他们保证他们的数据不会被用于训练模型或离开他们的设备（只有当这些是真的时）。
- en: Don’t ask users to do the impossible. For example, if you collect comparative
    signals from users, don’t ask them to choose between two options they don’t understand.
    For example, I was once stumped when ChatGPT asked me to choose between two possible
    answers to a statistical question, as shown in [Figure 10-20](#ch10_figure_20_1730130985262906).
    I wish there was an option for me to say, “I don’t know”.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 不要要求用户做不可能的事情。例如，如果您从用户那里收集比较信号，不要要求他们在他们不理解的两个选项之间进行选择。例如，我曾经在ChatGPT要求我选择一个统计问题的两个可能答案时感到困惑，如图10-20所示。我希望有一个选项让我可以说，“我不知道”。
- en: '![A screenshot of a chat  Description automatically generated](assets/aien_1020.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![聊天截图  自动生成的描述](assets/aien_1020.png)'
- en: Figure 10-20\. An example of ChatGPT asking a user to select the response the
    user prefers. However, for mathematical questions like this, the right answer
    shouldn’t be a matter of preference.
  id: totrans-321
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-20。ChatGPT要求用户选择他们偏好的回答的例子。然而，对于像这样的数学问题，正确答案不应该是主观偏好的问题。
- en: Add icons and tooltips to an option if they help people understand it. Avoid
    a design that can confuse users. Ambiguous instructions can lead to noisy feedback.
    I once hosted a GPU optimization workshop, using Luma to collect feedback. When
    I was reading the negative feedback, I was confused. Even though the responses
    were positive, the star ratings were 1/5\. When I dug deeper, I realized that
    Luma used emojis to represent numbers in their feedback collection form, but the
    angry emoji, corresponding to a one-star rating, was put where the five-star rating
    should be, as shown in [Figure 10-21](#ch10_figure_21_1730130985262932).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图标和工具提示有助于人们理解选项，请添加它们。避免可能让用户困惑的设计。含糊不清的说明可能导致嘈杂的反馈。我曾经举办了一个GPU优化研讨会，使用Luma收集反馈。当我阅读负面反馈时，我很困惑。尽管这些回应是积极的，但星级评价是1/5。当我深入了解时，我意识到Luma在他们的反馈收集表中使用表情符号来表示数字，但愤怒的表情符号（对应一星评价）被放在了本应放置五星评价的位置，如图10-21所示。
- en: Be mindful of whether you want users’ feedback to be private or public. For
    example, if a user likes something, do you want this information shown to other
    users? In its early days, Midjourney’s feedback—someone choosing to upscale an
    image, generate variations, or regenerate another batch of images—was public.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 注意您是否希望用户的反馈是私密的还是公开的。例如，如果用户喜欢某样东西，您是否希望将此信息显示给其他用户？在Midjourney的早期，其反馈——有人选择提升图像、生成变体或重新生成另一批图像——是公开的。
- en: '![A screenshot of a computer screen  Description automatically generated](assets/aien_1021.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](assets/aien_1021.png)'
- en: Figure 10-21\. Because Luma put the angry emoji, corresponding to a one-star
    rating, where a five-star rating should’ve been, some users mistakenly picked
    it for positive reviews.
  id: totrans-325
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-21。因为Luma将愤怒的表情符号（对应一星评价）放在了本应放置五星评价的位置，一些用户错误地将其选为正面评价。
- en: The visibility of a signal can profoundly impact user behavior, user experience,
    and the quality of the feedback. Users tend to be more candid in private—there’s
    a lower chance of their activities being judged^([11](ch10.html#id1846))—which
    can result in higher-quality signals. In 2024, X (formerly Twitter) made “likes”
    [private](https://x.com/elonmusk/status/1800905349148664295). Elon Musk, the owner
    of X, claimed a significant [uptick in the number of likes](https://x.com/elonmusk/status/1801045558318313746)
    after this change.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 信号的可见性可以深刻影响用户行为、用户体验和反馈质量。用户在私下里往往更坦率——他们活动的被评判机会更低^([11](ch10.html#id1846))，这可能导致更高质量的信号。在2024年，X（前身为Twitter）将“点赞”[设为私有](https://x.com/elonmusk/status/1800905349148664295)。X的所有者埃隆·马斯克声称，在此变更后点赞数量显著[上升](https://x.com/elonmusk/status/1801045558318313746)。
- en: However, private signals can reduce discoverability and explainability. For
    example, hiding likes prevents users from finding tweets their connections have
    liked. If X recommends tweets based on the likes of the people you follow, hiding
    likes could result in users’ confusion about why certain tweets appear in their
    feeds.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，私人信号可能会降低可发现性和可解释性。例如，隐藏点赞阻止用户找到他们关注的人点赞的推文。如果X根据你关注的人的点赞来推荐推文，隐藏点赞可能会导致用户对某些推文出现在他们的动态中感到困惑。
- en: Feedback Limitations
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反馈限制
- en: There’s no doubt of the value of user feedback to an application developer.
    However, feedback isn’t a free lunch. It comes with its own limitations.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 用户反馈对应用开发者来说无疑具有价值。然而，反馈并非免费的午餐。它有其自身的局限性。
- en: Biases
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏差
- en: 'Like any other data, user feedback has biases. It’s important to understand
    these biases and design your feedback system around them. Each application has
    its own biases. Here are a few examples of feedback biases to give you an idea
    of what to look out for:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何其他数据一样，用户反馈也有偏差。了解这些偏差，并围绕它们设计你的反馈系统是很重要的。每个应用都有自己的偏差。以下是一些反馈偏差的例子，以帮助你了解需要注意什么：
- en: Leniency bias
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 宽容偏差
- en: Leniency bias is the tendency for people to rate items more positively than
    warranted, often to avoid conflict because they feel compelled to be nice or because
    it’s the easiest option. Imagine you’re in a hurry, and an app asks you to rate
    a transaction. You aren’t happy with the transaction, but you know that if you
    rate it negatively, you’ll be asked to provide reasons, so you just choose positive
    to be done with it. This is also why you shouldn’t make people do extra work for
    your feedback.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 宽容偏差是指人们倾向于对物品给予比应得更积极的评价，这通常是为了避免冲突，因为他们感到有必要表现得友好，或者因为这是最容易的选择。想象一下你很匆忙，一个应用要求你评价一笔交易。你对这笔交易不满意，但你知道如果你给出负面评价，你将需要提供理由，所以你只是选择正面评价来结束这件事。这也是为什么你不应该让人们为你提供反馈而做额外工作的原因。
- en: On a five-star rating scale, four and five stars are typically meant to indicate
    a good experience. However, in many cases, users may feel pressured to give five-star
    ratings, reserving four stars for when something goes wrong. According to [Uber](https://oreil.ly/18tY4),
    in 2015, the average driver’s rating was 4.8, with scores below 4.6 putting drivers
    at risk of being deactivated.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在五星级评分尺度上，四星和五星通常用来表示良好的体验。然而，在许多情况下，用户可能会感到有压力给出五星评价，将四星保留给出现问题时的情况。根据[Uber](https://oreil.ly/18tY4)，2015年，平均司机的评分是4.8，评分低于4.6的司机面临被停用的风险。
- en: This bias isn’t necessarily a dealbreaker. Uber’s goal is to differentiate good
    drivers from bad drivers. Even with this bias, their rating system seems to help
    them achieve this goal. It’s essential to look at the distribution of your user
    ratings to detect this bias.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这种偏差并不一定是致命的。Uber的目标是区分好司机和坏司机。即使有这种偏差，他们的评分系统似乎也帮助他们实现了这一目标。查看你用户评分的分布，以检测这种偏差是至关重要的。
- en: 'If you want more granular feedback, removing the strong negative connotation
    associated with low ratings can help people break out of this bias. For example,
    instead of showing users numbers one to five, show users options such as the following:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要更细致的反馈，移除与低评分相关的强烈负面含义可以帮助人们摆脱这种偏差。例如，与其显示一至五的数字，不如显示以下选项：
- en: “Great ride. Great driver.”
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “不错的行程。不错的司机。”
- en: “Pretty good.”
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “相当不错。”
- en: “Nothing to complain about but nothing stellar either.”
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “没有什么可抱怨的，也没有什么出色的。”
- en: “Could’ve been better.”
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “可以更好。”
- en: “Don’t match me with this driver again.”^([12](ch10.html#id1852))
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “别再给我匹配这个司机了。”^([12](ch10.html#id1852))
- en: Randomness
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 随机性
- en: Users often provide random feedback, not out of malice, but because they lack
    motivation to give more thoughtful input. For example, when two long responses
    are shown side by side for comparative evaluation, users might not want to read
    both of them and just click on one at random. In the case of Midjourney, users
    might also randomly choose one image to generate variations.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 用户经常提供随机的反馈，并非出于恶意，而是因为他们缺乏提供更深思熟虑的输入的动力。例如，当两个长响应并排展示以进行比较评估时，用户可能不想阅读它们，只是随机点击其中一个。在Midjourney的情况下，用户也可能随机选择一个图像来生成变体。
- en: Position bias
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 位置偏差
- en: The position in which an option is presented to users influences how this option
    is perceived. Users are generally more likely to click on the first suggestion
    than the second. If a user clicks on the first suggestion, this doesn’t necessarily
    mean that it’s a good suggestion.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 用户看到选项的位置会影响他们对这个选项的感知。用户通常更倾向于点击第一个建议而不是第二个。如果用户点击了第一个建议，这并不一定意味着它是一个好的建议。
- en: When designing your feedback system, this bias can be mitigated by randomly
    varying the positions of your suggestions or by building a model to compute a
    suggestion’s true success rate based on its position.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计你的反馈系统时，可以通过随机变化建议的位置或构建一个模型来计算建议的真实成功率（基于其位置）来减轻这种偏差。
- en: Preference bias
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好偏差
- en: Many other biases can affect a person’s feedback, some of which have been discussed
    in this book. For example, people might prefer the longer response in a side-by-side
    comparison, even if the longer response is less accurate—length is easier to notice
    than inaccuracies. Another bias is [*recency bias*](https://oreil.ly/acfq0), where
    people tend to favor the answer they see last when comparing two answers.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 许多其他偏差会影响一个人的反馈，其中一些在本书中已经讨论过。例如，人们在并排比较中可能会更喜欢较长的响应，即使较长的响应不太准确——长度比不准确更容易注意到。另一个偏差是[*近期偏差*](https://oreil.ly/acfq0)，当比较两个答案时，人们倾向于更喜欢他们看到的最后一个答案。
- en: It’s important to inspect your user feedback to uncover its biases. Understanding
    these biases will help you interpret the feedback correctly, avoiding misleading
    product decisions.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 检查你的用户反馈以揭示其偏差是很重要的。理解这些偏差将帮助你正确地解释反馈，避免误导性的产品决策。
- en: Degenerate feedback loop
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 退化的反馈循环
- en: Keep in mind that user feedback is incomplete. You only get feedback on what
    you show users.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，用户反馈是不完整的。你只能得到你向用户展示的内容的反馈。
- en: In a system where user feedback is used to modify a model’s behavior, *degenerate
    feedback loops* can arise. A degenerate feedback loop can happen when the predictions
    themselves influence the feedback, which, in turn, influences the next iteration
    of the model, amplifying initial biases.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个使用用户反馈来修改模型行为的系统中，可能会出现*退化的反馈循环*。当预测本身影响反馈，而反馈反过来又影响模型的下一轮迭代，从而放大初始偏差时，就会发生退化的反馈循环。
- en: Imagine you’re building a system to recommend videos. The videos that rank higher
    show up first, so they get more clicks, reinforcing the system’s belief that they’re
    the best picks. Initially, the difference between the two videos, A and B, might
    be minor, but because A was ranked slightly higher, it got more clicks, and the
    system kept boosting it. Over time, A’s ranking soared, leaving B behind. This
    feedback loop is why popular videos stay popular, making it tough for new ones
    to break through. This issue is known as “exposure bias,” “popularity bias,” or
    “filter bubbles,” and it’s a well-studied problem.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你正在构建一个推荐视频的系统。排名更高的视频会首先出现，因此它们会获得更多的点击，这加强了系统认为它们是最好的选择的信念。最初，视频A和B之间的差异可能很小，但由于A的排名略高，它获得了更多的点击，系统持续地提升它。随着时间的推移，A的排名飙升，而B则落后。这个反馈循环是为什么热门视频保持热门，使得新视频难以突破的原因。这个问题被称为“曝光偏差”、“流行度偏差”或“过滤气泡”，并且是一个被广泛研究的问题。
- en: A degenerate feedback loop can alter your product’s focus and use base. Imagine
    that initially, a small number of users give feedback that they like cat photos.
    The system picks up on this and starts generating more photos with cats. This
    attracts cat lovers, who give more feedback that cat photos are good, encouraging
    the system to generate even more cats. Before long, your application becomes a
    cat haven. Here, I use cat photos as an example, but the same mechanism can amplify
    other biases, such as racism, sexism, and preference for explicit content.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 一个退化的反馈循环可能会改变你产品的焦点和使用基础。想象一下，最初，少数用户反馈说他们喜欢猫的照片。系统捕捉到这一点，并开始生成更多带有猫的照片。这吸引了猫爱好者，他们给出了更多反馈，认为猫照片很好，这鼓励系统生成更多的猫。不久，你的应用程序就变成了一个猫的天堂。在这里，我使用猫照片作为例子，但同样的机制可以放大其他偏见，例如种族主义、性别歧视和对露骨内容的偏好。
- en: Acting on user feedback can also turn a conversational agent into, for lack
    of a better word, a liar. Multiple studies have shown that training a model on
    user feedback can teach it to give users what it thinks users want, even if that
    isn’t what’s most accurate or beneficial ([Stray, 2023](https://oreil.ly/jtt2m)).
    [Sharma et al. (2023)](https://arxiv.org/abs/2310.13548) show that AI models trained
    on human feedback tend toward. sycophancy. They are more likely to present user
    responses matching this user’s view.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 对用户反馈采取行动也可能使对话代理变成，如果找不到更好的词，就是一个说谎者。多项研究表明，在用户反馈上训练模型可以教会它给出用户认为用户想要的东西，即使这并不最准确或最有益（[Stray,
    2023](https://oreil.ly/jtt2m)）。[Sharma等人（2023）](https://arxiv.org/abs/2310.13548)表明，在人类反馈上训练的人工智能模型倾向于谄媚。它们更有可能呈现与用户观点相符的用户响应。
- en: User feedback is crucial for improving user experience, but if used indiscriminately,
    it can perpetuate biases and destroy your product. Before incorporating feedback
    into your product, make sure that you understand the limitations of this feedback
    and its potential impact.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 用户反馈对于改善用户体验至关重要，但如果不加区分地使用，它可能会延续偏见并破坏你的产品。在将反馈纳入你的产品之前，确保你了解这种反馈的限制及其潜在影响。
- en: Summary
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: If each previous chapter focused on a specific aspect of AI engineering, this
    chapter looked into the process of building applications on top of foundation
    models as a whole.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前几章专注于人工智能工程的特定方面，那么这一章则整体探讨了在基础模型之上构建应用程序的过程。
- en: The chapter consisted of two parts. The first part discussed a common architecture
    for AI applications. While the exact architecture for an application might vary,
    this high-level architecture provides a framework for understanding how different
    components fit together. I used the step-by-step approach in building this architecture
    to discuss the challenges at each step and the techniques you can use to address
    them.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分为两部分。第一部分讨论了人工智能应用的通用架构。虽然每个应用的精确架构可能有所不同，但这个高级架构提供了一个框架，用于理解不同组件如何相互配合。我使用逐步构建这个架构的方法来讨论每个步骤的挑战以及你可以用来解决这些挑战的技术。
- en: While it’s necessary to separate components to keep your system modular and
    maintainable, this separation is fluid. There are many ways components can overlap
    in functionalities. For example, guardrails can be implemented in the inference
    service, the model gateway, or as a standalone component.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有必要分离组件以保持系统模块化和可维护，但这种分离是流动的。组件在功能上可以以多种方式重叠。例如，防护栏可以实现在推理服务、模型网关或作为独立组件中。
- en: Each additional component can potentially make your system more capable, safer,
    or faster but will also increase the system’s complexity, exposing it to new failure
    modes. One integral part of any complex system is monitoring and observability.
    Observability involves understanding how your system fails, designing metrics
    and alerts around failures, and ensuring that your system is designed in a way
    that makes these failures detectable and traceable. While many observability best
    practices and tools from software engineering and traditional machine learning
    are applicable to AI engineering applications, foundation models introduce new
    failure modes, which require additional metrics and design considerations.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 每增加一个组件，都可能使你的系统更强大、更安全或更快，但也会增加系统的复杂性，使其暴露于新的故障模式。任何复杂系统的一个基本组成部分是监控和可观察性。可观察性涉及理解你的系统如何失败，围绕故障设计指标和警报，并确保你的系统设计得使得这些故障可检测和可追踪。虽然许多来自软件工程和传统机器学习的可观察性最佳实践和工具适用于人工智能工程应用，但基础模型引入了新的故障模式，这需要额外的指标和设计考虑。
- en: At the same time, the conversational interface enables new types of user feedback,
    which you can leverage for analytics, product improvement, and the data flywheel.
    The second part of the chapter discussed various forms of conversational feedback
    and how to design your application to effectively collect it.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，对话式界面使得可以收集新的用户反馈类型，您可以利用这些反馈进行数据分析、产品改进和数据飞轮。本章的第二部分讨论了各种形式的对话反馈以及如何设计您的应用程序以有效地收集这些反馈。
- en: Traditionally, user feedback design has been seen as a product responsibility
    rather than an engineering one, and as a result, it is often overlooked by engineers.
    However, since user feedback is a crucial source of data for continuously improving
    AI models, more AI engineers are now becoming involved in the process to ensure
    they receive the data they need. This reinforces the idea from [Chapter 1](ch01.html#ch01_introduction_to_building_ai_applications_with_foun_1730130814984319)
    that, compared to traditional ML engineering, AI engineering is moving closer
    to product. This is because of both the increasing importance of data flywheel
    and product experience as competitive advantages.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，用户反馈设计被视为产品责任而非工程责任，因此，它经常被工程师忽视。然而，由于用户反馈是持续改进AI模型的重要数据来源，越来越多的AI工程师现在参与到这一过程中，以确保他们获得所需的数据。这强化了[第1章](ch01.html#ch01_introduction_to_building_ai_applications_with_foun_1730130814984319)中的观点，即与传统的机器学习工程相比，AI工程正越来越接近产品。这是因为数据飞轮和产品体验作为竞争优势的重要性日益增加。
- en: Many AI challenges are, at their core, system problems. To solve them, it’s
    often necessary to step back and consider the system as a whole. A single problem
    might be addressed by different components working independently, or a solution
    could require the collaboration of multiple components. A thorough understanding
    of the system is essential to solving real problems, unlocking new possibilities,
    and ensuring safety.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 许多AI挑战在本质上都是系统问题。要解决这些问题，通常需要退后一步，将整个系统视为一个整体来考虑。一个单一问题可能由独立工作的不同组件来解决，或者可能需要多个组件的协作才能找到解决方案。对系统的深入了解对于解决实际问题、开启新的可能性以及确保安全至关重要。
- en: ^([1](ch10.html#id1761-marker)) An example is when a Samsung employee put Samsung’s
    proprietary information into ChatGPT, accidentally [leaking the company’s secrets](https://oreil.ly/_5RFN).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch10.html#id1761-marker)) 例如，当一名三星员工不小心将三星的专有信息放入ChatGPT时，意外[泄露了公司的机密](https://oreil.ly/_5RFN)。
- en: ^([2](ch10.html#id1763-marker)) It’s possible that users ask the model to return
    an empty response.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.html#id1763-marker)) 用户可能要求模型返回一个空响应。
- en: ^([3](ch10.html#id1769-marker)) A few early readers told me that the idea of
    ignoring guardrails in favor of latency gave them nightmares.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch10.html#id1769-marker)) 一些早期读者告诉我，忽略护栏而选择延迟的想法让他们做噩梦。
- en: ^([4](ch10.html#id1786-marker)) As of this writing, the aggregated market capitalization
    of a few of the largest observability companies (Datadog, Splunk, Dynatrace, New
    Relic) is close to $100 billion.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch10.html#id1786-marker)) 到本文写作时，一些最大的可观察性公司（如Datadog、Splunk、Dynatrace、New
    Relic）的市值总和接近1000亿美元。
- en: ^([5](ch10.html#id1787-marker)) My book, [*Designing Machine Learning Systems*](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/)
    (O’Reilly, 2022), also has a chapter on monitoring. An early draft of the chapter
    is available on my blog at [“Data Distribution Shifts and Monitoring”](https://huyenchip.com/2022/02/07/data-distribution-shifts-and-monitoring.html).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch10.html#id1787-marker)) 我的书[*设计机器学习系统*](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/)（O’Reilly，2022年），也有一章关于监控。该章节的早期草稿在我的博客“数据分布偏移和监控”中可用[“Data
    Distribution Shifts and Monitoring”](https://huyenchip.com/2022/02/07/data-distribution-shifts-and-monitoring.html)。
- en: ^([6](ch10.html#id1811-marker)) Because of this, some orchestrator tools want
    to be gateways. In fact, so many tools seem to want to become end-to-end platforms
    that do everything.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch10.html#id1811-marker)) 因此，一些编排工具希望成为网关。事实上，似乎有那么多工具都希望成为能够做一切事情的全端平台。
- en: ^([7](ch10.html#id1816-marker)) One key disadvantage of launching an open source
    application instead of a commercial application is that it’s a lot harder to collect
    user feedback. Users can take your open source application and deploy it themselves,
    and you have no idea how the application is used.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch10.html#id1816-marker)) 与其推出商业应用相比，推出开源应用的一个主要缺点是收集用户反馈要困难得多。用户可以将您的开源应用部署到自己的环境中，而您对应用的使用情况一无所知。
- en: ^([8](ch10.html#id1838-marker)) Not only can you collect feedback about AI applications,
    you can use AI to analyze feedback, too.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch10.html#id1838-marker)) 你不仅能够收集关于AI应用的反馈，还可以使用AI来分析这些反馈。
- en: ^([9](ch10.html#id1842-marker)) I wish there were inpainting for text-to-speech.
    I find text-to-speech works well 95% of the time, but the other 5% can be frustrating.
    AI might mispronounce a name or fail to pause during dialogues. I wish there were
    apps that let me edit just the mistakes instead of having to regenerate the whole
    audio.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch10.html#id1842-marker)) 我希望有文本到语音的修复功能。我发现文本到语音在95%的时间里工作得很好，但剩下的5%可能会让人感到沮丧。AI可能会误读一个名字或者在对话中未能正确停顿。我希望有应用程序能让我只编辑错误，而不是不得不重新生成整个音频。
- en: ^([10](ch10.html#id1844-marker)) When I ask this question at events I speak
    at, the responses are conflicted. Some people think showing full responses gives
    more reliable feedback because it gives users more information to make a decision.
    At the same time, some people think that once users have read full responses,
    there’s no incentive for them to click on the better one.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch10.html#id1844-marker)) 当我在参加的活动上提出这个问题时，得到的回答是矛盾的。有些人认为显示完整的响应能提供更可靠的反馈，因为它给用户提供了更多做出决定的信息。与此同时，有些人认为一旦用户阅读了完整的响应，他们就没有点击更好的选项的激励了。
- en: ^([11](ch10.html#id1846-marker)) See [“Ted Cruz Blames Staffer for ‘Liking’
    Porn Tweet”](https://oreil.ly/xKEVc) (Nelson and Everett, *POLITICO*, September
    2017) and [“Kentucky Senator Whose Twitter Account ‘Liked’ Obscene Tweets Says
    He Was Hacked”](https://oreil.ly/ve1DN) (Liam Niemeyer, WKU Public Radio, March
    2023).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch10.html#id1846-marker)) 请参阅[“Ted Cruz Blames Staffer for ‘Liking’ Porn
    Tweet”](https://oreil.ly/xKEVc)（Nelson和Everett，《POLITICO》，2017年9月）和[“Kentucky
    Senator Whose Twitter Account ‘Liked’ Obscene Tweets Says He Was Hacked”](https://oreil.ly/ve1DN)（Liam
    Niemeyer，WKU公共广播，2023年3月）。
- en: ^([12](ch10.html#id1852-marker)) The options suggested here are only to show
    how options can be rewritten. They haven’t been validated.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch10.html#id1852-marker)) 这里提出的选项只是为了展示选项可以被如何重写。它们尚未经过验证。
