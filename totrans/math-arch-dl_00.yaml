- en: 1 An overview of machine learning and deep learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: A first look at machine learning and deep learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A simple machine learning model: The cat brain'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding deep neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning has transformed computer vision, natural language and speech processing
    in particular, and artificial intelligence in general. From a bag of semi-discordant
    tricks, none of which worked satisfactorily on real-life problems, artificial
    intelligence has become a formidable tool to solve real problems faced by industry,
    at scale. This is nothing short of a revolution going on under our very noses.
    To lead the curve of this revolution, it is imperative to understand the underlying
    principles and abstractions rather than simply memorizing the “how-to” steps of
    some hands-on guide. This is where mathematics comes in.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'In this first chapter, we present an overview of deep learning. This will require
    us to use some concepts explained in subsequent chapters. Don’t worry if there
    are some open questions at the end of this chapter: it is aimed at orienting your
    mind toward this difficult subject. As individual concepts become clearer in subsequent
    chapters, you should consider coming back and re-reading this chapter.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '1.1 A first look at machine/deep learning: A paradigm shift in computation'
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Making decisions and/or predictions is a central requirement of life. Doing
    so essentially involves taking in a set of sensory or knowledge inputs and processing
    them to generate decisions or estimates.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, a cat’s brain is often trying to choose between the following
    options:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '*run away* from the object in front of it'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ignore* the object in front of it'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*approach* the object in front of it and purr.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cat’s brain makes that decision by processing sensory inputs like the perceived
    *hardness* of the object in front of it, the perceived *sharpness* of the object
    in front of it, and so on. This is an instance of a *classification* problem,
    where the output is one of a set of possible classes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Some other examples of classification problems in life are as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '*Buy* vs. *hold* vs. *sell* a certain stock, from inputs like the *price history
    of this stock* and the *change in price of the stock in recent times*'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Object recognition (from an image):'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is this a car or a giraffe?
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Is this a human or a non-human?
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Is this an inanimate object or a living object?
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Face recognition—is this Tom or Dick or Mary or Einstein or Messi?
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action recognition from a video:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is this person running or not running?
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Is this person picking something up or not?
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Is this person doing something violent or not?
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Natural language processing (NLP) from digital documents:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does this news article belong to the realm of politics or sports?
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Does this query phrase match a particular article in the archive?
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes life requires a *quantitative* estimation instead of a classification.
    A lion’s brain needs to estimate how far to jump so as to land on top of its prey,
    by processing inputs like
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Another instance of quantitative estimation is estimating a house’s price based
    on inputs like current income of the house’s owner, crime statistics for the neighborhood,
    and so on. Machines that make such quantitative estimators are called *regressors*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个定量估计的例子是根据房屋所有者的当前收入、社区犯罪统计数据等因素来估算房屋的价格。制造这种定量估计器的机器被称为*回归器*。
- en: 'Here are some other examples of quantitative estimations required in daily
    life:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些日常生活中需要的定量估计的例子：
- en: 'Object localization from an image: identifying the rectangle bounding the location
    of an object'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从图像中定位对象：识别包围对象位置的矩形
- en: Stock price prediction from historical stock prices and other world events
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从历史股价和其他世界事件中预测股价
- en: Similarity score between a pair of documents
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两份文档之间的相似度得分
- en: Sometimes a classification output can be generated from a quantitative estimate.
    For instance, the cat brain described earlier can combine the inputs (hardness,
    sharpness, and so on) to generate a quantitative threat score. If that threat
    score is high, the cat runs away. If the threat score is near zero, the cat ignores
    the object in front of it. If the threat score is negative, the cat approaches
    the object and purrs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，可以从定量估计中生成分类输出。例如，前面描述的猫脑可以将输入（硬度、尖锐度等）组合起来生成一个定量威胁分数。如果这个威胁分数很高，猫就会逃跑。如果威胁分数接近零，猫就会忽略它面前的物体。如果威胁分数是负数，猫就会走向物体并发出咕噜声。
- en: Many of these examples are shown in figure [1.1](#fig-decisions-quantestimates).
    In each instance, a machine—that is, a brain—transforms sensory or knowledge inputs
    into decisions or quantitative estimates. The goal of machine learning is to emulate
    that machine.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些例子都在图[1.1](#fig-decisions-quantestimates)中展示。在每个实例中，一个机器——即大脑——将感官或知识输入转换为决策或定量估计。机器学习的目标是模仿这种机器。
- en: Note that machine learning has a long way to go before it can catch up with
    the human brain. The human brain can single-handedly deal with thousands, if not
    millions, of such problems. On the other hand, at its present state of development,
    machine learning can hardly create a single general-purpose machine that makes
    a wide variety of decisions and estimates. We are mostly trying to make separate
    machines to solve individual tasks (such as a stock picker or a car recognizer).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，机器学习在达到与人类大脑相当的水平之前还有很长的路要走。人类大脑可以独立处理成千上万个这样的问题。另一方面，在目前的开发状态下，机器学习几乎无法创造一个能够做出广泛决策和估计的通用机器。我们主要是在尝试制造单独的机器来解决单个任务（如股票选择器或汽车识别器）。
- en: '![](../../OEBPS/Images/CH01_F01_Chaudhury.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH01_F01_Chaudhury.png)'
- en: Figure 1.1 Examples of decision making and quantitative estimations in life
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 生活中决策和定量估计的例子
- en: 'At this point, you may ask, “Wait: converting inputs to outputs—isn’t that
    exactly what computers have been doing for the last 30 or more ears? What is this
    paradigm shift I am hearing about?” The answer is that it *is* a paradigm shift
    because we do not provide a step-by-step instruction set—that is, a program—to
    the machine to convert the input to output. Instead, we develop a mathematical
    model for the problem.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你可能会有疑问，“等等：将输入转换为输出——这难道不就是计算机在过去30年或更长时间里一直在做的事情吗？我听说的这种范式转变究竟是什么？”答案是，这确实是一种范式转变，因为我们并没有为机器提供一个逐步的指令集——也就是说，一个程序——来将输入转换为输出。相反，我们为问题开发了一个数学模型。
- en: 'Let’s illustrate the idea with an example. For the sake of simplicity and concreteness,
    we will consider a hypothetical cat brain that needs to make only one decision
    in life: whether to *run away from the object in front of it* or *ignore the object*
    or *approach and purr*. This decision, then, is the output of the model we will
    discuss. And in this toy example, the decision is made based on only two quantitative
    inputs (aka features): the perceived hardness and sharpness of the object (as
    depicted in figure [1.1](#fig-decisions-quantestimates)). We do *not* provide
    any step-by-step instructions such as “if sharpness greater than some threshold,
    then run away.” Instead, we try to identify a *parameterized* function that takes
    the input and converts it to the desired decision or estimate. The simplest such
    function is a *weighted sum of inputs*:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来说明这个想法。为了简单和具体，我们将考虑一个假设的猫脑，它一生中只需要做出一个决定：是*从它面前逃跑*、*忽略物体*还是*靠近并咕噜咕噜叫*。这个决定，然后，就是我们将讨论的模型的输出。在这个玩具例子中，决策是基于仅有的两个定量输入（即特征）：物体的感知硬度和锐度（如图[1.1](#fig-decisions-quantestimates)所示）。我们*不*提供任何逐步指令，例如“如果锐度大于某个阈值，则逃跑。”相反，我们试图识别一个*参数化*的函数，该函数接受输入并将其转换为所需的决策或估计。最简单的此类函数是*输入的加权求和*：
- en: '*y*(*hardness*, *sharpness*) = *w*[0] × *hardness* + *w*[1] × *sharpness* +
    *b*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*y*(*硬度*, *锐度*) = *w*[0] × *硬度* + *w*[1] × *锐度* + *b*'
- en: The weights *w*[0], *w*[1] and the bias *b* are the parameters of the function.
    The output *y* can be interpreted as a threat score. If the threat score exceeds
    a threshold, the cat runs away. If it is close to 0, the cat ignores the object.
    If the threat score is negative, the cat approaches and purrs. For more complex
    tasks, we will use more sophisticated functions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 权重 *w*[0]、*w*[1] 和偏差 *b* 是函数的参数。输出 *y* 可以解释为威胁分数。如果威胁分数超过阈值，猫就会逃跑。如果它接近0，猫就会忽略物体。如果威胁分数为负，猫就会靠近并咕噜咕噜叫。对于更复杂的任务，我们将使用更复杂的函数。
- en: Note that the weights are not known at first; we need to estimate them. This
    is done through a process called *model training*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一开始权重是未知的；我们需要通过一个称为*模型训练*的过程来估计它们。
- en: 'Overall, solving a problem via machine learning has the following stages:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，通过机器学习解决问题有以下阶段：
- en: We design a parameterized model function (e.g., weighted sum) with unknown parameters
    (weights). This constitutes the *model architecture*. Choosing the right model
    architecture is where the expertise of the machine learning engineer comes into
    play.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们设计一个参数化模型函数（例如，加权求和）带有未知参数（权重）。这构成了*模型架构*。选择正确的模型架构是机器学习工程师专业知识发挥作用的领域。
- en: Then we estimate the weights via model training.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们通过模型训练来估计权重。
- en: Once the weights are estimated, we have a complete *model*. This model can take
    arbitrary inputs not necessarily seen before and generate outputs. The process
    in which a trained model processes an arbitrary real-life input and emits an output
    is called *inferencing*.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦估计了权重，我们就有一个完整的*模型*。这个模型可以接受任意输入，不一定之前见过，并生成输出。一个训练好的模型处理任意现实生活输入并发出输出的过程称为*推理*。
- en: 'In the most popular variety of machine learning, called *supervised learning*,
    we prepare the training data before we commence training. Training data comprises
    *example input items, each with its corresponding desired output*. [¹](#fn1) Training
    data is often created manually: a human goes over every single input item and
    produces the desired output (aka target output). This is usually the most arduous
    part of doing machine learning.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在最流行的机器学习类型中，称为*监督学习*，我们在开始训练之前准备训练数据。训练数据包括*示例输入项，每个都有其对应的期望输出*。[¹](#fn1) 训练数据通常是通过人工创建的：一个人检查每个输入项并产生期望的输出（即目标输出）。这通常是机器学习中最艰巨的部分。
- en: For instance, in our hypothetical cat brain example, some possible training
    data items are as follows
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的假设猫脑例子中，一些可能的训练数据项如下
- en: 'input: *hardness* = 0.01, *sharpness* = 0.02 → threat = —0.90 → *decision*:
    “approach and purr”'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：*硬度* = 0.01，*锐度* = 0.02 → 威胁 = -0.90 → *决策*：“靠近并咕噜咕噜叫”
- en: 'input: *hardness* = 0.50, *sharpness* = 0.60 → threat = 0.01   → *decision*:
    “ignore”'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：*硬度* = 0.50，*锐度* = 0.60 → 威胁 = 0.01 → *决策*：“忽略”
- en: 'input: *hardness* = 0.99, *sharpness* = 0.97 → threat = 0.90   → *decision*:
    “run away”'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：*硬度* = 0.99，*锐度* = 0.97 → 威胁 = 0.90 → *决策*：“逃跑”
- en: where the input values of hardness and sharpness are assumed to lie between
    0 and 1.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 假设硬度和锐度的输入值介于0和1之间。
- en: 'What exactly happens during training? Answer: we iteratively process the input
    training data items. For each input item, we know the desired aka target) output.
    On each iteration, we adjust the model weight values in a way that the output
    of the model function on that specific input item gets at least a little closer
    to the corresponding target output. For instance, suppose at a given iteration,
    the weight values are *w*[0] = 20 and *w*[1] = 10, and *b* = 50. On the input
    (*hardness* = 0.01, *sharpness* = 0.02), we get an output threat score *y* = 50.3,
    which is quite different from the desired *y* = −0.9. We will adjust the weights:
    for instance, reducing the bias so *w*[0] = 20, *w*[1] = 10, and *b* = 40. The
    corresponding threat score *y* = 40.3 is still nowhere near the desired value,
    but it has moved closer. After we do this on many training data items, the weights
    will start approaching their ideal values. Note that how to identify the adjustments
    to the weight values is not discussed here; it requires somewhat deeper math and
    will be discussed later.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中究竟发生了什么？回答：我们迭代处理输入的训练数据项。对于每个输入项，我们知道期望的（即目标）输出。在每次迭代中，我们调整模型权重值，以便模型函数在该特定输入项上的输出至少接近相应的目标输出。例如，假设在给定的一次迭代中，权重值是
    *w*[0] = 20 和 *w*[1] = 10，以及 *b* = 50。在输入（*硬度* = 0.01，*锐度* = 0.02）下，我们得到输出威胁分数
    *y* = 50.3，这与期望的 *y* = −0.9 相差甚远。我们将调整权重：例如，减少偏差，使 *w*[0] = 20，*w*[1] = 10，以及
    *b* = 40。相应的威胁分数 *y* = 40.3 仍然远远达不到期望值，但它已经接近了。在我们对许多训练数据项进行这样的操作后，权重将开始接近其理想值。请注意，如何确定权重值的调整方法在这里没有讨论；它需要一些更深入的数学知识，将在以后讨论。
- en: As stated earlier, this process of iteratively tuning weights is called *training*
    or *learning*. At the beginning of learning, the weights have random values, so
    the machine outputs often do not match desired outputs. But with time, more training
    iterations happen, and the machine “learns” to generate the correct output. That
    is when the model is ready for deployment in the real world. Given arbitrary input,
    the model will (hopefully) emit something close to the desired output during inferencing.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这种迭代调谐权重的过程被称为*训练*或*学习*。在学习开始时，权重具有随机值，因此机器的输出通常不符合期望的输出。但随着时间的推移，更多的训练迭代发生，机器“学习”生成正确的输出。那时，模型就准备好在现实世界中部署了。给定任意输入，模型将在推理过程中（希望）输出接近期望的输出。
- en: Come to think of it, that is probably how living brains work. They contain equivalents
    of mathematical models for various tasks. Here, the weights are the strengths
    of the connections (aka synapses) between the different neurons in the brain.
    In the beginning, the parameters are untuned; the brain repeatedly makes mistakes.
    For example, a baby’s brain often makes mistakes in identifying edible objects—anybody
    who has had a child will know what we are talking about. But each example tunes
    the parameters (eating green and white rectangular things with a $ sign on them
    invites much scolding—should not eat them in the future, etc.). Eventually, this
    machine tunes its parameters to yield better results.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 想想看，这可能就是活脑工作的方式。它们包含各种任务的数学模型等效物。在这里，权重是大脑中不同神经元之间连接（即突触）的强度。一开始，参数未调谐；大脑会反复犯错误。例如，婴儿的大脑在识别可食用物体时经常犯错误——任何有孩子的父母都知道我们在说什么。但每个例子都会调谐参数（吃带有$符号的绿色和白色矩形东西会招来很多责备——将来不应该吃它们等）。最终，这个机器调谐其参数以产生更好的结果。
- en: One subtle point should be noted here. During training, the machine is tuning
    its parameters so that it produces the desired outcome—*on the training data input
    only*. Of course, it sees only a small fraction of all possible inputs during
    training—we are *not* building a lookup table from known inputs to known outputs.
    Hence, when this machine is released in the world, it mostly runs on input data
    it has never seen before. What guarantee do we have that it will generate the
    right outcome on never-before-seen data? Frankly, there is no guarantee. Only,
    in most real-life problems, the inputs are not really random. They have a pattern.
    Hopefully, the machine will see enough during training to capture that pattern.
    Then its output on unseen input will be close to the desired value. The closer
    the distribution of the training data is to real life, the more likely that becomes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里有一个需要注意的微妙之处。在训练过程中，机器正在调整其参数，以便只在其训练数据输入上产生预期的结果——*仅限于训练数据输入*。当然，在训练过程中，它只能看到所有可能输入的一小部分——我们*不是*在从已知输入到已知输出的查找表中构建。因此，当这台机器被发布到世界上时，它主要运行的是它以前从未见过的输入数据。我们有什么保证它能对从未见过的数据进行正确的输出呢？坦白说，没有保证。只是在大多数现实生活中的问题中，输入并不是真正随机的。它们有一个模式。希望机器在训练过程中能看到足够的输入来捕捉这个模式。然后，它在未见输入上的输出将接近预期的值。训练数据的分布越接近现实生活，这种情况就越有可能发生。
- en: 1.2 A function approximation view of machine learning:Models and their training
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 机器学习的函数逼近视角：模型及其训练
- en: 'As stated in section [1.1](#sec-paradigm-shift), to create a brain-like machine
    that makes classifications or estimations, we have to find a mathematical function
    (model) that transforms inputs into corresponding desired outputs. Sadly, however,
    in typical real-life situations, we do not know that transformation function.
    For instance, we do not know the function that takes in past prices, world events,
    and so on and estimates the future price of a stock—something that stops us from
    building a stock price estimator and getting rich. All we have is the training
    data—a set of inputs on which the output is known. How do we proceed, then? Answer:
    we will try to model the unknown function. This means we will create a function
    that will be a proxy or surrogate to the unknown function. Viewed this way, machine
    learning is nothing but function approximation—we are simply trying to approximate
    the unknown classification or estimation function.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如第 [1.1](#sec-paradigm-shift) 节所述，为了创建一个像大脑一样的机器来进行分类或估计，我们必须找到一个数学函数（模型），它将输入转换为相应的期望输出。然而，遗憾的是，在典型的现实生活情况中，我们并不知道这个转换函数。例如，我们不知道一个函数，它接受过去的股价、世界事件等等，并估计股票的未来价格——这阻止了我们构建股价估计器并致富。我们拥有的只是训练数据——一组已知输出的输入。那么我们该怎么办呢？答案是：我们将尝试模拟未知函数。这意味着我们将创建一个函数，它将是未知函数的代理或替代品。从这个角度来看，机器学习不过是函数逼近——我们只是在尝试逼近未知的分类或估计函数。
- en: 'Let’s briefly recap the main ideas from the previous section. In machine learning,
    we try to solve problems that can be abstractly viewed as transforming a set of
    inputs to an output. The output is either a class or an estimated value. Since
    we do not know the true transformation function, we try to come up with a model
    function. We start by designing—using our physical understanding of the problem—a
    model function with tunable parameter values that can serve as a proxy for the
    true function. This is the *model architecture*, and the tunable parameters are
    also known as *weights*. The simplest model architecture is one where the output
    is a weighted sum of the input values. Determining the model architecture does
    not fully determine the model—we still need to determine the actual parameter
    values (weights). That is where *training* comes in. During training, we find
    an optimal set of weights that transform the training inputs to outputs that match
    the corresponding training outputs as closely as possible. Then we deploy this
    machine in the world: its weights are estimated and the function is fully determined,
    so on any input, it simply applies the function and generates an output. This
    is called *inferencing*. Of course, training inputs are only a fraction of all
    possible inputs, so there is no guarantee that inferencing will yield a desired
    result on all real inputs. The success of the model depends on the appropriateness
    of the chosen model architecture and the quality and quantity of training data.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要回顾一下上一节的主要思想。在机器学习中，我们试图解决那些可以抽象地看作是将一组输入转换为输出的问题。输出要么是一个类别，要么是一个估计值。由于我们不知道真正的转换函数，我们试图提出一个模型函数。我们首先通过使用我们对问题的物理理解来设计一个具有可调参数值的模型函数，它可以作为真实函数的代理。这就是*模型架构*，可调参数也被称为*权重*。最简单的模型架构是输出是输入值的加权总和。确定模型架构并不完全确定模型——我们还需要确定实际的参数值（权重）。这就是*训练*的作用所在。在训练过程中，我们找到一组最优的权重，将训练输入转换为尽可能接近相应训练输出的输出。然后我们将这个机器部署到世界上：其权重被估计，函数被完全确定，因此对于任何输入，它只需应用函数并生成输出。这被称为*推理*。当然，训练输入只是所有可能输入的一小部分，因此无法保证推理会在所有实际输入上产生期望的结果。模型的成功取决于所选模型架构的适当性和训练数据的质量和数量。
- en: Obtaining training data
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 获取训练数据
- en: After mastering machine learning, the biggest struggle turns out to be the procurement
    of training data. When practitioners can afford it, it is common practice to use
    humans to hand-generate the outputs corresponding to the training data inputs
    (these target outputs are sometimes referred to as *ground truth*). This process,
    known as *human labeling* or *human curation*, involves an army of human beings
    looking at a substantial number of training data inputs and producing the corresponding
    ground truth outputs. For some well-researched problems, we may be lucky enough
    to get training data on the internet; otherwise it becomes a daunting challenge.
    More on this later.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握机器学习之后，最大的挑战竟然是训练数据的获取。当从业者能够承担得起时，通常的做法是使用人类手动生成与训练数据输入相对应的输出（这些目标输出有时被称为*真实值*）。这个过程被称为*人工标注*或*人工整理*，涉及一群人类查看大量的训练数据输入并生成相应的真实值输出。对于一些经过深入研究的问题，我们可能足够幸运，可以从互联网上获得训练数据；否则，这变成了一项艰巨的挑战。关于这一点，我们稍后再详细讨论。
- en: 'Now, let’s study the process of model building with a concrete example: the
    cat brain machine shown in figure [1.1](#fig-decisions-quantestimates).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一个具体的例子来研究模型构建的过程：图[1.1](#fig-decisions-quantestimates)中所示的猫脑机器。
- en: '1.3 A simple machine learning model: The cat brain'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 一个简单的机器学习模型：猫脑
- en: 'For the sake of simplicity and concreteness, we will deal with a hypothetical
    cat that needs to make only one decision in life: whether to run away from the
    object in front of it, ignore it, or approach and purr. And it makes this decision
    based on only two quantitative inputs pertaining to the object in front of it
    (shown in figure [1.1](#fig-decisions-quantestimates)).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单和具体，我们将处理一个假设的猫，这只猫在一生中只需要做出一个决定：是否要从它面前的东西逃跑，忽略它，或者走近并咕噜咕噜叫。它根据它面前的东西的两个定量输入（如图[1.1](#fig-decisions-quantestimates)所示）来做出这个决定。
- en: NOTE This chapter is a lightweight overview of machine/deep learning. As such,
    it relies some on mathematical concepts that we will introduce later. You are
    encouraged to read this chapter now, nonetheless, and perhaps re-read it after
    digesting the chapters on vectors and matrices.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章是对机器/深度学习的一个轻量级概述。因此，它依赖于我们稍后将要介绍的一些数学概念。尽管如此，我们鼓励你现在阅读这一章，也许在消化了关于矢量和矩阵的章节后再重新阅读。
- en: 1.3.1 Input features
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 输入特征
- en: The input features are *x*[0], signifying *hardness*, and *x*[1], signifying
    *sharpness*. Without loss of generality, we can *normalize* the inputs. This is
    a pretty popular trick whereby the input values ranging between a minimum possible
    value *v[min]* and a maximum possible value *v[max]* are transformed to values
    between 0 and 1. To transform an arbitrary input value *v* to a normalized value
    *v[norm]*, we use the formula
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 输入特征是 *x*[0]，表示 *硬度*，和 *x*[1]，表示 *尖锐度*。不失一般性，我们可以 *归一化* 输入。这是一个相当流行的技巧，其中输入值在最小可能值
    *v[min]* 和最大可能值 *v[max]* 之间变换为介于 0 和 1 之间的值。要将任意输入值 *v* 转换为归一化值 *v[norm]*，我们使用以下公式
- en: '![](../../OEBPS/Images/eq_01-01.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_01-01.png)'
- en: Equation 1.1
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 1.1
- en: In mathematical parlance, transformation via equation [1.1](#eq-normalization),
    *v* ∈ [*v[min]*, *v[max]*] → *v[norm]* ∈ [0,1] maps the values *v* from the input
    domain [*v[min]*, *v[max]*] to the output values *v[norm]* in the range [0,1].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 用数学术语来说，通过方程式 [1.1](#eq-normalization)，*v* ∈ [*v[min]*, *v[max]*] → *v[norm]*
    ∈ [0,1] 将值 *v* 从输入域 [*v[min]*, *v[max]*] 映射到输出值 *v[norm]* 在范围 [0,1] 内。
- en: A two-element vector ![](../../OEBPS/Images/eq_01-01-a.png) represents a single
    input instance succinctly.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一个两元素向量 ![](../../OEBPS/Images/eq_01-01-a.png) 简洁地表示一个单个输入实例。
- en: 1.3.2 Output decisions
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.2 输出决策
- en: 'The final output is multiclass and can take one of three possible values: *0*,
    implying running away from the object in front of the cat; *1*, implying ignoring
    the object; and *2*, implying approaching the object and purring. It is possible
    in machine learning to compute the class directly. However, in this example, we
    will have our model estimate a *threat score*. It is interpreted as follows: threat
    high positive = run away, threat near zero = ignore, and threat high negative
    = approach and purr (negative threat is attractive).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出是多类别的，可以取三个可能值之一：*0*，表示猫避开前面的物体；*1*，表示忽略物体；*2*，表示接近物体并发出咕噜声。在机器学习中，可以直接计算类别。然而，在这个例子中，我们将让我们的模型估计一个
    *威胁分数*。它的解释如下：威胁高度正值 = 逃跑，威胁接近零 = 忽略，威胁高度负值 = 接近并咕噜（负威胁是有吸引力的）。
- en: 'We can make a final multiclass run/ignore/approach decision based on threat
    score by comparing the threat score *y* against a threshold *δ*, as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据威胁分数，通过将威胁分数 *y* 与阈值 *δ* 进行比较，来做出最终的运行/忽略/接近决策：
- en: '![](../../OEBPS/Images/eq_01-02.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_01-02.png)'
- en: Equation 1.2
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 1.2
- en: 1.3.3 Model estimation
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.3 模型估计
- en: 'Now for the all-important step: we need to estimate the function that transforms
    the input vector to the output. With slight abuse of terms, we will denote this
    function as well as the output by *y*. In mathematical notation, we want to estimate
    *y*(![](../../OEBPS/Images/AR_x.png)).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是至关重要的步骤：我们需要估计将输入向量转换为输出的函数。为了稍微滥用术语，我们将这个函数以及输出表示为 *y*。用数学符号表示，我们想要估计 *y*(![](../../OEBPS/Images/AR_x.png))。
- en: 'Of course, we do not know the ideal function. We will try to estimate this
    unknown function from the training data. This is accomplished in two steps:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们不知道理想函数。我们将尝试从训练数据中估计这个未知函数。这是通过两个步骤完成的：
- en: '*Model architecture selection*—Designing a parameterized function that we expect
    is a good proxy or surrogate for the unknown ideal function'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*模型架构选择*—设计一个参数化的函数，我们期望它是未知理想函数的良好代理或替代品'
- en: '*Training*—Estimating the parameters of that chosen function such that the
    outputs on training inputs match corresponding outputs as closely as possible'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*训练*—估计所选函数的参数，使得在训练输入上的输出尽可能接近相应的输出'
- en: 1.3.4 Model architecture selection
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.4 模型架构选择
- en: This is the step where various machine learning approaches differ from one another.
    In this toy cat brain example, we will use the simplest possible model. Our model
    has three parameters, *w*[0], *w*[1], *b*. They can be represented compactly with
    a single two-element vector ![](../../OEBPS/Images/eq_01-02-a2.png) and a constant
    bias *b* ∈ ℝ (here, ℝ denotes the set of all real numbers, ℝ² denotes the set
    of 2D vectors with both elements real, and so on). It emits the threat score,
    *y*, which is computed as
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是各种机器学习算法彼此不同的步骤。在这个玩具猫脑的例子中，我们将使用最简单的模型。我们的模型有三个参数，*w*[0]，*w*[1]，*b*。它们可以用一个单元素的两个元素向量
    ![](../../OEBPS/Images/eq_01-02-a2.png) 和一个常数偏置 *b* ∈ ℝ（在这里，ℝ 表示所有实数的集合，ℝ² 表示所有元素为实的二维向量的集合，等等）来紧凑地表示。它产生威胁分数，*y*，其计算如下
- en: '![](../../OEBPS/Images/eq_01-03.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_01-03.png)'
- en: Equation 1.3
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 1.3
- en: Note that *b* is a slightly special parameter. It is a constant that does not
    get multiplied by any of the inputs. It is common practice in machine learning
    to refer to it as *bias*; the other parameters are multiplied by inputs as weights.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*b* 是一个稍微特殊的参数。它是一个常数，不与任何输入相乘。在机器学习中，通常将其称为 *偏置*；其他参数作为权重与输入相乘。
- en: 1.3.5 Model training
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.5 模型训练
- en: Once the model architecture is chosen, we know the exact parametric function
    we are going to use to model the unknown function *y*(![](../../OEBPS/Images/AR_x.png))
    that transforms inputs to outputs. We still need to estimate the function’s parameters.
    Thus, we have a function with unknown parameters, and the parameters are to be
    estimated from a set of inputs with known outputs (training data). We will choose
    the parameters so that the outputs on the training data inputs match the corresponding
    outputs as closely as possible.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了模型架构，我们就知道将要使用精确的参数函数来模拟未知函数 *y*(![](../../OEBPS/Images/AR_x.png))，该函数将输入转换为输出。我们仍然需要估计该函数的参数。因此，我们有一个具有未知参数的函数，并且这些参数需要从一组具有已知输出的输入（训练数据）中估计。我们将选择参数，使得在训练数据输入上的输出尽可能接近相应的输出。
- en: Iterative training
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代训练
- en: This problem has been studied by mathematicians and is known as a *function-fitting*
    problem in mathematics. What changed with the advent of machine learning, however,
    is the sheer scale. In machine learning, we deal with training data comprising
    millions and millions of items. This altered the philosophy of the solution. Mathematicians
    use a *closed-form solution*, where the parameters are estimated by directly solving
    equations involving *all* the training data items together. In machine learning,
    we go for iterative solutions, dealing with a *few* training data items (or perhaps
    only one) at a time. In the iterative solution, there is no need to hold all the
    training data in the computer’s memory. We simply load small portions of it at
    a time and deal with only that portion. We will exemplify this with our cat brain
    example.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题已经被数学家研究，在数学上被称为 *函数拟合* 问题。然而，随着机器学习的出现，发生了变化的是规模。在机器学习中，我们处理包含数百万个项目的训练数据。这改变了解决方案的哲学。数学家使用
    *闭式解*，其中参数通过直接解决涉及 *所有* 训练数据项目的方程来估计。在机器学习中，我们寻求迭代解决方案，一次处理 *少量* 训练数据项目（或者可能只有一个）。在迭代解决方案中，没有必要将所有训练数据都保存在计算机的内存中。我们只需一次加载一小部分，并处理那一部分。我们将用我们的猫脑例子来举例说明。
- en: Concretely, the goal of the training process is to estimate the parameters *w*[0],
    *w*[1], *b* or, equivalently, the vector ![](../../OEBPS/Images/AR_w.png) along
    with constant *b* from equation [1.3](../Text/01.xhtml#eq-linear-predictor) in
    such a way that the output *y*(*x*[0], *x*[1]) on the training data input (*x*[0],
    *x*[1]) matches the corresponding known training data outputs (aka ground truth
    [GT]) as much as possible.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，训练过程的目标是估计参数 *w*[0]，*w*[1]，*b* 或者等价地，从方程 [1.3](../Text/01.xhtml#eq-linear-predictor)
    估计向量 ![](../../OEBPS/Images/AR_w.png) 以及常数 *b*，以便在训练数据输入 (*x*[0]，*x*[1]) 上的输出
    *y*(*x*[0]，*x*[1]) 尽可能地匹配相应的已知训练数据输出（即真实值 [GT]）。
- en: 'Let the training data consist of *N* + 1 inputs ![](../../OEBPS/Images/AR_x.png)^((0)),
    ![](../../OEBPS/Images/AR_x.png)^((1)), ⋯ ![](../../OEBPS/Images/AR_x.png)^((*N*)).
    Here, each ![](../../OEBPS/Images/AR_x.png)^((*i*)) is a 2 × 1 vector denoting
    a single training data input instance. The corresponding desired threat values
    (outputs) are *y**[gt]*^((0)), *y**[gt]*^((1)), ⋯ *y[gt]*^((*N*)), say (here,
    the subscript *gt* denotes ground truth). Equivalently, we can say that the training
    data consists of *N* + 1 (input, output) pairs:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让训练数据由 *N* + 1 个输入 ![](../../OEBPS/Images/AR_x.png)^((0))，![](../../OEBPS/Images/AR_x.png)^((1))，⋯
    ![](../../OEBPS/Images/AR_x.png)^((*N*)) 组成。在这里，每个 ![](../../OEBPS/Images/AR_x.png)^((*i*))
    是一个表示单个训练数据输入实例的 2 × 1 向量。相应的期望威胁值（输出）是 *y**[gt]*^((0))，*y**[gt]*^((1))，⋯ *y[gt]*^((*N*))，例如（在这里，下标
    *gt* 表示真实值）。等价地，我们可以说训练数据由 *N* + 1 个（输入，输出）对组成：
- en: (![](../../OEBPS/Images/AR_x.png)^((0)), *y**[gt]*^((0))), (![](../../OEBPS/Images/AR_x.png)^((1)),
    *y**[gt]*^((1)))⋯(![](../../OEBPS/Images/AR_x.png)^((*N*)), *y**[gt]*^((*N*)))
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: (![](../../OEBPS/Images/AR_x.png)^((0)), *y**[gt]*^((0))), (![](../../OEBPS/Images/AR_x.png)^((1)),
    *y**[gt]*^((1)))⋯(![](../../OEBPS/Images/AR_x.png)^((*N*)), *y**[gt]*^((*N*)))
- en: Suppose ![](../../OEBPS/Images/AR_w.png) denotes the (as-yet-unknown) optimal
    parameters for the model. Then, given an arbitrary input ![](../../OEBPS/Images/AR_x.png),
    the machine will estimate a threat value of *y**[predicted]* = ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b*. On the *i[th]* training data pair, (![](../../OEBPS/Images/AR_x.png)^((*i*)),
    *y**[gt]*^((*i*))) the machine will estimate
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 ![](../../OEBPS/Images/AR_w.png) 表示模型（尚未知的）最优参数。然后，给定一个任意的输入 ![](../../OEBPS/Images/AR_x.png)，机器将估计威胁值
    *y**[predicted]* = ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b*。在 *i[th]* 训练数据对（(![](../../OEBPS/Images/AR_x.png)^((*i*)), *y**[gt]*^((*i*)))），机器将估计
- en: '*y**[predicted]*^((*i*)) = ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)^((*i*))
    + *b*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*y**[predicted]*^((*i*)) = ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)^((*i*))
    + *b*'
- en: while the desired output is *y[gt]*^((*i*)). Thus the squared error (aka loss)
    made by the machine on the *i[th]* training data instance is [²](#fn2)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: while the desired output is *y[gt]*^((*i*)). Thus the squared error (aka loss)
    made by the machine on the *i[th]* training data instance is [²](#fn2)
- en: '*e[i]*² = (*y**[predicted]*^((*i*))−*y**[gt]*^((*i*)))²'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*e[i]*² = (*y**[predicted]*^((*i*))−*y**[gt]*^((*i*)))²'
- en: 'The overall loss on the entire training data set is obtained by adding the
    loss from each individual training data instance:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'The overall loss on the entire training data set is obtained by adding the
    loss from each individual training data instance:'
- en: '![](../../OEBPS/Images/eq_01-03-a.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_01-03-a.png)'
- en: The goal of training is to find the set of model parameters (aka weights), ![](../../OEBPS/Images/AR_w.png),
    that minimizes the total error *E*. Exactly how we do this will be described later.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的目的是找到一组模型参数（即权重），![](../../OEBPS/Images/AR_w.png)，以最小化总误差 *E*。我们如何做到这一点将在后面描述。
- en: In most cases, it is not possible to come up with a closed-form solution for
    the optimal ![](../../OEBPS/Images/AR_w.png), *b*. Instead, we take an iterative
    approach depicted in algorithm [1.1](../Text/01.xhtml#alg-supervised_training).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，不可能为最优的 ![](../../OEBPS/Images/AR_w.png)，*b* 提出一个封闭形式的解。相反，我们采取算法[1.1](../Text/01.xhtml#alg-supervised_training)中描述的迭代方法。
- en: Algorithm 1.1 Training a supervised model
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1.1 训练监督模型
- en: Initialize parameters ![](../../OEBPS/Images/AR_w.png), *b* with random values
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机值初始化参数 ![](../../OEBPS/Images/AR_w.png)，*b*
- en: ⊳ iterate while error not small enough
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ⊳ 当误差不够小的时候继续迭代
- en: '**while** (*E*[2] = Σ[*i* = 0]^(i=*N*) (![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)[i]
    ¸ *b* — *y[gt]*^((*i*)))² > *threshold*) **do**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**while** (*E*[2] = Σ[*i* = 0]^(i=*N*) (![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)[i]
    ¸ *b* — *y[gt]*^((*i*)))² > *threshold*) **do**'
- en: ⊳ iterate over all training data instances
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ⊳ 遍历所有训练数据实例
- en: '**for** ∀*[i]* ∈ 2 [0, *N*] **do**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**for** ∀*[i]* ∈ 2 [0, *N*] **do**'
- en: ⊳ details provided in section 3.3 after gradients are introduced
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ⊳ 在引入梯度后，3.3节中提供了详细信息
- en: Adjust ![](../../OEBPS/Images/AR_w.png), *b* so that *E*² is reduced
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 调整 ![](../../OEBPS/Images/AR_w.png)，*b* 以减少 *E*²
- en: '**end** **for**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**end** **for**'
- en: '**end** **while**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**end** **while**'
- en: ⊳ remember the final parameter values as optimal
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ⊳ 记录最终参数值作为最优值
- en: '![](../../OEBPS/Images/AR_w.png)[*]← ![](../../OEBPS/Images/AR_w.png), *b*[*]←
    *b*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_w.png)[*]← ![](../../OEBPS/Images/AR_w.png), *b*[*]←
    *b*'
- en: In this algorithm, we start with random parameter values and keep tuning the
    parameters so the total error goes down at least a little. We keep doing this
    until the error becomes sufficiently small.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个算法中，我们从一个随机的参数值开始，并不断调整参数，使得总误差至少减少一点。我们一直这样做，直到误差足够小。
- en: In a purely mathematical sense, we continue the iterations until the error is
    minimal. But in practice, we often stop when the results are accurate enough for
    the problem being solved. It is worth re-emphasizing that *error* here refers
    only to error on training data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在纯粹数学的意义上，我们继续迭代直到误差最小。但在实践中，我们通常在结果足够准确以解决问题时停止。值得再次强调的是，这里的 *误差* 仅指训练数据上的误差。
- en: 1.3.6 Inferencing
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.6 推断
- en: Finally, a trained machine (with optimal parameters ![](../../OEBPS/Images/AR_w.png)[*],
    *b*[*] is deployed in the world. It will receive new inputs ![](../../OEBPS/Images/AR_x.png)
    and will infer *y**[predicted]*(![](../../OEBPS/Images/AR_x.png)) = ![](../../OEBPS/Images/AR_w.png)[*]*^T*![](../../OEBPS/Images/AR_x.png)
    + *b*[*]. Classification will happen by thresholding *y**[predicted]*, as shown
    in equation [1.2](#eq-threat-thresholding).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个经过训练的机器（具有最优参数 ![图片](../../OEBPS/Images/AR_w.png)[*]，*b*[*]）被部署到世界中。它将接收新的输入
    ![图片](../../OEBPS/Images/AR_x.png) 并推断 *y**[预测]*(![图片](../../OEBPS/Images/AR_x.png))
    = ![图片](../../OEBPS/Images/AR_w.png)[*]*^T*![图片](../../OEBPS/Images/AR_x.png)
    + *b*[*]。分类将通过阈值 *y**[预测]* 来实现，如方程 [1.2](#eq-threat-thresholding) 所示。
- en: 1.4 Geometrical view of machine learning
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 机器学习的几何视图
- en: 'Each input to the cat brain model is an array of two numbers: *x*[0] (signifying
    hardness of the object), *x*[1] signifying sharpness of the object) or, equivalently,
    a 2 × 1 vector ![](../../OEBPS/Images/AR_x.png). A good mental picture is to think
    of the input as a point in a high-dimensional space. The input space is often
    called the *feature space*—a space where all the characteristic features to be
    examined by the model are represented. The feature space dimension is two in this
    case, but in real-life problems it will be in the hundreds or thousands or more.
    The exact dimensionality of the input changes from problem to problem, but the
    intuition that it is a point remains.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 猫脑模型每个输入都是一个包含两个数字的数组：*x*[0]（表示物体的硬度），*x*[1]（表示物体的锋利度）或者，等价地，一个 2 × 1 的向量 ![图片](../../OEBPS/Images/AR_x.png)。一个很好的心理图像是将输入视为高维空间中的一个点。输入空间通常被称为
    *特征空间*——一个模型要检查的所有特征都表示在这个空间中。在这种情况下，特征空间的维度是两个，但在现实生活中的问题中，它将是几百或几千或更多。输入的精确维度会随着问题而变化，但它是点的直觉仍然存在。
- en: The output *y* should also be viewed as a point in another high-dimensional
    space. In this toy problem, the dimensionality of the output space is one, but
    in real problems, it will be higher. Typically, however, the number of output
    dimensions is much smaller than the number of input dimensions.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 *y* 也应该被视为另一个高维空间中的一个点。在这个玩具问题中，输出空间的维度是一维的，但在现实问题中，它将更高。然而，通常输出维度的数量远小于输入维度的数量。
- en: Geometrically speaking, a machine learning model essentially maps a point in
    the feature space to a point in the output space. It is expected that the classification
    or estimation job to be performed by the model is easier in the output space than
    in the feature space. In particular, *for a classification job, input points belonging
    to separate classes are expected to map to separate clusters in output space*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何学的角度来看，机器学习模型本质上是将特征空间中的一个点映射到输出空间中的一个点。预期模型要执行的分类或估计工作在输出空间中比在特征空间中更容易。特别是，*对于分类工作，属于不同类别的输入点预期将映射到输出空间中的不同簇*。
- en: Let’s continue with our example cat brain model to illustrate the idea. As stated
    earlier, our feature space is 2D, with two coordinate axes *X*[0] signifying hardness
    and *X*[1] signifying sharpness.[³](#fn3) Individual points in this 2D space are
    denoted by coordinate values (*x*[0], *x*[1]) in lowercase (see figure [1.2](#fig-geometrical_view)).
    As shown in the diagram, a good way to model the threat score is to measure the
    distance from line *x*[0] + *x*[1] = 1.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用我们的猫脑模型来阐述这个想法。如前所述，我们的特征空间是二维的，有两个坐标轴 *X*[0] 表示硬度，*X*[1] 表示锋利度。[³](#fn3)
    这个二维空间中的单个点用坐标值 (*x*[0]，*x*[1]) 的小写表示（见图 [1.2](#fig-geometrical_view)）。如图所示，建模威胁分数的一个好方法是从线
    *x*[0] + *x*[1] = 1 的距离进行测量。
- en: '![](../../OEBPS/Images/CH01_F02_Chaudhury.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH01_F02_Chaudhury.png)'
- en: Figure 1.2 2D input point space for the cat brain model. The bottom-left corner
    shows objects with low hardness and low sharpness objects (–), while the top-right
    corner shows objects with high hardness and high sharpness (+). Intermediate values
    are near the diagonal ($).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 猫脑模型的 2D 输入点空间。左下角显示硬度低和锋利度低的物体（–），而右上角显示硬度高和锋利度高的物体（+）。中间值接近对角线（$）。
- en: From coordinate geometry, in a 2D space with coordinate axes *X*[0] and *X*[1],
    the signed distance of a point (*a*, *b*) from the line *x*[0] + *x*[1] = 1 is
    *y* = (*a*+*b*–1)/√2. Examining the sign of *y*, we can determine which side of
    the separator line the input point belongs to. In the simple situation depicted
    in figure [1.2](#fig-geometrical_view), observation tells us that the threat score
    can be proxied by the signed distance, *y*, from the diagonal line *x*[0] + *x*[1]
    – 1 = 0. We can make the run/ignore/approach decision by thresholding *y*. Values
    close to zero imply ignore, positive values imply run away, and negative values
    imply approach and purr. From high school geometry, the distance of an arbitrary
    input point (*x*[0]=*a*, *x*[1]=*b*) from line *x*[0] + *x*[1] – 1 = 0 is (*a*+*b*–1)/√2.
    Thus, the function *y*(*x*[0], *x*[1]) = (*x*[0] + *x*[1]–1)/√2 is a possible
    model for the cat brain threat estimator function. Training should converge to
    *w*[0] = 1/√2, *w*[1] = 1/√2 and *b* = –1/√2.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 从坐标几何学来看，在一个具有坐标轴 *X*[0] 和 *X*[1] 的二维空间中，点 (*a*, *b*) 到直线 *x*[0] + *x*[1] =
    1 的有向距离为 *y* = (*a*+*b*–1)/√2。通过检查 *y* 的符号，我们可以确定输入点属于分隔线的哪一侧。在图 [1.2](#fig-geometrical_view)
    所示的简单情况下，观察告诉我们威胁分数可以用从对角线 *x*[0] + *x*[1] – 1 = 0 的有向距离 *y* 来代理。我们可以通过阈值化 *y*
    来做出运行/忽略/接近的决策。接近零的值意味着忽略，正值意味着逃跑，负值意味着接近并咕噜。从高中几何学来看，任意输入点 (*x*[0]=*a*, *x*[1]=*b*)
    到直线 *x*[0] + *x*[1] – 1 = 0 的距离是 (*a*+*b*–1)/√2。因此，函数 *y*(*x*[0], *x*[1]) = (*x*[0]
    + *x*[1]–1)/√2 是猫脑威胁估计函数的一个可能模型。训练应该收敛到 *w*[0] = 1/√2, *w*[1] = 1/√2 和 *b* = –1/√2。
- en: Thus, our simplified cat brain threat score model is
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们简化的猫脑威胁分数模型是
- en: '![](../../OEBPS/Images/eq_01-04.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_01-04.png)'
- en: Equation 1.4
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 1.4
- en: 'It maps the 2D input points, signifying the hardness and sharpness of the object
    in front of the cat, to a 1D value corresponding to the signed distance from a
    separator line. This distance, physically interpretable as a threat score, makes
    it possible to separate the classes (negative threat, neutral, positive threat)
    via thresholding, as shown in equation [1.2](#eq-threat-thresholding). The separate
    classes form distinct clusters in the output space, depicted by +, –, and $ signs
    in the output space. Low values of inputs produce negative threats (the cat will
    approach and purr): for example, *y*(0, 0) = –1/√2. High values of inputs produce
    high threats (the cat will run away): for example, *y*(1, 1) = 1/√2. Medium values
    of inputs produce near-zero threats (the cat will ignore the object): for example,
    *y*(0.5, 0.5) = 0. Of course, because the problem is so simple, we could come
    up with the model parameters via simple observation. In real-life situations,
    this will need training.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 它将二维输入点，表示猫面前物体的硬度和尖锐度，映射到一个对应于从分隔线有向距离的1D值。这个距离，在物理上可以解释为威胁分数，使得通过阈值化可以分离出类别（负威胁、中性、正威胁），如图
    [1.2](#eq-threat-thresholding) 所示。不同的类别在输出空间中形成不同的簇，由输出空间中的 +, –, 和 $ 符号表示。输入的低值产生负威胁（猫会接近并咕噜）：例如，*y*(0,
    0) = –1/√2。输入的高值产生高威胁（猫会逃跑）：例如，*y*(1, 1) = 1/√2。输入的中值产生接近零的威胁（猫会忽略物体）：例如，*y*(0.5,
    0.5) = 0。当然，因为问题如此简单，我们可以通过简单的观察得出模型参数。在现实生活中的情况下，这将需要训练。
- en: The geometric view holds in higher dimensions, too. In general, an *n*-dimensional
    input vector ![](../../OEBPS/Images/AR_x.png) is mapped to an *m*-dimensional
    output vector (usually *m* < *n*) in such a way that the problem becomes much
    simpler in the output space. An example with 3D feature space is shown in figure
    [1.3](#fig-ml-as-mapping).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 几何视图在更高维的情况下也成立。一般来说，一个 *n*-维输入向量 ![](../../OEBPS/Images/AR_x.png) 被映射到一个 *m*-维输出向量（通常
    *m* < *n*），这样问题在输出空间中就变得简单得多。一个3D特征空间的例子如图 [1.3](#fig-ml-as-mapping) 所示。
- en: '![](../../OEBPS/Images/CH01_F03_Chaudhury.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH01_F03_Chaudhury.png)'
- en: Figure 1.3 A model maps the points from input (feature) space to an output space
    where it is easier to separate the classes. For instance, in this figure, input
    feature points belonging to two classes, red (+) and green (–) are distributed
    over the volume of a cylinder in a 3D feature space. The model unfurls the cylinder
    into a rectangle. The feature points are mapped onto a 2D planar output space
    where the two classes can be discriminated with a simple linear separator.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 一个模型将输入（特征）空间中的点映射到一个输出空间，在那里更容易分离类别。例如，在这个图中，属于两个类别（红色 (+) 和绿色 (–)）的输入特征点在3D特征空间的圆柱体体积中分布。模型将圆柱体展开成矩形。特征点被映射到一个二维平面输出空间，在那里可以通过简单的线性分隔器来区分两个类别。
- en: '![](../../OEBPS/Images/CH01_F04_Chaudhury.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH01_F04_Chaudhury.png)'
- en: Figure 1.4 The two classes (indicated by light and dark shades) cannot be separated
    by a line. A curved separator is needed. In 3D, this is equivalent to saying that
    no plane can separate the surfaces; a curved surface is necessary. In still higher-dimensional
    spaces, this is equivalent to saying that no hyperplane can separate the classes;
    a curved is needed.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 两个类别（由浅色和深色阴影表示）不能通过一条线来分离。需要一个曲线分离器。在 3D 中，这相当于说没有平面可以分离表面；需要一个曲面。在更高维度的空间中，这相当于说没有超平面可以分离类别；需要一个曲线。
- en: 1.5 Regression vs. classification in machine learning
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 机器学习中的回归与分类
- en: 'As briefly outlined in section [1.1](#sec-paradigm-shift), there are two types
    of machine learning models: *regressors* and *classifiers*.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如在 [1.1](#sec-paradigm-shift) 节中简要概述的，有两种类型的机器学习模型：*回归器* 和 *分类器*。
- en: In a *regressor*, the model tries to emit a desired value given a specific input.
    For instance, the first stage (threat-score estimator) of the cat brain model
    in section [1.3](../Text/01.xhtml#sec-cat_brain) is a regressor model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *回归器* 中，模型试图在给定特定输入的情况下发出一个期望的值。例如，[1.3](../Text/01.xhtml#sec-cat_brain) 节中猫脑模型的第一阶段（威胁评分估计器）是一个回归器模型。
- en: 'Classifiers, on the other hand, have a set of prespecified classes. Given a
    specific input, they try to emit the *class* to which the input belongs. For instance,
    the full cat brain model has three classes: 1) run away, (2) ignore, and (3) approach
    and purr. Thus, it takes an input (hardness and sharpness values) and emits an
    output decision (aka class).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，分类器有一组预定义的类别。给定一个特定的输入，它们试图发出输入所属的 *类别*。例如，完整的猫脑模型有三个类别：1) 逃跑，(2) 忽视，和
    (3) 接近并咕噜。因此，它接受一个输入（硬度和尖锐度值）并发出一个输出决策（即类别）。
- en: In this example, we convert a regressor into a classifier by thresholding the
    output of the regressor (see equation [1.2](#eq-threat-thresholding)). It is also
    possible to create models that directly output the class without having an intervening
    regressor.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们通过阈值化回归器的输出（见方程式 [1.2](#eq-threat-thresholding)）将回归器转换为分类器。也有可能创建直接输出类别而不需要中间回归器的模型。
- en: 1.6 Linear vs. nonlinear models
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 线性与非线性模型
- en: In figure [1.2](#fig-geometrical_view) we faced a rather simple situation where
    the classes could be separated by a line (a hyperplane in higher-dimensional surfaces).
    This does not happen often in real life. What if the points belonging to different
    classes are as shown in figure [1.4](#fig-non_linear_separator)? In such cases,
    our model architecture should no longer be a simple weighted combination. It is
    a nonlinear function. For instance, check the curved separator in figure [1.4](#fig-non_linear_separator).
    Nonlinear models make sense from the function approximation point of view as well.
    Ultimately, our goal is to approximate very complex and highly nonlinear functions
    that model the classification or estimation processes demanded by life. Intuitively,
    it seems better to use *nonlinear functions* to model them.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [1.2](#fig-geometrical_view) 中，我们遇到了一个相当简单的情况，其中类别可以通过一条线（在更高维度的表面上是一个超平面）来分离。这种情况在现实生活中并不常见。如果属于不同类别的点如图
    [1.4](#fig-non_linear_separator) 所示，会怎样呢？在这种情况下，我们的模型架构不应再是一个简单的加权组合。它是一个非线性函数。例如，检查图
    [1.4](#fig-non_linear_separator) 中的曲线分离器。从函数逼近的角度来看，非线性模型也是有意义的。最终，我们的目标是逼近非常复杂且高度非线性的函数，这些函数可以模拟生活所要求的分类或估计过程。直观上看，使用
    *非线性函数* 来模拟它们似乎更好。
- en: A very popular nonlinear function in machine learning is the *sigmoid* function,
    so named because it looks like the letter *S*. The sigmoid function is typically
    symbolized by the Greek letter *σ*. It is defined as
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中一个非常流行的非线性函数是 *sigmoid* 函数，之所以这样命名是因为它看起来像字母 *S*。sigmoid 函数通常用希腊字母 *σ*
    表示。它定义为
- en: '![](../../OEBPS/Images/eq_01-05.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_01-05.png)'
- en: Equation 1.5
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 1.5
- en: 'The graph of the sigmoid function is shown in figure [1.5](#fig-sigmoid). Thus
    we can use the following popular model architecture (still kind of simple) that
    takes the sigmoid without parameters) of the weighted sum of the inputs:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切函数的图形如图 [1.5](#fig-sigmoid) 所示。因此，我们可以使用以下流行的模型架构（仍然相当简单），它采用没有参数的 sigmoid
    函数的加权输入总和：
- en: '![](../../OEBPS/Images/eq_01-06.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_01-06.png)'
- en: Equation 1.6
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 1.6
- en: '![](../../OEBPS/Images/CH01_F05_Chaudhury.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH01_F05_Chaudhury.jpg)'
- en: Figure 1.5 The sigmoid graph
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 双曲正切函数图形
- en: The sigmoid imparts the nonlinearity. This architecture can handle relatively
    more complex classification tasks than the weighted sum alone. In fact, equation
    [1.6](#eq-logistic-regression) depicts the basic building block of a neural network.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数引入了非线性。这种架构可以处理比仅加权求和更复杂的分类任务。事实上，方程 [1.6](#eq-logistic-regression)
    描述了神经网络的基本构建块。
- en: '1.7 Higher expressive power through multiple nonlinear layers: Deep neural
    networks'
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7 通过多个非线性层提高表达能力：深度神经网络
- en: In section [1.6](#sec-non-linearity) we stated that adding nonlinearity to the
    basic weighted sum yielded a model architecture that is able to handle more complex
    tasks. In machine learning parlance, the nonlinear model has more *expressive
    power*.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [1.6](#sec-non-linearity) 节中，我们提到，将非线性添加到基本加权和中产生了一个能够处理更复杂任务的模型架构。在机器学习的术语中，非线性模型具有更多的
    *表达能力*。
- en: 'Now consider a real-life problem: say, building a dog recognizer. The input
    space comprises pixel locations and pixel colors (*x*, *y*, *r*, *g*, *b*, where
    *r*, *g*, *b* denote the red, green, and blue components of a pixel color). The
    input dimensionality is large (proportional to the number of pixels in the image).
    Figure [1.6](#fig-dog_images) gives a small glimpse of the possible variations
    in background and foreground that a typical deep learning system (such as a dog
    image recognizer) has to deal with.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑一个现实生活中的问题：比如，构建一个狗识别器。输入空间包括像素位置和像素颜色（*x*，*y*，*r*，*g*，*b*，其中 *r*，*g*，*b*
    表示像素颜色的红色、绿色和蓝色分量）。输入维度很大（与图像中的像素数量成正比）。图 [1.6](#fig-dog_images) 给出了典型深度学习系统（如狗图像识别器）必须处理的背景和前景可能变化的简要一瞥。
- en: '![](../../OEBPS/Images/CH01_F06abcd_Chaudhury.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH01_F06abcd_Chaudhury.jpg)'
- en: Figure 1.6 A glimpse into background and foreground variations that a typical
    deep learning system (here, a dog image recognizer) has to deal with
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6 深度学习系统（此处为狗图像识别器）必须处理的背景和前景变化的简要概述
- en: We need a machine with really high expressive power here. How do we create such
    a machine in a principled way?
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们需要一台具有真正高表达能力的机器。我们如何以原则性的方式创建这样的机器？
- en: Instead of generating the output from input in a single step, how about taking
    a cascaded approach? We will generate a set of intermediate or hidden outputs
    from the inputs, where each hidden output is essentially a single logistic regression
    unit. Then we add another layer that takes the output of the previous layer as
    input, and so on. Finally, we combine the outermost hidden layer outputs into
    the grand output.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是一次性从输入生成输出，而是采取级联的方法如何？我们将从输入生成一系列中间或隐藏输出，其中每个隐藏输出本质上是一个单个逻辑回归单元。然后我们添加另一个层，该层以前一层的输出作为输入，依此类推。最后，我们将最外层隐藏层的输出组合成最终输出。
- en: We describe the system in the following equations. Note that we have added a
    superscript to the weights to identify the layer (layer 0 is closest to the input;
    layer *L* is the last layer, furthest from the input). We have also made the subscripts
    twodimensional (so the weights for a given layer become a matrix). The first subscript
    identifies the destination node, and the second subscript identifies the source
    node (see figure 1.7).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以下列方程描述该系统。请注意，我们已对权重添加了上标以标识层（层 0 最接近输入；层 *L* 是最后一层，最远离输入）。我们还使下标成为二维的（因此给定层的权重成为一个矩阵）。第一个下标标识目标节点，第二个下标标识源节点（参见图
    1.7）。
- en: '![](../../OEBPS/Images/CH01_F07_Chaudhury.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH01_F07_Chaudhury.png)'
- en: Figure 1.7 Multilayered neural network
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 多层神经网络
- en: The astute reader may notice that the following equations do *not* have an explicit
    bias term. That is because, for simplicity of notation, we have rolled it into
    the set of weights and assumed that one of the inputs (say, *x*[0] = 1) and the
    corresponding weight (such as *w*[0]) is the bias.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 聪明的读者可能会注意到以下方程没有显式的偏差项。这是因为，为了符号的简洁性，我们将它滚入权重集中，并假设其中一个输入（例如，*x*[0] = 1）和相应的权重（例如，*w*[0]）是偏差。
- en: 'Layer 0: generates *n*[0] hidden outputs from *n* + 1 inputs'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 层 0：从 *n* + 1 个输入生成 *n* 个隐藏输出
- en: '![](../../OEBPS/Images/eq_01-07.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_01-07.png)'
- en: Equation 1.7
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 1.7
- en: 'Layer 1: generates *n*[1] hidden outputs from *n*[0] hidden outputs from layer'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 层 1：从层 0 的 *n*[0] 个隐藏输出生成 *n*[1] 个隐藏输出
- en: '![](../../OEBPS/Images/eq_01-08.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_01-08.png)'
- en: Equation 1.8
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 1.8
- en: 'Final layer (*L*): generates *m* + 1 visible outputs from *n*[*L* − 1] previous
    layer hidden outputs'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最终层 (*L*)：从 *n*[*L* − 1] 个前一层隐藏输出生成 *m* + 1 个可见输出
- en: '![](../../OEBPS/Images/eq_01-09.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_01-09.png)'
- en: Equation 1.9
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式1.9
- en: These equations are shown in figure [1.7](#fig-deep-neural-network). The machine
    depicted in figure [1.7](#fig-deep-neural-network) can be incredibly powerful,
    with huge expressive power. We can adjust its expressive power systematically
    to fit the problem at hand. It then is a neural network. We will devote the rest
    of the book to studying this.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方程式在图[1.7](#fig-deep-neural-network)中展示。图[1.7](#fig-deep-neural-network)中描述的机器可以非常强大，具有巨大的表达能力。我们可以系统地调整其表达能力以适应手头的问题。这样它就变成了一个神经网络。我们将用本书的其余部分来研究这一点。
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we gave an overview of machine learning, leading all the way
    up to deep learning. The ideas were illustrated with a toy cat brain example.
    Some mathematical notions (e.g., vectors) were used in this chapter without proper
    introduction, and you are encouraged to revisit this chapter after vectors and
    matrices have been introduced.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们概述了机器学习，一直谈到深度学习。用玩具猫脑的例子说明了这些思想。本章中使用了某些数学概念（例如，向量），但没有适当介绍，鼓励你在介绍了向量和矩阵之后重新阅读本章。
- en: 'We would like to leave you with the following mental pictures from this chapter:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望本章能给你留下以下心理图像：
- en: Machine learning is a fundamentally different paradigm of computing. In traditional
    computing, we provide a step-by-step instruction sequence to the computer, telling
    it what to do. In machine learning, we build a mathematical model that tries to
    approximate the unknown function that generates a classification or estimation
    from inputs.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习是一种与传统的计算范式根本不同的计算范式。在传统计算中，我们向计算机提供一系列逐步的指令序列，告诉它要做什么。在机器学习中，我们构建一个数学模型，试图逼近从输入中生成分类或估计的未知函数。
- en: The mathematical nature of the model function is stipulated from the physical
    nature and complexity of the classification or estimation task. Models have parameters.
    Parameter values are estimated from training data—inputs with known outputs. The
    parameter values are optimized so that the model output is as close as possible
    to training outputs on training inputs.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型函数的数学性质是由分类或估计任务的物理性质和复杂性规定的。模型有参数。参数值是从训练数据中估计的——具有已知输出的输入。参数值被优化，以便模型在训练输入上的输出尽可能接近训练输出。
- en: An alternative geometric view of a machine is a transformation that maps points
    in the multidimensional input space to a point in the output space.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器的另一种几何视图是将多维输入空间中的点映射到输出空间中的一个点的变换。
- en: The more complex the classification/estimation task, the more complex the approximating
    function. In machine learning parlance, complex tasks need machines with greater
    expressive power. Higher expressive power comes from nonlinearity (e.g., the sigmoid
    function; see equation [1.5](../Text/01.xhtml#eq-sigmoid)) and a layered combination
    of simpler machines. This takes us to deep learning, which is nothing but a multilayered
    nonlinear machine.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类/估计任务越复杂，逼近函数就越复杂。在机器学习的术语中，复杂任务需要具有更大表达能力的机器。更高的表达能力来自非线性（例如，Sigmoid函数；参见方程式[1.5](../Text/01.xhtml#eq-sigmoid)）和更简单机器的分层组合。这把我们带到了深度学习，它不过是一个多层非线性机器。
- en: Complex model functions are often built by combining simpler basis functions.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂的模型函数通常是通过组合更简单的基函数来构建的。
- en: 'Tighten your seat belts: the fun is about to get more intense.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 系好你的安全带：乐趣即将变得更加刺激。
- en: '* * *'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹  If you have some experience with machine learning, you will realize that
    we are talking about “supervised” learning here. There are also machines that
    do not need known outputs to learn—so-called “unsupervised” machines—and we will
    talk about them later. [↩](#fnref1)
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ¹  如果你有一些机器学习的经验，你会意识到我们在这里谈论的是“监督”学习。还有不需要已知输出就能学习的机器——所谓的“无监督”机器——我们稍后会讨论它们。[↩](#fnref1)
- en: ²  In this context, note that it is a common practice to square the error/loss
    to make it sign independent. If we desire an output of, say, 10, we are equally
    happy/unhappy if the output is 9.5 or 10.5. Thus, an error of + 5 or −5 is effectively
    the same; hence we make the error sign independent. [↩](#fnref2)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ²  在这个背景下，请注意，将误差/损失平方以使其符号独立是一个常见的做法。如果我们希望输出为10，那么输出为9.5或10.5，我们同样高兴或不高兴。因此，+5或-5的误差实际上是相同的；因此，我们使误差符号独立。[↩](#fnref2)
- en: ³  We use *X*[0], *X*[1] as coordinate symbols instead of the more familiar
    *X*, *Y* so as not to run out of symbols when going to higher-dimensional spaces.
    [↩](#fnref3)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ³  我们使用 *X*[0]，*X*[1] 作为坐标符号，而不是更常见的 *X*，*Y*，这样在进入更高维空间时不会用完符号。[↩](#fnref3)
- en: ⁴  In mathematics, vectors can have an infinite number of elements. Such vectors
    cannot be expressed as arrays—but we will mostly ignore them in this book. [↩](02.xhtml#fnref4)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴  在数学中，向量可以拥有无限多个元素。这样的向量不能表示为数组——但在这本书中我们主要会忽略它们。[↩](02.xhtml#fnref4)
