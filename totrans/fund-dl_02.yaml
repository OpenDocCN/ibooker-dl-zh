- en: Chapter 2\. Fundamentals of Probability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。概率基础
- en: Probability is a field of mathematics that quantifies our uncertainty regarding
    events. For example, when rolling dice or flipping a coin, barring any irregularities
    in the dice or coin themselves, we are uncertain about the result to come. However,
    we can quantify our belief in each of the potential outcomes via probabilities.
    We say, for example, that on every coin toss the probability of the coin showing
    up heads is <math alttext="one-half"><mfrac><mn>1</mn> <mn>2</mn></mfrac></math>
    . And on every dice roll, we say the probability of a die facing up with a five
    is <math alttext="one-sixth"><mfrac><mn>1</mn> <mn>6</mn></mfrac></math> . These
    are the sorts of probabilities we talk about with ease in our daily lives, but
    how can we define and utilize them effectively? In this chapter we’ll discuss
    the fundamentals of probability and how they connect to key concepts in deep learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概率是一门量化我们对事件的不确定性的数学领域。例如，当掷骰子或抛硬币时，除非骰子或硬币本身存在任何不规则性，否则我们对即将发生的结果感到不确定。然而，我们可以通过概率来量化我们对每种可能结果的信念。例如，我们说每次抛硬币时硬币出现正面的概率是
    <math alttext="one-half"><mfrac><mn>1</mn> <mn>2</mn></mfrac></math> 。每次掷骰子时，我们说骰子朝上的概率是
    <math alttext="one-sixth"><mfrac><mn>1</mn> <mn>6</mn></mfrac></math> 。这些是我们在日常生活中轻松谈论的概率，但我们如何定义和有效利用它们呢？在本章中，我们将讨论概率的基础知识以及它们与深度学习中的关键概念的联系。
- en: Events and Probability
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 事件和概率
- en: When running a trial such as rolling a dice or tossing a coin, we intuitively
    assign some belief to the trial’s possible outcomes. In this section, we aim to
    formalize some of these concepts. In particular, we will begin by working in this
    *discrete* space, where discrete signifies a finite or countably infinite number
    of possibilities. Both rolling a dice and tossing a coin are in the discrete space—when
    rolling a fair dice there are six possible outcomes and when tossing a fair coin
    there are two. We term the entire set of possibilities for an experiment the *sample
    space.* For example, the numbers one through six would make up the sample space
    for rolling a fair dice. We can define *events* as subsets of the sample space.
    The event of rolling at least a three corresponds with the dice facing up any
    number in the subset of three, four, five, and six in the sample space defined
    previously. A set of probabilities that sum to one over all outcomes in the sample
    space is termed a *probability distribution* over that sample space, and these
    distributions will be the main focus of our discussion.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当进行像掷骰子或抛硬币这样的试验时，我们直观地对试验的可能结果赋予一些信念。在本节中，我们旨在形式化其中一些概念。特别是，我们将从在这个*离散*空间中工作开始，其中离散表示有限或可数无限的可能性。掷骰子和抛硬币都在离散空间中——掷一个公平的骰子有六种可能结果，抛一个公平的硬币有两种可能。我们将实验的整个可能性集合称为*样本空间*。例如，从一到六的数字将构成掷一个公平骰子的样本空间。我们可以将*事件*定义为样本空间的子集。至少掷出三的事件对应于之前定义的样本空间中三、四、五和六中的任何数字朝上的骰子。一组在样本空间中所有结果上总和为一的概率被称为该样本空间上的*概率分布*，这些分布将是我们讨论的主要焦点。
- en: In general, we won’t worry too much about where exactly these probabilities
    come from, as that requires a much more rigorous and thorough examination beyond
    the scope of this text. However, we will give some intuition about the different
    interpretations. At a high level, the *frequentist* view sees the probability
    of an outcome as arising from its frequency over a long-run experiment. In the
    case of fair dice, this view claims we can say the probability of any side of
    the dice showing up on a given roll is <math alttext="one-sixth"><mfrac><mn>1</mn>
    <mn>6</mn></mfrac></math> , since performing a large number of rolls and counting
    up the occurrences of each side will give us an estimate that is roughly this
    fraction. As the number of rolls in the experiment grows, we see that this estimate
    gets closer and closer to the limit <math alttext="one-sixth"><mfrac><mn>1</mn>
    <mn>6</mn></mfrac></math> , the outcome’s probability.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们不会过多担心这些概率的确切来源，因为这需要进行更严格和彻底的检查，超出了本文的范围。然而，我们将对不同的解释提供一些直觉。在高层次上，*频率主义*观点认为结果的概率来自于长期实验中的频率。在公平骰子的情况下，这种观点声称我们可以说在给定的投掷中骰子的任何一面出现的概率是
    <math alttext="one-sixth"><mfrac><mn>1</mn> <mn>6</mn></mfrac></math> ，因为进行大量投掷并计算每一面出现的次数将给我们一个大致为这个分数的估计。随着实验中投掷次数的增加，我们看到这个估计越来越接近极限
    <math alttext="one-sixth"><mfrac><mn>1</mn> <mn>6</mn></mfrac></math> ，结果的概率。
- en: On the other hand, the *Bayesian* view of probability is based more on quantifying
    our prior belief in hypotheses and how we update our beliefs in light of new data.
    For a fair dice, the Bayesian view would claim there is no prior information,
    both from the dice’s structure and the rolling process, that would suggest any
    side of the dice as being more likely to turn up than any other side. Thus, we
    would say each outcome has probability <math alttext="one-sixth"><mfrac><mn>1</mn>
    <mn>6</mn></mfrac></math> , our prior belief. The set of probabilities, in this
    case all being <math alttext="one-sixth"><mfrac><mn>1</mn> <mn>6</mn></mfrac></math>
    , associated with each outcome is termed our *prior.* As we see new data, the
    Bayesian view gives us a methodology to update our prior accordingly, where we
    term this new belief our *posterior.* This Bayesian view is sometimes directly
    applied to neural network training, where we first assume that each weight in
    the network has some prior associated with it. As we train the network, we update
    the prior associated with each weight accordingly to better fit the data we see.
    At the end of the training, we are left with a posterior distribution associated
    with each weight.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'We will assume throughout this chapter that the probabilities associated with
    any outcome have been determined via reasonable methods, and focus on how we can
    manipulate these probabilities for use in our analyses. We start with the four
    tenets of probability, specifically in the discrete space:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: The sum of probabilities for all possible outcomes in a sample space must be
    equal to one. In other words, the probability distribution over the sample space
    must sum to one. This should make sense intuitively, since the set of all outcomes
    in the sample space must represent the entire set of possibilities. The probability
    distribution not summing to one would imply the existence of possibilities not
    accounted for, which is contradictory. Mathematically, we say that for any valid
    probability distribution, <math alttext="sigma-summation Underscript o Endscripts
    upper P left-parenthesis o right-parenthesis equals 1"><mrow><msub><mo>∑</mo>
    <mi>o</mi></msub> <mi>P</mi> <mrow><mo>(</mo> <mi>o</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mn>1</mn></mrow></math> , where <math alttext="o"><mi>o</mi></math> represents
    an outcome.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let <math alttext="upper E 1"><msub><mi>E</mi> <mn>1</mn></msub></math> be an
    event, and recall that we define an event as a subset of possible outcomes. We
    call <math alttext="upper E 1 Superscript c"><msubsup><mi>E</mi> <mn>1</mn> <mi>c</mi></msubsup></math>
    the *complement* of <math alttext="upper E 1"><msub><mi>E</mi> <mn>1</mn></msub></math>
    , or all possible outcomes in the sample space that are not in <math alttext="upper
    E 1"><msub><mi>E</mi> <mn>1</mn></msub></math> . The second tenet of probability
    is that <math alttext="upper P left-parenthesis upper E 1 right-parenthesis equals
    1 minus upper P left-parenthesis upper E 1 Superscript c Baseline right-parenthesis"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>E</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>=</mo>
    <mn>1</mn> <mo>-</mo> <mi>P</mi> <mrow><mo>(</mo> <msubsup><mi>E</mi> <mn>1</mn>
    <mi>c</mi></msubsup> <mo>)</mo></mrow></mrow></math> . This is just an application
    of the first tenet—if this were not true, it would clearly contradict the first
    tenet. In [Figure 2-1](#we_see_here_how_the_event_a), we see an example of this,
    where *S* represents the entire space of outcomes, and the event and its complement
    together form the entirety of *S*.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0201.png)'
  id: totrans-9
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 2-1\. Event A and its complement interact to form the entire set of possibilities,
    S. The complement simply defines all the possibilities not originally in A.
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Let <math alttext="upper E 1"><msub><mi>E</mi> <mn>1</mn></msub></math> and
    <math alttext="upper E 2"><msub><mi>E</mi> <mn>2</mn></msub></math> be two events,
    where <math alttext="upper E 1"><msub><mi>E</mi> <mn>1</mn></msub></math> is a
    subset (not necessarily strict) of <math alttext="upper E 2"><msub><mi>E</mi>
    <mn>2</mn></msub></math> . The third tenet is that <math alttext="upper P left-parenthesis
    upper E 1 right-parenthesis less-than-or-equal-to upper P left-parenthesis upper
    E 2 right-parenthesis"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <mo>≤</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></mrow></math> . This, again, shouldn’t be too surprising—the
    second event has at least as many outcomes as the first event, and all the outcomes
    the first event has since the second is a superset of the first. If this tenet
    were not true, that would imply the existence of outcomes with negative probability,
    which is impossible from our definitions.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设 <math alttext="upper E 1"><msub><mi>E</mi> <mn>1</mn></msub></math> 和 <math
    alttext="upper E 2"><msub><mi>E</mi> <mn>2</mn></msub></math> 是两个事件，其中 <math alttext="upper
    E 1"><msub><mi>E</mi> <mn>1</mn></msub></math> 是 <math alttext="upper E 2"><msub><mi>E</mi>
    <mn>2</mn></msub></math> 的子集（不一定是严格的）。第三个原则是 <math alttext="upper P left-parenthesis
    upper E 1 right-parenthesis less-than-or-equal-to upper P left-parenthesis upper
    E 2 right-parenthesis"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <mo>≤</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></mrow></math> 。再次强调，这并不会太令人惊讶——第二个事件至少有第一个事件的那么多结果，而且第二个事件是第一个事件的超集，包含了第一个事件的所有结果。如果这个原则不成立，那就意味着存在具有负概率的结果，这在我们的定义中是不可能的。
- en: The fourth and last tenet of probability is the principle of inclusion and exclusion,
    which states that <math alttext="upper P left-parenthesis upper A union upper
    B right-parenthesis equals upper P left-parenthesis upper A right-parenthesis
    plus upper P left-parenthesis upper B right-parenthesis minus upper P left-parenthesis
    upper A intersection upper B right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>A</mi>
    <mo>∪</mo> <mi>B</mi> <mo>)</mo> <mo>=</mo> <mi>P</mi> <mo>(</mo> <mi>A</mi> <mo>)</mo>
    <mo>+</mo> <mi>P</mi> <mo>(</mo> <mi>B</mi> <mo>)</mo> <mo>-</mo> <mi>P</mi> <mo>(</mo>
    <mi>A</mi> <mo>∩</mo> <mi>B</mi> <mo>)</mo></mrow></math> . For those not familiar
    with this terminology, the <math alttext="union"><mo>∪</mo></math> denotes the
    *union* of the two events, a set operation that takes the two events and returns
    an event that contains all elements from the two original sets. The <math alttext="intersection"><mo>∩</mo></math>
    , or *intersection,* is a set operation that returns an event that contains all
    elements belonging to both of the two original sets. The idea behind the equality
    presented is that by just naively summing the probabilities of *A* and *B,* we
    double-count the elements that belong to both sets. Thus, to accurately obtain
    the probability of the union, we must subtract the probability of the intersection.
    In [Figure 2-2](#the_middle_sliver_labeled), we show two events and what their
    intersection would look like physically, while the union is all the outcomes in
    the combined area of the events.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 概率的第四个也是最后一个原则是包含和排除原则，它规定了 <math alttext="upper P left-parenthesis upper A
    union upper B right-parenthesis equals upper P left-parenthesis upper A right-parenthesis
    plus upper P left-parenthesis upper B right-parenthesis minus upper P left-parenthesis
    upper A intersection upper B right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>A</mi>
    <mo>∪</mo> <mi>B</mi> <mo>)</mo> <mo>=</mo> <mi>P</mi> <mo>(</mo> <mi>A</mi> <mo>)</mo>
    <mo>+</mo> <mi>P</mi> <mo>(</mo> <mi>B</mi> <mo>)</mo> <mo>-</mo> <mi>P</mi> <mo>(</mo>
    <mi>A</mi> <mo>∩</mo> <mi>B</mi> <mo>)</mo></mrow></math> 。对于不熟悉这个术语的人来说，<math
    alttext="union"><mo>∪</mo></math> 表示两个事件的*并集*，这是一个集合操作，将两个事件返回一个包含来自两个原始集合的所有元素的事件。而
    <math alttext="intersection"><mo>∩</mo></math> ，或*交集*，是一个集合操作，返回一个包含属于两个原始集合的所有元素的事件。所述等式背后的思想是，通过简单地对*A*和*B*的概率求和，我们会重复计算属于两个集合的元素。因此，为了准确地获得并集的概率，我们必须减去交集的概率。在[图2-2](#the_middle_sliver_labeled)中，我们展示了两个事件及其交集在物理上的样子，而并集则是两个事件的组合区域中的所有结果。
- en: '![](Images/fdl2_0202.png)'
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](Images/fdl2_0202.png)'
- en: Figure 2-2\. The middle sliver is the overlap between the two sets, containing
    all the outcomes that are in both sets. The union is all the events in the combined
    area of the two circles; if we were to add their probabilities naively, we would
    double-count all the outcomes in the middle sliver.
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2。中间的薄片是两个集合之间的重叠部分，包含了同时在两个集合中的所有结果。并集是两个圆圈组合区域中的所有事件；如果我们简单地将它们的概率相加，我们将重复计算中间薄片中的所有结果。
- en: 'These tenets of probability find their way into everything that has to do with
    the field. For example, in deep learning, most of our problems fall into one of
    two categories: *regression* and *classification*. In the latter, we train a neural
    model that can predict the likelihood that the input belongs to one of a discrete
    number of classes. The famous MNIST digits dataset, for example, provides us with
    pictures of digits and associated numerical labels in the range of 0 through 9\.
    Our objective is to build a *classifier* that can take in this picture and return
    the most likely label as its guess. This is naturally formulated as a problem
    in probability—the classifier produces a probability distribution over the sample
    space, 0 through 9, for any given input and its best guess is the digit that is
    assigned the highest probability. How does this relate to our tenets? Since the
    classifier is producing a probability distribution, it must follow the tenets.
    For example, the probabilities associated with each digit must sum to one—a quick
    back-of-the-envelope check to ensure the model isn’t buggy. In the next section,
    we cover probabilities where we are initially given relevant information that
    affects our beliefs and how to use that information.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概率原则渗透到与该领域有关的一切事物中。例如，在深度学习中，我们的大多数问题可以归为两类：*回归*和*分类*。在分类问题中，我们训练一个神经模型，可以预测输入属于一组离散类别中的哪一个的可能性。例如，著名的MNIST数字数据集为我们提供了0到9范围内的数字图片和相关的数字标签。我们的目标是构建一个*分类器*，可以接收这张图片并返回最有可能的标签作为猜测。这自然地被制定为一个概率问题——分类器产生一个关于样本空间0到9的概率分布，对于任何给定的输入，它的最佳猜测是被分配了最高概率的数字。这与我们的原则有什么关系？由于分类器产生一个概率分布，它必须遵循这些原则。例如，与每个数字相关的概率必须相加为一——这是一个快速的粗略检查，以确保模型没有错误。在下一节中，我们将涵盖最初给定相关信息影响我们信念的概率以及如何使用该信息。
- en: Conditional Probability
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件概率
- en: Knowing information often changes our beliefs, and by consequence, our probabilities.
    Going back to our classic dice example, we may roll the dice thinking that it’s
    fair, while in reality there’s a hidden weight at the dice’s core, making it more
    likely to land up a number greater than three. As we roll the dice, we of course
    start to notice this pattern, and our belief regarding the dice’s fairness starts
    to shift. This is at the core of conditional probability itself. Instead of thinking
    simply about <math alttext="upper P left-parenthesis b i a s e d right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>b</mi> <mi>i</mi> <mi>a</mi> <mi>s</mi> <mi>e</mi> <mi>d</mi> <mo>)</mo></mrow></math>
    or <math alttext="upper P left-parenthesis f a i r right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>f</mi> <mi>a</mi> <mi>i</mi> <mi>r</mi> <mo>)</mo></mrow></math>
    , we have to think about probabilities like <math alttext="upper P left-parenthesis
    b i a s e d vertical-bar i n f o r m a t i o n right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>b</mi> <mi>i</mi> <mi>a</mi> <mi>s</mi> <mi>e</mi> <mi>d</mi> <mo>|</mo>
    <mi>i</mi> <mi>n</mi> <mi>f</mi> <mi>o</mi> <mi>r</mi> <mi>m</mi> <mi>a</mi> <mi>t</mi>
    <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>)</mo></mrow></math> instead. This quantity,
    which we term a *conditional probability,* is spoken as “the probability the dice
    is biased *given* the information we’ve seen.”
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 了解信息通常会改变我们的信念，从而改变我们的概率。回到我们经典的骰子示例，我们可能认为掷骰子是公平的，而实际上骰子的核心有一个隐藏的重量，使得它更有可能掷出大于三的数字。当我们掷骰子时，当然会开始注意到这种模式，我们对骰子公平性的信念开始转变。这正是条件概率的核心。我们不再简单地考虑*P(偏向)*或*P(公平)*，而是要考虑像*P(偏向|信息)*这样的概率。这个量，我们称之为*条件概率*，可以理解为“在我们看到的信息的情况下，骰子偏向的概率”。
- en: How do we think about such probabilities intuitively? For starters, we must
    imagine that we are now in a different universe than the one we started in. The
    new universe is one that incorporates the information we’ve seen since the start
    of the experiment, e.g., our past dice rolls. Going back to our MNIST example,
    the probability distribution that the trained neural net produces is actually
    a conditional probability distribution. The probability that the input image is
    zero, for example, can be seen as *P(0|input).* In plain English, we want to find
    the probability of a zero given all of the pixels that make up the specific input
    image we fed into our neural net. Our new universe is the universe in which the
    input pixels have taken on this specific configuration of values. This is distinct
    from simply looking at *P(0),* the probability of returning a zero, which we can
    think about in terms of prior belief. Without any knowledge of the input pixel
    configuration, we’d have no reason to believe that the possibility of returning
    a zero is any more or less likely than that of any other digit.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何直观地思考这些概率？首先，我们必须想象我们现在处于一个不同的宇宙中，而不是我们开始时的那个宇宙。新的宇宙是一个包含了我们自实验开始以来看到的信息的宇宙，例如我们过去的骰子点数。回到我们的MNIST示例，训练好的神经网络产生的概率分布实际上是一个条件概率分布。例如，输入图像为零的概率可以看作是*P(0|input)*。简单来说，我们想找到的是在我们馈送给神经网络的特定输入图像中组成的所有像素的情况下零的概率。我们的新宇宙是输入像素已经具有这种特定值配置的宇宙。这与简单地看*P(0)*，即返回零的概率是不同的，我们可以从先验信念的角度来思考。如果没有任何关于输入像素配置的知识，我们没有理由相信返回零的可能性比其他数字更有可能或更不可能。
- en: 'Sometimes, seeing certain information does not change our probabilities—we
    call this property *independence.* For example, Tom Brady may have thrown a touchdown
    pass after the third roll of our experiment, but incorporating that information
    into our new universe should (hopefully!) have no impact on the likelihood of
    the dice being biased. We state this independenceproperty as *P(biased|Tom Brady
    throws a touchdown pass) =* *P(biased).* Note that any two events <math alttext="upper
    E 1"><msub><mi>E</mi> <mn>1</mn></msub></math> and <math alttext="upper E 2"><msub><mi>E</mi>
    <mn>2</mn></msub></math> that satisfy this property are independent. Perhaps slightly
    more counterintuitively, if it happens to be the case that all of our dice rolls
    so far don’t numerically change our prior belief regarding the dice’s fairness
    (maybe the dice rolls so far have shown up evenly across one through six and our
    initial prior belief was that the dice was fair), we’d still say that these events
    are independent. Finally, note that independence is symmetric: if <math alttext="upper
    P left-parenthesis upper E 1 vertical-bar upper E 2 right-parenthesis equals upper
    P left-parenthesis upper E 1 right-parenthesis"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>E</mi> <mn>1</mn></msub> <mo>|</mo> <msub><mi>E</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow></mrow></math> , then it is also the case that <math alttext="upper
    P left-parenthesis upper E 2 vertical-bar upper E 1 right-parenthesis equals upper
    P left-parenthesis upper E 2 right-parenthesis"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>E</mi> <mn>2</mn></msub> <mo>|</mo> <msub><mi>E</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></mrow></math> .'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，看到某些信息并不会改变我们的概率——我们称之为*独立性*。例如，汤姆·布雷迪可能在我们的实验第三次掷骰子后投出了一个触摸得分，但将这些信息纳入我们的新宇宙中应该（希望如此！）不会对骰子有偏倚的可能性产生影响。我们将这种独立性属性表述为*P(有偏|汤姆·布雷迪投出触摸得分)
    = P(有偏)*。请注意，任何满足这一属性的两个事件<math alttext="上标E1"><msub><mi>E</mi> <mn>1</mn></msub></math>和<math
    alttext="上标E2"><msub><mi>E</mi> <mn>2</mn></msub></math>都是独立的。也许稍微有些违反直觉的是，如果到目前为止我们所有的掷骰子结果在数值上并没有改变我们对骰子公平性的先验信念（也许到目前为止的掷骰子结果在一到六之间均匀出现，而我们最初的先验信念是骰子是公平的），我们仍然会说这些事件是独立的。最后，请注意独立性是对称的：如果<math
    alttext="P(E1|E2) = P(E1)"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi>
    <mn>1</mn></msub> <mo>|</mo> <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>，那么也有<math
    alttext="P(E2|E1) = P(E2)"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi>
    <mn>2</mn></msub> <mo>|</mo> <msub><mi>E</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>。
- en: 'In the previous section, we introduced intersection and union notation. It
    turns out that we can break down the intersection operation into a product of
    probabilities. We have the following equality: <math alttext="upper P left-parenthesis
    upper E 1 intersection upper E 2 right-parenthesis equals upper P left-parenthesis
    upper E 1 vertical-bar upper E 2 right-parenthesis asterisk upper P left-parenthesis
    upper E 2 right-parenthesis"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi>
    <mn>1</mn></msub> <mo>∩</mo> <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>1</mn></msub> <mo>|</mo>
    <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math> . Let’s break
    down the intuition here. On the left side, we have the probability that both events
    <math alttext="upper E 1"><msub><mi>E</mi> <mn>1</mn></msub></math> and <math
    alttext="upper E 2"><msub><mi>E</mi> <mn>2</mn></msub></math> have occurred. On
    the right side, we have the same idea, but expressed slightly differently. In
    the universe where both events have occurred, one way to arrive in this universe
    is to first have <math alttext="upper E 2"><msub><mi>E</mi> <mn>2</mn></msub></math>
    occur, followed by <math alttext="upper E 1"><msub><mi>E</mi> <mn>1</mn></msub></math>
    . Porting this intuition into mathematical terms, we must first find the probability
    that <math alttext="upper E 2"><msub><mi>E</mi> <mn>2</mn></msub></math> has occurred,
    followed by the probability that <math alttext="upper E 1"><msub><mi>E</mi> <mn>1</mn></msub></math>
    has occurred in the universe where <math alttext="upper E 2"><msub><mi>E</mi>
    <mn>2</mn></msub></math> has already occurred. How do we combine these two probabilities?
    Intuitively, it makes sense that we multiply them—we must have both events occur,
    the first unconditionally and the second in the universe where the first has already
    occurred. Note that the order of these events doesn’t really matter, as both paths
    get us to the same universe. So, more completely, <math alttext="upper P left-parenthesis
    upper E 1 intersection upper E 2 right-parenthesis equals upper P left-parenthesis
    upper E 1 vertical-bar upper E 2 right-parenthesis asterisk upper P left-parenthesis
    upper E 2 right-parenthesis equals upper P left-parenthesis upper E 2 vertical-bar
    upper E 1 right-parenthesis asterisk upper P left-parenthesis upper E 1 right-parenthesis"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>E</mi> <mn>1</mn></msub> <mo>∩</mo> <msub><mi>E</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi>
    <mn>1</mn></msub> <mo>|</mo> <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>2</mn></msub> <mo>|</mo>
    <msub><mi>E</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>E</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math> .'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们介绍了交集和并集符号。事实证明，我们可以将交集操作分解为概率的乘积。我们有以下等式：<math alttext="upper P left-parenthesis
    upper E 1 intersection upper E 2 right-parenthesis equals upper P left-parenthesis
    upper E 1 vertical-bar upper E 2 right-parenthesis asterisk upper P left-parenthesis
    upper E 2 right-parenthesis"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi>
    <mn>1</mn></msub> <mo>∩</mo> <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>1</mn></msub> <mo>|</mo>
    <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>。让我们解释一下这里的直觉。在左边，我们有两个事件<math
    alttext="upper E 1"><msub><mi>E</mi> <mn>1</mn></msub></math>和<math alttext="upper
    E 2"><msub><mi>E</mi> <mn>2</mn></msub></math>同时发生的概率。在右边，我们有相同的想法，但表达略有不同。在这两个事件都发生的宇宙中，到达这个宇宙的一种方式是首先发生<math
    alttext="upper E 2"><msub><mi>E</mi> <mn>2</mn></msub></math>，然后是<math alttext="upper
    E 1"><msub><mi>E</mi> <mn>1</mn></msub></math>。将这种直觉转化为数学术语，我们必须首先找到<math alttext="upper
    E 2"><msub><mi>E</mi> <mn>2</mn></msub></math>发生的概率，然后是在<math alttext="upper E
    2"><msub><mi>E</mi> <mn>2</mn></msub></math>已经发生的宇宙中<math alttext="upper E 1"><msub><mi>E</mi>
    <mn>1</mn></msub></math>发生的概率。我们如何结合这两个概率？直觉上，将它们相乘是有意义的——我们必须让两个事件都发生，第一个是无条件的，第二个是在第一个已经发生的宇宙中。请注意，这些事件的顺序并不重要，因为这两条路径都将我们带到同一个宇宙。因此，更完整地说，<math
    alttext="upper P left-parenthesis upper E 1 intersection upper E 2 right-parenthesis
    equals upper P left-parenthesis upper E 1 vertical-bar upper E 2 right-parenthesis
    asterisk upper P left-parenthesis upper E 2 right-parenthesis equals upper P left-parenthesis
    upper E 2 vertical-bar upper E 1 right-parenthesis asterisk upper P left-parenthesis
    upper E 1 right-parenthesis"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi>
    <mn>1</mn></msub> <mo>∩</mo> <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>1</mn></msub> <mo>|</mo>
    <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>E</mi> <mn>2</mn></msub> <mo>|</mo> <msub><mi>E</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow></mrow></math>。
- en: However, some of these paths make much more physical sense than others. For
    example, if we think of *<math alttext="upper E 1"><msub><mi>E</mi> <mn>1</mn></msub></math>*
    as the event where someone contracts a disease, and <math alttext="upper E 2"><msub><mi>E</mi>
    <mn>2</mn></msub></math> as the event where the patient shows symptoms of the
    disease, the path in which the patient contracts the disease and then shows symptoms
    makes much more physical sense than the reverse.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，其中一些路径比其他路径更有物理意义。例如，如果我们将*<math alttext="upper E 1"><msub><mi>E</mi> <mn>1</mn></msub></math>*看作某人感染疾病的事件，将<math
    alttext="upper E 2"><msub><mi>E</mi> <mn>2</mn></msub></math>看作患者出现疾病症状的事件，那么患者先感染疾病然后出现症状的路径比反过来的路径更有物理意义。
- en: In the case where the two events are independent, we have that <math alttext="upper
    P left-parenthesis upper E 1 intersection upper E 2 right-parenthesis equals upper
    P left-parenthesis upper E 1 vertical-bar upper E 2 right-parenthesis asterisk
    upper P left-parenthesis upper E 2 right-parenthesis equals upper P left-parenthesis
    upper E 1 right-parenthesis asterisk upper P left-parenthesis upper E 2 right-parenthesis
    period"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>1</mn></msub> <mo>∩</mo>
    <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>E</mi> <mn>1</mn></msub> <mo>|</mo> <msub><mi>E</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <mo>.</mo></mrow></math> Hopefully this makes some intuitive
    sense. In the independence scenario, the fact that <math alttext="upper E 2"><msub><mi>E</mi>
    <mn>2</mn></msub></math> has occurred doesn’t affect the chances of <math alttext="upper
    E 1"><msub><mi>E</mi> <mn>1</mn></msub></math> occurring; i.e., incorporating
    this information into the new universe doesn’t affect the probability of the next
    event. In the next section, we cover random variables, which are relevant summaries
    of events and also have their own probability distributions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个事件独立的情况下，我们有<math alttext="upper P left-parenthesis upper E 1 intersection
    upper E 2 right-parenthesis equals upper P left-parenthesis upper E 1 vertical-bar
    upper E 2 right-parenthesis asterisk upper P left-parenthesis upper E 2 right-parenthesis
    equals upper P left-parenthesis upper E 1 right-parenthesis asterisk upper P left-parenthesis
    upper E 2 right-parenthesis period"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi>
    <mn>1</mn></msub> <mo>∩</mo> <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mn>1</mn></msub> <mo>|</mo>
    <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>E</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>E</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mo>.</mo></mrow></math>
    希望这些能有一些直观的理解。在独立的情况下，事件<math alttext="upper E 2"><msub><mi>E</mi> <mn>2</mn></msub></math>
    的发生不会影响事件<math alttext="upper E 1"><msub><mi>E</mi> <mn>1</mn></msub></math> 发生的概率；即，将这些信息纳入新的宇宙中不会影响下一个事件的概率。在接下来的部分中，我们将讨论随机变量，它们是事件的相关总结，也有自己的概率分布。
- en: Random Variables
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机变量
- en: Once again, let’s consider the coin flipping experiment. If we flip a coin some
    finite number of times, natural questions start to arise. How many heads did we
    encounter during our experiment? How many tails? How many tails until the first
    head? Every outcome in such an experiment has an answer to each of the listed
    questions. If we flip a coin say, five times, and we receive the sequence TTHHT,
    we have seen two heads, three tails, and two tails until the first head.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们考虑抛硬币的实验。如果我们抛硬币有限次数，自然会产生一些问题。在我们的实验中遇到了多少次正面？多少次反面？第一个正面前有多少次反面？在这样一个实验中，每个结果都有对应的答案。比如，如果我们抛硬币5次，得到了序列TTHHT，我们看到了两次正面，三次反面，以及第一个正面前有两次反面。
- en: We can think of a *random variable* as a map, or a function, from the sample
    space to another space, such as the integers in [Figure 2-3](#the_random_variables_x_y_andz).
    Such a function would take as input the sequence TTHHT and output one of the three
    answers listed depending on the question we ask. The value that the random variable
    takes on would be the output associated with result of the experiment. Although
    random variables are deterministic in that they map a given input to a single
    output, they are not deterministic in that they also have a distribution associated
    with their output space. This is due to the inherent randomness in the experiment—depending
    on the probability of the input outcome, its corresponding output may be more
    or less likely than other outputs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将*随机变量*看作是一个从样本空间到另一个空间的映射或函数，比如[图2-3](#the_random_variables_x_y_andz)中的整数。这样一个函数将以TTHHT作为输入，并根据我们提出的问题输出三个答案中的一个。随机变量取得的值将是与实验结果相关联的输出。虽然随机变量是确定性的，因为它们将给定的输入映射到单个输出，但它们在输出空间中也有与之相关的分布。这是由于实验中固有的随机性——根据输入结果的概率，其相应的输出可能比其他输出更有可能。
- en: '![](Images/fdl2_0203.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0203.png)'
- en: Figure 2-3\. Random variables X, Y, and Z all act on the same sample space,
    but have varying outputs. It’s important to keep in mind what you’re measuring!
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3. 随机变量X、Y和Z都作用于相同的样本空间，但具有不同的输出。记住你正在测量什么是很重要的！
- en: Note
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that multiple inputs could map to the same output. For example, *X(HHH)*
    = 3 in addition to *X(HHTH)* in [Figure 2-3](#the_random_variables_x_y_andz).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，多个输入可能映射到相同的输出。例如，*X(HHH)* = 3，除了*X(HHTH)*在[图2-3](#the_random_variables_x_y_andz)中也是如此。
- en: One easy way to begin is to just think of this map as an identity function—whatever
    we flip or roll, its map in the output space is exactly the same as the input.
    Encoding a heads as a one and a tails as a zero, we can define a random variable
    representing the coin flip as whether the coin came up heads, i.e., *C(1) = 1,*
    where *C* is our random variable. In the dice scenario, the mapped output is the
    same as whatever we rolled, i.e., *D**(5) = 5,* where *D*is our random variable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的开始方法是将这个映射看作是一个恒等函数——无论我们抛硬币还是掷骰子，它在输出空间中的映射与输入完全相同。将正面编码为1，反面编码为0，我们可以定义一个代表硬币翻转的随机变量，即硬币是正面的情况，即*C(1)
    = 1*，其中*C*是我们的随机变量。在掷骰子的情况下，映射的输出与我们掷出的数字相同，即*D(5) = 5*，其中*D*是我们的随机变量。
- en: Why should we care about random variables and their distributions? It turns
    out they play a vital role in deep learning and machine learning as a whole. For
    example, in [Chapter 4](ch04.xhtml#training_feed_forward), we will cover the concept
    of *dropout*, a technique for mitigating overfitting in neural networks. The idea
    of a dropout layer is that, during training, it independently and at random masks
    every neuron in the previous layer with some probability. This prevents the network
    from becoming overly dependent on specific connections or subnetworks. We can
    think of every neuron in the previous layer as representing a coin flip-type experiment.
    The only difference is that we set the probability of this experiment, rather
    than a fair coin having the default probability <math alttext="one-half"><mfrac><mn>1</mn>
    <mn>2</mn></mfrac></math> of showing up either side. Each neuron has a random
    variable *X* associated with it, with input one if the dropout layer decides to
    mask it and zero otherwise. *X* is an identity function from the input space to
    the output space, i.e., *X(1) = 1* and *X(0) = 0\.*
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们要关心随机变量及其分布？事实证明，它们在深度学习和机器学习中起着至关重要的作用。例如，在[第4章](ch04.xhtml#training_feed_forward)中，我们将介绍*辍学*的概念，这是一种在神经网络中减少过拟合的技术。辍学层的想法是，在训练期间，它独立且随机地以一定概率屏蔽前一层中的每个神经元。这可以防止网络过度依赖特定连接或子网络。我们可以将前一层中的每个神经元看作代表硬币翻转类型实验。唯一的区别是，我们设置了这个实验的概率，而不是一个公平硬币具有默认概率*mfrac
    1 2*的概率。每个神经元都有与之关联的随机变量*X*，如果辍学层决定屏蔽它，则输入为1，否则为0。*X*是一个从输入空间到输出空间的恒等函数，即*X(1)
    = 1*和*X(0) = 0*。
- en: Random variables, in general, need not be the identity map. Most functions you
    can think of are valid methods of mapping the input space to an output space where
    the random variable is defined. For example, if the input space were every possible
    length *n* sequence of coin flips, the function could be to count the number of
    heads in the sequence and square it. Some random variables can even be expressed
    as functions of other random variables, or a function of a function, as we will
    cover later. If we again consider the input space of every possible length *n*
    sequence of coin flips, the random variable counting the number of heads in the
    input sequence is the same as counting whether each individual coin flip turned
    up heads and taking a sum of all of those values. In mathematical terms, we say
    <math alttext="upper X equals sigma-summation Underscript i equals 1 Overscript
    n Endscripts upper C Subscript i"><mrow><mi>X</mi> <mo>=</mo> <msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <msub><mi>C</mi>
    <mi>i</mi></msub></mrow></math> , where *X* is the random variable representing
    the total number of heads, and <math alttext="upper C Subscript i"><msub><mi>C</mi>
    <mi>i</mi></msub></math> is the binary random variable associated with the *i*th
    coin flip. Back to the dropout example, we can think of the random variable representing
    the total number of masked-out neurons as the sum of binary random variables representing
    each neuron.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量，一般来说，不必是恒等映射。您可以想到的大多数函数都是将输入空间映射到定义了随机变量的输出空间的有效方法。例如，如果输入空间是每个可能长度为*n*的硬币翻转序列，函数可以是计算序列中头的数量并对其进行平方。一些随机变量甚至可以表示为其他随机变量的函数，或者是函数的函数，我们稍后会讨论。如果我们再次考虑每个可能长度为*n*的硬币翻转序列的输入空间，那么计算输入序列中头的数量的随机变量与计算每个单独硬币翻转是否为头并将所有这些值求和的随机变量是相同的。在数学术语中，我们说*X等于sigma-summation
    Underscript i equals 1 Overscript n Endscripts upper C Subscript i*，其中*X*是表示头的总数的随机变量，*C
    i*是与第*i*次硬币翻转相关的二进制随机变量。回到辍学的例子，我们可以将代表被屏蔽神经元总数的随机变量看作是代表每个神经元的二进制随机变量之和。
- en: In the future, when we want to refer to the event where the random variable
    takes on a specific value *c* (the domain being the output space we’ve been referring
    to, e.g., the number of heads in a sequence of coin flips), we will write this
    concisely as *X = c*.We denote the probability that the random variable takes
    on a specific value as *P(X = c),* for example. The probability that the random
    variable takes on any given value in the output space is just the sum of the probabilities
    of the inputs that map to it. This should make some intuitive sense, as this is
    basically the fourth tenet of probability where the intersection between any two
    events is the empty set since all the events we start from are individual, distinct
    inputs. Note that *P(X)* itself is also a probability distribution that follows
    all the basic tenets of probability described in the first section. In the next
    section, we consider statistics regarding random variables.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 将来，当我们想要提及随机变量取特定值*c*的事件时（域是我们一直在提到的输出空间，例如硬币翻转序列中的头数），我们将简洁地写为*X = c*。我们将随机变量取特定值的概率表示为*P(X
    = c)*，例如。随机变量在输出空间中取任何给定值的概率只是映射到它的输入的概率之和。这应该有一些直观的意义，因为这基本上是概率的第四原则，其中任何两个事件之间的交集是空集，因为我们从的所有事件都是独立的、不同的输入。请注意，*P(X)*本身也是一个遵循第一节描述的概率的所有基本原则的概率分布。在下一节中，我们将考虑关于随机变量的统计数据。
- en: Expectation
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 期望值
- en: As we discussed, a random variable is a map from input space to output space,
    where inputs are generated according to some probability distribution. The random
    variable can be thought of as a relevant summary of the input, and can take on
    many forms depending on the question we ask. Sometimes, it’s useful to understand
    statistics regarding the random variable. For example, if we flip a coin eight
    times, how many heads do we expect to see on average? And, of course, we don’t
    see the average number of heads all the time—how much does the number of heads
    we see tend to vary? The first quantity is what we call the random variable’s
    *expectation*, and the second is the random variable’s *variance*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论的，随机变量是从输入空间到输出空间的映射，其中输入根据某种概率分布生成。随机变量可以被视为输入的相关摘要，并且根据我们提出的问题可以采用多种形式。有时，了解关于随机变量的统计数据是有用的。例如，如果我们抛硬币八次，我们平均期望看到多少次正面？当然，我们并不总是看到平均头数——我们看到的头数会有多大变化？第一个数量是我们称之为随机变量的*期望*，第二个是随机变量的*方差*。
- en: For a random variable *X*, we denote its expectation as <math alttext="double-struck
    upper E left-bracket upper X right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo> <mi>X</mi>
    <mo>]</mo></mrow></math> . We can think of this as the average value that *X*
    takes on, weighted by the probability of each of those outcomes. Mathematically,
    this is written as <math alttext="double-struck upper E left-bracket upper X right-bracket
    equals sigma-summation Underscript o Endscripts o asterisk upper P left-parenthesis
    upper X equals o right-parenthesis"><mrow><mi>𝔼</mi> <mrow><mo>[</mo> <mi>X</mi>
    <mo>]</mo></mrow> <mo>=</mo> <msub><mo>∑</mo> <mi>o</mi></msub> <mi>o</mi> <mo>*</mo>
    <mi>P</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>=</mo> <mi>o</mi> <mo>)</mo></mrow></mrow></math>
    . Note that if all outcomes *o* are equally likely, we get a simple average of
    all the outcomes. It makes sense to use the probability of the outcome as a weighting,
    since some outcomes are more likely than others, and the average value we observe
    will be skewed toward such outcomes. For a single fair coin flip, the expected
    number of heads would be <math alttext="sigma-summation Underscript o element-of
    StartSet 0 comma 1 EndSet Endscripts o asterisk upper P left-parenthesis o right-parenthesis
    equals 0 asterisk 0.5 plus 1 asterisk 0.5 equals 0.5"><mrow><msub><mo>∑</mo> <mrow><mi>o</mi><mo>∈</mo><mo>{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>}</mo></mrow></msub>
    <mi>o</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>o</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mn>0</mn> <mo>*</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mo>+</mo> <mn>1</mn>
    <mo>*</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn></mrow></math>
    . In other words, we’d expect to see half of a head for any given fair coin flip.
    Of course, this makes no physical sense in that we could never possibly flip half
    of a head, but this gives you an idea of the proportions we’d expect to see over
    a long run experiment.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机变量*X*，我们将其期望表示为<math alttext="double-struck upper E left-bracket upper X
    right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo> <mi>X</mi> <mo>]</mo></mrow></math>。我们可以将其视为*X*取值的平均值，按照每个结果的概率加权。数学上，这被写为<math
    alttext="double-struck upper E left-bracket upper X right-bracket equals sigma-summation
    Underscript o Endscripts o asterisk upper P left-parenthesis upper X equals o
    right-parenthesis"><mrow><mi>𝔼</mi> <mrow><mo>[</mo> <mi>X</mi> <mo>]</mo></mrow>
    <mo>=</mo> <msub><mo>∑</mo> <mi>o</mi></msub> <mi>o</mi> <mo>*</mo> <mi>P</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>=</mo> <mi>o</mi> <mo>)</mo></mrow></mrow></math>。请注意，如果所有结果*o*是等可能的，我们得到所有结果的简单平均值。使用结果的概率作为加权是有意义的，因为某些结果比其他结果更有可能发生，我们观察到的平均值将偏向这些结果。对于一次公平抛硬币，预期的正面数量将是<math
    alttext="sigma-summation Underscript o element-of StartSet 0 comma 1 EndSet Endscripts
    o asterisk upper P left-parenthesis o right-parenthesis equals 0 asterisk 0.5
    plus 1 asterisk 0.5 equals 0.5"><mrow><msub><mo>∑</mo> <mrow><mi>o</mi><mo>∈</mo><mo>{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>}</mo></mrow></msub>
    <mi>o</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>o</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mn>0</mn> <mo>*</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mo>+</mo> <mn>1</mn>
    <mo>*</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn></mrow></math>。换句话说，我们预计在任何给定的公平抛硬币中看到半个正面。当然，这在物理上没有意义，因为我们永远不可能抛出半个正面，但这给了你一个关于我们在长期实验中预期看到的比例的想法。
- en: Returning to our example of length *n* sequences of coin flips, let’s try to
    find the expected number of heads in such a sequence. We have *n* + 1 possible
    number of heads, and according to our formula, we’d need to find the probability
    of attaining each possible number to use as our weights. Mathematically, we’d
    need to compute <math alttext="sigma-summation Underscript x element-of StartSet
    0 comma ellipsis comma n EndSet Endscripts x asterisk upper P left-parenthesis
    upper X equals x right-parenthesis"><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><mo>{</mo><mn>0</mn><mo>,</mo><mo>...</mo><mo>,</mo><mi>n</mi><mo>}</mo></mrow></msub>
    <mi>x</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>=</mo> <mi>x</mi>
    <mo>)</mo></mrow></mrow></math> , where *X* is the random variable representing
    the total number of heads. However, as *n* gets larger and larger, performing
    this calculation starts to become more and more complicated.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的例子，长度为*n*的硬币序列，让我们尝试找到这样一个序列中预期的正面数量。我们有*n* + 1种可能的正面数量，根据我们的公式，我们需要找到获得每个可能数量的概率作为我们的权重。数学上，我们需要计算<math
    alttext="sigma-summation Underscript x element-of StartSet 0 comma ellipsis comma
    n EndSet Endscripts x asterisk upper P left-parenthesis upper X equals x right-parenthesis"><mrow><msub><mo>∑</mo>
    <mrow><mi>x</mi><mo>∈</mo><mo>{</mo><mn>0</mn><mo>,</mo><mo>...</mo><mo>,</mo><mi>n</mi><mo>}</mo></mrow></msub>
    <mi>x</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>=</mo> <mi>x</mi>
    <mo>)</mo></mrow></mrow></math>，其中*X*是代表正面总数的随机变量。然而，随着*n*变得越来越大，执行这个计算开始变得越来越复杂。
- en: Instead, let’s denote <math alttext="upper X Subscript i"><msub><mi>X</mi> <mi>i</mi></msub></math>
    as the binary random variable for the *i*th coin flip and use the observation
    we made in the last section of being able to break up the total number of heads
    into a sum over heads/tails for all the individual coin flips. Since we know <math
    alttext="upper X equals upper X 1 plus upper X 2 plus ellipsis plus upper X Subscript
    n"><mrow><mi>X</mi> <mo>=</mo> <msub><mi>X</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>X</mi>
    <mn>2</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>X</mi> <mi>n</mi></msub></mrow></math>
    , we can also say that <math alttext="double-struck upper E left-bracket upper
    X right-bracket equals double-struck upper E left-bracket upper X 1 plus upper
    X 2 plus ellipsis plus upper X Subscript n Baseline right-bracket"><mrow><mi>𝔼</mi>
    <mrow><mo>[</mo> <mi>X</mi> <mo>]</mo></mrow> <mo>=</mo> <mi>𝔼</mi> <mrow><mo>[</mo>
    <msub><mi>X</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>X</mi> <mn>2</mn></msub>
    <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>X</mi> <mi>n</mi></msub> <mo>]</mo></mrow></mrow></math>
    . How does making this substitution make our problem easier? We now introduce
    the concept of *linearity of expectation,* which states we can break up the right
    side into the sum <math alttext="double-struck upper E left-bracket upper X 1
    right-bracket plus double-struck upper E left-bracket upper X 2 right-bracket
    plus ellipsis plus double-struck upper E left-bracket upper X Subscript n Baseline
    right-bracket"><mrow><mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>X</mi> <mn>1</mn></msub>
    <mo>]</mo></mrow> <mo>+</mo> <mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>X</mi> <mn>2</mn></msub>
    <mo>]</mo></mrow> <mo>+</mo> <mo>...</mo> <mo>+</mo> <mi>𝔼</mi> <mrow><mo>[</mo>
    <msub><mi>X</mi> <mi>n</mi></msub> <mo>]</mo></mrow></mrow></math> . We know that
    the expected number of heads for each flip is 0.5, so the expected number of heads
    in a sequence of *n* flips is just 0.5**n*. This is much simpler than going down
    the previous route, as this approach’s difficulty does not scale with the number
    of flips.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go over the simplification we made in a bit more detail. Mathematically,
    if we have any two independent random variables *A* and *B*:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="double-struck upper E left-bracket upper A plus upper B right-bracket
    equals sigma-summation Underscript a comma b Endscripts left-parenthesis a plus
    b right-parenthesis asterisk upper P left-parenthesis upper A equals a comma upper
    B equals b right-parenthesis"><mrow><mi>𝔼</mi> <mrow><mo>[</mo> <mi>A</mi> <mo>+</mo>
    <mi>B</mi> <mo>]</mo></mrow> <mo>=</mo> <msub><mo>∑</mo> <mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>*</mo>
    <mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>=</mo> <mi>a</mi> <mo>,</mo> <mi>B</mi>
    <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E left-bracket upper A plus upper B right-bracket
    equals sigma-summation Underscript a comma b Endscripts left-parenthesis a plus
    b right-parenthesis asterisk upper P left-parenthesis upper A equals a comma upper
    B equals b right-parenthesis"><mrow><mi>𝔼</mi> <mrow><mo>[</mo> <mi>A</mi> <mo>+</mo>
    <mi>B</mi> <mo>]</mo></mrow> <mo>=</mo> <msub><mo>∑</mo> <mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>*</mo>
    <mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>=</mo> <mi>a</mi> <mo>,</mo> <mi>B</mi>
    <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math>
- en: <math alttext="equals sigma-summation Underscript a comma b Endscripts left-parenthesis
    a plus b right-parenthesis asterisk upper P left-parenthesis upper A equals a
    right-parenthesis asterisk upper P left-parenthesis upper B equals b right-parenthesis"><mrow><mo>=</mo>
    <msub><mo>∑</mo> <mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>a</mi> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mi>A</mi> <mo>=</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mi>B</mi> <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals sigma-summation Underscript a comma b Endscripts left-parenthesis
    a plus b right-parenthesis asterisk upper P left-parenthesis upper A equals a
    right-parenthesis asterisk upper P left-parenthesis upper B equals b right-parenthesis"><mrow><mo>=</mo>
    <msub><mo>∑</mo> <mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>a</mi> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mi>A</mi> <mo>=</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mi>B</mi> <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math>
- en: <math alttext="equals sigma-summation Underscript a comma b Endscripts a asterisk
    upper P left-parenthesis upper A equals a right-parenthesis asterisk upper P left-parenthesis
    upper B equals b right-parenthesis plus b asterisk upper P left-parenthesis upper
    A equals a right-parenthesis asterisk upper P left-parenthesis upper B equals
    b right-parenthesis"><mrow><mo>=</mo> <msub><mo>∑</mo> <mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></msub>
    <mi>a</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>=</mo> <mi>a</mi>
    <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>B</mi> <mo>=</mo>
    <mi>b</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>b</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mi>A</mi> <mo>=</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mi>B</mi> <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals sigma-summation Underscript a comma b Endscripts a asterisk
    upper P left-parenthesis upper A equals a right-parenthesis asterisk upper P left-parenthesis
    upper B equals b right-parenthesis plus b asterisk upper P left-parenthesis upper
    A equals a right-parenthesis asterisk upper P left-parenthesis upper B equals
    b right-parenthesis"><mrow><mo>=</mo> <msub><mo>∑</mo> <mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></msub>
    <mi>a</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>=</mo> <mi>a</mi>
    <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>B</mi> <mo>=</mo>
    <mi>b</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>b</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mi>A</mi> <mo>=</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mi>B</mi> <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math>
- en: <math alttext="equals sigma-summation Underscript a comma b Endscripts a asterisk
    upper P left-parenthesis upper A equals a right-parenthesis asterisk upper P left-parenthesis
    upper B equals b right-parenthesis plus sigma-summation Underscript a comma b
    Endscripts b asterisk upper P left-parenthesis upper A equals a right-parenthesis
    asterisk upper P left-parenthesis upper B equals b right-parenthesis"><mrow><mo>=</mo>
    <msub><mo>∑</mo> <mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></msub> <mi>a</mi>
    <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>=</mo> <mi>a</mi> <mo>)</mo></mrow>
    <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>B</mi> <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow>
    <mo>+</mo> <msub><mo>∑</mo> <mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></msub>
    <mi>b</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>=</mo> <mi>a</mi>
    <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>B</mi> <mo>=</mo>
    <mi>b</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals sigma-summation Underscript a comma b Endscripts a asterisk
    upper P left-parenthesis upper A equals a right-parenthesis asterisk upper P left-parenthesis
    upper B equals b right-parenthesis plus sigma-summation Underscript a comma b
    Endscripts b asterisk upper P left-parenthesis upper A equals a right-parenthesis
    asterisk upper P left-parenthesis upper B equals b right-parenthesis"><mrow><mo>=</mo>
    <msub><mo>∑</mo> <mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></msub> <mi>a</mi>
    <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>=</mo> <mi>a</mi> <mo>)</mo></mrow>
    <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>B</mi> <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow>
    <mo>+</mo> <msub><mo>∑</mo> <mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></msub>
    <mi>b</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>=</mo> <mi>a</mi>
    <mo>)</mo></mrow> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>B</mi> <mo>=</mo>
    <mi>b</mi> <mo>)</mo></mrow></mrow></math>
- en: <math alttext="equals sigma-summation Underscript a Endscripts a asterisk upper
    P left-parenthesis upper A equals a right-parenthesis sigma-summation Underscript
    b Endscripts upper P left-parenthesis upper B equals b right-parenthesis plus
    sigma-summation Underscript b Endscripts b asterisk upper P left-parenthesis upper
    B equals b right-parenthesis sigma-summation Underscript a Endscripts upper P
    left-parenthesis upper A equals a right-parenthesis"><mrow><mo>=</mo> <msub><mo>∑</mo>
    <mi>a</mi></msub> <mi>a</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>A</mi>
    <mo>=</mo> <mi>a</mi> <mo>)</mo></mrow> <msub><mo>∑</mo> <mi>b</mi></msub> <mi>P</mi>
    <mrow><mo>(</mo> <mi>B</mi> <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>+</mo>
    <msub><mo>∑</mo> <mi>b</mi></msub> <mi>b</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mi>B</mi> <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow> <msub><mo>∑</mo> <mi>a</mi></msub>
    <mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>=</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals sigma-summation Underscript a Endscripts a asterisk upper
    P left-parenthesis upper A equals a right-parenthesis sigma-summation Underscript
    b Endscripts upper P left-parenthesis upper B equals b right-parenthesis plus
    sigma-summation Underscript b Endscripts b asterisk upper P left-parenthesis upper
    B equals b right-parenthesis sigma-summation Underscript a Endscripts upper P
    left-parenthesis upper A equals a right-parenthesis"><mrow><mo>=</mo> <msub><mo>∑</mo>
    <mi>a</mi></msub> <mi>a</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>A</mi>
    <mo>=</mo> <mi>a</mi> <mo>)</mo></mrow> <msub><mo>∑</mo> <mi>b</mi></msub> <mi>P</mi>
    <mrow><mo>(</mo> <mi>B</mi> <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>+</mo>
    <msub><mo>∑</mo> <mi>b</mi></msub> <mi>b</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mi>B</mi> <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow> <msub><mo>∑</mo> <mi>a</mi></msub>
    <mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>=</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
- en: <math alttext="equals sigma-summation Underscript a Endscripts a asterisk upper
    P left-parenthesis upper A equals a right-parenthesis plus sigma-summation Underscript
    b Endscripts b asterisk upper P left-parenthesis upper B equals b right-parenthesis"><mrow><mo>=</mo>
    <msub><mo>∑</mo> <mi>a</mi></msub> <mi>a</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mi>A</mi> <mo>=</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>+</mo> <msub><mo>∑</mo>
    <mi>b</mi></msub> <mi>b</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>B</mi>
    <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals sigma-summation Underscript a Endscripts a asterisk upper
    P left-parenthesis upper A equals a right-parenthesis plus sigma-summation Underscript
    b Endscripts b asterisk upper P left-parenthesis upper B equals b right-parenthesis"><mrow><mo>=</mo>
    <msub><mo>∑</mo> <mi>a</mi></msub> <mi>a</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mi>A</mi> <mo>=</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>+</mo> <msub><mo>∑</mo>
    <mi>b</mi></msub> <mi>b</mi> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>B</mi>
    <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math>
- en: <math alttext="equals double-struck upper E left-bracket upper A right-bracket
    plus double-struck upper E left-bracket upper B right-bracket"><mrow><mo>=</mo>
    <mi>𝔼</mi> <mo>[</mo> <mi>A</mi> <mo>]</mo> <mo>+</mo> <mi>𝔼</mi> <mo>[</mo> <mi>B</mi>
    <mo>]</mo></mrow></math>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals double-struck upper E left-bracket upper A right-bracket
    plus double-struck upper E left-bracket upper B right-bracket"><mrow><mo>=</mo>
    <mi>𝔼</mi> <mo>[</mo> <mi>A</mi> <mo>]</mo> <mo>+</mo> <mi>𝔼</mi> <mo>[</mo> <mi>B</mi>
    <mo>]</mo></mrow></math>
- en: Note
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that we made the independence assumption we talked about earlier in the
    chapter here when we broke up the probability of the event *A = a* and the event
    *B = b* into a product of the two individual probabilities. The rest of the derivation
    doesn’t require additional assumptions, so we recommend working through the algebra
    on your own. Although we won’t show this for the dependent case, linearity of
    expectation also holds for dependent random variables.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Going back to the dropout example, the expectation of the total number of masked
    neurons can be broken up into a sum of expectations over each neuron. The expected
    number of masked neurons, similarly to the expected number of heads in a sequence
    of coin flips, is *p*n,* where *p* is the probability of being masked (and the
    expectation of each individual binary random variable representing a neuron) and
    *n* is the number of neurons.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, we don’t always see the expected number of occurrences of an event
    in every repetition of an experiment. In some cases, such as the expected number
    of heads in a single, fair coin flip from earlier, we never see it! Next, we will
    quantify the average deviation, or variance, from the expected value we see in
    repetitions of an experiment.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Variance
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We define the variance, or Var(*X*), as <math alttext="double-struck upper
    E left-bracket left-parenthesis upper X minus mu right-parenthesis squared right-bracket"><mrow><mi>𝔼</mi>
    <mo>[</mo> <msup><mrow><mo>(</mo><mi>X</mi><mo>-</mo><mi>μ</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>]</mo></mrow></math> , where we let <math alttext="mu equals
    double-struck upper E left-bracket upper X right-bracket"><mrow><mi>μ</mi> <mo>=</mo>
    <mi>𝔼</mi> <mo>[</mo> <mi>X</mi> <mo>]</mo></mrow></math> . In plain English,
    this measure represents the average squared difference between the value *X* takes
    on and its expectation. Note that <math alttext="left-parenthesis upper X minus
    mu right-parenthesis squared"><msup><mrow><mo>(</mo><mi>X</mi><mo>-</mo><mi>μ</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></math> itself is also a random variable since it is a function
    of a function (*X*), which is still a function. Although we won’t get into too
    much detail about why we use this formula in particular, we encourage you to think
    about why we don’t use a formula such as <math alttext="double-struck upper E
    left-bracket upper X minus mu right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo> <mi>X</mi>
    <mo>-</mo> <mi>μ</mi> <mo>]</mo></mrow></math> instead. To obtain a slightly simpler
    form for the variance, we can perform the following simplification:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义方差，或Var(*X*)，为<math alttext="double-struck upper E left-bracket left-parenthesis
    upper X minus mu right-parenthesis squared right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo>
    <msup><mrow><mo>(</mo><mi>X</mi><mo>-</mo><mi>μ</mi><mo>)</mo></mrow> <mn>2</mn></msup>
    <mo>]</mo></mrow></math>，其中我们让<math alttext="mu equals double-struck upper E left-bracket
    upper X right-bracket"><mrow><mi>μ</mi> <mo>=</mo> <mi>𝔼</mi> <mo>[</mo> <mi>X</mi>
    <mo>]</mo></mrow></math>。简单来说，这个度量表示值*X*取值与其期望之间的平均平方差。请注意，<math alttext="left-parenthesis
    upper X minus mu right-parenthesis squared"><msup><mrow><mo>(</mo><mi>X</mi><mo>-</mo><mi>μ</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></math>本身也是一个随机变量，因为它是一个函数的函数（*X*），而函数仍然是一个函数。虽然我们不会详细讨论为什么我们特别使用这个公式，但我们鼓励您思考为什么我们不使用<math
    alttext="double-struck upper E left-bracket upper X minus mu right-bracket"><mrow><mi>𝔼</mi>
    <mo>[</mo> <mi>X</mi> <mo>-</mo> <mi>μ</mi> <mo>]</mo></mrow></math>这样的公式。为了获得方差的稍微简化形式，我们可以进行以下简化：
- en: <math alttext="double-struck upper E left-bracket left-parenthesis upper X minus
    mu right-parenthesis squared right-bracket equals double-struck upper E left-bracket
    upper X squared minus 2 mu upper X plus mu squared right-bracket"><mrow><mi>𝔼</mi>
    <mrow><mo>[</mo> <msup><mrow><mo>(</mo><mi>X</mi><mo>-</mo><mi>μ</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>]</mo></mrow> <mo>=</mo> <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mi>X</mi>
    <mn>2</mn></msup> <mo>-</mo> <mn>2</mn> <mi>μ</mi> <mi>X</mi> <mo>+</mo> <msup><mi>μ</mi>
    <mn>2</mn></msup> <mo>]</mo></mrow></mrow></math>
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E left-bracket left-parenthesis upper X minus
    mu right-parenthesis squared right-bracket equals double-struck upper E left-bracket
    upper X squared minus 2 mu upper X plus mu squared right-bracket"><mrow><mi>𝔼</mi>
    <mrow><mo>[</mo> <msup><mrow><mo>(</mo><mi>X</mi><mo>-</mo><mi>μ</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>]</mo></mrow> <mo>=</mo> <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mi>X</mi>
    <mn>2</mn></msup> <mo>-</mo> <mn>2</mn> <mi>μ</mi> <mi>X</mi> <mo>+</mo> <msup><mi>μ</mi>
    <mn>2</mn></msup> <mo>]</mo></mrow></mrow></math>
- en: <math alttext="equals double-struck upper E left-bracket upper X squared right-bracket
    minus double-struck upper E left-bracket 2 mu upper X right-bracket plus double-struck
    upper E left-bracket mu squared right-bracket"><mrow><mo>=</mo> <mi>𝔼</mi> <mrow><mo>[</mo>
    <msup><mi>X</mi> <mn>2</mn></msup> <mo>]</mo></mrow> <mo>-</mo> <mi>𝔼</mi> <mrow><mo>[</mo>
    <mn>2</mn> <mi>μ</mi> <mi>X</mi> <mo>]</mo></mrow> <mo>+</mo> <mi>𝔼</mi> <mrow><mo>[</mo>
    <msup><mi>μ</mi> <mn>2</mn></msup> <mo>]</mo></mrow></mrow></math>
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals double-struck upper E left-bracket upper X squared right-bracket
    minus double-struck upper E left-bracket 2 mu upper X right-bracket plus double-struck
    upper E left-bracket mu squared right-bracket"><mrow><mo>=</mo> <mi>𝔼</mi> <mrow><mo>[</mo>
    <msup><mi>X</mi> <mn>2</mn></msup> <mo>]</mo></mrow> <mo>-</mo> <mi>𝔼</mi> <mrow><mo>[</mo>
    <mn>2</mn> <mi>μ</mi> <mi>X</mi> <mo>]</mo></mrow> <mo>+</mo> <mi>𝔼</mi> <mrow><mo>[</mo>
    <msup><mi>μ</mi> <mn>2</mn></msup> <mo>]</mo></mrow></mrow></math>
- en: <math alttext="equals double-struck upper E left-bracket upper X squared right-bracket
    minus 2 mu double-struck upper E left-bracket upper X right-bracket plus mu squared"><mrow><mo>=</mo>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mi>X</mi> <mn>2</mn></msup> <mo>]</mo></mrow>
    <mo>-</mo> <mn>2</mn> <mi>μ</mi> <mi>𝔼</mi> <mrow><mo>[</mo> <mi>X</mi> <mo>]</mo></mrow>
    <mo>+</mo> <msup><mi>μ</mi> <mn>2</mn></msup></mrow></math>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals double-struck upper E left-bracket upper X squared right-bracket
    minus 2 mu double-struck upper E left-bracket upper X right-bracket plus mu squared"><mrow><mo>=</mo>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mi>X</mi> <mn>2</mn></msup> <mo>]</mo></mrow>
    <mo>-</mo> <mn>2</mn> <mi>μ</mi> <mi>𝔼</mi> <mrow><mo>[</mo> <mi>X</mi> <mo>]</mo></mrow>
    <mo>+</mo> <msup><mi>μ</mi> <mn>2</mn></msup></mrow></math>
- en: <math alttext="equals double-struck upper E left-bracket upper X squared right-bracket
    minus 2 double-struck upper E left-bracket upper X right-bracket squared plus
    double-struck upper E left-bracket upper X right-bracket squared"><mrow><mo>=</mo>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mi>X</mi> <mn>2</mn></msup> <mo>]</mo></mrow>
    <mo>-</mo> <mn>2</mn> <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>X</mi><mo>]</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>X</mi><mo>]</mo></mrow>
    <mn>2</mn></msup></mrow></math>
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals double-struck upper E left-bracket upper X squared right-bracket
    minus 2 double-struck upper E left-bracket upper X right-bracket squared plus
    double-struck upper E left-bracket upper X right-bracket squared"><mrow><mo>=</mo>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mi>X</mi> <mn>2</mn></msup> <mo>]</mo></mrow>
    <mo>-</mo> <mn>2</mn> <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>X</mi><mo>]</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>X</mi><mo>]</mo></mrow>
    <mn>2</mn></msup></mrow></math>
- en: <math alttext="equals double-struck upper E left-bracket upper X squared right-bracket
    minus double-struck upper E left-bracket upper X right-bracket squared"><mrow><mo>=</mo>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mi>X</mi> <mn>2</mn></msup> <mo>]</mo></mrow>
    <mo>-</mo> <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>X</mi><mo>]</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals double-struck upper E left-bracket upper X squared right-bracket
    minus double-struck upper E left-bracket upper X right-bracket squared"><mrow><mo>=</mo>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mi>X</mi> <mn>2</mn></msup> <mo>]</mo></mrow>
    <mo>-</mo> <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>X</mi><mo>]</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: 'Let’s take a moment to go through each of these steps. In the first step, we
    fully express the random variable as all of its component terms via classic binomial
    expansion. In the second step, we perform linearity of expectation to break out
    the component terms into their own, individual expectations. In the third step,
    we note that <math alttext="mu"><mi>μ</mi></math> , or <math alttext="double-struck
    upper E left-bracket upper X right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo> <mi>X</mi>
    <mo>]</mo></mrow></math> , and its square are both constants and thus can be pulled
    out of the surrounding expectation. They are constants since they are not a function
    of the value *X* takes on and are instead evaluated using the entire domain (the
    set of values *X* can take on). Constants can be seen as random variables that
    can take on only one value, which is the constant itself. Thus, their expectations,
    or the average value the random variable takes on, is the constant itself since
    we always see the constant. The final steps are algebraic manipulations that bring
    us to the simplified result. Let’s use this formula to find the variance of the
    binary random variable representing a single neuron under dropout, and *p* is
    the probability of the neuron being masked out:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间逐步进行这些步骤。在第一步中，我们通过经典的二项式展开完全表达随机变量作为其所有组成项。在第二步中，我们执行期望的线性性，将组成项分解为它们自己的单独期望。在第三步中，我们注意到<math
    alttext="mu"><mi>μ</mi></math>，或者<math alttext="double-struck upper E left-bracket
    upper X right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo> <mi>X</mi> <mo>]</mo></mrow></math>，及其平方都是常数，因此可以从周围的期望中提取出来。它们是常数，因为它们不是值*X*的函数，而是使用整个域（*X*可以取的值集合）进行评估。常数可以看作是只能取一个值的随机变量，即常数本身。因此，它们的期望值，或者随机变量取值的平均值，就是常数本身，因为我们总是看到这个常数。最后的步骤是代数操作，将我们带到简化的结果。让我们使用这个公式来找到表示单个神经元在辍学下的二进制随机变量的方差，*p*是神经元被屏蔽的概率：
- en: <math alttext="double-struck upper E left-bracket upper X squared right-bracket
    minus double-struck upper E left-bracket upper X right-bracket squared equals
    sigma-summation Underscript x element-of 0 comma 1 Endscripts x squared asterisk
    upper P left-parenthesis upper X equals x right-parenthesis minus left-parenthesis
    sigma-summation Underscript x element-of 0 comma 1 Endscripts x asterisk upper
    P left-parenthesis upper X equals x right-parenthesis right-parenthesis squared"><mrow><mi>𝔼</mi>
    <mrow><mo>[</mo> <msup><mi>X</mi> <mn>2</mn></msup> <mo>]</mo></mrow> <mo>-</mo>
    <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>X</mi><mo>]</mo></mrow> <mn>2</mn></msup>
    <mo>=</mo> <msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow></mrow></msub>
    <msup><mi>x</mi> <mn>2</mn></msup> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>X</mi>
    <mo>=</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>-</mo> <msup><mrow><mo>(</mo><msub><mo>∑</mo>
    <mrow><mi>x</mi><mo>∈</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow></mrow></msub>
    <mi>x</mi><mo>*</mo><mi>P</mi><mrow><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E left-bracket upper X squared right-bracket
    minus double-struck upper E left-bracket upper X right-bracket squared equals
    sigma-summation Underscript x element-of 0 comma 1 Endscripts x squared asterisk
    upper P left-parenthesis upper X equals x right-parenthesis minus left-parenthesis
    sigma-summation Underscript x element-of 0 comma 1 Endscripts x asterisk upper
    P left-parenthesis upper X equals x right-parenthesis right-parenthesis squared"><mrow><mi>𝔼</mi>
    <mrow><mo>[</mo> <msup><mi>X</mi> <mn>2</mn></msup> <mo>]</mo></mrow> <mo>-</mo>
    <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>X</mi><mo>]</mo></mrow> <mn>2</mn></msup>
    <mo>=</mo> <msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow></mrow></msub>
    <msup><mi>x</mi> <mn>2</mn></msup> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>X</mi>
    <mo>=</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>-</mo> <msup><mrow><mo>(</mo><msub><mo>∑</mo>
    <mrow><mi>x</mi><mo>∈</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow></mrow></msub>
    <mi>x</mi><mo>*</mo><mi>P</mi><mrow><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math>
- en: <math alttext="equals sigma-summation Underscript x element-of 0 comma 1 Endscripts
    x squared asterisk upper P left-parenthesis upper X equals x right-parenthesis
    minus p squared"><mrow><mo>=</mo> <msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow></mrow></msub>
    <msup><mi>x</mi> <mn>2</mn></msup> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>X</mi>
    <mo>=</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>-</mo> <msup><mi>p</mi> <mn>2</mn></msup></mrow></math>
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals sigma-summation Underscript x element-of 0 comma 1 Endscripts
    x squared asterisk upper P left-parenthesis upper X equals x right-parenthesis
    minus p squared"><mrow><mo>=</mo> <msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow></mrow></msub>
    <msup><mi>x</mi> <mn>2</mn></msup> <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>X</mi>
    <mo>=</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>-</mo> <msup><mi>p</mi> <mn>2</mn></msup></mrow></math>
- en: <math alttext="equals p minus p squared"><mrow><mo>=</mo> <mi>p</mi> <mo>-</mo>
    <msup><mi>p</mi> <mn>2</mn></msup></mrow></math>
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals p minus p squared"><mrow><mo>=</mo> <mi>p</mi> <mo>-</mo>
    <msup><mi>p</mi> <mn>2</mn></msup></mrow></math>
- en: <math alttext="equals p left-parenthesis 1 minus p right-parenthesis"><mrow><mo>=</mo>
    <mi>p</mi> <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow></math>
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals p left-parenthesis 1 minus p right-parenthesis"><mrow><mo>=</mo>
    <mi>p</mi> <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow></math>
- en: 'These simplifications should make sense. We know from [“Expectation”](#expectation_sect1)
    that the expectation of the binary random variable representing a neuron is just
    *p,* and the rest is algebraic simplifications. We highly encourage you to work
    through these derivations on your own. As we start to think about the random variable
    representing the number of masked neurons in the entire layer, we naturally ask
    the question of whether there exists a similar linearity property for variance
    as there does for expectation. Unfortunately, the property does not hold in general:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这些简化应该是合理的。我们从[“期望”](#expectation_sect1)中知道，表示神经元的二进制随机变量的期望值只是*p*，其余是代数简化。我们强烈鼓励您自己进行这些推导。当我们开始思考代表整个层中被屏蔽神经元数量的随机变量时，我们自然会问是否存在与期望相似的方差线性性质。不幸的是，该性质通常不成立：
- en: <math alttext="upper V a r left-parenthesis upper A plus upper B right-parenthesis
    equals double-struck upper E left-bracket left-parenthesis upper A plus upper
    B right-parenthesis squared right-bracket minus double-struck upper E left-bracket
    upper A plus upper B right-bracket squared"><mrow><mi>V</mi> <mi>a</mi> <mi>r</mi>
    <mrow><mo>(</mo> <mi>A</mi> <mo>+</mo> <mi>B</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mrow><mo>(</mo><mi>A</mi><mo>+</mo><mi>B</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>]</mo></mrow> <mo>-</mo> <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>A</mi><mo>+</mo><mi>B</mi><mo>]</mo></mrow>
    <mn>2</mn></msup></mrow></math>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper V a r left-parenthesis upper A plus upper B right-parenthesis
    equals double-struck upper E left-bracket left-parenthesis upper A plus upper
    B right-parenthesis squared right-bracket minus double-struck upper E left-bracket
    upper A plus upper B right-bracket squared"><mrow><mi>V</mi> <mi>a</mi> <mi>r</mi>
    <mrow><mo>(</mo> <mi>A</mi> <mo>+</mo> <mi>B</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mrow><mo>(</mo><mi>A</mi><mo>+</mo><mi>B</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>]</mo></mrow> <mo>-</mo> <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>A</mi><mo>+</mo><mi>B</mi><mo>]</mo></mrow>
    <mn>2</mn></msup></mrow></math>
- en: <math alttext="equals double-struck upper E left-bracket upper A squared plus
    2 asterisk upper A asterisk upper B plus upper B squared right-bracket minus left-parenthesis
    double-struck upper E left-bracket upper A right-bracket plus double-struck upper
    E left-bracket upper B right-bracket right-parenthesis squared"><mrow><mo>=</mo>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mi>A</mi> <mn>2</mn></msup> <mo>+</mo> <mn>2</mn>
    <mo>*</mo> <mi>A</mi> <mo>*</mo> <mi>B</mi> <mo>+</mo> <msup><mi>B</mi> <mn>2</mn></msup>
    <mo>]</mo></mrow> <mo>-</mo> <msup><mrow><mo>(</mo><mi>𝔼</mi><mrow><mo>[</mo><mi>A</mi><mo>]</mo></mrow><mo>+</mo><mi>𝔼</mi><mrow><mo>[</mo><mi>B</mi><mo>]</mo></mrow><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals double-struck upper E left-bracket upper A squared plus
    2 asterisk upper A asterisk upper B plus upper B squared right-bracket minus left-parenthesis
    double-struck upper E left-bracket upper A right-bracket plus double-struck upper
    E left-bracket upper B right-bracket right-parenthesis squared"><mrow><mo>=</mo>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mi>A</mi> <mn>2</mn></msup> <mo>+</mo> <mn>2</mn>
    <mo>*</mo> <mi>A</mi> <mo>*</mo> <mi>B</mi> <mo>+</mo> <msup><mi>B</mi> <mn>2</mn></msup>
    <mo>]</mo></mrow> <mo>-</mo> <msup><mrow><mo>(</mo><mi>𝔼</mi><mrow><mo>[</mo><mi>A</mi><mo>]</mo></mrow><mo>+</mo><mi>𝔼</mi><mrow><mo>[</mo><mi>B</mi><mo>]</mo></mrow><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math>
- en: <math alttext="equals double-struck upper E left-bracket upper A squared right-bracket
    plus 2 double-struck upper E left-bracket upper A asterisk upper B right-bracket
    plus double-struck upper E left-bracket upper B squared right-bracket minus double-struck
    upper E left-bracket upper A right-bracket squared minus 2 double-struck upper
    E left-bracket upper A right-bracket double-struck upper E left-bracket upper
    B right-bracket minus double-struck upper E left-bracket upper B right-bracket
    squared"><mrow><mo>=</mo> <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mi>A</mi> <mn>2</mn></msup>
    <mo>]</mo></mrow> <mo>+</mo> <mn>2</mn> <mi>𝔼</mi> <mrow><mo>[</mo> <mi>A</mi>
    <mo>*</mo> <mi>B</mi> <mo>]</mo></mrow> <mo>+</mo> <mi>𝔼</mi> <mrow><mo>[</mo>
    <msup><mi>B</mi> <mn>2</mn></msup> <mo>]</mo></mrow> <mo>-</mo> <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>A</mi><mo>]</mo></mrow>
    <mn>2</mn></msup> <mo>-</mo> <mn>2</mn> <mi>𝔼</mi> <mrow><mo>[</mo> <mi>A</mi>
    <mo>]</mo></mrow> <mi>𝔼</mi> <mrow><mo>[</mo> <mi>B</mi> <mo>]</mo></mrow> <mo>-</mo>
    <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>B</mi><mo>]</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals double-struck upper E left-bracket upper A squared right-bracket
    plus 2 double-struck upper E left-bracket upper A asterisk upper B right-bracket
    plus double-struck upper E left-bracket upper B squared right-bracket minus double-struck
    upper E left-bracket upper A right-bracket squared minus 2 double-struck upper
    E left-bracket upper A right-bracket double-struck upper E left-bracket upper
    B right-bracket minus double-struck upper E left-bracket upper B right-bracket
    squared"><mrow><mo>=</mo> <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mi>A</mi> <mn>2</mn></msup>
    <mo>]</mo></mrow> <mo>+</mo> <mn>2</mn> <mi>𝔼</mi> <mrow><mo>[</mo> <mi>A</mi>
    <mo>*</mo> <mi>B</mi> <mo>]</mo></mrow> <mo>+</mo> <mi>𝔼</mi> <mrow><mo>[</mo>
    <msup><mi>B</mi> <mn>2</mn></msup> <mo>]</mo></mrow> <mo>-</mo> <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>A</mi><mo>]</mo></mrow>
    <mn>2</mn></msup> <mo>-</mo> <mn>2</mn> <mi>𝔼</mi> <mrow><mo>[</mo> <mi>A</mi>
    <mo>]</mo></mrow> <mi>𝔼</mi> <mrow><mo>[</mo> <mi>B</mi> <mo>]</mo></mrow> <mo>-</mo>
    <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>B</mi><mo>]</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: <math alttext="equals double-struck upper E left-bracket upper A squared right-bracket
    minus double-struck upper E left-bracket upper A right-bracket squared plus double-struck
    upper E left-bracket upper B squared right-bracket minus double-struck upper E
    left-bracket upper B right-bracket squared plus 2 double-struck upper E left-bracket
    upper A asterisk upper B right-bracket minus 2 double-struck upper E left-bracket
    upper A right-bracket double-struck upper E left-bracket upper B right-bracket"><mrow><mo>=</mo>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mi>A</mi> <mn>2</mn></msup> <mo>]</mo></mrow>
    <mo>-</mo> <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>A</mi><mo>]</mo></mrow> <mn>2</mn></msup>
    <mo>+</mo> <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mi>B</mi> <mn>2</mn></msup> <mo>]</mo></mrow>
    <mo>-</mo> <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>B</mi><mo>]</mo></mrow> <mn>2</mn></msup>
    <mo>+</mo> <mn>2</mn> <mi>𝔼</mi> <mrow><mo>[</mo> <mi>A</mi> <mo>*</mo> <mi>B</mi>
    <mo>]</mo></mrow> <mo>-</mo> <mn>2</mn> <mi>𝔼</mi> <mrow><mo>[</mo> <mi>A</mi>
    <mo>]</mo></mrow> <mi>𝔼</mi> <mrow><mo>[</mo> <mi>B</mi> <mo>]</mo></mrow></mrow></math>
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals double-struck upper E left-bracket upper A squared right-bracket
    minus double-struck upper E left-bracket upper A right-bracket squared plus double-struck
    upper E left-bracket upper B squared right-bracket minus double-struck upper E
    left-bracket upper B right-bracket squared plus 2 double-struck upper E left-bracket
    upper A asterisk upper B right-bracket minus 2 double-struck upper E left-bracket
    upper A right-bracket double-struck upper E left-bracket upper B right-bracket"><mrow><mo>=</mo>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mi>A</mi> <mn>2</mn></msup> <mo>]</mo></mrow>
    <mo>-</mo> <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>A</mi><mo>]</mo></mrow> <mn>2</mn></msup>
    <mo>+</mo> <mi>𝔼</mi> <mrow><mo>[</mo> <msup><mi>B</mi> <mn>2</mn></msup> <mo>]</mo></mrow>
    <mo>-</mo> <mi>𝔼</mi> <msup><mrow><mo>[</mo><mi>B</mi><mo>]</mo></mrow> <mn>2</mn></msup>
    <mo>+</mo> <mn>2</mn> <mi>𝔼</mi> <mrow><mo>[</mo> <mi>A</mi> <mo>*</mo> <mi>B</mi>
    <mo>]</mo></mrow> <mo>-</mo> <mn>2</mn> <mi>𝔼</mi> <mrow><mo>[</mo> <mi>A</mi>
    <mo>]</mo></mrow> <mi>𝔼</mi> <mrow><mo>[</mo> <mi>B</mi> <mo>]</mo></mrow></mrow></math>
- en: <math alttext="equals upper V a r left-parenthesis upper A right-parenthesis
    plus upper V a r left-parenthesis upper B right-parenthesis plus 2 left-parenthesis
    double-struck upper E left-bracket upper A asterisk upper B right-bracket minus
    double-struck upper E left-bracket upper A right-bracket double-struck upper E
    left-bracket upper B right-bracket right-parenthesis"><mrow><mo>=</mo> <mi>V</mi>
    <mi>a</mi> <mi>r</mi> <mo>(</mo> <mi>A</mi> <mo>)</mo> <mo>+</mo> <mi>V</mi> <mi>a</mi>
    <mi>r</mi> <mo>(</mo> <mi>B</mi> <mo>)</mo> <mo>+</mo> <mn>2</mn> <mo>(</mo> <mi>𝔼</mi>
    <mo>[</mo> <mi>A</mi> <mo>*</mo> <mi>B</mi> <mo>]</mo> <mo>-</mo> <mi>𝔼</mi> <mo>[</mo>
    <mi>A</mi> <mo>]</mo> <mi>𝔼</mi> <mo>[</mo> <mi>B</mi> <mo>]</mo> <mo>)</mo></mrow></math>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals upper V a r left-parenthesis upper A right-parenthesis
    plus upper V a r left-parenthesis upper B right-parenthesis plus 2 left-parenthesis
    double-struck upper E left-bracket upper A asterisk upper B right-bracket minus
    double-struck upper E left-bracket upper A right-bracket double-struck upper E
    left-bracket upper B right-bracket right-parenthesis"><mrow><mo>=</mo> <mi>V</mi>
    <mi>a</mi> <mi>r</mi> <mo>(</mo> <mi>A</mi> <mo>)</mo> <mo>+</mo> <mi>V</mi> <mi>a</mi>
    <mi>r</mi> <mo>(</mo> <mi>B</mi> <mo>)</mo> <mo>+</mo> <mn>2</mn> <mo>(</mo> <mi>𝔼</mi>
    <mo>[</mo> <mi>A</mi> <mo>*</mo> <mi>B</mi> <mo>]</mo> <mo>-</mo> <mi>𝔼</mi> <mo>[</mo>
    <mi>A</mi> <mo>]</mo> <mi>𝔼</mi> <mo>[</mo> <mi>B</mi> <mo>]</mo> <mo>)</mo></mrow></math>
- en: <math alttext="equals upper V a r left-parenthesis upper A right-parenthesis
    plus upper V a r left-parenthesis upper B right-parenthesis plus 2 upper C o v
    left-parenthesis upper A comma upper B right-parenthesis"><mrow><mo>=</mo> <mi>V</mi>
    <mi>a</mi> <mi>r</mi> <mo>(</mo> <mi>A</mi> <mo>)</mo> <mo>+</mo> <mi>V</mi> <mi>a</mi>
    <mi>r</mi> <mo>(</mo> <mi>B</mi> <mo>)</mo> <mo>+</mo> <mn>2</mn> <mi>C</mi> <mi>o</mi>
    <mi>v</mi> <mo>(</mo> <mi>A</mi> <mo>,</mo> <mi>B</mi> <mo>)</mo></mrow></math>
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals upper V a r left-parenthesis upper A right-parenthesis
    plus upper V a r left-parenthesis upper B right-parenthesis plus 2 upper C o v
    left-parenthesis upper A comma upper B right-parenthesis"><mrow><mo>=</mo> <mi>V</mi>
    <mi>a</mi> <mi>r</mi> <mo>(</mo> <mi>A</mi> <mo>)</mo> <mo>+</mo> <mi>V</mi> <mi>a</mi>
    <mi>r</mi> <mo>(</mo> <mi>B</mi> <mo>)</mo> <mo>+</mo> <mn>2</mn> <mi>C</mi> <mi>o</mi>
    <mi>v</mi> <mo>(</mo> <mi>A</mi> <mo>,</mo> <mi>B</mi> <mo>)</mo></mrow></math>
- en: As we can see from the last line, the final term in the expression, which we
    call the *covariance* between the two random variables, ruins our hope for linearity.
    However, covariance is another key concept in probability—the intuition for covariance
    is that it measures the dependence between two random variables. As one random
    variable more completely determines the value of another random variable (think
    of *A* as the number of heads in a sequence of coin flips and *B* as the number
    of tails in the same sequence of coin flips), the magnitude of the covariance
    increases. Thus, it stands to reason that if *A* and *B* are independent random
    variables, the covariance between them should be zero, and linearity should hold
    in this special case. We highly encourage you to work through the math and show
    this on your own.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从最后一行可以看到的那样，表达式中的最后一项，我们称之为两个随机变量之间的*协方差*，破坏了我们对线性的希望。然而，协方差是概率中的另一个关键概念——协方差的直觉是它衡量了两个随机变量之间的依赖关系。当一个随机变量更完全地确定另一个随机变量的值（想象*A*是一系列抛硬币的正面数量，*B*是同一系列抛硬币的反面数量），协方差的大小会增加。因此，可以推断，如果*A*和*B*是独立的随机变量，它们之间的协方差应该为零，在这种特殊情况下线性应该成立。我们强烈鼓励您通过数学来证明这一点。
- en: Back to the dropout example, the variance of the total number of masked neurons
    can be broken up into a sum of variances over each neuron, since each neuron is
    masked independently. The variance of the number of masked neurons is *p(1 – p)*n,*
    where *p(1 – p)* is the variance for any given neuron and *n* is the number of
    neurons. Expectation and variance in dropout allow us to understand more deeply
    what we expect to see when applying such a layer in a deep neural network.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 回到辍学的例子，被屏蔽神经元的总数的方差可以分解为每个神经元的方差之和，因为每个神经元都是独立屏蔽的。被屏蔽神经元的数量的方差是*p(1-p)*n*，其中*p(1-p)*是任何给定神经元的方差，*n*是神经元的数量。辍学中的期望和方差使我们能够更深入地理解在深度神经网络中应用这样一个层时我们期望看到什么。
- en: Bayes’ Theorem
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯定理
- en: Returning to our discussion on conditional probability, we noted that the probability
    of intersection between two events could be written as a product of a conditional
    distribution and a distribution over a single event. Let’s translate this into
    the language of random variables, now that we have introduced this new terminology.
    We denote *A* to be one random variable, and *B* to denote a second. Let *a* be
    a value that *A* can take on, and *b* be a value that *B* can take on. The analogy
    to the intersection operation for random variables is the *joint probability distribution
    P(A=a,B=b),* which denotes the event where *A = a* and *B = b.* We can think of
    *A = a* and *B = b* as individual events, and when we write *P(A = a,B = b)*,
    we are considering the probability that both events have occurred, i.e., their
    intersection <math alttext="upper P left-parenthesis upper A equals a intersection
    upper B equals b right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>A</mi> <mo>=</mo>
    <mi>a</mi> <mo>∩</mo> <mi>B</mi> <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow></math>
    . Note that we generally write the joint probability distribution as *P(A,B),*
    since this encompasses all possible joint settings of the random variables *A*
    and *B.*
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们关于条件概率的讨论，我们注意到两个事件之间的交集的概率可以写成条件分布和单个事件的分布的乘积。现在让我们将这个翻译成随机变量的语言，现在我们已经介绍了这个新术语。我们将*A*表示为一个随机变量，*B*表示第二个随机变量。让*a*是*A*可以取的值，*b*是*B*可以取的值。对于随机变量的交集操作的类比是*联合概率分布P(A=a,B=b)*，表示*A=a*和*B=b*的事件。我们可以将*A=a*和*B=b*看作是单独的事件，当我们写*P(A=a,B=b)*时，我们考虑的是两个事件都发生的概率，即它们的交集。请注意，我们通常将联合概率分布写为*P(A,B)*，因为这包含了随机变量*A*和*B*的所有可能的联合设置。
- en: 'We mentioned earlier that intersection operations could be written as the product
    of a conditional distribution and a distribution over a single event. Rewriting
    this in the format for random variables, we have *P(A = a,B = b) = P(A = a|B =
    b)P(B = b)*. And more generally, considering all possible joint settings of the
    two random variables, we have *P(A,B) = P(A|B)P(B).* We also discussed how there
    always exists a second way of writing this joint distribution as a product: *P(A
    = a,B = b) = P(B = b|A = a)P(A=a),* and more generally*, P(A,B) = P(B|A)P(A).*
    We noted that sometimes one of these paths makes more sense than the other. For
    example, in the case where symptoms are represented by *A* and disease is represented
    by *B,* the path in which *B* takes on a value *b*, and then *A* takes on a value
    *a* in that universe makes much more sense than the reverse since, biologically,
    people contract a disease first and only then show symptoms for that disease.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，交集操作可以写成条件分布和单个事件的分布的乘积。将这个重写为随机变量的格式，我们有*P(A=a,B=b) = P(A=a|B=b)P(B=b)*。更一般地，考虑两个随机变量的所有可能的联合设置，我们有*P(A,B)
    = P(A|B)P(B)*。我们还讨论了总是存在第二种写这个联合分布的方法：*P(A=a,B=b) = P(B=b|A=a)P(A=a)*，更一般地，*P(A,B)
    = P(B|A)P(A)*。我们注意到有时其中一种路径比另一种更有意义。例如，在症状由*A*表示，疾病由*B*表示的情况下，*B*取一个值*b*，然后*A*在那个宇宙中取一个值*a*的路径比反向更有意义，因为从生物学上讲，人们先感染疾病，然后才表现出该疾病的症状。
- en: 'However, this doesn’t mean that the reverse isn’t useful. It is almost universally
    the case that people show up at a hospital with mild symptoms, and medical professionals
    must try to infer the most likely disease from these symptoms to effectively treat
    the underlying disease. *Bayes’ Theorem* gives us a way of calculating the probability
    of a disease given the observed symptoms. Since the same joint probability distribution
    can be written in the two ways mentioned in the previous paragraph, we have the
    following equality:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P left-parenthesis upper B vertical-bar upper A right-parenthesis
    equals StartFraction upper P left-parenthesis upper A vertical-bar upper B right-parenthesis
    upper P left-parenthesis upper B right-parenthesis Over upper P left-parenthesis
    upper A right-parenthesis EndFraction"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>B</mi>
    <mo>|</mo> <mi>A</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mi>A</mi><mo>|</mo><mi>B</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>B</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis upper B vertical-bar upper A right-parenthesis
    equals StartFraction upper P left-parenthesis upper A vertical-bar upper B right-parenthesis
    upper P left-parenthesis upper B right-parenthesis Over upper P left-parenthesis
    upper A right-parenthesis EndFraction"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>B</mi>
    <mo>|</mo> <mi>A</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mi>A</mi><mo>|</mo><mi>B</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>B</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></mfrac></mrow></math>
- en: 'If *B* represents disease, while *A* represents symptoms, this gives us a method
    for computing the likelihood of any disease given the observed symptoms. Let’s
    analyze the right side to see if the equality also makes intuitive sense. The
    likelihood of symptoms given the disease times the likelihood of the disease is
    just the joint distribution, which makes sense as the numerator here. The denominator
    is the likelihood of seeing those symptoms, which can also be expressed as a sum
    of the numerator over all possible diseases. This is an instance of a more general
    process called *marginalization,* or removing a subset of random variables from
    a joint distribution by summing over all possible configurations of the subset:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P left-parenthesis upper A right-parenthesis equals sigma-summation
    Underscript b Endscripts upper P left-parenthesis upper A comma upper B equals
    b right-parenthesis"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mo>∑</mo> <mi>b</mi></msub> <mi>P</mi> <mrow><mo>(</mo> <mi>A</mi>
    <mo>,</mo> <mi>B</mi> <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis upper A right-parenthesis equals sigma-summation
    Underscript b Endscripts upper P left-parenthesis upper A comma upper B equals
    b right-parenthesis"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mo>∑</mo> <mi>b</mi></msub> <mi>P</mi> <mrow><mo>(</mo> <mi>A</mi>
    <mo>,</mo> <mi>B</mi> <mo>=</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math>
- en: 'In more concise terms, we have:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P left-parenthesis upper B equals b Subscript q u e r y
    Baseline vertical-bar upper A right-parenthesis equals StartFraction upper P left-parenthesis
    upper B equals b Subscript q u e r y Baseline comma upper A right-parenthesis
    Over sigma-summation Underscript b Endscripts upper P left-parenthesis upper B
    equals b comma upper A right-parenthesis EndFraction"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <mi>B</mi> <mo>=</mo> <msub><mi>b</mi> <mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub>
    <mo>|</mo> <mi>A</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>=</mo><msub><mi>b</mi>
    <mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub> <mo>,</mo><mi>A</mi><mo>)</mo></mrow>
    <mrow><msub><mo>∑</mo> <mi>b</mi></msub> <mi>P</mi><mrow><mo>(</mo><mi>B</mi><mo>=</mo><mi>b</mi><mo>,</mo><mi>A</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis upper B equals b Subscript q u e r y
    Baseline vertical-bar upper A right-parenthesis equals StartFraction upper P left-parenthesis
    upper B equals b Subscript q u e r y Baseline comma upper A right-parenthesis
    Over sigma-summation Underscript b Endscripts upper P left-parenthesis upper B
    equals b comma upper A right-parenthesis EndFraction"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <mi>B</mi> <mo>=</mo> <msub><mi>b</mi> <mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub>
    <mo>|</mo> <mi>A</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>=</mo><msub><mi>b</mi>
    <mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub> <mo>,</mo><mi>A</mi><mo>)</mo></mrow>
    <mrow><msub><mo>∑</mo> <mi>b</mi></msub> <mi>P</mi><mrow><mo>(</mo><mi>B</mi><mo>=</mo><mi>b</mi><mo>,</mo><mi>A</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: Bayes’ Theorem is a very valuable application of probability in the real world,
    especially in the case of disease prediction. Additionally, if we replace the
    random variable for symptoms with a random variable representing the result of
    a test for a specific disease, and the random variable over all diseases with
    a random variable over presence of the specific disease, we can infer the likelihood
    of actually having a specific disease given a positive test for it using Bayes’
    Theorem. This is a common problem in most hospitals, and is especially relevant
    to epidemiology given the outbreak of COVID-19.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Entropy, Cross Entropy, and KL Divergence
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Probability distributions, by definition, give us a way of comparing the likelihoods
    of various possible events. However, even if we know the most likely event (or
    events) that is to occur, when running the experiment we are bound to see all
    sorts of events. In this section, we first consider the problem of defining a
    single metric that encapsulates all of the uncertainty within a probability distribution,
    which we will define as the distribution’s *entropy*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Let’s set up the following scenario. I am a researcher who is running an experiment.
    The experiment could be something as simple as flipping a coin or rolling a dice.
    You are recording the results of the experiment. We are both in different rooms,
    but connected through a phone line. I run the experiment and receive a result,
    and communicate that result to you via the phone. You record that result in a
    notebook, where you pick some binary string representation of that result as what
    you write down. As a scribe, you are necessary in this situation—I may run hundreds
    of trials and my memory is limited, so I cannot remember the results of all of
    my trials.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: For example, if I roll a dice and neither of us knows anything about the fairness
    of the dice, you could denote the outcome one as “0,” two as “1,” three as “10,”
    four as “11,” five as “100,” and six as “101.” Whenever I communicate a result
    of the experiment to you, you add that result’s corresponding string representation
    to the end of the string consisting of all results so far. If I were to roll a
    one, followed by two twos, and finally a one, using the encoding scheme defined
    so far you would have written down “0110.”
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: After all runs of the experiment have ended, I have a meeting with you and try
    to decipher this string “0110” into a sequence of outcomes for use in my research.
    However, as the researcher, I am puzzled by this string—does it represent a one,
    followed by two twos, and finally a one? Or does it represent a one, followed
    by a two, followed by a three? Or even a one, followed by a four, followed by
    a one? It seems that there are at least a few possible translations of this string
    into outcomes using the encoding scheme.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有实验运行结束后，我与您开会，尝试将这个字符串“0110”解密为一系列结果，以供我的研究使用。然而，作为研究人员，我对这个字符串感到困惑——它代表一个一，接着两个二，最后一个一吗？还是代表一个一，接着一个二，再接着一个三？甚至是一个一，接着一个四，再接着一个一？看起来至少有几种可能的翻译方式可以使用编码方案将这个字符串转换为结果。
- en: To prevent this situation from ever occurring again, we decide to enforce some
    limitations on the binary strings you can use to represent outcomes. We use what
    is called a *prefix code*, which disallows binary string representations of different
    outcomes from being prefixes of each other. It’s not too difficult to see why
    this would result in a unique translation of string to outcomes. Let’s say we
    have a binary string, some prefix of which we have been able to successfully decode
    into a series of outcomes. To decode the rest of the string, or the suffix, we
    must first find the next outcome in the series. When we find a prefix of this
    suffix that translates to an outcome, we already know that, by definition, there
    is no smaller prefix that translates to a valid outcome. We now have a larger
    prefix of the binary string that has been successfully translated to a series
    of outcomes. We then recursively use this logic until we have reached the end
    of the string.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止这种情况再次发生，我们决定对您用于表示结果的二进制字符串施加一些限制。我们使用所谓的*前缀编码*，它不允许不同结果的二进制字符串表示成为彼此的前缀。不难理解为什么这会导致字符串到结果的唯一翻译。假设我们有一个二进制字符串，其中的某个前缀我们已成功解码为一系列结果。要解码剩余的字符串，或者后缀，我们必须首先找到系列中的下一个结果。当我们找到这个后缀的前缀被翻译为一个结果时，我们已经知道，根据定义，没有更小的前缀可以翻译为有效的结果。现在我们有一个更大的前缀二进制字符串已成功翻译为一系列结果。然后我们递归使用这种逻辑，直到达到字符串的末尾。
- en: Now that we have some guidelines on string representations for outcomes, we
    redo the original experiment with one as “0,” two as “10,” three as “110,” four
    as “1110,” five as “11110,” and six as “111110.” However, as noted earlier, I
    may carry out hundreds of trials, and as the scribe you probably want to limit
    the amount of writing you have to do. With no information about the dice, we can’t
    do too much better than this. Assuming each outcome shows up with probability
    <math alttext="one-sixth"><mfrac><mn>1</mn> <mn>6</mn></mfrac></math> , the expected
    number of letters you’d need to write down per trial is 3.5\. We could get down
    to 3 if we set one as “000,” two as “001,” three as “010,” four as “011,” five
    as “100,” and six as “101,” for example.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一些关于结果的字符串表示的指导方针，我们使用“0”表示一，使用“10”表示二，使用“110”表示三，使用“1110”表示四，使用“11110”表示五，使用“111110”表示六重新进行原始实验。然而，正如前面提到的，我可能进行数百次试验，作为抄写员，您可能希望限制您需要写入的数量。在没有关于骰子的信息的情况下，我们无法做得比这更好。假设每个结果出现的概率为1/6，您每次试验需要写下的预期字母数量为3.5。例如，如果我们将一设置为“000”，将二设置为“001”，将三设置为“010”，将四设置为“011”，将五设置为“100”，将六设置为“101”，我们可以降至3。
- en: But what if we knew information about the dice? For example, what if it were
    a weighted dice that showed up six almost all of the time? In that case, you probably
    want to assign a shorter binary string to six, for example “0” (instead of assigning
    “0” to one) so you can limit the expected amount of writing you have to do. It
    makes intuitive sense that, as the result of any single trial becomes more and
    more certain, the expected number of characters you’d need to write becomes lower
    by assigning the shortest binary strings to the most likely outcomes.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们知道有关骰子的信息呢？例如，如果它是一个加权骰子，几乎总是出现六点？在这种情况下，您可能希望为六分配一个更短的二进制字符串，例如“0”（而不是将“0”分配给一），这样您就可以限制您需要写入的预期数量。直观地讲，随着任何单次试验的结果变得越来越确定，通过将最可能的结果分配给最短的二进制字符串，您需要写入的预期字符数量就会降低。
- en: 'This raises the question: given a probability distribution over outcomes, what
    is the optimal encoding scheme, where optimal is defined as the fewest expected
    number of characters you’d need to write per trial? Although this whole situation
    may feel a bit contrived, it provides us with a slightly different lens through
    which we can understand the uncertainty within a probability distribution. As
    we noted, as the result of an experiment becomes more and more certain, the optimal
    encoding scheme would allow the scribe to write fewer and fewer characters in
    expectation per trial. For example, in the extreme case where we already knew
    beforehand that a six would always show up, the scribe wouldn’t need to write
    anything down.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这引发了一个问题：在结果上给定一个概率分布，什么是最佳的编码方案，其中最佳被定义为每次试验需要写入的最少预期字符数量？尽管整个情况可能有点刻意，但它为我们提供了一个稍微不同的视角，通过它我们可以理解概率分布中的不确定性。正如我们所指出的，随着实验结果变得越来越确定，最佳的编码方案将允许抄写员每次试验的预期字符数量变得越来越少。例如，在极端情况下，如果我们事先知道六点总是会出现，抄写员就不需要写任何东西。
- en: 'It turns out that, although we won’t show it here, the best you can do is assign
    a binary string of length <math alttext="log Subscript 2 Baseline StartFraction
    1 Over p left-parenthesis x Subscript i Baseline right-parenthesis EndFraction"><mrow><msub><mo
    form="prefix">log</mo> <mn>2</mn></msub> <mfrac><mn>1</mn> <mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mfrac></mrow></math> to each possible outcome
    <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math> , where
    <math alttext="p left-parenthesis x Subscript i Baseline right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow></math> is its
    probability. The expected string length of any given trial would then be:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，尽管我们在这里不会展示，但你可以做的最好的事情是为每个可能结果*x_i*分配一个长度为*log_2(1/p(x_i))*的二进制字符串，其中*p(x_i)*是其概率。然后，任何给定试验的预期字符串长度将是：
- en: <math alttext="double-struck upper E Subscript p left-parenthesis x right-parenthesis
    Baseline left-bracket log Subscript 2 Baseline StartFraction 1 Over p left-parenthesis
    x right-parenthesis EndFraction right-bracket equals sigma-summation Underscript
    x Subscript i Baseline Endscripts p left-parenthesis x Subscript i Baseline right-parenthesis
    log Subscript 2 Baseline StartFraction 1 Over p left-parenthesis x Subscript i
    Baseline right-parenthesis EndFraction"><mrow><msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <msub><mo form="prefix">log</mo> <mn>2</mn></msub> <mfrac><mn>1</mn>
    <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>]</mo></mrow>
    <mo>=</mo> <msub><mo>∑</mo> <msub><mi>x</mi> <mi>i</mi></msub></msub> <mi>p</mi>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <msub><mo
    form="prefix">log</mo> <mn>2</mn></msub> <mfrac><mn>1</mn> <mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E Subscript p left-parenthesis x right-parenthesis
    Baseline left-bracket log Subscript 2 Baseline StartFraction 1 Over p left-parenthesis
    x right-parenthesis EndFraction right-bracket equals sigma-summation Underscript
    x Subscript i Baseline Endscripts p left-parenthesis x Subscript i Baseline right-parenthesis
    log Subscript 2 Baseline StartFraction 1 Over p left-parenthesis x Subscript i
    Baseline right-parenthesis EndFraction"><mrow><msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <msub><mo form="prefix">log</mo> <mn>2</mn></msub> <mfrac><mn>1</mn>
    <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>]</mo></mrow>
    <mo>=</mo> <msub><mo>∑</mo> <msub><mi>x</mi> <mi>i</mi></msub></msub> <mi>p</mi>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <msub><mo
    form="prefix">log</mo> <mn>2</mn></msub> <mfrac><mn>1</mn> <mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mfrac></mrow></math>
- en: <math alttext="equals minus sigma-summation Underscript x Subscript i Baseline
    Endscripts p left-parenthesis x Subscript i Baseline right-parenthesis log Subscript
    2 Baseline p left-parenthesis x Subscript i Baseline right-parenthesis"><mrow><mo>=</mo>
    <mo>-</mo> <msub><mo>∑</mo> <msub><mi>x</mi> <mi>i</mi></msub></msub> <mi>p</mi>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <msub><mo
    form="prefix">log</mo> <mn>2</mn></msub> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals minus sigma-summation Underscript x Subscript i Baseline
    Endscripts p left-parenthesis x Subscript i Baseline right-parenthesis log Subscript
    2 Baseline p left-parenthesis x Subscript i Baseline right-parenthesis"><mrow><mo>=</mo>
    <mo>-</mo> <msub><mo>∑</mo> <msub><mi>x</mi> <mi>i</mi></msub></msub> <mi>p</mi>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <msub><mo
    form="prefix">log</mo> <mn>2</mn></msub> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: This expression is defined as the *entropy* of a probability distribution. In
    the case where we are completely certain of the final outcome (e.g., the dice
    always lands up six), we can evaluate the expression for entropy and see that
    we get a result of 0.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式被定义为概率分布的*熵*。在我们完全确定最终结果的情况下（例如，骰子总是掷出六点），我们可以评估熵的表达式，看到我们得到的结果是0。
- en: In the case where we are completely certain of the final outcome (e.g., the
    dice always lands up six), we can evaluate the expression for entropy and see
    that we get a result of 0\. Additionally, the probability distribution that has
    the highest entropy is the one that places equal probability over all possible
    outcomes. This is because, for any given trial, we are no more certain that a
    particular outcome will appear as opposed to any other outcome. As a result, we
    cannot use the strategy of assigning a shorter string to any single outcome.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们完全确定最终结果的情况下（例如，骰子总是掷出六点），我们可以评估熵的表达式，看到我们得到的结果是0。此外，具有最高熵的概率分布是将等概率分布在所有可能结果上的分布。这是因为对于任何给定的试验，我们对某个特定结果出现与其他结果出现一样的确定性。因此，我们不能使用将较短的字符串分配给任何单个结果的策略。
- en: Now that we have defined entropy, we can discuss cross entropy, which provides
    us a way of measuring the distinctness of two distributions.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了熵，我们可以讨论交叉熵，它为我们提供了一种衡量两个分布之间差异的方法。
- en: Equation 2-1\. Cross entropy
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程2-1. 交叉熵
- en: <math alttext="upper C upper E left-parenthesis p StartAbsoluteValue EndAbsoluteValue
    q right-parenthesis equals double-struck upper E Subscript p left-parenthesis
    x right-parenthesis Baseline left-bracket log Subscript 2 Baseline StartFraction
    1 Over q left-parenthesis x right-parenthesis EndFraction right-bracket equals
    sigma-summation Underscript x Endscripts p left-parenthesis x right-parenthesis
    log Subscript 2 Baseline StartFraction 1 Over q left-parenthesis x right-parenthesis
    EndFraction equals minus sigma-summation Underscript x Endscripts p left-parenthesis
    x right-parenthesis log Subscript 2 Baseline q left-parenthesis x right-parenthesis"><mrow><mi>C</mi>
    <mi>E</mi> <mrow><mo>(</mo> <mi>p</mi> <mo>|</mo> <mo>|</mo> <mi>q</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <msub><mo form="prefix">log</mo> <mn>2</mn></msub> <mfrac><mn>1</mn>
    <mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>]</mo></mrow>
    <mo>=</mo> <msub><mo>∑</mo> <mi>x</mi></msub> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow> <msub><mo form="prefix">log</mo> <mn>2</mn></msub> <mfrac><mn>1</mn>
    <mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>=</mo> <mo>-</mo>
    <msub><mo>∑</mo> <mi>x</mi></msub> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <msub><mo form="prefix">log</mo> <mn>2</mn></msub> <mrow><mi>q</mi> <mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper C upper E left-parenthesis p StartAbsoluteValue EndAbsoluteValue
    q right-parenthesis equals double-struck upper E Subscript p left-parenthesis
    x right-parenthesis Baseline left-bracket log Subscript 2 Baseline StartFraction
    1 Over q left-parenthesis x right-parenthesis EndFraction right-bracket equals
    sigma-summation Underscript x Endscripts p left-parenthesis x right-parenthesis
    log Subscript 2 Baseline StartFraction 1 Over q left-parenthesis x right-parenthesis
    EndFraction equals minus sigma-summation Underscript x Endscripts p left-parenthesis
    x right-parenthesis log Subscript 2 Baseline q left-parenthesis x right-parenthesis"><mrow><mi>C</mi>
    <mi>E</mi> <mrow><mo>(</mo> <mi>p</mi> <mo>|</mo> <mo>|</mo> <mi>q</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <msub><mo form="prefix">log</mo> <mn>2</mn></msub> <mfrac><mn>1</mn>
    <mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>]</mo></mrow>
    <mo>=</mo> <msub><mo>∑</mo> <mi>x</mi></msub> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow> <msub><mo form="prefix">log</mo> <mn>2</mn></msub> <mfrac><mn>1</mn>
    <mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>=</mo> <mo>-</mo>
    <msub><mo>∑</mo> <mi>x</mi></msub> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <msub><mo form="prefix">log</mo> <mn>2</mn></msub> <mrow><mi>q</mi> <mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math>
- en: Note that cross entropy has a <math alttext="log StartFraction 1 Over q left-parenthesis
    x right-parenthesis EndFraction"><mrow><mo form="prefix">log</mo> <mfrac><mn>1</mn>
    <mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow></math> term,
    which can be interpreted as the optimal binary string length assigned to each
    outcome, assuming outcomes appear according to probability distribution *q(x)*.
    However, note that this is an expectation with respect to *p(x)*, so how do we
    interpret this entire expression? Well, we can understand the cross entropy to
    mean the expected string length for any trial given we have optimized for the
    encoding scheme for distribution *q(x)* while, in reality, all of the outcomes
    are appearing according to the distribution *p(x)*. This can definitely happen
    in an experiment where we have only limited a priori information about the experiment,
    so we assume some distribution *q(x)* to optimize our encoding scheme, but as
    we carry out trials, we learn more information that gets us closer to the true
    distribution *p(x)*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意交叉熵中有一个项，可以解释为对每个结果分配的最佳二进制字符串长度，假设结果按概率分布*q(x)*出现。然而，请注意这是相对于*p(x)*的期望，那么我们如何解释整个表达式呢？嗯，我们可以理解交叉熵是指在为分布*q(x)*优化编码方案的情况下，对于任何试验的预期字符串长度，而实际上，所有结果都是根据分布*p(x)*出现的。这在实验中肯定会发生，因为我们对实验的先验信息有限，所以我们假设某个分布*q(x)*来优化我们的编码方案，但随着我们进行试验，我们学到了更多信息，使我们更接近真实分布*p(x)*。
- en: 'The KL divergence takes this logic a bit further. If we take the cross entropy,
    which tells us the expected number of bits per trial given we have optimized our
    encoding for the incorrect distribution *q(x),* and subtract from that the entropy,
    which tells us the expected number of bits per trial given we have optimized for
    the correct distribution *p(x*), we get the expected number of extra bits required
    to represent a trial when using *q(x)* compared to *p(x)*. Here is the expression
    for the KL divergence:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: KL散度将这种逻辑推得更远。如果我们取交叉熵，告诉我们在为不正确的分布*q(x)*优化我们的编码时每次试验预期的比特数，然后从中减去熵，告诉我们在为正确的分布*p(x)*优化时每次试验预期的比特数，我们得到了使用*q(x)*比*p(x)*时表示试验所需的额外比特数的预期值。以下是KL散度的表达式：
- en: <math alttext="upper K upper L left-parenthesis p StartAbsoluteValue EndAbsoluteValue
    q right-parenthesis equals double-struck upper E Subscript p left-parenthesis
    x right-parenthesis Baseline left-bracket log Subscript 2 Baseline StartFraction
    1 Over q left-parenthesis x right-parenthesis EndFraction minus log Subscript
    2 Baseline StartFraction 1 Over p left-parenthesis x right-parenthesis EndFraction
    right-bracket equals double-struck upper E Subscript p left-parenthesis x right-parenthesis
    Baseline left-bracket log Subscript 2 Baseline StartFraction p left-parenthesis
    x right-parenthesis Over q left-parenthesis x right-parenthesis EndFraction right-bracket"><mrow><mi>K</mi>
    <mi>L</mi> <mrow><mo>(</mo> <mi>p</mi> <mo>|</mo> <mo>|</mo> <mi>q</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <msub><mo form="prefix">log</mo> <mn>2</mn></msub> <mfrac><mn>1</mn>
    <mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>-</mo> <msub><mo
    form="prefix">log</mo> <mn>2</mn></msub> <mfrac><mn>1</mn> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac>
    <mo>]</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <msub><mo form="prefix">log</mo> <mn>2</mn></msub> <mfrac><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow>
    <mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>]</mo></mrow></mrow></math>
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper K upper L left-parenthesis p StartAbsoluteValue EndAbsoluteValue
    q right-parenthesis equals double-struck upper E Subscript p left-parenthesis
    x right-parenthesis Baseline left-bracket log Subscript 2 Baseline StartFraction
    1 Over q left-parenthesis x right-parenthesis EndFraction minus log Subscript
    2 Baseline StartFraction 1 Over p left-parenthesis x right-parenthesis EndFraction
    right-bracket equals double-struck upper E Subscript p left-parenthesis x right-parenthesis
    Baseline left-bracket log Subscript 2 Baseline StartFraction p left-parenthesis
    x right-parenthesis Over q left-parenthesis x right-parenthesis EndFraction right-bracket"><mrow><mi>K</mi>
    <mi>L</mi> <mrow><mo>(</mo> <mi>p</mi> <mo>|</mo> <mo>|</mo> <mi>q</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <msub><mo form="prefix">log</mo> <mn>2</mn></msub> <mfrac><mn>1</mn>
    <mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>-</mo> <msub><mo
    form="prefix">log</mo> <mn>2</mn></msub> <mfrac><mn>1</mn> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac>
    <mo>]</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <msub><mo form="prefix">log</mo> <mn>2</mn></msub> <mfrac><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow>
    <mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>]</mo></mrow></mrow></math>
- en: At the unique global minimum *q(x)* = *p(x)*, the KL divergence is exactly zero.
    Why this is the unique minimum is a bit beyond the scope of this text, so we leave
    that as an exercise for you.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在唯一的全局最小值*q(x)* = *p(x)*处，KL散度恰好为零。为什么这是唯一的最小值有点超出了本文的范围，所以我们把它留给你作为一个练习。
- en: In practice, when trying to match the true distribution *p(x)* with a learned
    distribution *q(x)*, KL divergence is often minimized as an objective function.
    Most models will actually minimize the cross entropy in place of the KL divergence,
    which is effectively the same optimization problem due to the KL being a difference
    between the cross entropy and the entropy of *p(x)*, where the entropy of *p(x)*
    is a constant and has no dependence on the weights that parameterize *q(x)*. Thus,
    the gradient with respect to the weights that parameterize *q(x)* when using either
    objective is the same.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: One common example where cross-entropy/KL divergence is optimized is in the
    standard training of a neural network classifier. The neural network’s objective
    is to learn a distribution over target classes such that, for any given example
    *x[i]*, <math alttext="p Subscript theta Baseline left-parenthesis y vertical-bar
    x equals x Subscript i Baseline right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>y</mi> <mo>|</mo> <mi>x</mi> <mo>=</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math> matches the true distribution
    *p(y|x* *= x[i])*, which has all of its probability mass placed over the true
    label *y[i]* and zero probability over all other classes. Minimizing the sum of
    cross entropies between the learned distribution and the true distribution over
    all examples is actually the exact same as minimizing the negative log likelihood
    of the data. Both are valid interpretations of how neural networks are trained,
    and lead to the same objective function. We encourage you to try writing out both
    expressions independently to see this.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Probability Distributions
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have looked at probability distributions through the lens of discrete
    outcomes and events. However, as it turns out, probability distributions aren’t
    just for sets of discrete outcomes like the CIFAR-10 target classes or the MNIST
    digits. We can define probability distributions over sample spaces of infinite
    size, such as all the real numbers. In this section, we will extend principles
    covered in the previous sections to the continuous realm.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: In the continuous realm, probability distributions are often referred to as
    *probability density functions*, or PDFs. PDFs are nonnegative functions over
    a sample space, such as all the reals, that integrate to one. Recall from calculus
    that the integration of a function is the area of the region underneath the function,
    bounded by the *x*-axis. PDFs follow the basic tenets introduced in the first
    section, but instead of adding the probability of outcomes to get the probability
    of an event, we use integration. For example, say *X* is a continuous random variable
    that is defined over all the real numbers. If we’d like to know the probability
    of the event <math alttext="upper P left-parenthesis upper X less-than-or-equal-to
    2 right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>X</mi> <mo>≤</mo> <mn>2</mn>
    <mo>)</mo></mrow></math> , all we’d need to do is integrate the PDF of *X* from
    negative infinity to 2.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: But how about the probability of any individual outcome, say *P*(*X* = 2)? Since
    we use integration to find probabilities in the continuous space, the probability
    of any individual outcome is actually zero due to the width of the region being
    infinitesimal. We instead use the term *likelihood* to distinguish between the
    probability of events and the value that the PDF evaluates to when we input a
    setting of *X*. Likelihoods are still valuable, as they tell us what individual
    outcomes we are most likely to see when performing an experiment over a continuous
    space. Going forward, when considering continuous probability distributions, we
    will only refer to events as having probability, rather than individual outcomes.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: One famous example of a continuous probability distribution is the *uniform
    distribution* over some interval on the real line. Under the uniform distribution,
    the likelihood of each outcome is the same, meaning that no outcome is any more
    likely to appear than another. Thus, the uniform distribution looks like a rectangle,
    where the base of the rectangle is the interval constituting its domain, and the
    height, or the likelihood for each outcome, is the value that makes the area of
    the rectangle equal to one. [Figure 2-4](#c299) shows the uniform distribution
    over the interval [0,0.5].
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0204.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. The uniform distribution has uniform height over its entire area,
    which shows that each value in the domain of the distribution has equal likelihood.
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This example was chosen specifically to show a concrete difference between likelihoods
    and probabilities in the continuous realm. The height of the rectangle being 2
    was no error—there is no constraint on the magnitude of the likelihood in continuous
    distributions, unlike probabilities, which must be less than or equal to 1.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Another famous example of a continuous probability distribution is the *Gaussian
    distribution*, which is one of the more common ways in which data presents itself
    in the real world. The Gaussian distribution is defined by two parameters: its
    mean <math alttext="mu"><mi>μ</mi></math> and its standard deviation <math alttext="sigma"><mi>σ</mi></math>
    . The PDF of a Gaussian distribution is:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="f left-parenthesis x semicolon mu comma sigma right-parenthesis
    equals StartFraction 1 Over sigma StartRoot 2 pi EndRoot EndFraction e Superscript
    minus one-half left-parenthesis StartFraction x minus mu Over sigma EndFraction
    right-parenthesis squared"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>;</mo>
    <mi>μ</mi> <mo>,</mo> <mi>σ</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <mrow><mi>σ</mi><msqrt><mrow><mn>2</mn><mi>π</mi></mrow></msqrt></mrow></mfrac>
    <msup><mi>e</mi> <mrow><mo>-</mo><mfrac><mn>1</mn> <mn>2</mn></mfrac><msup><mrow><mo>(</mo><mfrac><mrow><mi>x</mi><mo>-</mo><mi>μ</mi></mrow>
    <mi>σ</mi></mfrac><mo>)</mo></mrow> <mn>2</mn></msup></mrow></msup></mrow></math>
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="f left-parenthesis x semicolon mu comma sigma right-parenthesis
    equals StartFraction 1 Over sigma StartRoot 2 pi EndRoot EndFraction e Superscript
    minus one-half left-parenthesis StartFraction x minus mu Over sigma EndFraction
    right-parenthesis squared"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>;</mo>
    <mi>μ</mi> <mo>,</mo> <mi>σ</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <mrow><mi>σ</mi><msqrt><mrow><mn>2</mn><mi>π</mi></mrow></msqrt></mrow></mfrac>
    <msup><mi>e</mi> <mrow><mo>-</mo><mfrac><mn>1</mn> <mn>2</mn></mfrac><msup><mrow><mo>(</mo><mfrac><mrow><mi>x</mi><mo>-</mo><mi>μ</mi></mrow>
    <mi>σ</mi></mfrac><mo>)</mo></mrow> <mn>2</mn></msup></mrow></msup></mrow></math>
- en: Why this function integrates to 1 over the real domain is beyond the scope of
    this chapter, but one important characteristic of a Gaussian distribution is that
    its mean is also its unique mode. In other words, the outcome with the highest
    likelihood is also, uniquely, the mean outcome. This is not the case for all distributions.
    For example, [Figure 2-4](#c299) does not have this property. The graph of a standard
    Gaussian, which has mean zero and unit variance, is shown in [Figure 2-5](#c302)
    (the PDF asymptotically reaches zero in the limit in both directions).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0205.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. The Gaussian distribution has a bell shape, with highest likelihood
    in the center and dropping exponentially as the value in question gets farther
    and farther from the center.
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Why is the Gaussian distribution so prevalent in real-world data? One reason
    for this is a theorem called the *Central Limit Theorem* (CLT). This theorem states
    that sums of independent random variables converge to a Gaussian distribution
    as the number of variables in the sum goes to infinity, even if each variable
    is not distributed as a Gaussian. One example is the number of masked neurons
    after a dropout layer is applied. As the number of neurons from the previous layer
    goes to infinity, the number of masked neurons (which is a sum of independent
    Bernoulli random variables, as discussed in [“Random Variables”](#random-variablesonce)),
    when standardized correctly, is approximately distributed as a standard Gaussian
    distribution. We won’t cover CLT in much depth here, but it has more recently
    been extended to weakly dependent variables under certain special conditions.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Many real-world datasets can be seen as approximately sums of many random variables.
    For example, the distribution of a disease prevalence within a given population,
    similarly to the number of masked neurons after applying dropout, is the sum of
    many Bernoulli random variables (where each person is a Bernoulli random variable
    that has a value of 1 if they have the disease and a value of 0 if they do not)—although
    likely dependent.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuous random variables are still functions, just as we defined discrete
    random variables. The only difference is that the range of this function is a
    continuous space. To compute the expectation and variance of a continuous random
    variable, all we need to do is replace our summations with integrations, as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="double-struck upper E left-bracket upper X right-bracket equals
    integral Underscript x Endscripts x asterisk f left-parenthesis upper X equals
    x right-parenthesis d x"><mrow><mi>𝔼</mi> <mrow><mo>[</mo> <mi>X</mi> <mo>]</mo></mrow>
    <mo>=</mo> <msub><mo>∫</mo> <mi>x</mi></msub> <mi>x</mi> <mo>*</mo> <mi>f</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>=</mo> <mi>x</mi> <mo>)</mo></mrow> <mi>d</mi>
    <mi>x</mi></mrow></math>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E left-bracket upper X right-bracket equals
    integral Underscript x Endscripts x asterisk f left-parenthesis upper X equals
    x right-parenthesis d x"><mrow><mi>𝔼</mi> <mrow><mo>[</mo> <mi>X</mi> <mo>]</mo></mrow>
    <mo>=</mo> <msub><mo>∫</mo> <mi>x</mi></msub> <mi>x</mi> <mo>*</mo> <mi>f</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>=</mo> <mi>x</mi> <mo>)</mo></mrow> <mi>d</mi>
    <mi>x</mi></mrow></math>
- en: <math alttext="upper V a r left-parenthesis upper X right-parenthesis equals
    integral Underscript x Endscripts left-parenthesis x minus double-struck upper
    E left-bracket upper X right-bracket right-parenthesis squared asterisk f left-parenthesis
    upper X equals x right-parenthesis d x"><mrow><mi>V</mi> <mi>a</mi> <mi>r</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mo>∫</mo> <mi>x</mi></msub>
    <msup><mrow><mo>(</mo><mi>x</mi><mo>-</mo><mi>𝔼</mi><mrow><mo>[</mo><mi>X</mi><mo>]</mo></mrow><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>*</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>=</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mi>d</mi> <mi>x</mi></mrow></math>
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper V a r left-parenthesis upper X right-parenthesis equals
    integral Underscript x Endscripts left-parenthesis x minus double-struck upper
    E left-bracket upper X right-bracket right-parenthesis squared asterisk f left-parenthesis
    upper X equals x right-parenthesis d x"><mrow><mi>V</mi> <mi>a</mi> <mi>r</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mo>∫</mo> <mi>x</mi></msub>
    <msup><mrow><mo>(</mo><mi>x</mi><mo>-</mo><mi>𝔼</mi><mrow><mo>[</mo><mi>X</mi><mo>]</mo></mrow><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>*</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>=</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mi>d</mi> <mi>x</mi></mrow></math>
- en: 'As an example, let’s evaluate the expectation for our uniform random variable
    defined earlier. But first, confirm that it makes intuitive sense that the expectation
    should be 0.25, since the endpoints of the interval are 0 and 0.5 and all values
    in between are of equal likelihood. Now, let’s evaluate the integral and see if
    the computation matches our intuition:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="integral Subscript 0 Superscript 0.5 Baseline x asterisk f left-parenthesis
    x right-parenthesis d x equals integral Subscript 0 Superscript 0.5 Baseline 2
    x d x"><mrow><msubsup><mo>∫</mo> <mn>0</mn> <mrow><mn>0</mn><mo>.</mo><mn>5</mn></mrow></msubsup>
    <mi>x</mi> <mo>*</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mi>d</mi> <mi>x</mi> <mo>=</mo> <msubsup><mo>∫</mo> <mn>0</mn> <mrow><mn>0</mn><mo>.</mo><mn>5</mn></mrow></msubsup>
    <mn>2</mn> <mi>x</mi> <mi>d</mi> <mi>x</mi></mrow></math>
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="integral Subscript 0 Superscript 0.5 Baseline x asterisk f left-parenthesis
    x right-parenthesis d x equals integral Subscript 0 Superscript 0.5 Baseline 2
    x d x"><mrow><msubsup><mo>∫</mo> <mn>0</mn> <mrow><mn>0</mn><mo>.</mo><mn>5</mn></mrow></msubsup>
    <mi>x</mi> <mo>*</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mi>d</mi> <mi>x</mi> <mo>=</mo> <msubsup><mo>∫</mo> <mn>0</mn> <mrow><mn>0</mn><mo>.</mo><mn>5</mn></mrow></msubsup>
    <mn>2</mn> <mi>x</mi> <mi>d</mi> <mi>x</mi></mrow></math>
- en: <math alttext="equals x squared vertical-bar Subscript 0 Baseline Superscript
    0.5 Baseline"><mrow><mo>=</mo> <msup><mi>x</mi> <mn>2</mn></msup> <msubsup><mrow><mo>|</mo></mrow>
    <mn>0</mn> <mrow><mn>0</mn><mo>.</mo><mn>5</mn></mrow></msubsup></mrow></math>
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals x squared vertical-bar Subscript 0 Baseline Superscript
    0.5 Baseline"><mrow><mo>=</mo> <msup><mi>x</mi> <mn>2</mn></msup> <msubsup><mrow><mo>|</mo></mrow>
    <mn>0</mn> <mrow><mn>0</mn><mo>.</mo><mn>5</mn></mrow></msubsup></mrow></math>
- en: <math alttext="equals 0.25"><mrow><mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>25</mn></mrow></math>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="equals 0.25"><mrow><mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>25</mn></mrow></math>
- en: Where the superscript and the subscript of the | symbol represent the values
    at which we will evaluate the preceding function, which we will then difference
    to get the value of the integral. We see that the expectation comes out to the
    same value as our intuition, which is a great sanity check.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayes’ Theorem also holds for continuous variables. The only major difference
    is when marginalizing out a subset of variables, you will need to integrate over
    the entire domain of the marginalized subset rather than taking a discrete sum
    over all possible configurations of the marginalized subset. Again, this is an
    example of extending the tenets of probability to the continuous space by replacing
    summations with integrations. Here is Bayes’ Theorem for continuous probability
    distributions, following the notation from [“Bayes’ Theorem”](#bayes-theorem-sect):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P left-parenthesis upper B equals b Subscript q u e r y
    Baseline vertical-bar upper A right-parenthesis equals StartFraction upper P left-parenthesis
    upper A vertical-bar upper B equals b Subscript q u e r y Baseline right-parenthesis
    upper P left-parenthesis upper B equals b Subscript q u e r y Baseline right-parenthesis
    Over upper P left-parenthesis upper A right-parenthesis EndFraction equals StartFraction
    upper P left-parenthesis upper A vertical-bar upper B equals b Subscript q u e
    r y Baseline right-parenthesis upper P left-parenthesis upper B equals b Subscript
    q u e r y Baseline right-parenthesis Over integral Underscript b Endscripts upper
    P left-parenthesis upper A comma upper B equals b right-parenthesis d b EndFraction"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mi>B</mi> <mo>=</mo> <msub><mi>b</mi> <mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub>
    <mo>|</mo> <mi>A</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mrow><mo>(</mo><mi>A</mi><mo>|</mo><mi>B</mi><mo>=</mo><msub><mi>b</mi>
    <mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub> <mo>)</mo></mrow><mi>P</mi><mrow><mo>(</mo><mi>B</mi><mo>=</mo><msub><mi>b</mi>
    <mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub> <mo>)</mo></mrow></mrow>
    <mrow><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>P</mi><mrow><mo>(</mo><mi>A</mi><mo>|</mo><mi>B</mi><mo>=</mo><msub><mi>b</mi>
    <mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub> <mo>)</mo></mrow><mi>P</mi><mrow><mo>(</mo><mi>B</mi><mo>=</mo><msub><mi>b</mi>
    <mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub> <mo>)</mo></mrow></mrow>
    <mrow><msub><mo>∫</mo> <mi>b</mi></msub> <mi>P</mi><mrow><mo>(</mo><mi>A</mi><mo>,</mo><mi>B</mi><mo>=</mo><mi>b</mi><mo>)</mo></mrow><mi>d</mi><mi>b</mi></mrow></mfrac></mrow></math>
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis upper B equals b Subscript q u e r y
    Baseline vertical-bar upper A right-parenthesis equals StartFraction upper P left-parenthesis
    upper A vertical-bar upper B equals b Subscript q u e r y Baseline right-parenthesis
    upper P left-parenthesis upper B equals b Subscript q u e r y Baseline right-parenthesis
    Over upper P left-parenthesis upper A right-parenthesis EndFraction equals StartFraction
    upper P left-parenthesis upper A vertical-bar upper B equals b Subscript q u e
    r y Baseline right-parenthesis upper P left-parenthesis upper B equals b Subscript
    q u e r y Baseline right-parenthesis Over integral Underscript b Endscripts upper
    P left-parenthesis upper A comma upper B equals b right-parenthesis d b EndFraction"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mi>B</mi> <mo>=</mo> <msub><mi>b</mi> <mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub>
    <mo>|</mo> <mi>A</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mrow><mo>(</mo><mi>A</mi><mo>|</mo><mi>B</mi><mo>=</mo><msub><mi>b</mi>
    <mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub> <mo>)</mo></mrow><mi>P</mi><mrow><mo>(</mo><mi>B</mi><mo>=</mo><msub><mi>b</mi>
    <mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub> <mo>)</mo></mrow></mrow>
    <mrow><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>P</mi><mrow><mo>(</mo><mi>A</mi><mo>|</mo><mi>B</mi><mo>=</mo><msub><mi>b</mi>
    <mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub> <mo>)</mo></mrow><mi>P</mi><mrow><mo>(</mo><mi>B</mi><mo>=</mo><msub><mi>b</mi>
    <mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub> <mo>)</mo></mrow></mrow>
    <mrow><msub><mo>∫</mo> <mi>b</mi></msub> <mi>P</mi><mrow><mo>(</mo><mi>A</mi><mo>,</mo><mi>B</mi><mo>=</mo><mi>b</mi><mo>)</mo></mrow><mi>d</mi><mi>b</mi></mrow></mfrac></mrow></math>
- en: 'And finally, we have our discussion on entropy, cross entropy, and KL divergence.
    All three of these extend nicely to the continuous space as well. We replace our
    summations with integrations and note that the properties introduced in the previous
    section still hold. For example, over a given domain, the distribution with the
    highest entropy is the uniform distribution, and the KL divergence between two
    distributions is zero if and only if the two distributions are the exact same.
    Here are the definitions in their continuous form, following [Equation 2-1](#cross-entropy-formula):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper H left-parenthesis f left-parenthesis x right-parenthesis
    right-parenthesis equals minus integral Underscript x Endscripts f left-parenthesis
    x right-parenthesis log Subscript 2 Baseline f left-parenthesis x right-parenthesis
    d x"><mrow><mi>H</mi> <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo> <msub><mo>∫</mo> <mi>x</mi></msub>
    <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <msub><mo form="prefix">log</mo>
    <mn>2</mn></msub> <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mi>d</mi>
    <mi>x</mi></mrow></math>
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper H left-parenthesis f left-parenthesis x right-parenthesis
    right-parenthesis equals minus integral Underscript x Endscripts f left-parenthesis
    x right-parenthesis log Subscript 2 Baseline f left-parenthesis x right-parenthesis
    d x"><mrow><mi>H</mi> <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo> <msub><mo>∫</mo> <mi>x</mi></msub>
    <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <msub><mo form="prefix">log</mo>
    <mn>2</mn></msub> <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mi>d</mi>
    <mi>x</mi></mrow></math>
- en: <math alttext="upper K upper L left-parenthesis f left-parenthesis x right-parenthesis
    StartAbsoluteValue EndAbsoluteValue g left-parenthesis x right-parenthesis right-parenthesis
    equals integral Underscript x Endscripts f left-parenthesis x right-parenthesis
    log Subscript 2 Baseline StartFraction f left-parenthesis x right-parenthesis
    Over g left-parenthesis x right-parenthesis EndFraction d x"><mrow><mi>K</mi>
    <mi>L</mi> <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>|</mo> <mo>|</mo> <mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mo>∫</mo> <mi>x</mi></msub> <mi>f</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <msub><mo form="prefix">log</mo> <mn>2</mn></msub>
    <mfrac><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow> <mrow><mi>g</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac>
    <mi>d</mi> <mi>x</mi></mrow></math>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper K upper L left-parenthesis f left-parenthesis x right-parenthesis
    StartAbsoluteValue EndAbsoluteValue g left-parenthesis x right-parenthesis right-parenthesis
    equals integral Underscript x Endscripts f left-parenthesis x right-parenthesis
    log Subscript 2 Baseline StartFraction f left-parenthesis x right-parenthesis
    Over g left-parenthesis x right-parenthesis EndFraction d x"><mrow><mi>K</mi>
    <mi>L</mi> <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>|</mo> <mo>|</mo> <mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mo>∫</mo> <mi>x</mi></msub> <mi>f</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <msub><mo form="prefix">log</mo> <mn>2</mn></msub>
    <mfrac><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow> <mrow><mi>g</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac>
    <mi>d</mi> <mi>x</mi></mrow></math>
- en: <math alttext="upper C upper E left-parenthesis f left-parenthesis x right-parenthesis
    StartAbsoluteValue EndAbsoluteValue g left-parenthesis x right-parenthesis right-parenthesis
    equals minus integral Underscript x Endscripts f left-parenthesis x right-parenthesis
    log Subscript 2 Baseline g left-parenthesis x right-parenthesis d x"><mrow><mi>C</mi>
    <mi>E</mi> <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>|</mo> <mo>|</mo> <mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo> <msub><mo>∫</mo> <mi>x</mi></msub> <mi>f</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <msub><mo form="prefix">log</mo>
    <mn>2</mn></msub> <mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mi>d</mi>
    <mi>x</mi></mrow></math>
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper C upper E left-parenthesis f left-parenthesis x right-parenthesis
    StartAbsoluteValue EndAbsoluteValue g left-parenthesis x right-parenthesis right-parenthesis
    equals minus integral Underscript x Endscripts f left-parenthesis x right-parenthesis
    log Subscript 2 Baseline g left-parenthesis x right-parenthesis d x"><mrow><mi>C</mi>
    <mi>E</mi> <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>|</mo> <mo>|</mo> <mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo> <msub><mo>∫</mo> <mi>x</mi></msub> <mi>f</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <msub><mo form="prefix">log</mo>
    <mn>2</mn></msub> <mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mi>d</mi>
    <mi>x</mi></mrow></math>
- en: Our extension of these concepts to the continuous space will come in handy in
    [Chapter 10](ch10.xhtml#ch10), where we model many distributions as Gaussians.
    Additionally, we use the KL divergence/cross-entropy terms as a regularization
    procedure on the complexity of one of our learned distributions. Since KL divergence
    is only zero when the query distribution matches the target distribution, setting
    the target distribution to a Gaussian forces the learned distribution to approximate
    a Gaussian.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we covered the fundamentals of probability, first building the
    intuition behind the basics of probability distributions and then moving to relevant
    applications of probability, such as conditional probability, random variables,
    expectation, and variance. We saw the applications of probability in deep learning,
    such as how a neural net parametrizes a probability distribution during classification
    tasks, and how we can quantify the mathematical properties of dropout, a regularization
    technique in neural nets. Finally, we discussed measurements of uncertainty in
    probability distributions such as entropy, and generalized these concepts to the
    continuous realm.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Probability is a field that affects the choices in our everyday lives, and it’s
    key to understand the meaning behind the numbers. Additionally, we hope that this
    introduction puts the rest of the book in perspective and allows you to more rigorously
    understand future concepts. In the next chapter, we will discuss the structure
    of neural networks, and the motivations behind their design.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
