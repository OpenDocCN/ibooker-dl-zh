["```py\nimport pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\ndata={'Player':['L. Messi','R. Lewandowski','C. Ronaldo','Neymar Jr','K. \n➥ Mbappé','E.Haaland','H. Kane','Luka Modrić','L. Goretzka','M. Salah'],\n➥    'Age':[36,34,38,22,24,35,29,37,28,31],\n➥    'Nationality':['Argentina','Poland','Portugal','Brazil','France','Norway',\n➥    'England','Croatia','Germany','Egypt'],\n➥    'Club':['Inter Miami','Barcelona','Al-Nassr','Al-Hilal ','PSG','Manchester\n➥    City','Tottenham Hotspur','Real Madrid','Bayern Munich','Liverpool'],\n➥    'League':['Major League Soccer ','Spain Primera Division','Saudi Arabia\n➥ League','Saudi Arabia League','French Ligue 1','English Premier\n➥ League','English Premier League','Spain Primera Division','German 1.\n➥ Bundesliga','English Premier League']}\ndf=pd.DataFrame.from_dict(data)\n```", "```py\nG = nx.Graph()                                                                      ①\nfor index, row in df.iterrows():\n    G.add_edge(row['Player'], row['Club'], relationship='plays_for')                ②\nfor index, row in df.iterrows():\n    G.add_edge(row['Player'], row['Nationality'], relationship='belongs_to')        ③\npos = nx.kamada_kawai_layout(G)                                                     ④\nplt.figure(figsize=(20, 14))                                                        ⑤\nplayer_nodes = df['Player'].unique().tolist()                                       ⑥\nclub_nodes = df['Club'].unique().tolist()                                           ⑥\nnationality_nodes = df['Nationality'].unique().tolist()                             ⑥\nnx.draw_networkx_nodes(G, pos, nodelist=player_nodes, node_color='blue',\n➥ label='Player Name', node_shape='o')                                             ⑦\nnx.draw_networkx_nodes(G, pos, nodelist=club_nodes, node_color='red', label='Club', ⑦\n➥ node_shape='d')                                                                  ⑦\nnx.draw_networkx_nodes(G, pos, nodelist=nationality_nodes, node_color='gray',       ⑦\n➥ label='Nationality', node_shape='v')                                             ⑦\nnx.draw_networkx_edges(G, pos)                                                      ⑧\nedge_labels = nx.get_edge_attributes(G, 'relationship')                             ⑨\nnx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=12)         ⑨\nnx.draw_networkx_labels(G, pos)                                                     ⑩\nplt.legend(fontsize=13, loc='upper right')\nplt.show()\n```", "```py\n$conda install pytorch torchvision -c pytorch\n$conda install torch_scatter\n$conda install torch_sparse\n$conda install torch_cluster\n$conda install torch-spline-conv\n$conda install torch_geometric\n```", "```py\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.datasets import Planetoid\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.utils import to_networkx\n```", "```py\ndataset = Planetoid(root='/tmp/Cora', name='Cora')\n```", "```py\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(dataset.num_node_features, 16)\n        self.conv2 = GCNConv(16, dataset.num_classes)\n        self.dropout = torch.nn.Dropout(0.5)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return x\n```", "```py\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')        ①\nmodel = GCN().to(device)                                                     ②\ndata = dataset[0].to(device)                                                 ③\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4) ④\n\nmodel.train()                                                                ⑤\nfor epoch in range(200):                                                     ⑤\n    optimizer.zero_grad()                                                    ⑤\n    out = model(data)                                                        ⑤\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])         ⑤\n    loss.backward()                                                          ⑤\n    optimizer.step()                                                         ⑤\n\nmodel.eval()                                                                 ⑥\nembeddings_pyg = model(data).detach().cpu().numpy()                          ⑦\n```", "```py\nRandomly initialize the weights of each neuron\nFor each step s=1 to iteration limit:\n    Randomly pick an input vector from the dataset\n        Traverse each node in the map\n            Calculate Euclidean distance as a similarity measure\n            Determine the node that produces the smallest distance (winning neuron)\n        Adapt the weights of each neuron v according to the following rule\n        Wv(s+1)=Wv(s)+ α(s).θ(u,v,s).‖Dt-Wv(s)‖\n```", "```py\nimport torch\nfrom torch import nn\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\ndef celestial_to_euclidean(ra, dec):             ①\n    x = np.cos(dec)*np.cos(ra)\n    y = np.cos(dec)*np.sin(ra)\n    z = np.sin(dec)\n    return x, y, z\n\ndef euclidean_to_celestial(x, y, z):             ②\n    sindec = z\n    cosdec = (x*x + y*y).sqrt()\n    sinra = y / cosdec\n    cosra = x / cosdec\n    ra = torch.atan2(sinra, cosra)\n    dec = torch.atan2(sindec, cosdec)\n    return ra, dec\n\ndef sphere_dist(x,y):                            ③\n    if x.ndim == 1:\n        x = x.unsqueeze(0)\n    if y.ndim == 1:\n        y = y.unsqueeze(0)\n    assert x.ndim == y.ndim == 2\n    inner = (x*y).sum(-1)\n    return torch.arccos(inner)\n```", "```py\nclass c_convex(nn.Module):                                                ①\n    def __init__(self, n_components=4, gamma=0.5, seed=None):\n        super().__init__()\n        self.n_components = n_components\n        self.gamma = gamma\n\n        if seed is not None:                                              ②\n            torch.manual_seed(seed)                                       ②\n        self.ys = torch.randn(n_components, 3)                            ②\n        self.ys = self.ys / torch.norm(self.ys, 2, dim=-1, keepdim=True)  ②\n        self.alphas = .7*torch.rand(self.n_components)                    ②\n        self.params = torch.cat((self.ys.view(-1), self.alphas.view(-1))) ②\n\n    def forward(self, xyz):                                               ③\n        cs = []                                                           ③\n        for y, alpha in zip(self.ys, self.alphas):                        ③\n            ci = 0.5*sphere_dist(y, xyz)**2 + alpha                       ③\n            cs.append(ci)\n        cs = torch.stack(cs)\n        if self.gamma == None or self.gamma == 0.:\n            z = cs.min(dim=0).values \n        else:\n            z = -self.gamma*(-cs/self.gamma).logsumexp(dim=0) \n        return z\n```", "```py\nseeds = [8,9,2,31,4,20,16,7]           ①\nfs = [c_convex(seed=i) for i in seeds] ②\nn_params = len(fs[0].params)           ③\n```", "```py\nclass AmortizedModel(nn.Module):\n    def __init__(self, n_params):            ①\n        super().__init__()\n        self.base = nn.Sequential(\n\n            nn.Linear(n_params, n_hidden),\n            nn.ReLU(inplace=True),\n            nn.Linear(n_hidden, n_hidden),\n            nn.ReLU(inplace=True),\n            nn.Linear(n_hidden, 3)\n        )                                    ②\n\n    def forward(self, p):                    ③\n        squeeze = p.ndim == 1\n        if squeeze:\n            p = p.unsqueeze(0)\n        assert p.ndim == 2\n        z = self.base(p)\n        z = z / z.norm(dim=-1, keepdim=True)\n        if squeeze:\n            z = z.squeeze(0)\n        return z\n```", "```py\nn_hidden = 128                                               ①\ntorch.manual_seed(0)                                         ②\nmodel = AmortizedModel(n_params=n_params)                    ③\nopt = torch.optim.Adam(model.parameters(), lr=5e-4)          ④\n\nxs = []                                                      ⑤\nnum_iterations = 100\n\npbar = tqdm(range(num_iterations), desc=\"Training Progress\")\n\nfor i in pbar:                                               ⑥\n    losses = []                                              ⑦\n    xis = []                                                 ⑦\n    for f in fs:                                             ⑧\n        pred_opt = model(f.params)\n        xis.append(pred_opt)\n        losses.append(f(pred_opt))\n    with torch.no_grad():                                    ⑧\n        xis = torch.stack(xis)\n        xs.append(xis)\n    loss = sum(losses)\n\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n\n    pbar.set_postfix({\"Loss\": loss.item()})\n\nxs = torch.stack(xs, dim=1)\n```", "```py\nimport os\nimport math\nimport itertools\nimport numpy as np\nimport networkx as nx\nfrom scipy.spatial.distance import pdist, squareform\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.nn import DataParallel\n\nfrom learning_tsp.problems.tsp.problem_tsp import TSP\nfrom learning_tsp.utils import load_model, move_to\n\nfrom gurobipy import *\n\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n```", "```py\nclass opts:\n    dataset_path = \"learning_tsp/data/tsp20-50_concorde.txt\"\n    batch_size = 16\n    num_samples = 1280 \n\n    neighbors = 0.20\n    knn_strat = 'percentage'\n\n    model = \n➥ \"learning_tsp/pretrained/tspsl_20-50/sl-ar-var-20pnn-gnn-max_20200308T172931\"\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n```", "```py\ndataset = TSP.make_dataset(\n    filename=opts.dataset_path, batch_size=opts.batch_size,\n➥ num_samples=opts.num_samples, \n➥ neighbors=opts.neighbors, knn_strat=opts.knn_strat, supervised=True\n)\n```", "```py\ndataloader = DataLoader(dataset, batch_size=opts.batch_size, shuffle=False,\n➥ num_workers=0)\n```", "```py\nmodel, model_args = load_model(opts.model, extra_logging=True) ①\nmodel.to(opts.device)\n\nif isinstance(model, DataParallel):                            ②\n    model = model.module                                       ②\n\nmodel.set_decode_type(\"greedy\")                                ③\n\nmodel.eval()                                                   ④\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\nimport requests\nimport os\nfrom tqdm import tqdm\n\nfrom som_tsp.helper import read_tsp, normalize, get_neighborhood, get_route,\n➥ select_closest, route_distance, plot_network, plot_route\n\nurl = 'https://raw.githubusercontent.com/Optimization-Algorithms-Book/Code-\n➥Listings/256207c4a8badc0977286c48a6e1cfd33237a51d/Appendix%20B/data/TSP/' ①\n\ntsp='qa194.tsp'                                                             ②\n\nresponse = requests.get(url+tsp)                                            ③\nresponse.raise_for_status()                                                 ③\nproblem_text = response.text                                                ③\nwith open(tsp, 'w') as file:                                                ③\n    file.write(problem_text)                                                ③\n\nproblem = read_tsp(tsp)                                                     ④\n\ncities = problem.copy()                                                     ⑤\ncities[['x', 'y']] = normalize(cities[['x', 'y']])                          ⑤\n```", "```py\nnumber_of_neurons = cities.shape[0] * 8         ①\n\niterations = 12000                              ②\n\nlearning_rate=0.8                               ③\nnetwork = np.random.rand(number_of_neurons, 2)  ④\n```", "```py\nroute_lengths = []                                                        ①\npaths_x = []                                                              ②\npaths_y = []                                                              ②\n\nfor i in tqdm(range(iterations)):                                         ③\n    if not i % 100:\n        print('\\t> Iteration {}/{}'.format(i, iterations), end=\"\\r\")      ④\n\n    city = cities.sample(1)[['x', 'y']].values                            ⑤\n    winner_idx = select_closest(network, city)                            ⑥\n\n    gaussian = get_neighborhood(winner_idx, number_of_neurons // 10, \n➥  network.shape[0])                                                      ⑦\n\n    network += gaussian[:, np.newaxis] * learning_rate * (city - network) ⑧\n\n    paths_x.append(network[:, 0].copy())                                  ⑨\n    paths_y.append(network[:, 1].copy())                                  ⑨\n\n    learning_rate = learning_rate * 0.99997                               ⑩\n    number_of_neurons = number_of_neurons * 0.9997                        ⑩\n\n    if not i % 1000:                                                      ⑪\n        plot_network(cities, network, name='diagrams/{:05d}.png'.format(i))\n\n    if number_of_neurons < 1:                                             ⑫\n        print('Radius has completely decayed, finishing execution',\n              ➥ 'at {} iterations'.format(i))\n        break\n    if learning_rate < 0.001:\n        print('Learning rate has completely decayed, finishing execution',\n              ➥ 'at {} iterations'.format(i))\n        break\n\n    route = get_route(cities, network)                                    ⑬\n    problem = problem.reindex(route)                                      ⑬\n    distance = route_distance(problem)                                    ⑬\n    route_lengths.append(distance)                                        ⑬\n\nelse:\n    print('Completed {} iterations.'.format(iterations))                  ⑭\n```", "```py\nplt.figure(figsize=(8, 6))\nplt.plot(range(len(route_lengths)), route_lengths, label='Route Length')\nplt.xlabel('Iterations')\nplt.ylabel('Route Length')\nplt.title('Route Length per Iteration')\nplt.grid(True)\nplt.show()\n```", "```py\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import ConvexHull\n\nfrom ptrnets.Data import display_points_with_hull, cyclic_permute, \n➥ Scatter2DDataset,Disp_results\nfrom ptrnets.ptr_net import ConvexNet, AverageMeter, masked_accuracy,\n➥ calculate_hull_overlap\n\nmin_samples = 5\nmax_samples = 50\nn_rows_train = 100000\nn_rows_val = 1000\n\ntorch.random.manual_seed(231)\ntrain_dataset = Scatter2DDataset(n_rows_train, min_samples, max_samples)\nval_dataset = Scatter2DDataset(n_rows_val, min_samples, max_samples)\n```", "```py\nTOKENS = {'<eos>': 0 } \nc_inputs = 2 + len(TOKENS)\nc_embed = 16\nc_hidden = 16\nn_heads = 4\nn_layers = 3\ndropout = 0.0\nuse_cuda = True\nn_workers = 2\nn_epochs = 5\nbatch_size = 16\nlr = 1e-3\nlog_interval = 500\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\")\n```", "```py\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size,\n➥  num_workers=n_workers)\n\nval_loader = DataLoader(val_dataset, batch_size=batch_size,\n➥  num_workers=n_workers)\n```", "```py\nmodel = ConvexNet(c_inputs=c_inputs, c_embed=c_embed, n_heads=n_heads,\n➥  n_layers=n_layers, dropout=dropout, c_hidden=c_hidden).to(device) ①\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)               ②\ncriterion = torch.nn.NLLLoss(ignore_index=TOKENS['<eos>'])            ③\n\ntrain_loss = AverageMeter()                                           ④\ntrain_accuracy = AverageMeter()                                       ④\nval_loss = AverageMeter()                                             ④\nval_accuracy = AverageMeter()                                         ④\n```", "```py\nfor epoch in range(n_epochs): \n  model.train()                                                         ①\n  for bat, (batch_data, batch_labels, batch_lengths) in enumerate(train_loader):                                                                ②\n    batch_data = batch_data.to(device)\n    batch_labels = batch_labels.to(device)\n    batch_lengths = batch_lengths.to(device)\n\n    optimizer.zero_grad()                                               ③\n    log_pointer_scores, pointer_argmaxs = model(batch_data, batch_lengths, \n➥    batch_labels=batch_labels) \n    loss = criterion(log_pointer_scores.view(-1, log_pointer_scores.    ④\n➥ shape[-1]), batch_labels.reshape(-1))\n\n    assert not np.isnan(loss.item()), 'Model diverged with loss = NaN'  ⑤\n\n    loss.backward()                                                     ⑥\n    optimizer.step()                                                    ⑥\n\n    train_loss.update(loss.item(), batch_data.size(0))                  ⑦\n    mask = batch_labels != TOKENS['<eos>']                              ⑦\n    acc = masked_accuracy(pointer_argmaxs, batch_labels, mask).item()   ⑦\n    train_accuracy.update(acc, mask.int().sum().item())                 ⑦\n\n    if bat % log_interval == 0:                                         ⑧\n      print(f'Epoch {epoch}: '\n            f'Train [{bat * len(batch_data):9d}/{len(train_dataset):9d} '\n            f'Loss: {train_loss.avg:.6f}\\tAccuracy: {train_accuracy.avg:3.4%}')\n```", "```py\nmodel.eval()                                                         ①\n  hull_overlaps = []                                                 ②\n  for bat, (batch_data, batch_labels, batch_lengths) \n  ➥ in enumerate(val_loader):                                       ③\n    batch_data = batch_data.to(device)\n    batch_labels = batch_labels.to(device)\n    batch_lengths = batch_lengths.to(device)\n    log_pointer_scores, pointer_argmaxs = model(batch_data, batch_lengths,\n➥      batch_labels=batch_labels)                                   ④\n    loss = criterion(log_pointer_scores.view(-1, log_pointer_scores. ⑤\n➥ shape[-1]),batch_labels.reshape(-1)) \n\n    assert not np.isnan(loss.item()), 'Model diverged with loss = NaN'\n    val_loss.update(loss.item(), batch_data.size(0))                  ⑥\n    mask = batch_labels != TOKENS['<eos>']                            ⑦\n    acc = masked_accuracy(pointer_argmaxs, batch_labels, mask).item() ⑧\n    val_accuracy.update(acc, mask.int().sum().item())                 ⑨\n\nfor data, length, ptr in zip(batch_data.cpu(), batch_lengths.cpu(),\n➥        pointer_argmaxs.cpu()):                                     ⑩\n      hull_overlaps.append(calculate_hull_overlap(data, length, ptr)) ⑪\n  print(f'Epoch {epoch}: Val\\tLoss: {val_loss.avg:.6f} '\n        f'\\tAccuracy: {val_accuracy.avg:3.4%} '\n        f'\\tOverlap: {np.mean(hull_overlaps):3.4%}')                  ⑫\n  train_loss.reset()                                                  ⑬\n  train_accuracy.reset()                                              ⑬\n  val_loss.reset()                                                    ⑬\n  val_accuracy.reset()                                                ⑬\n```", "```py\nDisp_results(train_loss, train_accuracy, val_loss, val_accuracy, n_epochs)\n```", "```py\nBest Scores:\ntrain_loss: 0.0897 (ep: 9)\ntrain_accuracy 96.61% (ep: 9)\nval_loss: 0.0937 (ep: 7)\nval_accuracy: 96.54% (ep: 7)\n```", "```py\nn_rows_test = 1000                                                            ①\n\ndef test(model, n_rows_test, n_per_row):                                      ②\n  test_dataset = Scatter2DDataset(n_rows_test, n_per_row, n_per_row)          ③\n  test_loader = DataLoader(test_dataset, batch_size=batch_size,               ③\n   ➥ num_workers=n_workers)                                                  ③\n\n  test_accuracy = AverageMeter()\n  hull_overlaps = []\n  model.eval()\n\n  for _, (batch_data, batch_labels, batch_lengths) in enumerate(test_loader): ④\n    batch_data = batch_data.to(device)\n    batch_labels = batch_labels.to(device)\n    batch_lengths = batch_lengths.to(device)\n\n    _, pointer_argmaxs = model(batch_data, batch_lengths)\n\n    val_loss.update(loss.item(), batch_data.size(0))                          ⑤\n    mask = batch_labels != TOKENS['<eos>']                                    ⑤\n    acc = masked_accuracy(pointer_argmaxs, batch_labels, mask).item()         ⑤\n    test_accuracy.update(acc, mask.int().sum().item())                        ⑤\n\n    for data, length, ptr in zip(batch_data.cpu(), batch_lengths.cpu(),       ⑥\n       ➥ pointer_argmaxs.cpu()):                                             ⑥\n      hull_overlaps.append(calculate_hull_overlap(data, length, ptr))         ⑥\n\n  print(f'# Test Samples: {n_per_row:3d}\\t '                                  ⑦\n        f'\\tAccuracy: {test_accuracy.avg:3.1%} '                              ⑦\n        f'\\tOverlap: {np.mean(hull_overlaps):3.1%}')                          ⑦\n\nfor i in range(5,50,5):                                                       ⑧\n  test(model, n_rows_test, i)                                                 ⑧\n```", "```py\n# Test Samples:   5     Accuracy: 54.8%     Overlap: 43.7%\n# Test Samples:  10     Accuracy: 72.1%     Overlap: 79.1%\n# Test Samples:  15     Accuracy: 79.0%     Overlap: 90.1%\n# Test Samples:  20     Accuracy: 84.8%     Overlap: 92.7%\n# Test Samples:  25     Accuracy: 80.6%     Overlap: 92.3%\n# Test Samples:  30     Accuracy: 80.3%     Overlap: 91.6%\n# Test Samples:  35     Accuracy: 77.8%     Overlap: 91.9%\n# Test Samples:  40     Accuracy: 75.8%     Overlap: 92.1%\n# Test Samples:  45     Accuracy: 72.4%     Overlap: 90.4%\n```", "```py\nidx = 0\nn_per_row = 50                                                             ①\n\ntest_dataset = Scatter2DDataset(n_rows_test, n_per_row, n_per_row)         ②\ntest_loader = DataLoader(test_dataset, batch_size=batch_size,              ③\n➥  num_workers=n_workers)                                                 ③\nbatch_data, batch_labels, batch_lengths = next(iter(test_loader))\nprint(batch_data.shape,batch_lengths.shape) \nlog_pointer_scores, pointer_argmaxs = model(batch_data.to(device),\n➥  batch_lengths.to(device))                                              ④\npred_hull_idxs = pointer_argmaxs[idx].cpu()                                ⑤\npred_hull_idxs = pred_hull_idxs[pred_hull_idxs > 1] – 2                    ⑥\npoints = batch_data[idx, 2:batch_lengths[idx], :2]                         ⑦\npoints1 = batch_data[idx, 1:batch_lengths[idx], :2]                        ⑦\nprint(points.shape,)                                                       ⑦\ntrue_hull_idxs = ConvexHull(points).vertices.tolist()                      ⑧\ntrue_hull_idxs = cyclic_permute(true_hull_idxs, np.argmin(true_hull_idxs)) ⑧\n\noverlap = calculate_hull_overlap(batch_data[idx].cpu(), batch_lengths[idx].cpu(),\n➥  pointer_argmaxs[idx].cpu())                                             ⑨\n\nprint(f'Predicted: {pred_hull_idxs.tolist()}')                             ⑩\nprint(f'True:      {true_hull_idxs}')                                      ⑩\nprint(f'Hull overlap: {overlap:3.2%}')                                     ⑩\n```", "```py\ntorch.Size([16, 51, 3]) torch.Size([16])\ntorch.Size([49, 2])\nPredicted: [0, 3, 5, 31, 45, 47, 48, 40, 10]\nTrue:      [0, 3, 5, 31, 45, 47, 48, 40, 10]\nHull overlap: 100.00%\n```", "```py\nplt.rcParams['figure.figsize'] = (10, 6)                ①\nplt.subplot(1, 2, 1)                                    ①\n\ntrue_hull_idxs = ConvexHull(points).vertices.tolist()   ②\ndisplay_points_with_hull(points, true_hull_idxs)        ③\n_ = plt.title('SciPy Convex Hull')                      ③\n\nplt.subplot(1, 2, 2)                                    ④\ndisplay_points_with_hull(points, pred_hull_idxs)        ⑤\n_ = plt.title('ConvexNet Convex Hull')                  ⑤\n```"]