<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">12</span> </span> <span class="chapter-title-text">Causal decisions and reinforcement learning</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Using causal models to automate decisions</li>
<li class="readable-text" id="p3">Setting up causal bandit algorithms</li>
<li class="readable-text" id="p4">How to incorporate causality into reinforcement learning</li>
</ul>
</div>
<div class="readable-text" id="p5">
<p>When we apply methods from statistics and machine learning, it is typically in service of making a decision or automating decision-making. Algorithms for automated decision-making, such as <em>bandit</em> and <em>reinforcement learning</em> (RL) algorithms, involve agents that <em>learn </em>how to make good decisions. In both cases, decision-making is fundamentally a causal problem: a decision to take some course of action leads to consequences, and the objective is to choose the action that leads to consequences favorable to the decision-maker. That motivates a causal framing.</p>
</div>
<div class="readable-text intended-text" id="p6">
<p>Often, the path from action to consequences has a degree of randomness. For example, your choice of how to play a hand of poker may be optimal, but you still might lose due to chance. That motivates a probabilistic modeling approach. </p>
</div>
<div class="readable-text intended-text" id="p7">
<p>The causal probabilistic modeling approach we’ve used so far in this book is a stone that hits both these birds. This chapter will provide a <em>causality-first </em>introduction to basic ideas in statistical decision theory, sequential decision-making, bandits, and RL. By “causality-first,” I mean I’ll use the foundation we’ve built in previous chapters to introduce these ideas in a causal light. I’ll also present the ideas in a way that is compatible with our probabilistic ML framing. Even if you are already familiar with these decision-making and RL concepts, I encourage you to read on and see them again through a causal lens. Once we do that, we’ll see cases where the causal approach to RL gets a better result than the noncausal approach.</p>
</div>
<div class="readable-text" id="p8">
<h2 class="readable-text-h2" id="sigil_toc_id_290"><span class="num-string">12.1</span> A causal primer on decision theory</h2>
</div>
<div class="readable-text" id="p9">
<p>Decision theory is concerned with the reasoning underlying an agent’s choice of some course of action. An “agent” here is an entity that chooses an action. </p>
</div>
<div class="readable-text intended-text" id="p10">
<p>For example, suppose you were deciding whether to invest in a company by purchasing equity or purchasing debt (i.e., loaning money to the company and receiving interest payments). We’ll call this variable <em>X</em>. Whether the company is successful (<em>Y</em><em>  </em>) depends on the type of investment it receives.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p11">
<img alt="figure" height="109" src="../Images/CH12_F01_Ness.png" width="316"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.1</span> A simple causal DAG where action <em>X</em> causes some outcome <em>Y</em>. Decision theory is a causal problem because if deciding on an action didn’t have causal consequences, what would be the point of making decisions?</h5>
</div>
<div class="readable-text intended-text" id="p12">
<p>Since <em>X</em> causally drives <em>Y</em>, we can immediately introduce a causal DAG, as in figure 12.1.</p>
</div>
<div class="readable-text intended-text" id="p13">
<p>We’ll use this example to illustrate basic concepts in decision theory from a causal point of view.</p>
</div>
<div class="readable-text" id="p14">
<h3 class="readable-text-h3" id="sigil_toc_id_291"><span class="num-string">12.1.1</span> Utility, reward, loss, and cost</h3>
</div>
<div class="readable-text" id="p15">
<p>The agent generally chooses actions that will cause them to gain some utility (or minimize some loss). In decision modeling, you can define a utility function (aka a reward function) that quantifies the desirability of various outcomes of a decision. Suppose you invest at $1,000:</p>
</div>
<ul>
<li class="readable-text" id="p16"> If the company becomes successful, you get $100,000. Your utility is 100,000 – 1,000 = $99,000. </li>
<li class="readable-text" id="p17"> If the company fails, you get $0 and lose your investment. Your utility is –1,000. </li>
</ul>
<div class="readable-text" id="p18">
<p>We can add this utility as a node on the graph, as in figure 12.2.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p19">
<img alt="figure" height="109" src="../Images/CH12_F02_Ness.png" width="493"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.2</span> A utility node can represent utility/reward, loss/cost.</h5>
</div>
<div class="readable-text" id="p20">
<p>Note that utility is a deterministic function of <em>Y</em> in this model, which we’ll denote <em>U</em>(<em>Y</em>).<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p21">
<img alt="figure" height="73" src="../Images/ness-ch12-eqs-0x.png" width="307"/>
</div>
<div class="readable-text" id="p22">
<p>Instead of a utility/reward function, we could define a loss function (aka, a cost function), which is simply –1 times the utility/reward function. For example, in the second scenario, where you purchase stock and the company fails, your utility is –$1,000 and your loss is $1,000.</p>
</div>
<div class="readable-text intended-text" id="p23">
<p>While the agent’s goal is to decide on a course of action that will maximize utility, doing so is challenging because there is typically some uncertainty in whether an action will lead to the desired result. In our example, it may seem obvious to invest in equity because equity will lead to business success, and business success will definitely lead to more utility. But there is some uncertainty in whether an equity investment will lead to business success. In other words we don’t assume <em>P</em><em> </em>(<em>Y</em><em> </em>=<em> </em>success|<em>X</em><em> </em>=<em> </em>equity) = 1. Both success and failure have nonzero probability in <em>P</em><em> </em>(<em>Y</em><em> </em>|<em>X</em><em> </em>=<em> </em>equity).</p>
</div>
<div class="readable-text" id="p24">
<h3 class="readable-text-h3" id="sigil_toc_id_292"><span class="num-string">12.1.2</span> Uncertainty comes from other causes</h3>
</div>
<div class="readable-text" id="p25">
<p>In causal terms, given action <em>X</em>, there is still some uncertainty in the outcome <em>Y</em> because there are other causal factors driving that outcome. For example, suppose the success of the business depends on economic conditions, as in figure 12.3.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p26">
<img alt="figure" height="205" src="../Images/CH12_F03_Ness.png" width="493"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.3</span> We typically have uncertainty in our decision-making. From a causal perspective, uncertainty is because of other causal factors out of our control that affect variables downstream of our actions.</h5>
</div>
<div class="readable-text" id="p27">
<p>Alternatively, those other causal factors could affect utility directly. For example, rather than the two discrete scenarios of profit or loss I outlined for our business investment, the amount of utility (or loss) could depend on how well or how poorly the economy fares, as in figure 12.4. We can leverage statistical and probability modeling to address this uncertainty. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p28">
<img alt="figure" height="205" src="../Images/CH12_F04_Ness.png" width="493"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.4</span> Causal factors outside of our control can impact utility (or loss) directly.</h5>
</div>
<div class="readable-text" id="p29">
<p>Suppose you are thinking about whether to invest in this business. You want your decision to be data-driven, so you research what other investors in this market have done before. You consider the causal DAG in figure 12.5.</p>
</div>
<div class="browsable-container figure-container" id="p30">
<img alt="figure" height="205" src="../Images/CH12_F05_Ness.png" width="493"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.5</span> In this DAG, economic conditions drive how investors choose to invest.</h5>
</div>
<div class="readable-text" id="p31">
<p>Based on your research, you conclude that past investors’ equity vs. debt choice also depends on the economic conditions. <em>P</em><em> </em>(<em>X</em><em>  </em>|<em>C</em>) represents an action distribution—the distribution of actions that the population of investors you are studying take.</p>
</div>
<div class="readable-text intended-text" id="p32">
<p>However, the goal of your analysis centers on yourself, not other investors. You want to answer questions like “what if <em>I </em>bought equity?” That question puts us in causal territory. We are not reasoning about observational investment trends; we are reasoning about conditional hypotheticals. That is an indicator that we need to introduce intervention-based reasoning and counterfactual notation.</p>
</div>
<div class="readable-text" id="p33">
<h2 class="readable-text-h2" id="sigil_toc_id_293"><span class="num-string">12.2</span> Causal decision theory</h2>
</div>
<div class="readable-text" id="p34">
<p>In this section, we’ll highlight decision-making as a causal query and examine what that means for modeling decision-making.</p>
</div>
<div class="readable-text" id="p35">
<h3 class="readable-text-h3" id="sigil_toc_id_294"><span class="num-string">12.2.1</span> Decisions as a level 2 query</h3>
</div>
<div class="readable-text" id="p36">
<p>A major source of confusion for causal decision modeling is the difference between actions and interventions. In many decision contexts, especially in RL, the action is a thing that the agent <em>does </em>that changes their environment. Yet, the action is also a variable <em>driven by</em> the environment. We see this when we look at the investment example, shown again in figure 12.6.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p37">
<img alt="figure" height="205" src="../Images/CH12_F06_Ness.png" width="493"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.6</span> In this version of the investment DAG, the choice of action is caused by external factors.</h5>
</div>
<div class="readable-text" id="p38">
<p>The action of selecting equity or debt is a variable causally driven by the economy. What does that mean? Is an action a variable with causes, or is it an intervention?</p>
</div>
<div class="readable-text intended-text" id="p39">
<p>The answer is <em>both</em>, depending on context. When it is which depends on the question we are asking and where that question sits in the causal hierarchy (discussed in chapter 10). When we are talking about what actions usually happen, such as when we are observing the actions of other agents (or even when reflecting on our own past actions) and what results those actions led to, we are reflecting on trends in population, and we are on level 1 of the causal hierarchy. In the case of our investment example, we’re reasoning about <em>P</em><em> </em>(<em>C</em>, <em>X</em>, <em>Y</em>, <em>U</em><em> </em>). But if we’re asking questions like “what would happen if I made an equity investment?” then we’re asking a level 2 question, and we need the proposed action as an intervention.</p>
</div>
<div class="readable-text intended-text" id="p40">
<p>Next, we’ll characterize common decision rules using our causal notation.</p>
</div>
<div class="readable-text" id="p41">
<h3 class="readable-text-h3" id="sigil_toc_id_295"><span class="num-string">12.2.2</span> Causal characterization of decision rules and policies</h3>
</div>
<div class="readable-text" id="p42">
<p>A decision rule is a rule for choosing an action based on the utility distribution <em>P</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>)). The agent chooses an optimal action according to a decision rule. For example, a common decision rule is choosing the action that minimizes loss or cost or maximizes utility or reward.</p>
</div>
<div class="readable-text intended-text" id="p43">
<p>In automated decision-making, the decision rule is often called a “policy.” In public health settings, decision rules are sometimes called “treatment regimes.”</p>
</div>
<div class="readable-text" id="p44">
<h4 class="readable-text-h4 sigil_not_in_toc">Maximizing expected utility</h4>
</div>
<div class="readable-text" id="p45">
<p>The most intuitive and commonly seen decision rule is to choose the action that maximizes expected utility. First, we can look at the expectation of the utility distribution. Since utility is a deterministic function of <em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>, this is just the expectation of <em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>) over the intervention distribution of <em>Y</em>.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p46">
<img alt="figure" height="127" src="../Images/ness-ch12-eqs-1x.png" width="565"/>
</div>
<div class="readable-text" id="p47">
<p>We then choose the action (value of <em>x</em>) that maximizes expected utility:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p48">
<img alt="figure" height="37" src="../Images/ness-ch12-eqs-2x.png" width="204"/>
</div>
<div class="readable-text" id="p49">
<p>In our investment example, this means choosing the investment approach that is expected to make you the most money.</p>
</div>
<div class="readable-text" id="p50">
<h4 class="readable-text-h4 sigil_not_in_toc">Minimax decision rules</h4>
</div>
<div class="readable-text" id="p51">
<p>To understand the minimax decision rule, recall that the terms “utility” and “loss” are two sides of the same coin; utility == negative loss. Let <em>L</em>(<em>y</em><em> </em>) = –<em> </em><em>U</em>(<em>y</em><em> </em>). Then a minimax decision rule is<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p52">
<img alt="figure" height="37" src="../Images/ness-ch12-eqs-3x.png" width="211"/>
</div>
<div class="readable-text" id="p53">
<p>In plain English, this means “choose the action that minimizes the maximum amount of possible loss.” In our investment example, this means choosing the investment approach that will minimize the amount of money you’d lose in the worst case scenario. There are many variants of minimax rules, but they have the same flavor—minimizing loss or maximizing utility during bad times.</p>
</div>
<div class="readable-text" id="p54">
<h4 class="readable-text-h4 sigil_not_in_toc">Softmax rules</h4>
</div>
<div class="readable-text" id="p55">
<p>A softmax decision rule randomly selects an action with a probability proportional to the resulting utility.</p>
</div>
<div class="readable-text intended-text" id="p56">
<p>Let’s define <em>C</em><em> </em>(<em>x</em><em> </em>) as the probability of choosing the action <em>x</em>. Then <em>C</em><em> </em>(<em>x</em><em> </em>) is defined as a probability value proportional to<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p57">
<img alt="figure" height="27" src="../Images/ness-ch12-eqs-4x.png" width="198"/>
</div>
<div class="readable-text" id="p58">
<p>The noise parameter <em class="obliqued">α</em> modulates between the two extremes. When <em class="obliqued">α</em>=0, we have a uniform distribution on all the choices. As <em class="obliqued">α</em> gets larger, we approach maximizing expected utility.</p>
</div>
<div class="readable-text intended-text" id="p59">
<p>Sometimes our goal is to model the decision-making of other agents, such as in inverse RL. The softmax decision rule is useful when agents don’t always make the utility-optimizing choice. The softmax decision rule provides a simple, analytically tractable, and empirically validated model of suboptimal choice.</p>
</div>
<div class="readable-text intended-text" id="p60">
<p>Another reason we might want to use the softmax rule is when there is a trade-off between <em>exploring </em>and <em>exploiting, </em>such as with bandit problems. Suppose the agent is uncertain about the shape of the distibution <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>). The optimal action according to an incorrect model of <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>) might be different from the optimal choice according to the correct model of <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>). The softmax decision rule allows us to choose various actions, get some data on the results, and use that data to update our model of <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>). When this is done in sequence, it’s often called <em>Thompson sampling</em>. </p>
</div>
<div class="readable-text intended-text" id="p61">
<p>In our investment analogy, suppose we were to invest in several businesses. Perhaps, according to our current model, equity investment maximizes expected utility, but we’re not fully confident in our current model, so we opt to select debt investment even though the current model says its less optimal. The goal is to add diversity to our dataset, so that we can learn a better model.</p>
</div>
<div class="readable-text" id="p62">
<h4 class="readable-text-h4 sigil_not_in_toc">Other types of decision rules</h4>
</div>
<div class="readable-text" id="p63">
<p>There are other types of decision rules, and they can become complicated, especially when they involve statistical estimation. For example, using <em>p</em>-values in statistical hypothesis testing involves a nuanced utility function that balances the chances of a false positive (incorrectly choosing the alternative hypothesis) and a false negative (incorrectly choosing the null hypothesis).</p>
</div>
<div class="readable-text intended-text" id="p64">
<p>Fortunately, when we work with probabilistic causal models, the math tends to be easier, and we get a nice guarantee called <em>admissibility</em>.</p>
</div>
<div class="readable-text" id="p65">
<h3 class="readable-text-h3" id="sigil_toc_id_296"><span class="num-string">12.2.3</span> Causal probabilistic decision-modeling and admissibility</h3>
</div>
<div class="readable-text" id="p66">
<p>In this section, I’ll provide a short justification for choosing a causal probabilistic modeling approach to decision-making. When you implement an automated decision-making algorithm in a production setting, you might have to explain why your implementation is better than another. In that setting, it is useful if you know if your algorithm is <em>admissible</em>.</p>
</div>
<div class="readable-text intended-text" id="p67">
<p>A decision rule is <em>admissible </em>if there are no other rules that dominate it. A decision rule dominates another rule if the performance of the former is sometimes better, and never worse, than that of the other rule with respect to the utility function. For example, the softmax decision rule is dominated by maximizing expected utility (assuming you know the true shape of <em>P</em><em> </em>(<em>Y</em><em><sub>X</sub></em><em> </em><sub>=</sub><em> </em><em><sub>x</sub></em><em> </em>)) because sometimes it will select suboptimal actions, and it is thus inadmissible. Determining <em>admissibility </em>is a key task in decision theory. </p>
</div>
<div class="readable-text intended-text" id="p68">
<p>The challenge for us occurs when we use data and statistics to deal with unknowns, such as parameters or latent variables. If we want to use data to estimate a parameter or work with latent variables, there are usually a variety of statistical approaches to choose from. If our decision-making algorithm depends on a statistical procedure, the choice of procedure can influence which action is considered optimal. How do we know if our statistical decision-making procedure is admissible?</p>
</div>
<div class="readable-text intended-text" id="p69">
<p>Probabilistic modeling libraries like Pyro leverage Bayesian inference to estimate parameters or impute latent variables. Bayesian decision theory tells us that <em>Bayes rules</em>, (not to be confused with Bayes’s rule) decision rules that optimize posterior expected utility, have an admissibility guarantee under mild regularity conditions. This means that if we use Bayesian inference in Pyro or similar libraries to calculate and optimize posterior expected loss, we have an admissibility guarantee (if those mild conditions hold, and they usually do). That means you needn’t worry that someone else’s decision-making model (that makes the same modeling assumptions, has the same utility function, and uses the same data) will beat yours.</p>
</div>
<div class="readable-text" id="p70">
<h3 class="readable-text-h3" id="sigil_toc_id_297"><span class="num-string">12.2.4</span> The deceptive alignment of argmax values of causal and non-causal expectations</h3>
</div>
<div class="readable-text" id="p71">
<p>Most conventional approaches to decision-making, including in RL, focus on maximizing <em>E</em><em> </em>(<em>U</em>(<em>Y</em><em>  </em>)|<em>X</em><em>  </em>=<em> </em><em>x</em>) rather than <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><em> </em><sub><em>x</em></sub>)). Let’s implement the model in figure 12.6 with pgmpy and compare the two approaches.</p>
</div>
<div class="readable-text intended-text" id="p72">
<p>First, we’ll build the DAG in the model. </p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p73">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Setting up your environment</h5>
</div>
<div class="readable-text" id="p74">
<p>This code was written with pgmpy version 0.1.24. See the chapter notes at <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a> for a link to the notebook that runs this code.</p>
</div>
</div>
<div class="browsable-container listing-container" id="p75">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.1</span> DAG for investment decision model</h5>
<div class="code-area-container">
<pre class="code-area">from pgmpy.models import BayesianNetwork
from pgmpy.factors.discrete import TabularCPD
from pgmpy.inference import VariableElimination
import numpy as np

model = BayesianNetwork([    <span class="aframe-location"/> #1
    ('C', 'X'),    #1
    ('C', 'Y'),     #1
    ('X', 'Y'),     #1
    ('Y', 'U')    #1
])    #1</pre>
<div class="code-annotations-overlay-container">
     #1 Set up the DAG
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p76">
<p>Next we’ll build the causal Markov kernels for <em>Economy</em> (<em>C</em><em>  </em>), <em>Debt vs. Equity </em>(<em>X</em><em>  </em>), and <em>Business Success</em> (<em>Y</em>). The causal Markov kernel for <em>Economy</em> (<em>C</em><em>  </em>) will take two values: “bear” for bad economic conditions and “bull” for good. The causal Markov kernel for <em>Debt vs. Equity</em> (<em>X</em><em>  </em>) will depend on <em>C</em>, reflecting the fact that investors tend to prefer equity in a bull economy and debt in a bear economy. <em>Success</em> (<em>Y</em><em>  </em>) depends on the economy and the choice of debt or equity investment.</p>
</div>
<div class="browsable-container listing-container" id="p77">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.2</span> Create causal Markov kernels for <em>C</em>, <em>X</em>, and <em>Y</em></h5>
<div class="code-area-container">
<pre class="code-area">cpd_c = TabularCPD(   <span class="aframe-location"/> #1
    variable='C',   #1
    variable_card=2,   #1
    values=[[0.5], [0.5]],   #1
    state_names={'C': ['bear', 'bull']}   #1
)    #1

cpd_x = TabularCPD(   <span class="aframe-location"/> #2
    variable='X',     #2
    variable_card=2,   #2
    values=[[0.8, 0.2], [0.2, 0.8]],     #2
    evidence=['C'],     #2
    evidence_card=[2],     #2
    state_names={'X': ['debt', 'equity'], 'C': ['bear', 'bull']}     #2
)    #2

cpd_y = TabularCPD(    <span class="aframe-location"/> #3
    variable='Y',     #3
    variable_card=2,    #3
    values= [[0.3, 0.9, 0.7, 0.6], [0.7, 0.1, 0.3, 0.4]],    #3
    evidence=['X', 'C'],    #3
    evidence_card=[2, 2],     #3
    state_names={     #3
        'Y': ['failure', 'success'],   #3
        'X': ['debt', 'equity'],     #3
        'C': ['bear', 'bull']     #3
    }    #3
)     #3</pre>
<div class="code-annotations-overlay-container">
     #1 Set up causal Markov kernel for C (economy). It takes two values: “bull” and “bear”.
     <br/>#2 Set up causal Markov kernel for action X, either making a debt investment or equity investment depending on the economy.
     <br/>#3 Set up causal Markov kernel for business outcome Y, either success or failure, depending on the type of investment provided (X) and the economy (C).
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p78">
<p>Finally, we’ll add the <em>Utility</em> node (<em>U</em><em>  </em>). We use probabilities of 1 and 0 to represent a deterministic function of <em>Y</em>. We end by adding all the kernels to the model.</p>
</div>
<div class="browsable-container listing-container" id="p79">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.3</span> Implement the utility node and initialize the model</h5>
<div class="code-area-container">
<pre class="code-area">cpd_u = TabularCPD(    <span class="aframe-location"/> #1
    variable='U',    #1
    variable_card=2,    #1
    values=[[1., 0.], [0., 1.]],  #1
    evidence=['Y'],   #1
    evidence_card=[2],    #1
    state_names={'U': [-1000, 99000], 'Y': ['failure', 'success']}  #1 
)    #1
print(cpd_u)    #1
model.add_cpds(cpd_c, cpd_x, cpd_y, cpd_u)   #1</pre>
<div class="code-annotations-overlay-container">
     #1 Set up the utility node.
     <br/>#2 Set up the utility node.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p80">
<p>This code prints out the following conditional probability tables for our causal Markov kernels. This one is for the <em>Utility</em> variable:</p>
</div>
<div class="browsable-container listing-container" id="p81">
<div class="code-area-container">
<pre class="code-area">+----------+------------+------------+
| Y        | Y(failure) | Y(success) |
+----------+------------+------------+
| U(-1000) | 1.0        | 0.0        |
+----------+------------+------------+
| U(99000) | 0.0        | 1.0        |
+----------+------------+------------+</pre>
</div>
</div>
<div class="readable-text" id="p82">
<p>This reflects the investor trends of favoring equity investments in a bull market and debt investments in a bear market. </p>
</div>
<div class="readable-text intended-text" id="p83">
<p>The following probability table is for the <em>Business Success</em> variable <em>Y</em><em> </em>:</p>
</div>
<div class="browsable-container listing-container" id="p84">
<div class="code-area-container">
<pre class="code-area">+------------+---------+---------+-----------+-----------+
| X          | X(debt) | X(debt) | X(equity) | X(equity) |
+------------+---------+---------+-----------+-----------+
| C          | C(bear) | C(bull) | C(bear)   | C(bull)   |
+------------+---------+---------+-----------+-----------+
| Y(failure) | 0.3     | 0.9     | 0.7       | 0.6       |
+------------+---------+---------+-----------+-----------+
| Y(success) | 0.7     | 0.1     | 0.3       | 0.4       |
+------------+---------+---------+-----------+-----------+</pre>
</div>
</div>
<div class="readable-text" id="p85">
<p>This reflects debt being a less preferred source of financing in a bear market when interest rate payments are higher, and equity being preferred in a bull market because equity is cheaper.</p>
</div>
<div class="readable-text intended-text" id="p86">
<p>Finally, the <em>Utility</em> node is a simple deterministic function that maps <em>Y</em> to utility values:</p>
</div>
<div class="browsable-container listing-container" id="p87">
<div class="code-area-container">
<pre class="code-area">+----------+------------+------------+
| Y        | Y(failure) | Y(success) |
+----------+------------+------------+
| U(-1000) | 1.0        | 0.0        |
+----------+------------+------------+
| U(99000) | 0.0        | 1.0        |
+----------+------------+------------+</pre>
</div>
</div>
<div class="readable-text" id="p88">
<p><em>Next, we’ll calculate </em><em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>)) and <em>E</em><em> </em>(<em>U</em>(<em>Y</em><em>  </em>)|<em>X</em><em> </em>=<em> </em><em>x</em><em> </em>). Before proceeding, download and load a helper function that implements an ideal intervention. To allay any security concerns of directly executing downloaded code, the code prints the downloaded script and prompts you to confirm before executing the script.</p>
</div>
<div class="browsable-container listing-container" id="p89">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.4</span> Download helper function for implementing an ideal intervention</h5>
<div class="code-area-container">
<pre class="code-area">import requests

url = "https://raw.githubusercontent.com/altdeep/causalML/master/book/pgmpy_do.py"   <span class="aframe-location"/> #1
response = requests.get(url)    #1
content = response.text    #1

print("Downloaded script content:\n")    <span class="aframe-location"/> #2
print(content)    #2
confirm = input("\nDo you want to execute this script? (yes/no): ")    #2
if confirm.lower() == 'yes':    #2
    exec(content)   #2
else:    #2
    print("Script execution cancelled.")    #2</pre>
<div class="code-annotations-overlay-container">
     #1 Load an implementation of an ideal intervention.
     <br/>#2 To allay security concerns, you can inspect the downloaded script and confirm it before running.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p90">
<p>By now, in this book, you should not be surprised that <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub><em> </em>)) is different from <em>E</em><em> </em>(<em>U</em>(<em>Y</em><em>  </em>)|<em>X</em><em> </em>=<em> </em><em>x</em><em> </em>). Let’s look at these values.</p>
</div>
<div class="browsable-container listing-container" id="p91">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.5</span> Calculate <em>E</em>(<em>U</em>(<em>Y</em>)|<em>X</em>=<em>x</em>) and <em>E</em>(<em>U</em>(<em>Y</em><em><sub>X</sub></em><sub>=</sub><em><sub>x</sub></em>))</h5>
<div class="code-area-container">
<pre class="code-area">def get_expectation(marginal):    <span class="aframe-location"/> #1
    u_values = marginal.state_names["U"]    #1
    probs = marginal.values    #1
    expectation = sum([x * p for x, p in zip(u_values, probs)])   #1
    return expectation    #1

infer = VariableElimination(model)    <span class="aframe-location"/> #2
marginal_u_given_debt = infer.query( #2
    variables=['U'], evidence={'X': 'debt'})     #2
marginal_u_given_equity = infer.query( #2
    variables=['U'], evidence={'X': 'equity'})    #2
e_u_given_x_debt = get_expectation(marginal_u_given_debt)    #2
e_u_given_x_equity = get_expectation(marginal_u_given_equity)    #2
print("E(U(Y)|X=debt)=", e_u_given_x_debt)    #2
print("E(U(Y)|X=equity)=", e_u_given_x_equity)     #2

int_model_x_debt = do(model, {"X": "debt"})    <span class="aframe-location"/> #3
infer_debt = VariableElimination(int_model_x_debt)    #3
marginal_u_given_debt = infer_debt.query(variables=['U'])    #3
expectation_u_given_debt = get_expectation(marginal_u_given_debt)    #3
print("E(U(Y_{X=debt}))=", expectation_u_given_debt)     #3
int_model_x_equity = do(model, {"X": "equity"})     #3
infer_equity = VariableElimination(int_model_x_equity)     #3
marginal_u_given_equity = infer_equity.query(variables=['U'])    #3
expectation_u_given_equity = get_expectation(marginal_u_given_equity)     #3
print("E(U(Y_{X=equity}))=", expectation_u_given_equity)     #3</pre>
<div class="code-annotations-overlay-container">
     #1 A helper function for calculating the expected utility
     <br/>#2 Set X by intervention to debt and equity and calculate the expectation of U under each intervention.
     <br/>#3 Condition on X = debt and X = equity, and calculate the expectation of U.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p92">
<p>This gives us the following conditional expected utilities (I’ve marked the highest with *):</p>
</div>
<ul>
<li class="readable-text" id="p93"> <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><em>  </em>)|<em>X</em><em> </em>=<em> </em>debt) = 57000 * </li>
<li class="readable-text" id="p94"> <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><em>  </em>)|<em>X</em><em> </em>=<em> </em>equity) = 37000 </li>
</ul>
<div class="readable-text" id="p95">
<p>It also gives us the following interventional expected utilities:</p>
</div>
<ul>
<li class="readable-text" id="p96"> <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=debt</sub>)) = 39000 * </li>
<li class="readable-text" id="p97"> <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=equity</sub>)) = 34000 </li>
</ul>
<div class="readable-text" id="p98">
<p>So <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><em>  </em>)|<em>X</em><em> </em>=<em> </em>debt) is different from <em>E</em>(<em>U</em>(<em>Y</em><sub><em>X</em></sub><sub>=debt</sub>)), and <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><em>  </em>)|<em>X</em><em> </em>=<em> </em>equity) is different from <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><em> </em><sub>equity</sub>)). However, our goal is to optimize expected utility, and in this case, debt maximizes both <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><em>  </em>)|<em>X</em><em> </em>=<em> </em>x) and <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>)).<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p99">
<img alt="figure" height="123" src="../Images/ness-ch12-eqs-5x.png" width="261"/>
</div>
<div class="readable-text" id="p100">
<p>If “debt” maximizes both queries, what is the point of causal decision theory? What does it matter if <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><em>  </em>)|<em>X</em><em> </em>=<em> </em><em>x</em>) and <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>)) are different if the optimal action for both is the same?</p>
</div>
<div class="readable-text intended-text" id="p101">
<p>In decision problems, it is quite common that a causal formulation of the problem provides the same answer as more traditional noncausal formulations. This is especially true in higher dimensional problems common in RL. You might observe this and wonder why the causal formulation is needed at all.</p>
</div>
<div class="readable-text intended-text" id="p102">
<p>To answer, watch what happens when we make a slight change to the parameters of <em>Y</em> in the model. Specifically, we’ll change the parameter for <em>P</em><em> </em>(<em>Y</em><em> </em>=<em> </em>success|<em>X</em><em> </em>=<em> </em>equity, <em>C</em><em> </em>=<em> </em>bull) from .4 to .6. First, we’ll rebuild the model with the parameter change.</p>
</div>
<div class="browsable-container listing-container" id="p103">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.6</span> Change a parameter in the causal Markov kernel for <em>Y</em></h5>
<div class="code-area-container">
<pre class="code-area">model2 = BayesianNetwork([  <span class="aframe-location"/> #1
    ('C', 'X'),    #1
    ('C', 'Y'),    #1
    ('X', 'Y'),    #1
    ('Y', 'U')     #1
])

cpd_y2 = TabularCPD(   <span class="aframe-location"/> #2
    variable='Y',
    variable_card=2,
    values=[[0.3, 0.9, 0.7, 0.4],  [0.7, 0.1, 0.3, 0.6]],  <span class="aframe-location"/> #3
    evidence=['X', 'C'],
    evidence_card=[2, 2],
    state_names={
        'Y': ['failure', 'success'],
        'X': ['debt', 'equity'],
        'C': ['bear', 'bull']
    }
)

model2.add_cpds(cpd_c, cpd_x, cpd_y2, cpd_u)  <span class="aframe-location"/> #4</pre>
<div class="code-annotations-overlay-container">
     #1 Initialize a new model.
     <br/>#2 Create a new conditional probability distribution for Y.
     <br/>#3 Change the parameter P(Y=success|X=equity, C=bull) = 0.4 (the last parameter in the first list) to 0.6.
     <br/>#4 Add the causal Markov kernels to the model.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p104">
<p>Next, we rerun inference.</p>
</div>
<div class="browsable-container listing-container" id="p105">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.7</span> Compare outcomes with changed parameters</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">infer = VariableElimination(model2)   <span class="aframe-location"/> #1
marginal_u_given_debt = infer.query(variables=['U'],    #1
<span class="">↪</span>evidence={'X': 'debt'})    #1
marginal_u_given_equity = infer.query(variables=['U'],   #1
<span class="">↪</span>evidence={'X': 'equity'})    #1
e_u_given_x_debt = get_expectation(marginal_u_given_debt)   #1
e_u_given_x_equity = get_expectation(marginal_u_given_equity)     #1
print("E(U(Y)|X=debt)=", e_u_given_x_debt)   #1
print("E(U(Y)|X=equity)=", e_u_given_x_equity)   #1

int_model_x_debt = do(model2, {"X": "debt"})   <span class="aframe-location"/> #2
infer_debt = VariableElimination(int_model_x_debt)    #2
marginal_u_given_debt = infer_debt.query(variables=['U'])   #2
expectation_u_given_debt = get_expectation(marginal_u_given_debt)    #2
print("E(U(Y_{X=debt}))=", expectation_u_given_debt)     #2
int_model_x_equity = do(model2, {"X": "equity"})     #2
infer_equity = VariableElimination(int_model_x_equity)    #2
marginal_u_given_equity = infer_equity.query(variables=['U'])    #2
expectation_u_given_equity = get_expectation(marginal_u_given_equity)     #2
print("E(U(Y_{X=equity}))=", expectation_u_given_equity)    #2</pre>
<div class="code-annotations-overlay-container">
     #1 Set X by intervention to debt and equity, and calculate the expectation of U under each intervention.
     <br/>#2 Condition on X = debt and X = equity, and calculate the expectation of U.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p106">
<p>This gives us the following conditional expectations (* indicates the optimal choice):</p>
</div>
<ul>
<li class="readable-text" id="p107"> <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><em>  </em>)|<em>X</em><em> </em>=<em> </em>debt) = 57000 * </li>
<li class="readable-text" id="p108"> <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><em>  </em>)|<em>X</em><em> </em>=<em> </em>equity) = 53000 </li>
</ul>
<div class="readable-text" id="p109">
<p>It also gives us the following interventional expectations:</p>
</div>
<ul>
<li class="readable-text" id="p110"> <em>E</em><em> </em>(<em>U</em>(<em>Y</em><sub><em>X</em></sub><sub>=debt</sub><em> </em>)) = 39000 </li>
<li class="readable-text" id="p111"> <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=equity</sub>)) = 44000 * </li>
</ul>
<div class="readable-text" id="p112">
<p>With that slight change in a single parameter, “debt” is still the optimal value of <em>x</em> in <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><em>  </em>)|<em>X</em>=<em>x</em><em> </em>), but now “equity” is the optimal value of <em>x</em> in <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>)). This is a case where the causal answer and the answer from conditioning on evidence are different. Since we are trying to answer a level 2 query, the causal approach is the right approach.</p>
</div>
<div class="readable-text intended-text" id="p113">
<p>This means that while simply optimizing a conditional expectation often gets you the right answer, you are vulnerable to getting the wrong answer in certain circumstances. Compare this to our discussion of semi-supervised learning in chapter 4—often the unlabeled data can help with learning, but, in specific circumstances, the unlabeled data adds no value. Causal analysis helped us characterize those circumstances in precise terms. Similarly, in this case, there are specific scenarios where the causal formulation of the problem will lead to a different and more correct result relative to the traditional noncausal formulation. Even the most popular decision-optimization algorithms, including the deep learning-based approaches used in deep RL, can improve performance by leveraging the causal structure of a decision problem.</p>
</div>
<div class="readable-text intended-text" id="p114">
<p>Next, we’ll see another example with Newcomb’s paradox.</p>
</div>
<div class="readable-text" id="p115">
<h3 class="readable-text-h3" id="sigil_toc_id_298"><span class="num-string">12.2.5</span> Newcomb’s paradox</h3>
</div>
<div class="readable-text" id="p116">
<p>A famous thought experiment called Newcomb’s paradox contrasts the causal approach to decision theory, maximizing utility under intervention, with the conventional approach of maximizing utility conditional on some action. We’ll look at an AI-inspired version of this thought experiment in this section, and the next section will show how to approach it with a formal causal model.</p>
</div>
<div class="readable-text intended-text" id="p117">
<p>There are two boxes designated A and B as shown in figure 12.7. Box A always contains $1,000. Box B contains either $1,000,000 or $0. The decision-making agent must choose between taking only box B or <em>both </em>boxes. The agent does not know what is in box B until they decide. Given this information, it is obvious the agent should take both boxes—choosing both yields either $1,000 or $1,001,000, while choosing only B yields either $0 or $1,000,000.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p118">
<img alt="figure" height="148" src="../Images/CH12_F07_Ness.png" width="301"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.7</span> An illustration of the boxes in Newcomb's paradox</h5>
</div>
<div class="readable-text intended-text" id="p119">
<p>Now, suppose there is an AI that can predict with high accuracy what choice the agent intends to make. If the AI predicts that the agent intends to take both boxes, it will put no money in box B. If the AI is correct and the agent takes both boxes, the agent only gets $1,000. However, if the AI predicts that the agent intends to take only box B, it will put $1,000,000 in box B. If the AI predicts correctly, the agent gets the $1,000,000 in box B but not the $1,000 in box A. The agent does not know for sure what the AI predicted or what box B contains until they make their choice.</p>
</div>
<div class="readable-text intended-text" id="p120">
<p>The traditional paradox arises as follows. A causality-minded agent reasons that the actions of the AI are out of their control. They only focus on what they can control—the causal consequences of their choice. They can’t <em>cause</em> the content of box B, so they pick both boxes on the off-chance box B has the million, just as one would if the AI didn’t exist. But if the agent knows how the AI works, doesn’t it make more sense to choose only box B and get the million with certainty? </p>
</div>
<div class="readable-text intended-text" id="p121">
<p>Let’s dig in further by enumerating the possible outcomes and their probabilities. Let’s assume the AI’s predictions are 95% accurate. If the agent chooses both boxes, there is a 95% chance the AI will have guessed the agent’s choice and put no money in B, in which case the agent only gets the $1,000. There is a 5% chance the algorithm will guess wrong, in which case it puts 1,000,000 in box B, and the agent wins $1,001,000. If the agent chooses only box B, there is a 95% chance the AI will have predicted the choice and placed $1,000,000 in box B, giving the agent $1,000,000 in winnings. There is a 5% chance it will not, and the agent will take home nothing. We see these outcomes in table 12.1. The expected utility calculations are shown in table 12.2.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p122">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 12.1</span> Newcomb’s problem outcomes and their probabilities</h5>
<table>
<thead>
<tr>
<th>
<div>
        Strategy
       </div></th>
<th>
<div>
        AI action
       </div></th>
<th>
<div>
        Winnings
       </div></th>
<th>
<div>
        Probability
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td> Choose both<br/></td>
<td> Put $0 in box B<br/></td>
<td> $1,000<br/></td>
<td> .95<br/></td>
</tr>
<tr>
<td> Choose both<br/></td>
<td> Put $1,000,000 in box B<br/></td>
<td> $1,001,000<br/></td>
<td> .05<br/></td>
</tr>
<tr>
<td> Choose only box B<br/></td>
<td> Put $1,000,000 in box B<br/></td>
<td> $1,000,000<br/></td>
<td> .95<br/></td>
</tr>
<tr>
<td> Choose only box B<br/></td>
<td> Put $0 in box B<br/></td>
<td> $0<br/></td>
<td> .05<br/></td>
</tr>
</tbody>
</table>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p123">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 12.2</span> Expected utility of each choice in Newcomb’s problem</h5>
<table>
<thead>
<tr>
<th>
<div>
        Strategy (
        <em>x</em>)
       </div></th>
<th>
<div>
<em>E</em>(
        <em>U</em>|
        <em>X</em>=
        <em>x</em>)
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td> Choose both<br/></td>
<td> 1,000 <span class="regular-symbol">×</span> .95 + 1,001,000 <span class="regular-symbol">×</span> .05 = $51,000<br/></td>
</tr>
<tr>
<td> Choose only box B<br/></td>
<td> 1,000,000 <span class="regular-symbol">×</span> .05 + 0 <span class="regular-symbol">×</span> .05 = $950,000<br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p124">
<p>The conventional approach suggests choosing box only box B.</p>
</div>
<div class="readable-text intended-text" id="p125">
<p>When the paradox was created, taking a causal approach to the problem meant only attending to the causal consequences of one’s actions. Remember that the AI makes the prediction <em>before </em>the agent acts. Since effects cannot precede causes in time, the AI’s behavior is not a consequence of the agent’s actions, so the agent with the causal view ignores the AI and goes with the original strategy of choosing both boxes. </p>
</div>
<div class="readable-text intended-text" id="p126">
<p>It would seem that the agent with the causal view is making an error in failing to account for the actions of the AI. But we can resolve this error by having the agent use a formal causal model.</p>
</div>
<div class="readable-text" id="p127">
<h3 class="readable-text-h3" id="sigil_toc_id_299"><span class="num-string">12.2.6</span> Newcomb’s paradox with a causal model</h3>
</div>
<div class="readable-text" id="p128">
<p>In the traditional formulation of Newcomb’s paradox, the assumption is that the agent using causal decision theory only attends to the consequences of their actions—they are reasoning on the causal DAG in figure 12.8. But the true data generating process (DGP) is better captured by figure 12.9.</p>
</div>
<div class="browsable-container figure-container" id="p129">
<img alt="figure" height="144" src="../Images/CH12_F08_Ness.png" width="211"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.8</span> Newcomb’s paradox assumes a version of causal decision theory where a naive agent uses this incorrect causal DAG.</h5>
</div>
<div class="browsable-container figure-container" id="p130">
<img alt="figure" height="144" src="../Images/CH12_F09_Ness.png" width="385"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.9</span> A better causal DAG representing the framing of Newcomb’s paradox</h5>
</div>
<div class="readable-text" id="p131">
<p>The choice of the agent can’t <em>cause </em>the AI’s prediction, because the prediction happens first. Thus, we assume the AI agent is inferring the agent’s <em>intent</em>, and thus the intent of the agent is the cause of the AI’s prediction. </p>
</div>
<div class="readable-text intended-text" id="p132">
<p>The causal decision-making agent would prefer the graph in figure 12.9 because it is a better representation of the DGP. The clever agent wouldn’t focus on maximizing <em>E</em><em> </em>(<em>U</em><sub><em>choice</em></sub><sub>=</sub><sub><em>x</em></sub>). The clever agent is aware of its own intention, and knowing that this intention is a cause of the content of box B, it focuses on optimizing <em>E</em><em> </em>(<em>U</em><sub><em>choice</em></sub><sub>=</sub><sub><em>x</em></sub>|<em>intent</em><em>  </em>=<em>i</em><em> </em>), where <em>i</em> is their original intention of which box to pick.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p133">
<img alt="figure" height="37" src="../Images/ness-ch12-eqs-6x.png" width="310"/>
</div>
<div class="readable-text" id="p134">
<p>We’ll assume the agent’s initial intention is an impulse it cannot control. But while they can’t control their initial intent, they can do some introspection and become aware of this intent. Further, we’ll assume that upon doing so, they have the ability to change their choice to something different from what it initially intended, after the AI has made their prediction and set the contents of box B. Let’s model this system in pgmpy and evaluate maximizing <em>E</em><em> </em>(<em>U</em><sub><em>choice</em></sub><sub>=</sub><sub><em>x</em></sub>|<em>intent</em><em>  </em>=<em>i</em><em> </em>).</p>
</div>
<div class="readable-text intended-text" id="p135">
<p>First, let’s build the DAG.</p>
</div>
<div class="browsable-container listing-container" id="p136">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.8</span> Create the DAG</h5>
<div class="code-area-container">
<pre class="code-area">model = BayesianNetwork(    
    [    
        ('intent', 'AI prediction'),    
        ('intent', 'choice'),    
        ('AI prediction', 'box B'),    
        ('choice', 'U'),    
        ('box B', 'U'),    
    ]    
)</pre>
</div>
</div>
<div class="readable-text" id="p137">
<p>Next, we’ll create causal Markov kernels for intent and choice.</p>
</div>
<div class="browsable-container listing-container" id="p138">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.9</span> Create causal Markov kernels for intent and choice</h5>
<div class="code-area-container">
<pre class="code-area">cpd_intent = TabularCPD(    <span class="aframe-location"/> #1
    'intent', 2, [[0.5], [0.5]],    #1
    state_names={'intent': ['B', 'both']}    #1
)   #1
print(cpd_intent)

cpd_choice = TabularCPD(   <span class="aframe-location"/> #2
    'choice', 2, [[1, 0], [0, 1]],    #2
    evidence=['intent'],     #2
    evidence_card=[2],    #2
    state_names={     #2
        'choice': ['B', 'both'],     #2
        'intent': ['B', 'both']   #2
    }     #2
)   #2
print(cpd_choice)</pre>
<div class="code-annotations-overlay-container">
     #1 We assume a 50-50 chance the agent will prefer both boxes vs. box B.
     <br/>#2 We assume the agent’s choice is deterministically driven by their intent.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p139">
<p>Similarly, we’ll create the causal Markov kernels for the AI’s decision and the content of box B.</p>
</div>
<div class="browsable-container listing-container" id="p140">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.10</span> Create causal Markov kernels for AI prediction and box B content</h5>
<div class="code-area-container">
<pre class="code-area">cpd_AI = TabularCPD(    <span class="aframe-location"/> #1
    'AI prediction', 2, [[.95, 0.05], [.05, .95]],   #1
    evidence=['intent'],   #1
    evidence_card=[2],   #1
    state_names={    #1
        'AI prediction': ['B', 'both'],    #1
        'intent': ['B', 'both']    #1
    }    #1
)    #1
print(cpd_AI)

cpd_box_b_content = TabularCPD(   <span class="aframe-location"/> #2
    'box B', 2, [[0, 1], [1, 0]],    #2
    evidence=['AI prediction'],    #2
    evidence_card=[2],    #2
    state_names={    #2
        'box B': [0, 1000000],    #2
        'AI prediction': ['B', 'both']   #2
    }     #2
)    #2
print(cpd_box_b_content)</pre>
<div class="code-annotations-overlay-container">
     #1 The AI’s prediction is 95% accurate.
     <br/>#2 Box B contents are set deterministically by the AI’s prediction.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p141">
<p>Finally, we’ll create a causal Markov kernel for utility and add all the kernels to the model.</p>
</div>
<div class="browsable-container listing-container" id="p142">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.11</span> Create utility kernel and build the model</h5>
<div class="code-area-container">
<pre class="code-area">cpd_u = TabularCPD(   <span class="aframe-location"/> #1
    'U', 4,    #1
    [    #1
        [1, 0, 0, 0],  #1
        [0, 1, 0, 0],   #1
        [0, 0, 1, 0],   #1
        [0, 0, 0, 1],    #1
    ],   #1
    evidence=['box B', 'choice'],    #1
    evidence_card=[2, 2],    #1
    state_names={    #1
        'U': [0, 1000, 1000000, 1001000],    #1
        'box B': [0, 1000000],    #1
        'choice': ['B', 'both']    #1
    }   #1
)    #1
print(cpd_u)

model.add_cpds(cpd_intent, cpd_choice, cpd_AI, cpd_box_b_content, cpd_u) <span class="aframe-location"/> #2</pre>
<div class="code-annotations-overlay-container">
     #1 Set up the utility node.
     <br/>#2 Build the model.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p143">
<p>Now we’ll evaluate maximizing <em>E</em>(<em>U</em><sub><em>choice</em></sub><sub>=</sub><sub><em>x</em></sub>|<em>intent</em>=<em>i</em>).</p>
</div>
<div class="browsable-container listing-container" id="p144">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 12.12</span> Infer optimal choice using intervention and conditioning on intent</h5>
<div class="code-area-container">
<pre class="code-area">int_model_x_both = do(model, {"choice": "both"})   <span class="aframe-location"/> #1
infer_both = VariableElimination(int_model_x_both)   #1
marginal_u_given_both = infer_both.query(   #1
    variables=['U'], evidence={'intent': 'both'})    #1
expectation_u_given_both = get_expectation(marginal_u_given_both)    #1
print("E(U(Y_{choice=both}|intent=both))=", expectation_u_given_both)    #1
int_model_x_box_B = do(model, {"choice": "B"})    <span class="aframe-location"/> #2
infer_box_B = VariableElimination(int_model_x_box_B)    #2
marginal_u_given_box_B = infer_box_B.query(    #2
    variables=['U'], evidence={'intent': 'both'})     #2
expectation_u_given_box_B = get_expectation(marginal_u_given_box_B)    #2
print("E(U(Y_{choice=box B}|intent=both))=", expectation_u_given_box_B)    #2
int_model_x_both = do(model, {"choice": "both"})    <span class="aframe-location"/> #3
infer_both = VariableElimination(int_model_x_both)     #3
marginal_u_given_both = infer_both.query(     #3
    variables=['U'], evidence={'intent': 'B'})    #3
expectation_u_given_both = get_expectation(marginal_u_given_both)     #3
print("E(U(Y_{choice=both}|intent=B))=", expectation_u_given_both)     #3
int_model_x_box_B = do(model, {"choice": "B"})    <span class="aframe-location"/> #4
infer_box_B = VariableElimination(int_model_x_box_B)  #4
marginal_u_given_box_B = infer_box_B.query(    #4
    variables=['U'], evidence={'intent': 'B'}) #4
expectation_u_given_box_B = get_expectation(marginal_u_given_box_B)     #4
print("E(U(Y_{choice=box B}|intent=B))=", expectation_u_given_box_B)     #4</pre>
<div class="code-annotations-overlay-container">
     #1 Infer E(U(Y
     <sub>choice=both</sub>|intent=both)).
     <br/>#2 Infer E(U(Y
     <sub>choice=box B</sub>|intent=both)).
     <br/>#3 Infer E(U(Y
     <sub>choice=both</sub>|intent=B)).
     <br/>#4 Infer E(U(Y
     <sub>choice=box B</sub>|intent=B)).
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p145">
<p>This code produces the following results (* indicates the optimal choice for a given intent):</p>
</div>
<ul>
<li class="readable-text" id="p146"> <em>E</em>(<em>U</em>(<em>Y</em><sub><em>choice</em></sub><sub>=both</sub>|<em>intent</em><em> </em>=both)) = 51000 * </li>
<li class="readable-text" id="p147"> <em>E</em>(<em>U</em>(<em>Y</em><sub><em>choice</em></sub><sub>=box B</sub>|<em>intent</em><em> </em>=both)) = 50000 </li>
<li class="readable-text" id="p148"> <em>E</em>(<em>U</em>(<em>Y</em><sub><em>choice</em></sub><sub>=both</sub>|<em>intent</em><em> </em>=B)) = 951000 * </li>
<li class="readable-text" id="p149"> <em>E</em>(<em>U</em>(<em>Y</em><sub><em>choice</em></sub><sub>=box B</sub>|<em>intent</em><em> </em>=B)) = 950000 </li>
</ul>
<div class="readable-text" id="p150">
<p>When the agent’s initial intention is to select both, the best choice is to select both. When the agent intends to choose only box B, the best choice is to ignore those intentions and choose both. Either way, the agent should choose both. Note that when the agent initially intends to choose only box B, switching to both boxes gives them an expected utility of $951,000 which is greater than the optimal choice utility of $950,000 in the noncausal approach.</p>
</div>
<div class="readable-text intended-text" id="p151">
<p>The agent, unfortunately, cannot control their initial intent; if they could, they would deliberately ‘intend’ to pick box B and then switch at the last minute to choosing both boxes after the AI placed the million in box B. However, they can engage in a form of introspection, factoring their initial intent into their decision and, in so doing, accounting for the AI’s behavior rather than ignoring it.</p>
</div>
<div class="readable-text" id="p152">
<h3 class="readable-text-h3" id="sigil_toc_id_300"><span class="num-string">12.2.7</span> Introspection in causal decision theory</h3>
</div>
<div class="readable-text" id="p153">
<p>Newcomb’s problem illustrates a key capability of causal decision theory—the ability for us to include introspection as part of the DGP.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p154">
<img alt="figure" height="222" src="../Images/CH12_F10_Ness.png" width="277"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.10</span> Often our actions are simply reactions to our environment, rather than the result of deliberate decision-making.</h5>
</div>
<div class="readable-text intended-text" id="p155">
<p> To illustrate, consider that often our actions are simply <em>reactions </em>to our environment, as in figure 12.10.</p>
</div>
<div class="readable-text" id="p156">
<p> For example, you might have purchased a chocolate bar <em>because </em>you were hungry and it was positioned to tempt you as you waited in the checkout aisle of the grocery store. Rather than go through some deliberative decision-making process, you had a simple, perhaps even unconscious, <em>reaction </em>to your craving and an easy way to satisfy it.</p>
</div>
<div class="readable-text" id="p157">
<p>However, humans are capable of introspection—observing and thinking about their internal states. A human might consider their normal reactive behavior as part of the DGP. This introspection is illustrated in figure 12.11.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p158">
<img alt="figure" height="485" src="../Images/CH12_F11_Ness.png" width="677"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.11</span> Humans and some other agents can think about a DGP that includes them as a component of that process.</h5>
</div>
<div class="readable-text" id="p159">
<p>Through this introspection, the agent can perform level 2 hierarchical reasoning about what would happen if they did not react as usual but acted deliberately (e.g., sticking to their diet and not buying the chocolate bar), as in figure 12.12.</p>
</div>
<div class="browsable-container figure-container" id="p160">
<img alt="figure" height="615" src="../Images/CH12_F12_Ness.png" width="928"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.12</span> The agent reasons about a DGP that includes them as a component. They then use that reasoning in asking level 2 “what would happen if...” questions about that process. </h5>
</div>
<div class="readable-text intended-text" id="p161">
<p>In many cases, the agent may not know the full state of their environment. However, if the agent can disentangle their urge to react a certain way from their action, they can use that “urge” as evidence in deliberative decision-making, as in figure 12.13.</p>
</div>
<div class="browsable-container figure-container" id="p162">
<img alt="figure" height="618" src="../Images/CH12_F13_Ness.png" width="925"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.13</span> The agent may not know the states of other variables in the environment, but through introspection, they may have an intuition about those variables. That intuition can be used as evidence in conditional causal inferences.</h5>
</div>
<div class="readable-text" id="p163">
<p>We saw this pattern in the Newcomb example; the agent does not know what the AI has predicted, but, through introspection, they can use their initial intention to choose both boxes as <em>evidence </em>of what the AI has chosen.</p>
</div>
<div class="readable-text intended-text" id="p164">
<p>Was there ever a time where you noticed you had started to make clumsy errors in your work and used that as evidence that you were fatigued, even though you didn’t feel so, and you thought, “what if I take a break?” Have you had a gut feeling that something was off, despite not knowing what, and based on this feeling started to make different decisions? Causal modeling, particularly with causal generative models, make it easy to write algorithms that capture this type of self-introspection in decision-making.</p>
</div>
<div class="readable-text intended-text" id="p165">
<p>Next, we’ll look at causal modeling of sequential decision-making.</p>
</div>
<div class="readable-text" id="p166">
<h2 class="readable-text-h2" id="sigil_toc_id_301"><span class="num-string">12.3</span> Causal DAGs and sequential decisions</h2>
</div>
<div class="readable-text" id="p167">
<p>Sequential decision processes are processes of back-to-back decision-making. These processes can involve sequential decisions made by humans or by algorithms and engineered agents.</p>
</div>
<div class="readable-text intended-text" id="p168">
<p>When I model decision processes in sequence, I use a subscript to indicate a discrete step in the series, such as <em>Y</em><sub>1</sub>, <em>Y</em><sub>2</sub>, <em>Y</em><sub>3</sub>. When I want to indicate an intervention subscript, I’ll place it to the right of the time-step subscript, as in <em>Y</em><sub>1,</sub><em> </em><sub><em>X</em></sub><sub>=</sub><em> </em><sub><em>x</em></sub>, <em>Y</em><sub>2,</sub><sub><em>X</em></sub><sub>=</sub><em> </em><sub><em>x</em></sub>, <em>Y</em><sub>3,</sub><em> </em><sub><em>X</em></sub><sub>=</sub><em> </em><sub><em>x</em></sub>.</p>
</div>
<div class="readable-text intended-text" id="p169">
<p>In this section, I’ll show causal DAGs for several canonical sequential decision-making processes, but you should view these as templates, not as fixed structures. You can add or remove edges in whatever way you deem appropriate for a given problem.</p>
</div>
<div class="readable-text intended-text" id="p170">
<p>Let’s look at the simplest case, bandit feedback.</p>
</div>
<div class="readable-text" id="p171">
<h3 class="readable-text-h3" id="sigil_toc_id_302"><span class="num-string">12.3.1</span> Bandit feedback</h3>
</div>
<div class="readable-text" id="p172">
<p><em>Bandit feedback</em> refers to cases where, at each step in the sequence, there is an act <em>X</em> that leads to an outcome <em>Y</em>, with some utility <em>U</em><em> </em>(<em>Y</em><em>  </em>). A bandit sequence has two key features. The first is that, at every step, there is instant feedback after an act occurs. The second is independent trials, meaning that the variables at the <em>t</em><em>  </em><sup>th</sup> timestep are independent of variables at other timesteps. The term “bandit” comes from an analogy to “one-armed bandits,” which is a slang term for casino slot machines that traditionally have an arm that the player pulls to initiate gameplay. Slot machine gameplay provides bandit feedback—you deposit a token, pull the arm, and instantly find out if you win or lose. That outcome is independent of previous plays. </p>
</div>
<div class="readable-text intended-text" id="p173">
<p>We can capture bandit feedback with the causal DAG in figure 12.14.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p174">
<img alt="figure" height="146" src="../Images/CH12_F14_Ness.png" width="505"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.14</span> A causal DAG illustrating simple bandit feedback</h5>
</div>
<div class="readable-text" id="p175">
<p>The causal DAG in figure 12.14 captures instant feedback with a utility node at each timestep, and with a lack of edges, reflecting an independence of variables across timesteps.</p>
</div>
<div class="readable-text" id="p176">
<h3 class="readable-text-h3" id="sigil_toc_id_303"><span class="num-string">12.3.2</span> Contextual bandit feedback</h3>
</div>
<div class="readable-text" id="p177">
<p>In contextual bandit feedback, one or more variables are common causes for both the act and the outcome. In figure 12.15, the context variable <em>C</em> is common to each {<em>X</em>, <em>Y</em><em>  </em>} tuple in the sequence. In this case, the context variable <em>C</em> could represent the profile of a particular individual, and the act variable <em>X</em> is that user’s behavior.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p178">
<img alt="figure" height="239" src="../Images/CH12_F15_Ness.png" width="505"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.15</span> A causal DAG illustrating contextual bandit feedback</h5>
</div>
<div class="readable-text" id="p179">
<p>Alternatively, the context variable could change at each step, as in figure 12.16.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p180">
<img alt="figure" height="238" src="../Images/CH12_F16_Ness.png" width="505"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.16</span> A causal DAG illustrating contextual bandit feedback where the context changes at each timestep</h5>
</div>
<div class="readable-text" id="p181">
<p>We can vary this template in different ways. For example, we could have the actions drive the context variables in the next timestep, as in figure 12.17. The choice depends on your specific problem.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p182">
<img alt="figure" height="238" src="../Images/CH12_F17_Ness.png" width="505"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.17</span> A causal DAG where the action at one timestep influences the context at the next timestep</h5>
</div>
<div class="readable-text" id="p183">
<h3 class="readable-text-h3" id="sigil_toc_id_304"><span class="num-string">12.3.3</span> Delayed feedback</h3>
</div>
<div class="readable-text" id="p184">
<p>In a delayed-feedback setting, the outcome variable and corresponding utility are no longer instant feedback. Instead, they come at the end of a sequence. Let’s consider an example where a context variable drives the acts. The acts affect the next instance of the context variable. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p185">
<img alt="figure" height="238" src="../Images/CH12_F18_Ness.png" width="505"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.18</span> Example of a causal DAG for sequential decision making with delayed feedback</h5>
</div>
<div class="readable-text intended-text" id="p186">
<p> Again, figure 12.18 shows an example of this approach based on the previous model. Here the act at time <em>k</em> influences the context variable (<em>C</em><em>  </em>) at time <em>k</em> + 1, which in turn affects the act at time <em>k</em> + 1.</p>
</div>
<div class="readable-text" id="p187">
<p>Consider a case of chronic pain. Here the context variable represents whether a subject is experiencing pain (<em>C</em><em>  </em>). The presence of pain drives the act of taking a painkiller (<em>X</em><em>  </em>). Taking the painkiller (or not) affects whether there is pain in the next step. Figure 12.19 illustrates this DAG.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p188">
<img alt="figure" height="282" src="../Images/CH12_F19_Ness.png" width="505"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.19</span> A causal DAG representing the treatment of chronic pain</h5>
</div>
<div class="readable-text" id="p189">
<p><em>Y</em> here is the ultimate health outcome of the subject, and it is driven both by the overall amount of pain over time, and the amount of drugs the subject took (because perhaps overuse of painkillers has a detrimental health effect).</p>
</div>
<div class="readable-text" id="p190">
<h3 class="readable-text-h3" id="sigil_toc_id_305"><span class="num-string">12.3.4</span> Causal queries on a sequential model</h3>
</div>
<div class="readable-text" id="p191">
<p>We may want to calculate some causal query for our sequential decision problem. For example, given the DAG in figure 12.19, we might want to calculate the causal effect of <em>X</em><sub>0</sub> on <em>U</em>(<em>Y</em><em>  </em>): <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p192">
<img alt="figure" height="27" src="../Images/ness-ch12-eqs-7x.png" width="259"/>
</div>
<div class="readable-text" id="p193">
<p>Or perhaps we might be interested in the causal effect of the full sequence of acts on <em>U</em>(<em>Y</em><em>  </em>):<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p194">
<img alt="figure" height="27" src="../Images/ness-ch12-eqs-8x.png" width="464"/>
</div>
<div class="readable-text" id="p195">
<p>Either way, now that we have framed the sequential problem as a causal model, we are in familiar territory; we can simply use the causal inference tools we’ve learned in previous chapters to answer causal queries with this model.</p>
</div>
<div class="readable-text intended-text" id="p196">
<p>As usual, we must be attentive to the possibility of latent causes that can confound our causal inference. In the case of causal effects, our concern is latent common confounding causes between acts (<em>X</em><em>  </em>) and outcomes (<em>Y</em><em>  </em>), or alternatively between acts (<em>X</em><em> </em>) and utilities (<em>U</em><em>  </em>). Figure 12.20 is the same as figure 12.15, except it introduces a latent <em>Z</em> confounder.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p197">
<img alt="figure" height="239" src="../Images/CH12_F20_Ness.png" width="505"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.20</span> Contextual bandit with a latent confounder</h5>
</div>
<div class="readable-text" id="p198">
<p>Similarly, we could have a unique confounder at every timestep, as in figure 12.21.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p199">
<img alt="figure" height="239" src="../Images/CH12_F21_Ness.png" width="595"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.21</span> Bandit feedback with a different context and latent confounders at each timestep</h5>
</div>
<div class="readable-text" id="p200">
<p>Similarly, figure 12.22 shows a second version of the chronic pain graph where the confounders affect each other and the context variables. This confounder could be some external factor in the subject’s environment that triggers the pain and affects well-being.</p>
</div>
<div class="readable-text intended-text" id="p201">
<p>These confounders become an issue when we want to infer the causal effect of a sequence of actions on <em>U</em><em> </em>(<em>Y</em><em>  </em>).</p>
</div>
<div class="browsable-container figure-container" id="p202">
<img alt="figure" height="339" src="../Images/CH12_F22_Ness.png" width="494"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.22</span> A version of the chronic pain DAG where the confounders affect each other and the context variables</h5>
</div>
<div class="readable-text" id="p203">
<p>Next, we’ll look at how we can view policies for automatic decision making in sequential decision-making processes as stochastic interventions.</p>
</div>
<div class="readable-text" id="p204">
<h2 class="readable-text-h2" id="sigil_toc_id_306"><span class="num-string">12.4</span> Policies as stochastic interventions</h2>
</div>
<div class="readable-text" id="p205">
<p>In automated sequential decision-making, the term “policy” is preferred to “decision rule.” I’ll introduce a special notation for a policy: <em class="obliqued">π</em>(.). It will be a function that takes in observed outcomes of other variables and returns an action.</p>
</div>
<div class="readable-text intended-text" id="p206">
<p> To consider how a policy affects the model, we’ll contrast the DAG before and after a policy is implemented. Figure 12.23 illustrates a simple example with a context variable <em>C</em> and a latent variable <em>Z</em>. The policy uses context <em>C</em> to select a value of <em>X</em>.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p207">
<img alt="figure" height="200" src="../Images/CH12_F23_Ness.png" width="459"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.23</span> The dashed lines show edges modulated by the policy. The policy breaks the influence of the confounder <em>Z</em> like an ideal intervention, but dependence on <em>C</em> remains through the policy.</h5>
</div>
<div class="readable-text" id="p208">
<p><strong> </strong>The policy is a type of stochastic intervention; it selects a intervention value for <em>X</em> from some process that depends on <em>C</em>. Like an ideal intervention, it changes the graph. The left of figure 12.23 shows the DAG prior to deployment of the policy. On the right is the DAG after the policy is deployed. I add a special policy node to the graph to illustrate how the policy modulates the graph. The dashed edges highlight edges modulated by the policy. Just like an ideal intervention, the policy-generated intervention removes <em>X</em>’s original incoming edges <em>C</em><em>  </em>→<em>X</em> and <em>Z</em><em>  </em>→<em>X</em>. However, because the policy depends on <em>C</em>, the dashed edges illustrate the new flow of influence from <em>C </em>to <em>X</em>. </p>
</div>
<div class="readable-text intended-text" id="p209">
<p>Suppose we are interested in what value <em>Y</em> would have for a policy-selected action <em>X</em><em> </em>=<em> </em><span class="regular-symbol">Π</span>. In counterfactual notation, we write <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p210">
<img alt="figure" height="25" src="../Images/ness-ch12-eqs-9x.png" width="117"/>
</div>
<div class="readable-text" id="p211">
<p>In sequence settings, the policy applies a stochastic intervention at multiple steps in the sequence. From a possible worlds perspective, each intervention induces a new hypothetical world. This can stretch the counterfactual notation a bit, so going forward, I’ll simplify the counterfactual notation to look like this:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p212">
<img alt="figure" height="24" src="../Images/ness-ch12-eqs-10x.png" width="97"/>
</div>
<div class="readable-text" id="p213">
<p>This means <em>Y</em><sub>3</sub> (<em>Y</em> at timestep 3) is under influence of the policy’s outcomes at times 0, 1, and 2.</p>
</div>
<div class="readable-text" id="p214">
<h3 class="readable-text-h3" id="sigil_toc_id_307"><span class="num-string">12.4.1</span> Examples in sequential decision-making</h3>
</div>
<div class="readable-text" id="p215">
<p>In the case of bandit feedback, the actions are produced by a <em>bandit algorithm</em>, which is a type of policy that incorporates the entire history of actions and utility outcomes in deciding the optimal current action. Though actions and outcomes in the bandit feedback process are independent at each time step, the policy introduces dependence on past actions and outcomes, as shown in figure 12.24. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p216">
<img alt="figure" height="733" src="../Images/CH12_F24_Ness.png" width="1017"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.24</span> Bandit feedback where a bandit policy algorithm selects the next action based on past actions and reward outcomes</h5>
</div>
<div class="readable-text" id="p217">
<p>Recall our previous example of an agent taking pain medication in response to the onset of pain. Figure 12.25 shows how a policy would take in the history of degree of pain and how much medication was provided.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p218">
<img alt="figure" height="681" src="../Images/CH12_F25_Ness.png" width="659"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.25</span> In the pain example, the policy considers the history of recorded levels of pain and corresponding dosages of medication.</h5>
</div>
<div class="readable-text" id="p219">
<p>The policy is like a doctor making the rounds on a hospital floor. They come to a patient’s bed, and the patient reports some level of pain. The doctor looks at that patient’s history of pain reports and the subsequent dosages of medication and uses that information to decide what dosage to provide this time. The doctor’s utility function is in terms of pain, risk of overdose, and risk of addiction. They need to consider historic data, not just the current level of pain, to optimize this utility function.</p>
</div>
<div class="readable-text" id="p220">
<h3 class="readable-text-h3" id="sigil_toc_id_308"><span class="num-string">12.4.2</span> How policies can introduce confounding</h3>
</div>
<div class="readable-text" id="p221">
<p>As stochastic interventions, policies introduce interventions conditional on other nodes in the graph. Because of this, there is a possibility that the policy will introduce new backdoor paths that can confound causal inferences. For example, consider again the DAG in figure 12.26.</p>
</div>
<div class="browsable-container figure-container" id="p222">
<img alt="figure" height="213" src="../Images/CH12_F26_Ness.png" width="339"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.26</span> The policy eliminates the backdoor path through <em>Z</em> but not the backdoor path through <em>C</em>.</h5>
</div>
<div class="readable-text" id="p223">
<p>The policy breaks the backdoor path from <em>X</em> to <em>Y</em> through <em>Z</em>, but there is still a path from <em>X</em> to <em>Y</em> through <em>C</em>. Thus, typical causal queries involving <em>X</em> and <em>Y</em> would have to condition on or adjust for <em>C</em>.</p>
</div>
<div class="readable-text intended-text" id="p224">
<p>In the next section, we’ll characterize causal RL in causal terms.</p>
</div>
<div class="readable-text" id="p225">
<h2 class="readable-text-h2" id="sigil_toc_id_309"><span class="num-string">12.5</span> Causal reinforcement learning</h2>
</div>
<div class="readable-text" id="p226">
<p>Reinforcement learning (RL) is a branch of machine learning that generally involves an agent learning policies that maximize cumulative reward (utility). The agent learns from the consequences of its actions, rather than from being explicitly taught, and adjusts its behavior based on the rewards or losses (reinforcements) it receives. Many sequential decision-making problems can be cast as RL problems.</p>
</div>
<div class="readable-text" id="p227">
<h3 class="readable-text-h3" id="sigil_toc_id_310"><span class="num-string">12.5.1</span> Connecting causality and Markov decision processes</h3>
</div>
<div class="readable-text" id="p228">
<p>RL typically casts a decision process as a Markov decision process (MDP). A canonical toy example of an MDP is a grid world, illustrated in figure 12.27. </p>
</div>
<div class="readable-text intended-text" id="p229">
<p>Figure 12.27 presents a 3 <span class="regular-symbol">× </span>4 grid world. An agent can act within this grid world with a fixed set of actions, moving up, down, left, and right. The agent wants to execute a set of actions that deliver it to the upper-right corner {0, 3}, where it gains a reward of 100. The agent wants to avoid the middle-right square {1, 3}, where it has a reward of –100 (a <em>loss </em>of 100). Position {1, 1} contains an obstacle the agent cannot traverse.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p230">
<img alt="figure" height="165" src="../Images/CH12_F27_Ness.png" width="239"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.27</span> A simple grid world</h5>
</div>
<div class="readable-text intended-text" id="p231">
<p>We can think of it as a game. When the game starts, the agent “spawns” randomly in one of the squares, except for {0, 3}, {1, 3}, and {1, 1}. When the agent moves into a goal square, the game ends. To win, the agent must navigate around the obstacle in {1, 1}, avoid {1, 3}, and reach {0, 3}.</p>
</div>
<div class="readable-text intended-text" id="p232">
<p>A Markov decision process models this and much more complicated “worlds” (aka domains, problems, etc.) with abstractions for states, actions, transition functions, and rewards.</p>
</div>
<div class="readable-text" id="p233">
<h4 class="readable-text-h4 sigil_not_in_toc">States</h4>
</div>
<div class="readable-text" id="p234">
<p>States are a set that represents the current situation or context that the agent is in, within its environment. In the grid-world example, a state represents the agent being at a specific cell. In this grid, there are 12 different states (the cell at {1, 1} is an unreachable state). We assume the agent has some way of knowing which state they are in.</p>
</div>
<div class="readable-text intended-text" id="p235">
<p>We’ll denote state as a variable <em>S</em>. In a grid world, <em>S</em> is a discrete variable, but in other problems, <em>S</em> could be continuous.</p>
</div>
<div class="readable-text" id="p236">
<h4 class="readable-text-h4 sigil_not_in_toc">Actions</h4>
</div>
<div class="readable-text" id="p237">
<p>Actions are the things the agent can do, and they lead to a change of state. Some actions might not be available when in a particular state. For example, in the grid world, the borders of the grid are constraints on the movements of the agent. If the agent is in the bottom-left square {2, 0}, and they try to move left or down, they will stay in place. Similarly, the cell at {1, 1} is an obstacle the agent must navigate around. We denote actions with the variable <em>A</em>, which has four possible outcomes {up, down, right, left}.</p>
</div>
<div class="readable-text" id="p238">
<h4 class="readable-text-h4 sigil_not_in_toc">Transition function</h4>
</div>
<div class="readable-text" id="p239">
<p>The transition function is a probability distribution function. It tells us the probability of moving to a specific next state, given the current state and the action taken.</p>
</div>
<div class="readable-text intended-text" id="p240">
<p>If states are discrete, the transition function looks like this:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p241">
<img alt="figure" height="20" src="../Images/ness-ch12-eqs-11x.png" width="231"/>
</div>
<div class="readable-text" id="p242">
<p>Here, <em>S</em><em><sub>t</sub></em><em>  </em>=<em> </em><em>s </em>means the agent is currently in state <em>s</em>. <em>A</em><em><sub>t</sub></em><em>  </em>=<em> </em><em>a </em>means the agent performs action <em>a</em>. <em>P</em><em> </em>(<em>S</em><em> </em><em><sub>t</sub></em><em> </em><em><sub>+1</sub></em><em> </em>=<em> </em><em>s'|S</em><em><sub>t</sub></em><em>  </em>=<em> </em><em>s, A</em><em><sub>t</sub></em><em>  </em>=<em> </em><em>a</em>) is the probability that the agent transitions to a new state s' given it is in state s and performs action <em>a</em>. When the action leads to a new state with complete certainty, this probability distribution function becomes degenerate (all probability is concentrated on one value).</p>
</div>
<div class="readable-text" id="p243">
<h4 class="readable-text-h4 sigil_not_in_toc">Rewards</h4>
</div>
<div class="readable-text" id="p244">
<p>The term “reward” is preferred to “utility” in RL. In the context of MDPs, the reward function will always take a state <em>s</em> as an argument. We will write it as <em>U</em><em> </em>(<em>s</em><em> </em>).</p>
</div>
<div class="readable-text intended-text" id="p245">
<p>In the grid-world example, <em>U</em><em> </em>({0, 3}) = 100, <em>U</em><em> </em>({1, 3)) = –100. The reward of all other states is 0. Note that sometimes in the MDP/RL literature, <em>U</em><em> </em>() is a function of state and an action, as in <em>U</em><em> </em>(<em>s</em>, <em>a</em><em> </em>). We don’t lose anything by just having actions be a function of state because you can always fold actions into the definition of a state.</p>
</div>
<div class="readable-text" id="p246">
<h3 class="readable-text-h3" id="sigil_toc_id_311"><span class="num-string">12.5.2</span> The MDP as a causal DAG</h3>
</div>
<div class="readable-text" id="p247">
<p>Figure 12.28 shows the MDP as a causal DAG.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p248">
<img alt="figure" height="240" src="../Images/CH12_F28_Ness.png" width="481"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.28</span> The Markov decision process represented as a DAG</h5>
</div>
<div class="readable-text" id="p249">
<p> As a causal DAG, the MDP looks like the other sequential decision processes we’ve outlined, except that we limit ourselves to states, actions, and rewards. In figure 12.28, the process continues until we reach a terminal state (<em>S</em><sub><em>k</em></sub>), such as getting to the terminal cells in the grid-world example.</p>
</div>
<div class="readable-text" id="p250">
<h4 class="readable-text-h4 sigil_not_in_toc">The causal Markov property and the MDP</h4>
</div>
<div class="readable-text" id="p251">
<p>The “Markov” in “Markov decision process” comes from the fact that the current state is independent of the full history of states given the last state. Contrast this with the causal Markov property of causal DAGs: a node in the DAG is independent of indirect “ancestor” causes given its direct causal parents. We can see that when we view the MDP as a causal DAG, this Markovian assumption is equivalent to the causal Markov property. That means we can use our d-separation-based causal reasoning, including the do-calculus, in the MDP graphical setting.</p>
</div>
<div class="readable-text" id="p252">
<h4 class="readable-text-h4 sigil_not_in_toc">The transition function and the causal Markov kernel</h4>
</div>
<div class="readable-text" id="p253">
<p>Note that based on this DAG, the parents of a state <em>S</em><sub>(</sub><em><sub>t</sub></em><sub>+1)</sub> are the previous state <em>S</em><em><sub>t</sub></em> and the action <em>A</em><em><sub>t</sub></em> taken when in that previous state. Therefore, the causal Markov kernel is <em>P</em><em> </em>(<em>S</em><em> </em><em><sub>t</sub></em><em> </em><em><sub>+1</sub></em><em> </em>=<em> </em><em>s'|S</em><em><sub>t</sub></em><em>  </em>=<em> </em><em>s, A</em><em><sub>t</sub></em><em>  </em>=<em> </em><em>a</em>), i.e., the transition function. Thus, the transition function is the causal Markov kernel for a given state.</p>
</div>
<div class="readable-text" id="p254">
<h3 class="readable-text-h3" id="sigil_toc_id_312"><span class="num-string">12.5.3</span> Partially observable MDPs</h3>
</div>
<div class="readable-text" id="p255">
<p>An extension of MDPs is <em>partially observed MDPs </em>(POMDPs). In a POMDP, the agent doesn’t know with certainty what state they are in, and they must make inferences about that state given incomplete evidence from their environment. This applies to many practical problems where the agent cannot observe the full state of the environment.</p>
</div>
<div class="readable-text intended-text" id="p256">
<p>A POMDP can entail different causal structures depending on our assumptions about the causal relationships between the unobserved and observed states. For example, suppose a latent state <em>S</em> is a cause of the observed state <em>X</em>. The observed state <em>X</em> now drives the act <em>A</em> instead of <em>S</em>. Figure 12.29 illustrates this formulation of a POMDP as a causal DAG.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p257">
<img alt="figure" height="332" src="../Images/CH12_F29_Ness.png" width="507"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.29</span> A POMDP where a latent state <em>S</em> causes an observed state <em>X</em>. <em>X</em> drives the actions <em>A</em>.</h5>
</div>
<div class="readable-text" id="p258">
<p>In contrast, figure 12.30 illustrates an example where the latent state is a latent common cause (denoted Z) of the observed state (mediated through the agent’s action) and the utility (note a slight change of notation from <em>U</em><em> </em>(<em>S</em><sub><em>i</em></sub>) to <em>U</em><sub><em>i</em></sub><em> </em>). Here, unobserved factors influence both the agent’s behavior and the resulting utility of that behavior.</p>
</div>
<div class="readable-text intended-text" id="p259">
<p>Again, the basic MDP and POMDP DAGs should be seen as templates for starting our analysis. Once we understand what causal queries we are interested in answering, we can explicitly represent various components of observed and unobserved states as specific nodes in the graph, and then use identification and adjustment techniques to answer our causal queries.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p260">
<img alt="figure" height="310" src="../Images/CH12_F30_Ness.png" width="481"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.30</span> A POMPD formulation where the unobserved states are latent common causes that could act as confounders in causal inferences</h5>
</div>
<div class="readable-text" id="p261">
<h3 class="readable-text-h3" id="sigil_toc_id_313"><span class="num-string">12.5.4</span> Policy in an MDP</h3>
</div>
<div class="readable-text" id="p262">
<p>As before, policies in an MDP act as stochastic interventions. Figure 12.31 illustrates a policy that selects an optimal action based on the current state in a way that disrupts any influence on the action from a confounder.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p263">
<img alt="figure" height="782" src="../Images/CH12_F31_Ness.png" width="612"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 12.31</span> Modification of an MDP DAG by a policy</h5>
</div>
<div class="readable-text" id="p264">
<p>Figure 12.31 is simple in that it only selects an action based on the current state. The challenge is in the implementation, because in most RL settings, states can be high-dimensional objects.</p>
</div>
<div class="readable-text" id="p265">
<h3 class="readable-text-h3" id="sigil_toc_id_314"><span class="num-string">12.5.5</span> Causal Bellman equation</h3>
</div>
<div class="readable-text" id="p266">
<p>RL is about searching for the optimal policy, which is characterized with the Bellman equation, often written as follows:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p267">
<img alt="figure" height="74" src="../Images/ness-ch12-eqs-12x.png" width="465"/>
</div>
<div class="readable-text" id="p268">
<p>In plain words, we’re looking for a policy <span class="regular-symbol">Π</span><sup>*</sup> maximizes the cumulative reward over time. Here <em class="obliqued">γ</em> is a discount rate, a value between 0 and 1, that makes sure the agent values rewards in the near future more than rewards in the far future.</p>
</div>
<div class="readable-text intended-text" id="p269">
<p>Since we’re reasoning about what would happen if we deployed the policy, the causal formulation would be as follows:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p270">
<img alt="figure" height="73" src="../Images/ness-ch12-eqs-13x.png" width="424"/>
</div>
<div class="readable-text" id="p271">
<p>Note that we could do the same causal rewrite for other variants of the Bellman equation, such as the Q-function used in Q-learning.</p>
</div>
<div class="readable-text intended-text" id="p272">
<p>The difference between the noncausal and causal formulations of the Bellman equation is the same as the difference between optimizing <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><em>  </em>)|<em>X</em>=<em>x</em><em> </em>) and <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub><em> </em>)) in section 12.2.4. The process of solving the causally naive version of the Bellman equation may introduce biases from latent confounders or from conditioning on colliders and mediators. Our causally attuned approach can help avoid these biases. In many cases, the solution of the naive approach will coincide with the causal approach because those biases might not affect the ranking of the top policy relative to others. However, as in the <em>E</em><em> </em>(<em>U</em>(<em>Y</em><em>  </em>)|<em>X</em><em> </em>=<em> </em><em>x</em><em> </em>) versus <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>)) example, there will be cases where the solutions to the noncausal and causal formulations differ, and your RL problem might be one of those cases.</p>
</div>
<div class="readable-text" id="p273">
<h2 class="readable-text-h2" id="sigil_toc_id_315"><span class="num-string">12.6</span> Counterfactual reasoning for decision-theory</h2>
</div>
<div class="readable-text" id="p274">
<p>So far, we’ve discussed the problem of choosing optimal actions with respect to a utility function as a level 2 query on the causal hierarchy. Is there a use for level 3 counterfactual reasoning in decision theory? In this section, we’ll briefly review some applications for level 3 reasoning.</p>
</div>
<div class="readable-text" id="p275">
<h3 class="readable-text-h3" id="sigil_toc_id_316"><span class="num-string">12.6.1</span> Counterfactual policy evaluation</h3>
</div>
<div class="readable-text" id="p276">
<p>Counterfactual policy evaluation involves taking logged data from a policy in production and asking, “given we used this policy and got this cumulative reward, how much cumulative reward would we have gotten had we used a different policy?” See the chapter notes at <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a> for references to techniques such as <em>counterfactually guided policy search</em> and <em>counterfactual risk minimization</em>.</p>
</div>
<div class="readable-text" id="p277">
<h3 class="readable-text-h3" id="sigil_toc_id_317"><span class="num-string">12.6.2</span> Counterfactual regret minimization</h3>
</div>
<div class="readable-text" id="p278">
<p>In chapters 8 and 9, I introduced <em>regret</em> as a counterfactual concept. We can further clarify the idea now that we have introduced the language of decision-making; regret is the difference between the utility/reward that was realized given a specific action or set of actions, and the utility/reward that would have been realized had another action or set of actions been taken. </p>
</div>
<div class="readable-text intended-text" id="p279">
<p><em>Counterfactual regret minimization</em> is an approach to optimizing policies that seeks to minimize regret. To illustrate, suppose we have a policy variable <em class="obliqued">Π</em>, which can return one of several available policies. The policies take in the context and return an action. The action leads to some reward <em>U</em>.</p>
</div>
<div class="readable-text intended-text" id="p280">
<p>Suppose, for a single instance in our logged data, the policy was <em class="obliqued">Π</em>=<em class="obliqued">π</em> and the context was <em>C</em><em> </em>=<em>c</em>. We get a certain action <em>A</em><em> </em>=<em class="obliqued">π</em>(<em>c</em><em> </em>) and reward <em>U</em><em> </em>=<em>u</em>. For some policy <em class="obliqued">π</em><em>'</em>, regret is the answer to the counterfactual question, “How much more reward would we have gotten if the policy had been <em class="obliqued">π</em><em> </em>=<em> </em><em class="obliqued">π</em><em>'</em>?” In terms of expectation,<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p281">
<img alt="figure" height="62" src="../Images/ness-ch12-eqs-14x.png" width="526"/>
</div>
<div class="readable-text" id="p282">
<p>Again, this is regret for a single instance in logged data where the context was <em>C</em><em> </em>=<em>c</em> and the utility was <em>u</em>. There are many variations, but the general idea is to find the policy that would have minimized cumulative regret over all the cases of <em>C</em><em> </em>=<em>c</em> in the logged data, with the goal of favoring that policy in cases of <em>C</em><em> </em>=<em>c</em> in the future.</p>
</div>
<div class="readable-text" id="p283">
<h3 class="readable-text-h3" id="sigil_toc_id_318"><span class="num-string">12.6.3</span> Making level 3 assumptions in decision problems</h3>
</div>
<div class="readable-text" id="p284">
<p>The question, of course, is how to make the level 3 assumptions that enable counterfactual inferences. One approach would be to specify an SCM and use the general algorithm for counterfactual reasoning (discussed in chapter 9). For example, in RL, the transition function <em>P</em><em> </em>(<em>S</em><em> </em><em><sub>t</sub></em><em> </em><em><sub>+1</sub></em><em> </em>=<em> </em><em>s'|S</em><em><sub>t</sub></em><em>  </em>=<em> </em><em>s, A</em><em><sub>t</sub></em><em>  </em>=<em> </em><em>a</em>) captures the rules of state changes in the environment. As I mentioned, <em>P</em><em> </em>(<em>S</em><em> </em><em><sub>t</sub></em><em> </em><em><sub>+</sub></em><em><sub>1</sub></em><em>|S</em><em><sub>t</sub></em><em>  </em>=<em> </em><em>s, A</em><em><sub>t</sub></em><em>  </em>=<em> </em><em>a</em>) is the causal Markov kernel for a given state <em>S</em><em> </em><em><sub>t</sub></em><em> </em><em><sub>+1</sub></em>. We could specify an SCM with an assignment function that entails that causal Markov kernel, and write that assignment function as<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p285">
<img alt="figure" height="25" src="../Images/ness-ch12-eqs-15x.png" width="181"/>
</div>
<div class="readable-text" id="p286">
<p>Here, <em>n</em><sub><em>s</em></sub><em><sub>'</sub></em> is the value of an exogenous variable for <em>S</em><em><sub>t</sub></em>.</p>
</div>
<div class="readable-text intended-text" id="p287">
<p>The challenge is specifying assignment functions that encode the correct counterfactual distributions. This is easier in domains where we know more about the underlying causal mechanisms. A key example is in rule-based games; game rules can provide the level 3 constraints that enable simulation of counterfactuals. Recall how, in chapter 9, the simple rules of the Monte Hall problem enabled us to simulate counterfactual outcomes for stay versus switch strategies. Or consider multiplayer games like poker, where in a round of play each player is dealt a hand of cards and can take certain actions (check, bet, call, raise, or fold) that lead to outcomes (win, lose, tie) based on simple rules, which in turn determine the amount of chips won or lost in that round. A player’s counterfactual regret is the difference between the chips they netted and the most they could have netted had they decided on different actions. This is done while accounting for the information available at the time of the decision, not using hindsight about the opponents’ cards. </p>
</div>
<div class="readable-text intended-text" id="p288">
<p>Counterfactual regret minimization algorithms in this domain attempt to find game playing policies that minimize counterfactual regret across multiple players. The concrete rules of the game enable simulation of counterfactual game trajectories. The challenge lies in searching for optimal policies within a space of possible counterfactual trajectories that is quite large because of multiple player interactions over several rounds of play. See the chapter notes on counterfactual regret minimization in multiagent games at <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a> for references.</p>
</div>
<div class="readable-text" id="p289">
<h2 class="readable-text-h2" id="sigil_toc_id_319">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p290"> Decision-making is naturally a causal problem because decisions cause consequences, and our goal is to make the decision that leads to favorable consequences. </li>
<li class="readable-text" id="p291"> Choosing the optimal decision is a level 2 query as we are asking “what would happen if I made this decision?” </li>
<li class="readable-text" id="p292"> <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em> </em>=<em>x</em><em> </em>)) and <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub><em> </em>)) are different quantities. Usually, people want to know the value of <em>X</em> that optimizes <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub><em> </em>)), but optimizing <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em> </em>=<em> </em><em>x</em><em> </em>)) will often yield the same answer without the bother of specifying a causal model. </li>
<li class="readable-text" id="p293"> This is especially true in reinforcement learning (RL), where the analogs to <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em> </em>=<em>x</em>)) and <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>)) are, respectively, the conventional and causal formulations of the Bellman equation. Confounder, mediator, and collider biases may be present in conventional approaches to solving the Bellman equation. But those bias often don’t influence the ranking of the top policy relative to other policies. </li>
<li class="readable-text" id="p294"> Nonetheless, sometimes the value of <em>X</em> that optimizes <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em> </em>=<em>x</em>)) is different from that which optimizes <em>E</em><em> </em>(<em>U</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub><em> </em>)). Similarly, addressing causal nuances when solving the Bellman equation may result in a different policy than ignoring them. If your decision problem falls into this category, causal approaches are the better choice. </li>
<li class="readable-text" id="p295"> Newcomb’s paradox is a thought experiment meant to contrast causal and noncausal approaches to decision theory. The “paradox” is less mysterious once we use a formal causal model. </li>
<li class="readable-text" id="p296"> Causal decision theory, combined with probabilistic modeling tools like Pyro and pgmpy, is well suited to modeling introspection, where an agent reflects on their internal state (feelings, intuition, urges, intent) and uses that information to predict the “what-if” outcomes of their decisions. </li>
<li class="readable-text" id="p297"> When we represent a sequential decision process with a causal DAG, we can employ all the tools of graphical causal inference in that decision problem. </li>
<li class="readable-text" id="p298"> Policies operate like stochastic interventions. They change the graph but still have dependence on observed nodes in the past, and that dependence can introduce backdoor confounding. </li>
<li class="readable-text" id="p299"> In causal RL, we can represent MDPs and POMDPs as causal DAGs and, again, make use of graphical causal inference theory. </li>
<li class="readable-text" id="p300"> We can use template DAGs to represent sequential decision processes, but you should tailor these templates for your problem. </li>
<li class="readable-text" id="p301"> Common use cases for counterfactual reasoning in decision theory are counterfactual policy evaluation and counterfactual regret minimization. </li>
<li class="readable-text" id="p302"> If you have access to the rules underlying state transitions in your MDP, such as in physical systems or games, you could build an SCM that is counter- factually faithful to those rules, and use it to handle counterfactual use cases in decision-making. </li>
</ul>
</div></body></html>