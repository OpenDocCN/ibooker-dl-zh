<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 7. AI Risks and Challenges"><div class="chapter" id="ch07_ai_risks_and_challenges_1748358220835353">
      <h1><span class="label">Chapter 7. </span>AI Risks and Challenges</h1>
      <p>In <a data-type="xref" href="ch02.html#ch02_essential_background_on_generative_ai_1748358211788823">Chapter 2</a>, we discussed the <a contenteditable="false" data-type="indexterm" data-primary="generative AI" data-secondary="limitations of" id="id775"/>limitations of generative AI. This chapter will go over the risks and challenges that derive from those limitations. It’s important to understand that limitations and risks are two separate but related concepts. Limitations explain what AI can’t do. For example, generative AI can only provide insights using previously published data; it doesn’t “know” anything without first ingesting the data that gives it the information it needs.</p>
      <p>The limitations of AI are what lead to <a contenteditable="false" data-type="indexterm" data-primary="generative AI" data-secondary="risks of" data-see="risks of generative AI" id="id776"/><a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" id="id777"/>risks, and these can be devastating to your SEO, brand, and revenue. Various risks can result in revenue-impacting consequences, including litigation, loss of search engine rank, brand damage, and potential penalties. However, many of the risks of AI can be mitigated by careful human reviews. As much as AI reduces overhead and time to perform repeatable SEO tasks, it cannot function well without the critical thinking of human reviewers.</p>
      <p>In this chapter, we’ll cover some of the common pitfalls you may encounter when using AI and how you can overcome them. If you recall the Gartner Hype Cycle <a contenteditable="false" data-type="indexterm" data-primary="Gartner Hype Cycle" id="id778"/><a contenteditable="false" data-type="indexterm" data-primary="disillusionment" id="id779"/>from <a data-type="xref" href="ch02.html#ch02_essential_background_on_generative_ai_1748358211788823">Chapter 2</a> (<a data-type="xref" href="ch02.html#ch02_figure_3_1748358211760542">Figure 2-3</a>), one of the phases is disillusionment. Many of AI’s pitfalls can contribute to disillusionment, but these challenges can be managed with the right strategies, which we will also discuss in this chapter. </p>
      <section data-type="sect1" data-pdf-bookmark="The Ultimate Risk: Low-Quality Content"><div class="sect1" id="ch07_the_ultimate_risk_low_quality_content_1748358220835491">
        <h1>The Ultimate Risk: Low-Quality Content</h1>
        <p>The<a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" data-secondary="low-quality content" id="risks-low-quality"/><a contenteditable="false" data-type="indexterm" data-primary="low-quality content" id="low-quality-content"/><a contenteditable="false" data-type="indexterm" data-primary="content SEO" data-secondary="low-quality content" id="content-seo-low-quality"/> core of an SEO practitioner’s job is to produce a site that adds value to the internet by offering users a place to find factual, insightful, engaging content. As soon as you lose sight of that goal, you risk everything, and improper use of AI is a sure way to go down the wrong path. AI is a powerful tool, but with its power comes the responsibility of ensuring that you use it correctly. </p>
        <p>Most of this risk results from being able to generate content at scale. If you were producing content at scale years ago, you’d have writers generating several pieces a week. Quality human writers typically produce a relatively low percentage of low-quality content compared to AI, which can generate hundreds—even thousands—of pieces a week. The percentage of low-quality content increases with scale, so the majority of the AI-generated pieces are often vague and unengaging, with what feels like empty text.</p>
        <p>Once Google picks up on low-quality signals from your content, you will likely suffer major drops in your search engine traffic. For this reason, we strongly suggest that you don’t use generative AI to create content at scale (meaning hundreds of pieces per week) <em>unless</em> you have a very large staff to review and fix the myriad problems that you will find within that content.</p>
        <p>Google algorithms know when your site may have some low-quality content; it’s when your content is unhelpful and unengaging that you start to see a loss in search ranking. Low quality is subjective, but unengaging and unhelpful content often leads to lower user engagement, and this is a significant Google ranking factor. Users leave your page and perform a search to find another site. This behavior sends low-quality signals to algorithms, which leads to a loss of search ranking. Not only are a loss in ranking and Google penalties the biggest risks, but they are also the most difficult to bounce back from. It can take years to recover, and it’s possible that you will never regain algorithmic trust. As more content pollutes the internet, algorithmic trust will be much more difficult to obtain, making it harder for smaller brands to compete. Generating massive amounts of content using AI can be initially tempting, but it is highly likely to lead to long-term algorithmic loss in search engine rankings.</p>
        <p>As an SEO practitioner, your job is to build pages that answer the intent of a potential customer’s search engine query. When generative AI interest boomed in 2023, Google released a <a href="https://oreil.ly/F1VP0">statement</a> in February saying that “appropriate use of AI or automation is not against our guidelines.” Google never intended that to apply to using generative AI to create poor-quality content. </p>
        <p>Let’s look at a short example related to SEO. The prompts in <a data-type="xref" href="#ch07_figure_1_1748358220824643">Figure 7-1</a> ask ChatGPT to give the top SEO tip and the top SEO tip for ranking a technology site, and to be specific. </p>
        <p>Notice that even though the first request said to be specific, the output is vague. “High quality, user-focused content” is a tip for any site across all industries. The second prompt asks for an SEO tip for ranking a technology site. Notice that the second output is generally the same as the first. Both outputs feel empty and unengaging. If this were a snippet in your content, you would want a human editor to add substance and specificity. </p>
        <figure><div id="ch07_figure_1_1748358220824643" class="figure">
          <img src="assets/ugai_0701.png" width="754" height="685"/>
          <h6><span class="label">Figure 7-1. </span>ChatGPT output for SEO tips</h6>
        </div></figure>
        <p>Errors by omission are difficult to detect but can be harmful to your brand, especially if your brand relies on being an authority in the industry. Suppose you sell a software as a service (SaaS) product that offers the same two features that your competitor does, but your value-added proposition is an additional feature. This feature was recently introduced, but your competitor has a larger market share. You need to create content that compares your product to your competitor’s and use it to introduce your new feature. If you create content using generative AI, it’s likely that output will tell users that both SaaS products—yours and your competitor’s—have the same two features, but it will omit your newly designed feature. If you automate content creation, you might find that your site’s new content is worthless since it doesn’t promote the feature that gives your SaaS product a competitive edge. Not only that, but you don’t want to just mention this new feature—it should be a significant focus of the content since that’s your competitive advantage. While the content is accurate, it’s missing critical information for your marketing.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
          <p>For a deep dive into examples of errors resulting from AI limitations, see <a data-type="xref" href="ch02.html#ch02_limitations_of_generative_ai_1748358211790682">“Limitations of Generative AI”</a>. </p>
        </div>
        <p>Now let’s look at a technical example. Suppose you sell database services and you need to show potential customers that you understand different database engines, specifically MongoDB and Microsoft SQL Server. Let’s ask ChatGPT to give us two paragraphs on the differences between the two databases. The output is shown in <a data-type="xref" href="#ch07_figure_2_1748358220824678">Figure 7-2</a>.</p>
        <figure class="pagebreak-after"><div id="ch07_figure_2_1748358220824678" class="figure">
          <img src="assets/ugai_0702.png" width="754" height="662"/>
          <h6><span class="label">Figure 7-2. </span>ChatGPT 4 output comparing MongoDB and SQL Server</h6>
        </div></figure>
        <p>In <a data-type="xref" href="#ch07_figure_2_1748358220824678">Figure 7-2</a>, ChatGPT mentions that SQL Server supports a concept called <em>ACID</em> (atomicity, consistency, isolation, and durability) and then says that MongoDB offers other advantages. If you look at official MongoDB resource pages, they indicate that MongoDB is also <a href="https://oreil.ly/diqfG">ACID compliant</a>, as shown in <a data-type="xref" href="#ch07_figure_3_1748358220824703">Figure 7-3</a>. </p>
        <figure><div id="ch07_figure_3_1748358220824703" class="figure">
          <img src="assets/ugai_0703.png" width="736" height="189"/>
          <h6><span class="label">Figure 7-3. </span>MongoDB resource page information on ACID compliance</h6>
        </div></figure>
        <p>Although the ChatGPT content in <a data-type="xref" href="#ch07_figure_2_1748358220824678">Figure 7-2</a> does not explicitly say that MongoDB is <em>not</em> ACID compliant, it omits the fact that MongoDB is ACID compliant, so the information it gives is incorrect. It’s easy to see how this type of poor-quality content could cause brand damage by giving potential customers the sense that your brand is not truly an authority in the subject. The ultimate result is fewer sales for your brand and potentially a loss in search engine rankings.</p>
        <p>You can use other LLMs to verify information from ChatGPT or cross-reference facts. Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch07_figure_4_1748358220824726">7-4</a> through <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch07_figure_6_1748358220824769">7-6</a> show output for the same query in Claude, Microsoft Copilot, and Gemini. </p>
        <figure><div id="ch07_figure_4_1748358220824726" class="figure">
          <img src="assets/ugai_0704.png" width="691" height="633"/>
          <h6><span class="label">Figure 7-4. </span>Claude output comparing MongoDB and SQL Server</h6>
        </div></figure>
        <figure><div id="ch07_figure_5_1748358220824748" class="figure">
          <img src="assets/ugai_0705.png" width="1066" height="942"/>
          <h6><span class="label">Figure 7-5. </span>Microsoft Copilot output comparing MongoDB and SQL Server</h6>
        </div></figure>
        <figure><div id="ch07_figure_6_1748358220824769" class="figure">
          <img src="assets/ugai_0706.png" width="714" height="583"/>
          <h6><span class="label">Figure 7-6. </span>Gemini output comparing MongoDB and SQL Server</h6>
        </div></figure>
        <p>To overcome errors of omission, you need a true SME who can identify the missing pieces, understand where to look for them, and validate that the information is correct. In this example, a reviewer who understands databases is required to know that both databases are ACID compliant and to validate the information on the vendor’s site.</p>
        <p>When creating content at scale, remember that your primary goal is to offer value to the user.<em> </em>The meat of the issue is low-quality content, and the other risks we cover in the rest of this chapter feed into those concerns about<a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" data-secondary="low-quality content" data-startref="risks-low-quality" id="id780"/><a contenteditable="false" data-type="indexterm" data-primary="low-quality content" data-startref="low-quality-content" id="id781"/><a contenteditable="false" data-type="indexterm" data-primary="content SEO" data-secondary="low-quality content" data-startref="content-seo-low-quality" id="id782"/> low-quality content. </p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Speed of Publishing: Faster Is Not Always Better for SEO"><div class="sect1" id="ch07_speed_of_publishing_faster_is_not_always_better_f_1748358220835564">
        <h1>Speed of Publishing: Faster Is Not Always Better for SEO</h1>
        <p>Hiring <a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" data-secondary="publishing speed" id="risks-speed"/><a contenteditable="false" data-type="indexterm" data-primary="low-quality content" data-secondary="publishing speed and" id="low-quality-content-speed"/><a contenteditable="false" data-type="indexterm" data-primary="publishing speed, low-quality content and" id="publish-speed-low-quality"/><a contenteditable="false" data-type="indexterm" data-primary="speed of publishing, low-quality content and" id="speed-publish-low-quality"/>writers was always a big overhead cost and a time-consuming endeavor for SEO practitioners, so the thought of having AI become one hundred writers overnight is tempting. As Google (and other search engines) continue to evolve and attempt to control the mass publication of AI-generated content, it’s important to realize that faster publication doesn’t always mean improved quality and better search engine placement. As a matter of fact, if you previously published one or two articles a week and are now suddenly publishing hundreds of articles a week, you might be sending the wrong quality signals to search algorithms.</p>
        <p>As we discussed in previous sections, Google downranks low-quality AI-generated content. AI models used to generate content aren’t at the point where no human editor is necessary, but large numbers of organizations tried to use it that way anyway in 2024, and as a result, we saw massive chaos in search engines indexing misinformation on the internet because all of the low-quality content that was published. Users complained, and Google rolled back its initial stance on AI-generated content being fine for users and ranking.</p>
        <p>A component of search algorithms is <a contenteditable="false" data-type="indexterm" data-primary="trustworthiness" id="id783"/>the concept of <em>trust</em>. Trust is often measured in the age of a site and the long-term quality signals sent to the search engine. For example, if your site is BMW and you’re trying to rank for car-related content, Google will likely trust your site much more than newer sites. We’ve observed that Google has rolled back changes and returned to trusting older long-term brand sites in spaces where there is a lot of AI-generated noise from massive content publishing. Google trust factors are difficult to measure, but you can avoid downranking by always reviewing your AI-generated content.</p>
        <p>Every site has its low-quality content, but that should be kept to a minimum. If you publish hundreds of new articles a week after you traditionally published only a few articles a week, that will likely send low-quality signals to search engines. The idea of trust is to consistently but slowly ramp up production to ensure that quality content remains a primary goal. Scaling up slowly will avoid sending any red flags to quality algorithms. As an example, you might have a 50:1 ratio of quality content, where 1 out of 50 articles sends low-quality signals. If you ramp up automated content too much without checking for quality, your quality ratio might rapidly change to 60:30, meaning nearly half your content is low quality. A worse ratio will probably affect the overall search ranking of your<a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" data-secondary="publishing speed" data-startref="risks-speed" id="id784"/><a contenteditable="false" data-type="indexterm" data-primary="low-quality content" data-secondary="publishing speed and" data-startref="low-quality-content-speed" id="id785"/><a contenteditable="false" data-type="indexterm" data-primary="publishing speed, low-quality content and" data-startref="publish-speed-low-quality" id="id786"/><a contenteditable="false" data-type="indexterm" data-primary="speed of publishing, low-quality content and" data-startref="speed-publish-low-quality" id="id787"/> site.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Copyright: Unique Content Doesn’t Mean You Own It"><div class="sect1" id="ch07_copyright_unique_content_doesn_t_mean_you_own_it_1748358220835634">
        <h1>Copyright: Unique Content Doesn’t Mean You Own It</h1>
        <p>Your <a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" data-secondary="copyright infringement" id="risks-copyright"/><a contenteditable="false" data-type="indexterm" data-primary="copyright" id="copyright-infringement"/>content may pass plagiarism checkers, but AI puts a new spin on copyright. Because AI generates content based on the writing of others, you’re creating content that isn’t truly your own, even though that sequence of words is not found anywhere else on the internet. Content written in the same tone, voice, and style as another author could be considered copyright infringement. We say “could” be copyright infringement because as of the writing of this book, lawsuits are still pending.</p>
        <p>Eight large, well-known publishers are suing OpenAI—the creators of ChatGPT—for <a href="https://oreil.ly/TQlnf">copyright infringement</a>. At the foundation of the lawsuits is the claim that ChatGPT content uses their original work and their journalists’ original ideas to generate similar content. The lawsuit claims that OpenAI used their work to train models, which begs the question if AI can ever generate original content if some data it ingests is unusable.</p>
        <p>Copyright infringement is still an unknown factor in future regulations, but SEO practitioners should ensure that they neither prompt AI to create content in the same style and tone as any other author nor create content using a single source of information. Neglecting this could increase the likelihood of <a href="https://oreil.ly/Jf9mb">infringement lawsuits</a>. Some lawsuits such as <em>Concord Music Group, Inc. v. Anthropic PBC</em> and <em>The New York Times v. Microsoft and OpenAI</em> provide the courts with output that copies original works verbatim. LLM providers are <a href="https://oreil.ly/QiEBt">contesting the allegations</a> and claim that “regurgitation” of content is a bug rather than a feature. SEO practitioners using generative AI to produce content <a href="https://oreil.ly/X_eqE">can’t copyright it either</a>, which can cause issues if your own content is stolen.</p>
        <p>To avoid copyright infringement, diversify your sources and provide references—with a link or footnote—to sources used in your content. Some SEO professionals choose to work with multiple LLMs to diversify output and validate information. For example, you might use ChatGPT to generate content and use Claude to verify that content. You can also specify using multiple sources in your prompts and ask ChatGPT to provide a list of sources for<a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" data-secondary="copyright infringement" data-startref="risks-copyright" id="id788"/><a contenteditable="false" data-type="indexterm" data-primary="copyright" data-startref="copyright-infringement" id="id789"/> its content.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Plagiarism: Did You Write This Content?"><div class="sect1" id="ch07_plagiarism_did_you_write_this_content_1748358220835697">
        <h1>Plagiarism: Did You Write This Content?</h1>
        <p>Every<a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" data-secondary="plagiarism" id="risks-plagiarism"/><a contenteditable="false" data-type="indexterm" data-primary="plagiarism" id="plagiarism"/> teacher has students who try to pass off plagiarized content as their own. Generative AI can be your bad student if you use it to create content without thoroughly checking it. Instead of having one writer produce plagiarized content, AI could be the equivalent of one hundred writers producing plagiarized content. For SEO practitioners, having the same content as multiple other sites lowers quality signals sent to search engines and could affect your search ranking.</p>
        <p>Plagiarism on the internet is a bit different than academic plagiarism. Academically, you can’t rewrite a whitepaper from another researcher and call it your own just because it passes plagiarism checkers. This activity is still plagiarism. But the internet works differently than academics. The same idea is often written about dozens of times on the internet. Your content strategies might overlap these same ideas, but you want to add value that can’t be found in other brands’ content strategies. “Value” is subjective and depends on your target audience, but your goal should be to provide information related to the search query and a call to action to instruct the reader where to go next. This will offer value to readers and search engines, which will in turn improve your ranking and trust signals.</p>
        <p>When you have writers working for you, you check their content for plagiarism. An example of a tool that does this is <a href="https://www.copyscape.com">Copyscape</a>, but you can find many other tools using your favorite search engine. These tools aren’t perfect, but they provide a good first check to ensure that your writers aren’t plagiarizing. You should also compare your content with what’s already published on the internet. You can copy and paste full or partial sentences into Google to see if large blocks of content were plagiarized by the writer.</p>
        <p>Separately, <a contenteditable="false" data-type="indexterm" data-primary="AI-generated content" data-secondary="detecting" id="id790"/><a contenteditable="false" data-type="indexterm" data-primary="detecting AI-generated content" id="id791"/>you should check that your writers are not using generative AI tools to create content and then calling it their own work. As we discussed in <a data-type="xref" href="ch03.html#ch03_getting_started_with_generative_ai_1748358214007893">Chapter 3</a>, you run the risk that writers won’t do a great job of reviewing the content for errors, omissions, copyright infringement, and the like. There are many tools out there for checking if the content has been written by AI, such as Copyleaks and ZeroGPT. See <a data-type="xref" href="ch03.html#ch03_ai_detection_tools_1748358214008704">“AI-Detection Tools”</a> for a longer list.</p>
        <div data-type="warning" epub:type="warning"><h6>Warning</h6>
          <p>We should note that tools that check content to see if it was written by AI can be notoriously inaccurate. As a matter of fact, AI-generation checkers say that the <a href="https://oreil.ly/6sYiP">US Constitution was written by AI</a>. Nevertheless, you should perform these checks as they may flag some issues. Be aware that they will also sometimes flag content that was not written by AI and say that it was. So treat the output of these tools as directionally correct, but have humans verify their assessments.</p>
        </div>
        <p class="pagebreak-after">Have editors on your staff review the work of your writers to look for other issues indicating that writers may be taking excessive shortcuts using generative AI to create content. If you have the development capacity, the best way to guarantee that content is original is to use a CMS (or equivalent technology) that requires writers to write in the tool, without copying and pasting from external documents. You can then employ keystroke biometrics algorithms or tools like <a href="https://oreil.ly/ugIvQ">TypingDNA</a> or <a href="http://typing.ai">Typing AI</a> that fingerprint the writer’s speed and keystroke dynamics to make sure they are doing original work. </p>
        <p>AI-generated content takes input from other sources, but it can be used to create unique content when summarizing something that a person has written. Citing sources will give proper credit to the original author. LLMs now include links to original sources so that you can give attribution when posting content to your site. </p>
        <p>Originality checkers still can’t identify AI-generated content with 100% accuracy, but using them will help flag potential plagiarism and AI-generated content to verify that your writers aren’t using AI. For all AI-generated content, you should have a human editor check it for errors, omissions, and <a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" data-secondary="plagiarism" data-startref="risks-plagiarism" id="id792"/><a contenteditable="false" data-type="indexterm" data-primary="plagiarism" data-startref="plagiarism" id="id793"/>awkward <span class="keep-together">wording.</span> </p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Automation Complacency: You Can’t “Set It and Forget It”"><div class="sect1" id="ch07_automation_complacency_you_can_t_set_it_and_forg_1748358220835760">
        <h1>Automation Complacency: You Can’t “Set It and Forget It”</h1>
        <p>AI is<a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" data-secondary="automation complacency" id="risks-complacency"/><a contenteditable="false" data-type="indexterm" data-primary="SEO (search engine optimization)" data-secondary="automation in" data-tertiary="complacency" id="seo-automation-complacency"/><a contenteditable="false" data-type="indexterm" data-primary="automation in SEO" data-secondary="complacency" id="automation-complacency"/> a powerful automation tool. We discussed some automation use cases in <a data-type="xref" href="ch06.html#ch06_advanced_use_cases_for_generative_ai_in_seo_1748359259642070">Chapter 6</a>. You can perform so much with AI—either with external tools that work with AI (e.g., Google Analytics) or by building your own tools. For example, you might use AI in Microsoft Excel macros to gather data and create descriptions or identify trends. You can do a lot with AI that gives you a competitive edge, such as day-to-day automated information about competitor rankings or a gap analysis between a competitor’s ranking and their published content versus your own published content. </p>
        <p>Automating in SEO with AI is essential for efficiency, but AI changes rapidly (primarily due to the extremely high pace of innovation in AI and its popularity). As more bugs and innovations are introduced, AI companies will adapt and offer new models that provide significantly better results. <a data-type="xref" href="ch02.html#ch02_essential_background_on_generative_ai_1748358211788823">Chapter 2</a> covers the evolution of popular models, and there will likely be another iteration soon after the release of this book. </p>
        <p>Changes to AI should benefit users, including SEO executives, but you should ensure that your automation scripts take these changes into account. AI automation should be consistently reviewed for any industry changes that could affect accuracy in output. For example, <a contenteditable="false" data-type="indexterm" data-primary="Reddit" id="id794"/>Google struck a <a href="https://oreil.ly/Yltl0">$60 million deal</a> with Reddit to use Reddit content to help train its generative AI tools. Reddit has millions of users, and Gemini can benefit from the human-generated content. If you have any kind of automation based on content generation by Gemini, the changes to Reddit and its content “tone” can dramatically influence the type of content created from it. This tone and messaging could diverge from your internal policies, product voice, or audience concerns.</p>
        <p>Another <a contenteditable="false" data-type="indexterm" data-primary="competitive gap analysis" id="comp-gap-analysis-ch7"/><a contenteditable="false" data-type="indexterm" data-primary="gap analysis" data-secondary="AI-powered tools for" id="gap-analysis-ch7"/><a contenteditable="false" data-type="indexterm" data-primary="backlink analysis" id="backlink-ch7"/>common automation opportunity is using AI for competitor analysis or backlink opportunities. Both can take hours out of a single SEO practitioner’s time, so using AI greatly reduces overhead without eliminating essential research. Using automation, you can scan competitors’ links, review their latest content strategies, get a list of their backlinks, and apply this research to building your own strategies. It might even give you insight into your competitors’ seasonal habits and product launches. AI models and products continually change, and in doing so they affect your reports. Your SEO team will still need to review this work for accuracy, but it can significantly speed up the process of doing the work.</p>
        <p>However, if you do not update your own scripts and monitor the results, you could be skewing results unknowingly. In a scenario where you use generative AI for competitor analysis, perhaps it picks up new domains that weren’t being analyzed before, or changes to a site aren’t being used in current models. This scenario again represents the need to always work with human reviewers for generative AI output. Human reviewers can check for inaccuracies and awkward word choices, and they can edit content to have a more conversational tone. Small changes from human reviewers can make content much more engaging to readers and avoid inaccuracies. The same is true for any automation tools used in critical research and <!--<a contenteditable="false" data-type="indexterm" data-primary="competitive gap analysis" data-startref="comp-gap-analysis-ch7">&nbsp;</a>--><!--<a contenteditable="false" data-type="indexterm" data-primary="gap analysis" data-secondary="AI-powered tools for" data-startref="gap-analysis-ch7">&nbsp;</a>--><!--<a contenteditable="false" data-type="indexterm" data-primary="backlink analysis" data-startref="backlink-ch7">&nbsp;</a>-->reporting.</p>
        <p>If you fall into the trap of “set it and forget it,” you will eventually <a contenteditable="false" data-type="indexterm" data-primary="low-quality content" data-secondary="automation complacency and" id="id795"/>generate low-quality content at scale, or you will mistakenly target queries that could be too costly for their ROI. Let’s say that you create content based on the top three ranking competitors. Your AI automation may scan competitor pages, extract keywords, and create content based on a gap analysis. But it likely won’t fully understand the context of what pages it makes sense to create. </p>
        <p>For example, what if Google’s AIOs take a majority of screen real estate on mobile, so users are forced to scroll a long way to see the search listing for your site? Users may not want to look that far. In addition, the AIO may answer the question well enough that users don’t bother to scroll down the page.</p>
        <p>If you generate a large number of articles based on a single gap analysis, you might be deploying content for a query that will never result in a positive ROI. For example, if you use Google Analytics to track traffic and engagement, you might notice much fewer clicks on your call to action or fewer organic search visitors. It can be costly if you have infrastructure and other expensive events (e.g., API queries from third-party vendors) without a return on your investment. This is one example of why you should always review automation and never inherently trust it even if the AI results are correct for most of your<a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" data-secondary="automation complacency" data-startref="risks-complacency" id="id796"/><a contenteditable="false" data-type="indexterm" data-primary="SEO (search engine optimization)" data-secondary="automation in" data-tertiary="complacency" data-startref="seo-automation-complacency" id="id797"/><a contenteditable="false" data-type="indexterm" data-primary="automation in SEO" data-secondary="complacency" data-startref="automation-complacency" id="id798"/> automated events.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="The SEO Nightmare: Losing Search Engine Rank or Google Penalties"><div class="sect1" id="ch07_the_seo_nightmare_losing_search_engine_rank_or_go_1748358220835822">
        <h1>The SEO Nightmare: Losing Search Engine Rank or Google Penalties</h1>
        <p>Losing <a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" data-secondary="penalties and losing rankings" id="risk-penalty-ranking"/><a contenteditable="false" data-type="indexterm" data-primary="Google" data-secondary="penalties and losing rankings" id="google-penalty-ranking"/><a contenteditable="false" data-type="indexterm" data-primary="ranking" data-secondary="losing" id="ranking-losing"/><a contenteditable="false" data-type="indexterm" data-primary="losing rankings" id="losing-ranking"/><a contenteditable="false" data-type="indexterm" data-primary="penalties" id="penalty"/>rankings (or receiving a manual penalty) is the bane of an SEO practitioner’s existence. These downfalls occur quickly, and fixing the issue can require months of work. Some sites never fully recover. To avoid this nightmare, SEO practitioners strive to follow Google’s guidelines while still breaking barriers with unique and innovative strategies to help their brand. </p>
        <p>Losing rankings and Google penalties are two separate phenomena. <em>Losing rankings</em> means you have negative factors affecting algorithmic trust and quality signals. A <em>penalty</em> is manual action on your site meant to keep your site from ranking well. Both are difficult to fix, but they require two separate strategies. Losing rankings requires reviewing your site for low-quality signals and technical issues, but a penalty means that a serious violation of Google’s policies must be remedied.</p>
        <p>Google wants to integrate generative AI into the search results, but it also knows that human users must be satisfied with the search results. A poor user experience would push users to Google’s competitors like Bing and other LLMs with a direct answer, and this would destroy brand reputation for Google’s dominant search engine. Contrasting with this is the existence of market pressures caused by the massive buzz around generative AI. If Google isn’t seen as a leader in this area, this could directly affect its overall reputation as the technology leader in search. It’s a confusing conflict of interest for SEO practitioners who might assume that anything AI is looked on favorably by Google engineers.</p>
        <p>Then there is<a contenteditable="false" data-type="indexterm" data-primary="AI-generated content" data-secondary="Google policy on" id="id799"/><a contenteditable="false" data-type="indexterm" data-primary="Google" data-secondary="policy on AI-generated content" id="id800"/> the matter of how Google views AI-generated content. In February 2023, Google released <a href="https://oreil.ly/F1VP0">guidance on AI-generated content</a> indicating that its main concern with content was its quality, not whether it was written by AI. However, Google knows content created by generative AI doesn’t offer any unique value beyond what is already contained on the web and is often of poor quality unless it’s heavily edited by a human SME. Yet many sites will go ahead and publish content written by generative AI without such review. For that reason, Google may try to detect AI-generated content, and it’s likely that Google has AI-detection tools that are more advanced than the tools that are currently commercially available in the market. </p>
        <p class="pagebreak-before">For example, in March 2024 Google announced <a href="https://oreil.ly/PvyjN">a new update to its core search functionality</a>, promising that less “spammy” content would be released. Rollouts happened in May 2024, and many SEO practitioners who did not heed the warning saw a penalty in their Search Console. The penalty mentioned “spammy” content, and SEO practitioners with poor-quality, AI-generated content experienced a massive drop in search visibility.</p>
        <p>Despite the risks, most of <a href="https://oreil.ly/hrds3">Google’s announcements at its Google I/O conference in 2024</a> focused on AI because that is the hot new technology area. Google had AI for video, text, art, scripting, storytelling, health, and science. Everything in Google I/O had an element of AI to it, but the search engine engineers emphasized the importance of user engagement. Suffice to say that Google supports generative AI, but site owners must maintain quality content, even if it’s generated by AI. Google’s search engine users are most important to Google’s growth and sustainability. Therefore, SEO practitioners should have human reviewers for all content uploaded to their <a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" data-secondary="penalties and losing rankings" data-startref="risk-penalty-ranking" id="id801"/><a contenteditable="false" data-type="indexterm" data-primary="Google" data-secondary="penalties and losing rankings" data-startref="google-penalty-ranking" id="id802"/><a contenteditable="false" data-type="indexterm" data-primary="ranking" data-secondary="losing" data-startref="ranking-losing" id="id803"/><a contenteditable="false" data-type="indexterm" data-primary="losing rankings" data-startref="losing-ranking" id="id804"/><a contenteditable="false" data-type="indexterm" data-primary="penalties" data-startref="penalty" id="id805"/>sites.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Compliance and Changes in Regulations"><div class="sect1" id="ch07_compliance_and_changes_in_regulations_1748358220835879">
        <h1>Compliance and Changes in Regulations</h1>
        <p>The <a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" data-secondary="changes in regulations" id="id806"/><a contenteditable="false" data-type="indexterm" data-primary="regulations" data-secondary="changes in" id="id807"/><a contenteditable="false" data-type="indexterm" data-primary="EU Artificial Intelligence Act (AI Act)" id="id808"/><a contenteditable="false" data-type="indexterm" data-primary="AI Act (EU Artificial Intelligence Act)" id="id809"/>introduction of ChatGPT in 2023 brought AI to the forefront as the next “must-have” technology. AI has received a lot of attention from media outlets, SEO practitioners, marketing people, and businesses looking to leverage it for their own benefit. Governments took notice and are coming up with ways to mitigate risks and oversee ethics and legal compliance related to data usage and AI-generated content. </p>
        <p>At the time of writing, the EU had passed <a href="https://oreil.ly/c4ST6">the AI Act</a> to oversee the creation of AI products and the ethics behind services. The AI Act focuses on accountability and transparency for technology involved in AI. The US will likely follow suit with <a href="https://oreil.ly/-mL_h">state-level regulations</a>. For example, Vermont created a <a href="https://oreil.ly/SGqan">Division of Artificial Intelligence</a> to identify the ways AI affects Vermont residents. Future regulations in other states are highly probable.</p>
        <p>Because AI integration is relatively new, regulations are still in their infancy and will likely develop rapidly. What regulations look like will change as AI evolves. It’s important to keep notified of the latest regulations surrounding AI and your industry. Some regulations will be specific to your organization or clients. For example, there is likely to be more regulation overseeing health care as AI is introduced to health care Internet of Things, patient diagnosis, and tools used for treatments. This is just one example, but any industry looking to integrate AI will likely run into compliance issues and regulations to oversee the way AI can be marketed and used. To ensure compliance, you can hire consultants to audit your environment and text for any privacy or regulation concerns. If your business supports multiple countries, you might need auditors with knowledge of EU or other country regulations.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Removing Bias: Human Emotions Are Not Objective"><div class="sect1" id="ch07_removing_bias_human_emotions_are_not_objective_1748358220835938">
        <h1>Removing Bias: Human Emotions Are Not Objective</h1>
        <p>For AI to<a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" data-secondary="bias" id="risk-bias"/><a contenteditable="false" data-type="indexterm" data-primary="bias in AI responses" id="bias-ch7"/> be effective, you need it to be neutral, bipartisan, and unbiased. <em>Bias</em> is a phenomenon in AI where output takes stereotypes or human subjectivity into account. It usually focuses on human biases, but bias can take different forms. For example, if you want to determine if someone should be accepted for a home loan, AI should not take race or religion into consideration even if this information is a part of the dataset. An SEO practitioner generating content for a site needs to ensure that output does not have bias. Bias is difficult to avoid because it’s human nature to inject our own experiences and opinions into content. A second human review can help avoid publishing content with bias. </p>
        <p>Most people watch for human bias and demographic stereotypes. Bias in AI can be a compliance issue, so it can result in monetary fines for certain businesses if it isn’t caught before decision making. As an example, US financial loan regulations specify the types of data that cannot be included in prediction analysis for mortgage approvals. Should an SEO practitioner unknowingly use this type of data to market or falsely advertise, that could lead to potential legal troubles.</p>
        <p>Let’s look at the results from a query in ChatGPT. The query asks, “What is the best security tool?” This question is likely too open for generative AI, but you might be creating a draft article for a cybersecurity company to compare some tools and drive search engine traffic to your site. OpenAI, the vendor for ChatGPT, is partnered with Microsoft. Microsoft has invested more than <a href="https://oreil.ly/CxLlb">$13 billion</a> in OpenAI since its partnership in 2019. It should be no surprise, then, when ChatGPT has biased output favoring Microsoft-based tools, as can be seen in <a data-type="xref" href="#ch07_figure_7_1748358220824797">Figure 7-7</a>.</p>
        <figure><div id="ch07_figure_7_1748358220824797" class="figure">
          <img src="assets/ugai_0707.png" width="748" height="507"/>
          <h6><span class="label">Figure 7-7. </span>ChatGPT 4.0 output for the query “What is the best security tool?” </h6>
        </div></figure>
        <p>Bias is difficult to identify, especially when it’s an unknown bias injected into data used for generative AI. You should run tests on your own output and have human reviewers validate your results. Data should be diverse, but this step also assumes that you are building your own tools. For smaller businesses, having a development team isn’t always feasible, but you should manually audit generative AI output to identify any potential bias flaws. For most businesses, building your own LLM is out of budget, but you can build a knowledge base from preexisting LLMs. It also helps to use trusted data sources. For example, the CIA offers <a href="https://oreil.ly/9LdDT"><em>The World Factbook</em></a> to help data scientists and businesses collect accurate, unbiased data on some topics.</p>
        <p>In most SEO environments, you don’t control the data, but you can control the output. It’s best to have a second person review your results or compare them against results from other tools. Not only can multiple result sets help you identify bias, but they can also help optimize your reporting. For example, you might discover better backlinking opportunities from one tool’s results as compared with another tool’s. As we have mentioned several times before, AI cannot replace human critical thinking, so your output should always be monitored and reviewed to ensure that you have the best results for<a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" data-secondary="bias" data-startref="risk-bias" id="id810"/><a contenteditable="false" data-type="indexterm" data-primary="bias in AI responses" data-startref="bias-ch7" id="id811"/> your sites.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Theft of Intellectual Property"><div class="sect1" id="ch07_theft_of_intellectual_property_1748358220835994">
        <h1>Theft of Intellectual Property</h1>
        <p>A challenge in <a contenteditable="false" data-type="indexterm" data-primary="risks of generative AI" data-secondary="intellectual property theft" id="id812"/><a contenteditable="false" data-type="indexterm" data-primary="intellectual property" id="id813"/>the digital landscape is the difficulty controlling who can access your intellectual property, particularly sophisticated AI bots designed to scrape the internet and steal content. While preventing such activity is often impossible, SEO practitioners should be aware of this threat, especially when it comes to protecting their brand’s reputation.</p>
        <p>Content theft can negatively affect your SEO efforts in several ways. For example, a malicious actor might republish your content on their own site in the hopes of ranking for your target keywords. A copycat site could use your brand’s reputation to fraudulently sell products and services, stealing your revenue. One way to combat this is to use AI to help you monitor the web to detect content theft.</p>
        <p>Generative AI introduces new dimensions to content theft. Malicious actors can now leverage tools to not just copy but also re-create and repurpose your content in ways that may be harder to detect and combat. In the US, you can always try reporting the problem to the National Intellectual Property Rights Coordination Center, but this is a manual process, and having to report each incident can become an overwhelming task. </p>
        <p>Then there is the flip side of the coin—if you are using AI to generate logos, characters, and images, make sure that the content created does not accidentally infringe on a trademark.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Impact of AI Limitations on SEO"><div class="sect1" id="ch07_impact_of_ai_limitations_on_seo_1748358220836058">
        <h1>Impact of AI Limitations on SEO</h1>
        <p>Google has <a contenteditable="false" data-type="indexterm" data-primary="generative AI" data-secondary="advantages and disadvantages of" id="generative-ai-advantage"/>always pushed quality over quantity, but generative AI has made quantity much more attainable. <a href="https://oreil.ly/yVsdZ">A study by WebFX on generative AI and its impact on search ranking</a> highlights the advantages and disadvantages of content generation.</p>
        <p>Advantages:</p>
        <ul>
          <li>
            <p>Content creation is more efficient and faster, even with a human editor involved.</p>
          </li>
          <li>
            <p>Content creation is more cost-effective without the time and human resources needed to generate new ideas and content.</p>
          </li>
          <li>
            <p>You can generate content ideas continuously rather than relying on human research.</p>
          </li>
          <li>
            <p>You can pool research and fact-checking into a single location.</p>
          </li>
        </ul>
        <p>Disadvantages:</p>
        <ul>
          <li>
            <p>There is no personal touch. In human-written content, personal experiences and anecdotes often help engage readers. AI-generated content has none of these traits and can sound robotic.</p>
          </li>
          <li>
            <p>It’s important to fact-check and review all AI-generated content to avoid errors. Error by omission is also a common disadvantage, meaning content might be technically accurate in what it says but a lack of context or additional information makes the statement misinformation.</p>
          </li>
          <li>
            <p>Generative AI models are trained on current data, so the content created is on evergreen knowledge. You won’t get any current news or new information that the model has not been trained on. Models are trained only a few times a year, so information might be outdated.</p>
          </li>
        </ul>
        <p>The <a contenteditable="false" data-type="indexterm" data-primary="generative AI" data-secondary="advantages and disadvantages of" data-startref="generative-ai-advantage" id="id814"/>WebFX case study indicates that AI-generated content could harm ranking, especially if your pages are already ranking. The example given was a lawn care site already ranking for targeted keywords. AI-generated content was added to the site, but it was regurgitated information already on the web. Pages with AI-generated content not only saw minimal traffic but eventually lost 100% of their ranking after a short time. </p>
        <p>It’s important <a contenteditable="false" data-type="indexterm" data-primary="ranking" data-secondary="losing" id="id815"/><a contenteditable="false" data-type="indexterm" data-primary="losing rankings" id="id816"/><a contenteditable="false" data-type="indexterm" data-primary="EEAT (experience, expertise, authoritativeness, trustworthiness)" id="id817"/>to remember that Google hires contractors to review ranking pages. Google asks contractors to evaluate the <a href="https://oreil.ly/Lo1fr">EEAT of content</a>, which means that content must display experience, expertise, authoritativeness, and trustworthiness. The same WebFX study said that a financial site with mostly AI-generated content saw a 99.3% drop in ranking after the November 2023 EEAT core update. AI-generated content doesn’t usually exhibit strong EEAT, so have your human editors determine if content could be considered high quality based on EEAT. </p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="ch07_conclusion_1748358220836113">
        <h1>Conclusion</h1>
        <p>In this chapter, we went over how you can overcome some of the common pitfalls when using AI. Succumbing to these risks can have devastating effects on your brand and revenue. Here’s a brief recap: </p>
        <dl>
          <dt>Low-quality content</dt>
          <dd>
            <p>Whether it’s generating images, video or text, AI has its limitations, and a human reviewer should be injected into your procedures to ensure content quality. The main risk of poor generative AI strategies is low-quality content. It might not seem like the worst risk, but the consequences can be significant (e.g., loss of search rank or search engine penalties, which both result in fewer sales).</p>
          </dd>
          <dt>Copyright and plagiarism</dt>
          <dd>
            <p>Generative AI platforms ingest data from the internet, so output can be too similar to the original content. Several ethical and legal issues stem from this challenge. </p>
          </dd>
          <dt>Automation complacency</dt>
          <dd>
            <p>Automation is one of AI’s greatest benefits, but it’s not a “set it and forget it” application. We discussed SEO automation in <a data-type="xref" href="ch06.html#ch06_advanced_use_cases_for_generative_ai_in_seo_1748359259642070">Chapter 6</a>. AI results must be constantly reviewed by humans, regardless of whether you use it for just a few pieces of content or you deploy it at scale. </p>
          </dd>
          <dt>Google authority loss and penalties</dt>
          <dd>
            <p>User experience is key to following Google’s search engine guidelines and ranking well, but analysis shows that AI-produced content published without detailed human review results in poor user experiences and can cause your search rankings to plummet.</p>
          </dd>
          <dt>Compliance</dt>
          <dd>
            <p>Is the content output compliant with local and federal laws? The EU passed the AI Act, which aims to regulate AI and its products. The AI Act focuses on two concerns: it bans social scoring similar to China’s social scoring system, and it regulates CV (resume) ranking tools, which must follow strict regulations. US regulations are soon to follow.</p>
          </dd>
          <dt>Biases</dt>
          <dd>
            <p>For large agencies or businesses with the staff to create their own models and tools, it’s critical that output is tested for any biases that can creep into your content and cause issues with readers and local laws. For example, feed ChatGPT the query “Should marijuana be fully legalized nationwide in the US?” and notice that the response doesn’t give you a direct answer. </p>
          </dd>
          <dt>Theft of intellectual property</dt>
          <dd>
            <p>An uncontrollable but related issue is the use of AI to bypass common blocks (e.g., <em>robots.txt</em>) on traffic or bots used to steal your intellectual property. For example, if you use <em>robots.txt</em> to block traffic, you have no guarantee that crawlers will honor it. After stealing your content, AI can be used to create intellectual property like your own without detection. </p>
          </dd>
        </dl>
        <p>This chapter is meant to inform rather than deter. You need to know what to avoid and integrate these challenges into your SEO strategies. These challenges also shape the future of AI, how it will evolve, and the regulations surrounding it. The next chapter will cover the future of AI and SEO and what you can look forward to (or possibly avoid) as you continue using it.</p>
      </div></section>
    </div></section></div></div></body></html>