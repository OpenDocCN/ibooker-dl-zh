- en: Chapter 11\. Real-Time Object Classification on iOS with Core ML
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。在iOS上使用Core ML进行实时对象分类
- en: So far, we have seen our deep learning models running on the desktop, the cloud,
    and the browser. Although there are definite upsides to such a setup, it might
    not be ideal for all scenarios. In this chapter, we explore making predictions
    using deep learning models on mobile devices.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到我们的深度学习模型在桌面、云端和浏览器上运行。尽管这种设置有明显的优势，但并不适用于所有情况。在本章中，我们将探讨在移动设备上使用深度学习模型进行预测。
- en: 'Bringing the computation closer to the user’s device, rather than a distant
    remote server, can be advantageous for many reasons:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 将计算带到用户设备附近，而不是远程服务器，对于许多原因都是有利的：
- en: Latency and interactivity
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟和互动性
- en: 'Sending an image, processing it in the cloud, and returning the result can
    take several seconds depending on the network quality and quantity of data being
    transferred. This can make for a poor UX. Decades of UX research, including Jakob
    Nielsen’s findings in 1993, published in his book *Usability Engineering* (Elsevier),
    showed the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 发送图像，将其在云端处理，然后返回结果可能需要几秒钟，具体取决于网络质量和传输数据的数量。这可能导致糟糕的用户体验。几十年的用户体验研究，包括Jakob
    Nielsen在1993年发表在他的书《可用性工程》（Elsevier）中的发现，显示如下：
- en: 0.1 second is about the limit for having the user feel that the system is reacting
    instantaneously.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0.1秒是让用户感觉系统反应即时的极限。
- en: 1 second is about the limit for the user’s flow of thought to stay uninterrupted.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1秒是用户思维流畅保持不间断的极限。
- en: 10 seconds is about the limit for keeping the user’s attention focused.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 10秒是保持用户注意力集中的极限。
- en: About two decades later, Google published findings that half of all mobile browser
    users abandon a web page if it takes longer than three seconds to load. Forget
    three seconds, even a 100 ms increase in latency would result in a 1% decrease
    in sales for Amazon. That is a lot of lost revenue. Mitigating this by processing
    on the device instantaneously can make for rich and interactive UXs. Running deep
    learning models in real time, as is done with Snapchat Lenses, can increase engagement
    with users.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大约两十年后，谷歌发布了一项研究结果，发现如果移动浏览器加载一个网页超过三秒，一半的用户会放弃。甚至忘记三秒，延迟增加100毫秒都会导致亚马逊销售额下降1%。这是很多损失的收入。通过立即在设备上处理来减轻这种情况可以实现丰富和互动的用户体验。实时运行深度学习模型，就像Snapchat
    Lenses所做的那样，可以增加与用户的互动。
- en: 24/7 availability and reduced cloud costs
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 全天候可用性和降低云成本
- en: Obviously, sending less data to the cloud equates to less computing costs for
    the developer, leading to monetary savings. This reduces scaling costs, as well,
    when an app gains traction and grows to a large user base. For the users, computation
    on the edge is helpful, too, because they don’t need to worry about data plan
    costs. Additionally, processing locally means 24/7 availability, without the fear
    of losing connectivity.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，向云端发送的数据越少，对于开发者来说意味着更少的计算成本，从而节省金钱。当应用程序获得关注并发展成大量用户群时，这也减少了扩展成本。对于用户来说，在边缘计算上进行计算也很有帮助，因为他们不需要担心数据计划成本。此外，本地处理意味着全天候可用性，无需担心失去连接的恐惧。
- en: Privacy
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私
- en: For the user, local computation preserves privacy by not sharing the data externally,
    which could potentially be mined for user information. For the developer, this
    ensures less headache dealing with Personally Identifiable Information (PII).
    With the European Union’s General Data Protection Regulation (GDPR) and other
    user data protection laws coming up around the world, this becomes even more important.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于用户来说，本地计算通过不外部共享数据来保护隐私，这些数据可能被挖掘用于用户信息。对于开发者来说，这确保了处理个人可识别信息（PII）时更少的麻烦。随着欧盟的《通用数据保护条例》（GDPR）和其他全球用户数据保护法律的出台，这变得更加重要。
- en: 'Hopefully, these arguments were convincing for why AI on mobile is important.
    For someone building any kind of serious application, the following are a few
    common questions to consider during the course of development:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这些论点能够说明为什么移动设备上的人工智能很重要。对于构建任何严肃应用程序的人来说，在开发过程中考虑以下几个常见问题：
- en: How do I convert my model to run on a smartphone?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将我的模型转换为在智能手机上运行？
- en: Will my model run on other platforms?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的模型是否能在其他平台上运行？
- en: How do I run my model fast?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何快速运行我的模型？
- en: How do I minimize the size of the app?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何最小化应用程序的大小？
- en: How do I ensure that my app doesn’t drain the battery?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何确保我的应用程序不会耗尽电池？
- en: How do I update my model without going through the roughly two-day app review
    process?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在不经历大约两天的应用审核过程的情况下更新我的模型？
- en: How do I A/B test my models?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何进行A/B测试我的模型？
- en: Can I train a model on a device?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我能在设备上训练模型吗？
- en: How do I protect my intellectual property (i.e., model) from being stolen?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何保护我的知识产权（即模型）不被盗窃？
- en: In the next three chapters, we look at how to run deep learning algorithms on
    smartphones using different frameworks. In the process, we answer these questions
    as they come up.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的三章中，我们将探讨如何使用不同的框架在智能手机上运行深度学习算法。在这个过程中，我们会回答这些问题。
- en: In this chapter, we delve into the world of mobile AI for iOS devices. We first
    look at the general end-to-end software life cycle (shown in [Figure 11-1](part0013.html#the_mobile_ai_development_life_cycle))
    and see how the different parts fit in with one another. We explore the Core ML
    ecosystem, its history, and the features offered. Next, we deploy a real-time
    object classification app on an iOS device, and learn performance optimizations
    and benchmarks. And finally, we analyze some real-world apps built on Core ML.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了iOS设备的移动人工智能世界。我们首先看一下一般的端到端软件生命周期（在[图11-1](part0013.html#the_mobile_ai_development_life_cycle)中显示），看看不同部分如何相互配合。我们探索Core
    ML生态系统，其历史和提供的功能。接下来，我们在iOS设备上部署一个实时对象分类应用程序，并学习性能优化和基准测试。最后，我们分析了一些基于Core ML构建的真实应用程序。
- en: Time to look at the big picture.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候看看全局了。
- en: The Development Life Cycle for Artificial Intelligence on Mobile
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移动设备上人工智能的开发生命周期
- en: '[Figure 11-1](part0013.html#the_mobile_ai_development_life_cycle) depicts the
    typical life cycle for AI on mobile devices.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11-1](part0013.html#the_mobile_ai_development_life_cycle)描述了移动设备上AI的典型生命周期。'
- en: '![The mobile AI development life cycle](../images/00199.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![移动AI开发生命周期](../images/00199.jpeg)'
- en: Figure 11-1\. The mobile AI development life cycle
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-1\. 移动AI开发生命周期
- en: 'Let’s look at the phases in [Figure 11-1](part0013.html#the_mobile_ai_development_life_cycle)
    a bit more closely:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看一下[图11-1](part0013.html#the_mobile_ai_development_life_cycle)中的阶段：
- en: '*Collect data*: The data we collect should reflect the context in which the
    app would be used. Pictures taken by real users using smartphone cameras tend
    to be better training examples than pictures taken by professional photographers.
    We might not have this data on day one, but we can progressively collect more
    and more data as usage grows. A good starting point in many cases is to download
    images from search engines.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*收集数据*: 我们收集的数据应反映应用程序的使用环境。由智能手机摄像头拍摄的真实用户照片往往比专业摄影师拍摄的照片更好作为训练示例。我们可能在第一天没有这些数据，但随着使用量的增长，我们可以逐渐收集更多数据。在许多情况下，一个很好的起点是从搜索引擎下载图像。'
- en: '*Label data*: We need associated labels for the data samples that we want our
    model to predict. High-quality (i.e., correct) labels are vital to a good model.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*标记数据*: 我们需要与我们希望我们的模型预测的数据样本相关联的标签。高质量（即正确）的标签对于一个好的模型至关重要。'
- en: '*Train model*: We build the highest-accuracy neural network possible with the
    data and associated labels we have so far.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*训练模型*: 我们利用迄今为止拥有的数据和相关标签构建最高准确度的神经网络。'
- en: '*Convert model*: Export the model from the training framework into the mobile-compatible
    framework.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*转换模型*: 将模型从训练框架导出到移动兼容的框架。'
- en: '*Optimize performance*: Due to the resource-constrained nature of a mobile
    device, it is crucial to make the model efficient for memory, energy, and processor
    usage.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*优化性能*: 由于移动设备资源受限，对于内存、能源和处理器的使用效率至关重要。'
- en: '*Deploy*: Add the model to the app and ship it to users.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*部署*: 将模型添加到应用程序中并将其交付给用户。'
- en: '*Monitor*: Track the app usage in the real world to find opportunities to improve
    further. Additionally, gather samples of real-word data from consenting users
    to feed to this life cycle, and then back to Step 1.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*监控*: 跟踪应用在现实世界中的使用情况，找到进一步改进的机会。此外，收集愿意同意的用户的真实数据样本，以供这个生命周期使用，然后回到第一步。'
- en: In the first portion of the book, we primarily explored phases 1, 2, and 3,
    and additionally general performance improvements. In this chapter, we focus on
    phases 4, 5, and 6\. And in the next few chapters, we explore all of these phases
    in the context of mobile development.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前部分，我们主要探讨了阶段1、2和3，以及一般性能改进。在本章中，我们将重点放在阶段4、5和6上。在接下来的几章中，我们将探讨所有这些阶段在移动开发的背景下的应用。
- en: Tip
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Before getting the app in the hands of real users (who might hate it if the
    app doesn’t perform as well as they expected), it’s common practice to gather
    feedback through a process called dogfooding. As the saying goes: Eat Your Own
    Dog Food. The process involves having an inner circle of loyal users who get to
    test early releases, thereby identifying bugs before they go out to the general
    public. For AI development, this inner circle might also contribute data and assess
    the success of the AI models in the real world. And as the models improve, they
    can be deployed to a gradually increasing number of test users before finally
    deploying to the public.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在将应用交到真实用户手中之前（如果应用表现不如他们期望的那样，他们可能会讨厌它），通过一种称为“dogfooding”的过程收集反馈是常见做法。俗话说：吃自己的狗粮。这个过程涉及拥有一群忠实用户的内部圈子，他们可以测试早期版本，从而在发布给公众之前识别错误。对于AI开发，这个内部圈子也可能贡献数据，并评估AI模型在现实世界中的成功。随着模型的改进，它们可以逐渐部署到越来越多的测试用户，最终部署到公众。
- en: Let’s jump right in!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: A Brief History of Core ML
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Core ML简史
- en: Core ML provides one of the simplest ways to run a deep neural network for inference
    on an Apple device such as the iPhone and iPad as well as MacBook, Apple TV, and
    Apple Watch. In addition to being easy to use, it is also optimized for the underlying
    hardware architecture. Alternative frameworks have become a lot better in the
    past few years, but it’s difficult to beat the simplicity and performance offered
    by Core ML.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Core ML为在苹果设备（如iPhone和iPad以及MacBook、Apple TV和Apple Watch）上运行深度神经网络提供了一种最简单的方式。除了易于使用外，它还针对底层硬件架构进行了优化。在过去几年中，替代框架已经变得更好了，但很难超越Core
    ML提供的简单性和性能。
- en: Traditionally, to quickly run a CNN on an Apple device, developers *needed*
    to write on Metal, a library that was offered to game developers to better utilize
    the GPU. Unfortunately, developing on Metal was akin to writing in assembly language
    or CUDA code for an NVIDIA GPU. It was tedious, error prone, and difficult to
    debug. Few developers dared to tread that path. DeepLearningKit (December 2015)
    by Amund Tveit was one effort to build an abstraction over Metal for deploying
    CNNs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，要在苹果设备上快速运行CNN，开发人员*需要*使用Metal编写，这是一个为游戏开发人员提供的库，用于更好地利用GPU。不幸的是，使用Metal进行开发就像是在汇编语言或CUDA代码中编写一样。这是繁琐、容易出错且难以调试的。很少有开发人员敢于踏上这条道路。Amund
    Tveit于2015年12月推出的DeepLearningKit是为部署CNN构建一个对Metal的抽象的努力。
- en: At the Apple Worldwide Developers Conference (WWDC) 2016, the company announced
    Metal Performance Shaders (MPS), a framework built on top of Metal, as a high-performance
    library for optimizing graphics and certain compute operations. It abstracted
    away a lot of low-level details, giving basic building blocks such as Convolution,
    Pooling, and ReLU. It allowed developers to write deep neural networks by combining
    these operations in code. For anyone who comes from the Keras world, this is a
    familiar and not-so-daunting task. Unfortunately, there is a lot of bookkeeping
    involved when writing MPS code, because you need to manually keep track of input
    and output dimensions at each step of the process. As an example, the code sample
    released by Apple for running the InceptionV3 model for recognizing 1,000 object
    categories was well over 2,000 lines long, with most of them defining the network.
    Now imagine changing the model slightly during training, and then having to dig
    through all 2,000 lines of code to reflect the same update in iOS code. Forge
    (April 2017), a library by Matthijs Hollemans, was an effort to simplify development
    over MPS by reducing the boilerplate code necessary to get a model running.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在2016年的苹果全球开发者大会（WWDC）上，该公司宣布了Metal Performance Shaders（MPS），这是建立在Metal之上的一个高性能库，用于优化图形和某些计算操作。它抽象了许多底层细节，提供了基本构建模块，如卷积、池化和ReLU。它允许开发人员通过在代码中组合这些操作来编写深度神经网络。对于来自Keras世界的任何人来说，这是一个熟悉而不那么令人畏惧的任务。不幸的是，在编写MPS代码时涉及了大量的繁琐工作，因为您需要在每个步骤中手动跟踪输入和输出维度。例如，苹果发布的用于识别1,000个对象类别的InceptionV3模型的代码示例超过2,000行，其中大部分定义了网络。现在想象一下在训练过程中稍微更改模型，然后不得不查看所有2,000行代码以在iOS代码中反映相同的更新。Forge（2017年4月），由Matthijs
    Hollemans开发的一个库，是为了简化MPS开发，减少启动模型所需的样板代码。
- en: 'All of these hardships went away when Apple announced Core ML at WWDC 2017\.
    This included an inference engine on iOS and an open source Python package called
    Core ML Tools to serialize CNN models from other frameworks like Keras and Caffe.
    The general workflow for building an app was: train a model in other packages,
    convert it to a *.mlmodel* file, and deploy it in an iOS app running on the Core
    ML platform.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当苹果在2017年的WWDC上宣布Core ML时，所有这些困难都消失了。这包括iOS上的推理引擎和一个名为Core ML Tools的开源Python包，用于序列化来自Keras和Caffe等其他框架的CNN模型。构建应用程序的一般工作流程是：在其他软件包中训练模型，将其转换为*.mlmodel*文件，并部署在运行在Core
    ML平台上的iOS应用程序中。
- en: Core ML supports importing a broad range of machine learning models built with
    first- and third-party frameworks and file formats. [Figure 11-2](part0013.html#frameworks_compatible_with_core_ml_for_m)
    showcases a few of them (clockwise, starting in the upper left) such as TensorFlow,
    Keras, ONNX, scikit-learn, Caffe2, Apple’s Create ML, LIBSVM, and TuriCreate (also
    from Apple). ONNX itself supports a large variety of frameworks, including PyTorch
    (Facebook), MXNet (Amazon), Cognitive Toolkit (Microsoft), PaddlePaddle (Baidu),
    and more, thereby ensuring compatibility with any major framework under the sky.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Core ML支持导入使用第一方和第三方框架和文件格式构建的广泛范围的机器学习模型。[图11-2](part0013.html#frameworks_compatible_with_core_ml_for_m)展示了其中一些（顺时针，从左上角开始），如TensorFlow、Keras、ONNX、scikit-learn、Caffe2、苹果的Create
    ML、LIBSVM和TuriCreate（也来自苹果）。ONNX本身支持各种框架，包括PyTorch（Facebook）、MXNet（亚马逊）、Cognitive
    Toolkit（微软）、PaddlePaddle（百度）等，从而确保与天下任何主要框架的兼容性。
- en: '![Frameworks compatible with Core ML for model interchange as of 2019](../images/00181.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![截至2019年与Core ML兼容的框架](../images/00181.jpeg)'
- en: Figure 11-2\. Frameworks compatible with Core ML for model interchange as of
    2019
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-2。截至2019年与Core ML兼容的框架
- en: Alternatives to Core ML
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Core ML的替代方案
- en: Depending on the platform, there are a handful of options to achieve real-time
    predictions. These include general-purpose inference frameworks such as Core ML
    (from Apple), TensorFlow Lite (from Google), ML Kit (also from Google), and Fritz,
    as well as chip-specific accelerator frameworks including Snapdragon Neural Processing
    Engine (from Qualcomm) and Huawei AI Mobile Computing Platform (for Huawei’s Neural
    Processing Unit). [Table 11-1](part0013.html#a_comparison_of_mobile_device_ai_framewo)
    presents a high-level comparison of all these frameworks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 根据平台的不同，有几种选项可以实现实时预测。这些包括通用推理框架，如Core ML（来自苹果）、TensorFlow Lite（来自谷歌）、ML Kit（也来自谷歌）和Fritz，以及特定芯片加速器框架，包括骁龙神经处理引擎（来自高通）和华为AI移动计算平台（用于华为的神经处理单元）。[表11-1](part0013.html#a_comparison_of_mobile_device_ai_framewo)提供了所有这些框架的高级比较。
- en: Table 11-1\. A comparison of mobile device AI frameworks
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-1。移动设备AI框架的比较
- en: '| **Framework** | **Available for iOS** | **Available for Android** | **Dynamic
    updates** | **A/B testing** | **On-device training** | **Model encryption** |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **框架** | **适用于iOS** | **适用于Android** | **动态更新** | **A/B测试** | **设备上的训练**
    | **模型加密** |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Core ML | ✓ | — | ✓ | — | ✓ | — |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Core ML | ✓ | — | ✓ | — | ✓ | — |'
- en: '| TensorFlow Lite | ✓ | ✓ | — | — | Releases late 2019 | — |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| TensorFlow Lite | ✓ | ✓ | — | — | 2019年底发布 | — |'
- en: '| ML Kit | ✓ | ✓ | ✓ | ✓ | — | — |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| ML Kit | ✓ | ✓ | ✓ | ✓ | — | — |'
- en: '| Fritz | ✓ | ✓ | ✓ | ✓ | — | ✓ |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Fritz | ✓ | ✓ | ✓ | ✓ | — | ✓ |'
- en: TensorFlow Lite
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Lite
- en: In November 2017, Google announced an on-device inference engine called TensorFlow
    Lite with the intention of extending the TensorFlow ecosystem beyond just servers
    and PCs. Prior to this, the options within the TensorFlow ecosystem were porting
    the entire TensorFlow library, itself ported to iOS (which was heavy and slow),
    and later on, its slightly stripped-down version called TensorFlow Mobile (which
    was still pretty bulky).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年11月，谷歌宣布推出了一个名为TensorFlow Lite的设备上推理引擎，旨在将TensorFlow生态系统扩展到服务器和个人电脑之外。在此之前，TensorFlow生态系统内的选项包括将整个TensorFlow库移植到iOS（这很沉重且缓慢），以及稍后推出的其略微简化版本称为TensorFlow
    Mobile（仍然相当庞大）。
- en: TensorFlow Lite was rebuilt from the ground up targeting mobile and edge devices,
    optimizing for speed, model and interpreter size, and power consumption. It added
    support for GPU backend delegates, meaning that as long as there was GPU support
    implemented for a hardware platform, TensorFlow Lite could take advantage of the
    power of the GPU. On iOS, the GPU delegate uses Metal for acceleration. We discuss
    TensorFlow Lite at greater length in [Chapter 13](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite从头开始重建，针对移动和边缘设备进行优化，优化速度、模型和解释器大小以及功耗。它增加了对GPU后端代理的支持，这意味着只要为硬件平台实现了GPU支持，TensorFlow
    Lite就可以利用GPU的强大功能。在iOS上，GPU代理使用Metal进行加速。我们在[第13章](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b)中更详细地讨论了TensorFlow
    Lite。
- en: ML Kit
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ML Kit
- en: ML Kit is a high-level library from Google that provides many computer vision,
    NLP, and AI functionalities out of the box, including the ability to run TensorFlow
    Lite models. Some of the features include face detection, barcode scanning, smart
    reply, on-device translation, and language identification. However, the main selling
    point of ML Kit is its integration with Google Firebase. Features offered by Firebase
    include dynamic model updates, A/B testing, and remote configuration-driven dynamic
    model selection (fancy words for choosing which model to use based on the customer).
    We explore ML Kit in greater detail in [Chapter 13](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ML Kit是谷歌提供的一个高级库，提供了许多计算机视觉、自然语言处理和人工智能功能，包括运行TensorFlow Lite模型的能力。一些功能包括人脸检测、条形码扫描、智能回复、设备上的翻译和语言识别。然而，ML
    Kit的主要卖点是与Google Firebase的集成。Firebase提供的功能包括动态模型更新、A/B测试和基于远程配置驱动的动态模型选择（根据客户选择使用哪个模型的花哨词）。我们在[第13章](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b)中更详细地探讨了ML
    Kit。
- en: Fritz
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Fritz
- en: Fritz is a startup founded with the goal of making the end-to-end process of
    mobile inference easier. It bridges the gap between machine learning practitioners
    and mobile engineers by providing easy-to-use command-line tools. On one side,
    it integrates training in Keras directly into the deployment pipeline, so a machine
    learning engineer could add a single line of Keras callback to deploy the model
    to users immediately after it finishes training. On the other side, a mobile engineer
    can benchmark the model without even needing to deploy to a physical device, simulating
    a model’s performance virtually, assess a Keras model’s compatibility with Core
    ML, and get analytics for each model. One unique selling point of Fritz is the
    model protection feature that prevents a model from deep inspection through obfuscation
    in the event that a phone is jailbroken.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Fritz是一家创立的初创公司，旨在使移动推断的端到端过程更加简单。它通过提供易于使用的命令行工具，弥合了机器学习从业者和移动工程师之间的鸿沟。一方面，它将Keras中的训练直接集成到部署流水线中，因此机器学习工程师可以在训练完成后添加一行Keras回调即可立即将模型部署给用户。另一方面，移动工程师可以在不需要部署到物理设备的情况下对模型进行基准测试，通过虚拟模拟模型的性能，评估Keras模型与Core
    ML的兼容性，并为每个模型获取分析数据。Fritz的一个独特卖点是模型保护功能，通过混淆防止模型在手机越狱时进行深度检查。
- en: Apple’s Machine Learning Architecture
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 苹果的机器学习架构
- en: To gain a better understanding of the Core ML ecosystem, it’s useful to see
    the high-level picture of all the different APIs that Apple offers as well as
    how they fit with one another. [Figure 11-3](part0013.html#different_levels_of_apis_provided_by_app)
    gives us a look at the different components that make up Apple’s machine learning
    architecture.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解Core ML生态系统，有必要看到苹果提供的所有不同API以及它们如何相互配合。[图11-3](part0013.html#different_levels_of_apis_provided_by_app)让我们看到了构成苹果机器学习架构的不同组件。
- en: '![Different levels of APIs provided by Apple for app developers](../images/00240.jpeg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![苹果为应用开发人员提供的不同级别的API](../images/00240.jpeg)'
- en: Figure 11-3\. Different levels of APIs provided by Apple for app developers
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-3\. 苹果为应用开发人员提供的不同级别的API
- en: Domain-Based Frameworks
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于领域的框架
- en: To simplify common tasks in machine learning without requiring domain expertise,
    Apple provides many APIs out of the box, in domains including Vision, Natural
    Language, Speech, and Sound Analysis. [Table 11-2](part0013.html#out-of-the-box_machine_learning_function)
    gives a detailed outline of the functionalities available on Apple operating systems.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化机器学习中的常见任务，不需要领域专业知识，苹果提供了许多开箱即用的API，涵盖视觉、自然语言、语音和声音分析等领域。[表11-2](part0013.html#out-of-the-box_machine_learning_function)详细概述了苹果操作系统上可用的功能。
- en: Table 11-2\. Out-of-the-box machine learning functionality in Apple operating
    systems
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-2\. 苹果操作系统中的开箱即用机器学习功能
- en: '| **Vision** | **Natural language** | **Other** |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| **视觉** | **自然语言** | **其他** |'
- en: '| --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Facial landmark detection
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部特征点检测
- en: Image similarity
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像相似度
- en: Saliency detection
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显著性检测
- en: Optical character recognition
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 光学字符识别
- en: Rectangle detection
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩形检测
- en: Face detection
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人脸检测
- en: Object classification
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象分类
- en: Barcode detection
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条形码检测
- en: Horizon detection
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地平线检测
- en: Human and animal detection
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类和动物检测
- en: Object tracking (for video)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象跟踪（用于视频）
- en: '|'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Tokenization
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记化
- en: Language identification
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言识别
- en: Parts of speech identification
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性识别
- en: Text embedding
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本嵌入
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Speech recognition (on-device and on-cloud)
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音识别（设备上和云端）
- en: Sound classification
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声音分类
- en: '|'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: ML Framework
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ML框架
- en: Core ML gives the ability to run inference on deep learning and machine learning
    models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Core ML具有在深度学习和机器学习模型上运行推断的能力。
- en: ML Performance Primitives
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ML性能基元
- en: 'The following are some of the machine learning primitives in the Apple stack:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是苹果堆栈中的一些机器学习基元：
- en: MPS
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: MPS
- en: Provides low-level and high-performance primitives that utilize the GPU to aid
    running most CNN-based networks fast. And if Core ML does not support a model,
    MPS provides all of the building blocks to enable us to build them. Additionally,
    we might consider using MPS to hand roll a model for performance reasons (like
    guaranteeing that the model runs on the GPU).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 提供低级和高性能的基元，利用GPU来快速运行大多数基于CNN的网络。如果Core ML不支持某个模型，MPS提供了所有构建块，使我们能够构建它们。此外，我们可能考虑使用MPS手动创建一个模型以提高性能（例如保证模型在GPU上运行）。
- en: Accelerate and Basic Neural Network Subroutine
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 加速和基本神经网络子程序
- en: Accelerate is Apple’s implementation of the Basic Linear Algebra Subprogram
    (BLAS) library. It provides functionality for high-performance large-scale mathematical
    computations and image calculations, like Basic Neural Network Subroutine (BNNS),
    which helps to implement and run neural networks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 加速是苹果对基本线性代数子程序（BLAS）库的实现。它提供了用于高性能大规模数学计算和图像计算的功能，如基本神经网络子程序（BNNS），有助于实现和运行神经网络。
- en: Now that we have seen how Core ML and the domain-specific APIs fit into the
    overall architecture, let’s see how little work is needed to run a machine learning
    model using Core ML and the Vision framework on an iOS app.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经看到了Core ML和特定领域API如何融入整体架构，让我们看看在iOS应用程序上运行机器学习模型所需的工作量是多么少。
- en: Tip
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Apple provides several downloadable models ([Figure 11-4](part0013.html#ready-to-use_models_from_the_apple_machi))
    for various computer-vision tasks, from classification to detecting objects (with
    bounding boxes), segmenting (identifying the pixels), depth estimation, and more.
    You can find them at [*https://developer.apple.com/machine-learning/models/*](https://developer.apple.com/machine-learning/models).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果提供了几个可下载的模型（[图11-4](part0013.html#ready-to-use_models_from_the_apple_machi)），用于各种计算机视觉任务，从分类到检测物体（带有边界框），分割（识别像素），深度估计等。您可以在[*https://developer.apple.com/machine-learning/models/*](https://developer.apple.com/machine-learning/models)找到它们。
- en: For classification, you can find many pretrained Core ML models on Apple’s Machine
    Learning website, including MobileNet, SqueezeNet, ResNet-50, and VGG16.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类，您可以在苹果的机器学习网站上找到许多预训练的Core ML模型，包括MobileNet、SqueezeNet、ResNet-50和VGG16。
- en: '![Ready-to-use models from the Apple Machine Learning website](../images/00074.jpeg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![苹果机器学习网站上的现成模型](../images/00074.jpeg)'
- en: Figure 11-4\. Ready-to-use models from the Apple Machine Learning website
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-4\. 苹果机器学习网站上的现成模型
- en: Building a Real-Time Object Recognition App
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建实时物体识别应用程序
- en: Although we don’t intend to teach iOS development, we want to demonstrate running
    an object recognition model that classifies among 1,000 ImageNet categories in
    real time on an iOS device.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不打算教授iOS开发，但我们想演示在iOS设备上实时运行一个能够在1,000个ImageNet类别中进行分类的物体识别模型。
- en: 'We will examine the minimal amount of code necessary to get this app running.
    The general outline is as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将检查运行此应用程序所需的最少代码量。一般的概述如下：
- en: Drag and drop an *.mlmodel* file to the Xcode project.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将*.mlmodel*文件拖放到Xcode项目中。
- en: Load the model into a Vision container (`VNCoreMLModel`).
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型加载到Vision容器（`VNCoreMLModel`）中。
- en: Create a request (`VNCoreMLRequest`) based on that container and provide a function
    that would be called when the request is completed.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于该容器创建一个请求（`VNCoreMLRequest`），并提供一个在请求完成时将被调用的函数。
- en: Create a request handler that can process the request based on an image provided.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个请求处理程序，可以根据提供的图像处理请求。
- en: Execute the request and print out the results.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行请求并打印结果。
- en: 'Let’s take a look at the code to do this:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何编写代码来实现这一点：
- en: '[PRE0]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this code, `cgImage` can be an image from a variety of sources. It can be
    a photo from the photo library, or from the web.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，`cgImage`可以是来自各种来源的图像。它可以是来自照片库或网络的照片。
- en: We can also power real-time scenarios using the camera and pass the individual
    camera frames into this function. A regular iPhone camera can shoot up to 60 frames
    per second (FPS).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用摄像头实时场景，并将单个摄像头帧传递到此函数中。普通的iPhone摄像头可以每秒拍摄高达60帧。
- en: 'By default, Core ML crops the image along the longer edge. In other words,
    if a model requires square dimensions, Core ML will extract the biggest square
    in the center of the image. This can be a source of confusion for developers who
    find the top and bottom strips of an image are being ignored for making predictions.
    Depending on the scenario, we might want to use `.centerCrop`, `.scaleFit`, or
    the `.scaleFill` option, as illustrated in [Figure 11-5](part0013.html#how_different_scaling_options_modify_the),
    such as the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Core ML沿着较长的边裁剪图像。换句话说，如果模型需要方形尺寸，Core ML将提取图像中心的最大方形。这可能会让开发人员感到困惑，因为他们发现图像的顶部和底部条被忽略以进行预测。根据情况，我们可能希望使用`.centerCrop`、`.scaleFit`或`.scaleFill`选项，如[图11-5](part0013.html#how_different_scaling_options_modify_the)所示：
- en: '[PRE1]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![How different scaling options modify the input image to Core ML models](../images/00054.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![不同缩放选项如何修改输入图像到Core ML模型](../images/00054.jpeg)'
- en: Figure 11-5\. How different scaling options modify the input image to Core ML
    models
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-5\. 不同缩放选项如何修改输入图像到Core ML模型
- en: 'Now that we have gone through the meat of it, what could be more fun? How about
    actually running it on a phone! We’ve made a complete app available on the book’s
    GitHub website (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    at *code/chapter-11*. With an iPhone or an iPad, we can deploy this pretty quickly
    and experiment with it, even without knowledge of iOS development. Here are the
    steps (note: this requires a Mac):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经深入了解了这个主题，还有什么比这更有趣的呢？怎么样在手机上实际运行它！我们在本书的GitHub网站上提供了一个完整的应用程序（请参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）位于*code/chapter-11*。通过iPhone或iPad，我们可以相当快速地部署它并进行实验，即使没有iOS开发知识也可以。以下是步骤（注意：这需要Mac）：
- en: Download Xcode from the Apple developer website or the Mac App Store.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从苹果开发者网站或Mac App Store下载Xcode。
- en: Plug in an iOS device. The phone needs to remain unlocked during deployment.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插入iOS设备。手机在部署过程中需要保持解锁状态。
- en: 'Change the current working directory to `CameraApp`:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将当前工作目录更改为`CameraApp`：
- en: '[PRE2]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Download the Core ML models from the Apple website using the available Bash
    script:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从苹果网站下载Core ML模型，使用提供的Bash脚本：
- en: '[PRE3]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Open the Xcode project:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Xcode项目：
- en: '[PRE4]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the Project Hierarchy Navigator, in the upper-left corner, click the CameraApp
    project, as shown in [Figure 11-6](part0013.html#project_information_view_within_xcode),
    to open the Project Information view.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在项目层次结构导航器中，点击左上角的CameraApp项目，如[图11-6](part0013.html#project_information_view_within_xcode)所示，打开项目信息视图。
- en: '![Project information view within Xcode](../images/00228.jpeg)'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Xcode中的项目信息视图](../images/00228.jpeg)'
- en: Figure 11-6\. Project information view within Xcode
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-6. Xcode中的项目信息视图
- en: Because Xcode reserves a unique bundle identifier, use a unique name to identify
    the project.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为Xcode保留了唯一的包标识符，所以使用一个唯一的名称来标识项目。
- en: Log in to an Apple account to let Xcode sign the app and deploy it to the device.
    Select a team to do the signing, as shown in [Figure 11-7](part0013.html#select_a_team_and_let_xcode_automaticall).
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到苹果账户，让Xcode签署应用程序并将其部署到设备上。选择一个团队进行签署，如[图11-7](part0013.html#select_a_team_and_let_xcode_automaticall)所示。
- en: '![Select a team and let Xcode automatically manage code signing](../images/00281.jpeg)'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![选择一个团队，让Xcode自动管理代码签名](../images/00281.jpeg)'
- en: Figure 11-7\. Select a team and let Xcode automatically manage code signing
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-7. 选择一个团队，让Xcode自动管理代码签名
- en: Click the “Build and Run” button (the right-facing triangle) to deploy the app
    on the device, as shown in [Figure 11-8](part0013.html#select_the_device_and_click_the_quotatio).
    This should typically take around 30 to 60 seconds.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“构建和运行”按钮（右向三角形）将应用程序部署到设备上，如[图11-8](part0013.html#select_the_device_and_click_the_quotatio)所示。这通常需要大约30到60秒。
- en: '![Select the device and click the “Build and Run” button to deploy the app](../images/00261.jpeg)'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![选择设备，点击“构建和运行”按钮来部署应用程序](../images/00261.jpeg)'
- en: Figure 11-8\. Select the device and click the “Build and Run” button to deploy
    the app
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-8. 选择设备，点击“构建和运行”按钮来部署应用程序
- en: The device will not run the app right away, because it is not trusted. Go to
    Settings > General > Profiles and Device Management and select the row with your
    information on it, and then tap “Trust {your_email_id},” as shown in [Figure 11-9](part0013.html#profiles_and_device_management_screen).
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设备不会立即运行应用程序，因为它没有受信任。前往设置 > 通用 > 配置文件与设备管理，选择包含您信息的行，然后点击“信任{your_email_id}”，如[图11-9](part0013.html#profiles_and_device_management_screen)所示。
- en: '![Profiles and Device Management screen](../images/00187.jpeg)'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![配置文件和设备管理屏幕](../images/00187.jpeg)'
- en: Figure 11-9\. Profiles and Device Management screen
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-9. 配置文件和设备管理屏幕
- en: On the home screen, find CameraApp and run the app.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在主屏幕上找到CameraApp并运行应用程序。
- en: '![Screenshot of the app](../images/00167.jpeg)'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![应用程序的屏幕截图](../images/00167.jpeg)'
- en: Figure 11-10\. Screenshot of the app
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-10. 应用程序的屏幕截图
- en: The output shows predictions of “notebook, notebook computer” with 84% confidence
    and “laptop, laptop computer” with 11% confidence.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出显示“笔记本电脑，笔记本电脑”预测的置信度为84%，“笔记本电脑，笔记本电脑”预测的置信度为11%。
- en: 'That was all fun and good. Let’s now get down to more serious business: converting
    models from different frameworks to Core ML models.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都很有趣。现在让我们转向更严肃的业务：将不同框架的模型转换为Core ML模型。
- en: Tip
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: A great thing about Xcode is that the model input and output parameters are
    shown when we load the *.mlmodel* file into Xcode, as demonstrated in [Figure 11-11](part0013.html#xcode_model_inspector_showing_the_inputs).
    This is especially helpful when we have not trained the model ourselves and don’t
    necessarily want to write code (like `model.summary()` in Keras) to explore the
    model architecture.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Xcode的一个很棒之处是当我们将*.mlmodel*文件加载到Xcode中时，会显示模型的输入和输出参数，如[图11-11](part0013.html#xcode_model_inspector_showing_the_inputs)所示。当我们没有自己训练模型并且不想编写代码（比如在Keras中的`model.summary()`）来探索模型架构时，这是特别有帮助的。
- en: '![Xcode model inspector showing the inputs and outputs of the MobileNetV2 model](../images/00145.jpeg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![Xcode模型检查器显示MobileNetV2模型的输入和输出](../images/00145.jpeg)'
- en: Figure 11-11\. Xcode model inspector showing the inputs and outputs of the MobileNetV2
    model
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-11. Xcode模型检查器显示MobileNetV2模型的输入和输出
- en: Conversion to Core ML
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换为Core ML
- en: In the code sample we built, you saw the *Inceptionv3.mlmodel* file. Ever wonder
    how that file came to be? Inception was, after all, trained by Google using TensorFlow.
    That file was converted from a *.pb* file into a Core ML model. We might similarly
    have models that need to be converted from Keras, Caffe or any other framework
    to Core ML. The following are a few tools that enable model conversion to Core
    ML.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建的代码示例中，您看到了*Inceptionv3.mlmodel*文件。您是否想知道这个文件是如何生成的？毕竟，Inception是由谷歌使用TensorFlow训练的。该文件是从*.pb*文件转换为Core
    ML模型的。我们可能也有需要将模型从Keras、Caffe或任何其他框架转换为Core ML。以下是一些工具，可以实现模型转换为Core ML。
- en: 'Core ML Tools (Apple): from Keras (*.h5*), Caffe (*.caffemodel*), as well as
    machine learning libraries such as LIBSVM, scikit-learn, and XGBoost.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Core ML工具（苹果）：从Keras（*.h5*）、Caffe（*.caffemodel*）以及LIBSVM、scikit-learn和XGBoost等机器学习库。
- en: 'tf-coreml (Google): from TensorFlow (*.pb*).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tf-coreml（谷歌）：从TensorFlow（*.pb*）。
- en: 'onnx-coreml (ONNX): from ONNX (*.onnx*).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: onnx-coreml（ONNX）：从ONNX（*.onnx*）。
- en: We take a look at the first two converters in detail.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们详细查看前两个转换器。
- en: Conversion from Keras
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Keras转换
- en: 'Core ML Tools aids the translation of Keras, ONNX, and other model formats
    to the Core ML format (*.mlmodel*). Install the `coremltools` framework using
    `pip`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Core ML工具有助于将Keras、ONNX和其他模型格式转换为Core ML格式（*.mlmodel*）。使用`pip`安装`coremltools`框架：
- en: '[PRE5]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, let’s see how we can use an existing Keras model and convert it to Core
    ML. The conversion is just a one-line command and then we can save the converted
    model, as shown here:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用现有的Keras模型并将其转换为Core ML。转换只需一行命令，然后我们可以保存转换后的模型，如下所示：
- en: '[PRE6]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: That’s it! It can’t get any simpler. We discuss how to convert models with unsupported
    layers (such as MobileNet) in [Chapter 12](part0014.html#DB7S3-13fa565533764549a6f0ab7f11eed62b).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！它再简单不过了。我们将讨论如何转换具有不受支持层（如MobileNet）的模型，详见[第12章](part0014.html#DB7S3-13fa565533764549a6f0ab7f11eed62b)。
- en: Conversion from TensorFlow
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从TensorFlow进行转换
- en: Apple recommends using `tf-coreml` (from Google) for converting from TensorFlow-based
    models. In the following steps, we convert a pretrained TensorFlow model to Core
    ML. This process is a bit more involved than the single line of code we saw earlier.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果建议使用`tf-coreml`（来自Google）将基于TensorFlow的模型转换为Core ML。在以下步骤中，我们将一个预训练的TensorFlow模型转换为Core
    ML。这个过程比我们之前看到的单行代码要复杂一些。
- en: 'First, we perform the installation of `tfcoreml` using `pip`:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`pip`执行`tfcoreml`的安装：
- en: '[PRE7]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To do the conversion, we need to know the first and last layers of the model.
    We can ascertain this by inspecting the model architecture using a model visualization
    tool like [Netron](https://oreil.ly/hJoly). After loading the MobileNet model
    (*.pb* file) in Netron, we can visualize the entire model in the graphical interface.
    [Figure 11-12](part0013.html#output_layer_of_mobilenet-v1_as_viewed_i) shows a
    small portion of the MobileNet model; in particular, the output layer.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行转换，我们需要知道模型的第一层和最后一层。我们可以通过使用模型可视化工具（如[Netron](https://oreil.ly/hJoly)）检查模型架构来确定这一点。在Netron中加载MobileNet模型（*.pb*文件）后，我们可以在图形界面中可视化整个模型。[图11-12](part0013.html#output_layer_of_mobilenet-v1_as_viewed_i)显示了MobileNet模型的一个小部分；特别是输出层。
- en: '![Output layer of MobileNet as seen in Netron](../images/00146.jpeg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![在Netron中查看的MobileNet的输出层](../images/00146.jpeg)'
- en: Figure 11-12\. Output layer of MobileNet as seen in Netron
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-12. 在Netron中查看的MobileNet的输出层
- en: 'We simply need to plug that into the following Python code as an argument and
    run it:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要将其作为参数插入以下Python代码中并运行：
- en: '[PRE8]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As the script runs, we see each of the operations from each layer being converted
    to their corresponding Core ML equivalents. When finished, we should find the
    *.mlmodel* exported in the directory.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当脚本运行时，我们会看到每个层的每个操作被转换为相应的Core ML等效操作。完成后，我们应该在目录中找到导出的*.mlmodel*。
- en: Our Core ML model is now ready. Drag it into Xcode to test it.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Core ML模型现在已经准备好了。将其拖入Xcode进行测试。
- en: Dynamic Model Deployment
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态模型部署
- en: As developers, we will likely want to keep improving our models over time. After
    all, we’d want our users to have access to the latest and greatest model as quickly
    as we can hand it to them. One way to handle that is to send an update to the
    App Store every time we want to deploy a new model. That approach does not work
    very well, because we’d need to wait about two days for Apple’s approval each
    time, potentially causing significant delays.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 作为开发人员，我们可能会希望随着时间的推移不断改进我们的模型。毕竟，我们希望我们的用户尽快获得最新和最好的模型。处理这个问题的一种方法是每次部署新模型时向App
    Store发送更新。这种方法效果不是很好，因为每次我们都需要等待大约两天才能获得苹果的批准，可能会导致显著的延迟。
- en: 'An alternate recommended approach would be to have the app dynamically download
    the *.mlmodel* file and compile it within the user’s device. There are several
    reasons why we might want to try this approach:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种推荐的方法是让应用程序动态下载*.mlmodel*文件，并在用户设备内编译它。有几个原因我们可能想尝试这种方法：
- en: We want to regularly update the model, independent of the cadence of our App
    Store releases.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望定期更新模型，而不受我们App Store发布节奏的限制。
- en: We want to keep the app download size small and have the user eventually download
    only the models relevant for their usage scenarios. This will reduce storage needs
    and bandwidth costs. Apple places a limit of 200 MB on App Store downloads via
    cellular networks, so it’s vital to remain under that limit to not lose out on
    potential downloads.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望保持应用程序下载大小较小，并让用户最终仅下载与其使用场景相关的模型。这将减少存储需求和带宽成本。苹果在通过蜂窝网络下载App Store的应用程序上设置了200
    MB的限制，因此保持在该限制以下至关重要，以免错失潜在的下载机会。
- en: We want to A/B test different models on different sets of users to further improve
    the quality of the model.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望在不同用户集合上对不同模型进行A/B测试，以进一步提高模型的质量。
- en: We want to use different models for different user segments, regions, and locales.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望为不同的用户段、区域和语言环境使用不同的模型。
- en: 'The process to achieve this is rather simple: host the *.mlmodel* on a server
    and design our app to dynamically download the file. After the model is on the
    user’s device, we can run `MLModel.compileModel` on that file to generate a compiled
    version of the model:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一目标的过程相当简单：将*.mlmodel*托管在服务器上，并设计我们的应用程序以动态下载文件。在模型位于用户设备上之后，我们可以在该文件上运行`MLModel.compileModel`来生成模型的编译版本：
- en: '[PRE9]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Keep in mind that `compiledModelUrl` is an address to a temporary location.
    If you want to keep the model on the device longer beyond a session, you must
    move it to permanent storage.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`compiledModelUrl`是一个临时位置的地址。如果您希望在会话之外将模型保存在设备上更长时间，您必须将其移动到永久存储中。
- en: Note
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Although we can manually do model management directly with Core ML, it would
    still involve writing a lot of boilerplate code, on the backend and on the device.
    We would need to manually manage the versions of each model, the file storage,
    and configuration infrastructure, as well as any errors in the process. This is
    where ML Kit and Fritz really shine by providing these features out of the box.
    We discuss this in more detail in [Chapter 13](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以直接使用Core ML手动进行模型管理，但这仍然涉及大量样板代码的编写，包括后端和设备端。我们需要手动管理每个模型的版本、文件存储和配置基础设施，以及过程中的任何错误。这就是ML
    Kit和Fritz真正出色的地方，它们提供了这些功能。我们将在[第13章](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b)中更详细地讨论这一点。
- en: On-Device Training
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在设备上训练
- en: So far, we have looked at scenarios that would be served well by a “one size
    fits all” neural network. However, some use cases would simply not work without
    personalizing the models. An example would be an app that organizes a user’s photo
    gallery based on recognizing the faces of people in each picture. Given that an
    average user’s phone is mostly filled with pictures of friends and family, a generic
    model trained on Danny DeVito’s and Tom Hanks’s faces would not be very useful
    for users (unless, of course, they belong to the DeVito or Hanks families).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了一些适合“一刀切”的神经网络的场景。然而，一些用例如果没有个性化模型将无法正常工作。一个例子是一个根据每张图片中人脸识别来组织用户照片库的应用。鉴于普通用户的手机大多充斥着朋友和家人的照片，一个训练有丹尼·德维托和汤姆·汉克斯脸部的通用模型对用户来说并不是很有用（除非当然他们属于德维托或汉克斯家族）。
- en: A real-life example would be on the iOS system keyboard, which learns a user’s
    language patterns over time and begins to give more and more relevant suggestions
    for that user. And this becomes even more evident when that person uses slang
    words, nicknames, domain-specific terms, and so on, which might not be part of
    the common language dictionary. Personalized suggestions based on such data would
    be useless for any other user.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一个现实生活中的例子是 iOS 系统键盘，它会随着时间学习用户的语言模式，并开始为该用户提供越来越相关的建议。当用户使用俚语、昵称、领域特定术语等时，这变得更加明显，这些可能不是常见语言词典的一部分。基于这些数据的个性化建议对于其他用户来说是无用的。
- en: In these cases, we want to collect the data for that user and train a personalized
    model for that individual only. One way to accomplish this would be to collect
    and send the data to the cloud, train a new model there, and send back the updated
    model to the user. This approach reeks of scalability, cost, and privacy issues.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，我们希望为该用户收集数据并仅为该个人训练个性化模型。实现这一目标的一种方法是收集并发送数据到云端，在那里训练一个新模型，然后将更新后的模型发送回用户。这种方法存在可扩展性、成本和隐私问题。
- en: Instead, Core ML provides functionality for on-device training, so that user’s
    data never needs to leave the device. A Core ML model is updatable when its `isUpdatable`
    property is set to `true`. Additionally, the specific set of layers that need
    to be retrained (usually toward the tail end of the network) also need to have
    the same property set to `true`. Additional training parameters such as learning
    rate and optimizers can also be set on the model.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，Core ML 提供了设备上训练的功能，因此用户的数据永远不需要离开设备。当`isUpdatable`属性设置为`true`时，Core ML 模型是可更新的。此外，需要重新训练的特定层集（通常是网络末端）也需要将相同属性设置为`true`。还可以在模型上设置额外的训练参数，如学习率和优化器。
- en: Even though training can consume GPU and Neural Processor Unit (NPU; more on
    these in [Chapter 13](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b)) cycles,
    the training can be scheduled in the background (using the `BackgroundTasks` framework),
    even when the device is idle and charging, typically at night. This will have
    the least impact on UX.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管训练可能会消耗 GPU 和神经处理器单元（NPU；更多信息请参阅[第13章](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b)），但训练可以在后台进行调度（使用`BackgroundTasks`框架），即使设备处于空闲和充电状态，通常在晚上。这对用户体验的影响最小。
- en: 'To do on-device training, we can call the `MLUpdateTask` function, along with
    the new data with which we will be retraining the model. We would also need to
    pass in the path for the newly updated model in the function call. After training
    is complete, the model at that path is ready to go:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行设备上的训练，我们可以调用`MLUpdateTask`函数，以及我们将用于重新训练模型的新数据。我们还需要在函数调用中传入新更新模型的路径。训练完成后，该路径上的模型就准备就绪了：
- en: '[PRE10]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Federated Learning
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 联邦学习
- en: 'On-device training is great—except for one downside: the generic global model
    does not get a chance to improve. Wouldn’t it be great for a developer to somehow
    use that data being generated on each user’s device to improve the global model
    without needing to transfer their data from their device? This is where *federated
    learning* comes in.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 设备上的训练很棒，除了一个缺点：通用全局模型没有机会改进。开发人员是否可以以某种方式利用每个用户设备上生成的数据来改进全局模型，而无需从其设备传输数据？这就是*联邦学习*的作用。
- en: Federated learning is a collaborative distributed training process. It essentially
    takes on-device training one step further by sending the incremental updates (on
    the personalized models from users’ devices) to the cloud and aggregating many
    of these over the entire user base, subsequently enriching the global model for
    everyone. Keep in mind that no user data is being transmitted here and it is not
    possible to reverse engineer the user data from the aggregated feature set. With
    this approach, we are able to respect our users’ privacy while also benefiting
    everyone through collective participation.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习是一种协作分布式训练过程。它通过将增量更新（来自用户设备的个性化模型）发送到云端，并在整个用户群体中聚合这些更新，从而为每个人丰富全局模型，将设备上的训练推进了一步。请记住，这里没有传输任何用户数据，也不可能从聚合的特征集中逆向工程用户数据。通过这种方法，我们能够尊重用户的隐私，同时通过集体参与使每个人受益。
- en: On-device training is a vital stepping stone for federated learning. Even though
    we are not there yet, this is the direction in which the industry is moving. We
    can expect to see more support for federated learning over time.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 设备上的训练是联邦学习的一个重要步骤。尽管我们还没有达到这一步，但这是行业发展的方向。我们可以期待随着时间的推移看到对联邦学习的更多支持。
- en: TensorFlow Federated is one such implementation of federated learning, powering
    training for Google’s GBoard, a keyboard app. Training on user devices occurs
    in the background when they are being charged.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Federated 是联邦学习的一个实现，为谷歌的 GBoard 键盘应用提供训练支持。用户设备上的训练发生在它们正在充电时的后台。
- en: Performance Analysis
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能分析
- en: Writing prototypes is one thing. Making production-ready apps is another ball
    game entirely. There are several factors that influence the user’s experience,
    and it is important to understand the trade-offs. Some of these include the supported
    device models, minimum operating system (OS) version, processing frame rate, and
    choice of the deep learning model. In this section, we explore the impact that
    some of these factors can have on the quality and performance of the product.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 编写原型是一回事。制作适用于生产的应用程序则完全是另一回事。有几个因素会影响用户体验，了解权衡是很重要的。其中一些因素包括支持的设备型号、最低操作系统（OS）版本、处理帧率以及深度学习模型的选择。在本节中，我们探讨了这些因素对产品质量和性能的影响。
- en: Benchmarking Models on iPhones
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在iPhone上对模型进行基准测试
- en: As is the case with benchmarking, it is best to perform experiments on publicly
    available models that we can download easily and get our hands dirty with!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 与基准测试一样，最好在公开可下载的模型上进行实验，以便我们可以轻松下载并进行实践！
- en: First, we ran our real-time object classification app on multiple iPhones produced
    from 2013 to 2018\. [Table 11-3](part0013.html#benchmarking_inference_time_on_different)
    catalogs the results of those experiments.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在2013年至2018年生产的多款iPhone上运行了我们的实时对象分类应用程序。[表11-3](part0013.html#benchmarking_inference_time_on_different)列出了这些实验的结果。
- en: Table 11-3\. Benchmarking inference time on different models on different iPhone
    versions
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-3。不同iPhone版本上不同型号的基准推理时间
- en: '| Device model | iPhone 5s | iPhone 6 | iPhone 6s | iPhone 7+ | iPhone X |
    iPhone XS | iPhone 11 Pro |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 设备型号 | iPhone 5s | iPhone 6 | iPhone 6s | iPhone 7+ | iPhone X | iPhone XS
    | iPhone 11 Pro |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **Year released** | 2013 | 2014 | 2015 | 2016 | 2017 | 2018 | 2019 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| **发布年份** | 2013 | 2014 | 2015 | 2016 | 2017 | 2018 | 2019 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **RAM** | 1 GB | 1 GB | 2 GB | 2 GB | 2 GB | 4 GB | 4GB |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| **RAM** | 1 GB | 1 GB | 2 GB | 2 GB | 2 GB | 4 GB | 4GB |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **Processor chip** | A7 | A8 | A9 | A10 | A11 | A12 | A13 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| **处理器芯片** | A7 | A8 | A9 | A10 | A11 | A12 | A13 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **Model** | **Accuracy (%)** | **Size (MB)** | **FPS** | **FPS** | **FPS**
    | **FPS** | **FPS** | **FPS** | **FPS** |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **准确率（%）** | **大小（MB）** | **FPS** | **FPS** | **FPS** | **FPS**
    | **FPS** | **FPS** | **FPS** |'
- en: '| **VGG-16** | 71 | 553 | 0.1 | 0.2 | 4.2 | 5.5 | 6.9 | 27.8 | 34.5 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| **VGG-16** | 71 | 553 | 0.1 | 0.2 | 4.2 | 5.5 | 6.9 | 27.8 | 34.5 |'
- en: '| **InceptionV3** | 78 | 95 | 1.4 | 1.5 | 9 | 11.1 | 12.8 | 35.7 | 41.7 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| **InceptionV3** | 78 | 95 | 1.4 | 1.5 | 9 | 11.1 | 12.8 | 35.7 | 41.7 |'
- en: '| **ResNet-50** | 75 | 103 | 1.9 | 1.7 | 11.6 | 13.5 | 14.1 | 38.5 | 50 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| **ResNet-50** | 75 | 103 | 1.9 | 1.7 | 11.6 | 13.5 | 14.1 | 38.5 | 50 |'
- en: '| **MobileNet** | 71 | 17 | 7.8 | 9 | 19.6 | 28.6 | 28.6 | 55.6 | 71.4 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| **MobileNet** | 71 | 17 | 7.8 | 9 | 19.6 | 28.6 | 28.6 | 55.6 | 71.4 |'
- en: '| **SqueezeNet** | 57 | 5 | 13.3 | 12.4 | 29.4 | 33.3 | 34.5 | 66.7 | 76.9
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| **SqueezeNet** | 57 | 5 | 13.3 | 12.4 | 29.4 | 33.3 | 34.5 | 66.7 | 76.9
    |'
- en: The difference between inference times in 2014 and 2015 is particularly striking.
    What happened in 2015? If you guessed GPUs, you’d be right. The iPhone 6S introduced
    a dedicated GPU for the first time, powering features such as “Hey Siri.”
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年和2015年推理时间之间的差异尤为显著。2015年发生了什么？如果你猜到了GPU，你是对的。iPhone 6S首次引入了专用GPU，支持诸如“嘿Siri”之类的功能。
- en: On the other hand, let’s look at the device market share among iPhones in the
    release month of the iPhone XS (Sept. 2018) in the United States, as shown in
    [Table 11-4](part0013.html#us_device_market_share_among_iphones_as). Note that
    releases typically happen in September of each year.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，让我们看看2018年9月iPhone XS发布月份在美国iPhone中的市场份额，如[表11-4](part0013.html#us_device_market_share_among_iphones_as)所示。请注意，发布通常在每年的9月份进行。
- en: Table 11-4\. US device market share among iPhones as of September 2018 (the
    release month of the iPhone XS; data from [Flurry Analytics](https://oreil.ly/L47c0)
    adjusted to exclude iPhone XS, XS Plus, and XS Max)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-4。截至2018年9月的美国iPhone设备市场份额（iPhone XS发布月份；数据来自[Flurry Analytics](https://oreil.ly/L47c0)，排除iPhone
    XS、XS Plus和XS Max）
- en: '| Year released | iPhone model | Percentage |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 发布年份 | iPhone型号 | 百分比 |'
- en: '| --- | --- | --- |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 2017 | 8 Plus | 10.8% |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | 8 Plus | 10.8% |'
- en: '| 2017 | X | 10.3% |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | X | 10.3% |'
- en: '| 2017 | 8 | 8.1% |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | 8 | 8.1% |'
- en: '| 2016 | 7 | 15.6% |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | 7 | 15.6% |'
- en: '| 2016 | 7 Plus | 12.9% |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | 7 Plus | 12.9% |'
- en: '| 2016 | SE | 4.2% |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | SE | 4.2% |'
- en: '| 2015 | 6S | 12.5% |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | 6S | 12.5% |'
- en: '| 2015 | 6S Plus | 6.1% |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | 6S Plus | 6.1% |'
- en: '| 2014 | 6 | 10.7% |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | 6 | 10.7% |'
- en: '| 2014 | 6 Plus | 3.3% |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | 6 Plus | 3.3% |'
- en: '| 2013 | 5S | 3.4% |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | 5S | 3.4% |'
- en: '| 2013 | 5C | 0.8% |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | 5C | 0.8% |'
- en: '| 2012 | 5 | 0.8% |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | 5 | 0.8% |'
- en: '| 2011 | 4S | 0.4% |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 2011 | 4S | 0.4% |'
- en: '| 2010 | 4 | 0.2% |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 2010 | 4 | 0.2% |'
- en: Deriving the cumulative percentages from [Table 11-4](part0013.html#us_device_market_share_among_iphones_as),
    we get [Table 11-5](part0013.html#cumulative_market_share_of_iphonescomma). This
    table expresses our potential market share based on the oldest device we choose
    to support. For example, iPhones released after September 2016 (two years before
    September 2018) have a total market share of 61.9%.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '从[表11-4](part0013.html#us_device_market_share_among_iphones_as)中推导出累积百分比，我们得到了[表11-5](part0013.html#cumulative_market_share_of_iphonescomma)。该表根据我们选择支持的最旧设备来表达我们的潜在市场份额。例如，2016年9月之后发布的iPhone（2018年9月之前的两年）总市场份额为61.9%。 '
- en: Table 11-5\. Cumulative market share of iPhones, year-on-year, going back in
    time
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-5。iPhone年度累积市场份额
- en: '| Years under | Cumulative percentage |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 累积百分比 |'
- en: '| --- | --- |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 | 29.2% |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 29.2% |'
- en: '| 2 | 61.9% |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 61.9% |'
- en: '| 3 | 80.5% |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 80.5% |'
- en: '| 4 | 94.5% |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 94.5% |'
- en: '| 5 | 98.7% |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 98.7% |'
- en: '| 6 | 99.5% |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 99.5% |'
- en: 'Combining the benchmark and market share, there are some design choices and
    optimizations available to us:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 结合基准和市场份额，我们有一些设计选择和优化可供选择：
- en: Using a faster model
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更快的模型
- en: On an iPhone 6, VGG-16 runs about 40 times slower than MobileNet. On an iPhone
    XS, it’s still about two times slower. Just choosing a more efficient model can
    have a dramatic impact on performance, often without the need to compromise an
    equivalent amount of accuracy. It’s worth noting that MobileNetV2 and EfficientNet
    offer an even better combination of speed and accuracy.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在iPhone 6上，VGG-16的运行速度比MobileNet慢大约40倍。在iPhone XS上，它仍然比MobileNet慢大约两倍。仅仅选择一个更高效的模型就可以对性能产生显著影响，通常无需牺牲等量的准确性。值得注意的是，MobileNetV2和EfficientNet提供了更好的速度和准确性的组合。
- en: Determining the minimum FPS to support
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 确定支持的最低FPS
- en: A feature to apply filters on a real-time camera feed will need to be able to
    process a high number of the frames delivered by the camera every second to ensure
    a smooth experience. In contrast, an app that processes a single image due to
    a user-initiated action does not need to worry about performance nearly as much.
    A lot of applications are going to be somewhere in between. It’s important to
    determine the minimum necessary FPS and perform benchmarking similar to [Table 11-5](part0013.html#cumulative_market_share_of_iphonescomma)
    to arrive at the best model(s) for the scenario across iPhone generations.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 一个在实时摄像头视频上应用滤镜的功能需要能够处理摄像头每秒传送的大量帧，以确保流畅的体验。相比之下，由于用户发起的操作，一个处理单个图像的应用并不需要太过担心性能。很多应用程序会处于两者之间的某个位置。重要的是确定最低必要的FPS，并进行类似于[表11-5](part0013.html#cumulative_market_share_of_iphonescomma)的基准测试，以确定跨iPhone世代的最佳模型。
- en: Batch processing
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理
- en: GPUs are really good at parallel processing. So, it comes as no surprise that
    processing a batch of data is more efficient than performing inferences on each
    item individually. Core ML takes advantage of this fact by exposing batch processing
    APIs. Some user experiences, particularly those that are asynchronous and/or memory
    intensive, can vastly benefit from these batching APIs. For example, any bulk
    processing of photos in the photo gallery. Instead of processing one image at
    a time, we can send a list of images together to the batch API, so that Core ML
    can optimize the performance on the GPU.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: GPU非常擅长并行处理。因此，将一批数据进行处理比对每个项目进行推断更有效并不奇怪。Core ML利用这一事实通过暴露批处理API来实现。一些用户体验，特别是异步和/或内存密集型的体验，可以从这些批处理API中获益。例如，在照片库中进行任何批量处理照片。我们可以将一组图像一起发送到批处理API，以便Core
    ML可以优化GPU上的性能，而不是逐个处理图像。
- en: Dynamic model selection
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 动态模型选择
- en: Depending on the use case, we might not care nearly as much about high levels
    of accuracy as we do about a smooth experience (at some minimum FPS threshold).
    In these scenarios, we might choose a lighter, less-accurate model on a slower,
    older device, and a larger, more-accurate model on faster, modern devices. This
    can be used in conjunction with model deployment via the cloud so that we don’t
    needlessly increase the app size.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 根据使用情况，我们可能对高准确度并不像对流畅体验（在某个最低FPS阈值下）那样在意。在这些情况下，我们可能会在较慢、较旧的设备上选择一个更轻、准确性较低的模型，而在更快、现代化的设备上选择一个更大、更准确的模型。这可以与通过云端进行模型部署结合使用，这样我们就不会不必要地增加应用的大小。
- en: Using *Sherlocking* to our advantage
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 利用“Sherlocking”来获益
- en: “Sherlocking” is a term used for when first-party vendors (Apple in particular)
    make third-party software obsolete by baking in a feature themselves. A good example
    of that is when Apple released the Flashlight feature within the iPhone rendering
    all third-party flashlight apps (whether paid or free) obsolete. For illustration
    here, take the example of a hypothetical app that released a face tracking feature
    in 2017\. A year later, Apple added a more accurate face tracking API within Core
    ML for iOS 12\. Because the neural networks for the face tracking come baked into
    the OS, the app could update its code to use the built-in API. However, because
    the API would still not be available for any users in iOS 11, the app could use
    a hybrid approach in which it would remain backward compatible by using the old
    code path for iOS 11\. The app developer could then stop bundling the neural network
    into the app, thereby reducing its size, potentially by several megabytes, and
    dynamically download the older model for users of iOS 11.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: “Sherlocking”是一个术语，用于描述第一方供应商（尤其是苹果）通过内置功能使第三方软件变得过时。一个很好的例子是，当苹果发布了iPhone内置的手电筒功能时，所有第三方手电筒应用程序（无论是付费还是免费）都变得过时了。举个例子，假设一个应用在2017年发布了一个面部跟踪功能。一年后，苹果在iOS
    12的Core ML中添加了一个更准确的面部跟踪API。由于面部跟踪的神经网络已经内置在操作系统中，该应用可以更新其代码以使用内置的API。然而，由于API在iOS
    11中仍然不可用，该应用可以采用混合方法，通过使用旧代码路径保持向后兼容性。应用开发人员可以停止将神经网络捆绑到应用中，从而减小其大小，可能减少几兆字节，并动态下载旧模型以供iOS
    11用户使用。
- en: Graceful degradation
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 优雅降级
- en: Sometimes an older phone just cannot handle newer deep learning models due to
    performance needs. That’s a perfectly reasonable situation to be in, particularly
    if the feature is cutting edge. In those scenarios, it’s acceptable to use one
    of two approaches. The first is to offload the computation to the cloud. This
    obviously comes at the cost of interactivity and privacy. The other alternative
    is to disable the feature for those users and display a message to them stating
    why it is not available.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，由于性能需求，较旧的手机可能无法处理较新的深度学习模型。这是一个完全合理的情况，特别是如果该功能是尖端的。在这些情况下，可以采用两种方法之一。第一种是将计算外包到云端。这显然会牺牲互动性和隐私。另一种选择是为这些用户禁用该功能，并向他们显示为什么该功能不可用的消息。
- en: Measuring Energy Impact
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量能源影响
- en: In the previous chapters, we focused on hosting classifiers on servers. Although
    there are several factors that affect design decisions, energy consumption is
    often not among them. However, on the client side, battery power tends to be limited,
    and minimizing its consumption becomes a priority. How users perceive a product
    depends a lot on how much energy it consumes. Remember the good old days when
    GPS was a battery hog. Many apps that required location access used to get tons
    of one-star ratings for eating up too much battery power. And we definitely don’t
    want that.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-270
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: People are pretty generous with bad reviews when they realize an app is eating
    their battery faster than anticipated, as the reviews shown in [Figure 11-13](part0013.html#app_store_reviews_for_youtube_and_snapch)
    demonstrate.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '![App Store reviews for YouTube and Snapchat that complain about heavy battery
    consumption](../images/00102.jpeg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
- en: Figure 11-13\. App Store reviews for YouTube and Snapchat that complain about
    heavy battery consumption
  id: totrans-273
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here we utilize the Energy Impact tab ([Figure 11-14](part0013.html#xcode_debug_navigator_tab))
    in Xcode’s Debug Navigator and generate [Figure 11-15](part0013.html#xcode_energy_impact_chart_on_an_ipad_pro).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![Xcode Debug Navigator tab](../images/00063.jpeg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
- en: Figure 11-14\. Xcode Debug Navigator tab
  id: totrans-276
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Xcode Energy Impact chart on an iPad Pro 2017 (note: this screenshot was
    taken at a different time than Figure 11-14, which is why the numbers are slightly
    different)](../images/00188.jpeg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-15\. Xcode Energy Impact chart on an iPad Pro 2017 (note: this screenshot
    was taken at a different time than [Figure 11-14](part0013.html#xcode_debug_navigator_tab),
    which is why the numbers are slightly different)'
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 11-15](part0013.html#xcode_energy_impact_chart_on_an_ipad_pro) demonstrates
    that the energy impact of running the process on the iPad Pro 2017 is high. A
    big reason for this is that in our sample code, we were processing every single
    frame that we were receiving from the camera. This meant that each frame that
    the camera captured was being sent to the GPU for processing, resulting in higher
    energy consumption. In many real-world applications, it is not necessary to classify
    every single frame. Even processing every other frame can result in significant
    energy savings without generally affecting the UX. In the next section, we explore
    the relationship between the frame processing rate and the energy impact.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The ratio of CPU to GPU usage is affected by a model’s architecture, specifically
    the number of convolutional operations. Here we profile the average CPU and GPU
    utilization for different models ([Figure 11-16](part0013.html#comparing_cpu_and_gpu_utilization_for_di)).
    These numbers can be extracted from the Energy Impact chart in Xcode. Note that
    we’d ideally prefer models with more GPU utilization for performance efficiency.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing CPU and GPU utilization for different models on iOS 11](../images/00310.jpeg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: Figure 11-16\. Comparing CPU and GPU utilization for different models on iOS
    11
  id: totrans-283
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Benchmarking Load
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you would expect, running CNN models in real time on each frame causes high
    GPU/CPU utilization, which in turn rapidly drains the battery. Users might also
    notice the phone heating up.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Instead of running an analysis on every frame, how about we skip a few frames?
    MobileNet takes 20 ms to analyze one frame on an iPad Pro 2017\. So, it classifies
    about 50 FPS. Instead, if we run it at 1 FPS, the GPU utilization is reduced from
    42% to a mere 7%—a reduction of more than 83%! For many applications, processing
    at 1 FPS might be sufficient while still keeping the device cool. For example,
    a security camera might perform reasonably well even with processing frames once
    every couple of seconds.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: By varying the number of frames analyzed per second and measuring the percentage
    of GPU utilization, we observe a fascinating trend. It is evident from the graph
    in [Figure 11-17](part0013.html#varying_the_fps_and_analyzing_the_load_o) that
    the higher the FPS, the more the GPU is utilized. That has a significant impact
    on energy consumption. For apps that need to be run for longer periods of time,
    it might be beneficial to reduce the number of inferences per second.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '![Varying the FPS and analyzing the load on an iPad Pro 2017](../images/00269.jpeg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: Figure 11-17\. Varying the FPS and analyzing the load on an iPad Pro 2017
  id: totrans-289
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The values shown in the graph were derived from the Core Animation Instrument
    of Xcode Instruments. Following is the process we used to generate our results:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: From Xcode, click Product and then select Profile.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the Instruments window appears, select the Core Animation Instrument,
    as shown in [Figure 11-18](part0013.html#the_instruments_window_in_xcode_instrume).
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![The Instruments window in Xcode Instruments](../images/00227.jpeg)'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11-18\. The Instruments window in Xcode Instruments
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Press the Record button to start running the app in the Profiling mode.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait a few seconds to begin collecting instrumentation data.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure the values in the GPU Hardware Utilization column ([Figure 11-19](part0013.html#an_app_being_profiled_live_in_the_core_a)).
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![An app being profiled live in the Core Animation instrument](../images/00192.jpeg)'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11-19\. An app being profiled live in the Core Animation instrument
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: So far in this chapter, we’ve explored techniques for making production-worthy
    apps using Core ML. In the next section, we discuss examples of such apps in the
    real world.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Reducing App Size
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'App size can be crucially important to developers in certain markets. Apple
    does not allow apps larger than 200 MB to be downloaded via the cellular network.
    This becomes a crucial point of contention for apps that are used frequently on
    the go, such as ride-hailing apps like Uber and Lyft. Interesting tidbit here:
    Uber had to do some really stringent optimizations such as heavily reducing the
    number of Swift Optionals, Structs, and Protocols (which otherwise help with code
    maintainability) in order to bring the app binary size below the App Store limit
    (at the time, 150 MB). Without doing that, the company would have lost a lot of
    new users.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: It’s a given that any new AI functionality we introduce in the app will result
    in additional usage of storage. There are a few strategies that we can employ
    to reduce the impact of that.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-304
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Segment, a San Francisco-based data analytics company, wanted to determine how
    much of an impact app size has on the number of installs. To experiment, the company
    purchased a mortgage calculator app that had a steady number of app downloads
    (approximately 50 per day). It took the app that was originally 3 MB in size and
    kept bloating it repeatedly with…well, images of Taylor Swift albums (hey, it
    was in the name of science and research!). Engineers observed significant drops
    in the number of daily installs as the company increased the app size. When the
    app crossed 100 MB (the maximum limit for cellular network downloads from the
    App Store at the time), the number of daily installs dropped a whopping 44%! Additionally,
    the app attracted several negative reviews with users expressing incredulity at
    the app size.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: The moral of the story here is that the app size is far more crucial than we
    think, and we ought to be mindful of the amount of space our app consumes before
    releasing it out to the public.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Avoid Bundling the Model
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If possible, we should avoid bundling the model with the App Store binary. The
    same amount of data needs to be downloaded anyway, so as long as the UX is not
    affected, we should delay model download to when the feature is actually going
    to be used. Additionally, we should prefer to download the model when on WiFi
    in order to preserve cellular bandwidth. Microsoft Translator and Google Translate
    implement a form of this where they do only cloud-based translations at the outset.
    Knowing that travelers use these apps a lot (where they might not have good internet
    access), they also offer an offline mode where the required language models are
    downloaded at the user’s prompt in the background.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Use Quantization
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed in [Chapter 6](part0008.html#7K4G3-13fa565533764549a6f0ab7f11eed62b),
    quantization is a good strategy to reduce model size drastically while still preserving
    performance. Essentially, it reduces 32-bit floating-point weights down to 16-bit
    floating point, and 8-bit integers all the way down to 1-bit. We definitely won’t
    recommend going below 8 bits, though, due to the resulting loss in accuracy. We
    can achieve quantization with Core ML Tools for Keras models in just a couple
    of lines:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To illustrate the impact of quantization on a challenging dataset where small
    modifications could lead to drastic changes, we chose to build a classifier for
    Oxford’s 102 Category Flower Dataset with Keras (roughly 14 MB in size), and quantized
    it to different bit representations while measuring its accuracy and decreasing
    its size. To measure the change in predictions, we compare the percent match between
    the full-precision model and the quantized model. We tested three quantization
    modes.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Simple `linear` quantization, which we described in [Chapter 6](part0008.html#7K4G3-13fa565533764549a6f0ab7f11eed62b).
    The intervals are distributed equally in this strategy.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear quantization using a lookup table, or `linear_lut`. In this technique,
    the intervals are distributed unequally, with denser areas getting smaller and
    more intervals, whereas sparser areas would have fewer and larger intervals. Because
    these intervals are unequal, they need to be stored in the lookup table, rather
    than being directly computed with simple arithmetic.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lookup table generated by *k*-means, or `kmeans_lut`, which is often used in
    nearest neighbor classifiers.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Table 11-6](part0013.html#quantization_results_for_different_targe) shows
    our observations.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-6\. Quantization results for different target bit sizes and different
    quantization modes
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '| **Quantized to** | **Percent size reduction (approx.)** | **Percent match
    with 32-bit results** |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '| **`linear`** | **`linear_lut`** | **`kmeans_lut`** |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: '| 16-bit | 50% | 100% | 100% | 100% |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '| 8-bit | 75% | 88.37% | 80.62% | 98.45% |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
- en: '| 4-bit | 88% | 0% | 0% | 81.4% |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: '| 2-bit | 94% | 0% | 0% | 10.08% |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: '| 1-bit | 97% | 0% | 0% | 7.75% |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: This gives us a few insights.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Lowering representation to 16-bits has no effect at all on accuracy. We can
    essentially halve the model size with no perceivable difference in accuracy.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k*-means when used for building the lookup table outperforms simple linear
    division methods. Even at 4-bit, only 20% accuracy is lost, which is remarkable.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going to 8-bit quantized models gives 4 times smaller modes with little loss
    in accuracy (especially for `kmeans_lut` mode).
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantizing to lower than 8-bits results in a drastic drop in accuracy, particularly
    with the linear modes.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Create ML
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Create ML is a tool from Apple to train models by simply dragging and dropping
    data into a GUI application on the Mac. It provides several templates including
    object classification/detection, sound classification, and text classification,
    among others, putting training AI in the hands of novices without requiring any
    domain expertise. It uses transfer learning to tune only the handful of layers
    that will be necessary for our task. Because of this, the training process can
    be completed in just a few minutes. The OS ships with the bulk of the layers (that
    are usable across several tasks), whereas the task-specific layers can be shipped
    as part of the app. The resulting exported model ends up being really small (as
    little as 17 KB, as we will see shortly). We get hands-on with Create ML in the
    next chapter.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: Create ML是苹果的一个工具，通过简单地将数据拖放到Mac上的GUI应用程序中来训练模型。它提供了几个模板，包括对象分类/检测、声音分类和文本分类等，使得即使没有领域专业知识的新手也可以进行训练AI。它使用迁移学习来调整只有少数几层对我们的任务是必要的。因此，训练过程可以在几分钟内完成。操作系统内置了大部分层（可用于多个任务），而特定任务的层可以作为应用程序的一部分进行发布。最终导出的模型非常小（仅有17
    KB，我们很快就会看到）。我们将在下一章中亲自体验Create ML。
- en: Case Studies
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究
- en: Let’s take a look at some real-world examples that use Core ML for mobile inferences.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些使用Core ML进行移动推断的真实案例。
- en: Magic Sudoku
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 魔术数独
- en: 'Soon after the launch of [ARKit on iOS](https://developer.apple.com/arkit)
    in 2017, the [Magic Sudoku](https://magicsudoku.com) app from Hatchlings, a game
    and mobile app startup, came out as one of the break-out hits. Simply point the
    phone at a Sudoku puzzle, and the app will show a solved Sudoku right on the piece
    of paper. As we might have guessed at this point, the system is using Core ML
    to run a CNN-based digit recognizer. The system goes through the following steps:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年[ARKit在iOS上推出](https://developer.apple.com/arkit)后不久，来自Hatchlings的游戏和移动应用初创公司的[魔术数独](https://magicsudoku.com)应用成为了突破性的热门应用之一。只需将手机对准数独谜题，应用程序就会在纸上显示一个已解决的数独。正如我们此时可能已经猜到的那样，该系统使用Core
    ML来运行基于CNN的数字识别器。系统经过以下步骤：
- en: Use ARKit to get camera frames.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用ARKit获取相机帧。
- en: Use iOS Vision Framework to find rectangles.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用iOS Vision Framework查找矩形。
- en: Determine whether it is a Sudoku grid.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定它是否为数独网格。
- en: Extract 81 squares from the Sudoku image.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数独图像中提取81个方块。
- en: Recognize digits in each square using Core ML.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Core ML识别每个方块中的数字。
- en: Use a Sudoku solver to finish the empty squares.
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数独求解器完成空方块。
- en: Project the finished Sudoku on the surface of original paper using ARKit, as
    shown in [Figure 11-20](part0013.html#step-by-step_solution_to_solving_arkit_l).
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用ARKit在原始纸张表面上投影完成的数独，如[图11-20](part0013.html#step-by-step_solution_to_solving_arkit_l)所示。
- en: '![Step-by-step solution to solving ARKit (image credit)](../images/00153.jpeg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![解决ARKit的逐步解决方案（图片来源）](../images/00153.jpeg)'
- en: Figure 11-20\. Step-by-step solution to solving ARKit ([image source](https://oreil.ly/gzmb9))
  id: totrans-346
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-20\. 解决ARKit的逐步解决方案（[图片来源](https://oreil.ly/gzmb9)）
- en: The Hatchlings team first started with using the MNIST digit recognition model,
    which mostly consists of handwritten digits. But this was not robust for printed
    fonts. The team then photographed thousands of pages of Sudoku books, extracting
    the squares using its pipeline until it had a large number of images of individual
    digits in a variety of fonts. To label this data, the team requested its fans
    classify each item as 0 through 9 and empty classes. Within 24 hours, the team
    got 600,000 digits scanned. The next step was to train a custom CNN, which needed
    to work fast, because the system needed to classify 81 square images. Deploying
    this model using Core ML, the app launched and become an overnight hit.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: Hatchlings团队首先使用了MNIST数字识别模型，该模型主要由手写数字组成。但对于印刷字体来说并不稳健。然后，团队拍摄了数千页数独书，使用其流水线提取方块，直到获得了大量不同字体中的单个数字图像。为了标记这些数据，团队请求其粉丝将每个项目分类为0到9和空类。在24小时内，团队获得了60万个数字扫描。下一步是训练一个定制的CNN，因为系统需要快速工作，因为需要对81个方块图像进行分类。使用Core
    ML部署这个模型后，应用程序推出并一夜之间成为了热门。
- en: Running out with new users in the wild brought new cases not previously anticipated.
    Because most users do not have a Sudoku puzzle in front of them, they often search
    for Sudoku on computer screens, and this was difficult for the model to always
    recognize precisely. Additionally, due to a fixed focal length limitation of ARKit,
    the input image could be slightly blurry. To remedy this, the Hatchlings team
    collected additional examples of photos of computer screens with puzzles, blurred
    them slightly, and trained a new CNN with the additional data. With an App Store
    update, the whole experience became much more robust. In summary, while launching
    an app or a service, the app builder needs to constantly learn from new scenarios
    not previously anticipated.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 与野外的新用户一起运行带来了以前未预料到的新情况。因为大多数用户面前没有数独谜题，他们经常在计算机屏幕上搜索数独，这对模型总是准确识别是困难的。此外，由于ARKit的固定焦距限制，输入图像可能会稍微模糊。为了解决这个问题，Hatchlings团队收集了额外的计算机屏幕上带有谜题的照片示例，稍微模糊了它们，并使用额外的数据训练了一个新的CNN。通过App
    Store更新，整个体验变得更加稳健。总之，在推出应用程序或服务时，应用程序构建者需要不断从以前未预料到的新场景中学习。
- en: Seeing AI
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Seeing AI
- en: '[Seeing AI](https://oreil.ly/hJxUE) is a talking camera app from Microsoft
    Research designed for the blind and low-vision community. It uses computer vision
    to describe people, text, handwriting, objects, currency, and more through spoken
    audio. Much of the image processing happens locally on the device, using Core
    ML. One of the core user needs is to recognize products—this can usually be determined
    by scanning close-ups of barcodes. But for a blind user, the location of a barcode
    is unknown, making most barcode apps difficult to use. To solve this, the team
    built a custom CNN trained with images containing barcodes at a variety of angles,
    sizes, lighting, and orientation. The user now attempts to rotate the object in
    front of the iPhone, and when the CNN classifies the presence of a barcode (running
    in real time, frame by frame), it signals with an audible beep. The rate of beeping
    directly correlates with the area of the barcode visible to the camera. As the
    blind users begin to bring the barcode closer, it beeps faster. When it is close
    enough for the barcode reading library to be able to clearly see the barcode,
    the app decodes the Universal Product Code and speaks the product name. In the
    past, blind users typically had to purchase and carry around bulky laser barcode
    scanners, which usually cost more than $1,300\. In fact, a charity raised millions
    to donate these hardware barcode readers to those who needed them. Now, deep learning
    can solve this problem for free. This is a good example of mixing computer vision
    and UX to solve a real-world problem.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: HomeCourt
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For anything in life, regular practice is a must if we want to get better at
    something, be it writing, playing a musical instrument, or cooking. However, the
    quality of practice is far more important than quantity. Using data to support
    our practice and monitor our progress does wonders to the rate at which we acquire
    a skill. This is exactly what NEX Team, a San Jose, California-based startup,
    set out to do for basketball practice with the help of its HomeCourt app. Running
    the app is easy: set it up on the ground or a tripod, point the camera at the
    basketball court, and hit record.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'The app runs an object detector in real time on top of Core ML to track the
    ball, the people, and the basketball hoop. Among these people, the big question
    remains: who shot the ball? When the ball goes near the hoop, the app rewinds
    the video to identify the player who shot the ball. Then, it performs human pose
    estimation on the player to track the player’s movements. If this wasn’t impressive
    enough, it uses geometric transformations to convert this 3D scene of the court
    into a 2D map (as shown in the upper right of [Figure 11-21](part0013.html#the_homecourt_app_tracking_a_playerapost))
    to track the locations from which the player shot the ball. The important thing
    to note here is that multiple models are being run simultaneously. Based on the
    tracked objects and the player’s body position, joints, and so on, the app provides
    the player with stats and visualizations for the release height of each throw,
    release angle, release position, release time, player speed, and more. These stats
    are important because they have a high correlation with a successful attempt.
    Players are able to record an entire game as they play and then go home and analyze
    each shot so that they can determine their weak areas in order to improve for
    the next time.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: In just under two years of existence, this tiny startup attracted hundreds of
    thousands of users as word of mouth spread, from amateurs to professionals. Even
    the National Basketball Association (NBA) partnered with NEX Team to help improve
    their players’ performance using HomeCourt. All of this happened because they
    saw an unmet need and came up with a creative solution using deep learning.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '![The HomeCourt app tracking a player’s shots live as they are playing](../images/00104.jpeg)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
- en: Figure 11-21\. The HomeCourt app tracking a player’s shots live as they are
    playing
  id: totrans-356
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-21。HomeCourt应用程序在玩家进行比赛时实时跟踪其投篮
- en: InstaSaber + YoPuppet
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InstaSaber + YoPuppet
- en: 'Do you know the largest profit maker in the *Star Wars* franchise? It’s not
    the movie ticket sales, and it’s definitely not from selling DVDs. Here’s a hint:
    it rhymes with merchandise. Lucas Film (and now Disney) made a lot of money selling
    R2D2 toys and Chewbacca costumes. Though the all-time favorite still remains the
    dear lightsaber. However, instead of looking cool like in the movies, the merchandise
    lightsabers are mostly made of plastic and frankly don’t look as sci-fi. Plus,
    you swing it around a few times and there’s a good chance you’ll accidentally
    whack someone in the face.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道《星球大战》系列中最大的利润制造者是谁吗？不是电影票房收入，绝对不是从销售DVD中获得的。这里有一个提示：它押韵的是商品。卢卡斯影业（现在是迪士尼）通过销售R2D2玩具和Chewbacca服装赚了很多钱。尽管永远的最爱仍然是心爱的光剑。然而，与电影中看起来很酷不同，商品光剑大多由塑料制成，实际上看起来不太科幻。此外，你挥舞几次，很可能会不小心打到别人的脸。
- en: Hart Woolery, the founder of 2020CV, decided to change that by bringing Hollywood-level
    VFX to our phone with the InstaSaber app. Simply roll up a sheet of paper, grip
    it, and point your phone’s camera toward it and watch it transform into a glowing
    lightsaber, as illustrated in [Figure 11-22](part0013.html#screenshots_of_instasaber_and_yopuppet).
    Wave your hand and not just watch it track it realistically in real time, but
    also hear the same sound effects as when Luke fought his father, (spoiler alert!)
    Darth Vader.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 2020CV的创始人哈特·伍勒利决定改变这一点，通过InstaSaber应用程序将好莱坞级别的视觉效果带到我们的手机上。只需卷起一张纸，握住它，将手机的摄像头对准它，看着它变成一个发光的光剑，如[图11-22](part0013.html#screenshots_of_instasaber_and_yopuppet)所示。挥动你的手，不仅可以实时跟踪它，还可以听到与卢克与他的父亲（剧透！）达斯维达战斗时相同的声音效果。
- en: Taking the tracking magic to the next level, he built YoPuppet, which tracks
    the joints in hands in real time to build virtual puppets that mimic the motion
    of the hand. The suspension of disbelief works only if it is truly real time with
    no lag, is accurate, and realistic looking.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 将追踪魔法提升到下一个级别，他开发了YoPuppet，该应用程序实时跟踪手部关节，构建出模仿手部运动的虚拟木偶。只有当它真正实时无延迟，准确且看起来逼真时，才能产生一种悬置现实的效果。
- en: In addition to being realistic, these apps were a lot of fun to play with. No
    wonder InstaSaber became an insta-hit with millions of viral views online and
    in the news. The AI potential even got billionaire investor Mark Cuban to say,
    “you’ve got a deal” and invest in 2020CV.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 除了逼真，这些应用程序玩起来也很有趣。难怪InstaSaber在网上和新闻中成为一夜爆红，获得数百万次的病毒浏览。AI潜力甚至让亿万富翁投资者马克·库班说：“你得到了一笔交易”，并投资于2020CV。
- en: '![Screenshots of InstaSaber and YoPuppet](../images/00069.jpeg)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![InstaSaber和YoPuppet的屏幕截图](../images/00069.jpeg)'
- en: Figure 11-22\. Screenshots of InstaSaber and YoPuppet
  id: totrans-363
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-22。InstaSaber和YoPuppet的屏幕截图
- en: Summary
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we went on a whirlwind tour of the world of Core ML, which
    provides an inference engine for running machine learning and deep learning algorithms
    on an iOS device. Analyzing the minimal code needed for running a CNN, we built
    a real-time object recognition app that classifies among 1,000 ImageNet categories.
    Along the way, we discussed some useful facts about model conversion. Moving to
    the next level, we learned about practical techniques like dynamic model deployment
    and on-device training, while also benchmarking different deep learning models
    on various iOS devices, providing a deeper understanding of the battery and resource
    constraints. We also looked at how we can optimize the app size using model quantization.
    Finally, to take some inspiration from the industry, we explored real-life examples
    where Core ML is being used in production apps.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们快速浏览了Core ML的世界，它为在iOS设备上运行机器学习和深度学习算法提供了推理引擎。通过分析运行CNN所需的最小代码，我们构建了一个实时物体识别应用程序，可以对1,000个ImageNet类别进行分类。在此过程中，我们讨论了一些关于模型转换的有用信息。进入下一个级别，我们学习了关于动态模型部署和设备上训练等实用技术，同时还在各种iOS设备上对不同的深度学习模型进行了基准测试，从而更深入地了解了电池和资源约束。我们还看了如何使用模型量化来优化应用程序大小。最后，为了从行业中获得一些灵感，我们探索了Core
    ML在生产应用程序中的使用实例。
- en: In the next chapter, we build an end-to-end application by training using Create
    ML (among other tools), deploying our custom trained classifier, and running it
    using Core ML.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过使用Create ML（以及其他工具）进行训练，部署我们的自定义训练分类器，并使用Core ML运行一个端到端的应用程序。
