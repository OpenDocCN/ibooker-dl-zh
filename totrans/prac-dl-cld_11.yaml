- en: Chapter 11\. Real-Time Object Classification on iOS with Core ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen our deep learning models running on the desktop, the cloud,
    and the browser. Although there are definite upsides to such a setup, it might
    not be ideal for all scenarios. In this chapter, we explore making predictions
    using deep learning models on mobile devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bringing the computation closer to the user’s device, rather than a distant
    remote server, can be advantageous for many reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Latency and interactivity
  prefs: []
  type: TYPE_NORMAL
- en: 'Sending an image, processing it in the cloud, and returning the result can
    take several seconds depending on the network quality and quantity of data being
    transferred. This can make for a poor UX. Decades of UX research, including Jakob
    Nielsen’s findings in 1993, published in his book *Usability Engineering* (Elsevier),
    showed the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 0.1 second is about the limit for having the user feel that the system is reacting
    instantaneously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 second is about the limit for the user’s flow of thought to stay uninterrupted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10 seconds is about the limit for keeping the user’s attention focused.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About two decades later, Google published findings that half of all mobile browser
    users abandon a web page if it takes longer than three seconds to load. Forget
    three seconds, even a 100 ms increase in latency would result in a 1% decrease
    in sales for Amazon. That is a lot of lost revenue. Mitigating this by processing
    on the device instantaneously can make for rich and interactive UXs. Running deep
    learning models in real time, as is done with Snapchat Lenses, can increase engagement
    with users.
  prefs: []
  type: TYPE_NORMAL
- en: 24/7 availability and reduced cloud costs
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, sending less data to the cloud equates to less computing costs for
    the developer, leading to monetary savings. This reduces scaling costs, as well,
    when an app gains traction and grows to a large user base. For the users, computation
    on the edge is helpful, too, because they don’t need to worry about data plan
    costs. Additionally, processing locally means 24/7 availability, without the fear
    of losing connectivity.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy
  prefs: []
  type: TYPE_NORMAL
- en: For the user, local computation preserves privacy by not sharing the data externally,
    which could potentially be mined for user information. For the developer, this
    ensures less headache dealing with Personally Identifiable Information (PII).
    With the European Union’s General Data Protection Regulation (GDPR) and other
    user data protection laws coming up around the world, this becomes even more important.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hopefully, these arguments were convincing for why AI on mobile is important.
    For someone building any kind of serious application, the following are a few
    common questions to consider during the course of development:'
  prefs: []
  type: TYPE_NORMAL
- en: How do I convert my model to run on a smartphone?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will my model run on other platforms?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I run my model fast?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I minimize the size of the app?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I ensure that my app doesn’t drain the battery?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I update my model without going through the roughly two-day app review
    process?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I A/B test my models?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can I train a model on a device?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I protect my intellectual property (i.e., model) from being stolen?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next three chapters, we look at how to run deep learning algorithms on
    smartphones using different frameworks. In the process, we answer these questions
    as they come up.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we delve into the world of mobile AI for iOS devices. We first
    look at the general end-to-end software life cycle (shown in [Figure 11-1](part0013.html#the_mobile_ai_development_life_cycle))
    and see how the different parts fit in with one another. We explore the Core ML
    ecosystem, its history, and the features offered. Next, we deploy a real-time
    object classification app on an iOS device, and learn performance optimizations
    and benchmarks. And finally, we analyze some real-world apps built on Core ML.
  prefs: []
  type: TYPE_NORMAL
- en: Time to look at the big picture.
  prefs: []
  type: TYPE_NORMAL
- en: The Development Life Cycle for Artificial Intelligence on Mobile
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Figure 11-1](part0013.html#the_mobile_ai_development_life_cycle) depicts the
    typical life cycle for AI on mobile devices.'
  prefs: []
  type: TYPE_NORMAL
- en: '![The mobile AI development life cycle](../images/00199.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-1\. The mobile AI development life cycle
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s look at the phases in [Figure 11-1](part0013.html#the_mobile_ai_development_life_cycle)
    a bit more closely:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Collect data*: The data we collect should reflect the context in which the
    app would be used. Pictures taken by real users using smartphone cameras tend
    to be better training examples than pictures taken by professional photographers.
    We might not have this data on day one, but we can progressively collect more
    and more data as usage grows. A good starting point in many cases is to download
    images from search engines.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Label data*: We need associated labels for the data samples that we want our
    model to predict. High-quality (i.e., correct) labels are vital to a good model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Train model*: We build the highest-accuracy neural network possible with the
    data and associated labels we have so far.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Convert model*: Export the model from the training framework into the mobile-compatible
    framework.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Optimize performance*: Due to the resource-constrained nature of a mobile
    device, it is crucial to make the model efficient for memory, energy, and processor
    usage.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Deploy*: Add the model to the app and ship it to users.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Monitor*: Track the app usage in the real world to find opportunities to improve
    further. Additionally, gather samples of real-word data from consenting users
    to feed to this life cycle, and then back to Step 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the first portion of the book, we primarily explored phases 1, 2, and 3,
    and additionally general performance improvements. In this chapter, we focus on
    phases 4, 5, and 6\. And in the next few chapters, we explore all of these phases
    in the context of mobile development.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Before getting the app in the hands of real users (who might hate it if the
    app doesn’t perform as well as they expected), it’s common practice to gather
    feedback through a process called dogfooding. As the saying goes: Eat Your Own
    Dog Food. The process involves having an inner circle of loyal users who get to
    test early releases, thereby identifying bugs before they go out to the general
    public. For AI development, this inner circle might also contribute data and assess
    the success of the AI models in the real world. And as the models improve, they
    can be deployed to a gradually increasing number of test users before finally
    deploying to the public.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s jump right in!
  prefs: []
  type: TYPE_NORMAL
- en: A Brief History of Core ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Core ML provides one of the simplest ways to run a deep neural network for inference
    on an Apple device such as the iPhone and iPad as well as MacBook, Apple TV, and
    Apple Watch. In addition to being easy to use, it is also optimized for the underlying
    hardware architecture. Alternative frameworks have become a lot better in the
    past few years, but it’s difficult to beat the simplicity and performance offered
    by Core ML.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, to quickly run a CNN on an Apple device, developers *needed*
    to write on Metal, a library that was offered to game developers to better utilize
    the GPU. Unfortunately, developing on Metal was akin to writing in assembly language
    or CUDA code for an NVIDIA GPU. It was tedious, error prone, and difficult to
    debug. Few developers dared to tread that path. DeepLearningKit (December 2015)
    by Amund Tveit was one effort to build an abstraction over Metal for deploying
    CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: At the Apple Worldwide Developers Conference (WWDC) 2016, the company announced
    Metal Performance Shaders (MPS), a framework built on top of Metal, as a high-performance
    library for optimizing graphics and certain compute operations. It abstracted
    away a lot of low-level details, giving basic building blocks such as Convolution,
    Pooling, and ReLU. It allowed developers to write deep neural networks by combining
    these operations in code. For anyone who comes from the Keras world, this is a
    familiar and not-so-daunting task. Unfortunately, there is a lot of bookkeeping
    involved when writing MPS code, because you need to manually keep track of input
    and output dimensions at each step of the process. As an example, the code sample
    released by Apple for running the InceptionV3 model for recognizing 1,000 object
    categories was well over 2,000 lines long, with most of them defining the network.
    Now imagine changing the model slightly during training, and then having to dig
    through all 2,000 lines of code to reflect the same update in iOS code. Forge
    (April 2017), a library by Matthijs Hollemans, was an effort to simplify development
    over MPS by reducing the boilerplate code necessary to get a model running.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of these hardships went away when Apple announced Core ML at WWDC 2017\.
    This included an inference engine on iOS and an open source Python package called
    Core ML Tools to serialize CNN models from other frameworks like Keras and Caffe.
    The general workflow for building an app was: train a model in other packages,
    convert it to a *.mlmodel* file, and deploy it in an iOS app running on the Core
    ML platform.'
  prefs: []
  type: TYPE_NORMAL
- en: Core ML supports importing a broad range of machine learning models built with
    first- and third-party frameworks and file formats. [Figure 11-2](part0013.html#frameworks_compatible_with_core_ml_for_m)
    showcases a few of them (clockwise, starting in the upper left) such as TensorFlow,
    Keras, ONNX, scikit-learn, Caffe2, Apple’s Create ML, LIBSVM, and TuriCreate (also
    from Apple). ONNX itself supports a large variety of frameworks, including PyTorch
    (Facebook), MXNet (Amazon), Cognitive Toolkit (Microsoft), PaddlePaddle (Baidu),
    and more, thereby ensuring compatibility with any major framework under the sky.
  prefs: []
  type: TYPE_NORMAL
- en: '![Frameworks compatible with Core ML for model interchange as of 2019](../images/00181.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-2\. Frameworks compatible with Core ML for model interchange as of
    2019
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Alternatives to Core ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on the platform, there are a handful of options to achieve real-time
    predictions. These include general-purpose inference frameworks such as Core ML
    (from Apple), TensorFlow Lite (from Google), ML Kit (also from Google), and Fritz,
    as well as chip-specific accelerator frameworks including Snapdragon Neural Processing
    Engine (from Qualcomm) and Huawei AI Mobile Computing Platform (for Huawei’s Neural
    Processing Unit). [Table 11-1](part0013.html#a_comparison_of_mobile_device_ai_framewo)
    presents a high-level comparison of all these frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-1\. A comparison of mobile device AI frameworks
  prefs: []
  type: TYPE_NORMAL
- en: '| **Framework** | **Available for iOS** | **Available for Android** | **Dynamic
    updates** | **A/B testing** | **On-device training** | **Model encryption** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Core ML | ✓ | — | ✓ | — | ✓ | — |'
  prefs: []
  type: TYPE_TB
- en: '| TensorFlow Lite | ✓ | ✓ | — | — | Releases late 2019 | — |'
  prefs: []
  type: TYPE_TB
- en: '| ML Kit | ✓ | ✓ | ✓ | ✓ | — | — |'
  prefs: []
  type: TYPE_TB
- en: '| Fritz | ✓ | ✓ | ✓ | ✓ | — | ✓ |'
  prefs: []
  type: TYPE_TB
- en: TensorFlow Lite
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In November 2017, Google announced an on-device inference engine called TensorFlow
    Lite with the intention of extending the TensorFlow ecosystem beyond just servers
    and PCs. Prior to this, the options within the TensorFlow ecosystem were porting
    the entire TensorFlow library, itself ported to iOS (which was heavy and slow),
    and later on, its slightly stripped-down version called TensorFlow Mobile (which
    was still pretty bulky).
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Lite was rebuilt from the ground up targeting mobile and edge devices,
    optimizing for speed, model and interpreter size, and power consumption. It added
    support for GPU backend delegates, meaning that as long as there was GPU support
    implemented for a hardware platform, TensorFlow Lite could take advantage of the
    power of the GPU. On iOS, the GPU delegate uses Metal for acceleration. We discuss
    TensorFlow Lite at greater length in [Chapter 13](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b).
  prefs: []
  type: TYPE_NORMAL
- en: ML Kit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML Kit is a high-level library from Google that provides many computer vision,
    NLP, and AI functionalities out of the box, including the ability to run TensorFlow
    Lite models. Some of the features include face detection, barcode scanning, smart
    reply, on-device translation, and language identification. However, the main selling
    point of ML Kit is its integration with Google Firebase. Features offered by Firebase
    include dynamic model updates, A/B testing, and remote configuration-driven dynamic
    model selection (fancy words for choosing which model to use based on the customer).
    We explore ML Kit in greater detail in [Chapter 13](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b).
  prefs: []
  type: TYPE_NORMAL
- en: Fritz
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fritz is a startup founded with the goal of making the end-to-end process of
    mobile inference easier. It bridges the gap between machine learning practitioners
    and mobile engineers by providing easy-to-use command-line tools. On one side,
    it integrates training in Keras directly into the deployment pipeline, so a machine
    learning engineer could add a single line of Keras callback to deploy the model
    to users immediately after it finishes training. On the other side, a mobile engineer
    can benchmark the model without even needing to deploy to a physical device, simulating
    a model’s performance virtually, assess a Keras model’s compatibility with Core
    ML, and get analytics for each model. One unique selling point of Fritz is the
    model protection feature that prevents a model from deep inspection through obfuscation
    in the event that a phone is jailbroken.
  prefs: []
  type: TYPE_NORMAL
- en: Apple’s Machine Learning Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To gain a better understanding of the Core ML ecosystem, it’s useful to see
    the high-level picture of all the different APIs that Apple offers as well as
    how they fit with one another. [Figure 11-3](part0013.html#different_levels_of_apis_provided_by_app)
    gives us a look at the different components that make up Apple’s machine learning
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![Different levels of APIs provided by Apple for app developers](../images/00240.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-3\. Different levels of APIs provided by Apple for app developers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Domain-Based Frameworks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To simplify common tasks in machine learning without requiring domain expertise,
    Apple provides many APIs out of the box, in domains including Vision, Natural
    Language, Speech, and Sound Analysis. [Table 11-2](part0013.html#out-of-the-box_machine_learning_function)
    gives a detailed outline of the functionalities available on Apple operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-2\. Out-of-the-box machine learning functionality in Apple operating
    systems
  prefs: []
  type: TYPE_NORMAL
- en: '| **Vision** | **Natural language** | **Other** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Facial landmark detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image similarity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saliency detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optical character recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rectangle detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barcode detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizon detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human and animal detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object tracking (for video)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language identification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parts of speech identification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition (on-device and on-cloud)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sound classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: ML Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Core ML gives the ability to run inference on deep learning and machine learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: ML Performance Primitives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are some of the machine learning primitives in the Apple stack:'
  prefs: []
  type: TYPE_NORMAL
- en: MPS
  prefs: []
  type: TYPE_NORMAL
- en: Provides low-level and high-performance primitives that utilize the GPU to aid
    running most CNN-based networks fast. And if Core ML does not support a model,
    MPS provides all of the building blocks to enable us to build them. Additionally,
    we might consider using MPS to hand roll a model for performance reasons (like
    guaranteeing that the model runs on the GPU).
  prefs: []
  type: TYPE_NORMAL
- en: Accelerate and Basic Neural Network Subroutine
  prefs: []
  type: TYPE_NORMAL
- en: Accelerate is Apple’s implementation of the Basic Linear Algebra Subprogram
    (BLAS) library. It provides functionality for high-performance large-scale mathematical
    computations and image calculations, like Basic Neural Network Subroutine (BNNS),
    which helps to implement and run neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen how Core ML and the domain-specific APIs fit into the
    overall architecture, let’s see how little work is needed to run a machine learning
    model using Core ML and the Vision framework on an iOS app.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Apple provides several downloadable models ([Figure 11-4](part0013.html#ready-to-use_models_from_the_apple_machi))
    for various computer-vision tasks, from classification to detecting objects (with
    bounding boxes), segmenting (identifying the pixels), depth estimation, and more.
    You can find them at [*https://developer.apple.com/machine-learning/models/*](https://developer.apple.com/machine-learning/models).
  prefs: []
  type: TYPE_NORMAL
- en: For classification, you can find many pretrained Core ML models on Apple’s Machine
    Learning website, including MobileNet, SqueezeNet, ResNet-50, and VGG16.
  prefs: []
  type: TYPE_NORMAL
- en: '![Ready-to-use models from the Apple Machine Learning website](../images/00074.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-4\. Ready-to-use models from the Apple Machine Learning website
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Building a Real-Time Object Recognition App
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we don’t intend to teach iOS development, we want to demonstrate running
    an object recognition model that classifies among 1,000 ImageNet categories in
    real time on an iOS device.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will examine the minimal amount of code necessary to get this app running.
    The general outline is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Drag and drop an *.mlmodel* file to the Xcode project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the model into a Vision container (`VNCoreMLModel`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a request (`VNCoreMLRequest`) based on that container and provide a function
    that would be called when the request is completed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a request handler that can process the request based on an image provided.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the request and print out the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s take a look at the code to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this code, `cgImage` can be an image from a variety of sources. It can be
    a photo from the photo library, or from the web.
  prefs: []
  type: TYPE_NORMAL
- en: We can also power real-time scenarios using the camera and pass the individual
    camera frames into this function. A regular iPhone camera can shoot up to 60 frames
    per second (FPS).
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Core ML crops the image along the longer edge. In other words,
    if a model requires square dimensions, Core ML will extract the biggest square
    in the center of the image. This can be a source of confusion for developers who
    find the top and bottom strips of an image are being ignored for making predictions.
    Depending on the scenario, we might want to use `.centerCrop`, `.scaleFit`, or
    the `.scaleFill` option, as illustrated in [Figure 11-5](part0013.html#how_different_scaling_options_modify_the),
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![How different scaling options modify the input image to Core ML models](../images/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-5\. How different scaling options modify the input image to Core ML
    models
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now that we have gone through the meat of it, what could be more fun? How about
    actually running it on a phone! We’ve made a complete app available on the book’s
    GitHub website (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    at *code/chapter-11*. With an iPhone or an iPad, we can deploy this pretty quickly
    and experiment with it, even without knowledge of iOS development. Here are the
    steps (note: this requires a Mac):'
  prefs: []
  type: TYPE_NORMAL
- en: Download Xcode from the Apple developer website or the Mac App Store.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plug in an iOS device. The phone needs to remain unlocked during deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change the current working directory to `CameraApp`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download the Core ML models from the Apple website using the available Bash
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open the Xcode project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the Project Hierarchy Navigator, in the upper-left corner, click the CameraApp
    project, as shown in [Figure 11-6](part0013.html#project_information_view_within_xcode),
    to open the Project Information view.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Project information view within Xcode](../images/00228.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11-6\. Project information view within Xcode
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Because Xcode reserves a unique bundle identifier, use a unique name to identify
    the project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to an Apple account to let Xcode sign the app and deploy it to the device.
    Select a team to do the signing, as shown in [Figure 11-7](part0013.html#select_a_team_and_let_xcode_automaticall).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Select a team and let Xcode automatically manage code signing](../images/00281.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11-7\. Select a team and let Xcode automatically manage code signing
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Click the “Build and Run” button (the right-facing triangle) to deploy the app
    on the device, as shown in [Figure 11-8](part0013.html#select_the_device_and_click_the_quotatio).
    This should typically take around 30 to 60 seconds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Select the device and click the “Build and Run” button to deploy the app](../images/00261.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11-8\. Select the device and click the “Build and Run” button to deploy
    the app
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The device will not run the app right away, because it is not trusted. Go to
    Settings > General > Profiles and Device Management and select the row with your
    information on it, and then tap “Trust {your_email_id},” as shown in [Figure 11-9](part0013.html#profiles_and_device_management_screen).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Profiles and Device Management screen](../images/00187.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11-9\. Profiles and Device Management screen
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: On the home screen, find CameraApp and run the app.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Screenshot of the app](../images/00167.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11-10\. Screenshot of the app
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The output shows predictions of “notebook, notebook computer” with 84% confidence
    and “laptop, laptop computer” with 11% confidence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'That was all fun and good. Let’s now get down to more serious business: converting
    models from different frameworks to Core ML models.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A great thing about Xcode is that the model input and output parameters are
    shown when we load the *.mlmodel* file into Xcode, as demonstrated in [Figure 11-11](part0013.html#xcode_model_inspector_showing_the_inputs).
    This is especially helpful when we have not trained the model ourselves and don’t
    necessarily want to write code (like `model.summary()` in Keras) to explore the
    model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![Xcode model inspector showing the inputs and outputs of the MobileNetV2 model](../images/00145.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-11\. Xcode model inspector showing the inputs and outputs of the MobileNetV2
    model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Conversion to Core ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the code sample we built, you saw the *Inceptionv3.mlmodel* file. Ever wonder
    how that file came to be? Inception was, after all, trained by Google using TensorFlow.
    That file was converted from a *.pb* file into a Core ML model. We might similarly
    have models that need to be converted from Keras, Caffe or any other framework
    to Core ML. The following are a few tools that enable model conversion to Core
    ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'Core ML Tools (Apple): from Keras (*.h5*), Caffe (*.caffemodel*), as well as
    machine learning libraries such as LIBSVM, scikit-learn, and XGBoost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'tf-coreml (Google): from TensorFlow (*.pb*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'onnx-coreml (ONNX): from ONNX (*.onnx*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We take a look at the first two converters in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Conversion from Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Core ML Tools aids the translation of Keras, ONNX, and other model formats
    to the Core ML format (*.mlmodel*). Install the `coremltools` framework using
    `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s see how we can use an existing Keras model and convert it to Core
    ML. The conversion is just a one-line command and then we can save the converted
    model, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: That’s it! It can’t get any simpler. We discuss how to convert models with unsupported
    layers (such as MobileNet) in [Chapter 12](part0014.html#DB7S3-13fa565533764549a6f0ab7f11eed62b).
  prefs: []
  type: TYPE_NORMAL
- en: Conversion from TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apple recommends using `tf-coreml` (from Google) for converting from TensorFlow-based
    models. In the following steps, we convert a pretrained TensorFlow model to Core
    ML. This process is a bit more involved than the single line of code we saw earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we perform the installation of `tfcoreml` using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To do the conversion, we need to know the first and last layers of the model.
    We can ascertain this by inspecting the model architecture using a model visualization
    tool like [Netron](https://oreil.ly/hJoly). After loading the MobileNet model
    (*.pb* file) in Netron, we can visualize the entire model in the graphical interface.
    [Figure 11-12](part0013.html#output_layer_of_mobilenet-v1_as_viewed_i) shows a
    small portion of the MobileNet model; in particular, the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Output layer of MobileNet as seen in Netron](../images/00146.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-12\. Output layer of MobileNet as seen in Netron
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We simply need to plug that into the following Python code as an argument and
    run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As the script runs, we see each of the operations from each layer being converted
    to their corresponding Core ML equivalents. When finished, we should find the
    *.mlmodel* exported in the directory.
  prefs: []
  type: TYPE_NORMAL
- en: Our Core ML model is now ready. Drag it into Xcode to test it.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic Model Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As developers, we will likely want to keep improving our models over time. After
    all, we’d want our users to have access to the latest and greatest model as quickly
    as we can hand it to them. One way to handle that is to send an update to the
    App Store every time we want to deploy a new model. That approach does not work
    very well, because we’d need to wait about two days for Apple’s approval each
    time, potentially causing significant delays.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternate recommended approach would be to have the app dynamically download
    the *.mlmodel* file and compile it within the user’s device. There are several
    reasons why we might want to try this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: We want to regularly update the model, independent of the cadence of our App
    Store releases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to keep the app download size small and have the user eventually download
    only the models relevant for their usage scenarios. This will reduce storage needs
    and bandwidth costs. Apple places a limit of 200 MB on App Store downloads via
    cellular networks, so it’s vital to remain under that limit to not lose out on
    potential downloads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to A/B test different models on different sets of users to further improve
    the quality of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to use different models for different user segments, regions, and locales.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The process to achieve this is rather simple: host the *.mlmodel* on a server
    and design our app to dynamically download the file. After the model is on the
    user’s device, we can run `MLModel.compileModel` on that file to generate a compiled
    version of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Keep in mind that `compiledModelUrl` is an address to a temporary location.
    If you want to keep the model on the device longer beyond a session, you must
    move it to permanent storage.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although we can manually do model management directly with Core ML, it would
    still involve writing a lot of boilerplate code, on the backend and on the device.
    We would need to manually manage the versions of each model, the file storage,
    and configuration infrastructure, as well as any errors in the process. This is
    where ML Kit and Fritz really shine by providing these features out of the box.
    We discuss this in more detail in [Chapter 13](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b).
  prefs: []
  type: TYPE_NORMAL
- en: On-Device Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have looked at scenarios that would be served well by a “one size
    fits all” neural network. However, some use cases would simply not work without
    personalizing the models. An example would be an app that organizes a user’s photo
    gallery based on recognizing the faces of people in each picture. Given that an
    average user’s phone is mostly filled with pictures of friends and family, a generic
    model trained on Danny DeVito’s and Tom Hanks’s faces would not be very useful
    for users (unless, of course, they belong to the DeVito or Hanks families).
  prefs: []
  type: TYPE_NORMAL
- en: A real-life example would be on the iOS system keyboard, which learns a user’s
    language patterns over time and begins to give more and more relevant suggestions
    for that user. And this becomes even more evident when that person uses slang
    words, nicknames, domain-specific terms, and so on, which might not be part of
    the common language dictionary. Personalized suggestions based on such data would
    be useless for any other user.
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, we want to collect the data for that user and train a personalized
    model for that individual only. One way to accomplish this would be to collect
    and send the data to the cloud, train a new model there, and send back the updated
    model to the user. This approach reeks of scalability, cost, and privacy issues.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, Core ML provides functionality for on-device training, so that user’s
    data never needs to leave the device. A Core ML model is updatable when its `isUpdatable`
    property is set to `true`. Additionally, the specific set of layers that need
    to be retrained (usually toward the tail end of the network) also need to have
    the same property set to `true`. Additional training parameters such as learning
    rate and optimizers can also be set on the model.
  prefs: []
  type: TYPE_NORMAL
- en: Even though training can consume GPU and Neural Processor Unit (NPU; more on
    these in [Chapter 13](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b)) cycles,
    the training can be scheduled in the background (using the `BackgroundTasks` framework),
    even when the device is idle and charging, typically at night. This will have
    the least impact on UX.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do on-device training, we can call the `MLUpdateTask` function, along with
    the new data with which we will be retraining the model. We would also need to
    pass in the path for the newly updated model in the function call. After training
    is complete, the model at that path is ready to go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Federated Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On-device training is great—except for one downside: the generic global model
    does not get a chance to improve. Wouldn’t it be great for a developer to somehow
    use that data being generated on each user’s device to improve the global model
    without needing to transfer their data from their device? This is where *federated
    learning* comes in.'
  prefs: []
  type: TYPE_NORMAL
- en: Federated learning is a collaborative distributed training process. It essentially
    takes on-device training one step further by sending the incremental updates (on
    the personalized models from users’ devices) to the cloud and aggregating many
    of these over the entire user base, subsequently enriching the global model for
    everyone. Keep in mind that no user data is being transmitted here and it is not
    possible to reverse engineer the user data from the aggregated feature set. With
    this approach, we are able to respect our users’ privacy while also benefiting
    everyone through collective participation.
  prefs: []
  type: TYPE_NORMAL
- en: On-device training is a vital stepping stone for federated learning. Even though
    we are not there yet, this is the direction in which the industry is moving. We
    can expect to see more support for federated learning over time.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Federated is one such implementation of federated learning, powering
    training for Google’s GBoard, a keyboard app. Training on user devices occurs
    in the background when they are being charged.
  prefs: []
  type: TYPE_NORMAL
- en: Performance Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Writing prototypes is one thing. Making production-ready apps is another ball
    game entirely. There are several factors that influence the user’s experience,
    and it is important to understand the trade-offs. Some of these include the supported
    device models, minimum operating system (OS) version, processing frame rate, and
    choice of the deep learning model. In this section, we explore the impact that
    some of these factors can have on the quality and performance of the product.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking Models on iPhones
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As is the case with benchmarking, it is best to perform experiments on publicly
    available models that we can download easily and get our hands dirty with!
  prefs: []
  type: TYPE_NORMAL
- en: First, we ran our real-time object classification app on multiple iPhones produced
    from 2013 to 2018\. [Table 11-3](part0013.html#benchmarking_inference_time_on_different)
    catalogs the results of those experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-3\. Benchmarking inference time on different models on different iPhone
    versions
  prefs: []
  type: TYPE_NORMAL
- en: '| Device model | iPhone 5s | iPhone 6 | iPhone 6s | iPhone 7+ | iPhone X |
    iPhone XS | iPhone 11 Pro |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Year released** | 2013 | 2014 | 2015 | 2016 | 2017 | 2018 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **RAM** | 1 GB | 1 GB | 2 GB | 2 GB | 2 GB | 4 GB | 4GB |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Processor chip** | A7 | A8 | A9 | A10 | A11 | A12 | A13 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Model** | **Accuracy (%)** | **Size (MB)** | **FPS** | **FPS** | **FPS**
    | **FPS** | **FPS** | **FPS** | **FPS** |'
  prefs: []
  type: TYPE_TB
- en: '| **VGG-16** | 71 | 553 | 0.1 | 0.2 | 4.2 | 5.5 | 6.9 | 27.8 | 34.5 |'
  prefs: []
  type: TYPE_TB
- en: '| **InceptionV3** | 78 | 95 | 1.4 | 1.5 | 9 | 11.1 | 12.8 | 35.7 | 41.7 |'
  prefs: []
  type: TYPE_TB
- en: '| **ResNet-50** | 75 | 103 | 1.9 | 1.7 | 11.6 | 13.5 | 14.1 | 38.5 | 50 |'
  prefs: []
  type: TYPE_TB
- en: '| **MobileNet** | 71 | 17 | 7.8 | 9 | 19.6 | 28.6 | 28.6 | 55.6 | 71.4 |'
  prefs: []
  type: TYPE_TB
- en: '| **SqueezeNet** | 57 | 5 | 13.3 | 12.4 | 29.4 | 33.3 | 34.5 | 66.7 | 76.9
    |'
  prefs: []
  type: TYPE_TB
- en: The difference between inference times in 2014 and 2015 is particularly striking.
    What happened in 2015? If you guessed GPUs, you’d be right. The iPhone 6S introduced
    a dedicated GPU for the first time, powering features such as “Hey Siri.”
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, let’s look at the device market share among iPhones in the
    release month of the iPhone XS (Sept. 2018) in the United States, as shown in
    [Table 11-4](part0013.html#us_device_market_share_among_iphones_as). Note that
    releases typically happen in September of each year.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-4\. US device market share among iPhones as of September 2018 (the
    release month of the iPhone XS; data from [Flurry Analytics](https://oreil.ly/L47c0)
    adjusted to exclude iPhone XS, XS Plus, and XS Max)
  prefs: []
  type: TYPE_NORMAL
- en: '| Year released | iPhone model | Percentage |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | 8 Plus | 10.8% |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | X | 10.3% |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | 8 | 8.1% |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | 7 | 15.6% |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | 7 Plus | 12.9% |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | SE | 4.2% |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | 6S | 12.5% |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | 6S Plus | 6.1% |'
  prefs: []
  type: TYPE_TB
- en: '| 2014 | 6 | 10.7% |'
  prefs: []
  type: TYPE_TB
- en: '| 2014 | 6 Plus | 3.3% |'
  prefs: []
  type: TYPE_TB
- en: '| 2013 | 5S | 3.4% |'
  prefs: []
  type: TYPE_TB
- en: '| 2013 | 5C | 0.8% |'
  prefs: []
  type: TYPE_TB
- en: '| 2012 | 5 | 0.8% |'
  prefs: []
  type: TYPE_TB
- en: '| 2011 | 4S | 0.4% |'
  prefs: []
  type: TYPE_TB
- en: '| 2010 | 4 | 0.2% |'
  prefs: []
  type: TYPE_TB
- en: Deriving the cumulative percentages from [Table 11-4](part0013.html#us_device_market_share_among_iphones_as),
    we get [Table 11-5](part0013.html#cumulative_market_share_of_iphonescomma). This
    table expresses our potential market share based on the oldest device we choose
    to support. For example, iPhones released after September 2016 (two years before
    September 2018) have a total market share of 61.9%.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-5\. Cumulative market share of iPhones, year-on-year, going back in
    time
  prefs: []
  type: TYPE_NORMAL
- en: '| Years under | Cumulative percentage |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 29.2% |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 61.9% |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 80.5% |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 94.5% |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 98.7% |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 99.5% |'
  prefs: []
  type: TYPE_TB
- en: 'Combining the benchmark and market share, there are some design choices and
    optimizations available to us:'
  prefs: []
  type: TYPE_NORMAL
- en: Using a faster model
  prefs: []
  type: TYPE_NORMAL
- en: On an iPhone 6, VGG-16 runs about 40 times slower than MobileNet. On an iPhone
    XS, it’s still about two times slower. Just choosing a more efficient model can
    have a dramatic impact on performance, often without the need to compromise an
    equivalent amount of accuracy. It’s worth noting that MobileNetV2 and EfficientNet
    offer an even better combination of speed and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the minimum FPS to support
  prefs: []
  type: TYPE_NORMAL
- en: A feature to apply filters on a real-time camera feed will need to be able to
    process a high number of the frames delivered by the camera every second to ensure
    a smooth experience. In contrast, an app that processes a single image due to
    a user-initiated action does not need to worry about performance nearly as much.
    A lot of applications are going to be somewhere in between. It’s important to
    determine the minimum necessary FPS and perform benchmarking similar to [Table 11-5](part0013.html#cumulative_market_share_of_iphonescomma)
    to arrive at the best model(s) for the scenario across iPhone generations.
  prefs: []
  type: TYPE_NORMAL
- en: Batch processing
  prefs: []
  type: TYPE_NORMAL
- en: GPUs are really good at parallel processing. So, it comes as no surprise that
    processing a batch of data is more efficient than performing inferences on each
    item individually. Core ML takes advantage of this fact by exposing batch processing
    APIs. Some user experiences, particularly those that are asynchronous and/or memory
    intensive, can vastly benefit from these batching APIs. For example, any bulk
    processing of photos in the photo gallery. Instead of processing one image at
    a time, we can send a list of images together to the batch API, so that Core ML
    can optimize the performance on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic model selection
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the use case, we might not care nearly as much about high levels
    of accuracy as we do about a smooth experience (at some minimum FPS threshold).
    In these scenarios, we might choose a lighter, less-accurate model on a slower,
    older device, and a larger, more-accurate model on faster, modern devices. This
    can be used in conjunction with model deployment via the cloud so that we don’t
    needlessly increase the app size.
  prefs: []
  type: TYPE_NORMAL
- en: Using *Sherlocking* to our advantage
  prefs: []
  type: TYPE_NORMAL
- en: “Sherlocking” is a term used for when first-party vendors (Apple in particular)
    make third-party software obsolete by baking in a feature themselves. A good example
    of that is when Apple released the Flashlight feature within the iPhone rendering
    all third-party flashlight apps (whether paid or free) obsolete. For illustration
    here, take the example of a hypothetical app that released a face tracking feature
    in 2017\. A year later, Apple added a more accurate face tracking API within Core
    ML for iOS 12\. Because the neural networks for the face tracking come baked into
    the OS, the app could update its code to use the built-in API. However, because
    the API would still not be available for any users in iOS 11, the app could use
    a hybrid approach in which it would remain backward compatible by using the old
    code path for iOS 11\. The app developer could then stop bundling the neural network
    into the app, thereby reducing its size, potentially by several megabytes, and
    dynamically download the older model for users of iOS 11.
  prefs: []
  type: TYPE_NORMAL
- en: Graceful degradation
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes an older phone just cannot handle newer deep learning models due to
    performance needs. That’s a perfectly reasonable situation to be in, particularly
    if the feature is cutting edge. In those scenarios, it’s acceptable to use one
    of two approaches. The first is to offload the computation to the cloud. This
    obviously comes at the cost of interactivity and privacy. The other alternative
    is to disable the feature for those users and display a message to them stating
    why it is not available.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Energy Impact
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we focused on hosting classifiers on servers. Although
    there are several factors that affect design decisions, energy consumption is
    often not among them. However, on the client side, battery power tends to be limited,
    and minimizing its consumption becomes a priority. How users perceive a product
    depends a lot on how much energy it consumes. Remember the good old days when
    GPS was a battery hog. Many apps that required location access used to get tons
    of one-star ratings for eating up too much battery power. And we definitely don’t
    want that.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: People are pretty generous with bad reviews when they realize an app is eating
    their battery faster than anticipated, as the reviews shown in [Figure 11-13](part0013.html#app_store_reviews_for_youtube_and_snapch)
    demonstrate.
  prefs: []
  type: TYPE_NORMAL
- en: '![App Store reviews for YouTube and Snapchat that complain about heavy battery
    consumption](../images/00102.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-13\. App Store reviews for YouTube and Snapchat that complain about
    heavy battery consumption
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here we utilize the Energy Impact tab ([Figure 11-14](part0013.html#xcode_debug_navigator_tab))
    in Xcode’s Debug Navigator and generate [Figure 11-15](part0013.html#xcode_energy_impact_chart_on_an_ipad_pro).
  prefs: []
  type: TYPE_NORMAL
- en: '![Xcode Debug Navigator tab](../images/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-14\. Xcode Debug Navigator tab
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Xcode Energy Impact chart on an iPad Pro 2017 (note: this screenshot was
    taken at a different time than Figure 11-14, which is why the numbers are slightly
    different)](../images/00188.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-15\. Xcode Energy Impact chart on an iPad Pro 2017 (note: this screenshot
    was taken at a different time than [Figure 11-14](part0013.html#xcode_debug_navigator_tab),
    which is why the numbers are slightly different)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 11-15](part0013.html#xcode_energy_impact_chart_on_an_ipad_pro) demonstrates
    that the energy impact of running the process on the iPad Pro 2017 is high. A
    big reason for this is that in our sample code, we were processing every single
    frame that we were receiving from the camera. This meant that each frame that
    the camera captured was being sent to the GPU for processing, resulting in higher
    energy consumption. In many real-world applications, it is not necessary to classify
    every single frame. Even processing every other frame can result in significant
    energy savings without generally affecting the UX. In the next section, we explore
    the relationship between the frame processing rate and the energy impact.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The ratio of CPU to GPU usage is affected by a model’s architecture, specifically
    the number of convolutional operations. Here we profile the average CPU and GPU
    utilization for different models ([Figure 11-16](part0013.html#comparing_cpu_and_gpu_utilization_for_di)).
    These numbers can be extracted from the Energy Impact chart in Xcode. Note that
    we’d ideally prefer models with more GPU utilization for performance efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing CPU and GPU utilization for different models on iOS 11](../images/00310.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-16\. Comparing CPU and GPU utilization for different models on iOS
    11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Benchmarking Load
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you would expect, running CNN models in real time on each frame causes high
    GPU/CPU utilization, which in turn rapidly drains the battery. Users might also
    notice the phone heating up.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of running an analysis on every frame, how about we skip a few frames?
    MobileNet takes 20 ms to analyze one frame on an iPad Pro 2017\. So, it classifies
    about 50 FPS. Instead, if we run it at 1 FPS, the GPU utilization is reduced from
    42% to a mere 7%—a reduction of more than 83%! For many applications, processing
    at 1 FPS might be sufficient while still keeping the device cool. For example,
    a security camera might perform reasonably well even with processing frames once
    every couple of seconds.
  prefs: []
  type: TYPE_NORMAL
- en: By varying the number of frames analyzed per second and measuring the percentage
    of GPU utilization, we observe a fascinating trend. It is evident from the graph
    in [Figure 11-17](part0013.html#varying_the_fps_and_analyzing_the_load_o) that
    the higher the FPS, the more the GPU is utilized. That has a significant impact
    on energy consumption. For apps that need to be run for longer periods of time,
    it might be beneficial to reduce the number of inferences per second.
  prefs: []
  type: TYPE_NORMAL
- en: '![Varying the FPS and analyzing the load on an iPad Pro 2017](../images/00269.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-17\. Varying the FPS and analyzing the load on an iPad Pro 2017
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The values shown in the graph were derived from the Core Animation Instrument
    of Xcode Instruments. Following is the process we used to generate our results:'
  prefs: []
  type: TYPE_NORMAL
- en: From Xcode, click Product and then select Profile.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the Instruments window appears, select the Core Animation Instrument,
    as shown in [Figure 11-18](part0013.html#the_instruments_window_in_xcode_instrume).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![The Instruments window in Xcode Instruments](../images/00227.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11-18\. The Instruments window in Xcode Instruments
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Press the Record button to start running the app in the Profiling mode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait a few seconds to begin collecting instrumentation data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure the values in the GPU Hardware Utilization column ([Figure 11-19](part0013.html#an_app_being_profiled_live_in_the_core_a)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![An app being profiled live in the Core Animation instrument](../images/00192.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11-19\. An app being profiled live in the Core Animation instrument
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: So far in this chapter, we’ve explored techniques for making production-worthy
    apps using Core ML. In the next section, we discuss examples of such apps in the
    real world.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing App Size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'App size can be crucially important to developers in certain markets. Apple
    does not allow apps larger than 200 MB to be downloaded via the cellular network.
    This becomes a crucial point of contention for apps that are used frequently on
    the go, such as ride-hailing apps like Uber and Lyft. Interesting tidbit here:
    Uber had to do some really stringent optimizations such as heavily reducing the
    number of Swift Optionals, Structs, and Protocols (which otherwise help with code
    maintainability) in order to bring the app binary size below the App Store limit
    (at the time, 150 MB). Without doing that, the company would have lost a lot of
    new users.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s a given that any new AI functionality we introduce in the app will result
    in additional usage of storage. There are a few strategies that we can employ
    to reduce the impact of that.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Segment, a San Francisco-based data analytics company, wanted to determine how
    much of an impact app size has on the number of installs. To experiment, the company
    purchased a mortgage calculator app that had a steady number of app downloads
    (approximately 50 per day). It took the app that was originally 3 MB in size and
    kept bloating it repeatedly with…well, images of Taylor Swift albums (hey, it
    was in the name of science and research!). Engineers observed significant drops
    in the number of daily installs as the company increased the app size. When the
    app crossed 100 MB (the maximum limit for cellular network downloads from the
    App Store at the time), the number of daily installs dropped a whopping 44%! Additionally,
    the app attracted several negative reviews with users expressing incredulity at
    the app size.
  prefs: []
  type: TYPE_NORMAL
- en: The moral of the story here is that the app size is far more crucial than we
    think, and we ought to be mindful of the amount of space our app consumes before
    releasing it out to the public.
  prefs: []
  type: TYPE_NORMAL
- en: Avoid Bundling the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If possible, we should avoid bundling the model with the App Store binary. The
    same amount of data needs to be downloaded anyway, so as long as the UX is not
    affected, we should delay model download to when the feature is actually going
    to be used. Additionally, we should prefer to download the model when on WiFi
    in order to preserve cellular bandwidth. Microsoft Translator and Google Translate
    implement a form of this where they do only cloud-based translations at the outset.
    Knowing that travelers use these apps a lot (where they might not have good internet
    access), they also offer an offline mode where the required language models are
    downloaded at the user’s prompt in the background.
  prefs: []
  type: TYPE_NORMAL
- en: Use Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed in [Chapter 6](part0008.html#7K4G3-13fa565533764549a6f0ab7f11eed62b),
    quantization is a good strategy to reduce model size drastically while still preserving
    performance. Essentially, it reduces 32-bit floating-point weights down to 16-bit
    floating point, and 8-bit integers all the way down to 1-bit. We definitely won’t
    recommend going below 8 bits, though, due to the resulting loss in accuracy. We
    can achieve quantization with Core ML Tools for Keras models in just a couple
    of lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To illustrate the impact of quantization on a challenging dataset where small
    modifications could lead to drastic changes, we chose to build a classifier for
    Oxford’s 102 Category Flower Dataset with Keras (roughly 14 MB in size), and quantized
    it to different bit representations while measuring its accuracy and decreasing
    its size. To measure the change in predictions, we compare the percent match between
    the full-precision model and the quantized model. We tested three quantization
    modes.
  prefs: []
  type: TYPE_NORMAL
- en: Simple `linear` quantization, which we described in [Chapter 6](part0008.html#7K4G3-13fa565533764549a6f0ab7f11eed62b).
    The intervals are distributed equally in this strategy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear quantization using a lookup table, or `linear_lut`. In this technique,
    the intervals are distributed unequally, with denser areas getting smaller and
    more intervals, whereas sparser areas would have fewer and larger intervals. Because
    these intervals are unequal, they need to be stored in the lookup table, rather
    than being directly computed with simple arithmetic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lookup table generated by *k*-means, or `kmeans_lut`, which is often used in
    nearest neighbor classifiers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Table 11-6](part0013.html#quantization_results_for_different_targe) shows
    our observations.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-6\. Quantization results for different target bit sizes and different
    quantization modes
  prefs: []
  type: TYPE_NORMAL
- en: '| **Quantized to** | **Percent size reduction (approx.)** | **Percent match
    with 32-bit results** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **`linear`** | **`linear_lut`** | **`kmeans_lut`** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 16-bit | 50% | 100% | 100% | 100% |'
  prefs: []
  type: TYPE_TB
- en: '| 8-bit | 75% | 88.37% | 80.62% | 98.45% |'
  prefs: []
  type: TYPE_TB
- en: '| 4-bit | 88% | 0% | 0% | 81.4% |'
  prefs: []
  type: TYPE_TB
- en: '| 2-bit | 94% | 0% | 0% | 10.08% |'
  prefs: []
  type: TYPE_TB
- en: '| 1-bit | 97% | 0% | 0% | 7.75% |'
  prefs: []
  type: TYPE_TB
- en: This gives us a few insights.
  prefs: []
  type: TYPE_NORMAL
- en: Lowering representation to 16-bits has no effect at all on accuracy. We can
    essentially halve the model size with no perceivable difference in accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k*-means when used for building the lookup table outperforms simple linear
    division methods. Even at 4-bit, only 20% accuracy is lost, which is remarkable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going to 8-bit quantized models gives 4 times smaller modes with little loss
    in accuracy (especially for `kmeans_lut` mode).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantizing to lower than 8-bits results in a drastic drop in accuracy, particularly
    with the linear modes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Create ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Create ML is a tool from Apple to train models by simply dragging and dropping
    data into a GUI application on the Mac. It provides several templates including
    object classification/detection, sound classification, and text classification,
    among others, putting training AI in the hands of novices without requiring any
    domain expertise. It uses transfer learning to tune only the handful of layers
    that will be necessary for our task. Because of this, the training process can
    be completed in just a few minutes. The OS ships with the bulk of the layers (that
    are usable across several tasks), whereas the task-specific layers can be shipped
    as part of the app. The resulting exported model ends up being really small (as
    little as 17 KB, as we will see shortly). We get hands-on with Create ML in the
    next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Case Studies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s take a look at some real-world examples that use Core ML for mobile inferences.
  prefs: []
  type: TYPE_NORMAL
- en: Magic Sudoku
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Soon after the launch of [ARKit on iOS](https://developer.apple.com/arkit)
    in 2017, the [Magic Sudoku](https://magicsudoku.com) app from Hatchlings, a game
    and mobile app startup, came out as one of the break-out hits. Simply point the
    phone at a Sudoku puzzle, and the app will show a solved Sudoku right on the piece
    of paper. As we might have guessed at this point, the system is using Core ML
    to run a CNN-based digit recognizer. The system goes through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Use ARKit to get camera frames.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use iOS Vision Framework to find rectangles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine whether it is a Sudoku grid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract 81 squares from the Sudoku image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recognize digits in each square using Core ML.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a Sudoku solver to finish the empty squares.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Project the finished Sudoku on the surface of original paper using ARKit, as
    shown in [Figure 11-20](part0013.html#step-by-step_solution_to_solving_arkit_l).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Step-by-step solution to solving ARKit (image credit)](../images/00153.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-20\. Step-by-step solution to solving ARKit ([image source](https://oreil.ly/gzmb9))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Hatchlings team first started with using the MNIST digit recognition model,
    which mostly consists of handwritten digits. But this was not robust for printed
    fonts. The team then photographed thousands of pages of Sudoku books, extracting
    the squares using its pipeline until it had a large number of images of individual
    digits in a variety of fonts. To label this data, the team requested its fans
    classify each item as 0 through 9 and empty classes. Within 24 hours, the team
    got 600,000 digits scanned. The next step was to train a custom CNN, which needed
    to work fast, because the system needed to classify 81 square images. Deploying
    this model using Core ML, the app launched and become an overnight hit.
  prefs: []
  type: TYPE_NORMAL
- en: Running out with new users in the wild brought new cases not previously anticipated.
    Because most users do not have a Sudoku puzzle in front of them, they often search
    for Sudoku on computer screens, and this was difficult for the model to always
    recognize precisely. Additionally, due to a fixed focal length limitation of ARKit,
    the input image could be slightly blurry. To remedy this, the Hatchlings team
    collected additional examples of photos of computer screens with puzzles, blurred
    them slightly, and trained a new CNN with the additional data. With an App Store
    update, the whole experience became much more robust. In summary, while launching
    an app or a service, the app builder needs to constantly learn from new scenarios
    not previously anticipated.
  prefs: []
  type: TYPE_NORMAL
- en: Seeing AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Seeing AI](https://oreil.ly/hJxUE) is a talking camera app from Microsoft
    Research designed for the blind and low-vision community. It uses computer vision
    to describe people, text, handwriting, objects, currency, and more through spoken
    audio. Much of the image processing happens locally on the device, using Core
    ML. One of the core user needs is to recognize products—this can usually be determined
    by scanning close-ups of barcodes. But for a blind user, the location of a barcode
    is unknown, making most barcode apps difficult to use. To solve this, the team
    built a custom CNN trained with images containing barcodes at a variety of angles,
    sizes, lighting, and orientation. The user now attempts to rotate the object in
    front of the iPhone, and when the CNN classifies the presence of a barcode (running
    in real time, frame by frame), it signals with an audible beep. The rate of beeping
    directly correlates with the area of the barcode visible to the camera. As the
    blind users begin to bring the barcode closer, it beeps faster. When it is close
    enough for the barcode reading library to be able to clearly see the barcode,
    the app decodes the Universal Product Code and speaks the product name. In the
    past, blind users typically had to purchase and carry around bulky laser barcode
    scanners, which usually cost more than $1,300\. In fact, a charity raised millions
    to donate these hardware barcode readers to those who needed them. Now, deep learning
    can solve this problem for free. This is a good example of mixing computer vision
    and UX to solve a real-world problem.'
  prefs: []
  type: TYPE_NORMAL
- en: HomeCourt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For anything in life, regular practice is a must if we want to get better at
    something, be it writing, playing a musical instrument, or cooking. However, the
    quality of practice is far more important than quantity. Using data to support
    our practice and monitor our progress does wonders to the rate at which we acquire
    a skill. This is exactly what NEX Team, a San Jose, California-based startup,
    set out to do for basketball practice with the help of its HomeCourt app. Running
    the app is easy: set it up on the ground or a tripod, point the camera at the
    basketball court, and hit record.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The app runs an object detector in real time on top of Core ML to track the
    ball, the people, and the basketball hoop. Among these people, the big question
    remains: who shot the ball? When the ball goes near the hoop, the app rewinds
    the video to identify the player who shot the ball. Then, it performs human pose
    estimation on the player to track the player’s movements. If this wasn’t impressive
    enough, it uses geometric transformations to convert this 3D scene of the court
    into a 2D map (as shown in the upper right of [Figure 11-21](part0013.html#the_homecourt_app_tracking_a_playerapost))
    to track the locations from which the player shot the ball. The important thing
    to note here is that multiple models are being run simultaneously. Based on the
    tracked objects and the player’s body position, joints, and so on, the app provides
    the player with stats and visualizations for the release height of each throw,
    release angle, release position, release time, player speed, and more. These stats
    are important because they have a high correlation with a successful attempt.
    Players are able to record an entire game as they play and then go home and analyze
    each shot so that they can determine their weak areas in order to improve for
    the next time.'
  prefs: []
  type: TYPE_NORMAL
- en: In just under two years of existence, this tiny startup attracted hundreds of
    thousands of users as word of mouth spread, from amateurs to professionals. Even
    the National Basketball Association (NBA) partnered with NEX Team to help improve
    their players’ performance using HomeCourt. All of this happened because they
    saw an unmet need and came up with a creative solution using deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![The HomeCourt app tracking a player’s shots live as they are playing](../images/00104.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-21\. The HomeCourt app tracking a player’s shots live as they are
    playing
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: InstaSaber + YoPuppet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Do you know the largest profit maker in the *Star Wars* franchise? It’s not
    the movie ticket sales, and it’s definitely not from selling DVDs. Here’s a hint:
    it rhymes with merchandise. Lucas Film (and now Disney) made a lot of money selling
    R2D2 toys and Chewbacca costumes. Though the all-time favorite still remains the
    dear lightsaber. However, instead of looking cool like in the movies, the merchandise
    lightsabers are mostly made of plastic and frankly don’t look as sci-fi. Plus,
    you swing it around a few times and there’s a good chance you’ll accidentally
    whack someone in the face.'
  prefs: []
  type: TYPE_NORMAL
- en: Hart Woolery, the founder of 2020CV, decided to change that by bringing Hollywood-level
    VFX to our phone with the InstaSaber app. Simply roll up a sheet of paper, grip
    it, and point your phone’s camera toward it and watch it transform into a glowing
    lightsaber, as illustrated in [Figure 11-22](part0013.html#screenshots_of_instasaber_and_yopuppet).
    Wave your hand and not just watch it track it realistically in real time, but
    also hear the same sound effects as when Luke fought his father, (spoiler alert!)
    Darth Vader.
  prefs: []
  type: TYPE_NORMAL
- en: Taking the tracking magic to the next level, he built YoPuppet, which tracks
    the joints in hands in real time to build virtual puppets that mimic the motion
    of the hand. The suspension of disbelief works only if it is truly real time with
    no lag, is accurate, and realistic looking.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to being realistic, these apps were a lot of fun to play with. No
    wonder InstaSaber became an insta-hit with millions of viral views online and
    in the news. The AI potential even got billionaire investor Mark Cuban to say,
    “you’ve got a deal” and invest in 2020CV.
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshots of InstaSaber and YoPuppet](../images/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-22\. Screenshots of InstaSaber and YoPuppet
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went on a whirlwind tour of the world of Core ML, which
    provides an inference engine for running machine learning and deep learning algorithms
    on an iOS device. Analyzing the minimal code needed for running a CNN, we built
    a real-time object recognition app that classifies among 1,000 ImageNet categories.
    Along the way, we discussed some useful facts about model conversion. Moving to
    the next level, we learned about practical techniques like dynamic model deployment
    and on-device training, while also benchmarking different deep learning models
    on various iOS devices, providing a deeper understanding of the battery and resource
    constraints. We also looked at how we can optimize the app size using model quantization.
    Finally, to take some inspiration from the industry, we explored real-life examples
    where Core ML is being used in production apps.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we build an end-to-end application by training using Create
    ML (among other tools), deploying our custom trained classifier, and running it
    using Core ML.
  prefs: []
  type: TYPE_NORMAL
