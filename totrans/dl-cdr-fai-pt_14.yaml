- en: Chapter 11\. Data Munging with fastai’s Mid-Level API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen what `Tokenizer` and `Numericalize` do to a collection of texts,
    and how they’re used inside the data block API, which handles those transforms
    for us directly using the `TextBlock`. But what if we want to apply only one of
    those transforms, either to see intermediate results or because we have already
    tokenized texts? More generally, what can we do when the data block API is not
    flexible enough to accommodate our particular use case? For this, we need to use
    fastai’s *mid-level API* for processing data. The data block API is built on top
    of that layer, so it will allow you to do everything the data block API does,
    and much much more.
  prefs: []
  type: TYPE_NORMAL
- en: Going Deeper into fastai’s Layered API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The fastai library is built on a *layered API*. In the very top layer are *applications*
    that allow us to train a model in five lines of code, as we saw in [Chapter 1](ch01.xhtml#chapter_intro).
    In the case of creating `DataLoaders` for a text classifier, for instance, we
    used this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The factory method `TextDataLoaders.from_folder` is very convenient when your
    data is arranged the exact same way as the IMDb dataset, but in practice, that
    often won’t be the case. The data block API offers more flexibility. As we saw
    in the preceding chapter, we can get the same result with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: But it’s sometimes not flexible enough. For debugging purposes, for instance,
    we might need to apply just parts of the transforms that come with this data block.
    Or we might want to create a `DataLoaders` for an application that isn’t directly
    supported by fastai. In this section, we’ll dig into the pieces that are used
    inside fastai to implement the data block API. Understanding these will enable
    you to leverage the power and flexibility of this mid-tier API.
  prefs: []
  type: TYPE_NORMAL
- en: Mid-Level API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The mid-level API does not contain only functionality for creating `DataLoaders`.
    It also has the *callback* system, which allows us to customize the training loop
    any way we like, and the *general optimizer*. Both will be covered in [Chapter 16](ch16.xhtml#chapter_accel_sgd).
  prefs: []
  type: TYPE_NORMAL
- en: Transforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we studied tokenization and numericalization in the preceding chapter,
    we started by grabbing a bunch of texts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We then showed how to tokenize them with a `Tokenizer`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'and how to numericalize, including automatically creating the vocab for our
    corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The classes also have a `decode` method. For instance, `Numericalize.decode`
    gives us back the string tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`Tokenizer.decode` turns this back into a single string (it may not, however,
    be exactly the same as the original string; this depends on whether the tokenizer
    is *reversible*, which the default word tokenizer is not at the time we’re writing
    this book):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`decode` is used by fastai’s `show_batch` and `show_results`, as well as some
    other inference methods, to convert predictions and mini-batches into a human-understandable
    representation.'
  prefs: []
  type: TYPE_NORMAL
- en: For each of `tok` or `num` in the preceding examples, we created an object called
    the `setup` method (which trains the tokenizer if needed for `tok` and creates
    the vocab for `num`), applied it to our raw texts (by calling the object as a
    function), and then finally decoded the result back to an understandable representation.
    These steps are needed for most data preprocessing tasks, so fastai provides a
    class that encapsulates them. This is the `Transform` class. Both `Tokenize` and
    `Numericalize` are `Transform`s.
  prefs: []
  type: TYPE_NORMAL
- en: In general, a `Transform` is an object that behaves like a function and has
    an optional `setup` method that will initialize an inner state (like the vocab
    inside `num`) and an optional `decode` method that will reverse the function (this
    reversal may not be perfect, as we saw with `tok`).
  prefs: []
  type: TYPE_NORMAL
- en: 'A good example of `decode` is found in the `Normalize` transform that we saw
    in [Chapter 7](ch07.xhtml#chapter_sizing_and_tta): to be able to plot the images,
    its `decode` method undoes the normalization (i.e., it multiplies by the standard
    deviation and adds back the mean). On the other hand, data augmentation transforms
    do not have a `decode` method, since we want to show the effects on images to
    make sure the data augmentation is working as we want.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A special behavior of `Transform`s is that they always get applied over tuples.
    In general, our data is always a tuple `(input,target)` (sometimes with more than
    one input or more than one target). When applying a transform on an item like
    this, such as `Resize`, we don’t want to resize the tuple as a whole; instead,
    we want to resize the input (if applicable) and the target (if applicable) separately.
    It’s the same for batch transforms that do data augmentation: when the input is
    an image and the target is a segmentation mask, the transform needs to be applied
    (the same way) to the input and the target.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see this behavior if we pass a tuple of texts to `tok`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Writing Your Own Transform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you want to write a custom transform to apply to your data, the easiest
    way is to write a function. As you can see in this example, a `Transform` will
    be applied only to a matching type, if a type is provided (otherwise, it will
    always be applied). In the following code, the `:int` in the function signature
    means that `f` gets applied only to `ints`. That’s why `tfm(2.0)` returns `2.0`,
    but `tfm(2)` returns `3` here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here, `f` is converted to a `Transform` with no `setup` and no `decode` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Python has a special syntax for passing a function (like `f`) to another function
    (or something that behaves like a function, known as a *callable* in Python),
    called a *decorator*. A decorator is used by prepending a callable with `@` and
    placing it before a function definition (there are lots of good online tutorials
    about Python decorators, so take a look at one if this is a new concept for you).
    The following is identical to the previous code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If you need either `setup` or `decode`, you will need to subclass `Transform`
    to implement the actual encoding behavior in `encodes`, then (optionally) the
    setup behavior in `setups` and the decoding behavior in `decodes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `NormalizeMean` will initialize a certain state during the setup (the
    mean of all elements passed); then the transformation is to subtract that mean.
    For decoding purposes, we implement the reverse of that transformation by adding
    the mean. Here is an example of `NormalizeMean` in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the method called and the method implemented are different, for each
    of these methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | To call | To implement |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Module` (PyTorch) | `()` (i.e., call as function) | `forward` |'
  prefs: []
  type: TYPE_TB
- en: '| `Transform` | `()` | `encodes` |'
  prefs: []
  type: TYPE_TB
- en: '| `Transform` | `decode()` | `decodes` |'
  prefs: []
  type: TYPE_TB
- en: '| `Transform` | `setup()` | `setups` |'
  prefs: []
  type: TYPE_TB
- en: So, for instance, you would never call `setups` directly, but instead would
    call `setup`. The reason is that `setup` does some work before and after calling
    `setups` for you. To learn more about `Transform`s and how you can use them to
    implement different behavior depending on the type of input, be sure to check
    the tutorials in the fastai docs.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To compose several transforms together, fastai provides the `Pipeline` class.
    We define a `Pipeline` by passing it a list of `Transform`s; it will then compose
    the transforms inside it. When you call a `Pipeline` on an object, it will automatically
    call the transforms inside, in order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'And you can call `decode` on the result of your encoding, to get back something
    you can display and analyze:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The only part that doesn’t work the same way as in `Transform` is the setup.
    To properly set up a `Pipeline` of `Transform`s on some data, you need to use
    a `TfmdLists`.
  prefs: []
  type: TYPE_NORMAL
- en: 'TfmdLists and Datasets: Transformed Collections'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your data is usually a set of raw items (like filenames, or rows in a DataFrame)
    to which you want to apply a succession of transformations. We just saw that a
    succession of transformations is represented by a `Pipeline` in fastai. The class
    that groups this `Pipeline` with your raw items is called `TfmdLists`.
  prefs: []
  type: TYPE_NORMAL
- en: TfmdLists
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is the short way of doing the transformation we saw in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'At initialization, the `TfmdLists` will automatically call the `setup` method
    of each `Transform` in order, providing each not with the raw items but the items
    transformed by all the previous `Transform`s, in order. We can get the result
    of our `Pipeline` on any raw element just by indexing into the `TfmdLists`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'And the `TfmdLists` knows how to decode for show purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In fact, it even has a `show` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `TfmdLists` is named with an “s” because it can handle a training and a
    validation set with a `splits` argument. You just need to pass the indices of
    the elements that are in the training set and the indices of the elements that
    are in the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then access them through the `train` and `valid` attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: If you have manually written a `Transform` that performs all of your preprocessing
    at once, turning raw items into a tuple with inputs and targets, then `TfmdLists`
    is the class you need. You can directly convert it to a `DataLoaders` object with
    the `dataloaders` method. This is what we will do in our Siamese example later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, though, you will have two (or more) parallel pipelines of transforms:
    one for processing your raw items into inputs and one to process your raw items
    into targets. For instance, here, the pipeline we defined processes only the raw
    text into inputs. If we want to do text classification, we also have to process
    the labels into targets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we need to do two things. First we take the label name from the parent
    folder. There is a function, `parent_label`, for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we need a `Transform` that will grab the unique items and build a vocab
    with them during setup, then transform the string labels into integers when called.
    fastai provides this for us; it’s called `Categorize`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'To do the whole setup automatically on our list of files, we can create a `TfmdLists`
    as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: But then we end up with two separate objects for our inputs and targets, which
    is not what we want. This is where `Datasets` comes to the rescue.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Datasets` will apply two (or more) pipelines in parallel to the same raw object
    and build a tuple with the result. Like `TfmdLists`, it will automatically do
    the setup for us, and when we index into a `Datasets`, it will return us a tuple
    with the results of each pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Like a `TfmdLists`, we can pass along `splits` to a `Datasets` to split our
    data between training and validation sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'It can also decode any processed tuple or show it directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step is to convert our `Datasets` object to a `DataLoaders`, which
    can be done with the `dataloaders` method. Here we need to pass along a special
    argument to take care of the padding problem (as we saw in the preceding chapter).
    This needs to happen just before we batch the elements, so we pass it to `before_batch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '`dataloaders` directly calls `DataLoader` on each subset of our `Datasets`.
    fastai’s `DataLoader` expands the PyTorch class of the same name and is responsible
    for collating the items from our datasets into batches. It has a lot of points
    of customization, but the most important ones that you should know are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`after_item`'
  prefs: []
  type: TYPE_NORMAL
- en: Applied on each item after grabbing it inside the dataset. This is the equivalent
    of `item_tfms` in `DataBlock`.
  prefs: []
  type: TYPE_NORMAL
- en: '`before_batch`'
  prefs: []
  type: TYPE_NORMAL
- en: Applied on the list of items before they are collated. This is the ideal place
    to pad items to the same size.
  prefs: []
  type: TYPE_NORMAL
- en: '`after_batch`'
  prefs: []
  type: TYPE_NORMAL
- en: Applied on the batch as a whole after its construction. This is the equivalent
    of `batch_tfms` in `DataBlock`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a conclusion, here is the full code necessary to prepare the data for text
    classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The two differences from the previous code are the use of `GrandparentSplitter`
    to split our training and validation data, and the `dl_type` argument. This is
    to tell `dataloaders` to use the `SortedDL` class of `DataLoader`, and not the
    usual one. `SortedDL` constructs batches by putting samples of roughly the same
    lengths into batches.
  prefs: []
  type: TYPE_NORMAL
- en: 'This does the exact same thing as our previous `DataBlock`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: But now you know how to customize every single piece of it!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s practice what we just learned about using this mid-level API for data
    preprocessing on a computer vision example now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying the Mid-Level Data API: SiamesePair'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A *Siamese model* takes two images and has to determine whether they are of
    the same class. For this example, we will use the Pet dataset again and prepare
    the data for a model that will have to predict whether two images of pets are
    of the same breed. We will explain here how to prepare the data for such a model,
    and then we will train that model in [Chapter 15](ch15.xhtml#chapter_arch_details).
  prefs: []
  type: TYPE_NORMAL
- en: 'First things first—let’s get the images in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: If we didn’t care about showing our objects at all, we could directly create
    one transform to completely preprocess that list of files. We will want to look
    at those images, though, so we need to create a custom type. When you call the
    `show` method on a `TfmdLists` or a `Datasets` object, it will decode items until
    it reaches a type that contains a `show` method and use it to show the object.
    That `show` method gets passed a `ctx`, which could be a `matplotlib` axis for
    images or a row of a DataFrame for texts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we create a `SiameseImage` object that subclasses `Tuple` and is intended
    to contain three things: two images, and a Boolean that’s `True` if the images
    are of the same breed. We also implement the special `show` method, such that
    it concatenates the two images with a black line in the middle. Don’t worry too
    much about the part that is in the `if` test (which is to show the `SiameseImage`
    when the images are Python images, not tensors); the important part is in the
    last three lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s create a first `SiameseImage` and check that our `show` method works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_11in01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also try with a second image that’s not from the same class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_11in02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The important thing with transforms that we saw before is that they dispatch
    over tuples or their subclasses. That’s precisely why we chose to subclass `Tuple`
    in this instance—this way, we can apply any transform that works on images to
    our `SiameseImage`, and it will be applied on each image in the tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_11in03.png)'
  prefs: []
  type: TYPE_IMG
- en: Here the `Resize` transform is applied to each of the two images, but not the
    Boolean flag. Even if we have a custom type, we can thus benefit from all the
    data augmentation transforms inside the library.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to build the `Transform` that we will use to get our data
    ready for a Siamese model. First, we will need a function to determine the classes
    of all our images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'For each image, our transform will, with a probability of 0.5, draw an image
    from the same class and return a `SiameseImage` with a true label, or draw an
    image from another class and return a `SiameseImage` with a false label. This
    is all done in the private `_draw` function. There is one difference between the
    training and validation sets, which is why the transform needs to be initialized
    with the splits: on the training set, we will make that random pick each time
    we read an image, whereas on the validation set, we make this random pick once
    and for all at initialization. This way, we get more varied samples during training,
    but always the same validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then create our main transform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_11in04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the mid-level API for data collection, we have two objects that can help
    us apply transforms on a set of items: `TfmdLists` and `Datasets`. If you remember
    what we have just seen, one applies a `Pipeline` of transforms and the other applies
    several `Pipeline`s of transforms in parallel, to build tuples. Here, our main
    transform already builds the tuples, so we use `TfmdLists`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_11in05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And we can finally get our data in `DataLoaders` by calling the `dataloaders`
    method. One thing to be careful of here is that this method does not take `item_tfms`
    and `batch_tfms` like a `DataBlock`. The fastai `DataLoader` has several hooks
    that are named after events; here what we apply on the items after they are grabbed
    is called `after_item`, and what we apply on the batch once it’s built is called
    `after_batch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we need to pass more transforms than usual—that’s because the data
    block API usually adds them automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ToTensor` is the one that converts images to tensors (again, it’s applied
    on every part of the tuple).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IntToFloatTensor` converts the tensor of images containing integers from 0
    to 255 to a tensor of floats, and divides by 255 to make the values between 0
    and 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can now train a model using this `DataLoaders`. It will need a bit more customization
    than the usual model provided by `cnn_learner` since it has to take two images
    instead of one, but we will see how to create such a model and train it in [Chapter 15](ch15.xhtml#chapter_arch_details).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: fastai provides a layered API. It takes one line of code to grab the data when
    it’s in one of the usual settings, making it easy for beginners to focus on training
    a model without spending too much time assembling the data. Then, the high-level
    data block API gives you more flexibility by allowing you to mix and match building
    blocks. Underneath it, the mid-level API gives you greater flexibility to apply
    transformations on your items. In your real-world problems, this is probably what
    you will need to use, and we hope it makes the step of data-munging as easy as
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: Questionnaire
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why do we say that fastai has a “layered” API? What does it mean?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why does a `Transform` have a `decode` method? What does it do?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why does a `Transform` have a `setup` method? What does it do?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a `Transform` work when called on a tuple?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which methods do you need to implement when writing your own `Transform`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a `Normalize` transform that fully normalizes items (subtract the mean
    and divide by the standard deviation of the dataset), and that can decode that
    behavior. Try not to peek!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a `Transform` that does the numericalization of tokenized texts (it should
    set its vocab automatically from the dataset seen and have a `decode` method).
    Look at the source code of fastai if you need help.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a `Pipeline`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a `TfmdLists`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a `Datasets`? How is it different from a `TfmdLists`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are `TfmdLists` and `Datasets` named with an “s”?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you build a `DataLoaders` from a `TfmdLists` or a `Datasets`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you pass `item_tfms` and `batch_tfms` when building a `DataLoaders` from
    a `TfmdLists` or a `Datasets`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What do you need to do when you want to have your custom items work with methods
    like `show_batch` or `show_results`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why can we easily apply fastai data augmentation transforms to the `SiamesePair`
    we built?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the mid-level API to prepare the data in `DataLoaders` on your own datasets.
    Try this with the Pet dataset and the Adult dataset from [Chapter 1](ch01.xhtml#chapter_intro).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look at the Siamese tutorial in the [fastai documentation](https://docs.fast.ai)
    to learn how to customize the behavior of `show_batch` and `show_results` for
    new types of items. Implement it in your own project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Understanding fastai’s Applications: Wrap Up'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations—you’ve completed all of the chapters in this book that cover
    the key practical parts of training models and using deep learning! You know how
    to use all of fastai’s built-in applications, and how to customize them using
    the data block API and loss functions. You even know how to create a neural network
    from scratch and train it! (And hopefully you now know some of the questions to
    ask to make sure your creations help improve society too.)
  prefs: []
  type: TYPE_NORMAL
- en: The knowledge you already have is enough to create full working prototypes of
    many types of neural network application. More importantly, it will help you understand
    the capabilities and limitations of deep learning models, and how to design a
    system that’s well adapted to them.
  prefs: []
  type: TYPE_NORMAL
- en: In the rest of this book, we will be pulling apart those applications, piece
    by piece, to understand the foundations they are built on. This is important knowledge
    for a deep learning practitioner, because it allows you to inspect and debug models
    that you build and to create new applications that are customized for your particular
    projects.
  prefs: []
  type: TYPE_NORMAL
