<?xml version="1.0" encoding="utf-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <title>chapter-5</title>
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css" />
 </head>
 <body>
  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">5</span> </span><span class="chapter-title-text">RAG evaluation: Accuracy, relevance, and faithfulness</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header"><span class="CharOverride-1">This chapter covers</span></h3> 
   <ul> 
    <li class="readable-text" id="p2"><span class="CharOverride-2">The need and requirements for evaluating RAG pipelines</span></li> 
    <li class="readable-text" id="p3"><span class="CharOverride-2">Metrics, frameworks, and benchmarks for RAG evaluation</span></li> 
    <li class="readable-text" id="p4"><span class="CharOverride-2">Current limitations and future course of RAG evaluation</span></li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>Chapters 3 and 4 discussed the development of retrieval-augmented generation (RAG) systems using the indexing and generation pipelines. RAG promises to reduce hallucinations and ground the large language model (LLM) responses in the provided context, which is done by creating a non-parametric memory or knowledge base for the system and then retrieving information from it. </p> 
  </div> 
  <div class="readable-text intended-text" id="p6"> 
   <p>This chapter covers the methods used to evaluate how well the RAG system is functioning. We need to make sure that the components of the two RAG pipelines are performing per the expectations. At a high level, we need to ensure that the information being retrieved is relevant to the input query and that the LLM is generating responses grounded in the retrieved context. To this end, there have been several frameworks developed over time. Here we discuss some popular frameworks and the metrics they calculate. </p> 
  </div> 
  <div class="readable-text intended-text" id="p7"> 
   <p>There is also a second aspect to evaluation. While the frameworks allow for the calculation of metrics, how do you make sure that your RAG pipelines are working better than those developed by other developers? The evaluations cannot be done in isolation. For this purpose, several benchmarks have been established. These benchmarks evaluate the RAG systems on preset data, such as question–answer sets, for accurate comparison of different RAG pipelines. These benchmarks help developers evaluate the performance of their systems vis-&agrave;-vis those developed by other developers. </p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>Finally, like RAG techniques, the research on RAG evaluations is still in progress. There are still some limitations in the current set of evaluation parameters. We discuss these limitations and some ideas on the way forward for RAG evaluations. </p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>By the end of this chapter, you should</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p10">Know the fundamentals of RAG evaluations.</li> 
   <li class="readable-text" id="p11">Be aware of the popular frameworks, metrics, and benchmarks for RAG evaluation.</li> 
   <li class="readable-text" id="p12">Understand the limitations and best practices.</li> 
   <li class="readable-text" id="p13">Be able to evaluate the RAG pipeline in Python.</li> 
  </ul> 
  <div class="readable-text" id="p14"> 
   <p>For RAG to live up to the promise of grounding the LLM responses in data, you will need to go beyond the simple implementation of indexing, retrieval, augmentation, and generation. We will discuss these advanced strategies in chapter 6. However, to improve something, you need to first measure the performance. RAG evaluations help in setting up the baseline of your RAG system performance for you to then improve it. First, we look at the fundamental aspects of RAG systems evaluation.</p> 
  </div> 
  <div class="readable-text" id="p15"> 
   <h2 class=" readable-text-h2"><span class="num-string">5.1</span> Key aspects of RAG evaluation</h2> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>Building a PoC RAG pipeline is not overtly complex. It is achievable through brief training and verification of a limited set of examples. However, to enhance its robustness, thorough testing on a dataset that accurately mirrors the production use case is imperative. RAG pipelines can suffer from hallucinations of their own. This can be because </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p17">The retriever fails to retrieve the entire context or retrieves irrelevant context.</li> 
   <li class="readable-text" id="p18">Despite being provided the context, the LLM does not consider it.</li> 
   <li class="readable-text" id="p19">The LLM picks irrelevant information from the context instead of answering the query.</li> 
  </ul> 
  <div class="readable-text" id="p20"> 
   <p>Retrieval and generation are two processes that need special focus from an evaluation perspective. This is because these two steps produce outputs that can be evaluated. (While indexing and augmentation will have a bearing on the outputs, they do not produce measurable outcomes). Here are several questions we need to ask ourselves about these two processes: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p21">How good is the retrieval of the context from the knowledge base? </li> 
   <li class="readable-text" id="p22">Is it relevant to the query?</li> 
   <li class="readable-text" id="p23">How much noise (irrelevant information) is present?</li> 
   <li class="readable-text" id="p24">How good is the generated response? </li> 
   <li class="readable-text" id="p25">Is the response grounded in the provided context? </li> 
   <li class="readable-text" id="p26">Is the response relevant to the query?</li> 
  </ul> 
  <div class="readable-text" id="p27"> 
   <p>You can ask many more questions such as these to assess the performance of your RAG system. Contemporary research has discovered certain scores to assess the quality and abilities of a RAG system. The following sections discuss three predominant quality scores and four main abilities.</p> 
  </div> 
  <div class="readable-text" id="p28"> 
   <h3 class=" readable-text-h3"><span class="num-string">5.1.1</span> Quality scores</h3> 
  </div> 
  <div class="readable-text" id="p29"> 
   <p>There are three quality score dimensions prevalent in the discourse on RAG evaluation. They measure the quality of retrieval and generation:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p30"><em>Context relevanc</em><em>e</em>—This dimension evaluates how relevant the retrieved information or context is to the user query. It calculates metrics such as the precision and recall with which context is retrieved from the knowledge base. </li> 
   <li class="readable-text" id="p31"><em>Answer faithfulness (also called groundedness</em><em>)</em>—This dimension evaluates whether the answer generated by the system is using the retrieved information. </li> 
   <li class="readable-text" id="p32"><em>Answer relevanc</em><em>e</em>—This dimension evaluates how relevant the answer generated by the system is to the original user query.</li> 
  </ul> 
  <div class="readable-text" id="p33"> 
   <p>We discuss how these scores are calculated in section 5.2</p> 
  </div> 
  <div class="readable-text" id="p34"> 
   <h3 class=" readable-text-h3"><span class="num-string">5.1.2</span> Required abilities</h3> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>The quality scores are important for measuring how well the retrieval and the generation components of the RAG system are performing. At an overall level, there are certain critical abilities that a RAG system should possess:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p36"><em>Noise robustnes</em><em>s</em>—It is impractical to assume that the information stored in the knowledge base for RAG systems is perfectly curated to answer the questions that can be potentially asked. It is very probable that a document is related to the user query but does not have any meaningful information to answer it. The ability of the RAG system to separate these noisy documents from the relevant ones is termed noise robustness.</li> 
   <li class="readable-text" id="p37"><em>Negative rejectio</em><em>n</em>—By nature, LLMs always generate text. There may be no information about the user query in the documents in the knowledge base. The ability of the RAG system not to give an answer when there is no relevant information is called negative rejection.</li> 
   <li class="readable-text" id="p38"><em>Information integratio</em><em>n</em>—To obtain a comprehensive answer to a user query, it is also very likely the information must be retrieved from multiple documents. This ability of the system to assimilate information from multiple documents is called information integration.</li> 
   <li class="readable-text" id="p39"><em>Counterfactual robustnes</em><em>s</em>—Sometimes the information in the knowledge base might itself be inaccurate. A high-quality RAG system should be able to address this problem and reject known inaccuracies in the retrieved information. This ability is known as counterfactual robustness.</li> 
  </ul> 
  <div class="readable-text" id="p40"> 
   <p>Noise robustness is an ability that the retrieval component should possess, and other abilities are largely related to the generation component. </p> 
  </div> 
  <div class="readable-text intended-text" id="p41"> 
   <p>Apart from these, <em>latency</em> is another often-mentioned capability. Although it is a non-functional requirement, it is quite critical in generative AI applications. Latency is the delay that happens between the user query and the response. You may have observed that LLMs themselves have considerable latency before the final response is generated. Add to it the task of retrieval and augmentation, and the latency is bound to increase. Therefore, it is important to monitor how much time your RAG system takes from user input to response.</p> 
  </div> 
  <div class="readable-text intended-text" id="p42"> 
   <p>Ethical considerations are also at the forefront of generative AI adoption. For some RAG applications, it is important to measure the degree of <em>bias</em> and <em>toxicity</em><strong> </strong>in the system responses. This is also influenced by the underlying data in the knowledge base. While it is not specific to RAG, it is important to evaluate the outputs for bias and toxicity.</p> 
  </div> 
  <div class="readable-text intended-text" id="p43"> 
   <p>Another aspect to check is the <em>robustness</em> of the system, that is, its ability to handle different types of queries. Some queries may be simple, while others may involve complex reasoning. Some queries may require comparing two pieces of information, while others may involve complex post-processing, like mathematical calculations. We will look at some types of queries when we discuss CRAG, a benchmark, in section 5.4.</p> 
  </div> 
  <div class="readable-text intended-text" id="p44"> 
   <p>Finally, it is important to mention that these are scores and abilities that approach RAG at the core technique level. RAG, after all, is a means to solving the end use case. Therefore, you may have to build a <em>use case-specific</em><strong> </strong>evaluation criteria for your RAG system. For example, a question-answering system may use an exact match (EM) or F1 score as a metric, and a summarization service may use ROUGE scores. Modern search engines using RAG may look at user interaction metrics, accuracy of source attribution, and similar.</p> 
  </div> 
  <div class="readable-text intended-text" id="p45"> 
   <p>This is the main idea behind evaluating RAG pipelines. The quality scores and the abilities that we discussed before need to be measured and benchmarked. There are two critical enablers of RAG evaluations: frameworks and benchmarks.</p> 
  </div> 
  <div class="readable-text intended-text" id="p46"> 
   <p><em>Frameworks</em> are tools designed to facilitate evaluation, offering automation of the evaluation process and data generation. They are used to streamline the evaluation process by providing a structured environment for testing different aspects of RAG systems. They are flexible and can be adapted to different datasets and metrics. We will discuss the popular evaluation frameworks in section 5.3.</p> 
  </div> 
  <div class="readable-text intended-text" id="p47"> 
   <p><em>Benchmarks</em> are standardized datasets and their evaluation metrics used to measure the performance of RAG systems. Benchmarks provide a common ground for comparing different RAG approaches. They ensure consistency across the evaluations by considering a fixed set of tasks and their evaluation criteria. For example, HotpotQA focuses on multi-hop reasoning and retrieval capabilities using metrics such as Exact Match and F1 scores.</p> 
  </div> 
  <div class="readable-text intended-text" id="p48"> 
   <p>Benchmarks are used to establish a baseline for performance and identify strengths/weaknesses in specific tasks or domains. We will discuss a few benchmarks and their characteristics in section 5.4</p> 
  </div> 
  <div class="readable-text intended-text" id="p49"> 
   <p>Developers can use frameworks to integrate evaluation in their development process and use benchmarks to compare their development with established standards. The frameworks and benchmarks both calculate <em>metrics</em> that focus on retrieval and the RAG quality scores. We will begin our discussion about the metrics in the next section before moving on to the popular benchmarks and frameworks.</p> 
  </div> 
  <div class="readable-text" id="p50"> 
   <h2 class=" readable-text-h2"><span class="num-string">5.2</span> Evaluation metrics</h2> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>Metrics quantify the assessment of the RAG system performance. We will classify the evaluation metrics into two broad groups: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p52">Retrieval metrics that are commonly used in information retrieval tasks</li> 
   <li class="readable-text" id="p53">RAG-specific metrics that have evolved as RAG has found more application</li> 
  </ul> 
  <div class="readable-text" id="p54"> 
   <p>It is noteworthy that there are natural-language-generation-specific metrics such as BLEU, ROUGE, and METEOR that focus on fluency and measure relevance and semantic similarity. They play an important role in analyzing and benchmarking the performance of LLMs. This book discusses metrics specific to retrieval and RAG.</p> 
  </div> 
  <div class="readable-text" id="p55"> 
   <h3 class=" readable-text-h3"><span class="num-string">5.2.1</span> Retrieval metrics</h3> 
  </div> 
  <div class="readable-text" id="p56"> 
   <p>The retrieval component of RAG can be evaluated independently to determine how well the retrievers are satisfying the user query. The primary retrieval evaluation metrics include accuracy, precision, recall, F1-score, mean reciprocal rank (MRR), mean average precision (MAP), and normalized discounted cumulative gain (nDCG).</p> 
  </div> 
  <div class="readable-text" id="p57"> 
   <h4 class=" readable-text-h4">Accuracy </h4> 
  </div> 
  <div class="readable-text" id="p58"> 
   <p>Accuracy is typically defined as the proportion of correct predictions (both true positives and true negatives) among the total number of cases examined. In the context of information retrieval, it could be interpreted as</p> 
  </div> 
  <div class="browsable-container figure-container " id="p59"> 
   <img src="../Images/kimothi-ch5-eqs-0x.png" alt="A diagram of a machine

Description automatically generated" style="width: 100%; max-width: max-content;" /> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>Although accuracy is a simple, intuitive metric, it is not the primary metric for retrieval. In a large knowledge base, a majority of documents are usually irrelevant to any given query, which can lead to misleadingly high accuracy scores. It does not consider the ranking of the retrieved results.</p> 
  </div> 
  <div class="readable-text" id="p61"> 
   <h4 class=" readable-text-h4">Precision</h4> 
  </div> 
  <div class="readable-text" id="p62"> 
   <p>Precision focuses on the quality of the retrieved results. It measures the proportion of retrieved documents relevant to the user query. It answers the question, “Of all the documents that were retrieved, how many were relevant?”</p> 
  </div> 
  <div class="browsable-container figure-container " id="p63"> 
   <img src="../Images/kimothi-ch5-eqs-1x.png" alt="A diagram of a machine

Description automatically generated" style="width: 100%; max-width: max-content;" /> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>A higher precision means that the retriever is performing well and retrieving mostly relevant documents.</p> 
  </div> 
  <div class="readable-text" id="p65"> 
   <h4 class=" readable-text-h4">Precision@k</h4> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>Precision@k is a variation of precision that measures the proportion of relevant documents among the top ‘k’ retrieved results. It is particularly important because it focuses on the top results rather than all the retrieved documents. For RAG, it is important because only the top results are most likely to be used for augmentation. For example, if you restrict your RAG system to use only the top five retrieved documents for context augmentation, Precision@5 will be the metric to calculate:</p> 
  </div> 
  <div class="browsable-container figure-container " id="p67"> 
   <img src="../Images/kimothi-ch5-eqs-2x.png" alt="A diagram of a machine

Description automatically generated" style="width: 100%; max-width: max-content;" /> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>where ‘k’ is a chosen cut-off point. A precision@5 of .8 means that out of the top five retrieved documents, four were relevant.</p> 
  </div> 
  <div class="readable-text intended-text" id="p69"> 
   <p>Precision@k is also useful to compare systems when the total number of results retrieved may be different in different systems. However, the limitation is that the choice of ‘k’ can be arbitrary, and this metric doesn’t look beyond the chosen ‘k’.</p> 
  </div> 
  <div class="readable-text" id="p70"> 
   <h4 class=" readable-text-h4">Recall</h4> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>Recall focuses on the coverage that the retriever provides. It measures the proportion of the relevant documents retrieved from all the relevant documents in the corpus. It answers the question, “Of all the relevant documents, how many were retrieved?”</p> 
  </div> 
  <div class="browsable-container figure-container " id="p72"> 
   <img src="../Images/kimothi-ch5-eqs-3x.png" alt="A diagram of a machine

Description automatically generated" style="width: 100%; max-width: max-content;" /> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>Note that, unlike precision, calculation of recall requires prior knowledge of the total number of relevant documents. This requirement can become challenging in large-scale systems, which have many documents in the knowledge base.</p> 
  </div> 
  <div class="readable-text intended-text" id="p74"> 
   <p>Like precision, recall also doesn’t consider the ranking of the retrieved documents. It can also be misleading as retrieving all documents in the knowledge base will result in a perfect recall value. Figure 5.1 visualizes various precision and recall scenarios.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p75">  
   <img src="../Images/CH05_F01_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 5.1</span><span class=""> </span><span class="">Precision and recall</span></h5>
  </div> 
  <div class="readable-text" id="p76"> 
   <h4 class=" readable-text-h4">F1-score</h4> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>F1-score is the harmonic mean of precision and recall. It provides a single metric that balances both the quality and coverage of the retriever: </p> 
  </div> 
  <div class="browsable-container figure-container " id="p78"> 
   <img src="../Images/kimothi-ch5-eqs-4x.png" alt="A diagram of a machine

Description automatically generated" style="width: 100%; max-width: max-content;" /> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>The equation is such that the F1-score penalizes either variable having a low score; a high F1 score is only possible when both recall and precision values are high. This means that the score cannot be positively skewed by a single variable. Figure 5.2 illustrates how the F1-score balances precision and recall.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p80">  
   <img src="../Images/CH05_F02_Kimothi.png" alt="A diagram of a graph

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 5.2</span><span class=""> </span><span class="">F1-score balances precision and recall. A medium value of both precision and recall gets a higher F1-score than if one value is very high and the other is very low.</span></h5>
  </div> 
  <div class="readable-text" id="p81"> 
   <p>F1-score provides a single, balanced measure that can be used to easily compare different systems. However, it does not take ranking into account and gives equal weight to precision and recall, which might not always be ideal.</p> 
  </div> 
  <div class="readable-text" id="p82"> 
   <h4 class=" readable-text-h4">Mean reciprocal rank</h4> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>Mean reciprocal rank, or MRR, is particularly useful in evaluating the rank of the relevant document. It measures the reciprocal of the ranks of the first relevant document in the list of results. MRR is calculated over a set of queries:</p> 
  </div> 
  <div class="browsable-container figure-container " id="p84"> 
   <img src="../Images/kimothi-ch5-eqs-5x.png" alt="A diagram of a machine

Description automatically generated" style="width: 100%; max-width: max-content;" /> 
  </div> 
  <div class="readable-text" id="p85"> 
   <p>where N is the total number of queries, and rank<span class="_Superscript CharOverride-5">i</span><span class="_Superscript _idGenCharOverride-1"> </span>is the rank of the first relevant document of the i-th query.</p> 
  </div> 
  <div class="readable-text intended-text" id="p86"> 
   <p>MRR is particularly useful when you’re interested in how quickly the system can find a relevant document and consider the ranking of the results. However, since it doesn’t look at anything beyond the first relevant result, it may not be useful when multiple relevant results are important. Figure 5.3 shows how the mean reciprocal rank is calculated.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p87">  
   <img src="../Images/CH05_F03_Kimothi.png" alt="A paper with a number of results

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 5.3</span><span class=""> </span><span class="">MRR considers the ranking but doesn’t consider all the documents.</span></h5>
  </div> 
  <div class="readable-text" id="p88"> 
   <h4 class=" readable-text-h4">Mean average precision</h4> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>Mean average precision, or MAP, is a metric that combines precision and recall at different cut-off levels of ‘k’, that is, the cut-off number for the top results. It calculates a measure called average precision and then averages it across all queries:</p> 
  </div> 
  <div class="browsable-container figure-container " id="p90"> 
   <img src="../Images/kimothi-ch5-eqs-6x.png" alt="A diagram of a machine

Description automatically generated" style="width: 100%; max-width: max-content;" /> 
  </div> 
  <div class="readable-text" id="p91"> 
   <p>where R<span class="_Superscript _idGenCharOverride-1">i</span> is the number of relevant documents for query i, Precision@k is the precision at cut-off ‘k’, and rel@k is a binary flag indicating the relevance of the document at rank k.</p> 
  </div> 
  <div class="readable-text intended-text" id="p92"> 
   <p>Mean average precision is the mean of the average precision over all the N queries:</p> 
  </div> 
  <div class="browsable-container figure-container " id="p93"> 
   <img src="../Images/kimothi-ch5-eqs-7x.png" alt="A diagram of a machine

Description automatically generated" style="width: 100%; max-width: max-content;" /> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>MAP provides a single measure of quality across recall levels. It is quite suitable when result ranking is important but complex to calculate. Let’s look at an example MAP calculation in figure 5.4.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p95">  
   <img src="../Images/CH05_F04_Kimothi.png" alt="A screenshot of a computer

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 5.4</span><span class=""> </span><span class="">MAP considers all the retrieved documents and gives a higher score for better ranking</span></h5>
  </div> 
  <div class="readable-text" id="p96"> 
   <h4 class=" readable-text-h4">Normalized discounted cumulative gain</h4> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>Normalized discounted cumulative gain (nDCG) evaluates the ranking quality by considering the position of relevant documents in the result list and assigning higher scores to relevant documents appearing earlier. It is particularly effective for scenarios where documents have varying degrees of relevance. To calculate discounted cumulative gain (DCG), each document in the retrieved list is assigned a relevance score, rel, and a discount factor reduces the weight of documents as their rank position increases: </p> 
  </div> 
  <div class="browsable-container figure-container " id="p98"> 
   <img src="../Images/kimothi-ch5-eqs-8x.png" alt="A diagram of a machine

Description automatically generated" style="width: 100%; max-width: max-content;" /> 
  </div> 
  <div class="readable-text" id="p99"> 
   <p>where rel<span class="_Superscript _idGenCharOverride-1">i</span> is the graded relevance of the document at position I, and IDCG is the ideal DCG, which is the DCG for perfect ranking. </p> 
  </div> 
  <div class="readable-text intended-text" id="p100"> 
   <p>nDCG is calculated as the ratio between actual DCG and the IDCG:</p> 
  </div> 
  <div class="browsable-container figure-container " id="p101"> 
   <img src="../Images/kimothi-ch5-eqs-9x.png" alt="A diagram of a machine

Description automatically generated" style="width: 100%; max-width: max-content;" /> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>Figure 5.5 shows an example of nDCG calculation.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p103">  
   <img src="../Images/CH05_F05_Kimothi.png" alt="A paper with numbers and letters

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 5.5</span><span class=""> </span><span class="">nDCG addresses degrees of relevance in documents and penalizes incorrect ranking.</span></h5>
  </div> 
  <div class="readable-text" id="p104"> 
   <p>nDCG is a complex metric to calculate. It requires documents to have a relevance score, which may lead to subjectivity, and the choice of the discount factor affects the values significantly, but it accounts for varying degrees of relevance in documents and gives more weight to higher-ranked items.</p> 
  </div> 
  <div class="readable-text intended-text" id="p105"> 
   <p>Retrieval systems are not just used in RAG but also in a variety of other application areas such as web and enterprise search engines, e-commerce product search and personalized recommendations, social media ad retrieval, archival systems, databases, virtual assistants, and more. The retrieval metrics help in assessing and improving the performance to effectively meet user needs. Table 5.1 summarizes different retrieval metrics. </p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p106"> 
   <h5 class=" browsable-container-h5">Table 5.1 Retrieval metrics</h5> 
   <table id="table001" class="No-Table-Style _idGenTablePara-1"> 
    <colgroup> 
     <col class="_idGenTableRowColumn-1" /> 
     <col class="_idGenTableRowColumn-1" /> 
     <col class="_idGenTableRowColumn-1" /> 
     <col class="_idGenTableRowColumn-1" /> 
     <col class="_idGenTableRowColumn-1" /> 
    </colgroup> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-2"> 
      <td class="No-Table-Style CellOverride-1"> <p class="_TableHead">Metric</p> </td> 
      <td class="No-Table-Style CellOverride-2"> <p class="_TableHead">What it measures</p> </td> 
      <td class="No-Table-Style CellOverride-2"> <p class="_TableHead">Strengths</p> </td> 
      <td class="No-Table-Style CellOverride-2"> <p class="_TableHead">Use cases</p> </td> 
      <td class="No-Table-Style CellOverride-3"> <p class="_TableHead">Considerations</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <td class="No-Table-Style CellOverride-4"> <p class="_TableBody">Accuracy</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">Overall correctness of retrieval</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">Simple to understand; includes true negatives</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">General performance in balanced datasets</p> </td> 
      <td class="No-Table-Style CellOverride-6"> <p class="_TableBody">Can be misleading in imbalanced datasets; doesn’t consider ranking</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Precision</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Quality of retrieved results</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Easy to understand and calculate</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">General retrieval quality assessment</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Doesn’t consider ranking or completeness of retrieval</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-5"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Precision@k</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Quality of top k retrieved results</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Focuses on most relevant results for RAG</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">When only top k results are used for augmentation</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Choose k based on your RAG system’s usage</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Recall</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Coverage of relevant documents</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Measures completeness of retrieval</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Assessing if important information is missed</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Requires knowing all relevant documents in the corpus</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">F1-score</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Balance between precision and recall</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Single metric combining quality and coverage</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Overall retrieval performance</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">May obscure tradeoffs between precision and recall</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Mean reciprocal rank (MRR)</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">How quickly a relevant document is found</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Emphasizes finding at least one relevant result quickly</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">When finding one good result is sufficient</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Less useful when multiple relevant results are needed</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Mean average precision (MAP)</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Precision at different recall levels</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Considers both precision and ranking</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Comprehensive evaluation of ranked retrieval results</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">More complex to calculate and interpret</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <td class="No-Table-Style CellOverride-10"> <p class="_TableBody">Normalized discounted cumulative gain (nDCG)</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Ranking quality with graded relevance</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Accounts for varying degrees of relevance and ranking</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">When documents have different levels of relevance</p> </td> 
      <td class="No-Table-Style CellOverride-12"> <p class="_TableBody">Requires relevance scoring for documents</p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p107"> 
   <p>Not all retrieval metrics are popular for evaluation. Often, the more complex metrics are overlooked for the sake of explainability. The usage of these metrics depends on the stage of improvement in the evolution of system performance you are in. For example, to start with, you may just be trying to improve precision, while at an evolved stage, you may be looking for better ranking. </p> 
  </div> 
  <div class="readable-text intended-text" id="p108"> 
   <p>While these metrics focus on retrieval in general, some metrics have been created specifically for RAG applications. These metrics focus on the three quality scores discussed in section 5.1. </p> 
  </div> 
  <div class="readable-text" id="p109"> 
   <h3 class=" readable-text-h3"><span class="num-string">5.2.2</span> RAG-specific metrics</h3> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>The three quality scores used to evaluate RAG applications are context relevance, answer relevance, and answer faithfulness. These scores specifically answer the following three questions: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p111">Is the information retrieval relevant to the user query?</li> 
   <li class="readable-text" id="p112">Is the generated answer rooted in the retrieved information? </li> 
   <li class="readable-text" id="p113">Is the generated answer relevant to the user query?</li> 
  </ul> 
  <div class="readable-text" id="p114"> 
   <p>Let’s now take a look at each of these scores. </p> 
  </div> 
  <div class="readable-text" id="p115"> 
   <h4 class=" readable-text-h4">Context relevance</h4> 
  </div> 
  <div class="readable-text" id="p116"> 
   <p>Context relevance evaluates how well the retrieved documents relate to the original query. The key aspects are topical alignment, information usefulness, and redundancy. There are human evaluation methods, as well as semantic similarity measures to calculate context relevance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p117"> 
   <p>One such measure is employed by the Retrieval-Augmented Generation Assessment (RAGAs) framework (further discussed in section 5.3). The retrieved context should contain information only relevant to the query or the prompt. For context relevance, a metric S is estimated, where S is the number of sentences in the retrieved context relevant for responding to the query or the prompt:</p> 
  </div> 
  <div class="browsable-container figure-container " id="p118"> 
   <img src="../Images/kimothi-ch5-eqs-10x.png" alt="A diagram of a machine

Description automatically generated" style="width: 100%; max-width: max-content;" /> 
  </div> 
  <div class="readable-text" id="p119"> 
   <p>Figure 5.6 is an illustrative example of high and low context relevance.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p120">  
   <img src="../Images/CH05_F06_Kimothi.png" alt="A screenshot of a test

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 5.6</span><span class=""> </span><span class="">Context relevance evaluates the degree to which the retrieved information is relevant <br />to the query.</span></h5>
  </div> 
  <div class="readable-text" id="p121"> 
   <p>The number of relevant sentences is also sometimes customized to the sum of similarity scores of each of the sentences with the query. Context relevance ensures that the generation component has access to appropriate information.</p> 
  </div> 
  <div class="readable-text" id="p122"> 
   <h4 class=" readable-text-h4">Answer faithfulness</h4> 
  </div> 
  <div class="readable-text" id="p123"> 
   <p>Answer faithfulness is the measure of the extent to which the response is factually grounded in the retrieved context. Faithfulness ensures that the facts in the response do not contradict the context and can be traced back to the source. It also ensures that the LLM is not hallucinating. In the RAGAs framework, faithfulness first identifies the number of claims made in the response and calculates the proportion of those claims present in the context:</p> 
  </div> 
  <div class="browsable-container figure-container " id="p124"> 
   <img src="../Images/kimothi-ch5-eqs-11x.png" alt="A diagram of a machine

Description automatically generated" style="width: 100%; max-width: max-content;" /> 
  </div> 
  <div class="readable-text" id="p125"> 
   <p>Let’s look at an example in figure 5.7</p> 
  </div> 
  <div class="browsable-container figure-container " id="p126">  
   <img src="../Images/CH05_F07_Kimothi.png" alt="A white paper with text and numbers

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 5.7</span><span class=""> </span><span class="">Answer faithfulness evaluates the closeness of the generated response to the retrieved context.</span></h5>
  </div> 
  <div class="readable-text" id="p127"> 
   <p>Faithfulness is not a complete measure of factual accuracy but only evaluates the groundedness to the context. An inverse metric for faithfulness is also the <em>hallucination rate</em><strong>,</strong> which can calculate the proportion of generated claims in the response that are not present in the retrieved context.</p> 
  </div> 
  <div class="readable-text intended-text" id="p128"> 
   <p>Another related metric to faithfulness is <em>coverage</em>. Coverage measures the number of relevant claims in the context and calculates the proportion of relevant claims present in the generated response. It measures how much of the relevant information from the retrieved passages is included in the generated answer:</p> 
  </div> 
  <div class="browsable-container figure-container " id="p129"> 
   <img src="../Images/kimothi-ch5-eqs-12x.png" alt="A diagram of a machine

Description automatically generated" style="width: 100%; max-width: max-content;" /> 
  </div> 
  <div class="readable-text" id="p130"> 
   <h4 class=" readable-text-h4">Answer relevance</h4> 
  </div> 
  <div class="readable-text" id="p131"> 
   <p>Like context relevance measures the relevance of the retrieved context to the query, answer relevance is the measure of the extent to which the response is relevant to the query. This metric focuses on key aspects such as the system’s ability to comprehend the query, the response being pertinent to the query, and the completeness of the response. </p> 
  </div> 
  <div class="readable-text intended-text" id="p132"> 
   <p>In RAGAs, for this metric, a response is generated for the initial query or prompt. To compute the score, the LLM is then prompted to generate questions for the generated response several times. The mean cosine similarity between these questions and the original one is then calculated. The concept is that if the answer addresses the initial question correctly, the LLM should generate questions from it that match the original question:</p> 
  </div> 
  <div class="browsable-container figure-container " id="p133"> 
   <img src="../Images/kimothi-ch5-eqs-13x.png" alt="A diagram of a machine

Description automatically generated" style="width: 100%; max-width: max-content;" /> 
  </div> 
  <div class="readable-text" id="p134"> 
   <p>where N is the number of queries generated by the LLM. </p> 
  </div> 
  <div class="readable-text intended-text" id="p135"> 
   <p>Note that answer relevance is not a measure of truthfulness but only of relevance. The response may or may not be factually accurate, but it may be relevant. Figure 5.8 is an illustration of the answer relevance calculation. Can you find the reason why the relevance is not very high? (Hint: The answer may have some irrelevant facts.) Answer relevance ensures that the RAG system provides useful and appropriate responses, enhancing user satisfaction and the system’s practical utility.</p> 
  </div> 
  <div class="readable-text" id="p136"> 
   <h4 class=" readable-text-h4">Tradeoffs and other considerations</h4> 
  </div> 
  <div class="readable-text" id="p137"> 
   <p>These three metrics and their derivatives form the core of RAG quality evaluation. Furthermore, these metrics are interconnected and sometimes involve tradeoffs. High context relevance usually leads to better faithfulness, as the system has access to more pertinent information. However, high faithfulness doesn’t always guarantee high answer relevance. A system might faithfully reproduce information from the retrieved passages but fail to directly address the query. Optimizing for answer relevance without considering faithfulness might lead to responses that seem appropriate but contain hallucinated or incorrect information.</p> 
  </div> 
  <div class="readable-text intended-text" id="p138"> 
   <p>We have discussed quite a few metrics in this section. Effective interpretation of these metrics is crucial for performance improvement. As creators of RAG systems, you should use these metrics to compare with similar systems. You can also look at consistent trends to identify the strengths and weaknesses of your system. A low-precision high-recall system may indicate that your system is retrieving a lot of documents, and you may need to make your retriever more selective. A low-precision low-recall system points out fundamental problems with retrieval, and you may need to reassess the indexing pipeline itself. The same problem may be indicated by a low MAP or a low context-relevance score. Similarly, a low MRR or a low nDCG value may indicate a problem with the ranking algorithm of the retriever. To address low-answer faithfulness or low-answer relevance, you may need to improve your prompts or fine-tune the LLM. </p> 
  </div> 
  <div class="browsable-container figure-container " id="p139">  
   <img src="../Images/CH05_F08_Kimothi.png" alt="A screenshot of a diagram

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 5.8</span><span class=""> </span><span class="">Answer relevance is calculated as the mean of cosine similarity between the original and synthetic questions.</span></h5>
  </div> 
  <div class="readable-text intended-text" id="p140"> 
   <p>There may also exist some tradeoffs that you will need to balance. Improving precision often reduces recall and vice-versa. Highly relevant but brief contexts may lead to incomplete answers, and high answer faithfulness may sometimes come at the cost of answer relevance. </p> 
  </div> 
  <div class="readable-text intended-text" id="p141"> 
   <p>The relative importance of each metric will depend on your use case and user requirements. You may need to include other metrics specific to your downstream use case, such as summarization to measure conciseness, and chatbots to emphasize conversation coherence.</p> 
  </div> 
  <div class="readable-text intended-text" id="p142"> 
   <p>Developers can code these metrics from scratch and integrate them in the development and deployment process of their RAG system. However, you’ll find evaluation frameworks that are readily available quite handy. We discuss three popular frameworks in the next section.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p143"> 
    <h5 class=" callout-container-h5 readable-text-h5">Human evaluations and ground truth data</h5> 
   </div> 
   <div class="readable-text" id="p144"> 
    <p>Most of the metrics we discussed talk about a concept of relevant documents. For example, precision is calculated as the number of relevant documents retrieved, divided by the total number of retrieved documents. The question that arises is, how does one establish that a document is relevant? </p> 
   </div> 
   <div class="readable-text" id="p145"> 
    <p>The simple answer is a human evaluation approach. A subject matter expert looks at the documents and determines the relevance. Human evaluation brings in subjectivity, and therefore, human evaluations are done by a panel of experts rather than an individual. But human evaluations are restrictive from a scale and a cost perspective.</p> 
   </div> 
   <div class="readable-text" id="p146"> 
    <p>Any data that can reliably establish relevance becomes extremely useful consequently. Ground truth is information known to be real or true. In RAG, and the generative AI domain in general, ground truth is a prepared set of prompt–context–response or question–context–response examples, akin to labeled data in supervised machine learning parlance. Ground truth data created for your knowledge base can be used for the evaluation of your RAG system. </p> 
   </div> 
   <div class="readable-text" id="p147"> 
    <p>How does one go about creating the ground truth data? It can be viewed as a one-time exercise where a group of experts creates this data. However, generating hundreds of QCA (question–context–answer) samples from documents manually can be a time-consuming and labor-intensive task. Additionally, if the knowledge base is dynamic, the ground truth data will also need updates. Questions created by humans may face challenges in achieving the necessary level of complexity for a comprehensive evaluation, potentially affecting the overall quality of the assessment. </p> 
   </div> 
   <div class="readable-text" id="p148"> 
    <p>LLMs can be used to address these challenges. Synthetic data generation uses LLMs to generate diverse questions and answers from the documents in the knowledge base. LLMs can be prompted to create questions such as simple questions, multi-context questions, conditional questions, reasoning questions, and similar using the documents from the knowledge base as context.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p149"> 
   <h2 class=" readable-text-h2"><span class="num-string">5.3</span> Frameworks</h2> 
  </div> 
  <div class="readable-text" id="p150"> 
   <p>Frameworks provide a structured approach to RAG evaluations. They can be used to automate the evaluation process. Some go beyond and assist in the synthetic ground truth data generation. While new evaluation frameworks continue to be introduced, there are two popular ones that we discuss here:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p151">RAGAs (Retrieval-Augmented Generation Assessment)</li> 
   <li class="readable-text" id="p152">ARES (Automated RAG Evaluation System)</li> 
  </ul> 
  <div class="readable-text" id="p153"> 
   <h3 class=" readable-text-h3"><span class="num-string">5.3.1</span> RAGAs</h3> 
  </div> 
  <div class="readable-text" id="p154"> 
   <p>Retrieval-Augmented Generation Assessment, or RAGAs, is a framework developed by Exploding Gradients that assesses the retrieval and generation components of RAG systems without relying on extensive human annotations. RAGAs</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p155">Synthetically generate a test dataset that can be used to evaluate a RAG pipeline.</li> 
   <li class="readable-text" id="p156">Use metrics to measure the performance of the pipeline.</li> 
   <li class="readable-text" id="p157">Monitor the quality of the application in production.</li> 
  </ul> 
  <div class="readable-text" id="p158"> 
   <p>We will continue with our example of the Wikipedia page of the 2023 Cricket World Cup, but we first create a synthetic test dataset using RAGAs and then use the RAGAs metrics to evaluate the performance of the RAG pipeline we created in chapters 3 and 4.</p> 
  </div> 
  <div class="readable-text" id="p159"> 
   <h4 class=" readable-text-h4">Synthetic test dataset generation (ground truths)</h4> 
  </div> 
  <div class="readable-text" id="p160"> 
   <p>Section 5.2 pointed out that ground truths data is necessary to calculate evaluation metrics for assessing the quality of RAG pipelines. While this data can be manually curated, RAGAs provides the functionality of generating this dataset from the documents in the knowledge base. </p> 
  </div> 
  <div class="readable-text intended-text" id="p161"> 
   <p>RAGAs does this using an LLM. It analyses the documents in the knowledge base and uses an LLM to generate seed questions from chunks in the knowledge base. These questions are based on the document chunks from the knowledge base. These chunks act as the context for the questions. Another LLM is used to generate the answer to these questions. This is how it generates a question–context–answer data based on the documents in the knowledge base. RAGAs also has an evolver module that creates more difficult questions (e.g., multi-context, reasoning, and conditional) for a more comprehensive evaluation. Figure 5.9 illustrates the process of synthetic data generation using RAGAs.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p162">  
   <img src="../Images/CH05_F09_Kimothi.png" alt="A diagram of questions and arrows

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 5.9</span><span class=""> </span><span class="">Synthetic ground truths data generation using RAGAs</span></h5>
  </div> 
  <div class="readable-text" id="p163"> 
   <p>To evaluate our RAG pipeline, let’s recreate the documents from the Wikipedia page like we did in chapter 3. Note that we will have to install the packages used in the previous chapters to continue with the following code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p164"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong>#Importing the AsyncHtmlLoader</strong>
from langchain_community.document_loaders import AsyncHtmlLoader

<strong>#This is the URL of the Wikipedia page on the 2023 Cricket World Cup</strong>
url=&quot;https://en.wikipedia.org/wiki/2023_Cricket_World_Cup&quot;

<strong>#Instantiating the AsyncHtmlLoader</strong>
loader = AsyncHtmlLoader (url)

<strong>#Loading the extracted information</strong>
html_data = loader.load()

from langchain_community.document_transformers import Html2TextTransformer

<strong>#Instantiate the Html2TextTransformer function</strong>
html2text = Html2TextTransformer()


<strong>#Call transform_documents</strong>
html_data_transformed = html2text.transform_documents(html_data)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p165"> 
   <p>The <code>html_data_transformed</code> contains the necessary document format of the Wikipedia page. We will use RAGAs library to generate the dataset from these documents. For that, we will first need to install the RAGAs library:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p166"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">%pip install ragas== 0.2.13

<strong># Import necessary libraries</strong>
<strong>from ragas.llms import LangchainLLMWrapper</strong>
<strong>from ragas.embeddings import LangchainEmbeddingsWrapper</strong>
from ragas.testset import TestsetGenerator
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

<strong># Instantiate the models</strong>
generator_llm = 
LangchainLLMWrapper(
ChatOpenAI(model=&quot;gpt-4o-mini&quot;)
)

generator_embeddings = 
LangchainEmbeddingsWrapper(
OpenAIEmbeddings(model=&quot;text-embedding-3-small&quot;)
)

<strong># Create the TestsetGenerator</strong>
generator = 
TestsetGenerator(
llm=generator_llm, 
embedding_model=generator_embeddings
)

<strong># Call the generator</strong>
testset = 
generator.generate_with_langchain_docs
(
          	html_data_transformed, 
test_size=20, 
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p167"> 
   <p>The <code>testset</code> that we created contains 20 questions based on our document, along with the chunk of the document that the question was based on, and the ground truth answer. A screenshot of the dataset is shown in figure 5.10.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p168">  
   <img src="../Images/CH05_F10_Kimothi.png" alt="A close-up of a table

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 5.10</span><span class=""> </span><span class="">Synthetic test data generated using RAGAs</span></h5>
  </div> 
  <div class="readable-text" id="p169"> 
   <p>We will use this dataset to evaluate our RAG pipeline.</p> 
  </div> 
  <div class="readable-text" id="p170"> 
   <h4 class=" readable-text-h4">Recreating the RAG pipeline</h4> 
  </div> 
  <div class="readable-text" id="p171"> 
   <p>From the created test dataset, we use the <code>question</code> and the <code>ground_truth</code> information. We pass the questions to our RAG pipeline and generate answers. We compare these answers with the <code>ground_truth</code> to calculate the evaluation metrics. First, we recreate our RAG pipeline. Again, it is important to note that we will have to install the packages we used in the previous chapters to continue with the code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p172"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong># Import FAISS class from vectorstore library</strong>
from langchain_community.vectorstores import FAISS
<strong># Import OpenAIEmbeddings &amp; ChatOpenAI from the library</strong>
from langchain_openai import OpenAIEmbeddings, ChatOpenAI

def rag_function(query, db_path, index_name):    

<strong># Instantiate the embeddings object</strong>

embeddings=OpenAIEmbeddings(model=&quot;text-embedding-3-small&quot;)

<strong># Load the database stored in the local directory</strong>

db=FAISS.load_local(
folder_path=db_path, 
index_name=index_name, 
embeddings=embeddings,
allow_dangerous_deserialization=True
)

<strong># Ranking the chunks in descending order of similarity and selecting the top 2 queries</strong>

retrieved_docs = db.similarity_search(query, k=2)

<strong># Keeping text of top 2 retrieved chunks</strong>

retrieved_context=[ retrieved_docs[0].page_content 
+retrieved_docs[1].page_content]

<strong># Creating the prompt</strong>

augmented_prompt=f&quot;&quot;&quot;

Given the context below, answer the question.

Question: {query} 

Context : {retrieved_context}

Remember to answer only based on the context 
provided and not from any other source. 

If the question cannot be answered based 
on the provided context, say I don't know.

&quot;&quot;&quot;

<strong># Instantiate the LLM</strong>
<strong>llm = ChatOpenAI(</strong>
<strong>model=</strong><strong>&quot;</strong><strong>gpt-4o-mini</strong><strong>&quot;</strong><strong>,</strong>
<strong>temperature=0,</strong>
<strong>max_tokens=None,</strong>
<strong>timeout=None,</strong>
<strong>max_retries=2</strong>
<strong>)</strong>

<strong># Create message to send to the LLM</strong>

<strong>messages=[(</strong><strong>&quot;</strong><strong>human</strong><strong>&quot;</strong><strong>,augmented_prompt)]</strong>


<strong># Make the API call passing the message to the LLM</strong>

response = llm.invoke(messages)

<strong># Extract the answer from the response object</strong>

answer=response.content

return retrieved_context, answer</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p173"> 
   <p>We can try this pipeline to generate answers.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p174"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong># Location of the stored vector index created by the indexing pipeline</strong>
db_path='../../Assets/Data'

<strong># User Question</strong>
query=&quot;Who won the 2023 cricket world cup?&quot;

<strong># Index Name</strong>
index_name=&quot;CWC_index&quot;

<strong># Calling the RAG function</strong>
rag_function(query, db_path, index_name)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p175"> 
   <p>Now that we have the RAG pipeline function, we can evaluate this pipeline using the questions that have been synthetically generated.</p> 
  </div> 
  <div class="readable-text" id="p176"> 
   <h4 class=" readable-text-h4">Evaluations</h4> 
  </div> 
  <div class="readable-text" id="p177"> 
   <p>We first generate answers to the questions in the synthetic test data using our RAG pipeline. We then compare the answers to the ground truth answers. We first generate the answers: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p178"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong># Create Lists for Questions and Ground Truths from testset</strong>
sample_queries = 
dataset.to_pandas()['user_input'].to_list()

expected_responses=
dataset.to_pandas()['reference'].to_list()


<strong># Iterate through the testset to generate responses to questions</strong>

<strong>dataset_to_eval=[]</strong>

for query, reference in zip(sample_queries,expected_responses):

<strong># Call the RAG function</strong>
rag_context, rag_answer=rag_function(query,db_path,index_name)

<strong># Create a dictionary of question, answer, context, and ground truth</strong>
dataset_to_eval.append(
            {
                    &quot;user_input&quot;:query,
                    &quot;retrieved_contexts&quot;:relevant_docs,
                    &quot;response&quot;:response,
                    &quot;reference&quot;:reference
            }
                ) </pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p179"> 
   <p>For RAGAs, the evaluation set needs to be in the <code>Dataset</code> format:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p180"> 
   <div class="code-area-container"> 
    <pre class="code-area"><strong># Import the EvaluationDataset library</strong>
from ragas import EvaluationDataset

evaluation_dataset = EvaluationDataset.from_list(dataset_to_eval)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p181"> 
   <p>Now that we have the complete evaluation dataset, we can invoke the metrics:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p182"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong>#Import all the libraries</strong>
from ragas import evaluate

from ragas.metrics import (
        LLMContextRecall, 
Faithfulness, 
FactualCorrectness, 
AnswerCorrectness, 
ResponseRelevancy)


<strong>#Set the judge LLM for evaluation</strong>

evaluator_llm = 
LangchainLLMWrapper(
ChatOpenAI(model=&quot;gpt-4o-mini&quot;)
)

<strong># Calculate the metrics for the dataset </strong>

result = evaluate(
dataset=evaluation_dataset,
metrics=[
LLMContextRecall(), 
Faithfulness(), 
AnswerCorrectness(), 
ResponseRelevancy(),
FactualCorrectness()],
llm=evaluator_llm)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p183"> 
   <p>You can also check the official documentation of RAGAs for more information (<a href="https://docs.ragas.io/en/stable/"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/docs.ragas.io/en/stable/</span></a>). RAGAs calculates a bunch of metrics that are useful for assessing the quality of the RAG pipeline. RAGAs uses an LLM to do this, somewhat subjective, task. For example, to calculate faithfulness for a given question–context–answer record, RAGAs first breaks down the answer into simple statements. Then, for each statement, it asks the LLM whether the statement can be inferred from the context. The LLM provides a 0 or 1 response along with a reason. This process is repeated a couple of times. Finally, faithfulness is calculated as the proportion of statements judged by the LLM as faithful (i.e., 1). Several other metrics are calculated using this LLM-based approach. This approach, where an LLM is used in evaluating a task, is also popularly called <em>LLM as a judge</em> approach. An important point to note here is that the accuracy of this evaluation is also dependent on the quality of the LLM being used as the judge.</p> 
  </div> 
  <div class="readable-text" id="p184"> 
   <h3 class=" readable-text-h3"><span class="num-string">5.3.2</span> Automated RAG evaluation system</h3> 
  </div> 
  <div class="readable-text" id="p185"> 
   <p>Automated RAG evaluation system, or ARES, is a framework developed by researchers at Stanford University and Databricks. Like RAGAs, ARES uses an LLM as a judge approach for evaluations. Both request a language model to classify answer relevance, context relevance, and faithfulness for a given query. However, there are some differences:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p186">RAGAs relies on heuristically written prompts sent to the LLM for evaluation. ARES, in contrast, trains a classifier using a language model.</li> 
   <li class="readable-text" id="p187">RAGAs aggregates the responses from the LLM to arrive at a score. ARES provides confidence intervals for the scores using a framework called Prediction-Powered Inference (PPI).</li> 
   <li class="readable-text" id="p188">RAGAs generates a simple synthetic question–context–answer dataset for evaluation from the documents. ARES generate synthetic datasets comprising both positive and negative examples of query–passage–answer triples. </li> 
  </ul> 
  <div class="readable-text" id="p189"> 
   <p>ARES requires more data than RAGAs. To use ARES, you need the following three datasets:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p190"><em>In-domain passage se</em><em>t</em>—This is a collection of passages relevant to the specific domain being evaluated. The passages should be suitable for generating queries and answers. In our case, it will be the documents that we created from the Wikipedia article. </li> 
   <li class="readable-text" id="p191"><em>Human preference validation se</em><em>t</em>—A minimum of approximately 150 annotated data points is required. This set is used to validate the preferences of human annotators regarding the relevance of the generated query-passage–answer triples.</li> 
   <li class="readable-text" id="p192"><em>Few-shot example</em><em>s</em>—At least five examples of in-domain queries and answers are needed. These examples help prompt the LLMs during the synthetic data generation process.</li> 
  </ul> 
  <div class="readable-text" id="p193"> 
   <p>The need for a human-preference validation set and fine-tuning of language models for classification makes applying ARES more complex. The application of ARES is out of the scope of this book. However, ARES is a robust framework. It provides a detailed analysis of system performance with statistical confidence intervals, making it suitable for in-depth RAG system evaluations. RAGAs promises a faster evaluation cycle without reliance on human annotations. More details on the ARES application can be found in the official GitHub repository (<a href="https://github.com/stanford-futuredata/ARES"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/github.com/stanford-futuredata/ARES</span></a>).</p> 
  </div> 
  <div class="readable-text intended-text" id="p194"> 
   <p>While RAGAs and ARES have gained popularity, there are other frameworks, such as TruLens, DeepEval, and RAGChecker, that have also gotten acceptance amongst RAG developers. </p> 
  </div> 
  <div class="readable-text intended-text" id="p195"> 
   <p>Frameworks provide a standardized method of automating the evaluation of your RAG pipelines. Your choice of the evaluation framework should depend on your use case requirements. For quick and easy evaluations that are widely understood, RAGAs may be your choice. For robustness across diverse domains and question types, ARES might suit better. Most of the proprietary service providers (vector DBs, LLMs, etc.) have their evaluation features you may use. You can also develop your metrics.</p> 
  </div> 
  <div class="readable-text intended-text" id="p196"> 
   <p>Next, we look at benchmarks. Benchmarks are used to compare competing RAG systems with one another. </p> 
  </div> 
  <div class="readable-text" id="p197"> 
   <h2 class=" readable-text-h2"><span class="num-string">5.4</span> Benchmarks</h2> 
  </div> 
  <div class="readable-text" id="p198"> 
   <p>Benchmarks provide a standard point of reference to evaluate the quality and performance of a system. RAG benchmarks are a set of standardized tasks, and a dataset used to compare the efficiency of different RAG systems in retrieving relevant information and generating accurate responses. There has been a surge in creating benchmarks since 2023, when RAG started gaining popularity, but there have been benchmarks on question-answering tasks that were introduced before that. Benchmarks such as Stanford Question Answering Dataset (SQuAD), WikiQA, Natural Question (NQ), and HotpotQA are open domain question-answering datasets that primarily evaluate the retriever component using metrics such as Exact Match (EM) and F1-score. BEIR or benchmarking information retrieval is a comprehensive, heterogeneous benchmark based on 9 IR tasks and 19 question–answer datasets. This section discusses three of the popular RAG-specific benchmarks and their evaluation. </p> 
  </div> 
  <div class="readable-text" id="p199"> 
   <h3 class=" readable-text-h3"><span class="num-string">5.4.1</span> RGB</h3> 
  </div> 
  <div class="readable-text" id="p200"> 
   <p>Retrieval-augmented generation benchmark (RGB) was introduced in a December 2023 paper (<a href="https://arxiv.org/pdf/2309.01431"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/pdf/2309.01431</span></a>). It comprises 600 base questions and 400 additional questions, evenly split between English and Chinese. The corpus was constructed using a multistep process that involved collecting recent news articles, generating questions and answers using ChatGPT, retrieving relevant web pages through Google’s API, and selecting the most pertinent text chunks using a dense retrieval model. It is a benchmark that focuses on four key abilities of a RAG system: noise robustness, negative rejection, information integration, and counterfactual robustness, as illustrated in figure 5.11.</p> 
  </div> 
  <div class="readable-text intended-text" id="p201"> 
   <p>RGB focuses on the following metrics for evaluation:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p202"><em>Accurac</em><em>y</em>—Used for noise robustness and information integration. It is based on the exact matching of the generated text with the correct answer.</li> 
   <li class="readable-text" id="p203"><em>Rejection rat</em><em>e</em>—Used for negative rejection. It is measured by exact matching of the model’s output with a specific rejection phrase. The rejection rate is also </li> 
  </ul> 
  <div class="browsable-container figure-container " id="p204">  
   <img src="../Images/CH05_F11_Kimothi.png" alt="A group of text boxes

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 5.11</span><span class=""> </span><span class="">Four abilities required of RAG systems. Source: Benchmarking Large Language Models in Retrieval-Augmented Generation by Chen et al., </span><a href="https://arxiv.org/pdf/2309.0143"><span class=""><span class="Hyperlink CharOverride-3">https:</span></span><span class=""><span class="Hyperlink CharOverride-3">/</span></span><span class=""><span class="Hyperlink CharOverride-3">/arxiv.org/pdf/2309.0143</span></span></a><span class="">.</span> </h5>
  </div> 
  <ul> 
   <li class="readable-text" style="list-style: none;" id="p205">evaluated using ChatGPT to determine whether the responses contain rejection information.</li> 
  </ul> 
  <ul> 
   <li class="readable-text" id="p206"><em>Error detection rat</em><em>e</em>—Used for counterfactual robustness. It is measured by exact matching of the model’s output with a specific error-detection phrase and is also evaluated using ChatGPT.</li> 
   <li class="readable-text" id="p207"><em>Error correction rat</em><em>e</em>—Used for counterfactual robustness. It measures whether the model can provide the correct answer after identifying errors.</li> 
  </ul> 
  <div class="readable-text" id="p208"> 
   <p>You can use the GitHub repository to implement RGB (<a href="https://github.com/chen700564/RGB"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/github.com/chen700564/RGB</span></a>).</p> 
  </div> 
  <div class="readable-text" id="p209"> 
   <h4 class=" readable-text-h4">Multi-hop RAG</h4> 
  </div> 
  <div class="readable-text" id="p210"> 
   <p>Curated by researchers at HKUST, multi-hop RAG contains 2556 queries, with evidence for each query distributed across two to four documents. The queries also involve document metadata, reflecting complex scenarios commonly found in real-world RAG applications. It contains four types of queries:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p211"><em>Inferenc</em><em>e</em>—Synthesizing information across multiple sources (e.g., Which report discusses the supply chain risk of Apple—the 2019 annual report or the 2020 annual report?)</li> 
   <li class="readable-text" id="p212"><em>Compariso</em><em>n</em>—Comparing facts from different sources (e.g., Did Netflix or Google report higher revenue for the year 2023?)</li> 
   <li class="readable-text" id="p213"><em>Tempora</em><em>l</em>—Analyzing the temporal ordering of events (e.g., e.g. Did Apple introduce the AirTag tracking device before or after the launch of the 5th generation iPad Pro?)</li> 
   <li class="readable-text" id="p214"><em>Nul</em><em>l</em>—Queries not answerable from the knowledge base</li> 
  </ul> 
  <div class="readable-text" id="p215"> 
   <p>Full implementation code is available at <a href="https://github.com/yixuantt/MultiHop-RAG"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/github.com/yixuantt/MultiHop-RAG</span></a>.</p> 
  </div> 
  <div class="readable-text" id="p216"> 
   <h4 class=" readable-text-h4">CRAG</h4> 
  </div> 
  <div class="readable-text" id="p217"> 
   <p>Comprehensive RAG benchmark (CRAG), curated by Meta and HKUST, is a factual question-answering benchmark of 4,409 question–answer pairs and mock APIs to simulate web and knowledge graph (KG) search. It contains eight types (simple, conditions, comparison questions, aggregation questions, multi-hop questions, set queries, post-processing-heavy questions, and false-premise questions, as illustrated in figure 5.12) of queries across five domains (finance, sports, music, movie, and open domain). </p> 
  </div> 
  <div class="readable-text intended-text" id="p218"> 
   <p>For each question in the evaluation set, CRAG labels the answer with one of four classes:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p219"><em>Perfec</em><em>t</em>—The response correctly answers the user’s question and contains no hallucinated content (scored as +1).</li> 
   <li class="readable-text" id="p220"><em>Acceptabl</em><em>e</em>—The response provides a useful answer to the user’s question but may contain minor errors that do not harm the usefulness of the answer (scored as +0.5).</li> 
   <li class="readable-text" id="p221"><em>Missin</em><em>g</em>—The response is “I don’t know”, “I’m sorry I can’t find ...”, a system error such as an empty response, or a request from the system to clarify the original question (scored as 0).</li> 
   <li class="readable-text" id="p222"><em>Incorrec</em><em>t</em>—The response provides wrong or irrelevant information to answer the user’s question (scored as −1).</li> 
  </ul> 
  <div class="browsable-container figure-container " id="p223">  
   <img src="../Images/CH05_F12_Kimothi.png" alt="A table of questions with text

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 5.12</span><span class=""> </span><span class="">Eight question types in CRAG</span></h5>
  </div> 
  <div class="readable-text" id="p224"> 
   <p>For automatic evaluation, CRAG classifies an answer as perfect if it exactly matches the ground truth. If not, then it asks an LLM to do the classification. It uses two LLM evaluators. You can read more about CRAG at <a href="https://arxiv.org/pdf/2406.04744"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/pdf/2406.04744</span></a>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p225"> 
   <p>Other noteworthy benchmark datasets are MedRAG (<a href="https://github.com/Teddy-XiongGZ/MedRAG"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/github.com/Teddy-XiongGZ/MedRAG</span></a>), which focuses on Medical Information, CRUD-RAG (<a href="https://arxiv.org/pdf/2401.17043"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/pdf/2401.17043</span></a>), which focuses on the Chinese language, and FeB4RAG (<a href="https://arxiv.org/abs/2402.11891"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/abs/2402.11891</span></a>), which focuses on federated search. If you’re developing an LLM application that has accurate and contextual generation as its core proposition, you’ll be able to communicate the quality of your application by showing how it performs on different benchmarks. Table 5.2 compares the different benchmarks.</p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p226"> 
   <h5 class=" browsable-container-h5">Table 5.2 RAG benchmarks</h5> 
   <table id="table002" class="No-Table-Style TableOverride-1"> 
    <colgroup> 
     <col class="_idGenTableRowColumn-6" /> 
     <col class="_idGenTableRowColumn-1" /> 
     <col class="_idGenTableRowColumn-1" /> 
     <col class="_idGenTableRowColumn-1" /> 
     <col class="_idGenTableRowColumn-7" /> 
    </colgroup> 
    <thead> 
     <tr class="No-Table-Style _idGenTableRowColumn-8"> 
      <td class="No-Table-Style CellOverride-13" scope="col"> <p class="_TableHead">Benchmark</p> </td> 
      <td class="No-Table-Style CellOverride-14" scope="col"> <p class="_TableHead">Dataset</p> </td> 
      <td class="No-Table-Style CellOverride-14" scope="col"> <p class="_TableHead">Task</p> </td> 
      <td class="No-Table-Style CellOverride-14" scope="col"> <p class="_TableHead">Metrics</p> </td> 
      <td class="No-Table-Style CellOverride-15" scope="col"> <p class="_TableHead">Applicability</p> </td> 
     </tr> 
    </thead> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <td class="No-Table-Style CellOverride-16"> <p class="_TableBody">SQuAD</p> </td> 
      <td class="No-Table-Style CellOverride-17"> <p class="_TableBody">Stanford Question Answering Dataset</p> </td> 
      <td class="No-Table-Style CellOverride-17"> <p class="_TableBody">Open domain QA</p> </td> 
      <td class="No-Table-Style CellOverride-17"> <p class="_TableBody">Exact match (EM), F1-score</p> </td> 
      <td class="No-Table-Style CellOverride-18"> <p class="_TableBody">General QA tasks, model evaluation on comprehension accuracy</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-9"> 
      <td class="No-Table-Style CellOverride-19"> <p class="_TableBody">Natural questions</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Real Google search queries</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Open domain QA</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">F1-score</p> </td> 
      <td class="No-Table-Style CellOverride-21"> <p class="_TableBody">Real-world QA, information retrieval from large corpora</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style CellOverride-19"> <p class="_TableBody">HotpotQA</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Wikipedia-based QA</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Multi-hop QA</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">EM, F1-score</p> </td> 
      <td class="No-Table-Style CellOverride-21"> <p class="_TableBody">QA involving multiple documents, complex reasoning tasks</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style CellOverride-19"> <p class="_TableBody">BEIR</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Multiple datasets</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Information retrieval</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">nDCG@10</p> </td> 
      <td class="No-Table-Style CellOverride-21"> <p class="_TableBody">Comprehensive IR model evaluation across multiple domains</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-10"> 
      <td class="No-Table-Style CellOverride-19"> <p class="_TableBody">RGB</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody ParaOverride-30">News articles, ChatGPT-generated QA</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Robust QA</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Accuracy, rejection rate, error detection rate, error correction rate</p> </td> 
      <td class="No-Table-Style CellOverride-21"> <p class="_TableBody">Robustness and reliability of RAG systems</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-11"> 
      <td class="No-Table-Style CellOverride-19"> <p class="_TableBody">Multi-hop RAG</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">HKUST-curated queries</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Complex QA</p> </td> 
      <td class="No-Table-Style CellOverride-20"> <p class="_TableBody">Various</p> </td> 
      <td class="No-Table-Style CellOverride-21"> <p class="_TableBody">RAG applications requiring multi-source synthesis</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style CellOverride-22"> <p class="_TableBody">CRAG</p> </td> 
      <td class="No-Table-Style CellOverride-23"> <p class="_TableBody">Multiple sources (finance, sports, music, etc.)</p> </td> 
      <td class="No-Table-Style CellOverride-23"> <p class="_TableBody">Factual QA</p> </td> 
      <td class="No-Table-Style CellOverride-23"> <p class="_TableBody">Four-class evaluation (perfect, acceptable, missing, and incorrect)</p> </td> 
      <td class="No-Table-Style CellOverride-24"> <p class="_TableBody">Evaluating factual QA with diverse question types</p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p227"> 
   <p>We have looked frameworks that help in automating the calculation of evaluation metrics and benchmarks that enable comparisons across different implementations and approaches. Frameworks will assist you in improving the performance of your system, and benchmarks will facilitate comparing it with other systems available in the market. </p> 
  </div> 
  <div class="readable-text intended-text" id="p228"> 
   <p>However, as with any evolving field, there are some limitations and challenges to consider. The next section examines these limitations and discusses best practices that have emerged to address them, ensuring a more holistic and nuanced approach to RAG evaluation.</p> 
  </div> 
  <div class="readable-text" id="p229"> 
   <h2 class=" readable-text-h2"><span class="num-string">5.5</span> Limitations and best practices</h2> 
  </div> 
  <div class="readable-text" id="p230"> 
   <p>There has been a lot of progress made in the frameworks and benchmarks used for RAG evaluation. The complexity in evaluation arises due to the interplay between the retrieval and generation components. In practice, there’s a significant reliance on human judgements, which are subjective and difficult to scale. What follows are a few common challenges and some guidelines to navigate them.</p> 
  </div> 
  <div class="readable-text" id="p231"> 
   <h4 class=" readable-text-h4">Lack of standardized metrics</h4> 
  </div> 
  <div class="readable-text" id="p232"> 
   <p>There’s no consensus on what the best metrics are to evaluate RAG systems. Precision, recall, and F1-score are commonly measured for retrieval but do not fully capture the nuances of generative response. Similarly, commonly used generation metrics such as BLEU, ROUGE, and similar do not fully capture the context awareness required for RAG. Using RAG-specific metrics such as answer relevance, context relevance, and faithfulness for evaluation brings in the necessary nuances required for RAG evaluation. However, even for these metrics, there’s no standard way of calculation and each framework brings in its methodology.</p> 
  </div> 
  <div class="readable-text intended-text" id="p233"> 
   <p><em>Best practice</em>: Compare the results on RAG specific metrics from different frameworks. Sometimes, it may be warranted to change the calculation method with respect to the use case. </p> 
  </div> 
  <div class="readable-text" id="p234"> 
   <h4 class=" readable-text-h4">Overreliance on LLM as a judge</h4> 
  </div> 
  <div class="readable-text" id="p235"> 
   <p>The evaluation of RAG-specific metrics (in RAGAs, ARES, etc.) relies on using an LLM as a judge. An LLM is prompted or fine-tuned to classify a response as relevant or not. This adds to the complexity of the LLMs’ ability to do this task. It may be possible that the LLM may not be very accurate in judging for your specific documents and knowledge bases. Another problem that arises is that of self-reference. It is possible that if the judge LLM is the same as the generation LLM in your system, you will get a more favorable evaluation. </p> 
  </div> 
  <div class="readable-text intended-text" id="p236"> 
   <p><em>Best practice</em>: Sample a few results from the judge LLM and evaluate whether the results are in line with commonly understood business practice. To avoid the self-reference problem, make sure to use a judge LLM different from the generation LLM. It may also help if you use multiple judge LLMs and aggregate their results. </p> 
  </div> 
  <div class="readable-text" id="p237"> 
   <h4 class=" readable-text-h4">Lack of use case subjectivity</h4> 
  </div> 
  <div class="readable-text" id="p238"> 
   <p>Most frameworks have a generalized approach to evaluation. They may not capture the subjective nature of the task relevant to your use case (content generation versus chatbot versus question-answering, etc.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p239"> 
   <p><em>Best practice</em>: Focus on use-case-specific metrics to assess quality, coherence, usefulness, and similar. Incorporate human judgements in your workflow with techniques such as user feedback, crowd-sourcing, or expert ratings.</p> 
  </div> 
  <div class="readable-text" id="p240"> 
   <h4 class=" readable-text-h4">Benchmarks are static</h4> 
  </div> 
  <div class="readable-text" id="p241"> 
   <p>Most benchmarks are static and do not account for the evolving nature of information. RAG systems need to adapt to real-time information changes, which are not currently tested effectively. There is a lack of evaluation for how well RAG models learn and adapt to new data over time. Most benchmarks are domain-agnostic, which may not reflect the performance of RAG systems in your specific domain.</p> 
  </div> 
  <div class="readable-text intended-text" id="p242"> 
   <p><em>Best practice</em>: Use a benchmark that is tailored to your domain. The static nature of benchmarks is limiting. Do not overly rely on benchmarks, and augment the use of benchmarks with regularly updating data. </p> 
  </div> 
  <div class="readable-text" id="p243"> 
   <h4 class=" readable-text-h4">Scalability and cost</h4> 
  </div> 
  <div class="readable-text" id="p244"> 
   <p>Evaluating large-scale RAG systems is more complex than evaluating basic RAG pipelines. It requires significant computational resources. Benchmarks and frameworks also generally do not account for metrics such as latency and efficiency, which are critical for real-world applications. </p> 
  </div> 
  <div class="readable-text intended-text" id="p245"> 
   <p><em>Best practice</em>: Employ careful sampling of test cases for evaluation. Incorporate workflows to measure latency and efficiency. </p> 
  </div> 
  <div class="readable-text intended-text" id="p246"> 
   <p>Apart from these, you should also carefully consider the aspects of bias and toxicity, focusing on information integration and negative rejection, which the frameworks do not evaluate well. It is also important to keep an eye on how these evaluation frameworks and benchmarks evolve.</p> 
  </div> 
  <div class="readable-text intended-text" id="p247"> 
   <p>In this chapter, we comprehensively examined the evaluation metrics, frameworks, and benchmarks that will help you evaluate your RAG pipelines. We used RAGAs to evaluate the pipeline that we have been building. </p> 
  </div> 
  <div class="readable-text intended-text" id="p248"> 
   <p>Until now, we have looked at building and evaluating a simple RAG system. This also marks the second part 2 of this book. You are now familiar with the creation of the RAG knowledge brain using the indexing pipeline, enabling real-time interaction using the generation pipeline and evaluating your RAG system using frameworks and benchmarks.</p> 
  </div> 
  <div class="readable-text intended-text" id="p249"> 
   <p>In the next part, we will move toward discussing the production aspects of RAG systems. In chapter 6, we will look at strategies and advanced techniques to improve our RAG pipeline, which should also reflect in better evaluation metrics. In chapter 7, we will look at the LLMOps stack that enables RAG in production.</p> 
  </div> 
  <div class="readable-text" id="p250"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <div class="readable-text" id="p251"> 
   <h3 class=" readable-text-h3">RAG evaluation fundamentals</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p252">RAG evaluation assesses how well systems reduce hallucinations and ground responses in the provided context.</li> 
   <li class="readable-text" id="p253">Three key quality scores for RAG evaluation are context relevance, answer faithfulness, and answer relevance.</li> 
   <li class="readable-text" id="p254">Four critical abilities required of RAG systems include noise robustness, negative rejection, information integration, and counterfactual robustness.</li> 
   <li class="readable-text" id="p255">Additional considerations include latency, robustness, bias, and toxicity of responses.</li> 
   <li class="readable-text" id="p256">Custom use-case-specific metrics should be developed to evaluate performance.</li> 
  </ul> 
  <div class="readable-text" id="p257"> 
   <h3 class=" readable-text-h3">Evaluation metrics</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p258">Retrieval metrics include precision, recall, F1-score, mean reciprocal rank (MRR), mean average precision (MAP), and normalized discounted cumulative gain (nDCG).</li> 
   <li class="readable-text" id="p259">Accuracy, precision, recall, and F1-score do not consider the ranking order of the results.</li> 
   <li class="readable-text" id="p260">RAG-specific metrics focus on context relevance, answer faithfulness, and answer relevance.</li> 
   <li class="readable-text" id="p261">Human evaluations and ground truth data play a crucial role in RAG assessment.</li> 
  </ul> 
  <div class="readable-text" id="p262"> 
   <h3 class=" readable-text-h3">Evaluation frameworks</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p263">RAGAs is an easy-to-implement framework that can be used for quick evaluation of RAG pipelines.</li> 
   <li class="readable-text" id="p264">ARES uses a more complex approach, including classifier training and confidence interval calculations.</li> 
  </ul> 
  <div class="readable-text" id="p265"> 
   <h3 class=" readable-text-h3">Benchmarks</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p266">Benchmarks provide standardized datasets and metrics for comparing different RAG implementations on specific tasks.</li> 
   <li class="readable-text" id="p267">Popular benchmarks such as SQuAD, natural questions, HotpotQA, and BEIR focus on retrieval quality.</li> 
   <li class="readable-text" id="p268">Recent benchmarks such as RGB, multi-hop RAG, and CRAG are more holistic from a RAG perspective.</li> 
   <li class="readable-text" id="p269">Benchmarks focus on different aspects of RAG performance, such as multi-hop reasoning or specific domains.</li> 
  </ul> 
  <div class="readable-text" id="p270"> 
   <h3 class=" readable-text-h3">Limitations and best practices</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p271">Challenges in RAG evaluation include lack of standardized metrics, overreliance on LLMs as judges, and static nature of benchmarks.</li> 
   <li class="readable-text" id="p272">Best practices include using multiple frameworks, incorporating use-case-specific metrics, and regularly updating evaluation data.</li> 
   <li class="readable-text" id="p273">Balancing automated metrics with human judgment and considering use-case-specific requirements is crucial.</li> 
   <li class="readable-text" id="p274">The field of RAG evaluation is evolving, with new frameworks and benchmarks constantly emerging.</li> 
   <li class="readable-text" id="p275">Developers should stay informed about new developments and adapt their evaluation strategies accordingly.</li> 
  </ul>
 </body>
</html>