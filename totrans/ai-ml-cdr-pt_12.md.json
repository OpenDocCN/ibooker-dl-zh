["```py\nclass CNN1D(nn.Module):\n    def __init__(self, input_size):\n        super(CNN1D, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=1,\n                              out_channels=128,\n                              kernel_size=3,\n                              padding=1)\n\n        conv_output_size = input_size  # Same padding maintains input size\n\n        self.relu = nn.ReLU()\n        self.flatten = nn.Flatten()\n        self.dense1 = nn.Linear(128 * conv_output_size, 28)\n        self.dense2 = nn.Linear(28, 10)\n        self.dense3 = nn.Linear(10, 1)\n\n    def forward(self, x):\n        # Transpose input from [batch_size, sequence_length] \n        # to [batch_size, 1, sequence_length]\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)\n        elif len(x.shape) == 3 and x.shape[1] != 1:\n            x = x.transpose(1, 2)\n\n        x = self.relu(self.conv1(x))\n        x = self.flatten(x)\n        x = self.relu(self.dense1(x))\n        x = self.relu(self.dense2(x))\n        x = self.dense3(x)\n        return x\n```", "```py\n        self.conv1 = nn.Conv1d(in_channels=1,\n                              out_channels=128,\n                              kernel_size=3,\n                              padding=1)\n```", "```py\nconv_output_size = input_size  # Same padding maintains input size\n```", "```py\n        self.dense1 = nn.Linear(128 * conv_output_size, 28)\n\n```", "```py\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)\n        elif len(x.shape) == 3 and x.shape[1] != 1:\n            x = x.transpose(1, 2)\n```", "```py\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n```", "```py\n# Create DataLoaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, \n                          shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n```", "```py\ndef predict(model, loader):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.eval()\n    predictions = []\n\n    with torch.no_grad():\n        for inputs, _ in loader:\n            inputs = inputs.to(device)\n            batch_predictions = model(inputs)\n            predictions.append(batch_predictions.cpu().numpy())\n\n    return np.concatenate(predictions)\n\n```", "```py\n# Make predictions\ntrain_predictions = predict(model, train_loader)\nval_predictions = predict(model, val_loader)\n```", "```py\ndef evaluate_predictions(model, loader):\n    \"\"\"Generate predictions and calculate metrics\"\"\"\n    model.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    all_predictions = []\n    all_targets = []\n\n    with torch.no_grad():\n        for inputs, targets in loader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            all_predictions.extend(outputs.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n\n    predictions = np.array(all_predictions)\n    targets = np.array(all_targets)\n\n    # Calculate metrics\n    mae = mean_absolute_error(targets, predictions)\n\n    return predictions, targets, mae\n```", "```py\n# Generate predictions\nval_predictions, val_targets, val_mae \n             = evaluate_predictions(model, val_loader)\n```", "```py\ndef plot_predictions(val_pred, val_true):\n    \"\"\"Plot the predictions against actual values\"\"\"\n    plt.figure(figsize=(15, 6))\n    # Plot validation data\n    offset = len(val_true)\n    plt.plot(range(offset, offset + len(val_true)), \n                   val_true, 'b-', label='Validation Actual')\n    plt.plot(range(offset, offset + len(val_pred)), \n                   val_pred, 'r-', label='Validation Predicted')\n    plt.title('Time Series Prediction vs Actual')\n    plt.xlabel('Time Step')\n    plt.ylabel('Value')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n```", "```py\n# Define the search space\nnum_conv_layers_options = [1, 2]  # Reduced for initial testing\nconv_channels_options = [\n    [32],\n    [64],\n    [32, 16],\n    [64, 32],\n]\nkernel_sizes = [3, 5]\ndense_sizes_options = [\n    [16],\n    [32, 16],\n    [64, 32],\n]\nlearning_rates = [0.001, 0.0001]\n```", "```py\n# Generate valid configurations\nconfigurations = []\nfor num_conv_layers in num_conv_layers_options:\n    for channels in conv_channels_options:\n        # Only use channel configs that match layer count\n        if len(channels) == num_conv_layers:  \n            for kernel_size in kernel_sizes:\n                for dense_sizes in dense_sizes_options:\n                    for lr in learning_rates:\n                        configurations.append({\n                            'num_conv_layers': num_conv_layers,\n                            'conv_channels': channels,\n                            'kernel_size': kernel_size,\n                            'dense_sizes': dense_sizes,\n                            'learning_rate': lr\n                        })\n```", "```py\nfor idx, config in enumerate(configurations):\n    print(f\"\\nTrying configuration {idx + 1}/{len(configurations)}:\")\n    print(config)\n\n    try:\n        model = CNN1D(\n            input_size=input_size,\n            num_conv_layers=config['num_conv_layers'],\n            conv_channels=config['conv_channels'],\n            kernel_size=config['kernel_size'],\n            dense_sizes=config['dense_sizes']\n        ).to(device)\n\n        criterion = nn.MSELoss()\n        optimizer = torch.optim.Adam(model.parameters(), \n                                     lr=config['learning_rate'])\n```", "```py\ntrained_model, val_loss = train_model(\n    model, train_loader, val_loader, criterion, optimizer,\n    epochs=100, device=device, early_stopping_patience=10\n)\n```", "```py\n# Early stopping check\nif val_loss < best_val_loss:\n    best_val_loss = val_loss\n    best_model = deepcopy(model)\n    patience_counter = 0\nelse:\n    patience_counter += 1\n\nif patience_counter >= early_stopping_patience:\n    print(f'Early stopping triggered after {epoch} epochs')\n    break\n```", "```py\ndef get_data():\n    data_file = \"station.csv\"\n    f = open(data_file)\n    data = f.read()\n    f.close()\n    lines = data.split('\\n')\n    header = lines[0].split(',')\n    lines = lines[1:]\n    temperatures=[]\n    for line in lines:\n        if line:\n            linedata = line.split(',')\n            linedata = linedata[1:13]\n            for item in linedata:\n                if item:\n                    temperatures.append(float(item))\n\n    series = np.asarray(temperatures)\n    time = np.arange(len(temperatures), dtype=\"float32\")\n    return time, series\n```", "```py\nimport numpy as np\n\ndef normalize_series(data, missing_value=999.9):\n    # Convert to numpy array if not already\n    data = np.array(data, dtype=np.float64)\n\n    # Create mask for valid values (not NaN and not missing_value)\n    valid_mask = (data != missing_value) & (~np.isnan(data))\n\n    # Keep only valid values\n    clean_data = data[valid_mask]\n\n    # Normalize using only valid values\n    mean = np.mean(clean_data)\n    std = np.std(clean_data)\n    normalized = (clean_data - mean) / std\n\n    return normalized\n\ntime, series = get_data()\nseries_normalized = normalize_series(series)\n```", "```py\nseries_tensor = torch.tensor(series_normalized, dtype=torch.float32)\nwindow_size = 48\nfeatures, targets = create_sliding_windows_with_target(series_tensor, \n                    window_size=window_size, shift=1)\n```", "```py\nsplit_location = 800\n# Create the full dataset\nfull_dataset = TensorDataset(features, targets)\n\n# Calculate split indices\n# Note: Since we're using windows, we need to account for the overlap\ntrain_size = 800 - window_size + 1  # Adjust for window overlap\ntotal_windows = len(full_dataset)\ntrain_indices = list(range(train_size))\nval_indices = list(range(train_size, total_windows))\n\n# Create training and validation datasets using Subset\ntrain_dataset = Subset(full_dataset, train_indices)\nval_dataset = Subset(full_dataset, val_indices)\n```", "```py\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, \n                          shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n```", "```py\nclass SimpleRNNModel(nn.Module):\n    def __init__(self, input_size=1, hidden_size=100, \n                       output_size=1, dropout_rate=0.3):\n        super(SimpleRNNModel, self).__init__()\n\n        self.rnn1 = nn.RNN(input_size=input_size,\n                          hidden_size=hidden_size,\n                          batch_first=True,\n                          dropout=dropout_rate)  # Add dropout to RNN\n\n        self.rnn2 = nn.RNN(input_size=hidden_size,\n                          hidden_size=hidden_size,\n                          batch_first=True,\n                          dropout=dropout_rate)  # Add dropout to RNN\n\n        self.dropout = nn.Dropout(dropout_rate)  # Additional dropout layer\n        self.linear = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        out1, _ = self.rnn1(x)\n        out2, _ = self.rnn2(out1)\n        last_out = out2[:, –1, :]\n        last_out = self.dropout(last_out)  # Add dropout before final layer\n        output = self.linear(last_out)\n        return output\n```", "```py\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n```", "```py\nimport numpy as np\ndef get_data():\n    data_file = \"tdaily_cet.dat.txt\"\n    f = open(data_file)\n    data = f.read()\n    f.close()\n    lines = data.split('\\n')\n    temperatures=[]\n    for line in lines:\n        if line:\n            linedata = line.split(' ')\n            temperatures.append(float(linedata[1]))\n\n    series = np.asarray(temperatures)\n    time = np.arange(len(temperatures), dtype=\"float32\")\n    return time, series\n\n```", "```py\nsplit_location = 80000\n\nfeatures = features.unsqueeze(1)\n# Create the full dataset\nfull_dataset = TensorDataset(features, targets)\n\n# Calculate split indices\n# Note: Since we're using windows, we need to account for the overlap\ntrain_size = split_location - window_size + 1  # Adjust for window overlap\ntotal_windows = len(full_dataset)\ntrain_indices = list(range(train_size))\nval_indices = list(range(train_size, total_windows))\n\n# Create training and validation datasets using Subset\ntrain_dataset = Subset(full_dataset, train_indices)\nval_dataset = Subset(full_dataset, val_indices)\n\n```", "```py\nclass SimpleRNNModel(nn.Module):\n    def __init__(self, input_size=1, hidden_size=100, \n                       output_size=1, dropout_rate=0.3):\n        super(SimpleRNNModel, self).__init__()\n\n        self.rnn1 = nn.GRU(input_size=input_size,\n                          hidden_size=hidden_size,\n                          batch_first=True,\n                          dropout=dropout_rate)  \n\n        self.rnn2 = nn.GRU(input_size=hidden_size,\n                          hidden_size=hidden_size,\n                          batch_first=True,\n                          dropout=dropout_rate)  \n\n        self.dropout = nn.Dropout(dropout_rate)  \n        self.linear = nn.Linear(hidden_size, output_size)\n\n```", "```py\n# LSTM Optional Architecture\nimport torch.nn as nn\n\nclass SimpleLSTMModel(nn.Module):\n    def __init__(self, input_size=1, hidden_size=100, \n                       output_size=1, dropout_rate=0.3):\n        super(SimpleLSTMModel, self).__init__()\n\n        self.lstm1 = nn.LSTM(input_size=input_size,\n                            hidden_size=hidden_size,\n                            batch_first=True,\n                            dropout=dropout_rate)  # Add dropout to LSTM\n\n        self.lstm2 = nn.LSTM(input_size=hidden_size,\n                            hidden_size=hidden_size,\n                            batch_first=True,\n                            dropout=dropout_rate)  # Add dropout to LSTM\n\n        # Add more layers before final output\n        self.fc1 = nn.Linear(hidden_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.linear = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        out1, _ = self.lstm1(x)  # LSTM returns (output, (h_n, c_n))\n        out2, _ = self.lstm2(out1) # We ignore both hidden and cell states with _\n        last_out = out2[:, –1, :]\n        output = self.linear(last_out)\n        return output\n\n```", "```py\nclass SimpleRNNModel(nn.Module):\n    def __init__(self, input_size=1, hidden_size=100, \n                       output_size=1, dropout_rate=0.1):\n        super(SimpleRNNModel, self).__init__()\n\n        self.rnn1 = nn.GRU(input_size=input_size,\n                          hidden_size=hidden_size,\n                          batch_first=True,\n                          `dropout``=``dropout_rate``)`  \n\n        self.rnn2 = nn.GRU(input_size=hidden_size,\n                          hidden_size=hidden_size,\n                          batch_first=True,\n                          `dropout``=``dropout_rate``)`  \n\n        self.dropout = nn.Dropout(dropout_rate)  \n        self.linear = nn.Linear(hidden_size, output_size)\n```", "```py\nclass BidirectionalGRUModel(nn.Module):\n    def __init__(self, input_size=1, hidden_size=100, \n                       output_size=1, dropout_rate=0.1):\n        super(BidirectionalGRUModel, self).__init__()\n\n        self.gru1 = nn.GRU(input_size=input_size,\n                          hidden_size=hidden_size,\n                          batch_first=True,\n                          dropout=dropout_rate,\n                          bidirectional=True)\n\n        self.gru2 = nn.GRU(input_size=hidden_size * 2,\n                          hidden_size=hidden_size,\n                          batch_first=True,\n                          dropout=dropout_rate,\n                          bidirectional=True)\n\n        # Additional layers\n        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.linear = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        out1, _ = self.gru1(x)\n        out2, _ = self.gru2(out1)\n        last_out = out2[:, –1, :]\n\n        # Additional processing\n        x = self.fc1(last_out)\n        x = self.relu(x)\n        x = self.dropout(x)\n        output = self.linear(x)\n        return output\n\n```"]