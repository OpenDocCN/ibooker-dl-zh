- en: 'Chapter 2\. What’s in the Picture: Image Classification with Keras'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have skimmed through deep learning literature, you might have come across
    a barrage of academic explanations laced with intimidating mathematics. Don’t
    worry. We will ease you into practical deep learning with an example of classifying
    images with just a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we take a closer look at the Keras framework, discuss its place
    in the deep learning landscape, and then use it to classify a few images using
    existing state-of-the-art classifiers. We visually investigate how these classifiers
    operate by using *heatmaps*. With these heatmaps, we make a fun project in which
    we classify objects in videos.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall from the [“Recipe for the Perfect Deep Learning Solution”](part0003.html#2RHT2-13fa565533764549a6f0ab7f11eed62b)
    that we need four ingredients to create our deep learning recipe: hardware, dataset,
    framework, and model. Let’s see how each of these comes into play in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with the easy one: *hardware*. Even an inexpensive laptop would suffice
    for what we we’re doing in this chapter. Alternatively, you can run the code in
    this chapter by opening the GitHub notebook (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    in Colab. This is just a matter of a few mouse clicks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because we won’t be training a neural network just yet, we don’t need a *dataset*
    (other than a handful of sample photos to test with).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we come to the *framework*. This chapter’s title has Keras in it, so that
    is what we will be using for now. In fact, we use Keras for our training needs
    throughout a good part of the book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One way to approach a deep learning problem is to obtain a dataset, write the
    code to train it, spend a lot of time and energy (both human and electrical) in
    training that model, and then use it for making predictions. But we are not gluttons
    for punishment. So, we will use a *pretrained model* instead. After all, the research
    community has already spent blood, sweat, and tears training and publishing many
    of the standard models that are now publicly available. We will be reusing one
    of the more famous models called ResNet-50, the little sibling of ResNet-152 that
    won the ILSVRC in 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will get hands-on with some code in this chapter. As we all know, the best
    way to learn is by doing. You might be wondering, though, what’s the theory behind
    this? That comes in later chapters, in which we delve deeper into the nuts and
    bolts of CNNs using this chapter as a foundation.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As [Chapter 1](part0003.html#2RHM3-13fa565533764549a6f0ab7f11eed62b) discussed,
    Keras started in 2015 as an easy-to-use abstraction layer over other libraries,
    making rapid prototyping possible. This made the learning curve a lot less steep
    for beginners of deep learning. At the same time, it made deep learning experts
    more productive by helping them rapidly iterate on experiments. In fact, the majority
    of the winning teams on *[Kaggle.com](http://Kaggle.com)* (which hosts data science
    competitions) have used Keras. Eventually, in 2017, the full implementation of
    Keras was available directly in TensorFlow, thereby combining the high scalability,
    performance, and vast ecosystem of TensorFlow with the ease of Keras. On the web,
    we often see the TensorFlow version of Keras referred to as `tf.keras`.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter and [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b),
    we write all of the code exclusively in Keras. That includes boilerplate functions
    such as file reading, image manipulation (augmentation), and so on. We do this
    primarily for ease of learning. From [Chapter 5](part0007.html#6LJU3-13fa565533764549a6f0ab7f11eed62b)
    onward, we begin to gradually use more of the native performant TensorFlow functions
    directly for more configurability and control.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting an Image’s Category
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In layperson’s terms, image classification answers the question: “what object
    does this image contain?” More specifically, “This image contains *X* object with
    what probability,” where *X* is from a predefined list of categories of objects.
    If the probability is higher than a minimum threshold, the image is likely to
    contain one or more instances of *X*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple image classification pipeline would consist of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Load an image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Resize it to a predefined size such as 224 x 224 pixels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scale the values of the pixel to the range [0,1] or [–1,1], a.k.a. normalization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a pretrained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the pretrained model on the image to get a list of category predictions
    and their respective probabilities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Display a few of the highest probability categories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The GitHub link is provided on the website [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai).
    Navigate to `code/chapter-2` where you will find the Jupyter notebook `1-predict-class.ipynb`
    that has all the steps detailed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by importing all of the necessary modules from the Keras and Python
    packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we load and display the image that we want to classify (see [Figure 2-1](part0004.html#plot_showing_the_contents_of_the_input_f)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Plot showing the contents of the input file](../images/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. Plot showing the contents of the input file
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Yup, it’s a cat (although the filename kind of gave it away). And that’s what
    our model should ideally be predicting.
  prefs: []
  type: TYPE_NORMAL
- en: Before feeding any image to Keras, we want to convert it to a standard format.
    This is because pretrained models expect the input to be of a specific size. The
    standardization in our case involves resizing the image to 224 x 224 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: Most deep learning models expect a batch of images as input. But what do we
    do when we have just one image? We create a batch of one image, of course! That
    essentially involves making an array consisting of that one object. Another way
    to look at this is to expand the number of dimensions from three (representing
    the three channels of the image) to four (the extra one for the length of the
    array itself).
  prefs: []
  type: TYPE_NORMAL
- en: 'If that is not clear, consider this scenario: for a batch of 64 images of size
    224 x 224 pixels, each containing three channels (RGB), the object representing
    that batch would have a shape 64 x 224 x 224 x 3\. In the code that follows, where
    we’d be using only one 224 x 224 x 3 image, we’d create a batch of just that image
    by expanding the dimensions from three to four. The shape of this newly created
    batch would be 1 x 224 x 224 x 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In machine learning, models perform best when they are fed with data within
    a consistent range. Ranges typically include [0,1] and [–1,1]. Given that image
    pixel values are between 0 and 255, running the `preprocess_input` function from
    Keras on input images will normalize each pixel to a standard range. *Normalization*
    or *feature scaling* is one of the core steps in preprocessing images to make
    them suitable for deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now comes the model. We will be using a *Convolutional Neural Network* (CNN)
    called ResNet-50\. The very first question we should ask is, “Where will I find
    the model?” Of course, we could hunt for it on the internet to find something
    that is compatible with our deep learning framework (Keras). *But ain’t nobody
    got time for that!* Luckily, Keras loves to make things easy and provides it to
    us in a single function call. After we call this function for the first time,
    the model will be downloaded from a remote server and cached locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: When predicting with this model, the results include probability predictions
    for each class. Keras also provides the `decode_predictions` function, which tells
    us the probability of each category of objects contained in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see the entire code in one handy function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The predicted categories for this image are various types of felines. Why doesn’t
    it simply predict the word “cat,” instead? The short answer is that the ResNet-50
    model was trained on a granular dataset with many categories and does not include
    the more general “cat.” We investigate this dataset in more detail a little later,
    but first, let’s load another sample image (see [Figure 2-2](part0004.html#plot_showing_the_contents_of_the_file_do)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Plot showing the contents of the file dog.jpg](../images/00236.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. Plot showing the contents of the file dog.jpg
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'And, again, we run our handy function from earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As expected, we get different breeds of canines (and not just the “dog” category).
    If you are unfamiliar with the Corgi breed of dogs, the word “corgi” literally
    means “dwarf dog” in Welsh. The Cardigan and Pembroke are subbreeds of the Corgi
    family, which happen to look pretty similar to one another. It’s no wonder our
    model thinks that way, too.
  prefs: []
  type: TYPE_NORMAL
- en: Notice the predicted probability of each category. Usually, the prediction with
    the highest probability is considered the answer. Alternatively, any value over
    a predefined threshold can be considered as the answer, too. In the dog example,
    if we set a threshold of 0.5, Cardigan would be our answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Running the notebook on Google Colab using the browser](../images/00183.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Running the notebook on Google Colab using the browser
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can follow along with the code in this chapter and execute it interactively
    without any installations in the browser itself with Google Colab. Simply find
    the “Run on Colab” link at the top of each notebook on GitHub that you’d like
    to experiment with. Then, click the “Run Cell” button; this should execute the
    code within that cell, as shown in [Figure 2-3](part0004.html#running_the_notebook_on_google_colab_usi).
  prefs: []
  type: TYPE_NORMAL
- en: Investigating the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We got the predictions from our model, great! But what factors led to those
    predictions? There are a few questions that we need to ask here:'
  prefs: []
  type: TYPE_NORMAL
- en: What dataset was the model trained on?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there other models that I can use? How good are they? Where can I get them?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why does my model predict what it predicts?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We look into the answers to each of these questions in this section.
  prefs: []
  type: TYPE_NORMAL
- en: ImageNet Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s investigate the ImageNet dataset on which ResNet-50 was trained. [ImageNet](http://www.image-net.org/),
    as the name suggests, is a network of images; that is, a dataset of images organized
    as a network, as demonstrated in [Figure 2-4](part0004.html#the_categories_and_subcategories_in_the).
    It is arranged in a hierarchical manner (like the WordNet hierarchy) such that
    the parent node encompasses a collection of images of all different varieties
    possible within that parent. For example, within the “animal” parent node, there
    are fish, birds, mammals, invertebrates, and so on. Each category has multiple
    subcategories, and these have subsubcategories, and so forth. For example, the
    category “American water spaniel” is eight levels from the root. The dog category
    contains 189 total subcategories in five hierarchical levels.
  prefs: []
  type: TYPE_NORMAL
- en: Visually, we developed the tree diagram shown in [Figure 2-5](part0004.html#tree_map_of_imagenet_and_its_classes)
    to help you to understand the wide variety of high-level entities that the ImageNet
    dataset contains. This treemap also shows the relative percentage of different
    categories that make up the ImageNet dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![The categories and subcategories in the ImageNet dataset](../images/00258.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. The categories and subcategories in the ImageNet dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Treemap of ImageNet and its classes](../images/00319.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. Treemap of ImageNet and its classes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The ImageNet dataset was the basis for the famous ILSVRC that started in 2010
    to benchmark progress in computer vision and challenge researchers to innovate
    on tasks including object classification. Recall from [Chapter 1](part0003.html#2RHM3-13fa565533764549a6f0ab7f11eed62b)
    that the ImageNet challenge saw submissions that drastically improved in accuracy
    each year. When it started out, the error rate was nearly 30%. And now, it is
    2.2%, already better than how an average human would perform at this task. This
    dataset and challenge are considered the single biggest reasons for the recent
    advancements in computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Wait, AI has better-than-human accuracy? If the dataset was created by humans,
    won’t humans have 100% accuracy? Well, the dataset was created by experts, with
    each image verified by multiple people. Then Stanford researcher (and now of Tesla
    fame) Andrej Karpathy attempted to figure out how much a normal human would fare
    on ImageNet-1000\. Turns out he achieved an accuracy of 94.9%, well short of the
    100% we all expected. Andrej painstakingly spent a week going over 1,500 images,
    spending approximately one minute per image in tagging it. How did he misclassify
    5.1% of the images? The reasons are a bit subtle:'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-grained recognition
  prefs: []
  type: TYPE_NORMAL
- en: For many people, it is really tough to distinguish a Siberian husky from a Alaskan
    Malamute. Someone who is really familiar with dog breeds would be able to tell
    them apart because they look for finer-level details that distinguish both breeds.
    It turns out that neural networks are capable of learning those finer-level details
    much more easily than humans.
  prefs: []
  type: TYPE_NORMAL
- en: Category unawareness
  prefs: []
  type: TYPE_NORMAL
- en: Not everyone is aware of all the 120 breeds of dogs and most certainly not each
    one of the 1,000 classes. But the AI is. After all, it was trained on it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Similar to ImageNet, speech datasets like Switchboard report a 5.1% error rate
    for speech transcription (coincidentally the same number as ImageNet). It’s clear
    that humans have a limit, and AI is gradually beating us.
  prefs: []
  type: TYPE_NORMAL
- en: One of the other key reasons for this fast pace of improvement was that researchers
    were openly sharing models trained on datasets like ImageNet. In the next section,
    we learn about model reuse in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Model Zoos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A model zoo is a place where organizations or individuals can publicly upload
    models that they have built for others to reuse and improve upon. These models
    can be trained using any framework (e.g., Keras, TensorFlow, MXNet), for any task
    (classification, detection, etc.), or trained on any dataset (e.g., ImageNet,
    Street View House Numbers (SVHN)).
  prefs: []
  type: TYPE_NORMAL
- en: The tradition of model zoos started with Caffe, one of the first deep learning
    frameworks, developed at the University of California, Berkeley. Training a deep
    learning model from scratch on a multimillion-image database requires weeks of
    training time and lots of GPU computational energy, making it a difficult task.
    The research community recognized this as a bottleneck, and the organizations
    that participated in the ImageNet competition open sourced their trained models
    on Caffe’s website. Other frameworks soon followed suit.
  prefs: []
  type: TYPE_NORMAL
- en: When starting out on a new deep learning project, it’s a good idea to first
    explore whether there’s already a model that performs a similar task and was trained
    on a similar dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The [model zoo](https://keras.io/applications/) in Keras is a collection of
    various architectures trained using the Keras framework on the ImageNet dataset.
    We tabulate their details in [Table 2-1](part0004.html#architectural_details_of_select_pretrain).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. Architectural details of select pretrained ImageNet models
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Size** | **Top-1 accuracy** | **Top-5 accuracy** | **Parameters**
    | **Depth** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| VGG16 | 528 MB | 0.713 | 0.901 | 138,357,544 | 23 |'
  prefs: []
  type: TYPE_TB
- en: '| VGG19 | 549 MB | 0.713 | 0.9 | 143,667,240 | 26 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-50 | 98 MB | 0.749 | 0.921 | 25,636,712 | 50 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-101 | 171 MB | 0.764 | 0.928 | 44,707,176 | 101 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-152 | 232 MB | 0.766 | 0.931 | 60,419,944 | 152 |'
  prefs: []
  type: TYPE_TB
- en: '| InceptionV3 | 92 MB | 0.779 | 0.937 | 23,851,784 | 159 |'
  prefs: []
  type: TYPE_TB
- en: '| InceptionResNetV2 | 215 MB | 0.803 | 0.953 | 55,873,736 | 572 |'
  prefs: []
  type: TYPE_TB
- en: '| NASNetMobile | 23 MB | 0.744 | 0.919 | 5,326,716 | — |'
  prefs: []
  type: TYPE_TB
- en: '| NASNetLarge | 343 MB | 0.825 | 0.96 | 88,949,818 | — |'
  prefs: []
  type: TYPE_TB
- en: '| MobileNet | 16 MB | 0.704 | 0.895 | 4,253,864 | 88 |'
  prefs: []
  type: TYPE_TB
- en: '| MobileNetV2 | 14 MB | 0.713 | 0.901 | 3,538,984 | 88 |'
  prefs: []
  type: TYPE_TB
- en: 'The column “Top-1 accuracy” indicates how many times the best guess was the
    correct answer, and the column “Top-5 accuracy” indicates how many times at least
    one out of five guesses were correct. The “Depth” of the network indicates how
    many layers are present in the network. The “Parameters” column indicates the
    size of the model; that is, how many individual weights the model has: the more
    parameters, the “heavier” the model is, and the slower it is to make predictions.
    In this book, we often use ResNet-50 (the most common architecture cited in research
    papers for high accuracy) and MobileNet (for a good balance between speed, size,
    and accuracy).'
  prefs: []
  type: TYPE_NORMAL
- en: Class Activation Maps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image saliency, usually famous in UX research, is trying to answer the question
    “What part of the image are users paying attention to?” This is facilitated with
    the help of eye-tracking studies and represented in heatmaps. For example, big,
    bold fonts or people’s faces usually get more attention than backgrounds. It’s
    easy to guess how useful these heatmaps would be to designers and advertisers,
    who can then adapt their content to maximize users’ attention. Taking inspiration
    from this human version of saliency, wouldn’t it be great to learn which part
    of the image the neural network is paying attention to? That’s precisely what
    we will be experimenting with.
  prefs: []
  type: TYPE_NORMAL
- en: In our experiment, we will be overlaying a *class activation map* (or colloquially
    a *heatmap*) on top of a video in order to understand what the network pays attention
    to. The heatmap tells us something like “In this picture, these pixels were responsible
    for the prediction of the class `dog` where “dog” was the category with the highest
    probability. The “hot” pixels are represented with warmer colors such as red,
    orange, and yellow, whereas the “cold” pixels are represented using blue. The
    “hotter” a pixel is, the higher the signal it provides toward the prediction.
    [Figure 2-6](part0004.html#original_image_of_a_dog_and_its_generate) gives us
    a clearer picture. (If you’re reading the print version, refer to the book’s GitHub
    for the original color image.)
  prefs: []
  type: TYPE_NORMAL
- en: '![Original image of a dog and its generated heatmap](../images/00148.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. Original image of a dog and its generated heatmap
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the GitHub repository (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)),
    navigate to *code/chapter-2*. There, you’ll find a handy Jupyter notebook, *2-class-activation-map-on-video.ipynb,*
    which describes the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to install `keras-vis` using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We then run the visualization script on a single image to generate the heatmap
    for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We should see a newly created file called *dog-output.jpg* that shows a side-by-side
    view of the original image and its heatmap. As we can see from [Figure 2-6](part0004.html#original_image_of_a_dog_and_its_generate),
    the right half of the image indicates the “areas of heat” along with the correct
    prediction of a “Cardigan” (i.e., Welsh Corgi).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we want to visualize the heatmap for frames in a video. For that, we need
    `FFmpeg`, an open source multimedia framework. You can find the download binary
    as well as the installation instructions for your operating system at [*https://www.ffmpeg.org*](https://www.ffmpeg.org).
  prefs: []
  type: TYPE_NORMAL
- en: 'We use `ffmpeg` to split up a video into individual frames (at 25 frames per
    second) and then run our visualization script on each of those frames. We must
    first create a directory to store these frames and pass its name as part of the
    `ffmpeg` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We then run the visualization script with the path of the directory containing
    the frames from the previous step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We should see a newly created *kitchen-output* directory that contains all of
    the heatmaps for the frames from the input directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, compile a video from those frames using `ffmpeg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Perfect! The result is the original video side by side with a copy of the heatmap
    overlaid on it. This is a useful tool, in particular, to discover whether the
    model has learned the correct features or if it picked up stray artifacts during
    its training.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine generating heatmaps to analyze the strong points and shortfalls of our
    trained model or a pretrained model.
  prefs: []
  type: TYPE_NORMAL
- en: You should try this experiment out on your own by shooting a video with your
    smartphone camera and running the aforementioned scripts on the file. Don’t forget
    to post your videos on Twitter, tagging [@PracticalDLBook](https://www.twitter.com/PracticalDLBook)!
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Heatmaps are a great way to visually detect bias in the data. The quality of
    a model’s predictions depends heavily on the data on which it was trained. If
    the data is biased, that will reflect in the predictions. A great example of this
    is (although probably an urban legend) one in which the US Army wanted to use
    neural networks to detect enemy tanks camouflaged in trees.^([1](part0004.html#ch02fn2))
    The researchers who were building the model took photographs—50% containing camouflaged
    tanks and 50% with just trees. Model training yielded 100% accuracy. A cause for
    celebration? That sadly wasn’t the case when the US Army tested it. The model
    had performed very poorly—no better than random guesses. Investigation revealed
    that photos with the tanks were taken on cloudy (overcast) days and those without
    the tanks on clear, sunny days. And the neural network model began looking for
    the sky instead of the tank. If the researchers had visualized the model using
    heatmaps, they would have caught that issue pretty early.
  prefs: []
  type: TYPE_NORMAL
- en: As we collect data, we must be vigilant at the outset of potential bias that
    can pollute our model’s learning. For example, when collecting images to build
    a food classifier, we should verify that the other artifacts such as plates and
    utensils are not being learned as food. Otherwise, the presence of chopsticks
    might get our food classified as chow mein. Another term to define this is *co-occurrence*.
    Food very frequently co-occurs with cutlery. So watch out for these artifacts
    seeping into your classifier’s training.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we got a glimpse of the deep learning universe using Keras.
    It’s an easy-to-use yet powerful framework that we use in the next several chapters.
    We observed that there is often no need to collect millions of images and use
    powerful GPUs to train a custom model because we can use a pretrained model to
    predict the category of an image. By diving deeper into datasets like ImageNet,
    we learned the kinds of categories these pretrained models can predict. We also
    learned about finding these models in model zoos that exist for most frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b), we explore
    how we can tweak an existing pretrained model to make predictions on classes of
    input for which it was not originally intended. As with the current chapter, our
    approach is geared toward obtaining output without needing millions of images
    and lots of hardware resources to train a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](part0004.html#ch02fn2-marker)) [“Artificial Intelligence as a Positive
    and Negative Factor in Global Risk”](https://oreil.ly/-svD0) by Eliezer Yudkowsky
    in *Global Catastrophic Risks* (Oxford University Press).
  prefs: []
  type: TYPE_NORMAL
