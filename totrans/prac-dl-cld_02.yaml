- en: 'Chapter 2\. What’s in the Picture: Image Classification with Keras'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have skimmed through deep learning literature, you might have come across
    a barrage of academic explanations laced with intimidating mathematics. Don’t
    worry. We will ease you into practical deep learning with an example of classifying
    images with just a few lines of code.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we take a closer look at the Keras framework, discuss its place
    in the deep learning landscape, and then use it to classify a few images using
    existing state-of-the-art classifiers. We visually investigate how these classifiers
    operate by using *heatmaps*. With these heatmaps, we make a fun project in which
    we classify objects in videos.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall from the [“Recipe for the Perfect Deep Learning Solution”](part0003.html#2RHT2-13fa565533764549a6f0ab7f11eed62b)
    that we need four ingredients to create our deep learning recipe: hardware, dataset,
    framework, and model. Let’s see how each of these comes into play in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with the easy one: *hardware*. Even an inexpensive laptop would suffice
    for what we we’re doing in this chapter. Alternatively, you can run the code in
    this chapter by opening the GitHub notebook (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    in Colab. This is just a matter of a few mouse clicks.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because we won’t be training a neural network just yet, we don’t need a *dataset*
    (other than a handful of sample photos to test with).
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we come to the *framework*. This chapter’s title has Keras in it, so that
    is what we will be using for now. In fact, we use Keras for our training needs
    throughout a good part of the book.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One way to approach a deep learning problem is to obtain a dataset, write the
    code to train it, spend a lot of time and energy (both human and electrical) in
    training that model, and then use it for making predictions. But we are not gluttons
    for punishment. So, we will use a *pretrained model* instead. After all, the research
    community has already spent blood, sweat, and tears training and publishing many
    of the standard models that are now publicly available. We will be reusing one
    of the more famous models called ResNet-50, the little sibling of ResNet-152 that
    won the ILSVRC in 2015.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will get hands-on with some code in this chapter. As we all know, the best
    way to learn is by doing. You might be wondering, though, what’s the theory behind
    this? That comes in later chapters, in which we delve deeper into the nuts and
    bolts of CNNs using this chapter as a foundation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Keras
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As [Chapter 1](part0003.html#2RHM3-13fa565533764549a6f0ab7f11eed62b) discussed,
    Keras started in 2015 as an easy-to-use abstraction layer over other libraries,
    making rapid prototyping possible. This made the learning curve a lot less steep
    for beginners of deep learning. At the same time, it made deep learning experts
    more productive by helping them rapidly iterate on experiments. In fact, the majority
    of the winning teams on *[Kaggle.com](http://Kaggle.com)* (which hosts data science
    competitions) have used Keras. Eventually, in 2017, the full implementation of
    Keras was available directly in TensorFlow, thereby combining the high scalability,
    performance, and vast ecosystem of TensorFlow with the ease of Keras. On the web,
    we often see the TensorFlow version of Keras referred to as `tf.keras`.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter and [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b),
    we write all of the code exclusively in Keras. That includes boilerplate functions
    such as file reading, image manipulation (augmentation), and so on. We do this
    primarily for ease of learning. From [Chapter 5](part0007.html#6LJU3-13fa565533764549a6f0ab7f11eed62b)
    onward, we begin to gradually use more of the native performant TensorFlow functions
    directly for more configurability and control.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Predicting an Image’s Category
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In layperson’s terms, image classification answers the question: “what object
    does this image contain?” More specifically, “This image contains *X* object with
    what probability,” where *X* is from a predefined list of categories of objects.
    If the probability is higher than a minimum threshold, the image is likely to
    contain one or more instances of *X*.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 用通俗的语言来说，图像分类回答了一个问题：“这张图像包含什么对象？”更具体地说，“这张图像包含* X *对象的概率是多少”，其中* X *来自预定义的对象类别列表。如果概率高于最小阈值，则图像很可能包含一个或多个*
    X *实例。
- en: 'A simple image classification pipeline would consist of the following steps:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的图像分类流程包括以下步骤：
- en: Load an image.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一张图像。
- en: Resize it to a predefined size such as 224 x 224 pixels.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其调整为预定义大小，如224 x 224像素。
- en: Scale the values of the pixel to the range [0,1] or [–1,1], a.k.a. normalization.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将像素值缩放到范围[0,1]或[–1,1]，也就是归一化。
- en: Select a pretrained model.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个预训练模型。
- en: Run the pretrained model on the image to get a list of category predictions
    and their respective probabilities.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在图像上运行预训练模型，以获取类别预测列表及其相应的概率。
- en: Display a few of the highest probability categories.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示几个最高概率类别。
- en: Tip
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The GitHub link is provided on the website [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai).
    Navigate to `code/chapter-2` where you will find the Jupyter notebook `1-predict-class.ipynb`
    that has all the steps detailed.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub链接在网站[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)上提供。导航到`code/chapter-2`，您将找到详细步骤的Jupyter笔记本`1-predict-class.ipynb`。
- en: 'We begin by importing all of the necessary modules from the Keras and Python
    packages:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从Keras和Python包中导入所有必要的模块：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we load and display the image that we want to classify (see [Figure 2-1](part0004.html#plot_showing_the_contents_of_the_input_f)):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载并显示要分类的图像（参见[图2-1](part0004.html#plot_showing_the_contents_of_the_input_f)）：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Plot showing the contents of the input file](../images/00032.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![显示输入文件内容的图表](../images/00032.jpeg)'
- en: Figure 2-1\. Plot showing the contents of the input file
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1。显示输入文件内容的图表
- en: Yup, it’s a cat (although the filename kind of gave it away). And that’s what
    our model should ideally be predicting.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这是一只猫（尽管文件名有点暴露了）。这就是我们的模型理想情况下应该预测的内容。
- en: Before feeding any image to Keras, we want to convert it to a standard format.
    This is because pretrained models expect the input to be of a specific size. The
    standardization in our case involves resizing the image to 224 x 224 pixels.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在将任何图像传递给Keras之前，我们希望将其转换为标准格式。这是因为预训练模型期望输入具有特定大小。在我们的情况下，标准化涉及将图像调整为224 x
    224像素。
- en: Most deep learning models expect a batch of images as input. But what do we
    do when we have just one image? We create a batch of one image, of course! That
    essentially involves making an array consisting of that one object. Another way
    to look at this is to expand the number of dimensions from three (representing
    the three channels of the image) to four (the extra one for the length of the
    array itself).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度学习模型期望输入一批图像。但是当我们只有一张图像时该怎么办？当然，我们创建一个包含一张图像的批次！这实质上涉及制作一个由该对象组成的数组。另一种看待这个问题的方式是将维度的数量从三（表示图像的三个通道）扩展到四（额外的一个用于数组本身长度）。
- en: 'If that is not clear, consider this scenario: for a batch of 64 images of size
    224 x 224 pixels, each containing three channels (RGB), the object representing
    that batch would have a shape 64 x 224 x 224 x 3\. In the code that follows, where
    we’d be using only one 224 x 224 x 3 image, we’d create a batch of just that image
    by expanding the dimensions from three to four. The shape of this newly created
    batch would be 1 x 224 x 224 x 3:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这不清楚，考虑这种情况：对于一个包含64张尺寸为224 x 224像素的图像的批次，每张图像包含三个通道（RGB），表示该批次的对象将具有形状64
    x 224 x 224 x 3。在接下来的代码中，我们将只使用一张尺寸为224 x 224 x 3的图像，我们将通过将维度从三扩展到四来创建一个只包含该图像的批次。这个新创建的批次的形状将是1
    x 224 x 224 x 3：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In machine learning, models perform best when they are fed with data within
    a consistent range. Ranges typically include [0,1] and [–1,1]. Given that image
    pixel values are between 0 and 255, running the `preprocess_input` function from
    Keras on input images will normalize each pixel to a standard range. *Normalization*
    or *feature scaling* is one of the core steps in preprocessing images to make
    them suitable for deep learning.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，模型在接收到一致范围内的数据时表现最佳。范围通常包括[0,1]和[–1,1]。鉴于图像像素值在0到255之间，运行Keras的`preprocess_input`函数对输入图像进行归一化，将每个像素归一化到一个标准范围。*归一化*或*特征缩放*是图像预处理的核心步骤之一，使其适用于深度学习。
- en: 'Now comes the model. We will be using a *Convolutional Neural Network* (CNN)
    called ResNet-50\. The very first question we should ask is, “Where will I find
    the model?” Of course, we could hunt for it on the internet to find something
    that is compatible with our deep learning framework (Keras). *But ain’t nobody
    got time for that!* Luckily, Keras loves to make things easy and provides it to
    us in a single function call. After we call this function for the first time,
    the model will be downloaded from a remote server and cached locally:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是模型的时间。我们将使用一个名为ResNet-50的*卷积神经网络*（CNN）。我们应该问的第一个问题是：“我在哪里找到这个模型？”当然，我们可以在互联网上搜索，找到与我们的深度学习框架（Keras）兼容的内容。*但是没人有时间这样做！*幸运的是，Keras喜欢简化事情，并通过一个函数调用将其提供给我们。在第一次调用此函数后，模型将从远程服务器下载并在本地缓存：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When predicting with this model, the results include probability predictions
    for each class. Keras also provides the `decode_predictions` function, which tells
    us the probability of each category of objects contained in the image.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用这个模型进行预测时，结果包括每个类别的概率预测。Keras还提供了`decode_predictions`函数，告诉我们图像中包含的每个对象类别的概率。
- en: 'Now, let’s see the entire code in one handy function:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一个方便的函数中的整个代码：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The predicted categories for this image are various types of felines. Why doesn’t
    it simply predict the word “cat,” instead? The short answer is that the ResNet-50
    model was trained on a granular dataset with many categories and does not include
    the more general “cat.” We investigate this dataset in more detail a little later,
    but first, let’s load another sample image (see [Figure 2-2](part0004.html#plot_showing_the_contents_of_the_file_do)):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这幅图像的预测类别是各种类型的猫科动物。为什么它不简单地预测“猫”这个词呢？简短的答案是，ResNet-50模型是在一个包含许多类别的细粒度数据集上训练的，不包括更一般的“猫”。我们稍后会更详细地调查这个数据集，但首先让我们加载另一张样本图像（参见[图2-2](part0004.html#plot_showing_the_contents_of_the_file_do)）：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Plot showing the contents of the file dog.jpg](../images/00236.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![显示文件dog.jpg内容的图](../images/00236.jpeg)'
- en: Figure 2-2\. Plot showing the contents of the file dog.jpg
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2\. 显示文件dog.jpg内容的图
- en: 'And, again, we run our handy function from earlier:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们再次运行之前的便捷函数：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As expected, we get different breeds of canines (and not just the “dog” category).
    If you are unfamiliar with the Corgi breed of dogs, the word “corgi” literally
    means “dwarf dog” in Welsh. The Cardigan and Pembroke are subbreeds of the Corgi
    family, which happen to look pretty similar to one another. It’s no wonder our
    model thinks that way, too.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们得到了不同品种的犬类（不仅仅是“狗”类别）。如果你对柯基品种的狗不熟悉，那么“corgi”这个词在威尔士语中的意思就是“侏儒犬”。卡迪根和彭布罗克是柯基家族的亚品种，它们看起来非常相似。我们的模型也认为是这样，这一点也不奇怪。
- en: Notice the predicted probability of each category. Usually, the prediction with
    the highest probability is considered the answer. Alternatively, any value over
    a predefined threshold can be considered as the answer, too. In the dog example,
    if we set a threshold of 0.5, Cardigan would be our answer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意每个类别的预测概率。通常，具有最高概率的预测被认为是答案。或者，任何高于预定义阈值的值也可以被视为答案。在狗的例子中，如果我们设置阈值为0.5，那么卡迪根将是我们的答案。
- en: '![Running the notebook on Google Colab using the browser](../images/00183.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![在浏览器中使用Google Colab运行笔记本](../images/00183.jpeg)'
- en: Figure 2-3\. Running the notebook on Google Colab using the browser
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3\. 在浏览器中使用Google Colab运行笔记本
- en: Tip
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can follow along with the code in this chapter and execute it interactively
    without any installations in the browser itself with Google Colab. Simply find
    the “Run on Colab” link at the top of each notebook on GitHub that you’d like
    to experiment with. Then, click the “Run Cell” button; this should execute the
    code within that cell, as shown in [Figure 2-3](part0004.html#running_the_notebook_on_google_colab_usi).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本章中跟随代码并在浏览器中交互式地执行，而无需进行任何安装，只需使用Google Colab。只需在GitHub上每个您想要尝试的笔记本顶部找到“在Colab上运行”链接。然后，点击“运行单元格”按钮；这应该执行该单元格中的代码，如[图2-3](part0004.html#running_the_notebook_on_google_colab_usi)所示。
- en: Investigating the Model
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调查模型
- en: 'We got the predictions from our model, great! But what factors led to those
    predictions? There are a few questions that we need to ask here:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从模型中得到了预测，太棒了！但是是什么因素导致了这些预测？这里有一些问题我们需要问：
- en: What dataset was the model trained on?
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是在哪个数据集上训练的？
- en: Are there other models that I can use? How good are they? Where can I get them?
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我可以使用其他模型吗？它们有多好？我可以在哪里获取它们？
- en: Why does my model predict what it predicts?
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我的模型会做出这样的预测？
- en: We look into the answers to each of these questions in this section.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中探讨这些问题的答案。
- en: ImageNet Dataset
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageNet数据集
- en: Let’s investigate the ImageNet dataset on which ResNet-50 was trained. [ImageNet](http://www.image-net.org/),
    as the name suggests, is a network of images; that is, a dataset of images organized
    as a network, as demonstrated in [Figure 2-4](part0004.html#the_categories_and_subcategories_in_the).
    It is arranged in a hierarchical manner (like the WordNet hierarchy) such that
    the parent node encompasses a collection of images of all different varieties
    possible within that parent. For example, within the “animal” parent node, there
    are fish, birds, mammals, invertebrates, and so on. Each category has multiple
    subcategories, and these have subsubcategories, and so forth. For example, the
    category “American water spaniel” is eight levels from the root. The dog category
    contains 189 total subcategories in five hierarchical levels.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调查ResNet-50训练的ImageNet数据集。正如其名称所示，[ImageNet](http://www.image-net.org/)是一个图像网络；也就是说，这是一个以网络形式组织的图像数据集，如[图2-4](part0004.html#the_categories_and_subcategories_in_the)所示。它以分层方式排列（类似于WordNet层次结构），使得父节点包含该父节点内所有可能的各种图像的集合。例如，在“动物”父节点内，有鱼类、鸟类、哺乳动物、无脊椎动物等。每个类别都有多个子类别，这些子类别又有子子类别，依此类推。例如，“美国水猎犬”类别距离根节点有八个级别。狗类别包含了总共五个层次中的189个子类别。
- en: Visually, we developed the tree diagram shown in [Figure 2-5](part0004.html#tree_map_of_imagenet_and_its_classes)
    to help you to understand the wide variety of high-level entities that the ImageNet
    dataset contains. This treemap also shows the relative percentage of different
    categories that make up the ImageNet dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上看，我们制作了[图2-5](part0004.html#tree_map_of_imagenet_and_its_classes)中显示的树状图，以帮助您了解ImageNet数据集包含的各种高级实体。这个树状图还显示了构成ImageNet数据集的不同类别的相对百分比。
- en: '![The categories and subcategories in the ImageNet dataset](../images/00258.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![ImageNet数据集中的类别和子类别](../images/00258.jpeg)'
- en: Figure 2-4\. The categories and subcategories in the ImageNet dataset
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4\. ImageNet数据集中的类别和子类别
- en: '![Treemap of ImageNet and its classes](../images/00319.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![ImageNet及其类别的树状图](../images/00319.jpeg)'
- en: Figure 2-5\. Treemap of ImageNet and its classes
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5\. ImageNet及其类别的树状图
- en: The ImageNet dataset was the basis for the famous ILSVRC that started in 2010
    to benchmark progress in computer vision and challenge researchers to innovate
    on tasks including object classification. Recall from [Chapter 1](part0003.html#2RHM3-13fa565533764549a6f0ab7f11eed62b)
    that the ImageNet challenge saw submissions that drastically improved in accuracy
    each year. When it started out, the error rate was nearly 30%. And now, it is
    2.2%, already better than how an average human would perform at this task. This
    dataset and challenge are considered the single biggest reasons for the recent
    advancements in computer vision.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'Wait, AI has better-than-human accuracy? If the dataset was created by humans,
    won’t humans have 100% accuracy? Well, the dataset was created by experts, with
    each image verified by multiple people. Then Stanford researcher (and now of Tesla
    fame) Andrej Karpathy attempted to figure out how much a normal human would fare
    on ImageNet-1000\. Turns out he achieved an accuracy of 94.9%, well short of the
    100% we all expected. Andrej painstakingly spent a week going over 1,500 images,
    spending approximately one minute per image in tagging it. How did he misclassify
    5.1% of the images? The reasons are a bit subtle:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Fine-grained recognition
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: For many people, it is really tough to distinguish a Siberian husky from a Alaskan
    Malamute. Someone who is really familiar with dog breeds would be able to tell
    them apart because they look for finer-level details that distinguish both breeds.
    It turns out that neural networks are capable of learning those finer-level details
    much more easily than humans.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Category unawareness
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Not everyone is aware of all the 120 breeds of dogs and most certainly not each
    one of the 1,000 classes. But the AI is. After all, it was trained on it.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Similar to ImageNet, speech datasets like Switchboard report a 5.1% error rate
    for speech transcription (coincidentally the same number as ImageNet). It’s clear
    that humans have a limit, and AI is gradually beating us.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: One of the other key reasons for this fast pace of improvement was that researchers
    were openly sharing models trained on datasets like ImageNet. In the next section,
    we learn about model reuse in more detail.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Model Zoos
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A model zoo is a place where organizations or individuals can publicly upload
    models that they have built for others to reuse and improve upon. These models
    can be trained using any framework (e.g., Keras, TensorFlow, MXNet), for any task
    (classification, detection, etc.), or trained on any dataset (e.g., ImageNet,
    Street View House Numbers (SVHN)).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: The tradition of model zoos started with Caffe, one of the first deep learning
    frameworks, developed at the University of California, Berkeley. Training a deep
    learning model from scratch on a multimillion-image database requires weeks of
    training time and lots of GPU computational energy, making it a difficult task.
    The research community recognized this as a bottleneck, and the organizations
    that participated in the ImageNet competition open sourced their trained models
    on Caffe’s website. Other frameworks soon followed suit.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: When starting out on a new deep learning project, it’s a good idea to first
    explore whether there’s already a model that performs a similar task and was trained
    on a similar dataset.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: The [model zoo](https://keras.io/applications/) in Keras is a collection of
    various architectures trained using the Keras framework on the ImageNet dataset.
    We tabulate their details in [Table 2-1](part0004.html#architectural_details_of_select_pretrain).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. Architectural details of select pretrained ImageNet models
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Size** | **Top-1 accuracy** | **Top-5 accuracy** | **Parameters**
    | **Depth** |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '| VGG16 | 528 MB | 0.713 | 0.901 | 138,357,544 | 23 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '| VGG19 | 549 MB | 0.713 | 0.9 | 143,667,240 | 26 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: '| ResNet-50 | 98 MB | 0.749 | 0.921 | 25,636,712 | 50 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: '| ResNet-101 | 171 MB | 0.764 | 0.928 | 44,707,176 | 101 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
- en: '| ResNet-152 | 232 MB | 0.766 | 0.931 | 60,419,944 | 152 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
- en: '| InceptionV3 | 92 MB | 0.779 | 0.937 | 23,851,784 | 159 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
- en: '| InceptionResNetV2 | 215 MB | 0.803 | 0.953 | 55,873,736 | 572 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: '| NASNetMobile | 23 MB | 0.744 | 0.919 | 5,326,716 | — |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: '| NASNetLarge | 343 MB | 0.825 | 0.96 | 88,949,818 | — |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
- en: '| MobileNet | 16 MB | 0.704 | 0.895 | 4,253,864 | 88 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
- en: '| MobileNetV2 | 14 MB | 0.713 | 0.901 | 3,538,984 | 88 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
- en: 'The column “Top-1 accuracy” indicates how many times the best guess was the
    correct answer, and the column “Top-5 accuracy” indicates how many times at least
    one out of five guesses were correct. The “Depth” of the network indicates how
    many layers are present in the network. The “Parameters” column indicates the
    size of the model; that is, how many individual weights the model has: the more
    parameters, the “heavier” the model is, and the slower it is to make predictions.
    In this book, we often use ResNet-50 (the most common architecture cited in research
    papers for high accuracy) and MobileNet (for a good balance between speed, size,
    and accuracy).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Class Activation Maps
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image saliency, usually famous in UX research, is trying to answer the question
    “What part of the image are users paying attention to?” This is facilitated with
    the help of eye-tracking studies and represented in heatmaps. For example, big,
    bold fonts or people’s faces usually get more attention than backgrounds. It’s
    easy to guess how useful these heatmaps would be to designers and advertisers,
    who can then adapt their content to maximize users’ attention. Taking inspiration
    from this human version of saliency, wouldn’t it be great to learn which part
    of the image the neural network is paying attention to? That’s precisely what
    we will be experimenting with.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: In our experiment, we will be overlaying a *class activation map* (or colloquially
    a *heatmap*) on top of a video in order to understand what the network pays attention
    to. The heatmap tells us something like “In this picture, these pixels were responsible
    for the prediction of the class `dog` where “dog” was the category with the highest
    probability. The “hot” pixels are represented with warmer colors such as red,
    orange, and yellow, whereas the “cold” pixels are represented using blue. The
    “hotter” a pixel is, the higher the signal it provides toward the prediction.
    [Figure 2-6](part0004.html#original_image_of_a_dog_and_its_generate) gives us
    a clearer picture. (If you’re reading the print version, refer to the book’s GitHub
    for the original color image.)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![Original image of a dog and its generated heatmap](../images/00148.jpeg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. Original image of a dog and its generated heatmap
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the GitHub repository (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)),
    navigate to *code/chapter-2*. There, you’ll find a handy Jupyter notebook, *2-class-activation-map-on-video.ipynb,*
    which describes the following steps:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to install `keras-vis` using `pip`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then run the visualization script on a single image to generate the heatmap
    for it:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We should see a newly created file called *dog-output.jpg* that shows a side-by-side
    view of the original image and its heatmap. As we can see from [Figure 2-6](part0004.html#original_image_of_a_dog_and_its_generate),
    the right half of the image indicates the “areas of heat” along with the correct
    prediction of a “Cardigan” (i.e., Welsh Corgi).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Next, we want to visualize the heatmap for frames in a video. For that, we need
    `FFmpeg`, an open source multimedia framework. You can find the download binary
    as well as the installation instructions for your operating system at [*https://www.ffmpeg.org*](https://www.ffmpeg.org).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'We use `ffmpeg` to split up a video into individual frames (at 25 frames per
    second) and then run our visualization script on each of those frames. We must
    first create a directory to store these frames and pass its name as part of the
    `ffmpeg` command:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We then run the visualization script with the path of the directory containing
    the frames from the previous step:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We should see a newly created *kitchen-output* directory that contains all of
    the heatmaps for the frames from the input directory.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, compile a video from those frames using `ffmpeg`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Perfect! The result is the original video side by side with a copy of the heatmap
    overlaid on it. This is a useful tool, in particular, to discover whether the
    model has learned the correct features or if it picked up stray artifacts during
    its training.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Imagine generating heatmaps to analyze the strong points and shortfalls of our
    trained model or a pretrained model.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: You should try this experiment out on your own by shooting a video with your
    smartphone camera and running the aforementioned scripts on the file. Don’t forget
    to post your videos on Twitter, tagging [@PracticalDLBook](https://www.twitter.com/PracticalDLBook)!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Heatmaps are a great way to visually detect bias in the data. The quality of
    a model’s predictions depends heavily on the data on which it was trained. If
    the data is biased, that will reflect in the predictions. A great example of this
    is (although probably an urban legend) one in which the US Army wanted to use
    neural networks to detect enemy tanks camouflaged in trees.^([1](part0004.html#ch02fn2))
    The researchers who were building the model took photographs—50% containing camouflaged
    tanks and 50% with just trees. Model training yielded 100% accuracy. A cause for
    celebration? That sadly wasn’t the case when the US Army tested it. The model
    had performed very poorly—no better than random guesses. Investigation revealed
    that photos with the tanks were taken on cloudy (overcast) days and those without
    the tanks on clear, sunny days. And the neural network model began looking for
    the sky instead of the tank. If the researchers had visualized the model using
    heatmaps, they would have caught that issue pretty early.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: As we collect data, we must be vigilant at the outset of potential bias that
    can pollute our model’s learning. For example, when collecting images to build
    a food classifier, we should verify that the other artifacts such as plates and
    utensils are not being learned as food. Otherwise, the presence of chopsticks
    might get our food classified as chow mein. Another term to define this is *co-occurrence*.
    Food very frequently co-occurs with cutlery. So watch out for these artifacts
    seeping into your classifier’s training.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we got a glimpse of the deep learning universe using Keras.
    It’s an easy-to-use yet powerful framework that we use in the next several chapters.
    We observed that there is often no need to collect millions of images and use
    powerful GPUs to train a custom model because we can use a pretrained model to
    predict the category of an image. By diving deeper into datasets like ImageNet,
    we learned the kinds of categories these pretrained models can predict. We also
    learned about finding these models in model zoos that exist for most frameworks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b), we explore
    how we can tweak an existing pretrained model to make predictions on classes of
    input for which it was not originally intended. As with the current chapter, our
    approach is geared toward obtaining output without needing millions of images
    and lots of hardware resources to train a classifier.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](part0004.html#ch02fn2-marker)) [“Artificial Intelligence as a Positive
    and Negative Factor in Global Risk”](https://oreil.ly/-svD0) by Eliezer Yudkowsky
    in *Global Catastrophic Risks* (Oxford University Press).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
