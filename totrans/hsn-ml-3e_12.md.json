["```py\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import Perceptron\n\niris = load_iris(as_frame=True)\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = (iris.target == 0)  # Iris setosa\n\nper_clf = Perceptron(random_state=42)\nper_clf.fit(X, y)\n\nX_new = [[2, 0.5], [3, 1]]\ny_pred = per_clf.predict(X_new)  # predicts True and False for these 2 flowers\n```", "```py\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nhousing = fetch_california_housing()\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    housing.data, housing.target, random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_train_full, y_train_full, random_state=42)\n\nmlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], random_state=42)\npipeline = make_pipeline(StandardScaler(), mlp_reg)\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_valid)\nrmse = mean_squared_error(y_valid, y_pred, squared=False)  # about 0.505\n```", "```py\nimport tensorflow as tf\n\nfashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\nX_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\nX_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n```", "```py\n>>> X_train.shape\n(55000, 28, 28)\n>>> X_train.dtype\ndtype('uint8')\n```", "```py\nX_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255.\n```", "```py\nclass_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n```", "```py\n>>> class_names[y_train[0]]\n'Ankle boot'\n```", "```py\ntf.random.set_seed(42)\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Input(shape=[28, 28]))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(300, activation=\"relu\"))\nmodel.add(tf.keras.layers.Dense(100, activation=\"relu\"))\nmodel.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n```", "```py\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=[28, 28]),\n    tf.keras.layers.Dense(300, activation=\"relu\"),\n    tf.keras.layers.Dense(100, activation=\"relu\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\n```", "```py\n>>> model.summary()\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #\n=================================================================\n flatten (Flatten)           (None, 784)               0\n\n dense (Dense)               (None, 300)               235500\n\n dense_1 (Dense)             (None, 100)               30100\n\n dense_2 (Dense)             (None, 10)                1010\n\n=================================================================\nTotal params: 266,610\nTrainable params: 266,610\nNon-trainable params: 0\n_________________________________________________________________\n```", "```py\n>>> model.layers\n[<keras.layers.core.flatten.Flatten at 0x7fa1dea02250>,\n <keras.layers.core.dense.Dense at 0x7fa1c8f42520>,\n <keras.layers.core.dense.Dense at 0x7fa188be7ac0>,\n <keras.layers.core.dense.Dense at 0x7fa188be7fa0>]\n>>> hidden1 = model.layers[1]\n>>> hidden1.name\n'dense'\n>>> model.get_layer('dense') is hidden1\nTrue\n```", "```py\n>>> weights, biases = hidden1.get_weights()\n>>> weights\narray([[ 0.02448617, -0.00877795, -0.02189048, ...,  0.03859074, -0.06889391],\n [ 0.00476504, -0.03105379, -0.0586676 , ..., -0.02763776, -0.04165364],\n ...,\n [ 0.07061854, -0.06960931,  0.07038955, ..., 0.00034875,  0.02878492],\n [-0.06022581,  0.01577859, -0.02585464, ..., 0.00272203, -0.06793761]],\n dtype=float32)\n>>> weights.shape\n(784, 300)\n>>> biases\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., ...,  0., 0., 0.], dtype=float32)\n>>> biases.shape\n(300,)\n```", "```py\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"sgd\",\n              metrics=[\"accuracy\"])\n```", "```py\n>>> history = model.fit(X_train, y_train, epochs=30,\n...                     validation_data=(X_valid, y_valid))\n...\nEpoch 1/30\n1719/1719 [==============================] - 2s 989us/step\n - loss: 0.7220 - sparse_categorical_accuracy: 0.7649\n - val_loss: 0.4959 - val_sparse_categorical_accuracy: 0.8332\nEpoch 2/30\n1719/1719 [==============================] - 2s 964us/step\n - loss: 0.4825 - sparse_categorical_accuracy: 0.8332\n - val_loss: 0.4567 - val_sparse_categorical_accuracy: 0.8384\n[...]\nEpoch 30/30\n1719/1719 [==============================] - 2s 963us/step\n - loss: 0.2235 - sparse_categorical_accuracy: 0.9200\n - val_loss: 0.3056 - val_sparse_categorical_accuracy: 0.8894\n```", "```py\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\npd.DataFrame(history.history).plot(\n    figsize=(8, 5), xlim=[0, 29], ylim=[0, 1], grid=True, xlabel=\"Epoch\",\n    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])\nplt.show()\n```", "```py\n>>> model.evaluate(X_test, y_test)\n313/313 [==============================] - 0s 626us/step\n - loss: 0.3243 - sparse_categorical_accuracy: 0.8864\n[0.32431697845458984, 0.8863999843597412]\n```", "```py\n>>> X_new = X_test[:3]\n>>> y_proba = model.predict(X_new)\n>>> y_proba.round(2)\narray([[0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0.01, 0\\.  , 0.02, 0\\.  , 0.97],\n [0\\.  , 0\\.  , 0.99, 0\\.  , 0.01, 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  ],\n [0\\.  , 1\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  ]],\n dtype=float32)\n```", "```py\n>>> import numpy as np\n>>> y_pred = y_proba.argmax(axis=-1)\n>>> y_pred\narray([9, 2, 1])\n>>> np.array(class_names)[y_pred]\narray(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')\n```", "```py\n>>> y_new = y_test[:3]\n>>> y_new\narray([9, 2, 1], dtype=uint8)\n```", "```py\ntf.random.set_seed(42)\nnorm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\nmodel = tf.keras.Sequential([\n    norm_layer,\n    tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dense(1)\n])\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\nmodel.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\nnorm_layer.adapt(X_train)\nhistory = model.fit(X_train, y_train, epochs=20,\n                    validation_data=(X_valid, y_valid))\nmse_test, rmse_test = model.evaluate(X_test, y_test)\nX_new = X_test[:3]\ny_pred = model.predict(X_new)\n```", "```py\nnormalization_layer = tf.keras.layers.Normalization()\nhidden_layer1 = tf.keras.layers.Dense(30, activation=\"relu\")\nhidden_layer2 = tf.keras.layers.Dense(30, activation=\"relu\")\nconcat_layer = tf.keras.layers.Concatenate()\noutput_layer = tf.keras.layers.Dense(1)\n\ninput_ = tf.keras.layers.Input(shape=X_train.shape[1:])\nnormalized = normalization_layer(input_)\nhidden1 = hidden_layer1(normalized)\nhidden2 = hidden_layer2(hidden1)\nconcat = concat_layer([normalized, hidden2])\noutput = output_layer(concat)\n\nmodel = tf.keras.Model(inputs=[input_], outputs=[output])\n```", "```py\ninput_wide = tf.keras.layers.Input(shape=[5])  # features 0 to 4\ninput_deep = tf.keras.layers.Input(shape=[6])  # features 2 to 7\nnorm_layer_wide = tf.keras.layers.Normalization()\nnorm_layer_deep = tf.keras.layers.Normalization()\nnorm_wide = norm_layer_wide(input_wide)\nnorm_deep = norm_layer_deep(input_deep)\nhidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(norm_deep)\nhidden2 = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1)\nconcat = tf.keras.layers.concatenate([norm_wide, hidden2])\noutput = tf.keras.layers.Dense(1)(concat)\nmodel = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[output])\n```", "```py\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\nmodel.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n\nX_train_wide, X_train_deep = X_train[:, :5], X_train[:, 2:]\nX_valid_wide, X_valid_deep = X_valid[:, :5], X_valid[:, 2:]\nX_test_wide, X_test_deep = X_test[:, :5], X_test[:, 2:]\nX_new_wide, X_new_deep = X_test_wide[:3], X_test_deep[:3]\n\nnorm_layer_wide.adapt(X_train_wide)\nnorm_layer_deep.adapt(X_train_deep)\nhistory = model.fit((X_train_wide, X_train_deep), y_train, epochs=20,\n                    validation_data=((X_valid_wide, X_valid_deep), y_valid))\nmse_test = model.evaluate((X_test_wide, X_test_deep), y_test)\ny_pred = model.predict((X_new_wide, X_new_deep))\n```", "```py\n[...]  # Same as above, up to the main output layer\noutput = tf.keras.layers.Dense(1)(concat)\naux_output = tf.keras.layers.Dense(1)(hidden2)\nmodel = tf.keras.Model(inputs=[input_wide, input_deep],\n                       outputs=[output, aux_output])\n```", "```py\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\nmodel.compile(loss=(\"mse\", \"mse\"), loss_weights=(0.9, 0.1), optimizer=optimizer,\n              metrics=[\"RootMeanSquaredError\"])\n```", "```py\nnorm_layer_wide.adapt(X_train_wide)\nnorm_layer_deep.adapt(X_train_deep)\nhistory = model.fit(\n    (X_train_wide, X_train_deep), (y_train, y_train), epochs=20,\n    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid))\n)\n```", "```py\neval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))\nweighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse = eval_results\n```", "```py\ny_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))\n```", "```py\ny_pred_tuple = model.predict((X_new_wide, X_new_deep))\ny_pred = dict(zip(model.output_names, y_pred_tuple))\n```", "```py\nclass WideAndDeepModel(tf.keras.Model):\n    def __init__(self, units=30, activation=\"relu\", **kwargs):\n        super().__init__(**kwargs)  # needed to support naming the model\n        self.norm_layer_wide = tf.keras.layers.Normalization()\n        self.norm_layer_deep = tf.keras.layers.Normalization()\n        self.hidden1 = tf.keras.layers.Dense(units, activation=activation)\n        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)\n        self.main_output = tf.keras.layers.Dense(1)\n        self.aux_output = tf.keras.layers.Dense(1)\n\n    def call(self, inputs):\n        input_wide, input_deep = inputs\n        norm_wide = self.norm_layer_wide(input_wide)\n        norm_deep = self.norm_layer_deep(input_deep)\n        hidden1 = self.hidden1(norm_deep)\n        hidden2 = self.hidden2(hidden1)\n        concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n        output = self.main_output(concat)\n        aux_output = self.aux_output(hidden2)\n        return output, aux_output\n\nmodel = WideAndDeepModel(30, activation=\"relu\", name=\"my_cool_model\")\n```", "```py\nmodel.save(\"my_keras_model\", save_format=\"tf\")\n```", "```py\nmodel = tf.keras.models.load_model(\"my_keras_model\")\ny_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))\n```", "```py\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_checkpoints\",\n                                                   save_weights_only=True)\nhistory = model.fit([...], callbacks=[checkpoint_cb])\n```", "```py\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n                                                     restore_best_weights=True)\nhistory = model.fit([...], callbacks=[checkpoint_cb, early_stopping_cb])\n```", "```py\nclass PrintValTrainRatioCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs):\n        ratio = logs[\"val_loss\"] / logs[\"loss\"]\n        print(f\"Epoch={epoch}, val/train={ratio:.2f}\")\n```", "```py\n%pip install -q -U tensorboard-plugin-profile\n```", "```py\nfrom pathlib import Path\nfrom time import strftime\n\ndef get_run_logdir(root_logdir=\"my_logs\"):\n    return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n\nrun_logdir = get_run_logdir()  # e.g., my_logs/run_2022_08_01_17_25_59\n```", "```py\ntensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir,\n                                                profile_batch=(100, 200))\nhistory = model.fit([...], callbacks=[tensorboard_cb])\n```", "```py\nmy_logs\n\u251c\u2500\u2500 run_2022_08_01_17_25_59\n\u2502   \u251c\u2500\u2500 train\n\u2502   \u2502   \u251c\u2500\u2500 events.out.tfevents.1659331561.my_host_name.42042.0.v2\n\u2502   \u2502   \u251c\u2500\u2500 events.out.tfevents.1659331562.my_host_name.profile-empty\n\u2502   \u2502   \u2514\u2500\u2500 plugins\n\u2502   \u2502       \u2514\u2500\u2500 profile\n\u2502   \u2502           \u2514\u2500\u2500 2022_08_01_17_26_02\n\u2502   \u2502               \u251c\u2500\u2500 my_host_name.input_pipeline.pb\n\u2502   \u2502               \u2514\u2500\u2500 [...]\n\u2502   \u2514\u2500\u2500 validation\n\u2502       \u2514\u2500\u2500 events.out.tfevents.1659331562.my_host_name.42042.1.v2\n\u2514\u2500\u2500 run_2022_08_01_17_31_12\n    \u2514\u2500\u2500 [...]\n\n```", "```py\n%load_ext tensorboard\n%tensorboard --logdir=./my_logs\n```", "```py\ntest_logdir = get_run_logdir()\nwriter = tf.summary.create_file_writer(str(test_logdir))\nwith writer.as_default():\n    for step in range(1, 1000 + 1):\n        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n\n        data = (np.random.randn(100) + 2) * step / 100  # gets larger\n        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n\n        images = np.random.rand(2, 32, 32, 3) * step / 1000  # gets brighter\n        tf.summary.image(\"my_images\", images, step=step)\n\n        texts = [\"The step is \" + str(step), \"Its square is \" + str(step ** 2)]\n        tf.summary.text(\"my_text\", texts, step=step)\n\n        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)\n```", "```py\nimport keras_tuner as kt\n\ndef build_model(hp):\n    n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2)\n    n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=256)\n    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2,\n                             sampling=\"log\")\n    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n    if optimizer == \"sgd\":\n        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n    else:\n        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Flatten())\n    for _ in range(n_hidden):\n        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n                  metrics=[\"accuracy\"])\n    return model\n```", "```py\nrandom_search_tuner = kt.RandomSearch(\n    build_model, objective=\"val_accuracy\", max_trials=5, overwrite=True,\n    directory=\"my_fashion_mnist\", project_name=\"my_rnd_search\", seed=42)\nrandom_search_tuner.search(X_train, y_train, epochs=10,\n                           validation_data=(X_valid, y_valid))\n```", "```py\ntop3_models = random_search_tuner.get_best_models(num_models=3)\nbest_model = top3_models[0]\n```", "```py\n>>> top3_params = random_search_tuner.get_best_hyperparameters(num_trials=3)\n>>> top3_params[0].values  # best hyperparameter values\n{'n_hidden': 5,\n 'n_neurons': 70,\n 'learning_rate': 0.00041268008323824807,\n 'optimizer': 'adam'}\n```", "```py\n>>> best_trial = random_search_tuner.oracle.get_best_trials(num_trials=1)[0]\n>>> best_trial.summary()\nTrial summary\nHyperparameters:\nn_hidden: 5\nn_neurons: 70\nlearning_rate: 0.00041268008323824807\noptimizer: adam\nScore: 0.8736000061035156\n```", "```py\n>>> best_trial.metrics.get_last_value(\"val_accuracy\")\n0.8736000061035156\n```", "```py\nbest_model.fit(X_train_full, y_train_full, epochs=10)\ntest_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n```", "```py\nclass MyClassificationHyperModel(kt.HyperModel):\n    def build(self, hp):\n        return build_model(hp)\n\n    def fit(self, hp, model, X, y, **kwargs):\n        if hp.Boolean(\"normalize\"):\n            norm_layer = tf.keras.layers.Normalization()\n            X = norm_layer(X)\n        return model.fit(X, y, **kwargs)\n```", "```py\nhyperband_tuner = kt.Hyperband(\n    MyClassificationHyperModel(), objective=\"val_accuracy\", seed=42,\n    max_epochs=10, factor=3, hyperband_iterations=2,\n    overwrite=True, directory=\"my_fashion_mnist\", project_name=\"hyperband\")\n```", "```py\nroot_logdir = Path(hyperband_tuner.project_dir) / \"tensorboard\"\ntensorboard_cb = tf.keras.callbacks.TensorBoard(root_logdir)\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=2)\nhyperband_tuner.search(X_train, y_train, epochs=10,\n                       validation_data=(X_valid, y_valid),\n                       callbacks=[early_stopping_cb, tensorboard_cb])\n```", "```py\nbayesian_opt_tuner = kt.BayesianOptimization(\n    MyClassificationHyperModel(), objective=\"val_accuracy\", seed=42,\n    max_trials=10, alpha=1e-4, beta=2.6,\n    overwrite=True, directory=\"my_fashion_mnist\", project_name=\"bayesian_opt\")\nbayesian_opt_tuner.search([...])\n```"]