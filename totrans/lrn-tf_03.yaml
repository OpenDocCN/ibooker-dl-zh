- en: Chapter 3\. Understanding TensorFlow Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter demonstrates the key concepts of how TensorFlow is built and how
    it works with simple and intuitive examples. You will get acquainted with the
    basics of TensorFlow as a numerical computation library using dataflow graphs.
    More specifically, you will learn how to manage and create a graph, and be introduced
    to TensorFlow’s “building blocks,” such as constants, placeholders, and Variables.
  prefs: []
  type: TYPE_NORMAL
- en: Computation Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow allows us to implement machine learning algorithms by creating and
    computing operations that interact with one another. These interactions form what
    we call a “computation graph,” with which we can intuitively represent complicated
    functional architectures.
  prefs: []
  type: TYPE_NORMAL
- en: What Is a Computation Graph?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We assume a lot of readers have already come across the mathematical concept
    of a graph. For those to whom this concept is new, a graph refers to a set of
    interconnected *entities*, commonly called *nodes* or *vertices*. These nodes
    are connected to each other via edges. In a dataflow graph, the edges allow data
    to “flow” from one node to another in a directed manner.
  prefs: []
  type: TYPE_NORMAL
- en: In TensorFlow, each of the graph’s nodes represents an operation, possibly applied
    to some input, and can generate an output that is passed on to other nodes. By
    analogy, we can think of the graph computation as an assembly line where each
    machine (node) either gets or creates its raw material (input), processes it,
    and then passes the output to other machines in an orderly fashion, producing
    *subcomponents* and eventually a final *product* when the assembly process comes
    to an end.
  prefs: []
  type: TYPE_NORMAL
- en: Operations in the graph include all kinds of functions, from simple arithmetic
    ones such as subtraction and multiplication to more complex ones, as we will see
    later on. They also include more general operations like the creation of summaries,
    generating constant values, and more.
  prefs: []
  type: TYPE_NORMAL
- en: The Benefits of Graph Computations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow optimizes its computations based on the graph’s connectivity.​ Each
    graph has its own set of node dependencies. When the input of node `y` is affected
    by the output of node `x`, we say that node `y` is dependent on node `x`. We call
    it a *direct dependency* when the two are connected via an edge, and an *indirect
    dependency* otherwise. For example, in [Figure 3-1](#graph_dependencies) (A),
    node `e` is directly dependent on node `c`, indirectly dependent on node `a`, and
    independent of node `d`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. (A) Illustration of graph dependencies. (B) Computing node e results
    in the minimal amount of computations according to the graph’s dependencies—in
    this case computing only nodes c, b, and a.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can always identify the full set of dependencies for each node in the graph. This
    is a fundamental characteristic of the graph-based computation format. Being able
    to locate dependencies between units of our model allows us to both distribute
    computations across available resources and avoid performing redundant computations
    of irrelevant subsets, resulting in a faster and more efficient way of computing
    things.
  prefs: []
  type: TYPE_NORMAL
- en: Graphs, Sessions, and Fetches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Roughly speaking, working with TensorFlow involves two main phases: (1) constructing
    a graph and (2) executing it. Let’s jump into our first example and create something
    very basic.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Right after we import TensorFlow (with `import tensorflow as tf`), a specific
    empty default graph is formed. All the nodes we create are automatically associated
    with that default graph.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `tf.<*operator*>` methods, we will create six nodes assigned to arbitrarily
    named variables. The contents of these variables should be regarded as the output
    of the operations, and not the operations themselves. For now we refer to both
    the operations and their outputs with the names of their corresponding variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first three nodes are each told to output a constant value. The values
    `5`, `2`, and `3` are assigned to `a`, `b`, and `c`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Each of the next three nodes gets two existing variables as inputs, and performs
    simple arithmetic operations on them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Node `d` multiplies the outputs of nodes `a` and `b`. Node `e` adds the outputs
    of nodes `b` and `c`. Node `f` subtracts the output of node `e` from that of node
    `d`.
  prefs: []
  type: TYPE_NORMAL
- en: And *voilà*! We have our first TensorFlow graph! [Figure 3-2](#our_first_constructed_graph)
    shows an illustration of the graph we’ve just created.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-2\. An illustration of our first constructed graph. Each node, denoted
    by a lowercase letter, performs the operation indicated above it: Const for creating
    constants and Add, Mul, and Sub for addition, multiplication, and subtraction,
    respectively. The integer next to each edge is the output of the corresponding
    node’s operation.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that for some arithmetic and logical operations it is possible to use operation
    shortcuts instead of having to apply `tf.<*operator*>`. For example, in this graph
    we could have used `*`/+/`-` instead of `tf.multiply()`/`tf.add()`/`tf.subtract()`
    (like we did in the “hello world” example in [Chapter 2](ch02.html#go_with_the_flow),
    where we used + instead of `tf.add()`). [Table 3-1](#table0301) lists the available
    shortcuts.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-1\. Common TensorFlow operations and their respective shortcuts
  prefs: []
  type: TYPE_NORMAL
- en: '| TensorFlow operator | Shortcut | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.add()` | `a + b` | Adds `a` and `b`, element-wise. |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.multiply()` | `a * b` | Multiplies `a` and `b`, element-wise. |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.subtract()` | `a - b` | Subtracts `b` from `a`, element-wise. |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.divide()` | `a / b` | Computes Python-style division of `a` by `b`. |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.pow()` | `a ** b` | Returns the result of raising each element in `a`
    to its corresponding element `b`, element-wise. |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.mod()` | `a % b` | Returns the element-wise modulo. |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.logical_and()` | `a & b` | Returns the truth table of `a & b`, element-wise.
    `dtype` must be `tf.bool`. |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.greater()` | `a > b` | Returns the truth table of `a > b`, element-wise.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.greater_equal()` | `a >= b` | Returns the truth table of `a >= b`, element-wise.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.less_equal()` | `a <= b` | Returns the truth table of `a <= b`, element-wise.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.less()` | `a < b` | Returns the truth table of `a < b`, element-wise.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.negative()` | `-a` | Returns the negative value of each element in `a`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.logical_not()` | `~a` | Returns the logical NOT of each element in `a`.
    Only compatible with Tensor objects with `dtype` of `tf.bool`. |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.abs()` | `abs(a)` | Returns the absolute value of each element in `a`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.logical_or()` | `a &#124; b` | Returns the truth table of `a &#124; b`,
    element-wise. `dtype` must be `tf.bool`. |'
  prefs: []
  type: TYPE_TB
- en: Creating a Session and Running It
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we are done describing the computation graph, we are ready to run the
    computations that it represents. For this to happen, we need to create and run
    a session. We do this by adding the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: First, we launch the graph in a `tf.Session`. A `Session` object is the part
    of the TensorFlow API that communicates between Python objects and data on our
    end, and the actual computational system where memory is allocated for the objects
    we define, intermediate variables are stored, and finally results are fetched
    for us.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The execution itself is then done with the `.run()` method of the `Session`
    object. When called, this method completes one set of computations in our graph
    in the following manner: it starts at the requested output(s) and then works backward,
    computing nodes that must be executed according to the set of dependencies. Therefore, the
    part of the graph that will be computed depends on our output query.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we requested that node `f` be computed and got its value, `5`, as
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When our computation task is completed, it is good practice to close the session
    using the `sess.close()` command, making sure the resources used by our session
    are freed up. This is an important practice to maintain even though we are not
    obligated to do so for things to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Example 3-1\. Try it yourself! [Figure 3-3](#fig0303) shows another two graph
    examples. See if you can produce these graphs yourself.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](assets/letf_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Can you create graphs A and B?  (To produce the sine function,
    use tf.sin(x)).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Constructing and Managing Our Graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned, as soon as we import TensorFlow, a default graph is automatically
    created for us. We can create additional graphs and control their association
    with some given operations. `tf.Graph()` creates a new graph, represented as a
    TensorFlow object. In this example we create another graph and assign it to the
    variable `g`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: At this point we have two graphs: the default graph and the empty graph in `g`.
    Both are revealed as TensorFlow objects when printed. Since `g` hasn’t been assigned
    as the default graph, any operation we create will not be associated with it,
    but rather with the default one.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check which graph is currently set as the default by using `tf.get_default_graph()`.
    Also, for a given node, we can view the graph it’s associated with by using the
    `*<node>*.graph` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this code example we see that the operation we’ve created is associated with
    the default graph and not with the graph in `g`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sure our constructed nodes are associated with the right graph we can
    construct them using a very useful Python construct: the `with` statement.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The with statement**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `with` statement is used to wrap the execution of a block with methods defined
    by a context manager—an object that has the special method functions `.__enter__()`
    to set up a block of code and `.__exit__()` to exit the block.
  prefs: []
  type: TYPE_NORMAL
- en: In layman’s terms, it’s very convenient in many cases to execute some code that
    requires “setting up” of some kind (like opening a file, SQL table, etc.) and
    then always “tearing it down” at the end, regardless of whether the code ran well
    or raised any kind of exception. In our case we use `with` to set up a graph and
    make sure every piece of code will be performed in the context of that graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `with` statement together with the `as_default()` command, which
    returns a context manager that makes this graph the default one. This comes in
    handy when working with multiple graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `with` statement can also be used to start a session without having to explicitly
    close it. This convenient trick will be used in the following examples.
  prefs: []
  type: TYPE_NORMAL
- en: Fetches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our initial graph example, we request one specific node (node `f`) by passing
    the variable it was assigned to as an argument to the `sess.run()` method. This
    argument is called `fetches`, corresponding to the elements of the graph we wish
    to compute. We can also ask `sess.run()` for multiple nodes’ outputs simply by
    inputting a list of requested nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We get back a list containing the outputs of the nodes according to how they
    were ordered in the input list. The data in each item of the list is of type NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NumPy is a popular and useful Python package for numerical computing that offers
    many functionalities related to working with arrays. We assume some basic familiarity
    with this package, and it will not be covered in this book. TensorFlow and NumPy
    are tightly coupled—for example, the output returned by `sess.run()` is a NumPy
    array. In addition, many of TensorFlow’s operations share the same syntax as functions
    in NumPy. To learn more about NumPy, we refer the reader to Eli Bressert’s book
    [*SciPy and NumPy*](http://shop.oreilly.com/product/0636920020219.do) (O’Reilly).
  prefs: []
  type: TYPE_NORMAL
- en: 'We mentioned that TensorFlow computes only the essential nodes according to
    the set of dependencies. This is also manifested in our example: when we ask for
    the output of node `d`, only the outputs of nodes `a` and `b` are computed. Another
    example is shown in [Figure 3-1(B)](#graph_dependencies). This is a great advantage
    of TensorFlow—it doesn’t matter how big and complicated our graph is as a whole,
    since we can run just a small portion of it as needed.'
  prefs: []
  type: TYPE_NORMAL
- en: Automatically closing the session
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Opening a session using the `with` clause will ensure the session is automatically
    closed once all computations are done.
  prefs: []
  type: TYPE_NORMAL
- en: Flowing Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section we will get a better understanding of how nodes and edges are
    actually represented in TensorFlow, and how we can control their characteristics.
    To demonstrate how they work, we will focus on source operations, which are used
    to initialize values.
  prefs: []
  type: TYPE_NORMAL
- en: Nodes Are Operations, Edges Are Tensor Objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we construct a node in the graph, like we did with `tf.add()`, we are actually
    creating an operation instance. These operations do not produce actual values
    until the graph is executed, but rather reference their to-be-computed result
    as a handle that can be passed on—*flow*—to another node. These handles, which
    we can think of as the edges in our graph, are referred to as Tensor objects,
    and this is where the name TensorFlow originates from.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow is designed such that first a skeleton graph is created with all
    of its components. At this point no actual data flows in it and no computations
    take place. It is only upon execution, when we run the session, that data enters
    the graph and computations occur (as illustrated in [Figure 3-4](#pre_and_post_running_a_section)).
    This way, computations can be much more efficient, taking the entire graph structure
    into consideration.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Illustrations of before (A) and after (B) running a session. When
    the session is run, actual data “flows” through the graph.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the previous section’s example, `tf.constant()` created a node with the corresponding
    passed value. Printing the output of the constructor, we see that it’s actually
    a Tensor object instance. These objects have methods and attributes that control
    their behavior and that can be defined upon creation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, the variable `c` stores a Tensor object with the name `Const_52:0`,
    designated to contain a 32-bit floating-point scalar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: A note on constructors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `tf.*<operator>*` function could be thought of as a constructor, but to
    be more precise, this is actually not a constructor at all, but rather a factory
    method that sometimes does quite a bit more than just creating the operator objects.
  prefs: []
  type: TYPE_NORMAL
- en: Setting attributes with source operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each Tensor object in TensorFlow has attributes such as `name`, `shape`, and
    `dtype` that help identify and set the characteristics of that object. These attributes
    are optional when creating a node, and are set automatically by TensorFlow when
    missing. In the next section we will take a look at these attributes. We will
    do so by looking at Tensor objects created by ops known as *source operations*.
    Source operations are operations that create data, usually without using any previously
    processed inputs. With these operations we can create scalars, as we already encountered
    with the `tf.constant()` method, as well as arrays and other types of data.
  prefs: []
  type: TYPE_NORMAL
- en: Data Types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The basic units of data that pass through a graph are numerical, Boolean, or
    string elements.  When we print out the Tensor object `c ` from our last code
    example, we see that its data type is a floating-point number. Since we didn’t
    specify the type of data, TensorFlow inferred it automatically. For example `5`
    is regarded as an integer, while anything with a decimal point, like `5.1`, is
    regarded as a floating-point number.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can explicitly choose what data type we want to work with by specifying
    it when we create the Tensor object. We can see what type of data was set for
    a given Tensor object by using the attribute `dtype`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Explicitly asking for (appropriately sized) integers is on the one hand more
    memory conserving, but on the other may result in reduced accuracy as a consequence
    of not tracking digits after the decimal point.
  prefs: []
  type: TYPE_NORMAL
- en: Casting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is important to make sure our data types match throughout the graph—performing
    an operation with two nonmatching data types will result in an exception. To change
    the data type setting of a Tensor object, we can use the `tf.cast()` operation,
    passing the relevant Tensor and the new data type of interest as the first and
    second arguments, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow supports many data types. These are listed in [Table 3-2](#table0302).
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-2\. Supported Tensor data types
  prefs: []
  type: TYPE_NORMAL
- en: '| Data type | Python type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_FLOAT` | `tf.float32` | 32-bit floating point. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_DOUBLE` | `tf.float64` | 64-bit floating point. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_INT8` | `tf.int8` | 8-bit signed integer. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_INT16` | `tf.int16` | 16-bit signed integer. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_INT32` | `tf.int32` | 32-bit signed integer. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_INT64` | `tf.int64` | 64-bit signed integer. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_UINT8` | `tf.uint8` | 8-bit unsigned integer. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_UINT16` | `tf.uint16` | 16-bit unsigned integer. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_STRING` | `tf.string` | Variable-length byte array. Each element of a
    Tensor is a byte array. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_BOOL` | `tf.bool` | Boolean. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_COMPLEX64` | `tf.complex64` | Complex number made of two 32-bit floating
    points: real and imaginary parts. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_COMPLEX128` | `tf.complex128` | Complex number made of two 64-bit floating
    points: real and imaginary parts. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_QINT8` | `tf.qint8` | 8-bit signed integer used in quantized ops. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_QINT32` | `tf.qint32` | 32-bit signed integer used in quantized ops.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_QUINT8` | `tf.quint8` | 8-bit unsigned integer used in quantized ops.
      |'
  prefs: []
  type: TYPE_TB
- en: Tensor Arrays and Shapes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A source of potential confusion is that two different things are referred to
    by the name, *Tensor*. As used in the previous sections, *Tensor* is the name
    of an object used in the Python API as a handle for the result of an operation
    in the graph. However, *tensor* is also a mathematical term for *n*-dimensional
    arrays. For example, a 1×1 tensor is a scalar, a 1×*n* tensor is a vector, an
    *n*×*n* tensor is a matrix, and an *n*×*n*×*n* tensor is just a three-dimensional
    array. This, of course, generalizes to any dimension. TensorFlow regards all the
    data units that flow in the graph as tensors, whether they are multidimensional
    arrays, vectors, matrices, or scalars. The TensorFlow objects called Tensors are
    named after these mathematical tensors.
  prefs: []
  type: TYPE_NORMAL
- en: To clarify the distinction between the two, from now on we will refer to the
    former as Tensors with a capital T and the latter as tensors with a lowercase
    t.
  prefs: []
  type: TYPE_NORMAL
- en: As with `dtype`, unless stated explicitly, TensorFlow automatically infers the
    shape of the data. When we printed out the Tensor object at the beginning of this
    section, it showed that its shape was `()`, corresponding to the shape of a scalar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using scalars is good for demonstration purposes, but most of the time it’s
    much more practical to work with multidimensional arrays. To initialize high-dimensional
    arrays, we can use Python lists or NumPy arrays as inputs. In the following example,
    we use as inputs a 2×3 matrix using a Python list and then a 3D NumPy array of
    size 2×3×4 (two matrices of size 3×4):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `get_shape()` method returns the shape of the tensor as a tuple of integers.
    The number of integers corresponds to the number of dimensions of the tensor,
    and each integer is the number of array entries along that dimension. For example,
    a shape of `(2,3)` indicates a matrix, since it has two integers, and the size
    of the matrix is 2×3.
  prefs: []
  type: TYPE_NORMAL
- en: Other types of source operation constructors are very useful for initializing
    constants in TensorFlow, like filling a constant value, generating random numbers,
    and creating sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Random-number generators have special importance as they are used in many cases
    to create the initial values for TensorFlow Variables, which will be introduced
    shortly. For example, we can generate random numbers from a *normal distribution*
    using `tf.random.normal()`, passing the shape, mean, and standard deviation as
    the first, second, and third arguments, respectively. Another two examples for
    useful random initializers are the *truncated normal* that, as its name implies,
    cuts off all values below and above two standard deviations from the mean, and
    the *uniform* initializer that samples values uniformly within some interval `[a,b)`.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of sampled values for each of these methods are shown in [Figure 3-5](#random_samples).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. 50,000 random samples generated from (A) standard normal distribution,
    (B) truncated normal, and (C) uniform [–2,2).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Those who are familiar with NumPy will recognize some of the initializers, as
    they share the same syntax. One example is the sequence generator `tf.linspace(a,
    b, *n*)` that creates `*n*` evenly spaced values from `a` to `b`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A feature that is convenient to use when we want to explore the data content
    of an object is `tf.InteractiveSession()`. Using it and the `.eval()` method,
    we can get a full look at the values without the need to constantly refer to the
    session object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Interactive sessions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`tf.InteractiveSession()` allows you to replace the usual `tf.Session()`, so
    that you don’t need a variable holding the session for running ops. This can be
    useful in interactive Python environments, like when writing IPython notebooks,
    for instance.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve mentioned only a few of the available source operations. [Table 3-2](#table0303)
    provides short descriptions of more useful initializers.
  prefs: []
  type: TYPE_NORMAL
- en: '| TensorFlow operation | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.constant(*value*)` | Creates a tensor populated with the value or values
    specified by the argument `*value*` |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.fill(*shape*, *value*)` | Creates a tensor of shape `*shape*` and fills
    it with `*value*`  |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.zeros(*shape*)` | Returns a tensor of shape `*shape*` with all elements
    set to 0 |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.zeros_like(*tensor*)` | Returns a tensor of the same type and shape as
    `*tensor*` with all elements set to 0 |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.ones(*shape*)` | Returns a tensor of shape `*shape*` with all elements
    set to 1 |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.ones_like(*tensor*)` | Returns a tensor of the same type and shape as
    `*tensor*` with all elements set to 1 |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.random_normal(*shape*, *mean*, *stddev*)` | Outputs random values from
    a normal distribution |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.truncated_normal(*shape*, *mean*, *stddev*)` | Outputs random values
    from a truncated normal distribution (values whose magnitude is more than two
    standard deviations from the mean are dropped and re-picked) |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.random_uniform(*shape*, *minval*, *maxval*)` | Generates values from
    a uniform distribution in the range `[*minval*, *maxval*)` |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.random_shuffle(*tensor*)` | Randomly shuffles a tensor along its first
    dimension  |'
  prefs: []
  type: TYPE_TB
- en: Matrix multiplication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This very useful arithmetic operation is performed in TensorFlow via the `tf.matmul(A,B)`
    function for two Tensor objects `A` and `B`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we have a Tensor storing a matrix `A` and another storing a vector `x`,
    and we wish to compute the matrix product of the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Ax* = *b*'
  prefs: []
  type: TYPE_NORMAL
- en: Before using `matmul()`, we need to make sure both have the same number of dimensions
    and that they are aligned correctly with respect to the intended multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, a matrix `A` and a vector `x` are created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In order to multiply them, we need to add a dimension to `x`, transforming it
    from a 1D vector to a 2D single-column matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can add another dimension by passing the Tensor to `tf.expand_dims()`, together
    with the position of the added dimension as the second argument. By adding another
    dimension in the second position (index 1), we get the desired outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: If we want to flip an array, for example turning a column vector into a row
    vector or vice versa, we can use the `tf.transpose()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Names
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each Tensor object also has an identifying name. This name is an intrinsic
    string name, not to be confused with the name of the variable. As with `dtype`,
    we can use the `.name` attribute to see the name of the object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The name of the Tensor object is simply the name of its corresponding operation
    (“c”; concatenated with a colon), followed by the index of that tensor in the
    outputs of the operation that produced it—it is possible to have more than one.
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate names
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Objects residing within the same graph cannot have the same name—TensorFlow
    forbids it. As a consequence, it will automatically add an underscore and a number
    to distinguish the two. Of course, both objects can have the same name when they
    are associated with different graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Name scopes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes when dealing with a large, complicated graph, we would like to create
    some node grouping to make it easier to follow and manage. For that we can hierarchically
    group nodes together by name. We do so by using `tf.name_scope("*prefix*")` together
    with the useful `with` clause again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this example we’ve grouped objects contained in variables `c2` and `c3` under
    the scope `prefix_name`, which shows up as a prefix in their names.
  prefs: []
  type: TYPE_NORMAL
- en: Prefixes are especially useful when we would like to divide a graph into subgraphs
    with some semantic meaning. These parts can later be used, for instance, for visualization
    of the graph structure.
  prefs: []
  type: TYPE_NORMAL
- en: Variables, Placeholders, and Simple Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section we will cover two important types of Tensor objects: Variables
    and placeholders. We then move forward to the main event: optimization. We will
    briefly talk about all the basic components for optimizing a model, and then do
    some simple demonstration that puts everything together.'
  prefs: []
  type: TYPE_NORMAL
- en: Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The optimization process serves to tune the parameters of some given model.
    For that purpose, TensorFlow uses special objects called *Variables*. Unlike other
    Tensor objects that are “refilled” with data each time we run the session, Variables
    can maintain a fixed state in the graph. This is important because their current
    state might influence how they change in the following iteration. Like other Tensors,
    Variables can be used as input for other operations in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Using Variables is done in two stages. First we call the `tf.Variable()` function
    in order to create a Variable and define what value it will be initialized with.
    We then have to explicitly perform an initialization operation by running the
    session with the `tf.global_variables_initializer()` method, which allocates the
    memory for the Variable and sets its initial values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like other Tensor objects, Variables are computed only when the model runs,
    as we can see in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that if we run the code again, we see that a new variable is created each
    time, as indicated by the automatic concatenation of `_1` to its name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This could be very inefficient when we want to reuse the model (complex models
    could have many variables!); for example, when we wish to feed it with several
    different inputs. To reuse the same variable, we can use the `tf.get_variables()`
    function instead of `tf.Variable()`. More on this can be found in [“Model Structuring”](app01.html#model_structuring)
    of the appendix.
  prefs: []
  type: TYPE_NORMAL
- en: Placeholders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far we’ve used source operations to create our input data. TensorFlow, however,
    has designated built-in structures for feeding input values. These structures
    are called *placeholders*. Placeholders can be thought of as empty Variables that
    will be filled with data later on. We use them by first constructing our graph
    and only when it is executed feeding them with the input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Placeholders have an optional `shape` argument. If a shape is not fed or is
    passed as `None`, then the placeholder can be fed with data of any size. It is
    common to use `None` for the dimension of a matrix that corresponds to the number
    of samples (usually rows), while having the length of the features (usually columns) fixed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Whenever we define a placeholder, we must feed it with some input values or
    else an exception will be thrown. The input data is passed to the `session.run()`
    method as a dictionary, where each key corresponds to a placeholder variable name,
    and the matching values are the data values given in the form of a list or a NumPy
    array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see how it looks with another graph example, this time with placeholders
    for two inputs: a matrix `x` and a vector `w`. These inputs are matrix-multiplied
    to create a five-unit vector `xw` and added with a constant vector `b` filled
    with the value `-1`. Finally, the variable `s` takes the maximum value of that
    vector by using the `tf.reduce_max()` operation. The word *reduce* is used because
    we are reducing a five-unit vector to a single scalar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we turn to optimization. We first describe the basics of training a model, giving
    a short description of each component in the process, and show how it is performed
    in TensorFlow. We then demonstrate a full working example of an optimization process
    of a simple regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Training to predict
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have some target variable <math><mi>y</mi></math> , which we want to explain
    using some feature vector <math alttext="x"><mi>x</mi></math> . To do so, we first
    choose a model that relates the two. Our training data points will be used for “tuning”
    the model so that it best captures the desired relation. In the following chapters
    we focus on deep neural network models, but for now we will settle for a simple
    regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by describing our regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f*(*x*[*i*]) = *w*^(*T*)*x*[*i*] + *b*'
  prefs: []
  type: TYPE_NORMAL
- en: (*w* is initialized as a row vector; therefore, transposing *x* will yield the
    same result as in the equation above.)
  prefs: []
  type: TYPE_NORMAL
- en: '*y*[*i*] = *f*(*x*[*i*]) + *ε*[*i*]'
  prefs: []
  type: TYPE_NORMAL
- en: '*f*(*x*[*i*]) is assumed to be a linear combination of some input data *x*[*i*],
    with a set of weights *w* and an intercept *b*. Our target output *y*[*i*] is
    a noisy version of *f*(*x*[*i*]) after being summed with Gaussian noise *ε*[*i*] (where *i* denotes
    a given sample).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the previous example, we will need to create the appropriate placeholders
    for our input and output data and Variables for our weights and intercept:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the placeholders and Variables are defined, we can write down our model.
    In this example, it’s simply a multivariate linear regression—our predicted output
    `y_pred` is the result of a matrix multiplication of our input container `x` and
    our weights `w` plus a bias term `b`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Defining a loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we need a good measure with which we can evaluate the model’s performance.
    To capture the discrepancy between our model’s predictions and the observed targets, we
    need a measure reflecting “distance.” This distance is often referred to as an
    *objective* or a *loss* function, and we optimize the model by finding the set
    of parameters (weights and bias in this case) that minimize it.
  prefs: []
  type: TYPE_NORMAL
- en: There is no ideal loss function, and choosing the most suitable one is often
    a blend of art and science. The choice may depend on several factors, like the
    assumptions of our model, how easy it is to minimize, and what types of mistakes
    we prefer to avoid.
  prefs: []
  type: TYPE_NORMAL
- en: MSE and cross entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perhaps the most commonly used loss is the MSE (mean squared error), where
    for all samples we average the squared distances between the real target and what
    our model predicts across samples:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper L left-parenthesis y comma ModifyingAbove y With caret
    right-parenthesis equals StartFraction 1 Over n EndFraction normal upper Sigma
    Subscript i equals 1 Superscript n Baseline left-parenthesis y Subscript i Baseline
    minus ModifyingAbove y With caret Subscript i Baseline right-parenthesis squared"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mi>y</mi> <mo>,</mo> <mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <msubsup><mi>Σ</mi>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This loss has intuitive interpretation—it minimizes the mean square difference
    between an observed value and the model’s fitted value (these differences are
    referred to as *residuals*).
  prefs: []
  type: TYPE_NORMAL
- en: 'In our linear regression example, we take the difference between the vector `y_true`
    (*y*), the true targets, and `y_pred` (ŷ), the model’s predictions, and use `tf.square()`
    to compute the square of the difference vector. This operation is applied element-wise.
    We then average the squared differences using the `tf.reduce_mean()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Another very common loss, especially for categorical data, is the *cross entropy*,
    which we used in the softmax classifier in the previous chapter. The cross entropy
    is given by
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>H</mi> <mo stretchy="false">(</mo> <mi>p</mi>
    <mo>,</mo> <mi>q</mi> <mo stretchy="false">)</mo> <mo>=</mo> <mo>-</mo> <munder><mo>∑</mo>
    <mi>x</mi></munder> <mi>p</mi> <mo stretchy="false">(</mo> <mi>x</mi> <mo stretchy="false">)</mo>
    <mi>log</mi> <mi>q</mi> <mo stretchy="false">(</mo> <mi>x</mi> <mo stretchy="false">)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: and for classification with a single correct label (as is the case in an overwhelming
    majority of the cases) reduces to the negative log of the probability placed by
    the classifier on the correct label.
  prefs: []
  type: TYPE_NORMAL
- en: 'In TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Cross entropy is a measure of similarity between two distributions. Since the
    classification models used in deep learning typically output probabilities for
    each class, we can compare the true class (distribution *p*) with the probabilities
    of each class given by the model (distribution *q*). The more similar the two
    distributions, the smaller our cross entropy will be.
  prefs: []
  type: TYPE_NORMAL
- en: The gradient descent optimizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next thing we need to figure out is how to minimize the loss function. While
    in some cases it is possible to find the global minimum analytically (when it
    exists), in the great majority of cases we will have to use an optimization algorithm.
    Optimizers update the set of weights iteratively in a way that decreases the loss
    over time.
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used approach is gradient descent, where we use the loss’s
    gradient with respect to the set of weights. In slightly more technical terms,
    if our loss is some multivariate function *F*(*w̄*), then in the neighborhood
    of some point *w̄*[0], the “steepest” direction of decrease of *F*(*w̄*) is obtained
    by moving from *w̄*[0] in the direction of the negative gradient of *F* at *w̄*[0].
  prefs: []
  type: TYPE_NORMAL
- en: '*So if* *w̄*[1] = *w̄*[0]-γ∇*F*(*w̄*[0]) *where ∇*F*(*w̄*[0]) is the gradient
    of *F* evaluated at *w̄*[0], then for a small enough γ:*'
  prefs: []
  type: TYPE_NORMAL
- en: '*F*(*w̄*[0]) ⩾ *F*(*w̄*[1])'
  prefs: []
  type: TYPE_NORMAL
- en: The gradient descent algorithms work well on highly complicated network architectures
    and therefore are suitable for a wide variety of problems. More specifically,
    recent advances make it possible to compute these gradients by utilizing massively
    parallel systems, so the approach scales well with dimensionality (though it can
    still be painfully time-consuming for large real-world problems). While convergence
    to the global minimum is guaranteed for convex functions, for nonconvex problems
    (which are essentially all problems in the world of deep learning) they can get
    stuck in local minima. In practice, this is often good enough, as is evidenced
    by the huge success of the field of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The gradient of the objective is computed with respect to the model parameters
    and evaluated using a given set of input samples, *x*[*s*]. How many of the samples
    should we take for this calculation? Intuitively, it makes sense to calculate
    the gradient for the entire set of samples in order to benefit from the maximum
    amount of available information. This method, however, has some shortcomings. For
    example, it can be very slow and is intractable when the dataset requires more
    memory than is available.
  prefs: []
  type: TYPE_NORMAL
- en: A more popular technique is the stochastic gradient descent (SGD), where instead
    of feeding the entire dataset to the algorithm for the computation of each step,
    a subset of the data is sampled sequentially. The number of samples ranges from
    one sample at a time to a few hundred, but the most common sizes are between around
    50 to around 500 (usually referred to as *mini-batches*).
  prefs: []
  type: TYPE_NORMAL
- en: Using smaller batches usually works faster, and the smaller the size of the
    batch, the faster are the calculations. However, there is a trade-off in that
    small samples lead to lower hardware utilization and tend to have high variance, causing
    large fluctuations to the objective function. Nevertheless, it turns out that
    some fluctuations are beneficial since they enable the set of parameters to jump
    to new and potentially better local minima. Using a relatively smaller batch size
    is therefore effective in that regard, and is currently overall the preferred
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent in TensorFlow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TensorFlow makes it very easy and intuitive to use gradient descent algorithms.
    Optimizers in TensorFlow compute the gradients simply by adding new operations
    to the graph, and the gradients are calculated using automatic differentiation.
    This means, in general terms, that TensorFlow automatically computes the gradients
    on its own, “deriving” them from the operations and structure of the computation
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: An important parameter to set is the algorithm’s learning rate, determining
    how aggressive each update iteration will be (or in other words, how large the
    step will be in the direction of the negative gradient). We want the decrease
    in the loss to be fast enough on the one hand, but on the other hand not large
    enough so that we over-shoot the target and end up at a point with a higher value
    of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first create an optimizer by using the `GradientDescentOptimizer()` function
    with the desired learning rate. We then create a train operation that updates
    our variables by calling the `optimizer.minimize()` function and passing in the
    loss as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The train operation is then executed when it is fed to the `sess.run()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping it up with examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’re all set to go!  Let’s combine all the components we’ve discussed in this
    section and optimize the parameters of two models: linear and logistic regression.
    In these examples we will create synthetic data with known properties, and see
    how the model is able to recover these properties with the process of optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 1: linear regression'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this problem we are interested in retrieving a set of weights *w* and a bias
    term *b*, assuming our target value is a linear combination of some input vector *x*,
    with an additional Gaussian noise *ε*[*i*] added to each sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this exercise we will generate synthetic data using NumPy. We create 2,000
    samples of *x*, a vector with three features, take the inner product of each *x*
    sample with a set of weights *w* ([0.3, 0.5, 0.1]), and add a bias term *b* (–0.2) and
    Gaussian noise to the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The noisy samples are shown in [Figure 3-6](#data_for_linear_regression).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-6\. Generated data to use for linear regression: each filled circle
    represents a sample, and the dashed line shows the expected values without the
    noise component (the diagonal).'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Next, we estimate our set of weights *w* and bias *b* by optimizing the model
    (i.e., finding the best parameters) so that its predictions match the real targets
    as closely as possible. Each iteration computes one update to the current parameters.
    In this example we run 10 iterations, printing our estimated parameters every
    5 iterations using the `sess.run()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Don’t forget to initialize the variables! In this example we initialize both
    the weights and the bias with zeros; however, there are “smarter” initialization
    techniques to choose, as we will see in the next chapters. We use name scopes
    to group together parts that are related to inferring the output, defining the
    loss, and setting and creating the train object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'And we get the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: After only 10 iterations, the estimated weights and bias are  <math alttext="ModifyingAbove
    w With caret"><mover accent="true"><mi>w</mi> <mo>^</mo></mover></math> = [0.301,
    0.498, 0.098] and <math alttext="ModifyingAbove b With caret"><mover accent="true"><mi>b</mi>
    <mo>^</mo></mover></math> = –0.198\. The original parameter values were *w* =
    [0.3,0.5,0.1] and *b* = –0.2.
  prefs: []
  type: TYPE_NORMAL
- en: Almost a perfect match!
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2: logistic regression'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Again we wish to retrieve the weights and bias components in a simulated data
    setting, this time in a logistic regression framework. Here the linear component *w*^T*x*
    + *b* is the input of a nonlinear function called the logistic function. What
    it effectively does is squash the values of the linear part into the interval [0,
    1]:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Pr*(*y*[*i*] = 1|*x*[*i*]) = <math><mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mo
    form="prefix">exp</mo> <mrow><mo>-</mo><mo>(</mo><mi>w</mi><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>+</mo><mi>b</mi><mo>)</mo></mrow></msup></mrow></mfrac></math>'
  prefs: []
  type: TYPE_NORMAL
- en: We then regard these values as probabilities from which binary yes/1 or no/0
    outcomes are generated. This is the nondeterministic (noisy) part of the model.
  prefs: []
  type: TYPE_NORMAL
- en: The logistic function is more general, and can be used with a different set
    of parameters for the steepness of the curve and its maximum value. This special
    case of a logistic function we are using is also referred to as a *sigmoid function*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We generate our samples by using the same set of weights and biases as in the
    previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The outcome samples before and after the binarization of the output are shown
    in [Figure 3-7](#data_for_logistic_regression).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0307.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-7\. Generated data to use for logistic regression: each circle represents
    a sample. In the left plot we see the probabilities generated by inputting the
    linear combination of the input data to the logistic function. The right plot
    shows the binary target output, randomly sampled from the probabilities in the
    left image.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The only thing we need to change in the code is the loss function we use.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss we want to use here is the binary version of the cross entropy, which
    is also the likelihood of the logistic regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Luckily, TensorFlow already has a designated function we can use instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'To which we simply need to pass the true outputs and the model’s linear predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: It takes a few more iterations to converge, and more samples are required than
    in the previous linear regression example, but eventually we get results that
    are quite similar to the original chosen weights.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we learned about computation graphs and what we can use them
    for. We saw how to create a graph and how to compute its outputs. We introduced
    the main building blocks of TensorFlow—the Tensor object, representing the graph’s
    operations, placeholders for our input data, and Variables we tune as part of
    the model training process. We learned about tensor arrays and covered the data
    type, shape, and name attributes. Finally, we discussed the model optimization
    process and saw how to implement it in TensorFlow. In the next chapter we will
    go into more advanced deep neural networks used in computer vision.
  prefs: []
  type: TYPE_NORMAL
