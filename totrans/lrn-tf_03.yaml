- en: Chapter 3\. Understanding TensorFlow Basics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章。理解TensorFlow基础知识
- en: This chapter demonstrates the key concepts of how TensorFlow is built and how
    it works with simple and intuitive examples. You will get acquainted with the
    basics of TensorFlow as a numerical computation library using dataflow graphs.
    More specifically, you will learn how to manage and create a graph, and be introduced
    to TensorFlow’s “building blocks,” such as constants, placeholders, and Variables.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章演示了TensorFlow是如何构建和使用简单直观的示例的关键概念。您将了解TensorFlow作为一个数据流图的数值计算库的基础知识。更具体地说，您将学习如何管理和创建图，并介绍TensorFlow的“构建块”，如常量、占位符和变量。
- en: Computation Graphs
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算图
- en: TensorFlow allows us to implement machine learning algorithms by creating and
    computing operations that interact with one another. These interactions form what
    we call a “computation graph,” with which we can intuitively represent complicated
    functional architectures.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow允许我们通过创建和计算相互作用的操作来实现机器学习算法。这些交互形成了我们所谓的“计算图”，通过它我们可以直观地表示复杂的功能架构。
- en: What Is a Computation Graph?
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是计算图？
- en: We assume a lot of readers have already come across the mathematical concept
    of a graph. For those to whom this concept is new, a graph refers to a set of
    interconnected *entities*, commonly called *nodes* or *vertices*. These nodes
    are connected to each other via edges. In a dataflow graph, the edges allow data
    to “flow” from one node to another in a directed manner.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设很多读者已经接触过图的数学概念。对于那些对这个概念是新的人来说，图指的是一组相互连接的*实体*，通常称为*节点*或*顶点*。这些节点通过边连接在一起。在数据流图中，边允许数据以有向方式从一个节点流向另一个节点。
- en: In TensorFlow, each of the graph’s nodes represents an operation, possibly applied
    to some input, and can generate an output that is passed on to other nodes. By
    analogy, we can think of the graph computation as an assembly line where each
    machine (node) either gets or creates its raw material (input), processes it,
    and then passes the output to other machines in an orderly fashion, producing
    *subcomponents* and eventually a final *product* when the assembly process comes
    to an end.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，图的每个节点代表一个操作，可能应用于某些输入，并且可以生成一个输出，传递给其他节点。类比地，我们可以将图计算看作是一个装配线，其中每台机器（节点）要么获取或创建其原材料（输入），处理它，然后按顺序将输出传递给其他机器，生产*子组件*，最终在装配过程结束时产生一个最终*产品*。
- en: Operations in the graph include all kinds of functions, from simple arithmetic
    ones such as subtraction and multiplication to more complex ones, as we will see
    later on. They also include more general operations like the creation of summaries,
    generating constant values, and more.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的操作包括各种函数，从简单的算术函数如减法和乘法到更复杂的函数，我们稍后会看到。它们还包括更一般的操作，如创建摘要、生成常量值等。
- en: The Benefits of Graph Computations
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图计算的好处
- en: TensorFlow optimizes its computations based on the graph’s connectivity.​ Each
    graph has its own set of node dependencies. When the input of node `y` is affected
    by the output of node `x`, we say that node `y` is dependent on node `x`. We call
    it a *direct dependency* when the two are connected via an edge, and an *indirect
    dependency* otherwise. For example, in [Figure 3-1](#graph_dependencies) (A),
    node `e` is directly dependent on node `c`, indirectly dependent on node `a`, and
    independent of node `d`.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow根据图的连接性优化其计算。每个图都有自己的节点依赖关系集。当节点`y`的输入受到节点`x`的输出的影响时，我们说节点`y`依赖于节点`x`。当两者通过边连接时，我们称之为*直接依赖*，否则称为*间接依赖*。例如，在[图3-1](#graph_dependencies)（A）中，节点`e`直接依赖于节点`c`，间接依赖于节点`a`，独立于节点`d`。
- en: '![](assets/letf_0301.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/letf_0301.png)'
- en: Figure 3-1\. (A) Illustration of graph dependencies. (B) Computing node e results
    in the minimal amount of computations according to the graph’s dependencies—in
    this case computing only nodes c, b, and a.
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1\.（A）图依赖的示例。 （B）计算节点e根据图的依赖关系进行最少量的计算—在这种情况下仅计算节点c、b和a。
- en: We can always identify the full set of dependencies for each node in the graph. This
    is a fundamental characteristic of the graph-based computation format. Being able
    to locate dependencies between units of our model allows us to both distribute
    computations across available resources and avoid performing redundant computations
    of irrelevant subsets, resulting in a faster and more efficient way of computing
    things.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们始终可以识别图中每个节点的完整依赖关系。这是基于图的计算格式的一个基本特征。能够找到模型单元之间的依赖关系使我们能够在可用资源上分配计算，并避免执行与无关子集的冗余计算，从而以更快更有效的方式计算事物。
- en: Graphs, Sessions, and Fetches
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图、会话和获取
- en: 'Roughly speaking, working with TensorFlow involves two main phases: (1) constructing
    a graph and (2) executing it. Let’s jump into our first example and create something
    very basic.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 粗略地说，使用TensorFlow涉及两个主要阶段：（1）构建图和（2）执行图。让我们跳入我们的第一个示例，并创建一些非常基本的东西。
- en: Creating a Graph
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建图
- en: Right after we import TensorFlow (with `import tensorflow as tf`), a specific
    empty default graph is formed. All the nodes we create are automatically associated
    with that default graph.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 导入TensorFlow后（使用`import tensorflow as tf`），会形成一个特定的空默认图。我们创建的所有节点都会自动与该默认图关联。
- en: Using the `tf.<*operator*>` methods, we will create six nodes assigned to arbitrarily
    named variables. The contents of these variables should be regarded as the output
    of the operations, and not the operations themselves. For now we refer to both
    the operations and their outputs with the names of their corresponding variables.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`tf.<*operator*>`方法，我们将创建六个节点，分配给任意命名的变量。这些变量的内容应被视为操作的输出，而不是操作本身。现在我们用它们对应变量的名称来引用操作和它们的输出。
- en: 'The first three nodes are each told to output a constant value. The values
    `5`, `2`, and `3` are assigned to `a`, `b`, and `c`, respectively:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 前三个节点各被告知输出一个常量值。值`5`、`2`和`3`分别分配给`a`、`b`和`c`：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Each of the next three nodes gets two existing variables as inputs, and performs
    simple arithmetic operations on them:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的三个节点中的每一个都将两个现有变量作为输入，并对它们进行简单的算术运算：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Node `d` multiplies the outputs of nodes `a` and `b`. Node `e` adds the outputs
    of nodes `b` and `c`. Node `f` subtracts the output of node `e` from that of node
    `d`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 节点`d`将节点`a`和`b`的输出相乘。节点`e`将节点`b`和`c`的输出相加。节点`f`将节点`e`的输出从节点`d`的输出中减去。
- en: And *voilà*! We have our first TensorFlow graph! [Figure 3-2](#our_first_constructed_graph)
    shows an illustration of the graph we’ve just created.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*voilà*！我们有了我们的第一个TensorFlow图！[图3-2](#our_first_constructed_graph)显示了我们刚刚创建的图的示例。'
- en: '![](assets/letf_0302.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/letf_0302.png)'
- en: 'Figure 3-2\. An illustration of our first constructed graph. Each node, denoted
    by a lowercase letter, performs the operation indicated above it: Const for creating
    constants and Add, Mul, and Sub for addition, multiplication, and subtraction,
    respectively. The integer next to each edge is the output of the corresponding
    node’s operation.'
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2\. 我们第一个构建的图的示例。每个由小写字母表示的节点执行其上方指示的操作：Const用于创建常量，Add、Mul和Sub分别用于加法、乘法和减法。每条边旁边的整数是相应节点操作的输出。
- en: Note that for some arithmetic and logical operations it is possible to use operation
    shortcuts instead of having to apply `tf.<*operator*>`. For example, in this graph
    we could have used `*`/+/`-` instead of `tf.multiply()`/`tf.add()`/`tf.subtract()`
    (like we did in the “hello world” example in [Chapter 2](ch02.html#go_with_the_flow),
    where we used + instead of `tf.add()`). [Table 3-1](#table0301) lists the available
    shortcuts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于一些算术和逻辑操作，可以使用操作快捷方式，而不必应用`tf.<*operator*>`。例如，在这个图中，我们可以使用`*`/+/`-`代替`tf.multiply()`/`tf.add()`/`tf.subtract()`（就像我们在[第2章](ch02.html#go_with_the_flow)的“hello
    world”示例中使用+代替`tf.add()`一样）。[表3-1](#table0301)列出了可用的快捷方式。
- en: Table 3-1\. Common TensorFlow operations and their respective shortcuts
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-1\. 常见的TensorFlow操作及其相应的快捷方式
- en: '| TensorFlow operator | Shortcut | Description |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| TensorFlow运算符 | 快捷方式 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `tf.add()` | `a + b` | Adds `a` and `b`, element-wise. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| `tf.add()` | `a + b` | 对`a`和`b`进行逐元素相加。 |'
- en: '| `tf.multiply()` | `a * b` | Multiplies `a` and `b`, element-wise. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| `tf.multiply()` | `a * b` | 对`a`和`b`进行逐元素相乘。 |'
- en: '| `tf.subtract()` | `a - b` | Subtracts `b` from `a`, element-wise. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| `tf.subtract()` | `a - b` | 对`a`和`b`进行逐元素相减。 |'
- en: '| `tf.divide()` | `a / b` | Computes Python-style division of `a` by `b`. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| `tf.divide()` | `a / b` | 计算`a`除以`b`的Python风格除法。 |'
- en: '| `tf.pow()` | `a ** b` | Returns the result of raising each element in `a`
    to its corresponding element `b`, element-wise. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| `tf.pow()` | `a ** b` | 返回将`a`中的每个元素提升到其对应元素`b`的结果，逐元素。 |'
- en: '| `tf.mod()` | `a % b` | Returns the element-wise modulo. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| `tf.mod()` | `a % b` | 返回逐元素取模。 |'
- en: '| `tf.logical_and()` | `a & b` | Returns the truth table of `a & b`, element-wise.
    `dtype` must be `tf.bool`. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| `tf.logical_and()` | `a & b` | 返回`a & b`的真值表，逐元素。`dtype`必须为`tf.bool`。 |'
- en: '| `tf.greater()` | `a > b` | Returns the truth table of `a > b`, element-wise.
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| `tf.greater()` | `a > b` | 返回`a > b`的真值表，逐元素。 |'
- en: '| `tf.greater_equal()` | `a >= b` | Returns the truth table of `a >= b`, element-wise.
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| `tf.greater_equal()` | `a >= b` | 返回`a >= b`的真值表，逐元素。 |'
- en: '| `tf.less_equal()` | `a <= b` | Returns the truth table of `a <= b`, element-wise.
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| `tf.less_equal()` | `a <= b` | 返回`a <= b`的真值表，逐元素。 |'
- en: '| `tf.less()` | `a < b` | Returns the truth table of `a < b`, element-wise.
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| `tf.less()` | `a < b` | 返回`a < b`的真值表，逐元素。 |'
- en: '| `tf.negative()` | `-a` | Returns the negative value of each element in `a`.
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| `tf.negative()` | `-a` | 返回`a`中每个元素的负值。 |'
- en: '| `tf.logical_not()` | `~a` | Returns the logical NOT of each element in `a`.
    Only compatible with Tensor objects with `dtype` of `tf.bool`. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| `tf.logical_not()` | `~a` | 返回`a`中每个元素的逻辑非。仅与`dtype`为`tf.bool`的张量对象兼容。 |'
- en: '| `tf.abs()` | `abs(a)` | Returns the absolute value of each element in `a`.
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| `tf.abs()` | `abs(a)` | 返回`a`中每个元素的绝对值。 |'
- en: '| `tf.logical_or()` | `a &#124; b` | Returns the truth table of `a &#124; b`,
    element-wise. `dtype` must be `tf.bool`. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| `tf.logical_or()` | `a &#124; b` | 返回`a &#124; b`的真值表，逐元素。`dtype`必须为`tf.bool`。
    |'
- en: Creating a Session and Running It
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建会话并运行
- en: 'Once we are done describing the computation graph, we are ready to run the
    computations that it represents. For this to happen, we need to create and run
    a session. We do this by adding the following code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成描述计算图，我们就准备运行它所代表的计算。为了实现这一点，我们需要创建并运行一个会话。我们通过添加以下代码来实现这一点：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: First, we launch the graph in a `tf.Session`. A `Session` object is the part
    of the TensorFlow API that communicates between Python objects and data on our
    end, and the actual computational system where memory is allocated for the objects
    we define, intermediate variables are stored, and finally results are fetched
    for us.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在`tf.Session`中启动图。`Session`对象是TensorFlow API的一部分，它在Python对象和我们端的数据之间进行通信，实际的计算系统在其中为我们定义的对象分配内存，存储中间变量，并最终为我们获取结果。
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The execution itself is then done with the `.run()` method of the `Session`
    object. When called, this method completes one set of computations in our graph
    in the following manner: it starts at the requested output(s) and then works backward,
    computing nodes that must be executed according to the set of dependencies. Therefore, the
    part of the graph that will be computed depends on our output query.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，执行本身是通过`Session`对象的`.run()`方法完成的。当调用时，该方法以以下方式完成我们图中的一组计算：从请求的输出开始，然后向后工作，计算必须根据依赖关系集执行的节点。因此，将计算的图部分取决于我们的输出查询。
- en: 'In our example, we requested that node `f` be computed and got its value, `5`, as
    output:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们请求计算节点`f`并获得其值`5`作为输出：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When our computation task is completed, it is good practice to close the session
    using the `sess.close()` command, making sure the resources used by our session
    are freed up. This is an important practice to maintain even though we are not
    obligated to do so for things to work:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的计算任务完成时，最好使用`sess.close()`命令关闭会话，确保我们的会话使用的资源被释放。即使我们不必这样做也能使事情正常运行，这是一个重要的实践：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Example 3-1\. Try it yourself! [Figure 3-3](#fig0303) shows another two graph
    examples. See if you can produce these graphs yourself.
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-1。自己试试吧！[图3-3](#fig0303)显示了另外两个图示例。看看你能否自己生成这些图。
- en: '![](assets/letf_0303.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/letf_0303.png)'
- en: Figure 3-3\. Can you create graphs A and B?  (To produce the sine function,
    use tf.sin(x)).
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3。你能创建图A和图B吗？（要生成正弦函数，请使用tf.sin(x)）。
- en: Constructing and Managing Our Graph
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建和管理我们的图
- en: 'As mentioned, as soon as we import TensorFlow, a default graph is automatically
    created for us. We can create additional graphs and control their association
    with some given operations. `tf.Graph()` creates a new graph, represented as a
    TensorFlow object. In this example we create another graph and assign it to the
    variable `g`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，一旦导入TensorFlow，一个默认图会自动为我们创建。我们可以创建额外的图并控制它们与某些给定操作的关联。`tf.Graph()`创建一个新的图，表示为一个TensorFlow对象。在这个例子中，我们创建另一个图并将其分配给变量`g`：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: At this point we have two graphs: the default graph and the empty graph in `g`.
    Both are revealed as TensorFlow objects when printed. Since `g` hasn’t been assigned
    as the default graph, any operation we create will not be associated with it,
    but rather with the default one.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此时我们有两个图：默认图和`g`中的空图。当打印时，它们都会显示为TensorFlow对象。由于`g`尚未分配为默认图，我们创建的任何操作都不会与其相关联，而是与默认图相关联。
- en: 'We can check which graph is currently set as the default by using `tf.get_default_graph()`.
    Also, for a given node, we can view the graph it’s associated with by using the
    `*<node>*.graph` attribute:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`tf.get_default_graph()`来检查当前设置为默认的图。此外，对于给定节点，我们可以使用`*<node>*.graph`属性查看它关联的图：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this code example we see that the operation we’ve created is associated with
    the default graph and not with the graph in `g`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码示例中，我们看到我们创建的操作与默认图相关联，而不是与`g`中的图相关联。
- en: 'To make sure our constructed nodes are associated with the right graph we can
    construct them using a very useful Python construct: the `with` statement.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们构建的节点与正确的图相关联，我们可以使用一个非常有用的Python构造：`with`语句。
- en: '**The with statement**'
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**with语句**'
- en: The `with` statement is used to wrap the execution of a block with methods defined
    by a context manager—an object that has the special method functions `.__enter__()`
    to set up a block of code and `.__exit__()` to exit the block.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`with`语句用于使用上下文管理器定义的方法包装一个代码块——一个具有特殊方法函数`.__enter__()`用于设置代码块和`.__exit__()`用于退出代码块的对象。'
- en: In layman’s terms, it’s very convenient in many cases to execute some code that
    requires “setting up” of some kind (like opening a file, SQL table, etc.) and
    then always “tearing it down” at the end, regardless of whether the code ran well
    or raised any kind of exception. In our case we use `with` to set up a graph and
    make sure every piece of code will be performed in the context of that graph.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通俗地说，在许多情况下，执行一些需要“设置”（如打开文件、SQL表等）的代码，然后在最后“拆除”它总是非常方便的，无论代码是否运行良好或引发任何异常。在我们的例子中，我们使用`with`来设置一个图，并确保每一段代码都将在该图的上下文中执行。
- en: 'We use the `with` statement together with the `as_default()` command, which
    returns a context manager that makes this graph the default one. This comes in
    handy when working with multiple graphs:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`with`语句与`as_default()`命令一起使用，它返回一个上下文管理器，使这个图成为默认图。在处理多个图时，这非常方便：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `with` statement can also be used to start a session without having to explicitly
    close it. This convenient trick will be used in the following examples.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`with`语句也可以用于启动一个会话，而无需显式关闭它。这个方便的技巧将在接下来的示例中使用。'
- en: Fetches
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Fetches
- en: 'In our initial graph example, we request one specific node (node `f`) by passing
    the variable it was assigned to as an argument to the `sess.run()` method. This
    argument is called `fetches`, corresponding to the elements of the graph we wish
    to compute. We can also ask `sess.run()` for multiple nodes’ outputs simply by
    inputting a list of requested nodes:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的初始图示例中，我们通过将分配给它的变量作为`sess.run()`方法的参数来请求一个特定节点（节点`f`）。这个参数称为`fetches`，对应于我们希望计算的图的元素。我们还可以通过输入请求节点的列表来要求`sess.run()`多个节点的输出：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We get back a list containing the outputs of the nodes according to how they
    were ordered in the input list. The data in each item of the list is of type NumPy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到一个包含节点输出的列表，根据它们在输入列表中的顺序。列表中每个项目的数据类型为NumPy。
- en: NumPy
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NumPy
- en: NumPy is a popular and useful Python package for numerical computing that offers
    many functionalities related to working with arrays. We assume some basic familiarity
    with this package, and it will not be covered in this book. TensorFlow and NumPy
    are tightly coupled—for example, the output returned by `sess.run()` is a NumPy
    array. In addition, many of TensorFlow’s operations share the same syntax as functions
    in NumPy. To learn more about NumPy, we refer the reader to Eli Bressert’s book
    [*SciPy and NumPy*](http://shop.oreilly.com/product/0636920020219.do) (O’Reilly).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy是一个流行且有用的Python包，用于数值计算，提供了许多与数组操作相关的功能。我们假设读者对这个包有一些基本的了解，本书不会涉及这部分内容。TensorFlow和NumPy紧密耦合——例如，`sess.run()`返回的输出是一个NumPy数组。此外，许多TensorFlow的操作与NumPy中的函数具有相同的语法。要了解更多关于NumPy的信息，我们建议读者参考Eli
    Bressert的书籍[*SciPy and NumPy*](http://shop.oreilly.com/product/0636920020219.do)（O'Reilly）。
- en: 'We mentioned that TensorFlow computes only the essential nodes according to
    the set of dependencies. This is also manifested in our example: when we ask for
    the output of node `d`, only the outputs of nodes `a` and `b` are computed. Another
    example is shown in [Figure 3-1(B)](#graph_dependencies). This is a great advantage
    of TensorFlow—it doesn’t matter how big and complicated our graph is as a whole,
    since we can run just a small portion of it as needed.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到TensorFlow仅根据依赖关系集计算必要的节点。这也体现在我们的示例中：当我们请求节点`d`的输出时，只计算节点`a`和`b`的输出。另一个示例显示在[图3-1(B)](#graph_dependencies)中。这是TensorFlow的一个巨大优势——不管我们的整个图有多大和复杂，因为我们可以根据需要仅运行其中的一小部分。
- en: Automatically closing the session
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动关闭会话
- en: Opening a session using the `with` clause will ensure the session is automatically
    closed once all computations are done.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`with`子句打开会话将确保会话在所有计算完成后自动关闭。
- en: Flowing Tensors
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流动的Tensor
- en: In this section we will get a better understanding of how nodes and edges are
    actually represented in TensorFlow, and how we can control their characteristics.
    To demonstrate how they work, we will focus on source operations, which are used
    to initialize values.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将更好地理解节点和边在TensorFlow中实际上是如何表示的，以及如何控制它们的特性。为了演示它们的工作原理，我们将重点放在用于初始化值的源操作上。
- en: Nodes Are Operations, Edges Are Tensor Objects
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 节点是操作，边是Tensor对象
- en: When we construct a node in the graph, like we did with `tf.add()`, we are actually
    creating an operation instance. These operations do not produce actual values
    until the graph is executed, but rather reference their to-be-computed result
    as a handle that can be passed on—*flow*—to another node. These handles, which
    we can think of as the edges in our graph, are referred to as Tensor objects,
    and this is where the name TensorFlow originates from.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在图中构造一个节点，就像我们用`tf.add()`做的那样，实际上是创建了一个操作实例。这些操作直到图被执行时才产生实际值，而是将它们即将计算的结果作为一个可以传递给另一个节点的句柄。我们可以将这些句柄视为图中的边，称为Tensor对象，这也是TensorFlow名称的由来。
- en: TensorFlow is designed such that first a skeleton graph is created with all
    of its components. At this point no actual data flows in it and no computations
    take place. It is only upon execution, when we run the session, that data enters
    the graph and computations occur (as illustrated in [Figure 3-4](#pre_and_post_running_a_section)).
    This way, computations can be much more efficient, taking the entire graph structure
    into consideration.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的设计是首先创建一个带有所有组件的骨架图。在这一点上，没有实际数据流入其中，也没有进行任何计算。只有在执行时，当我们运行会话时，数据进入图中并进行计算（如[图3-4](#pre_and_post_running_a_section)所示）。这样，计算可以更加高效，考虑整个图结构。
- en: '![](assets/letf_0304.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/letf_0304.png)'
- en: Figure 3-4\. Illustrations of before (A) and after (B) running a session. When
    the session is run, actual data “flows” through the graph.
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4。在运行会话之前（A）和之后（B）的示例。当会话运行时，实际数据会“流”通过图。
- en: In the previous section’s example, `tf.constant()` created a node with the corresponding
    passed value. Printing the output of the constructor, we see that it’s actually
    a Tensor object instance. These objects have methods and attributes that control
    their behavior and that can be defined upon creation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节的示例中，`tf.constant()`创建了一个带有传递值的节点。打印构造函数的输出，我们看到它实际上是一个Tensor对象实例。这些对象有控制其行为的方法和属性，可以在创建时定义。
- en: 'In this example, the variable `c` stores a Tensor object with the name `Const_52:0`,
    designated to contain a 32-bit floating-point scalar:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，变量`c`存储了一个名为`Const_52:0`的Tensor对象，用于包含一个32位浮点标量：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: A note on constructors
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构造函数说明
- en: The `tf.*<operator>*` function could be thought of as a constructor, but to
    be more precise, this is actually not a constructor at all, but rather a factory
    method that sometimes does quite a bit more than just creating the operator objects.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.*<operator>*`函数可以被视为构造函数，但更准确地说，这实际上根本不是构造函数，而是一个工厂方法，有时做的事情远不止创建操作对象。'
- en: Setting attributes with source operations
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用源操作设置属性
- en: Each Tensor object in TensorFlow has attributes such as `name`, `shape`, and
    `dtype` that help identify and set the characteristics of that object. These attributes
    are optional when creating a node, and are set automatically by TensorFlow when
    missing. In the next section we will take a look at these attributes. We will
    do so by looking at Tensor objects created by ops known as *source operations*.
    Source operations are operations that create data, usually without using any previously
    processed inputs. With these operations we can create scalars, as we already encountered
    with the `tf.constant()` method, as well as arrays and other types of data.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow中的每个Tensor对象都有属性，如`name`、`shape`和`dtype`，帮助识别和设置该对象的特性。在创建节点时，这些属性是可选的，当缺失时TensorFlow会自动设置。在下一节中，我们将查看这些属性。我们将通过查看由称为*源操作*的操作创建的Tensor对象来实现。源操作是创建数据的操作，通常不使用任何先前处理过的输入。通过这些操作，我们可以创建标量，就像我们已经使用`tf.constant()`方法遇到的那样，以及数组和其他类型的数据。
- en: Data Types
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据类型
- en: The basic units of data that pass through a graph are numerical, Boolean, or
    string elements.  When we print out the Tensor object `c ` from our last code
    example, we see that its data type is a floating-point number. Since we didn’t
    specify the type of data, TensorFlow inferred it automatically. For example `5`
    is regarded as an integer, while anything with a decimal point, like `5.1`, is
    regarded as a floating-point number.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 通过图传递的数据的基本单位是数字、布尔值或字符串元素。当我们打印出上一个代码示例中的Tensor对象`c`时，我们看到它的数据类型是浮点数。由于我们没有指定数据类型，TensorFlow会自动推断。例如，`5`被视为整数，而带有小数点的任何内容，如`5.1`，被视为浮点数。
- en: 'We can explicitly choose what data type we want to work with by specifying
    it when we create the Tensor object. We can see what type of data was set for
    a given Tensor object by using the attribute `dtype`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在创建Tensor对象时指定数据类型来明确选择要使用的数据类型。我们可以使用属性`dtype`来查看给定Tensor对象设置的数据类型：
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Explicitly asking for (appropriately sized) integers is on the one hand more
    memory conserving, but on the other may result in reduced accuracy as a consequence
    of not tracking digits after the decimal point.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 明确要求（适当大小的）整数一方面更节省内存，但另一方面可能会导致减少精度，因为不跟踪小数点后的数字。
- en: Casting
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换
- en: 'It is important to make sure our data types match throughout the graph—performing
    an operation with two nonmatching data types will result in an exception. To change
    the data type setting of a Tensor object, we can use the `tf.cast()` operation,
    passing the relevant Tensor and the new data type of interest as the first and
    second arguments, respectively:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 确保图中的数据类型匹配非常重要——使用两个不匹配的数据类型进行操作将导致异常。要更改Tensor对象的数据类型设置，我们可以使用`tf.cast()`操作，将相关的Tensor和感兴趣的新数据类型作为第一个和第二个参数传递：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: TensorFlow supports many data types. These are listed in [Table 3-2](#table0302).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow支持许多数据类型。这些列在[表3-2](#table0302)中。
- en: Table 3-2\. Supported Tensor data types
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的张量数据类型表3-2
- en: '| Data type | Python type | Description |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: 数据类型 | Python类型 | 描述 |
- en: '| --- | --- | --- |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `DT_FLOAT` | `tf.float32` | 32-bit floating point. |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| `DT_FLOAT` | `tf.float32` | 32位浮点数。 |'
- en: '| `DT_DOUBLE` | `tf.float64` | 64-bit floating point. |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| `DT_DOUBLE` | `tf.float64` | 64位浮点数。 |'
- en: '| `DT_INT8` | `tf.int8` | 8-bit signed integer. |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| `DT_INT8` | `tf.int8` | 8位有符号整数。 |'
- en: '| `DT_INT16` | `tf.int16` | 16-bit signed integer. |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| `DT_INT16` | `tf.int16` | 16位有符号整数。 |'
- en: '| `DT_INT32` | `tf.int32` | 32-bit signed integer. |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| `DT_INT32` | `tf.int32` | 32位有符号整数。 |'
- en: '| `DT_INT64` | `tf.int64` | 64-bit signed integer. |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| `DT_INT64` | `tf.int64` | 64位有符号整数。 |'
- en: '| `DT_UINT8` | `tf.uint8` | 8-bit unsigned integer. |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| `DT_UINT8` | `tf.uint8` | 8位无符号整数。 |'
- en: '| `DT_UINT16` | `tf.uint16` | 16-bit unsigned integer. |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| `DT_UINT16` | `tf.uint16` | 16位无符号整数。 |'
- en: '| `DT_STRING` | `tf.string` | Variable-length byte array. Each element of a
    Tensor is a byte array. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| `DT_STRING` | `tf.string` | 变长字节数组。张量的每个元素都是一个字节数组。 |'
- en: '| `DT_BOOL` | `tf.bool` | Boolean. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| `DT_BOOL` | `tf.bool` | 布尔值。 |'
- en: '| `DT_COMPLEX64` | `tf.complex64` | Complex number made of two 32-bit floating
    points: real and imaginary parts. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| `DT_COMPLEX64` | `tf.complex64` | 由两个32位浮点数组成的复数：实部和虚部。 |'
- en: '| `DT_COMPLEX128` | `tf.complex128` | Complex number made of two 64-bit floating
    points: real and imaginary parts. |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| `DT_COMPLEX128` | `tf.complex128` | 由两个64位浮点数组成的复数：实部和虚部。 |'
- en: '| `DT_QINT8` | `tf.qint8` | 8-bit signed integer used in quantized ops. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| `DT_QINT8` | `tf.qint8` | 用于量化操作的8位有符号整数。 |'
- en: '| `DT_QINT32` | `tf.qint32` | 32-bit signed integer used in quantized ops.
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| `DT_QINT32` | `tf.qint32` | 用于量化操作的32位有符号整数。 |'
- en: '| `DT_QUINT8` | `tf.quint8` | 8-bit unsigned integer used in quantized ops.
      |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| `DT_QUINT8` | `tf.quint8` | 用于量化操作的8位无符号整数。 |'
- en: Tensor Arrays and Shapes
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量数组和形状
- en: A source of potential confusion is that two different things are referred to
    by the name, *Tensor*. As used in the previous sections, *Tensor* is the name
    of an object used in the Python API as a handle for the result of an operation
    in the graph. However, *tensor* is also a mathematical term for *n*-dimensional
    arrays. For example, a 1×1 tensor is a scalar, a 1×*n* tensor is a vector, an
    *n*×*n* tensor is a matrix, and an *n*×*n*×*n* tensor is just a three-dimensional
    array. This, of course, generalizes to any dimension. TensorFlow regards all the
    data units that flow in the graph as tensors, whether they are multidimensional
    arrays, vectors, matrices, or scalars. The TensorFlow objects called Tensors are
    named after these mathematical tensors.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在混淆的一个来源是，*Tensor*这个名字指的是两个不同的东西。在前面的部分中使用的*Tensor*是Python API中用作图中操作结果的句柄的对象的名称。然而，*tensor*也是一个数学术语，用于表示*n*维数组。例如，一个1×1的张量是一个标量，一个1×*n*的张量是一个向量，一个*n*×*n*的张量是一个矩阵，一个*n*×*n*×*n*的张量只是一个三维数组。当然，这可以推广到任何维度。TensorFlow将流经图中的所有数据单元都视为张量，无论它们是多维数组、向量、矩阵还是标量。TensorFlow对象称为Tensors是根据这些数学张量命名的。
- en: To clarify the distinction between the two, from now on we will refer to the
    former as Tensors with a capital T and the latter as tensors with a lowercase
    t.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清两者之间的区别，从现在开始我们将前者称为大写T的张量，将后者称为小写t的张量。
- en: As with `dtype`, unless stated explicitly, TensorFlow automatically infers the
    shape of the data. When we printed out the Tensor object at the beginning of this
    section, it showed that its shape was `()`, corresponding to the shape of a scalar.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 与`dtype`一样，除非明确说明，TensorFlow会自动推断数据的形状。当我们在本节开始时打印出Tensor对象时，它显示其形状为`()`，对应于标量的形状。
- en: 'Using scalars is good for demonstration purposes, but most of the time it’s
    much more practical to work with multidimensional arrays. To initialize high-dimensional
    arrays, we can use Python lists or NumPy arrays as inputs. In the following example,
    we use as inputs a 2×3 matrix using a Python list and then a 3D NumPy array of
    size 2×3×4 (two matrices of size 3×4):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '使用标量对于演示目的很好，但大多数情况下，使用多维数组更实用。要初始化高维数组，我们可以使用Python列表或NumPy数组作为输入。在以下示例中，我们使用Python列表作为输入，创建一个2×3矩阵，然后使用一个大小为2×3×4的3D
    NumPy数组作为输入（两个大小为3×4的矩阵）： '
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `get_shape()` method returns the shape of the tensor as a tuple of integers.
    The number of integers corresponds to the number of dimensions of the tensor,
    and each integer is the number of array entries along that dimension. For example,
    a shape of `(2,3)` indicates a matrix, since it has two integers, and the size
    of the matrix is 2×3.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_shape()`方法返回张量的形状，以整数元组的形式。整数的数量对应于张量的维数，每个整数是沿着该维度的数组条目的数量。例如，形状为`(2,3)`表示一个矩阵，因为它有两个整数，矩阵的大小为2×3。'
- en: Other types of source operation constructors are very useful for initializing
    constants in TensorFlow, like filling a constant value, generating random numbers,
    and creating sequences.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类型的源操作构造函数对于在TensorFlow中初始化常量非常有用，比如填充常量值、生成随机数和创建序列。
- en: Random-number generators have special importance as they are used in many cases
    to create the initial values for TensorFlow Variables, which will be introduced
    shortly. For example, we can generate random numbers from a *normal distribution*
    using `tf.random.normal()`, passing the shape, mean, and standard deviation as
    the first, second, and third arguments, respectively. Another two examples for
    useful random initializers are the *truncated normal* that, as its name implies,
    cuts off all values below and above two standard deviations from the mean, and
    the *uniform* initializer that samples values uniformly within some interval `[a,b)`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 随机数生成器在许多情况下具有特殊重要性，因为它们用于创建TensorFlow变量的初始值，这将很快介绍。例如，我们可以使用`tf.random.normal()`从*正态分布*生成随机数，分别将形状、平均值和标准差作为第一、第二和第三个参数传递。另外两个有用的随机初始化器示例是*截断正态*，它像其名称暗示的那样，截断了所有低于和高于平均值两个标准差的值，以及*均匀*初始化器，它在某个区间`[a,b)`内均匀采样值。
- en: Examples of sampled values for each of these methods are shown in [Figure 3-5](#random_samples).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的示例值显示在[图3-5](#random_samples)中。
- en: '![](assets/letf_0305.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/letf_0305.png)'
- en: Figure 3-5\. 50,000 random samples generated from (A) standard normal distribution,
    (B) truncated normal, and (C) uniform [–2,2).
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5\. 从（A）标准正态分布、（B）截断正态分布和（C）均匀分布[–2,2]生成的50,000个随机样本。
- en: Those who are familiar with NumPy will recognize some of the initializers, as
    they share the same syntax. One example is the sequence generator `tf.linspace(a,
    b, *n*)` that creates `*n*` evenly spaced values from `a` to `b`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉NumPy的人会认识到一些初始化器，因为它们具有相同的语法。一个例子是序列生成器`tf.linspace(a, b, *n*)`，它从`a`到`b`创建`*n*`个均匀间隔的值。
- en: 'A feature that is convenient to use when we want to explore the data content
    of an object is `tf.InteractiveSession()`. Using it and the `.eval()` method,
    we can get a full look at the values without the need to constantly refer to the
    session object:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要探索对象的数据内容时，使用`tf.InteractiveSession()`是一个方便的功能。使用它和`.eval()`方法，我们可以完整查看值，而无需不断引用会话对象：
- en: '[PRE14]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Interactive sessions
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交互式会话
- en: '`tf.InteractiveSession()` allows you to replace the usual `tf.Session()`, so
    that you don’t need a variable holding the session for running ops. This can be
    useful in interactive Python environments, like when writing IPython notebooks,
    for instance.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.InteractiveSession()`允许您替换通常的`tf.Session()`，这样您就不需要一个变量来保存会话以运行操作。这在交互式Python环境中非常有用，比如在编写IPython笔记本时。'
- en: We’ve mentioned only a few of the available source operations. [Table 3-2](#table0303)
    provides short descriptions of more useful initializers.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只提到了一些可用源操作。[表3-2](#table0303)提供了更多有用初始化器的简短描述。
- en: '| TensorFlow operation | Description |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| TensorFlow操作 | 描述 |'
- en: '| --- | --- |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `tf.constant(*value*)` | Creates a tensor populated with the value or values
    specified by the argument `*value*` |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| `tf.constant(*value*)` | 创建一个由参数`*value*`指定的值或值填充的张量 |'
- en: '| `tf.fill(*shape*, *value*)` | Creates a tensor of shape `*shape*` and fills
    it with `*value*`  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| `tf.fill(*shape*, *value*)` | 创建一个形状为`*shape*`的张量，并用`*value*`填充 |'
- en: '| `tf.zeros(*shape*)` | Returns a tensor of shape `*shape*` with all elements
    set to 0 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| `tf.zeros(*shape*)` | 返回一个形状为`*shape*`的张量，所有元素都设置为0 |'
- en: '| `tf.zeros_like(*tensor*)` | Returns a tensor of the same type and shape as
    `*tensor*` with all elements set to 0 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| `tf.zeros_like(*tensor*)` | 返回一个与`*tensor*`相同类型和形状的张量，所有元素都设置为0 |'
- en: '| `tf.ones(*shape*)` | Returns a tensor of shape `*shape*` with all elements
    set to 1 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| `tf.ones(*shape*)` | 返回一个形状为`*shape*`的张量，所有元素都设置为1 |'
- en: '| `tf.ones_like(*tensor*)` | Returns a tensor of the same type and shape as
    `*tensor*` with all elements set to 1 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| `tf.ones_like(*tensor*)` | 返回一个与`*tensor*`相同类型和形状的张量，所有元素都设置为1 |'
- en: '| `tf.random_normal(*shape*, *mean*, *stddev*)` | Outputs random values from
    a normal distribution |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| `tf.random_normal(*shape*, *mean*, *stddev*)` | 从正态分布中输出随机值 |'
- en: '| `tf.truncated_normal(*shape*, *mean*, *stddev*)` | Outputs random values
    from a truncated normal distribution (values whose magnitude is more than two
    standard deviations from the mean are dropped and re-picked) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| `tf.truncated_normal(*shape*, *mean*, *stddev*)` | 从截断正态分布中输出随机值（其大小超过平均值两个标准差的值被丢弃并重新选择）
    |'
- en: '| `tf.random_uniform(*shape*, *minval*, *maxval*)` | Generates values from
    a uniform distribution in the range `[*minval*, *maxval*)` |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| `tf.random_uniform(*shape*, *minval*, *maxval*)` | 在范围`[*minval*, *maxval*)`内生成均匀分布的值
    |'
- en: '| `tf.random_shuffle(*tensor*)` | Randomly shuffles a tensor along its first
    dimension  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| `tf.random_shuffle(*tensor*)` | 沿着其第一个维度随机洗牌张量 |'
- en: Matrix multiplication
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵乘法
- en: This very useful arithmetic operation is performed in TensorFlow via the `tf.matmul(A,B)`
    function for two Tensor objects `A` and `B`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个非常有用的算术运算是通过TensorFlow中的`tf.matmul(A,B)`函数来执行的，其中`A`和`B`是两个Tensor对象。
- en: 'Say we have a Tensor storing a matrix `A` and another storing a vector `x`,
    and we wish to compute the matrix product of the two:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个存储矩阵`A`的张量和另一个存储向量`x`的张量，并且我们希望计算这两者的矩阵乘积：
- en: '*Ax* = *b*'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*Ax* = *b*'
- en: Before using `matmul()`, we need to make sure both have the same number of dimensions
    and that they are aligned correctly with respect to the intended multiplication.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`matmul()`之前，我们需要确保两者具有相同数量的维度，并且它们与预期的乘法正确对齐。
- en: 'In the following example, a matrix `A` and a vector `x` are created:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，创建了一个矩阵`A`和一个向量`x`：
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In order to multiply them, we need to add a dimension to `x`, transforming it
    from a 1D vector to a 2D single-column matrix.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将它们相乘，我们需要向`x`添加一个维度，将其从一维向量转换为二维单列矩阵。
- en: 'We can add another dimension by passing the Tensor to `tf.expand_dims()`, together
    with the position of the added dimension as the second argument. By adding another
    dimension in the second position (index 1), we get the desired outcome:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将张量传递给 `tf.expand_dims()` 来添加另一个维度，以及作为第二个参数的添加维度的位置。通过在第二个位置（索引 1）添加另一个维度，我们可以得到期望的结果：
- en: '[PRE16]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If we want to flip an array, for example turning a column vector into a row
    vector or vice versa, we can use the `tf.transpose()` function.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想翻转一个数组，例如将列向量转换为行向量或反之亦然，我们可以使用 `tf.transpose()` 函数。
- en: Names
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 名称
- en: 'Each Tensor object also has an identifying name. This name is an intrinsic
    string name, not to be confused with the name of the variable. As with `dtype`,
    we can use the `.name` attribute to see the name of the object:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 每个张量对象也有一个标识名称。这个名称是一个固有的字符串名称，不要与变量的名称混淆。与 `dtype` 一样，我们可以使用 `.name` 属性来查看对象的名称：
- en: '[PRE17]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The name of the Tensor object is simply the name of its corresponding operation
    (“c”; concatenated with a colon), followed by the index of that tensor in the
    outputs of the operation that produced it—it is possible to have more than one.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 张量对象的名称只是其对应操作的名称（“c”；与冒号连接），后跟产生它的操作的输出中的张量的索引——可能有多个。
- en: Duplicate names
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重复的名称
- en: Objects residing within the same graph cannot have the same name—TensorFlow
    forbids it. As a consequence, it will automatically add an underscore and a number
    to distinguish the two. Of course, both objects can have the same name when they
    are associated with different graphs.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一图中的对象不能具有相同的名称——TensorFlow禁止这样做。因此，它会自动添加下划线和数字以区分两者。当它们与不同的图关联时，当然，这两个对象可以具有相同的名称。
- en: Name scopes
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 名称范围
- en: 'Sometimes when dealing with a large, complicated graph, we would like to create
    some node grouping to make it easier to follow and manage. For that we can hierarchically
    group nodes together by name. We do so by using `tf.name_scope("*prefix*")` together
    with the useful `with` clause again:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 有时在处理大型、复杂的图时，我们希望创建一些节点分组，以便更容易跟踪和管理。为此，我们可以通过名称对节点进行层次分组。我们可以使用 `tf.name_scope("*prefix*")`
    以及有用的 `with` 子句来实现：
- en: '[PRE18]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this example we’ve grouped objects contained in variables `c2` and `c3` under
    the scope `prefix_name`, which shows up as a prefix in their names.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将包含在变量 `c2` 和 `c3` 中的对象分组到作用域 `prefix_name` 下，这显示为它们名称中的前缀。
- en: Prefixes are especially useful when we would like to divide a graph into subgraphs
    with some semantic meaning. These parts can later be used, for instance, for visualization
    of the graph structure.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望将图分成具有一定语义意义的子图时，前缀特别有用。这些部分以后可以用于可视化图结构。
- en: Variables, Placeholders, and Simple Optimization
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变量、占位符和简单优化
- en: 'In this section we will cover two important types of Tensor objects: Variables
    and placeholders. We then move forward to the main event: optimization. We will
    briefly talk about all the basic components for optimizing a model, and then do
    some simple demonstration that puts everything together.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍两种重要的张量对象类型：变量和占位符。然后我们将继续进行主要事件：优化。我们将简要讨论优化模型的所有基本组件，然后进行一些简单的演示，将所有内容整合在一起。
- en: Variables
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变量
- en: The optimization process serves to tune the parameters of some given model.
    For that purpose, TensorFlow uses special objects called *Variables*. Unlike other
    Tensor objects that are “refilled” with data each time we run the session, Variables
    can maintain a fixed state in the graph. This is important because their current
    state might influence how they change in the following iteration. Like other Tensors,
    Variables can be used as input for other operations in the graph.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 优化过程用于调整给定模型的参数。为此，TensorFlow使用称为 *变量* 的特殊对象。与其他每次运行会话时都会“重新填充”数据的张量对象不同，变量可以在图中保持固定状态。这很重要，因为它们当前的状态可能会影响它们在下一次迭代中的变化。与其他张量一样，变量可以用作图中其他操作的输入。
- en: Using Variables is done in two stages. First we call the `tf.Variable()` function
    in order to create a Variable and define what value it will be initialized with.
    We then have to explicitly perform an initialization operation by running the
    session with the `tf.global_variables_initializer()` method, which allocates the
    memory for the Variable and sets its initial values.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 使用变量分为两个阶段。首先，我们调用 `tf.Variable()` 函数来创建一个变量并定义它将被初始化的值。然后，我们必须显式执行初始化操作，通过使用
    `tf.global_variables_initializer()` 方法运行会话，该方法为变量分配内存并设置其初始值。
- en: 'Like other Tensor objects, Variables are computed only when the model runs,
    as we can see in the following example:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他张量对象一样，变量只有在模型运行时才会计算，如下例所示：
- en: '[PRE19]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Note that if we run the code again, we see that a new variable is created each
    time, as indicated by the automatic concatenation of `_1` to its name:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果我们再次运行代码，我们会看到每次都会创建一个新变量，这可以通过自动将 `_1` 连接到其名称来表示：
- en: '[PRE20]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This could be very inefficient when we want to reuse the model (complex models
    could have many variables!); for example, when we wish to feed it with several
    different inputs. To reuse the same variable, we can use the `tf.get_variables()`
    function instead of `tf.Variable()`. More on this can be found in [“Model Structuring”](app01.html#model_structuring)
    of the appendix.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要重用模型时（复杂模型可能有许多变量！）可能会非常低效；例如，当我们希望用多个不同的输入来喂它时。为了重用相同的变量，我们可以使用 `tf.get_variables()`
    函数而不是 `tf.Variable()`。有关更多信息，请参阅附录中的 [“模型结构”](app01.html#model_structuring)。
- en: Placeholders
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 占位符
- en: So far we’ve used source operations to create our input data. TensorFlow, however,
    has designated built-in structures for feeding input values. These structures
    are called *placeholders*. Placeholders can be thought of as empty Variables that
    will be filled with data later on. We use them by first constructing our graph
    and only when it is executed feeding them with the input data.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用源操作来创建我们的输入数据。然而，TensorFlow为输入值提供了专门的内置结构。这些结构被称为*占位符*。占位符可以被认为是将在稍后填充数据的空变量。我们首先构建我们的图形，只有在执行时才用输入数据填充它们。
- en: 'Placeholders have an optional `shape` argument. If a shape is not fed or is
    passed as `None`, then the placeholder can be fed with data of any size. It is
    common to use `None` for the dimension of a matrix that corresponds to the number
    of samples (usually rows), while having the length of the features (usually columns) fixed:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 占位符有一个可选的`shape`参数。如果没有提供形状或传递为`None`，那么占位符可以接受任何大小的数据。通常在对应于样本数量（通常是行）的矩阵维度上使用`None`，同时固定特征的长度（通常是列）：
- en: '[PRE21]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Whenever we define a placeholder, we must feed it with some input values or
    else an exception will be thrown. The input data is passed to the `session.run()`
    method as a dictionary, where each key corresponds to a placeholder variable name,
    and the matching values are the data values given in the form of a list or a NumPy
    array:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们定义一个占位符，我们必须为它提供一些输入值，否则将抛出异常。输入数据通过`session.run()`方法传递给一个字典，其中每个键对应一个占位符变量名，匹配的值是以列表或NumPy数组形式给出的数据值：
- en: '[PRE22]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s see how it looks with another graph example, this time with placeholders
    for two inputs: a matrix `x` and a vector `w`. These inputs are matrix-multiplied
    to create a five-unit vector `xw` and added with a constant vector `b` filled
    with the value `-1`. Finally, the variable `s` takes the maximum value of that
    vector by using the `tf.reduce_max()` operation. The word *reduce* is used because
    we are reducing a five-unit vector to a single scalar:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看另一个图形示例，这次使用两个输入的占位符：一个矩阵`x`和一个向量`w`。这些输入进行矩阵乘法，创建一个五单位向量`xw`，并与填充值为`-1`的常量向量`b`相加。最后，变量`s`通过使用`tf.reduce_max()`操作取该向量的最大值。单词*reduce*之所以被使用，是因为我们将一个五单位向量减少为一个标量：
- en: '[PRE23]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Optimization
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化
- en: Now we turn to optimization. We first describe the basics of training a model, giving
    a short description of each component in the process, and show how it is performed
    in TensorFlow. We then demonstrate a full working example of an optimization process
    of a simple regression model.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向优化。我们首先描述训练模型的基础知识，对过程中的每个组件进行简要描述，并展示在TensorFlow中如何执行。然后我们演示一个简单回归模型优化过程的完整工作示例。
- en: Training to predict
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练预测
- en: We have some target variable <math><mi>y</mi></math> , which we want to explain
    using some feature vector <math alttext="x"><mi>x</mi></math> . To do so, we first
    choose a model that relates the two. Our training data points will be used for “tuning”
    the model so that it best captures the desired relation. In the following chapters
    we focus on deep neural network models, but for now we will settle for a simple
    regression problem.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一些目标变量<math><mi>y</mi></math>，我们希望用一些特征向量<math alttext="x"><mi>x</mi></math>来解释它。为此，我们首先选择一个将两者联系起来的模型。我们的训练数据点将用于“调整”模型，以便最好地捕捉所需的关系。在接下来的章节中，我们将专注于深度神经网络模型，但现在我们将满足于一个简单的回归问题。
- en: 'Let’s start by describing our regression model:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从描述我们的回归模型开始：
- en: '*f*(*x*[*i*]) = *w*^(*T*)*x*[*i*] + *b*'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*x*[*i*]) = *w*^(*T*)*x*[*i*] + *b*'
- en: (*w* is initialized as a row vector; therefore, transposing *x* will yield the
    same result as in the equation above.)
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: （*w*被初始化为行向量；因此，转置*x*将产生与上面方程中相同的结果。）
- en: '*y*[*i*] = *f*(*x*[*i*]) + *ε*[*i*]'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*y*[*i*] = *f*(*x*[*i*]) + *ε*[*i*]'
- en: '*f*(*x*[*i*]) is assumed to be a linear combination of some input data *x*[*i*],
    with a set of weights *w* and an intercept *b*. Our target output *y*[*i*] is
    a noisy version of *f*(*x*[*i*]) after being summed with Gaussian noise *ε*[*i*] (where *i* denotes
    a given sample).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*x*[*i*])被假定为一些输入数据*x*[*i*]的线性组合，带有一组权重*w*和一个截距*b*。我们的目标输出*y*[*i*]是*f*(*x*[*i*])与高斯噪声*ε*[*i*]相加后的嘈杂版本（其中*i*表示给定样本）。'
- en: 'As in the previous example, we will need to create the appropriate placeholders
    for our input and output data and Variables for our weights and intercept:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的例子一样，我们需要为输入和输出数据创建适当的占位符，为权重和截距创建变量：
- en: '[PRE24]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Once the placeholders and Variables are defined, we can write down our model.
    In this example, it’s simply a multivariate linear regression—our predicted output
    `y_pred` is the result of a matrix multiplication of our input container `x` and
    our weights `w` plus a bias term `b`:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了占位符和变量，我们就可以写下我们的模型。在这个例子中，它只是一个多元线性回归——我们预测的输出`y_pred`是我们的输入容器`x`和我们的权重`w`以及一个偏置项`b`的矩阵乘法的结果：
- en: '[PRE25]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Defining a loss function
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义损失函数
- en: Next, we need a good measure with which we can evaluate the model’s performance.
    To capture the discrepancy between our model’s predictions and the observed targets, we
    need a measure reflecting “distance.” This distance is often referred to as an
    *objective* or a *loss* function, and we optimize the model by finding the set
    of parameters (weights and bias in this case) that minimize it.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个好的度量标准，用来评估模型的性能。为了捕捉我们模型预测和观察目标之间的差异，我们需要一个反映“距离”的度量标准。这个距离通常被称为一个*目标*或*损失*函数，我们通过找到一组参数（在这种情况下是权重和偏置）来最小化它来优化模型。
- en: There is no ideal loss function, and choosing the most suitable one is often
    a blend of art and science. The choice may depend on several factors, like the
    assumptions of our model, how easy it is to minimize, and what types of mistakes
    we prefer to avoid.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 没有理想的损失函数，选择最合适的损失函数通常是艺术和科学的结合。选择可能取决于几个因素，比如我们模型的假设、它有多容易最小化，以及我们更喜欢避免哪种类型的错误。
- en: MSE and cross entropy
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 均方误差和交叉熵
- en: 'Perhaps the most commonly used loss is the MSE (mean squared error), where
    for all samples we average the squared distances between the real target and what
    our model predicts across samples:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 也许最常用的损失是MSE（均方误差），其中对于所有样本，我们平均了真实目标与我们的模型在样本间预测之间的平方距离：
- en: <math alttext="upper L left-parenthesis y comma ModifyingAbove y With caret
    right-parenthesis equals StartFraction 1 Over n EndFraction normal upper Sigma
    Subscript i equals 1 Superscript n Baseline left-parenthesis y Subscript i Baseline
    minus ModifyingAbove y With caret Subscript i Baseline right-parenthesis squared"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mi>y</mi> <mo>,</mo> <mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <msubsup><mi>Σ</mi>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper L left-parenthesis y comma ModifyingAbove y With caret
    right-parenthesis equals StartFraction 1 Over n EndFraction normal upper Sigma
    Subscript i equals 1 Superscript n Baseline left-parenthesis y Subscript i Baseline
    minus ModifyingAbove y With caret Subscript i Baseline right-parenthesis squared"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mi>y</mi> <mo>,</mo> <mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <msubsup><mi>Σ</mi>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: This loss has intuitive interpretation—it minimizes the mean square difference
    between an observed value and the model’s fitted value (these differences are
    referred to as *residuals*).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这种损失具有直观的解释——它最小化了观察值与模型拟合值之间的均方差差异（这些差异被称为*残差*）。
- en: 'In our linear regression example, we take the difference between the vector `y_true`
    (*y*), the true targets, and `y_pred` (ŷ), the model’s predictions, and use `tf.square()`
    to compute the square of the difference vector. This operation is applied element-wise.
    We then average the squared differences using the `tf.reduce_mean()` function:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的线性回归示例中，我们取向量 `y_true`（*y*），真实目标，与 `y_pred`（ŷ），模型的预测之间的差异，并使用 `tf.square()`
    计算差异向量的平方。这个操作是逐元素应用的。然后使用 `tf.reduce_mean()` 函数对平方差异进行平均：
- en: '[PRE26]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Another very common loss, especially for categorical data, is the *cross entropy*,
    which we used in the softmax classifier in the previous chapter. The cross entropy
    is given by
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常常见的损失函数，特别适用于分类数据，是*交叉熵*，我们在上一章中在softmax分类器中使用过。交叉熵由以下公式给出
- en: <math display="block"><mrow><mi>H</mi> <mo stretchy="false">(</mo> <mi>p</mi>
    <mo>,</mo> <mi>q</mi> <mo stretchy="false">)</mo> <mo>=</mo> <mo>-</mo> <munder><mo>∑</mo>
    <mi>x</mi></munder> <mi>p</mi> <mo stretchy="false">(</mo> <mi>x</mi> <mo stretchy="false">)</mo>
    <mi>log</mi> <mi>q</mi> <mo stretchy="false">(</mo> <mi>x</mi> <mo stretchy="false">)</mo></mrow></math>
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>H</mi> <mo stretchy="false">(</mo> <mi>p</mi>
    <mo>,</mo> <mi>q</mi> <mo stretchy="false">)</mo> <mo>=</mo> <mo>-</mo> <munder><mo>∑</mo>
    <mi>x</mi></munder> <mi>p</mi> <mo stretchy="false">(</mo> <mi>x</mi> <mo stretchy="false">)</mo>
    <mi>log</mi> <mi>q</mi> <mo stretchy="false">(</mo> <mi>x</mi> <mo stretchy="false">)</mo></mrow></math>
- en: and for classification with a single correct label (as is the case in an overwhelming
    majority of the cases) reduces to the negative log of the probability placed by
    the classifier on the correct label.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有单个正确标签的分类（在绝大多数情况下都是这样），简化为分类器放置在正确标签上的概率的负对数。
- en: 'In TensorFlow:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中：
- en: '[PRE27]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Cross entropy is a measure of similarity between two distributions. Since the
    classification models used in deep learning typically output probabilities for
    each class, we can compare the true class (distribution *p*) with the probabilities
    of each class given by the model (distribution *q*). The more similar the two
    distributions, the smaller our cross entropy will be.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵是两个分布之间相似性的度量。由于深度学习中使用的分类模型通常为每个类别输出概率，我们可以将真实类别（分布 *p*）与模型给出的每个类别的概率（分布
    *q*）进行比较。两个分布越相似，我们的交叉熵就越小。
- en: The gradient descent optimizer
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度下降优化器
- en: The next thing we need to figure out is how to minimize the loss function. While
    in some cases it is possible to find the global minimum analytically (when it
    exists), in the great majority of cases we will have to use an optimization algorithm.
    Optimizers update the set of weights iteratively in a way that decreases the loss
    over time.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来需要弄清楚如何最小化损失函数。在某些情况下，可以通过解析方法找到全局最小值（存在时），但在绝大多数情况下，我们将不得不使用优化算法。优化器会迭代更新权重集，以逐渐减少损失。
- en: The most commonly used approach is gradient descent, where we use the loss’s
    gradient with respect to the set of weights. In slightly more technical terms,
    if our loss is some multivariate function *F*(*w̄*), then in the neighborhood
    of some point *w̄*[0], the “steepest” direction of decrease of *F*(*w̄*) is obtained
    by moving from *w̄*[0] in the direction of the negative gradient of *F* at *w̄*[0].
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的方法是梯度下降，其中我们使用损失相对于权重集的梯度。稍微更技术性的说法是，如果我们的损失是某个多元函数 *F*(*w̄*)，那么在某点 *w̄*[0]
    的邻域内，*F*(*w̄*) 的“最陡”减少方向是通过从 *w̄*[0] 沿着 *F* 在 *w̄*[0] 处的负梯度方向移动而获得的。
- en: '*So if* *w̄*[1] = *w̄*[0]-γ∇*F*(*w̄*[0]) *where ∇*F*(*w̄*[0]) is the gradient
    of *F* evaluated at *w̄*[0], then for a small enough γ:*'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*所以如果* *w̄*[1] = *w̄*[0]-γ∇*F*(*w̄*[0]) *其中 ∇*F*(*w̄*[0]) 是在 *w̄*[0] 处评估的 *F*
    的梯度，那么对于足够小的 γ：*'
- en: '*F*(*w̄*[0]) ⩾ *F*(*w̄*[1])'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '*F*(*w̄*[0]) ⩾ *F*(*w̄*[1])'
- en: The gradient descent algorithms work well on highly complicated network architectures
    and therefore are suitable for a wide variety of problems. More specifically,
    recent advances make it possible to compute these gradients by utilizing massively
    parallel systems, so the approach scales well with dimensionality (though it can
    still be painfully time-consuming for large real-world problems). While convergence
    to the global minimum is guaranteed for convex functions, for nonconvex problems
    (which are essentially all problems in the world of deep learning) they can get
    stuck in local minima. In practice, this is often good enough, as is evidenced
    by the huge success of the field of deep learning.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法在高度复杂的网络架构上表现良好，因此适用于各种问题。更具体地说，最近的进展使得可以利用大规模并行系统来计算这些梯度，因此这种方法在维度上具有很好的扩展性（尽管对于大型实际问题仍可能非常耗时）。对于凸函数，收敛到全局最小值是有保证的，但对于非凸问题（在深度学习领域基本上都是非凸问题），它们可能会陷入局部最小值。在实践中，这通常已经足够好了，正如深度学习领域的巨大成功所证明的那样。
- en: Sampling methods
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 采样方法
- en: The gradient of the objective is computed with respect to the model parameters
    and evaluated using a given set of input samples, *x*[*s*]. How many of the samples
    should we take for this calculation? Intuitively, it makes sense to calculate
    the gradient for the entire set of samples in order to benefit from the maximum
    amount of available information. This method, however, has some shortcomings. For
    example, it can be very slow and is intractable when the dataset requires more
    memory than is available.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 目标的梯度是针对模型参数计算的，并使用给定的输入样本集*x*[s]进行评估。对于这个计算，我们应该取多少样本？直觉上，计算整个样本集的梯度是有意义的，以便从最大可用信息中受益。然而，这种方法也有一些缺点。例如，当数据集需要的内存超过可用内存时，它可能会非常慢且难以处理。
- en: A more popular technique is the stochastic gradient descent (SGD), where instead
    of feeding the entire dataset to the algorithm for the computation of each step,
    a subset of the data is sampled sequentially. The number of samples ranges from
    one sample at a time to a few hundred, but the most common sizes are between around
    50 to around 500 (usually referred to as *mini-batches*).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更流行的技术是随机梯度下降（SGD），在这种技术中，不是将整个数据集一次性提供给算法进行每一步的计算，而是顺序地抽取数据的子集。样本数量从一次一个样本到几百个不等，但最常见的大小在大约50到大约500之间（通常称为*mini-batches*）。
- en: Using smaller batches usually works faster, and the smaller the size of the
    batch, the faster are the calculations. However, there is a trade-off in that
    small samples lead to lower hardware utilization and tend to have high variance, causing
    large fluctuations to the objective function. Nevertheless, it turns out that
    some fluctuations are beneficial since they enable the set of parameters to jump
    to new and potentially better local minima. Using a relatively smaller batch size
    is therefore effective in that regard, and is currently overall the preferred
    approach.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 通常使用较小的批次会更快，批次的大小越小，计算速度就越快。然而，这样做存在一个权衡，即小样本导致硬件利用率降低，并且往往具有较高的方差，导致目标函数出现大幅波动。然而，事实证明，一些波动是有益的，因为它们使参数集能够跳到新的、潜在更好的局部最小值。因此，使用相对较小的批次大小在这方面是有效的，目前是首选的方法。
- en: Gradient descent in TensorFlow
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow中的梯度下降
- en: TensorFlow makes it very easy and intuitive to use gradient descent algorithms.
    Optimizers in TensorFlow compute the gradients simply by adding new operations
    to the graph, and the gradients are calculated using automatic differentiation.
    This means, in general terms, that TensorFlow automatically computes the gradients
    on its own, “deriving” them from the operations and structure of the computation
    graph.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow非常容易和直观地使用梯度下降算法。TensorFlow中的优化器通过向图中添加新操作来计算梯度，并且梯度是使用自动微分计算的。这意味着，一般来说，TensorFlow会自动计算梯度，从计算图的操作和结构中“推导”出梯度。
- en: An important parameter to set is the algorithm’s learning rate, determining
    how aggressive each update iteration will be (or in other words, how large the
    step will be in the direction of the negative gradient). We want the decrease
    in the loss to be fast enough on the one hand, but on the other hand not large
    enough so that we over-shoot the target and end up at a point with a higher value
    of the loss function.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 设置的一个重要参数是算法的学习率，确定每次更新迭代的侵略性有多大（或者换句话说，负梯度方向的步长有多大）。我们希望损失的减少速度足够快，但另一方面又不要太大，以至于我们超过目标并最终到达损失函数值更高的点。
- en: 'We first create an optimizer by using the `GradientDescentOptimizer()` function
    with the desired learning rate. We then create a train operation that updates
    our variables by calling the `optimizer.minimize()` function and passing in the
    loss as an argument:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用所需的学习率使用`GradientDescentOptimizer()`函数创建一个优化器。然后，我们通过调用`optimizer.minimize()`函数并将损失作为参数传递来创建一个train操作，用于更新我们的变量：
- en: '[PRE28]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The train operation is then executed when it is fed to the `sess.run()` method.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当传递给`sess.run()`方法时，train操作就会执行。
- en: Wrapping it up with examples
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用示例总结
- en: 'We’re all set to go!  Let’s combine all the components we’ve discussed in this
    section and optimize the parameters of two models: linear and logistic regression.
    In these examples we will create synthetic data with known properties, and see
    how the model is able to recover these properties with the process of optimization.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备就绪！让我们将本节讨论的所有组件结合起来，优化两个模型的参数：线性回归和逻辑回归。在这些示例中，我们将创建具有已知属性的合成数据，并看看模型如何通过优化过程恢复这些属性。
- en: 'Example 1: linear regression'
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例1：线性回归
- en: In this problem we are interested in retrieving a set of weights *w* and a bias
    term *b*, assuming our target value is a linear combination of some input vector *x*,
    with an additional Gaussian noise *ε*[*i*] added to each sample.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题中，我们感兴趣的是检索一组权重*w*和一个偏差项*b*，假设我们的目标值是一些输入向量*x*的线性组合，每个样本还添加了一个高斯噪声*ε*[i]。
- en: 'For this exercise we will generate synthetic data using NumPy. We create 2,000
    samples of *x*, a vector with three features, take the inner product of each *x*
    sample with a set of weights *w* ([0.3, 0.5, 0.1]), and add a bias term *b* (–0.2) and
    Gaussian noise to the result:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用NumPy生成合成数据。我们创建了2,000个*x*样本，一个具有三个特征的向量，将每个*x*样本与一组权重*w*（[0.3,
    0.5, 0.1]）的内积取出，并添加一个偏置项*b*（-0.2）和高斯噪声到结果中：
- en: '[PRE29]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The noisy samples are shown in [Figure 3-6](#data_for_linear_regression).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 嘈杂的样本显示在[图3-6](#data_for_linear_regression)中。
- en: '![](assets/letf_0306.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/letf_0306.png)'
- en: 'Figure 3-6\. Generated data to use for linear regression: each filled circle
    represents a sample, and the dashed line shows the expected values without the
    noise component (the diagonal).'
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-6\. 用于线性回归的生成数据：每个填充的圆代表一个样本，虚线显示了没有噪声成分（对角线）的预期值。
- en: Next, we estimate our set of weights *w* and bias *b* by optimizing the model
    (i.e., finding the best parameters) so that its predictions match the real targets
    as closely as possible. Each iteration computes one update to the current parameters.
    In this example we run 10 iterations, printing our estimated parameters every
    5 iterations using the `sess.run()` method.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过优化模型（即找到最佳参数）来估计我们的权重*w*和偏置*b*，使其预测尽可能接近真实目标。每次迭代计算对当前参数的更新。在这个例子中，我们运行10次迭代，使用`sess.run()`方法在每5次迭代时打印我们估计的参数。
- en: 'Don’t forget to initialize the variables! In this example we initialize both
    the weights and the bias with zeros; however, there are “smarter” initialization
    techniques to choose, as we will see in the next chapters. We use name scopes
    to group together parts that are related to inferring the output, defining the
    loss, and setting and creating the train object:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记初始化变量！在这个例子中，我们将权重和偏置都初始化为零；然而，在接下来的章节中，我们将看到一些“更智能”的初始化技术可供选择。我们使用名称作用域来将推断输出、定义损失、设置和创建训练对象的相关部分分组在一起：
- en: '[PRE30]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'And we get the results:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到了结果：
- en: '[PRE31]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: After only 10 iterations, the estimated weights and bias are  <math alttext="ModifyingAbove
    w With caret"><mover accent="true"><mi>w</mi> <mo>^</mo></mover></math> = [0.301,
    0.498, 0.098] and <math alttext="ModifyingAbove b With caret"><mover accent="true"><mi>b</mi>
    <mo>^</mo></mover></math> = –0.198\. The original parameter values were *w* =
    [0.3,0.5,0.1] and *b* = –0.2.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 仅经过10次迭代，估计的权重和偏置分别为 <math alttext="ModifyingAbove w With caret"><mover accent="true"><mi>w</mi>
    <mo>^</mo></mover></math> = [0.301, 0.498, 0.098] 和 <math alttext="ModifyingAbove
    b With caret"><mover accent="true"><mi>b</mi> <mo>^</mo></mover></math> = -0.198。原始参数值为
    *w* = [0.3,0.5,0.1] 和 *b* = -0.2。
- en: Almost a perfect match!
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎完美匹配！
- en: 'Example 2: logistic regression'
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例2：逻辑回归
- en: 'Again we wish to retrieve the weights and bias components in a simulated data
    setting, this time in a logistic regression framework. Here the linear component *w*^T*x*
    + *b* is the input of a nonlinear function called the logistic function. What
    it effectively does is squash the values of the linear part into the interval [0,
    1]:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次希望在模拟数据设置中检索权重和偏置组件，这次是在逻辑回归框架中。这里，线性部分 *w*^T*x* + *b* 是一个称为逻辑函数的非线性函数的输入。它的有效作用是将线性部分的值压缩到区间 [0,
    1]：
- en: '*Pr*(*y*[*i*] = 1|*x*[*i*]) = <math><mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mo
    form="prefix">exp</mo> <mrow><mo>-</mo><mo>(</mo><mi>w</mi><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>+</mo><mi>b</mi><mo>)</mo></mrow></msup></mrow></mfrac></math>'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '*Pr*(*y*[*i*] = 1|*x*[*i*]) = <math><mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mo
    form="prefix">exp</mo> <mrow><mo>-</mo><mo>(</mo><mi>w</mi><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>+</mo><mi>b</mi><mo>)</mo></mrow></msup></mrow></mfrac></math>'
- en: We then regard these values as probabilities from which binary yes/1 or no/0
    outcomes are generated. This is the nondeterministic (noisy) part of the model.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这些值视为概率，从中生成二进制的是/1或否/0的结果。这是模型的非确定性（嘈杂）部分。
- en: The logistic function is more general, and can be used with a different set
    of parameters for the steepness of the curve and its maximum value. This special
    case of a logistic function we are using is also referred to as a *sigmoid function*.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑函数更加通用，可以使用不同的参数集合来控制曲线的陡峭程度和最大值。我们使用的这种逻辑函数的特殊情况也被称为*sigmoid函数*。
- en: 'We generate our samples by using the same set of weights and biases as in the
    previous example:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用与前面示例中相同的权重和偏置来生成我们的样本：
- en: '[PRE32]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The outcome samples before and after the binarization of the output are shown
    in [Figure 3-7](#data_for_logistic_regression).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出进行二值化之前和之后的结果样本显示在[图3-7](#data_for_logistic_regression)中。
- en: '![](assets/letf_0307.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/letf_0307.png)'
- en: 'Figure 3-7\. Generated data to use for logistic regression: each circle represents
    a sample. In the left plot we see the probabilities generated by inputting the
    linear combination of the input data to the logistic function. The right plot
    shows the binary target output, randomly sampled from the probabilities in the
    left image.'
  id: totrans-261
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7\. 用于逻辑回归的生成数据：每个圆代表一个样本。在左图中，我们看到通过将输入数据的线性组合输入到逻辑函数中生成的概率。右图显示了从左图中的概率中随机抽样得到的二进制目标输出。
- en: The only thing we need to change in the code is the loss function we use.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在代码中唯一需要更改的是我们使用的损失函数。
- en: 'The loss we want to use here is the binary version of the cross entropy, which
    is also the likelihood of the logistic regression model:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要在这里使用的损失函数是交叉熵的二进制版本，这也是逻辑回归模型的似然度：
- en: '[PRE33]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Luckily, TensorFlow already has a designated function we can use instead:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，TensorFlow已经有一个指定的函数可以代替我们使用：
- en: '[PRE34]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To which we simply need to pass the true outputs and the model’s linear predictions:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要传递真实输出和模型的线性预测：
- en: '[PRE35]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let’s see what we get:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们得到了什么：
- en: '[PRE36]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: It takes a few more iterations to converge, and more samples are required than
    in the previous linear regression example, but eventually we get results that
    are quite similar to the original chosen weights.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 需要更多的迭代才能收敛，比前面的线性回归示例需要更多的样本，但最终我们得到的结果与原始选择的权重非常相似。
- en: Summary
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we learned about computation graphs and what we can use them
    for. We saw how to create a graph and how to compute its outputs. We introduced
    the main building blocks of TensorFlow—the Tensor object, representing the graph’s
    operations, placeholders for our input data, and Variables we tune as part of
    the model training process. We learned about tensor arrays and covered the data
    type, shape, and name attributes. Finally, we discussed the model optimization
    process and saw how to implement it in TensorFlow. In the next chapter we will
    go into more advanced deep neural networks used in computer vision.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了计算图以及我们可以如何使用它们。我们看到了如何创建一个图以及如何计算它的输出。我们介绍了 TensorFlow 的主要构建模块——Tensor
    对象，代表图的操作，用于输入数据的占位符，以及作为模型训练过程中调整的变量。我们学习了张量数组，并涵盖了数据类型、形状和名称属性。最后，我们讨论了模型优化过程，并看到了如何在
    TensorFlow 中实现它。在下一章中，我们将深入探讨在计算机视觉中使用的更高级的深度神经网络。
