- en: 6 Dimensionality reduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 维度降低
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: t-distributed stochastic neighbor embedding
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t分布随机邻域嵌入
- en: Multidimensional scaling
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多维尺度
- en: Uniform manifold approximation and projection
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均匀流形近似与投影
- en: Python implementations of the algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法的Python实现
- en: Life is really simple, but we insist on making it complicated.—Confucius
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 生活其实很简单，但我们却坚持让它变得复杂。——孔子
- en: Simplicity is a virtue—both in life and in data science. We have discussed a
    lot of algorithms so far. A few of them are simple enough, and some of them are
    a bit complicated. In part 1 of the book, we studied simpler clustering algorithms,
    and in the last chapter, we examined advanced clustering algorithms. Similarly,
    we studied a few dimensionality algorithms like principal component analysis (PCA)
    in chapter 3\. Continuing on the same note, we will study three advanced dimensionality
    reduction techniques in this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 简单是一种美德——在生活中和在数据科学中。我们迄今为止讨论了很多算法。其中一些足够简单，而一些则稍微复杂。在本书的第一部分，我们研究了简单的聚类算法，在最后一章，我们考察了高级聚类算法。同样，我们在第3章研究了几个维度降低算法，如主成分分析（PCA）。继续同样的思路，我们将在本章研究三种高级维度降低技术。
- en: The advanced topics we cover in this and the next part of the book are meant
    to prepare you for complex problems. While you can apply these advanced solutions,
    it is always advisable to start with the classical solutions like PCA for dimensionality
    reduction. And if that solution doesn’t appropriately address the problem, then
    you can try the advanced solutions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章和下一部分书中涵盖的高级主题旨在为你准备复杂问题。虽然你可以应用这些高级解决方案，但始终建议从经典的解决方案开始，如主成分分析（PCA）进行维度降低。如果该解决方案不能适当地解决该问题，那么你可以尝试高级解决方案。
- en: Dimensionality reduction is one of the most sought-after solutions, particularly
    when we have a large number of variables. Recall the “curse of dimensionality”
    we discussed in chapter 3\. You are advised to refresh your memory on chapter
    3 before moving forward if needed. We will cover t-distributed stochastic neighbor
    embedding (t-SNE), multidimensional scaling (MDS), and uniform manifold approximation
    and projection (UMAP) in this chapter. This chapter will cover some mathematical
    concepts that create the foundation of the advanced techniques we are going to
    discuss. As always, the concept discussion will be followed by a Python solution.
    This chapter also has a short case study. We will also develop a solution using
    an images dataset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 维度降低是人们最渴望寻求的解决方案之一，尤其是在我们拥有大量变量时。回想一下我们在第3章讨论的“维度诅咒”。如果需要，建议你在继续前进之前复习第3章的内容。在本章中，我们将介绍t分布随机邻域嵌入（t-SNE）、多维尺度（MDS）和均匀流形近似与投影（UMAP）。本章将涵盖一些数学概念，这些概念构成了我们将讨论的高级技术的基石。一如既往，概念讨论之后将跟随一个Python解决方案。本章还包括一个简短的案例研究。我们还将使用图像数据集开发一个解决方案。
- en: 'There may be a dilemma in your mind: What is the level of mathematics required,
    and is an in-depth statistical knowledge a prerequisite? The answer is both yes
    and no. While having a mathematical understanding will allow you to understand
    the algorithms and appreciate the process in greater depth; at the same time,
    for real-world business implementation, sometimes one might want to skip the mathematics
    and directly move to the examples in Python. We suggest having at least more than
    a basic understanding of the mathematics to fully grasp the concept. In this book,
    we provide that level of mathematical support without going into too much depth,
    presenting instead an optimal mix of practical world and mathematical concepts.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你心中可能存在一个困境：需要多少数学水平，深入统计知识是否是先决条件？答案是既是也是。虽然拥有数学理解能力将使你能够理解算法并更深入地欣赏过程；同时，对于现实世界的商业实施，有时人们可能想跳过数学，直接转向Python中的示例。我们建议至少要有基本的数学理解，以便完全掌握概念。在这本书中，我们提供了这种水平的数学支持，而不深入探讨，而是呈现了实际世界和数学概念的优化组合。
- en: Welcome to the sixth chapter, and all the very best!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到第六章，祝一切顺利！
- en: 6.1 Technical toolkit
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 技术工具箱
- en: We will continue to use the same version of Python and Jupyter Notebook as we
    have used so far. The codes and datasets used in this chapter have been checked
    in at [https://mng.bz/XxOv](https://mng.bz/XxOv).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用迄今为止使用的相同版本的Python和Jupyter Notebook。本章中使用的代码和数据集已在[https://mng.bz/XxOv](https://mng.bz/XxOv)检查过。
- en: 'You will need to install `Keras` as an additional Python library in this chapter.
    Along with this, you will need the regular modules: `numpy`, `pandas`, `matplotlib`,
    `seaborn`, and `sklearn`.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您需要安装`Keras`作为额外的Python库。此外，您还需要以下常规模块：`numpy`、`pandas`、`matplotlib`、`seaborn`和`sklearn`。
- en: 6.2 Multidimensional scaling
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 多维尺度
- en: As you know, maps prove to be quite handy while traveling. Now imagine you are
    given a task. You receive distances between some cities around the world—for example,
    between London and New York, London and Paris, Paris and New Delhi, and so forth.
    Then you are asked to re-create the map from which these distances have been derived.
    If we have to re-create that 2D map, that will be through trial and error; we
    will make some assumptions and move ahead with the process. It will surely be
    a tiring exercise prone to error and quite time-consuming indeed. MDS can do this
    task easily for us.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，地图在旅行中非常有用。现在想象你接到了一个任务。你收到了世界各地一些城市之间的距离——例如，伦敦和纽约、伦敦和巴黎、巴黎和新德里等等。然后你被要求重新创建一个地图，这些距离就是从这个地图中得出的。如果我们必须重新创建那个二维地图，那将是通过试错；我们将做一些假设并继续这个过程。这肯定是一项既费时又容易出错的任务。
- en: NOTE  While thinking of the preceding example, ignore the fact that the earth
    is not flat, and assume that the distance measurement metric is constant—for example,
    there is no confusion in miles or kilometers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在思考前面的例子时，忽略地球不是平的这一事实，并假设距离测量指标是恒定的——例如，在英里或千米之间没有混淆。
- en: As an illustration, consider figure 6.1\. Formally put, if we have *x* data
    points, MDS can help us convert the information of the pairwise distance between
    these *x* points to a configuration of points in a Cartesian space. Or, simply
    put, MDS transforms a large dimensional dataset into a lower dimensional one and,
    in the process, keeps the distance or the similarity between the points the same.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，考虑图6.1。正式地说，如果我们有*x*个数据点，MDS可以帮助我们将这些*x*点之间的成对距离信息转换为笛卡尔空间中点的配置。或者简单地说，MDS将高维数据集转换为低维数据集，在这个过程中保持点之间的距离或相似性不变。
- en: '![figure](../Images/CH06_F01_Verdhan.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH06_F01_Verdhan.png)'
- en: Figure 6.1 Illustration of distance between the cities and if they are represented
    on a map. The figure is only to help develop an understanding and does not represent
    the actual results.
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1展示了城市之间的距离以及它们在地图上的表示。该图仅用于帮助理解，并不代表实际结果。
- en: 'To simplify, consider figure 6.2\. Here we have three points: A, B, and C.
    We are representing these points in a 3D space. Then we represent the three points
    in a 2D space, and finally they are represented in a 1D space. The distance between
    the points is not up to scale in the diagrams in the figure. The example represents
    the meaning of lowering the number of dimensions.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，考虑图6.2。这里我们有三个点：A、B和C。我们在三维空间中表示这些点。然后我们在二维空间中表示这三个点，最后在一维空间中表示。图中的图例中点之间的距离没有按比例。这个例子表示了降低维度数量的意义。
- en: '![figure](../Images/CH06_F02_Verdhan.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH06_F02_Verdhan.png)'
- en: Figure 6.2 Representation of three points
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.2三个点的表示
- en: Hence, in MDS, multidimensional data is reduced to a lower number of dimensions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在MDS中，多维数据被降低到更少的维度。
- en: 'There are three types of MDS algorithms:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: MDS算法有三种类型：
- en: Classical MDS
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经典MDS
- en: Metric multidimensional scaling
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 度量多维尺度
- en: Nonmetric multidimensional scaling
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非度量多维尺度
- en: 6.2.1 Classic MDS
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 经典MDS
- en: 'We will examine the metric MDS process in detail in the book, while we will
    cover the classical and nonmetric briefly. Imagine we have two points: *i* and
    *j*. Let us assume that the original distance between two points is *d*[*i*][*j*]
    and the corresponding distance in the lower dimensional space is *d*[*i*][*j*].'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在书中详细检查度量MDS的过程，而我们将简要介绍经典和非度量MDS。想象我们有两个点：*i*和*j*。让我们假设两点之间的原始距离是*d*[*i*][*j*]，而在低维空间中的对应距离是*d*[*i*][*j*]。
- en: 'In classical MDS, the distances between the points are treated as Euclidean
    distances, and the original and fitted distances are represented in the same metric.
    It means that if the original distances in a higher dimensional space are calculated
    using the Euclidean method, the fitted distances in the lower dimensional space
    are also calculated using Euclidean distance. We already know how to calculate
    Euclidean distances. For example, we have to find the distance between points
    *i* and *j*, and let’s say the distance is *d*[*i*][*j*]. The distance can be
    given by the Euclidean distance formula given by equation 6.1 in a 2D space:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典的多维尺度分析（MDS）中，点之间的距离被视为欧几里得距离，原始距离和拟合距离表示在相同的度量下。这意味着如果使用欧几里得方法在更高维空间中计算原始距离，那么在低维空间中计算的拟合距离也将使用欧几里得距离。我们已经知道如何计算欧几里得距离。例如，我们需要找到点
    *i* 和 *j* 之间的距离，假设距离为 *d*[*i*][*j*]。在二维空间中，距离可以通过方程6.1给出的欧几里得距离公式给出：
- en: '![figure](../Images/verdhan-ch6-eqs-0x.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/verdhan-ch6-eqs-0x.png)'
- en: (6.1)
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (6.1)
- en: Recall in chapter 2, we discussed other distance functions like Manhattan distance,
    Euclidean distance, etc. You are advised to refresh your memory on chapter 2.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 回想第2章，我们讨论了其他距离函数，如曼哈顿距离、欧几里得距离等。建议您复习第2章的内容。
- en: 6.2.2 Nonmetric MDS
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 非度量MDS
- en: 'We just now noted that Euclidean distance can be used to calculate the distance
    between two points. Sometimes it is not possible to take the actual values of
    the distances, like when *d*[*i*][*j*] is the result of an experiment where subjective
    assessments were made or, in other words, where a rank was allocated to the various
    data parameters. For example, if the distance between points 2 and 5 was at rank
    4 in the original data, in such a scenario, it will not be wise to use absolute
    values of *d*[*i*][*j*], and hence relative values or *rank values* have to be
    used. Here, distance can mean a kind of ranking—for example, who came first in
    a race. This is the process in nonmetric MDS. For example, imagine we have four
    points: A, B, C, and D. We wish to rank the respective distances between these
    four points. The respective combinations of points can be A and B, A and C, A
    and D, B and C, B and D, and C and D. Their distances can be ranked as shown in
    table 6.1.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才提到，欧几里得距离可以用来计算两点之间的距离。有时无法获取距离的实际值，例如当 *d*[*i*][*j*] 是实验结果，其中进行了主观评估，换句话说，为各种数据参数分配了排名时。例如，如果点2和点5之间的距离在原始数据中排名为4，在这种情况下，使用
    *d*[*i*][*j*] 的绝对值是不明智的，因此必须使用相对值或*排名值*。在这里，距离可以指一种排名——例如，谁在比赛中排名第一。这是非度量多维尺度分析的过程。例如，想象我们有四个点：A、B、C
    和 D。我们希望对这些四个点之间的相应距离进行排名。点的相应组合可以是 A 和 B、A 和 C、A 和 D、B 和 C、B 和 D 以及 C 和 D。它们的距离可以按照表6.1所示进行排名。
- en: Table 6.1 The respective distance between four points and the ranks of the distances
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.1 四点之间的相应距离及其距离的排名
- en: '| Pair of points | Distance | Ranks of the respective distances |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 点对 | 距离 | 相应距离的排名 |'
- en: '| --- | --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| A and B  | 100  | 3  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| A 和 B  | 100  | 3  |'
- en: '| A and C  | 105  | 4  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| A 和 C  | 105  | 4  |'
- en: '| A and D  | 95  | 2  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| A 和 D  | 95  | 2  |'
- en: '| B and C  | 205  | 6  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| B 和 C  | 205  | 6  |'
- en: '| B and D  | 150  | 5  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| B 和 D  | 150  | 5  |'
- en: '| C and D  | 55  | 1  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| C 和 D  | 55  | 1  |'
- en: So, in the nonmetric MDS method, instead of using the actual distances, we use
    the respective ranks of the distance. We next move on to the metric MDS method.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在非度量MDS方法中，我们不是使用实际距离，而是使用相应距离的排名。接下来，我们将转向度量MDS方法。
- en: We know that in classical MDS, the original and fitted distances are represented
    in the same metric. In metric MDS, it is assumed that the values of *d*[*i*][*j*]
    can be transformed into Euclidean distances by employing some parametric transformation
    on the datasets. In some articles, you might find classical and metric MDS used
    interchangeably.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，在经典的多维尺度分析（MDS）中，原始距离和拟合距离表示在相同的度量下。在度量MDS中，假设 *d*[*i*][*j*] 的值可以通过对数据集应用某些参数变换而转换为欧几里得距离。在某些文章中，您可能会发现经典和度量MDS被交替使用。
- en: In MDS, as a first step, the respective distances between the points are calculated.
    Once the respective distances have been calculated, then MDS will try to represent
    the higher dimensional data point in a lower dimensional space. To perform this,
    an optimization process has to be carried out so that the optimum number of resultant
    dimensions can be chosen. Hence, a loss function or cost function has to be optimized.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDS中，作为第一步，计算点之间的相应距离。一旦计算了相应的距离，MDS将尝试在低维空间中表示高维数据点。为了执行此操作，必须进行优化过程，以便选择最佳的结果维度数。因此，必须优化损失函数或成本函数。
- en: Cost function
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 成本函数
- en: We use algorithms to predict the values of a variable. For example, we might
    use some algorithm to predict the expected demand of a product next year. We would
    want the algorithm to predict as accurately as possible. Cost functions are a
    simple method to check the performance of the algorithms.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用算法来预测变量的值。例如，我们可能使用某些算法来预测明年产品的预期需求。我们希望算法尽可能准确地预测。成本函数是检查算法性能的简单方法。
- en: Cost function is a simple technique to measure the effectiveness of our algorithms.
    It is the most common method used to gauge the performance of a predictive model.
    It compares the original values and the predicted values by the algorithm and
    calculates how wrong the model is in its prediction.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数是一种简单的技术，用于衡量我们算法的有效性。它是衡量预测模型性能最常用的方法。它通过比较算法预测的原始值和预测值来计算模型在预测中的错误程度。
- en: As you would imagine, in an ideal solution, we would want the predicted values
    to be the same as the actual values, which is very difficult to achieve. If the
    predicted values differ a lot from the actual values, the output of a cost function
    is higher. If the predicted values are closer to the actual values, then the value
    of a cost function is lower. A robust solution is one that has the lowest value
    of the cost function. Hence, the objective to optimize any algorithm will be to
    minimize the value of the cost function. Cost function is also referred to as
    loss function; these two terms can be used interchangeably.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所想象的那样，在理想解决方案中，我们希望预测值与实际值相同，这非常难以实现。如果预测值与实际值差异很大，成本函数的输出就更高。如果预测值更接近实际值，那么成本函数的值就较低。一个稳健的解决方案是具有最低成本函数值的解决方案。因此，优化任何算法的目标将是使成本函数的值最小化。成本函数也被称为损失函数；这两个术语可以互换使用。
- en: 'In metric MDS, we can also call the cost function *stress.* It is just another
    name for cost function. The formula for stress is given in equation 6.2:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在度量MDS中，我们也可以将成本函数称为应力。这只是成本函数的另一个名称。应力的公式在方程6.2中给出：
- en: '![figure](../Images/verdhan-ch6-eqs-1x.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch6-eqs-1x.png)'
- en: (6.2)
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (6.2)
- en: In the equation,
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程中，
- en: Term Stress[*D*] is the value the MDS function has to minimize.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 术语应力[*D*] 是MDS函数需要最小化的值。
- en: The data points with the new set of coordinates in a lower dimensional space
    are represented by *x*[1], *x*[2], *x*[3]…. *x*[*N*].
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在低维空间中具有新坐标集的数据点表示为 *x*[1]，*x*[2]，*x*[3]… *x*[*N*]。
- en: The term ||*x*[*i*]– *x*[*j*]|| is the distance between two points in their
    lower dimensional space.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 术语 ||*x*[*i*]– *x*[*j*]|| 表示它们在低维空间中两点之间的距离。
- en: The term *d*[*i*][*j*] is the original distance between the two points in the
    original multidimensional space.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 术语 *d*[*i*][*j*] 是两点在原始多维空间中的原始距离。
- en: By looking at the equation, we can see that if the values of ||*x*[*i*]– *x*[*j*]||
    and *d*[*i*][*j*] are close to each other, the value of the resultant stress will
    be small.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察方程，我们可以看到，如果 ||*x*[*i*]– *x*[*j*]|| 和 *d*[*i*][*j*] 的值彼此接近，则结果应力的值将很小。
- en: NOTE  Minimizing the value of stress is the objective of the loss function.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：最小化应力的值是损失函数的目标。
- en: To optimize this loss function, we can use multiple approaches. One of the most
    famous methods is using a gradient descent that was originally proposed by Kruskal
    and Wish in 1978\. The gradient descent method is very simple to understand and
    can be explained using a simple analogy.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化这个损失函数，我们可以使用多种方法。其中最著名的方法是使用Kruskal和Wish在1978年最初提出的梯度下降法。梯度下降法非常容易理解，可以用一个简单的类比来解释。
- en: Imagine you are standing on top of a mountain and you want to get down. You
    want to choose the fastest path because you want to get down as fast as possible
    (no, you cannot jump!). So, to take the first step, you look around and, whichever
    is the steepest path, you take a step in that direction and reach a new point.
    Then again, you take a step in the steepest direction. This process is shown in
    the first diagram in figure 6.3\.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你站在山顶上，你想下山。你想要选择最快的路径，因为你想要尽可能快地下山（不，你不能跳下去！）。所以，为了迈出第一步，你四处张望，然后选择最陡峭的路径，朝那个方向迈出一步，到达一个新的点。然后，你再次朝最陡峭的方向迈出一步。这个过程在图6.3的第一幅图中展示。
- en: '![figure](../Images/CH06_F03_Verdhan.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F03_Verdhan.png)'
- en: Figure 6.3 A person standing on top of a mountain and trying to get down. The
    process of gradient descent follows this method (left). The actual process of
    optimization of a cost function in gradient descent process. Note that at the
    point of convergence, the value of the cost function is minimal (right).
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3 一个站在山顶上试图下山的人。梯度下降的过程遵循这种方法（左）。在梯度下降过程中成本函数优化的实际过程。注意，在收敛点上，成本函数的值是最小的（右）。
- en: Now say an algorithm has to achieve a similar feat; the process is represented
    in the right diagram in figure 6.3, wherein a loss function starts at a point
    and finally reaches the point of convergence. At this point of convergence, the
    cost function is minimal.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设一个算法必须完成类似的壮举；这个过程在图6.3的右图中表示，其中损失函数从一个点开始，最终达到收敛点。在这个收敛点上，成本函数是最小的。
- en: MDS differs from the other dimensionality reduction techniques. As compared
    to techniques like PCA, MDS does not make any assumptions about the dataset and
    hence can be used for a larger number of datasets. Moreover, MDS allows the use
    of any distance measurement metric. Unlike PCA, MDS is not an eigenvalue-eigenvector
    technique. Recall in PCA, the first axis captures the maximum amount of variance,
    the second axis has the next best variance, and so on. In MDS, there is no such
    condition. The axes in MDS can be inverted or rotated as needed. Also, in most
    of the other dimensional reduction methods used, the algorithms do calculate a
    lot of axes, but they cannot be viewed. In MDS, a smaller number of dimensions
    are explicitly chosen at the start. Hence there is less ambiguity in the solution.
    Further, in other algorithms, generally, there is only one unique solution, whereas
    MDS tries to iteratively find the most acceptable solution. It means that in MDS
    there can be multiple solutions for the same dataset.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: MDS 与其他降维技术不同。与PCA等技术相比，MDS 对数据集没有任何假设，因此可以用于更多的数据集。此外，MDS 允许使用任何距离度量指标。与PCA不同，MDS
    不是一个特征值-特征向量技术。回想一下，在PCA中，第一条轴捕捉最大的方差，第二条轴有下一个最好的方差，依此类推。在MDS中，没有这样的条件。MDS中的轴可以根据需要反转或旋转。此外，在大多数其他降维方法中，算法确实计算了很多轴，但它们无法被查看。在MDS中，在开始时明确选择了一个较小的维度数。因此，在解决方案中存在较少的歧义。进一步来说，在其他算法中，通常只有一个唯一的解，而MDS试图迭代地找到最可接受的解。这意味着在MDS中，对于同一数据集可以有多个解。
- en: But at the same time, the computation time required for MDS is greater for bigger
    datasets—and there is a catch in the gradient descent method used for optimization
    (see figure 6.4). Let’s refer to the mountain example we covered earlier. Imagine
    that while you are coming down from the top of the mountain, the starting point
    is A, and the bottom of the mountain is point C. While you are coming down, you
    reach point B. As you can see in the left diagram in the figure, there is a slight
    elevation around point B. At this point B, you might incorrectly conclude that
    you have reached the bottom of the mountain. In other words, you will think that
    you have finished your task. This is the problem of the local minima.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 但与此同时，对于更大的数据集，MDS 所需的计算时间更长——并且在用于优化的梯度下降法中存在一个陷阱（见图6.4）。让我们回顾一下我们之前提到的山岳例子。想象一下，当你从山顶下来时，起点是A，山顶的底部是点C。在你下坡的过程中，你到达了点B。如图中左图所示，点B周围有一个轻微的隆起。在这个点B，你可能会错误地认为你已经到达了山的底部。换句话说，你会认为你已经完成了任务。这就是局部最小值的问题。
- en: '![figure](../Images/CH06_F04_Verdhan.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F04_Verdhan.png)'
- en: Figure 6.4 While the first figure is the point of convergence and represents
    the gradient descent method, note that in the second figure the global minima
    is somewhere else, while the algorithm can be stuck at a local minima. The algorithm
    might check that it has optimized the cost function and reached the point of global
    minima, whereas it has only reached the local minima. In a local minima, there
    is no direction that is ascending; all the directions descend. The algorithm,
    if purely local, has no information about other deeper minima existing beyond
    a potentially small hill.
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4 虽然第一个图表示的是收敛点，代表了梯度下降法，但请注意，在第二个图中，全局最小值位于别处，而算法可能会陷入局部最小值。算法可能会检查它已经优化了成本函数并达到了全局最小值，而实际上它只达到了局部最小值。在局部最小值处，没有上升的方向；所有方向都是下降的。如果算法纯粹是局部的，它对存在在可能的小山之外的更深最小值没有信息。
- en: It is a possibility that instead of a global minimum, the loss function might
    be stuck in a local minima. The algorithm might think that it has reached the
    point of convergence, while the complete convergence might not have been achieved,
    and we are at a local minimum.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种可能性，损失函数可能不是全局最小值，而是陷入局部最小值。算法可能会认为它已经达到了收敛点，而完整的收敛可能还没有实现，我们处于局部最小值。
- en: There is still a question to be answered about the efficacy of the MDS solution.
    How can we measure the effectiveness of the solution? In the original paper, Kruskal
    recommended the stress values to measure the goodness-of-fit of the solution,
    which are shown in table 6.2\. The recommendations are mostly based on the empirical
    experience of Kruskal. These stress values are based on Kruskal’s experience.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 关于MDS解决方案的有效性仍有一个问题需要回答。我们如何衡量解决方案的有效性？在原始论文中，Kruskal推荐使用应力值来衡量解决方案的拟合优度，这些应力值显示在表6.2中。这些推荐主要基于Kruskal的经验。这些应力值是基于Kruskal的经验。
- en: Table 6.2 Stress values and their goodness of fit
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.2 应力值及其拟合优度
- en: '| Stress values | Goodness of fit |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 应力值 | 拟合优度 |'
- en: '| --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0.200  | Poor  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 0.200  | 差  |'
- en: '| 0.100  | Fair  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 0.100  | 一般  |'
- en: '| 0.050  | Good  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 0.050  | 好  |'
- en: '| 0.025  | Excellent  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 0.025  | 优秀  |'
- en: '| 0.000  | Perfect  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 0.000  | 完美  |'
- en: 'The next logical question is: How many final dimensions should we choose? A
    scree plot provides the answer, as shown in figure 6.5\. Recall in chapter 2 we
    used a similar elbow method to choose the optimal number of clusters in k-means
    clustering. For MDS too, we can use the elbow method to determine the optimal
    number of components to represent the data.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个逻辑问题是：我们应该选择多少个最终维度？散点图提供了答案，如图6.5所示。回想一下，在第2章中，我们使用类似的手肘法来选择k-means聚类的最佳聚类数。对于MDS，我们也可以使用手肘法来确定表示数据的最佳成分数。
- en: '![figure](../Images/CH06_F05_Verdhan.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F05_Verdhan.png)'
- en: Figure 6.5 Scree plot to find the optimal number of components. It is similar
    to the k-means solution; we have to look for the elbow in the plot.
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.5 找到最佳成分数的散点图。它与k-means解决方案类似；我们必须在图中寻找手肘点。
- en: Exercise 6.1
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习6.1
- en: 'Answer these questions to check your understanding:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题以检查你的理解：
- en: What is the difference between metric and nonmetric MDS algorithms?
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 度量空间MDS和非度量空间MDS算法之间的区别是什么？
- en: Gradient descent is used to maximize the cost. True or False?
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度下降用于最大化成本。对还是错？
- en: Explain the gradient descent method using a simple example.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一个简单的例子解释梯度下降法。
- en: 6.3 Python implementation of MDS
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 MDS的Python实现
- en: For the Python implementation of the MDS method we will use the famous Iris
    dataset, which we have used previously. Using the algorithm is quite simple, thanks
    to the libraries available in the `scikit learn` package.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MDS方法的Python实现，我们将使用著名的Iris数据集，我们之前已经使用过。由于`scikit learn`包中提供了库，使用算法相当简单。
- en: NOTE  The implementation is generally simple as the heavy lifting is done by
    the libraries.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：实现通常很简单，因为重活是由库完成的。
- en: 'The steps are as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Load the libraries. The usual suspects are `sklearn`, `matplotlib`, and `numpy`,
    and we also load `MDS` from `sklearn`:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载库。常用的库有 `sklearn`、`matplotlib` 和 `numpy`，我们还从 `sklearn` 中加载了 `MDS`：
- en: '[PRE0]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '2\. Load the dataset. The Iris dataset is available in the `sklearn` library,
    so we need not import Excel or .csv files here:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 加载数据集。Iris数据集在`sklearn`库中可用，因此我们在这里不需要导入Excel或.csv文件：
- en: '[PRE1]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '3\. A requirement for MDS is that the dataset should be scaled before the actual
    visualization is done. We use the `MixMaxScalar()` function to achieve this. MinMax
    scaling simply scales the data using the formula in equation 6.3:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. MDS的要求是在实际可视化之前对数据集进行缩放。我们使用`MixMaxScalar()`函数来实现这一点。MinMax缩放简单地使用方程6.3中的公式缩放数据：
- en: '![figure](../Images/verdhan-ch6-eqs-2x.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch6-eqs-2x.png)'
- en: (6.3)
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (6.3)
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As an output of this step, the data is scaled and ready for the next step of
    modeling.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这一步骤的输出，数据已缩放并准备好进行下一阶段的建模。
- en: '4\. Invoke the MDS method from the `sklearn` library. The `random_state` value
    allows us to reproduce the results. We have chosen the number of components as
    3 for the example:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 从`sklearn`库中调用MDS方法。`random_state`值允许我们重现结果。我们在此示例中将组件数量选为3：
- en: '[PRE3]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '5\. Fit the scaled data created earlier using the MDS model:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 使用MDS模型拟合之前创建的缩放数据：
- en: '[PRE4]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '6\. Declare the colors we wish to use for visualization. Next, the data points
    are visualized in a scatter plot:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6. 声明我们希望用于可视化的颜色。接下来，数据点在散点图中进行可视化：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output of the preceding code is shown in figure 6.6.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出显示在图6.6中。
- en: '![figure](../Images/CH06_F06_Verdhan.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F06_Verdhan.png)'
- en: Figure 6.6 Output for the Iris data
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.6 Iris数据的输出
- en: This example of Python implementation is a visualization of the Iris data. It
    is quite a simple example, as it does not involve stress and optimization for
    the number of components. In other words, we need a more complex dataset to really
    optimize MDS. We will now work on a curated dataset to implement MDS (see figure
    6.7).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Python实现的例子是Iris数据的可视化。这是一个相当简单的例子，因为它不涉及应力和组件数量的优化。换句话说，我们需要一个更复杂的数据集才能真正优化MDS。我们现在将使用一个精选的数据集来实施MDS（见图6.7）。
- en: '![figure](../Images/CH06_F07_Verdhan.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F07_Verdhan.png)'
- en: Figure 6.7 Various cities and their respective distances between each other
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.7 各个城市及其相互之间的距离
- en: 'Let us assume we have five cities and the respective distance between them
    is given in figure 6.7\. The steps are as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们拥有五个城市，它们之间的距离在图6.7中给出。步骤如下：
- en: 'We have already imported the libraries in the last code:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经在上一段代码中导入了库：
- en: '[PRE6]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '2\. Create the dataset. Although we create a dataset here, in real business
    scenarios, it will be in the form of distances only (see figure 6.8):'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 创建数据集。虽然我们在这里创建了一个数据集，但在实际业务场景中，它将以距离的形式存在（见图6.8）：
- en: '[PRE7]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![figure](../Images/CH06_F08_Verdhan.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F08_Verdhan.png)'
- en: Figure 6.8 Creating the dataset
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.8 创建数据集
- en: '3\. Use the `MinMaxScalar()` function to scale the dataset as we did in the
    last coding exercise:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 使用`MinMaxScalar()`函数缩放数据集，就像我们在上一个编码练习中所做的那样：
- en: '[PRE8]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now we work toward finding the most optimal number of components. We will iterate
    for different values of the number of components. For each of the values of the
    number of components, we will get the value of stress. The point at which a kink
    is observed is the optimal number of components.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们致力于寻找最优的组件数量。我们将对不同的组件数量进行迭代。对于每个组件数量的值，我们将得到应力的值。观察到拐点的点即为最优的组件数量。
- en: 'As a first step, we will declare an empty dataframe, which can be used to store
    the values of the number of components and corresponding stress values. Then we
    iterate from 1 to 10 in a `for` loop. Finally, for each of the values of components
    (1 to 10), we get the respective values of stress:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们将声明一个空的数据框，它可以用来存储组件数量及其对应应力值的值。然后我们在`for`循环中从1迭代到10。最后，对于组件的每个值（1到10），我们得到相应的应力值：
- en: '[PRE9]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '4\. Now that we have the values of stress, we will plot these values in a graph.
    The respective labels for each of the axes are also given. Look at the kink at
    values 2 and 3 in figure 6.9\. These can be the optimal values of the number of
    components:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 现在我们已经得到了应力的值，我们将这些值绘制在图表中。每个坐标轴的相应标签也已给出。请看图6.9中值2和3处的拐点。这些可能是组件数量的最优值：
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![figure](../Images/CH06_F09_Verdhan.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F09_Verdhan.png)'
- en: Figure 6.9 Scree plot to select the optimized number of components
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.9 切片图以选择优化的组件数量
- en: '5\. Run the solution for the number of components = 3\. If we look at the values
    of stress, number of components = 3, it generates the minimum value of stress
    as 0.00665 (see figure 6:10):'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 运行组件数量为3的解决方案。如果我们查看应力的值，组件数量为3时，它生成了最小的应力值0.00665（见图6.10）：
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This concludes our discussion on the MDS algorithm. We discussed the foundation
    and concepts, pros and cons, algorithm assessment, and Python implementation of
    MDS. As one of the nonlinear dimensionality reduction methods, it is a great solution
    for visualization and dimensionality reductions.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对MDS算法的讨论。我们讨论了其基础和概念、优缺点、算法评估以及MDS的Python实现。作为非线性降维方法之一，它对于可视化和降维是一个很好的解决方案。
- en: '![figure](../Images/CH06_F10_Verdhan.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F10_Verdhan.png)'
- en: 'Figure 6.10 Output for the MDS dataset: representation of the five cities in
    a plot'
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.10 MDS数据集的输出：图中展示了五个城市的表示
- en: 6.4 t-distributed stochastic neighbor embedding
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 t分布随机近邻嵌入
- en: If a dataset is really high dimensional, the analysis becomes cumbersome. The
    visualization is even more confusing. We have covered that in great detail in
    the curse of dimensionality section in chapter 3\. You are advised to revisit
    the concept before proceeding if you need a refresher.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集的维度非常高，分析就会变得繁琐。可视化甚至更加混乱。我们在第3章的维度诅咒部分已经详细讨论了这一点。如果你需要复习，建议在继续之前重新回顾这个概念。
- en: One such really high-dimensional dataset can be image data. We find it difficult
    to comprehend such data due to anything beyond 3 dimensions being increasingly
    difficult for us to intuit.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一个真正高维度的数据集可以是图像数据。我们发现，由于超过3维度的内容对我们来说越来越难以直观理解，因此我们很难理解这类数据。
- en: 'You may have used facial recognition software on your smartphone. For such
    solutions, facial images have to be analyzed, and machine learning models have
    to be trained. Look at the pictures in figure 6.11: we have a human face, a bike,
    a vacuum cleaner, and a screen capture of a phone.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经在你的智能手机上使用过面部识别软件。对于这样的解决方案，面部图像需要被分析，机器学习模型需要被训练。看看图6.11中的图片：我们有一个人脸、一辆自行车、一个吸尘器和一部手机的屏幕截图。
- en: Image is a complex data type. Each image is made up of pixels, and each pixel
    can be made up of RGB (red, green, blue) values. Values for each of the RGB can
    range from 0 to 255\. The resulting dataset will be a very high-dimensional dataset.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图像是复杂的数据类型。每张图像由像素组成，每个像素可以由RGB（红色、绿色、蓝色）值组成。RGB的值可以从0到255。生成的数据集将是一个非常高维的数据集。
- en: '![figure](../Images/CH06_F11_Verdhan.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F11_Verdhan.png)'
- en: Figure 6.11 Images are quite complex to decipher by an algorithm. Images can
    be of any form and can be of a person, a piece of equipment, or even a phone screen.
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.11中的图像对于算法来说非常复杂，难以解读。图像可以是任何形式，可以是人物、设备，甚至是手机屏幕。
- en: Now recall PCA, which we studied in chapter 3\. PCA is a linear algorithm. Thus,
    its capability to resolve nonlinear and complex polynomial functions is limited.
    Moreover, when a high-dimensional dataset has to be represented in a low-dimensional
    space, the algorithm should keep similar data points close to each other, which
    can be a challenge in linear algorithms. PCA, as a linear dimension reduction
    technique, tries to separate the different data points as far away from each other
    as possible, and tries to maximize the variance captured in the data. The resulting
    analysis is not robust and might not be best suited for further use and visualization.
    Hence, we have nonlinear algorithms like t-SNE to help.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回想一下我们在第3章学习的PCA。PCA是一个线性算法。因此，它在解决非线性复杂多项式函数方面的能力有限。此外，当需要将高维数据集表示在低维空间中时，算法应该将相似的数据点保持得尽可能接近，这在线性算法中可能是一个挑战。作为线性降维技术，PCA试图尽可能地将不同的数据点分开，并试图最大化数据中捕获的方差。这种分析的结果可能不够稳健，可能不适合进一步的使用和可视化。因此，我们有非线性算法如t-SNE来帮助。
- en: t-SNE is a nonlinear dimensionality reduction technique that is quite handy
    for high-dimensional data. It is based on stochastic neighbor embedding, which
    was developed by Sam Roweis and Geoffrey Hinton. The t-distributed variant was
    proposed by Lauren van der Maaten. Thus, t-SNE is an improvement of the SNE algorithm.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE是一种非线性降维技术，对于高维数据来说非常实用。它基于随机近邻嵌入，由Sam Roweis和Geoffrey Hinton开发。t分布的变体是由Lauren
    van der Maaten提出的。因此，t-SNE是SNE算法的改进。
- en: At a high level, SNE measures the similarity between instance pairs in a high-dimensional
    space and in a low-dimensional space. A good solution is where the difference
    between these similarity measures is the least, and SNE then optimizes these similarity
    measures using a cost function similar to what we have discussed for MDS.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，SNE测量高维空间和低维空间中实例对之间的相似度。一个好的解决方案是这些相似度度量之间的差异最小，然后SNE使用类似于我们之前讨论的MDS的成本函数来优化这些相似度度量。
- en: 'We examine the step-by-step process of t-SNE next. The process described is
    a little heavy on mathematics:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来检查t-SNE的逐步过程。描述的过程在数学上有点复杂：
- en: Consider a high-dimensional space and some points in it.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑一个高维空间及其中的某些点。
- en: Measure the similarities between the various points in the high-dimensional
    space mentioned in the last point. For a point *x*[*i*], we will then create a
    Gaussian distribution centered at that point. We have already studied Gaussian
    or normal distribution in chapter 2\. The Gaussian distribution is shown in figure
    6.12.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量上一条中提到的高维空间中各个点之间的相似度。对于点 *x*[*i*]，我们将创建一个以该点为中心的高斯分布。我们已经在第2章中学习了高斯或正态分布。高斯分布如图6.12所示。
- en: '![figure](../Images/CH06_F12_Verdhan.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F12_Verdhan.png)'
- en: Figure 6.12 Gaussian or normal distribution.
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.12 高斯或正态分布。
- en: 3\. Measure the density of points (let’s say *x*[*j*]) that fall under that
    Gaussian distribution and then renormalize them to get the respective conditional
    probabilities (*p*[*j*][|][*i*]). For the points that are nearby and hence similar,
    this conditional probability will be high, and for the points that are far and
    dissimilar, the value of conditional probabilities (*p*[*j*][|][*i*]) will be
    very small. These values of probabilities are those in the high-dimensional space.
    For curious readers, the mathematical formula for this conditional probability
    is presented as equation 6.4
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 测量位于该高斯分布下的点（比如说 *x*[*j*]）的密度，然后重新归一化它们以获得相应的条件概率（*p*[*j*][|][*i*]）。对于附近且相似的点，这个条件概率将很高，而对于远离且不相似的点，条件概率（*p*[*j*][|][*i*]）的值将非常小。这些概率值是在高维空间中的。对于好奇的读者，这个条件概率的数学公式以方程6.4表示。
- en: '![figure](../Images/verdhan-ch6-eqs-3x.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch6-eqs-3x.png)'
- en: (6.4)
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: （6.4）
- en: where *σ* is the variance of the Gaussian distribution centered at *x*[*i*].
    The mathematical proof is beyond the scope of this book.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *σ* 是以 *x*[*i*] 为中心的高斯分布的方差。数学证明超出了本书的范围。
- en: 4\. Measure one more set of probabilities in the low-dimensional space. For
    this set of measurements, we use the Cauchy distribution, described next. We use
    Kullback-Liebler (KL) divergence for measuring the difference between two probability
    distributions.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 在低维空间中测量另一组概率。对于这组测量，我们使用下一节中描述的柯西分布。我们使用Kullback-Liebler（KL）散度来测量两个概率分布之间的差异。
- en: 6.4.1 Cauchy distribution
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 柯西分布
- en: The Cauchy distribution belongs to the family of continuous probability distributions.
    Though there is a resemblance with the normal distribution, as we have represented
    in figure 6.13, the Cauchy distribution has a narrower peak and spreads out more
    slowly. It means that, compared to a normal distribution, the probability of obtaining
    values far from the peaks is higher. Sometimes, the Cauchy distribution is known
    as the *Lorentz distribution.* It is interesting to note that Cauchy does not
    have a well-defined mean, but the median is the center of symmetry.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 柯西分布属于连续概率分布族。尽管它与我们在图6.13中表示的正态分布有相似之处，但柯西分布的峰值更窄，扩散得更慢。这意味着，与正态分布相比，获得远离峰值的值的概率更高。有时，柯西分布也被称为*洛伦兹分布*。值得注意的是，柯西分布没有明确定义的平均值，但中位数是对称中心。
- en: '![figure](../Images/CH06_F13_Verdhan.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F13_Verdhan.png)'
- en: 'Figure 6.13 Comparison of Gaussian distribution vs. Cauchy distribution. (Image
    source: Quora)'
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.13 高斯分布与柯西分布的比较。（图片来源：Quora）
- en: 'Consider we get *y*[*i*] and *y*[*j*] as the low-dimensional counterparts for
    the high-dimensional data points *x*[*i*] and *x*[*j*]. So we can calculate the
    probability score like we did in the last step. Using the Cauchy distribution,
    we can get a second set of probabilities *q*[*j*][|][*i*] too. The mathematical
    formula is shown in equation 6.5:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们得到了 *y*[*i*] 和 *y*[*j*]，它们是高维数据点 *x*[*i*] 和 *x*[*j*] 的低维对应物。因此，我们可以像上一步一样计算概率得分。使用高斯分布，我们还可以得到第二组概率
    *q*[*j*][|][*i*]。数学公式在方程 6.5 中展示：
- en: '![figure](../Images/verdhan-ch6-eqs-4x.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch6-eqs-4x.png)'
- en: (6.5)
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (6.5)
- en: 2\. So far, we have calculated two set of probabilities (*p*[*j*][|][*i*]) and
    (*q*[*j*][|][*i*]). In this step, we compare the two distributions and measure
    the difference between the two. In other words, while calculating (*p*[*j*][|][*i*])
    we measured the probability of similarity in a high-dimensional space whereas
    for (*q*[*j*][|][*i*]) we did the same in a low-dimensional space. Ideally, the
    mapping of the two spaces is similar, and for that, there should not be any difference
    between (*p*[*j*][|][*i*]) and (*q*[*j*][|][*i*]). So the SNE algorithm tries
    to minimize the difference in the conditional probabilities (*p*[*j*][|][*i*])
    and (*q*[*j*][|][*i*]), similar to what we have done with MDS for the distance
    in high- and low-dimensional spaces.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 到目前为止，我们已经计算了两组概率 (*p*[*j*][|][*i*]) 和 (*q*[*j*][|][*i*])。在这一步，我们比较这两个分布并测量它们之间的差异。换句话说，在计算
    (*p*[*j*][|][*i*]) 时，我们测量了高维空间中相似性的概率，而对于 (*q*[*j*][|][*i*])，我们在低维空间中做了同样的操作。理想情况下，两个空间的映射应该是相似的，为此，(*p*[*j*][|][*i*])
    和 (*q*[*j*][|][*i*]) 之间不应该有任何差异。因此，SNE算法试图最小化条件概率 (*p*[*j*][|][*i*]) 和 (*q*[*j*][|][*i*])
    之间的差异，这与我们在高维和低维空间中使用MDS进行距离测量的做法类似。
- en: 3\. The difference between the two probability distributions is done using KL
    divergence.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 使用KL散度来测量两个概率分布之间的差异。
- en: 4\. To minimize the KL cost function, we use the gradient descent approach.
    We have already discussed the gradient descent approach in section 6.2 where we
    discussed the MDS algorithm.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 为了最小化KL代价函数，我们使用梯度下降方法。我们已经在第6.2节中讨论了梯度下降方法，当时我们讨论了MDS算法。
- en: DEFINITION  KL divergence or relative entropy is used to measure the difference
    between two probability distributions. Usually, one probability distribution is
    the data or the measured scores, and the second probability distribution is an
    approximation or the prediction of the original probability distribution—for example,
    if the original probability distribution is *X* and the approximated one is *Y*.
    KL divergence can be used to measure the difference between *X* and *Y* probability
    distributions. In absolute terms, if the value is 0, then it means that the two
    distributions are identical. The KL divergence is applicable for neurosciences,
    statistics, and fluid mechanics, among others.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 定义：KL散度或相对熵用于测量两个概率分布之间的差异。通常，一个概率分布是数据或测量的得分，而第二个概率分布是对原始概率分布的近似或预测——例如，如果原始概率分布是
    *X*，而近似的是 *Y*。KL散度可以用来测量 *X* 和 *Y* 概率分布之间的差异。在绝对意义上，如果值为0，则意味着两个分布是相同的。KL散度适用于神经科学、统计学和流体力学等领域。
- en: There is one more important factor we should be aware of while we work on t-SNE,
    and that is *perplexity*. Perplexity is a hyperparameter that allows us to control
    and optimize the number of close neighbors each of the data points has.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行t-SNE工作时，还有一个重要的因素我们应该意识到，那就是 *困惑度*。困惑度是一个超参数，它允许我们控制和优化每个数据点拥有的邻近点的数量。
- en: NOTE  As per the official paper, a typical value for perplexity lies between
    5 and 50\.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：根据官方论文，困惑度的典型值介于5到50之间。
- en: 'There can be one additional nuance: the output of a t-SNE algorithm might never
    be the same on successive runs. We have to optimize the values of the hyperparameters
    to receive the best output.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 可能还有一个额外的细微差别：t-SNE算法的输出可能在连续运行中永远不会相同。我们必须优化超参数的值以获得最佳输出。
- en: Exercise 6.2
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习6.2
- en: 'Answer these questions to check your understanding:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题以检查你的理解：
- en: Explain Cauchy distribution in your own words.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用你自己的话解释高斯分布。
- en: PCA is a nonlinear algorithm. True or False?
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCA是一个非线性算法。对或错？
- en: KL divergence is used to measure the difference between two probability distributions.
    True or False?
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KL散度用于测量两个概率分布之间的差异。对或错？
- en: 6.4.2 Python implementation of t-SNE
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 t-SNE的Python实现
- en: 'We will use two datasets in this example. The first one is the Iris dataset,
    which we have already used more than once in this book. The second dataset is
    quite an interesting one: the MNIST dataset is a database of handwritten digits.
    It is one of the most famous datasets used to train image processing solutions
    and generally is considered the “Hello World” program for image detection solutions.
    An image representation is shown figure 6.14\.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F14_Verdhan.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 MNIST dataset
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The steps for the Iris dataset are as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary libraries. Note that we have imported the MNIST dataset
    from the `keras` library.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: TIP  If you are not able to install modules in your Python code, refer to the
    appendix where we provide a solution.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Load the Iris dataset. The dataset comprises two parts: one is the “data”
    and the second is the respective label or “target” for it. It means that “data”
    is the description of the data and “target” is the type of iris. We print the
    features and the labels using code:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '3\. Invoke the t-SNE algorithm. We are using the `n_components=2`, `verbose=1`,
    and `random_state=5` to reproduce the results. Then the algorithm is used to fit
    the data (see figure 6.15):'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![figure](../Images/CH06_F15_Verdhan.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 Output of the code when we are fitting the algorithm
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4\. Plot the data. This step allows us to visualize the data fitted by the algorithm
    in the last step.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we will initiate an empty dataframe. We will add three columns, one
    at a time. We start with `iris_targ`et, followed by `tSNE_first_component` and
    `tSNE_second_ component`. `tSNE_first_component` is the first column of the `fitted_data`
    dataframe, and therefore the index is `0`. `tSNE_second_component` is the second
    column of the `fitted_data` dataframe and hence the index is `1`. Finally, we
    represent the data in a scatterplot in figure 6.16:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![figure](../Images/CH06_F16_Verdhan.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 t-SNE projection of the Iris dataset. Note how we are getting three
    separate clusters for the three classes we have in the dataset.
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To implement the algorithm for the MNIST dataset, load the libraries and dataset.
    The libraries were already loaded in the last code example. Now load the dataset.
    The dataset requires `reshape`, which is done here (see figure 6.17):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![figure](../Images/CH06_F17_Verdhan.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 Output of t-SNE for the 10 classes of digits represented in different
    shades of gray
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are a few important points to keep in mind while running t-SNE:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Run the algorithm with different values of hyperparameters before finalizing
    a solution.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideally, perplexity should be between 5 and 50, and for an optimized solution,
    the value of perplexity should be less than the number of points.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T-SNE guesses the number of close neighbors for each of the points. For this
    reason, a dataset that is denser will require a much higher perplexity value.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that perplexity is the hyperparameter that balances the attention given
    to both the local and the global aspects of the data.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，困惑度是平衡数据局部和全局方面的超参数。
- en: t-SNE is a widely popular algorithm. It can be used for studying the topology
    of an area, but a single t-SNE cannot be used for making a final assessment. Instead,
    multiple t-SNE plots should be created to make any final recommendation. Sometimes
    there are complaints that t-SNE is a black-box algorithm. This might be true to
    a certain extent. What makes the adoption of t-SNE harder is that it does not
    generate the same results in successive iterations. Hence, you might find t-SNE
    recommended only for exploratory analysis.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE是一个广泛流行的算法。它可以用于研究一个区域的拓扑结构，但单个t-SNE不能用于做出最终评估。相反，应该创建多个t-SNE图来做出任何最终建议。有时有人抱怨t-SNE是一个黑盒算法。这在一定程度上可能是正确的。使t-SNE的采用变得更困难的是，它不会在连续迭代中产生相同的结果。因此，你可能会发现t-SNE仅被推荐用于探索性分析。
- en: 6.5 Uniform manifold approximation projection
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 统一流形近似投影
- en: UMAP is a powerful and popular dimensionality reduction technique. It is designed
    to preserve both the local and global structures of the dataset while reducing
    the complexity and dimensions of the high-dimensional dataset to a low-dimensional
    dataset.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP是一种强大且流行的降维技术。它旨在在降低高维数据集的复杂性和维度到低维数据集的同时，保留数据集的局部和全局结构。
- en: UMAP was introduced in 2018 by Lealand McInnes, John Healy, and James Melville.
    UMAP makes the data more suitable for visualizations and data analysis. This relates
    to the concepts of topology and manifold theory. UMAP assumes that the high-dimensional
    dataset often lies on a manifold, which means a low-dimensional structure is embedded
    in a higher-dimensional space. Hence, it attempts to project this manifold into
    a lower dimensional space, preserving both the nearest neighbor relationships,
    which is nothing but the local structure, and the larger relationships, which
    is the global structure.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP是由Lealand McInnes、John Healy和James Melville于2018年提出的。UMAP使数据更适合可视化与分析。这与拓扑和流形理论的概念相关。UMAP假设高维数据集通常位于一个流形上，这意味着低维结构嵌入在更高维的空间中。因此，它试图将这个流形投影到低维空间，同时保留最近邻关系，这仅仅是局部结构，以及更大的关系，即全局结构。
- en: 6.5.1 Working with UMAP
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1 与UMAP一起工作
- en: UMAP methodology uses the concept of fuzzy simplicity sets. These sets represent
    the probability distribution of distances between various data points and capture
    the underlying manifold structures.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP方法使用模糊简单集的概念。这些集合代表不同数据点之间距离的概率分布，并捕捉潜在的流形结构。
- en: The first step in UMAP is to construct a weighted graph where each of the data
    points is connected to its nearest neighbor based on a distance metric. Generally,
    the Euclidean distance is used as the distance metric. This graph construction
    is an abstract representation of the data structure in high dimensions.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP的第一步是构建一个加权图，其中每个数据点根据距离度量与其最近邻连接。通常，欧几里得距离被用作距离度量。这个图构建是对高维数据结构的一种抽象表示。
- en: The next step is to optimize the graph. The graph is optimized in a lower dimension
    space by minimizing cross-entropy loss between the original high-dimensional relationships
    and the newly created low-dimensional relationships. This uses the stochastic
    gradient descent, producing the UMAP embeddings. We will study stochastic gradient
    descent in chapter 9.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是优化图。通过最小化原始高维关系和新创建的低维关系之间的交叉熵损失，在低维空间中优化图。这使用了随机梯度下降，产生了UMAP嵌入。我们将在第9章研究随机梯度下降。
- en: 'There are two key parameters for UMAP:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP有两个关键参数：
- en: '`n_neighbours`—The number of nearest neighbors to consider for each point.
    Using this parameter, we balance the preservation of the local data structure
    as compared to the global data structure.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_neighbours`——考虑每个点的最近邻数量。使用此参数，我们平衡局部数据结构和全局数据结构之间的保留。'
- en: '`min_dist`—This is used to control how tightly the points are clustered together.
    Smaller values of minimum distance keep the points much closer and hence create
    deeper clusters. The larger value for minimum distance will create lighter clusters,
    which are spread out.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_dist`——这个参数用于控制点之间的簇聚紧密程度。较小的最小距离值会使点更靠近，从而创建更深的簇。较大的最小距离值将创建较稀疏的簇，这些簇分布较广。'
- en: 6.5.2 Using UMAP
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.2 使用UMAP
- en: 'The various uses of UMAP are as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP的用途如下：
- en: One of the most popular uses of UMAP is the visualization of high-dimensional
    datasets in the bioinformatics field. Gene datasets are quite complex and multidimensional,
    where each data point might be represented by hundreds or thousands of attributes.
    Using UMAP, researchers can virtually inspect the clusters and the underlying
    relationships in the dataset. The solution helps them identify cell types, developmental
    stages, and gene expression patterns.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UMAP在生物信息学领域的高维数据集可视化中是最受欢迎的应用之一。基因数据集相当复杂且多维，每个数据点可能由数百或数千个属性表示。使用UMAP，研究人员可以虚拟地检查数据集中的簇和潜在关系。这种解决方案帮助他们识别细胞类型、发育阶段和基因表达模式。
- en: UMAP is also applied to the natural language processing field by reducing the
    dimensionality of embeddings. It helps in the visualization of relationships between
    words or sentences or documents, making it easier to understand the similarities.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UMAP还应用于自然语言处理领域，通过降低嵌入的维度。它有助于可视化单词、句子或文档之间的关系，使理解相似性变得更容易。
- en: UMAP can also be applied to images. It helps in the visualization of the plastering
    of images based on various similarities, hence it is quite useful for competitive
    vision tasks to understand how similar images can be clustered together.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UMAP还可以应用于图像。它有助于基于各种相似性可视化图像的叠加，因此对于理解如何将相似图像聚类在一起，在竞争视觉任务中非常有用。
- en: UMAP can be used with other clustering algorithms like k-means or DBSCAN. It
    can uncover the hidden patterns in large datasets and since it preserves both
    local and global structures, the clusters found in lower dimensional representations
    often provide more important groupings as compared to the original high-dimensional
    dataset.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UMAP可以与其他聚类算法（如k-means或DBSCAN）一起使用。它可以揭示大型数据集中的隐藏模式，并且由于它保留了局部和全局结构，因此在低维表示中找到的簇通常比原始高维数据集提供更重要的分组。
- en: In addition to helping with visualizations, UMAP can also be used as a preprocessing
    step to reduce the dimensions of data. It can be used as an alternative to PCA
    or other solutions. By reducing the number of dimensions in a dataset, the model’s
    performance might be improved and the computation time is reduced.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 除了帮助可视化外，UMAP还可以用作预处理步骤来降低数据的维度。它可以作为PCA或其他解决方案的替代品。通过减少数据集中的维度数量，可以提高模型的性能并减少计算时间。
- en: The use of UMAP in Python is straightforward. The library `umap-learn` allows
    us to use the power of UMAP.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中使用UMAP非常简单。`umap-learn`库允许我们使用UMAP的力量。
- en: 6.5.3 Key points of UMAP
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.3 UMAP的关键点
- en: 'Let’s now cover the key points of UMAP and compare it to other algorithms:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来介绍UMAP的关键点，并将其与其他算法进行比较：
- en: Since UMAP is a nonlinear solution, it can capture more complex datasets and
    patterns as compared to PCA. Recall that PCA is a linear dimensionality tradition
    technique, so when the data is not on a simple linear manifold, UMAP proves to
    be more accurate.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于UMAP是一种非线性解法，它比PCA能够捕捉更复杂的数据集和模式。回想一下，PCA是一种线性降维技术，所以当数据不在简单的线性流形上时，UMAP证明更加准确。
- en: The goal of PCA is to explain the maximum variance in the entire dataset. On
    the other hand, UMAP balances both local and global structures and hence is more
    versatile for tasks like anomaly detection.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA的目标是解释整个数据集的最大方差。另一方面，UMAP平衡了局部和全局结构，因此在异常检测等任务上更加灵活。
- en: As compared to PCA, UMAP can be used for larger datasets.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与PCA相比，UMAP可以用于更大的数据集。
- en: UMAP is faster than the other nonlinear solution, t-SNE. t-SNE can preserve
    the local structure of the data, but it struggles with preserving the global structure,
    and it can lead to a misleading interpretation of the clusters. UMAP does a better
    job as it preserves both local and global structures.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UMAP比其他非线性解法t-SNE更快。t-SNE可以保留数据的局部结构，但在保留全局结构方面存在困难，可能会导致对簇的误导性解释。UMAP做得更好，因为它保留了局部和全局结构。
- en: UMAP results are much more stable and consistent across multiple iterations.
    For other algorithms, the results can be unstable and might change with different
    values of random seeds.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UMAP的结果在多次迭代中更加稳定和一致。对于其他算法，结果可能不稳定，并且可能随着随机种子不同值的变化而变化。
- en: UMAP has gained a lot of popularity recently and has become a go-to tool for
    machine learning and AI solutions. It is fast and can preserve both local and
    global data structures. Hence it is a strong option compared to other dimensionality
    reduction solutions like PCA, t-SNE, and autoencoders.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP最近获得了大量的关注，并已成为机器学习和AI解决方案的首选工具。它速度快，可以保留局部和全局数据结构。因此，与其他降维解决方案（如PCA、t-SNE和自动编码器）相比，它是一个强大的选择。
- en: 6.6 Case study
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 案例研究
- en: In chapter 3, we explored a case study for the telecom industry reducing dimensionality.
    In this chapter, we will examine a small case study wherein t-SNE or MDS can be
    utilized for dimensionality reduction.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们探讨了电信行业减少维度的案例研究。在本章中，我们将考察一个小型案例研究，其中可以使用t-SNE或MDS进行降维。
- en: 'Have you heard about hyperspectral images? As you know, we humans see the colors
    of visible light in mostly three bands: long, medium, and short wavelengths. The
    long wavelengths are perceived as red, medium as green, and short as blue. All
    the other colors human beings perceive are simply mixtures of these three, and
    that is what allows screens and printers to work with only three colors. Spectral
    imaging, on the other hand, divides the spectrum into many more bands, and this
    technique can be extended beyond the visible and hence is used across biology,
    physics, geoscience, astronomy, agriculture, and many more avenues. Hyperspectral
    imaging collects and processes information from across the electromagnetic spectrum.
    It obtains the spectrum for each of the pixels in the image. See figure 6.18.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你听说过高光谱图像吗？正如你所知，我们人类在大多数情况下在三个波段中看到可见光的颜色：长波、中波和短波。长波被感知为红色，中波为绿色，短波为蓝色。人类感知到的所有其他颜色都是这三种颜色的混合，这就是为什么屏幕和打印机只需要三种颜色就能工作。另一方面，光谱成像将光谱分成更多的波段，这种技术可以扩展到可见光之外，因此它在生物学、物理学、地球科学、天文学、农业等多个领域都有应用。高光谱成像收集和处理整个电磁谱的信息。它为图像中的每个像素获取光谱。见图6.18。
- en: '![figure](../Images/CH06_F18_Verdhan.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F18_Verdhan.png)'
- en: 'Figure 6.18 Hyperspectral image of “sugar end” potato strips shows invisible
    defects (Source: SortingExpert, CC BY-SA 3.0)'
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.18 “糖端”土豆条的高光谱图像显示了不可见的缺陷（来源：SortingExpert，CC BY-SA 3.0）
- en: One such dataset is the Pavia University dataset. The dataset is curated by
    the ROSIS sensor in Pavia, northern Italy. The details of the dataset are given
    next, and the dataset can be downloaded from [https://mng.bz/nRVa](https://mng.bz/nRVa).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个这样的数据集是帕维亚大学数据集。该数据集由意大利帕维亚的ROSIS传感器整理。接下来将给出数据集的详细信息，并且可以从[https://mng.bz/nRVa](https://mng.bz/nRVa)下载数据集。
- en: There are 103 spectral bands in this dataset. The HIS size is 610 * 340 pixels,
    and it contains nine classes. Such a type of data can be used for crop analysis,
    mineral examination and exploration, etc. Since this data also contains information
    about geological patterns, it is quite useful for scientific purposes. Before
    developing any image recognition solution, we have to reduce the number of dimensions
    for this dataset. The computation cost will be much higher if we have a large
    number of dimensions. Hence, we want a lower representative number of dimensions.
    Figure 6.19 shows a few example bands. You are advised to download the dataset
    (which is also pushed at the GitHub repository) and use the various dimensionality
    reduction techniques on the dataset to reduce the number of dimensions. There
    can be many other image datasets and complex business problems where t-SNE and
    MDS can be of pragmatic use.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中有103个光谱波段。HIS的大小是610 * 340像素，包含九个类别。这种类型的数据可以用于作物分析、矿物检验和勘探等。由于这些数据还包含有关地质模式的信息，因此在科学目的上非常有用。在开发任何图像识别解决方案之前，我们必须减少这个数据集的维度数量。如果我们有大量的维度，计算成本将会更高。因此，我们希望有一个更低的代表性维度数量。图6.19展示了几个示例波段。建议您下载数据集（该数据集也已在GitHub仓库中推送），并在数据集上使用各种降维技术来减少维度数量。可能会有许多其他图像数据集和复杂的企业问题，其中t-SNE和MDS可以具有实际的应用价值。
- en: '![figure](../Images/CH06_F19_Verdhan.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F19_Verdhan.png)'
- en: Figure 6.19 Example of bands in the dataset. These are only random examples.
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.19 数据集中的波段示例。这些只是随机示例。
- en: 6.7 Concluding thoughts
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.7 总结思考
- en: Dimensionality reduction is quite an interesting and useful field. It makes
    machine learning less expensive and less time-consuming. Imagine that you have
    a dataset with thousands of attributes or features. You do not know the data very
    well, the business understanding is limited, and, at the same time, you have to
    find the patterns in the dataset. You are not even sure if the variables are all
    relevant or just random noise. At such a moment, when we want to make the dataset
    less complex to crack and reduce the computational time, dimensionality reduction
    is the solution.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'We covered dimensionality reduction techniques earlier in the book. This chapter
    covers three advanced techniques: t-SNE, MDS, and UMAP. All three techniques should
    not be considered a substitute for the other, easier techniques we discussed.
    Rather, they can be useful if we are not getting meaningful results with basic
    algorithms. It is always advised to use PCA first and then try the advanced techniques.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: The complexity of the book is increasing. This chapter started with images—but
    we have only wet our toes. In the next chapter, we deal with text data. Perhaps
    you will find it very interesting and useful.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 6.8 Practical next steps and suggested readings
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following provides suggestions for what to do next and offers some helpful
    reading:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Use the vehicles dataset used in chapter 2 for clustering and implement MDS
    on it. Compare the performance on clustering before and after implementing MDS.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the datasets used in chapter 2 for Python examples and use them for implementing
    MDS.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For MDS, refer to the following research papers:'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“Dimensionality Reduction: A Comparative Review,” by Lauren van der Maaten,
    Eric Postma, and H. Japp Van Den Herik: [https://mng.bz/eyxQ](https://mng.bz/eyxQ)'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '“Multidimensional Scaling-Based Data Dimension Reduction Method for Application
    in Short-Term Traffic Flow Prediction for Urban Road Network,” by Satish V. Ukkusuri
    and Jian Lu: [https://mng.bz/pKmz](https://mng.bz/pKmz)'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Get t-SNE research papers from the following links and study them:'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“Visualizing Data Using t-SNE,” by Laurens van der Maaten and Geoffrey Hinton:
    [https://mng.bz/OBaE](https://mng.bz/OBaE)'
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '“The Art of Using t-SNE for Single Cell Transcriptomics”: [https://mng.bz/YD9A](https://mng.bz/YD9A)'
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See the paper “Performance Evaluation of t-SNE and MDS Dimensionality Reduction
    Techniques with KNN, SNN, and SVM Classifiers”: [https://arxiv.org/pdf/2007.13487.pdf](https://arxiv.org/pdf/2007.13487.pdf)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MDS is a dimensionality reduction technique that transforms high-dimensional
    data into a lower-dimensional space while preserving distances.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are three types of MDS: classical, metric, and nonmetric.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classical MDS uses Euclidean distances, aligning original and fitted distances.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonmetric MDS ranks distances rather than using absolute values.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metric MDS transforms distances to fit a lower dimensional space.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MDS involves calculating distances and optimizing a stress cost function with
    gradient descent, though it can be computationally intensive and is prone to local
    minima problems.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MDS 涉及计算距离和通过梯度下降优化应力成本函数，尽管它可能计算密集且容易陷入局部最小值问题。
- en: MDS works iteratively and does not make assumptions about data distribution,
    making it versatile for choosing distance metrics compared to PCA.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MDS 通过迭代工作，不对数据分布做出假设，这使得它在选择距离度量方面比 PCA 更灵活。
- en: t-SNE is a nonlinear dimensionality reduction technique and is particularly
    effective for high-dimensional and complex datasets like images.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t-SNE 是一种非线性降维技术，特别适用于高维和复杂的数据集，如图像。
- en: t-SNE optimizes similarity between data points in both high- and low-dimensional
    spaces using the Cauchy distribution and KL divergence.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t-SNE 使用柯西分布和 KL 散度优化高维和低维空间中数据点的相似性。
- en: t-SNE has an edge over PCA due to its nonlinear nature, though it involves hyperparameters
    like perplexity.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其非线性特性，t-SNE 在 PCA 上具有优势，尽管它涉及超参数如困惑度。
- en: UMAP is another dimensionality reduction method that efficiently preserves both
    local and global data structures and is faster and more stable than t-SNE.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UMAP 是另一种高效保留局部和全局数据结构的降维方法，比 t-SNE 更快、更稳定。
- en: Python implementations are available for both MDS and t-SNE.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 实现了 MDS 和 t-SNE。
- en: MDS is one of the advanced dimensionality reduction techniques, requiring optimization
    of a loss function or cost function.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MDS 是一种高级降维技术，需要优化损失函数或成本函数。
