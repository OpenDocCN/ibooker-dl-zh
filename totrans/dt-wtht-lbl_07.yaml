- en: 6 Dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: t-distributed stochastic neighbor embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multidimensional scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uniform manifold approximation and projection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python implementations of the algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Life is really simple, but we insist on making it complicated.—Confucius
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Simplicity is a virtue—both in life and in data science. We have discussed a
    lot of algorithms so far. A few of them are simple enough, and some of them are
    a bit complicated. In part 1 of the book, we studied simpler clustering algorithms,
    and in the last chapter, we examined advanced clustering algorithms. Similarly,
    we studied a few dimensionality algorithms like principal component analysis (PCA)
    in chapter 3\. Continuing on the same note, we will study three advanced dimensionality
    reduction techniques in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The advanced topics we cover in this and the next part of the book are meant
    to prepare you for complex problems. While you can apply these advanced solutions,
    it is always advisable to start with the classical solutions like PCA for dimensionality
    reduction. And if that solution doesn’t appropriately address the problem, then
    you can try the advanced solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction is one of the most sought-after solutions, particularly
    when we have a large number of variables. Recall the “curse of dimensionality”
    we discussed in chapter 3\. You are advised to refresh your memory on chapter
    3 before moving forward if needed. We will cover t-distributed stochastic neighbor
    embedding (t-SNE), multidimensional scaling (MDS), and uniform manifold approximation
    and projection (UMAP) in this chapter. This chapter will cover some mathematical
    concepts that create the foundation of the advanced techniques we are going to
    discuss. As always, the concept discussion will be followed by a Python solution.
    This chapter also has a short case study. We will also develop a solution using
    an images dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'There may be a dilemma in your mind: What is the level of mathematics required,
    and is an in-depth statistical knowledge a prerequisite? The answer is both yes
    and no. While having a mathematical understanding will allow you to understand
    the algorithms and appreciate the process in greater depth; at the same time,
    for real-world business implementation, sometimes one might want to skip the mathematics
    and directly move to the examples in Python. We suggest having at least more than
    a basic understanding of the mathematics to fully grasp the concept. In this book,
    we provide that level of mathematical support without going into too much depth,
    presenting instead an optimal mix of practical world and mathematical concepts.'
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the sixth chapter, and all the very best!
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Technical toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will continue to use the same version of Python and Jupyter Notebook as we
    have used so far. The codes and datasets used in this chapter have been checked
    in at [https://mng.bz/XxOv](https://mng.bz/XxOv).
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need to install `Keras` as an additional Python library in this chapter.
    Along with this, you will need the regular modules: `numpy`, `pandas`, `matplotlib`,
    `seaborn`, and `sklearn`.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Multidimensional scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you know, maps prove to be quite handy while traveling. Now imagine you are
    given a task. You receive distances between some cities around the world—for example,
    between London and New York, London and Paris, Paris and New Delhi, and so forth.
    Then you are asked to re-create the map from which these distances have been derived.
    If we have to re-create that 2D map, that will be through trial and error; we
    will make some assumptions and move ahead with the process. It will surely be
    a tiring exercise prone to error and quite time-consuming indeed. MDS can do this
    task easily for us.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  While thinking of the preceding example, ignore the fact that the earth
    is not flat, and assume that the distance measurement metric is constant—for example,
    there is no confusion in miles or kilometers.
  prefs: []
  type: TYPE_NORMAL
- en: As an illustration, consider figure 6.1\. Formally put, if we have *x* data
    points, MDS can help us convert the information of the pairwise distance between
    these *x* points to a configuration of points in a Cartesian space. Or, simply
    put, MDS transforms a large dimensional dataset into a lower dimensional one and,
    in the process, keeps the distance or the similarity between the points the same.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F01_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 Illustration of distance between the cities and if they are represented
    on a map. The figure is only to help develop an understanding and does not represent
    the actual results.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To simplify, consider figure 6.2\. Here we have three points: A, B, and C.
    We are representing these points in a 3D space. Then we represent the three points
    in a 2D space, and finally they are represented in a 1D space. The distance between
    the points is not up to scale in the diagrams in the figure. The example represents
    the meaning of lowering the number of dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F02_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 Representation of three points
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Hence, in MDS, multidimensional data is reduced to a lower number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three types of MDS algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: Classical MDS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metric multidimensional scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonmetric multidimensional scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.2.1 Classic MDS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will examine the metric MDS process in detail in the book, while we will
    cover the classical and nonmetric briefly. Imagine we have two points: *i* and
    *j*. Let us assume that the original distance between two points is *d*[*i*][*j*]
    and the corresponding distance in the lower dimensional space is *d*[*i*][*j*].'
  prefs: []
  type: TYPE_NORMAL
- en: 'In classical MDS, the distances between the points are treated as Euclidean
    distances, and the original and fitted distances are represented in the same metric.
    It means that if the original distances in a higher dimensional space are calculated
    using the Euclidean method, the fitted distances in the lower dimensional space
    are also calculated using Euclidean distance. We already know how to calculate
    Euclidean distances. For example, we have to find the distance between points
    *i* and *j*, and let’s say the distance is *d*[*i*][*j*]. The distance can be
    given by the Euclidean distance formula given by equation 6.1 in a 2D space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch6-eqs-0x.png)'
  prefs: []
  type: TYPE_IMG
- en: (6.1)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Recall in chapter 2, we discussed other distance functions like Manhattan distance,
    Euclidean distance, etc. You are advised to refresh your memory on chapter 2.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Nonmetric MDS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We just now noted that Euclidean distance can be used to calculate the distance
    between two points. Sometimes it is not possible to take the actual values of
    the distances, like when *d*[*i*][*j*] is the result of an experiment where subjective
    assessments were made or, in other words, where a rank was allocated to the various
    data parameters. For example, if the distance between points 2 and 5 was at rank
    4 in the original data, in such a scenario, it will not be wise to use absolute
    values of *d*[*i*][*j*], and hence relative values or *rank values* have to be
    used. Here, distance can mean a kind of ranking—for example, who came first in
    a race. This is the process in nonmetric MDS. For example, imagine we have four
    points: A, B, C, and D. We wish to rank the respective distances between these
    four points. The respective combinations of points can be A and B, A and C, A
    and D, B and C, B and D, and C and D. Their distances can be ranked as shown in
    table 6.1.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.1 The respective distance between four points and the ranks of the distances
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Pair of points | Distance | Ranks of the respective distances |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| A and B  | 100  | 3  |'
  prefs: []
  type: TYPE_TB
- en: '| A and C  | 105  | 4  |'
  prefs: []
  type: TYPE_TB
- en: '| A and D  | 95  | 2  |'
  prefs: []
  type: TYPE_TB
- en: '| B and C  | 205  | 6  |'
  prefs: []
  type: TYPE_TB
- en: '| B and D  | 150  | 5  |'
  prefs: []
  type: TYPE_TB
- en: '| C and D  | 55  | 1  |'
  prefs: []
  type: TYPE_TB
- en: So, in the nonmetric MDS method, instead of using the actual distances, we use
    the respective ranks of the distance. We next move on to the metric MDS method.
  prefs: []
  type: TYPE_NORMAL
- en: We know that in classical MDS, the original and fitted distances are represented
    in the same metric. In metric MDS, it is assumed that the values of *d*[*i*][*j*]
    can be transformed into Euclidean distances by employing some parametric transformation
    on the datasets. In some articles, you might find classical and metric MDS used
    interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: In MDS, as a first step, the respective distances between the points are calculated.
    Once the respective distances have been calculated, then MDS will try to represent
    the higher dimensional data point in a lower dimensional space. To perform this,
    an optimization process has to be carried out so that the optimum number of resultant
    dimensions can be chosen. Hence, a loss function or cost function has to be optimized.
  prefs: []
  type: TYPE_NORMAL
- en: Cost function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We use algorithms to predict the values of a variable. For example, we might
    use some algorithm to predict the expected demand of a product next year. We would
    want the algorithm to predict as accurately as possible. Cost functions are a
    simple method to check the performance of the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Cost function is a simple technique to measure the effectiveness of our algorithms.
    It is the most common method used to gauge the performance of a predictive model.
    It compares the original values and the predicted values by the algorithm and
    calculates how wrong the model is in its prediction.
  prefs: []
  type: TYPE_NORMAL
- en: As you would imagine, in an ideal solution, we would want the predicted values
    to be the same as the actual values, which is very difficult to achieve. If the
    predicted values differ a lot from the actual values, the output of a cost function
    is higher. If the predicted values are closer to the actual values, then the value
    of a cost function is lower. A robust solution is one that has the lowest value
    of the cost function. Hence, the objective to optimize any algorithm will be to
    minimize the value of the cost function. Cost function is also referred to as
    loss function; these two terms can be used interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: 'In metric MDS, we can also call the cost function *stress.* It is just another
    name for cost function. The formula for stress is given in equation 6.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch6-eqs-1x.png)'
  prefs: []
  type: TYPE_IMG
- en: (6.2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the equation,
  prefs: []
  type: TYPE_NORMAL
- en: Term Stress[*D*] is the value the MDS function has to minimize.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data points with the new set of coordinates in a lower dimensional space
    are represented by *x*[1], *x*[2], *x*[3]…. *x*[*N*].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term ||*x*[*i*]– *x*[*j*]|| is the distance between two points in their
    lower dimensional space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term *d*[*i*][*j*] is the original distance between the two points in the
    original multidimensional space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By looking at the equation, we can see that if the values of ||*x*[*i*]– *x*[*j*]||
    and *d*[*i*][*j*] are close to each other, the value of the resultant stress will
    be small.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Minimizing the value of stress is the objective of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: To optimize this loss function, we can use multiple approaches. One of the most
    famous methods is using a gradient descent that was originally proposed by Kruskal
    and Wish in 1978\. The gradient descent method is very simple to understand and
    can be explained using a simple analogy.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you are standing on top of a mountain and you want to get down. You
    want to choose the fastest path because you want to get down as fast as possible
    (no, you cannot jump!). So, to take the first step, you look around and, whichever
    is the steepest path, you take a step in that direction and reach a new point.
    Then again, you take a step in the steepest direction. This process is shown in
    the first diagram in figure 6.3\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F03_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 A person standing on top of a mountain and trying to get down. The
    process of gradient descent follows this method (left). The actual process of
    optimization of a cost function in gradient descent process. Note that at the
    point of convergence, the value of the cost function is minimal (right).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now say an algorithm has to achieve a similar feat; the process is represented
    in the right diagram in figure 6.3, wherein a loss function starts at a point
    and finally reaches the point of convergence. At this point of convergence, the
    cost function is minimal.
  prefs: []
  type: TYPE_NORMAL
- en: MDS differs from the other dimensionality reduction techniques. As compared
    to techniques like PCA, MDS does not make any assumptions about the dataset and
    hence can be used for a larger number of datasets. Moreover, MDS allows the use
    of any distance measurement metric. Unlike PCA, MDS is not an eigenvalue-eigenvector
    technique. Recall in PCA, the first axis captures the maximum amount of variance,
    the second axis has the next best variance, and so on. In MDS, there is no such
    condition. The axes in MDS can be inverted or rotated as needed. Also, in most
    of the other dimensional reduction methods used, the algorithms do calculate a
    lot of axes, but they cannot be viewed. In MDS, a smaller number of dimensions
    are explicitly chosen at the start. Hence there is less ambiguity in the solution.
    Further, in other algorithms, generally, there is only one unique solution, whereas
    MDS tries to iteratively find the most acceptable solution. It means that in MDS
    there can be multiple solutions for the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: But at the same time, the computation time required for MDS is greater for bigger
    datasets—and there is a catch in the gradient descent method used for optimization
    (see figure 6.4). Let’s refer to the mountain example we covered earlier. Imagine
    that while you are coming down from the top of the mountain, the starting point
    is A, and the bottom of the mountain is point C. While you are coming down, you
    reach point B. As you can see in the left diagram in the figure, there is a slight
    elevation around point B. At this point B, you might incorrectly conclude that
    you have reached the bottom of the mountain. In other words, you will think that
    you have finished your task. This is the problem of the local minima.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F04_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 While the first figure is the point of convergence and represents
    the gradient descent method, note that in the second figure the global minima
    is somewhere else, while the algorithm can be stuck at a local minima. The algorithm
    might check that it has optimized the cost function and reached the point of global
    minima, whereas it has only reached the local minima. In a local minima, there
    is no direction that is ascending; all the directions descend. The algorithm,
    if purely local, has no information about other deeper minima existing beyond
    a potentially small hill.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It is a possibility that instead of a global minimum, the loss function might
    be stuck in a local minima. The algorithm might think that it has reached the
    point of convergence, while the complete convergence might not have been achieved,
    and we are at a local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: There is still a question to be answered about the efficacy of the MDS solution.
    How can we measure the effectiveness of the solution? In the original paper, Kruskal
    recommended the stress values to measure the goodness-of-fit of the solution,
    which are shown in table 6.2\. The recommendations are mostly based on the empirical
    experience of Kruskal. These stress values are based on Kruskal’s experience.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.2 Stress values and their goodness of fit
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Stress values | Goodness of fit |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.200  | Poor  |'
  prefs: []
  type: TYPE_TB
- en: '| 0.100  | Fair  |'
  prefs: []
  type: TYPE_TB
- en: '| 0.050  | Good  |'
  prefs: []
  type: TYPE_TB
- en: '| 0.025  | Excellent  |'
  prefs: []
  type: TYPE_TB
- en: '| 0.000  | Perfect  |'
  prefs: []
  type: TYPE_TB
- en: 'The next logical question is: How many final dimensions should we choose? A
    scree plot provides the answer, as shown in figure 6.5\. Recall in chapter 2 we
    used a similar elbow method to choose the optimal number of clusters in k-means
    clustering. For MDS too, we can use the elbow method to determine the optimal
    number of components to represent the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F05_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 Scree plot to find the optimal number of components. It is similar
    to the k-means solution; we have to look for the elbow in the plot.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Exercise 6.1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between metric and nonmetric MDS algorithms?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradient descent is used to maximize the cost. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the gradient descent method using a simple example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 6.3 Python implementation of MDS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the Python implementation of the MDS method we will use the famous Iris
    dataset, which we have used previously. Using the algorithm is quite simple, thanks
    to the libraries available in the `scikit learn` package.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  The implementation is generally simple as the heavy lifting is done by
    the libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the libraries. The usual suspects are `sklearn`, `matplotlib`, and `numpy`,
    and we also load `MDS` from `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '2\. Load the dataset. The Iris dataset is available in the `sklearn` library,
    so we need not import Excel or .csv files here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '3\. A requirement for MDS is that the dataset should be scaled before the actual
    visualization is done. We use the `MixMaxScalar()` function to achieve this. MinMax
    scaling simply scales the data using the formula in equation 6.3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch6-eqs-2x.png)'
  prefs: []
  type: TYPE_IMG
- en: (6.3)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As an output of this step, the data is scaled and ready for the next step of
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Invoke the MDS method from the `sklearn` library. The `random_state` value
    allows us to reproduce the results. We have chosen the number of components as
    3 for the example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '5\. Fit the scaled data created earlier using the MDS model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '6\. Declare the colors we wish to use for visualization. Next, the data points
    are visualized in a scatter plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The output of the preceding code is shown in figure 6.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F06_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 Output for the Iris data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This example of Python implementation is a visualization of the Iris data. It
    is quite a simple example, as it does not involve stress and optimization for
    the number of components. In other words, we need a more complex dataset to really
    optimize MDS. We will now work on a curated dataset to implement MDS (see figure
    6.7).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F07_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 Various cities and their respective distances between each other
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let us assume we have five cities and the respective distance between them
    is given in figure 6.7\. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already imported the libraries in the last code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '2\. Create the dataset. Although we create a dataset here, in real business
    scenarios, it will be in the form of distances only (see figure 6.8):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH06_F08_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 Creating the dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '3\. Use the `MinMaxScalar()` function to scale the dataset as we did in the
    last coding exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now we work toward finding the most optimal number of components. We will iterate
    for different values of the number of components. For each of the values of the
    number of components, we will get the value of stress. The point at which a kink
    is observed is the optimal number of components.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, we will declare an empty dataframe, which can be used to store
    the values of the number of components and corresponding stress values. Then we
    iterate from 1 to 10 in a `for` loop. Finally, for each of the values of components
    (1 to 10), we get the respective values of stress:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '4\. Now that we have the values of stress, we will plot these values in a graph.
    The respective labels for each of the axes are also given. Look at the kink at
    values 2 and 3 in figure 6.9\. These can be the optimal values of the number of
    components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH06_F09_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 Scree plot to select the optimized number of components
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '5\. Run the solution for the number of components = 3\. If we look at the values
    of stress, number of components = 3, it generates the minimum value of stress
    as 0.00665 (see figure 6:10):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This concludes our discussion on the MDS algorithm. We discussed the foundation
    and concepts, pros and cons, algorithm assessment, and Python implementation of
    MDS. As one of the nonlinear dimensionality reduction methods, it is a great solution
    for visualization and dimensionality reductions.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F10_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10 Output for the MDS dataset: representation of the five cities in
    a plot'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.4 t-distributed stochastic neighbor embedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If a dataset is really high dimensional, the analysis becomes cumbersome. The
    visualization is even more confusing. We have covered that in great detail in
    the curse of dimensionality section in chapter 3\. You are advised to revisit
    the concept before proceeding if you need a refresher.
  prefs: []
  type: TYPE_NORMAL
- en: One such really high-dimensional dataset can be image data. We find it difficult
    to comprehend such data due to anything beyond 3 dimensions being increasingly
    difficult for us to intuit.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have used facial recognition software on your smartphone. For such
    solutions, facial images have to be analyzed, and machine learning models have
    to be trained. Look at the pictures in figure 6.11: we have a human face, a bike,
    a vacuum cleaner, and a screen capture of a phone.'
  prefs: []
  type: TYPE_NORMAL
- en: Image is a complex data type. Each image is made up of pixels, and each pixel
    can be made up of RGB (red, green, blue) values. Values for each of the RGB can
    range from 0 to 255\. The resulting dataset will be a very high-dimensional dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F11_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 Images are quite complex to decipher by an algorithm. Images can
    be of any form and can be of a person, a piece of equipment, or even a phone screen.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now recall PCA, which we studied in chapter 3\. PCA is a linear algorithm. Thus,
    its capability to resolve nonlinear and complex polynomial functions is limited.
    Moreover, when a high-dimensional dataset has to be represented in a low-dimensional
    space, the algorithm should keep similar data points close to each other, which
    can be a challenge in linear algorithms. PCA, as a linear dimension reduction
    technique, tries to separate the different data points as far away from each other
    as possible, and tries to maximize the variance captured in the data. The resulting
    analysis is not robust and might not be best suited for further use and visualization.
    Hence, we have nonlinear algorithms like t-SNE to help.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE is a nonlinear dimensionality reduction technique that is quite handy
    for high-dimensional data. It is based on stochastic neighbor embedding, which
    was developed by Sam Roweis and Geoffrey Hinton. The t-distributed variant was
    proposed by Lauren van der Maaten. Thus, t-SNE is an improvement of the SNE algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, SNE measures the similarity between instance pairs in a high-dimensional
    space and in a low-dimensional space. A good solution is where the difference
    between these similarity measures is the least, and SNE then optimizes these similarity
    measures using a cost function similar to what we have discussed for MDS.
  prefs: []
  type: TYPE_NORMAL
- en: 'We examine the step-by-step process of t-SNE next. The process described is
    a little heavy on mathematics:'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a high-dimensional space and some points in it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure the similarities between the various points in the high-dimensional
    space mentioned in the last point. For a point *x*[*i*], we will then create a
    Gaussian distribution centered at that point. We have already studied Gaussian
    or normal distribution in chapter 2\. The Gaussian distribution is shown in figure
    6.12.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F12_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 Gaussian or normal distribution.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 3\. Measure the density of points (let’s say *x*[*j*]) that fall under that
    Gaussian distribution and then renormalize them to get the respective conditional
    probabilities (*p*[*j*][|][*i*]). For the points that are nearby and hence similar,
    this conditional probability will be high, and for the points that are far and
    dissimilar, the value of conditional probabilities (*p*[*j*][|][*i*]) will be
    very small. These values of probabilities are those in the high-dimensional space.
    For curious readers, the mathematical formula for this conditional probability
    is presented as equation 6.4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch6-eqs-3x.png)'
  prefs: []
  type: TYPE_IMG
- en: (6.4)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: where *σ* is the variance of the Gaussian distribution centered at *x*[*i*].
    The mathematical proof is beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Measure one more set of probabilities in the low-dimensional space. For
    this set of measurements, we use the Cauchy distribution, described next. We use
    Kullback-Liebler (KL) divergence for measuring the difference between two probability
    distributions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 6.4.1 Cauchy distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Cauchy distribution belongs to the family of continuous probability distributions.
    Though there is a resemblance with the normal distribution, as we have represented
    in figure 6.13, the Cauchy distribution has a narrower peak and spreads out more
    slowly. It means that, compared to a normal distribution, the probability of obtaining
    values far from the peaks is higher. Sometimes, the Cauchy distribution is known
    as the *Lorentz distribution.* It is interesting to note that Cauchy does not
    have a well-defined mean, but the median is the center of symmetry.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F13_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.13 Comparison of Gaussian distribution vs. Cauchy distribution. (Image
    source: Quora)'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Consider we get *y*[*i*] and *y*[*j*] as the low-dimensional counterparts for
    the high-dimensional data points *x*[*i*] and *x*[*j*]. So we can calculate the
    probability score like we did in the last step. Using the Cauchy distribution,
    we can get a second set of probabilities *q*[*j*][|][*i*] too. The mathematical
    formula is shown in equation 6.5:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch6-eqs-4x.png)'
  prefs: []
  type: TYPE_IMG
- en: (6.5)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 2\. So far, we have calculated two set of probabilities (*p*[*j*][|][*i*]) and
    (*q*[*j*][|][*i*]). In this step, we compare the two distributions and measure
    the difference between the two. In other words, while calculating (*p*[*j*][|][*i*])
    we measured the probability of similarity in a high-dimensional space whereas
    for (*q*[*j*][|][*i*]) we did the same in a low-dimensional space. Ideally, the
    mapping of the two spaces is similar, and for that, there should not be any difference
    between (*p*[*j*][|][*i*]) and (*q*[*j*][|][*i*]). So the SNE algorithm tries
    to minimize the difference in the conditional probabilities (*p*[*j*][|][*i*])
    and (*q*[*j*][|][*i*]), similar to what we have done with MDS for the distance
    in high- and low-dimensional spaces.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3\. The difference between the two probability distributions is done using KL
    divergence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4\. To minimize the KL cost function, we use the gradient descent approach.
    We have already discussed the gradient descent approach in section 6.2 where we
    discussed the MDS algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DEFINITION  KL divergence or relative entropy is used to measure the difference
    between two probability distributions. Usually, one probability distribution is
    the data or the measured scores, and the second probability distribution is an
    approximation or the prediction of the original probability distribution—for example,
    if the original probability distribution is *X* and the approximated one is *Y*.
    KL divergence can be used to measure the difference between *X* and *Y* probability
    distributions. In absolute terms, if the value is 0, then it means that the two
    distributions are identical. The KL divergence is applicable for neurosciences,
    statistics, and fluid mechanics, among others.
  prefs: []
  type: TYPE_NORMAL
- en: There is one more important factor we should be aware of while we work on t-SNE,
    and that is *perplexity*. Perplexity is a hyperparameter that allows us to control
    and optimize the number of close neighbors each of the data points has.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  As per the official paper, a typical value for perplexity lies between
    5 and 50\.
  prefs: []
  type: TYPE_NORMAL
- en: 'There can be one additional nuance: the output of a t-SNE algorithm might never
    be the same on successive runs. We have to optimize the values of the hyperparameters
    to receive the best output.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 6.2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: Explain Cauchy distribution in your own words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PCA is a nonlinear algorithm. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: KL divergence is used to measure the difference between two probability distributions.
    True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 6.4.2 Python implementation of t-SNE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will use two datasets in this example. The first one is the Iris dataset,
    which we have already used more than once in this book. The second dataset is
    quite an interesting one: the MNIST dataset is a database of handwritten digits.
    It is one of the most famous datasets used to train image processing solutions
    and generally is considered the “Hello World” program for image detection solutions.
    An image representation is shown figure 6.14\.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F14_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 MNIST dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The steps for the Iris dataset are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary libraries. Note that we have imported the MNIST dataset
    from the `keras` library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: TIP  If you are not able to install modules in your Python code, refer to the
    appendix where we provide a solution.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Load the Iris dataset. The dataset comprises two parts: one is the “data”
    and the second is the respective label or “target” for it. It means that “data”
    is the description of the data and “target” is the type of iris. We print the
    features and the labels using code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '3\. Invoke the t-SNE algorithm. We are using the `n_components=2`, `verbose=1`,
    and `random_state=5` to reproduce the results. Then the algorithm is used to fit
    the data (see figure 6.15):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH06_F15_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 Output of the code when we are fitting the algorithm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4\. Plot the data. This step allows us to visualize the data fitted by the algorithm
    in the last step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we will initiate an empty dataframe. We will add three columns, one
    at a time. We start with `iris_targ`et, followed by `tSNE_first_component` and
    `tSNE_second_ component`. `tSNE_first_component` is the first column of the `fitted_data`
    dataframe, and therefore the index is `0`. `tSNE_second_component` is the second
    column of the `fitted_data` dataframe and hence the index is `1`. Finally, we
    represent the data in a scatterplot in figure 6.16:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH06_F16_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 t-SNE projection of the Iris dataset. Note how we are getting three
    separate clusters for the three classes we have in the dataset.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To implement the algorithm for the MNIST dataset, load the libraries and dataset.
    The libraries were already loaded in the last code example. Now load the dataset.
    The dataset requires `reshape`, which is done here (see figure 6.17):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH06_F17_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 Output of t-SNE for the 10 classes of digits represented in different
    shades of gray
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are a few important points to keep in mind while running t-SNE:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the algorithm with different values of hyperparameters before finalizing
    a solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideally, perplexity should be between 5 and 50, and for an optimized solution,
    the value of perplexity should be less than the number of points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T-SNE guesses the number of close neighbors for each of the points. For this
    reason, a dataset that is denser will require a much higher perplexity value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that perplexity is the hyperparameter that balances the attention given
    to both the local and the global aspects of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: t-SNE is a widely popular algorithm. It can be used for studying the topology
    of an area, but a single t-SNE cannot be used for making a final assessment. Instead,
    multiple t-SNE plots should be created to make any final recommendation. Sometimes
    there are complaints that t-SNE is a black-box algorithm. This might be true to
    a certain extent. What makes the adoption of t-SNE harder is that it does not
    generate the same results in successive iterations. Hence, you might find t-SNE
    recommended only for exploratory analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Uniform manifold approximation projection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: UMAP is a powerful and popular dimensionality reduction technique. It is designed
    to preserve both the local and global structures of the dataset while reducing
    the complexity and dimensions of the high-dimensional dataset to a low-dimensional
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: UMAP was introduced in 2018 by Lealand McInnes, John Healy, and James Melville.
    UMAP makes the data more suitable for visualizations and data analysis. This relates
    to the concepts of topology and manifold theory. UMAP assumes that the high-dimensional
    dataset often lies on a manifold, which means a low-dimensional structure is embedded
    in a higher-dimensional space. Hence, it attempts to project this manifold into
    a lower dimensional space, preserving both the nearest neighbor relationships,
    which is nothing but the local structure, and the larger relationships, which
    is the global structure.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.1 Working with UMAP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: UMAP methodology uses the concept of fuzzy simplicity sets. These sets represent
    the probability distribution of distances between various data points and capture
    the underlying manifold structures.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in UMAP is to construct a weighted graph where each of the data
    points is connected to its nearest neighbor based on a distance metric. Generally,
    the Euclidean distance is used as the distance metric. This graph construction
    is an abstract representation of the data structure in high dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to optimize the graph. The graph is optimized in a lower dimension
    space by minimizing cross-entropy loss between the original high-dimensional relationships
    and the newly created low-dimensional relationships. This uses the stochastic
    gradient descent, producing the UMAP embeddings. We will study stochastic gradient
    descent in chapter 9.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two key parameters for UMAP:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_neighbours`—The number of nearest neighbors to consider for each point.
    Using this parameter, we balance the preservation of the local data structure
    as compared to the global data structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_dist`—This is used to control how tightly the points are clustered together.
    Smaller values of minimum distance keep the points much closer and hence create
    deeper clusters. The larger value for minimum distance will create lighter clusters,
    which are spread out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.5.2 Using UMAP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The various uses of UMAP are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular uses of UMAP is the visualization of high-dimensional
    datasets in the bioinformatics field. Gene datasets are quite complex and multidimensional,
    where each data point might be represented by hundreds or thousands of attributes.
    Using UMAP, researchers can virtually inspect the clusters and the underlying
    relationships in the dataset. The solution helps them identify cell types, developmental
    stages, and gene expression patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UMAP is also applied to the natural language processing field by reducing the
    dimensionality of embeddings. It helps in the visualization of relationships between
    words or sentences or documents, making it easier to understand the similarities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UMAP can also be applied to images. It helps in the visualization of the plastering
    of images based on various similarities, hence it is quite useful for competitive
    vision tasks to understand how similar images can be clustered together.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UMAP can be used with other clustering algorithms like k-means or DBSCAN. It
    can uncover the hidden patterns in large datasets and since it preserves both
    local and global structures, the clusters found in lower dimensional representations
    often provide more important groupings as compared to the original high-dimensional
    dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to helping with visualizations, UMAP can also be used as a preprocessing
    step to reduce the dimensions of data. It can be used as an alternative to PCA
    or other solutions. By reducing the number of dimensions in a dataset, the model’s
    performance might be improved and the computation time is reduced.
  prefs: []
  type: TYPE_NORMAL
- en: The use of UMAP in Python is straightforward. The library `umap-learn` allows
    us to use the power of UMAP.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.3 Key points of UMAP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s now cover the key points of UMAP and compare it to other algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: Since UMAP is a nonlinear solution, it can capture more complex datasets and
    patterns as compared to PCA. Recall that PCA is a linear dimensionality tradition
    technique, so when the data is not on a simple linear manifold, UMAP proves to
    be more accurate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of PCA is to explain the maximum variance in the entire dataset. On
    the other hand, UMAP balances both local and global structures and hence is more
    versatile for tasks like anomaly detection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As compared to PCA, UMAP can be used for larger datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UMAP is faster than the other nonlinear solution, t-SNE. t-SNE can preserve
    the local structure of the data, but it struggles with preserving the global structure,
    and it can lead to a misleading interpretation of the clusters. UMAP does a better
    job as it preserves both local and global structures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UMAP results are much more stable and consistent across multiple iterations.
    For other algorithms, the results can be unstable and might change with different
    values of random seeds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UMAP has gained a lot of popularity recently and has become a go-to tool for
    machine learning and AI solutions. It is fast and can preserve both local and
    global data structures. Hence it is a strong option compared to other dimensionality
    reduction solutions like PCA, t-SNE, and autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: 6.6 Case study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In chapter 3, we explored a case study for the telecom industry reducing dimensionality.
    In this chapter, we will examine a small case study wherein t-SNE or MDS can be
    utilized for dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Have you heard about hyperspectral images? As you know, we humans see the colors
    of visible light in mostly three bands: long, medium, and short wavelengths. The
    long wavelengths are perceived as red, medium as green, and short as blue. All
    the other colors human beings perceive are simply mixtures of these three, and
    that is what allows screens and printers to work with only three colors. Spectral
    imaging, on the other hand, divides the spectrum into many more bands, and this
    technique can be extended beyond the visible and hence is used across biology,
    physics, geoscience, astronomy, agriculture, and many more avenues. Hyperspectral
    imaging collects and processes information from across the electromagnetic spectrum.
    It obtains the spectrum for each of the pixels in the image. See figure 6.18.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F18_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.18 Hyperspectral image of “sugar end” potato strips shows invisible
    defects (Source: SortingExpert, CC BY-SA 3.0)'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: One such dataset is the Pavia University dataset. The dataset is curated by
    the ROSIS sensor in Pavia, northern Italy. The details of the dataset are given
    next, and the dataset can be downloaded from [https://mng.bz/nRVa](https://mng.bz/nRVa).
  prefs: []
  type: TYPE_NORMAL
- en: There are 103 spectral bands in this dataset. The HIS size is 610 * 340 pixels,
    and it contains nine classes. Such a type of data can be used for crop analysis,
    mineral examination and exploration, etc. Since this data also contains information
    about geological patterns, it is quite useful for scientific purposes. Before
    developing any image recognition solution, we have to reduce the number of dimensions
    for this dataset. The computation cost will be much higher if we have a large
    number of dimensions. Hence, we want a lower representative number of dimensions.
    Figure 6.19 shows a few example bands. You are advised to download the dataset
    (which is also pushed at the GitHub repository) and use the various dimensionality
    reduction techniques on the dataset to reduce the number of dimensions. There
    can be many other image datasets and complex business problems where t-SNE and
    MDS can be of pragmatic use.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F19_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 Example of bands in the dataset. These are only random examples.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.7 Concluding thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dimensionality reduction is quite an interesting and useful field. It makes
    machine learning less expensive and less time-consuming. Imagine that you have
    a dataset with thousands of attributes or features. You do not know the data very
    well, the business understanding is limited, and, at the same time, you have to
    find the patterns in the dataset. You are not even sure if the variables are all
    relevant or just random noise. At such a moment, when we want to make the dataset
    less complex to crack and reduce the computational time, dimensionality reduction
    is the solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We covered dimensionality reduction techniques earlier in the book. This chapter
    covers three advanced techniques: t-SNE, MDS, and UMAP. All three techniques should
    not be considered a substitute for the other, easier techniques we discussed.
    Rather, they can be useful if we are not getting meaningful results with basic
    algorithms. It is always advised to use PCA first and then try the advanced techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: The complexity of the book is increasing. This chapter started with images—but
    we have only wet our toes. In the next chapter, we deal with text data. Perhaps
    you will find it very interesting and useful.
  prefs: []
  type: TYPE_NORMAL
- en: 6.8 Practical next steps and suggested readings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following provides suggestions for what to do next and offers some helpful
    reading:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the vehicles dataset used in chapter 2 for clustering and implement MDS
    on it. Compare the performance on clustering before and after implementing MDS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the datasets used in chapter 2 for Python examples and use them for implementing
    MDS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For MDS, refer to the following research papers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“Dimensionality Reduction: A Comparative Review,” by Lauren van der Maaten,
    Eric Postma, and H. Japp Van Den Herik: [https://mng.bz/eyxQ](https://mng.bz/eyxQ)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '“Multidimensional Scaling-Based Data Dimension Reduction Method for Application
    in Short-Term Traffic Flow Prediction for Urban Road Network,” by Satish V. Ukkusuri
    and Jian Lu: [https://mng.bz/pKmz](https://mng.bz/pKmz)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Get t-SNE research papers from the following links and study them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“Visualizing Data Using t-SNE,” by Laurens van der Maaten and Geoffrey Hinton:
    [https://mng.bz/OBaE](https://mng.bz/OBaE)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '“The Art of Using t-SNE for Single Cell Transcriptomics”: [https://mng.bz/YD9A](https://mng.bz/YD9A)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See the paper “Performance Evaluation of t-SNE and MDS Dimensionality Reduction
    Techniques with KNN, SNN, and SVM Classifiers”: [https://arxiv.org/pdf/2007.13487.pdf](https://arxiv.org/pdf/2007.13487.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MDS is a dimensionality reduction technique that transforms high-dimensional
    data into a lower-dimensional space while preserving distances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are three types of MDS: classical, metric, and nonmetric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classical MDS uses Euclidean distances, aligning original and fitted distances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonmetric MDS ranks distances rather than using absolute values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metric MDS transforms distances to fit a lower dimensional space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MDS involves calculating distances and optimizing a stress cost function with
    gradient descent, though it can be computationally intensive and is prone to local
    minima problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MDS works iteratively and does not make assumptions about data distribution,
    making it versatile for choosing distance metrics compared to PCA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: t-SNE is a nonlinear dimensionality reduction technique and is particularly
    effective for high-dimensional and complex datasets like images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: t-SNE optimizes similarity between data points in both high- and low-dimensional
    spaces using the Cauchy distribution and KL divergence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: t-SNE has an edge over PCA due to its nonlinear nature, though it involves hyperparameters
    like perplexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UMAP is another dimensionality reduction method that efficiently preserves both
    local and global data structures and is faster and more stable than t-SNE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python implementations are available for both MDS and t-SNE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MDS is one of the advanced dimensionality reduction techniques, requiring optimization
    of a loss function or cost function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
