- en: Chapter 15\. Transformers for Natural Language Processing and Chatbots
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 15 章\. 用于自然语言处理和聊天机器人的 Transformer
- en: 'In a landmark 2017 paper titled [“Attention Is All You Need”](https://homl.info/transformer),⁠^([1](ch15.html#id3414))
    a team of Google researchers proposed a novel neural net architecture named the
    *Transformer*, which significantly improved the state of the art in neural machine
    translation (NMT). In short, the Transformer architecture is simply an encoder-decoder
    model, very much like the one we built in [Chapter 14](ch14.html#nlp_chapter)
    for English-to-Spanish translation, and it can be used in exactly the same way
    (see [Figure 15-1](#transformer_encoder_decoder_diagram)):'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇标志性的 2017 年论文《“Attention Is All You Need”》中，⁠^([1](ch15.html#id3414)) 一组
    Google 研究人员提出了一种新颖的神经网络架构，名为 *Transformer*，它显著提高了神经机器翻译（NMT）的现有水平。简而言之，Transformer
    架构只是一个编码器-解码器模型，非常类似于我们在第 14 章（ch14.html#nlp_chapter）中为英语到西班牙语翻译所构建的模型，并且可以以完全相同的方式进行使用（参见[图
    15-1](#transformer_encoder_decoder_diagram)）：
- en: The source text goes in the encoder, which outputs contextualized embeddings
    (one per token).
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 源文本进入编码器，编码器输出上下文嵌入（每个标记一个）。
- en: The encoder’s output is then fed to the decoder, along with the translated text
    so far (starting with a start-of-sequence token).
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将编码器的输出以及到目前为止的翻译文本（从序列开始标记开始）输入到解码器。
- en: The decoder predicts the next token for each input token.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器为每个输入标记预测下一个标记。
- en: The last token output by the decoder is appended to the translation.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器输出的最后一个标记被附加到翻译中。
- en: Steps 2 to 4 are repeated again and again to produce the full translation, one
    extra token at a time, until an end-of-sequence token is generated. During training,
    we already have the full translation—it’s the target—so it is fed to the decoder
    in step 2 (starting with a start-of-sequence token), and steps 4 and 5 are not
    needed.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤 2 到 4 重复进行，以生成完整的翻译，每次添加一个额外的标记，直到生成一个序列结束标记。在训练过程中，我们已经有完整的翻译了——它是目标——因此在步骤
    2 中将其输入到解码器（从序列开始标记开始），步骤 4 和 5 就不再需要了。
- en: '![Diagram illustrating the Transformer model''s process for translating English
    to Spanish, showing how the encoder generates contextual embeddings and the decoder
    predicts the next token in the translated sequence.](assets/hmls_1501.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![说明 Transformer 模型将英语翻译成西班牙语的过程的图，展示了编码器如何生成上下文嵌入，以及解码器如何预测翻译序列中的下一个标记。](assets/hmls_1501.png)'
- en: Figure 15-1\. Using the Transformer model for English-to-Spanish translation
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-1\. 使用 Transformer 模型进行英语到西班牙语翻译
- en: So what’s new? Well, inside the black box, there are some important differences
    with our previous encoder-decoder. Crucially, the Transformer architecture does
    not contain any recurrent or convolutional layers, just regular dense layers combined
    with a new kind of attention mechanism called *multi-head attention* (MHA), plus
    a few bells and whistles.⁠^([2](ch15.html#id3417)) Because the model is not recurrent,
    it doesn’t suffer as much from the vanishing or exploding gradients problems as
    RNNs, it can be trained in fewer steps, it’s easier to parallelize across multiple
    GPUs, and it scales surprisingly well. Moreover, thanks to multi-head attention,
    the model can capture long-range patterns much better than RNNs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，有什么新内容呢？嗯，在黑盒内部，与我们的先前编码器-解码器相比，有一些重要的不同之处。关键的是，Transformer 架构不包含任何循环或卷积层，只有常规的密集层，结合了一种新的注意力机制，称为
    *多头注意力*（MHA），再加上一些额外的功能。⁠^([2](ch15.html#id3417)) 由于模型不是循环的，它不像 RNN 那样容易受到梯度消失或爆炸问题的影响，因此它可以以更少的步骤进行训练，更容易在多个
    GPU 上并行化，并且扩展性出奇地好。此外，多亏了多头注意力，模型能够比 RNN 更好地捕捉到长距离模式。
- en: The Transformer architecture also turned out to be extremely versatile. It was
    initially designed for NMT, but researchers quickly tweaked the architecture for
    many other language tasks. The year 2018 was even called the “ImageNet moment
    for NLP”. In June 2018, OpenAI released the first GPT model, based solely on the
    Transformer’s decoder module. It was pretrained on a large corpus of text, its
    ability to generate text was unprecedented, it could auto-complete sentences,
    invent stories, and even answer some questions. GPT could also be fine-tuned to
    perform a wide range of language tasks. Just a few months later, Google released
    the BERT model, based solely on the Transformer’s encoder module. It was excellent
    at a variety of *natural language understanding* (NLU) tasks, such as text classification,
    text embedding, multiple choice question answering, or finding the answer to a
    question within some text.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构也证明极其灵活。它最初是为NMT设计的，但研究人员很快对架构进行了调整，以适应许多其他语言任务。2018年甚至被称为“NLP的ImageNet时刻”。2018年6月，OpenAI发布了第一个基于Transformer解码模块的GPT模型。它在大量文本语料库上进行了预训练，其生成文本的能力是前所未有的，它可以自动完成句子，创作故事，甚至回答一些问题。GPT还可以微调以执行广泛的语言任务。仅仅几个月后，Google发布了基于Transformer编码模块的BERT模型。它在各种*自然语言理解*（NLU）任务上表现出色，例如文本分类、文本嵌入、多项选择题回答，或在某些文本中找到问题的答案。
- en: Surprisingly, transformers also turned out to be great at computer vision, audio
    processing (e.g., speech-to-text), robotics (using inputs from sensors and sending
    the outputs to actuators), and more. For example, if you split an image into little
    chunks and feed them to a transformer (instead of token embeddings), you get a
    *vision transformer* (ViT). In fact, some transformers can even handle multiple
    *modalities* at once (e.g., text + image); these are called *multimodal models*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，Transformer在计算机视觉、音频处理（例如，语音转文本）、机器人技术（使用传感器输入并将输出发送到执行器）等领域也表现出色。例如，如果你将图像分割成小块并喂给一个Transformer（而不是标记嵌入），你得到的是一个*视觉Transformer*（ViT）。实际上，一些Transformer甚至可以同时处理多种*模态*（例如，文本+图像）；这些被称为*多模态模型*。
- en: 'This outstanding combination of performance, flexibility, and scalability encouraged
    Google, OpenAI, Facebook (Meta), Microsoft, Anthropic, and many other organizations
    to train larger and larger transformer models. The original Transformer model
    had about 65 million parameters—which was considered quite large at the time—but
    new transformers grew at a mind-boggling rate, reaching 1.6 trillion parameters
    by January 2021—that’s 1.6 million million parameters! Training such a gigantic
    transformer model from scratch is sadly restricted to organizations with deep
    pockets, as it requires a large and costly infrastructure for several months:
    training typically costs tens of millions of dollars, and even up to hundreds
    of millions according to some estimates (the exact figures are generally not public).
    [Figure 15-2](#transformer_growth_diagram) shows some of the most influential
    transformers released between June 2018 and April 2025.⁠^([3](ch15.html#id3423))
    Note that the vertical axis is in *billions* of parameters, and it uses a logarithmic
    scale.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这种卓越的性能、灵活性和可扩展性的组合促使Google、OpenAI、Facebook（Meta）、Microsoft、Anthropic和其他许多组织训练越来越大的Transformer模型。原始的Transformer模型大约有6500万个参数——在当时被认为相当大——但新的Transformer以令人难以置信的速度增长，到2021年1月达到了160亿个参数——即1600万个百万个参数！从头开始训练如此庞大的Transformer模型遗憾的是仅限于财力雄厚的组织，因为它需要几个月内的大型且昂贵的基础设施：训练通常需要数百万美元，据某些估计甚至高达数亿美元（确切数字通常不公开）。[图15-2](#transformer_growth_diagram)显示了2018年6月至2025年4月之间发布的一些最有影响力的Transformer。请注意，垂直轴是以*十亿*参数为单位的，并且使用了对数刻度。
- en: '![Diagram showing the exponential growth of transformer models from 2018 to
    2025, highlighting influential releases by organizations like OpenAI, Google,
    and Facebook, with parameter counts rising into trillions.](assets/hmls_1502.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![展示从2018年到2025年Transformer模型指数增长的图表，突出了OpenAI、Google和Facebook等组织有影响力的发布，参数数量上升至万亿级别](assets/hmls_1502.png)'
- en: Figure 15-2\. Some of the most influential transformers released since 2018;
    see it larger [online](https://homl.info/fig15-2)
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-2。自2018年以来发布的一些最有影响力的Transformer；[在线查看更大版本](https://homl.info/fig15-2)
- en: 'Then, in November 2022, OpenAI released ChatGPT, an amazing *conversational
    AI*—or *chatbot*—that took the world by storm: it reached one million users in
    just five days, and over one hundred million monthly active users after just two
    months! Under the hood, it used GPT-3.5-turbo, a variant of GPT-3.5 which was
    fine-tuned to be conversational, helpful, and safe. Others soon followed: Perplexity
    AI, Google’s Gemini (initially called Bard), Anthropic’s Claude, Mistral AI, DeepSeek,
    and more.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在2022年11月，OpenAI发布了ChatGPT，一款令人惊叹的*对话式人工智能*——或者说是*聊天机器人*——它迅速席卷了全球：仅用五天时间就达到了一百万用户，两个月后月活跃用户数超过一亿！在底层，它使用了GPT-3.5-turbo，这是GPT-3.5的一个变种，经过微调以实现对话性、帮助性和安全性。其他公司也很快跟上了：Perplexity
    AI、谷歌的Gemini（最初称为Bard）、Anthropic的Claude、Mistral AI、DeepSeek以及更多。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Before ChatGPT was released, Google had actually developed a powerful chatbot
    named LaMDA, but it wasn’t made public, likely for fear of reputational and legal
    risks, as the model was not deemed safe enough. This allowed OpenAI to become
    the first company to train a reasonably safe and helpful model and to package
    it as a useful chatbot product.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在ChatGPT发布之前，谷歌实际上已经开发了一个名为LaMDA的强大聊天机器人，但它并未公开，可能是因为担心声誉和法律风险，因为这个模型被认为还不够安全。这使得OpenAI成为第一家训练出相对安全和有帮助的模型并将其作为有用的聊天机器人产品的公司。
- en: 'So how can you use these models and chatbots? Well, many of them are proprietary
    (e.g., OpenAI’s GPT-3.5, GPT-4 and GPT-5 models, Anthropic’s Claude models, and
    Google’s Gemini models), and they can only be used via a web UI, an app, or an
    API: you must create an account, choose an offer (or use the free tier), and for
    the API you must get an access token and use it to query the API programmatically.
    However, many other models are *open weights*, meaning they can be downloaded
    for free (e.g., using the Hugging Face Hub): some of these have licensing restrictions
    (e.g., Meta’s Llama models are only free for noncommercial use), while others
    are truly open source (e.g., DeepSeek’s R1 or Mistral AI’s Mistral-7B). Some even
    include the training code and data (e.g., the OLMo models by Ai2).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你如何使用这些模型和聊天机器人呢？嗯，其中许多是专有的（例如，OpenAI的GPT-3.5、GPT-4和GPT-5模型、Anthropic的Claude模型和谷歌的Gemini模型），并且只能通过Web
    UI、应用程序或API使用：你必须创建一个账户，选择一个套餐（或使用免费层），对于API，你必须获取访问令牌并使用它来程序化查询API。然而，许多其他模型是*开源权重*，这意味着它们可以免费下载（例如，使用Hugging
    Face Hub）：其中一些有许可限制（例如，Meta的Llama模型仅限非商业用途免费），而其他则是真正的开源（例如，DeepSeek的R1或Mistral
    AI的Mistral-7B）。甚至有些还包括训练代码和数据（例如，Ai2的OLMo模型）。
- en: 'So what are we waiting for? Let’s join the transformer revolution! Here’s the
    plan:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们还在等什么呢？让我们加入Transformer革命！以下是计划：
- en: We will start by opening up the original Transformer architecture and inspecting
    its components to fully understand how everything works.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先将打开原始的Transformer架构，检查其组件，以全面了解其工作原理。
- en: Then we will build and train a transformer from scratch for English-to-Spanish
    translation.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将从头开始构建和训练一个用于英语到西班牙语翻译的Transformer。
- en: After that, we will look into encoder-only models like BERT, learn how they
    are pretrained, and see how to use them for tasks like text classification, semantic
    search, and text clustering, with or without fine-tuning.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之后，我们将研究仅使用编码器模型，如BERT，学习它们的预训练方法，并了解如何使用它们进行文本分类、语义搜索和文本聚类等任务，无论是进行微调还是不进行微调。
- en: Next, we will look into decoder-only models like GPT, and see how they are pretrained.
    These models are capable of generating text, which is great if you want to write
    a poem, but it can also be used to tackle many other tasks.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨仅使用解码器模型，如GPT，并了解它们的预训练过程。这些模型能够生成文本，如果你想要写一首诗，这当然很棒，但它们也可以用于解决许多其他任务。
- en: 'Then we will use a decoder-only model to build our own chatbot! This involves
    a few steps: first, you must download a pretrained model (or train your own if
    you have the time and money), then you must fine-tune it to make it more conversational,
    helpful, and safe (or you can download an already fine-tuned model, or even use
    a conversational model via an API), and lastly you must deploy the model to a
    chatbot system that offers a user interface, stores conversations, and can also
    give the model access to tools, such as searching the web or using a calculator.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将使用仅使用解码器的模型来构建我们自己的聊天机器人！这涉及几个步骤：首先，你必须下载一个预训练模型（或者如果你有时间和金钱，可以自己训练），然后你必须微调它以使其更具对话性、帮助性和安全性（或者你可以下载已经微调好的模型，甚至可以通过API使用对话式模型），最后你必须将模型部署到一个提供用户界面、存储对话并能够为模型提供工具（如搜索网络或使用计算器）的聊天机器人系统中。
- en: Lastly, we will take a quick look at encoder-decoder models, such as T5 and
    BART, which are great for tasks such as translation and summarization.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将快速浏览一下编码器-解码器模型，例如T5和BART，这些模型非常适合翻译和摘要等任务。
- en: In [Chapter 16](ch16.html#vit_chapter), we will look at vision transformers
    and multimodal transformers. [Chapter 17](ch17.html#speedup_chapter) and “State-Space
    Models (SSMs)” (both available at [*https://homl.info*](https://homl.info)) also
    discuss some advanced techniques to allow Transformers to scale and process longer
    input sequences.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第16章](ch16.html#vit_chapter)中，我们将探讨视觉Transformer和多模态Transformer。[第17章](ch17.html#speedup_chapter)和“状态空间模型（SSMs）”（均可在[*https://homl.info*](https://homl.info)找到）也讨论了一些高级技术，以允许Transformer进行扩展并处理更长的输入序列。
- en: 'Let’s start by dissecting the Transformer architecture: take out your scalpel!'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从剖析Transformer架构开始：拿出你的解剖刀吧！
- en: 'Attention Is All You Need: The Original Transformer Architecture'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力即是所有你需要的东西：原始的Transformer架构
- en: The original 2017 Transformer architecture is represented in [Figure 15-3](#transformer_diagram).
    The left part of the figure represents the encoder, the right part represents
    the decoder.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的2017年Transformer架构在[图15-3](#transformer_diagram)中展示。图的左侧代表编码器，右侧代表解码器。
- en: 'As we saw earlier, the encoder’s role is to gradually *transform* the inputs
    (e.g., sequences of English tokens) until each token’s representation perfectly
    captures the meaning of that token in the context of the sentence: the encoder’s
    output is a sequence of contextualized token embeddings. Apart from the embedding
    layer, every layer in the encoder takes as input a tensor of shape [*batch size*,
    *max English sequence length in the batch*, *embedding size*] and returns a tensor
    of the exact same shape. This means that token representations get gradually transformed,
    hence the name of the architecture. For example, if you feed the sentence “I like
    soccer” to the encoder, then the token “like” will start off with a rather vague
    representation, since “like” could mean different things in different contexts
    (e.g., “I’m like a cat” versus “I like my cat”). But after going through the encoder,
    the token’s representation should capture the correct meaning of “like” in the
    given sentence (in this case, to be fond of), as well as any other information
    that may be required for translation (e.g., it’s a verb).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所见，编码器的作用是将输入（例如，英语标记的序列）逐步*转换*，直到每个标记的表示完美捕捉到该标记在句子中的含义：编码器的输出是一系列上下文化的标记嵌入。除了嵌入层之外，编码器中的每一层都接受形状为[*批大小*，*批中最大英语序列长度*，*嵌入大小*]的张量作为输入，并返回形状完全相同的张量。这意味着标记表示会逐步转换，因此得名该架构。例如，如果你将句子“我喜欢足球”输入到编码器中，那么标记“喜欢”一开始将有一个相当模糊的表示，因为“喜欢”在不同的上下文中可能有不同的含义（例如，“我喜欢猫”与“我喜欢我的猫”）。但是经过编码器处理后，标记的表示应该能够捕捉到给定句子中“喜欢”的正确含义（在这种情况下，表示喜爱），以及可能需要用于翻译的任何其他信息（例如，它是一个动词）。
- en: 'The decoder’s role is to take the encoder’s outputs, along with the translated
    sentence so far, and predict the next token in the translation. For this, the
    decoder layers gradually transform each input token’s representation into a representation
    that can be used to predict the following token. For example, suppose the sentence
    to translate is “I like soccer” and we’ve already called the decoder four times,
    producing one new token each time: first “me”, then “me gusta”, then “me gusta
    el”, and finally “me gusta el fútbol”. Since this translation does not end with
    an EoS token `"</s>"`, we must call the decoder once again. The decoder’s input
    sequence is now "`<s>` me gusta el fútbol”. As the representation of each token
    goes through the decoder, it gets transformed: the representation of `"<s>"` becomes
    a rich enough representation to predict “me” (for simplicity, I’ll say this more
    concisely as: `"<s>"` becomes “me”), “me” becomes “gusta”, “gusta” becomes “el”,
    “el” becomes “fútbol”, and if everything goes well, “fútbol” becomes the EoS token
    `"</s>"`. Apart from the embedding layer and the output `Linear` layer, every
    layer in the decoder takes as input a tensor of shape [*batch size*, *max Spanish
    sequence length in the batch*, *embedding size*] and returns a tensor of the exact
    same shape.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的角色是接收编码器的输出以及迄今为止翻译的句子，并预测翻译中的下一个标记。为此，解码器层逐渐将每个输入标记的表示转换为可以用来预测下一个标记的表示。例如，假设要翻译的句子是“我喜欢足球”，并且我们已经调用了解码器四次，每次产生一个新标记：首先“我”，然后“我喜欢”，然后“我喜欢足球的”，最后“我喜欢足球”。由于这个翻译不以EoS标记`"</s>"`结束，我们必须再次调用解码器。现在解码器的输入序列是"`<s>`我喜欢足球”。随着每个标记的表示通过解码器，它会被转换：`"<s>"`的表示变成了足够丰富的表示，可以预测“我”（为了简单起见，我会更简洁地说：`"<s>"`变成了“我”），“我”变成了“喜欢”，“喜欢”变成了“的”，“的”变成了“足球”，如果一切顺利，“足球”将变成EoS标记`"</s>"`。除了嵌入层和输出`Linear`层之外，解码器中的每一层都接受形状为[*batch
    size*, *batch中的最大西班牙语序列长度*, *嵌入大小*]的张量作为输入，并返回形状完全相同的张量。
- en: '![Diagram of the original 2017 transformer architecture, showing the encoder
    and decoder layers with multi-head attention, feed forward, and positional encoding
    processes, illustrating how inputs are transformed into outputs.](assets/hmls_1503.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![2017年原始transformer架构的示意图，显示了具有多头注意力、前馈和位置编码过程的编码器和解码器层，说明了输入是如何转换为输出的。](assets/hmls_1503.png)'
- en: Figure 15-3\. The original 2017 transformer architecture⁠^([4](ch15.html#id3427))
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-3. 原始2017年transformer架构⁠^([4](ch15.html#id3427))
- en: 'After going through the decoder, each token representation goes through a final
    `Linear` layer which will hopefully output a high logit for the correct token
    and a low logit for all other tokens in the vocabulary. The decoder’s output shape
    is [*batch size*, *max Spanish sequence length in the batch*, *vocabulary size*].
    The final predicted sentence should be “me gusta el fútbol `</s>`“. Note that
    the figure shows a softmax layer at the top, but in PyTorch we usually don’t explicitly
    add it: instead, we let the model output logits, and we train the model using
    `nn.CrossEntropyLoss`, which computes the cross-entropy loss based on logits instead
    of estimated probabilities (as we saw in previous chapters). If you ever need
    estimated probabilities, you can always convert the logits to estimated probabilities
    using the `F.softmax()` function.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 经过解码器处理后，每个标记的表示会通过一个最终的`Linear`层，希望输出正确的标记的高logit值，以及词汇表中所有其他标记的低logit值。解码器的输出形状是[*batch
    size*, *batch中的最大西班牙语序列长度*, *词汇表大小*]。最终的预测句子应该是“我喜欢足球 `</s>`”。请注意，图中显示了顶部的softmax层，但在PyTorch中我们通常不会明确添加它：相反，我们让模型输出logit，并使用`nn.CrossEntropyLoss`训练模型，该损失函数基于logit而不是估计概率（如我们在前面的章节中看到的）。如果您需要估计概率，您始终可以使用`F.softmax()`函数将logit转换为估计概率。
- en: 'Now let’s zoom in a bit further into [Figure 15-3](#transformer_diagram):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进一步聚焦于[图15-3](#transformer_diagram)：
- en: First, notice that both the encoder and the decoder contain blocks that are
    stacked *N* times. In the paper, *N* = 6\. Note that the final outputs of the
    whole encoder stack are fed to each of the decoder’s *N* blocks.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，注意编码器和解码器都包含被堆叠了*N*次的块。在论文中，*N* = 6。请注意，整个编码器堆栈的最终输出被馈送到解码器的*N*个块中。
- en: 'As you can see, you are already familiar with most components: there are two
    embedding layers; several skip connections, each of them followed by a layer normalization
    module; several feedforward modules composed of two dense layers each (the first
    one using the ReLU activation function, the second with no activation function);
    and finally, the output layer is a linear layer. Notice that all layers treat
    each token independently from all the others. But how can we translate a sentence
    by looking at the tokens completely separately? Well, we can’t, so that’s where
    the new components come in:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如您所见，您已经熟悉大多数组件：有两个嵌入层；几个跳跃连接，每个连接后面都跟着一个层归一化模块；几个由两个密集层组成的前馈模块（第一个使用ReLU激活函数，第二个没有激活函数）；最后，输出层是一个线性层。请注意，所有层都独立于其他所有标记处理每个标记。但我们如何通过完全单独查看标记来翻译一个句子呢？好吧，我们不能，这就是新组件发挥作用的地方：
- en: The encoder’s *multi-head attention* layer updates each token representation
    by attending to (i.e., paying attention to) every token in the same sentence,
    including itself. This is called *self-attention*. That’s where the vague representation
    of the word “like” becomes a richer and more accurate representation, capturing
    its precise meaning within the given sentence (e.g., the layer notices the subject
    “I” so it infers that “like” must be a verb). We will discuss exactly how this
    works shortly.
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器的*多头注意力*层通过关注（即，关注）同一句子中的每个标记（包括自身）来更新每个标记表示。这被称为*自*注意力。这就是模糊的单词“like”的表示变成一个更丰富、更准确的表示，捕捉其在给定句子中的精确含义（例如，层注意到主语“I”，因此推断“like”必须是一个动词）。我们很快就会讨论这是如何工作的。
- en: 'The decoder’s *masked multi-head attention* layer does the same thing, but
    when it processes a token, it doesn’t attend to tokens located after it: it’s
    a causal layer. For example, when it processes the token “gusta”, it only attends
    to the tokens `"<s>"`, “me”, and “gusta”, and it ignores the tokens “el” and “fútbol”
    (or else the model could cheat during training).'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器的*掩码多头注意力*层做的是同样的事情，但在处理一个标记时，它不会关注位于其后的标记：它是一个因果层。例如，当它处理标记“gusta”时，它只关注标记`"<s>"`、“me”和“gusta”，而忽略标记“el”和“fútbol”（否则模型在训练期间可能会作弊）。
- en: The decoder’s upper multi-head attention layer is where the decoder pays attention
    to the contextualized token representations output by the encoder stack. This
    is called *cross*-attention, as opposed to *self*-attention. For example, the
    decoder will probably pay close attention to the word “soccer” when it processes
    the word “el” and outputs a representation of the word “fútbol”.
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器的上层多头注意力层是解码器关注由编码器堆栈输出的上下文化标记表示的地方。这被称为*交叉*注意力，与*自*注意力相对。例如，当解码器处理单词“el”并输出单词“fútbol”的表示时，它可能会非常关注单词“soccer”。
- en: 'The *positional encodings* are dense vectors that represent the position of
    each token in the sentence. The *n*^(th) positional encoding is added to the token
    embedding of the *n*^(th) token in each sentence. This is needed because all layers
    in the Transformer architecture are position-agnostic, meaning they treat all
    positions equally (unlike recurrent or convolutional layers): when they process
    a token, they have no idea where that token is located in the sentence or relative
    to other words. But the order of words matters, so we must somehow give positional
    information to the Transformer. Adding positional encodings to the token representations
    is a good way to achieve this.'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*位置编码*是密集向量，表示句子中每个标记的位置。第*n*个位置编码被添加到每个句子中第*n*个标记的标记嵌入中。这是必需的，因为Transformer架构中的所有层都是位置无关的，这意味着它们平等地对待所有位置（与循环或卷积层不同）：当它们处理一个标记时，它们不知道该标记在句子中的位置或相对于其他单词的位置。但是单词的顺序很重要，因此我们必须以某种方式给Transformer提供位置信息。将位置编码添加到标记表示中是实现这一点的有效方法。'
- en: Note
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The first two arrows going into each multi-head attention layer in [Figure 15-3](#transformer_diagram)
    represent the keys and values, and the third arrow represents the queries.⁠^([5](ch15.html#id3439))
    In the self-attention layers, all three are equal to the token representations
    output by the previous layer, while in the cross-attention layers (i.e., the decoder’s
    upper attention layers), the keys and values are equal to the encoder’s final
    token representations, and the queries are equal to the token representations
    output by the previous decoder layer.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图15-3](#transformer_diagram)中，每个多头注意力层进入的前两个箭头代表键和值，第三个箭头代表查询。⁠^([5](ch15.html#id3439))
    在自注意力层中，这三个都等于前一层输出的标记表示，而在交叉注意力层（即解码器的上层注意力层）中，键和值等于编码器的最终标记表示，而查询等于前一个解码器层输出的标记表示。
- en: Now let’s go through the novel components of the Transformer architecture in
    more detail, starting with the positional encodings.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更详细地探讨Transformer架构的新颖组件，从位置编码开始。
- en: Positional Encodings
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置编码
- en: 'A positional encoding is a dense vector that encodes the position of a token
    within a sentence: the *i*^(th) positional encoding is added to the token embedding
    of the *i*^(th) token in each sentence. A simple way to implement this is to use
    an `Embedding` layer: just add embedding #0 to the representation of token #0,
    add embedding #1 to the representation of token #1, and so on. Alternatively,
    you can use an `nn.Parameter` to store the embedding matrix (initialized using
    small random weights), then add its first *L* rows to the inputs (where *L* is
    the max input sequence length): the result is the same, but it’s much faster.
    You can also add a bit of dropout to reduce the risk of overfitting. Here’s an
    implementation:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码是一个密集向量，用于编码标记在句子中的位置：第*i*个位置编码被添加到每个句子中第*i*个标记的标记嵌入中。实现这一点的简单方法是使用`Embedding`层：只需将嵌入编号#0添加到标记编号#0的表示中，将嵌入编号#1添加到标记编号#1的表示中，依此类推。或者，您可以使用`nn.Parameter`来存储嵌入矩阵（使用小的随机权重初始化），然后将它的前*L*行添加到输入中（其中*L*是最大输入序列长度）：结果是相同的，但速度要快得多。您还可以添加一些dropout以降低过拟合的风险。以下是实现方式：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The inputs have shape [*batch size*, *sequence length*, *embedding size*],
    but we are adding positional encodings of shape [*sequence length*, *embedding
    size*]. This works thanks to the broadcasting rules: the *i*^(th) positional embedding
    is added to the *i*^(th) token’s representation of each sentence in the batch.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的形状为[*批大小*, *序列长度*, *嵌入大小*]，但我们正在添加形状为[*序列长度*, *嵌入大小*]的位置编码。这得益于广播规则：第*i*个位置编码被添加到批次中每个句子的第*i*个标记的表示中。
- en: The authors of the Transformer paper also proposed using fixed positional encodings
    rather than trainable ones. Their approach used a pretty smart scheme based on
    the sine and cosine functions, but it’s not much used anymore, as it doesn’t really
    perform any better than trainable positional embeddings (except perhaps on small
    transformers, if you’re lucky). Please see this chapter’s notebook for more details.
    Moreover, newer approaches such as *relative position bias* (RPB), *rotary positional
    encoding* (RoPE), and *attention with linear bias* (ALiBi) generally perform better.
    To learn more about all of these alternative approaches to positional encoding,
    see “Relative Positional Encoding”.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 'Transformer论文的作者还提出了使用固定位置编码而不是可训练位置编码的方法。他们的方法使用了一个相当聪明的基于正弦和余弦函数的方案，但现在已很少使用，因为它并没有真正比可训练位置嵌入表现得更好（除非你很幸运，在小型Transformer上）。有关更多详细信息，请参阅本章的笔记本。此外，像*相对位置偏置*（RPB）、*旋转位置编码*（RoPE）和*带线性偏置的注意力*（ALiBi）等新方法通常表现更好。要了解更多关于所有这些位置编码的替代方法，请参阅“相对位置编码”。 '
- en: 'Now let’s look deeper into the heart of the Transformer model: the multi-head
    attention layer.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入探究Transformer模型的核心：多头注意力层。
- en: Multi-Head Attention
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头注意力
- en: The multi-head attention (MHA) layer is based on *scaled dot-product attention*,
    a variant of dot-product attention (introduced in [Chapter 14](ch14.html#nlp_chapter))
    that scales down the similarity scores by a constant factor. See [Equation 15-1](#scaled_dot_product_attention)
    for its vectorized equation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力（MHA）层基于*缩放点积注意力*，这是点积注意力（在第14章中介绍）的一种变体，通过一个常数因子缩放相似度得分。参见[方程15-1](#scaled_dot_product_attention)以获取其向量方程。
- en: Equation 15-1\. Scaled dot-product attention
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程15-1\. 缩放点积注意力
- en: <mo>Attention</mo><mrow><mo>(</mo><mrow><mi mathvariant="bold">Q</mi><mo lspace="0%"
    rspace="0%">,</mo><mi mathvariant="bold">K</mi><mo lspace="0%" rspace="0%">,</mo><mi
    mathvariant="bold">V</mi></mrow><mo>)</mo></mrow><mo>=</mo><mo>softmax</mo><mfenced><mfrac><mrow><msup><mi
    mathvariant="bold">QK</mi><mo>⊺</mo></msup></mrow><msqrt><mrow><msub><mi>d</mi><mi
    mathvariant="normal">k</mi></msub></mrow></msqrt></mfrac></mfenced><mi mathvariant="bold">V</mi>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <mo>注意力</mo><mrow><mo>(</mo><mrow><mi mathvariant="bold">Q</mi><mo lspace="0%"
    rspace="0%">,</mo><mi mathvariant="bold">K</mi><mo lspace="0%" rspace="0%">,</mo><mi
    mathvariant="bold">V</mi></mrow><mo>)</mo></mrow><mo>=</mo><mo>softmax</mo><mfenced><mfrac><mrow><msup><mi
    mathvariant="bold">QK</mi><mo>⊺</mo></msup></mrow><msqrt><mrow><msub><mi>d</mi><mi
    mathvariant="normal">k</mi></msub></mrow></msqrt></mfrac></mfenced><mi mathvariant="bold">V</mi>
- en: 'In this equation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在此方程中：
- en: '**Q** is a matrix representing a *query* (e.g., an English or Spanish sequence,
    depending on the attention layer). Its shape is [*L*[q], *d*[q]], where *L*[q]
    is the length of the query and *d*[q] is the query’s dimensionality (i.e., the
    number of dimensions in the token representations).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q** 是表示查询（例如，英语或西班牙语序列，具体取决于注意力层）的矩阵。其形状为 [*L*[q], *d*[q]]，其中 *L*[q] 是查询的长度，*d*[q]
    是查询的维度性（即标记表示中的维度数）。'
- en: '**K** is a matrix representing a key. Its shape is [*L*[k], *d*[k]], where
    *L*[k] is the length of the key and *d*[k] is the key’s dimensionality. Note that
    *d*[k] must equal *d*[q].'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K** 是表示密钥的矩阵。其形状为 [*L*[k], *d*[k]]，其中 *L*[k] 是密钥的长度，*d*[k] 是密钥的维度性。请注意，*d*[k]
    必须等于 *d*[q]。'
- en: '**V** is a matrix representing a value. Its shape is [*L*[v], *d*[v]], where
    *L*[v] is the length of the value and *d*[v] is the value’s dimensionality. Note
    that *L*[v] must equal *L*[k].'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**V** 是表示值的矩阵。其形状为 [*L*[v], *d*[v]]，其中 *L*[v] 是值的长度，*d*[v] 是值的维度性。请注意，*L*[v]
    必须等于 *L*[k]。'
- en: 'The shape of **Q** **K**^⊺ is [*L*[q], *L*[k]]: it contains one similarity
    score for each query/key pair. To prevent this matrix from being huge, the input
    sequences must not be too long: this is the critical *quadratic context window*
    problem (we will discuss various ways to alleviate this issue in Chapters [16](ch16.html#vit_chapter)
    and [17](ch17.html#speedup_chapter). The softmax function is applied to each row:
    the output has the same shape as the input, but now each row sums up to 1\. The
    final output has a shape of [*L*[q], *d*[v]]. There is one row per query token,
    and each row represents the query result: a weighted sum of the value tokens,
    favoring value tokens whose corresponding key tokens are most aligned with the
    given query token.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q** **K**^⊺ 的形状为 [*L*[q], *L*[k]]：它包含每个查询/键对的相似度分数。为了防止这个矩阵变得太大，输入序列不能太长：这是关键的**二次上下文窗口**问题（我们将在第
    [16](ch16.html#vit_chapter) 章和第 [17](ch17.html#speedup_chapter) 章中讨论缓解此问题的各种方法）。softmax
    函数应用于每一行：输出具有与输入相同的形状，但现在每一行的总和为 1。最终输出具有 [*L*[q], *d*[v]] 的形状。每一行对应一个查询标记，每一行代表查询结果：值标记的加权和，优先考虑与给定查询标记最对齐的键标记的值标记。'
- en: The scaling factor 1 / <msqrt><msub><mi>d</mi> <mrow><mi>k</mi></mrow></msub></msqrt>
    scales down the similarity scores to avoid saturating the softmax function, which
    would lead to tiny gradients. This factor was empirically shown to speed up and
    stabilize training.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放因子 1 / <msqrt><msub><mi>d</mi> <mrow><mi>k</mi></mrow></msub></msqrt> 将相似度分数缩小，以避免饱和
    softmax 函数，这会导致梯度非常小。这个因子在经验上被证明可以加速和稳定训练。
- en: It is possible to mask out some key/value pairs by adding a very large negative
    value to the corresponding similarity scores, just before computing the softmax
    (in practice, we can add `–torch.inf`). The resulting weights will be equal to
    zero. This is useful to mask padding tokens, as well as future tokens in the masked
    multi-head attention layer.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算 softmax 之前，可以通过向相应的相似度分数添加一个非常大的负值来屏蔽一些键/值对，这有助于屏蔽填充标记以及被屏蔽的多头注意力层中的后续标记。在实践中，我们可以添加
    `–torch.inf`。结果权重将等于零。
- en: PyTorch comes with the `F.scaled_dot_product_attention()` function. Its inputs
    are just like **Q**, **K**, and **V**, but these inputs can have extra dimensions
    at the start, such as the batch size and the number of heads (when used for multi-head
    attention). The equation is applied simultaneously across all of these extra dimensions.
    In other words, the function computes the results simultaneously across all sentences
    in the batch and across all attention heads, making it very efficient.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch自带`F.scaled_dot_product_attention()`函数。其输入与**Q**、**K**和**V**相同，但这些输入可以在开头有额外的维度，例如批大小和头的数量（当用于多头注意力时）。该方程会在所有这些额外维度上同时应用。换句话说，该函数会在批中所有句子和所有注意力头之间同时计算结果，这使得它非常高效。
- en: Now we’re ready to look at the multi-head attention layer. Its architecture
    is shown in [Figure 15-4](#multihead_attention_diagram).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备查看多头注意力层。其架构在[图15-4](#multihead_attention_diagram)中展示。
- en: As you can see, it is just a bunch of scaled dot-product attention layers, called
    *attention heads*, each preceded by a linear transformation of the values, keys,
    and queries (across all tokens). The outputs of all the attention heads are simply
    concatenated, and they go through a final linear transformation (again, across
    all tokens).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，它只是一系列缩放点积注意力层，称为*注意力头*，每个注意力头之前都有一个值、键和查询的线性变换（跨所有标记）。所有注意力头的输出简单地连接起来，然后通过最终的线性变换（再次，跨所有标记）。
- en: But why? What is the intuition behind this architecture? Well, consider once
    again the word “like” in the sentence “I like soccer”. The encoder was hopefully
    smart enough to encode its meaning, the fact that it’s a verb, and many other
    features that are useful for its translation, such as the fact that it is in the
    present tense. The token representation also includes the position, thanks to
    the positional encodings. In short, the token representation encodes many different
    characteristics of the token. If we just used a single scaled dot-product attention
    layer, we would only be able to query all of these characteristics in one shot.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么？这个架构背后的直觉是什么？好吧，再次考虑句子“I like soccer”中的单词“like”。编码器可能足够聪明，能够编码其含义，即它是一个动词，以及许多对翻译有用的其他特征，例如它是在现在时态。标记表示还包含了位置信息，多亏了位置编码。简而言之，标记表示编码了标记的许多不同特征。如果我们只使用单个缩放点积注意力层，我们只能一次性查询所有这些特征。
- en: '![Diagram illustrating the architecture of a multi-head attention layer, highlighting
    the flow from linear transformations of values, keys, and queries through split
    layers, scaled dot-product attention, concatenation, and final linear transformation.](assets/hmls_1504.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![展示多头注意力层架构的图解，突出显示值、键和查询的线性变换、缩放点积注意力、连接和最终线性变换的流程。](assets/hmls_1504.png)'
- en: Figure 15-4\. Multi-head attention layer architecture⁠^([6](ch15.html#id3452))
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-4\. 多头注意力层架构⁠^([6](ch15.html#id3452))
- en: Warning
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'The Transformer architecture is extremely flexible, so the model has plenty
    of freedom during training to choose its own knowledge representation and strategies.
    As a result, it ends up being somewhat of a black box: understanding how transformers
    truly “think” is an area of active research, called *model interpretability*.
    For example, check out this [fascinating post by Anthropic](https://homl.info/tracing-thoughts).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构极其灵活，因此在训练过程中模型有充分的自由度来选择自己的知识表示和策略。因此，它最终变成了一种黑盒：理解变换器真正“思考”的方式是一个活跃的研究领域，称为*模型可解释性*。例如，查看Anthropic的这篇[迷人的帖子](https://homl.info/tracing-thoughts)。
- en: 'This is why the MHA layer splits the values, keys, and queries across multiple
    heads: this way, each head can focus on specific characteristics of the token.
    The first linear layer lets the model choose which characteristics each head should
    focus on. For example, the linear layer may ensure that the first head gets a
    projection of the “like” token’s representation into a subspace where all that
    remains is the information that this token is a verb in the present tense. Another
    head may focus on the word’s meaning, and so on. Then the scaled dot-product attention
    layers implement the actual lookup phase, and finally the results are all concatenated
    and run through a final linear layer that lets the model reorganize the representation
    as it pleases.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么MHA层将值、键和查询分割到多个头中：这样，每个头可以专注于标记的特定特征。第一层线性层让模型选择每个头应该关注哪些特征。例如，线性层可能确保第一个头将“like”标记的表示投影到只包含该标记是现在时态动词的信息的子空间中。另一个头可能专注于单词的意义，等等。然后缩放点积注意力层实现实际的查找阶段，最后将所有结果连接起来，通过一个最终的线性层，让模型按照自己的意愿重新组织表示。
- en: 'To really understand the Transformer architecture, the key is to understand
    multi-head attention, and for this, it helps to look at a basic implementation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要真正理解Transformer架构，关键是理解多头注意力，为此，查看基本实现是有帮助的：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s go through this code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这段代码：
- en: The constructor stores the number of heads `self.h` and computes the number
    of dimensions per head `self.d`, then it creates the necessary modules. Note that
    the embedding size must be divisible by the number of heads.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数存储头数`self.h`并计算每个头的维度数`self.d`，然后创建必要的模块。注意，嵌入大小必须能被头数整除。
- en: 'The `split_heads()` method is used in the `forward()` method. It splits its
    input `X` along its last dimension (one split per head), converting it from a
    3D tensor of shape [*B*, *L*, *h* × *d*] to a 4D tensor of shape [*B*, *L*, *h*,
    *d*], where *B* is the batch size, *L* is the max length of the input sequences
    (specifically *L*[k] for the key and value, or *L*[q] for the query), *h* is the
    number of heads, and *d* is the number of dimensions per head (i.e., *h* × *d*
    = embedding size). The dimensions 1 and 2 are then swapped to get a tensor of
    shape [*B*, *h*, *L*, *d*]: since the matrix multiplication operator `@` only
    works on the last two dimensions, it won’t touch the first two dimensions *B*
    and *h*, so we will be able to use this operator to compute the scores across
    all instances in the batch and across all attention heads, all in one shot (`q
    @ k.transpose(2, 3)`). The same will be true when computing all the attention
    outputs (`weights @ v`).'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_heads()`方法在`forward()`方法中使用。它沿着其最后一个维度（每个头一个分割）分割其输入`X`，将其从形状为[*B*,
    *L*, *h* × *d*]的3D张量转换为形状为[*B*, *L*, *h*, *d*]的4D张量，其中*B*是批大小，*L*是输入序列的最大长度（对于键和值是*L*[k]，对于查询是*L*[q]），*h*是头的数量，*d*是每个头的维度数（即*h*
    × *d* = 嵌入大小）。然后交换维度1和2以获得形状为[*B*, *h*, *L*, *d*]的张量：由于矩阵乘法运算符`@`只作用于最后两个维度，它不会触及前两个维度*B*和*h*，因此我们可以使用此运算符一次性计算批中所有实例和所有注意力头之间的分数（`q
    @ k.transpose(2, 3)`）。在计算所有注意力输出时（`weights @ v`）也将如此。'
- en: 'The `forward()` method starts by applying a linear transformation to the query,
    key, and value, and passes the result through the `split_heads()` method. The
    next three lines compute [Equation 15-1](#scaled_dot_product_attention), plus
    a bit of dropout on the weights. Next we swap back dimensions 1 and 2 to ensure
    that the dimensions *h* and *d* are next to each other again, then we reshape
    the tensor back to 3D: this will concatenate the outputs of all heads. We can
    then apply the output linear transformation and return the result, along with
    the weights (in case we need them later).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward()`方法首先对查询、键和值应用线性变换，并将结果通过`split_heads()`方法传递。接下来的三行计算[方程15-1](#scaled_dot_product_attention)，并在权重上添加一些dropout。然后我们将维度1和2交换回来，以确保维度*h*和*d*再次相邻，然后将张量重新塑形回3D：这将连接所有头的输出。然后我们可以应用输出线性变换并返回结果，以及权重（以防以后需要）。'
- en: Tip
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Don’t worry if it takes some time to fully grasp this, it’s not easy. Of course,
    you can drive a car without fully understanding how the engine works, but some
    of the transformer improvements described in Chapters [16](ch16.html#vit_chapter)
    and [17](ch17.html#speedup_chapter) will only make sense if you understand MHA.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要一些时间才能完全掌握这一点，请不要担心，这并不容易。当然，您可以在不完全理解引擎工作原理的情况下驾驶汽车，但第 [16](ch16.html#vit_chapter)
    章和 [17](ch17.html#speedup_chapter) 中描述的一些 Transformer 改进只有在您理解 MHA 的情况下才有意义。
- en: 'But wait! We’re missing one important detail: masking. Indeed, as we discussed
    earlier, the decoder’s masked self-attention layers must only consider previous
    tokens when trying to predict what the next token is (or else it would be cheating).
    Moreover, if the key contains padding tokens, we want to ignore them as well.
    So let’s update the `forward()` method to support two additional arguments:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 但等等！我们遗漏了一个重要的细节：掩码。确实，正如我们之前讨论的，解码器的掩码自注意力层在尝试预测下一个标记时必须只考虑之前的标记（否则就是作弊）。此外，如果键包含填充标记，我们还想忽略它们。因此，让我们更新
    `forward()` 方法以支持两个额外的参数：
- en: '`attn_mask`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`attn_mask`'
- en: A boolean mask of shape [*L*[q], *L*[k]] that we will use to control which key
    tokens each query token should ignore (`True` to ignore, `False` to attend)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一个形状为 [*L*[q], *L*[k]] 的布尔掩码，我们将使用它来控制每个查询标记应该忽略哪些键标记（`True` 表示忽略，`False` 表示关注）
- en: '`key_padding_mask`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`key_padding_mask`'
- en: A boolean mask of shape [*B*, *L*[k]] to locate the padding tokens in each key
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一个形状为 [*B*, *L*[k]] 的布尔掩码，用于定位每个键中的填充标记
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This code replaces the scores we want to ignore with negative infinity, so
    the corresponding weights will be zero after the softmax operation (if we tried
    to zero out these weights directly, the remaining weights would not add up to
    1). Note that the masks are broadcast automatically: `attn_mask` is broadcast
    across the whole batch and all attention heads, and `key_padding_mask` is broadcast
    across all heads and all query tokens.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将我们想要忽略的分数替换为负无穷大，因此在 softmax 操作后相应的权重将为零（如果我们直接尝试将这些权重置零，剩余的权重将不会加起来等于 1）。请注意，掩码会自动广播：`attn_mask`
    在整个批次和所有注意力头之间广播，而 `key_padding_mask` 在所有头和所有查询标记之间广播。
- en: 'PyTorch has a very similar `nn.MultiheadAttention` module, which is much more
    optimized (e.g., it can often fuse the three input projections into one). It has
    the same arguments, which behave in exactly the same way. It also has a few more.
    Here are the most important:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 有一个非常相似的 `nn.MultiheadAttention` 模块，它进行了更多优化（例如，它通常可以将三个输入投影融合成一个）。它具有相同的参数，它们的行为方式完全相同。它还有一些额外的参数。以下是最重要的：
- en: The constructor has a `batch_first` argument which defaults to `False`, so the
    module expects the batch dimension to come after the sequence length dimension.
    You must set `batch_first=True` if you prefer the batch dimension to come first,
    like in our custom implementation.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数有一个 `batch_first` 参数，默认为 `False`，因此模块期望批次维度在序列长度维度之后。如果您希望批次维度先于序列长度维度，例如在我们的自定义实现中，您必须设置
    `batch_first=True`。
- en: The `forward()` method has a `need_weights` argument that defaults to `True`.
    If you don’t need to use the weights returned by this module, you should set this
    argument to `False`, as it sometimes allows for some optimizations. When `need_weights`
    is set to `False`, the method returns `None` instead of the weights.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward()` 方法有一个 `need_weights` 参数，默认为 `True`。如果您不需要使用此模块返回的权重，您应该将此参数设置为
    `False`，因为这有时允许进行一些优化。当 `need_weights` 设置为 `False` 时，该方法返回 `None` 而不是权重。'
- en: 'The `forward()` method also has an `is_causal` argument: if (and only if) the
    `attn_mask` is set and is a *causal mask*, then you can set `is_causal=True` to
    allow for some performance optimizations. A causal mask allows each query token
    to attend to all previous tokens (including itself), but doesn’t allow it to attend
    to tokens located after it. In other words, a causal mask contains `True` above
    the main diagonal, and `False` everywhere else. This is the mask needed for the
    masked self-attention layers.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward()` 方法还有一个 `is_causal` 参数：如果（并且仅当）`attn_mask` 被设置并且是一个 *causal mask*，那么您可以设置
    `is_causal=True` 以允许一些性能优化。因果掩码允许每个查询标记关注所有之前的标记（包括自身），但不允许它关注位于其后的标记。换句话说，因果掩码在主对角线上方包含
    `True`，在其他地方包含 `False`。这是掩码自注意力层所需的掩码。'
- en: Now that we have the main ingredient, we’re ready to implement the rest of the
    Transformer model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了主要成分，我们准备实现 Transformer 模型的其余部分。
- en: Building the Rest of the Transformer
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建 Transformer 的其余部分
- en: 'The rest of the Transformer architecture is much more straightforward. Let’s
    start with the encoder block. The following implementation closely matches the
    encoder block represented on the left side of [Figure 15-3](#transformer_diagram),
    except it sprinkles a bit of dropout after the self-attention layer and after
    both dense layers in the feedforward module:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构的其余部分要简单得多。让我们从编码器块开始。以下实现与 [图 15-3](#transformer_diagram) 左侧表示的编码器块非常相似，除了在自注意力层和前馈模块中的两个密集层之后都添加了一些
    dropout：
- en: '[PRE3]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that the feedforward block is composed of a first `Linear` layer that
    expands the dimensionality to 2048 (by default), followed by a nonlinearity (ReLU
    in this case), then a second `Linear` layer that projects the data back down to
    the original embedding size (also called the *model dimension*, `d_model`). This
    *reverse bottleneck* increases the expressive power of the nonlinearity, allowing
    the model to learn much richer combinations of features. This idea was explored
    further in the later MobileNetv2 paper, whose authors coined the term *inverted
    residual network*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，前馈块由一个首先将维度扩展到 2048（默认值）的 `Linear` 层组成，然后是一个非线性（在这种情况下是 ReLU），然后是一个将数据投影回原始嵌入大小的第二个
    `Linear` 层（也称为 *模型维度*，`d_model`）。这个 *反向瓶颈* 增加了非线性函数的表达能力，允许模型学习更丰富的特征组合。这个想法在后来的
    MobileNetv2 论文中得到了进一步探索，其作者提出了 *逆残差网络* 这个术语。
- en: In the encoder, the `src_mask` argument is generally not used, since the encoder
    allows each token to attend to all tokens, even ones located after it. However,
    the user is expected to set the `key_padding_mask` appropriately.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器中，`src_mask` 参数通常不使用，因为编码器允许每个标记关注所有标记，即使是在它之后的位置。然而，用户应适当地设置 `key_padding_mask`。
- en: 'Now here’s an implementation of the decoder block. It closely matches the decoder
    block represented on the righthand side of [Figure 15-3](#transformer_diagram),
    with some additional dropout:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是一个解码器块的实现。它与 [图 15-3](#transformer_diagram) 右侧表示的解码器块非常相似，增加了一些额外的 dropout：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `memory` argument corresponds to the output of the encoder. For full flexibility,
    we let the user pass the appropriate masks to the `forward()` method. In general,
    you will need to set the padding masks appropriately (both for the memory and
    target), and set the `tgt_mask` to a causal mask (we will see how shortly).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`memory` 参数对应于编码器的输出。为了提供最大的灵活性，我们允许用户将适当的掩码传递给 `forward()` 方法。通常，您需要适当地设置填充掩码（对于记忆和目标），并将
    `tgt_mask` 设置为因果掩码（我们很快就会看到）。'
- en: 'PyTorch actually provides `nn.TransformerEncoderLayer` and `nn.TransformerDecoderLayer`
    out of the box, with the same arguments, plus a few more: most importantly `batch_first`,
    which you must set to `True` if the batch dimension is first, plus one `*_is_causal`
    argument for each attention mask, and an `activation` argument that defaults to
    “relu”. Many state-of-the-art transformers use a more advanced activation such
    as GELU (introduced in [Chapter 11](ch11.html#deep_chapter)).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 实际上提供了 `nn.TransformerEncoderLayer` 和 `nn.TransformerDecoderLayer`，直接使用相同的参数，还有一些额外的参数：最重要的是
    `batch_first`，如果你将批维度设置为第一维，则必须将其设置为 `True`，以及每个注意力掩码的一个 `*_is_causal` 参数，还有一个默认为“relu”的
    `activation` 参数。许多最先进的转换器使用更先进的激活函数，如 GELU（在第 11 章中介绍）。
- en: 'PyTorch also provides three more transformer modules (writing a custom module
    for each of these is left as an exercise for the reader—see the notebook for a
    solution):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 还提供了三个额外的转换器模块（为这些每个编写自定义模块留给读者作为练习——参见笔记本以获取解决方案）：
- en: '`nn.TransformerEncoder`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.TransformerEncoder`'
- en: Simply chains the desired number of encoder layers. Its constructor takes an
    encoder layer plus the desired number of layers `num_layers`, and it clones the
    given encoder layer `num_layers` times. The constructor also takes an optional
    normalization layer, which (if provided) is applied to the final output.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地链接所需数量的编码器层。其构造函数接受一个编码器层以及所需的层数 `num_layers`，并将给定的编码器层复制 `num_layers` 次。构造函数还接受一个可选的归一化层，如果提供，则应用于最终输出。
- en: '`nn.TransformerDecoder`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.TransformerDecoder`'
- en: Same, except it chains decoder layers instead of encoder layers.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相同，只是它链接解码器层而不是编码器层。
- en: '`nn.Transformer`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.Transformer`'
- en: Creates an encoder and a decoder (both with layer norm), and chains them.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个编码器和解码器（两者都带有层归一化），并将它们链接起来。
- en: Congratulations! You now know how to build a full Transformer model from scratch.
    You only need to add a final `Linear` layer and use the `nn.CrossEntropyLoss`
    to get the full architecture shown in [Figure 15-3](#transformer_diagram) (as
    we saw in earlier chapters, the softmax layer is implicitly included in the loss).
    Now let’s see how to use a Transformer model to translate English to Spanish.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你现在知道如何从头开始构建一个完整的Transformer模型。你只需要添加一个最终的`Linear`层，并使用`nn.CrossEntropyLoss`来获得如图15-3所示的全架构（正如我们在前面的章节中看到的，softmax层隐含地包含在损失中）。现在让我们看看如何使用Transformer模型将英语翻译成西班牙语。
- en: Building an English-to-Spanish Transformer
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建英语到西班牙语的Transformer
- en: 'It’s time to build our NMT Transformer model. For this, we’ll use our `Positional​Embedding`
    module and PyTorch’s `nn.Transformer` (our custom `Transformer` module works fine,
    but it’s slower):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是构建我们的NMT Transformer模型的时候了。为此，我们将使用我们的`PositionalEmbedding`模块和PyTorch的`nn.Transformer`（我们的自定义`Transformer`模块也可以正常工作，但速度较慢）：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s go through this code:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行分析这段代码：
- en: 'The constructor is straightforward: we just create the necessary modules.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数很简单：我们只需创建必要的模块。
- en: The `forward()` method takes an `NmtPair` as input (this class was defined in
    [Chapter 14](ch14.html#nlp_chapter)). The method starts by embedding the input
    tokens for both the source and target inputs, and it adds the positional encodings
    to both.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward()`方法接受一个`NmtPair`作为输入（这个类在第14章中定义过）。方法首先对源输入和目标输入的输入标记进行嵌入，并给它们都添加位置编码。'
- en: Then the code uses the *not* operator (`~`) to invert both the source and target
    masks because they contain `False` for each padding token, but `nn.Multihead​Attention`
    expects `True` for tokens that it should ignore.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后代码使用`not`运算符（`~`）反转源和目标掩码，因为它们对每个填充标记包含`False`，但`nn.MultiheadAttention`期望应该忽略的标记为`True`。
- en: 'Next, we create a square matrix of shape [*L*[q], *L*[q]], full of `True`,
    and we get all elements above the main diagonal using the `torch.triu()` function,
    with the rest defaulting to `False`. This results in a causal mask that we can
    use as the `tgt_mask` for the transformer: it will use this mask for the masked
    self-attention layer. Alternatively, you could call `nn.Transformer.gen⁠erate_​square_subsequent_mask()`
    to create the causal mask: just pass it the sequence length (`pair.tgt_token_ids.size(1)`)
    and set `dtype=torch.bool`.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个形状为[*L*[q]，*L*[q]]的正方形矩阵，填充`True`，然后使用`torch.triu()`函数获取主对角线以上的所有元素，其余默认为`False`。这产生了一个因果掩码，我们可以将其用作Transformer的`tgt_mask`：它将使用此掩码进行掩码自注意力层。或者，你也可以调用`nn.Transformer.generate_square_subsequent_mask()`来创建因果掩码：只需传递序列长度（`pair.tgt_token_ids.size(1)`）并设置`dtype=torch.bool`。
- en: We then call the transformer, passing it the source and target embeddings, as
    well as all the appropriate masks.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们调用transformer，传递给它源和目标嵌入以及所有适当的掩码。
- en: Lastly, we pass the result through the output `Linear` layer, and we permute
    the last two dimensions because `nn.CrossEntropyLoss` expects the class dimension
    to be dimension 1.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将结果通过输出`Linear`层传递，并交换最后两个维度，因为`nn.CrossEntropyLoss`期望类维度是维度1。
- en: 'We can now create an instance of this model and train it exactly like our RNN
    encoder-decoder in [Chapter 14](ch14.html#nlp_chapter). To speed up training and
    reduce overfitting, you can shrink the transformer quite a bit—use 4 heads instead
    of 8, just 2 layers in both the encoder and the decoder, and use an embedding
    size of 128:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建这个模型的实例并像第14章中的RNN编码器-解码器一样训练它。为了加快训练速度并减少过拟合，你可以大幅度缩小Transformer的大小——使用4个头而不是8个，编码器和解码器中各只有2层，并且使用128的嵌入大小：
- en: '[PRE6]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s see how well this model performs:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个模型的表现如何：
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]  [PRE9][PRE10][PRE11][PRE12]` [PRE13][PRE14][PRE15][PRE16] # Encoder-Only
    Transformers for Natural Language Understanding    When Google released the [BERT
    model in 2018](https://homl.info/bert),⁠^([7](ch15.html#id3475)) it proved that
    an encoder-only transformer can tackle a wide variety of natural language tasks:
    sentence classification, token classification, multiple choice question answering,
    and more! BERT also confirmed the effectiveness of self-supervised pretraining
    on a large corpus for transfer learning: BERT can indeed achieve excellent performance
    on many tasks, just by fine-tuning on a fairly small dataset for each task. Let’s
    start by looking at BERT’s architecture, then we’ll look at how it was pretrained,
    and how you can fine-tune it for your own tasks.    ###### Warning    Encoder-only
    models are generally not used for text generation tasks, such as autocompletion,
    translation, summarization, or chatbots, because they’re much slower at this task
    than decoders. Decoders are faster because they are causal, so a good implementation
    can cache and reuse its previous state when predicting a new token. Conversely,
    encoders use nonmasked multi-head attention layers only, so they are naturally
    bidirectional; hence the B in BERT (Bidirectional Encoder Representations from
    Transformers). Whenever a new token is added, everything needs to be recomputed.    ##
    BERT’s Architecture    BERT’s architecture is almost identical to the original
    Transformer’s encoder, with just three differences:    1.  It’s much bigger. BERT-base
    has 12 encoder blocks, 12 attention heads, and 768-dimensional embeddings, and
    BERT-large has 24 blocks, 16 heads, and 1,024 dimensions (while the original Transformer
    has 6 blocks, 8 heads, and 512 dimensions). It also uses trainable positional
    embeddings and supports input sentences up to 512 tokens.           2.  It applies
    layer-norm just *before* each sublayer (attention or feedforward) rather than
    *after* each skip connection. This is called *pre-LN*, as opposed to *post-LN*,
    and it ensures that the inputs of each sublayer are normalized, which stabilizes
    training and reduces sensitivity to weight initialization. PyTorch’s transformer
    modules default to post-LN, but they have a `norm_first` argument which you can
    set to `True` if you prefer pre-LN (however, some optimizations may not be implemented
    for pre-LN).           3.  It lets you split the input sentence into two *segments*
    if needed. This is useful for tasks that require a pair of input sentences, such
    as natural language inference (i.e., does sentence A entail sentence B?) or multiple
    choice question answering (i.e., given question A, how good is answer B?). To
    pass two sentences to BERT, you must first append a *separation token* [SEP] to
    each one, then concatenate them. Furthermore, a trainable *segment embedding*
    is added to each token’s representation: segment embedding #0 is added to all
    tokens within segment #0, and segment embedding #1 is added to all tokens within
    segment #1\. In theory, we could have more segments, but BERT was only pretrained
    on inputs composed of one or two segments. Note that the positional encodings
    are also added to each token’s representation, as usual (i.e., relative to the
    full input sequence, not relative to the individual segments).              That’s
    all! Now let’s look at how BERT was pretrained.    ## BERT Pretraining    The
    authors proposed two self-supervised pretraining tasks:    Masked language model
    (MLM)      Each token in a sentence has a 15% probability of being replaced with
    a mask token, and the model is trained to predict what the original tokens were.
    This is often called a *cloze task* (i.e., fill in the blanks). For example, if
    the original sentence is “She had fun at the birthday party”, then the model may
    be given the sentence “She [MASK] fun at the [MASK] party” and it must predict
    the original sentence: the loss is only computed on the mask token outputs.    To
    be more precise, some of the masked tokens are not truly masked: 10% are instead
    replaced by random tokens, and 10% are just left alone, neither masked nor randomized.
    Why is that? Well, the random tokens force the model to perform well even when
    mask tokens are absent: this is important since most downstream tasks don’t use
    any mask tokens. As for the untouched tokens, they make the prediction trivial,
    which encourages the model to pay attention to the input token located at the
    position of the token being predicted. Without them, the model would soon learn
    to ignore this token and rely solely on the other tokens.      Next sentence prediction
    (NSP)      The model is trained to predict whether two sentences are consecutive
    or not. For example, it should predict that “The dog sleeps” and “It snores loudly”
    are consecutive sentences, while “The dog sleeps” and “The Earth orbits the Sun”
    are not consecutive.    This is a binary classification task, which the authors
    chose to implement by introducing a new *class token* [CLS]: this token is inserted
    at the beginning of the input sequence (position #0, segment #0), and during training
    the encoder’s output, this token is passed through a binary classification head
    (i.e., a linear layer with a single unit, followed by the sigmoid function, and
    trained using `nn.BCELoss`, or just a linear layer with a single unit trained
    using `nn.BCEWith​Lo⁠gitsLoss`).      BERT was pretrained on both MLM and NSP
    simultaneously (see [Figure 15-5](#bert_pretraining_diagram) and the left side
    of [Figure 15-6](#bert_diagram)), using a large corpus of text—specifically the
    English Wikipedia and BooksCorpus. The goal of NSP was to make the class token’s
    contextualized embedding a good representation of the whole input sequence. At
    first, it seemed that it indeed produced good sentence embeddings, but it was
    later shown that simply pooling all the contextualized embeddings (e.g., by computing
    their mean) yielded better results. In fact, researchers showed that NSP did not
    help much overall, so it was dropped in most later architectures.    In [Chapter 14](ch14.html#nlp_chapter),
    we saw how to use the Transformers library to download a pretrained BERT model
    and its tokenizer. But you may want to train a BERT model from scratch, for example,
    if you’re dealing with a domain-specific corpus of text. For this, one option
    is to build BERT yourself using the `nn.TransformerEncoder` module (e.g., based
    on an `nn.TransformerEncoderLayer` with `norm_first=True` to respect BERT’s architecture),
    then preprocess your dataset according to the MLM algorithm, and train your model.  ![Diagram
    illustrating BERT pretraining showing input and target sequences for MLM and NSP
    tasks, highlighting masked, random, and unchanged tokens.](assets/hmls_1505.png)  ######
    Figure 15-5\. Input and target during BERT pretraining, using MLM and NSP    However,
    there’s an easier way, using the Transformers library. Let’s start by creating
    a tokenizer and a randomly initialized BERT model. For simplicity, we use a pretrained
    tokenizer, but of course you can train one from scratch instead, if you prefer.
    Make sure to tweak the `BertConfig` depending on your training budget, and the
    size and complexity of your dataset:    [PRE17]    Next, let’s download the WikiText
    dataset (in real life, you would use your own dataset instead), and tokenize it:    [PRE18]    This
    is where MLM comes in. We create a data collator, whose role is to bundle samples
    into batches, and we set its `mlm` argument to `True` to activate MLM, and also
    set `mlm_probability=0.15`: each token has a 15% probability of being masked (or
    possibly randomized or left alone, as we just discussed). We also pass the tokenizer
    to the collator: it will not be used to tokenize the text—we’ve already done that—but
    it lets the data collator know the masking and padding token IDs, as well as the
    vocabulary size (which is needed to sample random token IDs). With that, we just
    need to specify the `TrainingArguments`, pass everything to the `Trainer`, and
    call its `train()` method:    [PRE19]    Once your model is pretrained, you can
    try it out using the pipelines API:    [PRE20]`` `>>>` `fill_mask` `=` `pipeline``(``"fill-mask"``,`
    `model``=``bert``,` `tokenizer``=``bert_tokenizer``)` [PRE21] `>>>` `top_predictions``[``0``]`
    `` `{''score'': 0.04916289076209068,`  `''token'': 1010,`  `''token_str'': '','',`  `''sequence'':
    ''the capital of, is rome.''}` `` [PRE22]` [PRE23]   [PRE24]  [PRE25] [PRE26]`py  [PRE27]`py[PRE28]py
    [PRE29]`py[PRE30]py[PRE31]py` # Decoder-Only Transformers    While Google was
    working on the first encoder-only model (i.e., BERT), Alec Radford and other OpenAI
    researchers were taking a different route: they built the first decoder-only model,
    named GPT.⁠^([11](ch15.html#id3567)) This model paved the way for today’s most
    impressive models, including most of the ones used in famous chatbots like ChatGPT
    or Claude.    The GPT model (now known as GPT-1) was released in June 2018\. GPT
    stands for *Generative Pre-Training*: it was pretrained on a dataset of about
    7,000 books and learned to predict the next token, so it can be used to generate
    text one token at a time, just like the original Transformer’s decoder. For example,
    if you feed it “Happy birthday”, it will predict “birthday to”, so you can append
    “to” to the input and repeat the process (see [Figure 15-12](#gpt_generation_diagram)).  ![Diagram
    illustrating how a decoder-only model, like GPT, generates text one token at a
    time by predicting the next word in the sequence, such as continuing "Happy birthday"
    with "to you!".](assets/hmls_1512.png)  ###### Figure 15-12\. Generating text
    one token at a time using a decoder-only model like GPT    Decoder-only models
    are great at *text generation* tasks, such as auto-completion, code generation,
    question answering (including free text answers), math and logical reasoning (to
    some extent), and chatbots. They can also be used for summarization or translation,
    but encoder-decoder models are still popular choices for these tasks, as they
    often have a better understanding of the source text, thanks to the encoder. Decoder-only
    models can also perform text classification quite well, but encoder-only models
    shine in this area, as they are faster and often provide a similar performance
    with a smaller model.    ###### Warning    At inference time, encoder-only models
    only need to look at their inputs once to make their predictions, while decoder-only
    models require one run per generated token (just like the decoder in encoder-decoder
    models). That’s because decoders are autoregressive, so the generation process
    is sequential. That said, decoders can hugely benefit from caching, as I mentioned
    earlier.    In this section, we will look at the architecture of GPT-1 and its
    successor GPT-2, and we will see how decoder-only models like these can be used
    for various tasks. We will also see that these models can perform tasks that they
    were never explicitly trained on (zero-shot learning) or for which they only saw
    a few examples (few-shot learning). Lastly, we will then use the Transformers
    library to download a small decoder-only model (GPT-2) then a large one (Mistral-7B)
    and use them to generate text and answer questions.    ## GPT-1 Architecture and
    Generative Pretraining    During pretraining, GPT-1 was fed batches of 64 sequences
    randomly sampled from the book corpus, and it was trained to predict the next
    token for every single input token. Each sequence was exactly 512 tokens long,
    so GPT-1 did not need any padding token. In fact, it didn’t use special tokens
    at all during pretraining, not even start-of-sequence or end-of-sequence tokens.
    Compared to BERT, it’s a much simpler pretraining process. It also provides the
    same amount of data for every token position, whereas BERT sees less data for
    the last positions than for the first, due to padding.    GPT-1’s architecture
    has two important differences compared to the original Transformer’s decoder:    *   There’s
    no cross-attention block since there’s no encoder output to attend to: each decoder
    block only contains a masked multi-head attention layer and a two-layer feedforward
    network (each with its own skip connection and layer norm).           *   It’s
    much bigger: it has 12 decoder layers instead of 6, the embedding size is 768
    instead of 512, and it has 12 attention heads instead of 8\. That’s a total of
    117 million parameters.              ###### Warning    Counterintuitively, you
    cannot use PyTorch’s `nn.TransformerDecoder` module to build a decoder-only model.
    That’s because it contains cross-attention layers that cannot be easily removed.
    Instead, you can use the `nn.TransformerEncoder` module, and always call it with
    a causal mask.    Out-of-the-box, GPT-1 was very impressive at text generation.
    For example, its authors asked it to tell the story of a scientist discovering
    a herd of English-speaking unicorns in an unexplored valley, and the story it
    generated seemed like it had been written by a human (you can read it at [*https://homl.info/unicorns*](https://homl.info/unicorns)).
    It’s not quite as impressive today, but back then it was truly mind-blowing.    The
    authors also fine-tuned GPT-1 on various tasks, including textual entailment,
    semantic similarity, reading comprehension, or common sense reasoning, and it
    beat the state of the art on many of them, confirming the power of pretraining
    for NLP. For each task, the authors only made minor changes to the architecture:    *   For
    text classification tasks, a classification head is added on top of the last token’s
    output embedding. See the righthand side of [Figure 15-13](#gpt_training_diagram).           *   For
    entailment and other classification tasks requiring two input sentences, the model
    is fed both sentences separated by a delimiter token (just a regular $ sign),
    and again a classification head is added on top of the last token’s output embedding.           *   For
    semantic similarity, since the order of the two sentences shouldn’t matter, the
    model gets called twice: once with sentence 1 $ sentence 2, and once with sentence
    2 $ sentence 1\. The last token’s output embeddings for both cases are added itemwise
    and the result is fed to a regression head.           *   For multiple choice
    question answering, the approach is very similar to BERT’s: the model is called
    once per possible answer, with both the context (including the question) and the
    possible answer as input, separated by a $ sign, then the last token’s output
    embedding is passed through a linear layer to get a score. All the answer scores
    are then passed through a softmax layer.           *   In all cases they added
    a start-of-sequence token and an end-of-sequence token.            ![Diagram illustrating
    GPT-1 pretraining with next token prediction using softmax and fine-tuning for
    classification with a sigmoid function.](assets/hmls_1513.png)  ###### Figure
    15-13\. Pretraining GPT-1 using next token prediction (NTP, left) and fine-tuning
    it for classification (right)    ## GPT-2 and Zero-Shot Learning    Just a few
    months later, in February 2019, Alec Radford, Jeffrey Wu, and other OpenAI researchers
    published the GPT-2 paper,⁠^([12](ch15.html#id3586)) which proposed a very similar
    architecture to GPT-1,⁠^([13](ch15.html#id3587)) but larger still. It came in
    four sizes, and the largest model had 48 decoder layers, 20 attention heads, an
    embedding size of 1,600, and a context window of 1,024 tokens, for a total of
    over 1.5 billion parameters!    For such a large model, the authors needed a gigantic
    dataset, so they initially tried using Common Crawl which contains over two billion
    web pages. However, many of these pages are just gibberish (e.g., long tables
    of data). So the authors built a higher-quality dataset named *WebText*, composed
    of about eight million pages linked from highly ranked Reddit pages.    Most importantly,
    GPT-2 performed incredibly well on many tasks without any fine-tuning: this is
    called *zero-shot learning* (ZSL). For example:    *   For question answering,
    you can simply append “A:” to the question (e.g., “What is the capital of New-Zealand?
    A:”) then feed this prompt to GPT-2\. It will complete it with the answer (e.g.,
    “Wellington”).           *   For summarization, you can append “TL;DR:” to the
    document you want to summarize, and GPT-2 will often produce a decent summary.           *   For
    translation, you can create a prompt containing a few examples to guide the model,
    such as “Bonjour papa = Hello dad” and “Le chien dort = The dog is sleeping”,
    then append the text you want to translate, for example “Elle aime le chocolat
    =”, and GPT-2 will hopefully complete the prompt with the correct English translation:
    “She loves chocolate”.              Importantly, the authors showed that ZSL performance
    seemed to increase regularly with the model size: doubling the model size offered
    a roughly constant improvement (that’s a log-linear relationship). Maybe creating
    a superhuman AI was just a matter of training a large enough transformer?    ######
    Note    GPT-2’s performance was so impressive that OpenAI initially chose not
    to release the largest model. Officially, this was for the public’s safety, citing
    risks like automated disinformation and spam. But skeptics argued that it was
    both a publicity stunt and a shift toward closed-source AI, and perhaps even a
    move to influence future regulation. The full GPT-2 model was eventually released
    months later, but it was the last open one from OpenAI until August 2025, when
    a couple of open-weight models were released (GPT-OSS).    ## GPT-3, In-Context
    Learning, One-Shot Learning, and Few-Shot Learning    Following their bigger-is-better
    philosophy, OpenAI created [GPT-3](https://homl.info/gpt3) in 2020.⁠^([14](ch15.html#id3595))
    It had roughly 40 billion parameters, and was trained on a monstrously large dataset
    of about 570 gigabytes (including WebCrawl this time).    This model indeed was
    far better across the board than GPT-2\. In particular, it was much better at
    zero-shot tasks. But most importantly, the authors showed that GPT-3 was incredibly
    good at generalizing from just a few examples. This is called *few-shot learning*
    (FSL), or *one-shot learning* (OSL) if there’s a single example. To tackle FSL
    or OSL tasks, the authors simply inserted the example(s) in the prompt: they dubbed
    this *in-context learning* (ICL). For example, if you feed the following prompt
    to GPT-3, can you guess what it will output?    [PRE32]py    That’s right, it
    will output the missing word, “bat”. The idea of feeding the model some examples
    in the prompt itself was already present in the GPT-2 paper (remember the translation
    example?), but it wasn’t really formalized, and the GPT-3 paper explored it in
    much more depth.    ###### Note    In-context learning is an increasingly popular
    approach to one-shot learning and few-shot learning, but there are many others.
    ICL is new, but OSL and FSL are old (like ZSL).    Let’s download GPT-2 and generate
    some text with it (we will play with GPT-3 via the API later in this chapter).    ##
    Using GPT-2 to Generate Text    As you might expect, we can use the Transformers
    library to download GPT-2\. By default, we get the small version (124M parameters):    [PRE33]py    Let’s
    go through this code:    *   After the imports, we load GPT-2’s pretrained tokenizer
    and the model itself.           *   To load the model, we use `AutoModelForCausalLM.from_pretrained()`,
    which returns an instance of the appropriate class based on the checkpoint we
    ask for (in this case it returns a `GPT2LMHeadModel`). Since it’s a causal language
    model, it’s capable of generating text, as we will see shortly.           *   The
    `device_map="auto"` option tells the function to automatically place the model
    on the best available device, typically the GPU. If you have multiple GPUs and
    the model is too large for one, it may even be sharded across GPUs.           *   The
    `dtype="auto"` option asks the function to choose the most appropriate data type
    for the model weights, based on what’s available in the model checkpoint and your
    hardware. Typically, it loads the model using 16-bit floats if your hardware supports
    it (e.g., a modern GPU with mixed-precision support), or it falls back to 32-bit
    floats. Using half precision (16-bit) uses half the memory, which lets you load
    larger models, and it also gives the model a substantial speed boost because modern
    GPUs have hardware accelerations for this, and half precision reduces the amount
    of data that needs to be transferred between the CPU and GPU.              Now
    let’s write a little wrapper function around the model’s `generate()` method to
    make it very easy to generate text:    [PRE34]py    Our `generate()` function
    tokenizes the given prompt, transfers the resulting token IDs to the GPU, calls
    the given model’s `generate()` method to extend the prompt, adding up to 50 new
    tokens (by default) or less if it runs into an end-of-sequence token, and lastly
    it decodes the resulting token IDs to return a nice string containing the extended
    text. Since GPT-2 was pretrained without padding, we must specify which token
    we want to use for padding when calling the model’s `generate()` method: it’s
    common to use the end-of-sequence token for this. This function processes a single
    prompt so there will be no padding anyway, but specifying the padding token avoids
    a pesky warning. Our function also accepts optional extra keyword arguments (`**generate_kwargs`)
    and passes them on to the model’s `generate()` method. This will come handy very
    soon.    ###### Note    Decoder-only models often pad on the left side, for more
    efficient generation, since new tokens are added on the right.    Now let’s try
    generating some text about a talking unicorn:    [PRE35]py   [PRE36]py Hmm, it
    starts out pretty well, but then it just repeats itself—what’s happening? Well,
    by default the `generate()` method simply picks the most likely token at each
    step, which is fine when you expect very structured output, or for tasks such
    as question answering, but for creative writing it often gets the model stuck
    in a loop, producing repetitive and uninteresting text. To fix this, we can set
    `do_sample=True` to make the `generate()` method randomly sample each token based
    on the model’s estimated probabilities for the possible tokens, like we did with
    our Shakespeare model in [Chapter 14](ch14.html#nlp_chapter). Let’s see if this
    works:    [PRE37]py   [PRE38]`py [PRE39]py`` [PRE40]py ## Using GPT-2 for Question
    Answering    Let’s write a little function that takes a country name and asks
    GPT-2 to return its capital city:    [PRE41]py    The function starts by creating
    a prompt from a *prompt template*: it replaces the `{country}` placeholder with
    the given country name. Note that the prompt template includes one example of
    the task to help GPT-2 understand what to do and what format we expect: that’s
    in-context learning. The function then calls our `generate()` function to add
    10 tokens to the prompt: this is more than we need to write the capital city’s
    name. Lastly, we do a bit of post-processing by removing the initial prompt as
    well as anything after the first line, and we strip away any extra spaces at the
    end. Let’s try it out!    [PRE42]py   `` `It works beautifully! Moreover, it’s
    quite flexible with its input; for example, if you ask it for the capital of “UK”,
    “The UK”, “England”, “Great Britain”, or even “Big Britane”, it will still return
    “London”. That said, it’s far from perfect:    *   It makes many common mistakes
    (e.g., for Canada, it answers Toronto instead of Ottawa). Sadly, since GPT-2 was
    trained on many pages from the web, it picked up people’s misconceptions and biases.           *   When
    it’s not sure, it just repeats the country’s name, roughly 30% of the time. This
    might be because several countries have a capital city of the same name (e.g.,
    Djibouti, Luxembourg, Singapore) or close (e.g., Guatemala City, Kuwait City).           *   When
    the input is not a country, the model often answers “Paris”, since that’s the
    only example it had in its prompt.              One way to fix these issues is
    to simply use a much bigger and smarter model. For example, try using “gpt2-xl”
    (1.5B parameters) instead of “gpt2” when loading the model, then run the code
    again. It still won’t be perfect, but you should notice a clear improvement. So
    let’s see if an much larger model can do even better!` ``  [PRE43]`py [PRE44]py``  [PRE45]`py[PRE46]py[PRE47]py[PRE48]py[PRE49]py[PRE50]py[PRE51]`py[PRE52]py[PRE53][PRE54][PRE55]py
    class BobTheChatbot:  # or ChatBob if you prefer     def __init__(self, model,
    tokenizer, introduction=bob_introduction,                  max_answer_length=10_000):         self.model
    = model         self.tokenizer = tokenizer         self.context = introduction         self.max_answer_length
    = max_answer_length      def chat(self, prompt):         self.context += "\nMe:
    " + prompt + "\nBob:"         context = self.context         start_index = len(context)         while
    True:             extended = generate(self.model, self.tokenizer, context,                                 max_new_tokens=100)             answer
    = extended[start_index:]             if ("\nMe: " in answer or extended == context
    or                 len(answer) >= self.max_answer_length): break             context
    = extended         answer = answer.split("\nMe: ")[0]         self.context +=
    answer         return answer.strip() [PRE56]py >>> bob = BobTheChatbot(mistral7b,
    mistral7b_tokenizer) `>>>` `bob``.``chat``(``"List some places I should visit
    in Paris."``)` [PRE57][PRE58]`` [PRE59][PRE60] [PRE61][PRE62]``py[PRE63]`` The
    first estimated probability is for the token “The” (3.27%), then “capital” (0.02%),
    and so on. The second sequence starts with a padding token, so you can ignore
    the first probability (0.14%). The estimated probabilities are the same in both
    sequences for the prompt tokens,⁠^([26](ch15.html#id3667)) but they differ for
    the answer tokens: 11.38% for “Buenos”, versus 0.00% for “Madrid”. The model seems
    to know a bit of geography! You may have expected a higher probability for “Buenos”,
    but tokens like “a”, “one”, and “the” were also quite likely after “is”. However,
    once the model saw “Buenos”, it was almost certain that the next token was going
    to be “Aires” (99.61%), and of course it was correct.    Now if we add up the
    log probabilities of all answer tokens (e.g., for “Buenos” and “Aires”), we get
    the estimated log probability for the whole answer given the previous tokens,
    which is precisely what we were looking for (i.e., log *p*(**y** | **x**)). In
    this example, it corresponds to an estimated probability of 11.38%:    [PRE64]   [PRE65]``
    [PRE66]` However, having to find the exact location of the answer is cumbersome,
    especially when dealing with padded batches. Luckily, we can actually compute
    the DPO loss using the log probability of the full input **xy** (including both
    the prompt **x** and the answer **y**), rather than the log probability of the
    answer **y** given the prompt **x**. In other words, we can replace every log
    *p*(**y** | **x**) with log *p*(**xy**) in [Equation 15-2](#dpo_loss_equation)
    (for both *p*[**θ**] and *p*[ref], and for both **y**[c] and **y**[r]). This is
    because log *p*(**xy**) = log *p*(**x**) + log *p*(**y** | **x**), and the extra
    *p*(**x**) for the chosen answer cancels out exactly with the extra *p*(**x**)
    for the rejected answer. We only need to mask the padding tokens—we can use the
    attention mask for that—then simply add up all the log probabilities for each
    sequence:    [PRE67] `>>>` `log_probas_sum` `` `tensor([-21.2500, -30.2500], device=''cuda:0'',
    dtype=torch.bfloat16)` `` [PRE68]   [PRE69] [PRE70]`py [PRE71]py`` [PRE72]py[PRE73][PRE74][PRE75]``
    [PRE76][PRE77]``py[PRE78][PRE79] def preprocess(example):     text = f"Human:
    {example[''instruction'']}\n"     if example[''input''] != "":         text +=
    f"-> {example[''input'']}\n"     text += f"\nAssistant: {example[''output'']}"     return
    {"text": text}  sft_dataset = sft_dataset.map(preprocess) [PRE80] >>> print(sft_dataset[1]["text"])
    `Human: What are the three primary colors?`  `Assistant: The three primary colors
    are red, blue, and yellow.` [PRE81][PRE82] from trl import SFTTrainer, SFTConfig  sft_model_dir
    = "./my_gpt2_sft_alpaca" training_args = SFTConfig(     output_dir=sft_model_dir,
    max_length=512,     per_device_train_batch_size=4, num_train_epochs=1, save_steps=50,     logging_steps=10,
    learning_rate=5e-5) sft_trainer = SFTTrainer("gpt2", train_dataset=sft_dataset,
    args=training_args) sft_train_output = sft_trainer.train() sft_trainer.model.save_pretrained(sft_model_dir)
    [PRE83] >>> pref_dataset = load_dataset("Anthropic/hh-rlhf", split="train") `>>>`
    `pref_dataset``[``2``]``.``keys``()` [PRE84] `Human: If you were going to steal
    from a convenience store, do you think it` `would be better in the morning or
    at night?`  `Assistant: I really couldn’t say, I’m not familiar with stealing
    convenience` `store items.` `>>>` `print``(``pref_dataset``[``2``][``"rejected"``]``.``strip``())`
    `` `Human: If you were going to steal from a convenience store, do you think it`
    `would be better in the morning or at night?`  `Assistant: It is good to consider
    the difference in human traffic at night, and` `also the security cameras.  Night
    time would probably be better [...]` `` [PRE85]` [PRE86][PRE87]`` [PRE88] from
    trl import DPOConfig, DPOTrainer  dpo_model_dir = "./my_gpt2_sft_alpaca_dpo_hh_rlhf"
    training_args = DPOConfig(     output_dir=dpo_model_dir, max_length=512, per_device_train_batch_size=4,     num_train_epochs=1,
    save_steps=50, logging_steps=10, learning_rate=2e-5) gpt2_tokenizer.pad_token
    = gpt2_tokenizer.eos_token dpo_trainer = DPOTrainer(     sft_model_dir, args=training_args,
    train_dataset=pref_dataset,     processing_class=gpt2_tokenizer) dpo_train_output
    = dpo_trainer.train() dpo_trainer.model.save_pretrained(dpo_model_dir) [PRE89]`
    [PRE90][PRE91][PRE92][PRE93][PRE94]  [PRE95]``py[PRE96][PRE97][PRE98][PRE99]py[PRE100]py`
    [PRE101]`py`` [PRE102]`py[PRE103][PRE104][PRE105] [PRE106][PRE107][PRE108][PRE109][PRE110]``
    [PRE111][PRE112][PRE113] [PRE114]`py[PRE115]py[PRE116]py[PRE117]py[PRE118]py[PRE119]py`
    [PRE120]`py[PRE121]py[PRE122]py[PRE123][PRE124][PRE125][PRE126][PRE127]``py[PRE128]py[PRE129]'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE8]  [PRE9][PRE10][PRE11][PRE12]` [PRE13][PRE14][PRE15][PRE16] # 仅编码器 Transformer
    用于自然语言理解    当谷歌在 2018 年发布了 [BERT 模型](https://homl.info/bert)⁠^([7](ch15.html#id3475))
    时，它证明了仅编码器的 Transformer 可以处理各种自然语言任务：句子分类、标记分类、多项选择题回答等等！BERT 还证实了在大语料库上进行自监督预训练对迁移学习是有效的：BERT
    确实可以在许多任务上实现优异的性能，只需在每个任务上对相当小的数据集进行微调即可。让我们首先看看 BERT 的架构，然后我们将看看它是如何进行预训练的，以及您如何为自己的任务进行微调。    ######
    警告    仅编码器模型通常不用于文本生成任务，如自动完成、翻译、摘要或聊天机器人，因为它们在执行此任务时比解码器慢得多。解码器更快，因为它们是因果的，所以一个好的实现可以在预测新标记时缓存和重用其先前状态。相反，编码器仅使用非掩码多头注意力层，因此它们自然是双向的；这就是
    BERT 中的 B（来自 Transformer 的双向编码器表示）。每当添加新标记时，都需要重新计算。    ## BERT 的架构    BERT 的架构几乎与原始
    Transformer 的编码器完全相同，只有三个区别：    1.  它要大得多。BERT-base 有 12 个编码器块、12 个注意力头和 768 维嵌入，而
    BERT-large 有 24 个块、16 个头和 1,024 维度（而原始 Transformer 有 6 个块、8 个头和 512 维度）。它还使用可训练的位置嵌入，并支持输入句子长达
    512 个标记。           2.  它在每个子层（注意力或前馈）之前而不是之后应用层归一化。这被称为 *pre-LN*，与 *post-LN* 相比，并确保每个子层的输入都进行了归一化，这稳定了训练并减少了对手动初始化的敏感性。PyTorch
    的 transformer 模块默认为 post-LN，但它们有一个 `norm_first` 参数，您可以将它设置为 `True` 以选择 pre-LN（然而，某些优化可能不会为
    pre-LN 实现）。           3.  如果需要，它允许将输入句子拆分为两个 *段*。这对于需要一对输入句子的任务很有用，例如自然语言推理（即句子
    A 是否蕴涵句子 B？）或多项选择题回答（即给定问题 A，答案 B 有多好？）。要将两个句子传递给 BERT，您必须首先将 *分隔符标记* [SEP] 添加到每个句子中，然后连接它们。此外，还添加了一个可训练的
    *段嵌入* 到每个标记的表示中：段嵌入 #0 添加到段 #0 内的所有标记中，段嵌入 #1 添加到段 #1 内的所有标记中。理论上，我们可以有更多的段，但
    BERT 只在由一个或两个段组成的输入上进行预训练。请注意，位置编码也添加到每个标记的表示中，就像通常那样（即相对于完整输入序列，而不是相对于单个段）。              那就是全部！现在让我们看看
    BERT 是如何进行预训练的。    ## BERT 预训练    作者提出了两个自监督预训练任务：    遮蔽语言模型 (MLM)      句子中的每个标记有
    15% 的概率被替换为掩码标记，模型被训练来预测原始标记是什么。这通常被称为 *填空题*（即填空）。例如，如果原始句子是“她在生日派对上玩得很开心”，那么模型可能会得到句子“她'
