- en: Chapter 15\. Transformers for Natural Language Processing and Chatbots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a landmark 2017 paper titled [“Attention Is All You Need”](https://homl.info/transformer),⁠^([1](ch15.html#id3414))
    a team of Google researchers proposed a novel neural net architecture named the
    *Transformer*, which significantly improved the state of the art in neural machine
    translation (NMT). In short, the Transformer architecture is simply an encoder-decoder
    model, very much like the one we built in [Chapter 14](ch14.html#nlp_chapter)
    for English-to-Spanish translation, and it can be used in exactly the same way
    (see [Figure 15-1](#transformer_encoder_decoder_diagram)):'
  prefs: []
  type: TYPE_NORMAL
- en: The source text goes in the encoder, which outputs contextualized embeddings
    (one per token).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The encoder’s output is then fed to the decoder, along with the translated text
    so far (starting with a start-of-sequence token).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The decoder predicts the next token for each input token.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last token output by the decoder is appended to the translation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Steps 2 to 4 are repeated again and again to produce the full translation, one
    extra token at a time, until an end-of-sequence token is generated. During training,
    we already have the full translation—it’s the target—so it is fed to the decoder
    in step 2 (starting with a start-of-sequence token), and steps 4 and 5 are not
    needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Diagram illustrating the Transformer model''s process for translating English
    to Spanish, showing how the encoder generates contextual embeddings and the decoder
    predicts the next token in the translated sequence.](assets/hmls_1501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-1\. Using the Transformer model for English-to-Spanish translation
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So what’s new? Well, inside the black box, there are some important differences
    with our previous encoder-decoder. Crucially, the Transformer architecture does
    not contain any recurrent or convolutional layers, just regular dense layers combined
    with a new kind of attention mechanism called *multi-head attention* (MHA), plus
    a few bells and whistles.⁠^([2](ch15.html#id3417)) Because the model is not recurrent,
    it doesn’t suffer as much from the vanishing or exploding gradients problems as
    RNNs, it can be trained in fewer steps, it’s easier to parallelize across multiple
    GPUs, and it scales surprisingly well. Moreover, thanks to multi-head attention,
    the model can capture long-range patterns much better than RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer architecture also turned out to be extremely versatile. It was
    initially designed for NMT, but researchers quickly tweaked the architecture for
    many other language tasks. The year 2018 was even called the “ImageNet moment
    for NLP”. In June 2018, OpenAI released the first GPT model, based solely on the
    Transformer’s decoder module. It was pretrained on a large corpus of text, its
    ability to generate text was unprecedented, it could auto-complete sentences,
    invent stories, and even answer some questions. GPT could also be fine-tuned to
    perform a wide range of language tasks. Just a few months later, Google released
    the BERT model, based solely on the Transformer’s encoder module. It was excellent
    at a variety of *natural language understanding* (NLU) tasks, such as text classification,
    text embedding, multiple choice question answering, or finding the answer to a
    question within some text.
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly, transformers also turned out to be great at computer vision, audio
    processing (e.g., speech-to-text), robotics (using inputs from sensors and sending
    the outputs to actuators), and more. For example, if you split an image into little
    chunks and feed them to a transformer (instead of token embeddings), you get a
    *vision transformer* (ViT). In fact, some transformers can even handle multiple
    *modalities* at once (e.g., text + image); these are called *multimodal models*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This outstanding combination of performance, flexibility, and scalability encouraged
    Google, OpenAI, Facebook (Meta), Microsoft, Anthropic, and many other organizations
    to train larger and larger transformer models. The original Transformer model
    had about 65 million parameters—which was considered quite large at the time—but
    new transformers grew at a mind-boggling rate, reaching 1.6 trillion parameters
    by January 2021—that’s 1.6 million million parameters! Training such a gigantic
    transformer model from scratch is sadly restricted to organizations with deep
    pockets, as it requires a large and costly infrastructure for several months:
    training typically costs tens of millions of dollars, and even up to hundreds
    of millions according to some estimates (the exact figures are generally not public).
    [Figure 15-2](#transformer_growth_diagram) shows some of the most influential
    transformers released between June 2018 and April 2025.⁠^([3](ch15.html#id3423))
    Note that the vertical axis is in *billions* of parameters, and it uses a logarithmic
    scale.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing the exponential growth of transformer models from 2018 to
    2025, highlighting influential releases by organizations like OpenAI, Google,
    and Facebook, with parameter counts rising into trillions.](assets/hmls_1502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-2\. Some of the most influential transformers released since 2018;
    see it larger [online](https://homl.info/fig15-2)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Then, in November 2022, OpenAI released ChatGPT, an amazing *conversational
    AI*—or *chatbot*—that took the world by storm: it reached one million users in
    just five days, and over one hundred million monthly active users after just two
    months! Under the hood, it used GPT-3.5-turbo, a variant of GPT-3.5 which was
    fine-tuned to be conversational, helpful, and safe. Others soon followed: Perplexity
    AI, Google’s Gemini (initially called Bard), Anthropic’s Claude, Mistral AI, DeepSeek,
    and more.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Before ChatGPT was released, Google had actually developed a powerful chatbot
    named LaMDA, but it wasn’t made public, likely for fear of reputational and legal
    risks, as the model was not deemed safe enough. This allowed OpenAI to become
    the first company to train a reasonably safe and helpful model and to package
    it as a useful chatbot product.
  prefs: []
  type: TYPE_NORMAL
- en: 'So how can you use these models and chatbots? Well, many of them are proprietary
    (e.g., OpenAI’s GPT-3.5, GPT-4 and GPT-5 models, Anthropic’s Claude models, and
    Google’s Gemini models), and they can only be used via a web UI, an app, or an
    API: you must create an account, choose an offer (or use the free tier), and for
    the API you must get an access token and use it to query the API programmatically.
    However, many other models are *open weights*, meaning they can be downloaded
    for free (e.g., using the Hugging Face Hub): some of these have licensing restrictions
    (e.g., Meta’s Llama models are only free for noncommercial use), while others
    are truly open source (e.g., DeepSeek’s R1 or Mistral AI’s Mistral-7B). Some even
    include the training code and data (e.g., the OLMo models by Ai2).'
  prefs: []
  type: TYPE_NORMAL
- en: 'So what are we waiting for? Let’s join the transformer revolution! Here’s the
    plan:'
  prefs: []
  type: TYPE_NORMAL
- en: We will start by opening up the original Transformer architecture and inspecting
    its components to fully understand how everything works.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we will build and train a transformer from scratch for English-to-Spanish
    translation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After that, we will look into encoder-only models like BERT, learn how they
    are pretrained, and see how to use them for tasks like text classification, semantic
    search, and text clustering, with or without fine-tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will look into decoder-only models like GPT, and see how they are pretrained.
    These models are capable of generating text, which is great if you want to write
    a poem, but it can also be used to tackle many other tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then we will use a decoder-only model to build our own chatbot! This involves
    a few steps: first, you must download a pretrained model (or train your own if
    you have the time and money), then you must fine-tune it to make it more conversational,
    helpful, and safe (or you can download an already fine-tuned model, or even use
    a conversational model via an API), and lastly you must deploy the model to a
    chatbot system that offers a user interface, stores conversations, and can also
    give the model access to tools, such as searching the web or using a calculator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, we will take a quick look at encoder-decoder models, such as T5 and
    BART, which are great for tasks such as translation and summarization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [Chapter 16](ch16.html#vit_chapter), we will look at vision transformers
    and multimodal transformers. [Chapter 17](ch17.html#speedup_chapter) and “State-Space
    Models (SSMs)” (both available at [*https://homl.info*](https://homl.info)) also
    discuss some advanced techniques to allow Transformers to scale and process longer
    input sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by dissecting the Transformer architecture: take out your scalpel!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention Is All You Need: The Original Transformer Architecture'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The original 2017 Transformer architecture is represented in [Figure 15-3](#transformer_diagram).
    The left part of the figure represents the encoder, the right part represents
    the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw earlier, the encoder’s role is to gradually *transform* the inputs
    (e.g., sequences of English tokens) until each token’s representation perfectly
    captures the meaning of that token in the context of the sentence: the encoder’s
    output is a sequence of contextualized token embeddings. Apart from the embedding
    layer, every layer in the encoder takes as input a tensor of shape [*batch size*,
    *max English sequence length in the batch*, *embedding size*] and returns a tensor
    of the exact same shape. This means that token representations get gradually transformed,
    hence the name of the architecture. For example, if you feed the sentence “I like
    soccer” to the encoder, then the token “like” will start off with a rather vague
    representation, since “like” could mean different things in different contexts
    (e.g., “I’m like a cat” versus “I like my cat”). But after going through the encoder,
    the token’s representation should capture the correct meaning of “like” in the
    given sentence (in this case, to be fond of), as well as any other information
    that may be required for translation (e.g., it’s a verb).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The decoder’s role is to take the encoder’s outputs, along with the translated
    sentence so far, and predict the next token in the translation. For this, the
    decoder layers gradually transform each input token’s representation into a representation
    that can be used to predict the following token. For example, suppose the sentence
    to translate is “I like soccer” and we’ve already called the decoder four times,
    producing one new token each time: first “me”, then “me gusta”, then “me gusta
    el”, and finally “me gusta el fútbol”. Since this translation does not end with
    an EoS token `"</s>"`, we must call the decoder once again. The decoder’s input
    sequence is now "`<s>` me gusta el fútbol”. As the representation of each token
    goes through the decoder, it gets transformed: the representation of `"<s>"` becomes
    a rich enough representation to predict “me” (for simplicity, I’ll say this more
    concisely as: `"<s>"` becomes “me”), “me” becomes “gusta”, “gusta” becomes “el”,
    “el” becomes “fútbol”, and if everything goes well, “fútbol” becomes the EoS token
    `"</s>"`. Apart from the embedding layer and the output `Linear` layer, every
    layer in the decoder takes as input a tensor of shape [*batch size*, *max Spanish
    sequence length in the batch*, *embedding size*] and returns a tensor of the exact
    same shape.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of the original 2017 transformer architecture, showing the encoder
    and decoder layers with multi-head attention, feed forward, and positional encoding
    processes, illustrating how inputs are transformed into outputs.](assets/hmls_1503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-3\. The original 2017 transformer architecture⁠^([4](ch15.html#id3427))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'After going through the decoder, each token representation goes through a final
    `Linear` layer which will hopefully output a high logit for the correct token
    and a low logit for all other tokens in the vocabulary. The decoder’s output shape
    is [*batch size*, *max Spanish sequence length in the batch*, *vocabulary size*].
    The final predicted sentence should be “me gusta el fútbol `</s>`“. Note that
    the figure shows a softmax layer at the top, but in PyTorch we usually don’t explicitly
    add it: instead, we let the model output logits, and we train the model using
    `nn.CrossEntropyLoss`, which computes the cross-entropy loss based on logits instead
    of estimated probabilities (as we saw in previous chapters). If you ever need
    estimated probabilities, you can always convert the logits to estimated probabilities
    using the `F.softmax()` function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s zoom in a bit further into [Figure 15-3](#transformer_diagram):'
  prefs: []
  type: TYPE_NORMAL
- en: First, notice that both the encoder and the decoder contain blocks that are
    stacked *N* times. In the paper, *N* = 6\. Note that the final outputs of the
    whole encoder stack are fed to each of the decoder’s *N* blocks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you can see, you are already familiar with most components: there are two
    embedding layers; several skip connections, each of them followed by a layer normalization
    module; several feedforward modules composed of two dense layers each (the first
    one using the ReLU activation function, the second with no activation function);
    and finally, the output layer is a linear layer. Notice that all layers treat
    each token independently from all the others. But how can we translate a sentence
    by looking at the tokens completely separately? Well, we can’t, so that’s where
    the new components come in:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoder’s *multi-head attention* layer updates each token representation
    by attending to (i.e., paying attention to) every token in the same sentence,
    including itself. This is called *self-attention*. That’s where the vague representation
    of the word “like” becomes a richer and more accurate representation, capturing
    its precise meaning within the given sentence (e.g., the layer notices the subject
    “I” so it infers that “like” must be a verb). We will discuss exactly how this
    works shortly.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The decoder’s *masked multi-head attention* layer does the same thing, but
    when it processes a token, it doesn’t attend to tokens located after it: it’s
    a causal layer. For example, when it processes the token “gusta”, it only attends
    to the tokens `"<s>"`, “me”, and “gusta”, and it ignores the tokens “el” and “fútbol”
    (or else the model could cheat during training).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The decoder’s upper multi-head attention layer is where the decoder pays attention
    to the contextualized token representations output by the encoder stack. This
    is called *cross*-attention, as opposed to *self*-attention. For example, the
    decoder will probably pay close attention to the word “soccer” when it processes
    the word “el” and outputs a representation of the word “fútbol”.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *positional encodings* are dense vectors that represent the position of
    each token in the sentence. The *n*^(th) positional encoding is added to the token
    embedding of the *n*^(th) token in each sentence. This is needed because all layers
    in the Transformer architecture are position-agnostic, meaning they treat all
    positions equally (unlike recurrent or convolutional layers): when they process
    a token, they have no idea where that token is located in the sentence or relative
    to other words. But the order of words matters, so we must somehow give positional
    information to the Transformer. Adding positional encodings to the token representations
    is a good way to achieve this.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The first two arrows going into each multi-head attention layer in [Figure 15-3](#transformer_diagram)
    represent the keys and values, and the third arrow represents the queries.⁠^([5](ch15.html#id3439))
    In the self-attention layers, all three are equal to the token representations
    output by the previous layer, while in the cross-attention layers (i.e., the decoder’s
    upper attention layers), the keys and values are equal to the encoder’s final
    token representations, and the queries are equal to the token representations
    output by the previous decoder layer.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s go through the novel components of the Transformer architecture in
    more detail, starting with the positional encodings.
  prefs: []
  type: TYPE_NORMAL
- en: Positional Encodings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A positional encoding is a dense vector that encodes the position of a token
    within a sentence: the *i*^(th) positional encoding is added to the token embedding
    of the *i*^(th) token in each sentence. A simple way to implement this is to use
    an `Embedding` layer: just add embedding #0 to the representation of token #0,
    add embedding #1 to the representation of token #1, and so on. Alternatively,
    you can use an `nn.Parameter` to store the embedding matrix (initialized using
    small random weights), then add its first *L* rows to the inputs (where *L* is
    the max input sequence length): the result is the same, but it’s much faster.
    You can also add a bit of dropout to reduce the risk of overfitting. Here’s an
    implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The inputs have shape [*batch size*, *sequence length*, *embedding size*],
    but we are adding positional encodings of shape [*sequence length*, *embedding
    size*]. This works thanks to the broadcasting rules: the *i*^(th) positional embedding
    is added to the *i*^(th) token’s representation of each sentence in the batch.'
  prefs: []
  type: TYPE_NORMAL
- en: The authors of the Transformer paper also proposed using fixed positional encodings
    rather than trainable ones. Their approach used a pretty smart scheme based on
    the sine and cosine functions, but it’s not much used anymore, as it doesn’t really
    perform any better than trainable positional embeddings (except perhaps on small
    transformers, if you’re lucky). Please see this chapter’s notebook for more details.
    Moreover, newer approaches such as *relative position bias* (RPB), *rotary positional
    encoding* (RoPE), and *attention with linear bias* (ALiBi) generally perform better.
    To learn more about all of these alternative approaches to positional encoding,
    see “Relative Positional Encoding”.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s look deeper into the heart of the Transformer model: the multi-head
    attention layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Head Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The multi-head attention (MHA) layer is based on *scaled dot-product attention*,
    a variant of dot-product attention (introduced in [Chapter 14](ch14.html#nlp_chapter))
    that scales down the similarity scores by a constant factor. See [Equation 15-1](#scaled_dot_product_attention)
    for its vectorized equation.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 15-1\. Scaled dot-product attention
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <mo>Attention</mo><mrow><mo>(</mo><mrow><mi mathvariant="bold">Q</mi><mo lspace="0%"
    rspace="0%">,</mo><mi mathvariant="bold">K</mi><mo lspace="0%" rspace="0%">,</mo><mi
    mathvariant="bold">V</mi></mrow><mo>)</mo></mrow><mo>=</mo><mo>softmax</mo><mfenced><mfrac><mrow><msup><mi
    mathvariant="bold">QK</mi><mo>⊺</mo></msup></mrow><msqrt><mrow><msub><mi>d</mi><mi
    mathvariant="normal">k</mi></msub></mrow></msqrt></mfrac></mfenced><mi mathvariant="bold">V</mi>
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q** is a matrix representing a *query* (e.g., an English or Spanish sequence,
    depending on the attention layer). Its shape is [*L*[q], *d*[q]], where *L*[q]
    is the length of the query and *d*[q] is the query’s dimensionality (i.e., the
    number of dimensions in the token representations).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K** is a matrix representing a key. Its shape is [*L*[k], *d*[k]], where
    *L*[k] is the length of the key and *d*[k] is the key’s dimensionality. Note that
    *d*[k] must equal *d*[q].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**V** is a matrix representing a value. Its shape is [*L*[v], *d*[v]], where
    *L*[v] is the length of the value and *d*[v] is the value’s dimensionality. Note
    that *L*[v] must equal *L*[k].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The shape of **Q** **K**^⊺ is [*L*[q], *L*[k]]: it contains one similarity
    score for each query/key pair. To prevent this matrix from being huge, the input
    sequences must not be too long: this is the critical *quadratic context window*
    problem (we will discuss various ways to alleviate this issue in Chapters [16](ch16.html#vit_chapter)
    and [17](ch17.html#speedup_chapter). The softmax function is applied to each row:
    the output has the same shape as the input, but now each row sums up to 1\. The
    final output has a shape of [*L*[q], *d*[v]]. There is one row per query token,
    and each row represents the query result: a weighted sum of the value tokens,
    favoring value tokens whose corresponding key tokens are most aligned with the
    given query token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scaling factor 1 / <msqrt><msub><mi>d</mi> <mrow><mi>k</mi></mrow></msub></msqrt>
    scales down the similarity scores to avoid saturating the softmax function, which
    would lead to tiny gradients. This factor was empirically shown to speed up and
    stabilize training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to mask out some key/value pairs by adding a very large negative
    value to the corresponding similarity scores, just before computing the softmax
    (in practice, we can add `–torch.inf`). The resulting weights will be equal to
    zero. This is useful to mask padding tokens, as well as future tokens in the masked
    multi-head attention layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch comes with the `F.scaled_dot_product_attention()` function. Its inputs
    are just like **Q**, **K**, and **V**, but these inputs can have extra dimensions
    at the start, such as the batch size and the number of heads (when used for multi-head
    attention). The equation is applied simultaneously across all of these extra dimensions.
    In other words, the function computes the results simultaneously across all sentences
    in the batch and across all attention heads, making it very efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Now we’re ready to look at the multi-head attention layer. Its architecture
    is shown in [Figure 15-4](#multihead_attention_diagram).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it is just a bunch of scaled dot-product attention layers, called
    *attention heads*, each preceded by a linear transformation of the values, keys,
    and queries (across all tokens). The outputs of all the attention heads are simply
    concatenated, and they go through a final linear transformation (again, across
    all tokens).
  prefs: []
  type: TYPE_NORMAL
- en: But why? What is the intuition behind this architecture? Well, consider once
    again the word “like” in the sentence “I like soccer”. The encoder was hopefully
    smart enough to encode its meaning, the fact that it’s a verb, and many other
    features that are useful for its translation, such as the fact that it is in the
    present tense. The token representation also includes the position, thanks to
    the positional encodings. In short, the token representation encodes many different
    characteristics of the token. If we just used a single scaled dot-product attention
    layer, we would only be able to query all of these characteristics in one shot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the architecture of a multi-head attention layer, highlighting
    the flow from linear transformations of values, keys, and queries through split
    layers, scaled dot-product attention, concatenation, and final linear transformation.](assets/hmls_1504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-4\. Multi-head attention layer architecture⁠^([6](ch15.html#id3452))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The Transformer architecture is extremely flexible, so the model has plenty
    of freedom during training to choose its own knowledge representation and strategies.
    As a result, it ends up being somewhat of a black box: understanding how transformers
    truly “think” is an area of active research, called *model interpretability*.
    For example, check out this [fascinating post by Anthropic](https://homl.info/tracing-thoughts).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is why the MHA layer splits the values, keys, and queries across multiple
    heads: this way, each head can focus on specific characteristics of the token.
    The first linear layer lets the model choose which characteristics each head should
    focus on. For example, the linear layer may ensure that the first head gets a
    projection of the “like” token’s representation into a subspace where all that
    remains is the information that this token is a verb in the present tense. Another
    head may focus on the word’s meaning, and so on. Then the scaled dot-product attention
    layers implement the actual lookup phase, and finally the results are all concatenated
    and run through a final linear layer that lets the model reorganize the representation
    as it pleases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To really understand the Transformer architecture, the key is to understand
    multi-head attention, and for this, it helps to look at a basic implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s go through this code:'
  prefs: []
  type: TYPE_NORMAL
- en: The constructor stores the number of heads `self.h` and computes the number
    of dimensions per head `self.d`, then it creates the necessary modules. Note that
    the embedding size must be divisible by the number of heads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `split_heads()` method is used in the `forward()` method. It splits its
    input `X` along its last dimension (one split per head), converting it from a
    3D tensor of shape [*B*, *L*, *h* × *d*] to a 4D tensor of shape [*B*, *L*, *h*,
    *d*], where *B* is the batch size, *L* is the max length of the input sequences
    (specifically *L*[k] for the key and value, or *L*[q] for the query), *h* is the
    number of heads, and *d* is the number of dimensions per head (i.e., *h* × *d*
    = embedding size). The dimensions 1 and 2 are then swapped to get a tensor of
    shape [*B*, *h*, *L*, *d*]: since the matrix multiplication operator `@` only
    works on the last two dimensions, it won’t touch the first two dimensions *B*
    and *h*, so we will be able to use this operator to compute the scores across
    all instances in the batch and across all attention heads, all in one shot (`q
    @ k.transpose(2, 3)`). The same will be true when computing all the attention
    outputs (`weights @ v`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `forward()` method starts by applying a linear transformation to the query,
    key, and value, and passes the result through the `split_heads()` method. The
    next three lines compute [Equation 15-1](#scaled_dot_product_attention), plus
    a bit of dropout on the weights. Next we swap back dimensions 1 and 2 to ensure
    that the dimensions *h* and *d* are next to each other again, then we reshape
    the tensor back to 3D: this will concatenate the outputs of all heads. We can
    then apply the output linear transformation and return the result, along with
    the weights (in case we need them later).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Don’t worry if it takes some time to fully grasp this, it’s not easy. Of course,
    you can drive a car without fully understanding how the engine works, but some
    of the transformer improvements described in Chapters [16](ch16.html#vit_chapter)
    and [17](ch17.html#speedup_chapter) will only make sense if you understand MHA.
  prefs: []
  type: TYPE_NORMAL
- en: 'But wait! We’re missing one important detail: masking. Indeed, as we discussed
    earlier, the decoder’s masked self-attention layers must only consider previous
    tokens when trying to predict what the next token is (or else it would be cheating).
    Moreover, if the key contains padding tokens, we want to ignore them as well.
    So let’s update the `forward()` method to support two additional arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`attn_mask`'
  prefs: []
  type: TYPE_NORMAL
- en: A boolean mask of shape [*L*[q], *L*[k]] that we will use to control which key
    tokens each query token should ignore (`True` to ignore, `False` to attend)
  prefs: []
  type: TYPE_NORMAL
- en: '`key_padding_mask`'
  prefs: []
  type: TYPE_NORMAL
- en: A boolean mask of shape [*B*, *L*[k]] to locate the padding tokens in each key
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This code replaces the scores we want to ignore with negative infinity, so
    the corresponding weights will be zero after the softmax operation (if we tried
    to zero out these weights directly, the remaining weights would not add up to
    1). Note that the masks are broadcast automatically: `attn_mask` is broadcast
    across the whole batch and all attention heads, and `key_padding_mask` is broadcast
    across all heads and all query tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch has a very similar `nn.MultiheadAttention` module, which is much more
    optimized (e.g., it can often fuse the three input projections into one). It has
    the same arguments, which behave in exactly the same way. It also has a few more.
    Here are the most important:'
  prefs: []
  type: TYPE_NORMAL
- en: The constructor has a `batch_first` argument which defaults to `False`, so the
    module expects the batch dimension to come after the sequence length dimension.
    You must set `batch_first=True` if you prefer the batch dimension to come first,
    like in our custom implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `forward()` method has a `need_weights` argument that defaults to `True`.
    If you don’t need to use the weights returned by this module, you should set this
    argument to `False`, as it sometimes allows for some optimizations. When `need_weights`
    is set to `False`, the method returns `None` instead of the weights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `forward()` method also has an `is_causal` argument: if (and only if) the
    `attn_mask` is set and is a *causal mask*, then you can set `is_causal=True` to
    allow for some performance optimizations. A causal mask allows each query token
    to attend to all previous tokens (including itself), but doesn’t allow it to attend
    to tokens located after it. In other words, a causal mask contains `True` above
    the main diagonal, and `False` everywhere else. This is the mask needed for the
    masked self-attention layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have the main ingredient, we’re ready to implement the rest of the
    Transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Rest of the Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The rest of the Transformer architecture is much more straightforward. Let’s
    start with the encoder block. The following implementation closely matches the
    encoder block represented on the left side of [Figure 15-3](#transformer_diagram),
    except it sprinkles a bit of dropout after the self-attention layer and after
    both dense layers in the feedforward module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the feedforward block is composed of a first `Linear` layer that
    expands the dimensionality to 2048 (by default), followed by a nonlinearity (ReLU
    in this case), then a second `Linear` layer that projects the data back down to
    the original embedding size (also called the *model dimension*, `d_model`). This
    *reverse bottleneck* increases the expressive power of the nonlinearity, allowing
    the model to learn much richer combinations of features. This idea was explored
    further in the later MobileNetv2 paper, whose authors coined the term *inverted
    residual network*.
  prefs: []
  type: TYPE_NORMAL
- en: In the encoder, the `src_mask` argument is generally not used, since the encoder
    allows each token to attend to all tokens, even ones located after it. However,
    the user is expected to set the `key_padding_mask` appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now here’s an implementation of the decoder block. It closely matches the decoder
    block represented on the righthand side of [Figure 15-3](#transformer_diagram),
    with some additional dropout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `memory` argument corresponds to the output of the encoder. For full flexibility,
    we let the user pass the appropriate masks to the `forward()` method. In general,
    you will need to set the padding masks appropriately (both for the memory and
    target), and set the `tgt_mask` to a causal mask (we will see how shortly).
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch actually provides `nn.TransformerEncoderLayer` and `nn.TransformerDecoderLayer`
    out of the box, with the same arguments, plus a few more: most importantly `batch_first`,
    which you must set to `True` if the batch dimension is first, plus one `*_is_causal`
    argument for each attention mask, and an `activation` argument that defaults to
    “relu”. Many state-of-the-art transformers use a more advanced activation such
    as GELU (introduced in [Chapter 11](ch11.html#deep_chapter)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch also provides three more transformer modules (writing a custom module
    for each of these is left as an exercise for the reader—see the notebook for a
    solution):'
  prefs: []
  type: TYPE_NORMAL
- en: '`nn.TransformerEncoder`'
  prefs: []
  type: TYPE_NORMAL
- en: Simply chains the desired number of encoder layers. Its constructor takes an
    encoder layer plus the desired number of layers `num_layers`, and it clones the
    given encoder layer `num_layers` times. The constructor also takes an optional
    normalization layer, which (if provided) is applied to the final output.
  prefs: []
  type: TYPE_NORMAL
- en: '`nn.TransformerDecoder`'
  prefs: []
  type: TYPE_NORMAL
- en: Same, except it chains decoder layers instead of encoder layers.
  prefs: []
  type: TYPE_NORMAL
- en: '`nn.Transformer`'
  prefs: []
  type: TYPE_NORMAL
- en: Creates an encoder and a decoder (both with layer norm), and chains them.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You now know how to build a full Transformer model from scratch.
    You only need to add a final `Linear` layer and use the `nn.CrossEntropyLoss`
    to get the full architecture shown in [Figure 15-3](#transformer_diagram) (as
    we saw in earlier chapters, the softmax layer is implicitly included in the loss).
    Now let’s see how to use a Transformer model to translate English to Spanish.
  prefs: []
  type: TYPE_NORMAL
- en: Building an English-to-Spanish Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s time to build our NMT Transformer model. For this, we’ll use our `Positional​Embedding`
    module and PyTorch’s `nn.Transformer` (our custom `Transformer` module works fine,
    but it’s slower):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s go through this code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The constructor is straightforward: we just create the necessary modules.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `forward()` method takes an `NmtPair` as input (this class was defined in
    [Chapter 14](ch14.html#nlp_chapter)). The method starts by embedding the input
    tokens for both the source and target inputs, and it adds the positional encodings
    to both.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then the code uses the *not* operator (`~`) to invert both the source and target
    masks because they contain `False` for each padding token, but `nn.Multihead​Attention`
    expects `True` for tokens that it should ignore.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we create a square matrix of shape [*L*[q], *L*[q]], full of `True`,
    and we get all elements above the main diagonal using the `torch.triu()` function,
    with the rest defaulting to `False`. This results in a causal mask that we can
    use as the `tgt_mask` for the transformer: it will use this mask for the masked
    self-attention layer. Alternatively, you could call `nn.Transformer.gen⁠erate_​square_subsequent_mask()`
    to create the causal mask: just pass it the sequence length (`pair.tgt_token_ids.size(1)`)
    and set `dtype=torch.bool`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then call the transformer, passing it the source and target embeddings, as
    well as all the appropriate masks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, we pass the result through the output `Linear` layer, and we permute
    the last two dimensions because `nn.CrossEntropyLoss` expects the class dimension
    to be dimension 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can now create an instance of this model and train it exactly like our RNN
    encoder-decoder in [Chapter 14](ch14.html#nlp_chapter). To speed up training and
    reduce overfitting, you can shrink the transformer quite a bit—use 4 heads instead
    of 8, just 2 layers in both the encoder and the decoder, and use an embedding
    size of 128:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see how well this model performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]  [PRE9][PRE10][PRE11][PRE12]` [PRE13][PRE14][PRE15][PRE16] # Encoder-Only
    Transformers for Natural Language Understanding    When Google released the [BERT
    model in 2018](https://homl.info/bert),⁠^([7](ch15.html#id3475)) it proved that
    an encoder-only transformer can tackle a wide variety of natural language tasks:
    sentence classification, token classification, multiple choice question answering,
    and more! BERT also confirmed the effectiveness of self-supervised pretraining
    on a large corpus for transfer learning: BERT can indeed achieve excellent performance
    on many tasks, just by fine-tuning on a fairly small dataset for each task. Let’s
    start by looking at BERT’s architecture, then we’ll look at how it was pretrained,
    and how you can fine-tune it for your own tasks.    ###### Warning    Encoder-only
    models are generally not used for text generation tasks, such as autocompletion,
    translation, summarization, or chatbots, because they’re much slower at this task
    than decoders. Decoders are faster because they are causal, so a good implementation
    can cache and reuse its previous state when predicting a new token. Conversely,
    encoders use nonmasked multi-head attention layers only, so they are naturally
    bidirectional; hence the B in BERT (Bidirectional Encoder Representations from
    Transformers). Whenever a new token is added, everything needs to be recomputed.    ##
    BERT’s Architecture    BERT’s architecture is almost identical to the original
    Transformer’s encoder, with just three differences:    1.  It’s much bigger. BERT-base
    has 12 encoder blocks, 12 attention heads, and 768-dimensional embeddings, and
    BERT-large has 24 blocks, 16 heads, and 1,024 dimensions (while the original Transformer
    has 6 blocks, 8 heads, and 512 dimensions). It also uses trainable positional
    embeddings and supports input sentences up to 512 tokens.           2.  It applies
    layer-norm just *before* each sublayer (attention or feedforward) rather than
    *after* each skip connection. This is called *pre-LN*, as opposed to *post-LN*,
    and it ensures that the inputs of each sublayer are normalized, which stabilizes
    training and reduces sensitivity to weight initialization. PyTorch’s transformer
    modules default to post-LN, but they have a `norm_first` argument which you can
    set to `True` if you prefer pre-LN (however, some optimizations may not be implemented
    for pre-LN).           3.  It lets you split the input sentence into two *segments*
    if needed. This is useful for tasks that require a pair of input sentences, such
    as natural language inference (i.e., does sentence A entail sentence B?) or multiple
    choice question answering (i.e., given question A, how good is answer B?). To
    pass two sentences to BERT, you must first append a *separation token* [SEP] to
    each one, then concatenate them. Furthermore, a trainable *segment embedding*
    is added to each token’s representation: segment embedding #0 is added to all
    tokens within segment #0, and segment embedding #1 is added to all tokens within
    segment #1\. In theory, we could have more segments, but BERT was only pretrained
    on inputs composed of one or two segments. Note that the positional encodings
    are also added to each token’s representation, as usual (i.e., relative to the
    full input sequence, not relative to the individual segments).              That’s
    all! Now let’s look at how BERT was pretrained.    ## BERT Pretraining    The
    authors proposed two self-supervised pretraining tasks:    Masked language model
    (MLM)      Each token in a sentence has a 15% probability of being replaced with
    a mask token, and the model is trained to predict what the original tokens were.
    This is often called a *cloze task* (i.e., fill in the blanks). For example, if
    the original sentence is “She had fun at the birthday party”, then the model may
    be given the sentence “She [MASK] fun at the [MASK] party” and it must predict
    the original sentence: the loss is only computed on the mask token outputs.    To
    be more precise, some of the masked tokens are not truly masked: 10% are instead
    replaced by random tokens, and 10% are just left alone, neither masked nor randomized.
    Why is that? Well, the random tokens force the model to perform well even when
    mask tokens are absent: this is important since most downstream tasks don’t use
    any mask tokens. As for the untouched tokens, they make the prediction trivial,
    which encourages the model to pay attention to the input token located at the
    position of the token being predicted. Without them, the model would soon learn
    to ignore this token and rely solely on the other tokens.      Next sentence prediction
    (NSP)      The model is trained to predict whether two sentences are consecutive
    or not. For example, it should predict that “The dog sleeps” and “It snores loudly”
    are consecutive sentences, while “The dog sleeps” and “The Earth orbits the Sun”
    are not consecutive.    This is a binary classification task, which the authors
    chose to implement by introducing a new *class token* [CLS]: this token is inserted
    at the beginning of the input sequence (position #0, segment #0), and during training
    the encoder’s output, this token is passed through a binary classification head
    (i.e., a linear layer with a single unit, followed by the sigmoid function, and
    trained using `nn.BCELoss`, or just a linear layer with a single unit trained
    using `nn.BCEWith​Lo⁠gitsLoss`).      BERT was pretrained on both MLM and NSP
    simultaneously (see [Figure 15-5](#bert_pretraining_diagram) and the left side
    of [Figure 15-6](#bert_diagram)), using a large corpus of text—specifically the
    English Wikipedia and BooksCorpus. The goal of NSP was to make the class token’s
    contextualized embedding a good representation of the whole input sequence. At
    first, it seemed that it indeed produced good sentence embeddings, but it was
    later shown that simply pooling all the contextualized embeddings (e.g., by computing
    their mean) yielded better results. In fact, researchers showed that NSP did not
    help much overall, so it was dropped in most later architectures.    In [Chapter 14](ch14.html#nlp_chapter),
    we saw how to use the Transformers library to download a pretrained BERT model
    and its tokenizer. But you may want to train a BERT model from scratch, for example,
    if you’re dealing with a domain-specific corpus of text. For this, one option
    is to build BERT yourself using the `nn.TransformerEncoder` module (e.g., based
    on an `nn.TransformerEncoderLayer` with `norm_first=True` to respect BERT’s architecture),
    then preprocess your dataset according to the MLM algorithm, and train your model.  ![Diagram
    illustrating BERT pretraining showing input and target sequences for MLM and NSP
    tasks, highlighting masked, random, and unchanged tokens.](assets/hmls_1505.png)  ######
    Figure 15-5\. Input and target during BERT pretraining, using MLM and NSP    However,
    there’s an easier way, using the Transformers library. Let’s start by creating
    a tokenizer and a randomly initialized BERT model. For simplicity, we use a pretrained
    tokenizer, but of course you can train one from scratch instead, if you prefer.
    Make sure to tweak the `BertConfig` depending on your training budget, and the
    size and complexity of your dataset:    [PRE17]    Next, let’s download the WikiText
    dataset (in real life, you would use your own dataset instead), and tokenize it:    [PRE18]    This
    is where MLM comes in. We create a data collator, whose role is to bundle samples
    into batches, and we set its `mlm` argument to `True` to activate MLM, and also
    set `mlm_probability=0.15`: each token has a 15% probability of being masked (or
    possibly randomized or left alone, as we just discussed). We also pass the tokenizer
    to the collator: it will not be used to tokenize the text—we’ve already done that—but
    it lets the data collator know the masking and padding token IDs, as well as the
    vocabulary size (which is needed to sample random token IDs). With that, we just
    need to specify the `TrainingArguments`, pass everything to the `Trainer`, and
    call its `train()` method:    [PRE19]    Once your model is pretrained, you can
    try it out using the pipelines API:    [PRE20]`` `>>>` `fill_mask` `=` `pipeline``(``"fill-mask"``,`
    `model``=``bert``,` `tokenizer``=``bert_tokenizer``)` [PRE21] `>>>` `top_predictions``[``0``]`
    `` `{''score'': 0.04916289076209068,`  `''token'': 1010,`  `''token_str'': '','',`  `''sequence'':
    ''the capital of, is rome.''}` `` [PRE22]` [PRE23]   [PRE24]  [PRE25] [PRE26]`py  [PRE27]`py[PRE28]py
    [PRE29]`py[PRE30]py[PRE31]py` # Decoder-Only Transformers    While Google was
    working on the first encoder-only model (i.e., BERT), Alec Radford and other OpenAI
    researchers were taking a different route: they built the first decoder-only model,
    named GPT.⁠^([11](ch15.html#id3567)) This model paved the way for today’s most
    impressive models, including most of the ones used in famous chatbots like ChatGPT
    or Claude.    The GPT model (now known as GPT-1) was released in June 2018\. GPT
    stands for *Generative Pre-Training*: it was pretrained on a dataset of about
    7,000 books and learned to predict the next token, so it can be used to generate
    text one token at a time, just like the original Transformer’s decoder. For example,
    if you feed it “Happy birthday”, it will predict “birthday to”, so you can append
    “to” to the input and repeat the process (see [Figure 15-12](#gpt_generation_diagram)).  ![Diagram
    illustrating how a decoder-only model, like GPT, generates text one token at a
    time by predicting the next word in the sequence, such as continuing "Happy birthday"
    with "to you!".](assets/hmls_1512.png)  ###### Figure 15-12\. Generating text
    one token at a time using a decoder-only model like GPT    Decoder-only models
    are great at *text generation* tasks, such as auto-completion, code generation,
    question answering (including free text answers), math and logical reasoning (to
    some extent), and chatbots. They can also be used for summarization or translation,
    but encoder-decoder models are still popular choices for these tasks, as they
    often have a better understanding of the source text, thanks to the encoder. Decoder-only
    models can also perform text classification quite well, but encoder-only models
    shine in this area, as they are faster and often provide a similar performance
    with a smaller model.    ###### Warning    At inference time, encoder-only models
    only need to look at their inputs once to make their predictions, while decoder-only
    models require one run per generated token (just like the decoder in encoder-decoder
    models). That’s because decoders are autoregressive, so the generation process
    is sequential. That said, decoders can hugely benefit from caching, as I mentioned
    earlier.    In this section, we will look at the architecture of GPT-1 and its
    successor GPT-2, and we will see how decoder-only models like these can be used
    for various tasks. We will also see that these models can perform tasks that they
    were never explicitly trained on (zero-shot learning) or for which they only saw
    a few examples (few-shot learning). Lastly, we will then use the Transformers
    library to download a small decoder-only model (GPT-2) then a large one (Mistral-7B)
    and use them to generate text and answer questions.    ## GPT-1 Architecture and
    Generative Pretraining    During pretraining, GPT-1 was fed batches of 64 sequences
    randomly sampled from the book corpus, and it was trained to predict the next
    token for every single input token. Each sequence was exactly 512 tokens long,
    so GPT-1 did not need any padding token. In fact, it didn’t use special tokens
    at all during pretraining, not even start-of-sequence or end-of-sequence tokens.
    Compared to BERT, it’s a much simpler pretraining process. It also provides the
    same amount of data for every token position, whereas BERT sees less data for
    the last positions than for the first, due to padding.    GPT-1’s architecture
    has two important differences compared to the original Transformer’s decoder:    *   There’s
    no cross-attention block since there’s no encoder output to attend to: each decoder
    block only contains a masked multi-head attention layer and a two-layer feedforward
    network (each with its own skip connection and layer norm).           *   It’s
    much bigger: it has 12 decoder layers instead of 6, the embedding size is 768
    instead of 512, and it has 12 attention heads instead of 8\. That’s a total of
    117 million parameters.              ###### Warning    Counterintuitively, you
    cannot use PyTorch’s `nn.TransformerDecoder` module to build a decoder-only model.
    That’s because it contains cross-attention layers that cannot be easily removed.
    Instead, you can use the `nn.TransformerEncoder` module, and always call it with
    a causal mask.    Out-of-the-box, GPT-1 was very impressive at text generation.
    For example, its authors asked it to tell the story of a scientist discovering
    a herd of English-speaking unicorns in an unexplored valley, and the story it
    generated seemed like it had been written by a human (you can read it at [*https://homl.info/unicorns*](https://homl.info/unicorns)).
    It’s not quite as impressive today, but back then it was truly mind-blowing.    The
    authors also fine-tuned GPT-1 on various tasks, including textual entailment,
    semantic similarity, reading comprehension, or common sense reasoning, and it
    beat the state of the art on many of them, confirming the power of pretraining
    for NLP. For each task, the authors only made minor changes to the architecture:    *   For
    text classification tasks, a classification head is added on top of the last token’s
    output embedding. See the righthand side of [Figure 15-13](#gpt_training_diagram).           *   For
    entailment and other classification tasks requiring two input sentences, the model
    is fed both sentences separated by a delimiter token (just a regular $ sign),
    and again a classification head is added on top of the last token’s output embedding.           *   For
    semantic similarity, since the order of the two sentences shouldn’t matter, the
    model gets called twice: once with sentence 1 $ sentence 2, and once with sentence
    2 $ sentence 1\. The last token’s output embeddings for both cases are added itemwise
    and the result is fed to a regression head.           *   For multiple choice
    question answering, the approach is very similar to BERT’s: the model is called
    once per possible answer, with both the context (including the question) and the
    possible answer as input, separated by a $ sign, then the last token’s output
    embedding is passed through a linear layer to get a score. All the answer scores
    are then passed through a softmax layer.           *   In all cases they added
    a start-of-sequence token and an end-of-sequence token.            ![Diagram illustrating
    GPT-1 pretraining with next token prediction using softmax and fine-tuning for
    classification with a sigmoid function.](assets/hmls_1513.png)  ###### Figure
    15-13\. Pretraining GPT-1 using next token prediction (NTP, left) and fine-tuning
    it for classification (right)    ## GPT-2 and Zero-Shot Learning    Just a few
    months later, in February 2019, Alec Radford, Jeffrey Wu, and other OpenAI researchers
    published the GPT-2 paper,⁠^([12](ch15.html#id3586)) which proposed a very similar
    architecture to GPT-1,⁠^([13](ch15.html#id3587)) but larger still. It came in
    four sizes, and the largest model had 48 decoder layers, 20 attention heads, an
    embedding size of 1,600, and a context window of 1,024 tokens, for a total of
    over 1.5 billion parameters!    For such a large model, the authors needed a gigantic
    dataset, so they initially tried using Common Crawl which contains over two billion
    web pages. However, many of these pages are just gibberish (e.g., long tables
    of data). So the authors built a higher-quality dataset named *WebText*, composed
    of about eight million pages linked from highly ranked Reddit pages.    Most importantly,
    GPT-2 performed incredibly well on many tasks without any fine-tuning: this is
    called *zero-shot learning* (ZSL). For example:    *   For question answering,
    you can simply append “A:” to the question (e.g., “What is the capital of New-Zealand?
    A:”) then feed this prompt to GPT-2\. It will complete it with the answer (e.g.,
    “Wellington”).           *   For summarization, you can append “TL;DR:” to the
    document you want to summarize, and GPT-2 will often produce a decent summary.           *   For
    translation, you can create a prompt containing a few examples to guide the model,
    such as “Bonjour papa = Hello dad” and “Le chien dort = The dog is sleeping”,
    then append the text you want to translate, for example “Elle aime le chocolat
    =”, and GPT-2 will hopefully complete the prompt with the correct English translation:
    “She loves chocolate”.              Importantly, the authors showed that ZSL performance
    seemed to increase regularly with the model size: doubling the model size offered
    a roughly constant improvement (that’s a log-linear relationship). Maybe creating
    a superhuman AI was just a matter of training a large enough transformer?    ######
    Note    GPT-2’s performance was so impressive that OpenAI initially chose not
    to release the largest model. Officially, this was for the public’s safety, citing
    risks like automated disinformation and spam. But skeptics argued that it was
    both a publicity stunt and a shift toward closed-source AI, and perhaps even a
    move to influence future regulation. The full GPT-2 model was eventually released
    months later, but it was the last open one from OpenAI until August 2025, when
    a couple of open-weight models were released (GPT-OSS).    ## GPT-3, In-Context
    Learning, One-Shot Learning, and Few-Shot Learning    Following their bigger-is-better
    philosophy, OpenAI created [GPT-3](https://homl.info/gpt3) in 2020.⁠^([14](ch15.html#id3595))
    It had roughly 40 billion parameters, and was trained on a monstrously large dataset
    of about 570 gigabytes (including WebCrawl this time).    This model indeed was
    far better across the board than GPT-2\. In particular, it was much better at
    zero-shot tasks. But most importantly, the authors showed that GPT-3 was incredibly
    good at generalizing from just a few examples. This is called *few-shot learning*
    (FSL), or *one-shot learning* (OSL) if there’s a single example. To tackle FSL
    or OSL tasks, the authors simply inserted the example(s) in the prompt: they dubbed
    this *in-context learning* (ICL). For example, if you feed the following prompt
    to GPT-3, can you guess what it will output?    [PRE32]py    That’s right, it
    will output the missing word, “bat”. The idea of feeding the model some examples
    in the prompt itself was already present in the GPT-2 paper (remember the translation
    example?), but it wasn’t really formalized, and the GPT-3 paper explored it in
    much more depth.    ###### Note    In-context learning is an increasingly popular
    approach to one-shot learning and few-shot learning, but there are many others.
    ICL is new, but OSL and FSL are old (like ZSL).    Let’s download GPT-2 and generate
    some text with it (we will play with GPT-3 via the API later in this chapter).    ##
    Using GPT-2 to Generate Text    As you might expect, we can use the Transformers
    library to download GPT-2\. By default, we get the small version (124M parameters):    [PRE33]py    Let’s
    go through this code:    *   After the imports, we load GPT-2’s pretrained tokenizer
    and the model itself.           *   To load the model, we use `AutoModelForCausalLM.from_pretrained()`,
    which returns an instance of the appropriate class based on the checkpoint we
    ask for (in this case it returns a `GPT2LMHeadModel`). Since it’s a causal language
    model, it’s capable of generating text, as we will see shortly.           *   The
    `device_map="auto"` option tells the function to automatically place the model
    on the best available device, typically the GPU. If you have multiple GPUs and
    the model is too large for one, it may even be sharded across GPUs.           *   The
    `dtype="auto"` option asks the function to choose the most appropriate data type
    for the model weights, based on what’s available in the model checkpoint and your
    hardware. Typically, it loads the model using 16-bit floats if your hardware supports
    it (e.g., a modern GPU with mixed-precision support), or it falls back to 32-bit
    floats. Using half precision (16-bit) uses half the memory, which lets you load
    larger models, and it also gives the model a substantial speed boost because modern
    GPUs have hardware accelerations for this, and half precision reduces the amount
    of data that needs to be transferred between the CPU and GPU.              Now
    let’s write a little wrapper function around the model’s `generate()` method to
    make it very easy to generate text:    [PRE34]py    Our `generate()` function
    tokenizes the given prompt, transfers the resulting token IDs to the GPU, calls
    the given model’s `generate()` method to extend the prompt, adding up to 50 new
    tokens (by default) or less if it runs into an end-of-sequence token, and lastly
    it decodes the resulting token IDs to return a nice string containing the extended
    text. Since GPT-2 was pretrained without padding, we must specify which token
    we want to use for padding when calling the model’s `generate()` method: it’s
    common to use the end-of-sequence token for this. This function processes a single
    prompt so there will be no padding anyway, but specifying the padding token avoids
    a pesky warning. Our function also accepts optional extra keyword arguments (`**generate_kwargs`)
    and passes them on to the model’s `generate()` method. This will come handy very
    soon.    ###### Note    Decoder-only models often pad on the left side, for more
    efficient generation, since new tokens are added on the right.    Now let’s try
    generating some text about a talking unicorn:    [PRE35]py   [PRE36]py Hmm, it
    starts out pretty well, but then it just repeats itself—what’s happening? Well,
    by default the `generate()` method simply picks the most likely token at each
    step, which is fine when you expect very structured output, or for tasks such
    as question answering, but for creative writing it often gets the model stuck
    in a loop, producing repetitive and uninteresting text. To fix this, we can set
    `do_sample=True` to make the `generate()` method randomly sample each token based
    on the model’s estimated probabilities for the possible tokens, like we did with
    our Shakespeare model in [Chapter 14](ch14.html#nlp_chapter). Let’s see if this
    works:    [PRE37]py   [PRE38]`py [PRE39]py`` [PRE40]py ## Using GPT-2 for Question
    Answering    Let’s write a little function that takes a country name and asks
    GPT-2 to return its capital city:    [PRE41]py    The function starts by creating
    a prompt from a *prompt template*: it replaces the `{country}` placeholder with
    the given country name. Note that the prompt template includes one example of
    the task to help GPT-2 understand what to do and what format we expect: that’s
    in-context learning. The function then calls our `generate()` function to add
    10 tokens to the prompt: this is more than we need to write the capital city’s
    name. Lastly, we do a bit of post-processing by removing the initial prompt as
    well as anything after the first line, and we strip away any extra spaces at the
    end. Let’s try it out!    [PRE42]py   `` `It works beautifully! Moreover, it’s
    quite flexible with its input; for example, if you ask it for the capital of “UK”,
    “The UK”, “England”, “Great Britain”, or even “Big Britane”, it will still return
    “London”. That said, it’s far from perfect:    *   It makes many common mistakes
    (e.g., for Canada, it answers Toronto instead of Ottawa). Sadly, since GPT-2 was
    trained on many pages from the web, it picked up people’s misconceptions and biases.           *   When
    it’s not sure, it just repeats the country’s name, roughly 30% of the time. This
    might be because several countries have a capital city of the same name (e.g.,
    Djibouti, Luxembourg, Singapore) or close (e.g., Guatemala City, Kuwait City).           *   When
    the input is not a country, the model often answers “Paris”, since that’s the
    only example it had in its prompt.              One way to fix these issues is
    to simply use a much bigger and smarter model. For example, try using “gpt2-xl”
    (1.5B parameters) instead of “gpt2” when loading the model, then run the code
    again. It still won’t be perfect, but you should notice a clear improvement. So
    let’s see if an much larger model can do even better!` ``  [PRE43]`py [PRE44]py``  [PRE45]`py[PRE46]py[PRE47]py[PRE48]py[PRE49]py[PRE50]py[PRE51]`py[PRE52]py[PRE53][PRE54][PRE55]py
    class BobTheChatbot:  # or ChatBob if you prefer     def __init__(self, model,
    tokenizer, introduction=bob_introduction,                  max_answer_length=10_000):         self.model
    = model         self.tokenizer = tokenizer         self.context = introduction         self.max_answer_length
    = max_answer_length      def chat(self, prompt):         self.context += "\nMe:
    " + prompt + "\nBob:"         context = self.context         start_index = len(context)         while
    True:             extended = generate(self.model, self.tokenizer, context,                                 max_new_tokens=100)             answer
    = extended[start_index:]             if ("\nMe: " in answer or extended == context
    or                 len(answer) >= self.max_answer_length): break             context
    = extended         answer = answer.split("\nMe: ")[0]         self.context +=
    answer         return answer.strip() [PRE56]py >>> bob = BobTheChatbot(mistral7b,
    mistral7b_tokenizer) `>>>` `bob``.``chat``(``"List some places I should visit
    in Paris."``)` [PRE57][PRE58]`` [PRE59][PRE60] [PRE61][PRE62]``py[PRE63]`` The
    first estimated probability is for the token “The” (3.27%), then “capital” (0.02%),
    and so on. The second sequence starts with a padding token, so you can ignore
    the first probability (0.14%). The estimated probabilities are the same in both
    sequences for the prompt tokens,⁠^([26](ch15.html#id3667)) but they differ for
    the answer tokens: 11.38% for “Buenos”, versus 0.00% for “Madrid”. The model seems
    to know a bit of geography! You may have expected a higher probability for “Buenos”,
    but tokens like “a”, “one”, and “the” were also quite likely after “is”. However,
    once the model saw “Buenos”, it was almost certain that the next token was going
    to be “Aires” (99.61%), and of course it was correct.    Now if we add up the
    log probabilities of all answer tokens (e.g., for “Buenos” and “Aires”), we get
    the estimated log probability for the whole answer given the previous tokens,
    which is precisely what we were looking for (i.e., log *p*(**y** | **x**)). In
    this example, it corresponds to an estimated probability of 11.38%:    [PRE64]   [PRE65]``
    [PRE66]` However, having to find the exact location of the answer is cumbersome,
    especially when dealing with padded batches. Luckily, we can actually compute
    the DPO loss using the log probability of the full input **xy** (including both
    the prompt **x** and the answer **y**), rather than the log probability of the
    answer **y** given the prompt **x**. In other words, we can replace every log
    *p*(**y** | **x**) with log *p*(**xy**) in [Equation 15-2](#dpo_loss_equation)
    (for both *p*[**θ**] and *p*[ref], and for both **y**[c] and **y**[r]). This is
    because log *p*(**xy**) = log *p*(**x**) + log *p*(**y** | **x**), and the extra
    *p*(**x**) for the chosen answer cancels out exactly with the extra *p*(**x**)
    for the rejected answer. We only need to mask the padding tokens—we can use the
    attention mask for that—then simply add up all the log probabilities for each
    sequence:    [PRE67] `>>>` `log_probas_sum` `` `tensor([-21.2500, -30.2500], device=''cuda:0'',
    dtype=torch.bfloat16)` `` [PRE68]   [PRE69] [PRE70]`py [PRE71]py`` [PRE72]py[PRE73][PRE74][PRE75]``
    [PRE76][PRE77]``py[PRE78][PRE79] def preprocess(example):     text = f"Human:
    {example[''instruction'']}\n"     if example[''input''] != "":         text +=
    f"-> {example[''input'']}\n"     text += f"\nAssistant: {example[''output'']}"     return
    {"text": text}  sft_dataset = sft_dataset.map(preprocess) [PRE80] >>> print(sft_dataset[1]["text"])
    `Human: What are the three primary colors?`  `Assistant: The three primary colors
    are red, blue, and yellow.` [PRE81][PRE82] from trl import SFTTrainer, SFTConfig  sft_model_dir
    = "./my_gpt2_sft_alpaca" training_args = SFTConfig(     output_dir=sft_model_dir,
    max_length=512,     per_device_train_batch_size=4, num_train_epochs=1, save_steps=50,     logging_steps=10,
    learning_rate=5e-5) sft_trainer = SFTTrainer("gpt2", train_dataset=sft_dataset,
    args=training_args) sft_train_output = sft_trainer.train() sft_trainer.model.save_pretrained(sft_model_dir)
    [PRE83] >>> pref_dataset = load_dataset("Anthropic/hh-rlhf", split="train") `>>>`
    `pref_dataset``[``2``]``.``keys``()` [PRE84] `Human: If you were going to steal
    from a convenience store, do you think it` `would be better in the morning or
    at night?`  `Assistant: I really couldn’t say, I’m not familiar with stealing
    convenience` `store items.` `>>>` `print``(``pref_dataset``[``2``][``"rejected"``]``.``strip``())`
    `` `Human: If you were going to steal from a convenience store, do you think it`
    `would be better in the morning or at night?`  `Assistant: It is good to consider
    the difference in human traffic at night, and` `also the security cameras.  Night
    time would probably be better [...]` `` [PRE85]` [PRE86][PRE87]`` [PRE88] from
    trl import DPOConfig, DPOTrainer  dpo_model_dir = "./my_gpt2_sft_alpaca_dpo_hh_rlhf"
    training_args = DPOConfig(     output_dir=dpo_model_dir, max_length=512, per_device_train_batch_size=4,     num_train_epochs=1,
    save_steps=50, logging_steps=10, learning_rate=2e-5) gpt2_tokenizer.pad_token
    = gpt2_tokenizer.eos_token dpo_trainer = DPOTrainer(     sft_model_dir, args=training_args,
    train_dataset=pref_dataset,     processing_class=gpt2_tokenizer) dpo_train_output
    = dpo_trainer.train() dpo_trainer.model.save_pretrained(dpo_model_dir) [PRE89]`
    [PRE90][PRE91][PRE92][PRE93][PRE94]  [PRE95]``py[PRE96][PRE97][PRE98][PRE99]py[PRE100]py`
    [PRE101]`py`` [PRE102]`py[PRE103][PRE104][PRE105] [PRE106][PRE107][PRE108][PRE109][PRE110]``
    [PRE111][PRE112][PRE113] [PRE114]`py[PRE115]py[PRE116]py[PRE117]py[PRE118]py[PRE119]py`
    [PRE120]`py[PRE121]py[PRE122]py[PRE123][PRE124][PRE125][PRE126][PRE127]``py[PRE128]py[PRE129]'
  prefs: []
  type: TYPE_NORMAL
