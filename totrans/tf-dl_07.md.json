["```py\ndef _read_words(filename):\n  with tf.gfile.GFile(filename, \"r\") as f:\n    if sys.version_info[0] >= 3:\n      return f.read().replace(\"\\n\", \"<eos>\").split()\n    else:\n      return f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\n```", "```py\ndef _build_vocab(filename):\n  data = _read_words(filename)\n\n  counter = collections.Counter(data)\n  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n\n  words, _ = list(zip(*count_pairs))\n  word_to_id = dict(zip(words, range(len(words))))\n\n  return word_to_id\n```", "```py\ndef _file_to_word_ids(filename, word_to_id):\n  data = _read_words(filename)\n  return [word_to_id[word] for word in data if word in word_to_id]\n```", "```py\ndef ptb_raw_data(data_path=None):\n  \"\"\"Load PTB raw data from data directory \"data_path\".\n\n Reads PTB text files, converts strings to integer ids,\n and performs mini-batching of the inputs.\n\n The PTB dataset comes from Tomas Mikolov's webpage:\n http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n\n Args:\n data_path: string path to the directory where simple-examples.tgz\n has been extracted.\n\n Returns:\n tuple (train_data, valid_data, test_data, vocabulary)\n where each of the data objects can be passed to PTBIterator.\n \"\"\"\n\n  train_path = os.path.join(data_path, \"ptb.train.txt\")\n  valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n  test_path = os.path.join(data_path, \"ptb.test.txt\")\n\n  word_to_id = _build_vocab(train_path)\n  train_data = _file_to_word_ids(train_path, word_to_id)\n  valid_data = _file_to_word_ids(valid_path, word_to_id)\n  test_data = _file_to_word_ids(test_path, word_to_id)\n  vocabulary = len(word_to_id)\n  return train_data, valid_data, test_data, vocabulary\n```", "```py\ndef ptb_producer(raw_data, batch_size, num_steps, name=None):\n  \"\"\"Iterate on the raw PTB data.\n\n This chunks up raw_data into batches of examples and returns\n Tensors that are drawn from these batches.\n\n Args:\n raw_data: one of the raw data outputs from ptb_raw_data.\n batch_size: int, the batch size.\n num_steps: int, the number of unrolls.\n name: the name of this operation (optional).\n\n Returns:\n A pair of Tensors, each shaped [batch_size, num_steps]. The\n second element of the tuple is the same data time-shifted to the\n right by one.\n\n Raises:\n tf.errors.InvalidArgumentError: if batch_size or num_steps are\n too high.\n \"\"\"\n  with tf.name_scope(name, \"PTBProducer\",\n                     [raw_data, batch_size, num_steps]):\n    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\",\n                                    dtype=tf.int32)\n\n    data_len = tf.size(raw_data)\n    batch_len = data_len // batch_size\n    data = tf.reshape(raw_data[0 : batch_size * batch_len],\n                      [batch_size, batch_len])\n\n    epoch_size = (batch_len - 1) // num_steps\n    assertion = tf.assert_positive(\n        epoch_size,\n        message=\"epoch_size == 0, decrease batch_size or num_steps\")\n    with tf.control_dependencies([assertion]):\n      epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n\n    i = tf.train.range_input_producer(epoch_size,\n                                      shuffle=False).dequeue()\n    x = tf.strided_slice(data, [0, i * num_steps],\n                         [batch_size, (i + 1) * num_steps])\n    x.set_shape([batch_size, num_steps])\n    y = tf.strided_slice(data, [0, i * num_steps + 1],\n                         [batch_size, (i + 1) * num_steps + 1])\n    y.set_shape([batch_size, num_steps])\n    return x, y\n```", "```py\ndef lstm_cell():\n  return tf.contrib.rnn.BasicLSTMCell(\n      size, forget_bias=0.0, state_is_tuple=True,\n      reuse=tf.get_variable_scope().reuse)\n```", "```py\nwith tf.device(\"/cpu:0\"):\n  embedding = tf.get_variable(\n      \"embedding\", [vocab_size, size], dtype=tf.float32)\n  inputs = tf.nn.embedding_lookup(embedding, input_.input_data)\n```", "```py\noutputs = []\nstate = self._initial_state\nwith tf.variable_scope(\"RNN\"):\n  for time_step in range(num_steps):\n    if time_step > 0: tf.get_variable_scope().reuse_variables()\n    (cell_output, state) = cell(inputs[:, time_step, :], state)\n    outputs.append(cell_output)\n```", "```py\n# use the contrib sequence loss and average over the batches\nloss = tf.contrib.seq2seq.sequence_loss(\n   logits,\n   input_.targets,\n   tf.ones([batch_size, num_steps], dtype=tf.float32),\n   average_across_timesteps=False,\n   average_across_batch=True\n)\n# update the cost variables\nself._cost = cost = tf.reduce_sum(loss)\n```"]