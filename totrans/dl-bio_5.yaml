- en: Chapter 5\. Detecting Skin Cancer in Medical Images
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章\. 在医学图像中检测皮肤癌
- en: In previous chapters, we focused on small-scale biological phenomena, such as
    the molecular properties of proteins, DNA sequences, and drug molecules. In this
    chapter, we will zoom out to a larger biological scale, applying deep learning
    to analyze tissue-level and disease-related processes. Specifically, we will train
    a skin cancer detection model to classify images of skin into various cancerous
    or benign categories.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们专注于小规模的生物现象，例如蛋白质的分子特性、DNA 序列和药物分子。在本章中，我们将扩大到更大的生物尺度，将深度学习应用于分析组织水平和与疾病相关的过程。具体来说，我们将训练一个皮肤癌检测模型，以对皮肤图像进行各种癌症或良性类别的分类。
- en: This is an exciting application because deep learning models have made significant
    strides in skin analysis, with studies achieving dermatologist-level accuracy
    in distinguishing malignant from benign lesions since at least 2018.^([1](ch05.html#id819))
    While challenges remain in integrating these models into clinical workflows—such
    as regulatory approval, data standardization, and prediction explainability—their
    potential to assist medical professionals by enhancing early detection and reducing
    unnecessary biopsies is highly promising.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个令人兴奋的应用，因为深度学习模型在皮肤分析方面取得了重大进展，自2018年以来，已有研究达到了皮肤科医生在区分良性病变和恶性病变方面的准确性。尽管将这些模型整合到临床工作流程中仍存在挑战——例如监管批准、数据标准化和预测可解释性——但它们通过增强早期检测和减少不必要的活检来协助医疗专业人士的潜力非常值得期待。
- en: We will be using skin cancer image data from the [International Skin Imaging
    Collaboration (ISIC)](https://oreil.ly/h2DiY), a project dedicated to advancing
    skin cancer imaging research and providing standardized datasets. Over the years,
    ISIC has released a range of challenges focused on skin lesion classification
    and pathology, with an increasing number of images available. To learn more, you
    can read this review paper of ISIC datasets and benchmarks.^([2](ch05.html#id820))
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自 [International Skin Imaging Collaboration (ISIC)](https://oreil.ly/h2DiY)
    的皮肤癌图像数据，这是一个致力于推进皮肤癌成像研究并提供标准化数据集的项目。多年来，ISIC 发布了一系列专注于皮肤病变分类和病理学的挑战，图像数量不断增加。要了解更多信息，您可以阅读
    ISIC 数据集和基准的这篇综述论文。[2](ch05.html#id820)
- en: The dataset we will use is available as the [“Skin Cancer ISIC” challenge on
    Kaggle](https://oreil.ly/_2jqU), making it well prepared and relatively easy to
    get started with. However, to ensure that we cover important lessons on handling
    real-world data challenges, we have intentionally chosen a dataset that is relatively
    small and has significant class imbalance, allowing us to explore techniques for
    mitigating these issues.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据集可在 [“Skin Cancer ISIC” challenge on Kaggle](https://oreil.ly/_2jqU)
    上找到，这使得它准备充分且相对容易开始。然而，为了确保我们涵盖处理现实世界数据挑战的重要课程，我们有意选择了一个相对较小且类别不平衡显著的数据集，这使我们能够探索缓解这些问题的技术。
- en: One advantage of working with image data is that humans are naturally skilled
    at interpreting visual information, enabling us to sanity-check both the dataset
    and model predictions. Throughout this chapter, we will examine many images to
    guide our modeling decisions. This will also highlight why skin cancer classification
    is a challenging problem—not only for humans, but for deep learning models as
    well.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 与图像数据一起工作的一个优点是，人类在解释视觉信息方面天生就有技能，这使得我们能够对数据集和模型预测进行合理性检查。在本章中，我们将检查许多图像以指导我们的建模决策。这还将突出为什么皮肤癌分类是一个具有挑战性的问题——不仅对人类，而且对深度学习模型也是如此。
- en: In terms of models, this chapter focuses on convolutional neural networks (CNNs)—specifically,
    ResNet CNNs, which have demonstrated strong performance across a wide range of
    image classification tasks. If you’d like to explore alternative approaches, consider
    checking out discussions and notebooks shared by other users on the [Kaggle discussion
    board](https://oreil.ly/bUHNt).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型方面，本章重点关注卷积神经网络（CNNs）——特别是表现出色于广泛图像分类任务的 ResNet CNNs。如果您想探索其他方法，可以考虑查看其他用户在
    [Kaggle 讨论板](https://oreil.ly/bUHNt) 上共享的讨论和笔记本。
- en: Tip
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: As always, to get the most out of this chapter, keep the companion Colab notebook
    from our repo open as you read. Experimenting with the code as you go will deepen
    your understanding and make the concepts stick.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 与往常一样，为了充分利用本章内容，请在阅读时保持我们仓库中的配套 Colab 笔记本打开。在前进的过程中尝试实验代码将加深你的理解并使概念更加牢固。
- en: Biology Primer
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物入门
- en: 'First, let’s introduce the biological phenomenon our models will address: skin
    cancer, its different types, and the challenges of classifying them.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们介绍我们的模型将解决的生物学现象：皮肤癌，其不同类型以及分类的挑战。
- en: Skin Cancer
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 皮肤癌
- en: Skin cancer is the most common type of cancer worldwide, with an estimated 1.5
    million new cases in 2022.^([3](ch05.html#id821)) It encompasses a wide range
    of conditions caused by the abnormal growth of skin cells, often driven by a combination
    of genetic factors and environmental carcinogens such as ultraviolet (UV) radiation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 皮肤癌是全球最常见的癌症类型，2022年估计有150万新病例。[3](ch05.html#id821)它包括由皮肤细胞异常生长引起的广泛条件，通常由遗传因素和环境致癌物（如紫外线辐射）的组合驱动。
- en: 'In this chapter, we will examine both malignant (cancerous) and benign lesions.
    The term *lesion* broadly refers to any mark or abnormality on the skin, ranging
    from harmless growths to those requiring medical intervention. The following are
    some of the most common types:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将检查恶性（癌症性）和良性病变。术语*病变*广义上指皮肤上的任何标记或异常，从无害的生长到需要医疗干预的生长。
- en: Malignant skin cancers
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 恶性皮肤癌
- en: Basal cell carcinoma
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 基底细胞癌
- en: The most common type of skin cancer, usually slow-growing and rarely spreading
    to other organs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 皮肤癌中最常见的类型，通常生长缓慢，很少扩散到其他器官。
- en: Squamous cell carcinoma
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 鳞状细胞癌
- en: Another common type, typically localized but capable of becoming invasive if
    left untreated.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见类型，通常局限于局部，但如果不治疗，可能会变得侵袭性。
- en: Melanoma
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 黑色素瘤
- en: The deadliest form of skin cancer, known for its ability to metastasize (spread
    to other parts of the body) quickly.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 皮肤癌中最致命的形式，以其快速转移（扩散到身体其他部位）的能力而闻名。
- en: Actinic keratosis
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 日光性角化病
- en: A precancerous lesion caused by sun damage. While not malignant, it can progress
    to squamous cell carcinoma if left untreated.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由阳光损伤引起的癌前病变。虽然不是恶性的，但如果不治疗，可能会进展为鳞状细胞癌。
- en: Benign skin lesions
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 良性皮肤病变
- en: Dermatofibroma
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 皮肤纤维瘤
- en: A firm, benign growth often found on the legs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一种坚硬的良性生长，通常在腿上发现。
- en: Nevus (mole)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 痣（母斑）
- en: A common benign growth that varies in size, shape, and color.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的良性生长，大小、形状和颜色各异。
- en: Pigmented benign keratosis
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 痣性良性角化病
- en: A noncancerous pigmented lesion, often resembling seborrheic keratosis.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非癌性色素性病变，通常类似于炎症性角化病。
- en: Seborrheic keratosis
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 炎症性角化病
- en: A benign, wartlike growth that can appear brown, black, or tan. Sometimes called
    “age spots” or “wisdom warts,” these are often found in older adults.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一种良性、疣状的生长，可以是棕色、黑色或黄褐色。有时被称为“老年斑”或“智慧疣”，这些通常在老年人中找到。
- en: Vascular lesions
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 血管病变
- en: Benign vascular growths such as hemangiomas and cherry angiomas, formed by abnormal
    blood vessel proliferation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由异常血管增殖形成的良性血管生长，如血管瘤和樱桃血管瘤。
- en: Many of these conditions can look quite similar. [Figure 5-1](#skin-lesion-types)
    shows example images from this chapter’s training dataset, illustrating the different
    lesion types.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些条件看起来相当相似。[图5-1](#skin-lesion-types)展示了本章训练数据集中的一些示例图像，说明了不同的病变类型。
- en: '![](assets/dlfb_0501.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0501.png)'
- en: Figure 5-1\. Grid displaying various types of skin lesions, both benign and
    malignant, from the dataset used for classification in this chapter.
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. 展示了用于本章分类的数据集中各种皮肤病变（良性和恶性）的网格。
- en: As you can probably imagine from looking at these example images, misclassifications
    are a key challenge in skin cancer detection. These errors arise because different
    lesion types can share similar visual characteristics, such as pigmentation, texture,
    or irregular borders. The most critical errors occur when melanoma is misclassified
    as a benign lesion (a false negative), potentially delaying life-saving treatment.
    Conversely, false positives, such as benign growths mistaken for melanoma, are
    less serious but can lead to unnecessary biopsies and patient anxiety.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从查看这些示例图像中，你可能可以想象到，误分类是皮肤癌检测中的一个关键挑战。这些错误是由于不同的病变类型可以共享相似的外观特征，如色素沉着、质地或不规则边缘。最关键的错误是将黑色素瘤误分类为良性病变（假阴性），可能会延迟救命治疗。相反，假阳性，如将良性生长误认为是黑色素瘤，虽然不太严重，但可能导致不必要的活检和患者焦虑。
- en: At the same time, there can be significant visual differences within a single
    class. For example, [Figure 5-2](#diverse-melanomas) shows how melanomas can vary
    widely in appearance, making classification even more challenging.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，同一类别内也可能存在显著的视觉差异。例如，[图5-2](#diverse-melanomas)展示了黑色素瘤在外观上可以有很大的变化，这使得分类变得更加困难。
- en: '![](assets/dlfb_0502.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0502.png)'
- en: Figure 5-2\. Melanomas can exhibit a wide range of visual characteristics, making
    consistent classification difficult. Some may appear dark brown or black with
    irregular borders, while others are lighter, reddish, or even patchy in color.
    Certain cases show a crusty texture, whereas others present as smooth, flat lesions.
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2\. 黑色素瘤可以表现出广泛的视觉特征，使得一致的分类变得困难。一些可能呈现深棕色或黑色，边缘不规则，而另一些则较浅，呈红色或甚至斑驳。某些病例显示有结痂的质地，而另一些则表现为光滑、平坦的病变。
- en: This visual variability stems from underlying biological differences—including
    the type of cells involved, how deep or aggressively the lesion grows, and how
    much pigment is produced. For melanoma specifically, changes in melanin production
    and growth pattern can result in widely varying appearances, even within the same
    class.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这种视觉变化源于潜在的生物学差异——包括涉及的细胞类型、病变生长的深度或侵略性以及产生的色素量。对于黑色素瘤而言，黑色素产生和生长模式的变化可能导致广泛不同的外观，即使在同一类别中也是如此。
- en: Causes and Risk Factors
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原因和风险因素
- en: Skin cancer develops when genetic mutations disrupt the normal regulation of
    skin cell growth and division. The most common trigger for these mutations is
    UV radiation—primarily from sunlight or artificial sources like tanning beds—which
    damages cellular DNA over time. If this damage isn’t properly repaired—for example,
    when the cell’s DNA repair systems make errors or become overwhelmed—it can lead
    to uncontrolled cell growth and, ultimately, tumor formation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 皮肤癌的发展是由于基因突变破坏了皮肤细胞生长和分裂的正常调节。这些突变的最常见触发因素是紫外线——主要来自阳光或人工来源，如日光浴床，随着时间的推移会损害细胞DNA。如果这种损伤没有得到适当的修复——例如，当细胞的DNA修复系统出错或不堪重负时——它可能导致细胞生长失控，最终形成肿瘤。
- en: 'Several factors increase the risk of developing skin cancer:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 几个因素会增加患皮肤癌的风险：
- en: '*Fair skin* contains less melanin, the pigment that provides some natural protection
    against UV damage.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*白皙的皮肤*含有较少的黑色素，这种色素可以提供一些对紫外线损伤的自然保护。'
- en: '*Frequent sunburns*, especially in childhood, suggest repeated UV-induced damage,
    which can accumulate over a lifetime.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*频繁晒伤*，尤其是在儿童时期，表明反复的紫外线诱导的损伤，这些损伤可能累积一生。'
- en: '*A high number of moles* (especially atypical or dysplastic moles) can reflect
    underlying instability in melanocyte behavior, increasing the chance that one
    could turn cancerous.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*大量痣*（尤其是不典型或发育不良的痣）可能反映黑素细胞行为的潜在不稳定，增加其癌变的机会。'
- en: '*Family history* may point to inherited genetic susceptibilities, such as mutations
    in tumor suppressor genes.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*家族史*可能指向遗传性基因易感性，例如肿瘤抑制基因中的突变。'
- en: '*Environmental carcinogens*, like arsenic or industrial chemicals, can also
    contribute to mutational burden.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*环境致癌物*，如砷或工业化学品，也可能导致突变负担。'
- en: In addition to external factors, *specific genetic mutations*—such as in the
    *BRAF* gene—are commonly found in melanoma. These mutations may arise spontaneously
    or in response to environmental triggers like UV radiation, and they play a key
    role in driving tumor growth. Importantly, understanding these mutations has enabled
    the development of targeted therapies that are tailored to a patient’s individual
    tumor profile, marking a shift toward more personalized cancer treatment.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 除了外部因素外，*特定的基因突变*——如*BRAF*基因中的突变——在黑色素瘤中很常见。这些突变可能自发产生或在环境触发因素（如紫外线辐射）的作用下产生，并在驱动肿瘤生长中发挥关键作用。重要的是，对这些突变的理解已经使得开发针对患者个体肿瘤特征的靶向疗法成为可能，标志着向更个性化的癌症治疗转变。
- en: How Skin Cancer Is Diagnosed
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 皮肤癌是如何被诊断的
- en: In clinical settings, dermatologists diagnose skin cancer through visual examination,
    dermoscopy, and biopsy. *Dermoscopy* is a noninvasive imaging technique that magnifies
    subsurface skin structures to help distinguish benign from malignant lesions.
    The images in commonly used skin cancer classification datasets primarily consist
    of dermoscopic images, captured using specialized *dermatoscopes* equipped with
    a light source and measurement markers, rather than standard cameras.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在临床环境中，皮肤科医生通过视觉检查、皮肤镜检查和活检来诊断皮肤癌。*皮肤镜检查*是一种非侵入性成像技术，通过放大皮肤下结构来帮助区分良性病变和恶性病变。常用皮肤癌分类数据集中的图像主要是由配备光源和测量标记的专用*皮肤镜*捕获的皮肤镜图像，而不是标准相机。
- en: If a lesion appears suspicious, a *biopsy* is performed—a small tissue sample
    is taken and examined under a microscope to detect cellular abnormalities, such
    as irregular nuclei, atypical cell shapes, disorganized tissue architecture, or
    uncontrolled mitotic activity. These features help confirm whether the lesion
    is malignant and determine its type and stage.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果病变看起来可疑，就会进行活检——取一小块组织样本并在显微镜下检查，以检测细胞异常，如不规则核、不典型细胞形状、组织结构紊乱或不受控制的细胞分裂活动。这些特征有助于确认病变是否为恶性，并确定其类型和阶段。
- en: The ABCDE rule (Asymmetry, Border irregularity, Color variation, Diameter >6
    mm, Evolving changes) helps assess lesions for signs of melanoma. AI models can
    assist by analyzing dermoscopic and clinical images to flag high-risk lesions
    for further evaluation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ABCDE 规则（不对称性、边缘不规则性、颜色变化、直径 >6 毫米、演变变化）有助于评估病变以寻找黑色素瘤的迹象。AI 模型可以通过分析皮肤镜和临床图像来协助标记高风险病变以进行进一步评估。
- en: Image-Based Skin Cancer Detection
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于图像的皮肤癌检测
- en: Deep learning has revolutionized image-based skin cancer detection, with AI
    models now achieving diagnostic accuracy comparable to expert dermatologists.
    A 2024 meta-analysis^([4](ch05.html#id828)) of 53 studies found that AI consistently
    outperformed general practitioners and less experienced dermatologists in distinguishing
    melanoma from benign lesions, while performing on par with specialists. This suggests
    that AI can serve as a valuable diagnostic aid, enhancing early detection and
    decision making.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已经彻底改变了基于图像的皮肤癌检测，AI 模型现在达到的诊断准确性可与专家皮肤科医生相媲美。一项 2024 年的 53 研究综述^([4](ch05.html#id828))
    发现，AI 在区分黑色素瘤和良性病变方面，始终优于普通开业医生和经验较少的皮肤科医生，同时与专家的表现相当。这表明 AI 可以作为有价值的诊断辅助工具，增强早期检测和决策。
- en: However, integrating AI into real-world clinical practice remains challenging.
    Most skin cancer models are trained on a handful of public datasets (e.g., ISIC,
    HAM10000), which lack diversity in skin types and imaging conditions, limiting
    generalizability. Additionally, AI performance in controlled, retrospective studies
    often does not translate to real-world settings, where factors like lighting,
    lesion presentation, and physician workflows introduce variability.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将 AI 集成到现实世界的临床实践中仍然具有挑战性。大多数皮肤癌模型都是在少数公开数据集（例如，ISIC、HAM10000）上训练的，这些数据集在皮肤类型和成像条件上缺乏多样性，限制了其泛化能力。此外，AI
    在受控的回顾性研究中的表现往往无法转化为现实世界环境，其中光照、病变表现和医生工作流程等因素引入了变异性。
- en: Another key barrier is explainability. Clinicians need to understand *why* a
    model makes a specific prediction, not just receive an isolated probability score.
    For example, if an AI predicts a skin lesion to be melanoma, is it due to the
    asymmetry, irregular borders, or color variation? And how are these different
    factors combined and weighted by the model? Machine learning methods like saliency
    maps and attention mechanisms help visualize what the model is focusing on, but
    they remain imperfect. Without clear reasoning, AI recommendations are difficult
    to trust or integrate into medical decision making.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键障碍是可解释性。临床医生需要了解模型做出特定预测的原因，而不仅仅是收到一个孤立的概率分数。例如，如果 AI 预测一个皮肤病变为黑色素瘤，这是由于不对称性、不规则边缘还是颜色变化？以及这些不同因素是如何被模型组合和加权的？像显著性图和注意力机制这样的机器学习方法有助于可视化模型关注的对象，但它们仍然不完美。没有清晰的推理，AI
    建议难以信任或整合到医疗决策中。
- en: Note
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Regulatory approval and clinical validation are major hurdles for deploying
    AI in healthcare. Diagnostic models must meet rigorous safety, accuracy, and transparency
    standards before receiving approval from regulatory bodies like the FDA or CE.
    This typically involves extensive clinical trials, reproducibility testing, and
    post-deployment monitoring. Moreover, AI tools must be integrated into existing
    workflows without disrupting clinician judgment or introducing new biases—all
    while maintaining patient privacy and data security.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 监管批准和临床验证是部署 AI 在医疗保健中的主要障碍。诊断模型必须在获得 FDA 或 CE 等监管机构的批准之前，满足严格的安全性、准确性和透明度标准。这通常涉及广泛的临床试验、可重复性测试和部署后的监控。此外，AI
    工具必须集成到现有的工作流程中，而不会干扰临床医生的判断或引入新的偏见，同时保持患者隐私和数据安全。
- en: While challenges remain, AI is steadily moving toward real-world deployment,
    with some dermatology AI systems already *CE-marked*—a certification indicating
    compliance with EU safety and efficacy standards—allowing for clinical use in
    the European Union. Additionally, smartphone apps such as SkinVision and Miiskin
    offer AI-based skin lesion analysis. While these apps are usually not approved
    for clinical decision making, they can still provide risk assessments and encourage
    users to seek medical evaluation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然仍存在挑战，但人工智能正在稳步向实际部署迈进，一些皮肤病学人工智能系统已经获得*CE标志*——这是一个表明符合欧盟安全性和有效性标准的认证——允许在欧洲联盟进行临床使用。此外，SkinVision和Miiskin等智能手机应用程序提供基于人工智能的皮肤病变分析。虽然这些应用程序通常未经批准用于临床决策，但它们仍然可以提供风险评估并鼓励用户寻求医疗评估。
- en: Machine Learning Primer
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习入门
- en: 'In [Chapter 3](ch03.html#learning-the-logic-of-dna), we applied a CNN to model
    1D sequence data—specifically, DNA sequences. However, CNNs are more commonly
    used for 2D image processing, powering tasks such as:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.html#learning-the-logic-of-dna)中，我们应用卷积神经网络来模拟一维序列数据——具体来说，是DNA序列。然而，卷积神经网络更常用于二维图像处理，支持以下任务：
- en: Image classification
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类
- en: Assigning an image to a specific category, such as identifying whether it contains
    a dog or a cat
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像分配到特定类别，例如确定它是否包含狗或猫
- en: Object detection
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对象检测
- en: Detecting and localizing objects within an image, such as drawing a bounding
    box around a cat
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像中检测和定位对象，例如在猫周围绘制边界框
- en: Segmentation
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 分割
- en: Partitioning an image into meaningful regions, such as labeling all pixels that
    belong to a cat
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像分割成有意义的区域，例如标记所有属于猫的像素
- en: 'This section provides a brief primer on how CNNs work for images. As additional
    learning material, we recommend the 3Blue1Brown introductory video titled [“But
    what is a convolution?”](https://oreil.ly/k8zoM), which offers a visual and intuitive
    explanation of CNNs. For a more in-depth exploration, the renowned [Stanford CS
    231n: Convolutional Neural Networks for Visual Recognition course](https://oreil.ly/C_wPx)
    provides a comprehensive introduction to the field.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本节简要介绍了卷积神经网络如何处理图像。作为额外的学习材料，我们推荐3Blue1Brown的入门视频“[但什么是卷积？](https://oreil.ly/k8zoM)”，它提供了卷积神经网络的直观和视觉解释。对于更深入的探索，著名的[斯坦福CS
    231n：视觉识别的卷积神经网络课程](https://oreil.ly/C_wPx)提供了该领域的全面介绍。
- en: Convolutional Neural Networks
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: 'CNNs are a specialized type of neural network designed for grid-like data,
    such as images (or sequences, as seen in [Chapter 3](ch03.html#learning-the-logic-of-dna)).
    They automatically learn *hierarchical* patterns, meaning they extract features
    at different levels of abstraction:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络是一种专门为网格状数据设计的神经网络，如图像（或序列，如第3章[第3章](ch03.html#learning-the-logic-of-dna)中所示）。它们自动学习*层次*模式，这意味着它们在不同抽象级别提取特征：
- en: Early layers in the neural network detect simple patterns like edges, textures,
    and color contrasts.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络中的早期层检测简单模式，如边缘、纹理和颜色对比。
- en: Middle layers recognize shapes and structures by combining these basic features.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中间层通过组合这些基本特征来识别形状和结构。
- en: Deeper layers build on these to identify complex objects or meaningful categories.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深层层基于这些来识别复杂对象或有意义类别。
- en: For example, in skin cancer detection, early CNN layers may detect edges and
    color variations in skin lesions, mid-level layers might recognize irregular borders
    or asymmetry, and the latest layers would combine these features into high-level
    learned patterns to distinguish between benign and malignant lesions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在皮肤癌检测中，早期的卷积神经网络层可能检测到皮肤病变的边缘和颜色变化，中级层可能识别出不规则边界或不对称性，而最新的层会将这些特征组合成高级学习模式，以区分良性病变和恶性病变。
- en: Warning
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Although we describe CNNs as learning hierarchical representations, it’s important
    to avoid *anthropomorphizing* them. CNNs don’t “see” objects the way humans do.
    Instead, they learn statistical patterns in pixel values that maximize predictive
    accuracy. Techniques like *activation mapping* (highlighting which parts of an
    image influence a classification) and *probing* (examining what kinds of features
    different layers encode) help us understand and visualize correlations within
    the model. However, these methods provide post hoc insights for human interpretation—they
    don’t imply that the model itself has a structured or explainable reasoning process.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们将卷积神经网络描述为学习层次化的表示，但避免将它们拟人化是很重要的。卷积神经网络并不像人类那样“看到”物体。相反，它们通过学习像素值中的统计模式来最大化预测准确性。像**激活映射**（突出显示哪些图像部分影响分类）和**探测**（检查不同层编码的特征类型）这样的技术帮助我们理解和可视化模型内部的关联。然而，这些方法为人类解释提供了事后洞察——它们并不暗示模型本身具有结构化或可解释的推理过程。
- en: Understanding a Convolution
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解卷积
- en: The core building block of a CNN is the *convolutional layer*, which applies
    *filters* (also called kernels) to extract features from the input image. A filter
    is a small grid of numbers that slides across the image, detecting local patterns
    such as edges, textures, or color transitions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络的核心构建块是**卷积层**，它将**过滤器**（也称为核）应用于输入图像以提取特征。过滤器是一个数字的小网格，它在图像上滑动，检测局部模式，如边缘、纹理或颜色过渡。
- en: An image is represented as a grid of pixels—in grayscale images, each pixel
    holds a single intensity value (ranging from 0 for black to 255 for white), while
    color images typically have three channels (usually red, green, and blue or *RGB*),
    each with its own intensity map.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图像被表示为像素的网格——在灰度图像中，每个像素包含一个强度值（从0的黑色到255的白色），而彩色图像通常有三个通道（通常是红色、绿色和蓝色或*RGB*），每个通道都有自己的强度图。
- en: 'As a filter moves across the image, it performs a *dot product* operation at
    each position: the filter values are multiplied element-wise with the corresponding
    pixel values in the image patch underneath, and the results are summed. This produces
    a single output value per position, building a new representation called a *feature
    map*, that highlights where the pattern encoded by the filter appears in the image.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当过滤器在图像上移动时，它在每个位置执行**点积**操作：过滤器值与图像块下对应的像素值逐元素相乘，然后将结果相加。这为每个位置产生一个输出值，构建一个新的表示，称为**特征图**，它突出显示过滤器编码的图案在图像中的位置。
- en: 'The specific values in the filter determine what it detects. For example, the
    following 3 × 3 filter emphasizes vertical *edges* by responding strongly to changes
    in pixel intensity along the horizontal (*x*) axis:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤器中的特定值决定了它检测的内容。例如，以下3 × 3过滤器通过在水平（*x*）轴上对像素强度变化做出强烈反应来强调垂直**边缘**：
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>[</mo> <mtable><mtr><mtd><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>[</mo> <mtable><mtr><mtd><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
- en: When applied to an image by sliding across it and computing the dot product
    with pixel values, this filter enhances areas where pixel intensity changes vertically,
    such as object boundaries, making it useful for edge detection.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当通过在图像上滑动并计算像素值的点积时，这个过滤器增强了像素强度垂直变化的区域，如物体边界，这使得它对边缘检测很有用。
- en: 'Similarly, a filter designed to detect horizontal edges responds to changes
    in pixel intensity along the vertical (*y*) axis. It typically looks like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，一个设计用于检测水平边缘的过滤器对垂直（*y*）轴上的像素强度变化做出反应。它通常看起来像这样：
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>[</mo> <mtable><mtr><mtd><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd> <mtd><mrow><mo>-</mo> <mn>1</mn></mrow></mtd> <mtd><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>[</mo> <mtable><mtr><mtd><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd> <mtd><mrow><mo>-</mo> <mn>1</mn></mrow></mtd> <mtd><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
- en: This filter activates in regions where there is a strong transition from dark
    to light (or vice versa) from top to bottom, highlighting horizontal structures
    in the image.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 该滤波器在从上到下从暗到亮（或反之）有强烈过渡的区域激活，突出图像中的水平结构。
- en: In a way, this concept is related to the idea of “filters” in social media,
    which often apply simple mathematical transformations (such as increasing contrast
    or sharpening details) to modify an image’s appearance.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种程度上，这个概念与社交媒体中“过滤器”的概念相关，它们通常会应用简单的数学变换（例如增加对比度或锐化细节）来修改图像的外观。
- en: Note
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Before CNNs, in a task called *feature engineering*, researchers manually designed
    and optimized these filter matrices to detect edges, textures, and other features—a
    labor-intensive process. Now, neural networks learn these filters automatically,
    optimizing them for the task at hand.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN之前，在称为*特征工程*的任务中，研究人员手动设计和优化这些滤波器矩阵以检测边缘、纹理和其他特征——这是一个劳动密集型过程。现在，神经网络自动学习这些滤波器，并针对当前任务进行优化。
- en: A single convolutional layer doesn’t just apply one filter; it typically learns
    multiple filters (e.g., 64) in parallel. Each filter captures different features,
    producing multiple *feature maps*, which are stacked together as separate channels
    in the layer’s output.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 单个卷积层不仅仅应用一个滤波器；它通常并行学习多个滤波器（例如，64个）。每个滤波器捕捉不同的特征，产生多个*特征图*，这些特征图作为单独的通道堆叠在层的输出中。
- en: Understanding Dimensions
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解维度
- en: If a grayscale image of size 256 × 256 (with a single channel) is passed through
    a convolutional layer with 64 filters, the output will have dimensions `(256,
    256, 64)`—assuming padding is used to preserve spatial dimensions. The height
    and width remain unchanged, while the channel dimension expands, as each of the
    64 filters extracts different feature representations from the image. This is
    true regardless of the filter size (e.g., 3 × 3, 5 × 5), as long as the stride
    and padding settings maintain spatial dimensions.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个256 × 256（单通道）的灰度图像通过一个有64个滤波器的卷积层，输出将具有维度`(256, 256, 64)`——假设使用填充来保留空间维度。高度和宽度保持不变，而通道维度扩展，因为每个64个滤波器从图像中提取不同的特征表示。这适用于任何滤波器大小（例如，3
    × 3，5 × 5），只要步长和填充设置保持空间维度。
- en: Note
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that neural network filters are usually randomly initialized, meaning
    they start off extracting no meaningful patterns. Through backpropagation, the
    filters gradually learn to detect useful visual features such as edges, textures,
    or shapes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，神经网络滤波器通常是随机初始化的，这意味着它们一开始无法提取任何有意义的模式。通过反向传播，滤波器逐渐学会检测有用的视觉特征，如边缘、纹理或形状。
- en: After convolution, a nonlinearity (such as the ReLU activation function) is
    typically applied to the feature maps. This is crucial because convolution alone
    is just a linear operation, meaning that without a nonlinearity, stacking multiple
    layers would be equivalent to just one big matrix multiplication. Adding the activation
    function allows the network to learn more complex, nonlinear patterns.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积之后，通常会对特征图应用非线性（例如ReLU激活函数）。这一点至关重要，因为卷积本身只是一个线性操作，意味着如果没有非线性，堆叠多层就相当于一个大的矩阵乘法。添加激活函数允许网络学习更复杂、非线性的模式。
- en: Now let’s consider a color image with dimensions `(256, 256, 3)`, where the
    three channels correspond to red, green, and blue (RGB). How does convolution
    work when an image has multiple input channels?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑一个尺寸为`(256, 256, 3)`的彩色图像，其中三个通道对应于红色、绿色和蓝色（RGB）。当图像有多个输入通道时，卷积是如何工作的呢？
- en: 'Unlike grayscale images, where each filter operates on a single channel, a
    convolutional filter in a color image must process all three channels simultaneously.
    Instead of being a simple 3 × 3 matrix, each filter is actually a 3 × 3 × 3 tensor,
    meaning:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与灰度图像不同，其中每个滤波器只对一个通道进行操作，彩色图像中的卷积滤波器必须同时处理所有三个通道。它不是一个简单的3 × 3矩阵，每个滤波器实际上是一个3
    × 3 × 3张量，这意味着：
- en: Each 3 × 3 slice of the filter is applied to the corresponding color channel
    (red, green, or blue) of the image.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滤波器中的每个3 × 3切片都应用于图像的相应颜色通道（红色、绿色或蓝色）。
- en: The results from all three channels are summed to produce a single output value
    per pixel. Typically, this is not a normal sum but actually a *weighted sum* where
    each channel’s contribution is multiplied by a separate learned weight.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有三个通道的结果相加，以生成每个像素的单个输出值。通常，这不仅仅是一个普通的和，而实际上是一个*加权和*，其中每个通道的贡献都乘以一个单独学习的权重。
- en: This process is repeated for all filters, creating multiple feature maps in
    the next layer.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个过程对所有滤波器重复进行，在下一层创建多个特征图。
- en: For example, applying a convolutional layer with 64 filters to an input of shape
    `(256, 256, 3)` results in an output of `(256, 256, 64)`, where each filter has
    combined the three input channels in different ways to extract meaningful patterns.
    We would then apply an activation function such as a ReLU as before.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，将具有64个滤波器的卷积层应用于形状为`(256, 256, 3)`的输入，结果得到一个形状为`(256, 256, 64)`的输出，其中每个滤波器以不同的方式组合了三个输入通道，以提取有意义的模式。然后我们像之前一样应用激活函数，例如ReLU。
- en: Tip
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'You may have noticed we called a 3 × 3 filter a *matrix*, but a 3 × 3 × 3 filter
    a *tensor*. To briefly clarify the terminology:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到我们称3 × 3滤波器为*矩阵*，但3 × 3 × 3滤波器为*张量*。为了简要说明术语：
- en: A *scalar* is a single number and can be seen as a 0D tensor (e.g., 5).
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*标量*是一个单独的数字，可以看作是0D张量（例如，5）。
- en: A *vector* is a 1D tensor (e.g., `[1, 2, 3]`).
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*向量*是一维张量（例如，`[1, 2, 3]`）。
- en: A *matrix* is a 2D tensor (e.g., a 3 × 3 filter).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*矩阵*是二维张量（例如，一个3 × 3滤波器）。
- en: A *tensor* is a general term for arrays of any number of dimensions, including
    3D+ structures like 3 × 3 × 3 filters.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*张量*是任何数量维度的数组的通用术语，包括3D+结构，如3 × 3 × 3滤波器。
- en: In short, tensors are the fundamental data structure in deep learning, generalizing
    scalars, vectors, and matrices to arbitrary dimensions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，张量是深度学习中的基本数据结构，将标量、向量和矩阵推广到任意维度。
- en: Pooling
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化
- en: Now that we’ve covered what convolutions do, let’s move on to *pooling layers*.
    Pooling reduces the spatial dimensions of feature maps, making computations more
    efficient and helping to prevent overfitting by keeping only the most prominent
    activations.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了卷积的作用，让我们继续介绍*池化层*。池化减少了特征图的空间维度，使计算更高效，并通过只保留最突出的激活来帮助防止过拟合。
- en: The most common type is max pooling, which selects the highest value in a given
    region of the feature map. This ensures that strong activations are preserved
    while reducing spatial resolution (downsampling).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的是最大池化，它选择特征图给定区域中的最高值。这确保了强激活被保留，同时减少了空间分辨率（下采样）。
- en: 'Consider a 4 × 4 feature map (the image-like output of a convolution) before
    applying 2 × 2 max pooling (with stride 2, meaning the filter moves two pixels
    at a time):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在应用2 × 2最大池化（步长为2，意味着滤波器每次移动两个像素）之前的一个4 × 4特征图（卷积的图像样输出）：
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>[</mo> <mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>3</mn></mtd> <mtd><mn>2</mn></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>4</mn></mtd>
    <mtd><mn>5</mn></mtd> <mtd><mn>7</mn></mtd> <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>3</mn></mtd>
    <mtd><mn>2</mn></mtd> <mtd><mn>9</mn></mtd> <mtd><mn>6</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>8</mn></mtd> <mtd><mn>6</mn></mtd> <mtd><mn>3</mn></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>[</mo> <mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>3</mn></mtd> <mtd><mn>2</mn></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>4</mn></mtd>
    <mtd><mn>5</mn></mtd> <mtd><mn>7</mn></mtd> <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>3</mn></mtd>
    <mtd><mn>2</mn></mtd> <mtd><mn>9</mn></mtd> <mtd><mn>6</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>8</mn></mtd> <mtd><mn>6</mn></mtd> <mtd><mn>3</mn></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
- en: 'After applying 2 × 2 max pooling, the highest value in each 2 × 2 block is
    retained, reducing the matrix to a size of 2 × 2:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用2 × 2最大池化后，保留每个2 × 2块中的最高值，将矩阵减少到2 × 2的大小：
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>[</mo> <mtable><mtr><mtd><mn>5</mn></mtd>
    <mtd><mn>7</mn></mtd></mtr> <mtr><mtd><mn>8</mn></mtd> <mtd><mn>9</mn></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>[</mo> <mtable><mtr><mtd><mn>5</mn></mtd>
    <mtd><mn>7</mn></mtd></mtr> <mtr><mtd><mn>8</mn></mtd> <mtd><mn>9</mn></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
- en: 'This greatly reduces the number of values while preserving the most important
    activations. Another common pooling type is *average pooling*, which takes the
    mean of each region instead of the max. This would result in this matrix:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这大大减少了值数量，同时保留了最重要的激活。另一种常见的池化类型是*平均池化*，它取每个区域的平均值而不是最大值。这将得到这个矩阵：
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>[</mo> <mtable><mtr><mtd><mrow><mn>3</mn>
    <mo lspace="0%" rspace="0%">.</mo> <mn>25</mn></mrow></mtd> <mtd><mrow><mn>3</mn>
    <mo lspace="0%" rspace="0%">.</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>3</mn>
    <mo lspace="0%" rspace="0%">.</mo> <mn>5</mn></mrow></mtd> <mtd><mrow><mn>6</mn>
    <mo lspace="0%" rspace="0%">.</mo> <mn>0</mn></mrow></mtd></mtr></mtable> <mo>]</mo></mrow></mtd></mtr></mtable>
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>[</mo> <mtable><mtr><mtd><mrow><mn>3</mn>
    <mo lspace="0%" rspace="0%">.</mo> <mn>25</mn></mrow></mtd> <mtd><mrow><mn>3</mn>
    <mo lspace="0%" rspace="0%">.</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>3</mn>
    <mo lspace="0%" rspace="0%">.</mo> <mn>5</mn></mrow></mtd> <mtd><mrow><mn>6</mn>
    <mo lspace="0%" rspace="0%">.</mo> <mn>0</mn></mrow></mtd></mtr></mtable> <mo>]</mo></mrow></mtd></mtr></mtable>
- en: 'To summarize, pooling:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，池化：
- en: Improves computational efficiency by shrinking the representation
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过缩小表示来提高计算效率
- en: Retains key information by preserving prominent activations (like maxima or
    averages)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过保留显著的激活（如最大值或平均值）来保留关键信息
- en: Adds a mild form of regularization by making the model less sensitive to small
    shifts in the input
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使模型对输入的小幅变化不那么敏感，添加了一种轻微的正则化形式
- en: However, pooling is not a substitute for more robust regularization methods
    like dropout or weight decay.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，池化并不能替代像dropout或权重衰减这样更稳健的正则化方法。
- en: Other Components of a CNN
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN的其他组件
- en: 'Beyond convolution and pooling, several other key components help CNNs function
    effectively:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 除了卷积和池化之外，还有几个其他关键组件有助于CNN有效地工作：
- en: Activation functions
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数
- en: As mentioned earlier, a nonlinearity (typically ReLU) is applied after each
    convolutional layer. This enhances the model’s expressivity, enabling it to learn
    complex, nonlinear relationships.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在每个卷积层之后应用非线性（通常是ReLU）。这增强了模型的表达能力，使其能够学习复杂的非线性关系。
- en: Batch normalization
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 批标准化
- en: This normalizes feature maps across a batch to keep activations in a stable
    range. It helps CNNs train more efficiently, reduces sensitivity to weight initialization,
    and enables the use of higher learning rates—especially useful for deep architectures
    (and many CNNs are quite deep).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这将批次的特征图进行归一化，以保持激活在一个稳定的范围内。它有助于CNN更有效地训练，减少对权重初始化的敏感性，并允许使用更高的学习率——这对于深度架构（许多CNN相当深）特别有用。
- en: Dropout
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout
- en: This is a regularization technique where random activations are set to zero
    during training to prevent overfitting. This forces the network to rely on multiple
    pathways for making predictions, improving generalization.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种正则化技术，在训练期间将随机激活设置为0，以防止过拟合。这迫使网络依赖多个路径进行预测，从而提高泛化能力。
- en: Fully connected layers
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层
- en: After feature extraction, the final layers of a CNN are typically fully connected.
    These layers combine the learned feature representations and produce the final
    output—such as class probabilities in an image classification task.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征提取之后，CNN的最后一层通常是全连接层。这些层结合了学习到的特征表示，并产生最终的输出——例如，在图像分类任务中的类别概率。
- en: 'With these components combined, we’ve covered the building blocks needed to
    train a fully functioning CNN. Next, we’ll explore a widely used architecture
    that brings them all together: ResNets.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 结合这些组件，我们已经涵盖了训练一个完全功能的CNN所需的构建块。接下来，我们将探讨一个广泛使用的架构，它将它们全部结合起来：ResNets。
- en: ResNets
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ResNets
- en: A ResNet (residual network) is a CNN architecture introduced in 2015^([5](ch05.html#id848))
    that enables the training of very deep networks without suffering from the *vanishing
    gradient problem*, a common issue in which deeper networks struggle to propagate
    gradients effectively, slowing down learning. ResNets address this by introducing
    *residual connections* (also called *skip connections*), which allow information
    to bypass some layers. This stabilizes training and improves performance, making
    ResNets a go-to architecture for computer vision tasks like image classification
    and object detection.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet（残差网络）是一种在2015年提出的CNN架构^([5](ch05.html#id848))，它使得训练非常深的网络而不会遭受**梯度消失问题**，这是一个深层网络难以有效传播梯度的常见问题，会减慢学习速度。ResNets通过引入**残差连接**（也称为**跳跃连接**）来解决这一问题，允许信息绕过一些层。这稳定了训练并提高了性能，使得ResNets成为计算机视觉任务（如图像分类和目标检测）的首选架构。
- en: 'The key idea behind a residual connection is simple: instead of learning a
    full transformation `f(x)`, the network learns `f(x) + x`, where `x` is the original
    input. This means that if `f(x)` is small or difficult to learn, the network can
    still default to simply passing through the input `x` unchanged (an identity function).
    This prevents layers from degrading the performance of deeper networks, effectively
    “skipping” operations that don’t contribute meaningful improvements.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接背后的关键思想很简单：不是学习完整的变换`f(x)`，网络学习`f(x) + x`，其中`x`是原始输入。这意味着如果`f(x)`很小或难以学习，网络仍然可以默认简单地通过输入`x`而不改变（一个恒等函数）。这防止了层降低深层网络的性能，有效地“跳过”了没有带来有意义改进的操作。
- en: 'Here’s a simplified implementation of a residual block in pseudocode:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个残差块在伪代码中的简化实现：
- en: '[PRE0]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This simple skip connection allows gradients to flow more easily through the
    network, making training deep architectures much more feasible.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的跳跃连接允许梯度更容易地通过网络流动，使得训练深层架构变得更加可行。
- en: 'We can rewrite this pseudocode in Flax using the Linen API:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Flax的Linen API重写以下伪代码：
- en: '[PRE1]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following are some notes on the key arguments to `nn.Conv`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对`nn.Conv`关键参数的一些说明：
- en: '`features`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`features`'
- en: Defines the number of learned filters. If `features=100`, an input `(224, 224,
    3)` becomes `(224, 224, 100)`, extracting 100 different feature maps.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了学习到的滤波器数量。如果`features=100`，输入`(224, 224, 3)`变为`(224, 224, 100)`，提取100个不同的特征图。
- en: '`kernel_size`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`kernel_size`'
- en: Defines the receptive field of the convolution. A small kernel like `(3, 3)`
    captures fine details, while a larger kernel like `(16, 16)` detects broader patterns.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了卷积的感受野。小的核如`(3, 3)`可以捕捉到细微的细节，而大的核如`(16, 16)`可以检测到更广泛的模式。
- en: '`strides`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`strides`'
- en: Controls the step size of the filter. `strides=(1, 1)` preserves resolution,
    while larger strides downsample the feature maps. Downsampling is more commonly
    done via pooling layers than by increasing the stride.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 控制滤波器的步长。`strides=(1, 1)`保留分辨率，而更大的步长会下采样特征图。下采样通常通过池化层而不是增加步长来实现。
- en: '`padding`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`padding`'
- en: Defines the padding for consistent shapes. `padding="SAME"` ensures that output
    dimensions match the input by adding necessary padding.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了保持一致形状的填充。`padding="SAME"`通过添加必要的填充确保输出维度与输入匹配。
- en: 'With this in mind, let’s implement a more complete residual block with the
    following additions:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，让我们实现一个更完整的残差块，以下是一些新增功能：
- en: Two stacked convolutional layers to extract deeper features
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个堆叠的卷积层以提取更深的特征
- en: A ReLU activation function to introduce nonlinearity
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个ReLU激活函数以引入非线性
- en: Batch normalization to stabilize training
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批标准化以稳定训练
- en: A check for channel mismatches, applying a 1 × 1 convolution when necessary
    to ensure compatibility for addition
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查通道不匹配，在必要时应用1 × 1卷积以确保加法兼容性。
- en: '[PRE2]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This implementation is now much closer to what you’ll encounter in real-world
    architectures.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现现在更接近你在现实世界的架构中会遇到的情况。
- en: 'With this foundation in CNNs and ResNets, we can now apply these techniques
    to our goal: building a deep learning model for skin cancer prediction.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN和ResNets的基础上，我们现在可以将这些技术应用到我们的目标上：构建一个用于皮肤癌预测的深度学习模型。
- en: Exploring the Data
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据
- en: As always, before building a model, we first need to understand our dataset.
    In this chapter, we will use data from [ISIC](https://oreil.ly/g2EyR), an initiative
    that fosters collaboration between medical professionals and AI researchers. ISIC
    regularly hosts machine learning challenges, encouraging researchers to develop
    and submit models for classifying skin lesion images.^([6](ch05.html#id863))
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 正如往常一样，在构建模型之前，我们首先需要了解我们的数据集。在本章中，我们将使用来自[ISIC](https://oreil.ly/g2EyR)的数据，这是一个促进医疗专业人士和AI研究人员之间合作的倡议。ISIC定期举办机器学习挑战，鼓励研究人员开发和提交用于分类皮肤病变图像的模型。[6](ch05.html#id863))
- en: The ISIC archive is a freely accessible repository containing tens of thousands
    of skin lesion images, making it a valuable resource for developing and benchmarking
    AI-based diagnostic tools. You can explore the dataset through their online portal,
    as shown in [Figure 5-3](#isic-screenshot).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ISIC存档是一个免费可访问的存储库，包含数万张皮肤病变图像，使其成为开发和使用AI诊断工具的宝贵资源。您可以通过他们的在线门户探索数据集，如图5-3所示。
- en: '![](assets/dlfb_0503.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0503.png)'
- en: Figure 5-3\. A [screenshot](https://oreil.ly/F6BwK) displaying a vast collection
    of 81,722 public skin lesion images.
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-3\. 展示了81,722张公共皮肤病变图像的大量集合的[屏幕截图](https://oreil.ly/F6BwK)。
- en: A First Glimpse
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初步了解
- en: Rather than using the ISIC dataset directly, we are working with a version hosted
    on [Kaggle](https://oreil.ly/cez_1). This allows us to explore the initial steps
    of working with a new dataset, including essential sanity checks to ensure its
    integrity.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是直接使用ISIC数据集，而是在[Kaggle](https://oreil.ly/cez_1)上托管的一个版本上工作。这使我们能够探索使用新数据集的初始步骤，包括确保其完整性的基本合理性检查。
- en: Warning
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: A key advantage of using a Kaggle dataset is that we can compare our approach
    to existing work and establish reasonable performance expectations. However, caution
    is needed—while Kaggle notebooks can be a great source of inspiration, they are
    not peer reviewed and may contain serious errors.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kaggle数据集的一个关键优势是，我们可以将我们的方法与现有工作进行比较，并建立合理的性能预期。然而，需要谨慎——虽然Kaggle笔记本可以是一个很好的灵感来源，但它们未经同行评审，可能包含严重的错误。
- en: For example, some models reported impressive performance on this dataset, but
    closer inspection revealed *data leakage*—where images from the training set also
    appeared in validation or test sets, leading to artificially inflated accuracy.
    In one extreme case, we even found a model that evaluated only on training images,
    making its results completely meaningless.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一些模型在这个数据集上报告了令人印象深刻的性能，但仔细检查揭示了*数据泄露*——训练集中的图像也出现在验证或测试集中，导致准确性人为地提高。在一个极端案例中，我们甚至发现了一个仅在训练图像上评估的模型，使其结果完全无意义。
- en: 'Let’s ensure that we build a robust data pipeline by carefully inspecting the
    dataset before proceeding. We start by exploring the raw dataset directory to
    understand its structure. By listing all *.jpg* files, we can get an initial impression
    of the available labels, which correspond to different types of skin lesions.
    The `rglob` method is particularly useful here, as it scans directories recursively:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过在继续之前仔细检查数据集来确保我们构建了一个健壮的数据管道。我们首先探索原始数据集目录，以了解其结构。通过列出所有*.jpg*文件，我们可以获得对可用标签的初步印象，这些标签对应于不同的皮肤病变类型。`rglob`方法在这里特别有用，因为它递归地扫描目录：
- en: '[PRE3]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Output:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE4]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Examining the filepath, we can see that the dataset has already been split into
    `Train` and `Test` sets, with subdirectories for each skin lesion type.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 检查文件路径，我们可以看到数据集已经被分割成`Train`和`Test`集，并为每种皮肤病变类型创建了子目录。
- en: 'Next, we will count the number of images per class across both splits. To do
    this efficiently, we define a helper function, `load_metadata`, that:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算两个分割中每个类别的图像数量。为此，我们定义了一个辅助函数`load_metadata`，它：
- en: Recursively collects all image filepaths
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归收集所有图像文件路径
- en: Extracts the dataset split (`Train`/`Test`) and class label from each path
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从每个路径中提取数据集分割（`Train`/`Test`）和类别标签
- en: Stores the results in a pandas `DataFrame` for easy inspection and visualization
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将结果存储在pandas的`DataFrame`中，以便于检查和可视化
- en: We will also track `frame_id`s, which serve as a reference for quickly retrieving
    specific images during our later sanity checks.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将跟踪`frame_id`，这将在我们后续的合理性检查中作为快速检索特定图像的参考。
- en: '[PRE5]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Output:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE6]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we count the number of images per class in both splits using the pandas
    `crosstab` function:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用pandas的`crosstab`函数计算两个分割中每个类别的图像数量：
- en: '[PRE7]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Output:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE8]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This dataset is relatively small, containing a total of 2,357 images, split
    into 2,239 for training and 118 for testing. Additionally, the class distribution
    is highly imbalanced, with some classes having very few examples to train on.
    Such an imbalance poses challenges for model generalization and requires careful
    handling to prevent biased predictions. It also makes evaluation less reliable,
    as performance metrics based on very few examples can be noisy and unrepresentative.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集相对较小，总共包含2,357张图像，分为2,239张用于训练和118张用于测试。此外，类别的分布高度不平衡，一些类别在训练上几乎没有示例。这种不平衡对模型的泛化能力构成挑战，需要谨慎处理以防止产生偏差预测。这也使得评估不太可靠，因为基于非常少的示例的性能指标可能会很嘈杂且不具有代表性。
- en: 'We visualize this distribution in [Figure 5-4](#class-vs-split-barplot):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[图5-4](#class-vs-split-barplot)中可视化了这种分布：
- en: '[PRE9]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](assets/dlfb_0504.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0504.png)'
- en: Figure 5-4\. Bar plot of the distribution of classes across original training
    and test sets.
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-4\. 原始训练和测试集中类别分布的条形图。
- en: 'We can draw several key insights from the bar plot of class counts:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从类别计数的条形图中得出几个关键见解：
- en: Class imbalance
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 类别不平衡
- en: Some categories, like pigmented benign keratosis, are overrepresented (454 total
    images), while others, like seborrheic keratosis, are severely underrepresented
    (80 total images). This imbalance could bias the model toward predicting the more
    frequent classes.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一些类别，如色素性良性角化病，过度代表（总共有454张图像），而其他类别，如脂溢性角化病，严重不足（总共有80张图像）。这种不平衡可能导致模型偏向于预测更频繁的类别。
- en: Small test set
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 小型测试集
- en: Some categories have as few as three images in the test set, making it difficult
    to evaluate model performance across different lesion types. Such a small test
    set can easily lead to misleading performance metrics. We can also see that the
    ratio of train to test data is not even across lesion classes.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集中一些类别只有三张图像，这使得评估不同病变类型的模型性能变得困难。如此小的测试集很容易导致误导性的性能指标。我们还可以看到，训练数据与测试数据之间的比例在病变类别中并不均匀。
- en: No validation set
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 没有验证集
- en: The dataset only provides `Train` and `Test` splits, but no `Valid` subset.
    A validation set is crucial for tuning hyperparameters and assessing model improvements
    without touching the final test set.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集仅提供`Train`和`Test`分割，但没有`Valid`子集。验证集对于调整超参数和评估模型改进至关重要，而不必触及最终的测试集。
- en: To effectively train a model on this dataset, we’ll need to address these points
    using techniques like data augmentation and resampling. Before diving into these,
    let’s visually inspect some images to better understand the dataset. This will
    help us verify that the images match the lesion types introduced earlier in this
    chapter.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地在这个数据集上训练模型，我们需要使用数据增强和重采样等技术来解决这些问题。在深入探讨这些技术之前，让我们先视觉检查一些图像，以便更好地理解数据集。这将帮助我们验证图像是否与本章早期介绍的病变类型相匹配。
- en: Note
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Another important consideration is that this dataset consists solely of images
    and labels, without additional metadata such as lesion location, patient demographics,
    or clinical notes. This lack of context makes the classification task more challenging,
    as real-world diagnosis often relies on more than just the visual appearance of
    a lesion. For example, a dark lesion on the scalp of a 70-year-old patient may
    increase the probability of melanoma, since both age and sun-exposed areas are
    known risk factors.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的考虑因素是，这个数据集仅由图像和标签组成，没有额外的元数据，例如病变位置、患者人口统计信息或临床笔记。这种缺乏上下文使得分类任务更具挑战性，因为现实世界的诊断往往不仅仅依赖于病变的视觉外观。例如，一位70岁患者的头皮上暗色的病变可能会增加黑色素瘤的可能性，因为年龄和阳光暴露区域都是已知的危险因素。
- en: Previewing the Images
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预览图像
- en: This is the first time in this book that we’re working with image data, so let’s
    take a moment to explore how to handle it in Python. A common library for loading
    and processing images is Pillow, the modern version of the original PIL (Python
    Imaging Library). It retains the `PIL` module name for compatibility and is widely
    used for image handling.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，这是我们第一次处理图像数据，所以让我们花点时间探索如何在Python中处理它。一个常见的用于加载和处理图像的库是Pillow，它是原始PIL（Python
    Imaging Library）的现代版本。它保留了`PIL`模块名称以保持兼容性，并且广泛用于图像处理。
- en: 'Now let’s take a look at one of the images. We’ll start with a filepath we
    retrieved earlier, as shown in [Figure 5-5](#first-single-image):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一张图像。我们将从之前检索到的文件路径开始，如图[图5-5](#first-single-image)所示：
- en: '[PRE10]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](assets/dlfb_0505.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0505.png)'
- en: Figure 5-5\. An example skin lesion image loaded using Pillow in Python.
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-5\. 使用Python中的Pillow加载的皮肤病变图像示例。
- en: 'We’d like to inspect images from specific lesion classes. To make this process
    easier, we’ll create a function that:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想检查特定病变类别的图像。为了使这个过程更容易，我们将创建一个函数：
- en: Randomly selects an image from a specified class in the dataset
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据集中随机选择指定类别的图像
- en: Loads the image using `PIL` (Pillow)
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`PIL`（Pillow）加载图像。
- en: Displays it with Matplotlib, including the class name as a title for clarity
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Matplotlib显示它，包括类别名称作为标题以提高清晰度
- en: '[PRE11]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s now use this code to examine an example melanoma image, as shown in [Figure 5-6](#first-single-melanoma):'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在使用这段代码来检查一个黑色素瘤图像的例子，如图[图5-6](#first-single-melanoma)所示：
- en: '[PRE12]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](assets/dlfb_0506.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0506.png)'
- en: Figure 5-6\. Example image from the “melanoma” class.
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-6\. “黑色素瘤”类别的示例图像。
- en: 'Next, let’s write a `plot_random_image_grid` to help us visualize a sample
    image from each skin lesion class. This will help us quickly get a sense of the
    dataset’s diversity and variation in lesion appearance:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们编写一个`plot_random_image_grid`来帮助我们可视化每个皮肤病变类别的样本图像。这将帮助我们快速了解数据集的多样性和病变外观的变化：
- en: '[PRE13]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s run this function (see [Figure 5-7](#skin-image-grid)):'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行这个函数（见图[图5-7](#skin-image-grid)）：
- en: '[PRE14]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](assets/dlfb_0507.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0507.png)'
- en: Figure 5-7\. Grid of skin images with their corresponding labels.
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-7\. 皮肤图像及其对应标签的网格。
- en: With a better visual understanding of our dataset, we’re now ready to build
    a flexible input pipeline that will support the rest of our experiments.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据集有了更好的视觉理解之后，我们现在准备构建一个灵活的输入管道，这将支持我们其余的实验。
- en: Addressing Dataset Issues
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决数据集问题
- en: Now that we’ve explored the dataset, let’s address some of its limitations.
    Since improving model performance will be an iterative process, it’s helpful to
    create a `DatasetBuilder` class that makes it easy to experiment with different
    dataset configurations. We’ll design it to be flexible enough to support both
    multiclass classification (all lesion types) and binary classification (e.g.,
    melanoma versus non-melanoma), allowing us to explore a variety of setups as we
    develop our models.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了数据集，让我们来讨论一下它的局限性。由于提高模型性能将是一个迭代的过程，创建一个`DatasetBuilder`类来方便地实验不同的数据集配置是有帮助的。我们将设计它以足够灵活，支持多类别分类（所有病变类型）和二进制分类（例如，黑色素瘤与非黑色素瘤），这样我们就可以在我们开发模型的过程中探索各种设置。
- en: 'First, to ensure proper evaluation, we define a 70/20/10 split for `train`,
    `valid`, and `test` sets:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了确保适当的评估，我们为`train`、`valid`和`test`集定义了70/20/10的分割：
- en: '[PRE15]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Output:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE16]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We have visualized the class distribution across the new splits in [Figure 5-8](#fig5-8):'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[图5-8](#fig5-8)中可视化了新分割中的类别分布：
- en: '[PRE17]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](assets/dlfb_0508.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0508.png)'
- en: Figure 5-8\. Bar plot of the distribution of classes across new training, validation,
    and test sets.
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-8\. 新训练、验证和测试集中类别分布的条形图。
- en: With this, we now have a better-structured dataset split—featuring a dedicated
    validation set, a larger test set, and consistent train/valid/test proportions
    across lesion classes. This sets the stage for building our `DatasetBuilder`,
    which will allow us to efficiently experiment with different training configurations.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在有一个更好的数据集分割——包括一个专门的验证集，更大的测试集，以及病变类别之间一致的训练/验证/测试比例。这为构建我们的`DatasetBuilder`奠定了基础，它将使我们能够高效地实验不同的训练配置。
- en: Balancing the batches
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平衡批次
- en: Some skin lesion classes are much more common than others in our dataset. For
    example, pigmented benign keratosis and nevus are heavily represented, while classes
    like seborrheic keratosis and dermatofibroma are much rarer. If we train directly
    on this imbalanced data, the model may focus on predicting the dominant classes
    correctly while ignoring the rare ones—a bias that can hurt generalization.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集中，一些皮肤病变类别比其他类别更常见。例如，色素性良性角化病和痣在数据集中有很高的代表性，而像脂溢性角化病和皮肤纤维瘤这样的类别则非常罕见。如果我们直接在这不平衡的数据上训练，模型可能会专注于正确预测主导类别，而忽略罕见类别——这种偏差可能会损害泛化。
- en: To mitigate this, we implemented a *balanced sampler* during training. This
    ensures that each batch contains an equal number of examples from each class,
    giving the model more exposure to underrepresented categories. The validation
    and test sets, however, maintain their original class distributions to reflect
    real-world class frequencies during evaluation.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这一点，我们在训练期间实施了一个*平衡采样器*。这确保了每个批次包含来自每个类别的相等数量的示例，使模型有更多机会接触代表性不足的类别。然而，验证和测试集保持其原始类别分布，以反映评估期间现实世界的类别频率。
- en: Although the balanced sampler didn’t lead to measurable gains in our experiments
    for this chapter, it remains a useful tool when working with severely imbalanced
    datasets, which we encourage you to try in your future projects.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在本章的实验中，平衡采样器没有导致可测量的收益，但它仍然是一个在处理严重不平衡数据集时非常有用的工具，我们鼓励你在未来的项目中尝试使用。
- en: Augmenting the dataset
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 增强数据集
- en: Our dataset is relatively small, and in deep learning, few things improve model
    performance more reliably than having access to more high-quality data. Since
    collecting new labeled images is expensive and time-consuming, we turn to *data
    augmentation*—a technique that synthetically increases dataset size by applying
    label-preserving transformations to existing images.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集相对较小，在深度学习中，没有多少事情能比获取更多高质量数据更能可靠地提高模型性能。由于收集新的标记图像既昂贵又耗时，我们转向了*数据增强*——一种通过对现有图像应用标签保留变换来合成增加数据集大小的技术。
- en: Augmentation encourages the model to generalize better by exposing it to natural
    variability in image appearance. For example, random rotations, flips, crops,
    color shifts, and brightness changes teach the model that the class of a lesion
    doesn’t change just because its lighting or orientation does. This helps reduce
    overfitting and improves robustness to real-world image variation.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 增强通过使模型接触到图像外观的自然变异性来鼓励模型更好地泛化。例如，随机旋转、翻转、裁剪、颜色变换和亮度变化教会模型，病变的类别不会因为其照明或方向的变化而改变。这有助于减少过拟合并提高对现实世界图像变化的鲁棒性。
- en: However, it’s important to remember that augmentation does *not* introduce truly
    new information. Augmented examples are variations of the same underlying data—so
    any existing dataset biases (e.g., skin tone imbalance or limited anatomical diversity)
    will still be present. What augmentation does offer is a way to nudge the model
    toward learning more general, abstract features rather than memorizing surface-level
    details.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，重要的是要记住，增强并不真正引入新的信息。增强示例是相同基础数据的变体——因此，任何现有的数据集偏差（例如，肤色不平衡或有限的解剖多样性）仍然存在。增强提供的是一种方法，可以引导模型学习更普遍、更抽象的特征，而不是记住表面细节。
- en: Rather than manually applying transformations, we use the [DeepMind PIX library](https://oreil.ly/3FxiK)—a
    JAX-compatible image augmentation toolkit. It integrates cleanly into our pipeline
    and allows us to dynamically apply transformations on the fly during training,
    keeping memory usage low and training efficient.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是手动应用变换，而是使用[DeepMind PIX库](https://oreil.ly/3FxiK)—一个兼容 JAX 的图像增强工具包。它干净地集成到我们的流程中，并允许我们在训练期间动态地即时应用变换，保持内存使用量低，提高训练效率。
- en: 'In our setup, each image is randomly transformed with a fixed probability.
    As with other JAX operations, the random number generator (`rng`) is explicitly
    managed, ensuring that augmentations are reproducible and traceable:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的设置中，每张图像都会以固定的概率随机变换。与其他 JAX 操作一样，随机数生成器（`rng`）被明确管理，确保增强是可重复和可追踪的：
- en: '[PRE18]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To illustrate augmentation in practice, we select a melanoma image and apply
    our transformation pipeline multiple times. Each variation is slightly different,
    simulating realistic image variability. See the result in [Figure 5-9](#skin-image-grid-augmentation):'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明增强的实际应用，我们选择了一张黑色素瘤图像并多次应用我们的变换流程。每个变体都有所不同，模拟了现实图像的变异性。请参见[图 5-9](#skin-image-grid-augmentation)的结果：
- en: '[PRE19]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](assets/dlfb_0509.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0509.png)'
- en: Figure 5-9\. Original skin image (top left) followed by different augmented
    versions, demonstrating transformations like flipping, rotation, and color shifts.
  id: totrans-250
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-9\. 原始皮肤图像（左上角）随后是不同的增强版本，展示了翻转、旋转和颜色变换等变换。
- en: This setup is a good starting point, but keep in mind that each augmentation
    method has tunable parameters—and the [PIX documentation](https://oreil.ly/74UjB)
    lists many more options. Selecting and tuning augmentations is effectively a hyperparameter
    search. What works best depends heavily on the dataset and task. For a broader
    overview of effective augmentation strategies, see this survey.^([7](ch05.html#id870))
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置是一个良好的起点，但请记住，每种增强方法都有可调整的参数——[PIX文档](https://oreil.ly/74UjB)列出了许多其他选项。选择和调整增强实际上是一个超参数搜索。什么效果最好很大程度上取决于数据集和任务。有关有效增强策略的更广泛概述，请参阅这份调查.^([7](ch05.html#id870))
- en: Preprocessing the images
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理图像
- en: Before feeding images into a neural network, we need to standardize them. This
    involves ensuring that all inputs have the same shape and pixel value range. Without
    this step, differences in resolution, aspect ratio, or intensity could prevent
    the model from learning consistent patterns.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在将图像输入神经网络之前，我们需要对它们进行标准化。这涉及到确保所有输入具有相同的形状和像素值范围。如果没有这一步，分辨率、宽高比或强度的差异可能会阻止模型学习一致的模式。
- en: Many deep learning computer vision models—including ImageNet-trained ResNets
    we will work with in this chapter—expect fixed-size square input images. However,
    skin lesion photos vary widely in shape and size. Simply resizing these directly
    to a square can distort important clinical features. To avoid this, we first resize
    the image while preserving its aspect ratio, then center-crop a square from the
    result. This produces consistent input shapes without stretching or compressing
    lesions.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 许多深度学习计算机视觉模型——包括我们在本章中将使用的ImageNet训练的ResNets——都期望固定大小的正方形输入图像。然而，皮肤病变照片在形状和大小上差异很大。直接将这些图像调整到正方形可能会扭曲重要的临床特征。为了避免这种情况，我们首先调整图像的大小，同时保持其宽高比，然后从结果中裁剪出一个正方形。这会产生一致的输入形状，而不会拉伸或压缩病变。
- en: '[PRE20]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We also need to normalize pixel values. Raw image intensities usually range
    from 0 to 255, but neural networks tend to train more effectively when inputs
    are scaled to a smaller, consistent range. A common method is *min-max scaling*,
    where each pixel is divided by 255 to bring values into the `[0, 1]` range. Another
    option is standardization, where each pixel is transformed to have zero mean and
    unit variance.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要归一化像素值。原始图像强度通常在0到255之间，但神经网络在输入缩放到更小、更一致的范围时训练效果更好。一种常见的方法是*最小-最大缩放*，其中每个像素被255除，将值带入
    `[0, 1]` 范围。另一种选择是标准化，其中每个像素被转换为零均值和单位方差。
- en: Warning
  id: totrans-257
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'The term *standardization* is overloaded in this context and refers to two
    different concepts:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，术语*标准化*是过载的，它指的是两个不同的概念：
- en: '*General image preprocessing*: Ensuring consistency in image size, format,
    and value ranges'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通用图像预处理*：确保图像大小、格式和值范围的一致性'
- en: '*A specific normalization strategy*: Transforming pixel values to have a mean
    of 0 and a standard deviation of 1'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特定的归一化策略*：将像素值转换为具有0均值和1标准差'
- en: Throughout this book, we’ll use *standardization* in the first sense—as a synonym
    for general input preprocessing.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用*标准化*的第一个含义——作为一般输入预处理的同义词。
- en: 'For our model,we’ll use min-max scaling to normalize pixel values, dividing
    by 255 to bring intensities into the `[0, 1]` range. This approach is widely used
    in CNN training and works well for image data:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的模型，我们将使用最小-最大缩放来归一化像素值，通过除以255将强度带入 `[0, 1]` 范围。这种方法在CNN训练中广泛使用，并且对图像数据效果良好：
- en: '[PRE21]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now let’s apply all preprocessing steps—reading JPEG images, resizing while
    preserving aspect ratio, center-cropping, and normalizing—to ensure that our dataset
    is standardized before training. While we could choose any image size, we’ll use
    224 × 224 pixels to match the expected input size of the original ResNet architecture,
    which we’ll explore later in the chapter. Since these transformations need to
    be applied only once, doing them in advance improves efficiency and avoids redundant
    computations during training.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们应用所有预处理步骤——读取JPEG图像、保持宽高比调整大小、中心裁剪和归一化——以确保在训练之前我们的数据集是标准化的。虽然我们可以选择任何图像大小，但我们将使用224
    × 224像素来匹配原始ResNet架构期望的输入大小，我们将在本章后面探讨这一点。由于这些转换只需要应用一次，提前进行可以提高效率并避免在训练期间进行冗余计算。
- en: 'In [Figure 5-10](#image-preprocessing-for-standarization), you can see an example
    of an image before and after preprocessing:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图5-10](#image-preprocessing-for-standarization)中，你可以看到预处理前后图像的示例：
- en: '[PRE22]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](assets/dlfb_0510.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0510.png)'
- en: Figure 5-10\. Images are standardized to ensure consistent dimensions and pixel
    value ranges. Note that although the preprocessed image has been rescaled from
    0-255 to 0-1, `imshow` automatically adjusts the display scale, so the visual
    appearance remains unchanged.
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-10。图像被标准化以确保一致的尺寸和像素值范围。请注意，尽管预处理过的图像已经从0-255缩放到0-1，但`imshow`会自动调整显示比例，因此视觉外观保持不变。
- en: With data preprocessing topics complete, we can now focus on efficiently storing
    and accessing the data—a crucial step for scaling training without overloading
    system memory.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成数据预处理主题后，我们现在可以专注于高效地存储和访问数据——这是在不超载系统内存的情况下扩展训练的关键步骤。
- en: Data storage with memory-mapped arrays
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用内存映射数组进行数据存储
- en: 'We’ve already emphasized the importance of efficiency in model training: faster
    data processing means faster iterations and ultimately better models. A major
    inefficiency we want to avoid is repeatedly preprocessing the same data every
    time a batch is passed to the model. At the same time, storing all processed images
    in memory isn’t feasible. So, how can we balance efficiency and memory constraints?'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经强调了模型训练中效率的重要性：更快的数据处理意味着更快的迭代，最终得到更好的模型。我们想要避免的一个主要低效问题是每次将批次传递给模型时都重复预处理相同的数据。同时，将所有处理过的图像存储在内存中也是不可行的。那么，我们如何平衡效率和内存限制呢？
- en: A practical solution is *memory-mapped NumPy arrays*. These allow us to store
    preprocessed images on disk while accessing them as if they were in memory, making
    retrieval fast and efficient. This way, we preprocess all images once before training
    and then load them dynamically during training without redundant computation.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 一个实用的解决方案是*内存映射NumPy数组*。这允许我们将预处理过的图像存储在磁盘上，同时像它们在内存中一样访问它们，使检索快速高效。这样，我们在训练前一次性预处理所有图像，然后在训练期间动态加载它们，而不进行冗余计算。
- en: 'The implementation is straightforward: we first define the storage file, specify
    the data type (`float32`), and set the shape of the dataset: 224 × 224 pixels
    with three color channels. This creates an empty file on disk, preallocating the
    required storage space. We then preprocess and store each image in the memory-mapped
    array:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 实现很简单：我们首先定义存储文件，指定数据类型（`float32`），并设置数据集的形状：224 × 224像素，三个颜色通道。这将在磁盘上创建一个空文件，预分配所需的存储空间。然后我们在内存映射数组中预处理和存储每个图像：
- en: '[PRE23]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This approach gives us the best of both worlds: efficient preprocessing without
    excessive memory consumption.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法让我们得到了两全其美的效果：高效的预处理而不会过度消耗内存。
- en: Building a DatasetBuilder
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建一个`DatasetBuilder`
- en: 'At this point, we have tackled the major challenges of working with this dataset:
    splitting it into training, validation, and test sets; handling class imbalances;
    preprocessing images; and augmenting the data. Now we will organize these components
    into a structured `DatasetBuilder` class that returns `Dataset` instances that
    have the data iterators that will actually fetch us batches of data that we can
    train or evaluate a model on.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经解决了使用这个数据集的主要挑战：将其分为训练集、验证集和测试集；处理类别不平衡；预处理图像；以及数据增强。现在，我们将这些组件组织成一个结构化的`DatasetBuilder`类，该类返回具有数据迭代器的`Dataset`实例，这些迭代器将实际为我们获取可以用于训练或评估模型的批次数据。
- en: Tip
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Designing a flexible dataset builder pays off when iterating on your model.
    It lets you quickly try different label mappings, sampling strategies, and preprocessing
    steps—all without rewriting core code. This modularity speeds up experimentation
    and helps you pinpoint which data choices actually improve performance.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 设计一个灵活的数据集构建器在迭代模型时会有回报。它让你可以快速尝试不同的标签映射、采样策略和预处理步骤——而无需重写核心代码。这种模块化加快了实验速度，并帮助你确定哪些数据选择实际上提高了性能。
- en: 'Let’s take a closer look at the `DatasetBuilder`, which coordinates the creation
    of dataset splits and preprocessing:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看`DatasetBuilder`，它协调数据集分割和预处理的创建：
- en: '[PRE24]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We’ve already implemented each of these components, but let’s highlight a few
    key details:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实现了这些组件中的每一个，但让我们强调一些关键细节：
- en: Metadata loading
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据加载
- en: '`MetadataLoader` ensures that all image filepaths and their associated class/split
    information are consistently saved to a CSV file. This avoids inconsistencies
    from relying on `rglob`, which does not guarantee a consistent file order. Because
    the image data is later stored in a memory-mapped array in the same order as the
    metadata, maintaining this consistency is crucial.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`MetadataLoader` 确保所有图像文件路径及其相关的类/分割信息都一致地保存到 CSV 文件中。这避免了依赖于 `rglob` 导致的不一致性，因为
    `rglob` 不能保证文件顺序的一致性。由于图像数据稍后以与元数据相同的顺序存储在内存映射数组中，保持这种一致性至关重要。'
- en: Flexible class mapping
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 灵活的类映射
- en: The `MetadataLoader` also supports binary or grouped classification setups via
    a customizable `class_map`. If no map is provided, it defaults to a standard multiclass
    configuration based on the directory structure.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`MetadataLoader` 通过可定制的 `class_map` 支持二进制或分组分类设置。如果没有提供映射，它将默认为基于目录结构的标准多类配置。'
- en: Image preprocessing
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图像预处理
- en: '`ImageLoader` handles reading, processing, and storing images in memory-mapped
    arrays. To avoid recomputation, images are stored in a compact raw format once,
    and then multiple preprocessing strategies (e.g., resizing or cropping) can be
    applied and stored separately as memmaps.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`ImageLoader` 负责在内存映射数组中读取、处理和存储图像。为了避免重复计算，图像一旦存储为紧凑的原始格式，就可以应用多种预处理策略（例如，调整大小或裁剪），并将它们分别作为
    memmaps 存储起来。'
- en: Dataset creation
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集创建
- en: 'The `build` method brings everything together: it loads metadata and images,
    shuffles the dataset, splits it into train/validation/test partitions, and wraps
    each into a `Dataset` object. Each dataset contains consistent metadata and preprocessed
    image views, ready for training.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`build` 方法将所有内容整合在一起：它加载元数据和图像，对数据集进行洗牌，将其分割为训练/验证/测试分区，并将每个分区包装成一个 `Dataset`
    对象。每个数据集包含一致的元数据和预处理的图像视图，准备好进行训练。'
- en: Loading the Metadata and Images
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载元数据和图像
- en: You can find the code for the supporting classes for the `MetadataLoader` and
    `ImageLoader` in the `dlfb.cancer.dataset.builder` library. Here we will briefly
    describe how they work and fulfill their responsibility to create the `Dataset`.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 `dlfb.cancer.dataset.builder` 库中找到 `MetadataLoader` 和 `ImageLoader` 的支持类代码。在这里，我们将简要描述它们的工作原理以及它们如何履行创建
    `Dataset` 的职责。
- en: 'The `MetadataLoader` constructs a metadata CSV file if one does not already
    exist. It extracts the original split and class information from the folder structure,
    applies any desired label mappings, and provides consistent row-level access to
    the dataset. Image loading is a bit more involved. While `MetadataLoader` handles
    the filepaths and class labels, `ImageLoader` is responsible for reading, preprocessing,
    and storing the actual image data. Here are some notes on how `ImageLoader` works:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不存在，`MetadataLoader` 将构建一个元数据 CSV 文件。它从文件夹结构中提取原始分割和类信息，应用任何所需的标签映射，并提供对数据集的行级访问。图像加载稍微复杂一些。虽然
    `MetadataLoader` 处理文件路径和类标签，但 `ImageLoader` 负责读取、预处理和存储实际的图像数据。以下是关于 `ImageLoader`
    的工作原理的一些说明：
- en: Image preprocessing and storage
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图像预处理和存储
- en: To avoid redundant computations, `ImageLoader` first stores the raw images as
    a single memory-mapped array. This array is compact, storing flattened versions
    of each image along with their original shapes and offsets so they can later be
    reconstructed. Once this raw storage is complete, various preprocessing functions
    (such as cropping or resizing) can be applied and the results saved into additional
    memory-mapped files. This design allows us to support multiple preprocessing strategies
    without reloading the original JPEGs or repeating expensive operations.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免冗余计算，`ImageLoader` 首先将原始图像存储为一个单一的内存映射数组。这个数组紧凑，存储了每个图像的展平版本以及它们的原始形状和偏移量，以便稍后可以重建。一旦完成这个原始存储，就可以应用各种预处理函数（如裁剪或调整大小），并将结果保存到额外的内存映射文件中。这种设计使我们能够支持多种预处理策略，而无需重新加载原始的
    JPEG 图像或重复昂贵的操作。
- en: Preprocessing flexibility
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理灵活性
- en: Preprocessing functions are applied in a modular way. Any number of transforms
    can be passed into the `ImageLoader`, and each will produce its own memmap. These
    preprocessed views are stored under different keys (e.g., `"crop"`, `"resize"`)
    inside the `Images` object, which acts as a unified interface to access them.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理函数以模块化的方式应用。可以将任意数量的转换传递给 `ImageLoader`，每个转换都会生成自己的内存映射。这些预处理的视图存储在 `Images`
    对象的不同键下（例如，“crop”，“resize”），该对象作为统一的接口来访问它们。
- en: Dataset assembly
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集组装
- en: Finally, the `DatasetBuilder.build()` method pulls all of this together. It
    loads the metadata, processes the images, shuffles the dataset, splits it into
    training, validation, and test sets, and returns `Dataset` objects for each. These
    objects provide clean access to both image arrays and class labels, ready for
    model training.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`DatasetBuilder.build()`方法将这些组件整合在一起。它加载元数据，处理图像，对数据集进行洗牌，将其分为训练、验证和测试集，并为每个返回`Dataset`对象。这些对象提供了对图像数组和类标签的干净访问，为模型训练做好准备。
- en: 'This is all stored in the `Dataset` class itself:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都存储在`Dataset`类本身中：
- en: '[PRE25]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Where the `Images` are:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '`Images`在哪里：'
- en: '[PRE26]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `Dataset` class is the primary interface for accessing data during training.
    It manages both the `metadata` (which tracks image paths and labels) and the memory-mapped
    `images`. Since we split the dataset into training, validation, and test sets,
    we can control their proportions using the `splits` argument.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset`类是训练期间访问数据的初级接口。它管理`metadata`（跟踪图像路径和标签）和内存映射的`images`。由于我们将数据集分为训练、验证和测试集，我们可以使用`splits`参数控制它们的比例。'
- en: Batching the data
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据批处理
- en: 'To streamline dataset handling, during training, we introduce the `BatchHandler`
    class. This class encapsulates the components required to generate batches from
    a `Dataset` object:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化数据集处理，在训练期间，我们引入了`BatchHandler`类。这个类封装了从`Dataset`对象生成批次的所需组件：
- en: The `preprocessor` selects which preprocessed image view (e.g., cropped, resized)
    to use.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preprocessor`选择要使用的预处理图像视图（例如，裁剪、调整大小）。'
- en: The `sampler` determines how to draw samples for each batch (e.g., balanced
    versus random sampling).
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampler`确定如何为每个批次抽取样本（例如，平衡抽样与随机抽样）。'
- en: The optional `augmentor` applies real-time image transformations such as flipping,
    color jitter, or rotation to improve generalization.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可选的`augmentor`应用实时图像变换，如翻转、颜色抖动或旋转，以提高泛化能力。
- en: '[PRE27]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `get_batches` method is the core of `BatchHandler`. It’s a generator function,
    meaning it yields one batch at a time, which is memory-efficient and well-suited
    for training loops. Here’s what happens inside:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_batches`方法是`BatchHandler`的核心。它是一个生成器函数，意味着它一次产生一个批次，这既节省内存又非常适合训练循环。以下是内部发生的情况：'
- en: It validates the requested `batch_size`.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它验证请求的`batch_size`。
- en: It samples batch indices using the `sampler` function.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它使用`sampler`函数抽取批次索引。
- en: It fetches preprocessed images and labels from the `Dataset`.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它从`Dataset`中获取预处理图像和标签。
- en: If an `augmentor` is provided, it applies augmentation per image using JAX’s
    `vmap` for efficiency.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果提供了`augmentor`，它将使用JAX的`vmap`对每张图像进行增强，以提高效率。
- en: It yields batches as dictionaries containing `frame_ids`, `images`, and `labels`.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它产生包含`frame_ids`、`images`和`labels`的批次字典。
- en: This design cleanly separates dataset structure from augmentation and sampling
    logic, making it easy to experiment with different batching strategies during
    training.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计将数据集结构、增强和抽样逻辑清晰地分离，使得在训练期间实验不同的批处理策略变得容易。
- en: Readying the Dataset
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据集
- en: 'We’re now ready to construct our dataset and sanity-check the outputs of the
    data pipeline:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好构建我们的数据集并检查数据管道的输出：
- en: '[PRE28]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Output:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE29]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This call initializes the full data pipeline: it loads metadata; processes
    images; splits the dataset into training, validation, and test partitions; and
    returns them as `Dataset` objects (as keys in the `dataset_splits` dict).'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 此调用初始化完整的数据管道：它加载元数据；处理图像；将数据集分为训练、验证和测试分区；并以`Dataset`对象（作为`dataset_splits`字典中的键）返回它们。
- en: 'We can now use the `BatchHandler` to generate a batch from the training set:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`BatchHandler`从训练集中生成一个批次：
- en: '[PRE30]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Each batch contains three keys—`images`, `labels`, and `frame_ids`:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 每个批次包含三个键—`images`、`labels`和`frame_ids`：
- en: '`images`: A float32 tensor with normalized pixel values.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images`：一个浮点32张量，包含归一化的像素值。'
- en: '`labels`: Integer class labels corresponding to each image.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`：与每个图像对应的整数类标签。'
- en: '`frame_ids`: Indices into the original metadata, useful for inspection and
    debugging.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frame_ids`：原始元数据的索引，用于检查和调试。'
- en: 'Let’s confirm the shape of the `images` tensor:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确认`images`张量的形状：
- en: 'Output:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE31]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Each batch consists of `32` images, and we can see that each image has dimensions
    `(224, 224, 3)`.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 每个批次包含`32`张图像，我们可以看到每张图像的维度为`(224, 224, 3)`。
- en: 'Before moving forward, let’s visualize the first image in the batch to ensure
    our preprocessing pipeline has worked as intended. This simple sanity check helps
    catch any unexpected artifacts early on (see [Figure 5-11](#skin-lesion-sanity-check-image)):'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们可视化批处理中的第一张图像，以确保我们的预处理管道按预期工作。这个简单的合理性检查有助于早期捕捉任何意外的伪影（参见[图5-11](#skin-lesion-sanity-check-image)）：
- en: '[PRE33]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![](assets/dlfb_0511.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0511.png)'
- en: Figure 5-11\. A processed skin lesion image from our dataset, confirming that
    our pipeline is working correctly.
  id: totrans-338
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-11。来自我们数据集的处理后的皮肤病变图像，证实我们的管道工作正常。
- en: 'Yup, it still looks like some sort of skin lesion! With a fully functional
    dataset pipeline prepared and validated, we’re ready for the next step: training
    our model.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，它仍然看起来像某种皮肤病变！在准备并验证了完全功能的数据集管道后，我们准备进行下一步：训练我们的模型。
- en: Building Skin Cancer Classification Models
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建皮肤癌分类模型
- en: 'With our dataset ready, we now turn to the central modeling exploration of
    this chapter. We’ll compare a range of architectures, gradually increasing in
    complexity. Here’s an overview of the models we’ll build and evaluate:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集准备就绪后，我们现在转向本章的核心建模探索。我们将比较一系列架构，逐渐增加其复杂性。以下是我们将构建和评估的模型概述：
- en: '`SimpleCNN`'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '`SimpleCNN`'
- en: A lightweight two-layer convolutional neural network that serves as our baseline.
    It shares the same classification head (`SkinLesionClassifierHead`) as the other
    models but is otherwise kept minimal. We won’t tune it heavily—its purpose is
    to establish a performance baseline without any architectural sophistication.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 一个轻量级的两层卷积神经网络，作为我们的基线。它与其他模型共享相同的分类头（`SkinLesionClassifierHead`），但除此之外保持最小化。我们不会对其进行大量调整——其目的是在不涉及任何架构复杂性的情况下建立性能基线。
- en: '`ResNetFromScratch`'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '`ResNetFromScratch`'
- en: A full ResNet50 architecture trained from randomly initialized weights. This
    model assesses the impact of the architecture alone, without any benefits from
    pretraining on ImageNet. It helps isolate the contribution of the ResNet design
    itself.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 从随机初始化的权重训练的完整ResNet50架构。该模型评估了架构本身的影响，而没有从ImageNet预训练中获益。它有助于隔离ResNet设计本身的贡献。
- en: '`FinetunedResNet`'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '`FinetunedResNet`'
- en: A ResNet50 initialized with pretrained weights, with all layers fine-tuned on
    our skin lesion dataset. This approach leverages transfer learning, incorporating
    knowledge learned from large-scale datasets like ImageNet to improve convergence
    speed and generalization.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练权重初始化的ResNet50，所有层都在我们的皮肤病变数据集上微调。这种方法利用迁移学习，结合从大规模数据集（如ImageNet）学习到的知识，以提高收敛速度和泛化能力。
- en: '`FinetunedHeadResNet`'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '`FinetunedHeadResNet`'
- en: Similar to the previous item, but with the pretrained ResNet50 backbone frozen.
    Only the final classification head is trained. This variant tests how far we can
    get by leveraging pretrained features without modifying the backbone.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一项类似，但预训练的ResNet50骨干被冻结。只有最终的分类头被训练。这个变体测试了在不修改骨干的情况下，利用预训练特征能走多远。
- en: This progression—from scratch to full fine-tuning—lets us directly compare training
    efficiency and classification performance across strategies. Training from scratch
    gives full control but demands more data and compute. Transfer learning, especially
    when freezing the backbone, is typically faster and more robust in low-data regimes
    like ours.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这种从零开始到完全微调的进展，使我们能够直接比较不同策略的训练效率和分类性能。从头开始训练提供了完全的控制，但需要更多的数据和计算。迁移学习，尤其是在冻结骨干的情况下，在我们的低数据环境中通常更快、更稳健。
- en: Note
  id: totrans-351
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: ResNet50 is a widely used model convolutional architecture with 50 layers. You
    can easily experiment with deeper variants such as [ResNet101](https://oreil.ly/P8Cvr)
    or ResNet200\. These offer greater representational power but come at the cost
    of increased compute and a higher risk of overfitting—especially when training
    data is limited. All variants are compatible with the same ImageNet preprocessor.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet50是一个广泛使用的具有50层的卷积神经网络架构。你可以轻松地实验更深层的变体，如[ResNet101](https://oreil.ly/P8Cvr)或ResNet200。这些提供了更大的表示能力，但代价是计算量增加和过拟合风险更高——尤其是在训练数据有限的情况下。所有变体都与相同的ImageNet预处理器兼容。
- en: 'To implement these models, we’ll follow these steps:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这些模型，我们将遵循以下步骤：
- en: Load the ResNet50 model for Flax from transformers.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从transformers加载Flax的ResNet50模型。
- en: Extract the model backbone.
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取模型骨干。
- en: Attach a custom classification head (`SkinLesionClassifierHead`).
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加自定义分类头（`SkinLesionClassifierHead`）。
- en: Train and evaluate each model variant.
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练和评估每个模型变体。
- en: Let’s start by loading the pretrained ResNet50 model.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先加载预训练的 ResNet50 模型。
- en: Loading the Flax ResNet50 Model
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载 Flax ResNet50 模型
- en: Flax offers a [prebuilt ResNet implementation](https://oreil.ly/Yaf_p), which
    provides a flexible and well-optimized architecture for feature extraction. However,
    prebuilt models aren’t always available—or pretrained—depending on your needs.
    To demonstrate how to work with pretrained models in Flax, we’ll load a ResNet50
    model from Hugging Face instead of using the Flax example directly.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: Flax 提供了一个[预构建的 ResNet 实现](https://oreil.ly/Yaf_p)，它提供了一个灵活且经过优化的架构，用于特征提取。然而，根据你的需求，预构建的模型可能不可用或未预训练。为了演示如何在
    Flax 中处理预训练模型，我们将从 Hugging Face 加载一个 ResNet50 模型，而不是直接使用 Flax 的示例。
- en: The Hugging Face [`FlaxResNetModel`](https://oreil.ly/xWVhz) provides a convenient
    way to import and use pretrained ResNet models, making it easy to fine-tune or
    extract features from standard models.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 的 `FlaxResNetModel` 提供了一种方便的方式来导入和使用预训练的 ResNet 模型，使得微调或从标准模型中提取特征变得容易。
- en: 'To see how this works in practice, we’ll follow Hugging Face’s example code
    and apply a pretrained ResNet50 to a sample image. We’ve been looking at skin
    lesions for quite a while, so let’s take a break with something more cheerful:
    a photo of cats lounging on a couch (see [Figure 5-12](#cats-on-couch)):'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到实际操作中的效果，我们将遵循 Hugging Face 的示例代码，并将预训练的 ResNet50 应用到样本图像上。我们已经研究了很长时间的皮肤病变，所以让我们换一个更愉快的话题：一张猫在沙发上悠闲地躺着的照片（见图
    [5-12](#cats-on-couch)）：
- en: '[PRE34]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](assets/dlfb_0512.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0512.png)'
- en: Figure 5-12\. Cats on a couch.
  id: totrans-365
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-12\. 沙发上的猫。
- en: 'Loading a pretrained ResNet model from Hugging Face is straightforward. We’ll
    use a ResNet50 model trained on ImageNet and pair it with an image preprocessor
    to ensure that our input image is correctly formatted:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Hugging Face 加载预训练的 ResNet 模型非常简单。我们将使用在 ImageNet 上训练的 ResNet50 模型，并配对一个图像预处理程序，以确保我们的输入图像格式正确：
- en: '[PRE35]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now let’s preprocess the cat image to obtain the `pixel_values` input format
    expected by the model:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来预处理猫的图像，以获得模型期望的 `pixel_values` 输入格式：
- en: '[PRE36]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Output:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE37]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let’s inspect what the preprocessor does to input images in [Figure 5-13](#cats-on-couch-standardized):'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查预处理程序对输入图像在[图 5-13](#cats-on-couch-standardized) 中做了什么：
- en: '[PRE38]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](assets/dlfb_0513.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0513.png)'
- en: Figure 5-13\. Cats on a couch after image processor standardization.
  id: totrans-375
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-13\. 经过图像处理器标准化的沙发上的猫。
- en: 'The processed image looks noticeably different from the original; the colors
    have shifted, and the image has been cropped to 224 × 224 pixels. This transformation
    is essential when using pretrained weights: models expect inputs normalized to
    the mean and standard deviations of the dataset they were trained on. This standardization
    ensures consistent performance, even if your input data (like medical images)
    has different color distributions.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的图像与原始图像明显不同；颜色发生了偏移，图像已被裁剪为 224 × 224 像素。当使用预训练权重时，这种转换是必要的：模型期望输入数据被归一化到它们训练数据集的均值和标准差。这种标准化确保了性能的一致性，即使你的输入数据（如医学图像）具有不同的颜色分布。
- en: 'Here are two important guidelines:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个重要的指南：
- en: When fine-tuning a pretrained model, normalize inputs using the original training
    dataset’s statistics (e.g., ImageNet mean and std).
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当微调预训练模型时，使用原始训练数据集的统计信息（例如，ImageNet 均值和标准差）来归一化输入。
- en: When training from scratch, you can normalize using your own dataset’s range
    or statistics (mean and standard deviations).
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当从头开始训练时，你可以使用自己的数据集范围或统计信息（均值和标准差）进行归一化。
- en: 'Now let’s make a prediction:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来进行预测：
- en: '[PRE39]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Output:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE40]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Success! Well, sort of—the model classifies the image as the class “tiger cat.”
    While not a perfect match, it’s a reasonable guess given the image. This highlights
    both the strengths and limitations of pretrained models: they excel at recognizing
    familiar patterns but may struggle with patterns outside of original training
    data distribution.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！好吧，算是吧——模型将图像分类为“虎猫”这一类别。虽然不是完美的匹配，但考虑到图像内容，这是一个合理的猜测。这突出了预训练模型的优点和局限性：它们擅长识别熟悉的模式，但可能难以处理原始训练数据分布之外的图案。
- en: 'Let’s now explore how we can reuse the core architecture of this model while
    adapting it to a new task: classifying skin lesions.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探讨如何重用该模型的核心架构，同时将其适应到新的任务：皮肤病变的分类。
- en: Extracting the ResNet Backbone
  id: totrans-386
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取 ResNet 骨干网络
- en: 'As mentioned, the pretrained ResNet50 model we’re using has been trained on
    ImageNet to classify images into one of 1,000 classes such as “tiger cat,” “airplane,”
    and “fire truck.” You might wonder: is melanoma one of the classes? Unfortunately,
    it is not. So how can we still use this model?'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，我们使用的预训练ResNet50模型是在ImageNet上训练的，用于将图像分类为1,000个类别之一，例如“老虎猫”、“飞机”和“消防车”。你可能想知道：黑色素瘤是这些类别之一吗？不幸的是，它不是。那么我们如何还能使用这个模型呢？
- en: 'The key lies in the layered architecture of deep learning models. The pretrained
    ResNet50 consists of:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于深度学习模型的分层架构。预训练的ResNet50由以下部分组成：
- en: A *backbone* (ResNet module) that extracts general features from images
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个从图像中提取通用特征的*骨干*（ResNet模块）
- en: A *classifier head* that maps these features to one of ImageNet’s 1,000 categories
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个将特征映射到ImageNet的1,000个类别之一的*分类器头部*
- en: Since we don’t need the ImageNet classification head, we’ll replace it with
    our own custom classifier for melanoma detection.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不需要ImageNet分类头部，我们将用我们自己的自定义分类器来检测黑色素瘤。
- en: Tip
  id: totrans-392
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: While Hugging Face offers `FlaxResNetModel`—a ResNet variant without the classification
    head—for exactly this use case, such headless versions aren’t available for every
    model. Learning how to manually extract and repurpose parts of a pretrained model
    is an essential skill. It gives you full control over what components are used
    and prepares you for working with a wider range of architectures where clean abstractions
    may not exist. That’s why we demonstrate the manual approach here.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Hugging Face提供了`FlaxResNetModel`——一个没有分类头部的ResNet变体，用于恰好这个用例，但并非每个模型都有无头版本。学习如何手动提取和重新利用预训练模型的部分是一个基本技能。它让你完全控制所使用的组件，并为你准备与更广泛的架构一起工作，在这些架构中，干净的抽象可能不存在。这就是为什么我们在这里展示了手动方法。
- en: 'To separate the backbone from the classifier head:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 将骨干网络与分类器头部分离：
- en: '[PRE41]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Here’s what each step does:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是每个步骤的作用：
- en: 'We extract the components of the `FlaxResNetForImageClassification` model provided
    by Hugging Face:'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们提取了Hugging Face提供的`FlaxResNetForImageClassification`模型的组件：
- en: '`module`: The full architecture definition'
  id: totrans-398
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`module`：完整的架构定义'
- en: '`variables`: The pretrained weights for all parts of the model'
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`variables`：模型所有部分的预训练权重'
- en: 'To isolate just the ResNet backbone:'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了仅隔离ResNet骨干：
- en: We `bind` the weights to the module.
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将权重绑定到模块上。
- en: We access the `resnet` submodule, which excludes the classification head.
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们访问`resnet`子模块，它排除了分类头部。
- en: We then `unbind` it, separating the backbone into its own callable Flax module
    with its own parameters.
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将其`unbind`，将骨干网络分离成具有自己参数的独立可调用的Flax模块。
- en: This technique gives you direct access to intermediate model components—even
    when they aren’t explicitly exposed. If you’re unsure what submodules are available,
    you can inspect the bound module by listing its attributes or checking the Hugging
    Face docs.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术让你能够直接访问中间模型组件——即使它们没有被明确暴露。如果你不确定有哪些子模块可用，你可以通过列出其属性或检查Hugging Face文档来检查绑定的模块。
- en: Now, we can use the `backbone_module` just like any other Flax model by calling
    its `apply` method. For input, we pass the `backbone_vars` and a preprocessed
    image.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以像调用任何其他Flax模型一样使用`backbone_module`，通过调用其`apply`方法。对于输入，我们传递`backbone_vars`和预处理后的图像。
- en: Warning
  id: totrans-406
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Unlike many Hugging Face transformer models, Flax models typically expect input
    images in NHWC format (batch, height, width, channels). If the image is in NCHW
    format (batch, channels, height, width), use `transpose` or `moveaxis` to first
    reorder the dimensions.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多Hugging Face transformer模型不同，Flax模型通常期望输入图像为NHWC格式（批次，高度，宽度，通道）。如果图像为NCHW格式（批次，通道，高度，宽度），请使用`transpose`或`moveaxis`首先重新排序维度。
- en: '[PRE42]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Output:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE43]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output shape `(1, 2048, 7, 7)` represents:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 输出形状`(1, 2048, 7, 7)`表示：
- en: '`2048`: The number of feature channels output by the final ResNet block.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2048`：最终ResNet块输出的特征通道数。'
- en: '`7, 7`: The spatial dimensions after all convolutions and pooling layers have
    been applied. In other words, these operations have downsampled the image from
    224 × 224 to 7 x 7 height and width.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`7, 7`：在应用所有卷积和池化层之后的空间维度。换句话说，这些操作将图像从224 × 224下采样到7 x 7的高度和宽度。'
- en: 'At this stage, the feature maps are highly abstract, far removed from the original
    cat image. However, they still capture meaningful patterns learned by the model.
    These 2,048 feature maps would then be fed into the fully connected layers for
    the purpose of actually classifying the image. If we visualize them, we get something
    like [Figure 5-14](#abstract-cat-features):'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，特征图高度抽象，与原始猫图像相去甚远。然而，它们仍然捕捉了模型学习到的有意义模式。这2,048个特征图将被输入到全连接层，用于实际分类图像。如果我们可视化它们，我们会得到类似[图5-14](#abstract-cat-features)的东西。
- en: '[PRE44]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](assets/dlfb_0514.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0514.png)'
- en: Figure 5-14\. Feature maps from the final convolutional layer of ResNet50, illustrating
    how the model “sees” the cat image at that stage. This visualization shows activations
    from only the first 64 of 2048 learned filters. Each square represents the response
    of a different convolutional filter, with bright regions indicating areas of high
    activation. Many filters remain largely inactive, while others detect specific
    patterns or textures.
  id: totrans-417
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-14\. ResNet50最终卷积层的特征图，展示了模型在该阶段“看到”的猫图像。这个可视化显示了2048个学习到的过滤器中的前64个激活。每个方块代表不同卷积滤波器的响应，明亮区域表示高激活区域。许多滤波器大部分处于不活跃状态，而其他滤波器则检测特定的模式或纹理。
- en: Some feature maps remain largely inactive (like those in the first row), while
    others highlight specific aspects of the cat image. At this late stage in the
    neural network, the representation of the cat image is highly abstract and no
    longer interpretable in a human-recognizable way. However, these learned patterns
    are still essential for the model’s classification process.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 一些特征图仍然大部分处于不活跃状态（例如第一行的那些），而其他特征图则突出了猫图像的特定方面。在这个神经网络的高级阶段，猫图像的表示变得高度抽象，不再以人类可识别的方式解释。然而，这些学习到的模式对于模型的分类过程仍然是必不可少的。
- en: Tip
  id: totrans-419
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: In this example, we simply examined the activations of late-stage convolutional
    filters in response to a cat image to demonstrate how to extract feature maps
    from a pretrained model. While useful as a sanity check, this doesn’t reveal what
    each filter has actually learned.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们简单地检查了猫图像对晚期卷积滤波器的激活，以展示如何从预训练模型中提取特征图。虽然作为合理性检查很有用，但这并不揭示每个滤波器实际上学到了什么。
- en: 'To dig deeper, try *feature visualization*: a technique where you iteratively
    modify an input image to maximize the activation of a specific filter. This reveals
    the kinds of patterns—like textures, edges, or object parts—that the filter responds
    to.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解，尝试*特征可视化*：这是一种迭代修改输入图像以最大化特定滤波器激活的技术。这揭示了滤波器响应的图案类型——如纹理、边缘或物体部分。
- en: For an excellent deep dive, check out the [classic Distill blog post from 2017](https://oreil.ly/XpdSL)
    on visualizing convolutional filters.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行深入探讨，请查看2017年的[经典Distill博客文章](https://oreil.ly/XpdSL)，关于可视化卷积滤波器。
- en: 'We can also access the model’s final `pooler_output`, which represents the
    last hidden state after global average pooling. This operation collapses the spatial
    dimensions, leaving us with a single 2,048-dimensional feature vector—the final
    output of the model’s feature extraction process. This vector serves as the condensed
    representation of the image, as shown in [Figure 5-15](#last-hidden-state-of-resnet50):'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以访问模型的最终`pooler_output`，它代表全局平均池化后的最后一个隐藏状态。这个操作折叠了空间维度，留下我们一个单一的2,048维特征向量——模型特征提取过程的最终输出。这个向量作为图像的浓缩表示，如图[图5-15](#last-hidden-state-of-resnet50)所示：
- en: '[PRE45]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![](assets/dlfb_0515.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0515.png)'
- en: Figure 5-15\. Visualization of the final pooled output of ResNet50 on the cat
    image input, also known as the last hidden state after global average pooling.
    This 2048-dimensional feature vector serves as the input to the classifier head.
  id: totrans-426
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-15\. 在猫图像输入上ResNet50的最终池化输出的可视化，也称为全局平均池化后的最后一个隐藏状态。这个2048维特征向量作为分类器头的输入。
- en: All right, we now have a standalone ResNet module extracted from the full classifier.
    We’ve also inspected its final output—a 2,048-dimensional feature vector produced
    by global average pooling. This vector captures the distilled representation of
    the input image and will serve as the input to our custom classifier head.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们现在已经从完整的分类器中提取了一个独立的ResNet模块。我们还检查了它的最终输出——由全局平均池化产生的2,048维特征向量。这个向量捕捉了输入图像的浓缩表示，并将作为我们自定义分类器头的输入。
- en: With the backbone in place, we’re now ready to construct the `SkinLesionClassifierHead`.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在骨干结构就位后，我们现在准备构建`SkinLesionClassifierHead`。
- en: Building the SkinLesionClassifierHead
  id: totrans-429
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建SkinLesionClassifierHead
- en: Now that we have extracted the pretrained ResNet50 backbone, we can add a new
    classification head tailored for skin lesion detection. This head will take the
    feature vector output from [Figure 5-15](#last-hidden-state-of-resnet50) and produce
    logits for our target classes.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经提取了预训练的ResNet50骨干网络，我们可以添加一个新的针对皮肤病变检测定制的分类头。这个头将从[图5-15](#last-hidden-state-of-resnet50)的特征向量输出中提取，并为我们的目标类别生成logits。
- en: 'Although Hugging Face follows a naming convention like `FlaxResNetForSkinLesionClassification`,
    we will slightly simplify and call our head `SkinLesionClassifierHead`. This will
    perform the final classification by returning the logits for each skin class we
    want to predict. This is a small feedforward neural network consisting of sequential
    `Dense` layers with ReLU activations. Let’s take a look at its implementation:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Hugging Face遵循类似于`FlaxResNetForSkinLesionClassification`的命名约定，但我们将稍微简化，将其称为`SkinLesionClassifierHead`。这将通过返回我们想要预测的每个皮肤类别的logits来完成最终的分类。这是一个由具有ReLU激活的顺序`Dense`层组成的小型前馈神经网络。让我们看看它的实现：
- en: '[PRE46]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The final layer has as many neurons as the number of target classes, defined
    by the `num_classes` argument.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 最终层有与目标类别数量相同的神经元，由`num_classes`参数定义。
- en: Note
  id: totrans-434
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In this chapter, we focus on multiclass classification, but the same code generalizes
    to binary classification. While a single `nn.Dense` neuron with sigmoid activation
    is perhaps more typical for binary tasks, we use a two-neuron output instead—one
    per class (melanoma or nonmelanoma)—producing logits that sum to 1 after softmax.
    This approach is slightly less efficient but more flexible, as it naturally extends
    to multiclass problems.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于多类分类，但相同的代码可以推广到二分类。虽然单个具有sigmoid激活的`nn.Dense`神经元对于二分类任务可能是更典型的，但我们使用两个神经元的输出，每个类别一个（黑色素瘤或非黑色素瘤），在softmax之后产生总和为1的logits。这种方法稍微低效一些，但更灵活，因为它可以自然地扩展到多类问题。
- en: 'You will also notice that we specify `nn.initializers.xavier_uniform()` for
    our weight initialization. While the most naïve approach is to initialize all
    weights to zero, this leads to a major problem: all neurons in a layer would receive
    the same gradients during backpropagation and update identically, resulting in
    redundant feature learning. This is known as a *failure to break symmetry*, and
    it can severely limit the network’s ability to learn.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会注意到，我们为权重初始化指定了`nn.initializers.xavier_uniform()`。虽然最简单的方法是将所有权重初始化为零，但这会导致一个主要问题：在反向传播过程中，一个层中的所有神经元都会收到相同的梯度并相同地更新，导致冗余的特征学习。这被称为*未能打破对称性*，并且可以严重限制网络的学习能力。
- en: Instead, we use *Xavier initialization* (also known as *Glorot initialization*),
    which assigns small, random values to each weight. This helps ensure that neurons
    start with diverse parameters and can learn different features. Xavier initialization
    is designed to maintain stable variance in both activations and gradients across
    layers, improving training dynamics.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们使用*Xavier初始化*（也称为*Glorot初始化*），它为每个权重分配小的随机值。这有助于确保神经元以不同的参数开始，并可以学习不同的特征。Xavier初始化旨在在层之间保持激活和梯度稳定方差，从而改善训练动态。
- en: Tip
  id: totrans-438
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Most modern deep learning libraries—including Flax—automatically apply good
    initializers for standard layers. We specify `xavier_uniform()` here for clarity,
    but in practice, you can often rely on the library’s defaults.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代深度学习库，包括Flax，都会自动为标准层应用良好的初始化器。我们在这里指定`xavier_uniform()`是为了清晰起见，但在实践中，你通常可以依赖库的默认设置。
- en: Building Our Models
  id: totrans-440
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建我们的模型
- en: We’re now ready to begin the core modeling section of this chapter. We’ll compare
    several architectures, gradually increasing in complexity. For the more promising
    ones, we’ll also explore enhancements to improve performance. Let’s begin with
    our baseline.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好开始本章的核心建模部分。我们将比较几个架构，逐渐增加其复杂性。对于更有前景的架构，我们还将探索增强功能以提高性能。让我们从我们的基线开始。
- en: SimpleCnn model as a baseline
  id: totrans-442
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SimpleCnn模型作为基线
- en: 'Our first model is a straightforward two-layer convolutional neural network
    trained from scratch. It doesn’t use any pretrained features, and we won’t spend
    time tuning it. The goal here is simple: to establish a reference point. How well
    can a minimal architecture perform on this task without any bells or whistles?'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一模型是一个简单的两层卷积神经网络，从头开始训练。它不使用任何预训练的特征，我们也不会花时间调整它。这里的目的是简单的：建立一个参考点。在没有花哨功能的情况下，最小架构在这个任务上能表现得多好？
- en: '[PRE47]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'As you can see, it uses our shared classification head, `SkinLesionClassifierHead`,
    and a custom lightweight backbone:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它使用我们共享的分类头`SkinLesionClassifierHead`和自定义的轻量级骨干：
- en: '[PRE48]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This backbone is intentionally simple—no residual connections, no batch normalization.
    It’s just a stack of convolutional layers followed by flattening. It won’t win
    any benchmarks, but it should give us a useful lower bound on performance for
    this task.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 这个骨干网络故意设计得简单——没有残差连接，没有批量归一化。它只是一个卷积层堆叠后跟平展。它不会赢得任何基准测试，但它应该给我们提供一个有用的性能下限，适用于这项任务。
- en: 'Even though this model doesn’t include batch normalization, we’ll still use
    a training state that supports it—`TrainStateWithBatchNorm`—to keep our training
    loop compatible across models:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个模型不包括批量归一化，我们仍然会使用支持它的训练状态`TrainStateWithBatchNorm`，以保持我们的训练循环在各个模型之间兼容：
- en: '[PRE49]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Using a unified training state lets us reuse the same training loop for both
    this simple CNN and the more advanced ResNet models we’ll explore next.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 使用统一的训练状态，让我们可以重用相同的训练循环来训练这个简单的CNN以及我们接下来将要探索的更高级的ResNet模型。
- en: Now let’s move on to training our ResNet variants to better understand the impact
    of architecture and transfer learning on skin lesion classification. With the
    ResNet50 backbone we extracted earlier and our custom skin lesion classification
    head already in place, we now have all the components we need to assemble and
    train more advanced models.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续训练我们的ResNet变体，以更好地理解架构和迁移学习对皮肤病变分类的影响。有了我们之前提取的ResNet50骨干网络和已经就位的自定义皮肤病变分类头，我们现在拥有了组装和训练更高级模型所需的所有组件。
- en: 'We’ll build three models, starting with the simplest: ResNet50 from scratch.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建三个模型，从最简单的开始：从头开始构建ResNet50。
- en: Building the ResNetFromScratch model
  id: totrans-453
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建ResNetFromScratch模型
- en: Our first ResNet variant is `ResNetFromScratch`, defined next. It uses the full
    ResNet50 architecture but does not load any pretrained weights—all parameters
    are initialized randomly and learned from scratch. While this model still benefits
    from the structure of a relatively deep, well-tested architecture, it doesn’t
    directly inherit any knowledge from ImageNet or other large-scale datasets.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个ResNet变体是`ResNetFromScratch`，定义如下。它使用完整的ResNet50架构，但不加载任何预训练权重——所有参数都是随机初始化并从头开始学习的。虽然这个模型仍然受益于一个相对较深、经过良好测试的架构的结构，但它并没有直接从ImageNet或其他大规模数据集中继承任何知识。
- en: 'This gives us a useful comparison point: how far can we get with a good architecture
    alone, without any pretrained weights? Here’s the code for the model:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一个有用的比较点：仅使用良好的架构，没有预训练权重，我们能走多远？以下是模型的代码：
- en: '[PRE50]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: In the `setup` method, we instantiate both the `backbone` and the `head`. These
    are then used sequentially in the `__call__` method to compute predictions. The
    backbone is accessed through the Hugging Face wrapper, so we reference it using
    `.module` to extract the underlying Flax-compatible model.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 在`setup`方法中，我们实例化了`backbone`和`head`。然后在`__call__`方法中依次使用它们来计算预测。骨干网络通过Hugging
    Face包装器访问，因此我们使用`.module`来提取底层的Flax兼容模型。
- en: Note
  id: totrans-458
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'You will have noticed the the model takes a `layer` parameter. It picks the
    ResNet model with the required number of layers from the Hugging Face library
    of preloaded models:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到模型有一个`layer`参数。它从Hugging Face库中预加载的模型中选择具有所需层数的ResNet模型：
- en: '[PRE51]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We have settled for a value of 50 to get the ResNet50 backbone, but the layers
    parameter could be conveniently explored as a hyperparameter.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了50这个值来获得ResNet50骨干网络，但层的参数可以方便地作为一个超参数进行探索。
- en: To prepare the model for training, we define a training state via the `create_train_state`
    method. This initializes the parameters, optionally transfers pretrained weights
    (not used here), and sets which parameters should remain trainable. While `ResNetFromScratch`
    doesn’t apply any changes to the weights, this method is designed to be overridden
    by subclasses—as we’ll see next with `FinetunedResNet`.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备模型进行训练，我们通过`create_train_state`方法定义一个训练状态。这初始化了参数，可选地转移预训练权重（此处未使用），并设置哪些参数应该保持可训练。虽然`ResNetFromScratch`不对权重进行任何更改，但此方法设计为可以被子类覆盖——正如我们接下来将看到的`FinetunedResNet`。
- en: Even though `ResNetFromScratch` does not use any pretrained weights, it defines
    a `transfer_parameters` method (which is a no-op here) and a `set_trainable_parameters`
    method (also a placeholder). These provide a consistent interface across models
    and make it easy to plug in transfer learning logic in downstream variants.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 `ResNetFromScratch` 不使用任何预训练权重，它也定义了一个 `transfer_parameters` 方法（在这里是一个空操作）和一个
    `set_trainable_parameters` 方法（也是一个占位符）。这些为模型提供了统一的接口，使得在下游变体中插入迁移学习逻辑变得容易。
- en: Since ResNet uses batch normalization, our training state must also track batch
    statistics—so we use the `TrainStateWithBatchNorm` class defined earlier.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 ResNet 使用批量归一化，我们的训练状态也必须跟踪批量统计信息——因此我们使用之前定义的 `TrainStateWithBatchNorm`
    类。
- en: Fully fine-tuning a pretrained model
  id: totrans-465
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完全微调预训练模型
- en: 'The second model, `FinetunedResNet50`, is a subclass of `ResNet50FromScratch`
    that modifies just one key detail: it loads pretrained weights from the Hugging
    Face ResNet50 model.'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个模型 `FinetunedResNet50` 是 `ResNet50FromScratch` 的子类，它只修改了一个关键细节：它从 Hugging
    Face ResNet50 模型加载预训练权重。
- en: 'This is done by overriding the `transfer_parameters` method. Unlike the base
    class (where this method is a no-op), here it copies over both the backbone’s
    weights and its batch normalization statistics. The classification head remains
    randomly initialized, since it’s specific to our skin lesion classification task:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 这通过覆盖 `transfer_parameters` 方法来完成。与基类（其中此方法是一个空操作）不同，在这里它复制了骨干的权重及其批量归一化统计信息。分类头保持随机初始化，因为它针对我们的皮肤病变分类任务：
- en: '[PRE52]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Once the parameters are loaded, we will fine-tune `all` of them—both the pretrained
    ResNet backbone and the newly added classification head. This setup allows the
    model to fully adapt to the skin cancer classification task, and should in theory
    lead to stronger performance when the source (ImageNet) and target (skin) domains
    share similar visual features, such as textures, colors, and edges, which should
    be the case here.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦加载了参数，我们将微调 `所有` 参数——包括预训练的 ResNet 骨干和新增的分类头。这种设置允许模型完全适应皮肤癌分类任务，并且理论上，当源（ImageNet）和目标（皮肤）域共享类似的视觉特征，如纹理、颜色和边缘时，应该会得到更强的性能，这在这里应该是情况。
- en: Freezing the backbone with FinetunedHeadResNet
  id: totrans-470
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 FinetunedHeadResNet 冻结骨干
- en: The third model, `FinetunedHeadResNet`, uses a different transfer learning strategy.
    Instead of fine-tuning all layers, it freezes the pretrained ResNet backbone and
    updates only the final classification head. This approach is common when working
    with small datasets. It allows us to leverage powerful pretrained features while
    reducing the number of trainable parameters—which can help avoid overfitting and
    reduce training time.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个模型 `FinetunedHeadResNet` 使用不同的迁移学习策略。它不是微调所有层，而是冻结预训练的 ResNet 骨干，只更新最终的分类头。这种方法在小数据集工作时很常见。它允许我们利用强大的预训练特征，同时减少可训练参数的数量——这有助于避免过拟合并减少训练时间。
- en: Flax and Optax make it easy to define separate optimizer behaviors for different
    parts of the model. Here, we apply our usual optimizer to the classification head
    and use a no-op optimizer (which performs no updates) for the frozen backbone,
    ensuring only the final layers are trained. This is implemented by overriding
    the `set_trainable_parameters` method.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: Flax 和 Optax 使得为模型的不同部分定义单独的优化器行为变得容易。在这里，我们将我们常用的优化器应用于分类头，并使用一个空操作优化器（它不执行更新）用于冻结的骨干，确保只有最终层被训练。这是通过覆盖
    `set_trainable_parameters` 方法实现的。
- en: 'This is implemented in the following model, which subclasses `FinetunedResNet`
    to inherit pre-trained weights but overrides the `set_trainable_parameters` method
    to freeze the backbone:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以下模型中实现，它通过子类化 `FinetunedResNet` 继承预训练权重，但覆盖了 `set_trainable_parameters` 方法以冻结骨干：
- en: '[PRE53]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: By subclassing `FinetunedResNet`, we inherit all the logic for loading pretrained
    weights and constructing the model—and only need to override the part that controls
    which parameters are trainable. This modular design maximizes code reuse while
    making it easy to explore different fine-tuning strategies with minimal duplication.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 通过子类化 `FinetunedResNet`，我们继承了加载预训练权重和构建模型的全部逻辑——只需覆盖控制哪些参数可训练的部分。这种模块化设计最大化了代码重用，同时使得探索不同的微调策略变得简单，且最小化重复。
- en: More customization options
  id: totrans-476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多定制选项
- en: It’s worth highlighting that fine-tuning doesn’t have to be all or nothing—you’re
    not limited to freezing the whole backbone or simply training all parameters.
    With tools like `optax.multi_transform`, we can easily assign different optimizer
    behaviors to different parts of the model.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 值得强调的是，微调不一定是全有或全无——你不仅限于冻结整个骨干或简单地训练所有参数。使用像 `optax.multi_transform` 这样的工具，我们可以轻松地为模型的不同部分分配不同的优化器行为。
- en: For example, a common strategy is to freeze the earliest convolutional layers
    (which tend to learn general-purpose features like edges and textures) while fine-tuning
    later layers that capture more task-specific patterns. Some layers can even be
    updated using a reduced learning rate, allowing for more cautious adaptation.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一种常见的策略是冻结最早的卷积层（这些层倾向于学习通用特征，如边缘和纹理），同时微调后期层以捕获更多特定于任务的模式。某些层甚至可以使用降低的学习率进行更新，从而允许更谨慎的适应。
- en: 'The following is an example model, `PartiallyFinetunedResNet`, which demonstrates
    this idea. It applies:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例模型，`PartiallyFinetunedResNet`，它展示了这个想法。它应用：
- en: A standard learning rate to the classification head and late-stage backbone
    layers
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标准学习率应用于分类头和后期骨干层
- en: A reduced learning rate to intermediate layers (i.e., `stages/3`)
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低中间层（例如，`stages/3`）的学习率
- en: 'A complete freezing of the early backbone layers:'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全冻结早期骨干层：
- en: '[PRE54]'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We won’t be training or evaluating this model in the rest of the chapter, but
    it’s useful to be aware of this level of flexibility. Once your model is structured
    cleanly, these kinds of fine-tuning schemes are straightforward to implement and
    can make a big difference when adapting to smaller or more specialized datasets.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章的其余部分不会训练或评估此模型，但了解这一级别的灵活性是有用的。一旦你的模型结构清晰，这类微调方案就很容易实现，并且当适应较小或更专业化的数据集时可以产生重大影响。
- en: Training the Models
  id: totrans-485
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: With our `DatasetBuilder` ready to generate data and all of our model variants
    defined, it’s time to train them. We’ll now implement a training loop that follows
    a familiar structure.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 `DatasetBuilder` 准备生成数据以及所有模型变体定义完毕，现在是时候训练它们了。我们将现在实现一个遵循熟悉结构的训练循环。
- en: The Training Loop
  id: totrans-487
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练循环
- en: 'The training loop follows the same approach as in previous chapters:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环遵循与之前章节相同的方法：
- en: Initialize the model’s training state.
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化模型的训练状态。
- en: Iterate over a defined number of steps.
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代定义的步数。
- en: At each step, fetch a batch of training data and update the model.
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个步骤中，获取一批训练数据并更新模型。
- en: Periodically evaluate on a validation set to track progress.
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定期在验证集上评估以跟踪进度。
- en: 'In code:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中：
- en: '[PRE55]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'At a high level, this function:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，此函数：
- en: Sets up a metrics logger and prepares the data loaders
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置指标记录器和准备数据加载器
- en: Iterates through the given number of training steps
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代给定的训练步数
- en: Calls `train_step` to apply a gradient update on each training batch
  id: totrans-498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 `train_step` 对每个训练批次应用梯度更新
- en: Periodically runs `eval_step` on validation batches
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定期在验证批次上运行 `eval_step`
- en: Logs and flushes metrics so we can monitor progress
  id: totrans-500
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录并刷新指标，以便我们可以监控进度
- en: This design gives us a steady stream of training and validation feedback. The
    validation metrics are useful not just for model selection but also for deciding
    when to stop training or intervene (for example, if a model is doing horrendously
    poorly, we can just kill the hopeless run).
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计为我们提供了稳定的训练和验证反馈流。验证指标不仅对模型选择有用，而且对于决定何时停止训练或干预（例如，如果模型表现极差，我们可以直接终止无望的运行）也很有用。
- en: Next, let’s take a look at what happens inside a single training step.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看单个训练步骤内部发生了什么。
- en: The training step
  id: totrans-503
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练步骤
- en: The core of our training process is the `train_step` function. This function
    wraps around a helper called `calculate_loss`, which handles forward propagation,
    loss computation, and gradient updates. This is a common design pattern in JAX
    code.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练过程的核心是 `train_step` 函数。此函数围绕一个名为 `calculate_loss` 的辅助函数，该函数处理前向传播、损失计算和梯度更新。这是
    JAX 代码中的常见设计模式。
- en: '[PRE56]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Inside `calculate_loss`:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `calculate_loss` 内部：
- en: Images and labels are extracted from the batch.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从批次中提取图像和标签。
- en: The model performs a forward pass to compute logits (unnormalized prediction
    scores for each class).
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型执行正向传递以计算 logits（每个类别的未归一化预测分数）。
- en: The predicted logits are compared to ground truth labels using `optax.softmax_cross_entropy_with_integer_labels`.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `optax.softmax_cross_entropy_with_integer_labels` 将预测的 logits 与真实标签进行比较。
- en: The mean cross-entropy loss is computed—averaged over the examples in the batch.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算平均交叉熵损失——在批次的示例上平均。
- en: Gradients are then calculated with respect to this loss and applied to update
    the model’s parameters.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后根据这个损失计算梯度，并应用于更新模型的参数。
- en: 'One important detail: if the model uses batch normalization (as ResNet does),
    we also need to update the running statistics tracked by the batch norm layers.
    The function `calculate_loss` handles both cases—whether batch norm is used or
    not—by conditionally including and updating the `batch_stats` entry in the variable
    collection.'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的细节：如果模型使用批归一化（如ResNet所做的那样），我们还需要更新批归一化层跟踪的运行统计信息。`calculate_loss`函数通过条件性地包括和更新变量集合中的`batch_stats`条目来处理这两种情况——无论是否使用批归一化。
- en: This structure makes the training step general and reusable across all models
    we’ve defined, regardless of their architectural complexity or normalization layers.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构使得训练步骤通用且可重用，适用于我们定义的所有模型，无论其架构复杂性或归一化层。
- en: The evaluation step
  id: totrans-514
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估步骤
- en: The evaluation step mirrors the training step but is simpler, as it does *not*
    update model weights. Its sole purpose is to compute performance metrics on a
    validation (or test) batch.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 评估步骤与训练步骤类似，但更简单，因为它不更新模型权重。它的唯一目的是在验证（或测试）批次上计算性能指标。
- en: '[PRE57]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Here’s how it works:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是它的工作原理：
- en: The model runs a forward pass with `is_training=False`, disabling training-time
    behavior like dropout and updating batch norm statistics.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型以`is_training=False`运行前向传播，禁用了训练时的行为，如dropout和更新批归一化统计信息。
- en: If batch normalization is used, the stored `batch_stats` are included in the
    variable collection—but they are *not* updated. Instead, we rely on the running
    averages (exponential moving averages of the mean and variance of activations)
    that were collected during training.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果使用批归一化，存储的`batch_stats`将包含在变量集合中——但它们不会被更新。相反，我们依赖于训练期间收集的运行平均值（激活的平均值和方差的指数移动平均值）。
- en: Logits are compared to the ground truth labels using the same loss function
    as during training.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用与训练期间相同的损失函数将logits与真实标签进行比较。
- en: In addition to computing the loss, the function returns performance metrics
    using `compute_metrics`, which includes weighted precision and recall (explained
    later).
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了计算损失之外，该函数还使用`compute_metrics`返回性能指标，包括加权精确度和召回率（稍后解释）。
- en: By keeping evaluation stateless and side-effect-free—that is, avoiding any updates
    to model parameters or internal statistics—we ensure that validation scores remain
    consistent, reliable, and comparable across runs.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 通过保持评估状态无状态和无副作用——也就是说，避免对模型参数或内部统计信息的任何更新——我们确保验证分数在运行之间保持一致、可靠和可比较。
- en: The evaluation metrics
  id: totrans-523
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估指标
- en: During training, we typically monitor the loss, but for evaluation, we will
    also compute *precision* and *recall* to gain a more complete picture of the model’s
    performance on the skin lesion classification task.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，我们通常监控损失，但在评估期间，我们还将计算*精确度*和*召回率*，以更全面地了解模型在皮肤病变分类任务上的性能。
- en: 'Let’s first discuss how these metrics work in the binary case—for example,
    distinguishing melanoma from nonmelanoma—and then generalize to the multiclass
    setting we actually care about:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先讨论这些指标在二分类情况下的工作方式——例如，区分黑色素瘤和非黑色素瘤——然后推广到我们真正关心的多分类设置：
- en: '*Precision* measures how many of the predicted positive cases (melanoma) are
    actually correct:'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*精确度*衡量预测的正例中有多少实际上是正确的（例如黑色素瘤）：'
- en: <mrow><mtext>Precision</mtext> <mo>=</mo> <mfrac><mrow><mtext>True</mtext><mtext>Positives</mtext></mrow>
    <mrow><mtext>True</mtext><mtext>Positives</mtext><mtext>+</mtext><mtext>False</mtext><mtext>Positives</mtext></mrow></mfrac></mrow>
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <mrow><mtext>精确度</mtext> <mo>=</mo> <mfrac><mrow><mtext>真阳性</mtext><mtext>正例</mtext></mrow>
    <mrow><mtext>真阳性</mtext><mtext>正例</mtext><mtext>+</mtext><mtext>假阳性</mtext><mtext>正例</mtext></mrow></mfrac></mrow>
- en: High precision means few false positives; so when the model flags a melanoma,
    it’s likely to be correct. This is important in medical settings to avoid unnecessary
    follow-up procedures and patient anxiety.
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高精确度意味着假阳性很少；因此，当模型标记黑色素瘤时，它很可能是正确的。这在医疗环境中很重要，可以避免不必要的后续程序和患者焦虑。
- en: '*Recall* (also called sensitivity) measures how many actual melanoma cases
    were correctly identified:'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*召回率*（也称为灵敏度）衡量有多少实际的黑色素瘤病例被正确识别：'
- en: <mrow><mtext>Recall</mtext> <mo>=</mo> <mfrac><mrow><mtext>True</mtext><mtext>Positives</mtext></mrow>
    <mrow><mtext>True</mtext><mtext>Positives</mtext><mtext>+</mtext><mtext>False</mtext><mtext>Negatives</mtext></mrow></mfrac></mrow>
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <mrow><mtext>召回率</mtext> <mo>=</mo> <mfrac><mrow><mtext>真正阳性</mtext><mtext>+</mtext><mtext>真正阳性</mtext><mtext>+</mtext><mtext>假阴性</mtext></mrow></mfrac></mrow>
- en: 'High recall means few missed melanoma cases: critical for ensuring early detection.
    In many clinical settings, recall is prioritized, since missing a true case (i.e.,
    making a false negative prediction) is more harmful than incorrectly flagging
    a benign case.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 高召回率意味着很少错过黑色素瘤病例：这对于确保早期检测至关重要。在许多临床环境中，召回率被优先考虑，因为错过一个真实病例（即做出错误的阴性预测）比错误地标记一个良性病例更有害。
- en: In our case, we’re working with multiple classes, not just melanoma versus nonmelanoma.
    To summarize performance across all classes, we use *weighted precision* and *weighted
    recall*. These metrics average per-class values while accounting for class imbalance—classes
    with more examples contribute more to the final score.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们处理的是多个类别，而不仅仅是黑色素瘤与非黑色素瘤之间的比较。为了总结所有类别的性能，我们使用*加权精确率*和*加权召回率*。这些指标在计算每类值的同时，还考虑了类别不平衡——具有更多示例的类别对最终分数的贡献更大。
- en: 'We’ll be using weighted precision and recall as our main evaluation metrics,
    and we will plot them across training and validation splits as training proceeds.
    Here is our `compute_metrics` code:'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用加权精确率和召回率作为我们的主要评估指标，并在训练过程中将它们绘制在训练和验证分割上。以下是我们的`compute_metrics`代码：
- en: '[PRE58]'
  id: totrans-534
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: This function uses the predicted logits to compute softmax scores, takes the
    most likely class, and then calculates weighted precision and recall based on
    the true and predicted labels.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数使用预测的logits来计算softmax分数，选择最可能的类别，然后根据真实标签和预测标签计算加权精确率和召回率。
- en: Warning
  id: totrans-536
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Weighted metrics are concise and helpful but can mask problems—for instance,
    if the model performs poorly on a rare class, this might not show up clearly in
    the overall score. Always check *per-class metrics* as well. This can highlight
    weaknesses that might justify upweighting certain classes in the loss function
    or resampling your dataset to better balance class representation.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 加权指标简洁且有用，但可能会掩盖问题——例如，如果模型在罕见类别上表现不佳，这可能在整体分数中不会明显显示。始终检查*每类指标*，这可以突出可能需要加权某些类别的损失函数或重新采样数据集以更好地平衡类别代表性的弱点。
- en: You may also encounter F1 scores in classification tasks. The F1 score is the
    harmonic mean of precision and recall—it provides a single number that balances
    both. Here, we’ll stick to just plotting both precision and recall, as they are
    often easier metrics to interpret and let you diagnose whether your model struggles
    more with false positives or false negatives.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可能在分类任务中遇到F1分数。F1分数是精确率和召回率的调和平均值——它提供了一个平衡两者的单一数字。在这里，我们将坚持只绘制精确率和召回率，因为它们通常是更容易解释的指标，并让你诊断模型是否在假阳性或假阴性方面有更大的困难。
- en: Faster evaluation metrics
  id: totrans-539
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更快的评估指标
- en: You may have noticed that the `compute_metrics` function is jitted. Standard
    evaluation metrics—such as those from `sklearn` or `scipy`—are not typically JIT-compatible,
    which can become a bottleneck during training.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到`compute_metrics`函数已经被JIT编译。标准的评估指标，例如来自`sklearn`或`scipy`的指标，通常不与JIT兼容，这可能会在训练过程中成为瓶颈。
- en: To address this, we’ve implemented JIT-friendly versions of precision and recall.
    These custom functions have been tested to match the outputs of their standard
    counterparts, but run significantly faster—especially during frequent batch-wise
    evaluation.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们实现了JIT友好的精确率和召回率版本。这些自定义函数已经过测试，以匹配其标准对应版本的输出，但运行速度显著更快——尤其是在频繁的批量评估期间。
- en: The metrics are computed on each batch and logged using the `MetricsLogger`,
    which aggregates results to provide full-dataset performance summaries.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 指标是在每个批次上计算的，并使用`MetricsLogger`进行记录，该记录器汇总结果以提供整个数据集的性能摘要。
- en: Note
  id: totrans-543
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注
- en: In large-scale production models, the overhead from non-jitted metrics might
    be negligible. But for the relatively small models and fast iteration cycles we
    use here, the speedup from JIT compatibility is noticeable.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模生产模型中，非JIT指标的额外开销可能可以忽略不计。但对我们这里使用的相对较小的模型和快速的迭代周期来说，JIT兼容性的加速效果是明显的。
- en: Creating the Multiclass Dataset
  id: totrans-545
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建多类数据集
- en: Finally, we can use our `DatasetBuilder` to construct the dataset that will
    be used to train and evaluate all models in this chapter. Using the same dataset
    setup across experiments allows us to make clean comparisons and attribute performance
    differences to model choices rather than data variation.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用我们的`DatasetBuilder`构建用于训练和评估本章中所有模型的训练集。在实验中使用相同的训练集设置允许我们进行干净的比较，并将性能差异归因于模型选择而不是数据变化。
- en: We also initialize and split a random seed up front, so we can reuse consistent
    seeds across model initializations and training runs—this helps ensure reproducibility.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提前初始化并分割一个随机种子，这样我们可以在模型初始化和训练运行中重用一致的种子——这有助于确保可重复性。
- en: '[PRE59]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'There are three preprocessing options implemented:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 实现了三种预处理选项：
- en: '`crop`'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '`crop`'
- en: Focuses on the central region of the image, where lesions are typically located.
    While simple and effective, it can discard potentially useful peripheral features
    such as hairs, texture, or pigment variation.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 专注于图像的中心区域，病变通常位于此处。虽然简单有效，但它可能会丢弃可能有用的边缘特征，如毛发、纹理或色素变化。
- en: '`skew`'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '`skew`'
- en: Retains as much of the lesion context as possible by resizing the entire image
    to a square without cropping—this means stretching or compressing the original
    aspect ratio. Since most of the input images are wider than they are tall, cropping
    to a square can cut out potentially useful surrounding tissue. Skewing avoids
    this by distorting the image to fit a square shape, preserving all pixels.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将整个图像调整到正方形而不裁剪，尽可能保留病变的上下文——这意味着拉伸或压缩原始的宽高比。由于大多数输入图像的宽度大于高度，裁剪到正方形可能会切掉可能有用的周围组织。倾斜通过扭曲图像以适应正方形形状，从而保留所有像素。
- en: While the image becomes slightly warped, it may still help the model capture
    broader contextual cues like skin texture or peripheral features.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然图像会略微变形，但它可能仍然有助于模型捕捉更广泛的环境线索，如皮肤纹理或边缘特征。
- en: '`resnet`'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '`resnet`'
- en: Applies standard preprocessing required for ResNet-based models, including resizing
    to the expected input shape and normalizing pixel values. Ensures compatibility
    with pretrained ResNet architectures.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 应用ResNet模型所需的标准化预处理，包括调整大小到预期的输入形状和归一化像素值。确保与预训练的ResNet架构兼容。
- en: These preprocessing steps are modular and can be swapped in and out easily.
    We can treat the choice between them as yet another tunable hyperparameter during
    experimentation.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预处理步骤是模块化的，可以轻松地替换。我们可以将这些选择之间的选择视为实验中另一个可调的超参数。
- en: Training the Baseline Model
  id: totrans-558
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练基线模型
- en: 'Let’s first train the baseline `SimpleCnn` model. There’s nothing fancy here—but
    the point is to show that even a naive architecture can learn a surprising amount
    from the raw images. Here is the training setup:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先训练基线`SimpleCnn`模型。这里没有什么花哨的——但重点是展示即使是原始架构也能从原始图像中学习到惊人的内容。以下是训练设置：
- en: '[PRE60]'
  id: totrans-560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'A few quick notes on this “no frills” setup:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个“无装饰”设置的几点快速说明：
- en: We use `optax.adamw` but set `weight_decay=0.0`, which makes it equivalent to
    plain `optax.adam`. This means no weight decay regularization is applied (we also
    do not need to worry about the `mask` parameter).
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`optax.adamw`，但将`weight_decay`设置为`0.0`，这使得它等同于普通的`optax.adam`。这意味着没有应用权重衰减正则化（我们也不必担心`mask`参数）。
- en: The preprocessor is set to crop.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理器设置为裁剪。
- en: We’re using the `repeating_sampler` to cycle through the data. This does not
    resample images to balance class frequencies.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在使用`repeating_sampler`来循环遍历数据。这并不会重新采样图像以平衡类别的频率。
- en: No data augmentation is applied (`augmentor=None`).
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有应用数据增强（`augmentor=None`）。
- en: Note
  id: totrans-566
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Although we specify the `optax.adamw` optimizer, we initially leave weight decay
    unused. Why? Defining it uniformly across all of our models ensures consistency
    and makes hyperparameter tuning much simpler. Later on, we’ll activate this placeholder
    to help reduce model overfitting.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们指定了`optax.adamw`优化器，但我们最初没有使用权重衰减。为什么？在所有我们的模型中统一定义它确保了一致性，并使超参数调整变得简单得多。稍后，我们将激活这个占位符以帮助减少模型过拟合。
- en: Let’s now evaluate the model’s performance by plotting its metrics over the
    training steps. The results are shown in [Figure 5-16](#resnet-from-scratch-evaluation-plot).
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过绘制其指标随训练步骤的变化来评估模型的性能。结果如图[图5-16](#resnet-from-scratch-evaluation-plot)所示。
- en: '[PRE61]'
  id: totrans-569
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '![](assets/dlfb_0516.png)'
  id: totrans-570
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0516.png)'
- en: Figure 5-16\. Evaluation metrics for the `SimpleCnn` model. The left plot shows
    the evolution of loss for the `train` and `valid` dataset splits, while the right
    plot tracks (weighted) precision and recall for the `valid` dataset.
  id: totrans-571
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-16\. `SimpleCnn` 模型的评估指标。左图显示了 `train` 和 `valid` 数据集分割的损失演变，而右图跟踪了 `valid`
    数据集的（加权）精确率和召回率。
- en: We see that this baseline model reaches around 0.4 weighted precision and recall
    on the validation set—not bad for such a simple architecture. This gives us a
    reference point to compare against more advanced models.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到这个基线模型在验证集上达到了大约 0.4 的加权精确度和召回率——对于这样一个简单的架构来说还不错。这为我们提供了一个参考点，可以与更先进的模型进行比较。
- en: Training metrics are much higher than validation metrics, indicating substantial
    overfitting. We also observe that validation loss increases over time, which is
    consistent with overfitting.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 训练指标远高于验证指标，表明过度拟合严重。我们还观察到验证损失随时间增加，这与过度拟合一致。
- en: 'Another useful way to assess model performance is through the confusion matrix,
    shown in [Figure 5-17](#resnet-from-scratch-confusion-matrix). This gives a detailed
    view of how predictions align with ground truth labels—including which classes
    are commonly confused:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型性能的另一种有用方法是混淆矩阵，如图 [图 5-17](#resnet-from-scratch-confusion-matrix) 所示。这提供了预测与真实标签对齐的详细视图——包括哪些类别经常被混淆：
- en: '[PRE62]'
  id: totrans-575
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '![](assets/dlfb_0517.png)'
  id: totrans-576
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0517.png)'
- en: Figure 5-17\. Confusion matrix for the `SimpleCnn` model on the nine-class skin
    lesion classification task. The rows represent the true labels, and the columns
    represent the predicted labels. Diagonal cells show correct predictions; off-diagonal
    cells indicate misclassifications. Each cell also includes example validation
    images for qualitative inspection, with the count of predictions shown in the
    lower-right. Precision (P:) and recall (R:) scores are provided per class.
  id: totrans-577
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-17\. 在九类皮肤病变分类任务上 `SimpleCnn` 模型的混淆矩阵。行代表真实标签，列代表预测标签。对角线单元格显示正确预测；对角线外的单元格表示错误分类。每个单元格还包括用于定性检查的示例验证图像，并在右下角显示预测计数。每个类别都提供了精确度（P:)
    和召回率（R:) 分数。
- en: 'Some guidance on reading this plot—which we’ll use throughout the chapter:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何阅读此图的指导——我们将在本章中一直使用：
- en: 'Diagonal is good: These represent correctly classified images. An ideal model
    would have predictions only on the diagonal.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对角线是好的：这些代表被正确分类的图像。一个理想的模型只会对角线上的预测。
- en: 'Off-diagonal is bad: These are misclassifications.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对角线外的部分是坏的：这些是错误分类。
- en: The number in the lower-right of each square indicates how many examples fall
    into that cell (in the validation set). Many confusion matrices simply report
    the counts here, but we also include actual images so that you can start to build
    intuition over what the different lesion types look like.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个方块的右下角的数字表示有多少个示例落入该单元格（在验证集中）。许多混淆矩阵只是在这里报告计数，但我们还包括实际图像，这样你就可以开始了解不同病变类型的外观。
- en: Precision (`P:`) is shown above each column (per predicted class), and recall
    (`R:`) is shown along each row (per true class).
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确度（`P:`）显示在每个列（每个预测类别）上方，而召回率（`R:`）显示在每个行（每个真实类别）上。
- en: The weighted precision and recall reported in the previous training plots are
    not simple averages of these values—they are class-weighted, meaning more common
    classes contribute more to the overall metric.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在之前的训练图中报告的加权精确率和召回率并不是这些值的简单平均值——它们是类别加权的，这意味着更常见的类别对整体指标贡献更多。
- en: 'With that in mind, interpreting the confusion matrix in [Figure 5-17](#resnet-from-scratch-confusion-matrix)
    specifically, we can see that:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，具体解释 [图 5-17](#resnet-from-scratch-confusion-matrix) 中的混淆矩阵，我们可以看到：
- en: This model is not that great—there are many off-diagonal entries.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个模型并不出色——有许多对角线外的条目。
- en: The model generally performs better on more common classes and struggles with
    rarer ones—we’ll try resampling later to address this.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型通常在更常见的类别上表现更好，而在罕见类别上表现较差——我们稍后会尝试重采样来解决这个问题。
- en: It performs best on vascular lesions (bottom-right corner), which are visually
    distinct and easier to recognize, even to untrained eyes.
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在血管病变（右下角）上表现最好，这些病变在视觉上明显且易于识别，即使是未经训练的眼睛也能识别。
- en: The model also does relatively well on pigmented benign keratosis, nevus, and
    melanoma—though there’s significant room for improvement.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在色素性良性角化病、痣和黑色素瘤上也相对表现良好——尽管还有很大的改进空间。
- en: A key issue is that melanomas are often misclassified as nevi or other benign
    categories. This is particularly concerning given the clinical importance of detecting
    melanoma. We’ll keep a close eye on whether more advanced models reduce these
    errors.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个关键问题是，黑色素瘤常常被错误地分类为痣或其他良性类别。考虑到检测黑色素瘤的临床重要性，这一点尤其令人担忧。我们将密切关注是否更先进的模型能减少这些错误。
- en: With this baseline in mind, let’s turn our attention to the `ResNetFromScratch`
    model.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这个基线，让我们将注意力转向 `ResNetFromScratch` 模型。
- en: Training the ResNetFromScratch Model
  id: totrans-591
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练 `ResNetFromScratch` 模型
- en: Recall that this model follows the ResNet architecture but starts from randomly
    initialized weights—meaning it will probably require the most training time to
    reach strong performance compared to models that makes use of pretrained weights.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这个模型遵循 ResNet 架构，但始于随机初始化的权重——这意味着与使用预训练权重的模型相比，它可能需要更多的时间来达到强大的性能。
- en: Before training, we need to set up both the dataset and model instance. We are
    aware of the class imbalance and small dataset size, and we have already prepared
    techniques like `balanced_sampler` and `rich_augmentor` to mitigate these issues.
    However, to better understand their individual impact, we will first run training
    without any augmentation or class balancing.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，我们需要设置数据集和模型实例。我们意识到类别不平衡和数据集大小较小，并且我们已经准备了像 `balanced_sampler` 和 `rich_augmentor`
    这样的技术来缓解这些问题。然而，为了更好地理解它们各自的影响，我们将首先在没有任何增强或类别平衡的情况下运行训练。
- en: For this model, we do not need to apply `resnet` preprocessing to the images,
    since we are training from scratch and don’t require compatibility with pretrained
    ResNet weights.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个模型，我们不需要对图像应用 `resnet` 预处理，因为我们是从零开始训练，并且不需要与预训练的 ResNet 权重兼容。
- en: 'Now, let’s train our model:'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练我们的模型：
- en: '[PRE63]'
  id: totrans-596
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'As before, let’s evaluate the model’s performance by plotting its metrics over
    the training steps. The results are shown in [Figure 5-18](#fine-tuned-resnet-learning-schedule):'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，让我们通过绘制模型在训练步骤上的指标来评估模型性能。结果如图 [图 5-18](#fine-tuned-resnet-learning-schedule)
    所示：
- en: '[PRE64]'
  id: totrans-598
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '![](assets/dlfb_0518.png)'
  id: totrans-599
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0518.png)'
- en: Figure 5-18\. Model performance over training and validation steps for the `ResNetFromScratch`
    multiclass skin lesion classification model.
  id: totrans-600
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-18\. `ResNetFromScratch` 多类皮肤病变分类模型在训练和验证步骤中的模型性能。
- en: The results are promising and already show an improvement over the baseline—precision
    and recall metrics are significantly higher. This makes sense, as the ResNet architecture
    is generally strong for image tasks.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 结果很有希望，并且已经显示出与基线相比的改进——精确率和召回率指标显著提高。这是有道理的，因为 ResNet 架构通常对图像任务很强。
- en: Importantly, both train and validation metrics increase steadily and remain
    closely aligned. Likewise, the train and validation losses both decrease over
    time, suggesting the model is not significantly overfitting. In fact, the training
    curves indicate that the model is still learning—so with more training steps,
    we might be able to achieve even better results.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，训练和验证指标稳步增加，并且保持紧密一致。同样，训练和验证损失随时间下降，表明模型没有显著过拟合。事实上，训练曲线表明模型仍在学习——因此，随着更多训练步骤的增加，我们可能会取得更好的结果。
- en: 'To see which classes benefit most from this improved model, let’s look again
    at the confusion matrix in [Figure 5-19](#fig518NEW):'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到哪些类别从这种改进的模型中受益最大，让我们再次查看 [图 5-19](#fig518NEW) 中的混淆矩阵：
- en: '[PRE65]'
  id: totrans-604
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '![](assets/dlfb_0519.png)'
  id: totrans-605
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0519.png)'
- en: Figure 5-19\. Confusion matrix for the `ResNetFromScratch` model on the nine-class
    skin lesion classification task.
  id: totrans-606
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-19\. 在九类皮肤病变分类任务中 `ResNetFromScratch` 模型的混淆矩阵。
- en: 'Some observations:'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 一些观察结果：
- en: 'Overall improvement: We see better recall and precision across most categories
    compared to the baseline model. The model is generally more confident and more
    accurate. The diagonal is visibly stronger overall, indicating more correct predictions.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体改进：与基线模型相比，我们在大多数类别中看到了更好的召回率和精确率。模型通常更加自信和准确。对角线整体上看起来更强，表明有更多的正确预测。
- en: Vascular lesions are now being classified almost perfectly (bottom-right corner).
    These lesions are visually distinctive, and the model is clearly picking up on
    that.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 血管病变现在几乎被完美分类（右下角）。这些病变在视觉上很独特，模型显然注意到了这一点。
- en: Nevus and melanoma both show solid improvements in recall. However, there is
    still some confusion between the two, as seen in the off-diagonal cells between
    those classes.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 痣和黑色素瘤在召回率方面都有显著的提高。然而，正如这些类别之间的非对角线单元格所示，两者之间仍然存在一些混淆。
- en: Pigmented benign keratosis shows a marked improvement in both precision and
    recall, with a notable jump from the baseline.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 痣性良性角化病在精确度和召回率方面都有显著改善，与基线相比有明显的跳跃。
- en: Actinic keratosis and seborrheic keratosis are now never predicted by the model—resulting
    in `NaN` precision values for those classes. This is odd and is something to revisit
    with future models.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日光性角化病和脂溢性角化病现在永远不会被模型预测——导致这些类别的精确度值为`NaN`。这很奇怪，是未来模型需要重新审视的问题。
- en: We now move on to the `FinetunedHeadResNet`, which incorporates pretrained ImageNet
    features. This should give us a much stronger model, especially given the small
    size of our training dataset.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在转向`FinetunedHeadResNet`，它结合了预训练的ImageNet特征。这应该给我们一个更强的模型，特别是考虑到我们的训练数据集很小。
- en: Training the FinetunedHeadResNet Model
  id: totrans-614
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练FinetunedHeadResNet模型
- en: This time around, we need to be careful about image preprocessing. Since we’re
    using pretrained weights from a ResNet model originally trained on ImageNet, our
    input images must be preprocessed in the same way as those used for ImageNet.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们需要小心图像预处理。由于我们使用的是在ImageNet上预先训练的ResNet模型的预训练权重，我们的输入图像必须以与用于ImageNet的相同方式预处理。
- en: Warning
  id: totrans-616
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: If the images deviate too much—for example, if they’re scaled differently or
    normalized inconsistently—the pretrained features may no longer be as applicable.
    This can force the model to overcorrect during training, reducing the benefits
    of transfer learning.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图像偏差太大——例如，如果它们缩放不同或归一化不一致——预训练的特征可能不再适用。这可能会迫使模型在训练过程中过度纠正，从而减少迁移学习的益处。
- en: 'To handle this, we apply the `resnet` preprocessing function, which ensures
    that input images are resized and normalized in a way that matches the expectations
    of the pretrained ResNet backbone:'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这个问题，我们应用`resnet`预处理函数，确保输入图像以与预训练的ResNet主干期望匹配的方式进行调整大小和归一化：
- en: '[PRE66]'
  id: totrans-619
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: We are now ready to train the `FinetunedHeadResNet` model, which freezes the
    pretrained backbone and trains only the classification head. To isolate the effects
    of transfer learning, we keep the setup minimal—no regularization, augmentation,
    or class rebalancing is applied at this stage.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备训练`FinetunedHeadResNet`模型，该模型冻结预训练的主干，仅训练分类头。为了隔离迁移学习的影响，我们保持设置最小——在这个阶段不应用正则化、增强或类别重平衡。
- en: '[PRE67]'
  id: totrans-621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Let’s examine the model’s performance in [Figure 5-20](#fine-tuned-resnet-evaluation-plot).
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查[图5-20](#fine-tuned-resnet-evaluation-plot)中模型的性能。
- en: '[PRE68]'
  id: totrans-623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '![](assets/dlfb_0520.png)'
  id: totrans-624
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0520.png)'
- en: Figure 5-20\. Model performance over training and validation steps for the `FinetunedHeadResNet`
    multiclass skin lesion classification model.
  id: totrans-625
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-20. `FinetunedHeadResNet` 多类皮肤病变分类模型在训练和验证步骤中的模型性能。
- en: 'Some observations:'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 一些观察结果：
- en: 'Loss curves: The training loss steadily decreases, as expected, but the validation
    loss begins to rise again after around step 250\. This is a classic sign of overfitting—the
    model is becoming more confident on the training set, but its generalization to
    new data begins to degrade.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失曲线：训练损失如预期地稳步下降，但验证损失在大约250步之后又开始上升。这是过拟合的经典迹象——模型对训练集的信心越来越强，但将其推广到新数据的能力开始下降。
- en: 'Train vs. validation gap: There is still a large gap between training and validation
    metrics, though it’s smaller than what we saw with the `SimpleCnn` baseline.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练与验证差距：尽管比`SimpleCnn`基线小，但训练和验证指标之间仍然存在很大的差距。
- en: 'Precision and recall: Interestingly, while the validation loss increases, the
    validation precision and recall metrics remain relatively stable. This suggests
    that although the model’s softmax confidence may be worsening, its actual classification
    decisions are not deteriorating significantly as training proceeds—at least not
    yet.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确度和召回率：有趣的是，尽管验证损失在增加，但验证精确度和召回率指标相对稳定。这表明，尽管模型的softmax置信度可能恶化，但它的实际分类决策在训练过程中并没有显著恶化——至少目前还没有。
- en: 'Plateauing metrics: Validation performance levels off around step 250\. This
    likely reflects the fact that we are only training the classification head—the
    majority of the model (the ResNet backbone) remains frozen. That limited flexibility
    may be capping performance, despite early gains.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标停滞：验证性能在大约250步时趋于平稳。这很可能反映了我们只训练分类头——模型的大部分（ResNet主干）仍然保持冻结。这种有限的灵活性可能限制了性能，尽管早期有所提升。
- en: It’s worth noting that validation performance here is not noticeably better
    than what we achieved by training ResNet from scratch (though notice how much
    faster `FinetunedHeadResNet` trained). This may suggest that training only the
    classification head doesn’t provide enough flexibility for the model to properly
    adapt to our dataset. The much higher training performance relative to validation
    performance also points to overfitting. While we could try to address this with
    regularization or augmentation, we’ll instead move on to what we expect will be
    a significantly stronger approach—fine-tuning the entire network.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，这里的验证性能并没有比我们从零开始训练ResNet所达到的性能明显更好（尽管请注意`FinetunedHeadResNet`训练得多快）。这可能表明仅训练分类头不足以提供足够的灵活性，使模型能够正确适应我们的数据集。相对于验证性能而言，训练性能显著更高也指向了过拟合。虽然我们可以尝试通过正则化或增强来解决此问题，但我们将转而进行我们预期将是一个显著更强的方法——微调整个网络。
- en: Training the FinetunedResNet Model
  id: totrans-632
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练FinetunedResNet模型
- en: 'Next, we’ll train a model that updates all the pretrained weights—not just
    the classification head. This approach, known as full fine-tuning, allows the
    model to gradually adapt its internal feature representations to better match
    our dataset:'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将训练一个更新所有预训练权重的模型——而不仅仅是分类头。这种方法被称为完全微调，允许模型逐渐调整其内部特征表示，以更好地匹配我们的数据集：
- en: '[PRE69]'
  id: totrans-634
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Let’s examine the model’s performance in [Figure 5-21](#last-lesion-model-multiclass-performance-over-steps):'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查[图5-21](#last-lesion-model-multiclass-performance-over-steps)中模型的性能：
- en: '[PRE70]'
  id: totrans-636
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '![](assets/dlfb_0521.png)'
  id: totrans-637
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0521.png)'
- en: Figure 5-21\. Model performance over training and validation steps for the initial
    (non-optimized) `FinetunedResNet` multiclass skin lesion classification model.
  id: totrans-638
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-21\. 初始（非优化）`FinetunedResNet`多类皮肤病变分类模型在训练和验证步骤中的模型性能。
- en: 'There are a few takeaways from these plots:'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些图中我们可以得出以下几点结论：
- en: 'Comparison to previous models: This model clearly outperforms the baseline
    `SimpleCnn`, the `ResNetFromScratch`, and the `FinetunedHeadResNet` models in
    terms of validation metrics.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与先前模型的比较：在验证指标方面，该模型明显优于基线`SimpleCnn`、`ResNetFromScratch`和`FinetunedHeadResNet`模型。
- en: 'Precision and recall: Validation metrics are consistently higher than in previous
    models—with weighted precision and recall reaching around 0.65–0.7 by the end
    of training. However, there is still a noticeable gap between training and validation
    performance, indicating that overfitting remains an issue we’ll need to address.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确度和召回率：验证指标始终高于先前模型——经过训练后，加权精确度和召回率达到约0.65–0.7。然而，训练和验证性能之间仍然存在明显的差距，这表明过拟合仍然是我们需要解决的问题。
- en: 'Loss curves: The training loss steadily drops to near-zero, while the validation
    loss stays high and rises gradually. This is consistent with overfitting, although
    it’s worth remembering that rising softmax loss doesn’t always reflect a drop
    in classification quality (and indeed, here the validation metrics stay stable
    while validation loss rises).'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失曲线：训练损失稳步下降至接近零，而验证损失保持较高并逐渐上升。这与过拟合一致，尽管值得记住的是，上升的softmax损失并不总是反映分类质量的下降（实际上，在这里验证指标保持稳定，而验证损失上升）。
- en: In summary, this is our best-performing model so far—confirming the benefit
    of full fine-tuning when using pretrained backbones. Before examining the confusion
    matrix of the fully fine-tuned model, let’s first apply a few targeted optimizations
    to further improve headline metric performance.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这是我们迄今为止表现最好的模型——证实了在使用预训练骨干时完全微调的好处。在检查完全微调模型的混淆矩阵之前，让我们首先应用一些有针对性的优化，以进一步提高关键指标的性能。
- en: Optimizing the FinetunedResNet Model
  id: totrans-644
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化FinetunedResNet模型
- en: 'We’ll apply four key modifications:'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用四个关键修改：
- en: A warmup plus cosine decay learning rate schedule
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个预热加余弦衰减的学习率计划
- en: Data augmentation (as introduced in [“Augmenting the dataset”](#augmenting-the-dataset))
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强（如[“增强数据集”](#augmenting-the-dataset)中介绍）
- en: High-rate dropout to reduce overfitting (set to `0.7`)
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高比率dropout以减少过拟合（设置为`0.7`）
- en: Weight decay using the `adamw` optimizer (set to `1e-4`)
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`adamw`优化器进行权重衰减（设置为`1e-4`）
- en: We’ll walk through each of these in turn.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将依次介绍这些内容。
- en: Learning rate schedule
  id: totrans-651
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习率计划
- en: 'Fine-tuning a pretrained model requires care to preserve the valuable low-level
    features it learned from large datasets like ImageNet. A common strategy is to
    use a *warmup learning rate schedule*, which starts training with a small learning
    rate that gradually increases—allowing the model to adapt gently—before decaying
    smoothly over time. This helps:'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 微调预训练模型需要小心，以保留从像ImageNet这样的大型数据集中学习到的有价值的基本特征。一种常见策略是使用*预热学习率计划*，它以较小的学习率开始训练，并逐渐增加——允许模型温和地适应——然后随着时间的推移平滑衰减。这有助于：
- en: Stabilize training in the early stages
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在早期阶段稳定训练
- en: Prevent abrupt updates to pretrained weights
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止对预训练权重进行突然更新
- en: Allow gradual adaptation to our new task
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许逐渐适应我们的新任务
- en: 'We’ll use a warmup and cosine decay schedule where the learning rate will:'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用预热和余弦衰减计划，其中学习率将：
- en: Warm up for the first 20% of steps, gradually increasing the learning rate
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前20%的步骤中进行预热，逐渐增加学习率
- en: Peak at a learning rate of `0.001` (`1e-3`)
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在学习率为`0.001`（`1e-3`）时达到峰值
- en: Smooth decay to `0.00001` (`1e-5`) by the end of training
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到训练结束时平滑衰减到`0.00001`（`1e-5`）
- en: 'Let’s visualize how the learning rate evolves in [Figure 5-22](#fig521NEW):'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化学习率在[图5-22](#fig521NEW)中的演变：
- en: '[PRE71]'
  id: totrans-661
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '![](assets/dlfb_0522.png)'
  id: totrans-662
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0522.png)'
- en: Figure 5-22\. Learning rate evolution during training, following a warmup-cosine
    decay schedule. The learning rate starts small, gradually increases over the first
    20% of training steps (warmup), then peaks and decays smoothly.
  id: totrans-663
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-22\. 训练过程中学习率的变化，遵循预热余弦衰减计划。学习率开始较小，在训练步骤的前20%（预热）中逐渐增加，然后达到峰值并平滑衰减。
- en: Data augmentation
  id: totrans-664
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据增强
- en: We previously observed overfitting even in our best model. One way to mitigate
    this is to apply data augmentation, which increases the effective diversity of
    the training set by introducing small, label-preserving transformations (e.g.,
    flips, brightness jitter, crops). As discussed earlier in this chapter, augmentation
    helps the model generalize more robustly by preventing it from memorizing the
    training images.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前观察到即使在我们的最佳模型中也存在过拟合。减轻这种过拟合的一种方法是通过应用数据增强，通过引入小的、标签保留的变换（例如，翻转、亮度抖动、裁剪）来增加训练集的有效多样性。正如本章前面所讨论的，增强有助于模型更稳健地泛化，因为它防止模型记住训练图像。
- en: We’ll apply these augmentation strategies by using the `rich_augmentor` defined
    earlier in the chapter.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用本章前面定义的`rich_augmentor`来应用这些增强策略。
- en: Regularization via dropout and adamw
  id: totrans-667
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过dropout和adamw进行正则化
- en: 'In addition to data augmentation, we’ll also apply explicit regularization
    using two methods:'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据增强外，我们还将应用两种方法的显式正则化：
- en: Dropout
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout
- en: Randomly disables neurons during training, encouraging the network to rely on
    distributed representations and reducing co-adaptation. We set a high dropout
    rate of 0.7 to strongly combat overfitting.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中随机禁用神经元，鼓励网络依赖分布式表示并减少协同适应。我们设置了高dropout率0.7以强烈对抗过拟合。
- en: Weight decay
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 权重衰减
- en: Discourages overly large weights by penalizing them in the loss function. We
    apply this using `optax.adamw`, which is designed to work correctly with L2 regularization.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在损失函数中对过大的权重进行惩罚来阻止过大的权重。我们使用`optax.adamw`来实现这一点，它被设计为与L2正则化一起正确工作。
- en: Warning
  id: totrans-673
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: You cannot safely use L2 regularization with the standard Adam optimizer—it
    won’t apply weight decay in the intended way. This is exactly why `adamw` was
    introduced, and it should be used when adding weight decay.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 您不能安全地使用标准Adam优化器与L2正则化一起使用——它不会以预期的方式应用权重衰减。这正是引入`adamw`的原因，并且在添加权重衰减时应使用它。
- en: 'It’s also standard practice *not* to apply weight decay to:'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 标准做法是*不*对以下内容应用权重衰减：
- en: Bias parameters
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差参数
- en: These are typically small and few in number and don’t contribute substantially
    to model complexity. Regularizing them doesn’t tend to help generalization and
    can sometimes hurt performance.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 这些通常很小且数量很少，并不显著增加模型复杂性。对它们进行正则化通常不会帮助泛化，有时甚至可能损害性能。
- en: Batch normalization parameters
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化参数
- en: These include the learned scale (`gamma`) and shift (`beta`) terms, as well
    as running statistics. Applying weight decay to these can destabilize training,
    especially in fine-tuning scenarios, as they control the distribution of activations
    rather than model capacity.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包括学习到的尺度（`gamma`）和偏移（`beta`）项，以及运行统计量。对这些应用权重衰减可能会使训练不稳定，尤其是在微调场景中，因为它们控制激活的分布而不是模型容量。
- en: Instead, weight decay is usually applied only to the main weights of convolutional
    or linear layers, where it can help prevent overfitting by discouraging overly
    complex solutions. To enforce this, we’ll use a parameter mask when constructing
    the optimizer—applying decay only to the relevant parameters.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，权重衰减通常只应用于卷积层或线性层的主要权重，这有助于通过阻止过于复杂的解决方案来防止过拟合。为了强制执行这一点，我们在构建优化器时将使用参数掩码——仅对相关参数应用衰减。
- en: Training the Optimized FinetunedResNet Model
  id: totrans-681
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练优化的FinetunedResNet模型
- en: 'Now that we’ve introduced a learning rate schedule, data augmentation, dropout,
    and weight decay—it’s time to put everything together. We can train the optimized
    `FinetunedResNet` model and see whether these changes help reduce overfitting
    and improve validation performance:'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经引入了学习率调度、数据增强、dropout和权重衰减——是时候把所有东西放在一起了。我们可以训练优化的`FinetunedResNet`模型，看看这些变化是否有助于减少过拟合并提高验证性能：
- en: '[PRE72]'
  id: totrans-683
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Now, let’s analyze the model’s performance over the training steps, as shown
    in [Figure 5-23](#fig522NEW):'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们分析模型在训练步骤中的性能，如图[图5-23](#fig522NEW)所示：
- en: '[PRE73]'
  id: totrans-685
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '![](assets/dlfb_0523.png)'
  id: totrans-686
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0523.png)'
- en: Figure 5-23\. Model performance over training and validation steps for the optimized
    `FinetunedResNet` multiclass skin lesion classification model.
  id: totrans-687
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-23\. 优化的`FinetunedResNet`多类皮肤病变分类模型在训练和验证步骤中的模型性能。
- en: 'This training run demonstrates the cumulative benefit of regularization and
    optimization strategies. Some observations:'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 这次训练运行展示了正则化和优化策略的累积效益。一些观察结果：
- en: 'Precision and recall: Both metrics improve steadily on both the training and
    validation sets. Most notably, the validation precision and recall reach nearly
    0.75 by the end of training—our best performance so far. The gap between training
    and validation metrics is also now narrower and more stable.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精度和召回率：这两个指标在训练集和验证集上都稳步提高。最值得注意的是，验证集的精度和召回率在训练结束时接近0.75——这是我们迄今为止的最佳性能。训练和验证指标之间的差距现在也更窄且更稳定。
- en: 'Loss curves: While the validation loss remains higher than training loss, it’s
    still lower than before and appears to not impact the validation metrics—a major
    improvement.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失曲线：尽管验证损失仍然高于训练损失，但它仍然低于之前，并且似乎不会影响验证指标——这是一个重大的改进。
- en: In short, we’ve trained a model that generalizes much better and achieves strong,
    stable performance across all metrics. This serves as a great foundation for further
    experimentation.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们已经训练了一个泛化能力更强的模型，并在所有指标上实现了强大、稳定的性能。这为进一步的实验提供了一个很好的基础。
- en: 'To better understand this model’s behavior, let’s plot the confusion matrix
    as [Figure 5-24](#fig524):'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个模型的行为，让我们绘制混淆矩阵作为[图5-24](#fig524)：
- en: '[PRE74]'
  id: totrans-693
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Some observations about the model’s behavior:'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 关于模型行为的观察：
- en: 'Most predictions lie along the diagonal: This indicates strong overall performance
    and class-specific accuracy. Nearly all classes show high recall, with vascular
    lesions and basal cell carcinoma especially well classified.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数预测都位于对角线上：这表明整体性能强劲且类别特定准确率高。几乎所有类别都显示出高召回率，特别是血管病变和基底细胞癌被很好地分类。
- en: 'Melanoma versus nevus: While there are still some melanoma cases being misclassified
    as nevi, the number is much lower than in earlier models and melanoma recall is
    the highest level yet.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黑色素瘤与痣：尽管仍有少数黑色素瘤病例被误分类为痣，但数量远低于早期模型，黑色素瘤的召回率也是前所未有的最高。
- en: 'Rare classes: Performance on rare classes like actinic keratosis, dermatofibroma,
    and squamous cell carcinoma has improved. These categories are now being correctly
    identified more consistently, with fewer scattered misclassifications.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀有类别：对像角化棘层、皮肤纤维瘤和鳞状细胞癌这样的稀有类别的性能有所提高。这些类别现在被更一致地正确识别，误分类的分散情况更少。
- en: 'Emptier off-diagonal cells: Many potential misclassification combinations are
    completely empty. This indicates that the model is no longer making widespread
    or erratic mistakes—it’s more selective and confident.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空白对角线单元格：许多潜在的误分类组合完全为空。这表明模型不再犯广泛的或随机的错误——它变得更加有选择性和自信。
- en: 'Visual inspection: The image thumbnails in each cell help confirm that correct
    predictions tend to look visually consistent, and many of the remaining errors
    involve subtle or understandable confusions.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视检查：每个单元格中的图像缩略图有助于确认正确的预测在视觉上通常是一致的，而许多剩余的错误涉及细微或可理解的混淆。
- en: 'Altogether, this confusion matrix reinforces the earlier metrics: the optimized
    model is significantly more accurate, more reliable, and ultimately would be more
    clinically useful than earlier versions.'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这个混淆矩阵加强了早期的指标：优化的模型在准确性、可靠性方面显著提高，最终会比早期版本更具有临床实用性。
- en: '![](assets/dlfb_0524.png)'
  id: totrans-701
  prefs: []
  type: TYPE_IMG
  zh: '![图5-24](assets/dlfb_0524.png)'
- en: Figure 5-24\. Confusion matrix for the final optimized `FinetunedResNet` model
    on the nine-class classification task.
  id: totrans-702
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-24. 在九类分类任务上最终优化的`FinetunedResNet`模型的混淆矩阵。
- en: Tip
  id: totrans-703
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Confusion matrices can reveal a lot—but they won’t turn us into dermatologists.
    Interpreting model errors often requires deep domain knowledge. Collaborating
    with subject matter experts, like dermatologists in this case, can provide critical
    insights into misclassifications, uncover hidden biases in the data, and suggest
    clinically meaningful improvements. For any real-world machine learning project,
    involving domain experts early and often is one of the most effective ways to
    build useful, trustworthy models.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵可以揭示很多信息——但它们不会让我们变成皮肤科医生。解释模型错误通常需要深厚的领域知识。与领域专家合作，例如在本例中的皮肤科医生，可以提供对误分类的深入见解，揭示数据中的隐藏偏差，并提出具有临床意义的改进。对于任何实际的机器学习项目，尽早和经常涉及领域专家是构建有用、可信模型的最有效方法之一。
- en: Further Improving the Model
  id: totrans-705
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步改进模型
- en: 'We’ll stop optimizing this model for now, but it’s far from perfect—for example,
    it completely fails to predict seborrheic keratosis. Here are several ideas for
    pushing it further, roughly ordered by expected bang for your buck:'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将停止优化这个模型，但它远非完美——例如，它完全无法预测脂溢性角化病。以下是一些推动其进一步发展的想法，按预期效果排序：
- en: Longer training
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 更长的训练时间
- en: Metrics were still improving slightly at the end of 2000 steps. Training for
    longer could yield further gains, especially since we are using a warmup schedule
    (which starts learning slowly to protect pretrained weights).
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 在2000步结束时，指标仍在略微改善。更长时间的训练可能会带来进一步的收益，尤其是因为我们使用的是预热计划（它开始学习缓慢以保护预训练权重）。
- en: Ensembling
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 集成
- en: 'Combine predictions from multiple trained models to reduce variance and improve
    robustness:'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 结合多个训练模型的预测以减少方差并提高鲁棒性：
- en: The most common ensembling approach here would be to average the softmax probabilities
    across models. Alternatively, you could use majority voting on predicted labels
    (less smooth, but sometimes effective).
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里最常见的方法是将模型之间的softmax概率平均值。或者，您可以在预测标签上进行多数投票（不够平滑，但有时有效）。
- en: You can ensemble models trained with different random seeds (e.g., 2, 4, or
    8), or even ensemble structurally different models—such as the various ResNet
    models we’ve explored.
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以集成使用不同随机种子训练的模型（例如，2、4或8），或者甚至集成结构上不同的模型——例如我们探索的各种ResNet模型。
- en: Exploring augmentation strategies
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 探索增强策略
- en: There’s still plenty of room to be more creative with data augmentation. Try
    playing around with our augmentation setup (e.g., the specific transformations
    we encoded) and check their effect on model learning.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据增强方面还有很多空间可以发挥创意。尝试调整我们的增强设置（例如，我们编码的具体转换）并检查它们对模型学习的影响。
- en: Class re-weighting
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 类别重新加权
- en: Increase the loss contribution from rare or clinically important classes like
    melanoma. This can be done by computing inverse class frequencies and passing
    per-example weights into the loss.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 增加罕见或临床重要的类别（如黑色素瘤）的损失贡献。这可以通过计算类频率的倒数并将每个样本的权重传递到损失中来实现。
- en: Hard example mining
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 硬例子挖掘
- en: Focus on examples that are consistently misclassified (e.g., melanoma confused
    with nevus). You could maintain a pool of high-loss samples and upsample them
    in training batches, or alternatively, increase loss weights for these types of
    samples.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 专注于那些持续被误分类的例子（例如，黑色素瘤被误诊为痣）。你可以维护一个高损失样本池，并在训练批次中对这些样本进行上采样，或者相反，增加这些类型样本的损失权重。
- en: Stronger regularization
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 更强的正则化
- en: 'Tune `dropout_rate` and `weight_decay`, or explore additional regularization
    methods such as:'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 调整`dropout_rate`和`weight_decay`，或者探索额外的正则化方法，例如：
- en: Label smoothing
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 标签平滑
- en: Instead of using hard one-hot labels, you assign most of the probability to
    the true class (e.g., 0.9) and distribute the rest evenly across other classes.
    This reduces model overconfidence and can improve generalization.
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用硬的one-hot标签，您将大部分概率分配给真实类别（例如，0.9），并将剩余的均匀分配给其他类别。这减少了模型的过度自信，并可以提高泛化能力。
- en: Mixup or CutMix
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: Mixup或CutMix
- en: Blend or patch together two images and their labels. These techniques regularize
    the model and encourage the model to learn more robust decision boundaries between
    classes.
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 将两张图像及其标签混合或拼接在一起。这些技术可以正则化模型并鼓励模型在类别之间学习更鲁棒的决策边界。
- en: Multimodal data
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态数据
- en: Dig into the original dataset to see if you can incorporate metadata such as
    patient age or sex, anatomical site of the lesion, or dermoscopic features. These
    features can be embedded (e.g., as one-hot or learned embeddings) and concatenated
    with image features before classification. Metadata often provides key disambiguating
    context that pixels alone can’t capture.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 深入原始数据集，看看你是否可以整合诸如患者年龄或性别、病变的解剖部位或皮肤镜特征等元数据。这些特征可以嵌入（例如，作为独热编码或学习嵌入）并与图像特征连接起来，然后再进行分类。元数据通常提供了像素本身无法捕捉的关键区分背景。
- en: Tip
  id: totrans-727
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: It can be hard to know where to start when trying to improve a model—and changes
    can interact in unexpected ways. Progress is rarely linear, so think of it as
    an exploration process. Use intuition, literature, and diagnostics to guide you—and
    have fun getting to know your model.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试改进模型时，很难知道从哪里开始——变化可能会以意想不到的方式相互作用。进步很少是线性的，所以把它看作是一个探索过程。利用直觉、文献和诊断来引导你——并且享受了解你的模型的过程。
- en: Summary
  id: totrans-729
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Classifying skin lesions is a genuinely difficult task—if it weren’t, early
    detection would be routine, and fewer cases would be missed. Still, the models
    we built in this chapter demonstrate how machine learning can meaningfully assist
    in this challenge, offering scalable tools to support (not replace) clinical expertise.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 皮肤病变的分类是一项真正的困难任务——如果不是这样，早期检测就会是常规操作，并且会错过更少的病例。尽管如此，我们本章构建的模型展示了机器学习如何在这个挑战中提供有意义的帮助，提供了可扩展的工具来支持（而不是取代）临床专业知识。
- en: This project came with plenty of constraints, especially the limitations of
    the dataset. We had to adapt carefully—testing model variants, countering overfitting,
    and making the most of each part of the training setup. While more powerful models
    and larger datasets exist, we hope the mindset and practical tools introduced
    here help you tackle similar problems—particularly when working with limited or
    imperfect data.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目有很多限制，特别是数据集的限制。我们必须谨慎适应——测试模型变体、对抗过拟合并充分利用训练设置的每个部分。虽然存在更强大的模型和更大的数据集，但我们希望这里介绍的心态和实用工具能帮助你解决类似的问题——尤其是在处理有限或不完善的数据时。
- en: 'The techniques we explored—class balancing, data augmentation, fine-tuning
    pretrained models, diagnosing overfitting, and incrementally improving architectures—are
    widely applicable. Whether you’re detecting lung disease in chest X-rays, identifying
    tumors in MRIs, or analyzing retinal images for diabetic retinopathy, the same
    principles apply: adapt to the data, learn from errors, and iterate thoughtfully.'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索的技术——类别平衡、数据增强、微调预训练模型、诊断过拟合和逐步改进架构——具有广泛的应用性。无论你是检测胸片中的肺部疾病、在MRI中识别肿瘤，还是分析视网膜图像以诊断糖尿病视网膜病变，相同的原理都适用：适应数据、从错误中学习，并深思熟虑地迭代。
- en: 'Now let’s move on. We’ve seen more than enough skin lesion images for one chapter.
    But before we go, a final reminder: keep an eye on your own skin. If something
    looks unusual or changes over time, don’t hesitate to seek medical advice. Early
    detection truly does save lives.'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续前进。我们已经看到了足够多的皮肤病变图像来写一章。但在我们离开之前，有一个最后的提醒：留意你自己的皮肤。如果有什么看起来不寻常或随时间变化，不要犹豫去寻求医疗建议。早期检测确实可以挽救生命。
- en: ^([1](ch05.html#id819-marker)) You can read about one such study [online](https://oreil.ly/YmXDg).
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.html#id819-marker)) 你可以在线阅读关于此类研究的一个例子[这里](https://oreil.ly/YmXDg)。
- en: '^([2](ch05.html#id820-marker)) Cassidy, Bill, et al., [“Analysis of the ISIC
    Image Datasets: Usage, Benchmarks and Recommendations”](https://doi.org/10.1016/j.media.2021.102305),
    *Medical Image Analysis* 75 (January 2022).'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.html#id820-marker)) Cassidy, Bill, 等人，[“ISIC图像数据集的分析：使用、基准和推荐”](https://doi.org/10.1016/j.media.2021.102305)，*医学图像分析*
    75 (2022年1月)。
- en: ^([3](ch05.html#id821-marker)) Statistics by the [World Health Organization
    (WHO)](https://oreil.ly/79RmS).
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch05.html#id821-marker)) 世界卫生组织（WHO）的统计数据。[World Health Organization (WHO)](https://oreil.ly/79RmS)。
- en: '^([4](ch05.html#id828-marker)) M. P. Salinas, et al., [“A systematic review
    and meta-analysis of artificial intelligence versus clinicians for skin cancer
    diagnosis”](https://doi.org/10.1038/s41746-024-01103-x). *NPJ Digital Medicine*
    7, no. 1 (2024): 125.'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch05.html#id828-marker)) M. P. Salinas，等人，[“人工智能与临床医生在皮肤癌诊断中的系统评价和荟萃分析”](https://doi.org/10.1038/s41746-024-01103-x)。*NPJ数字医学*
    7，第1期 (2024年)：125。
- en: ^([5](ch05.html#id848-marker)) He, K., Zhang, X., Ren, S., & Sun, J. (2015,
    December 10). [Deep residual learning for image recognition](https://oreil.ly/ZqFSo).
    arXiv.org.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch05.html#id848-marker)) 他，K.，张，X.，任，S.，& 孙，J. (2015年12月10日). [深度残差学习在图像识别中的应用](https://oreil.ly/ZqFSo).
    arXiv.org.
- en: '^([6](ch05.html#id863-marker)) Codella, N. C. F., et al. (2018). Skin lesion
    analysis toward melanoma detection: A challenge at the 2017 International symposium
    on biomedical imaging (ISBI), hosted by the international skin imaging collaboration
    (ISIC). 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI), 168–172\.
    https://doi.org/10.1109/isbi.2018.8363547'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch05.html#id863-marker)) 科代拉，N. C. F. 等. (2018). 皮肤病变分析以检测黑色素瘤：2017年国际生物医学成像研讨会（ISBI）的挑战，由国际皮肤成像合作组织（ISIC）主办。2022年第19届国际生物医学成像研讨会（ISBI），第168-172页。https://doi.org/10.1109/isbi.2018.8363547
- en: ^([7](ch05.html#id870-marker)) Shorten, C., & Khoshgoftaar, T. M. (2019). [A
    survey on Image Data Augmentation for Deep Learning](https://doi.org/10.1186/s40537-019-0197-0).
    *Journal of Big Data*, 6(1).
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch05.html#id870-marker)) 短，C.，& 科什戈法塔尔，T. M. (2019). [关于深度学习图像数据增强的综述](https://doi.org/10.1186/s40537-019-0197-0).
    *大数据杂志*，6(1).
