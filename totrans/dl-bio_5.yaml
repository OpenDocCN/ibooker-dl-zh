- en: Chapter 5\. Detecting Skin Cancer in Medical Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we focused on small-scale biological phenomena, such as
    the molecular properties of proteins, DNA sequences, and drug molecules. In this
    chapter, we will zoom out to a larger biological scale, applying deep learning
    to analyze tissue-level and disease-related processes. Specifically, we will train
    a skin cancer detection model to classify images of skin into various cancerous
    or benign categories.
  prefs: []
  type: TYPE_NORMAL
- en: This is an exciting application because deep learning models have made significant
    strides in skin analysis, with studies achieving dermatologist-level accuracy
    in distinguishing malignant from benign lesions since at least 2018.^([1](ch05.html#id819))
    While challenges remain in integrating these models into clinical workflows—such
    as regulatory approval, data standardization, and prediction explainability—their
    potential to assist medical professionals by enhancing early detection and reducing
    unnecessary biopsies is highly promising.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using skin cancer image data from the [International Skin Imaging
    Collaboration (ISIC)](https://oreil.ly/h2DiY), a project dedicated to advancing
    skin cancer imaging research and providing standardized datasets. Over the years,
    ISIC has released a range of challenges focused on skin lesion classification
    and pathology, with an increasing number of images available. To learn more, you
    can read this review paper of ISIC datasets and benchmarks.^([2](ch05.html#id820))
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we will use is available as the [“Skin Cancer ISIC” challenge on
    Kaggle](https://oreil.ly/_2jqU), making it well prepared and relatively easy to
    get started with. However, to ensure that we cover important lessons on handling
    real-world data challenges, we have intentionally chosen a dataset that is relatively
    small and has significant class imbalance, allowing us to explore techniques for
    mitigating these issues.
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of working with image data is that humans are naturally skilled
    at interpreting visual information, enabling us to sanity-check both the dataset
    and model predictions. Throughout this chapter, we will examine many images to
    guide our modeling decisions. This will also highlight why skin cancer classification
    is a challenging problem—not only for humans, but for deep learning models as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of models, this chapter focuses on convolutional neural networks (CNNs)—specifically,
    ResNet CNNs, which have demonstrated strong performance across a wide range of
    image classification tasks. If you’d like to explore alternative approaches, consider
    checking out discussions and notebooks shared by other users on the [Kaggle discussion
    board](https://oreil.ly/bUHNt).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As always, to get the most out of this chapter, keep the companion Colab notebook
    from our repo open as you read. Experimenting with the code as you go will deepen
    your understanding and make the concepts stick.
  prefs: []
  type: TYPE_NORMAL
- en: Biology Primer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let’s introduce the biological phenomenon our models will address: skin
    cancer, its different types, and the challenges of classifying them.'
  prefs: []
  type: TYPE_NORMAL
- en: Skin Cancer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Skin cancer is the most common type of cancer worldwide, with an estimated 1.5
    million new cases in 2022.^([3](ch05.html#id821)) It encompasses a wide range
    of conditions caused by the abnormal growth of skin cells, often driven by a combination
    of genetic factors and environmental carcinogens such as ultraviolet (UV) radiation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will examine both malignant (cancerous) and benign lesions.
    The term *lesion* broadly refers to any mark or abnormality on the skin, ranging
    from harmless growths to those requiring medical intervention. The following are
    some of the most common types:'
  prefs: []
  type: TYPE_NORMAL
- en: Malignant skin cancers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Basal cell carcinoma
  prefs: []
  type: TYPE_NORMAL
- en: The most common type of skin cancer, usually slow-growing and rarely spreading
    to other organs.
  prefs: []
  type: TYPE_NORMAL
- en: Squamous cell carcinoma
  prefs: []
  type: TYPE_NORMAL
- en: Another common type, typically localized but capable of becoming invasive if
    left untreated.
  prefs: []
  type: TYPE_NORMAL
- en: Melanoma
  prefs: []
  type: TYPE_NORMAL
- en: The deadliest form of skin cancer, known for its ability to metastasize (spread
    to other parts of the body) quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Actinic keratosis
  prefs: []
  type: TYPE_NORMAL
- en: A precancerous lesion caused by sun damage. While not malignant, it can progress
    to squamous cell carcinoma if left untreated.
  prefs: []
  type: TYPE_NORMAL
- en: Benign skin lesions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dermatofibroma
  prefs: []
  type: TYPE_NORMAL
- en: A firm, benign growth often found on the legs.
  prefs: []
  type: TYPE_NORMAL
- en: Nevus (mole)
  prefs: []
  type: TYPE_NORMAL
- en: A common benign growth that varies in size, shape, and color.
  prefs: []
  type: TYPE_NORMAL
- en: Pigmented benign keratosis
  prefs: []
  type: TYPE_NORMAL
- en: A noncancerous pigmented lesion, often resembling seborrheic keratosis.
  prefs: []
  type: TYPE_NORMAL
- en: Seborrheic keratosis
  prefs: []
  type: TYPE_NORMAL
- en: A benign, wartlike growth that can appear brown, black, or tan. Sometimes called
    “age spots” or “wisdom warts,” these are often found in older adults.
  prefs: []
  type: TYPE_NORMAL
- en: Vascular lesions
  prefs: []
  type: TYPE_NORMAL
- en: Benign vascular growths such as hemangiomas and cherry angiomas, formed by abnormal
    blood vessel proliferation.
  prefs: []
  type: TYPE_NORMAL
- en: Many of these conditions can look quite similar. [Figure 5-1](#skin-lesion-types)
    shows example images from this chapter’s training dataset, illustrating the different
    lesion types.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Grid displaying various types of skin lesions, both benign and
    malignant, from the dataset used for classification in this chapter.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can probably imagine from looking at these example images, misclassifications
    are a key challenge in skin cancer detection. These errors arise because different
    lesion types can share similar visual characteristics, such as pigmentation, texture,
    or irregular borders. The most critical errors occur when melanoma is misclassified
    as a benign lesion (a false negative), potentially delaying life-saving treatment.
    Conversely, false positives, such as benign growths mistaken for melanoma, are
    less serious but can lead to unnecessary biopsies and patient anxiety.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, there can be significant visual differences within a single
    class. For example, [Figure 5-2](#diverse-melanomas) shows how melanomas can vary
    widely in appearance, making classification even more challenging.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Melanomas can exhibit a wide range of visual characteristics, making
    consistent classification difficult. Some may appear dark brown or black with
    irregular borders, while others are lighter, reddish, or even patchy in color.
    Certain cases show a crusty texture, whereas others present as smooth, flat lesions.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This visual variability stems from underlying biological differences—including
    the type of cells involved, how deep or aggressively the lesion grows, and how
    much pigment is produced. For melanoma specifically, changes in melanin production
    and growth pattern can result in widely varying appearances, even within the same
    class.
  prefs: []
  type: TYPE_NORMAL
- en: Causes and Risk Factors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Skin cancer develops when genetic mutations disrupt the normal regulation of
    skin cell growth and division. The most common trigger for these mutations is
    UV radiation—primarily from sunlight or artificial sources like tanning beds—which
    damages cellular DNA over time. If this damage isn’t properly repaired—for example,
    when the cell’s DNA repair systems make errors or become overwhelmed—it can lead
    to uncontrolled cell growth and, ultimately, tumor formation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several factors increase the risk of developing skin cancer:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Fair skin* contains less melanin, the pigment that provides some natural protection
    against UV damage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Frequent sunburns*, especially in childhood, suggest repeated UV-induced damage,
    which can accumulate over a lifetime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A high number of moles* (especially atypical or dysplastic moles) can reflect
    underlying instability in melanocyte behavior, increasing the chance that one
    could turn cancerous.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Family history* may point to inherited genetic susceptibilities, such as mutations
    in tumor suppressor genes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Environmental carcinogens*, like arsenic or industrial chemicals, can also
    contribute to mutational burden.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to external factors, *specific genetic mutations*—such as in the
    *BRAF* gene—are commonly found in melanoma. These mutations may arise spontaneously
    or in response to environmental triggers like UV radiation, and they play a key
    role in driving tumor growth. Importantly, understanding these mutations has enabled
    the development of targeted therapies that are tailored to a patient’s individual
    tumor profile, marking a shift toward more personalized cancer treatment.
  prefs: []
  type: TYPE_NORMAL
- en: How Skin Cancer Is Diagnosed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In clinical settings, dermatologists diagnose skin cancer through visual examination,
    dermoscopy, and biopsy. *Dermoscopy* is a noninvasive imaging technique that magnifies
    subsurface skin structures to help distinguish benign from malignant lesions.
    The images in commonly used skin cancer classification datasets primarily consist
    of dermoscopic images, captured using specialized *dermatoscopes* equipped with
    a light source and measurement markers, rather than standard cameras.
  prefs: []
  type: TYPE_NORMAL
- en: If a lesion appears suspicious, a *biopsy* is performed—a small tissue sample
    is taken and examined under a microscope to detect cellular abnormalities, such
    as irregular nuclei, atypical cell shapes, disorganized tissue architecture, or
    uncontrolled mitotic activity. These features help confirm whether the lesion
    is malignant and determine its type and stage.
  prefs: []
  type: TYPE_NORMAL
- en: The ABCDE rule (Asymmetry, Border irregularity, Color variation, Diameter >6
    mm, Evolving changes) helps assess lesions for signs of melanoma. AI models can
    assist by analyzing dermoscopic and clinical images to flag high-risk lesions
    for further evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Image-Based Skin Cancer Detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning has revolutionized image-based skin cancer detection, with AI
    models now achieving diagnostic accuracy comparable to expert dermatologists.
    A 2024 meta-analysis^([4](ch05.html#id828)) of 53 studies found that AI consistently
    outperformed general practitioners and less experienced dermatologists in distinguishing
    melanoma from benign lesions, while performing on par with specialists. This suggests
    that AI can serve as a valuable diagnostic aid, enhancing early detection and
    decision making.
  prefs: []
  type: TYPE_NORMAL
- en: However, integrating AI into real-world clinical practice remains challenging.
    Most skin cancer models are trained on a handful of public datasets (e.g., ISIC,
    HAM10000), which lack diversity in skin types and imaging conditions, limiting
    generalizability. Additionally, AI performance in controlled, retrospective studies
    often does not translate to real-world settings, where factors like lighting,
    lesion presentation, and physician workflows introduce variability.
  prefs: []
  type: TYPE_NORMAL
- en: Another key barrier is explainability. Clinicians need to understand *why* a
    model makes a specific prediction, not just receive an isolated probability score.
    For example, if an AI predicts a skin lesion to be melanoma, is it due to the
    asymmetry, irregular borders, or color variation? And how are these different
    factors combined and weighted by the model? Machine learning methods like saliency
    maps and attention mechanisms help visualize what the model is focusing on, but
    they remain imperfect. Without clear reasoning, AI recommendations are difficult
    to trust or integrate into medical decision making.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Regulatory approval and clinical validation are major hurdles for deploying
    AI in healthcare. Diagnostic models must meet rigorous safety, accuracy, and transparency
    standards before receiving approval from regulatory bodies like the FDA or CE.
    This typically involves extensive clinical trials, reproducibility testing, and
    post-deployment monitoring. Moreover, AI tools must be integrated into existing
    workflows without disrupting clinician judgment or introducing new biases—all
    while maintaining patient privacy and data security.
  prefs: []
  type: TYPE_NORMAL
- en: While challenges remain, AI is steadily moving toward real-world deployment,
    with some dermatology AI systems already *CE-marked*—a certification indicating
    compliance with EU safety and efficacy standards—allowing for clinical use in
    the European Union. Additionally, smartphone apps such as SkinVision and Miiskin
    offer AI-based skin lesion analysis. While these apps are usually not approved
    for clinical decision making, they can still provide risk assessments and encourage
    users to seek medical evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning Primer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 3](ch03.html#learning-the-logic-of-dna), we applied a CNN to model
    1D sequence data—specifically, DNA sequences. However, CNNs are more commonly
    used for 2D image processing, powering tasks such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Image classification
  prefs: []
  type: TYPE_NORMAL
- en: Assigning an image to a specific category, such as identifying whether it contains
    a dog or a cat
  prefs: []
  type: TYPE_NORMAL
- en: Object detection
  prefs: []
  type: TYPE_NORMAL
- en: Detecting and localizing objects within an image, such as drawing a bounding
    box around a cat
  prefs: []
  type: TYPE_NORMAL
- en: Segmentation
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning an image into meaningful regions, such as labeling all pixels that
    belong to a cat
  prefs: []
  type: TYPE_NORMAL
- en: 'This section provides a brief primer on how CNNs work for images. As additional
    learning material, we recommend the 3Blue1Brown introductory video titled [“But
    what is a convolution?”](https://oreil.ly/k8zoM), which offers a visual and intuitive
    explanation of CNNs. For a more in-depth exploration, the renowned [Stanford CS
    231n: Convolutional Neural Networks for Visual Recognition course](https://oreil.ly/C_wPx)
    provides a comprehensive introduction to the field.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CNNs are a specialized type of neural network designed for grid-like data,
    such as images (or sequences, as seen in [Chapter 3](ch03.html#learning-the-logic-of-dna)).
    They automatically learn *hierarchical* patterns, meaning they extract features
    at different levels of abstraction:'
  prefs: []
  type: TYPE_NORMAL
- en: Early layers in the neural network detect simple patterns like edges, textures,
    and color contrasts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Middle layers recognize shapes and structures by combining these basic features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deeper layers build on these to identify complex objects or meaningful categories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, in skin cancer detection, early CNN layers may detect edges and
    color variations in skin lesions, mid-level layers might recognize irregular borders
    or asymmetry, and the latest layers would combine these features into high-level
    learned patterns to distinguish between benign and malignant lesions.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although we describe CNNs as learning hierarchical representations, it’s important
    to avoid *anthropomorphizing* them. CNNs don’t “see” objects the way humans do.
    Instead, they learn statistical patterns in pixel values that maximize predictive
    accuracy. Techniques like *activation mapping* (highlighting which parts of an
    image influence a classification) and *probing* (examining what kinds of features
    different layers encode) help us understand and visualize correlations within
    the model. However, these methods provide post hoc insights for human interpretation—they
    don’t imply that the model itself has a structured or explainable reasoning process.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding a Convolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core building block of a CNN is the *convolutional layer*, which applies
    *filters* (also called kernels) to extract features from the input image. A filter
    is a small grid of numbers that slides across the image, detecting local patterns
    such as edges, textures, or color transitions.
  prefs: []
  type: TYPE_NORMAL
- en: An image is represented as a grid of pixels—in grayscale images, each pixel
    holds a single intensity value (ranging from 0 for black to 255 for white), while
    color images typically have three channels (usually red, green, and blue or *RGB*),
    each with its own intensity map.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a filter moves across the image, it performs a *dot product* operation at
    each position: the filter values are multiplied element-wise with the corresponding
    pixel values in the image patch underneath, and the results are summed. This produces
    a single output value per position, building a new representation called a *feature
    map*, that highlights where the pattern encoded by the filter appears in the image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The specific values in the filter determine what it detects. For example, the
    following 3 × 3 filter emphasizes vertical *edges* by responding strongly to changes
    in pixel intensity along the horizontal (*x*) axis:'
  prefs: []
  type: TYPE_NORMAL
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>[</mo> <mtable><mtr><mtd><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
  prefs: []
  type: TYPE_NORMAL
- en: When applied to an image by sliding across it and computing the dot product
    with pixel values, this filter enhances areas where pixel intensity changes vertically,
    such as object boundaries, making it useful for edge detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, a filter designed to detect horizontal edges responds to changes
    in pixel intensity along the vertical (*y*) axis. It typically looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>[</mo> <mtable><mtr><mtd><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd> <mtd><mrow><mo>-</mo> <mn>1</mn></mrow></mtd> <mtd><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
  prefs: []
  type: TYPE_NORMAL
- en: This filter activates in regions where there is a strong transition from dark
    to light (or vice versa) from top to bottom, highlighting horizontal structures
    in the image.
  prefs: []
  type: TYPE_NORMAL
- en: In a way, this concept is related to the idea of “filters” in social media,
    which often apply simple mathematical transformations (such as increasing contrast
    or sharpening details) to modify an image’s appearance.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Before CNNs, in a task called *feature engineering*, researchers manually designed
    and optimized these filter matrices to detect edges, textures, and other features—a
    labor-intensive process. Now, neural networks learn these filters automatically,
    optimizing them for the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: A single convolutional layer doesn’t just apply one filter; it typically learns
    multiple filters (e.g., 64) in parallel. Each filter captures different features,
    producing multiple *feature maps*, which are stacked together as separate channels
    in the layer’s output.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Dimensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If a grayscale image of size 256 × 256 (with a single channel) is passed through
    a convolutional layer with 64 filters, the output will have dimensions `(256,
    256, 64)`—assuming padding is used to preserve spatial dimensions. The height
    and width remain unchanged, while the channel dimension expands, as each of the
    64 filters extracts different feature representations from the image. This is
    true regardless of the filter size (e.g., 3 × 3, 5 × 5), as long as the stride
    and padding settings maintain spatial dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Remember that neural network filters are usually randomly initialized, meaning
    they start off extracting no meaningful patterns. Through backpropagation, the
    filters gradually learn to detect useful visual features such as edges, textures,
    or shapes.
  prefs: []
  type: TYPE_NORMAL
- en: After convolution, a nonlinearity (such as the ReLU activation function) is
    typically applied to the feature maps. This is crucial because convolution alone
    is just a linear operation, meaning that without a nonlinearity, stacking multiple
    layers would be equivalent to just one big matrix multiplication. Adding the activation
    function allows the network to learn more complex, nonlinear patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s consider a color image with dimensions `(256, 256, 3)`, where the
    three channels correspond to red, green, and blue (RGB). How does convolution
    work when an image has multiple input channels?
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike grayscale images, where each filter operates on a single channel, a
    convolutional filter in a color image must process all three channels simultaneously.
    Instead of being a simple 3 × 3 matrix, each filter is actually a 3 × 3 × 3 tensor,
    meaning:'
  prefs: []
  type: TYPE_NORMAL
- en: Each 3 × 3 slice of the filter is applied to the corresponding color channel
    (red, green, or blue) of the image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results from all three channels are summed to produce a single output value
    per pixel. Typically, this is not a normal sum but actually a *weighted sum* where
    each channel’s contribution is multiplied by a separate learned weight.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This process is repeated for all filters, creating multiple feature maps in
    the next layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, applying a convolutional layer with 64 filters to an input of shape
    `(256, 256, 3)` results in an output of `(256, 256, 64)`, where each filter has
    combined the three input channels in different ways to extract meaningful patterns.
    We would then apply an activation function such as a ReLU as before.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You may have noticed we called a 3 × 3 filter a *matrix*, but a 3 × 3 × 3 filter
    a *tensor*. To briefly clarify the terminology:'
  prefs: []
  type: TYPE_NORMAL
- en: A *scalar* is a single number and can be seen as a 0D tensor (e.g., 5).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *vector* is a 1D tensor (e.g., `[1, 2, 3]`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *matrix* is a 2D tensor (e.g., a 3 × 3 filter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *tensor* is a general term for arrays of any number of dimensions, including
    3D+ structures like 3 × 3 × 3 filters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In short, tensors are the fundamental data structure in deep learning, generalizing
    scalars, vectors, and matrices to arbitrary dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we’ve covered what convolutions do, let’s move on to *pooling layers*.
    Pooling reduces the spatial dimensions of feature maps, making computations more
    efficient and helping to prevent overfitting by keeping only the most prominent
    activations.
  prefs: []
  type: TYPE_NORMAL
- en: The most common type is max pooling, which selects the highest value in a given
    region of the feature map. This ensures that strong activations are preserved
    while reducing spatial resolution (downsampling).
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a 4 × 4 feature map (the image-like output of a convolution) before
    applying 2 × 2 max pooling (with stride 2, meaning the filter moves two pixels
    at a time):'
  prefs: []
  type: TYPE_NORMAL
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>[</mo> <mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>3</mn></mtd> <mtd><mn>2</mn></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>4</mn></mtd>
    <mtd><mn>5</mn></mtd> <mtd><mn>7</mn></mtd> <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>3</mn></mtd>
    <mtd><mn>2</mn></mtd> <mtd><mn>9</mn></mtd> <mtd><mn>6</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>8</mn></mtd> <mtd><mn>6</mn></mtd> <mtd><mn>3</mn></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
  prefs: []
  type: TYPE_NORMAL
- en: 'After applying 2 × 2 max pooling, the highest value in each 2 × 2 block is
    retained, reducing the matrix to a size of 2 × 2:'
  prefs: []
  type: TYPE_NORMAL
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>[</mo> <mtable><mtr><mtd><mn>5</mn></mtd>
    <mtd><mn>7</mn></mtd></mtr> <mtr><mtd><mn>8</mn></mtd> <mtd><mn>9</mn></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable>
  prefs: []
  type: TYPE_NORMAL
- en: 'This greatly reduces the number of values while preserving the most important
    activations. Another common pooling type is *average pooling*, which takes the
    mean of each region instead of the max. This would result in this matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>[</mo> <mtable><mtr><mtd><mrow><mn>3</mn>
    <mo lspace="0%" rspace="0%">.</mo> <mn>25</mn></mrow></mtd> <mtd><mrow><mn>3</mn>
    <mo lspace="0%" rspace="0%">.</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>3</mn>
    <mo lspace="0%" rspace="0%">.</mo> <mn>5</mn></mrow></mtd> <mtd><mrow><mn>6</mn>
    <mo lspace="0%" rspace="0%">.</mo> <mn>0</mn></mrow></mtd></mtr></mtable> <mo>]</mo></mrow></mtd></mtr></mtable>
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: Improves computational efficiency by shrinking the representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retains key information by preserving prominent activations (like maxima or
    averages)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adds a mild form of regularization by making the model less sensitive to small
    shifts in the input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, pooling is not a substitute for more robust regularization methods
    like dropout or weight decay.
  prefs: []
  type: TYPE_NORMAL
- en: Other Components of a CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Beyond convolution and pooling, several other key components help CNNs function
    effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, a nonlinearity (typically ReLU) is applied after each
    convolutional layer. This enhances the model’s expressivity, enabling it to learn
    complex, nonlinear relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization
  prefs: []
  type: TYPE_NORMAL
- en: This normalizes feature maps across a batch to keep activations in a stable
    range. It helps CNNs train more efficiently, reduces sensitivity to weight initialization,
    and enables the use of higher learning rates—especially useful for deep architectures
    (and many CNNs are quite deep).
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs: []
  type: TYPE_NORMAL
- en: This is a regularization technique where random activations are set to zero
    during training to prevent overfitting. This forces the network to rely on multiple
    pathways for making predictions, improving generalization.
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layers
  prefs: []
  type: TYPE_NORMAL
- en: After feature extraction, the final layers of a CNN are typically fully connected.
    These layers combine the learned feature representations and produce the final
    output—such as class probabilities in an image classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these components combined, we’ve covered the building blocks needed to
    train a fully functioning CNN. Next, we’ll explore a widely used architecture
    that brings them all together: ResNets.'
  prefs: []
  type: TYPE_NORMAL
- en: ResNets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A ResNet (residual network) is a CNN architecture introduced in 2015^([5](ch05.html#id848))
    that enables the training of very deep networks without suffering from the *vanishing
    gradient problem*, a common issue in which deeper networks struggle to propagate
    gradients effectively, slowing down learning. ResNets address this by introducing
    *residual connections* (also called *skip connections*), which allow information
    to bypass some layers. This stabilizes training and improves performance, making
    ResNets a go-to architecture for computer vision tasks like image classification
    and object detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea behind a residual connection is simple: instead of learning a
    full transformation `f(x)`, the network learns `f(x) + x`, where `x` is the original
    input. This means that if `f(x)` is small or difficult to learn, the network can
    still default to simply passing through the input `x` unchanged (an identity function).
    This prevents layers from degrading the performance of deeper networks, effectively
    “skipping” operations that don’t contribute meaningful improvements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simplified implementation of a residual block in pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This simple skip connection allows gradients to flow more easily through the
    network, making training deep architectures much more feasible.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rewrite this pseudocode in Flax using the Linen API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are some notes on the key arguments to `nn.Conv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`features`'
  prefs: []
  type: TYPE_NORMAL
- en: Defines the number of learned filters. If `features=100`, an input `(224, 224,
    3)` becomes `(224, 224, 100)`, extracting 100 different feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: '`kernel_size`'
  prefs: []
  type: TYPE_NORMAL
- en: Defines the receptive field of the convolution. A small kernel like `(3, 3)`
    captures fine details, while a larger kernel like `(16, 16)` detects broader patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '`strides`'
  prefs: []
  type: TYPE_NORMAL
- en: Controls the step size of the filter. `strides=(1, 1)` preserves resolution,
    while larger strides downsample the feature maps. Downsampling is more commonly
    done via pooling layers than by increasing the stride.
  prefs: []
  type: TYPE_NORMAL
- en: '`padding`'
  prefs: []
  type: TYPE_NORMAL
- en: Defines the padding for consistent shapes. `padding="SAME"` ensures that output
    dimensions match the input by adding necessary padding.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this in mind, let’s implement a more complete residual block with the
    following additions:'
  prefs: []
  type: TYPE_NORMAL
- en: Two stacked convolutional layers to extract deeper features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A ReLU activation function to introduce nonlinearity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch normalization to stabilize training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A check for channel mismatches, applying a 1 × 1 convolution when necessary
    to ensure compatibility for addition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This implementation is now much closer to what you’ll encounter in real-world
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this foundation in CNNs and ResNets, we can now apply these techniques
    to our goal: building a deep learning model for skin cancer prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As always, before building a model, we first need to understand our dataset.
    In this chapter, we will use data from [ISIC](https://oreil.ly/g2EyR), an initiative
    that fosters collaboration between medical professionals and AI researchers. ISIC
    regularly hosts machine learning challenges, encouraging researchers to develop
    and submit models for classifying skin lesion images.^([6](ch05.html#id863))
  prefs: []
  type: TYPE_NORMAL
- en: The ISIC archive is a freely accessible repository containing tens of thousands
    of skin lesion images, making it a valuable resource for developing and benchmarking
    AI-based diagnostic tools. You can explore the dataset through their online portal,
    as shown in [Figure 5-3](#isic-screenshot).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. A [screenshot](https://oreil.ly/F6BwK) displaying a vast collection
    of 81,722 public skin lesion images.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A First Glimpse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rather than using the ISIC dataset directly, we are working with a version hosted
    on [Kaggle](https://oreil.ly/cez_1). This allows us to explore the initial steps
    of working with a new dataset, including essential sanity checks to ensure its
    integrity.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A key advantage of using a Kaggle dataset is that we can compare our approach
    to existing work and establish reasonable performance expectations. However, caution
    is needed—while Kaggle notebooks can be a great source of inspiration, they are
    not peer reviewed and may contain serious errors.
  prefs: []
  type: TYPE_NORMAL
- en: For example, some models reported impressive performance on this dataset, but
    closer inspection revealed *data leakage*—where images from the training set also
    appeared in validation or test sets, leading to artificially inflated accuracy.
    In one extreme case, we even found a model that evaluated only on training images,
    making its results completely meaningless.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s ensure that we build a robust data pipeline by carefully inspecting the
    dataset before proceeding. We start by exploring the raw dataset directory to
    understand its structure. By listing all *.jpg* files, we can get an initial impression
    of the available labels, which correspond to different types of skin lesions.
    The `rglob` method is particularly useful here, as it scans directories recursively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Examining the filepath, we can see that the dataset has already been split into
    `Train` and `Test` sets, with subdirectories for each skin lesion type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will count the number of images per class across both splits. To do
    this efficiently, we define a helper function, `load_metadata`, that:'
  prefs: []
  type: TYPE_NORMAL
- en: Recursively collects all image filepaths
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracts the dataset split (`Train`/`Test`) and class label from each path
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stores the results in a pandas `DataFrame` for easy inspection and visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also track `frame_id`s, which serve as a reference for quickly retrieving
    specific images during our later sanity checks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we count the number of images per class in both splits using the pandas
    `crosstab` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This dataset is relatively small, containing a total of 2,357 images, split
    into 2,239 for training and 118 for testing. Additionally, the class distribution
    is highly imbalanced, with some classes having very few examples to train on.
    Such an imbalance poses challenges for model generalization and requires careful
    handling to prevent biased predictions. It also makes evaluation less reliable,
    as performance metrics based on very few examples can be noisy and unrepresentative.
  prefs: []
  type: TYPE_NORMAL
- en: 'We visualize this distribution in [Figure 5-4](#class-vs-split-barplot):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. Bar plot of the distribution of classes across original training
    and test sets.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can draw several key insights from the bar plot of class counts:'
  prefs: []
  type: TYPE_NORMAL
- en: Class imbalance
  prefs: []
  type: TYPE_NORMAL
- en: Some categories, like pigmented benign keratosis, are overrepresented (454 total
    images), while others, like seborrheic keratosis, are severely underrepresented
    (80 total images). This imbalance could bias the model toward predicting the more
    frequent classes.
  prefs: []
  type: TYPE_NORMAL
- en: Small test set
  prefs: []
  type: TYPE_NORMAL
- en: Some categories have as few as three images in the test set, making it difficult
    to evaluate model performance across different lesion types. Such a small test
    set can easily lead to misleading performance metrics. We can also see that the
    ratio of train to test data is not even across lesion classes.
  prefs: []
  type: TYPE_NORMAL
- en: No validation set
  prefs: []
  type: TYPE_NORMAL
- en: The dataset only provides `Train` and `Test` splits, but no `Valid` subset.
    A validation set is crucial for tuning hyperparameters and assessing model improvements
    without touching the final test set.
  prefs: []
  type: TYPE_NORMAL
- en: To effectively train a model on this dataset, we’ll need to address these points
    using techniques like data augmentation and resampling. Before diving into these,
    let’s visually inspect some images to better understand the dataset. This will
    help us verify that the images match the lesion types introduced earlier in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Another important consideration is that this dataset consists solely of images
    and labels, without additional metadata such as lesion location, patient demographics,
    or clinical notes. This lack of context makes the classification task more challenging,
    as real-world diagnosis often relies on more than just the visual appearance of
    a lesion. For example, a dark lesion on the scalp of a 70-year-old patient may
    increase the probability of melanoma, since both age and sun-exposed areas are
    known risk factors.
  prefs: []
  type: TYPE_NORMAL
- en: Previewing the Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the first time in this book that we’re working with image data, so let’s
    take a moment to explore how to handle it in Python. A common library for loading
    and processing images is Pillow, the modern version of the original PIL (Python
    Imaging Library). It retains the `PIL` module name for compatibility and is widely
    used for image handling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s take a look at one of the images. We’ll start with a filepath we
    retrieved earlier, as shown in [Figure 5-5](#first-single-image):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. An example skin lesion image loaded using Pillow in Python.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We’d like to inspect images from specific lesion classes. To make this process
    easier, we’ll create a function that:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly selects an image from a specified class in the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loads the image using `PIL` (Pillow)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Displays it with Matplotlib, including the class name as a title for clarity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now use this code to examine an example melanoma image, as shown in [Figure 5-6](#first-single-melanoma):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. Example image from the “melanoma” class.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, let’s write a `plot_random_image_grid` to help us visualize a sample
    image from each skin lesion class. This will help us quickly get a sense of the
    dataset’s diversity and variation in lesion appearance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s run this function (see [Figure 5-7](#skin-image-grid)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0507.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-7\. Grid of skin images with their corresponding labels.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With a better visual understanding of our dataset, we’re now ready to build
    a flexible input pipeline that will support the rest of our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing Dataset Issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we’ve explored the dataset, let’s address some of its limitations.
    Since improving model performance will be an iterative process, it’s helpful to
    create a `DatasetBuilder` class that makes it easy to experiment with different
    dataset configurations. We’ll design it to be flexible enough to support both
    multiclass classification (all lesion types) and binary classification (e.g.,
    melanoma versus non-melanoma), allowing us to explore a variety of setups as we
    develop our models.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, to ensure proper evaluation, we define a 70/20/10 split for `train`,
    `valid`, and `test` sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We have visualized the class distribution across the new splits in [Figure 5-8](#fig5-8):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0508.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-8\. Bar plot of the distribution of classes across new training, validation,
    and test sets.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With this, we now have a better-structured dataset split—featuring a dedicated
    validation set, a larger test set, and consistent train/valid/test proportions
    across lesion classes. This sets the stage for building our `DatasetBuilder`,
    which will allow us to efficiently experiment with different training configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Balancing the batches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some skin lesion classes are much more common than others in our dataset. For
    example, pigmented benign keratosis and nevus are heavily represented, while classes
    like seborrheic keratosis and dermatofibroma are much rarer. If we train directly
    on this imbalanced data, the model may focus on predicting the dominant classes
    correctly while ignoring the rare ones—a bias that can hurt generalization.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate this, we implemented a *balanced sampler* during training. This
    ensures that each batch contains an equal number of examples from each class,
    giving the model more exposure to underrepresented categories. The validation
    and test sets, however, maintain their original class distributions to reflect
    real-world class frequencies during evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Although the balanced sampler didn’t lead to measurable gains in our experiments
    for this chapter, it remains a useful tool when working with severely imbalanced
    datasets, which we encourage you to try in your future projects.
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our dataset is relatively small, and in deep learning, few things improve model
    performance more reliably than having access to more high-quality data. Since
    collecting new labeled images is expensive and time-consuming, we turn to *data
    augmentation*—a technique that synthetically increases dataset size by applying
    label-preserving transformations to existing images.
  prefs: []
  type: TYPE_NORMAL
- en: Augmentation encourages the model to generalize better by exposing it to natural
    variability in image appearance. For example, random rotations, flips, crops,
    color shifts, and brightness changes teach the model that the class of a lesion
    doesn’t change just because its lighting or orientation does. This helps reduce
    overfitting and improves robustness to real-world image variation.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s important to remember that augmentation does *not* introduce truly
    new information. Augmented examples are variations of the same underlying data—so
    any existing dataset biases (e.g., skin tone imbalance or limited anatomical diversity)
    will still be present. What augmentation does offer is a way to nudge the model
    toward learning more general, abstract features rather than memorizing surface-level
    details.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than manually applying transformations, we use the [DeepMind PIX library](https://oreil.ly/3FxiK)—a
    JAX-compatible image augmentation toolkit. It integrates cleanly into our pipeline
    and allows us to dynamically apply transformations on the fly during training,
    keeping memory usage low and training efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our setup, each image is randomly transformed with a fixed probability.
    As with other JAX operations, the random number generator (`rng`) is explicitly
    managed, ensuring that augmentations are reproducible and traceable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To illustrate augmentation in practice, we select a melanoma image and apply
    our transformation pipeline multiple times. Each variation is slightly different,
    simulating realistic image variability. See the result in [Figure 5-9](#skin-image-grid-augmentation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0509.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-9\. Original skin image (top left) followed by different augmented
    versions, demonstrating transformations like flipping, rotation, and color shifts.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This setup is a good starting point, but keep in mind that each augmentation
    method has tunable parameters—and the [PIX documentation](https://oreil.ly/74UjB)
    lists many more options. Selecting and tuning augmentations is effectively a hyperparameter
    search. What works best depends heavily on the dataset and task. For a broader
    overview of effective augmentation strategies, see this survey.^([7](ch05.html#id870))
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before feeding images into a neural network, we need to standardize them. This
    involves ensuring that all inputs have the same shape and pixel value range. Without
    this step, differences in resolution, aspect ratio, or intensity could prevent
    the model from learning consistent patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Many deep learning computer vision models—including ImageNet-trained ResNets
    we will work with in this chapter—expect fixed-size square input images. However,
    skin lesion photos vary widely in shape and size. Simply resizing these directly
    to a square can distort important clinical features. To avoid this, we first resize
    the image while preserving its aspect ratio, then center-crop a square from the
    result. This produces consistent input shapes without stretching or compressing
    lesions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We also need to normalize pixel values. Raw image intensities usually range
    from 0 to 255, but neural networks tend to train more effectively when inputs
    are scaled to a smaller, consistent range. A common method is *min-max scaling*,
    where each pixel is divided by 255 to bring values into the `[0, 1]` range. Another
    option is standardization, where each pixel is transformed to have zero mean and
    unit variance.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The term *standardization* is overloaded in this context and refers to two
    different concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '*General image preprocessing*: Ensuring consistency in image size, format,
    and value ranges'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A specific normalization strategy*: Transforming pixel values to have a mean
    of 0 and a standard deviation of 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout this book, we’ll use *standardization* in the first sense—as a synonym
    for general input preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our model,we’ll use min-max scaling to normalize pixel values, dividing
    by 255 to bring intensities into the `[0, 1]` range. This approach is widely used
    in CNN training and works well for image data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s apply all preprocessing steps—reading JPEG images, resizing while
    preserving aspect ratio, center-cropping, and normalizing—to ensure that our dataset
    is standardized before training. While we could choose any image size, we’ll use
    224 × 224 pixels to match the expected input size of the original ResNet architecture,
    which we’ll explore later in the chapter. Since these transformations need to
    be applied only once, doing them in advance improves efficiency and avoids redundant
    computations during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Figure 5-10](#image-preprocessing-for-standarization), you can see an example
    of an image before and after preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0510.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-10\. Images are standardized to ensure consistent dimensions and pixel
    value ranges. Note that although the preprocessed image has been rescaled from
    0-255 to 0-1, `imshow` automatically adjusts the display scale, so the visual
    appearance remains unchanged.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With data preprocessing topics complete, we can now focus on efficiently storing
    and accessing the data—a crucial step for scaling training without overloading
    system memory.
  prefs: []
  type: TYPE_NORMAL
- en: Data storage with memory-mapped arrays
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ve already emphasized the importance of efficiency in model training: faster
    data processing means faster iterations and ultimately better models. A major
    inefficiency we want to avoid is repeatedly preprocessing the same data every
    time a batch is passed to the model. At the same time, storing all processed images
    in memory isn’t feasible. So, how can we balance efficiency and memory constraints?'
  prefs: []
  type: TYPE_NORMAL
- en: A practical solution is *memory-mapped NumPy arrays*. These allow us to store
    preprocessed images on disk while accessing them as if they were in memory, making
    retrieval fast and efficient. This way, we preprocess all images once before training
    and then load them dynamically during training without redundant computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation is straightforward: we first define the storage file, specify
    the data type (`float32`), and set the shape of the dataset: 224 × 224 pixels
    with three color channels. This creates an empty file on disk, preallocating the
    required storage space. We then preprocess and store each image in the memory-mapped
    array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This approach gives us the best of both worlds: efficient preprocessing without
    excessive memory consumption.'
  prefs: []
  type: TYPE_NORMAL
- en: Building a DatasetBuilder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, we have tackled the major challenges of working with this dataset:
    splitting it into training, validation, and test sets; handling class imbalances;
    preprocessing images; and augmenting the data. Now we will organize these components
    into a structured `DatasetBuilder` class that returns `Dataset` instances that
    have the data iterators that will actually fetch us batches of data that we can
    train or evaluate a model on.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Designing a flexible dataset builder pays off when iterating on your model.
    It lets you quickly try different label mappings, sampling strategies, and preprocessing
    steps—all without rewriting core code. This modularity speeds up experimentation
    and helps you pinpoint which data choices actually improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a closer look at the `DatasetBuilder`, which coordinates the creation
    of dataset splits and preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ve already implemented each of these components, but let’s highlight a few
    key details:'
  prefs: []
  type: TYPE_NORMAL
- en: Metadata loading
  prefs: []
  type: TYPE_NORMAL
- en: '`MetadataLoader` ensures that all image filepaths and their associated class/split
    information are consistently saved to a CSV file. This avoids inconsistencies
    from relying on `rglob`, which does not guarantee a consistent file order. Because
    the image data is later stored in a memory-mapped array in the same order as the
    metadata, maintaining this consistency is crucial.'
  prefs: []
  type: TYPE_NORMAL
- en: Flexible class mapping
  prefs: []
  type: TYPE_NORMAL
- en: The `MetadataLoader` also supports binary or grouped classification setups via
    a customizable `class_map`. If no map is provided, it defaults to a standard multiclass
    configuration based on the directory structure.
  prefs: []
  type: TYPE_NORMAL
- en: Image preprocessing
  prefs: []
  type: TYPE_NORMAL
- en: '`ImageLoader` handles reading, processing, and storing images in memory-mapped
    arrays. To avoid recomputation, images are stored in a compact raw format once,
    and then multiple preprocessing strategies (e.g., resizing or cropping) can be
    applied and stored separately as memmaps.'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset creation
  prefs: []
  type: TYPE_NORMAL
- en: 'The `build` method brings everything together: it loads metadata and images,
    shuffles the dataset, splits it into train/validation/test partitions, and wraps
    each into a `Dataset` object. Each dataset contains consistent metadata and preprocessed
    image views, ready for training.'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the Metadata and Images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can find the code for the supporting classes for the `MetadataLoader` and
    `ImageLoader` in the `dlfb.cancer.dataset.builder` library. Here we will briefly
    describe how they work and fulfill their responsibility to create the `Dataset`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `MetadataLoader` constructs a metadata CSV file if one does not already
    exist. It extracts the original split and class information from the folder structure,
    applies any desired label mappings, and provides consistent row-level access to
    the dataset. Image loading is a bit more involved. While `MetadataLoader` handles
    the filepaths and class labels, `ImageLoader` is responsible for reading, preprocessing,
    and storing the actual image data. Here are some notes on how `ImageLoader` works:'
  prefs: []
  type: TYPE_NORMAL
- en: Image preprocessing and storage
  prefs: []
  type: TYPE_NORMAL
- en: To avoid redundant computations, `ImageLoader` first stores the raw images as
    a single memory-mapped array. This array is compact, storing flattened versions
    of each image along with their original shapes and offsets so they can later be
    reconstructed. Once this raw storage is complete, various preprocessing functions
    (such as cropping or resizing) can be applied and the results saved into additional
    memory-mapped files. This design allows us to support multiple preprocessing strategies
    without reloading the original JPEGs or repeating expensive operations.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing flexibility
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing functions are applied in a modular way. Any number of transforms
    can be passed into the `ImageLoader`, and each will produce its own memmap. These
    preprocessed views are stored under different keys (e.g., `"crop"`, `"resize"`)
    inside the `Images` object, which acts as a unified interface to access them.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset assembly
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `DatasetBuilder.build()` method pulls all of this together. It
    loads the metadata, processes the images, shuffles the dataset, splits it into
    training, validation, and test sets, and returns `Dataset` objects for each. These
    objects provide clean access to both image arrays and class labels, ready for
    model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is all stored in the `Dataset` class itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Where the `Images` are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `Dataset` class is the primary interface for accessing data during training.
    It manages both the `metadata` (which tracks image paths and labels) and the memory-mapped
    `images`. Since we split the dataset into training, validation, and test sets,
    we can control their proportions using the `splits` argument.
  prefs: []
  type: TYPE_NORMAL
- en: Batching the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To streamline dataset handling, during training, we introduce the `BatchHandler`
    class. This class encapsulates the components required to generate batches from
    a `Dataset` object:'
  prefs: []
  type: TYPE_NORMAL
- en: The `preprocessor` selects which preprocessed image view (e.g., cropped, resized)
    to use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `sampler` determines how to draw samples for each batch (e.g., balanced
    versus random sampling).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The optional `augmentor` applies real-time image transformations such as flipping,
    color jitter, or rotation to improve generalization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `get_batches` method is the core of `BatchHandler`. It’s a generator function,
    meaning it yields one batch at a time, which is memory-efficient and well-suited
    for training loops. Here’s what happens inside:'
  prefs: []
  type: TYPE_NORMAL
- en: It validates the requested `batch_size`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It samples batch indices using the `sampler` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It fetches preprocessed images and labels from the `Dataset`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If an `augmentor` is provided, it applies augmentation per image using JAX’s
    `vmap` for efficiency.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It yields batches as dictionaries containing `frame_ids`, `images`, and `labels`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This design cleanly separates dataset structure from augmentation and sampling
    logic, making it easy to experiment with different batching strategies during
    training.
  prefs: []
  type: TYPE_NORMAL
- en: Readying the Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’re now ready to construct our dataset and sanity-check the outputs of the
    data pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This call initializes the full data pipeline: it loads metadata; processes
    images; splits the dataset into training, validation, and test partitions; and
    returns them as `Dataset` objects (as keys in the `dataset_splits` dict).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use the `BatchHandler` to generate a batch from the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Each batch contains three keys—`images`, `labels`, and `frame_ids`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`images`: A float32 tensor with normalized pixel values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels`: Integer class labels corresponding to each image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frame_ids`: Indices into the original metadata, useful for inspection and
    debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s confirm the shape of the `images` tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Each batch consists of `32` images, and we can see that each image has dimensions
    `(224, 224, 3)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving forward, let’s visualize the first image in the batch to ensure
    our preprocessing pipeline has worked as intended. This simple sanity check helps
    catch any unexpected artifacts early on (see [Figure 5-11](#skin-lesion-sanity-check-image)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0511.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-11\. A processed skin lesion image from our dataset, confirming that
    our pipeline is working correctly.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Yup, it still looks like some sort of skin lesion! With a fully functional
    dataset pipeline prepared and validated, we’re ready for the next step: training
    our model.'
  prefs: []
  type: TYPE_NORMAL
- en: Building Skin Cancer Classification Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With our dataset ready, we now turn to the central modeling exploration of
    this chapter. We’ll compare a range of architectures, gradually increasing in
    complexity. Here’s an overview of the models we’ll build and evaluate:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SimpleCNN`'
  prefs: []
  type: TYPE_NORMAL
- en: A lightweight two-layer convolutional neural network that serves as our baseline.
    It shares the same classification head (`SkinLesionClassifierHead`) as the other
    models but is otherwise kept minimal. We won’t tune it heavily—its purpose is
    to establish a performance baseline without any architectural sophistication.
  prefs: []
  type: TYPE_NORMAL
- en: '`ResNetFromScratch`'
  prefs: []
  type: TYPE_NORMAL
- en: A full ResNet50 architecture trained from randomly initialized weights. This
    model assesses the impact of the architecture alone, without any benefits from
    pretraining on ImageNet. It helps isolate the contribution of the ResNet design
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: '`FinetunedResNet`'
  prefs: []
  type: TYPE_NORMAL
- en: A ResNet50 initialized with pretrained weights, with all layers fine-tuned on
    our skin lesion dataset. This approach leverages transfer learning, incorporating
    knowledge learned from large-scale datasets like ImageNet to improve convergence
    speed and generalization.
  prefs: []
  type: TYPE_NORMAL
- en: '`FinetunedHeadResNet`'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the previous item, but with the pretrained ResNet50 backbone frozen.
    Only the final classification head is trained. This variant tests how far we can
    get by leveraging pretrained features without modifying the backbone.
  prefs: []
  type: TYPE_NORMAL
- en: This progression—from scratch to full fine-tuning—lets us directly compare training
    efficiency and classification performance across strategies. Training from scratch
    gives full control but demands more data and compute. Transfer learning, especially
    when freezing the backbone, is typically faster and more robust in low-data regimes
    like ours.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ResNet50 is a widely used model convolutional architecture with 50 layers. You
    can easily experiment with deeper variants such as [ResNet101](https://oreil.ly/P8Cvr)
    or ResNet200\. These offer greater representational power but come at the cost
    of increased compute and a higher risk of overfitting—especially when training
    data is limited. All variants are compatible with the same ImageNet preprocessor.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement these models, we’ll follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the ResNet50 model for Flax from transformers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the model backbone.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attach a custom classification head (`SkinLesionClassifierHead`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train and evaluate each model variant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s start by loading the pretrained ResNet50 model.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the Flax ResNet50 Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flax offers a [prebuilt ResNet implementation](https://oreil.ly/Yaf_p), which
    provides a flexible and well-optimized architecture for feature extraction. However,
    prebuilt models aren’t always available—or pretrained—depending on your needs.
    To demonstrate how to work with pretrained models in Flax, we’ll load a ResNet50
    model from Hugging Face instead of using the Flax example directly.
  prefs: []
  type: TYPE_NORMAL
- en: The Hugging Face [`FlaxResNetModel`](https://oreil.ly/xWVhz) provides a convenient
    way to import and use pretrained ResNet models, making it easy to fine-tune or
    extract features from standard models.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how this works in practice, we’ll follow Hugging Face’s example code
    and apply a pretrained ResNet50 to a sample image. We’ve been looking at skin
    lesions for quite a while, so let’s take a break with something more cheerful:
    a photo of cats lounging on a couch (see [Figure 5-12](#cats-on-couch)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0512.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-12\. Cats on a couch.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Loading a pretrained ResNet model from Hugging Face is straightforward. We’ll
    use a ResNet50 model trained on ImageNet and pair it with an image preprocessor
    to ensure that our input image is correctly formatted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s preprocess the cat image to obtain the `pixel_values` input format
    expected by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s inspect what the preprocessor does to input images in [Figure 5-13](#cats-on-couch-standardized):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0513.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-13\. Cats on a couch after image processor standardization.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The processed image looks noticeably different from the original; the colors
    have shifted, and the image has been cropped to 224 × 224 pixels. This transformation
    is essential when using pretrained weights: models expect inputs normalized to
    the mean and standard deviations of the dataset they were trained on. This standardization
    ensures consistent performance, even if your input data (like medical images)
    has different color distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are two important guidelines:'
  prefs: []
  type: TYPE_NORMAL
- en: When fine-tuning a pretrained model, normalize inputs using the original training
    dataset’s statistics (e.g., ImageNet mean and std).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When training from scratch, you can normalize using your own dataset’s range
    or statistics (mean and standard deviations).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let’s make a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Success! Well, sort of—the model classifies the image as the class “tiger cat.”
    While not a perfect match, it’s a reasonable guess given the image. This highlights
    both the strengths and limitations of pretrained models: they excel at recognizing
    familiar patterns but may struggle with patterns outside of original training
    data distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now explore how we can reuse the core architecture of this model while
    adapting it to a new task: classifying skin lesions.'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the ResNet Backbone
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned, the pretrained ResNet50 model we’re using has been trained on
    ImageNet to classify images into one of 1,000 classes such as “tiger cat,” “airplane,”
    and “fire truck.” You might wonder: is melanoma one of the classes? Unfortunately,
    it is not. So how can we still use this model?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The key lies in the layered architecture of deep learning models. The pretrained
    ResNet50 consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: A *backbone* (ResNet module) that extracts general features from images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *classifier head* that maps these features to one of ImageNet’s 1,000 categories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we don’t need the ImageNet classification head, we’ll replace it with
    our own custom classifier for melanoma detection.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While Hugging Face offers `FlaxResNetModel`—a ResNet variant without the classification
    head—for exactly this use case, such headless versions aren’t available for every
    model. Learning how to manually extract and repurpose parts of a pretrained model
    is an essential skill. It gives you full control over what components are used
    and prepares you for working with a wider range of architectures where clean abstractions
    may not exist. That’s why we demonstrate the manual approach here.
  prefs: []
  type: TYPE_NORMAL
- en: 'To separate the backbone from the classifier head:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s what each step does:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We extract the components of the `FlaxResNetForImageClassification` model provided
    by Hugging Face:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`module`: The full architecture definition'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`variables`: The pretrained weights for all parts of the model'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To isolate just the ResNet backbone:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We `bind` the weights to the module.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We access the `resnet` submodule, which excludes the classification head.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We then `unbind` it, separating the backbone into its own callable Flax module
    with its own parameters.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This technique gives you direct access to intermediate model components—even
    when they aren’t explicitly exposed. If you’re unsure what submodules are available,
    you can inspect the bound module by listing its attributes or checking the Hugging
    Face docs.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can use the `backbone_module` just like any other Flax model by calling
    its `apply` method. For input, we pass the `backbone_vars` and a preprocessed
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unlike many Hugging Face transformer models, Flax models typically expect input
    images in NHWC format (batch, height, width, channels). If the image is in NCHW
    format (batch, channels, height, width), use `transpose` or `moveaxis` to first
    reorder the dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shape `(1, 2048, 7, 7)` represents:'
  prefs: []
  type: TYPE_NORMAL
- en: '`2048`: The number of feature channels output by the final ResNet block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`7, 7`: The spatial dimensions after all convolutions and pooling layers have
    been applied. In other words, these operations have downsampled the image from
    224 × 224 to 7 x 7 height and width.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At this stage, the feature maps are highly abstract, far removed from the original
    cat image. However, they still capture meaningful patterns learned by the model.
    These 2,048 feature maps would then be fed into the fully connected layers for
    the purpose of actually classifying the image. If we visualize them, we get something
    like [Figure 5-14](#abstract-cat-features):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0514.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-14\. Feature maps from the final convolutional layer of ResNet50, illustrating
    how the model “sees” the cat image at that stage. This visualization shows activations
    from only the first 64 of 2048 learned filters. Each square represents the response
    of a different convolutional filter, with bright regions indicating areas of high
    activation. Many filters remain largely inactive, while others detect specific
    patterns or textures.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some feature maps remain largely inactive (like those in the first row), while
    others highlight specific aspects of the cat image. At this late stage in the
    neural network, the representation of the cat image is highly abstract and no
    longer interpretable in a human-recognizable way. However, these learned patterns
    are still essential for the model’s classification process.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this example, we simply examined the activations of late-stage convolutional
    filters in response to a cat image to demonstrate how to extract feature maps
    from a pretrained model. While useful as a sanity check, this doesn’t reveal what
    each filter has actually learned.
  prefs: []
  type: TYPE_NORMAL
- en: 'To dig deeper, try *feature visualization*: a technique where you iteratively
    modify an input image to maximize the activation of a specific filter. This reveals
    the kinds of patterns—like textures, edges, or object parts—that the filter responds
    to.'
  prefs: []
  type: TYPE_NORMAL
- en: For an excellent deep dive, check out the [classic Distill blog post from 2017](https://oreil.ly/XpdSL)
    on visualizing convolutional filters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also access the model’s final `pooler_output`, which represents the
    last hidden state after global average pooling. This operation collapses the spatial
    dimensions, leaving us with a single 2,048-dimensional feature vector—the final
    output of the model’s feature extraction process. This vector serves as the condensed
    representation of the image, as shown in [Figure 5-15](#last-hidden-state-of-resnet50):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0515.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-15\. Visualization of the final pooled output of ResNet50 on the cat
    image input, also known as the last hidden state after global average pooling.
    This 2048-dimensional feature vector serves as the input to the classifier head.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All right, we now have a standalone ResNet module extracted from the full classifier.
    We’ve also inspected its final output—a 2,048-dimensional feature vector produced
    by global average pooling. This vector captures the distilled representation of
    the input image and will serve as the input to our custom classifier head.
  prefs: []
  type: TYPE_NORMAL
- en: With the backbone in place, we’re now ready to construct the `SkinLesionClassifierHead`.
  prefs: []
  type: TYPE_NORMAL
- en: Building the SkinLesionClassifierHead
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have extracted the pretrained ResNet50 backbone, we can add a new
    classification head tailored for skin lesion detection. This head will take the
    feature vector output from [Figure 5-15](#last-hidden-state-of-resnet50) and produce
    logits for our target classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although Hugging Face follows a naming convention like `FlaxResNetForSkinLesionClassification`,
    we will slightly simplify and call our head `SkinLesionClassifierHead`. This will
    perform the final classification by returning the logits for each skin class we
    want to predict. This is a small feedforward neural network consisting of sequential
    `Dense` layers with ReLU activations. Let’s take a look at its implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The final layer has as many neurons as the number of target classes, defined
    by the `num_classes` argument.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this chapter, we focus on multiclass classification, but the same code generalizes
    to binary classification. While a single `nn.Dense` neuron with sigmoid activation
    is perhaps more typical for binary tasks, we use a two-neuron output instead—one
    per class (melanoma or nonmelanoma)—producing logits that sum to 1 after softmax.
    This approach is slightly less efficient but more flexible, as it naturally extends
    to multiclass problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will also notice that we specify `nn.initializers.xavier_uniform()` for
    our weight initialization. While the most naïve approach is to initialize all
    weights to zero, this leads to a major problem: all neurons in a layer would receive
    the same gradients during backpropagation and update identically, resulting in
    redundant feature learning. This is known as a *failure to break symmetry*, and
    it can severely limit the network’s ability to learn.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we use *Xavier initialization* (also known as *Glorot initialization*),
    which assigns small, random values to each weight. This helps ensure that neurons
    start with diverse parameters and can learn different features. Xavier initialization
    is designed to maintain stable variance in both activations and gradients across
    layers, improving training dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Most modern deep learning libraries—including Flax—automatically apply good
    initializers for standard layers. We specify `xavier_uniform()` here for clarity,
    but in practice, you can often rely on the library’s defaults.
  prefs: []
  type: TYPE_NORMAL
- en: Building Our Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’re now ready to begin the core modeling section of this chapter. We’ll compare
    several architectures, gradually increasing in complexity. For the more promising
    ones, we’ll also explore enhancements to improve performance. Let’s begin with
    our baseline.
  prefs: []
  type: TYPE_NORMAL
- en: SimpleCnn model as a baseline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our first model is a straightforward two-layer convolutional neural network
    trained from scratch. It doesn’t use any pretrained features, and we won’t spend
    time tuning it. The goal here is simple: to establish a reference point. How well
    can a minimal architecture perform on this task without any bells or whistles?'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, it uses our shared classification head, `SkinLesionClassifierHead`,
    and a custom lightweight backbone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This backbone is intentionally simple—no residual connections, no batch normalization.
    It’s just a stack of convolutional layers followed by flattening. It won’t win
    any benchmarks, but it should give us a useful lower bound on performance for
    this task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though this model doesn’t include batch normalization, we’ll still use
    a training state that supports it—`TrainStateWithBatchNorm`—to keep our training
    loop compatible across models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Using a unified training state lets us reuse the same training loop for both
    this simple CNN and the more advanced ResNet models we’ll explore next.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s move on to training our ResNet variants to better understand the impact
    of architecture and transfer learning on skin lesion classification. With the
    ResNet50 backbone we extracted earlier and our custom skin lesion classification
    head already in place, we now have all the components we need to assemble and
    train more advanced models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll build three models, starting with the simplest: ResNet50 from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: Building the ResNetFromScratch model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our first ResNet variant is `ResNetFromScratch`, defined next. It uses the full
    ResNet50 architecture but does not load any pretrained weights—all parameters
    are initialized randomly and learned from scratch. While this model still benefits
    from the structure of a relatively deep, well-tested architecture, it doesn’t
    directly inherit any knowledge from ImageNet or other large-scale datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives us a useful comparison point: how far can we get with a good architecture
    alone, without any pretrained weights? Here’s the code for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: In the `setup` method, we instantiate both the `backbone` and the `head`. These
    are then used sequentially in the `__call__` method to compute predictions. The
    backbone is accessed through the Hugging Face wrapper, so we reference it using
    `.module` to extract the underlying Flax-compatible model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You will have noticed the the model takes a `layer` parameter. It picks the
    ResNet model with the required number of layers from the Hugging Face library
    of preloaded models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: We have settled for a value of 50 to get the ResNet50 backbone, but the layers
    parameter could be conveniently explored as a hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: To prepare the model for training, we define a training state via the `create_train_state`
    method. This initializes the parameters, optionally transfers pretrained weights
    (not used here), and sets which parameters should remain trainable. While `ResNetFromScratch`
    doesn’t apply any changes to the weights, this method is designed to be overridden
    by subclasses—as we’ll see next with `FinetunedResNet`.
  prefs: []
  type: TYPE_NORMAL
- en: Even though `ResNetFromScratch` does not use any pretrained weights, it defines
    a `transfer_parameters` method (which is a no-op here) and a `set_trainable_parameters`
    method (also a placeholder). These provide a consistent interface across models
    and make it easy to plug in transfer learning logic in downstream variants.
  prefs: []
  type: TYPE_NORMAL
- en: Since ResNet uses batch normalization, our training state must also track batch
    statistics—so we use the `TrainStateWithBatchNorm` class defined earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Fully fine-tuning a pretrained model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The second model, `FinetunedResNet50`, is a subclass of `ResNet50FromScratch`
    that modifies just one key detail: it loads pretrained weights from the Hugging
    Face ResNet50 model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is done by overriding the `transfer_parameters` method. Unlike the base
    class (where this method is a no-op), here it copies over both the backbone’s
    weights and its batch normalization statistics. The classification head remains
    randomly initialized, since it’s specific to our skin lesion classification task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Once the parameters are loaded, we will fine-tune `all` of them—both the pretrained
    ResNet backbone and the newly added classification head. This setup allows the
    model to fully adapt to the skin cancer classification task, and should in theory
    lead to stronger performance when the source (ImageNet) and target (skin) domains
    share similar visual features, such as textures, colors, and edges, which should
    be the case here.
  prefs: []
  type: TYPE_NORMAL
- en: Freezing the backbone with FinetunedHeadResNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third model, `FinetunedHeadResNet`, uses a different transfer learning strategy.
    Instead of fine-tuning all layers, it freezes the pretrained ResNet backbone and
    updates only the final classification head. This approach is common when working
    with small datasets. It allows us to leverage powerful pretrained features while
    reducing the number of trainable parameters—which can help avoid overfitting and
    reduce training time.
  prefs: []
  type: TYPE_NORMAL
- en: Flax and Optax make it easy to define separate optimizer behaviors for different
    parts of the model. Here, we apply our usual optimizer to the classification head
    and use a no-op optimizer (which performs no updates) for the frozen backbone,
    ensuring only the final layers are trained. This is implemented by overriding
    the `set_trainable_parameters` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is implemented in the following model, which subclasses `FinetunedResNet`
    to inherit pre-trained weights but overrides the `set_trainable_parameters` method
    to freeze the backbone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: By subclassing `FinetunedResNet`, we inherit all the logic for loading pretrained
    weights and constructing the model—and only need to override the part that controls
    which parameters are trainable. This modular design maximizes code reuse while
    making it easy to explore different fine-tuning strategies with minimal duplication.
  prefs: []
  type: TYPE_NORMAL
- en: More customization options
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s worth highlighting that fine-tuning doesn’t have to be all or nothing—you’re
    not limited to freezing the whole backbone or simply training all parameters.
    With tools like `optax.multi_transform`, we can easily assign different optimizer
    behaviors to different parts of the model.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a common strategy is to freeze the earliest convolutional layers
    (which tend to learn general-purpose features like edges and textures) while fine-tuning
    later layers that capture more task-specific patterns. Some layers can even be
    updated using a reduced learning rate, allowing for more cautious adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example model, `PartiallyFinetunedResNet`, which demonstrates
    this idea. It applies:'
  prefs: []
  type: TYPE_NORMAL
- en: A standard learning rate to the classification head and late-stage backbone
    layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A reduced learning rate to intermediate layers (i.e., `stages/3`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A complete freezing of the early backbone layers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We won’t be training or evaluating this model in the rest of the chapter, but
    it’s useful to be aware of this level of flexibility. Once your model is structured
    cleanly, these kinds of fine-tuning schemes are straightforward to implement and
    can make a big difference when adapting to smaller or more specialized datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our `DatasetBuilder` ready to generate data and all of our model variants
    defined, it’s time to train them. We’ll now implement a training loop that follows
    a familiar structure.
  prefs: []
  type: TYPE_NORMAL
- en: The Training Loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training loop follows the same approach as in previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the model’s training state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate over a defined number of steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each step, fetch a batch of training data and update the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Periodically evaluate on a validation set to track progress.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'At a high level, this function:'
  prefs: []
  type: TYPE_NORMAL
- en: Sets up a metrics logger and prepares the data loaders
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterates through the given number of training steps
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calls `train_step` to apply a gradient update on each training batch
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Periodically runs `eval_step` on validation batches
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Logs and flushes metrics so we can monitor progress
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This design gives us a steady stream of training and validation feedback. The
    validation metrics are useful not just for model selection but also for deciding
    when to stop training or intervene (for example, if a model is doing horrendously
    poorly, we can just kill the hopeless run).
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s take a look at what happens inside a single training step.
  prefs: []
  type: TYPE_NORMAL
- en: The training step
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The core of our training process is the `train_step` function. This function
    wraps around a helper called `calculate_loss`, which handles forward propagation,
    loss computation, and gradient updates. This is a common design pattern in JAX
    code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside `calculate_loss`:'
  prefs: []
  type: TYPE_NORMAL
- en: Images and labels are extracted from the batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model performs a forward pass to compute logits (unnormalized prediction
    scores for each class).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The predicted logits are compared to ground truth labels using `optax.softmax_cross_entropy_with_integer_labels`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mean cross-entropy loss is computed—averaged over the examples in the batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradients are then calculated with respect to this loss and applied to update
    the model’s parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One important detail: if the model uses batch normalization (as ResNet does),
    we also need to update the running statistics tracked by the batch norm layers.
    The function `calculate_loss` handles both cases—whether batch norm is used or
    not—by conditionally including and updating the `batch_stats` entry in the variable
    collection.'
  prefs: []
  type: TYPE_NORMAL
- en: This structure makes the training step general and reusable across all models
    we’ve defined, regardless of their architectural complexity or normalization layers.
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation step
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The evaluation step mirrors the training step but is simpler, as it does *not*
    update model weights. Its sole purpose is to compute performance metrics on a
    validation (or test) batch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: The model runs a forward pass with `is_training=False`, disabling training-time
    behavior like dropout and updating batch norm statistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If batch normalization is used, the stored `batch_stats` are included in the
    variable collection—but they are *not* updated. Instead, we rely on the running
    averages (exponential moving averages of the mean and variance of activations)
    that were collected during training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logits are compared to the ground truth labels using the same loss function
    as during training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to computing the loss, the function returns performance metrics
    using `compute_metrics`, which includes weighted precision and recall (explained
    later).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By keeping evaluation stateless and side-effect-free—that is, avoiding any updates
    to model parameters or internal statistics—we ensure that validation scores remain
    consistent, reliable, and comparable across runs.
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During training, we typically monitor the loss, but for evaluation, we will
    also compute *precision* and *recall* to gain a more complete picture of the model’s
    performance on the skin lesion classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first discuss how these metrics work in the binary case—for example,
    distinguishing melanoma from nonmelanoma—and then generalize to the multiclass
    setting we actually care about:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Precision* measures how many of the predicted positive cases (melanoma) are
    actually correct:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <mrow><mtext>Precision</mtext> <mo>=</mo> <mfrac><mrow><mtext>True</mtext><mtext>Positives</mtext></mrow>
    <mrow><mtext>True</mtext><mtext>Positives</mtext><mtext>+</mtext><mtext>False</mtext><mtext>Positives</mtext></mrow></mfrac></mrow>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: High precision means few false positives; so when the model flags a melanoma,
    it’s likely to be correct. This is important in medical settings to avoid unnecessary
    follow-up procedures and patient anxiety.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Recall* (also called sensitivity) measures how many actual melanoma cases
    were correctly identified:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <mrow><mtext>Recall</mtext> <mo>=</mo> <mfrac><mrow><mtext>True</mtext><mtext>Positives</mtext></mrow>
    <mrow><mtext>True</mtext><mtext>Positives</mtext><mtext>+</mtext><mtext>False</mtext><mtext>Negatives</mtext></mrow></mfrac></mrow>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'High recall means few missed melanoma cases: critical for ensuring early detection.
    In many clinical settings, recall is prioritized, since missing a true case (i.e.,
    making a false negative prediction) is more harmful than incorrectly flagging
    a benign case.'
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we’re working with multiple classes, not just melanoma versus nonmelanoma.
    To summarize performance across all classes, we use *weighted precision* and *weighted
    recall*. These metrics average per-class values while accounting for class imbalance—classes
    with more examples contribute more to the final score.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll be using weighted precision and recall as our main evaluation metrics,
    and we will plot them across training and validation splits as training proceeds.
    Here is our `compute_metrics` code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: This function uses the predicted logits to compute softmax scores, takes the
    most likely class, and then calculates weighted precision and recall based on
    the true and predicted labels.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Weighted metrics are concise and helpful but can mask problems—for instance,
    if the model performs poorly on a rare class, this might not show up clearly in
    the overall score. Always check *per-class metrics* as well. This can highlight
    weaknesses that might justify upweighting certain classes in the loss function
    or resampling your dataset to better balance class representation.
  prefs: []
  type: TYPE_NORMAL
- en: You may also encounter F1 scores in classification tasks. The F1 score is the
    harmonic mean of precision and recall—it provides a single number that balances
    both. Here, we’ll stick to just plotting both precision and recall, as they are
    often easier metrics to interpret and let you diagnose whether your model struggles
    more with false positives or false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: Faster evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may have noticed that the `compute_metrics` function is jitted. Standard
    evaluation metrics—such as those from `sklearn` or `scipy`—are not typically JIT-compatible,
    which can become a bottleneck during training.
  prefs: []
  type: TYPE_NORMAL
- en: To address this, we’ve implemented JIT-friendly versions of precision and recall.
    These custom functions have been tested to match the outputs of their standard
    counterparts, but run significantly faster—especially during frequent batch-wise
    evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: The metrics are computed on each batch and logged using the `MetricsLogger`,
    which aggregates results to provide full-dataset performance summaries.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In large-scale production models, the overhead from non-jitted metrics might
    be negligible. But for the relatively small models and fast iteration cycles we
    use here, the speedup from JIT compatibility is noticeable.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Multiclass Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we can use our `DatasetBuilder` to construct the dataset that will
    be used to train and evaluate all models in this chapter. Using the same dataset
    setup across experiments allows us to make clean comparisons and attribute performance
    differences to model choices rather than data variation.
  prefs: []
  type: TYPE_NORMAL
- en: We also initialize and split a random seed up front, so we can reuse consistent
    seeds across model initializations and training runs—this helps ensure reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'There are three preprocessing options implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '`crop`'
  prefs: []
  type: TYPE_NORMAL
- en: Focuses on the central region of the image, where lesions are typically located.
    While simple and effective, it can discard potentially useful peripheral features
    such as hairs, texture, or pigment variation.
  prefs: []
  type: TYPE_NORMAL
- en: '`skew`'
  prefs: []
  type: TYPE_NORMAL
- en: Retains as much of the lesion context as possible by resizing the entire image
    to a square without cropping—this means stretching or compressing the original
    aspect ratio. Since most of the input images are wider than they are tall, cropping
    to a square can cut out potentially useful surrounding tissue. Skewing avoids
    this by distorting the image to fit a square shape, preserving all pixels.
  prefs: []
  type: TYPE_NORMAL
- en: While the image becomes slightly warped, it may still help the model capture
    broader contextual cues like skin texture or peripheral features.
  prefs: []
  type: TYPE_NORMAL
- en: '`resnet`'
  prefs: []
  type: TYPE_NORMAL
- en: Applies standard preprocessing required for ResNet-based models, including resizing
    to the expected input shape and normalizing pixel values. Ensures compatibility
    with pretrained ResNet architectures.
  prefs: []
  type: TYPE_NORMAL
- en: These preprocessing steps are modular and can be swapped in and out easily.
    We can treat the choice between them as yet another tunable hyperparameter during
    experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Baseline Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s first train the baseline `SimpleCnn` model. There’s nothing fancy here—but
    the point is to show that even a naive architecture can learn a surprising amount
    from the raw images. Here is the training setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'A few quick notes on this “no frills” setup:'
  prefs: []
  type: TYPE_NORMAL
- en: We use `optax.adamw` but set `weight_decay=0.0`, which makes it equivalent to
    plain `optax.adam`. This means no weight decay regularization is applied (we also
    do not need to worry about the `mask` parameter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preprocessor is set to crop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’re using the `repeating_sampler` to cycle through the data. This does not
    resample images to balance class frequencies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No data augmentation is applied (`augmentor=None`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although we specify the `optax.adamw` optimizer, we initially leave weight decay
    unused. Why? Defining it uniformly across all of our models ensures consistency
    and makes hyperparameter tuning much simpler. Later on, we’ll activate this placeholder
    to help reduce model overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now evaluate the model’s performance by plotting its metrics over the
    training steps. The results are shown in [Figure 5-16](#resnet-from-scratch-evaluation-plot).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0516.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-16\. Evaluation metrics for the `SimpleCnn` model. The left plot shows
    the evolution of loss for the `train` and `valid` dataset splits, while the right
    plot tracks (weighted) precision and recall for the `valid` dataset.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We see that this baseline model reaches around 0.4 weighted precision and recall
    on the validation set—not bad for such a simple architecture. This gives us a
    reference point to compare against more advanced models.
  prefs: []
  type: TYPE_NORMAL
- en: Training metrics are much higher than validation metrics, indicating substantial
    overfitting. We also observe that validation loss increases over time, which is
    consistent with overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful way to assess model performance is through the confusion matrix,
    shown in [Figure 5-17](#resnet-from-scratch-confusion-matrix). This gives a detailed
    view of how predictions align with ground truth labels—including which classes
    are commonly confused:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0517.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-17\. Confusion matrix for the `SimpleCnn` model on the nine-class skin
    lesion classification task. The rows represent the true labels, and the columns
    represent the predicted labels. Diagonal cells show correct predictions; off-diagonal
    cells indicate misclassifications. Each cell also includes example validation
    images for qualitative inspection, with the count of predictions shown in the
    lower-right. Precision (P:) and recall (R:) scores are provided per class.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Some guidance on reading this plot—which we’ll use throughout the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Diagonal is good: These represent correctly classified images. An ideal model
    would have predictions only on the diagonal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Off-diagonal is bad: These are misclassifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number in the lower-right of each square indicates how many examples fall
    into that cell (in the validation set). Many confusion matrices simply report
    the counts here, but we also include actual images so that you can start to build
    intuition over what the different lesion types look like.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision (`P:`) is shown above each column (per predicted class), and recall
    (`R:`) is shown along each row (per true class).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weighted precision and recall reported in the previous training plots are
    not simple averages of these values—they are class-weighted, meaning more common
    classes contribute more to the overall metric.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With that in mind, interpreting the confusion matrix in [Figure 5-17](#resnet-from-scratch-confusion-matrix)
    specifically, we can see that:'
  prefs: []
  type: TYPE_NORMAL
- en: This model is not that great—there are many off-diagonal entries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model generally performs better on more common classes and struggles with
    rarer ones—we’ll try resampling later to address this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It performs best on vascular lesions (bottom-right corner), which are visually
    distinct and easier to recognize, even to untrained eyes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model also does relatively well on pigmented benign keratosis, nevus, and
    melanoma—though there’s significant room for improvement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A key issue is that melanomas are often misclassified as nevi or other benign
    categories. This is particularly concerning given the clinical importance of detecting
    melanoma. We’ll keep a close eye on whether more advanced models reduce these
    errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this baseline in mind, let’s turn our attention to the `ResNetFromScratch`
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Training the ResNetFromScratch Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that this model follows the ResNet architecture but starts from randomly
    initialized weights—meaning it will probably require the most training time to
    reach strong performance compared to models that makes use of pretrained weights.
  prefs: []
  type: TYPE_NORMAL
- en: Before training, we need to set up both the dataset and model instance. We are
    aware of the class imbalance and small dataset size, and we have already prepared
    techniques like `balanced_sampler` and `rich_augmentor` to mitigate these issues.
    However, to better understand their individual impact, we will first run training
    without any augmentation or class balancing.
  prefs: []
  type: TYPE_NORMAL
- en: For this model, we do not need to apply `resnet` preprocessing to the images,
    since we are training from scratch and don’t require compatibility with pretrained
    ResNet weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s train our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, let’s evaluate the model’s performance by plotting its metrics over
    the training steps. The results are shown in [Figure 5-18](#fine-tuned-resnet-learning-schedule):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0518.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-18\. Model performance over training and validation steps for the `ResNetFromScratch`
    multiclass skin lesion classification model.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The results are promising and already show an improvement over the baseline—precision
    and recall metrics are significantly higher. This makes sense, as the ResNet architecture
    is generally strong for image tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, both train and validation metrics increase steadily and remain
    closely aligned. Likewise, the train and validation losses both decrease over
    time, suggesting the model is not significantly overfitting. In fact, the training
    curves indicate that the model is still learning—so with more training steps,
    we might be able to achieve even better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see which classes benefit most from this improved model, let’s look again
    at the confusion matrix in [Figure 5-19](#fig518NEW):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0519.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-19\. Confusion matrix for the `ResNetFromScratch` model on the nine-class
    skin lesion classification task.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Some observations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall improvement: We see better recall and precision across most categories
    compared to the baseline model. The model is generally more confident and more
    accurate. The diagonal is visibly stronger overall, indicating more correct predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vascular lesions are now being classified almost perfectly (bottom-right corner).
    These lesions are visually distinctive, and the model is clearly picking up on
    that.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nevus and melanoma both show solid improvements in recall. However, there is
    still some confusion between the two, as seen in the off-diagonal cells between
    those classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pigmented benign keratosis shows a marked improvement in both precision and
    recall, with a notable jump from the baseline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actinic keratosis and seborrheic keratosis are now never predicted by the model—resulting
    in `NaN` precision values for those classes. This is odd and is something to revisit
    with future models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We now move on to the `FinetunedHeadResNet`, which incorporates pretrained ImageNet
    features. This should give us a much stronger model, especially given the small
    size of our training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Training the FinetunedHeadResNet Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This time around, we need to be careful about image preprocessing. Since we’re
    using pretrained weights from a ResNet model originally trained on ImageNet, our
    input images must be preprocessed in the same way as those used for ImageNet.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the images deviate too much—for example, if they’re scaled differently or
    normalized inconsistently—the pretrained features may no longer be as applicable.
    This can force the model to overcorrect during training, reducing the benefits
    of transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'To handle this, we apply the `resnet` preprocessing function, which ensures
    that input images are resized and normalized in a way that matches the expectations
    of the pretrained ResNet backbone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to train the `FinetunedHeadResNet` model, which freezes the
    pretrained backbone and trains only the classification head. To isolate the effects
    of transfer learning, we keep the setup minimal—no regularization, augmentation,
    or class rebalancing is applied at this stage.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Let’s examine the model’s performance in [Figure 5-20](#fine-tuned-resnet-evaluation-plot).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0520.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-20\. Model performance over training and validation steps for the `FinetunedHeadResNet`
    multiclass skin lesion classification model.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Some observations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Loss curves: The training loss steadily decreases, as expected, but the validation
    loss begins to rise again after around step 250\. This is a classic sign of overfitting—the
    model is becoming more confident on the training set, but its generalization to
    new data begins to degrade.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Train vs. validation gap: There is still a large gap between training and validation
    metrics, though it’s smaller than what we saw with the `SimpleCnn` baseline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Precision and recall: Interestingly, while the validation loss increases, the
    validation precision and recall metrics remain relatively stable. This suggests
    that although the model’s softmax confidence may be worsening, its actual classification
    decisions are not deteriorating significantly as training proceeds—at least not
    yet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Plateauing metrics: Validation performance levels off around step 250\. This
    likely reflects the fact that we are only training the classification head—the
    majority of the model (the ResNet backbone) remains frozen. That limited flexibility
    may be capping performance, despite early gains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s worth noting that validation performance here is not noticeably better
    than what we achieved by training ResNet from scratch (though notice how much
    faster `FinetunedHeadResNet` trained). This may suggest that training only the
    classification head doesn’t provide enough flexibility for the model to properly
    adapt to our dataset. The much higher training performance relative to validation
    performance also points to overfitting. While we could try to address this with
    regularization or augmentation, we’ll instead move on to what we expect will be
    a significantly stronger approach—fine-tuning the entire network.
  prefs: []
  type: TYPE_NORMAL
- en: Training the FinetunedResNet Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we’ll train a model that updates all the pretrained weights—not just
    the classification head. This approach, known as full fine-tuning, allows the
    model to gradually adapt its internal feature representations to better match
    our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s examine the model’s performance in [Figure 5-21](#last-lesion-model-multiclass-performance-over-steps):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0521.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-21\. Model performance over training and validation steps for the initial
    (non-optimized) `FinetunedResNet` multiclass skin lesion classification model.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are a few takeaways from these plots:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparison to previous models: This model clearly outperforms the baseline
    `SimpleCnn`, the `ResNetFromScratch`, and the `FinetunedHeadResNet` models in
    terms of validation metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Precision and recall: Validation metrics are consistently higher than in previous
    models—with weighted precision and recall reaching around 0.65–0.7 by the end
    of training. However, there is still a noticeable gap between training and validation
    performance, indicating that overfitting remains an issue we’ll need to address.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loss curves: The training loss steadily drops to near-zero, while the validation
    loss stays high and rises gradually. This is consistent with overfitting, although
    it’s worth remembering that rising softmax loss doesn’t always reflect a drop
    in classification quality (and indeed, here the validation metrics stay stable
    while validation loss rises).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, this is our best-performing model so far—confirming the benefit
    of full fine-tuning when using pretrained backbones. Before examining the confusion
    matrix of the fully fine-tuned model, let’s first apply a few targeted optimizations
    to further improve headline metric performance.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the FinetunedResNet Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll apply four key modifications:'
  prefs: []
  type: TYPE_NORMAL
- en: A warmup plus cosine decay learning rate schedule
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation (as introduced in [“Augmenting the dataset”](#augmenting-the-dataset))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-rate dropout to reduce overfitting (set to `0.7`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight decay using the `adamw` optimizer (set to `1e-4`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll walk through each of these in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate schedule
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Fine-tuning a pretrained model requires care to preserve the valuable low-level
    features it learned from large datasets like ImageNet. A common strategy is to
    use a *warmup learning rate schedule*, which starts training with a small learning
    rate that gradually increases—allowing the model to adapt gently—before decaying
    smoothly over time. This helps:'
  prefs: []
  type: TYPE_NORMAL
- en: Stabilize training in the early stages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prevent abrupt updates to pretrained weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow gradual adaptation to our new task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ll use a warmup and cosine decay schedule where the learning rate will:'
  prefs: []
  type: TYPE_NORMAL
- en: Warm up for the first 20% of steps, gradually increasing the learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peak at a learning rate of `0.001` (`1e-3`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smooth decay to `0.00001` (`1e-5`) by the end of training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s visualize how the learning rate evolves in [Figure 5-22](#fig521NEW):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0522.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-22\. Learning rate evolution during training, following a warmup-cosine
    decay schedule. The learning rate starts small, gradually increases over the first
    20% of training steps (warmup), then peaks and decays smoothly.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We previously observed overfitting even in our best model. One way to mitigate
    this is to apply data augmentation, which increases the effective diversity of
    the training set by introducing small, label-preserving transformations (e.g.,
    flips, brightness jitter, crops). As discussed earlier in this chapter, augmentation
    helps the model generalize more robustly by preventing it from memorizing the
    training images.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll apply these augmentation strategies by using the `rich_augmentor` defined
    earlier in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization via dropout and adamw
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to data augmentation, we’ll also apply explicit regularization
    using two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs: []
  type: TYPE_NORMAL
- en: Randomly disables neurons during training, encouraging the network to rely on
    distributed representations and reducing co-adaptation. We set a high dropout
    rate of 0.7 to strongly combat overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Weight decay
  prefs: []
  type: TYPE_NORMAL
- en: Discourages overly large weights by penalizing them in the loss function. We
    apply this using `optax.adamw`, which is designed to work correctly with L2 regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You cannot safely use L2 regularization with the standard Adam optimizer—it
    won’t apply weight decay in the intended way. This is exactly why `adamw` was
    introduced, and it should be used when adding weight decay.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s also standard practice *not* to apply weight decay to:'
  prefs: []
  type: TYPE_NORMAL
- en: Bias parameters
  prefs: []
  type: TYPE_NORMAL
- en: These are typically small and few in number and don’t contribute substantially
    to model complexity. Regularizing them doesn’t tend to help generalization and
    can sometimes hurt performance.
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization parameters
  prefs: []
  type: TYPE_NORMAL
- en: These include the learned scale (`gamma`) and shift (`beta`) terms, as well
    as running statistics. Applying weight decay to these can destabilize training,
    especially in fine-tuning scenarios, as they control the distribution of activations
    rather than model capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, weight decay is usually applied only to the main weights of convolutional
    or linear layers, where it can help prevent overfitting by discouraging overly
    complex solutions. To enforce this, we’ll use a parameter mask when constructing
    the optimizer—applying decay only to the relevant parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Optimized FinetunedResNet Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we’ve introduced a learning rate schedule, data augmentation, dropout,
    and weight decay—it’s time to put everything together. We can train the optimized
    `FinetunedResNet` model and see whether these changes help reduce overfitting
    and improve validation performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s analyze the model’s performance over the training steps, as shown
    in [Figure 5-23](#fig522NEW):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0523.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-23\. Model performance over training and validation steps for the optimized
    `FinetunedResNet` multiclass skin lesion classification model.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This training run demonstrates the cumulative benefit of regularization and
    optimization strategies. Some observations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision and recall: Both metrics improve steadily on both the training and
    validation sets. Most notably, the validation precision and recall reach nearly
    0.75 by the end of training—our best performance so far. The gap between training
    and validation metrics is also now narrower and more stable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loss curves: While the validation loss remains higher than training loss, it’s
    still lower than before and appears to not impact the validation metrics—a major
    improvement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In short, we’ve trained a model that generalizes much better and achieves strong,
    stable performance across all metrics. This serves as a great foundation for further
    experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand this model’s behavior, let’s plot the confusion matrix
    as [Figure 5-24](#fig524):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Some observations about the model’s behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most predictions lie along the diagonal: This indicates strong overall performance
    and class-specific accuracy. Nearly all classes show high recall, with vascular
    lesions and basal cell carcinoma especially well classified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Melanoma versus nevus: While there are still some melanoma cases being misclassified
    as nevi, the number is much lower than in earlier models and melanoma recall is
    the highest level yet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rare classes: Performance on rare classes like actinic keratosis, dermatofibroma,
    and squamous cell carcinoma has improved. These categories are now being correctly
    identified more consistently, with fewer scattered misclassifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Emptier off-diagonal cells: Many potential misclassification combinations are
    completely empty. This indicates that the model is no longer making widespread
    or erratic mistakes—it’s more selective and confident.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Visual inspection: The image thumbnails in each cell help confirm that correct
    predictions tend to look visually consistent, and many of the remaining errors
    involve subtle or understandable confusions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Altogether, this confusion matrix reinforces the earlier metrics: the optimized
    model is significantly more accurate, more reliable, and ultimately would be more
    clinically useful than earlier versions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0524.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-24\. Confusion matrix for the final optimized `FinetunedResNet` model
    on the nine-class classification task.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Confusion matrices can reveal a lot—but they won’t turn us into dermatologists.
    Interpreting model errors often requires deep domain knowledge. Collaborating
    with subject matter experts, like dermatologists in this case, can provide critical
    insights into misclassifications, uncover hidden biases in the data, and suggest
    clinically meaningful improvements. For any real-world machine learning project,
    involving domain experts early and often is one of the most effective ways to
    build useful, trustworthy models.
  prefs: []
  type: TYPE_NORMAL
- en: Further Improving the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll stop optimizing this model for now, but it’s far from perfect—for example,
    it completely fails to predict seborrheic keratosis. Here are several ideas for
    pushing it further, roughly ordered by expected bang for your buck:'
  prefs: []
  type: TYPE_NORMAL
- en: Longer training
  prefs: []
  type: TYPE_NORMAL
- en: Metrics were still improving slightly at the end of 2000 steps. Training for
    longer could yield further gains, especially since we are using a warmup schedule
    (which starts learning slowly to protect pretrained weights).
  prefs: []
  type: TYPE_NORMAL
- en: Ensembling
  prefs: []
  type: TYPE_NORMAL
- en: 'Combine predictions from multiple trained models to reduce variance and improve
    robustness:'
  prefs: []
  type: TYPE_NORMAL
- en: The most common ensembling approach here would be to average the softmax probabilities
    across models. Alternatively, you could use majority voting on predicted labels
    (less smooth, but sometimes effective).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can ensemble models trained with different random seeds (e.g., 2, 4, or
    8), or even ensemble structurally different models—such as the various ResNet
    models we’ve explored.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring augmentation strategies
  prefs: []
  type: TYPE_NORMAL
- en: There’s still plenty of room to be more creative with data augmentation. Try
    playing around with our augmentation setup (e.g., the specific transformations
    we encoded) and check their effect on model learning.
  prefs: []
  type: TYPE_NORMAL
- en: Class re-weighting
  prefs: []
  type: TYPE_NORMAL
- en: Increase the loss contribution from rare or clinically important classes like
    melanoma. This can be done by computing inverse class frequencies and passing
    per-example weights into the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Hard example mining
  prefs: []
  type: TYPE_NORMAL
- en: Focus on examples that are consistently misclassified (e.g., melanoma confused
    with nevus). You could maintain a pool of high-loss samples and upsample them
    in training batches, or alternatively, increase loss weights for these types of
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: Stronger regularization
  prefs: []
  type: TYPE_NORMAL
- en: 'Tune `dropout_rate` and `weight_decay`, or explore additional regularization
    methods such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Label smoothing
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using hard one-hot labels, you assign most of the probability to
    the true class (e.g., 0.9) and distribute the rest evenly across other classes.
    This reduces model overconfidence and can improve generalization.
  prefs: []
  type: TYPE_NORMAL
- en: Mixup or CutMix
  prefs: []
  type: TYPE_NORMAL
- en: Blend or patch together two images and their labels. These techniques regularize
    the model and encourage the model to learn more robust decision boundaries between
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal data
  prefs: []
  type: TYPE_NORMAL
- en: Dig into the original dataset to see if you can incorporate metadata such as
    patient age or sex, anatomical site of the lesion, or dermoscopic features. These
    features can be embedded (e.g., as one-hot or learned embeddings) and concatenated
    with image features before classification. Metadata often provides key disambiguating
    context that pixels alone can’t capture.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It can be hard to know where to start when trying to improve a model—and changes
    can interact in unexpected ways. Progress is rarely linear, so think of it as
    an exploration process. Use intuition, literature, and diagnostics to guide you—and
    have fun getting to know your model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classifying skin lesions is a genuinely difficult task—if it weren’t, early
    detection would be routine, and fewer cases would be missed. Still, the models
    we built in this chapter demonstrate how machine learning can meaningfully assist
    in this challenge, offering scalable tools to support (not replace) clinical expertise.
  prefs: []
  type: TYPE_NORMAL
- en: This project came with plenty of constraints, especially the limitations of
    the dataset. We had to adapt carefully—testing model variants, countering overfitting,
    and making the most of each part of the training setup. While more powerful models
    and larger datasets exist, we hope the mindset and practical tools introduced
    here help you tackle similar problems—particularly when working with limited or
    imperfect data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The techniques we explored—class balancing, data augmentation, fine-tuning
    pretrained models, diagnosing overfitting, and incrementally improving architectures—are
    widely applicable. Whether you’re detecting lung disease in chest X-rays, identifying
    tumors in MRIs, or analyzing retinal images for diabetic retinopathy, the same
    principles apply: adapt to the data, learn from errors, and iterate thoughtfully.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s move on. We’ve seen more than enough skin lesion images for one chapter.
    But before we go, a final reminder: keep an eye on your own skin. If something
    looks unusual or changes over time, don’t hesitate to seek medical advice. Early
    detection truly does save lives.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch05.html#id819-marker)) You can read about one such study [online](https://oreil.ly/YmXDg).
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch05.html#id820-marker)) Cassidy, Bill, et al., [“Analysis of the ISIC
    Image Datasets: Usage, Benchmarks and Recommendations”](https://doi.org/10.1016/j.media.2021.102305),
    *Medical Image Analysis* 75 (January 2022).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch05.html#id821-marker)) Statistics by the [World Health Organization
    (WHO)](https://oreil.ly/79RmS).
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch05.html#id828-marker)) M. P. Salinas, et al., [“A systematic review
    and meta-analysis of artificial intelligence versus clinicians for skin cancer
    diagnosis”](https://doi.org/10.1038/s41746-024-01103-x). *NPJ Digital Medicine*
    7, no. 1 (2024): 125.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch05.html#id848-marker)) He, K., Zhang, X., Ren, S., & Sun, J. (2015,
    December 10). [Deep residual learning for image recognition](https://oreil.ly/ZqFSo).
    arXiv.org.
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch05.html#id863-marker)) Codella, N. C. F., et al. (2018). Skin lesion
    analysis toward melanoma detection: A challenge at the 2017 International symposium
    on biomedical imaging (ISBI), hosted by the international skin imaging collaboration
    (ISIC). 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI), 168–172\.
    https://doi.org/10.1109/isbi.2018.8363547'
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch05.html#id870-marker)) Shorten, C., & Khoshgoftaar, T. M. (2019). [A
    survey on Image Data Augmentation for Deep Learning](https://doi.org/10.1186/s40537-019-0197-0).
    *Journal of Big Data*, 6(1).
  prefs: []
  type: TYPE_NORMAL
