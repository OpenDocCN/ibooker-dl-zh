- en: '7 Prompt engineering: Becoming an LLM whisperer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 提示工程：成为LLM的密语者
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: What a prompt is and how to make one
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示是什么以及如何制作一个
- en: Prompt engineering—more than just crafting a prompt
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程——不仅仅是制作一个提示
- en: Prompt engineering tooling available to make it all possible
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于实现所有这些的提示工程工具
- en: Advanced prompting techniques to answer the hardest questions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级提示技术以回答最困难的问题
- en: Behold, we put bits in the horses' mouths, that they may obey us; and we turn
    about their whole body.—James 3:3
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 看哪，我们在马嘴里放上了嚼子，这样它们就会服从我们；我们转动它们的全身。——雅各书 3:3
- en: In the last chapter, we discussed in depth how to deploy large language models
    and, before that, how to train them. In this chapter, we are going to talk a bit
    about how to use them. We mentioned before that one of the biggest draws to LLMs
    is that you don’t need to train them on every individual task. LLMs, especially
    the largest ones, have a deeper understanding of language, allowing them to act
    as a general-purpose tool.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们深入讨论了如何部署大型语言模型，在此之前，我们讨论了如何训练它们。在这一章中，我们将简要讨论如何使用它们。我们之前提到，LLMs的最大吸引力之一是你不需要在每一个单独的任务上对它们进行训练。LLMs，尤其是最大的那些，对语言有更深入的理解，使它们能够作为通用工具。
- en: Want to create a tutoring app that helps kids learn difficult concepts? What
    about a language translation app that helps bridge the gap between you and your
    in-laws? Need a cooking assistant to help you think up fun new recipes? With LLMs,
    you no longer have to start from scratch for every single use case; you can use
    the same model for each of these problems. It just becomes a matter of how you
    prompt your model. This is where prompt engineering, also called in-context learning,
    comes in. In this chapter, we are going to dive deep into the best ways to do
    that.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 想要创建一个帮助孩子们学习困难概念的教学应用？或者一个帮助你在与岳父岳母之间架起桥梁的语言翻译应用？需要一个烹饪助手来帮助你想出有趣的新食谱？有了LLMs，你不再需要为每个用例从头开始；你可以为这些问题中的每一个使用相同的模型。这仅仅是一个关于如何提示模型的问题。这就是提示工程，也称为情境学习，发挥作用的地方。在这一章中，我们将深入探讨实现这一点的最佳方法。
- en: 7.1 Prompting your model
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 提示你的模型
- en: What exactly is a prompt? We’ve used this word throughout this book, so it feels
    a bit late to be diving into definitions, but it’s worth discussing because in
    literature, a prompt is taken to mean many different things. In general, though,
    the most basic definition is that a prompt is the input to a language model. At
    this most basic level, you have already done lots of prompting at this point in
    the book. However, prompting often means more than that; it comes with the connotation
    that it is meaningful or done with thought. Of course, we know this isn’t usually
    the case in production with actual users. When we are prompting, we are doing
    more than just “chatting with a bot”; we are crafting an input to get a desired
    output.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 提示究竟是什么？我们在整本书中都在使用这个词汇，所以现在深入定义似乎有些晚了，但讨论它是值得的，因为在文学中，提示一词被理解为许多不同的含义。然而，总的来说，最基本的定义是提示是语言模型的输入。在这个最基本层面上，你在这本书的这个阶段已经做了很多提示。然而，提示往往意味着更多；它带有意义或经过思考的含义。当然，我们知道在实际生产中，这通常并不是情况。当我们进行提示时，我们做的不仅仅是“与机器人聊天”；我们正在制作一个输入以获得期望的输出。
- en: LLMs have access to vast vocabularies, terabytes of training data, and billions
    upon billions of weights, meaning that the information you’re looking to get out
    of the model has a decent chance of being in there somewhere—just not always up
    near the surface (read “the middle of the standard deviation of probable responses”)
    where you need it to be. The goal is to create a prompt that will guide the model
    in activating the parameters in the part of the model that contains the correct
    information. In essence, prompting is instruction given after the fact, and as
    such, it is important within app development because it doesn’t require expensive
    retraining of the model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs可以访问庞大的词汇量，数以TB计的训练数据，以及数十亿个权重，这意味着你从模型中想要获取的信息有相当大的可能性存在于某个地方——只是不一定总是在你需要的表面（阅读“标准差的中位数”）附近。目标是创建一个提示，以引导模型激活包含正确信息的模型部分中的参数。本质上，提示是在事后给出的指令，因此，在应用开发中它很重要，因为它不需要对模型进行昂贵的重新训练。
- en: With this in mind, prompt engineering is the process of designing, templating,
    and refining a prompt and then implementing our learnings into code. Prompt engineering
    is how we create meaningful and consistent user experiences out of the chaos of
    LLM-generated outputs. And it’s no joke. As LLMs are becoming more common in application
    workflows, we have seen the rise of titles like Prompt Engineer and AI Engineer,
    each of which commands impressive salaries.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，提示工程是设计、模板化和完善提示的过程，然后将其学习成果转化为代码。提示工程是我们如何从LLM生成的输出混乱中创造有意义的和一致的用户体验。这不是玩笑。随着LLM在应用工作流程中的普及，我们看到了像提示工程师和AI工程师这样的职位兴起，每个职位都要求令人印象深刻的薪水。
- en: 7.1.1 Few-shot prompting
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 少样本提示
- en: 'The most common form of prompt engineering is few-shot prompting because it’s
    both simple to do and extremely effective. Few-shot prompting entails giving a
    couple of examples of how you want the AI to act. Instead of searching for the
    tokens with the right distribution to get the response we want, we give the model
    several example distributions and ask it to mimic those. For example, if we wanted
    the model to do sentiment analysis defining reviews as positive or negative, we
    could give it a few examples before the input. Consider the following prompt:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的提示工程形式是少样本提示，因为它既简单又非常有效。少样本提示包括给出几个示例，说明你希望AI如何行动。我们不是寻找具有正确分布的标记以获得我们想要的响应，而是给出几个示例分布并要求模型模仿这些分布。例如，如果我们希望模型进行情感分析，将评论定义为正面或负面，我们可以在输入之前给出几个示例。考虑以下提示：
- en: 'Worked as advertised, 10/10: positive'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如广告所述，10/10：正面
- en: 'It was broken on delivery: negative'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 送达时已损坏：负面
- en: 'Worth every penny spent: positive'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 每一分钱都花得值得：正面
- en: 'Overly expensive for the quality: negative'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于质量来说过于昂贵：负面
- en: 'If this is the best quality they can do, call me mr president: negative'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是他们能提供的最佳质量，请叫我总统先生：负面
- en: '<Input data>:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: <输入数据>：
- en: 'Note, in this example, that we aren’t telling the model how to respond, but
    from the context, the LLM can figure out that it needs to respond with either
    the word *positive* or *negative*. In figure 7.1, we go ahead and plug the prompt
    in a model so you can see for yourself that it did indeed give a correct response
    in the expected format. Of course, there could be an array of acceptable responses,
    in which case giving instructions beforehand can help improve the results. To
    do this, we might append to our few-shot prompt with the following phrase, “Determine
    the sentiment of each review as one of the following: (positive, negative, neutral,
    strongly positive, strongly negative).” It’s also needed with most models; OpenAI
    includes language to restrict the output, such as “Please respond with only one
    option from the list with no explanation.” You might wonder why we’d suggest you
    say words like “please” to a model. The answer is pretty simple: in the training
    data, the highest-quality and most usefully structured human-to-human conversations
    follow certain conventions of politeness that you’re likely familiar with, like
    saying please and thank you. The same results can be achieved by using an excess
    of profanity and deep jargon on a topic because flouting those politeness conventions
    is another huge part of the training set, although that strategy isn’t as consistent,
    given that the companies training the models often “clean” their data of examples
    like that, regardless of their quality downstream. This type of prompting can
    be very useful when you need your response to be formatted in a certain way. If
    we need our response in JSON or XML, we could ask the model to return it in the
    format, but it will likely get the keys or typing wrong. We can easily fix that
    by showing the model several samples of expected results. Of course, prompting
    the model to return JSON will work, but JSON is a very opinionated data structure,
    and the model might hallucinate problems that are hard to catch, like using single
    instead of double quotes. We’ll go over tooling that can help with that later
    in the chapter.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个例子中，我们没有告诉模型如何回答，但根据上下文，大型语言模型可以推断出它需要用“正面”或“负面”中的一个词来回答。在图 7.1 中，我们直接将提示输入到模型中，以便你自己可以看到它确实以预期的格式给出了正确的回答。当然，可能会有多种可接受的回答，在这种情况下，提前给出指令可以帮助提高结果。为此，我们可以在我们的少次提示中添加以下短语：“确定每条评论的情感为以下之一：（正面，负面，中性，强烈正面，强烈负面）。”这在大多数模型中也是必要的；OpenAI
    包括限制输出的语言，例如“请只从列表中选择一个选项，无需解释。”你可能会想知道为什么我们会建议你对模型说“请”这样的词。答案很简单：在训练数据中，最高质量和最有用的结构化人机对话遵循某些礼貌的惯例，你很可能熟悉，比如说“请”和“谢谢”。通过在某个主题上使用过多的粗俗语言和深奥术语也可以达到相同的效果，因为违反这些礼貌惯例是训练集的另一个重要部分，尽管这种策略并不一致，因为训练模型的公司的通常会在下游质量不受影响的情况下“清理”这类例子。这种提示方式在你需要你的回答以特定方式格式化时非常有用。如果我们需要以
    JSON 或 XML 格式得到回答，我们可以要求模型以该格式返回，但它很可能会出错地得到键或类型。我们可以通过向模型展示几个预期的结果样本来轻松解决这个问题。当然，提示模型返回
    JSON 是可行的，但 JSON 是一个非常主观的数据结构，模型可能会产生难以捕捉的问题，比如使用单引号而不是双引号。我们将在本章后面讨论可以帮助解决这个问题的工具。
- en: '![figure](../Images/7-1.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7-1.png)'
- en: Figure 7.1 Few-shot prompting example
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.1 少次提示示例
- en: The one major downside to few-shot prompting is that examples can end up being
    quite long. For example, coding examples we might add and share can easily be
    thousands of tokens long, and that’s possible when defining a single function.
    Giving an example of an entire class, file, or project can easily push us out
    of our limits. Many models still have context limits restricted to 2K, 4K, or
    8K. Since token limits are often restrictive, it can be difficult to balance adding
    another example or giving the user more space. Also, we often pay per token, so
    few-shot prompting can be much more expensive than other prompting techniques.
    As a result, many have turned to one-shot prompting to be more efficient and save
    money.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 少次提示的一个主要缺点是示例可能会变得相当长。例如，我们可能添加和分享的编码示例很容易就有数千个标记长，定义单个函数时也是如此。给出整个类、文件或项目的示例可能会轻易超出我们的限制。许多模型仍然将上下文限制在
    2K、4K 或 8K。由于标记限制通常是限制性的，因此很难在添加另一个示例或给用户更多空间之间取得平衡。此外，我们通常按标记付费，因此少次提示可能比其他提示技术更昂贵。因此，许多人转向单次提示以更有效率并节省金钱。
- en: 7.1.2 One-shot prompting
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 单次提示
- en: One-shot learning is a machine learning concept where a model is expected to
    make accurate predictions given only a single example of each new class during
    training. In the context of LLMs and prompting, one-shot refers to situations
    where the model must understand and execute a task based on a single clear instruction
    or example in the prompt, often without seeing similar examples during training.
    It requires crafting the perfect example to get the expected results.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 单次学习是机器学习中的一个概念，其中模型在训练期间只根据每个新类的一个示例进行准确预测。在LLMs和提示的背景下，单次指的是模型必须根据提示中的单个清晰的指令或示例来理解和执行任务的情况，通常在训练期间没有看到类似的例子。这需要精心制作完美的例子以获得预期的结果。
- en: Consider our previous sentiment analysis example; if you give a model only one
    positive example, you will likely bias the model to give only positive classifications—especially
    if the model has never seen such a problem before. So how can one-shot prompting
    ever be achieved? Thankfully, while this seems impossible at the outset, it’s
    quite achievable. After all, few-shot prompting is very effective but follows
    the law of diminishing returns. Each new example improves only marginally. The
    first example always does the heaviest lifting.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们之前的情感分析示例；如果你只给模型一个积极的例子，你可能会使模型偏向于只给出积极的分类——特别是如果模型以前从未见过这样的问题。那么单次提示如何实现呢？幸运的是，虽然一开始这似乎是不可能的，但实际上是相当容易实现的。毕竟，少量样本提示非常有效，但遵循递减回报定律。每个新例子只带来微小的改进。第一个例子总是承担最重的负担。
- en: 'LLMs can perform well on one-shot tasks due to the extensive pretraining they
    undergo on large and diverse datasets. During this pretraining, the models learn
    a wide array of language patterns, concepts, and structures, giving them a broad
    understanding of language and the world. When given a new one-shot task, they
    use this learned understanding to comprehend the task and generate a response,
    even if the exact task was not part of their training data. Here’s an example
    of a prompt attempting to coerce the model using one-shot prompting to respond
    to a word problem correctly:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLMs在大型和多样化的数据集上进行了广泛的预训练，因此它们在单次任务上表现良好。在这次预训练中，模型学习到了广泛的语言模式、概念和结构，使它们对语言和世界有了广泛的理解。当被赋予一个新的单次任务时，它们会利用这种学习到的理解来理解任务并生成响应，即使这个任务不是它们训练数据的一部分。以下是一个尝试使用单次提示来诱使模型正确回答一个文字问题的提示示例：
- en: 'CB **User:** Answer this question. Think it through step by step, so that we
    know it’s correct: A dad and his son were in an accident and rushed to a hospital.
    The man’s son was in the operating room and the doctor said, “I can’t operate
    on you. You’re my son.” How is that possible?'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: CB **用户**：回答这个问题。一步一步地思考，这样我们才能知道它是正确的：一位父亲和他的儿子在事故中受伤，并被送往医院。这个男人的儿子在手术室里，医生说：“我不能给你做手术。你是我的儿子。”这是怎么可能的？
- en: '![chatGpt](../Images/chatGpt.png) **Assistant:** The son and the father were
    both injured, meaning that the only other person that the doctor could be and
    still have the same son is the father’s spouse, maybe a mother, or another father
    perhaps.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![chatGpt](../Images/chatGpt.png) **助手**：儿子和父亲都受伤了，这意味着医生唯一可能仍然是同一个儿子的其他人是父亲的配偶，可能是母亲，也可能是另一个父亲。'
- en: 'CB **User:** Answer this question. Think it through step by step, so that we
    know it’s correct: How could it be possible to be your own grandpa?'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: CB **用户**：回答这个问题。一步一步地思考，这样我们才能知道它是正确的：如何可能成为你自己的爷爷？
- en: '**CB Assistant**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**CB 助手**'
- en: In figure 7.2 we go ahead and plug this one-shot prompt into an LLM. Think about
    how you imagine it might answer. Given our example, do you think it will do well
    on the task?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7.2中，我们将这个单次提示插入到LLM中。想想你想象它可能会如何回答。根据我们的例子，你认为它在任务上会做得好吗？
- en: '![figure](../Images/7-2.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-2.png)'
- en: Figure 7.2 One-shot prompting example
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2 单次提示示例
- en: The ability of LLMs to handle one-shot tasks improves as they are scaled up.
    As the model size increases and they are trained on more diverse and larger datasets,
    their capacity to generalize from their training to unseen one-shot tasks also
    improves. Nonetheless, it’s worth noting that while LLMs can perform impressively
    on one-shot tasks, they are not perfect, and their performance can still vary
    based on the complexity and specificity of the task. One-shot prompting generally
    needs much less data and only one example to craft, making it more accessible,
    faster to craft, and easier to experiment with. One-shot prompting has led researchers
    to push the boundaries even further.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLMs规模的扩大，它们处理单样本任务的能力也在提高。随着模型规模的增加，它们在更多样化和更大的数据集上进行训练，它们从训练数据泛化到未见过的单样本任务的能力也在提高。尽管如此，值得注意的是，虽然LLMs在单样本任务上可以表现出色，但它们并不完美，它们的性能仍然会根据任务的复杂性和具体性而变化。单样本提示通常需要的数据更少，只需要一个例子来构建，这使得它更容易获得、更快地构建，并且更容易进行实验。单样本提示促使研究人员进一步拓展边界。
- en: 7.1.3 Zero-shot prompting
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 零样本提示
- en: 'Having just explained few-shot and one-shot prompting, we’re sure you have
    already guessed what zero-shot prompting is. But since this is a book, let’s spell
    it out: zero-shot prompting is figuring out how to craft a prompt to get us the
    expected results without giving any examples. Zero-shot prompts often don’t perform
    as consistently as few-shot or one-shot prompts, but they have the advantage of
    being ubiquitous since we don’t need any examples or data.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在刚刚解释了少量样本和单样本提示之后，我们相信你已经猜到了零样本提示是什么。但既然这是一本书，让我们明确指出：零样本提示是找出如何构建一个提示，以获得我们期望的结果，而不需要给出任何例子。零样本提示通常不如少量样本或单样本提示表现一致，但它们的优势在于无处不在，因为我们不需要任何例子或数据。
- en: 'A common zero-shot prompt is a very simple template:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的零样本提示是一个非常简单的模板：
- en: '“Q: [User’s Prompt] A:.”'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: “问：[用户提示] 答：。”
- en: With just a slight variation to the user’s prompt—adding it to a template that
    contains only two letters—we can get much better results by priming the model
    to answer the prompt as if it were a question—no examples necessary.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 只需对用户的提示进行轻微的修改——将其添加到只包含两个字母的模板中——我们就可以通过让模型像回答问题一样回答提示来获得更好的结果——无需任何例子。
- en: Most zero-shot prompts take advantage of Chain of Thought (CoT). Wei et al.[¹](#footnote-103)
    showed that by encouraging models to follow a step-by-step process, reasoning
    through multiple steps instead of jumping to conclusions, LLMs were more likely
    to answer math problems correctly—similar to how math teachers ask their students
    to show their work. Using few-shot prompting, the model was given several examples
    of reasoning through math problems. However, it was soon discovered that examples
    weren’t needed. You could elicit chain-of-thought behavior simply by asking the
    model to “think step by step.”[²](#footnote-104)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数零样本提示都利用了思维链（CoT）。Wei等人[¹](#footnote-103)表明，通过鼓励模型遵循逐步的过程，通过多步推理而不是直接得出结论，LLMs更有可能正确回答数学问题——类似于数学老师要求学生展示解题过程。使用少量样本提示，模型被提供了几个通过数学问题进行推理的例子。然而，很快发现例子并不是必需的。你只需要求模型“逐步思考”就能引发思维链行为。[²](#footnote-104)
- en: By appending four magic words to the end of our prompts, “think step by step,”
    models transformed from dunces into puzzle-solving Olympiads. It was truly a marvel.
    Of course, it came with some problems. Thinking through multiple steps led to
    longer responses and a less ideal user experience. This was compounded later with
    the phrases “a more elegant solution” and “get this through your head ********,”
    which worked just as well but were less consistent if the domain was less common,
    with the last one achieving very concise and correct responses. We like to get
    straight to the point, after all, and we are used to computers answering our math
    problems extremely quickly. From our own experience, we’ve often noticed that
    when models are giving longer answers, they also don’t know when to stop, continuing
    to generate responses long after giving an answer. Later, we’ll show you how to
    solve this problem by creating stopping criteria with tools like LangChain or
    Guidance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在我们的提示末尾附加四个魔法词“逐步思考”，模型从笨拙者变成了解决难题的奥林匹克选手。这确实是一个奇迹。当然，这也带来了一些问题。通过多步骤思考导致了更长的回答和不太理想的用户体验。后来，随着短语“一个更优雅的解决方案”和“让你明白
    ********”的出现，虽然效果同样好，但如果领域不太常见，它们的一致性会降低，最后一个短语达到了非常简洁和正确的回答。毕竟，我们喜欢直接切入要点，并且我们习惯于计算机快速回答我们的数学问题。根据我们的经验，我们经常注意到，当模型给出更长的回答时，它们也不知道何时停止，在给出答案后继续生成回答。稍后，我们将向您展示如何通过使用LangChain或Guidance等工具创建停止标准来解决此问题。
- en: There isn’t, of course, a perfect zero-shot prompt yet, and it’s a continuing
    part of research, although there likely never will be just one perfect prompt.
    We could, at most, get one perfect zero-shot prompt per model. Zhou et al. proposed
    an interesting strategy they termed “thread of thought.”[³](#footnote-105) Essentially,
    they figured they could do better than “think step by step” if they just used
    a few more words. So they generated 30 variations of the phrase and ran evaluations
    to determine which one worked best. From their work, they proposed that the prompt
    “Walk me through this context in manageable parts step by step, summarizing and
    analyzing as we go” would give better results when working with GPT-4\. It’s hard
    to know if this prompt works equally well with other models, but their strategy
    is interesting nonetheless.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，目前还没有一个完美的零样本提示，这仍然是研究的一部分，尽管可能永远不会有唯一一个完美的提示。我们最多只能为每个模型得到一个完美的零样本提示。周等人提出了一种他们称之为“思维线索”的有趣策略。[³](#footnote-105)
    实际上，他们认为如果只是使用更多一些的词语，他们就能做得比“逐步思考”更好。因此，他们生成了30种短语的变化，并进行了评估以确定哪一种效果最好。从他们的工作中，他们提出，当与GPT-4一起工作时，提示“逐步、分步骤地引导我理解这个情境，并在过程中总结和分析”会得到更好的结果。很难知道这个提示是否与其他模型同样有效，但他们的策略仍然很有趣。
- en: Some other notable findings have left researchers flabbergasted that the approach
    worked; for example, offering an imaginary tip to a model will return better results.
    One X (formerly known as Twitter) user suggested the solution as a joke and was
    confused to find it worked, and the model offered more info relative to the size
    of the tip (for the original tipping test, see [https://mng.bz/2gD9](https://mng.bz/2gD9)).
    Others later confirmed it helped with several other prompting principles.[⁴](#footnote-106)
    In addition, the authors have found strategies like telling the model you’ll lose
    your job if it doesn’t help you or even threatening to fire the model if it does
    a terrible job have elicited better results. Like the original “think step by
    step,” asking the model to “take a deep breath” can also ensure better outputs,
    particularly in math problems.[⁵](#footnote-107) It seems most strategies humans
    use, or use on other humans, to produce better work are fair game. Of course,
    the best trick will depend on which model you use and the underlying data it was
    trained on.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一些其他引人注目的发现让研究人员感到惊讶，因为这种方法有效；例如，向模型提供一个想象中的小费将返回更好的结果。一位X（以前称为Twitter）用户将这个解决方案作为玩笑提出，并惊讶地发现它有效，模型根据小费的大小提供了更多相关信息（有关原始小费测试，请参阅[https://mng.bz/2gD9](https://mng.bz/2gD9)）。其他人后来证实，它有助于几个其他的提示原则。[⁴](#footnote-106)
    此外，作者发现，告诉模型如果不帮助你会失去工作，或者甚至威胁要解雇表现糟糕的模型，这些策略也能产生更好的结果。就像原始的“逐步思考”一样，要求模型“深呼吸”也可以确保更好的输出，尤其是在数学问题中。[⁵](#footnote-107)
    似乎人类使用的或用于他人的大多数策略，以产生更好的工作都是公平的。当然，最好的技巧将取决于你使用的模型以及它所训练的基础数据。
- en: 7.2 Prompt engineering basics
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 提示工程基础
- en: 'We expect that most readers have probably done lots of prompting, but very
    few have done much of any prompt engineering yet. We’ve heard lots of jokes that
    prompt engineering isn’t a real discipline. We’ve also heard every other week
    that some library is “killing prompt engineering” by automatically prompting the
    model. One doubt about prompt engineering stems from how accessible prompting
    is to anyone who wants to try it and the lack of education needed to prompt effectively.
    All doubts about prompt engineering are the same doubts people express about linguistics
    as a discipline: “I’ve used language all my life; I know how it works.” So it
    makes sense that people similarly assume they know what language to use to effectively
    prompt an LLM. Anyone can learn effective strategies by simply playing with models
    or from purely online resources. In other words, it’s hard to believe that there
    is any real engineering going on when the majority of players are simply using
    the “guess and check” method. But this logic highlights a basic misunderstanding
    of what engineering is. There’s a big difference between getting a model to solve
    your problem once and getting it to solve every user’s problem every single time.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预计，大多数读者可能已经做了很多提示，但很少有人真正进行过提示工程。我们听到了很多关于提示工程不是真正学科的笑话。我们也每隔一周就会听到一些库通过自动提示模型“杀死”提示工程的言论。关于提示工程的怀疑之一源于提示对任何想要尝试的人的易用性以及有效提示所需的缺乏教育。关于提示工程的任何怀疑都与人们对语言学作为学科的怀疑相同：“我一生都在使用语言；我知道它是如何运作的。”因此，人们同样假设他们知道如何使用语言来有效地提示大型语言模型。任何人都可以通过简单地玩模型或仅从在线资源中学习来掌握有效的策略。换句话说，当大多数玩家只是使用“猜测和检查”方法时，很难相信有任何真正的工程在进行。但这种逻辑突显了对工程的基本误解。让模型一次性解决你的问题与让它在每次都解决每个用户的问题之间有很大的区别。
- en: There are several challenges with prompt engineering over regular prompting.
    For example, prompt engineering relies particularly on knowing the format the
    user expects the answer to be in. With prompting, you are the user, so you can
    keep trying until you see an answer you like; that doesn’t fly in prompt engineering.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在常规提示之外进行提示工程存在几个挑战。例如，提示工程特别依赖于了解用户期望答案的格式。在提示过程中，你是用户，因此你可以不断尝试，直到看到你喜欢的答案；但在提示工程中则不行。
- en: A bigger problem is that when building an application, your end users will have
    varying levels of knowledge of how to craft a prompt. Some may not have any skill
    and will struggle to get good responses, and others will have so much skill they
    will likely try to persuade your LLM to go off the rails you’ve set for it. Regardless,
    our goal is to build railings so that skilled users won’t be able to derail your
    application and unskilled users will have a smooth ride. A user’s skill in crafting
    a prompt shouldn’t be the determining factor of a successful experience.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的问题是，在构建应用程序时，你的最终用户对如何构建提示的了解水平各不相同。有些人可能没有任何技能，将难以获得良好的响应，而另一些人则可能技能高超，可能会试图说服你的大型语言模型偏离你为其设定的轨道。无论如何，我们的目标是建立围栏，以便有技能的用户无法使你的应用程序出轨，而缺乏技能的用户将能够顺利行驶。用户在构建提示方面的技能不应是成功体验的决定性因素。
- en: Another thing to call out is the decision process that you, as a product owner,
    must go through to get the model output to match the style you want. Should you
    finetune a new checkpoint, should you PEFT a LoRA, or can you achieve it through
    prompting? Unfortunately, due to the emergent nature of the behavior that we’re
    seeing with LLMs, there isn’t a good or at least definitive answer. Our recommendation
    at this point is to try prompt engineering first to see how good you can get without
    changing the model and then finetune from there as you see fit. I’ve seen some
    professional success using one base model and multiple LoRAs trained on different
    scenarios and styles of response combined with prompt engineering on the front,
    especially sanitizing and stylizing user input.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点需要指出的是，作为产品所有者，你必须经过的决策过程，以便使模型输出的内容与你想的风格相匹配。你应该微调一个新的检查点，应该使用PEFT对LoRA进行微调，还是可以通过提示来实现？遗憾的是，由于我们观察到的LLM行为的涌现性质，没有好的或至少是确定的答案。我们目前的建议是首先尝试提示工程，看看在不改变模型的情况下你能达到多好的效果，然后根据你的需要从那里进行微调。我见过一些专业人士使用一个基础模型和多个在不同场景和响应风格上训练的LoRAs，结合前端提示工程，特别是净化和风格化用户输入，取得了成功。
- en: Lastly, a good prompt engineer should be able to tell you rather quickly whether
    the solution you are trying to build can be done with prompt engineering at all.
    Even utilizing advanced techniques like retrieval-augmented generation (RAG),
    there are limitations on what you can do with prompt engineering alone. Knowing
    when you need to send a model back for additional finetuning is invaluable and
    can save your team from spinning their wheels for weeks without any progress.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个好的提示工程师应该能够很快地告诉你，你试图构建的解决方案是否可以通过提示工程来完成。即使利用像检索增强生成（RAG）这样的高级技术，单独使用提示工程也有其局限性。知道何时需要将模型送回进行额外的微调是无价的，并且可以避免你的团队在没有进展的情况下空转数周。
- en: To get started, we’ll need to cover the basics about what makes up a prompt.
    In this section, we’ll discuss the different parts of a prompt, additional parameters
    that can be tuned in a query, and notes about paying attention to a model’s training
    data that you should be aware of.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，我们需要了解构成提示的基本要素。在本节中，我们将讨论提示的不同部分、可以在查询中调整的附加参数，以及你应该注意的关于模型训练数据的注意事项。
- en: 7.2.1 Anatomy of a prompt
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 提示的结构
- en: To an engineer, a prompt is made up of a few elements, and identifying these
    elements makes it easier to create a framework to solve your use case and provide
    a better example for your users. Let’s say we are building an internal chatbot
    for our company to help answer HR-related questions based on internal documentation.
    One prompt we might expect from a user would be, “How much does the company match
    for our 401k?” This is the first element of a prompt, the input or user’s prompt.
    If you have only ever used LLM apps and have never built them, this is likely
    all you’ve ever seen. Generally, the input is gathered from a free-form text box,
    so it’s important to note that it can almost be anything. Often it will be awful,
    riddled with typos and mistakes, and not written in a manner to speak to a bot
    but to speak to another human.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于工程师来说，提示由几个元素组成，识别这些元素使得创建一个框架来解决你的用例并为你的用户提供更好的示例变得更容易。假设我们正在为公司构建一个内部聊天机器人，以帮助回答基于内部文档的人力资源相关问题。我们可能期望用户提出的一个提示是，“公司为我们的401k匹配多少？”这是提示的第一个元素，即输入或用户的提示。如果你只使用过LLM应用而从未构建过它们，这可能是你见过的所有内容。通常，输入是从自由文本框中收集的，因此请注意，它几乎可以是任何东西。通常它会很糟糕，满是拼写错误和错误，并且不是以与机器人交谈的方式而是以与另一个人交谈的方式书写。
- en: Let’s go ahead and pull back the curtain for a second and look at what the LLM
    likely saw based on that question with proper prompt engineering in place.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时揭开面纱，看看基于适当提示工程，LLM可能看到了什么。
- en: CB **System:** You are a helpful assistant who knows about all company policies
    at XYZ company. Be courteous and keep conversations strictly related to the company.
    Offer links when available so users can look at the documentation themselves.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 'CB **系统**: 你是一个了解XYZ公司所有公司政策的友好助手。保持礼貌，并确保对话严格与公司相关。当有链接时，提供链接，以便用户可以自己查看文档。'
- en: CB **User:** How much does the company match for our 401k?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 'CB **用户**: 公司为我们的401k匹配多少？'
- en: CB **Context:** `<Examples` `from` `a` `VectorDB` `search` `for` `closest` `document>`
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 'CB **上下文**: `<从VectorDB搜索中获取最接近的文档的示例>`'
- en: CB **System:** Strictly answer the user’s question, and only if it relates to
    company policies. If you don’t know the answer, simply say so. Be courteous and
    keep conversations strictly related to the company.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 'CB **系统**: 严格回答用户的问题，并且只有当它与公司政策相关时。如果你不知道答案，只需简单地说出来。保持礼貌，并确保对话严格与公司相关。'
- en: '![chatGpt](../Images/chatGpt.png)**Assistant:** Sure, I can help you with that!
    The company currently offers a 4% match to your 401k. You can find more details…'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![chatGpt](../Images/chatGpt.png)**助手**: 当然，我可以帮你！公司目前为你的401k提供4%的匹配。你可以找到更多详细信息...'
- en: This was a real example showcasing a situation in which the bot responded in
    the most probable way to the user’s satisfaction. Giving an LLM information in
    a structured format improves the model’s chance of responding correctly. So let’s
    break down what we are seeing.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个真实示例，展示了机器人以最可能的方式响应用户，使用户满意的情况。以结构化格式向LLM提供信息可以提高模型正确响应的机会。所以，让我们分析一下我们看到的内容。
- en: 'First, to improve results, we will often take the user’s prompt and inject
    it into an instruction set or template. One of the most basic templates and a
    great example is the Q&A bot template which we showed earlier and which would
    have looked like this: “Q: How much does the company match for our 401k? A:”.
    Generally, in this section, though, instructions will be given to direct the model.
    It doesn’t have to be much, but often it will be much more detailed. For example,
    “Answer the following question and explain it as if the user was a five-year-old.
    Q: How much does the company match for our 401k? A:”.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了提高结果，我们通常会取用户的提示并将其注入到一个指令集或模板中。最基本的一个模板，也是一个很好的例子，就是之前展示过的问答机器人模板，它看起来可能是这样的：“问：公司为我们的401k匹配多少？答：”。通常，在这个部分，我们会给出指令来指导模型。指令不必太多，但通常会更详细。例如，“回答以下问题，并像对五岁孩子解释一样。问：公司为我们的401k匹配多少？答：”。
- en: The next element is the context the model will need to respond appropriately.
    In our example, it’s very likely we haven’t finetuned a model to know XYZ’s company
    policies. What we need to do is give it to the model inside the prompt. In our
    example, we are likely doing this with RAG and where we would add the results
    from a semantic search.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个要素是模型需要适当响应的上下文。在我们的例子中，我们很可能没有微调一个模型来了解XYZ公司的政策。我们需要做的是在提示中将其提供给模型。在我们的例子中，我们很可能是通过RAG（Retrieval-Augmented
    Generation）来做到这一点，并添加语义搜索的结果。
- en: Context can be lots of different things and not just RAG search results. It
    could be the current time, weather information, current events, or even just the
    chat history. You will often also want to include some database lookup information
    about the user to provide a more personalized experience. All of this is information
    we might look up at the time of query, but context can often be static. For example,
    one of the most important pieces of information to include in the context is examples
    to help guide the model via few-shot or one-shot prompting. If your examples are
    static and not dynamic, they likely are hard-coded into the instruction template.
    The context often contains the answers to the users’ queries, and we are simply
    using the LLM to clean, summarize, and format an appropriate response. Ultimately,
    any pragmatics the model lacks will need to be given in the context.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可以是很多不同的事情，而不仅仅是RAG搜索结果。它可以是当前时间、天气信息、当前事件，甚至是聊天历史。你通常还希望包括一些关于用户的数据库查找信息，以提供更个性化的体验。所有这些都是在查询时可能会查找的信息，但上下文通常是静态的。例如，在上下文中包含的一些最重要的信息是例子，以通过少量或单次提示引导模型。如果你的例子是静态的而不是动态的，它们很可能是硬编码到指令模板中的。上下文通常包含用户查询的答案，我们只是使用LLM（大型语言模型）来清理、总结和格式化适当的响应。最终，模型缺乏的任何实用主义都需要在上下文中给出。
- en: 'The last element is the system prompt. The system prompt is a prompt that will
    be appended and used on every request by every user. It is designed to give a
    consistent user experience. Generally, it’s where we would include role prompting
    or style prompting. Some examples of such role prompting or style prompting could
    include the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个要素是系统提示。系统提示是一个将被附加并用于每个用户每个请求的提示。它旨在提供一致的用户体验。通常，我们会在这里包括角色提示或风格提示。以下是一些这样的角色提示或风格提示的例子：
- en: Take this paragraph and rephrase it to have a cheerful tone and be both informative
    and perky.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 将这段话改写成一种愉快、信息丰富且活泼的语气。
- en: You are a wise old owl who helps adventurers on their quest.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一只智慧的老猫头鹰，帮助探险家们在他们的旅途中。
- en: In the form of a poem written by a pirate.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以海盗写的诗的形式。
- en: The system prompt isn’t designed to be seen by end users, but obtaining the
    system prompt is often the goal of many prompt injection attacks—since knowing
    what it is (along with the model you are using) is essentially like stealing source
    code and allows the hacker to recreate your application. Of course, the system
    prompt itself is a great way to curb prompt injection and ensure your bot stays
    in character. Many great applications will include two system prompts, one at
    the front and one at the end, to avoid any “ignore previous instructions” type
    prompt injection attacks. It also helps keep the model focused on how we want
    it to behave since models tend to put more weight on what is said at the beginning
    and at the end. You may have noticed this in our previous example. Regardless,
    you shouldn’t keep any sensitive information in the system prompt.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 系统提示不是设计给最终用户看到的，但获取系统提示通常是许多提示注入攻击的目标——因为知道它是（以及你使用的模型）基本上就像窃取源代码一样，允许黑客重新创建你的应用程序。当然，系统提示本身是遏制提示注入并确保你的机器人保持角色的一种很好的方式。许多优秀应用将包括两个系统提示，一个在前面，一个在后面，以避免任何“忽略之前的指令”类型的提示注入攻击。这也帮助模型专注于我们希望它如何表现，因为模型往往更重视开始和结束时的说法。你可能已经在我们的前一个例子中注意到了这一点。无论如何，你不应该在系统提示中保留任何敏感信息。
- en: Parts of the prompt
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示的部分
- en: 'The following are the four parts of a prompt:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个提示的四个部分：
- en: '*Input*—What the user wrote; can be anything'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入*—用户所写的内容；可以是任何东西'
- en: '*Instruction*—The template used; often contains details and instructions to
    guide the model'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*指令*—使用的模板；通常包含细节和指导模型的信息'
- en: '*Context*—Pragmatics that the model needs to respond appropriately (e.g., examples,
    database lookups, RAG)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*上下文*—模型需要适当响应的语用学（例如，示例、数据库查找、RAG）'
- en: '*System prompt*—A specific instruction given to the model on every request
    to enforce a certain user experience (e.g., talk like a pirate)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*系统提示*—在每次请求中对模型给出的特定指令，以强制执行一定的用户体验（例如，像海盗一样说话）'
- en: 7.2.2 Prompting hyperparameters
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 提示超参数
- en: Another aspect of prompt engineering you won’t see with simple prompting is
    prompt hyperparameter tuning. There are several hyperparameters in addition to
    the prompt you can set when making a query to increase or decrease the diversity
    of responses. Depending on your objective, the value of these parameters can greatly
    improve or even be a detriment to the query results for your users. It is important
    to note that being able to set these depends on the LLM API endpoint you are querying
    to be set up to accept them.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单的提示中看不到的提示工程的一个方面是提示超参数调整。在制作查询时，除了提示之外，你还可以设置几个超参数，以增加或减少响应的多样性。根据你的目标，这些参数的值可以极大地改善甚至损害你用户的查询结果。重要的是要注意，能够设置这些参数取决于你查询的LLM
    API端点是否被设置为接受它们。
- en: First and foremost is temperature. The temperature parameter determines the
    level of randomness your model will account for when generating tokens. Setting
    it to zero will ensure the model will always respond exactly the same way when
    presented with identical prompts. This consistency is critical for jobs where
    we want our results to be predictable, but it can leave our models stuck in a
    rut. Setting it to a higher value will make it more creative. Setting it to negative
    will tell it to give you the opposite response to your prompt.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是温度。温度参数决定了模型在生成标记时将考虑的随机性水平。将其设置为零将确保模型在接收到相同的提示时将始终以完全相同的方式响应。这种一致性对于我们希望结果可预测的工作至关重要，但它可能会让我们的模型陷入僵局。将其设置为更高的值将使其更具创造性。将其设置为负值将指示它给出与提示相反的响应。
- en: To understand this parameter better, it might help to look closer at how a model
    determines the next token. Figure 7.3 shows an example of this process. Given
    the input, “I am a,” a language model will generate a vector of logits for each
    token in the model’s vocabulary. From here, we’ll apply a softmax, which will
    generate a list of probabilities for each token. These probabilities show the
    likelihood that each token will be chosen.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个参数，仔细观察模型如何确定下一个标记可能会有所帮助。图7.3展示了这个过程的一个示例。给定输入，“I am a”，语言模型将为模型词汇表中的每个标记生成一个logits向量。从这里，我们将应用softmax，这将生成每个标记的概率列表。这些概率显示了每个标记被选择的可能性。
- en: '![figure](../Images/7-3.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-3.png)'
- en: Figure 7.3 A simple path of how the next word is chosen. Given an input, a model
    will generate a vector of logits for each token in the model’s vocabulary. Using
    the softmax algorithm, these logits will be transformed into probabilities. These
    probabilities will correspond to how often that token is likely to be chosen.
    Temperature is applied during the softmax algorithm.
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3展示了选择下一个单词的简单路径。给定一个输入，模型将为模型词汇表中的每个标记生成一个logits向量。使用softmax算法，这些logits将被转换为概率。这些概率将对应于该标记被选择的频率。温度是在softmax算法中应用的。
- en: Temperature is applied during the softmax algorithm. A higher temperature will
    flatten out the probability distribution, giving less weight to tokens with large
    logits and more weight to tokens with smaller logits. A lower temperature does
    the opposite. A temperature of zero is actually impossible since we can’t divide
    by zero. Instead, we run an argmax algorithm, ensuring we pick the token with
    the highest logit.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 温度是在softmax算法中应用的。更高的温度会使概率分布变得平坦，减少对具有大logits的标记的权重，并增加对具有小logits的标记的权重。较低的温度则相反。实际上，温度为零是不可能的，因为我们不能除以零。相反，我们运行argmax算法，确保我们选择具有最高logits的标记。
- en: The next parameter to consider is the number of beams applied to the model’s
    beam search. Beam search is a heuristic search algorithm that explores the graph
    of your model’s to-be-generated text probabilities, expanding the graph’s most
    optimistic nodes. It helps balance time and memory usage and improves the flow
    and quality of the response. It’s similar to the minimax algorithm in chess, except
    instead of deciding the next best move, we are deciding the next best word. Selecting
    a higher number of beams will create a larger search, improving results at the
    cost of latency.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的下一个参数是应用于模型beam search的beam数量。Beam search是一种启发式搜索算法，它探索模型待生成文本概率的图，扩展图中最乐观的节点。它有助于平衡时间和内存使用，并提高响应的流畅性和质量。它与棋类游戏中的minimax算法类似，除了不是决定下一个最佳移动，而是决定下一个最佳单词。选择更多的beam数量将创建更大的搜索，以牺牲延迟为代价提高结果。
- en: Top K is an interesting parameter. Assuming a temperature that isn’t zero, top
    K allows us to filter the potential next tokens by the K most probable options.
    Consequently, we eliminate less-probable words on the tail end of the distribution
    from ever being picked and avoid generating tokens that are more likely to be
    incoherent. So in our example from figure 7.3, if k = 3, then the only tokens
    we would choose are woman, man, or boy, filtering out the rest.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Top K是一个有趣的参数。假设一个非零的温度，Top K允许我们通过K个最可能的选择来过滤潜在的下一个标记。因此，我们消除了分布尾部不太可能的单词，避免生成更可能是不连贯的标记。所以，在我们的7.3图例中，如果k
    = 3，那么我们只会选择女人、男人或男孩，过滤掉其他所有标记。
- en: Top P sets the threshold probability that the next token must reach to be selected.
    It’s similar to top K, but instead of considering the number of tokens, we are
    considering their distributions. A top P of 0.05 will only consider the next 5%
    most likely tokens and will lead to very rigid responses, while a top P of 0.95
    will have greater flexibility but may turn out more gibberish. From our example
    in figure 7.3, if P = 0.5, only the tokens woman or man would be chosen since
    their probabilities 0.35 and 0.26 add up to greater than 0.5.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Top P设置下一个标记必须达到的阈值概率才能被选中。它与Top K类似，但不是考虑标记的数量，而是考虑它们的分布。Top P为0.05将只考虑下一个5%最可能的标记，这将导致非常僵硬的响应，而Top
    P为0.95将具有更大的灵活性，但可能会产生更多混乱。从我们的7.3图例中，如果P = 0.5，只有女人或男人的标记会被选择，因为它们的概率0.35和0.26相加大于0.5。
- en: Language models can often get caught in generation loops, repeating themselves
    in circles. To prevent this, we can add penalties. A frequency penalty adds a
    penalty for reusing a word if it was recently used. It is good to help increase
    the diversity of language. For example, if the model keeps on reusing the word
    “great,” increasing the frequency penalty will push the model to use more diverse
    words like “awesome,” “fantastic,” and “amazing” to avoid the penalty of reusing
    the word “great.”
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型往往容易陷入生成循环，反复重复。为了防止这种情况，我们可以添加惩罚。频率惩罚会在单词最近被使用时对其重新使用添加惩罚。这有助于增加语言的多样性。例如，如果模型持续重复使用单词“伟大”，增加频率惩罚将推动模型使用更多样化的单词，如“惊人”、“了不起”和“惊人”，以避免重复使用单词“伟大”的惩罚。
- en: A presence penalty is similar to a frequency penalty in that we penalize repeated
    tokens, but a token that appears twice and a token that appears 100 times are
    penalized the same. Instead of just reducing overused words and phrases, we are
    aiming to reduce overused ideas and increase the likelihood of generating new
    topics.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 存在惩罚与频率惩罚类似，我们惩罚重复的标记，但出现两次的标记和出现100次的标记受到的惩罚是相同的。我们不仅旨在减少过度使用的单词和短语，还旨在减少过度使用的思想，并增加生成新主题的可能性。
- en: 7.2.3 Scrounging the training data
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 搜集训练数据
- en: The importance of prompt engineering for model performance has led to important
    discussions surrounding context windows and the efficacy of particular prompt
    structures, as LLMs responding quickly and accurately to the prompts has become
    a more widespread goal. In addition, a correlation has been drawn between cleaner
    examples and better responses from the model, emphasizing the need for better
    prompt engineering, even on the data side. While prompt engineering is often proposed
    as an alternative to finetuning, we’ve found the most success using both in conjunction
    to get two boosts in LLM performance as opposed to just one.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程对于模型性能的重要性导致了关于上下文窗口和特定提示结构有效性的重要讨论，因为快速准确地响应提示已成为一个更广泛的目标。此外，清洁的示例与模型更好的响应之间存在相关性，强调了更好的提示工程的需求，即使在数据方面也是如此。虽然提示工程通常被提议作为微调的替代方案，但我们发现同时使用两者可以获得两个提升LLM性能的成果，而不是仅仅一个。
- en: Knowing the lingo and the choice of words used to generate the model will help
    you craft better responses. Let’s explain with a personal example. For the birthday
    of this author’s wife, I finetuned a text-to-image Stable Diffusions model to
    replicate her image so she could create fun pictures and custom avatars. I used
    the DreamBooth (see figure 7.4).[⁶](#footnote-108) The finetuning method requires
    defining a base class that can be used as a starting point. My first attempts
    were naive, and using the base class of “a person” or “a woman” was terrible.
    A base class of “Asian woman” returned pictures of older Asian women, often stylized
    in black and white or sepia. I then tried “young Asian woman,” but this created
    weird images of Asian faces being plastered onto young white women’s bodies.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 了解用于生成模型的术语和词汇选择将帮助您构建更好的回答。让我们用一个个人例子来解释。为了作者妻子的生日，我微调了一个文本到图像的Stable Diffusions模型来复制她的形象，这样她就可以创建有趣的图片和定制头像。我使用了DreamBooth（见图7.4）。[⁶](#footnote-108)
    微调方法需要定义一个可以作为起点的基础类。我的第一次尝试很天真，使用“一个人”或“一个女人”作为基础类效果很差。将“亚洲女人”作为基础类返回了老一辈亚洲女性的图片，通常以黑白或棕褐色调呈现。然后我尝试了“年轻亚洲女人”，但这创造了奇怪的亚洲面孔被贴在年轻白人女性身体上的图像。
- en: '![figure](../Images/7-4.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/7-4.png)'
- en: Figure 7.4 Example of DreamBooth from Ruiz et al.⁷ DreamBooth allows you to
    finetune an image model to replicate an object’s likeness based on only a few
    sample input images. Here, with only four example images of a puppy, Dreambooth
    can put that same dog in many new scenarios.
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.4 DreamBooth的示例（来自Ruiz等人⁷）。DreamBooth允许您通过仅使用少量样本输入图像来微调图像模型以复制对象的相似性。在这里，仅使用四张小狗的示例图像，Dreambooth就可以将这只狗放在许多新的场景中。
- en: Giving up guessing, I went to the source, the LAION dataset ([https://laion.ai/blog/laion-400-open-dataset/](https://laion.ai/blog/laion-400-open-dataset/))
    the model was trained on. LAION comprises 400 million images scraped from the
    internet with their accompanying captions. It is a noncurated dataset quickly
    put together for research purposes (aka, it’s unclean with lots of duplicates,
    NSFW content, and poor captions). Searching the dataset, I discovered that there
    was not a single caption with the words “Asian woman.” Scrolling through, I quickly
    found that pictures of Asian women and models were identified with the words “Asian
    beauty.” Using these words as the base class, I was finally able to create great
    avatars for my wife.**[⁷](#footnote-109)**
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 放弃猜测，我转向了源头，即模型训练所用的LAION数据集([https://laion.ai/blog/laion-400-open-dataset/](https://laion.ai/blog/laion-400-open-dataset/))。LAION包含从互联网上抓取的4亿张图片及其相应的字幕。这是一个为了研究目的快速组装的非精选数据集（即，它不干净，有很多重复，NSFW内容，以及糟糕的字幕）。在数据集中搜索，我发现没有一张图片的标题包含“亚洲女人”这个词。滚动浏览，我很快发现亚洲女性和模特的图片被标记为“亚洲美女”。使用这些词作为基础类，我终于为我的妻子创建出了出色的头像。[⁷](#footnote-109)**
- en: There’s lots of social commentary that can be drawn from this example, much
    of it controversial, but the main point is that if you want to craft effective
    prompts, you have to know your data. If your model believes “woman” and “beauty”
    are two different things because of the training data, that is something you’ll
    need to know to engineer better prompts. This is why finetuning in conjunction
    with prompt engineering is powerful. You can set the seed with particular phrases
    and choice of words when finetuning and then use prompt engineering to help the
    model recall the information based on using those same phrases and choice of words.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子中可以得出很多社会评论，其中很多都是具有争议性的，但主要观点是，如果你想制作有效的提示，你必须了解你的数据。如果你的模型因为训练数据而认为“女人”和“美丽”是两件不同的事情，那么这就是你需要知道来制作更好的提示的事情。这就是为什么与提示工程结合的微调是强大的。在微调时，你可以使用特定的短语和词汇选择来设置种子，然后使用提示工程来帮助模型根据使用相同的短语和词汇选择回忆信息。
- en: 7.3 Prompt engineering tooling
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 提示工程工具
- en: If you are building any application that is more than just a wrapper around
    the LLM itself, you will want to do a bit of prompt engineering to inject function
    or personality into it. We’ve already gone over the basics of prompt engineering
    itself, but when building, it would be helpful to have some tools at your disposal
    to know how to make it all work. To that extent, let’s look at some of the most
    prominent tooling available and how to use them.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在构建一个不仅仅是LLM包装器的应用程序，你将想要进行一些提示工程，以注入功能或个性。我们已经讨论了提示工程的基本知识，但在构建时，拥有一些工具来了解如何使一切工作是有帮助的。在这方面，让我们看看一些最突出的工具及其使用方法。
- en: 7.3.1 LangChain
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 LangChain
- en: Anyone who’s built an LLM application before has probably spent some time working
    with LangChain. One of the most popular libraries, it’s known for extracting away
    all the complexity—and simplicity—of building a language application. It is known
    for its ease of creating language chains with what it calls the LangChain Expression
    Language (LCEL).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 任何之前构建过LLM应用程序的人可能都花了一些时间与LangChain一起工作。它是最受欢迎的库之一，以其简化构建语言应用程序的复杂性和简单性而闻名。它以其易于使用所谓的LangChain表达式语言（LCEL）创建语言链而知名。
- en: LCEL makes it easy to build complex chains from basic components. In the next
    listing, we demonstrate creating a very simple chain that creates a prompt from
    a template, sends it to an LLM model, and then parses the results, turning it
    into a string.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: LCEL使得从基本组件构建复杂的链变得容易。在下一列表中，我们展示了创建一个非常简单的链，该链从模板创建提示，将其发送到LLM模型，然后解析结果，将其转换为字符串。
- en: Listing 7.1 Example of creating a basic LangChain chain
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.1 创建基本LangChain链的示例
- en: '[PRE0]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To be honest, using LangChain for something like this is a bit of overkill for
    what is essentially an f-string prompt, but it demonstrates what is happening
    under the hood. For the most part, you are likely going to use one of the many
    chains already created by the community. In the next chapter, we will explain
    how to create a RAG system with the RetrievalQA chain, but many more chains are
    available. For example, there are chains for generating and running SQL, interacting
    with APIs, and generating synthetic data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 说实话，用LangChain来做这种事情有点过度设计，因为本质上只是一个f-string提示，但它展示了底层发生了什么。大部分情况下，你可能会使用社区已经创建的众多链之一。在下一章中，我们将解释如何使用RetrievalQA链创建一个RAG系统，但还有更多链可供选择。例如，有用于生成和运行SQL、与API交互以及生成合成数据的链。
- en: Once we have a chain, additional tools in the LangChain ecosystem help provide
    a more complete user experience. We can use LangServe to easily host it as an
    API. We can also use LangSmith, an in-depth logging tool that allows us to trace
    a chain invocation and see how the results change passing through each link in
    the chain.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了链，LangChain生态系统中的额外工具可以帮助提供更完整的用户体验。我们可以使用LangServe轻松将其托管为API。我们还可以使用LangSmith，这是一个深入的日志工具，它允许我们跟踪链的调用并查看结果如何通过链中的每个链接而变化。
- en: Chains don’t have to be linear like they are in this example. Several asynchronous
    components allow you to create a whole slew of complicated language processing
    logic. Ultimately, chains are just another type of data pipeline or DAG, except
    specialized for language models.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 链不必像这个例子中那样线性。几个异步组件允许你创建一系列复杂的语言处理逻辑。最终，链只是另一种类型的数据管道或DAG，但专门针对语言模型。
- en: 7.3.2 Guidance
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 指导
- en: Guidance is an open source library from Microsoft that enforces programmatic
    responses. We’ve heard from several developers that the best engineering when
    working with LLMs is the good ol’ prompt-and-pray method. Generate a prompt, and
    pray that it works. Guidance seeks to solve that problem and has tooling to constrain
    the response space and set custom stopping tokens, as well as complex templating.
    After looking at dozens of LangChain projects, we believe Guidance is likely what
    most people are looking for when considering prompt engineering tooling.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 指引是来自微软的开源库，它强制执行程序性响应。我们已从几位开发者那里听说，与LLM一起工作的最佳工程方法是古老的提示和祈祷方法。生成一个提示，然后祈祷它有效。Guidance旨在解决这个问题，并提供工具来约束响应空间和设置自定义停止标记，以及复杂的模板。在查看数十个LangChain项目之后，我们相信Guidance很可能是人们在考虑提示工程工具时最想要的。
- en: Guidance allows you to control the flow of generated responses. It’ll be easiest
    to show you what we mean. In listing 7.2, you’ll see several of the basic building
    blocks of Guidance where we can guide our LLM to respond in very specific ways—namely,
    loading a model with the guidance HF wrapper (models) and using the `gen` function
    to generate specific text and constraints like `select`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 指引允许您控制生成响应的流程。我们将最容易向您展示我们的意思。在列表7.2中，您将看到几个Guidance的基本构建块，我们可以引导我们的LLM以非常具体的方式响应——即，使用Guidance
    HF包装器（models）加载模型，并使用`gen`函数生成特定的文本和约束，如`select`。
- en: Listing 7.2 Guidance basics
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.2 Guidance基础
- en: '[PRE1]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Loads a Hugging Face Transformers model'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 加载Hugging Face Transformers模型'
- en: '#2 Sets a token limit that is an actual limit'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 设置一个实际的限制标记'
- en: '#3 Sets stopping tokens'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 设置停止标记'
- en: '#4 Writes a sentence about the printing press'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 写一句关于印刷机的话'
- en: '#5 Combines multiple limits'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 结合多个限制'
- en: '#6 Generates a specific response from a list'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 从列表中生成特定响应'
- en: '#7 Uses regular expressions to ensure the response matches a pattern'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 使用正则表达式确保响应匹配模式'
- en: With these basic building blocks that allow us to constrain the LLM’s response
    space, we are then able to create grammars. Grammars are a Guidance concept, and
    as the name implies, are language rules your model will have to follow. Grammars
    are composable and reusable and allow us to build neat applications quickly. In
    the next listing, we show you how to build simple parts of a speech application
    using guidance grammars. To create a grammar, we only need to create a function
    using the `@guidance` decorator.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些基本构建块，我们可以约束LLM的响应空间，然后我们能够创建语法。语法是Guidance的概念，正如其名所示，是模型必须遵循的语言规则。语法是可组合和可重用的，并允许我们快速构建
    neat 应用程序。在下一个列表中，我们将向您展示如何使用指导语法构建语音应用的基础部分。要创建语法，我们只需要使用`@guidance`装饰器创建一个函数。
- en: Listing 7.3 Building a parts-of-speech app with Guidance
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.3 使用Guidance构建语音应用
- en: '[PRE2]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Loads a Hugging Face Transformers model'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 加载Hugging Face Transformers模型'
- en: '#2 Creates functions to easily implement grammars'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建函数以轻松实现语法'
- en: 'Even though we are using a small language model, we get the exact output we’d
    expect. We no longer need to prompt and pray. Granted, the results in the actual
    parts of speech prediction aren’t that great, but we could easily improve that
    by using a more powerful LLM or finetuning on more representative data:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们使用的是一个小型语言模型，但我们得到了我们预期的确切输出。我们不再需要提示和祈祷。当然，在实际的语音预测部分中，结果并不那么出色，但我们可以通过使用更强大的LLM或使用更具代表性的数据进行微调来轻松改进这一点：
- en: '[PRE3]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The generated text is
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本是
- en: '[PRE4]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Guidance isn’t as popular as LangChain, and at least at the time of this writing,
    its documentation leaves a lot to be desired, so you might find it a bit harder
    to get started. However, it has a thriving community of its own with a strong
    core group of developers who continue to support it. We highly recommend checking
    it out.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 指引（Guidance）不如LangChain受欢迎，至少在撰写本文时，其文档仍有许多不足之处，因此您可能会发现入门有点困难。然而，它拥有一个充满活力的社区，以及一个强大的核心开发者团队，他们继续支持它。我们强烈推荐您尝试一下。
- en: 7.3.3 DSPy
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.3 DSPy
- en: Unlike other toolings mentioned, DSPy does not give you tools to create your
    own prompts; rather, it attempts to program prompting. DSPy, coming out of Stanford
    and heavily backed by many corporate sponsors, takes a unique approach by emphasizing
    tool augmentation, including retrieval, and is helpful if you would like to treat
    LLMs as deterministic and programmatic tools instead of emergent infinite syntax
    generators.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他提到的工具不同，DSPy 并不提供创建你自己的提示的工具；相反，它试图编程提示。DSPy 来自斯坦福大学，并得到了许多企业赞助商的大力支持，它采取了一种独特的方法，强调工具增强，包括检索，如果你希望将
    LLMs 作为确定性和程序性工具而不是涌现的无限语法生成器来处理，这将是有帮助的。
- en: Although this is not exactly what happens, you can think of DSPy as taking a
    similar logic to prompting that ONNX takes to saving models. Give it some dummy
    inputs, and it’ll compile a graph that can then infer prompts that work the best
    for your model and return the results you’re wanting. There’s a bit more work
    involved, though. You need to write validation logic and modules, essentially
    a workflow and unit tests, to check against. This effectively changes the dynamic
    from coming up with clever strings to something much closer to engineering software.
    Admittedly, it leaves open the question, “If you’re going to define everything
    programmatically anyway, why are you using an LLM?” Still, we’ve had good experiences
    with this and use it frequently.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这并不是确切发生的事情，但你可以将 DSPy 视为采用与 ONNX 保存模型类似的提示逻辑。给它一些虚拟输入，它将编译一个图，然后推断出最适合你模型的提示，并返回你想要的结果。不过，这需要做更多的工作。你需要编写验证逻辑和模块，本质上是一个工作流程和单元测试，以进行检查。这实际上将动态从提出巧妙的字符串转变为更接近于软件工程。诚然，它留下了这样的问题：“如果你无论如何都要程序化地定义一切，为什么还要使用
    LLM？”尽管如此，我们在这方面有很好的经验，并且经常使用它。
- en: 'The steps to using DSPy effectively are as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DSPy 的有效步骤如下：
- en: Create a signature or a description of the task(s) along with input and output
    fields.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个任务或任务的签名或描述，包括输入和输出字段。
- en: Create a predictor or generation style similar to chain of thought or retrieval.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个预测器或生成风格，类似于思维链或检索。
- en: Define the module or program.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模块或程序。
- en: Once these steps have been completed, you’ll compile the program. This will
    update the module based on the examples given before, similar to the training
    set. All of this will feel like machine learning for LLMs, with a training set
    (examples), a loss function (validation metric), and essentially an optimizer
    (teleprompter).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，你将编译程序。这将根据之前给出的示例更新模块，类似于训练集。所有这些都会感觉像是 LLMs 的机器学习，有一个训练集（示例）、一个损失函数（验证指标）以及本质上是一个优化器（提词器）。
- en: 'In lieu of writing another listing for this chapter showcasing another tool,
    we decided to point you to an excellent notebook created by the StanfordNLP team
    introducing DSPy along with local LLMs and custom datasets: [https://mng.bz/PNzg](https://mng.bz/PNzg)
    (it’s forked from here: [http://mng.bz/Ea4r](http://mng.bz/Ea4r)). Once you have
    a chance to explore this example, we also recommend checking out the DSPy documentation,
    as it has many more excellent examples.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本章没有展示其他工具的列表，我们决定向您推荐一个由 StanfordNLP 团队创建的优秀笔记本，其中介绍了 DSPy 以及本地 LLMs 和自定义数据集：[https://mng.bz/PNzg](https://mng.bz/PNzg)（它从这里分支出来：[http://mng.bz/Ea4r](http://mng.bz/Ea4r)）。一旦您有机会探索这个示例，我们还建议查看
    DSPy 文档，因为它包含更多优秀的示例。
- en: 7.3.4 Other tooling is available but …
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.4 其他工具可用，但……
- en: Beyond the previously mentioned tools, a whole host of tools are out there.
    A couple to note are MiniChain and AutoChain. Both aim to be lightweight alternatives
    to LangChain, which are sorely needed, as many complain about LangChain’s bulkiness.
    Promptify is an interesting project that is a full-feature alternative to LangChain.
    To be honest, we could list a dozen more, but there likely isn’t much point. While
    many of these projects drew vibrant communities to them when they started, most
    have been dormant for months already, with only the rare GitHub contribution.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前提到的工具之外，还有一大堆工具。值得注意的是 MiniChain 和 AutoChain。两者都旨在成为 LangChain 的轻量级替代品，这是非常需要的，因为许多人抱怨
    LangChain 的臃肿。Promptify 是一个有趣的项目，它是 LangChain 的全功能替代品。说实话，我们可以列出更多，但可能没有太多意义。虽然许多项目在开始时吸引了充满活力的社区，但大多数项目已经沉寂数月，GitHub
    的贡献也极为罕见。
- en: It’s hard to say exactly why the interest in these projects faltered, but one
    obvious reason is that most of these projects lacked the sponsorship that LangChain,
    Guidance, and DSPy have. Many of these projects started as personal projects in
    the middle of big waves from the hype of ChatGPT’s success, but hype energy is
    never enough to build software that lasts. Without proper backing, most open source
    projects fail.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 很难确切地说为什么对这些项目的兴趣减弱了，但一个明显的理由是，这些项目中的大多数缺乏 LangChain、Guidance 和 DSPy 所拥有的赞助。许多这些项目是在
    ChatGPT 成功的炒作中作为个人项目开始的，但炒作的能量永远不足以构建持久的软件。没有适当的支持，大多数开源项目都会失败。
- en: We’ve probably painted too bleak a picture. As of the time of this writing,
    though, it’s still too early to tell, and this space is still a growing sector.
    There are still plenty of interesting tools we recommend checking out that we
    just don’t have space to include, like Haystack, Langflow, and Llama Index. Outlines
    is particularly of note as a similar project to Guidance, which is also awesome.
    We mostly want to point out that readers should be careful when picking tooling
    in this space because everything is still so new. If you find a tool you like,
    contribute.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能描绘了一幅过于悲观的画面。然而，截至撰写本文时，情况仍然难以预料，这个领域仍在不断发展。我们推荐大家检查的有趣工具还有很多，但我们没有足够的空间来包括它们，比如
    Haystack、Langflow 和 Llama Index。Outlines 作为与 Guidance 类似的项目，尤其值得关注，后者也非常出色。我们主要想指出的是，读者在选择这个领域的工具时应该小心，因为一切仍然都很新。如果你找到一个你喜欢的工具，请做出贡献。
- en: 7.4 Advanced prompt engineering techniques
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 高级提示工程技术
- en: No matter how well designed your prompt is, there will be pragmatic context
    your model won’t have access to. For example, current events are a struggle. The
    model itself will only know about information up to its training date. Sure, we
    could feed that context in with RAG, as we’ve done so far, but that just shifts
    the burden to keeping our RAG system up to date. There’s another way. In this
    section, we will discuss giving models access to tools and what we can do with
    them once we do.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你的提示多么设计得巧妙，你的模型都无法访问到一些实际情境。例如，当前事件是一个挑战。模型本身只知道其训练日期之前的信息。当然，我们可以像我们迄今为止所做的那样，通过
    RAG 将这个情境喂给模型，但这只是将负担转移到了保持我们的 RAG 系统更新。还有另一种方法。在本节中，我们将讨论如何让模型访问工具，以及我们一旦这样做后可以做什么。
- en: 7.4.1 Giving LLMs tools
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 为 LLM 提供工具
- en: What if instead of a complicated prompt engineering system, we instead give
    our model access to the internet? If it knows how to search the internet, it can
    always find up-to-date information. While we are at it, we can give it access
    to a calculator so we don’t have to waste CPU cycles having the LLM itself do
    basic math. We can give it access to a clock so it knows the current time and
    maybe even a weather app so it can tell the weather. The sky’s the limit! We just
    need to train the model on how to use tools, and that’s where Toolformers comes
    in.[⁸](#footnote-110)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不是复杂的提示工程系统，而是让我们的模型访问互联网会怎样？如果它知道如何搜索互联网，它总能找到最新的信息。在此过程中，我们还可以让它访问计算器，这样我们就不必浪费
    CPU 周期让 LLM 本身做基本的数学运算。我们可以让它访问时钟，这样它就知道当前时间，甚至可能是一个天气应用程序，这样它就可以告诉我们天气情况。天空才是极限！我们只需要训练模型如何使用工具，这就是
    Toolformers 的作用所在。[⁸](#footnote-110)
- en: 'Toolformers is a marvelously simple idea. Let’s train a model to know it can
    run API calls to different tools using tags like `<API></API>`. Then, at inference,
    when we see these tags, we can tell our interpreter to run those API calls. If
    that sounds familiar, it’s because Toolformers just trained a model to use string
    interpolation! String interpolation is the process of evaluating a string literal
    containing placeholders, which are replaced with the actual values at run time.
    For example, in Python, we could take the string literal `print(f''2+2` `=` `{2+2}'')`,
    and once printed, we’d get `''2+2` `=` `4''`. The placeholder `{2+2}` was evaluated
    and executed as Python code, returning `4`. Schick et al. finetuned a GPT-J model
    to use five different tools: a question-answering database, a calculator, a Wikipedia
    search, a translator, and a calendar. With access to these tools, they were able
    to achieve impressive results, outperforming GPT-3 on many tasks.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Toolformers 是一个奇妙简单的想法。让我们训练一个模型，使其知道它可以使用像 `<API></API>` 这样的标签来调用不同的工具的 API
    调用。然后，在推理时，当我们看到这些标签时，我们可以告诉我们的解释器运行这些 API 调用。如果这听起来很熟悉，那是因为 Toolformers 只是训练了一个模型来使用字符串插值！字符串插值是评估包含占位符的字符串字面量的过程，这些占位符在运行时被替换为实际的值。例如，在
    Python 中，我们可以取字符串字面量 `print(f'2+2` `=` `{2+2}')`，一旦打印出来，我们会得到 `'2+2` `=` `4'`。占位符
    `{2+2}` 被评估并执行为 Python 代码，返回 `4`。Schick 等人微调了一个 GPT-J 模型来使用五种不同的工具：问答数据库、计算器、维基百科搜索、翻译器和日历。通过访问这些工具，他们能够取得令人印象深刻的成果，在许多任务上超过了
    GPT-3。
- en: While Schick et al.’s work paved the way, the major downside to this approach
    is that we don’t want to finetune a model every time we create a new tool. However,
    as we’ve discussed in this chapter, we don’t have to. Instead, we can use clever
    prompt engineering to introduce new tools using LangChain or Guidance. In the
    next listing, we demonstrate how to create simple math tools with Guidance. Guidance
    takes care of the heavy lifting by stopping generation when it recognizes a tool
    being called, running the tool, and starting generation again.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Schick 等人的工作开辟了道路，但这种方法的主要缺点是我们不希望每次创建一个新工具时都要微调一个模型。然而，正如我们在本章中讨论的，我们不必这样做。相反，我们可以使用巧妙的提示工程，通过
    LangChain 或 Guidance 引入新工具。在下一个列表中，我们展示了如何使用 Guidance 创建简单的数学工具。Guidance 通过在识别到被调用的工具时停止生成、运行工具并再次开始生成来处理繁重的工作。
- en: Listing 7.4 Giving tools to our LLM models with Guidance
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.4 使用 Guidance 给我们的 LLM 模型提供工具
- en: '[PRE5]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Loads a Hugging Face Transformers model'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 加载 Hugging Face Transformers 模型'
- en: While a simple example, it’s easy to imagine building more advanced tooling.
    Regardless of whether you use LangChain or Guidance, there are a few things to
    keep in mind when building tools. First, you’ll need to instruct your model in
    the prompt on where and how to use the tools you give it. This can be more or
    less difficult, depending on how open-ended your function is. Second, your model
    matters in its ease of extendibility. Some models we’ve worked with would never
    use the tools we gave them or would even hallucinate other tools that didn’t exist.
    Lastly, be really careful with the inputs and error handling for tools you give
    an LLM. The ones we used previously in this chapter are terrible and likely to
    break in several ways. For example, an LLM could easily try to run `add(one,`
    `two)` or `add(1,` `2,` `3)`, both of which would throw errors and crash the system.
    With Guidance, to make this easier, we can enforce tool inputs by building grammars
    to ensure our model inputs are always correct.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是一个简单的例子，但很容易想象构建更高级的工具。无论您使用 LangChain 还是 Guidance，在构建工具时都有几点需要注意。首先，您需要在提示中指导您的模型在哪里以及如何使用您提供的工具。这可能会更难或更容易，这取决于您的函数有多开放。其次，模型的可扩展性也很重要。我们合作的一些模型永远不会使用我们给它们的工具，甚至可能会产生不存在的其他工具。最后，对于您提供给
    LLM 的工具的输入和错误处理要非常小心。我们在这章中之前使用的是糟糕的，并且可能会以多种方式崩溃。例如，一个 LLM 可能会轻易尝试运行 `add(one,`
    `two)` 或 `add(1,` `2,` `3)`，这两者都会抛出错误并使系统崩溃。使用 Guidance，为了使这更容易，我们可以通过构建语法来强制工具输入，确保我们的模型输入始终正确。
- en: This discussion leads us to uncover some problems with LLMs using tools. First,
    we have to be careful what tools we give to an LLM since we never really know
    what input it will generate. Even if we ensure the tool doesn’t break, it may
    do something malicious we didn’t intend. Second, as you’ve probably gathered throughout
    this chapter, prompt engineering quickly grows our input and thus shrinks the
    token limit for our actual users; explaining tools and how to use them adds to
    that constraint. Often, this limitation reduces the number of tools we can give
    an LLM and, thus, its usefulness. Third, LLMs are still hit or miss as to whether
    they actually use a tool and can often end up using the wrong tool. For example,
    should the LLM use the web search tool or the weather tool to look up the 10-day
    forecast? This might not matter much to us as humans, but results can vary widely
    for a bot. Lastly, building tools can be difficult and error prone, as you need
    to build both a clean tool and an effective prompt.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这场讨论使我们发现了使用工具的LLM（大型语言模型）的一些问题。首先，我们必须小心我们提供给LLM的工具，因为我们从未真正知道它将生成什么输入。即使我们确保工具不会出错，它也可能执行我们未意图的恶意行为。其次，正如您可能在本章中了解到的那样，提示工程迅速增加了我们的输入，从而减少了我们实际用户的token限制；解释工具及其使用方法增加了这一限制。通常，这种限制减少了我们可以提供给LLM的工具数量，从而降低了其有用性。第三，LLM在使用工具方面仍然是不稳定的，它们往往最终会使用错误的工具。例如，LLM应该使用网络搜索工具还是天气工具来查找10天的天气预报？这对我们人类来说可能不是很重要，但对于机器人来说，结果可能会有很大差异。最后，构建工具可能很困难且容易出错，因为您需要构建一个干净的工具和一个有效的提示。
- en: OpenAI’s plugins
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: OpenAI的插件
- en: Toolformers opened the gates to OpenAI’s Plugins concept ([https://mng.bz/q0rE](https://mng.bz/q0rE)).
    Plugins allow third parties to easily integrate their tools into ChatGPT and provide
    a simple way for ChatGPT to call external APIs. Plugins were introduced relatively
    early in ChatGPT’s life, shortly after the Toolformers paper.^a All a third party
    had to do was create an OpenAPI config file and an ai-plugin.json file and host
    both where the API existed. OpenAPI is a specification language for APIs that
    standardizes and defines your API to make it easy for others to consume. (If you
    haven’t heard of OpenAPI and have APIs that customers use, it’s a good practice
    to follow. You can learn more at [https://www.openapis.org/](https://www.openapis.org/).)
    Plenty of tools can help you generate that file easily enough. The ai-plugin file
    created the plugin. Here, you could define a name for the plugin, how authentication
    should happen, and descriptions to be used to prompt ChatGPT. From here, the plugin
    could be registered with OpenAI in ChatGPT’s interface, and after a review process,
    your plugin could be added and used by users as they interacted with ChatGPT.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Toolformers开启了OpenAI插件概念的门户([https://mng.bz/q0rE](https://mng.bz/q0rE)). 插件允许第三方轻松地将他们的工具集成到ChatGPT中，并为ChatGPT调用外部API提供了一种简单的方式。插件在ChatGPT的生命周期中相对较早地被引入，在Toolformers论文发布后不久.^a
    第三方只需创建一个OpenAPI配置文件和一个ai-plugin.json文件，并将两者托管在API存在的地方。OpenAPI是一种用于API的规范语言，它标准化并定义了您的API，使其易于他人消费。(如果您还没有听说过OpenAPI并且有客户使用的API，遵循这一做法是个好习惯。您可以在[https://www.openapis.org/](https://www.openapis.org/)了解更多信息。)有很多工具可以帮助您轻松生成该文件。ai-plugin文件创建了插件。在这里，您可以定义插件的名称，如何进行身份验证，以及用于提示ChatGPT的描述。从这里，插件可以在ChatGPT的界面中注册给OpenAI，经过审查过程后，您的插件可以被用户添加并使用，当他们与ChatGPT互动时。
- en: Despite an initial fervor, plugins never left Beta—beyond OpenAI’s own web browsing
    plugin—and appear to be abandoned. There are lots of reasons for this, but in
    a since-taken-down report, the main reason came from Sam Altman when he suggested,
    “A lot of people thought they wanted their apps to be inside ChatGPT, but what
    they really wanted was ChatGPT in their apps” ([https://mng.bz/75Dg](https://mng.bz/75Dg)).
    As a result, there didn’t seem to be a product market fit for OpenAI’s plugins
    that would make the company money. But we think it’s too early to abandon the
    idea entirely.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一开始热情很高，但插件从未离开Beta版本——除了OpenAI自己的网页浏览插件——并且看起来已经被放弃。这有很多原因，但根据后来被撤下的报告，主要原因来自Sam
    Altman的建议：“很多人认为他们想要他们的应用在ChatGPT内部，但他们真正想要的其实是ChatGPT在他们的应用中”([https://mng.bz/75Dg](https://mng.bz/75Dg))。因此，似乎没有适合OpenAI插件的产品市场契合度，这会使公司赚钱。但我们认为，完全放弃这个想法还为时过早。
- en: As more companies integrate LLM technology into their apps, they are likely
    going to want access to third-party tools. Suppose you are going camping for the
    first time and you ask an LLM shopping assistant for advice on what to buy. In
    that case, it’d be really nice if it thought first to ask where and when you were
    going camping and then could use that information to identify weather-appropriate
    gear. The LLM shopping assistant for a particular brand or store is likely to
    have access to loads of products, but access to weather reports in a random geolocation?
    Not so much.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 随着越来越多的公司将LLM技术集成到他们的应用程序中，他们很可能会想要访问第三方工具。假设你是第一次去露营，你向一个LLM购物助手询问购买建议。在这种情况下，如果它能首先询问你打算去哪里以及什么时候去露营，然后利用这些信息来识别适合天气的装备，那就太好了。特定品牌或商店的LLM购物助手可能能够访问大量产品，但访问随机地理位置的天气预报？不太可能。
- en: While you can always build these tools, wouldn’t it be great if they were already
    created for you, and you could simply go to some hub, download the ones you wanted,
    and plug them in? Unfortunately, this option doesn’t exist yet, at least not to
    the extent we describe it here. We have high expectations that a marketplace or
    hub of some kind will be created in the future, like OpenAI’s plugins, that can
    be used with any LLM model. LLMs are still a new technology, and the ecosystems
    to be built around them are still forthcoming; we believe this will be one of
    them.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以始终构建这些工具，但如果它们已经为你准备好了，你只需去某个中心，下载你想要的工具，然后接入，那岂不是更好？不幸的是，这个选项目前还不存在，至少不是我们在这里描述的程度。我们期待未来会创建某种市场或中心，就像OpenAI的插件一样，可以与任何LLM模型一起使用。LLM仍然是一项新技术，围绕它们构建的生态系统还在发展中；我们相信这将是其中之一。
- en: '^a T. Schick et al., “Toolformer: Language models can teach themselves to use
    tools,” February 2023\.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ^a T. Schick等人，“Toolformer：语言模型可以教会自己使用工具”，2023年2月。
- en: Once we give our LLMs access to tools, it opens the gates to lots of cool prompt
    engineering techniques. Probably the most famous is the ReAct method.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们给我们的LLM提供工具访问权限，它就打开了众多酷炫提示工程技术的门户。可能最著名的就是ReAct方法。
- en: 7.4.2 ReAct
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 ReAct
- en: Reasoning and Acting (ReAct) is a few-shot framework for prompting that is meant
    to emulate how people reason and make decisions when learning new tasks.[⁹](#footnote-111)
    It involves a multistep process for the LLM, where a question is asked, the model
    determines an action, and then it observes and reasons upon the results of that
    action to determine subsequent actions.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 推理与行动（ReAct）是一个旨在模拟人们在学习新任务时推理和做决策的几步提示框架。[⁹](#footnote-111) 它涉及LLM的多步骤过程，其中提出一个问题，模型确定一个动作，然后观察并推理该动作的结果以确定后续动作。
- en: 'An example could look like this:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子可能看起来像这样：
- en: CB **Question:** What is the airspeed velocity of an unladen African swallow
    compared to a European swallow with the same load?
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: CB **问题：** 与同负载的欧洲燕子相比，未负载的非洲燕子的空速是多少？
- en: '![chatGpt](../Images/chatGpt.png) **Thought 1:** I need to search for airspeed
    velocity of a European swallow so I can compare it with an African swallow.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '![chatGpt](../Images/chatGpt.png) **想法1：** 我需要查找欧洲燕子的空速，以便与非洲燕子进行比较。'
- en: '![chatGpt](../Images/chatGpt.png) **Action 1:** Search[European Swallow airspeed
    velocity]'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![chatGpt](../Images/chatGpt.png) **动作1：** 搜索[欧洲燕子空速]'
- en: '![chatGpt](../Images/chatGpt.png) **Observation 1:** We need to know the Strouhal
    number in order to determine airspeed. The bird''s Strouhal number converges between
    0.2 and 0.4.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![chatGpt](../Images/chatGpt.png) **观察1：** 我们需要知道斯特劳哈数来确定空速。鸟的斯特劳哈数在0.2到0.4之间收敛。'
- en: '![chatGpt](../Images/chatGpt.png) **Thought 2:** The airspeed velocity is not
    directly mentioned, but the Strouhal number is clear. So I need to look up velocity
    compared to the Strouhal number.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '![chatGpt](../Images/chatGpt.png) **想法2：** 空速没有直接提及，但斯特劳哈数是明确的。所以我需要查找速度与斯特劳哈数的比较。'
- en: '![chatGpt](../Images/chatGpt.png) **Action 2:** Search[Strouhal wingbeat calculation]'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '![chatGpt](../Images/chatGpt.png) **动作2：** 搜索[斯特劳哈拍打翼计算]'
- en: '![chatGpt](../Images/chatGpt.png) **Observation 2:** (Result 1/1) A Swallow’s
    Strouhal number translates to ~12 wingbeats per second.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![chatGpt](../Images/chatGpt.png) **观察2：** (结果1/1) 一只燕子的斯特劳哈数相当于每秒约12次拍打翅膀。'
- en: '![chatGpt](../Images/chatGpt.png) **Thought 3:** An African swallow requires
    12 wingbeats per second to maintain airspeed velocity, so I need to look up how
    fast that makes it.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![chatGpt](../Images/chatGpt.png) **想法3：** 非洲燕子需要每秒12次拍打翅膀来维持空速，所以我需要查找这会使它飞得多快。'
- en: '![chatGpt](../Images/chatGpt.png) **Action 3:** Lookup[African Swallow wingbeat
    per second]'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '![chatGpt](../Images/chatGpt.png) **行动3**：查找[非洲燕子每秒振翅次数]'
- en: '![chatGpt](../Images/chatGpt.png) **Observation 3:** Each wingbeat propels
    an African swallow 2.5 feet per flap, making it go about 40.2mph'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![chatGpt](../Images/chatGpt.png) **观察3**：每次振翅推动非洲燕子2.5英尺，使其速度约为40.2英里/小时'
- en: '![chatGpt](../Images/chatGpt.png) **Thought 4:** I need to look up the European
    swallow to compare it.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '![chatGpt](../Images/chatGpt.png) **思考4**：我需要查找欧洲燕子的信息来比较。'
- en: '![chatGpt](../Images/chatGpt.png) **Action 4:** Search[European Swallow Wingbeat
    per second]'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![chatGpt](../Images/chatGpt.png) **行动4**：搜索[欧洲燕子每秒振翅次数]'
- en: '![chatGpt](../Images/chatGpt.png) **Observation 4:** A European swallow is
    about half the size of an African swallow, making its velocity about 20.1'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![chatGpt](../Images/chatGpt.png) **观察4**：欧洲燕子大约是非洲燕子大小的一半，其速度约为20.1'
- en: '![chatGpt](../Images/chatGpt.png) **Thought 5:** An African swallow has an
    airspeed velocity of 40.2, and a European swallow has an airspeed velocity of
    20.1, making the comparison 2x.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '![chatGpt](../Images/chatGpt.png) **思考5**：非洲燕子的空速速度为40.2，而欧洲燕子的空速速度为20.1，这使得比较是2倍。'
- en: '![chatGpt](../Images/chatGpt.png) **Action 5:** Finish[Two times the airspeed
    velocity]'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '![chatGpt](../Images/chatGpt.png) **行动5**：完成[两倍于空速速度]'
- en: As you can see, the purpose of ReAct is to force the model to think before it
    acts. This isn’t much different from the other prompting methods we have discussed.
    The big difference is that we allow the model to take actions. In our example,
    this included a “Search” action, or essentially an ability to look up information
    on the internet as a human would. We just showed you how to do this in the last
    section. The model can take that new information and observe what it learns from
    its actions to produce a result.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，ReAct的目的在于迫使模型在行动之前进行思考。这与我们之前讨论的其他提示方法并没有太大的不同。最大的区别在于我们允许模型采取行动。在我们的例子中，这包括一个“搜索”行动，或者说本质上是一种像人类一样在互联网上查找信息的能力。我们已经在上一节中展示了如何做到这一点。模型可以接受新信息，并观察从其行动中学到的东西以产生结果。
- en: Let’s explore this further with an example. We will use LangChain, which will
    make creating a ReAct agent seem a lot easier than it actually is. Listing 7.5
    shows how to utilize ReAct on an OpenAI model and LangChain. For our search engine,
    we will be utilizing serper.dev, as it integrates nicely with LangChain, and it
    offers a free tier you can sign up for. We will also need to use the calculator
    `"llm-math"`, which is one of the many tools in LangChain’s toolbelt.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子进一步探讨这个问题。我们将使用LangChain，这将使创建ReAct代理比实际要容易得多。列表7.5展示了如何在OpenAI模型和LangChain上利用ReAct。对于我们的搜索引擎，我们将使用serper.dev，因为它与LangChain很好地集成，并提供一个你可以注册的免费层。我们还需要使用计算器`"llm-math"`，这是LangChain工具包中的许多工具之一。
- en: Listing 7.5 Example ReAct with Langchain
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.5 示例：ReAct与Langchain
- en: '[PRE6]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Loads API keys; you will need to obtain these if you haven''t yet'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 加载API密钥；如果你还没有获取这些，你需要获取'
- en: The output is
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果
- en: '[PRE7]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Listing 7.5 shows how ReAct can be used with an LLM in conjunction with particular
    agent tools like `"google-serper"` and `"llm-math"` to help augment your prompts.
    Prompt engineering looks more like a full-time job now, not just “coming up with
    words,” huh?
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.5展示了如何结合特定的代理工具如`"google-serper"`和`"llm-math"`使用ReAct与LLM来帮助增强你的提示。现在提示工程看起来更像是一项全职工作，而不仅仅是“想出词语”，对吧？
- en: Knowing how to build tools and combine them to prompt LLMs to answer more in-depth
    questions is a growing field of study as well as an expanding part of the job
    market. To be perfectly honest, the rate of change in the prompt engineering field
    seems to drastically outpace most of the other topics we cover in this book. There’s
    a lot more to be discussed that we simply can’t cover in this book, so much so,
    in fact, that there are now entire books in and of themselves being written to
    this end. It was difficult to determine what would be valuable to our readers
    and what would be outdated quickly, but we think we’ve found a good balance and
    encourage you to look forward to researching more on the topic.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 了解如何构建工具并将它们组合起来以提示LLM回答更深入的问题，已经成为一个不断发展的研究领域以及就业市场的扩展部分。坦白说，提示工程领域的变革速度似乎大大超过了本书中讨论的其他大多数主题。还有许多内容需要讨论，但我们无法在本书中涵盖所有内容，事实上，现在已经有专门为此目的而写的整本书。很难确定什么对读者有价值，什么会很快过时，但我们认为我们已经找到了一个良好的平衡，并鼓励你期待在更多关于这个主题的研究。
- en: Overall, we’ve learned a lot throughout this chapter—how to craft a prompt and
    how to implement prompting in an engineering fashion. In the next chapter, we
    will put all of this knowledge to good use when we build LLM applications users
    can interact with.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most straightforward approach to prompting is to give a model examples
    of what you want it to do:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The more examples you can add to a prompt, the more accurate your results will
    be.
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The fewer examples you need to add, the more general and all-purpose your prompt
    will be.
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The four parts of a prompt are
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Input* —What the user writes'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Instruction* —The template with task-specific information encoded'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Context* —The information you add through RAG or other database lookups'
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*System* —The specific instructions given for every task; should be hidden
    from the user'
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowing your training data will help you craft better prompts by choosing a
    word order that matches the training data.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain is a popular tool that allows us to create chains or pipelines to
    utilize LLMs in an engineering fashion.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guidance is a powerful tool that gives us more fine-grained control over the
    LLMs’ actual generated text.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Toolformers teach LLMs how to use tools, giving them the ability to accomplish
    previously impossible tasks.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReAct is a few-shot framework for prompting that is meant to emulate how people
    reason and make decisions when learning new tasks.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#footnote-source-1) J. Wei et al., “Chain of thought prompting elicits
    reasoning in large language models,” January 2022, [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#footnote-source-2) T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa,
    “Large Language models are zero-shot reasoners,” May 2022, [https://arxiv.org/abs/2205.11916](https://arxiv.org/abs/2205.11916).'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#footnote-source-3) Y. Zhou et al., “Thread of thought unraveling chaotic
    contexts,” November 15, 2023, [https://arxiv.org/abs/2311.08734](https://arxiv.org/abs/2311.08734).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#footnote-source-4) Sondos Mahmoud Bsharat, Aidar Myrzakhan, and Z. Shen,
    “Principled instructions are all you need for questioning LLaMA-1/2, GPT-3.5/4,”
    December 2023, [https://doi.org/10.48550/arxiv.2312.16171](https://doi.org/10.48550/arxiv.2312.16171).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#footnote-source-5) C. Yang et al., “Large language models as optimizers,”
    September 6, 2023, [https://arxiv.org/abs/2309.03409](https://arxiv.org/abs/2309.03409).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#footnote-source-6) N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein,
    and K. Aberman, “DreamBooth: Fine tuning text-to-image diffusion models for subject-driven
    generation,” August 2022, [https://arxiv.org/abs/2208.12242](https://arxiv.org/abs/2208.12242)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#footnote-source-7) Ruiz et al., “DreamBooth.”'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#footnote-source-8) T. Schick et al., “Toolformer: Language models can
    teach themselves to use tools,” February 2023, [https://arxiv.org/abs/2302.04761](https://arxiv.org/abs/2302.04761).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#footnote-source-9) S. Yao et al., “ReAct: Synergizing reasoning and
    acting in language models,” March 10, 2023, [https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[[9]](#footnote-source-9) S. Yao等人，“ReAct: 在语言模型中协同推理和行动，”2023年3月10日，[https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629).'
