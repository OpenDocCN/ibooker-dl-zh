<html><head></head><body>
  <h1 class="tochead" id="heading_id_2"><a id="idTextAnchor011"/><a id="idTextAnchor012"/>Appendix B. K-nearest neighbors and support vector machines</h1>

  <p class="body">In this appendix, we examine classical machine learning algorithms with a more computational nature that we didn’t treat in the book because they are less frequently used nowadays and are considered outdated compared to decision tree ensembles in most applications. Overall, support vector machines (SVMs) are still a practical machine learning algorithm well suited for high-dimensional, noisy, or small-sized data applications. On the other end, k-nearest neighbors (k-NN) is well suited for running applications where the data has few features, there can be outliers, and it is not necessary to get a high degree of accuracy in predictions. For instance, SVMs can still be used to classify medical images, such as mammograms and X-rays; for vehicle detection and tracking in the automotive industry; or to detect email spam. Instead, k-NN is mainly applied in recommender systems, particularly collaborative filtering approaches, to recommend products or services based on users’ past behavior.</p>

  <p class="body"><a id="marker-463"/>They are suited in most tabular data situations when your data is not too small or exceedingly big<a id="idTextAnchor000"/>—as a rule of thumb, where there are fewer than 10,000 rows. We will start with k-NN, an algorithm that data scientists have used for decades in machine learning problems and that is easy to understand and implement. Then we will complete our overview with SVMs and a brief excursus on using GPUs to have these algorithms perform when using a moderately sized dataset. All the examples need the Airbnb NYC Dataset presented in chapter 4. You can reprise it by executing the following code snippet:</p>
  <pre class="programlisting">import numpy as np
import pandas as pd
excluding_list = [
    'price', 'id', 'latitude', 'longitude', 
    'host_id', 'last_review', 'name', 'host_name'
]                                                <span class="fm-combinumeral">①</span>
categorical = [
    'neighbourhood_group', 'neighbourhood', 
    'room_type'
]                                                <span class="fm-combinumeral">②</span>
continuous = [
    'minimum_nights', 'number_of_reviews', 'reviews_per_month', 
    'Calculated_host_listings_count'
]                                                <span class="fm-combinumeral">③</span>
data = pd.read_csv("./AB_NYC_2019.csv")
target_median = (
    data["price"] &gt; data["price"].median()
).astype(int)                                    <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> List of column names to be excluded from the analysis</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> List of names of columns that likely represent categorical variables in the dataset</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> List of names of columns that represent continuous numerical variables in the dataset</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> A binary balanced target</p>

  <p class="body">The code will load your dataset and define what features to be excluded from the analysis or considered as continuous or categorical for processing purposes.</p>

  <h2 class="fm-head" id="heading_id_3">B.1 k-NN<a id="idTextAnchor001"/></h2>

  <p class="body"><a id="marker-464"/>Applicable to regression and classification tasks, the k-NN algorithm is considered one of the simplest and most intuitive algorithms for making predictions. It finds the k (where k is an integer number) closest examples from the training set and uses their information to make a prediction. For example, if the task is a regression, it will take the average of the k closest examples. If the task is a classification, it will choose the most common class among the k closest examples.<a id="idIndexMarker000"/></p>

  <p class="body">Technically, k-NN is commonly regarded as an instance-based learning algorithm because it memorizes the training examples as they are. It is also regarded as a “lazy algorithm” because, contrary to most machine learning algorithms, there is little processing at training time. During training, there is usually some processing of the distances by optimized algorithms and data structures that render it less computationally costly afterward to look for the neighboring points near a training example. Most of the computational work is done at testing time (see figure B.1).</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/APPB_F01_Ryan2.png"/></p>

    <p class="figurecaption">Figure B.1 Classifying new samples (the triangles) with a k-NN where k = 3</p>
  </div>

  <p class="body">We apply a k-NN classifier to the Airbnb NYC data, as seen in chapter 4, in listing B.1. Since k-NN works based on distances, to obtain a functioning solution, features must be on the same scale, thus assuring an equal weight to each dimension in the distance measurement process. If a feature is on a different or smaller scale, it would be overweighted in the process. The contrary would happen if a larger scale characterizes a feature. To give an idea of the problem, let’s consider what happens when we compare distances based on kilometers, meters, and centimeters. Even if distances are comparable, meters and centimeters will numerically exceed kilometers measurements. This problem is usually solved by scaling features—for instance, by subtracting their mean and dividing by their standard deviation (an operation known as z-score normalization</p>

  <p class="body">or standardization). Also, techniques such as dimensional reduction or feature selection are helpful with this algorithm because rearranging predictors or different sets of predictors may result in more or less predictive performances on the problem.</p>

  <p class="body">In our case, as it is often with tabular data, the situation is complicated by categorical features, which, once one-hot encoded, will turn into binaries ranging from 0 to 1, with a different scale from normalized features. The solution we propose is first to discretize the numeric features, thus effectively turning them into binary features, each representing if a feature’s numeric values will fall into a specific range. Binarization of continuous features is obtained thanks to the KBinsDiscretizer class (<a id="idTextAnchor002"/><a class="url" href="https://mng.bz/N12N">https://mng.bz/N12N</a>) embedded into the <code class="fm-code-in-text">numeric_discretizing</code> pipeline, which will turn each numeric feature into five binary ones, each one covering a bin of values. At processing time, we also apply principal component analysis (PCA) to reduce the dimensionality and make all the features unrelated. However, we might attenuate nonlinearities inside the data since PCA is a technique based on linear combinations of the variables. Having uncorrelated resulting features is a characteristic of data processed by PCA, which suits k-NN: k-NN is based on distances, and distance measurement properly works if dimensions are unrelated. Therefore, any distance change is due to changes in a single dimension, not multiple ones. The following listing shows the code implementing the data transformation process and training the k-NN.<a id="idIndexMarker001"/><a id="idTextAnchor003"/><a id="marker-465"/><a id="idIndexMarker002"/></p>

  <p class="fm-code-listing-caption">Listing B.1 k-NN classifier</p>
  <pre class="programlisting">from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import KBinsDiscretizer
from sklearn.metrics import accuracy_score

categorical_onehot_encoding = OneHotEncoder(handle_unknown='ignore')

accuracy = make_scorer(accuracy_score)                            <span class="fm-combinumeral">①</span>
cv = KFold(5, shuffle=True, random_state=0)                       <span class="fm-combinumeral">②</span>
model = KNeighborsClassifier(n_neighbors=30,
                             weights="uniform",
                             algorithm="auto",
                             n_jobs=-1)                           <span class="fm-combinumeral">③</span>
 
column_transform = ColumnTransformer(
    [('categories', categorical_onehot_encoding, low_card_categorical),
     ('numeric', numeric_discretizing, continuous)],
    remainder='drop',
    verbose_feature_names_out=False,
    sparse_threshold=0.0)                                         <span class="fm-combinumeral">④</span>

model_pipeline = Pipeline(
    [('processing', column_transform),
     ('pca', PCA(n_components="mle")),
     ('modeling', model)])                                        <span class="fm-combinumeral">⑤</span>

cv_scores = cross_validate(estimator=model_pipeline,
                           X=data,
                           y=target_median,
                           scoring=accuracy,
                           cv=cv,
                           return_train_score=True,
                           return_estimator=True)                 <span class="fm-combinumeral">⑥</span>

mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})",
      f"fit: {fit_time:0.2f}",
      f"secs pred: {score_time:0.2f} secs")                       <span class="fm-combinumeral">⑦</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a scoring function using the accuracy_score metric</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a five-fold crossvalidation iterator with shuffling and a fixed random state</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates an instance of the KNeighborsClassifier with specified hyperparameters</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Defines a ColumnTransformer to preprocess features, applying one-hot encoding to categorical features with low cardinality and discretization to numerical features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Creates a pipeline that sequentially applies the column transformation, performs PCA dimensionality reduction, and then fits the k-nn model to the data</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Performs cross-validation on the data using the defined pipeline, with accuracy scoring</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Prints the mean and standard deviation of test scores</p>

  <p class="body"><a id="marker-466"/>After running the script, you will obtain a result that is close to the performance of the Naive Bayes solution:</p>
  <pre class="programlisting">0.814 (0.005) fit: 0.13 secs pred: 8.75 secs</pre>

  <p class="body">The performance is good, though the inference time is relatively high. Since this algorithm works by analogy (it will look for similar cases in your training to get an idea of the possible prediction), it performs better with large enough datasets where there is a higher likelihood of finding some instances resembling those to be predicted. Naturally, the right size for the dataset is dictated by the number of features used because the more features, the more cases you will need for the algorithm to generalize well.</p>

  <p class="body">Though often the emphasis is placed on setting the best value to the k parameter as the key to balancing the underfitting and overfitting of the algorithm to the training data, we instead raise attention to other aspects for an effective employ of this model. As the algorithm works by analogy and distances in complex spaces, we consider two important matters about this approach:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">The dimensions to measure and the curse of dimensionality</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The appropriate distance measure and how to process the features</p>
    </li>
  </ul>

  <p class="body">In k-NN, the classification or the regression estimates depend on the most similar examples based on a distance metric computed on the features. However, in a dataset, not all features can be deemed important in judging an example similar to the other, and not all of the features can be compared in the same way. Prior knowledge of the problem does count a lot when using k-NN because you have to select only the features relevant to the task you want to solve. If you assemble too many features for the problem, you will rely on too much complex space to navigate. Figure B.1 shows how a k-NN algorithm works with just two features (represented on the x and y dimensions), and you can intuitively grasp that classifying new instances (the triangles in the figure) may be difficult if there are mixed classes in an area or if there are no train examples near to a new instance. You have to rely on farther ones.</p>

  <p class="body"><a id="marker-467"/>Here comes into the game the curse of dimensionality, which says that as the number of features increases, the more examples you should have available to maintain a meaningful distance between data points. In addition, the curse implies that the number of necessary examples grows exponentially with respect to the number of features. For a k-NN algorithm, it means that if you provide too many features, it will work in an empty space if the number of examples is not enough. Looking for neighbors will become daunting. In addition, if you have just assembled relevant and irrelevant features, the risk is that the algorithm will mark as neighbors some examples that are very far from the case you have to predict, and the choice could be based on features that are not useful for the problem. Hence, if you are going to use k-NN, you should choose with great care the features to be used (if you don’t know which to use, you need to rely on feature selection) or be very familiar with the problem to determine what should go into the algorithm. Parsimony is essential for the proper working of a k-NN.</p>

  <p class="body">When you have decided about the features, regarding the distance metric you will be using, you will need to standardize, remove redundancies, and transform the features. This is because distance metrics are based on absolute measurements, and different scales can weigh in different ways. Consider using measurements in kilometers, meters, and centimeters together. The centimeters will likely predominate because they will easily have the largest numbers. Also, having features similar to each other (the problem of multicollinearity) can entail the distance measurement to overweight certain sets of features over others. Finally, a distance measurement implies having the same dimensions to measure. However, in a dataset, you may find different kinds of data— numeric, categorical, and time-based—which often need to fit together better in a distance calculation because they have different numeric characteristics.</p>

  <p class="body">For this reason, in addition to carefully selecting beforehand which features to use, when employing k-NN, we suggest using features that are all of the same kind (or all numeric or all categorical) to standardize them if necessary and also to reduce their informative redundancy by methods such as PCA (<a class="url" href="https://mng.bz/8OrZ">https://mng.bz/8OrZ</a>), which will reformulate the dataset into a new one where features are not correlated between themselves.</p>

  <h2 class="fm-head" id="heading_id_4">B.2 SVMs</h2>

  <p class="body"><a id="marker-468"/>Before the 2010s, SVMs had a reputation as the most promising algorithm for tabular problems. However, in the past 10 years, tree-based models have eclipsed SVMs as the go-to approach for tabular data. However, SVMs remain a family of techniques for handling binary, multiclass, regression, and anomaly/novelty detection. They are based on the idea that if your observations can be represented as points in a multidimensional space, there is a hyperplane (i.e., a separation plane cutting through multiple dimensions) that can separate them into classes or values that, by assuring the largest separation possible between them, also guarantees the most robust and reliable predictions. Figure B.2 shows a simple example of an SVM applied to a binary classification problem with two features, represented on the x- and y-axis, as predictors. The SVM model produces a separator line with the largest slack space between the two groups, as shown in the figure, where the dashed lines delimit the slack space. In doing so, it considers only a few points near the separator, called the support vectors. Instead, it ignores the points that are near but are confusing for the algorithm because, for instance, they are on the wrong side. It also ignores the points that are far away from the separator line. Outliers have little influence on this algorithm. <a id="idIndexMarker004"/><a id="idTextAnchor006"/><a id="idIndexMarker005"/><a id="idIndexMarker006"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/APPB_F02_Ryan2.png"/></p>

    <p class="figurecaption">Figure B.2 A separating hyperplane from a SVM</p>
  </div>

  <p class="body">The strong points of SVMs are their robust handling of overfitting, noise in data, and outliers and how they can successfully handle datasets that include numerous multicollinear features. Applying different nonlinear approaches to data, SVMs don’t require the transformations (such as polynomial expansion) we have seen for logistic regression. However, they can use domain-based feature engineering like all other machine learning algorithms.</p>

  <p class="body">On the weak side, SVM optimization is complex and can be feasible only for a limited number of examples. Moreover, they are best fit for binary predictions and for just class prediction; they are not a probabilistic algorithm, and you need to wrap them with another algorithm for calibration (such as logistic regression) to extract probabilities from them. That renders SVMs valid only for a limited range of tasks in risk estimation.</p>

  <p class="body">In our example, we reapply our problem with Airbnb NYC data using a binary classification SVM using a radial basis function kernel, an approach capable of automatically modeling complex nonlinear relationships between the provided features.<a id="marker-469"/><a id="idTextAnchor007"/></p>

  <p class="fm-code-listing-caption">Listing B.2 Support vector classifier</p>
  <pre class="programlisting">from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

numeric_standardization = Pipeline([
       ("imputation", SimpleImputer(strategy="constant", fill_value=0)),
       ("standardizing", StandardScaler())
       ])

accuracy = make_scorer(accuracy_score)                            <span class="fm-combinumeral">①</span>
cv = KFold(5, shuffle=True, random_state=0)                       <span class="fm-combinumeral">②</span>
model = SVC(
    C=1.0,
    kernel='rbf',
    gamma='scale',
    probability=False
)                                                                 <span class="fm-combinumeral">③</span>

column_transform = ColumnTransformer(
    [('categories', categorical_onehot_encoding, low_card_categorical),
     ('numeric', numeric_standardization, continuous)],
    remainder='drop',
    verbose_feature_names_out=False,
    sparse_threshold=0.0)                                         <span class="fm-combinumeral">④</span>

model_pipeline = Pipeline(
    [('processing', column_transform),
     ('modeling', model)])                                        <span class="fm-combinumeral">⑤</span>

cv_scores = cross_validate(estimator=model_pipeline,
                           X=data,
                           y=target_median,
                           scoring=accuracy,
                           cv=cv,
                           return_train_score=True,
                           return_estimator=True)                 <span class="fm-combinumeral">⑥</span>

mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})",
      f"fit: {fit_time:0.2f}",
      f"secs pred: {score_time:0.2f} secs")                       <span class="fm-combinumeral">⑦</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a scoring function using the accuracy_score metric</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a five-fold crossvalidation iterator with shuffling and a fixed random state</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates an instance of the Support Vector Classifier with specified hyperparameters</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Defines a ColumnTransformer to preprocess features, applying one-hot encoding to categorical features with low cardinality and standardization to numerical features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Creates a pipeline that sequentially applies the column transformation</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Performs cross-validation on the data using the defined pipeline, with accuracy scoring</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Prints the mean and standard deviation of test scores</p>

  <p class="body">The results are pretty interesting, and they can probably even turn better by adjusting the hyperparameters:</p>
  <pre class="programlisting">0.821 (0.004) fit: 102.28 secs pred: 9.80 secs</pre>

  <p class="body">However, the time needed for training a single fold is excessive compared to all the previous machine learning algorithms. In the next section of this appendix, we will discuss how GPU cards can speed up the process while still using the Scikit-learn API.</p>

  <h2 class="fm-head" id="heading_id_5">B.3 Using GPUs for machine learning</h2>

  <p class="body"><a id="marker-470"/>Due to the rapid rise of deep learning in the data science field, GPUs are now widespread and accessible both for local and cloud computing. Earlier, you only heard about GPUs in 3D gaming, graphic processing rendering, and animation. Since they are cheap and apt at fast matrix multiplication tasks, academics and practitioners have rapidly picked up GPUs for neural network computations. RAPIDS, developed by NVIDIA (one of the top manufacturers of GPUs), is a set of packages for doing the full spectrum of data science, not just deep learning, on GPUs. The RAPIDS packages promise to help in all phases of a machine learning pipeline, end to end. That’s a game changer for many classical machine learning algorithms, especially for the SVMs, the most credible choice for more complex tasks involving noisy, outlying observations and vast datasets (having a large set of features, especially if multicollinear or sparse ones). In the RAPIDS packages (table B.1), all commands have adopted existing APIs for their commands. Such assures an immediate market adoption of the packages, and for the user, there is no need to relearn how the wheel<a id="idTextAnchor009"/> works.<a id="idIndexMarker010"/><a id="idIndexMarker011"/><a id="idIndexMarker012"/><a id="idIndexMarker013"/></p>

  <p class="fm-table-caption">Table B.1 Rapids packages</p>

  <table border="1" class="contenttable-1-table" id="table001" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="33.33%"/>
      <col class="contenttable-0-col" span="1" width="33.33%"/>
      <col class="contenttable-0-col" span="1" width="33.33%"/>
    </colgroup>

    <thead class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <th class="contenttable-1-th">
          <p class="fm-table-head">Rapids package</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Task</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">API mimicked</p>
        </th>
      </tr>
    </thead>

    <tbody class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">cuPy</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Array operations</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">NumPy</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">cuDF</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Data processing</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">pandas</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">cuML</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Machine learning</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Scikit-learn</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">This section will focus on how easy it is to replace your Scikit-learn algorithms with the RAPIDS cuML package. Currently, this package includes implementations for linear models, k-NN, and SVMs, as well as for clustering and dimensionality reduction. The following listing shows the code for testing the support vector classifier with the radial basis function kernel we just tried in the previous section in its RAPIDS implementation (using a P100 GPU).<a id="marker-471"/></p>

  <p class="fm-code-listing-caption"><a id="idTextAnchor010"/>Listing B.3 Support vector classifier from RAPIDS cuML</p>
  <pre class="programlisting">from cuml.svm import SVC
from sklearn.metrics import accuracy_score

accuracy = make_scorer(accuracy_score)                            <span class="fm-combinumeral">①</span>
cv = KFold(5, shuffle=True, random_state=0)                       <span class="fm-combinumeral">②</span>
model = SVC(
    C=1.0,
    kernel='rbf',
    gamma='scale',
    probability=False
)                                                                 <span class="fm-combinumeral">③</span>

column_transform = ColumnTransformer(
    [('categories', categorical_onehot_encoding, low_card_categorical),
     ('numeric', numeric_standardization, continuous)],
    remainder='drop',
    verbose_feature_names_out=False,
    sparse_threshold=0.0)                                         <span class="fm-combinumeral">④</span>

model_pipeline = Pipeline(
    [('processing', column_transform),
     ('modeling', model)])                                        <span class="fm-combinumeral">⑤</span>

cv_scores = cross_validate(estimator=model_pipeline,
                           X=data,
                           y=target_median,
                           scoring=accuracy,
                           cv=cv,
                           return_train_score=True,
                           return_estimator=True)                 <span class="fm-combinumeral">⑥</span>

mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})",
      f"fit: {fit_time:0.2f}",
 
      f"secs pred: {score_time:0.2f} secs")                       <span class="fm-combinumeral">⑦</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a scoring function using the accuracy_score metric</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a five-fold crossvalidation iterator with shuffling and a fixed random state</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates an instance of a Support Vector Classifier from the GPU-accelerated cuML library with specified hyperparameters</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Defines a ColumnTransformer to preprocess features, applying one-hot encoding to categorical features with low cardinality and standardization to numerical features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Creates a pipeline that sequentially applies the column transformation and the model to the data</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Performs cross-validation on the data using the defined pipeline, with accuracy scoring</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Prints the mean and standard deviation of test scores</p>

  <p class="body">The results we obtained are</p>
  <pre class="programlisting">0.821 (0.004) fit: 4.09 secs pred: 0.11 secs</pre>

  <p class="body">As you can see, we obtained the same results by reusing the same code but relying on cuML. However, the processing time has dropped from 102 seconds per folder to 4 seconds per folder. If you calculate the time savings, that’s a 25x speed increase. The exact performance benefit depends on the GPU model you use; the more powerful the GPU, the speedier the results because it depends on how fast the GPU card can transfer data from CPU memory and how fast it can process a matrix multiplication.</p>

  <p class="body">Based on such performances on standard GPUs accessible to the general public, we recently saw applications fusing tabular data with large embeddings from deep learning models (such as text or images). SVMs work well with numerous features (but not more than the examples) and sparse values (many zero values). In such situations, SVMs can easily obtain a state-of-the-art result, outperforming other more popular tabular algorithms at this time—namely XGBoost and other gradient boosting implementations as well as end-to-end deep learning solutions, which are weaker when you don’t have enough cases to feed them with.</p>

  <p class="body"><a id="marker-472"/>Having a GPU and adapting your code to use RAPIDS algorithms makes certain classic algorithms for tabular machine learning quite competitive again, as a general rule, based on the principle that there is no free lunch in machine learning (more details about no-free-lunch theorems are available at <a class="url" href="http://www.no-free-lunch.org/">http://www.no-free-lunch.org/</a>). Taking into account your project constraints (for instance, you may not have certain resources available in your project environment), never exclude apriori testing your problem against all available algorithms, if this is feasible.</p>
</body></html>