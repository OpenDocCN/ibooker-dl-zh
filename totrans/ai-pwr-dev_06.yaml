- en: 5 Managing data with GitHub Copilot and Copilot Chat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Persisting data into a relational database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming data using Apache Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporating event-driven principles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing data to monitor the location using Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The last chapter laid the foundation for our information technology asset management
    (ITAM) system. However, this application will not fulfill our requirements without
    data. Data is the lifeblood of every application. That is what this chapter is
    all about: the various ways we can use generative AI to create data, stream data,
    transform data, react to data, and learn from data.'
  prefs: []
  type: TYPE_NORMAL
- en: Perceptive individuals may have noticed in the last chapter that our data access
    pattern would not have worked as it was incomplete. The opening section of this
    chapter will address this. After that, we will set up our database, fix the classes
    that access this data, and load some sample data to use in the rest of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Amassing our dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our first task will be to construct a substantial corpus of data to assist
    our experimentation in the remainder of the chapter. First we will use GitHub
    Copilot to generate 1,000 rows of asset information. We will soon find, however,
    that this may not be the tool best suited to this task. A key driver behind using
    these tools is the idea of discovery: testing their boundaries, pushing against
    them, and occasionally pushing back. But the journey is often where the joy is
    found. Once we have found this edge, we will be introduced to a new, previously
    unseen tool: GitHub Copilot Chat. Finally, when we have created our list of assets,
    we will add location information for those assets, again using GitHub Copilot
    Chat.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to get our database running before building our initial dataset. Docker
    makes this task trivial, allowing us to quickly spin up an empty PostgreSQL (or
    other RDBMS/NoSQL server) with minimal effort. Have you forgotten the command
    to do this? No worries—we can ask Copilot. Open a new file called data/initial_data_load.sql
    and enter the following prompt at the top of your newly minted SQL file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Copilot will slowly reveal the Docker command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once you run this command at your terminal or command line, we can build out
    our dataset. You should be able to connect to the locally running database. Notice
    that a database called `itam_db` is running in it. However, this database has
    no schema, tables, or data. Let’s first set up a new schema.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our initial_data_load.sql, we will add a prompt to have Copilot draft the
    schema creation command. The following prompt (and response from Copilot) will
    allow you to create a new schema called `itam` if executed from within your database
    client application (e.g., DataGrip, SQuirreL, pdAdmin, or even using the Docker
    `exec` command `docker exec -i itam_db psql -U postgres -c "create schema itam"`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, we should add a user for use in our application. This user will be able
    to perform CRUD (create, read, update, delete) operations on our data but will
    not be able to affect the structure of the database tables or procedures.
  prefs: []
  type: TYPE_NORMAL
- en: Note The lines that start with double dashes (`--`) are comments in SQL. Commenting
    out these lines is optional from Copilot’s perspective, as it will generate solutions
    without the comments; it makes it easier to copy and paste the code directly into
    our database tool of choice.
  prefs: []
  type: TYPE_NORMAL
- en: While we are at it, we will also add an administrative account to perform the
    operations that our read-write users cannot, such as creating or dropping tables.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1 Prompt to create new users
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we will grant ownership of this schema to the `itam_admin` account. Transferring
    this ownership will ensure that only this account can change the table structure
    (the data definition):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With the setup, account creation, and ownership of the system out of the way,
    we can start to focus on the data. We will begin by adding the reference data,
    which supports the assets: the depreciation strategies. This data is more static
    in nature; it changes less frequently, if at all. Let’s define and store these
    strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.2 Prompt to create the `depreciation_strategy` table
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We will use a sequence as this table’s primary key. Although this is not strictly
    necessary for a table that will not be very large and that has known values we
    can and will enter manually, adding this sequence will allow us to work with Copilot
    more and have it make some suggestions. Moreover, it is amusing to ask Copilot
    questions and have Copilot answer in a text file.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.3 Prompt to create a sequence to use as primary key
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Naturally, with the sequence in our proverbial hand, we need to know how to
    associate the sequence with the primary key column of the `depreciation_strategy`
    table. Luckily, Copilot has the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.4 Asking Copilot how to associate the sequence with the primary key
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we complete this table by inserting the following static entries into
    it. We will only use two depreciation strategies for now: straight-line and double
    declining balance.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.5 Adding the static entries to the `depreciation_strategy` table
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Next we will move on to the `funding_details` table. This information tells
    us how we financed our equipment, the resale value, and instructions for what
    should be done with an asset once its useful life is over. The sequence of steps
    will be identical to what we did for the depreciation strategies, with the exception
    that we will not add static entries, as this data is directly related to an individual
    asset. We will define the table, create the sequence, and apply that sequence
    to the table, for which it functions as the primary key.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.6 Complete code listing for the `funding_details` table
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The final information that we will define and generate is the assets themselves.
    This listing, too, is redundant but included for completeness. Finally, we create
    the table, make the sequence, and use it as the primary key.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.7 Complete code listing for the `assets` table
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: With the tables defined and created, we will now focus on creating the data.
    In our text file, we instruct Copilot with parameters for the dataset we are looking
    for. Copilot will likely attempt to assist you in outlining the attributes surrounding
    your new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.8 Creating a dataset for the assets table
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The solution that Copilot provides is novel. It builds a large series using
    a Postgres built-in function, meaning this solution would not be portable. However,
    given that this is the database we will use, it is an appropriate enough solution.
    The resulting dataset is refined. We would have gotten better results if we had
    used Python and asked for Copilot’s assistance in coding a script to generate
    a file to load into Postgres. However, given that this dataset is only for playing
    with the application, we do not need to be overly concerned with the data quality
    for now—although in the real world, data quality is everything.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 5.9 Copilot’s response: an `insert` statement built off of a series'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If we switch back to ChatGPT for a minute, we can get a second opinion about
    how to create such a dataset. ChatGPT suggests the Python library `faker`. The
    `faker` package is used to generate fake data, such as common English first names.
    `numpy` is used to generate the random float values for cost, useful life, and
    salvage value. `pandas` is used to manage the data in a `DataFrame` (the table).
    Additionally, we can save the `DataFrame` to a CSV file using the method `df.to_csv('assets.csv',
    index=False)`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.10 ChatGPT suggests `Faker` to generate the fake dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'For each of these assets, we will need funding details as well: how they were
    financed (purchased, in this case) and the depreciation details. Unsurprisingly,
    we get a similar solution from Copilot: generate a series of entries using a prompt
    similar to the one we used for the assets. We will need to ensure that for each
    of the asset identifiers (1–1000), we have a corresponding funding details entry.
    Otherwise we risk getting null pointers when running this code.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.11 Creating a dataset for the `funding_details` table
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: With the dataset generated and stored in the database, we should be able to
    wire up the remainder of our application to store and display assets using the
    REST APIs. However, because we previously stripped out all the metadata for SQLAlchemy
    during our build phase (see chapter 4), we need a way to wire this metadata with
    our adapters differently.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point we have reached the edge of Copilot’s capabilities. We are perplexed
    by what comes next and how we can solve our most recent dilemma. Tempting as it
    is, we cannot give up and go home. Therefore, it is time to introduce the most
    recent addition to the Copilot product suite: Copilot Chat. Copilot Chat is a
    GPT-4 model embedded in an IDE (currently supported only by Visual Studio Code).
    We will open the chat dialog and ask how to keep our business model clean while
    still using SQLAlchemy’s object-relational model (ORM) features. Figure 5.1 shows
    the response from ChatGPT.'
  prefs: []
  type: TYPE_NORMAL
- en: Copilot Chat suggests that we create a separate data access layer. This approach
    maps nicely onto the ports and adapters approach we have used thus far. In addition,
    Copilot Chat recommends modeling these classes similarly to the domain classes
    but including the metadata required for ORM functionality to work correctly. The
    resulting code is shown in listing 5.12.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F01_Crocker2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 GitHub Copilot Chat’s solution for how to solve our most recent quandary
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.12 ORM support outside of the domain classes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now that the external model classes have been created, we must map these ORM
    instances to our domain model before returning them to the system’s core. This
    may seem like over-engineered code for such a simple application, but it gives
    us great flexibility in how our domain model can operate. For example, our model
    can perform complex operations beyond just CRUD. We would be limited to these
    operations if we kept our domain model identity to the model used in the data
    access layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we will use Copilot and Copilot Chat to explore incorporating event-driven
    ideas into our application. Event-driven concepts will allow us to track our IT
    assets in real time: their location, status, and market value, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Monitoring our assets in real time with Kafka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will monitor our assets in real time to motivate our exploration of using
    generative AI in conjunction with event-driven architecture. We will take it as
    a given that some system external to the ISAM system fires events as our assets
    move from one location to another.
  prefs: []
  type: TYPE_NORMAL
- en: To delve into ITAM events, we will need to configure a few additional services.
    In this case, we will use Apache Kafka. Apache Kafka is a distributed streaming
    platform that is used to build real-time data pipelines and streaming apps. It’s
    designed to handle data streams from multiple sources and deliver them to multiple
    consumers, effectively acting as a middleman for our real-time data.
  prefs: []
  type: TYPE_NORMAL
- en: To start, we will ask Copilot Chat how to run Kafka locally using Docker. Apache
    Kafka has an undeserved reputation for being difficult to install and configure,
    and running in Docker will allow us to side-step this controversy. Using Copilot
    Chat, we can produce a Docker Compose file. However, as is often the case, the
    versions are very old, to the point of not supporting some hardware. Listing 5.13
    is an updated listing from Confluent’s (the company that offers commercial support
    for Kafka) official GitHub repository. Notice that the Docker Compose file’s contents
    include both Kafka and Zookeeper. Zookeeper is a distributed coordination service
    that Kafka uses to manage and coordinate the brokers in the cluster, at least
    for now. Future versions aim to remove dependency on Zookeeper.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.13 Docker Compose file to launch Kafka with Zookeeper
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'With our new Kafka instance running, we now need a consumer that will pull
    the updated locations off the topic `asset_location` (which we will create shortly)
    and update the inventory in the database. Again, we can ask Copilot Chat to provide
    us with a suggestion for how to use Python to subscribe to the topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| In our ITAM project, we would like a Python consumer for the asset_location
    Kafka topic. Once it receives a new updated location, it should update the domain
    model to reflect the new location. How should we do this? |'
  prefs: []
  type: TYPE_TB
- en: The code that Copilot Chat generates creates a consumer, listens to the topic,
    and uses a reference to the `AssetManager` class to update the location of the
    affected asset.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.14 Copilot Chat code to monitor assets’ locations
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'However, should the adapter be aware of `AssetManager`? The `AssetManager`
    does not directly sit inside the domain model; it is a critical intermediary.
    This coupling level between `AssetManager` and the adapter could arguably violate
    the hexagon in the hexagonal architecture model. Let’s have Copilot Chat weigh
    in. We can ask it this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Given that this project uses hexagonal architecture, is it a good idea to
    have an adapter aware of the AssetManager? |'
  prefs: []
  type: TYPE_TB
- en: 'Looking at figure 5.2, it appears that Copilot Chat agrees that this would
    indeed be considered a breach of the contractual responsibility of an adapter.
    We could add a new port to our `AssetManager` class that would use the Kafka port.
    However, let’s see if Copilot Chat has any other suggestions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| We do not want the Kafka consumer to interact directly with the AssetManager.
    Are there any ways to accomplish this? |'
  prefs: []
  type: TYPE_TB
- en: '![](../Images/CH05_F02_Crocker2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 GitHub Copilot Chat’s acceptance that it is, in fact, imperfect
  prefs: []
  type: TYPE_NORMAL
- en: Copilot Chat suggests that we apply the `Mediator` pattern, which resembles
    a port and adapter combination.
  prefs: []
  type: TYPE_NORMAL
- en: Mediator Pattern
  prefs: []
  type: TYPE_NORMAL
- en: The Mediator design pattern is a behavioral pattern that promotes loose coupling
    between objects by encapsulating their interactions in a mediator object. The
    mediator object acts as a central hub that facilitates communication between objects
    without the objects having to know about each other.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first take a look at the `Mediator` class that Copilot Chat generates.
    The class sits between `AssetManager` and the Kafka consumer.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.15 `Mediator` class between `AssetManager` and Kafka consumer
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This implementation closely resembles a message queue or message publisher,
    which is precisely the point: decoupling.'
  prefs: []
  type: TYPE_NORMAL
- en: Note Eagle-eyed readers will likely notice that we are playing a little fast
    and loose with the distinction between an `Asset` and an asset of type `Hardware`.
    In the original domain model, only `Hardware` had a location. Generally, we do
    not think of software as having a location. Of course, you could say that software
    is installed in its location, but it is arguable how convincing this argument
    is. As this project continues, we flatten the domain model for simplicity’s sake,
    as polymorphic structures in a persistence layer are a distractingly complex topic.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a strategy to decouple the Kafka consumer from `AssetManager`,
    we should update the Kafka consumer to take advantage of it. We need to pass the
    mediator into the class in its constructor. This way, `AssetManager` and the consumer
    will have access to the same instance, and messages can freely flow back and forth—
    or rather, in this case, the flow will be unidirectional. You should note that
    we intend to read and write JSON on this topic, so our value deserializer needs
    to understand this.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.16 Incorporating the mediator into the Kafka consumer class
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Next we will examine the changes that the `AssetManager` class requires to incorporate
    the ability to track these locations.
  prefs: []
  type: TYPE_NORMAL
- en: Note To run this project in its entirety, you would need to modify the `AssetManager`,
    `SQLAlchemyAssetRepository`, and `Asset` classes and also create a new table in
    your database called `itam.asset_locations`. The complete and updated source code
    is available on the book’s website ([www.manning.com/books/ai-powered-developer](https://www.manning.com/books/ai-powered-developer))
    and in the book’s GitHub repository ([https://github.com/nathanbcrocker/ai_assisted_dev_public](https://github.com/nathanbcrocker/ai_assisted_dev_public)).
    For now, we will focus on the changes needed to get the events flowing through
    our system and use the repository for reference if the reader so chooses.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 shows the changes required to the `AssetManager` class to begin to
    track the location of our assets in real time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F03_Crocker2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 `AssetManager` requires the addition of another constructor parameter
    and a method to handle the updates to its location objects.
  prefs: []
  type: TYPE_NORMAL
- en: There are two required changes for the `AssetManager` class. First, we need
    to add the `AssetLocationMediator` to the constructor, registering it to handle
    the `AssetLocationUpdated` event. And second, we need to add a method that will
    handle this event. In this case, we call the method `update_asset_location`. The
    abridged code is shown next.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.17 Updated constructor and an event handler for `AssetManager`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `add_location` method of the `Asset` class merely appends the new `Location`
    to the end of a list of `Location`s. More sophisticated domain models may include
    a `current_location` attribute, relegating the rest to a list of historical locations;
    however, given that we are trying to get our events flowing through the system,
    it behooves us to keep things simple.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is only one final item on our to-do list: create the topic. How do we
    do this? That is a good question. Fortunately, all the tools we need are available
    in our running Docker container. So, let’s log in to our Kafka Docker instance.
    We use the following command (assuming that your Docker instance is named `kafka`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing to check is whether any topics are already created. We can
    do that with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This command lists all the existing topics running on this Kafka cluster. As
    you can see, there aren’t any.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the need for a topic, let’s create it. Use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: If you run the `kafka-topics --list` command again, you will see the new topic.
    The partitions and replication-factor instructions we included in the create-topic
    command inform Kafka that we want one partition and a replication factor of 1\.
    If we were setting this up for production or any purpose other than testing, we
    would likely want them to be greater than that to ensure the availability of data.
    Table 5.1 provides you with some of the commonly used Kafka commands that you
    will need for this and other projects.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.1 Summary of Kafka console commands
  prefs: []
  type: TYPE_NORMAL
- en: '| Action | Command |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Create | `kafka-topics --create --bootstrap-server localhost:9092 --replication-factor
    1 --partitions 1 --topic asset_location` |'
  prefs: []
  type: TYPE_TB
- en: '| Read | `kafka-console-consumer --broker-list localhost:9092 --topic asset_location
    –from-beginning` |'
  prefs: []
  type: TYPE_TB
- en: '| Write | `kafka-console-producer --broker-list localhost:9092 --topic asset_location`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Delete | `kafka-topics --delete --topic asset_location --bootstrap-server
    localhost:9092` |'
  prefs: []
  type: TYPE_TB
- en: '| List | `kafka-topics --list --bootstrap-server localhost:9092` |'
  prefs: []
  type: TYPE_TB
- en: 'Now comes the fun part: observing the application in action. Kafka comes with
    a console producer that will allow us to publish messages to Kafka from standard
    input. To do this, launch the console producer with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: You will enter an interactive session allowing you to publish a message with
    every line. Let’s publish a few messages simulating our asset moving around or
    near Chicago.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.18 Entries for the Kafka console producer
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you enter these messages, you should see the output from your application
    indicating that the location has been updated.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a topic
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of completeness, there is one more command you should be aware
    of. You might make a mistake when entering these messages, and an invalid message
    could potentially break your consumer. One possible solution is to delete the
    topic. Deleting a topic may sound dramatic, but it will solve the problem. So
    here is that command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we have added the ability to see changes in the location of
    our `Asset`s in real-time tracking using Apache Kafka. In the final section of
    this chapter, we will work with Copilot Chat to extend the capacity by monitoring
    our assets in real time and attempting to determine if they are where they should
    be. We will explore using Spark and Kafka together to accomplish this analysis.
    Once completed, we will win the thanks of our Information Security team, who fear
    that too much of our core business and intellectual property exists on and in
    these `Asset`s.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Analyzing, learning, and tracking with Apache Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Real-time tracking of assets is a business-critical function. Your IT assets
    contain sensitive business data, client lists, sales figures, profit-and-loss
    (PnL) projections, and sales strategies, among many other items. A lost asset
    can be an existential event for a company. Therefore, careful management and monitoring
    are priority one for many InfoSec professionals. In this section, we aim to make
    their jobs substantially easier. Modern data platforms make it trivial to track
    your assets in real time and send notifications if questionable conditions arise.
    Let’s get into it.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark is a powerful open source data-processing engine built around speed,
    ease of use, and sophisticated analytics. It was developed to provide an improved
    alternative to MapReduce for processing big datasets and can handle batch and
    real-time analytics. Spark provides APIs for Scala, Java, Python, and R and a
    built-in module for SQL queries. Its core data structure, the resilient distributed
    dataset (RDD), enables fault-tolerant operation and allows data to be processed
    in parallel across a cluster of computers.
  prefs: []
  type: TYPE_NORMAL
- en: Spark also includes several libraries to broaden its capabilities, including
    MLlib for machine learning, Spark Streaming for processing live data streams,
    and Spark SQL and DataFrames for processing structured data. These tools make
    it well-suited for tasks ranging from machine learning to real-time data streaming
    and batch processing. Its in-memory processing capabilities make Spark significantly
    faster than its predecessor, so it is a popular choice for big data processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we will ask Copilot Chat to recommend a strategy for using Apache Spark
    to track our assets:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Let''s imagine that our company is in Chicago. We want to create a class
    called AssetLocationSparkAdapter that will continuously stream AssetLocation json
    messages from a Kafka topic called asset_location. An AssetLocation has the following
    attributes asset_id: int, latitude: float, longitude: float, and timestamp: datetime.
    The AssetLocations are stored in a Postgres database in a table called itam.asset_locations.
    AssetLocationSparkAdapter should calculate if the new AssetLocation is more than
    25 miles from Chicago. If it is, then it should write a message to the console.
    This should be using the latest version of Spark. How would we do this? |'
  prefs: []
  type: TYPE_TB
- en: Copilot Chat generates a class that you should be able to put into a file called
    asset_location_spark_adapter.py in the infrastructure package. Helpfully, it also
    includes comments for each line, so you should find the generated code easy to
    follow. The `import` statements include the Spark libraries as well as `geopy`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.19 The `import`s required to run Spark
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The class begins with an overstuffed constructor that defines the schema Spark
    will use when it translates the JSON to a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Note The `AssetLocationSparkAdapter`, as defined, is a blocking process. Therefore,
    your FastAPI application will not “fully” boot until the Spark process has been
    killed. You want this to be a standalone process, or you need to introduce an
    asynchronous framework to have these two processes run concomitantly.
  prefs: []
  type: TYPE_NORMAL
- en: Next it starts up a local Spark instance/session that will allow Spark to connect
    to the Kafka topic and continuously stream in the records.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.20 `AssessLocationSparkAdapter`, which processes the Kafka topic
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The final section of the `AssetLocationSparkAdapter` class calculates the distance
    from the asset’s current location to Chicago. If the difference is greater than
    25 miles, it sends the result set to the console. Additionally, it provides a
    method to start and stop the adapter.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.21 Calculating the distance from the `Asset` location to Chicago
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The `calculate_distance` method takes the longitude and latitude of the asset’s
    location and determines the distance from Chicago using the `geopy.distance` function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.22 Function to calculate the distance between Chi-town and `Asset`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In this instance, the code that Copilot Chat produced had some problems preventing
    it from running locally. After running it locally, encountering these problems,
    and trolling Stack Overflow, you would find a solution to the two main problems
    with the code: a missing environmental variable for running locally and failing
    to register your UDF (User Defined Function). Fortunately, you do not need to
    do the testing and research—a solution is provided in the following listing.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.23 Edits required to run the application locally
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Finally, to run your Spark application, update main.py with the following code
    in the `main` function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.24 Updates to the `main` function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As you enter locations for your asset into the Kafka console producer that are
    further than 25 miles from downtown Chicago, you will notice that entries are
    written to the console. It would be trivial to update the class to output these
    results to Twilio’s SMS API or an email service such as SendGrid.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.25 The streaming output from your asset location
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! You are tracking your assets in real time and sending real-time
    alerts in case your corporate resources grow legs and walk away.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GitHub Copilot Chat is an innovative tool that brings together the comprehensive
    language understanding of ChatGPT and the handy features of Copilot. It’s a noteworthy
    development in the realm of programming assistance, particularly for providing
    detailed and contextually relevant suggestions in real time, fostering a more
    efficient coding experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Mediator design pattern is a distinct behavioral pattern that facilitates
    a high level of decoupling between objects, thus enhancing the modularity of your
    code. By encompassing the interactions between objects in a mediator object, objects
    can communicate indirectly, which reduces dependencies and promotes code reusability
    and ease of modification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Kafka is a robust, distributed streaming platform engineered for creating
    real-time data pipelines and streaming applications. It can effectively handle
    data streams from a multitude of sources and transmit them to various consumers,
    making it an ideal solution for use cases that require handling substantial volumes
    of real-time or near-real-time data. It’s important to remember that Kafka is
    optimized for append-only, immutable data and not for use cases that need record
    updates or deletions, or complex querying.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark is a high-performance, distributed data processing engine renowned
    for its speed, user-friendliness, and advanced analytics capabilities. It’s highly
    suitable for scenarios necessitating real-time data processing or for operations
    on enormous datasets. However, for simpler tasks such as basic analytics and straightforward
    aggregations, a traditional relational database may be a more appropriate choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI, despite its rapid evolution, is not infallible. It’s crucial
    to meticulously review all generated output to ensure that it aligns with your
    specific requirements and quality standards. Generative AI is not a substitute
    for deep domain knowledge or coding expertise, but it significantly enhances productivity
    by providing valuable insights and reducing the time spent on routine tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
