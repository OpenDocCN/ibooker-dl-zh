["```py\n#include \"tensorflow/lite/micro/examples/hello_world/sine_model_data.h\"\n#include \"tensorflow/lite/micro/kernels/all_ops_resolver.h\"\n#include \"tensorflow/lite/micro/micro_error_reporter.h\"\n#include \"tensorflow/lite/micro/micro_interpreter.h\"\n#include \"tensorflow/lite/micro/testing/micro_test.h\"\n#include \"tensorflow/lite/schema/schema_generated.h\"\n#include \"tensorflow/lite/version.h\"\n```", "```py\nTF_LITE_MICRO_TESTS_BEGIN\n\nTF_LITE_MICRO_TEST(LoadModelAndPerformInference) {\n```", "```py\n// Set up logging\ntflite::MicroErrorReporter micro_error_reporter;\ntflite::ErrorReporter* error_reporter = &micro_error_reporter;\n```", "```py\ntflite::ErrorReporter* error_reporter = &micro_error_reporter;\n```", "```py\n// Map the model into a usable data structure. This doesn't involve any\n// copying or parsing, it's a very lightweight operation.\nconst tflite::Model* model = ::tflite::GetModel(g_sine_model_data);\nif (model->version() != TFLITE_SCHEMA_VERSION) {\nerror_reporter->Report(\n    \"Model provided is schema version %d not equal \"\n    \"to supported version %d.\\n\",\n    model->version(), TFLITE_SCHEMA_VERSION);\n    return 1;\n}\n```", "```py\nif (model->version() != TFLITE_SCHEMA_VERSION) {\n```", "```py\nerror_reporter->Report(\n    \"Model provided is schema version %d not equal \"\n    \"to supported version %d.\\n\",\n    model->version(), TFLITE_SCHEMA_VERSION);\n```", "```py\n// This pulls in all the operation implementations we need\ntflite::ops::micro::AllOpsResolver resolver;\n```", "```py\n// Create an area of memory to use for input, output, and intermediate arrays.\n// Finding the minimum value for your model may require some trial and error.\nconst int tensor_arena_size = 2 \u00d7 1024;\nuint8_t tensor_arena[tensor_arena_size];\n```", "```py\n// Build an interpreter to run the model with\ntflite::MicroInterpreter interpreter(model, resolver, tensor_arena,\n                                     tensor_arena_size, error_reporter);\n\n// Allocate memory from the tensor_arena for the model's tensors\ninterpreter.AllocateTensors();\n```", "```py\n// Obtain a pointer to the model's input tensor\nTfLiteTensor* input = interpreter.input(0);\n```", "```py\n// Make sure the input has the properties we expect\nTF_LITE_MICRO_EXPECT_NE(nullptr, input);\n// The property \"dims\" tells us the tensor's shape. It has one element for\n// each dimension. Our input is a 2D tensor containing 1 element, so \"dims\"\n// should have size 2.\nTF_LITE_MICRO_EXPECT_EQ(2, input->dims->size);\n// The value of each element gives the length of the corresponding tensor.\n// We should expect two single element tensors (one is contained within the\n// other).\nTF_LITE_MICRO_EXPECT_EQ(1, input->dims->data[0]);\nTF_LITE_MICRO_EXPECT_EQ(1, input->dims->data[1]);\n// The input is a 32 bit floating point value\nTF_LITE_MICRO_EXPECT_EQ(kTfLiteFloat32, input->type);\n```", "```py\nTF_LITE_MICRO_EXPECT_NE(nullptr, input);\n```", "```py\n[[0]]\n```", "```py\nTF_LITE_MICRO_EXPECT_EQ(2, input->dims->size);\n```", "```py\nTF_LITE_MICRO_EXPECT_EQ(1, input->dims->data[0]);\nTF_LITE_MICRO_EXPECT_EQ(1, input->dims->data[1]);\n```", "```py\nTF_LITE_MICRO_EXPECT_EQ(kTfLiteFloat32, input->type);\n```", "```py\n// Provide an input value\ninput->data.f[0] = 0.;\n\n// Run the model on this input and check that it succeeds\nTfLiteStatus invoke_status = interpreter.Invoke();\nif (invoke_status != kTfLiteOk) {\n error_reporter->Report(\"Invoke failed\\n\");\n}\nTF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, invoke_status);\n```", "```py\ninput->data.f[0] = 0.;\n```", "```py\n// A union of pointers that points to memory for a given tensor.\ntypedef union {\n  int32_t* i32;\n  int64_t* i64;\n  float* f;\n  TfLiteFloat16* f16;\n  char* raw;\n  const char* raw_const;\n  uint8_t* uint8;\n  bool* b;\n  int16_t* i16;\n  TfLiteComplex64* c64;\n  int8_t* int8;\n} TfLitePtrUnion;\n```", "```py\ninput->data.f[0] = 0.;\n```", "```py\nTfLiteStatus invoke_status = interpreter.Invoke();\n```", "```py\nif (invoke_status != kTfLiteOk) {\n    error_reporter->Report(\"Invoke failed\\n\");\n}\n```", "```py\nTF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, invoke_status);\n```", "```py\nTfLiteTensor* output = interpreter.output(0);\n```", "```py\nTF_LITE_MICRO_EXPECT_EQ(2, output->dims->size);\nTF_LITE_MICRO_EXPECT_EQ(1, input->dims->data[0]);\nTF_LITE_MICRO_EXPECT_EQ(1, input->dims->data[1]);\nTF_LITE_MICRO_EXPECT_EQ(kTfLiteFloat32, output->type);\n```", "```py\n// Obtain the output value from the tensor\nfloat value = output->data.f[0];\n```", "```py\n// Check that the output value is within 0.05 of the expected value\nTF_LITE_MICRO_EXPECT_NEAR(0., value, 0.05);\n```", "```py\n// Run inference on several more values and confirm the expected outputs\ninput->data.f[0] = 1.;\ninterpreter.Invoke();\nvalue = output->data.f[0];\nTF_LITE_MICRO_EXPECT_NEAR(0.841, value, 0.05);\n\ninput->data.f[0] = 3.;\ninterpreter.Invoke();\nvalue = output->data.f[0];\nTF_LITE_MICRO_EXPECT_NEAR(0.141, value, 0.05);\n\ninput->data.f[0] = 5.;\ninterpreter.Invoke();\nvalue = output->data.f[0];\nTF_LITE_MICRO_EXPECT_NEAR(-0.959, value, 0.05);\n```", "```py\n}\n\nTF_LITE_MICRO_TESTS_END\n```", "```py\ngit clone https://github.com/tensorflow/tensorflow.git\n```", "```py\ncd tensorflow\n```", "```py\nmake -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test\n```", "```py\nTesting LoadModelAndPerformInference\n1/1 tests passed\n~~~ALL TESTS PASSED~~~\n```", "```py\ntensorflow/lite/micro/examples/hello_world/hello_world_test.cc\n```", "```py\ninput->data.f[0] = 0.;\n```", "```py\ninput->data.f[0] = 1.;\n```", "```py\nmake -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test\n```", "```py\nTesting LoadModelAndPerformInference\n0.0486171 near value failed at tensorflow/lite/micro/examples/hello_world/\\\n  hello_world_test.cc:94\n0/1 tests passed\n~~~SOME TESTS FAILED~~~\n```", "```py\n#include \"tensorflow/lite/micro/examples/hello_world/main_functions.h\"\n#include \"tensorflow/lite/micro/examples/hello_world/constants.h\"\n#include \"tensorflow/lite/micro/examples/hello_world/output_handler.h\"\n#include \"tensorflow/lite/micro/examples/hello_world/sine_model_data.h\"\n#include \"tensorflow/lite/micro/kernels/all_ops_resolver.h\"\n#include \"tensorflow/lite/micro/micro_error_reporter.h\"\n#include \"tensorflow/lite/micro/micro_interpreter.h\"\n#include \"tensorflow/lite/schema/schema_generated.h\"\n#include \"tensorflow/lite/version.h\"\n```", "```py\nnamespace {\ntflite::ErrorReporter* error_reporter = nullptr;\nconst tflite::Model* model = nullptr;\ntflite::MicroInterpreter* interpreter = nullptr;\nTfLiteTensor* input = nullptr;\nTfLiteTensor* output = nullptr;\nint inference_count = 0;\n\n// Create an area of memory to use for input, output, and intermediate arrays.\n// Finding the minimum value for your model may require some trial and error.\nconstexpr int kTensorArenaSize = 2 \u00d7 1024;\nuint8_t tensor_arena[kTensorArenaSize];\n}  // namespace\n```", "```py\nvoid setup() {\n  // Set up logging.\n  static tflite::MicroErrorReporter micro_error_reporter;\n  error_reporter = &micro_error_reporter;\n\n  // Map the model into a usable data structure. This doesn't involve any\n  // copying or parsing, it's a very lightweight operation.\n  model = tflite::GetModel(g_sine_model_data);\n  if (model->version() != TFLITE_SCHEMA_VERSION) {\n    error_reporter->Report(\n        \"Model provided is schema version %d not equal \"\n        \"to supported version %d.\",\n        model->version(), TFLITE_SCHEMA_VERSION);\n    return;\n  }\n\n  // This pulls in all the operation implementations we need.\n  static tflite::ops::micro::AllOpsResolver resolver;\n\n  // Build an interpreter to run the model with.\n  static tflite::MicroInterpreter static_interpreter(\n      model, resolver, tensor_arena, kTensorArenaSize, error_reporter);\n  interpreter = &static_interpreter;\n\n  // Allocate memory from the tensor_arena for the model's tensors.\n  TfLiteStatus allocate_status = interpreter->AllocateTensors();\n  if (allocate_status != kTfLiteOk) {\n    error_reporter->Report(\"AllocateTensors() failed\");\n    return;\n  }\n```", "```py\n  // Obtain pointers to the model's input and output tensors.\n  input = interpreter->input(0);\n  output = interpreter->output(0);\n```", "```py\n  // Keep track of how many inferences we have performed.\n  inference_count = 0;\n}\n```", "```py\nvoid loop() {\n```", "```py\n// Calculate an x value to feed into the model. We compare the current\n// inference_count to the number of inferences per cycle to determine\n// our position within the range of possible x values the model was\n// trained on, and use this to calculate a value.\nfloat position = static_cast<float>(inference_count) /\n                    static_cast<float>(kInferencesPerCycle);\nfloat x_val = position * kXrange;\n```", "```py\n// Place our calculated x value in the model's input tensor\ninput->data.f[0] = x_val;\n\n// Run inference, and report any error\nTfLiteStatus invoke_status = interpreter->Invoke();\nif (invoke_status != kTfLiteOk) {\n  error_reporter->Report(\"Invoke failed on x_val: %f\\n\",\n                         static_cast<double>(x_val));\n  return;\n}\n\n// Read the predicted y value from the model's output tensor\nfloat y_val = output->data.f[0];\n```", "```py\n// Output the results. A custom HandleOutput function can be implemented\n// for each supported hardware target.\nHandleOutput(error_reporter, x_val, y_val);\n```", "```py\nvoid HandleOutput(tflite::ErrorReporter* error_reporter, float x_value,\n                  float y_value) {\n  // Log the current X and Y values\n  error_reporter->Report(\"x_value: %f, y_value: %f\\n\", x_value, y_value);\n}\n```", "```py\n// Increment the inference_counter, and reset it if we have reached\n// the total number per cycle\ninference_count += 1;\nif (inference_count \t>= kInferencesPerCycle) inference_count = 0;\n```", "```py\n#include \"tensorflow/lite/micro/examples/hello_world/main_functions.h\"\n```", "```py\nint main(int argc, char* argv[]) {\n  setup();\n  while (true) {\n    loop();\n  }\n}\n```", "```py\nmake -f tensorflow/lite/micro/tools/make/Makefile hello_world\n```", "```py\n# macOS:\ntensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/hello_world\n\n# Linux:\ntensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/hello_world\n\n# Windows\ntensorflow/lite/micro/tools/make/gen/windows_x86_64/bin/hello_world\n```", "```py\nx_value: 1.4137159*2^1, y_value: 1.374213*2^-2\n\nx_value: 1.5707957*2^1, y_value: -1.4249528*2^-5\n\nx_value: 1.7278753*2^1, y_value: -1.4295994*2^-2\n\nx_value: 1.8849551*2^1, y_value: -1.2867725*2^-1\n\nx_value: 1.210171*2^2, y_value: -1.7542461*2^-1\n```"]