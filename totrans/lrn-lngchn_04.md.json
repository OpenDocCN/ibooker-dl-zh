["```py\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"\"\"You are a helpful assistant. Answer all questions to the best \n of your ability.\"\"\"),\n    (\"placeholder\", \"{messages}\"),\n])\n\nmodel = ChatOpenAI()\n\nchain = prompt | model\n\nchain.invoke({\n    \"messages\": [\n        (\"human\",\"\"\"Translate this sentence from English to French: I love \n programming.\"\"\"),\n        (\"ai\", \"J'adore programmer.\"),\n        (\"human\", \"What did you just say?\"),\n    ],\n})\n```", "```py\nimport {ChatPromptTemplate} from '@langchain/core/prompts'\nimport {ChatOpenAI} from '@langchain/openai'\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\"system\", `You are a helpful assistant. Answer all questions to the best \n of your ability.`],\n  [\"placeholder\", \"{messages}\"],\n])\n\nconst model = new ChatOpenAI()\n\nconst chain = prompt.pipe(model)\n\nawait chain.invoke({\n  \"messages\": [\n    [\"human\",`Translate this sentence from English to French: I love \n programming.`],\n    [\"ai\", \"J'adore programmer.\"],\n    [\"human\", \"What did you just say?\"],\n  ],\n})\n```", "```py\nI said, \"J'adore programmer,\" which means \"I love programming\" in French.\n```", "```py\npip install langgraph\n```", "```py\nnpm i @langchain/langgraph\n```", "```py\nfrom typing import Annotated, TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\n\nclass State(TypedDict):\n    # Messages have the type \"list\". The `add_messages` \n    # function in the annotation defines how this state should \n    # be updated (in this case, it appends new messages to the \n    # list, rather than replacing the previous messages)\n\tmessages: Annotated[list, add_messages]\n\nbuilder = StateGraph(State)\n```", "```py\nimport {\n  StateGraph,\n  StateType,\n  Annotation,\n  messagesStateReducer,\n  START, END\n} from '@langchain/langgraph'\n\nconst State = {\n  /**\n * The State defines three things:\n * 1\\. The structure of the graph's state (which \"channels\" are available to\n * read/write)\n * 2\\. The default values for the state's channels\n * 3\\. The reducers for the state's channels. Reducers are functions that\n * determine how to apply updates to the state. Below, new messages are\n * appended to the messages array.\n */\n  messages: Annotation({\n    reducer: messagesStateReducer,\n    default: () => []\n  }),\n}\n\nconst builder = new StateGraph(State)\n```", "```py\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI()\n\ndef chatbot(state: State):\n    answer = model.invoke(state[\"messages\"])\n    return {\"messages\": [answer]}\n\n# The first argument is the unique node name\n# The second argument is the function or Runnable to run\nbuilder.add_node(\"chatbot\", chatbot)\n```", "```py\nimport {ChatOpenAI} from '@langchain/openai'\nimport {\n  AIMessage,\n  SystemMessage,\n  HumanMessage\n} from \"@langchain/core/messages\";\n\nconst model = new ChatOpenAI()\n\nasync function chatbot(state) {\n  const answer = await model.invoke(state.messages)\n  return {\"messages\": answer}\n}\n\nbuilder = builder.addNode('chatbot', chatbot)\n```", "```py\nbuilder.add_edge(START, 'chatbot')\nbuilder.add_edge('chatbot', END)\n\ngraph = builder.compile()\n```", "```py\nbuilder = builder\n  .addEdge(START, 'chatbot')\n  .addEdge('chatbot', END)\n\nlet graph = builder.compile()\n```", "```py\ngraph.get_graph().draw_mermaid_png()\n```", "```py\nawait graph.getGraph().drawMermaidPng()\n```", "```py\ninput = {\"messages\": [HumanMessage('hi!)]} `for` `chunk` `in` `graph``.``stream``(``input``):`\n    `print``(``chunk``)`\n```", "```py`*JavaScript*    ```", "```py    *The output:*    ```", "```py    Notice how the input to the graph was in the same shape as the `State` object we defined earlier; that is, we sent in a list of messages in the `messages` key of a dictionary. In addition, the `stream` function streams the full value of the state after each step of the graph.```", "```py`` # Adding Memory to StateGraph    LangGraph has built-in persistence, which is used in the same way for the simplest graph to the most complex. Let’s see what it looks like to apply it to this first architecture. We’ll recompile our graph, now attaching a *checkpointer*, which is a storage adapter for LangGraph. LangGraph ships with a base class that any user can subclass to create an adapter for their favorite database; at the time of writing, LangGraph ships with several adapters maintained by LangChain:    *   An in-memory adapter, which we’ll use for our examples here           *   A SQLite adapter, using the popular in-process database, appropriate for local apps and testing           *   A Postgres adapter, optimized for the popular relational database and appropriate for large-scale applications.              Many developers have written adapters for other database systems, such as Redis or MySQL:    *Python*    ```", "```py    *JavaScript*    ```", "```py    This returns a runnable object with the same methods as the one used in the previous code block. But now, it stores the state at the end of each step, so every invocation after the first doesn’t start from a blank slate. Any time the graph is called, it starts by using the checkpointer to fetch the most recent saved state, if any, and combines the new input with the previous state. And only then does it execute the first nodes.    Let’s see the difference in action:    *Python*    ```", "```py    *JavaScript*    ```", "```py    Notice the object `thread1`, which identifies the current interaction as belonging to a particular history of interactions—which are called *threads* in LangGraph. Threads are created automatically when first used. Any string is a valid identifier for a thread (usually, Universally Unique Identifiers [UUIDs] are used). The existence of threads helps you achieve an important milestone in your LLM application; it can now be used by multiple users with independent conversations that are never mixed up.    As before, the `chatbot` node is first called with a single message (the one we just passed in) and returns another message, both of which are then saved in the state.    The second time we execute the graph on the same thread, the `chatbot` node is called with three messages, the two saved from the first execution, and the next question from the user. This is the essence of memory: the previous state is still there, which makes it possible, for instance, to answer questions about something said before (and do many more interesting things, of which we will see more later).    You can also inspect and update the state directly; let’s see how:    *Python*    ```", "```py    *JavaScript*    ```", "```py    This returns the current state of this thread.    And you can update the state like this:    *Python*    ```", "```py   ```", "```py await graph.updateState(thread1, [new HumanMessage('I like LLMs!)]) ```", "```py`  ```", "```py from langchain_core.messages import SystemMessage, trim_messages from langchain_openai import ChatOpenAI  trimmer = trim_messages(     max_tokens=65,     strategy=\"last\",     token_counter=ChatOpenAI(model=\"gpt-4o\"),     include_system=True,     allow_partial=False,     start_on=\"human\", )  messages = [     SystemMessage(content=\"you're a good assistant\"),     HumanMessage(content=\"hi! I'm bob\"),     AIMessage(content=\"hi!\"),     HumanMessage(content=\"I like vanilla ice cream\"),     AIMessage(content=\"nice\"),     HumanMessage(content=\"what's 2 + 2\"),     AIMessage(content=\"4\"),     HumanMessage(content=\"thanks\"),     AIMessage(content=\"no problem!\"),     HumanMessage(content=\"having fun?\"),     AIMessage(content=\"yes!\"), ]  trimmer.invoke(messages) ```", "```py import {   AIMessage,   HumanMessage,   SystemMessage,   trimMessages, } from \"@langchain/core/messages\"; import { ChatOpenAI } from \"@langchain/openai\";  const trimmer = trimMessages({   maxTokens: 65,   strategy: \"last\",   tokenCounter: new ChatOpenAI({ modelName: \"gpt-4o\" }),   includeSystem: true,   allowPartial: false,   startOn: \"human\", });  const messages = [   new SystemMessage(\"you're a good assistant\"),   new HumanMessage(\"hi! I'm bob\"),   new AIMessage(\"hi!\"),   new HumanMessage(\"I like vanilla ice cream\"),   new AIMessage(\"nice\"),   new HumanMessage(\"what's 2 + 2\"),   new AIMessage(\"4\"),   new HumanMessage(\"thanks\"),   new AIMessage(\"no problem!\"),   new HumanMessage(\"having fun?\"),   new AIMessage(\"yes!\"), ]  const trimmed = await trimmer.invoke(messages); ```", "```py [SystemMessage(content=\"you're a good assistant\"),  HumanMessage(content='what's 2 + 2'),  AIMessage(content='4'),  HumanMessage(content='thanks'),  AIMessage(content='no problem!'),  HumanMessage(content='having fun?'),  AIMessage(content='yes!')] ```", "```py from langchain_core.messages import (     AIMessage,     HumanMessage,     SystemMessage,     filter_messages, )  messages = [     SystemMessage(\"you are a good assistant\", id=\"1\"),     HumanMessage(\"example input\", id=\"2\", name=\"example_user\"),     AIMessage(\"example output\", id=\"3\", name=\"example_assistant\"),     HumanMessage(\"real input\", id=\"4\", name=\"bob\"),     AIMessage(\"real output\", id=\"5\", name=\"alice\"), ]  filter_messages(messages, include_types=\"human\") ```", "```py import {   HumanMessage,   SystemMessage,   AIMessage,   filterMessages, } from \"@langchain/core/messages\";  const messages = [   new SystemMessage({content: \"you are a good assistant\", id: \"1\"}),   new HumanMessage({content: \"example input\", id: \"2\", name: \"example_user\"}),   new AIMessage({content: \"example output\", id: \"3\", name: \"example_assistant\"}),   new HumanMessage({content: \"real input\", id: \"4\", name: \"bob\"}),   new AIMessage({content: \"real output\", id: \"5\", name: \"alice\"}), ];  filterMessages(messages, { includeTypes: [\"human\"] }); ```", "```py [HumanMessage(content='example input', name='example_user', id='2'),  HumanMessage(content='real input', name='bob', id='4')] ```", "```py filter_messages(messages, exclude_names=[\"example_user\", \"example_assistant\"])  \"\"\" [SystemMessage(content='you are a good assistant', id='1'), HumanMessage(content='real input', name='bob', id='4'), AIMessage(content='real output', name='alice', id='5')] \"\"\"  filter_messages(     messages,      include_types=[HumanMessage, AIMessage],      exclude_ids=[\"3\"] )  \"\"\" [HumanMessage(content='example input', name='example_user', id='2'),  HumanMessage(content='real input', name='bob', id='4'),  AIMessage(content='real output', name='alice', id='5')] \"\"\" ```", "```py filterMessages(   messages,    { excludeNames: [\"example_user\",    \"example_assistant\"] } );  /* [SystemMessage(content='you are a good assistant', id='1'), HumanMessage(content='real input', name='bob', id='4'), AIMessage(content='real output', name='alice', id='5')] */  filterMessages(messages, { includeTypes: [\"human\", \"ai\"], excludeIds: [\"3\"] });  /* [HumanMessage(content='example input', name='example_user', id='2'),  HumanMessage(content='real input', name='bob', id='4'),  AIMessage(content='real output', name='alice', id='5')] */ ```", "```py model = ChatOpenAI()  filter_ = filter_messages(exclude_names=[\"example_user\", \"example_assistant\"])  chain = filter_ | model ```", "```py const model = new ChatOpenAI()  const filter = filterMessages({   excludeNames: [\"example_user\", \"example_assistant\"] })  const chain = filter.pipe(model) ```", "```py from langchain_core.messages import (     AIMessage,     HumanMessage,     SystemMessage,     merge_message_runs, )  messages = [     SystemMessage(\"you're a good assistant.\"),     SystemMessage(\"you always respond with a joke.\"),     HumanMessage(         [{\"type\": \"text\", \"text\": \"i wonder why it's called langchain\"}]     ),     HumanMessage(\"and who is harrison chasing anyway\"),     AIMessage(         '''Well, I guess they thought \"WordRope\" and \"SentenceString\" just   didn\\'t have the same ring to it!'''     ),     AIMessage(\"\"\"Why, he's probably chasing after the last cup of coffee in the   office!\"\"\"), ]  merge_message_runs(messages) ```", "```py import {   HumanMessage,   SystemMessage,   AIMessage,   mergeMessageRuns, } from \"@langchain/core/messages\";  const messages = [   new SystemMessage(\"you're a good assistant.\"),   new SystemMessage(\"you always respond with a joke.\"),   new HumanMessage({     content: [{ type: \"text\", text: \"i wonder why it's called langchain\" }],   }),   new HumanMessage(\"and who is harrison chasing anyway\"),   new AIMessage(     `Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t   have the same ring to it!`   ),   new AIMessage(     \"Why, he's probably chasing after the last cup of coffee in the office!\"   ), ];  mergeMessageRuns(messages); ```", "```py [SystemMessage(content=\"you're a good assistant.\\nyou always respond with a      joke.\"),  HumanMessage(content=[{'type': 'text', 'text': \"i wonder why it's called     langchain\"}, 'and who is harrison chasing anyway']),  AIMessage(content='Well, I guess they thought \"WordRope\" and \"SentenceString\"      just didn\\'t have the same ring to it!\\nWhy, he\\'s probably chasing after      the last cup of coffee in the office!')] ```", "```py model = ChatOpenAI() merger = merge_message_runs() chain = merger | model ```", "```py const model = new ChatOpenAI() const merger = mergeMessageRuns() const chain = merger.pipe(model) ```", "```py` ```"]