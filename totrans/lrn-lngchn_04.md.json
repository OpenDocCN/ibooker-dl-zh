["```py\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"\"\"You are a helpful assistant. Answer all questions to the best \n of your ability.\"\"\"),\n    (\"placeholder\", \"{messages}\"),\n])\n\nmodel = ChatOpenAI()\n\nchain = prompt | model\n\nchain.invoke({\n    \"messages\": [\n        (\"human\",\"\"\"Translate this sentence from English to French: I love \n programming.\"\"\"),\n        (\"ai\", \"J'adore programmer.\"),\n        (\"human\", \"What did you just say?\"),\n    ],\n})\n```", "```py\nimport {ChatPromptTemplate} from '@langchain/core/prompts'\nimport {ChatOpenAI} from '@langchain/openai'\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\"system\", `You are a helpful assistant. Answer all questions to the best \n of your ability.`],\n  [\"placeholder\", \"{messages}\"],\n])\n\nconst model = new ChatOpenAI()\n\nconst chain = prompt.pipe(model)\n\nawait chain.invoke({\n  \"messages\": [\n    [\"human\",`Translate this sentence from English to French: I love \n programming.`],\n    [\"ai\", \"J'adore programmer.\"],\n    [\"human\", \"What did you just say?\"],\n  ],\n})\n```", "```py\nI said, \"J'adore programmer,\" which means \"I love programming\" in French.\n```", "```py\npip install langgraph\n```", "```py\nnpm i @langchain/langgraph\n```", "```py\nfrom typing import Annotated, TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\n\nclass State(TypedDict):\n    # Messages have the type \"list\". The `add_messages` \n    # function in the annotation defines how this state should \n    # be updated (in this case, it appends new messages to the \n    # list, rather than replacing the previous messages)\n\tmessages: Annotated[list, add_messages]\n\nbuilder = StateGraph(State)\n```", "```py\nimport {\n  StateGraph,\n  StateType,\n  Annotation,\n  messagesStateReducer,\n  START, END\n} from '@langchain/langgraph'\n\nconst State = {\n  /**\n * The State defines three things:\n * 1\\. The structure of the graph's state (which \"channels\" are available to\n * read/write)\n * 2\\. The default values for the state's channels\n * 3\\. The reducers for the state's channels. Reducers are functions that\n * determine how to apply updates to the state. Below, new messages are\n * appended to the messages array.\n */\n  messages: Annotation({\n    reducer: messagesStateReducer,\n    default: () => []\n  }),\n}\n\nconst builder = new StateGraph(State)\n```", "```py\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI()\n\ndef chatbot(state: State):\n    answer = model.invoke(state[\"messages\"])\n    return {\"messages\": [answer]}\n\n# The first argument is the unique node name\n# The second argument is the function or Runnable to run\nbuilder.add_node(\"chatbot\", chatbot)\n```", "```py\nimport {ChatOpenAI} from '@langchain/openai'\nimport {\n  AIMessage,\n  SystemMessage,\n  HumanMessage\n} from \"@langchain/core/messages\";\n\nconst model = new ChatOpenAI()\n\nasync function chatbot(state) {\n  const answer = await model.invoke(state.messages)\n  return {\"messages\": answer}\n}\n\nbuilder = builder.addNode('chatbot', chatbot)\n```", "```py\nbuilder.add_edge(START, 'chatbot')\nbuilder.add_edge('chatbot', END)\n\ngraph = builder.compile()\n```", "```py\nbuilder = builder\n  .addEdge(START, 'chatbot')\n  .addEdge('chatbot', END)\n\nlet graph = builder.compile()\n```", "```py\ngraph.get_graph().draw_mermaid_png()\n```", "```py\nawait graph.getGraph().drawMermaidPng()\n```", "```py\ninput = {\"messages\": [HumanMessage('hi!)]}\nfor chunk in graph.stream(input):\n    print(chunk)\n```", "```py\nconst input = {messages: [new HumanMessage('hi!)]}\nfor await (const chunk of await graph.stream(input)) {\n  console.log(chunk)\n}\n```", "```py\n{ \"chatbot\": { \"messages\": [AIMessage(\"How can I help you?\")] } }\n```", "```py\nfrom langgraph.checkpoint.memory import MemorySaver\n\ngraph = builder.compile(checkpointer=MemorySaver())\n```", "```py\nimport {MemorySaver} from '@langchain/langgraph'\n\nconst graph = builder.compile({ checkpointer: new MemorySaver() })\n```", "```py\nthread1 = {\"configurable\": {\"thread_id\": \"1\"}}\nresult_1 = graph.invoke(\n    { \"messages\": [HumanMessage(\"hi, my name is Jack!\")] }, \n    thread1\n)\n// { \"chatbot\": { \"messages\": [AIMessage(\"How can I help you, Jack?\")] } }\n\nresult_2 = graph.invoke(\n    { \"messages\": [HumanMessage(\"what is my name?\")] }, \n    thread1\n)\n// { \"chatbot\": { \"messages\": [AIMessage(\"Your name is Jack\")] } }\n```", "```py\nconst thread1 = {configurable: {thread_id: '1'}}\nconst result_1 = await graph.invoke(\n  { \"messages\": [new HumanMessage(\"hi, my name is Jack!\")] },\n  thread1\n)\n// { \"chatbot\": { \"messages\": [AIMessage(\"How can I help you, Jack?\")] } }\n\nconst result_2 = await graph.invoke(\n  { \"messages\": [new HumanMessage(\"what is my name?\")] },\n  thread1\n)\n// { \"chatbot\": { \"messages\": [AIMessage(\"Your name is Jack\")] } }\n```", "```py\ngraph.get_state(thread1)\n```", "```py\nawait graph.getState(thread1)\n```", "```py\ngraph.update_state(thread1, [HumanMessage('I like LLMs!)])\n```", "```py\nawait graph.updateState(thread1, [new HumanMessage('I like LLMs!)])\n```", "```py\nfrom langchain_core.messages import SystemMessage, trim_messages\nfrom langchain_openai import ChatOpenAI\n\ntrimmer = trim_messages(\n    max_tokens=65,\n    strategy=\"last\",\n    token_counter=ChatOpenAI(model=\"gpt-4o\"),\n    include_system=True,\n    allow_partial=False,\n    start_on=\"human\",\n)\n\nmessages = [\n    SystemMessage(content=\"you're a good assistant\"),\n    HumanMessage(content=\"hi! I'm bob\"),\n    AIMessage(content=\"hi!\"),\n    HumanMessage(content=\"I like vanilla ice cream\"),\n    AIMessage(content=\"nice\"),\n    HumanMessage(content=\"what's 2 + 2\"),\n    AIMessage(content=\"4\"),\n    HumanMessage(content=\"thanks\"),\n    AIMessage(content=\"no problem!\"),\n    HumanMessage(content=\"having fun?\"),\n    AIMessage(content=\"yes!\"),\n]\n\ntrimmer.invoke(messages)\n```", "```py\nimport {\n  AIMessage,\n  HumanMessage,\n  SystemMessage,\n  trimMessages,\n} from \"@langchain/core/messages\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst trimmer = trimMessages({\n  maxTokens: 65,\n  strategy: \"last\",\n  tokenCounter: new ChatOpenAI({ modelName: \"gpt-4o\" }),\n  includeSystem: true,\n  allowPartial: false,\n  startOn: \"human\",\n});\n\nconst messages = [\n  new SystemMessage(\"you're a good assistant\"),\n  new HumanMessage(\"hi! I'm bob\"),\n  new AIMessage(\"hi!\"),\n  new HumanMessage(\"I like vanilla ice cream\"),\n  new AIMessage(\"nice\"),\n  new HumanMessage(\"what's 2 + 2\"),\n  new AIMessage(\"4\"),\n  new HumanMessage(\"thanks\"),\n  new AIMessage(\"no problem!\"),\n  new HumanMessage(\"having fun?\"),\n  new AIMessage(\"yes!\"),\n]\n\nconst trimmed = await trimmer.invoke(messages);\n```", "```py\n[SystemMessage(content=\"you're a good assistant\"),\n HumanMessage(content='what's 2 + 2'),\n AIMessage(content='4'),\n HumanMessage(content='thanks'),\n AIMessage(content='no problem!'),\n HumanMessage(content='having fun?'),\n AIMessage(content='yes!')]\n```", "```py\nfrom langchain_core.messages import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage,\n    filter_messages,\n)\n\nmessages = [\n    SystemMessage(\"you are a good assistant\", id=\"1\"),\n    HumanMessage(\"example input\", id=\"2\", name=\"example_user\"),\n    AIMessage(\"example output\", id=\"3\", name=\"example_assistant\"),\n    HumanMessage(\"real input\", id=\"4\", name=\"bob\"),\n    AIMessage(\"real output\", id=\"5\", name=\"alice\"),\n]\n\nfilter_messages(messages, include_types=\"human\")\n```", "```py\nimport {\n  HumanMessage,\n  SystemMessage,\n  AIMessage,\n  filterMessages,\n} from \"@langchain/core/messages\";\n\nconst messages = [\n  new SystemMessage({content: \"you are a good assistant\", id: \"1\"}),\n  new HumanMessage({content: \"example input\", id: \"2\", name: \"example_user\"}),\n  new AIMessage({content: \"example output\", id: \"3\", name: \"example_assistant\"}),\n  new HumanMessage({content: \"real input\", id: \"4\", name: \"bob\"}),\n  new AIMessage({content: \"real output\", id: \"5\", name: \"alice\"}),\n];\n\nfilterMessages(messages, { includeTypes: [\"human\"] });\n```", "```py\n[HumanMessage(content='example input', name='example_user', id='2'),\n HumanMessage(content='real input', name='bob', id='4')]\n```", "```py\nfilter_messages(messages, exclude_names=[\"example_user\", \"example_assistant\"])\n\n\"\"\"\n[SystemMessage(content='you are a good assistant', id='1'),\nHumanMessage(content='real input', name='bob', id='4'),\nAIMessage(content='real output', name='alice', id='5')]\n\"\"\"\n\nfilter_messages(\n    messages, \n    include_types=[HumanMessage, AIMessage], \n    exclude_ids=[\"3\"]\n)\n\n\"\"\"\n[HumanMessage(content='example input', name='example_user', id='2'),\n HumanMessage(content='real input', name='bob', id='4'),\n AIMessage(content='real output', name='alice', id='5')]\n\"\"\"\n```", "```py\nfilterMessages(\n  messages, \n  { excludeNames: [\"example_user\", \n  \"example_assistant\"] }\n);\n\n/*\n[SystemMessage(content='you are a good assistant', id='1'),\nHumanMessage(content='real input', name='bob', id='4'),\nAIMessage(content='real output', name='alice', id='5')]\n*/\n\nfilterMessages(messages, { includeTypes: [\"human\", \"ai\"], excludeIds: [\"3\"] });\n\n/*\n[HumanMessage(content='example input', name='example_user', id='2'),\n HumanMessage(content='real input', name='bob', id='4'),\n AIMessage(content='real output', name='alice', id='5')]\n*/\n```", "```py\nmodel = ChatOpenAI()\n\nfilter_ = filter_messages(exclude_names=[\"example_user\", \"example_assistant\"])\n\nchain = filter_ | model\n```", "```py\nconst model = new ChatOpenAI()\n\nconst filter = filterMessages({\n  excludeNames: [\"example_user\", \"example_assistant\"]\n})\n\nconst chain = filter.pipe(model)\n```", "```py\nfrom langchain_core.messages import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage,\n    merge_message_runs,\n)\n\nmessages = [\n    SystemMessage(\"you're a good assistant.\"),\n    SystemMessage(\"you always respond with a joke.\"),\n    HumanMessage(\n        [{\"type\": \"text\", \"text\": \"i wonder why it's called langchain\"}]\n    ),\n    HumanMessage(\"and who is harrison chasing anyway\"),\n    AIMessage(\n        '''Well, I guess they thought \"WordRope\" and \"SentenceString\" just \n didn\\'t have the same ring to it!'''\n    ),\n    AIMessage(\"\"\"Why, he's probably chasing after the last cup of coffee in the \n office!\"\"\"),\n]\n\nmerge_message_runs(messages)\n```", "```py\nimport {\n  HumanMessage,\n  SystemMessage,\n  AIMessage,\n  mergeMessageRuns,\n} from \"@langchain/core/messages\";\n\nconst messages = [\n  new SystemMessage(\"you're a good assistant.\"),\n  new SystemMessage(\"you always respond with a joke.\"),\n  new HumanMessage({\n    content: [{ type: \"text\", text: \"i wonder why it's called langchain\" }],\n  }),\n  new HumanMessage(\"and who is harrison chasing anyway\"),\n  new AIMessage(\n    `Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t \n have the same ring to it!`\n  ),\n  new AIMessage(\n    \"Why, he's probably chasing after the last cup of coffee in the office!\"\n  ),\n];\n\nmergeMessageRuns(messages);\n```", "```py\n[SystemMessage(content=\"you're a good assistant.\\nyou always respond with a \n    joke.\"),\n HumanMessage(content=[{'type': 'text', 'text': \"i wonder why it's called\n    langchain\"}, 'and who is harrison chasing anyway']),\n AIMessage(content='Well, I guess they thought \"WordRope\" and \"SentenceString\" \n    just didn\\'t have the same ring to it!\\nWhy, he\\'s probably chasing after \n    the last cup of coffee in the office!')]\n```", "```py\nmodel = ChatOpenAI()\nmerger = merge_message_runs()\nchain = merger | model\n```", "```py\nconst model = new ChatOpenAI()\nconst merger = mergeMessageRuns()\nconst chain = merger.pipe(model)\n```"]