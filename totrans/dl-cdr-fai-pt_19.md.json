["```py\nmodel_meta[resnet50]\n```", "```py\n{'cut': -2,\n 'split': <function fastai.vision.learner._resnet_split(m)>,\n 'stats': ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])}\n```", "```py\ncreate_head(20,2)\n```", "```py\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): Flatten()\n  (2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=20, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)\n```", "```py\nclass SiameseModel(Module):\n    def __init__(self, encoder, head):\n        self.encoder,self.head = encoder,head\n\n    def forward(self, x1, x2):\n        ftrs = torch.cat([self.encoder(x1), self.encoder(x2)], dim=1)\n        return self.head(ftrs)\n```", "```py\nencoder = create_body(resnet34, cut=-2)\n```", "```py\nhead = create_head(512*4, 2, ps=0.5)\n```", "```py\nmodel = SiameseModel(encoder, head)\n```", "```py\ndef loss_func(out, targ):\n    return nn.CrossEntropyLoss()(out, targ.long())\n```", "```py\ndef siamese_splitter(model):\n    return [params(model.encoder), params(model.head)]\n```", "```py\nlearn = Learner(dls, model, loss_func=loss_func,\n                splitter=siamese_splitter, metrics=accuracy)\nlearn.freeze()\n```", "```py\nlearn.fit_one_cycle(4, 3e-3)\n```", "```py\nlearn.unfreeze()\nlearn.fit_one_cycle(4, slice(1e-6,1e-4))\n```", "```py\nif self.n_emb != 0:\n    x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n    x = torch.cat(x, 1)\n    x = self.emb_drop(x)\nif self.n_cont != 0:\n    x_cont = self.bn_cont(x_cont)\n    x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\nreturn self.layers(x)\n```", "```py\nif self.n_emb != 0:\n```", "```py\n    x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n```", "```py\n    x = torch.cat(x, 1)\n```", "```py\n    x = self.emb_drop(x)\n```", "```py\nif self.n_cont != 0:\n```", "```py\n    x_cont = self.bn_cont(x_cont)\n```", "```py\n    x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n```", "```py\nreturn self.layers(x)\n```"]