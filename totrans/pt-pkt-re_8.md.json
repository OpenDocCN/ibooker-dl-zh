["```py\nimport torchvision\n\ntrain_data = torchvision.datasets.CIFAR10(\n          root=\".\",\n          train=True,\n          transform=None,\n          download=True)\n```", "```py\nimport torchvision\n\nmodel = torchvision.models.vgg16(pretrained=False)\n```", "```py\nfrom torchvision import transforms, datasets\n\ntrain_transforms = transforms.Compose([\n                      transforms.ToTensor(),\n                      transforms.Normalize(\n                      (0.4914, 0.4822, 0.4465),\n                      (0.2023, 0.1994, 0.2010)),\n                      ])\ntrain_data = datasets.CIFAR10(\n                  root=\".\",\n                  train=True,\n                  transform=train_transforms)\n```", "```py\n\n>>> transforms = torch.nn.Sequential(\n        transforms.CenterCrop(10),\n        transforms.Normalize(\n            (0.485, 0.456, 0.406), (0.229, 0.224,\n            0.225)),\n        )\n\n>>> scripted_transforms = torch.jit.script(transforms)\n\n```", "```py\nfrom torchtext.datasets import IMDB\n\ntrain_iter, test_iter = \\\n  IMDB(split=('train', 'test'))\n\nnext(train_iter)\n# out:\n# ('neg',\n# 'I rented I AM CURIOUS-YELLOW ...)\n```", "```py\nfrom torchtext.data.utils \\\n  import get_tokenizer\n\ntokenizer = get_tokenizer('basic_english')\n```", "```py\nfrom collections import Counter\nfrom torchtext.vocab import Vocab\n\ntrain_iter = IMDB(split='train')\ncounter = Counter()\nfor (label, line) in train_iter:\n    counter.update(tokenizer(line))\nvocab = Vocab(counter,\n              min_freq=10,\n              specials=('<unk>',\n                        '<BOS>',\n                        '<EOS>',\n                        '<PAD>'))\n```", "```py\ntext_transform = lambda x: [vocab['<BOS>']] \\\n  + [vocab[token] \\\n     for token in tokenizer(x)] + [vocab['<EOS>']]\n\nlabel_transform = lambda x: 1 \\\n  if x == 'pos' else 0\n\nprint(text_transform(\"programming is awesome\"))\n# out: [1, 8320, 12, 1156, 2]\n```", "```py\nfrom torch.utils.data import DataLoader\n\ntrain_iter = IMDB(split='train')\ntrain_dataloader = DataLoader(\n    list(train_iter),\n    batch_size=8,\n    shuffle=True)\n\n# for text, label in train_dataloader\n```", "```py\n%load_ext tensorboard\n%tensorboard --logdir ./runs/\n```", "```py\nfromtorch.utils.tensorboardimportSummaryWriterwriter=SummaryWriter()![1](Images/1.png)\n```", "```py\nN_EPOCHS=10forepochinrange(N_EPOCHS):epoch_loss=0.0forinputs,labelsintrainloader:inputs=inputs.to(device)labels=labels.to(device)optimizer.zero_grad()outputs=model(inputs)loss=criterion(outputs,labels)loss.backward()optimizer.step()epoch_loss+=loss.item()print(\"Epoch: {} Loss: {}\".format(epoch,epoch_loss/len(trainloader)))writer.add_scalar('Loss/train',epoch_loss/len(trainloader),epoch)![1](Images/1.png)\n```", "```py\nmodel = vgg16(preTrained=True)\nwriter.add_graph(model)\n```", "```py\nwriter.close()\n```"]