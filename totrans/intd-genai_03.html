<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">4 <a id="idTextAnchor000"/>The evolution of created content</h1>
<p class="co-summary-head">This chapter covers</p>
<ul class="calibre6">
<li class="co-summary-bullet">Creating and detecting synthetic media</li>
<li class="co-summary-bullet">Using generative AI for content creation</li>
<li class="co-summary-bullet">Introducing the ongoing debates around the use of copyrighted content</li>
</ul>
<p class="body"><a id="marker-73"/>In an image that was circulated widely on Twitter, Pope Francis is walking down a street, wearing a cross around his neck and his typical white zucchetto. More unusually, the octogenarian is sporting an eye-catching white puffer coat that bears a strong resemblance to one sold by the designer brand Balenciaga (for $3,350 retail). The pope’s “drip,” or style, was the talk of the internet. The only problem? The image wasn’t real—it was created by a construction worker in Chicago, who was tripping on shrooms while using the AI image-generation tool <a id="idTextAnchor001"/>Midjourney, and thought it would be funny to see Pope Francis dripped out <a class="url" href="https://www.buzzfeednews.com/article/chrisstokelwalker/pope-puffy-jacket-ai-midjourney-image-creator-interview">[1]</a>.</p>
<p class="body">Although the “Balenciaga Pope” meme was harmless fun, it fooled many users. Model and author Chrissy Teigen tweeted, “I thought the pope’s puffer jacket was real and didn’t give it a second thought. no way am I surviving the future of technology” <a class="url" href="https://twitter.com/chrissyteigen/status/1639802312632975360">[2]</a>. But the future of technology is here, and AI-generated media is quickly becoming indistinguishable from the forms it imitates. In this chapter, we’ll discuss the methods, risks, opportunities, and legal landscape of synthetic media, one of the foremost applications for LLMs and other types of generative AI.</p>
<h2 class="fm-head" id="heading_id_3"><a id="idTextAnchor002"/>The rise of synthetic media</h2>
<p class="body">Synthetic media, or more specifically, AI-generated media, is an umbrella term for content that has been created or altered with the help of AI. It’s sometimes used synonymously with “<a id="idTextAnchor003"/>deepfake” visual technology, but synthetic content (as shown in figure 4.1) is much broader and can span text, image, video, voice, and data. The term <i class="fm-italics">deepfak</i><i class="fm-italics">e</i>—a portmanteau of “deep learning” and “fake”—was coined by a Reddit user in 2017 who used face-swapping technology to alter pornographic videos <a class="url" href="https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained">[3]</a>. Deepfakes narrowly refer to faking a particular person’s physical characteristics or voice, most often to “fake” others into believing an event happened.<a id="idIndexMarker000"/><a id="idIndexMarker001"/><a id="marker-74"/></p>
<p class="fm-callout"><span class="fm-callout-head">Synthetic media</span> or, more specifically, AI-generated media, is an umbrella term for content that has been created or altered with the help of AI, which spans text, image, video, voice, and data.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="304" src="../../OEBPS/Images/CH04_F01_Dhamani.png" width="392"/></p>
<p class="figurecaption">Figure 4.1 The landscape of synthetic media<a id="idTextAnchor004"/></p>
</div>
<p class="body">Initially, deepfakes referred to a form of synthetic media in which a person in an image or video is replaced with someone else, but it has since expanded to include synthetic media applications, such as realistic-looking images of people who don’t exist, synthetic audio or video recordings that mimic a target, or targeted propaganda that resemble real news articles. Deepfakes have generally had a negative connotation, with prominent examples including a fake video of President Biden announcing a draft to send American soldiers to Ukraine (see <a class="url" href="http://mng.bz/p1Q2">http://mng.bz/p1Q2</a>); Mark Zuckerberg saying “whoever controls the data, controls the future” in an edited video (see <a class="url" href="http://mng.bz/OPVo">http://mng.bz/OPVo</a>); or Donald Trump’s viral deepfake asking Belgium to exit the Paris climate agreement (see <a class="url" href="http://mng.bz/YR8K">http://mng.bz/YR8K</a>). In fact, 9 out of 10 Americans believe that deepfakes could cause more harm than good <a class="url" href="https://thesentinel.ai/media/Deepfakes%202020:%20The%20Tipping%20Point,%20Sentinel.pdf">[4]</a>. As we’ll discuss, there are a number of potentially beneficial applications and use cases, so people in the space have been increasingly using the term <i class="fm-italics">AI-generated media</i>, or <i class="fm-italics">AI-generated synthetic media</i>, to move away from the negative connotation of the term <i class="fm-italics">deepfakes</i>.</p>
<h3 class="fm-head1" id="heading_id_4">Popular techniques for creating synthetic media</h3>
<p class="body"><a id="marker-75"/>We’ve previously discussed how large language models (LLMs) are used to generate text. Here, we’ll explore two commonly used techniques to alter or create images and videos (since videos are just sequences of images). The first technique, autoencoders, uses neural networks to compress and decompress images. You may remember the encoder-decoder framework from chapter 1, where text is encoded into a numeric representation for use by the model and then decoded back into a readable output. Similarly, an image can be fed into an encoder, which creates a compressed version of the same file. This compressed version of the file, also referred to as latent features or latent representation, contains a set of patterns that represent the characteristics of the original image. <a id="idIndexMarker002"/><a id="idIndexMarker003"/><a id="idIndexMarker004"/><a id="idIndexMarker005"/></p>
<p class="body">Let’s say that we passed an image of someone’s face through the encoder. Then, the latent features could include facial characteristic patterns such as expression, face angle, skin tone, and so on. These features are then passed into a decoder, which reconstructs the image based on the latent features. Autoencoders are often used in face-swapping technology, where the same encoder is used to create latent features from both faces, and then separate decoders are used to create the images from the latent features to best rebuild the original image. In figure 4.2, the same encoder creates the latent representations of Original Face A and Original Face B. Then, the decoder trained to rebuild Face B is fed the facial latent features of Face A (same encoder) to generate a seamless blend of the two faces. For example, the decoder can map characteristics such as the eyes, nose, mouth, and lighting to mix the two faces<a id="idTextAnchor005"/>.<a id="marker-76"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="459" src="../../OEBPS/Images/CH04_F02_Dhamani.png" width="725"/></p>
<p class="figurecaption">Figure 4.2 Deepfake creation through the use of autoencoders with a single encoder and two decoders</p>
</div>
<p class="body">The second technique for generating synthetic media<a id="idTextAnchor006"/> is Generative Adversarial Networks (GANs), which consist of two neural networks—a generator and a discriminator. For example, suppose there is a shop that buys authentic artworks that they later resell. But there is a criminal who sells fake artworks to make money. Initially, the criminal might make mistakes when trying to sell fake artworks, so the shop owner might be able to identify that it’s not an authentic artwork. Then, the criminal will likely learn what characteristics of the artwork the shop owner is looking at to determine if it’s real or not, so the criminal can use that knowledge to improve the process by which artworks can be sold as fake to eventually be successful. At the same time, when the shop owner accidentally buys and tries to resell some of the fake artworks, they would get feedback from customers or experts that some of their art pieces are counterfeit, so the shop owner also has to learn how to better distinguish between the fake and real artworks. <a id="idIndexMarker006"/><a id="idIndexMarker007"/></p>
<p class="body">As shown in figure 4.3, the goal of the criminal (generator) is to create fake artworks that are indistinguishable from real ones, while the goal of the shop owner (discriminator) is to be able to distinguish between real and fake artworks—this competitive feedback loop is the main idea behind GANs. The generator exists to create new data, such as images, and the discriminator verifies the authenticity of an image by comparing it to the training dataset to determine the difference between a fake and a real image. The ultimate goal of a generative network is to create images that are indistinguishable from authentic images<a id="idTextAnchor007"/>.<a id="marker-77"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="353" src="../../OEBPS/Images/CH04_F03_Dhamani.png" width="616"/></p>
<p class="figurecaption">Figure 4.3 Creation of GANs using a generator and discrimina<a id="idTextAnchor008"/>tor</p>
</div>
<h3 class="fm-head1" id="heading_id_5">The good and the bad of synthetic media</h3>
<p class="body">In Samsung NEXT’s “Synthetic Media Landscape” report, they argue that “this technology will transform the way we produce, consume, and distribute media.” They claim that synthetic media is the third evolutionary stage of media. The first, old media, made possible through broadcasting, enabled <i class="fm-italics">mass distribution</i> for a select few through TV, radio, and print. The second, new media, made possible through the internet, enabled <i class="fm-italics">democratized distribution</i> for everyone through social media. The third, synthetic media, made possible through AI and deep learning, will <i class="fm-italics">democratize media creation</i> and creativity for everyone. Samsung’s report highlights an important point here—synthetic media will democratize content creation <a class="url" href="https://www.syntheticmedialandscape.com/">[5]</a>. Now, anyone can produce high-quality content at low costs. This could democratize small-scale creators who could use synthetic media technology in the image/video synthesis space to bring their imagination to life without access to large film budgets. As we’ll discuss in the next section, we believe that synthetic media will usher in a new wave of creativity and art. <a id="idIndexMarker008"/><a id="idIndexMarker009"/><a id="idIndexMarker010"/><a id="idIndexMarker011"/><a id="idIndexMarker012"/><a id="marker-78"/></p>
<p class="body">Another potential benefit of synthetic media is its ability to anonymize photos and videos to enhance privacy. In an HBO documentary about anti-gay and lesbian purges, <i class="fm-italics">Welcome to Chechnya</i>, the film uses deepfake technology to guard the identities of the volunteers who told their stories to protect them from prosecution <a class="url" href="https://www.nytimes.com/2020/07/01/movies/deepfakes-documentary-welcome-to-chechnya.xhtml">[6]</a>. Similarly, we could also use synthetic media technology to anonymize our faces in images and videos on cameras in public spaces, retail stores, and social media accounts. Face anonymization can be used for privacy protection while preserving data utility.</p>
<p class="body">On the other hand, AI-generated media can also be a cause for concern. We can use the same technology to generate content (text, video, image, or speech) that is adversarial in nature. Malicious actors can disseminate intentionally misleading and adversarial narratives, which can disrupt discourse, create divisions, and undermine our trust in scientific, social, political, and economic institutions. The phenomenon of “seeing is believing” can also enable altered or inauthentic images and videos to spread more quickly. In this vein, in an article titled, “Deep Fakes: A Looming Challenge for Privacy, Democracy, and Social Security,” researchers identify a notable danger that they have termed <i class="fm-italics">the liar’s dividend</i>. Here, the idea is that as the general public becomes more aware of how convincingly synthetic media can be generated, they may become more skeptical of the authenticity of traditional real documentary evidence <a class="url" href="https://doi.org/10.2139/ssrn.3213954">[7]</a>. We’ll discuss dis/misinformation, and its implications on individuals and society, in detail in chapter 5. <a id="idIndexMarker013"/></p>
<p class="body">Synthetic media has also infamously been used for celebrity pornographic videos, revenge porn or cybersexual harassment, and fraud and espionage. Deepfakes can be used to impersonate an authorized decision-maker for financial transactions and various cybersecurity problems, such as showing an executive committing a crime or creating fake financial statements. Finally, celebrities can also be synthetically generated for brand advertisements, which can result in a loss of intellectual property (IP) revenue. Later in this chapter, we’ll talk about IP and copyright problems relate<a id="idTextAnchor009"/>d to LLMs.</p>
<h3 class="fm-head1" id="heading_id_6">AI or genuine: Detecting synthetic media</h3>
<p class="body"><a id="marker-79"/>There are various ongoing efforts to detect AI-generated media. In early 2023, OpenAI released a work-in-progress classifier to distinguish between machine-generated and human-written text to help mitigate concerns about running automated misinformation campaigns, among other problems. They acknowledged that their “classifier is not fully reliable” by correctly identifying AI-written text 26% of the time (true positives) and incorrectly labeling the human-written text as AI-written text 9% of the time (false positives). As of July 20, 2023, the classifier was taken down due to its low accuracy rate <a class="url" href="https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text">[8]</a>.<a id="idIndexMarker014"/><a id="idIndexMarker015"/></p>
<p class="body">Researchers have explored various techniques to detect machine-generated or manipulated images, videos, and speech, including digital, physical, and semantic analysis. In the Media Forensics (MediFor) program from the Defense Advanced Research Projects Agency (DARPA), researchers produced manipulation indicators by looking for inconsistencies in pixel representation and the physical environment, in combination with the semantic interpretation of the media <a class="url" href="https://www.darpa.mil/program/media-forensics">[9]</a>. Are there any pixel-level errors? That is, are there blurred edges or replicated pixels? For the physical environment, they look to see if the laws of physics are violated—are the shadows, reflections, lighting, and so on consistent with the laws of nature? Finally, they look at semantic integrity, which helps determine if the contextual information related to the piece of content is contradictory or inconsistent. So, they look for whether the image has been placed out of context or repurposed, and whether there are any date and time inaccuracies <a class="url" href="https://www.youtube.com/watch?v=Crfm3vGoBsM">[10]</a>. This program was followed by DARPA’s Semantic Forensics (SemaFor) with the goal of not only detecting manipulated media but also characterizing if the media was generated or manipulated for malicious purposes, and attributing the origination of the content to an individual or organization <a class="url" href="https://www.darpa.mil/news-events/2021-03-02">[11]</a>. <a id="idIndexMarker016"/><a id="idIndexMarker017"/><a id="idIndexMarker018"/><a id="idIndexMarker019"/><a id="idIndexMarker020"/><a id="idIndexMarker021"/></p>
<p class="body"><a id="marker-80"/>Similarly, there have been numerous studies on detecting face swapping by analyzing photo response nonuniformity (PRNU) <a class="url" href="https://www.researchgate.net/profile/Zeno-Geradts/publication/329814168_Detection_of_Deepfake_Video_Manipulation/links/5c1bdf7da6fdccfc705da03e/Detection-of-Deepfake-Video-Manipulation.pdf">[12]</a> and inconsistent artifacts in images and videos, such as facial characteristics or physiological signals <a class="url" href="https://arxiv.org/pdf/1806.02877.pdf">[13]</a> and image quality <a class="url" href="https://ieeexplore.ieee.org/abstract/document/8987375">[14]</a>. These techniques are promising but often limited, with solutions consisting of only detecting facial manipulations in a curated dataset. One study showed that entire generated faces can be detected via irregular pupil shapes, but the assumption of pupil shape regularity doesn’t always hold <a class="url" href="https://arxiv.org/pdf/2109.00162.pdf">[15]</a>. Other techniques to detect deepfakes include physiological analysis in videos to estimate whether the individual’s breathing and heart rate are normal <a class="url" href="https://doi.org/10.1007/978-3-030-87664-7_12">[16]</a>, and biometric analysis to analyze a specific individual’s mannerisms, including movement and style of speech, which can then be compared to distinguish fake from real <a class="url" href="https://doi.org/10.3390/jimaging9010018">[17]</a>. Biometric analysis has also been applied to deepfake audio detection, where audio analysis has proven to be effective in detecting deepfakes <a class="url" href="https://arxiv.org/pdf/2209.14098.pdf">[18]</a>. <a id="idIndexMarker022"/><a id="idIndexMarker023"/></p>
<p class="body">Because of their adversarial nature, no single magic bullet can detect <i class="fm-italics">all</i> the deepfakes <i class="fm-italics">all</i> the time, and a majority of detection techniques tend to have a low generalization capability—if they encounter a novel manipulation type that hasn’t been seen in the training dataset, then their performances drop significantly <a class="url" href="https://doi.org/10.3390/jimaging9010018">[17]</a>. While there has been significant progress in deepfake detection and notable solutions for addressing certain artifacts of synthetic media generation, we hope that efforts to raise awareness will motivate researchers to solve the shortcomings of current datasets used for testing these techniques, as well as developing techniques to perform well across various kinds of deepfake manipulation and generation. At some point, it will likely become extremely difficult, perhaps impossible, to confidently detect manipulated media at scale purely based on specific image characteristics.</p>
<p class="body">While technical solutions are certainly crucial to countering AI-generated and manipulated media, they don’t solve the problem in its entirety. Media literacy efforts to educate and inform the public are also essential steps to effectively respond to this problem. For visual deepfakes, such as images and videos, we can use artifacts of the generated images to help distinguish them from real images.<a id="marker-81"/></p>
<p class="body">While there isn’t a single tell-tale sign, image manipulations often use facial transformations, where we can pay attention to cheeks, forehead, eyes, eyebrows, lips, and facial hair. We can ask questions like these: Is the agedness of the skin consistent with the agedness of other facial features? Is the skin tone uneven? Are the shadows expected? Do facial hair transformations look natural? Is there not enough or too much glare with glasses? Does the person blink enough or too much? Do lip movements look natural? AI-generated images have also historically generated too many fingers on hands, given that hands are less visible than faces in many human images, which is what these models are trained on. In videos, the facial expressions or movements may not exactly line up with the voice. Generally, we’re looking for distortions with visual deepfakes. Additionally, media literacy efforts should emphasize understanding the source and context behind the content shared. Understanding the content’s origination, credibility, and context can help us decipher how much attention it should receive.</p>
<p class="body">Finally, as discussed in chapter 3, appropriate legislation to govern the use of the technology and how it’s distributed will be fundamental to the responsible use and dissemination of synthetic media. The United States alone has introduced several synthetic media bills, especially concerning pornographic content and manipulation of the democratic process <a class="url" href="https://www.malwarebytes.com/blog/news/2020/01/deepfakes-laws-and-proposals-flood-us">[19]</a>. In parallel, social media companies, including Facebook, Twitter, Reddit, YouTube, and TikTok, have developed content-moderation policies to ban any deepfakes with malicious intent <a id="idTextAnchor010"/>on their platforms. <a id="idIndexMarker024"/><a id="idIndexMarker025"/><a id="idIndexMarker026"/><a id="idIndexMarker027"/></p>
<h2 class="fm-head" id="heading_id_7">Generative AI: Transforming creative workflows</h2>
<p class="body">In June 2022, <i class="fm-italics">Cosmopolitan</i> fashion magazine unveiled the first cover made entirely by generative AI <a class="url" href="https://www.cosmopolitan.com/lifestyle/a40314356/dall-e-2-artificial-intelligence-cover/">[20]</a>. Synthetic media has opened up a new realm of possibilities for content creators. It has transformed creative work by eliminating monotonous tasks, increasing productivity and efficiency, and enabling people to express their creativity in new and unprecedented ways. From marketing and virtual influencers to art and film, we’ll unpack several creative applications of synthetic <a id="idTextAnchor011"/>media in this section. <a id="idIndexMarker028"/><a id="idIndexMarker029"/><a id="idIndexMarker030"/><a id="marker-82"/></p>
<h3 class="fm-head1" id="heading_id_8">Marketing applications</h3>
<p class="body">Marketing applications are perhaps the most common commercial use case for generative AI. There are countless examples of how individuals and brands are using synthetic media to create content for marketing purposes, accelerating the delivery of personalized content while adhering to a brand’s style and tone. They range from creating social media and blog posts to developing marketing videos and visual branding. Jasper (see <a class="url" href="https://www.jasper.ai/">www.jasper.ai/</a>), an AI content platform based on a collection of third-party models (including OpenAI’s GPT-3.5) and their own, is focused on content creation for businesses. It can produce various types of customer-facing content, including social media posts, website copy, emails, blogs, ads, and imagery. Jasper also can move between different formats, tones, and languages. The Jasper website boasts that they are “trusted by 100,000+ teams globally at innovative companies.”<a id="idIndexMarker031"/><a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="idIndexMarker034"/></p>
<p class="body">Some brands are using DALL-E 2 and other image-generation tools for advertising. DALL-E 2 is an OpenAI model that can generate realistic images and art given a natural language description <a class="url" href="https://openai.com/product/dall-e-2">[21]</a>. Heinz put together a marketing campaign, “AI Ketchup,” based on OpenAI’s DALL-E 2—<i class="fm-italics">EVEN A.I. KNOWS THAT KETCHUP IS HEINZ</i> <a class="url" href="https://www.youtube.com/watch?v=LFmpVy6eGXs">[22]</a>. As shown in figure 4.4, when we asked DALL-E 2 to create a series of generic ketchup-inspired pieces, the pictures overwhelmingly represented elements of Hei<a id="idTextAnchor012"/>nz’s signature branding.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="225" src="../../OEBPS/Images/CH04_F04_Dhamani.png" width="675"/></p>
<p class="figurecaption">Figure 4.4 From left to right, prompts to DALL-E 2: an impressionist painting of a ketchup bottle, a five-year-old’s drawing of a ketchup bottle, and an astronaut in space holding a ketchup bottle</p>
</div>
<p class="body">Nestlé used DALL-E’s Outpainting feature, which helps users extend an image beyond its original borders by adding visual elements in the same style (see <a class="url" href="http://mng.bz/z0JX">http://mng.bz/z0JX</a>). They advertised an extended version of Johannes Vermeer’s famous painting, <i class="fm-italics">The Milkmaid</i>, generated by DALL-E’s Outpainting feature, which was used to help sell Nestlé’s yogurt and dessert brand, La Laitière. The ad, created by Ogilvy Paris (see <a class="url" href="http://mng.bz/G98R">http://mng.bz/G98R</a>), a creative communications agency, extends the world of the original painting to show the kitchen maid preparing La Laitière–inspired treats <a class="url" href="https://www.adweek.com/creativity/nestle-brand-is-latest-to-venture-into-brave-new-world-of-ai-art-direction/">[23]</a>. Going back to the earlier example of an astronaut holding a ketchup bottle, we asked DALL-E Outpainting to extend the image<a id="idTextAnchor013"/>, as shown in figure 4.5.<a id="marker-83"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="300" src="../../OEBPS/Images/CH04_F05_Dhamani.png" width="450"/></p>
<p class="figurecaption">Figure 4.5 The result of DALL-E’s Outpainting feature given the prompt “a burger in outer space without ketchup”</p>
</div>
<p class="body">Creative agencies aren’t the only ones using generative AI for marketing applications—Ryan Reynolds, a Canadian American actor, asked ChatGPT to write a commercial for Mint Mobile in his voice using a joke, a curse word, and a callout to Mint’s holiday promo <a class="url" href="https://www.fastcompany.com/90833253/ryan-reynolds-used-chatgpt-to-make-a-mint-mobile-ad-and-the-results-were-mildly-terrifying">[24]</a>.</p>
<p class="body">As of May 2023, 19-year-old Miquela Sousa has 2.8 million followers on Instagram and 3.6 million followers on TikTok. More famously known as Lil Miquela, she is one of <i class="fm-italics">TIME Magazine</i>’s 25 Most Influential People on the Internet and is known to support Black Lives Matter, reproductive rights, and LGBTQ+ causes. She has also appeared in Calvin Klein ads, alongside American model Bella Hadid <a class="url" href="https://www.elle.com/uk/fashion/a27492073/bella-hadid-calvin-klein-lil-miquela/">[25]</a>. But Lil Miquela isn’t real—she is the most famous example of a virtual influencer, created by LA-based startup, Brud. Lil Miquela’s creators closed a $125 million Series B round in 2019 taking a bet on virtual influencers becoming the future of ads, fashion, and commerce <a class="url" href="https://techcrunch.com/2019/01/14/more-investors-are-betting-on-virtual-influencers-like-lil-miquela/">[26]</a>. Generative AI has increased the creation of virtual influencers, quickly being adopted in the workflows of their content production pipeline. Esther Olofsson, a Swedish virtual influencer, us<a id="idTextAnchor014"/>es four AI tools, including Stable Diffusion (a text-to-image model) to generate 3D images of Esther, and ChatGPT to generate her captions on Instagram. Creators of virtual influencers believe that synthetic media can scale their creative output and earning power, with the ability to generate a boundless amount of content without the real-world constraints of human influencers. Yet, virtual influencers also raise ethical questions for their creators, specifically around cultural appropriation and representation for creators who create virtual influencers with different demographic characteristics than their own. Virtual dark-skinned influencer, Shudu Gram, has been critiqued as “contrived by a white man who has noticed the ‘movement’ of dark-skinned women” by social theorist Patri<a id="idTextAnchor015"/>cia Hill Collins <a class="url" href="https://journals.sagepub.com/doi/full/10.1177/1527476420983745">[27]</a> <a class="url" href="https://www.newyorker.com/culture/culture-desk/shudu-gram-is-a-white-mans-digital-projection-of-real-life-black-womanhood">[28]</a>.<a id="marker-84"/></p>
<h3 class="fm-head1" id="heading_id_9">Artwork creation</h3>
<p class="body">Artistic creation is another area that has been disrupted by generative AI. In 2018, the <i class="fm-italics">Portrait of Edmond Belamy</i> was the first widely covered sale of an AI-generated artwork. The fictional portrait, created by Obvious, a Paris-based collective, was sold for a whopping $432,500 <a class="url" href="https://news.artnet.com/market/first-ever-artificial-intelligence-portrait-painting-sells-at-christies-1379902">[29]</a>. <a id="idIndexMarker035"/><a id="idIndexMarker036"/><a id="idIndexMarker037"/><a id="idIndexMarker038"/></p>
<p class="body">While algorithms have been used to generate art since the 1960s <a class="url" href="https://www.researchgate.net/publication/311104742_Algorithmic_Art_and_Its_Art-Historical_Relationships">[30]</a>, AI-generated art can produce art (image, film/video, and music) without an explicit set of programming instructions that have been provided by human artists. AI tools such as DALL-E 2, St<a id="idTextAnchor016"/>able Diffusion, Midjourney, and WOMBO Dream can be used to quickly create artworks given any descriptive text input. Although some artists have expressed concerns about copyright problems with these tools (explored later in section 4.3), they have also been a source of creative inspiration for many. Creators have used DALL-E to create fan art, comic books, and design sneakers (someone made a pair for Sam Altman, cofounder of OpenAI, after he tweeted them <a class="url" href="https://twitter.com/sama/status/1539670012536844289">[31]</a>). Tattoo artists are using DALL-E to generate tattoo designs together with their clients, while animation studios are using DALL-E to design characters and environments <a class="url" href="https://www.technologyreview.com/2022/12/16/1065005/generative-ai-revolution-art/">[32]</a>.</p>
<p class="body">Another well-known AI art generator tool is Google’s DeepDream, which takes an image as an input and outputs abstract, psychedelic art. The core idea behind generating these psychedelic images is to ask the network: “whatever you see there, I want more of it!” (see <a class="url" href="http://mng.bz/0lYl">http://mng.bz/0lYl</a>). In practice, this means that the model amplifies any patterns that it sees in the image. Figure 4.6 illustrates this idea by using the example image from DALL-E Outpainting (refer to figure 4.5) as<a id="idTextAnchor017"/> a base image for DeepDream.<a id="idIndexMarker039"/><a id="marker-85"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="295" src="../../OEBPS/Images/CH04_F06_Dhamani.png" width="450"/></p>
<p class="figurecaption">Figure 4.6 DeepDream applied to figure 4.5 with the input prompt, “a portrait of a beautiful female knight in silver armor with intricate golden details”</p>
</div>
<p class="body">Filmmakers have also been provided with new tools for creative possibilities. Generative AI is changing the way films are conceptualized, developed, and produced. The Writers Guild of America (WGA) is the first labor organization to take on generative AI—“The challenge is we want to make sure that these technologies are tools used by writers and not tools used to replace writers,” says John August, a member of the WGA’s 2023 negotiating committee <a class="url" href="https://www.hollywoodreporter.com/business/business-news/writers-strike-ai-chatgpt-1235478681/">[33]</a>. Filmmakers can generate scripts, storyboards, and scenes—as previously discussed, independent filmmakers can use generative AI to create compelling stories and visual elements without the need for a large budget, while studios can draw inspiration from these tools and use them to streamline content. Generative AI can also be used for improved visual effects by creating enhanced characters and environments without a manual labor-intensive process. <a id="idIndexMarker040"/><a id="idIndexMarker041"/><a id="marker-86"/></p>
<p class="body">A controversial application is the ability to render the dead digitally. In the 2016 film, <i class="fm-italics">Rogue One: A Star Wars Story</i>, filmmakers used face-swapping technology to digitally recreate the character played by the late Peter Cushing, who died in 1994 <a class="url" href="https://www.polygon.com/2016/12/27/14092060/rogue-one-star-wars-grand-moff-tarkin-princess-leia">[34]</a>. As for the ethics of digitally resurrecting dead actors, John Knoll, <i class="fm-italics">Rogue One: A Star Wars Story</i>’s visual effects supervisor, said, “We weren’t doing anything that I think Peter Cushing would’ve objected to. I think this work was done with a great deal of affection and care. We know that Peter Cushing was very proud of his involvement in Star Wars and had said as much, and that he regretted that he never got a chance to be in another Star Wars film because George [Lucas] had killed off his character” <a class="url" href="https://www.theguardian.com/film/2017/jan/16/rogue-one-vfx-jon-knoll-peter-cushing-ethics-of-digital-resurrections">[35]</a>. Filmmakers are also using generative AI to accelerate the postproduction workflow with assistance in editing footage, applying visual effects, sound design, and more. Finally, as with every industry, filmmakers can use generative AI for creative inspiration.</p>
<p class="body">Generative AI has also been a source of inspiration for architects and designers—one such example is the project, <i class="fm-italics">This House Does Not Exist</i> (see <a class="url" href="https://thishousedoesnotexist.org">https://thishousedoesnotexist.org</a>), which generates AI renderings of homes and buildings that don’t currently exist. AI-generated tools are making strides in architecture, with designers using them to rapidly iterate solutions that can then be augmented and tested using existing tools <a class="url" href="https://www.elledecor.com/life-culture/a42711299/generative-ai-design-architecture/">[36]</a>.<a id="idIndexMarker042"/></p>
<p class="body">In a similar vein, musicians are also exploring how humans and machines can collaborate, rather than compete. Pianist David Dolan performed with a semiautonomous AI system at the Stockholm University of the Arts, showing how generative AI may creatively supplement music <a class="url" href="https://www.youtube.com/watch?v=sIFbvgmYBA0">[37]</a>. The AI system was designed and overseen by Kingston University researcher Oded Ben-Tal, who says that musicians can use AI with pianists to improvise outside of their skillset or draw inspiration from AI compositions, for now <a class="url" href="https://www.wired.co.uk/article/generative-ai-music">[38]</a>.</p>
<p class="body">Musician Holly Herndon also used AI to clone her voice, dubbed Holly+, which she uses to sing in languages and styles she is unable to <a class="url" href="https://www.youtube.com/watch?v=5cbCYwgQkTE">[39]</a>. Holly+ is free to use by anyone, with Herndon and her team developing tools for anyone to be able to make art with her image and voice (see <a class="url" href="https://holly.plus/">https://holly.plus/</a>). Sir Paul McCartney and The Beatles released a new tune, “Now and Then,” in November 2023, by using generative AI to resurrect the voice of fellow bandmate, John Lennon <a class="url" href="https://www.cnn.com/2023/06/13/entertainment/paul-mccartney-ai-beatles-song/index.xhtml">[40]</a>. While these tools present an opportunity for musicians, some are worried about AI-generated music flooding streaming platforms and competing with real musicians. There are, of course, copyright concerns as well, which we’ll discuss in the next section. Universal Media Group, which backs superstars such as Taylor Swift and Nicki Minaj, urged Spotify and Apple Music to prohibit AI tools from scraping copyrighted songs <a class="url" href="https://www.ft.com/content/aec1679b-5a34-4dad-9fc9-f4d8cdd124b9">[41]</a>.<a id="marker-87"/></p>
<p class="body">There is an ongoing debate about whether AI-generated art should be considered art in the same way that human-generated art is, whether artists will be replaced, and, more broadly, what this means for creativity. In defense of AI-generated art, artists argue that the AI tool is a medium of conveying the significance or meaning that lies in the human mind, similar to a brush and a palette or a camera. Anna Ridler, an artist known for her work with GANs, believes that the idea of replacing artists comes from undermining the artistic process—she says:</p>
<p class="fm-quote">AI can’t handle concepts: collapsing moments in time, memory, thoughts, emotions—all of that is a real human skill, that makes a piece of art rather than something that visually looks pretty. <a class="url1" href="https://www.theguardian.com/technology/2022/nov/12/when-ai-can-make-art-what-does-it-mean-for-creativity-dall-e-midjourney">[42]</a></p>
<p class="body">Instead of replacing artists, AI-generated art can be understood as a collabor<a id="idTextAnchor018"/>ation between humans and machines.<a id="idIndexMarker043"/><a id="idIndexMarker044"/><a id="idIndexMarker045"/><a id="idIndexMarker046"/><a id="idIndexMarker047"/><a id="idIndexMarker048"/></p>
<h2 class="fm-head" id="heading_id_10">Intellectual property in the LLM era</h2>
<p class="body">While synthetic media pushes the boundaries of art, the tools and models used to create it are testing the boundaries of the legal system. In the following section, we’ll take a look at the relevant policy governing the collection of open web data, including text and images, and the generation of synthetic media us<a id="idTextAnchor019"/>ing models trained on those collections.<a id="idIndexMarker049"/></p>
<h3 class="fm-head1" id="heading_id_11">Copyright law and fair use</h3>
<p class="body">Pablo Picasso, one of the most renowned painters of the 20th century, allegedly said, “Good artists copy; great artists steal” <a class="url" href="https://www.bbc.com/culture/article/20141112-great-artists-steal">[43]</a>. It’s common practice in the literary and fine arts to imitate the styles of others, and it’s often seen as a prerequisite for creative success. Of course, such imitation has its limits, which are encoded into law as intellectual property (IP). The conception of IP as a type of property over which one could claim legal ownership dates back to England in the 17th century <a class="url" href="https://www.eff.org/issues/intellectual-property/the-term">[44]</a>. In the United States, Section 8 of Article I of the Constitution reads that Congress shall have the power <a id="idIndexMarker050"/><a id="idIndexMarker051"/><a id="idIndexMarker052"/><a id="marker-88"/></p>
<p class="fm-quote">to promote the progress of science and useful arts, by securing for limited times to authors and inventors the exclusive right to their respective writings and discoveries. <a class="url1" href="https://www.archives.gov/founding-docs/constitution-transcript">[45]</a></p>
<p class="body">While there are several different types of IP protections—patents for inventions, trademarks for corporate logos and symbols, trade secrets for proprietary information such as the formula for Coca-Cola—the most contentious legal questions for generative AI are about the potential copyright infringements in model training and model generations.</p>
<p class="body">Copyrights are exclusive rights to a work of creative expression, whether that’s an image, a text, a movie, or a song. Typically, the owner of the copyright is the only one authorized to copy, distribute, display, or perform the work for a limited period of time, after which the work enters the public domain (in the United States, the copyright dates from the time that the work is created, and the standard term lasts until 70 years after the death of the creator) <a class="url" href="https://www.copyright.gov/help/faq/faq-duration.xhtml">[46]</a>. The US Copyright Office has stated their policy to be that text, images, and other media generated by AI aren’t eligible for copyright protections, although works by humans that have AI-generated elements might be, as long as there is sufficient human creativity involved <a class="url" href="https://www.federalregister.gov/documents/2023/03/16/2023-05321/copyright-registration-guidance-works-containing-material-generated-by-artificial-intelligence">[47]</a>. The most pressing current legal question around LLMs, as well as generative image models, isn’t whether their work is copyrightable, but whether they are actually violating existing copyrights of artists and writers whose works comprise their training data.</p>
<p class="fm-callout"><span class="fm-callout-head">Copyrights</span> are exclusive rights to a work of creative expression, whether that’s an image, a text, a movie, or a song.</p>
<p class="body">Despite copyrights offering exclusive rights for use, these rights are by no means absolute. <i class="fm-italics">Fair use</i> is the legal doctrine that outlines when it’s acceptable to use copyrighted material without requiring the permission of the holder of the copyright <a class="url" href="https://www.copyright.gov/fair-use/">[48]</a>. For example, courts have typically considered parody to be fair use, which is why “Weird Al” Yankovic can commercially sell melodic duplicates (e.g., “Eat It” and “Like a Surgeon”) of copyrighted songs with his own comical lyrics (though Yankovic states on his website that he gets permission from the original writers anyway to maintain relationships he has built over the years). <a class="url" href="https://www.weirdal.com/archives/faq/">[49]</a> As defined in the US Copyright Act of 1976, fair use hinges<a id="idTextAnchor020"/> on four factors, as shown in figure 4.7.<a id="marker-89"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="195" src="../../OEBPS/Images/CH04_F07_Dhamani.png" width="345"/></p>
<p class="figurecaption">Figure 4.7 The four factors that determine fair use of copyrighted materials <a class="url" href="https://www.law.cornell.edu/uscode/text/17/107">[50]</a></p>
</div>
<p class="body">The first factor, “the purpose and character of the use,” refers to how and why the copyrighted material is used. Commercial use is less likely to be deemed fair as compared to nonprofit or educational purposes—for example, a college professor could distribute printouts of a painting for an art history lecture, but you might get in trouble for selling T-shirts with that same painting printed on them. “Transformative use” is another case that falls under this first factor. Essentially, US courts have found that when the character of the use is <i class="fm-italics">transformative</i>, adding a new element that fundamentally changes the work, that isn’t a copyright violation. Transformative use also hinges on the derivate work being used for a purpose that is different from the consumption or enjoyment of the original work and is an important defense for companies that develop LLMs.<a id="idIndexMarker053"/></p>
<p class="body"><a id="marker-90"/>The second factor, “the nature of the copyrighted work,” refers to the varying degrees of protection that different types of materials enjoy. Because the original intent of copyright was to incentivize free and creative expression, the use of more “creative” works, such as songs, plays, and novels, is more likely to be deemed fair use as compared to factual or technical copyrighted works. In other words, you could argue that referencing lines of poetry in a new verse is fair use, but it’d be harder to do the same for a piece of investigative reporting.</p>
<p class="body">The third factor assesses how much of the original source material was reused. If it’s a substantial portion or nearly all of it, that is less likely to be deemed fair use than a small amount.</p>
<p class="body">The fourth and final factor refers to if and how the use of copyrighted material will affect the market for that work. If an unauthorized seller is distributing a new movie online, for example, that would pose a serious threat to the digital sales or streaming revenue for that movie. Uses that hurt the market for the original work are unlikely to be considered fair <a class="url" href="https://www.copyright.gov/fair-use/">[48]</a>.</p>
<p class="body">If all of this seems a little blurry, that’s because it is—none of these single factors are hard-and-fast rules, and they are all weighed against each other if a copyright suit is brought. Before turning to the lawsuits that have been brought already against developers of LLMs, though, let’s first examine a case that hinges similarly on the use of vast amounts of copyrighted text from the internet: <i class="fm-italics">Authors Guild v. Google</i> <a class="url" href="https://www.copyright.gov/fair-use/summaries/authorsguild-google-2dcir2015.pdf">[51]</a>.<a id="idIndexMarker054"/></p>
<p class="body">In 2015, Google collaborated with several major research libraries to digitize their collections of books—some 20 million volumes. The tech giant accessed the books through partnerships, scanned them, and allowed people to search them for text snippets, all without the permission of the copyright owners, and without paying licensing fees. The case made it to the Second Circuit Court of Appeals, which concurred with a lower court’s opinion that Google’s digitization efforts constituted fair use because the search functionality gave the public access to information <i class="fm-italics">about</i> the books that they wouldn’t otherwise have, and because even though Google used the full text of the books, they only returned the snippets of matching text, rather than making the entire books available. This concept of using the entirety of source material for a fundamentally different tool is analogous to the training of LLMs.</p>
<p class="body">In general, the LLMs we’ve discussed thus far would seem to be protected by fair use because the model is a very different work than any of the documents, and thus the use of the materials is transformative. Complicating matters, users have shown that it’s occasionally possible to get LLMs to regurgitate text verbatim. It’s difficult to show examples of “memorizing” source material consistently, due to the probabilistic nature of LLMs. Because of the lack of understanding as to exactly what LLMs learn, even their developers are unlikely to be able to say for sure when the model will reproduce phrases or texts word for word. Still, under the precedent of <i class="fm-italics">Authors Guild v. Google</i>, the odds seem considerably in favor of LLMs being considered fair use.<a id="idIndexMarker055"/><a id="marker-91"/></p>
<p class="body">LLMs aren’t the only generative models making a splash in copyright—as mentioned previously, there are impressive generative models capable of creating all types of synthetic media, including images, audio, and videos. Some of the most popular models, including Midjourney and Stable Diffusion, are text-to-image models: users can describe what they want their picture to look like, and the model will generate it for them.</p>
<p class="body">Just like LLMs, generative image models train on huge amounts of data collected from the internet. As with text datasets such as Common C<a id="idTextAnchor021"/>rawl, there are common image datasets, such as LAION-5B, a dataset o<a id="idTextAnchor022"/>f 5.8 billion images compiled by the nonprofit Large-scale Artificial Intelligence Open Network (LAION). LAION-5B is used by Stability AI, the developer of Stable Diffusion, and other companies; it’s made up of images that are publicly available online, including stock photos and editorial photography. One German photographer, upon discovering that some of his stock images were used in LAION-5B, requested that they be removed; LAION responded that to fulfill such a request would be impossible because the database contained only links to images, so nothing was stored, and they could not readily identify which images were from his portfolio. German copyright law—like in many countries—does allow data mining if the data is “lawfully accessible” and deleted later, but the emergence of generative models has brought the problem under greater scrutiny <a class="url" href="https://www.vice.com/en/article/pkapb7/a-photographer-tried-to-get-his-photos-removed-from-an-ai-dataset-he-got-an-invoice-instead">[52]</a>. Stability AI later announced that they would honor opt-out requests from artists whose work was included in the LAION dataset <a class="url" href="https://arstechnica.com/information-technology/2022/12/stability-ai-plans-to-let-artists-opt-out-of-stable-diffusion-3-image-training/">[53]</a>.<a id="idIndexMarker056"/><a id="marker-92"/></p>
<p class="body">Stability is also currently being sued by Getty Images for using more than 12 million photographs from the Getty collection <a class="url" href="https://www.vice.com/en/article/pkapb7/a-photographer-tried-to-get-his-photos-removed-from-an-ai-dataset-he-got-an-invoice-instead">[52]</a>, <a class="url" href="https://fingfx.thomsonreuters.com/gfx/legaldocs/byvrlkmwnve/GETTY%20IMAGES%20AI%20LAWSUIT%20complaint.pdf">[54]</a>. In the complaint, the plaintiffs write:</p>
<p class="fm-quote">At great expense, over the course of nearly three decades, Getty Images has curated a collection of hundreds of millions of premium quality visual assets . . . Many of these images were created by Getty Images staff photographers as works made-for-hire, others have been acquired by Getty Images from third parties with an assignment of its associated copyrights, and the remainder have been licensed to Getty Images by its hundreds of content partners or hundreds of thousands of contributing photographers, who rely on the licensing income Getty Images generates for them. <a class="url1" href="https://fingfx.thomsonreuters.com/gfx/legaldocs/byvrlkmwnve/GETTY%20IMAGES%20AI%20LAWSUIT%20complaint.pdf">[55]</a></p>
<p class="body">The subtext is clear: generative AI models pose an existential threat to Getty and stock photography as an industry. Getty hopes to be compensated for their contributions and perceived copyright infringement, but as with the large text datasets, it’s difficult to ascertain how much information the model retains from any single image, and, again, the use by Stability AI would seem to be transformative.</p>
<p class="body">Amusingly, Getty might have a stronger case due to an artifact of the training data: the complaint further alleges the following:</p>
<p class="fm-quote">Often, the output generated by Stable Diffusion contains a modified version of a Getty Images watermark, creating confusion as to the source of the images and falsely implying an association with Getty Images. While some of the output generated through the use of Stable Diffusion is aesthetically pleasing, other output is of much lower quality and at times ranges from the bizarre to the grotesque. Stability AI’s incorporation of Getty Images’ marks into low quality, unappealing, or offensive images dilutes those marks in further violation of federal and state trademark laws. <a class="url1" href="https://fingfx.thomsonreuters.com/gfx/legaldocs/byvrlkmwnve/GETTY%20IMAGES%20AI%20LAWSUIT%20complaint.pdf">[55]</a></p>
<p class="body">Stable Diffusion or possibly its users could be found in violation of trademarks if the Getty Images watermark appears on images, although Stability AI will undoubtedly move quickly to address this behavior. Altogether, this is a relatively untested area of law.</p>
<p class="body"><a id="marker-93"/>Things get even dicier when a model has not only learned an image captured by a human artist but also encoded the style of that artist. In addition to generating photorealistic renderings, generative models such as Midjourney and Stable Diffusion are also capable of producing artwork in particular styles, as discussed in section Generative AI in Creative Workflows. Style isn’t generally copyrightable, but it’s easy to see how artists might think such imitation could devalue or diminish their work. Sarah<a id="idTextAnchor023"/> Andersen, a prominent cartoonist who publishes webcomics under the “Sarah’s Scribbles” collection, wrote a <i class="fm-italics">New York Times</i> opinion essay about her experience of alt-right internet trolls co-opting her comics by editing the words and frames to change their meaning. Figure 4.8 shows an example of artwork generated by an AI tool in her artistic style—with clearly garbled text, but some visual elements of Andersen’s work present. “When I checked the website <a class="url" href="https://haveibeentrained.com">https://haveibeentrained.com</a>, a site created to allow people to search LAION data sets, so much of my work was on there that it filled up my entire desktop screen,” Andersen attested, and worried that the AI tools would be used to twist her creations again <a class="url" href="https://www.nytimes.com/2022/12/31/opinion/sarah-andersen-how-algorithim-took-my-work.xhtml">[56]</a>. <a id="idIndexMarker057"/></p>
<p class="body">Andersen is one of three plaintiffs, with Karla Ortiz and Kelly McKernan, in a class-action lawsuit brought against Midjourney, Stability AI, and DeviantArt. Like Andersen, McKernan and Ortiz similarly found that the tools could generate images in their styles in a way that felt personally invasive. “They trained these models with our work. They took away our right to decide whether they wanted to b<a id="idTextAnchor024"/>e <a id="idTextAnchor025"/><a id="idTextAnchor026"/>a part of this or not,” said Ortiz <a class="url" href="https://www.nytimes.com/2022/12/31/opinion/sarah-andersen-how-algorithim-took-my-work.xhtml">[56]</a> <a class="url" href="https://www.buzzfeednews.com/article/pranavdixit/ai-art-generators-lawsuit-stable-diffusion-midjourney">[57]</a>.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="620" src="../../OEBPS/Images/CH04_F08_Dhamani.png" width="619"/></p>
<p class="figurecaption">Figure 4.8 An AI-created image using an open source image-generation model with the prompt “Sarah Andersen webcomic”</p>
</div>
<p class="body"><a id="marker-94"/>While it remains to be seen how Andersen, Ortiz, and McKernen’s suit will play out, these tools continue to be used by people around the world to generate and experiment with novel art forms. The permissive structure of fair use means that any substantial changes to the status quo would require a new precedent for use in training AI models. Yet, at the same time, many of the datasets and models that we’re talking about are already open source, meaning that anyone can either train their own model or make a new version of an existing one. Regardless of whether any particular company changes its dataset construction procedure, or ends up paying damages or licensing fees, AI-generated art, f<a id="idTextAnchor027"/>rom comics to music to poetry, is here to stay.<a id="idIndexMarker058"/><a id="idIndexMarker059"/></p>
<h3 class="fm-head1" id="heading_id_12">Open source and licenses</h3>
<p class="body">We’ve mentioned already that due to the enormous scale of data and compute required to produce LLMs, the exercise has thus far largely been left to a few major tech companies and some well-funded startups. That is already changing due to the open source community. <i class="fm-italics">Open source</i> refers to the source code of software being open and available to the public for reuse and modification. More than that, open source is a movement, whose advocates believe that open source software is a public good and leads to better software through more collaboration and participation, as well as lower barriers to entry. Similarly, the open data movement proponents suggest that when data is widely accessible, the public will be more informed, so data collected or produced by government and nonprofit organizations, scientific research, and other entities should be freely available to use and build on.<a id="idIndexMarker060"/><a id="idIndexMarker061"/><a id="marker-95"/></p>
<p class="fm-callout"><span class="fm-callout-head">Open source</span> refers to the source code of software being open and available to the public for reuse and modification.</p>
<p class="body">If anything, generative image models have been ahead of LLMs in this respect. Because of a keen interest in computer vision models, academics have compiled large image datasets since Fei-Fei Li, a professor of computer science at Stanford University, began a project called ImageNet. In 2006, Li had the prophetic idea that the biggest gains to be made in computer vision weren’t necessarily from new, better algorithms, but from better (and bigger) data. She began creating a database, ImageNet, that would eventually be composed of millions of images depicting hundreds of things: animals, household objects, land formations, and many other categories. After much initial skepticism, ImageNet became a standard against which all computer vision models measured their results. Not only did it kick-start the object detection problem (which is now considered “solved” on ImageNet, as state-of-the-art models can perform nearly perfectly), but it ushered in an era of sharing benchmark datasets for training and testing models. Of ImageNet’s influence, Li said, “There is a lot of mushrooming and blossoming of all kinds of datasets, from videos to speech to games to everything.” Of course, it was also a proof point for her original hypothesis, which was later also borne out by the success of LLMs <a class="url" href="https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world">[58]</a>.</p>
<p class="body">Across problem domains from natural language to images and videos, then, it pays to be greedy for data. Like later datasets, ImageNet was assembled from pictures from the internet and then labeled by workers on Amazon Mechanical Turk, a crowdsourcing platform. By writing a minimal amount of code, people can compile text and image data by programmatically accessing web pages and copying their contents. This practice is called <i class="fm-italics">web scraping</i>, which has been repeatedly found to be legal <a class="url" href="https://techcrunch.com/2022/04/18/web-scraping-legal-court/">[59]</a> as long as the data is publicly available—so almost anything that you would see by browsing online. Any website that is indexed by search engines, for example, is scraped by bots. Some of the companies that operate websites that are frequent data sources for LLMs, including Reddit, Twitter, and Stack Overflow, have publicly stated plans to charge AI developers to use that data, though it’s unclear what this would look like in practice—most likely, they would sell datasets that obviate the need for scraping <a class="url" href="https://www.zdnet.com/article/stack-overflow-joins-reddit-and-twitter-in-charging-ai-companies-for-training-data/">[60]</a>. People who maintain websites can add a robots.txt file, which is essentially a set of instructions for a bot, to tell the bot which pages it can scrape and which it shouldn’t. In practice, robots.txt files are only advisories, and malicious programs can easily ignore them. <a id="idIndexMarker062"/><a id="marker-96"/></p>
<p class="body">Although there are few legal restrictions for publicly available web content, both code and data have licenses. Some open source licenses explicitly allow all types of derivative uses. The MIT License, for example, is a permissive software license—in fact, the most popular license on GitHub—that allows for reuse within proprietary software <a class="url" href="https://github.blog/2015-03-09-open-source-license-usage-on-github-com/">[61]</a>. Other licenses allow reuse only for noncommercial purposes; still others might allow reuse with attribution, or several other conditions. Code and data licenses are legally enforceable <a class="url" href="https://www.nytimes.com/2008/08/14/technology/14commons.xhtml">[62]</a>. <a id="idIndexMarker063"/></p>
<p class="body">Code licenses are a central question in a class-action lawsuit brought by software developers against Microsoft, GitHub, and OpenAI over the LLM tool Copilot. Copilot is based on a variant of OpenAI’s GPT-3 model that is tailored especially for writing code, and it’s trained on thousands of GitHub repositories. Like the copyright question, there is litigation over the use of this code for training LLMs; it’s unclear how relying on licensing instead of fair use would work. The plaintiffs in the case argue that the use amounts to “software piracy on an unprecedented scale,” while the defendants say that it’s the plaintiffs who are undermining the principles of open source by requesting “an injunction and multi-billion dollar windfall” for “software that they willingly share” <a class="url" href="https://www.theverge.com/2023/1/28/23575919/microsoft-openai-github-dismiss-copilot-ai-copyright-lawsuit">[63]</a>.<a id="idIndexMarker064"/></p>
<p class="body"><a id="marker-97"/>Meanwhile, companies such as Hugging Face are bullish on open source principles, building and hosting models and datasets that are free to use <a class="url" href="https://huggingface.co/">[64]</a>. People unaffiliated with any of the prestigious AI labs are nonetheless able to access and, in some cases, improve upon state-of-the-art results in this ecosystem of rapid iteration and sharing. This carries with it certain risks because any limits put in to reduce certain harms can be removed by downstream users. It will be harder to prevent the creation of copycat content or enforce existing copyrights.</p>
<p class="body">Still, there are reasons to be hopeful that these problems won’t stifle creativity, but foster it. Cory Doctorow, an internet activist and author, has long been critical of copyright, pointing out that while the terms of these rights have gotten longer and broader over time, creators haven’t reaped the profits—companies that purchase their copyrights have <a class="url" href="https://doctorow.medium.com/copyright-wont-solve-creators-generative-ai-problem-92d7adbcc6e6">[65]</a>. Skeptical of broadening copyright even further to prevent generative models from accessing those works for their training, Doctorow wrote:</p>
<p class="fm-quote">Fundamentally, machine learning systems ingest a lot of works, analyze them, find statistical correlations between them, and then use those to make new works. It’s a math-heavy version of what every creator does: analyze how the works they admire are made, so they can make their own new works. If you go through the pages of an art-book analyzing the color schemes or ratios of noses to foreheads in paintings you like, you are not infringing copyright. We should not create a new right to decide who is allowed to think hard about your creative works and learn from them—such a right would make it impossible for the next generation of creators to (lawfully) learn their craft. <a class="url1" href="https://doctorow.medium.com/copyright-wont-solve-creators-generative-ai-problem-92d7adbcc6e6">[65]</a></p>
<p class="body">People may reasonably disagree over whether and how large-scale models should be trained on copyrighted data. It’s certain that we’ll get more clarity from a legal perspective as these cases continue to progress and as precedents are established. But earlier artists also worried over the invention of photography that no one would continue to paint or purchase paintings because they could no longer compete with the camera in the depiction of reality. Instead, artists continued to paint, but they conveyed scenes with their own interpretations and expressions <a class="url" href="https://www.thecollector.com/how-photography-transformed-art/">[66]</a>. It seems possible that generative models will become another medium, without ever entirely fulfilling the human need for bea<a id="idTextAnchor028"/>uty nor replacing the human impulse toward creativity.<a id="idIndexMarker065"/><a id="idIndexMarker066"/><a id="idIndexMarker067"/><a id="marker-98"/></p>
<h2 class="fm-head" id="heading_id_13">Summary</h2>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Synthetic media, or more specifically, AI-generated media, is an umbrella term for content that has been created or altered with the help of AI, which spans text, image, video, voice, and data.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The term <i class="fm-italics">deepfak</i><i class="fm-italics">e</i>—a portmanteau of “deep learning” and “fake”—is sometimes used synonymously with visual synthetic media, but it often has a negative connotation.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Autoencoders use neural networks to compress and decompress images, and they are often used in face-swapping technology.</p>
</li>
<li class="fm-list-bullet">
<p class="list">GANs consist of two neural networks—a generator and a discriminator. The generator exists to create new data, such as images, and the discriminator verifies the authenticity of an image by comparing it to the training dataset to determine the difference between a fake and a real image.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Synthetic media is democratizing content creation and creativity for everyone while ushering in a new wave of creativity and art.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Generative AI has also infamously been used to create mis/disinformation content, celebrity pornographic videos, revenge porn or cybersexual harassment, and fraud and espionage.</p>
</li>
<li class="fm-list-bullet">
<p class="list">A holistic approach to detect AI-generated media encompassing technical solutions, media literacy and education, and appropriate legislation to govern the use of the technology is essential to countering deepfakes.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Generative AI tools have transformed creative work by eliminating monotonous tasks, increasing productivity and efficiency, and enabling people to express their creativity in new and unprecedented ways.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Companies that develop LLMs have been accused of infringing on others’ intellectual property, specifically copyrights, via the training process.</p>
</li>
<li class="fm-list-bullet">
<p class="list">In the United States, fair use of copyrighted material is allowed without permission, and fair use is determined by four factors as established in the Copyright Act of 1976.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Although there are pending lawsuits, it seems that most of the activity in the generative AI space would be considered fair use under current precedent.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Open source</i> refers to the practice of making the source code of software accessible to the public to modify and reuse.<a id="idIndexMarker068"/></p>
</li>
<li class="fm-list-bullet">
<p class="list">The open source and open data movements have accelerated developments and continue to drive progress in AI.<a id="marker-99"/></p>
</li>
</ul>
</div></body></html>