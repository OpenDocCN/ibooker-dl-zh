["```py\nimport numpy as np\nimport torch\nfrom sklearn.datasets import load_sample_images\n\nsample_images = np.stack(load_sample_images()[\"images\"])\nsample_images = torch.tensor(sample_images, dtype=torch.float32) / 255\n```", "```py\n>>> sample_images.shape\ntorch.Size([2, 427, 640, 3])\n```", "```py\n>>> sample_images_permuted = sample_images.permute(0, 3, 1, 2)\n>>> sample_images_permuted.shape\ntorch.Size([2, 3, 427, 640])\n```", "```py\n>>> import torchvision\n>>> import torchvision.transforms.v2 as T\n>>> cropped_images = T.CenterCrop((70, 120))(sample_images_permuted)\n>>> cropped_images.shape\ntorch.Size([2, 3, 70, 120])\n```", "```py\nimport torch.nn as nn\n\ntorch.manual_seed(42)\nconv_layer = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=7)\nfmaps = conv_layer(cropped_images)\n```", "```py\n>>> fmaps.shape\ntorch.Size([2, 32, 64, 114])\n```", "```py\n>>> conv_layer = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=7,\n...                        padding=\"same\")\n...\n>>> fmaps = conv_layer(cropped_images)\n>>> fmaps.shape\ntorch.Size([2, 32, 70, 120])\n```", "```py\n>>> conv_layer.weight.shape\ntorch.Size([32, 3, 7, 7])\n>>> conv_layer.bias.shape\ntorch.Size([32])\n```", "```py\nmax_pool = nn.MaxPool2d(kernel_size=2)\n```", "```py\nimport torch.nn.functional as F\n\nclass DepthPool(torch.nn.Module):\n    def __init__(self, kernel_size, stride=None, padding=0):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, inputs):\n        batch, channels, height, width = inputs.shape\n        Z = inputs.view(batch, channels, height * width)  # merge spatial dims\n        Z = Z.permute(0, 2, 1)  # switch spatial and channels dims\n        Z = F.max_pool1d(Z, kernel_size=self.kernel_size, stride=self.stride,\n                         padding=self.padding)  # compute max pool\n        Z = Z.permute(0, 2, 1)  # switch back spatial and channels dims\n        return Z.view(batch, -1, height, width)  # unmerge spatial dims\n```", "```py\nglobal_avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\noutput = global_avg_pool(cropped_images)\n```", "```py\noutput = cropped_images.mean(dim=(2, 3), keepdim=True)\n```", "```py\nfrom functools import partial\n\nDefaultConv2d = partial(nn.Conv2d, kernel_size=3, padding=\"same\")\nmodel = nn.Sequential(\n    DefaultConv2d(in_channels=1, out_channels=64, kernel_size=7), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2),\n    DefaultConv2d(in_channels=64, out_channels=128), nn.ReLU(),\n    DefaultConv2d(in_channels=128, out_channels=128), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2),\n    DefaultConv2d(in_channels=128, out_channels=256), nn.ReLU(),\n    DefaultConv2d(in_channels=256, out_channels=256), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2),\n    nn.Flatten(),\n    nn.Linear(in_features=2304, out_features=128), nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(in_features=128, out_features=64), nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(in_features=64, out_features=10),\n).to(device)\n```", "```py\nclass SeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0):\n        super().__init__()\n        self.depthwise_conv = nn.Conv2d(\n            in_channels, in_channels, kernel_size, stride=stride,\n            padding=padding, groups=in_channels)\n        self.pointwise_conv = nn.Conv2d(\n            in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, inputs):\n        return self.pointwise_conv(self.depthwise_conv(inputs))\n```", "```py\nclass ResidualUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        DefaultConv2d = partial(\n            nn.Conv2d, kernel_size=3, stride=1, padding=1, bias=False)\n        self.main_layers = nn.Sequential(\n            DefaultConv2d(in_channels, out_channels, stride=stride),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            DefaultConv2d(out_channels, out_channels),\n            nn.BatchNorm2d(out_channels),\n        )\n        if stride > 1:\n            self.skip_connection = nn.Sequential(\n                DefaultConv2d(in_channels, out_channels, kernel_size=1,\n                              stride=stride, padding=0),\n                nn.BatchNorm2d(out_channels),\n            )\n        else:\n            self.skip_connection = nn.Identity()\n\n    def forward(self, inputs):\n        return F.relu(self.main_layers(inputs) + self.skip_connection(inputs))\n```", "```py\nclass ResNet34(nn.Module):\n    def __init__(self):\n        super().__init__()\n        layers = [\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2,\n                      padding=3, bias=False),\n            nn.BatchNorm2d(num_features=64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n        ]\n        prev_filters = 64\n        for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n            stride = 1 if filters == prev_filters else 2\n            layers.append(ResidualUnit(prev_filters, filters, stride=stride))\n            prev_filters = filters\n        layers += [\n            nn.AdaptiveAvgPool2d(output_size=1),\n            nn.Flatten(),\n            nn.LazyLinear(10),\n        ]\n        self.resnet = nn.Sequential(*layers)\n\n    def forward(self, inputs):\n        return self.resnet(inputs)\n```", "```py\nweights = torchvision.models.ConvNeXt_Base_Weights.IMAGENET1K_V1\nmodel = torchvision.models.convnext_base(weights=weights).to(device)\n```", "```py\ntransforms = weights.transforms()\npreprocessed_images = transforms(sample_images_permuted)\n```", "```py\nmodel.eval()\nwith torch.no_grad():\n    y_logits = model(preprocessed_images.to(device))\n```", "```py\n>>> y_pred = torch.argmax(y_logits, dim=1)\n>>> y_pred\ntensor([698, 985], device='cuda:0')\n```", "```py\n>>> class_names = weights.meta[\"categories\"]\n>>> [class_names[class_id] for class_id in y_pred]\n['palace', 'daisy']\n```", "```py\n>>> y_top3_logits, y_top3_class_ids = y_logits.topk(k=3, dim=1)\n>>> [[class_names[class_id] for class_id in top3] for top3 in y_top3_class_ids]\n[['palace', 'monastery', 'lakeside'], ['daisy', 'pot', 'ant']]\n```", "```py\n>>> y_top3_logits.softmax(dim=1)\ntensor([[0.8618, 0.1185, 0.0197],\n [0.8106, 0.0964, 0.0930]], device='cuda:0')\n```", "```py\nDefaultFlowers102 = partial(torchvision.datasets.Flowers102, root=\"datasets\",\n                            transform=weights.transforms(), download=True)\ntrain_set = DefaultFlowers102(split=\"train\")\nvalid_set = DefaultFlowers102(split=\"val\")\ntest_set = DefaultFlowers102(split=\"test\")\n```", "```py\nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\nvalid_loader = DataLoader(valid_set, batch_size=32)\ntest_loader = DataLoader(test_set, batch_size=32)\n```", "```py\nclass_names = ['pink primrose', ..., 'trumpet creeper', 'blackberry lily']\n```", "```py\n>>> [name for name, child in model.named_children()]\n['features', 'avgpool', 'classifier']\n```", "```py\n>>> model.classifier\nSequential(\n (0): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)\n (1): Flatten(start_dim=1, end_dim=-1)\n (2): Linear(in_features=1024, out_features=1000, bias=True)\n)\n```", "```py\nn_classes = 102  # len(class_names) == 102\nmodel.classifier[2] = nn.Linear(1024, n_classes).to(device)\n```", "```py\nfor param in model.parameters():\n    param.requires_grad = False\n\nfor param in model.classifier.parameters():\n    param.requires_grad = True\n```", "```py\nimport torchvision.transforms.v2 as T\n\ntransforms = T.Compose([\n    T.RandomHorizontalFlip(p=0.5),\n    T.RandomRotation(degrees=30),\n    T.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    T.ToImage(),\n    T.ToDtype(torch.float32, scale=True),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n```", "```py\nclass FlowerLocator(nn.Module):\n    def __init__(self, base_model):\n        super().__init__()\n        self.base_model = base_model\n        self.localization_head = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(base_model.classifier[2].in_features, 4)\n        )\n\n    def forward(self, X):\n        features = self.base_model.features(X)\n        pool = self.base_model.avgpool(features)\n        logits = self.base_model.classifier(pool)\n        bbox = self.localization_head(pool)\n        return logits, bbox\n\ntorch.manual_seed(42)\nlocator_model = FlowerLocator(model).to(device)\n```", "```py\npreproc_images = [...]  # a batch of preprocessed images\ny_pred_logits, y_pred_bbox = locator_model(preprocessed_images.to(device))\n```", "```py\nimport torchvision.tv_tensors\n\nbbox = torchvision.tv_tensors.BoundingBoxes(\n    [[377, 199, 248, 262]],  # center x=377, center y=199, width=248, height=262\n    format=\"CXCYWH\",  # other possible formats: \"XYXY\" and \"XYWH\"\n    canvas_size=(500, 754)  # raw image size before preprocessing\n)\n```", "```py\n>>> transform(bbox)\nBoundingBoxes([[ 90,  91, 120, 154]], format=BoundingBoxFormat.CXCYWH,\n canvas_size=(224, 224), clamping_mode=soft)\n```", "```py\nfirst_image = [...]  # load the first training image without any preprocessing\npreproc_image, preproc_target = transform(\n    (first_image, {\"label\": 0, \"bbox\": bbox})\n)\npreproc_bbox = preproc_target[\"bbox\"]\n```", "```py\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov9m.pt')  # n=nano, s=small, m=medium, x=large\nimages = [\"https://homl.info/soccer.jpg\", \"https://homl.info/traffic.jpg\"]\nresults = model(images)\n```", "```py\n>>> results[0].summary()[0]\n{'name': 'sports ball',\n 'class': 32,\n 'confidence': 0.96214,\n 'box': {'x1': 245.35733, 'y1': 286.03003, 'x2': 300.62509, 'y2': 343.57184}}\n```", "```py\nmy_video = \"https://homl.info/cars.mp4\"\nresults = model.track(source=my_video, stream=True, save=True)\nfor frame_results in results:\n    summary = frame_results.summary()  # similar summary as earlier + track id\n    track_ids = [obj[\"track_id\"] for obj in summary]\n    print(\"Track ids:\", track_ids)\n```"]