- en: Chapter 4\. Advanced Techniques for Text Generation with LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using simple prompt engineering techniques will often work for most tasks,
    but occasionally you’ll need to use a more powerful toolkit to solve complex generative
    AI problems. Such problems and tasks include:'
  prefs: []
  type: TYPE_NORMAL
- en: Context length
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing an entire book into a digestible synopsis.
  prefs: []
  type: TYPE_NORMAL
- en: Combining sequential LLM inputs/outputs
  prefs: []
  type: TYPE_NORMAL
- en: Creating a story for a book including the characters, plot, and world building.
  prefs: []
  type: TYPE_NORMAL
- en: Performing complex reasoning tasks
  prefs: []
  type: TYPE_NORMAL
- en: LLMs acting as an agent. For example, you could create an LLM agent to help
    you achieve your personal fitness goals.
  prefs: []
  type: TYPE_NORMAL
- en: To skillfully tackle such complex generative AI challenges, becoming acquainted
    with LangChain, an open source framework, is highly beneficial. This tool simplifies
    and enhances your LLM’s workflows substantially.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LangChain is a versatile framework that enables the creation of applications
    utilizing LLMs and is available as both a [Python](https://oreil.ly/YPid-) and
    a [TypeScript](https://oreil.ly/5Vl0W) package. Its central tenet is that the
    most impactful and distinct applications won’t merely interface with a language
    model via an API, but will also:'
  prefs: []
  type: TYPE_NORMAL
- en: Enhance data awareness
  prefs: []
  type: TYPE_NORMAL
- en: The framework aims to establish a seamless connection between a language model
    and external data sources.
  prefs: []
  type: TYPE_NORMAL
- en: Enhance agency
  prefs: []
  type: TYPE_NORMAL
- en: It strives to equip language models with the ability to engage with and influence
    their environment.
  prefs: []
  type: TYPE_NORMAL
- en: The LangChain framework illustrated in [Figure 4-1](#figure-4-1) provides a
    range of modular abstractions that are essential for working with LLMs, along
    with a broad selection of implementations for these abstractions.
  prefs: []
  type: TYPE_NORMAL
- en: '![pega 0401](assets/pega_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. The major modules of the LangChain LLM framework
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Each module is designed to be user-friendly and can be efficiently utilized
    independently or together. There are currently six common modules within LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: Model I/O
  prefs: []
  type: TYPE_NORMAL
- en: Handles input/output operations related to the model
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval
  prefs: []
  type: TYPE_NORMAL
- en: Focuses on retrieving relevant text for the LLM
  prefs: []
  type: TYPE_NORMAL
- en: Chains
  prefs: []
  type: TYPE_NORMAL
- en: Also known as *LangChain runnables*, chains enable the construction of sequences
    of LLM operations or function calls
  prefs: []
  type: TYPE_NORMAL
- en: Agents
  prefs: []
  type: TYPE_NORMAL
- en: Allows chains to make decisions on which tools to use based on high-level directives
    or instructions
  prefs: []
  type: TYPE_NORMAL
- en: Memory
  prefs: []
  type: TYPE_NORMAL
- en: Persists the state of an application between different runs of a chain
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks
  prefs: []
  type: TYPE_NORMAL
- en: For running additional code on specific events, such as when every new token
    is generated
  prefs: []
  type: TYPE_NORMAL
- en: Environment Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can install LangChain on your terminal with either of these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pip install langchain langchain-openai`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conda install -c conda-forge langchain langchain-openai`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you would prefer to install the package requirements for the entire book,
    you can use the [*requirements.txt*](https://oreil.ly/WKOma) file from the GitHub
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s recommended to install the packages within a virtual environment:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a virtual environment
  prefs: []
  type: TYPE_NORMAL
- en: '`python -m venv venv`'
  prefs: []
  type: TYPE_NORMAL
- en: Activate the virtual environment
  prefs: []
  type: TYPE_NORMAL
- en: '`source venv/bin/activate`'
  prefs: []
  type: TYPE_NORMAL
- en: Install the dependencies
  prefs: []
  type: TYPE_NORMAL
- en: '`pip install -r requirements.txt`'
  prefs: []
  type: TYPE_NORMAL
- en: LangChain requires integrations with one or more model providers. For example,
    to use OpenAI’s model APIs, you’ll need to install their Python package with `pip
    install openai`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed in [Chapter 1](ch01.html#five_principles_01), it’s best practice
    to set an environment variable called `OPENAI_API_KEY` in your terminal or load
    it from an *.env* file using [`python-dotenv`](https://oreil.ly/wvuO7). However,
    for prototyping you can choose to skip this step by passing in your API key directly
    when loading a chat model in LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Hardcoding API keys in scripts is not recommended due to security reasons. Instead,
    utilize environment variables or configuration files to manage your keys.
  prefs: []
  type: TYPE_NORMAL
- en: In the constantly evolving landscape of LLMs, you can encounter the challenge
    of disparities across different model APIs. The lack of standardization in interfaces
    can induce extra layers of complexity in prompt engineering and obstruct the seamless
    integration of diverse models into your projects.
  prefs: []
  type: TYPE_NORMAL
- en: This is where LangChain comes into play. As a comprehensive framework, LangChain
    allows you to easily consume the varying interfaces of different models.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain’s functionality ensures that you aren’t required to reinvent your
    prompts or code every time you switch between models. Its platform-agnostic approach
    promotes rapid experimentation with a broad range of models, such as [Anthropic](https://www.anthropic.com),
    [Vertex AI](https://cloud.google.com/vertex-ai), [OpenAI](https://openai.com),
    and [BedrockChat](https://oreil.ly/bedrock). This not only expedites the model
    evaluation process but also saves critical time and resources by simplifying complex
    model integrations.
  prefs: []
  type: TYPE_NORMAL
- en: In the sections that follow, you’ll be using the OpenAI package and their API
    in LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: Chat Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chat models such as GPT-4 have become the primary way to interface with OpenAI’s
    API. Instead of offering a straightforward “input text, output text” response,
    they propose an interaction method where *chat messages* are the input and output
    elements.
  prefs: []
  type: TYPE_NORMAL
- en: Generating LLM responses using chat models involves inputting one or more messages
    into the chat model. In the context of LangChain, the currently accepted message
    types are `AIMessage`, `HumanMessage`, and `SystemMessage`. The output from a
    chat model will always be an `AIMessage`.
  prefs: []
  type: TYPE_NORMAL
- en: SystemMessage
  prefs: []
  type: TYPE_NORMAL
- en: Represents information that should be instructions to the AI system. These are
    used to guide the AI’s behavior or actions in some way.
  prefs: []
  type: TYPE_NORMAL
- en: HumanMessage
  prefs: []
  type: TYPE_NORMAL
- en: Represents information coming from a human interacting with the AI system. This
    could be a question, a command, or any other input from a human user that the
    AI needs to process and respond to.
  prefs: []
  type: TYPE_NORMAL
- en: AIMessage
  prefs: []
  type: TYPE_NORMAL
- en: Represents information coming from the AI system itself. This is typically the
    AI’s response to a `HumanMessage` or the result of a `SystemMessage` instruction.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Make sure to leverage the `SystemMessage` for delivering explicit directions.
    OpenAI has refined GPT-4 and upcoming LLM models to pay particular attention to
    the guidelines given within this type of message.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create a joke generator in LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: First, you’ll import `ChatOpenAI`, `AIMessage`, `HumanMessage`, and `SystemMessage`.
    Then create an instance of the `ChatOpenAI` class with a temperature parameter
    of 0.5 (randomness).
  prefs: []
  type: TYPE_NORMAL
- en: After creating a model, a list named `messages` is populated with a `SystemMessage`
    object, defining the role for the LLM, and a `HumanMessage` object, which asks
    for a software engineer—related joke.
  prefs: []
  type: TYPE_NORMAL
- en: Calling the chat model with `.invoke(input=messages)` feeds the LLM with a list
    of messages, and then you retrieve the LLM’s response with `response.content`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a legacy method that allows you to directly call the `chat` object
    with `chat(messages=messages)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Streaming Chat Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You might have observed while using ChatGPT how words are sequentially returned
    to you, one character at a time. This distinct pattern of response generation
    is referred to as *streaming*, and it plays a crucial role in enhancing the performance
    of chat-based applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: When you call `chat.stream(messages)`, it yields chunks of the message one at
    a time. This means each segment of the chat message is individually returned.
    As each chunk arrives, it is then instantaneously printed to the terminal and
    flushed. This way, *streaming* allows for minimal latency from your LLM responses.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming holds several benefits from an end-user perspective. First, it dramatically
    reduces the waiting time for users. As soon as the text starts generating character
    by character, users can start interpreting the message. There’s no need for a
    full message to be constructed before it is seen. This, in turn, significantly
    enhances user interactivity and minimizes latency.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, this technique comes with its own set of challenges. One significant
    challenge is parsing the outputs while they are being streamed. Understanding
    and appropriately responding to the message as it is being formed can prove to
    be intricate, especially when the content is complex and detailed.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Multiple LLM Generations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There may be scenarios where you find it useful to generate multiple responses
    from LLMs. This is particularly true while creating dynamic content like social
    media posts. Rather than providing a list of messages, you provide a *list of
    message lists*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The benefit of using `.batch()` over `.invoke()` is that you can parallelize
    the number of API requests made to OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: 'For any runnable in LangChain, you can add a `RunnableConfig` argument to the
    `batch` function that contains many configurable parameters, including `max_``concurrency`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In computer science, *asynchronous (async) functions* are those that operate
    independently of other processes, thereby enabling several API requests to be
    run concurrently without waiting for each other. In LangChain, these async functions
    let you make many API requests all at once, not one after the other. This is especially
    helpful in more complex workflows and decreases the overall latency to your users.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the asynchronous functions within LangChain are simply prefixed with
    the letter `a`, such as `.ainvoke()` and `.abatch()`. If you would like to use
    the async API for more efficient task performance, then utilize these functions.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain Prompt Templates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until this point, you’ve been hardcoding the strings in the `ChatOpenAI`
    objects. As your LLM applications grow in size, it becomes increasingly important
    to utilize *prompt templates*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt templates* are good for generating reproducible prompts for AI language
    models. They consist of a *template*, a text string that can take in parameters,
    and construct a text prompt for a language model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Without prompt templates, you would likely use Python `f-string` formatting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'But why not simply use an `f-string` for prompt templating? Using LangChain’s
    prompt templates instead allows you to easily:'
  prefs: []
  type: TYPE_NORMAL
- en: Validate your prompt inputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combine multiple prompts together with composition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define custom selectors that will inject k-shot examples into your prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save and load prompts from *.yml* and *.json* files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create custom prompt templates that execute additional code or instructions
    when created
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain Expression Language (LCEL)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `|` pipe operator is a key component of LangChain Expression Language (LCEL)
    that allows you to chain together different components or *runnables* in a data
    processing pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'In LCEL, the `|` operator is similar to the Unix pipe operator. It takes the
    output of one component and feeds it as input to the next component in the chain.
    This allows you to easily connect and combine different components to create a
    complex chain of operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `|` operator is used to chain together the prompt and model components.
    The output of the prompt component is passed as input to the model component.
    This chaining mechanism allows you to build complex chains from basic components
    and enables the seamless flow of data between different stages of the processing
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, *the order matters*, so you could technically create this chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: But it would produce an error after using the `invoke` function, because the
    values returned from `model` are not compatible with the expected inputs for the
    prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a business name generator using prompt templates that will return
    five to seven relevant business names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: First, you’ll import `ChatOpenAI`, `SystemMessagePromptTemplate`, and `ChatPromptTemplate`.
    Then, you’ll define a prompt template with specific guidelines under `template`,
    instructing the LLM to generate business names. `ChatOpenAI()` initializes the
    chat, while `SystemMessagePromptTemplate.from_template(template)` and `ChatPromptTemplate.from_messages([system_prompt])`
    create your prompt template.
  prefs: []
  type: TYPE_NORMAL
- en: You create an LCEL `chain` by piping together `chat_prompt` and the `model`,
    which is then *invoked*. This replaces the `{industries}`, `{context}`, and `{principles}`
    placeholders in the prompt with the dictionary values within the `invoke` function.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you extract the LLM’s response as a string accessing the `.content`
    property on the `result` variable.
  prefs: []
  type: TYPE_NORMAL
- en: Give Direction and Specify Format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Carefully crafted instructions might include things like “You are a creative
    consultant brainstorming names for businesses” and “Please generate a numerical
    list of five to seven catchy names for a start-up.” Cues like these guide your
    LLM to perform the exact task you require from it.
  prefs: []
  type: TYPE_NORMAL
- en: Using PromptTemplate with Chat Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LangChain provides a more traditional template called `PromptTemplate`, which
    requires `input_variables` and `template` arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Output Parsers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#standard_practices_03), you used regular expressions
    (regex) to extract structured data from text that contained numerical lists, but
    it’s possible to do this automatically in LangChain with *output parsers*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Output parsers* are a higher-level abstraction provided by LangChain for parsing
    structured data from LLM string responses. Currently the available output parsers
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: List parser
  prefs: []
  type: TYPE_NORMAL
- en: Returns a list of comma-separated items.
  prefs: []
  type: TYPE_NORMAL
- en: Datetime parser
  prefs: []
  type: TYPE_NORMAL
- en: Parses an LLM output into datetime format.
  prefs: []
  type: TYPE_NORMAL
- en: Enum parser
  prefs: []
  type: TYPE_NORMAL
- en: Parses strings into enum values.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-fixing parser
  prefs: []
  type: TYPE_NORMAL
- en: Wraps another output parser, and if that output parser fails, it will call another
    LLM to fix any errors.
  prefs: []
  type: TYPE_NORMAL
- en: Pydantic (JSON) parser
  prefs: []
  type: TYPE_NORMAL
- en: Parses LLM responses into JSON output that conforms to a Pydantic schema.
  prefs: []
  type: TYPE_NORMAL
- en: Retry parser
  prefs: []
  type: TYPE_NORMAL
- en: Provides retrying a failed parse from a previous output parser.
  prefs: []
  type: TYPE_NORMAL
- en: Structured output parser
  prefs: []
  type: TYPE_NORMAL
- en: Can be used when you want to return multiple fields.
  prefs: []
  type: TYPE_NORMAL
- en: XML parser
  prefs: []
  type: TYPE_NORMAL
- en: Parses LLM responses into an XML-based format.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you’ll discover, there are two important functions for LangChain output
    parsers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.get_format_instructions()`'
  prefs: []
  type: TYPE_NORMAL
- en: This function provides the necessary instructions into your prompt to output
    a structured format that can be parsed.
  prefs: []
  type: TYPE_NORMAL
- en: '`.parse(llm_output: str)`'
  prefs: []
  type: TYPE_NORMAL
- en: This function is responsible for parsing your LLM responses into a predefined
    format.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, you’ll find that the Pydantic (JSON) parser with `ChatOpenAI()` provides
    the most flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: The Pydantic (JSON) parser takes advantage of the [Pydantic](https://oreil.ly/QIMih)
    library in Python. Pydantic is a data validation library that provides a way to
    validate incoming data using Python type annotations. This means that Pydantic
    allows you to create schemas for your data and automatically validates and parses
    input data according to those schemas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: After you’ve loaded the necessary libraries, you’ll set up a ChatOpenAI model.
    Then create `SystemMessagePromptTemplate` from your template and form a `ChatPromptTemplate`
    with it. You’ll use the Pydantic models `BusinessName` and `BusinessNames` to
    structure your desired output, a list of unique business names. You’ll create
    a `Pydantic` parser for parsing these models and format the prompt using user-inputted
    variables by calling the `invoke` function. Feeding this customized prompt to
    your model, you’re enabling it to produce creative, unique business names by using
    the `parser`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s possible to use output parsers inside of LCEL by using this syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Let’s add the output parser directly to the chain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The chain is now responsible for prompt formatting, LLM calling, and parsing
    the LLM’s response into a `Pydantic` object.
  prefs: []
  type: TYPE_NORMAL
- en: Specify Format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preceding prompts use Pydantic models and output parsers, allowing you explicitly
    tell an LLM your desired response format.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth knowing that by asking an LLM to provide structured JSON output,
    you can create a flexible and generalizable API from the LLM’s response. There
    are limitations to this, such as the size of the JSON created and the reliability
    of your prompts, but it still is a promising area for LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You should take care of edge cases as well as adding error handling statements,
    since LLM outputs might not always be in your desired format.
  prefs: []
  type: TYPE_NORMAL
- en: Output parsers save you from the complexity and intricacy of regular expressions,
    providing easy-to-use functionalities for a variety of use cases. Now that you’ve
    seen them in action, you can utilize output parsers to effortlessly structure
    and retrieve the data you need from an LLM’s output, harnessing the full potential
    of AI for your tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, using parsers to structure the data extracted from LLMs allows
    you to easily choose how to organize outputs for more efficient use. This can
    be useful if you’re dealing with extensive lists and need to sort them by certain
    criteria, like business names.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain Evals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As well as output parsers to check for formatting errors, most AI systems also
    make use of *evals*, or evaluation metrics, to measure the performance of each
    prompt response. LangChain has a number of off-the-shelf evaluators, which can
    be directly be logged in their [LangSmith](https://oreil.ly/0Fn94) platform for
    further debugging, monitoring, and testing. [Weights and Biases](https://wandb.ai/site)
    is alternative machine learning platform that offers similar functionality and
    tracing capabilities for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics are useful for more than just prompt testing, as they can
    be used to identify positive and negative examples for retrieval as well as to
    build datasets for fine-tuning custom models.
  prefs: []
  type: TYPE_NORMAL
- en: Most eval metrics rely on a set of test cases, which are input and output pairings
    where you know the correct answer. Often these reference answers are created or
    curated manually by a human, but it’s also common practice to use a smarter model
    like GPT-4 to generate the ground truth answers, which has been done for the following
    example. Given a list of descriptions of financial transactions, we used GPT-4
    to classify each transaction with a `transaction_category` and `transaction_type`.
    The process can be found in the `langchain-evals.ipynb` Jupyter Notebook in the
    [GitHub repository](https://oreil.ly/a4Hut) for the book.
  prefs: []
  type: TYPE_NORMAL
- en: With the GPT-4 answer being taken as the correct answer, it’s now possible to
    rate the accuracy of smaller models like GPT-3.5-turbo and Mixtral 8x7b (called
    `mistral-small` in the API). If you can achieve good enough accuracy with a smaller
    model, you can save money or decrease latency. In addition, if that model is available
    open source like [Mistral’s model](https://oreil.ly/Ec578), you can migrate that
    task to run on your own servers, avoiding sending potentially sensitive data outside
    of your organization. We recommend testing with an external API first, before
    going to the trouble of self-hosting an OS model.
  prefs: []
  type: TYPE_NORMAL
- en: '[Remember to sign up](https://mistral.ai) and subscribe to obtain an API key;
    then expose that as an environment variable by typing in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**export MISTRAL_API_KEY=api-key**`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following script is part of a [notebook](https://oreil.ly/DqDOf) that has
    previously defined a dataframe `df`. For brevity let’s investigate only the evaluation
    section of the script, assuming a dataframe is already defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The code does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from langchain_mistralai.chat_models import ChatMistralAI`: We import LangChain’s
    Mistral implementation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`from langchain.output_parsers import PydanticOutputParser`: Imports the `PydanticOutputParser`
    class, which is used for parsing output using Pydantic models. We also import
    a string output parser to handle an interim step where we remove backslashes from
    the JSON key (a common problem with responses from Mistral).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`mistral_api_key = os.environ["MISTRAL_API_KEY"]`: Retrieves the Mistral API
    key from the environment variables. This needs to be set prior to running the
    notebook.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`model = ChatMistralAI(model="mistral-small", mistral_api_key=mistral_api_key)`:
    Initializes an instance of `ChatMistralAI` with the specified model and API key.
    Mistral Small is what they call the Mixtral 8x7b model (also available open source)
    in their API.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`system_prompt` and `user_prompt`: These lines define templates for the system
    and user prompts used in the chat to classify the transactions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`class EnrichedTransactionInformation(BaseModel)`: Defines a Pydantic model
    `EnrichedTransactionInformation` with two fields: `transaction_type` and `transaction_category`,
    each with specific allowed values and the possibility of being `None`. This is
    what tells us if the output is in the correct format.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`def remove_back_slashes(string)`: Defines a function to remove backslashes
    from a string.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`chain = prompt | model | StrOutputParser() | remove_back_slashes | output_parser`:
    Updates the chain to include a string output parser and the `remove_back_slashes`
    function before the original output parser.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`transaction = df.iloc[0]["Transaction Description"]`: Extracts the first transaction
    description from a dataframe `df`. This dataframe is loaded earlier in the [Jupyter
    Notebook](https://oreil.ly/-koAO) (omitted for brevity).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`for i, row in tqdm(df.iterrows(), total=len(df))`: Iterates over each row
    in the dataframe `df`, with a progress bar.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`result = chain.invoke(...)`: Inside the loop, the chain is invoked for each
    transaction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`except`: In case of an exception, a default `EnrichedTransactionInformation`
    object with `None` values is created. These will be treated as errors in evaluation
    but will not break the processing loop.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df["mistral_transaction_type"] = transaction_types`, `df["mistral_transaction_category"]
    = transaction_categories`: Adds the transaction types and categories as new columns
    in the dataframe, which we then display with `df.head()`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the responses from Mistral saved in the dataframe, it’s possible to compare
    them to the transaction categories and types defined earlier to check the accuracy
    of Mistral. The most basic LangChain eval metric is to do an exact string match
    of a prediction against a reference answer, which returns a score of 1 if correct,
    and a 0 if incorrect. The notebook gives an example of how to [implement this](https://oreil.ly/vPUfI),
    which shows that Mistral’s accuracy is 77.5%. However, if all you are doing is
    comparing strings, you probably don’t need to implement it in LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Where LangChain is valuable is in its standardized and tested approaches to
    implementing more advanced evaluators using LLMs. The evaluator `labeled_pairwise_string`
    compares two outputs and gives a reason for choosing between them, using GPT-4\.
    One common use case for this type of evaluator is to compare the outputs from
    two different prompts or models, particularly if the models being tested are less
    sophisticated than GPT-4\. This evaluator using GPT-4 does still work for evaluating
    GPT-4 responses, but you should manually review the reasoning and scores to ensure
    it is doing a good job: if GPT-4 is bad at a task, it may also be bad at evaluating
    that task. In [the notebook](https://oreil.ly/9O7Mb), the same transaction classification
    was run again with the model changed to `model = ChatOpenAI(model="gpt-3.5-turbo-1106",
    model_kwargs={"response_format": {"type": "json_object"}},)`. Now it’s possible
    to do pairwise comparison between the Mistral and GPT-3.5 responses, as shown
    in the following example. You can see in the output the reasoning that is given
    to justify the score.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This code demonstrates the simple exact string matching evaluator from LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: '`evaluator = load_evaluator("labeled_pairwise_string")`: This is a helper function
    that can be used to load any LangChain evaluator by name. In this case, it is
    the `labeled_pairwise_string` evaluator being used.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`row = df.iloc[0]`: This line and the seven lines that follow get the first
    row and extract the values for the different columns needed. It includes the transaction
    description, as well as the Mistral and GPT-3.5 transaction category and types.
    This is showcasing a single transaction, but this code can easily run in a loop
    through each transaction, replacing this line with an `iterrows` function `for
    i, row in tqdm(df.iterrows(), total=len(df)):`, as is done later in [the notebook](https://oreil.ly/dcCOO).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`gpt3pt5_data = f"""{{`: To use the pairwise comparison evaluator, we need
    to pass the results in a way that is formatted correctly for the prompt. This
    is done for Mistral and GPT-3.5, as well as the reference data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`input_prompt = """You are an expert...`: The other formatting we have to get
    right is in the prompt. To get accurate evaluation scores, the evaluator needs
    to see the instructions that were given for the task.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`evaluator.evaluate_string_pairs(...`: All that remains is to run the evaluator
    by passing in the `prediction` and `prediction_b` (GPT-3.5 and Mistral, respectively),
    as well as the `input` prompt, and `reference` data, which serves as the ground
    truth.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Following this code [in the notebook](https://oreil.ly/hW8Wr), there is an example
    of looping through and running the evaluator on every row in the dataframe and
    then saving the results and reasoning back to the dataframe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This example demonstrates how to use a LangChain evaluator, but there are many
    different kinds of evaluator available. String distance ([Levenshtein](https://oreil.ly/Al5G3))
    or [embedding distance](https://oreil.ly/0p_nE) evaluators are often used in scenarios
    where answers are not an exact match for the reference answer, but only need to
    be close enough semantically. Levenshtein distance allows for fuzzy matches based
    on how many single-character edits would be needed to transform the predicted
    text into the reference text, and embedding distance makes use of vectors (covered
    in [Chapter 5](ch05.html#vector_databases_05)) to calculate similarity between
    the answer and reference.
  prefs: []
  type: TYPE_NORMAL
- en: The other kind of evaluator we often use in our work is pairwise comparisons,
    which are useful for comparing two different prompts or models, using a smarter
    model like GPT-4\. This type of comparison is helpful because reasoning is provided
    for each comparison, which can be useful in debugging why one approach was favored
    over another. The [notebook for this section](https://oreil.ly/iahTJ) shows an
    example of using a pairwise comparison evaluator to check GPT-3.5-turbo’s accuracy
    versus Mixtral 8x7b.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate Quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Without defining an appropriate set of eval metrics to define success, it can
    be difficult to tell if changes to the prompt or wider system are improving or
    harming the quality of responses. If you can automate eval metrics using smart
    models like GPT-4, you can iterate faster to improve results without costly or
    time-consuming manual human review.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Function Calling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Function calling* provides an alternative method to output parsers, leveraging
    fine-tuned OpenAI models. These models identify when a function should be executed
    and generate a JSON response with the *name and arguments* for a predefined function.
    Several use cases include:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing sophisticated chat bots
  prefs: []
  type: TYPE_NORMAL
- en: 'Capable of organizing and managing schedules. For example, you can define a
    function to schedule a meeting: `schedule_meeting(date: str, time: str, attendees:
    List[str])`.'
  prefs: []
  type: TYPE_NORMAL
- en: Convert natural language into actionable API calls
  prefs: []
  type: TYPE_NORMAL
- en: 'A command like “Turn on the hallway lights” can be converted to `control_device(device:
    str, action: ''on'' | ''off'')` for interacting with your home automation API.'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting structured data
  prefs: []
  type: TYPE_NORMAL
- en: 'This could be done by defining a function such as `extract_contextual_data(context:
    str, data_points: List[str])` or `search_database(query: str)`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each function that you use within function calling will require an appropriate
    *JSON schema*. Let’s explore an example with the `OpenAI` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: After importing `OpenAI` and `json`, you’ll create a function named `schedule_meeting`.
    This function is a mock-up, simulating the process of scheduling a meeting, and
    returns details such as `event_id`, `date`, `time`, and `attendees`. Following
    that, make an `OPENAI_FUNCTIONS` dictionary to map the function name to the actual
    function for ease of reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, define a `functions` list that provides the function’s JSON schema. This
    schema includes its name, a brief description, and the parameters it requires,
    guiding the LLM on how to interact with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Specify Format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When using function calling with your OpenAI models, always ensure to define
    a detailed JSON schema (including the name and description). This acts as a blueprint
    for the function, guiding the model to understand when and how to properly invoke
    it.
  prefs: []
  type: TYPE_NORMAL
- en: After defining the functions, let’s make an OpenAI API request. Set up a `messages`
    list with the user query. Then, using an OpenAI `client` object, you’ll send this
    message and the function schema to the model. The LLM analyzes the conversation,
    discerns a need to trigger a function, and provides the function name and arguments.
    The `function` and `function_args` are parsed from the LLM response. Then the
    function is executed, and its results are added back into the conversation. Then
    you call the model again for a user-friendly summary of the entire process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Several important points to note while function calling:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s possible to have many functions that the LLM can call.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI can hallucinate function parameters, so be more explicit within the `system`
    message to overcome this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `function_call` parameter can be set in various ways:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To mandate a specific function call: `tool_choice: {"type: "function", "function":
    {"name": "my_function"}}}`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a user message without function invocation: `tool_choice: "none"`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By default (`tool_choice: "auto"`), the model autonomously decides if and which
    function to call.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel Function Calling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can set your chat messages to include intents that request simultaneous
    calls to multiple tools. This strategy is known as *parallel function calling*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modifying the previously used code, the `messages` list is updated to mandate
    the scheduling of two meetings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Then, adjust the previous code section by incorporating a `for` loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: From this example, it’s clear how you can effectively manage multiple function
    calls. You’ve seen how the `schedule_meeting` function was called twice in a row
    to arrange different meetings. This demonstrates how flexibly and effortlessly
    you can handle varied and complex requests using AI-powered tools.
  prefs: []
  type: TYPE_NORMAL
- en: Function Calling in LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’d prefer to avoid writing JSON schema and simply want to extract structured
    data from an LLM response, then LangChain allows you to use function calling with
    Pydantic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: You’ll start by importing various modules, including `PydanticToolsParser` and
    `ChatPromptTemplate`, essential for parsing and templating your prompts. Then,
    you’ll define a Pydantic model, `Article`, to specify the structure of the information
    you want to extract from a given text. With the use of a custom prompt template
    and the ChatOpenAI model, you’ll instruct the AI to extract key points and contrarian
    views from an article. Finally, the extracted data is neatly converted into your
    predefined Pydantic model and printed out, allowing you to see the structured
    information pulled from the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several key points, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Converting Pydantic schema to OpenAI tools
  prefs: []
  type: TYPE_NORMAL
- en: '`tools = [convert_to_openai_tool(p) for p in pydantic_schemas]`'
  prefs: []
  type: TYPE_NORMAL
- en: Binding the tools directly to the LLM
  prefs: []
  type: TYPE_NORMAL
- en: '`model = model.bind_tools(tools=tools)`'
  prefs: []
  type: TYPE_NORMAL
- en: Creating an LCEL chain that contains a tools parser
  prefs: []
  type: TYPE_NORMAL
- en: '`chain = prompt | model | PydanticToolsParser(tools=pydantic_schemas)`'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting Data with LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `create_extraction_chain_pydantic` function provides a more concise version
    of the previous implementation. By simply inserting a Pydantic model and an LLM
    that supports function calling, you can easily achieve parallel function calling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The `Person` Pydantic model has two properties, `name` and `age`; by calling
    the `create_extraction_chain_pydantic` function with the input text, the LLM invokes
    the same function twice and creates two `People` objects.
  prefs: []
  type: TYPE_NORMAL
- en: Query Planning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You may experience problems when user queries have multiple intents with intricate
    dependencies. *Query planning* is an effective way to parse a user’s query into
    a series of steps that can be executed as a query graph with relevant dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Defining `QueryPlan` and `Query` allows you to first ask an LLM to parse a user’s
    query into multiple steps. Let’s investigate how to create the query plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Initiate a `ChatOpenAI` instance and create a `PydanticOutputParser` for the
    `QueryPlan` structure. Then the LLM response is called and parsed, producing a
    structured `query_graph` for your tasks with their unique dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Few-Shot Prompt Templates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with the generative capabilities of LLMs often involves making a choice
    between *zero-shot* and *few-shot learning (k-shot)*. While zero-shot learning
    requires no explicit examples and adapts to tasks based solely on the prompt,
    its dependence on the pretraining phase means it may not always yield precise
    results.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, with few-shot learning, which involves providing a few examples
    of the desired task performance in the prompt, you have the opportunity to optimize
    the model’s behavior, leading to more desirable outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the token LLM context length, you will often finding yourself competing
    between adding lots of high-quality k-shot examples into your prompts while still
    aiming to generate an effective and deterministic LLM output.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Even as the token context window limit within LLMs continues to increase, providing
    a specific number of k-shot examples helps you minimize API costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore two methods for adding k-shot examples into your prompts with
    *few-shot prompt templates*: using *fixed examples* and using an *example selector*.'
  prefs: []
  type: TYPE_NORMAL
- en: Fixed-Length Few-Shot Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s look at how to create a few-shot prompt template using a fixed
    number of examples. The foundation of this method lies in creating a robust set
    of few-shot examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Each example is a dictionary containing a `question` and `answer` key that will
    be used to create pairs of `HumanMessage` and `AIMessage` messages.
  prefs: []
  type: TYPE_NORMAL
- en: Formatting the Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, you’ll configure a `ChatPromptTemplate` for formatting the individual
    examples, which will then be inserted into a `FewShotChatMessagePromptTemplate`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Notice how `example_prompt` will create `HumanMessage` and `AIMessage` pairs
    with the prompt inputs of `{question}` and `{answer}`.
  prefs: []
  type: TYPE_NORMAL
- en: After running `few_shot_prompt.format()`, the few-shot examples are printed
    as a string. As you’d like to use these within a `ChatOpenAI()` LLM request, let’s
    create a new `ChatPromptTemplate`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: After invoking the LCEL chain on `final_prompt`, your few-shot examples are
    added after the `SystemMessage`.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the LLM only returns `'Washington, D.C.'` This is because after
    the LLMs response is returned, *it is parsed* by `StrOutputParser()`, an output
    parser. Adding `StrOutputParser()` is a common way to ensure that LLM responses
    in chains *return string values*. You’ll explore this more in depth while learning
    sequential chains in LCEL.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting Few-Shot Examples by Length
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before diving into the code, let’s outline your task. Imagine you’re building
    a storytelling application powered by GPT-4\. A user enters a list of character
    names with previously generated stories. However, each user’s list of characters
    might have a different length. Including too many characters might generate a
    story that surpasses the LLM’s context window limit. That’s where you can use
    `LengthBasedExampleSelector` to adapt the prompt according to the length of user
    input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: First, you set up a `PromptTemplate` that takes two input variables for each
    example. Then `LengthBasedExampleSelector` adjusts the number of examples according
    to the *length of the examples input*, ensuring your LLM doesn’t generate a story
    beyond its context window.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, you’ve customized the `get_text_length` function to use the `num_tokens_from_string`
    function that counts the total number of tokens using `tiktoken`. This means that
    `max_length=1000` represents the *number of tokens* rather than using the following
    default function:'
  prefs: []
  type: TYPE_NORMAL
- en: '`get_text_length: Callable[[str], int] = lambda x: len(re.split("\n| ", x))`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to tie all these elements together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Provide Examples and Specify Format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with few-shot examples, the length of the content matters in determining
    how many examples the AI model can take into account. Tune the length of your
    input content and provide apt examples for efficient results to prevent the LLM
    from generating content that might surpass its context window limit.
  prefs: []
  type: TYPE_NORMAL
- en: After formatting the prompt, you create a chat model with `ChatOpenAI()` and
    load the formatted prompt into a `SystemMessage` that creates a small story about
    Frodo from *Lord of the Rings*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than creating and formatting a `ChatPromptTemplate`, it’s often much
    easier to simply invoke a `SystemMesage` with a formatted prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Limitations with Few-Shot Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Few-shot learning has limitations. Although it can prove beneficial in certain
    scenarios, it might not always yield the expected high-quality results. This is
    primarily due to two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained models like GPT-4 can sometimes overfit to the few-shot examples,
    making them prioritize the examples over the actual prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs have a token limit. As a result, there will always be a trade-off between
    the number of examples and the length of the response. Providing more examples
    might limit the response length and vice versa.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These limitations can be addressed in several ways. First, if few-shot prompting
    is not yielding the desired results, consider using differently framed phrases
    or experimenting with the language of the prompts themselves. Variations in how
    the prompt is phrased can result in different responses, highlighting the trial-and-error
    nature of prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Second, think about including explicit instructions to the model to ignore the
    examples after it understands the task or to use the examples just for formatting
    guidance. This might influence the model to not overfit to the examples.
  prefs: []
  type: TYPE_NORMAL
- en: If the tasks are complex and the performance of the model with few-shot learning
    is not satisfactory, you might need to consider [fine-tuning](https://oreil.ly/S40bZ)
    your model. Fine-tuning provides a more nuanced understanding of a specific task
    to the model, thus improving the performance significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Saving and Loading LLM Prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To effectively leverage generative AI models such as GPT-4, it is beneficial
    to store prompts as files instead of Python code. This approach enhances the shareability,
    storage, and versioning of your prompts.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain supports both saving and loading prompts from JSON and YAML. Another
    key feature of LangChain is its support for detailed specification in one file
    or distributed across multiple files. This means you have the flexibility to store
    different components such as templates, examples, and others in distinct files
    and reference them as required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s learn how to save and load prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: After importing `PromptTemplate` and `load_prompt` from the `langchain.prompts`
    module, you define a `PromptTemplate` for English-to-Spanish translation tasks
    and save it as *translation_prompt.json*. Finally, you load the saved prompt template
    using the `load_prompt` function, which returns an instance of `PromptTemplate`.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Please be aware that LangChain’s prompt saving may not work with all types of
    prompt templates. To mitigate this, you can utilize the *pickle* library or *.txt*
    files to read and write any prompts that LangChain does not support.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ve learned how to create few-shot prompt templates using LangChain with
    two techniques: a fixed number of examples and using an example selector.'
  prefs: []
  type: TYPE_NORMAL
- en: The former creates a set of few-shot examples and uses a `ChatPromptTemplate`
    object to format these into chat messages. This forms the basis for creating a
    `FewShotChatMessagePromptTemplate` object.
  prefs: []
  type: TYPE_NORMAL
- en: The latter approach, using an example selector, is handy when user input varies
    significantly in length. In such scenarios, a `LengthBasedExampleSelector` can
    be utilized to adjust the number of examples based on user input length. This
    ensures your LLM does not exceed its context window limit.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, you’ve seen how easy it is to store/load prompts as files, enabling
    enhanced shareability, storage, and versioning.
  prefs: []
  type: TYPE_NORMAL
- en: Data Connection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Harnessing an LLM application, coupled with your data, uncovers a plethora of
    opportunities to boost efficiency while refining your decision-making processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your organization’s data may manifest in various forms:'
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured data
  prefs: []
  type: TYPE_NORMAL
- en: This could include Google Docs, threads from communication platforms such as
    Slack or Microsoft Teams, web pages, internal documentation, or code repositories
    on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Structured data
  prefs: []
  type: TYPE_NORMAL
- en: Data neatly housed within SQL, NoSQL, or Graph databases.
  prefs: []
  type: TYPE_NORMAL
- en: To query your unstructured data, a process of loading, transforming, embedding,
    and subsequently storing it within a vector database is necessary. A *vector database*
    is a specialized type of database designed to efficiently store and query data
    in the form of vectors, which represent complex data like text or images in a
    format suitable for machine learning and similarity search.
  prefs: []
  type: TYPE_NORMAL
- en: As for structured data, given its already indexed and stored state, you can
    utilize a LangChain agent to conduct an intermediate query on your database. This
    allows for the extraction of specific features, which can then be used within
    your LLM prompts.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple Python packages that can help with your data ingestion, including
    [Unstructured](https://oreil.ly/n0hDD), [LlamaIndex](https://www.llamaindex.ai),
    and [LangChain](https://oreil.ly/PjV9o).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-2](#figure-4-2) illustrates a standardized approach to data ingestion.
    It begins with the data sources, which are then loaded into documents. These documents
    are then chunked and stored within a vector database for later retrieval.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data Connection](assets/pega_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. A data connection to retrieval pipeline
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In particular LangChain equips you with essential components to load, modify,
    store, and retrieve your data:'
  prefs: []
  type: TYPE_NORMAL
- en: Document loaders
  prefs: []
  type: TYPE_NORMAL
- en: These facilitate uploading informational resources, or *documents*, from a diverse
    range of sources such as Word documents, PDF files, text files, or even web pages.
  prefs: []
  type: TYPE_NORMAL
- en: Document transformers
  prefs: []
  type: TYPE_NORMAL
- en: These tools allow the segmentation of documents, conversion into a Q&A layout,
    elimination of superfluous documents, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: Text embedding models
  prefs: []
  type: TYPE_NORMAL
- en: These can transform unstructured text into a sequence of floating-point numbers
    used for similarity search by vector stores.
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases (vector stores)
  prefs: []
  type: TYPE_NORMAL
- en: These databases can save and execute searches over embedded data.
  prefs: []
  type: TYPE_NORMAL
- en: Retrievers
  prefs: []
  type: TYPE_NORMAL
- en: These tools offer the capability to query and retrieve data.
  prefs: []
  type: TYPE_NORMAL
- en: Also, it’s worth mentioning that other LLM frameworks such as [LlamaIndex](https://oreil.ly/9NcTB)
    work seamlessly with LangChain. [LlamaHub](https://llamahub.ai) is another open
    source library dedicated to document loaders and can create LangChain-specific
    `Document` objects.
  prefs: []
  type: TYPE_NORMAL
- en: Document Loaders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s imagine you’ve been tasked with building an LLM data collection pipeline
    for NutriFusion Foods. The information that you need to gather for the LLM is
    contained within:'
  prefs: []
  type: TYPE_NORMAL
- en: A PDF of a book called *Principles of Marketing*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two *.docx* marketing reports in a public Google Cloud Storage bucket
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three *.csv* files showcasing the marketing performance data for 2021, 2022,
    and 2023
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new Jupyter Notebook or Python file in *content/chapter_4* of the [shared
    repository](https://oreil.ly/cVTyI), and then run `pip install pdf2image docx2txt
    pypdf`, which will install three packages.
  prefs: []
  type: TYPE_NORMAL
- en: All of the data apart from *.docx* files can be found in [*content/chapter_4/data*](https://oreil.ly/u9gMx).
    You can start by importing all of your various data loaders and creating an empty
    `all_documents` list to store all of the `Document` objects across your data sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Then using `PyPDFLoader`, you can import a *.pdf* file and split it into multiple
    pages using the `.load_and_split()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, it’s possible to add extra metadata to each page because the metadata
    is a Python dictionary on each `Document` object. Also, notice in the preceding
    output for `Document` objects the metadata `source` is attached to.
  prefs: []
  type: TYPE_NORMAL
- en: Using the package `glob`, you can easily find all of the *.csv* files and individually
    load these into LangChain `Document` objects with a `CSVLoader`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the two marketing reports are loaded from a public Google Cloud Storage
    bucket and are then split into 200 token-chunk sizes using a `text_splitter`.
  prefs: []
  type: TYPE_NORMAL
- en: This section equipped you with the necessary knowledge to create a comprehensive
    document-loading pipeline for NutriFusion Foods’ LLM. Starting with data extraction
    from a PDF, several CSV files and two .*docx* files, each document was enriched
    with relevant metadata for better context.
  prefs: []
  type: TYPE_NORMAL
- en: You now have the ability to seamlessly integrate data from a variety of document
    sources into a cohesive data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Text Splitters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Balancing the length of each document is also a crucial factor. If a document
    is too lengthy, it may surpass the *context length* of the LLM (the maximum number
    of tokens that an LLM can process within a single request). But if the documents
    are excessively fragmented into smaller chunks, there’s a risk of losing significant
    contextual information, which is equally undesirable.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might encounter specific challenges while text splitting, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Special characters such as hashtags, @ symbols, or links might not split as
    anticipated, affecting the overall structure of the split documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your document contains intricate formatting like tables, lists, or multilevel
    headings, the text splitter might find it difficult to retain the original formatting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are ways to overcome these challenges that we’ll explore later.
  prefs: []
  type: TYPE_NORMAL
- en: This section introduces you to text splitters in LangChain, tools utilized to
    break down large chunks of text to better adapt to your model’s context window.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There isn’t a perfect document size. Start by using good heuristics and then
    build a training/test set that you can use for LLM evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain provides a range of text splitters so that you can easily split by
    any of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Token count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursively by multiple characters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Character count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markdown headers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s explore three popular splitters: `CharacterTextSplitter`, `TokenTextSplitter`,
    and `RecursiveCharacterTextSplitter`.'
  prefs: []
  type: TYPE_NORMAL
- en: Text Splitting by Length and Token Size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#standard_practices_03), you learned how to count the
    number of tokens within a GPT-4 call with [tiktoken](https://oreil.ly/uz05O).
    You can also use tiktoken to split strings into appropriately sized chunks and
    documents.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to install tiktoken and langchain-text-splitters with `pip install
    tiktoken langchain-text-splitters`.
  prefs: []
  type: TYPE_NORMAL
- en: To split by token count in LangChain, you can use a `CharacterTextSplitter`
    with a `.from_tiktoken_encoder()` function.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll initially create a `CharacterTextSplitter` with a chunk size of 50 characters
    and no overlap. Using the `split_text` method, you’re chopping the text into pieces
    and then printing out the total number of chunks created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then you’ll do the same thing, but this time with a *chunk overlap* of 48 characters.
    This shows how the number of chunks changes based *on whether you allow overlap*,
    illustrating the impact of these settings on how your text gets divided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous section, you used the following to load and split the *.pdf*
    into LangChain documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pages = loader.load_and_split()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It’s possible for you to have more granular control on the size of each document
    by creating a `TextSplitter` and attaching it to your `Document` loading pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '`def load_and_split(text_splitter: TextSplitter | None = None) -> List[Document]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simply create a `TokenTextSplitter` with a `chunk_size=500` and a `chunk_overlap`
    of 50:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The *Principles of Marketing* book contains 497 pages, but after using a `TokenTextSplitter`
    with a `chunk_size` of 500 tokens, you’ve created 776 smaller LangChain `Document`
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: Text Splitting with Recursive Character Splitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dealing with sizable blocks of text can present unique challenges in text analysis.
    A helpful strategy for such situations involves the use of *recursive character
    splitting*. This method facilitates the division of a large body of text into
    manageable segments, making further analysis more accessible.
  prefs: []
  type: TYPE_NORMAL
- en: This approach becomes incredibly effective when handling generic text. It leverages
    a list of characters as parameters and sequentially splits the text based on these
    characters. The resulting sections continue to be divided until they reach an
    acceptable size. By default, the character list comprises `"\n\n"`, `"\n"`, `"
    "`, and `""`. This arrangement aims to retain the integrity of paragraphs, sentences,
    and words, preserving the semantic context.
  prefs: []
  type: TYPE_NORMAL
- en: The process hinges on the character list provided and sizes the resulting sections
    based on the character count.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into the code, it’s essential to understand what the `RecursiveCharacterTextSplitter`
    does. It takes a text and a list of delimiters (characters that define the boundaries
    for splitting the text). Starting from the first delimiter in the list, the splitter
    attempts to divide the text. If the resulting chunks are still too large, it proceeds
    to the next delimiter, and so on. This process continues *recursively* until the
    chunks are small enough or all delimiters are exhausted.
  prefs: []
  type: TYPE_NORMAL
- en: Using the preceding `text` variable, start by importing `RecursiveCharacterText​Splitter`.
    This instance will be responsible for splitting the text. When initializing the
    splitter, parameters `chunk_size`, `chunk_overlap`, and `length_function` are
    set. Here, `chunk_size` is set to 100, and `chunk_overlap` to 20.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `length_function` is defined as `len` to determine the size of the chunks.
    It’s also possible to modify the `length_function` argument to use a tokenizer
    count instead of using the default `len` function, which will count characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the `text_splitter` instance is ready, you can use `.split_text` to split
    the `text` variable into smaller chunks. These chunks are stored in the `texts`
    Python list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'As well as simply splitting the text with overlap into a list of strings, you
    can easily create LangChain `Document` objects with the `.create_documents` function.
    Creating `Document` objects is useful because it allows you to:'
  prefs: []
  type: TYPE_NORMAL
- en: Store documents within a vector database for semantic search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add metadata to specific pieces of text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterate over multiple documents to create a higher-level summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To add metadata, provide a list of dictionaries to the `metadatas` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: But what if your existing `Document` objects are too long?
  prefs: []
  type: TYPE_NORMAL
- en: 'You can easily handle that by using the `.split_documents` function with a
    `TextSplitter`. This will take in a list of `Document` objects and will return
    a new list of `Document` objects based on your `TextSplitter` class argument settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: You’ve now gained the ability to craft an efficient data loading pipeline, leveraging
    sources such as PDFs, CSVs, and Google Cloud Storage links. Furthermore, you’ve
    learned how to enrich the collected documents with relevant metadata, providing
    meaningful context for analysis and prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: With the introduction of text splitters, you can now strategically manage document
    sizes, optimizing for both the LLM’s context window and the preservation of context-rich
    information. You’ve navigated handling larger texts by employing recursion and
    character splitting. This newfound knowledge empowers you to work seamlessly with
    various document sources and integrate them into a robust data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Task Decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Task decomposition* is the strategic process of dissecting complex problems
    into a suite of manageable subproblems. This approach aligns seamlessly with the
    natural tendencies of software engineers, who often conceptualize tasks as interrelated
    subcomponents.'
  prefs: []
  type: TYPE_NORMAL
- en: In software engineering, by utilizing task decomposition you can reduce cognitive
    burden and harness the advantages of problem isolation and adherence to the single
    responsibility principle.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, LLMs stand to gain considerably from the application of task
    decomposition across a range of use cases. This approach aids in maximizing the
    utility and effectiveness of LLMs in problem-solving scenarios by enabling them
    to handle intricate tasks that would be challenging to resolve as a single entity,
    as illustrated in [Figure 4-3](#figure-4-3).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are several examples of LLMs using decomposition:'
  prefs: []
  type: TYPE_NORMAL
- en: Complex problem solving
  prefs: []
  type: TYPE_NORMAL
- en: In instances where a problem is multifaceted and cannot be solved through a
    single prompt, task decomposition is extremely useful. For example, solving a
    complex legal case could be broken down into understanding the case’s context,
    identifying relevant laws, determining legal precedents, and crafting arguments.
    Each subtask can be solved independently by an LLM, providing a comprehensive
    solution when combined.
  prefs: []
  type: TYPE_NORMAL
- en: Content generation
  prefs: []
  type: TYPE_NORMAL
- en: For generating long-form content such as articles or blogs, the task can be
    decomposed into generating an outline, writing individual sections, and then compiling
    and refining the final draft. Each step can be individually managed by GPT-4 for
    better results.
  prefs: []
  type: TYPE_NORMAL
- en: Large document summary
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing lengthy documents such as research papers or reports can be done
    more effectively by decomposing the task into several smaller tasks, like understanding
    individual sections, summarizing them independently, and then compiling a final
    summary.
  prefs: []
  type: TYPE_NORMAL
- en: Interactive conversational agents
  prefs: []
  type: TYPE_NORMAL
- en: For creating advanced chatbots, task decomposition can help manage different
    aspects of conversation such as understanding user input, maintaining context,
    generating relevant responses, and managing dialogue flow.
  prefs: []
  type: TYPE_NORMAL
- en: Learning and tutoring systems
  prefs: []
  type: TYPE_NORMAL
- en: In digital tutoring systems, decomposing the task of teaching a concept into
    understanding the learner’s current knowledge, identifying gaps, suggesting learning
    materials, and evaluating progress can make the system more effective. Each subtask
    can leverage GPT-4’s generative abilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![.Task decomposition with GPT-4.](assets/pega_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. Task decomposition with LLMs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Divide Labor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Task decomposition is a crucial strategy for you to tap into the full potential
    of LLMs. By dissecting complex problems into simpler, manageable tasks, you can
    leverage the problem-solving abilities of these models more effectively and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: In the sections ahead, you’ll learn how to create and integrate multiple LLM
    chains to orchestrate more complicated workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Chaining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often you’ll find that attempting to do a single task within one prompt is impossible.
    You can utilize a mixture of *prompt chaining*, which involves combining multiple
    prompt inputs/outputs with specifically tailored LLM prompts to build up an idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s imagine an example with a film company that would like to partially automate
    their film creation. This could be broken down into several key components, such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: Character creation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plot generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scenes/world building
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Figure 4-4](#figure-4-4) shows what the prompt workflow might look like.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sequential Story Creation Process](assets/pega_0404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. A sequential story creation process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sequential Chain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s decompose the task into *multiple chains* and recompose them into a single
    chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '`character_generation_chain`'
  prefs: []
  type: TYPE_NORMAL
- en: A chain responsible for creating multiple characters given a `'genre'`.
  prefs: []
  type: TYPE_NORMAL
- en: '`plot_generation_chain`'
  prefs: []
  type: TYPE_NORMAL
- en: A chain that will create the plot given the `'characters'` and `'genre'` keys.
  prefs: []
  type: TYPE_NORMAL
- en: '`scene_generation_chain`'
  prefs: []
  type: TYPE_NORMAL
- en: This chain will generate any missing scenes that were not initially generated
    from the `plot_generation_chain`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by creating three separate `ChatPromptTemplate` variables, one
    for each chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Notice that as the prompt templates flow from character to plot and scene generation,
    you add more placeholder variables from the previous steps.
  prefs: []
  type: TYPE_NORMAL
- en: The question remains, how can you guarantee that these extra strings are available
    for your downstream `ChatPromptTemplate` variables?
  prefs: []
  type: TYPE_NORMAL
- en: itemgetter and Dictionary Key Extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Within LCEL you can use the `itemgetter` function from the `operator` package
    to extract keys from the previous step, as long as a dictionary was present within
    the previous step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The `RunnablePassThrough` function simply passes any inputs directly to the
    next step. Then a new dictionary is created by using the same key within the `invoke`
    function; this key is extracted by using `itemgetter("genre")`.
  prefs: []
  type: TYPE_NORMAL
- en: It’s essential to use the `itemgetter` function throughout parts of your LCEL
    chains so that any subsequent `ChatPromptTemplate` placeholder variables will
    always have valid values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, you can use `lambda` or `RunnableLambda` functions within an
    LCEL chain to manipulate previous dictionary values. A lambda is an anonynous
    function within Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that you’re aware of how to use `RunnablePassThrough`, `itemgetter`, and
    `lambda` functions, let’s introduce one final piece of syntax: `RunnableParallel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: First, you import `RunnableParallel` and create two LCEL chains called `master_chain`
    and `master_chain_two`. These are then invoked with exactly the same arguments;
    the `RunnablePassthrough` then passes the dictionary into the second part of the
    chain.
  prefs: []
  type: TYPE_NORMAL
- en: The second part of `master_chain` and `master_chain_two` will return exactly
    the *same result.*
  prefs: []
  type: TYPE_NORMAL
- en: So rather than directly using a dictionary, you can choose to use a `RunnableParallel`
    function instead. These two chain outputs *are interchangeable*, so choose whichever
    syntax you find more comfortable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create three LCEL chains using the prompt templates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: After creating all the chains, you can then attach them to a master LCEL chain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The output is truncated when you see `...` to save space. However, in total
    there were five characters and nine scenes generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The scenes are split into separate items within a Python list. Then two new
    prompts are created to generate both a character script and a summarization prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Technically, you could generate all of the scenes asynchronously. However, it’s
    beneficial to know what each character has done in the *previous scene to avoid
    repeating points*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, you can create two LCEL chains, one for generating the character
    scripts per scene and the other for summarizations of previous scenes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: First, you’ll establish a `character_script_generation_chain` in your script,
    utilizing various runnables like `RunnablePassthrough` for smooth data flow. Crucially,
    this chain integrates model = `ChatOpenAI(model='gpt-3.5-turbo-16k')`, a powerful
    model with a generous 16k context window, ideal for extensive content generation
    tasks. When invoked, this chain adeptly generates character scripts, drawing on
    inputs such as character profiles, genre, and scene specifics.
  prefs: []
  type: TYPE_NORMAL
- en: You dynamically enrich each scene by adding the summary of the previous scene,
    creating a simple yet effective buffer memory. This technique ensures continuity
    and context in the narrative, enhancing the LLM’s ability to generate coherent
    character scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you’ll see how the `StrOutputParser` elegantly converts model
    outputs into structured strings, making the generated content easily usable.
  prefs: []
  type: TYPE_NORMAL
- en: Divide Labor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remember, designing your tasks in a sequential chain greatly benefits from the
    Divide Labor principle. Breaking tasks down into smaller, manageable chains can
    increase the overall quality of your output. Each chain in the sequential chain
    contributes its individual effort toward achieving the overarching task goal.
  prefs: []
  type: TYPE_NORMAL
- en: Using chains gives you the ability to use different models. For example, using
    a smart model for the ideation and a cheap model for the generation usually gives
    optimal results. This also means you can have fine-tuned models on each step.
  prefs: []
  type: TYPE_NORMAL
- en: Structuring LCEL Chains
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In LCEL you must ensure that the first part of your LCEL chain is a *runnable*
    type. The following code will throw an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'A Python dictionary with a value of 18 will not create a runnable LCEL chain.
    However, all of the following implementations will work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Sequential chains are great at incrementally building generated knowledge that
    is used by future chains, but they often yield slower response times due to their
    sequential nature. As such, `SequentialChain` data pipelines are best suited for
    server-side tasks, where immediate responses are not a priority and users aren’t
    awaiting real-time feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Document Chains
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s imagine that before accepting your generated story, the local publisher
    has requested that you provide a summary based on all of the character scripts.
    This is a good use case for *document chains* because you need to provide an LLM
    with a large amount of text that wouldn’t fit within a single LLM request due
    to the context length restrictions.
  prefs: []
  type: TYPE_NORMAL
- en: Before delving into the code, let’s first get a sense of the broader picture.
    The script you are going to see performs a text summarization task on a collection
    of scenes.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to install Pandas with `pip install pandas`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s start with the first set of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: These lines are importing all the necessary tools you need. `CharacterTextSplitter`
    and `load_summarize_chain` are from the LangChain package and will help with text
    processing, while Pandas (imported as `pd`) will help manipulate your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you’ll be dealing with your data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Here, you create a Pandas DataFrame from the `generated_scenes` variable, effectively
    converting your raw scenes into a tabular data format that Pandas can easily manipulate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then you need to consolidate your text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: In this line, you’re transforming the `character_script` column from your DataFrame
    into a single text string. Each entry in the column is converted into a list item,
    and all items are joined together with new lines in between, resulting in a single
    string that contains all character scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have your text ready, you prepare it for the summarization process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Here, you create a `CharacterTextSplitter` instance using its class method `from_tiktoken_encoder`,
    with specific parameters for chunk size and overlap. You then use this text splitter
    to split your consolidated script text into chunks suitable for processing by
    your summarization tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you set up your summarization tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: This line is about setting up your summarization process. You’re calling a function
    that loads a summarization chain with a chat model in a `map-reduce` style approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then you run the summarization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: This is where you actually perform the text summarization. The `invoke` method
    executes the summarization on the chunks of text you prepared earlier and stores
    the summary into a variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you print the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: This is the culmination of all your hard work. The resulting summary text is
    printed to the console for you to see.
  prefs: []
  type: TYPE_NORMAL
- en: 'This script takes a collection of scenes, consolidates the text, chunks it
    up, summarizes it, and then prints the summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: It’s worth noting that even though you’ve used a `map_reduce` chain, there are
    four core chains for working with `Document` objects within LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: Stuff
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The document insertion chain, also referred to as the *stuff* chain (drawing
    from the concept of *stuffing* or *filling*), is the simplest approach among various
    document chaining strategies. [Figure 4-5](#figure-4-5) illustrates the process
    of integrating multiple documents into a single LLM request.
  prefs: []
  type: TYPE_NORMAL
- en: '![Stuff Documents Chain](assets/pega_0405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. Stuff documents chain
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Refine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The refine documents chain ([Figure 4-6](#figure-4-6)) creates an LLM response
    through a cyclical process that *iteratively updates its output*. During each
    loop, it combines the current output (derived from the LLM) with the current document.
    Another LLM request is made to *update the current output*. This process continues
    until all documents have been processed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refine Documents Chain](assets/pega_0406.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. Refine documents chain
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Map Reduce
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The map reduce documents chain in [Figure 4-7](#figure-4-7) starts with an LLM
    chain to each separate document (a process known as the Map step), interpreting
    the resulting output as a newly generated document.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, all these newly created documents are introduced to a distinct
    combine documents chain to formulate a singular output (a process referred to
    as the Reduce step). If necessary, to ensure the new documents seamlessly fit
    into the context length, an optional compression process is used on the mapped
    documents. If required, this compression happens recursively.
  prefs: []
  type: TYPE_NORMAL
- en: '![Map Reduce Chain](assets/pega_0407.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-7\. Map reduce documents chain
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Map Re-rank
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is also map re-rank, which operates by executing an initial prompt on
    each document. This not only strives to fulfill a given task but also assigns
    a confidence score reflecting the certainty of its answer. The response with the
    highest confidence score is then selected and returned.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 4-1](#table-4-1) demonstrates the advantages and disadvantages for choosing
    a specific document chain strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-1\. Overview of document chain strategies
  prefs: []
  type: TYPE_NORMAL
- en: '| Approach | Advantages | Disadvantages |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Stuff Documents Chain | Simple to implement. Ideal for scenarios with small
    documents and few inputs. | May not be suitable for handling large documents or
    multiple inputs due to prompt size limitation. |'
  prefs: []
  type: TYPE_TB
- en: '| Refine Documents Chain | Allows iterative refining of the response. More
    control over each step of response generation. Good for progressive extraction
    tasks. | Might not be optimal for real-time applications due to the loop process.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Map Reduce Documents Chain | Enables independent processing of each document.
    Can handle large datasets by reducing them into manageable chunks. | Requires
    careful management of the process. Optional compression step can add complexity
    and loses document order. |'
  prefs: []
  type: TYPE_TB
- en: '| Map Re-rank Documents Chain | Provides a confidence score for each answer,
    allowing for better selection of responses. | The ranking algorithm can be complex
    to implement and manage. May not provide the best answer if the scoring mechanism
    is not reliable or well-tuned. |'
  prefs: []
  type: TYPE_TB
- en: You can read more about how to implement different document chains in [LangChain’s
    comprehensive API](https://oreil.ly/FQUK_) and [here](https://oreil.ly/9xr_6).
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, it’s possible to simply change the chain type within the `load_summarize_chain`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: There are newer, more customizable approaches to creating summarization chains
    using LCEL, but for most of your needs `load_summarize_chain` provides sufficient
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you comprehensively reviewed the LangChain framework and its
    essential components. You learned about the importance of document loaders for
    gathering data and the role of text splitters in handling large text blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, you were introduced to the concepts of task decomposition and prompt
    chaining. By breaking down complex problems into smaller tasks, you saw the power
    of problem isolation. Furthermore, you now grasp how prompt chaining can combine
    multiple inputs/outputs for richer idea generation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you’ll learn about vector databases, including how to integrate
    these with documents from LangChain, and this ability will serve a pivotal role
    in enhancing the accuracy of knowledge extraction from your data.
  prefs: []
  type: TYPE_NORMAL
