- en: Chapter 4\. Advanced Techniques for Text Generation with LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using simple prompt engineering techniques will often work for most tasks,
    but occasionally you’ll need to use a more powerful toolkit to solve complex generative
    AI problems. Such problems and tasks include:'
  prefs: []
  type: TYPE_NORMAL
- en: Context length
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing an entire book into a digestible synopsis.
  prefs: []
  type: TYPE_NORMAL
- en: Combining sequential LLM inputs/outputs
  prefs: []
  type: TYPE_NORMAL
- en: Creating a story for a book including the characters, plot, and world building.
  prefs: []
  type: TYPE_NORMAL
- en: Performing complex reasoning tasks
  prefs: []
  type: TYPE_NORMAL
- en: LLMs acting as an agent. For example, you could create an LLM agent to help
    you achieve your personal fitness goals.
  prefs: []
  type: TYPE_NORMAL
- en: To skillfully tackle such complex generative AI challenges, becoming acquainted
    with LangChain, an open source framework, is highly beneficial. This tool simplifies
    and enhances your LLM’s workflows substantially.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LangChain is a versatile framework that enables the creation of applications
    utilizing LLMs and is available as both a [Python](https://oreil.ly/YPid-) and
    a [TypeScript](https://oreil.ly/5Vl0W) package. Its central tenet is that the
    most impactful and distinct applications won’t merely interface with a language
    model via an API, but will also:'
  prefs: []
  type: TYPE_NORMAL
- en: Enhance data awareness
  prefs: []
  type: TYPE_NORMAL
- en: The framework aims to establish a seamless connection between a language model
    and external data sources.
  prefs: []
  type: TYPE_NORMAL
- en: Enhance agency
  prefs: []
  type: TYPE_NORMAL
- en: It strives to equip language models with the ability to engage with and influence
    their environment.
  prefs: []
  type: TYPE_NORMAL
- en: The LangChain framework illustrated in [Figure 4-1](#figure-4-1) provides a
    range of modular abstractions that are essential for working with LLMs, along
    with a broad selection of implementations for these abstractions.
  prefs: []
  type: TYPE_NORMAL
- en: '![pega 0401](assets/pega_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. The major modules of the LangChain LLM framework
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Each module is designed to be user-friendly and can be efficiently utilized
    independently or together. There are currently six common modules within LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: Model I/O
  prefs: []
  type: TYPE_NORMAL
- en: Handles input/output operations related to the model
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval
  prefs: []
  type: TYPE_NORMAL
- en: Focuses on retrieving relevant text for the LLM
  prefs: []
  type: TYPE_NORMAL
- en: Chains
  prefs: []
  type: TYPE_NORMAL
- en: Also known as *LangChain runnables*, chains enable the construction of sequences
    of LLM operations or function calls
  prefs: []
  type: TYPE_NORMAL
- en: Agents
  prefs: []
  type: TYPE_NORMAL
- en: Allows chains to make decisions on which tools to use based on high-level directives
    or instructions
  prefs: []
  type: TYPE_NORMAL
- en: Memory
  prefs: []
  type: TYPE_NORMAL
- en: Persists the state of an application between different runs of a chain
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks
  prefs: []
  type: TYPE_NORMAL
- en: For running additional code on specific events, such as when every new token
    is generated
  prefs: []
  type: TYPE_NORMAL
- en: Environment Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can install LangChain on your terminal with either of these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pip install langchain langchain-openai`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conda install -c conda-forge langchain langchain-openai`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you would prefer to install the package requirements for the entire book,
    you can use the [*requirements.txt*](https://oreil.ly/WKOma) file from the GitHub
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s recommended to install the packages within a virtual environment:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a virtual environment
  prefs: []
  type: TYPE_NORMAL
- en: '`python -m venv venv`'
  prefs: []
  type: TYPE_NORMAL
- en: Activate the virtual environment
  prefs: []
  type: TYPE_NORMAL
- en: '`source venv/bin/activate`'
  prefs: []
  type: TYPE_NORMAL
- en: Install the dependencies
  prefs: []
  type: TYPE_NORMAL
- en: '`pip install -r requirements.txt`'
  prefs: []
  type: TYPE_NORMAL
- en: LangChain requires integrations with one or more model providers. For example,
    to use OpenAI’s model APIs, you’ll need to install their Python package with `pip
    install openai`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed in [Chapter 1](ch01.html#five_principles_01), it’s best practice
    to set an environment variable called `OPENAI_API_KEY` in your terminal or load
    it from an *.env* file using [`python-dotenv`](https://oreil.ly/wvuO7). However,
    for prototyping you can choose to skip this step by passing in your API key directly
    when loading a chat model in LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Hardcoding API keys in scripts is not recommended due to security reasons. Instead,
    utilize environment variables or configuration files to manage your keys.
  prefs: []
  type: TYPE_NORMAL
- en: In the constantly evolving landscape of LLMs, you can encounter the challenge
    of disparities across different model APIs. The lack of standardization in interfaces
    can induce extra layers of complexity in prompt engineering and obstruct the seamless
    integration of diverse models into your projects.
  prefs: []
  type: TYPE_NORMAL
- en: This is where LangChain comes into play. As a comprehensive framework, LangChain
    allows you to easily consume the varying interfaces of different models.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain’s functionality ensures that you aren’t required to reinvent your
    prompts or code every time you switch between models. Its platform-agnostic approach
    promotes rapid experimentation with a broad range of models, such as [Anthropic](https://www.anthropic.com),
    [Vertex AI](https://cloud.google.com/vertex-ai), [OpenAI](https://openai.com),
    and [BedrockChat](https://oreil.ly/bedrock). This not only expedites the model
    evaluation process but also saves critical time and resources by simplifying complex
    model integrations.
  prefs: []
  type: TYPE_NORMAL
- en: In the sections that follow, you’ll be using the OpenAI package and their API
    in LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: Chat Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chat models such as GPT-4 have become the primary way to interface with OpenAI’s
    API. Instead of offering a straightforward “input text, output text” response,
    they propose an interaction method where *chat messages* are the input and output
    elements.
  prefs: []
  type: TYPE_NORMAL
- en: Generating LLM responses using chat models involves inputting one or more messages
    into the chat model. In the context of LangChain, the currently accepted message
    types are `AIMessage`, `HumanMessage`, and `SystemMessage`. The output from a
    chat model will always be an `AIMessage`.
  prefs: []
  type: TYPE_NORMAL
- en: SystemMessage
  prefs: []
  type: TYPE_NORMAL
- en: Represents information that should be instructions to the AI system. These are
    used to guide the AI’s behavior or actions in some way.
  prefs: []
  type: TYPE_NORMAL
- en: HumanMessage
  prefs: []
  type: TYPE_NORMAL
- en: Represents information coming from a human interacting with the AI system. This
    could be a question, a command, or any other input from a human user that the
    AI needs to process and respond to.
  prefs: []
  type: TYPE_NORMAL
- en: AIMessage
  prefs: []
  type: TYPE_NORMAL
- en: Represents information coming from the AI system itself. This is typically the
    AI’s response to a `HumanMessage` or the result of a `SystemMessage` instruction.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Make sure to leverage the `SystemMessage` for delivering explicit directions.
    OpenAI has refined GPT-4 and upcoming LLM models to pay particular attention to
    the guidelines given within this type of message.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create a joke generator in LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]` First, you’ll import `ChatOpenAI`, `AIMessage`, `HumanMessage`, and
    `SystemMessage`. Then create an instance of the `ChatOpenAI` class with a temperature
    parameter of 0.5 (randomness).    After creating a model, a list named `messages`
    is populated with a `SystemMessage` object, defining the role for the LLM, and
    a `HumanMessage` object, which asks for a software engineer—related joke.    Calling
    the chat model with `.invoke(input=messages)` feeds the LLM with a list of messages,
    and then you retrieve the LLM’s response with `response.content`.    There is
    a legacy method that allows you to directly call the `chat` object with `chat(messages=messages)`:    [PRE4]
    [PRE5]``  [PRE6][PRE7]`` [PRE8][PRE9]` # Streaming Chat Models    You might have
    observed while using ChatGPT how words are sequentially returned to you, one character
    at a time. This distinct pattern of response generation is referred to as *streaming*,
    and it plays a crucial role in enhancing the performance of chat-based applications:    [PRE10]    When
    you call `chat.stream(messages)`, it yields chunks of the message one at a time.
    This means each segment of the chat message is individually returned. As each
    chunk arrives, it is then instantaneously printed to the terminal and flushed.
    This way, *streaming* allows for minimal latency from your LLM responses.    Streaming
    holds several benefits from an end-user perspective. First, it dramatically reduces
    the waiting time for users. As soon as the text starts generating character by
    character, users can start interpreting the message. There’s no need for a full
    message to be constructed before it is seen. This, in turn, significantly enhances
    user interactivity and minimizes latency.    Nevertheless, this technique comes
    with its own set of challenges. One significant challenge is parsing the outputs
    while they are being streamed. Understanding and appropriately responding to the
    message as it is being formed can prove to be intricate, especially when the content
    is complex and detailed.    # Creating Multiple LLM Generations    There may be
    scenarios where you find it useful to generate multiple responses from LLMs. This
    is particularly true while creating dynamic content like social media posts. Rather
    than providing a list of messages, you provide a *list of message lists*.    Input:    [PRE11]    Output:    [PRE12]    The
    benefit of using `.batch()` over `.invoke()` is that you can parallelize the number
    of API requests made to OpenAI.    For any runnable in LangChain, you can add
    a `RunnableConfig` argument to the `batch` function that contains many configurable
    parameters, including `max_``concurrency`:    [PRE13]    ###### Note    In computer
    science, *asynchronous (async) functions* are those that operate independently
    of other processes, thereby enabling several API requests to be run concurrently
    without waiting for each other. In LangChain, these async functions let you make
    many API requests all at once, not one after the other. This is especially helpful
    in more complex workflows and decreases the overall latency to your users.    Most
    of the asynchronous functions within LangChain are simply prefixed with the letter
    `a`, such as `.ainvoke()` and `.abatch()`. If you would like to use the async
    API for more efficient task performance, then utilize these functions.    # LangChain
    Prompt Templates    Up until this point, you’ve been hardcoding the strings in
    the `ChatOpenAI` objects. As your LLM applications grow in size, it becomes increasingly
    important to utilize *prompt templates*.    *Prompt templates* are good for generating
    reproducible prompts for AI language models. They consist of a *template*, a text
    string that can take in parameters, and construct a text prompt for a language
    model.    Without prompt templates, you would likely use Python `f-string` formatting:    [PRE14]    But
    why not simply use an `f-string` for prompt templating? Using LangChain’s prompt
    templates instead allows you to easily:    *   Validate your prompt inputs           *   Combine
    multiple prompts together with composition           *   Define custom selectors
    that will inject k-shot examples into your prompt           *   Save and load
    prompts from *.yml* and *.json* files           *   Create custom prompt templates
    that execute additional code or instructions when created              # LangChain
    Expression Language (LCEL)    The `|` pipe operator is a key component of LangChain
    Expression Language (LCEL) that allows you to chain together different components
    or *runnables* in a data processing pipeline.    In LCEL, the `|` operator is
    similar to the Unix pipe operator. It takes the output of one component and feeds
    it as input to the next component in the chain. This allows you to easily connect
    and combine different components to create a complex chain of operations:    [PRE15]    The
    `|` operator is used to chain together the prompt and model components. The output
    of the prompt component is passed as input to the model component. This chaining
    mechanism allows you to build complex chains from basic components and enables
    the seamless flow of data between different stages of the processing pipeline.    Additionally,
    *the order matters*, so you could technically create this chain:    [PRE16]    But
    it would produce an error after using the `invoke` function, because the values
    returned from `model` are not compatible with the expected inputs for the prompt.    Let’s
    create a business name generator using prompt templates that will return five
    to seven relevant business names:    [PRE17]   [PRE18] 1\. SummarAI 2\. MediSummar
    3\. AutoDocs 4\. RecordAI 5\. SmartSummarize [PRE19]`  [PRE20][PRE21] # Using
    PromptTemplate with Chat Models    LangChain provides a more traditional template
    called `PromptTemplate`, which requires `input_variables` and `template` arguments.    Input:    [PRE22]    Output:    [PRE23]   [PRE24][PRE25]``py[PRE26]`py`
    [PRE27]py `"transaction_category": "``{``mistral_category``}``",`  `"transaction_type":
    "``{``mistral_type``}``"` `}}``"""`  `reference_data` `=` `f``"""``{{` [PRE28]`py
    [PRE29]py`` [PRE30]`  [PRE31][PRE32]py[PRE33]  [PRE34]` [PRE35] # Creating Few-Shot
    Prompt Templates    Working with the generative capabilities of LLMs often involves
    making a choice between *zero-shot* and *few-shot learning (k-shot)*. While zero-shot
    learning requires no explicit examples and adapts to tasks based solely on the
    prompt, its dependence on the pretraining phase means it may not always yield
    precise results.    On the other hand, with few-shot learning, which involves
    providing a few examples of the desired task performance in the prompt, you have
    the opportunity to optimize the model’s behavior, leading to more desirable outputs.    Due
    to the token LLM context length, you will often finding yourself competing between
    adding lots of high-quality k-shot examples into your prompts while still aiming
    to generate an effective and deterministic LLM output.    ###### Note    Even
    as the token context window limit within LLMs continues to increase, providing
    a specific number of k-shot examples helps you minimize API costs.    Let’s explore
    two methods for adding k-shot examples into your prompts with *few-shot prompt
    templates*: using *fixed examples* and using an *example selector*.    ## Fixed-Length
    Few-Shot Examples    First, let’s look at how to create a few-shot prompt template
    using a fixed number of examples. The foundation of this method lies in creating
    a robust set of few-shot examples:    [PRE36]py    Each example is a dictionary
    containing a `question` and `answer` key that will be used to create pairs of
    `HumanMessage` and `AIMessage` messages.    ## Formatting the Examples    Next,
    you’ll configure a `ChatPromptTemplate` for formatting the individual examples,
    which will then be inserted into a `FewShotChatMessagePromptTemplate`.    Input:    [PRE37]py    Output:    [PRE38]py    Notice
    how `example_prompt` will create `HumanMessage` and `AIMessage` pairs with the
    prompt inputs of `{question}` and `{answer}`.    After running `few_shot_prompt.format()`,
    the few-shot examples are printed as a string. As you’d like to use these within
    a `ChatOpenAI()` LLM request, let’s create a new `ChatPromptTemplate`.    Input:    [PRE39]py    Output:    [PRE40]py    After
    invoking the LCEL chain on `final_prompt`, your few-shot examples are added after
    the `SystemMessage`.    Notice that the LLM only returns `''Washington, D.C.''`
    This is because after the LLMs response is returned, *it is parsed* by `StrOutputParser()`,
    an output parser. Adding `StrOutputParser()` is a common way to ensure that LLM
    responses in chains *return string values*. You’ll explore this more in depth
    while learning sequential chains in LCEL.    ## Selecting Few-Shot Examples by
    Length    Before diving into the code, let’s outline your task. Imagine you’re
    building a storytelling application powered by GPT-4\. A user enters a list of
    character names with previously generated stories. However, each user’s list of
    characters might have a different length. Including too many characters might
    generate a story that surpasses the LLM’s context window limit. That’s where you
    can use `LengthBasedExampleSelector` to adapt the prompt according to the length
    of user input:    [PRE41]py    First, you set up a `PromptTemplate` that takes
    two input variables for each example. Then `LengthBasedExampleSelector` adjusts
    the number of examples according to the *length of the examples input*, ensuring
    your LLM doesn’t generate a story beyond its context window.    Also, you’ve customized
    the `get_text_length` function to use the `num_tokens_from_string` function that
    counts the total number of tokens using `tiktoken`. This means that `max_length=1000`
    represents the *number of tokens* rather than using the following default function:    `get_text_length:
    Callable[[str], int] = lambda x: len(re.split("\n| ", x))`    Now, to tie all
    these elements together:    [PRE42]py    Output:    [PRE43]py    # Provide Examples
    and Specify Format    When working with few-shot examples, the length of the content
    matters in determining how many examples the AI model can take into account. Tune
    the length of your input content and provide apt examples for efficient results
    to prevent the LLM from generating content that might surpass its context window
    limit.    After formatting the prompt, you create a chat model with `ChatOpenAI()`
    and load the formatted prompt into a `SystemMessage` that creates a small story
    about Frodo from *Lord of the Rings*.    Rather than creating and formatting a
    `ChatPromptTemplate`, it’s often much easier to simply invoke a `SystemMesage`
    with a formatted prompt:    [PRE44]py    # Limitations with Few-Shot Examples    Few-shot
    learning has limitations. Although it can prove beneficial in certain scenarios,
    it might not always yield the expected high-quality results. This is primarily
    due to two reasons:    *   Pretrained models like GPT-4 can sometimes overfit
    to the few-shot examples, making them prioritize the examples over the actual
    prompt.           *   LLMs have a token limit. As a result, there will always
    be a trade-off between the number of examples and the length of the response.
    Providing more examples might limit the response length and vice versa.              These
    limitations can be addressed in several ways. First, if few-shot prompting is
    not yielding the desired results, consider using differently framed phrases or
    experimenting with the language of the prompts themselves. Variations in how the
    prompt is phrased can result in different responses, highlighting the trial-and-error
    nature of prompt engineering.    Second, think about including explicit instructions
    to the model to ignore the examples after it understands the task or to use the
    examples just for formatting guidance. This might influence the model to not overfit
    to the examples.    If the tasks are complex and the performance of the model
    with few-shot learning is not satisfactory, you might need to consider [fine-tuning](https://oreil.ly/S40bZ)
    your model. Fine-tuning provides a more nuanced understanding of a specific task
    to the model, thus improving the performance significantly.    # Saving and Loading
    LLM Prompts    To effectively leverage generative AI models such as GPT-4, it
    is beneficial to store prompts as files instead of Python code. This approach
    enhances the shareability, storage, and versioning of your prompts.    LangChain
    supports both saving and loading prompts from JSON and YAML. Another key feature
    of LangChain is its support for detailed specification in one file or distributed
    across multiple files. This means you have the flexibility to store different
    components such as templates, examples, and others in distinct files and reference
    them as required.    Let’s learn how to save and load prompts:    [PRE45]py    After
    importing `PromptTemplate` and `load_prompt` from the `langchain.prompts` module,
    you define a `PromptTemplate` for English-to-Spanish translation tasks and save
    it as *translation_prompt.json*. Finally, you load the saved prompt template using
    the `load_prompt` function, which returns an instance of `PromptTemplate`.    ######
    Warning    Please be aware that LangChain’s prompt saving may not work with all
    types of prompt templates. To mitigate this, you can utilize the *pickle* library
    or *.txt* files to read and write any prompts that LangChain does not support.    You’ve
    learned how to create few-shot prompt templates using LangChain with two techniques:
    a fixed number of examples and using an example selector.    The former creates
    a set of few-shot examples and uses a `ChatPromptTemplate` object to format these
    into chat messages. This forms the basis for creating a `FewShotChatMessagePromptTemplate`
    object.    The latter approach, using an example selector, is handy when user
    input varies significantly in length. In such scenarios, a `LengthBasedExampleSelector`
    can be utilized to adjust the number of examples based on user input length. This
    ensures your LLM does not exceed its context window limit.    Moreover, you’ve
    seen how easy it is to store/load prompts as files, enabling enhanced shareability,
    storage, and versioning.    # Data Connection    Harnessing an LLM application,
    coupled with your data, uncovers a plethora of opportunities to boost efficiency
    while refining your decision-making processes.    Your organization’s data may
    manifest in various forms:    Unstructured data      This could include Google
    Docs, threads from communication platforms such as Slack or Microsoft Teams, web
    pages, internal documentation, or code repositories on GitHub.      Structured
    data      Data neatly housed within SQL, NoSQL, or Graph databases.      To query
    your unstructured data, a process of loading, transforming, embedding, and subsequently
    storing it within a vector database is necessary. A *vector database* is a specialized
    type of database designed to efficiently store and query data in the form of vectors,
    which represent complex data like text or images in a format suitable for machine
    learning and similarity search.    As for structured data, given its already indexed
    and stored state, you can utilize a LangChain agent to conduct an intermediate
    query on your database. This allows for the extraction of specific features, which
    can then be used within your LLM prompts.    There are multiple Python packages
    that can help with your data ingestion, including [Unstructured](https://oreil.ly/n0hDD),
    [LlamaIndex](https://www.llamaindex.ai), and [LangChain](https://oreil.ly/PjV9o).    [Figure 4-2](#figure-4-2)
    illustrates a standardized approach to data ingestion. It begins with the data
    sources, which are then loaded into documents. These documents are then chunked
    and stored within a vector database for later retrieval.  ![Data Connection](assets/pega_0402.png)  ######
    Figure 4-2\. A data connection to retrieval pipeline    In particular LangChain
    equips you with essential components to load, modify, store, and retrieve your
    data:    Document loaders      These facilitate uploading informational resources,
    or *documents*, from a diverse range of sources such as Word documents, PDF files,
    text files, or even web pages.      Document transformers      These tools allow
    the segmentation of documents, conversion into a Q&A layout, elimination of superfluous
    documents, and much more.      Text embedding models      These can transform
    unstructured text into a sequence of floating-point numbers used for similarity
    search by vector stores.      Vector databases (vector stores)      These databases
    can save and execute searches over embedded data.      Retrievers      These tools
    offer the capability to query and retrieve data.      Also, it’s worth mentioning
    that other LLM frameworks such as [LlamaIndex](https://oreil.ly/9NcTB) work seamlessly
    with LangChain. [LlamaHub](https://llamahub.ai) is another open source library
    dedicated to document loaders and can create LangChain-specific `Document` objects.    #
    Document Loaders    Let’s imagine you’ve been tasked with building an LLM data
    collection pipeline for NutriFusion Foods. The information that you need to gather
    for the LLM is contained within:    *   A PDF of a book called *Principles of
    Marketing*           *   Two *.docx* marketing reports in a public Google Cloud
    Storage bucket           *   Three *.csv* files showcasing the marketing performance
    data for 2021, 2022, and 2023              Create a new Jupyter Notebook or Python
    file in *content/chapter_4* of the [shared repository](https://oreil.ly/cVTyI),
    and then run `pip install pdf2image docx2txt pypdf`, which will install three
    packages.    All of the data apart from *.docx* files can be found in [*content/chapter_4/data*](https://oreil.ly/u9gMx).
    You can start by importing all of your various data loaders and creating an empty
    `all_documents` list to store all of the `Document` objects across your data sources.    Input:    [PRE46]py    Output:    [PRE47]py    Then
    using `PyPDFLoader`, you can import a *.pdf* file and split it into multiple pages
    using the `.load_and_split()` function.    Additionally, it’s possible to add
    extra metadata to each page because the metadata is a Python dictionary on each
    `Document` object. Also, notice in the preceding output for `Document` objects
    the metadata `source` is attached to.    Using the package `glob`, you can easily
    find all of the *.csv* files and individually load these into LangChain `Document`
    objects with a `CSVLoader`.    Finally, the two marketing reports are loaded from
    a public Google Cloud Storage bucket and are then split into 200 token-chunk sizes
    using a `text_splitter`.    This section equipped you with the necessary knowledge
    to create a comprehensive document-loading pipeline for NutriFusion Foods’ LLM.
    Starting with data extraction from a PDF, several CSV files and two .*docx* files,
    each document was enriched with relevant metadata for better context.    You now
    have the ability to seamlessly integrate data from a variety of document sources
    into a cohesive data pipeline.    # Text Splitters    Balancing the length of
    each document is also a crucial factor. If a document is too lengthy, it may surpass
    the *context length* of the LLM (the maximum number of tokens that an LLM can
    process within a single request). But if the documents are excessively fragmented
    into smaller chunks, there’s a risk of losing significant contextual information,
    which is equally undesirable.    You might encounter specific challenges while
    text splitting, such as:    *   Special characters such as hashtags, @ symbols,
    or links might not split as anticipated, affecting the overall structure of the
    split documents.           *   If your document contains intricate formatting
    like tables, lists, or multilevel headings, the text splitter might find it difficult
    to retain the original formatting.              There are ways to overcome these
    challenges that we’ll explore later.    This section introduces you to text splitters
    in LangChain, tools utilized to break down large chunks of text to better adapt
    to your model’s context window.    ###### Note    There isn’t a perfect document
    size. Start by using good heuristics and then build a training/test set that you
    can use for LLM evaluation.    LangChain provides a range of text splitters so
    that you can easily split by any of the following:    *   Token count           *   Recursively
    by multiple characters           *   Character count           *   Code           *   Markdown
    headers              Let’s explore three popular splitters: `CharacterTextSplitter`,
    `TokenTextSplitter`, and `RecursiveCharacterTextSplitter`.    # Text Splitting
    by Length and Token Size    In [Chapter 3](ch03.html#standard_practices_03), you
    learned how to count the number of tokens within a GPT-4 call with [tiktoken](https://oreil.ly/uz05O).
    You can also use tiktoken to split strings into appropriately sized chunks and
    documents.    Remember to install tiktoken and langchain-text-splitters with `pip
    install tiktoken langchain-text-splitters`.    To split by token count in LangChain,
    you can use a `CharacterTextSplitter` with a `.from_tiktoken_encoder()` function.    You’ll
    initially create a `CharacterTextSplitter` with a chunk size of 50 characters
    and no overlap. Using the `split_text` method, you’re chopping the text into pieces
    and then printing out the total number of chunks created.    Then you’ll do the
    same thing, but this time with a *chunk overlap* of 48 characters. This shows
    how the number of chunks changes based *on whether you allow overlap*, illustrating
    the impact of these settings on how your text gets divided:    [PRE48]py    Output:    [PRE49]py    In
    the previous section, you used the following to load and split the *.pdf* into
    LangChain documents:    *   `pages = loader.load_and_split()`    It’s possible
    for you to have more granular control on the size of each document by creating
    a `TextSplitter` and attaching it to your `Document` loading pipelines:    *   `def
    load_and_split(text_splitter: TextSplitter | None = None) -> List[Document]`    Simply
    create a `TokenTextSplitter` with a `chunk_size=500` and a `chunk_overlap` of
    50:    [PRE50]py    The *Principles of Marketing* book contains 497 pages, but
    after using a `TokenTextSplitter` with a `chunk_size` of 500 tokens, you’ve created
    776 smaller LangChain `Document` objects.    # Text Splitting with Recursive Character
    Splitting    Dealing with sizable blocks of text can present unique challenges
    in text analysis. A helpful strategy for such situations involves the use of *recursive
    character splitting*. This method facilitates the division of a large body of
    text into manageable segments, making further analysis more accessible.    This
    approach becomes incredibly effective when handling generic text. It leverages
    a list of characters as parameters and sequentially splits the text based on these
    characters. The resulting sections continue to be divided until they reach an
    acceptable size. By default, the character list comprises `"\n\n"`, `"\n"`, `"
    "`, and `""`. This arrangement aims to retain the integrity of paragraphs, sentences,
    and words, preserving the semantic context.    The process hinges on the character
    list provided and sizes the resulting sections based on the character count.    Before
    diving into the code, it’s essential to understand what the `RecursiveCharacterTextSplitter`
    does. It takes a text and a list of delimiters (characters that define the boundaries
    for splitting the text). Starting from the first delimiter in the list, the splitter
    attempts to divide the text. If the resulting chunks are still too large, it proceeds
    to the next delimiter, and so on. This process continues *recursively* until the
    chunks are small enough or all delimiters are exhausted.    Using the preceding
    `text` variable, start by importing `RecursiveCharacterText​Splitter`. This instance
    will be responsible for splitting the text. When initializing the splitter, parameters
    `chunk_size`, `chunk_overlap`, and `length_function` are set. Here, `chunk_size`
    is set to 100, and `chunk_overlap` to 20.    The `length_function` is defined
    as `len` to determine the size of the chunks. It’s also possible to modify the
    `length_function` argument to use a tokenizer count instead of using the default
    `len` function, which will count characters:    [PRE51]py    Once the `text_splitter`
    instance is ready, you can use `.split_text` to split the `text` variable into
    smaller chunks. These chunks are stored in the `texts` Python list:    [PRE52]py    As
    well as simply splitting the text with overlap into a list of strings, you can
    easily create LangChain `Document` objects with the `.create_documents` function.
    Creating `Document` objects is useful because it allows you to:    *   Store documents
    within a vector database for semantic search           *   Add metadata to specific
    pieces of text           *   Iterate over multiple documents to create a higher-level
    summary              To add metadata, provide a list of dictionaries to the `metadatas`
    argument:    [PRE53]py    But what if your existing `Document` objects are too
    long?    You can easily handle that by using the `.split_documents` function with
    a `TextSplitter`. This will take in a list of `Document` objects and will return
    a new list of `Document` objects based on your `TextSplitter` class argument settings:    [PRE54]py    You’ve
    now gained the ability to craft an efficient data loading pipeline, leveraging
    sources such as PDFs, CSVs, and Google Cloud Storage links. Furthermore, you’ve
    learned how to enrich the collected documents with relevant metadata, providing
    meaningful context for analysis and prompt engineering.    With the introduction
    of text splitters, you can now strategically manage document sizes, optimizing
    for both the LLM’s context window and the preservation of context-rich information.
    You’ve navigated handling larger texts by employing recursion and character splitting.
    This newfound knowledge empowers you to work seamlessly with various document
    sources and integrate them into a robust data pipeline.    # Task Decomposition    *Task
    decomposition* is the strategic process of dissecting complex problems into a
    suite of manageable subproblems. This approach aligns seamlessly with the natural
    tendencies of software engineers, who often conceptualize tasks as interrelated
    subcomponents.    In software engineering, by utilizing task decomposition you
    can reduce cognitive burden and harness the advantages of problem isolation and
    adherence to the single responsibility principle.    Interestingly, LLMs stand
    to gain considerably from the application of task decomposition across a range
    of use cases. This approach aids in maximizing the utility and effectiveness of
    LLMs in problem-solving scenarios by enabling them to handle intricate tasks that
    would be challenging to resolve as a single entity, as illustrated in [Figure 4-3](#figure-4-3).    Here
    are several examples of LLMs using decomposition:    Complex problem solving      In
    instances where a problem is multifaceted and cannot be solved through a single
    prompt, task decomposition is extremely useful. For example, solving a complex
    legal case could be broken down into understanding the case’s context, identifying
    relevant laws, determining legal precedents, and crafting arguments. Each subtask
    can be solved independently by an LLM, providing a comprehensive solution when
    combined.      Content generation      For generating long-form content such as
    articles or blogs, the task can be decomposed into generating an outline, writing
    individual sections, and then compiling and refining the final draft. Each step
    can be individually managed by GPT-4 for better results.      Large document summary      Summarizing
    lengthy documents such as research papers or reports can be done more effectively
    by decomposing the task into several smaller tasks, like understanding individual
    sections, summarizing them independently, and then compiling a final summary.      Interactive
    conversational agents      For creating advanced chatbots, task decomposition
    can help manage different aspects of conversation such as understanding user input,
    maintaining context, generating relevant responses, and managing dialogue flow.      Learning
    and tutoring systems      In digital tutoring systems, decomposing the task of
    teaching a concept into understanding the learner’s current knowledge, identifying
    gaps, suggesting learning materials, and evaluating progress can make the system
    more effective. Each subtask can leverage GPT-4’s generative abilities.    ![.Task
    decomposition with GPT-4.](assets/pega_0403.png)  ###### Figure 4-3\. Task decomposition
    with LLMs    # Divide Labor    Task decomposition is a crucial strategy for you
    to tap into the full potential of LLMs. By dissecting complex problems into simpler,
    manageable tasks, you can leverage the problem-solving abilities of these models
    more effectively and efficiently.    In the sections ahead, you’ll learn how to
    create and integrate multiple LLM chains to orchestrate more complicated workflows.    #
    Prompt Chaining    Often you’ll find that attempting to do a single task within
    one prompt is impossible. You can utilize a mixture of *prompt chaining*, which
    involves combining multiple prompt inputs/outputs with specifically tailored LLM
    prompts to build up an idea.    Let’s imagine an example with a film company that
    would like to partially automate their film creation. This could be broken down
    into several key components, such as:    *   Character creation           *   Plot
    generation           *   Scenes/world building              [Figure 4-4](#figure-4-4)
    shows what the prompt workflow might look like.  ![Sequential Story Creation Process](assets/pega_0404.png)  ######
    Figure 4-4\. A sequential story creation process    ## Sequential Chain    Let’s
    decompose the task into *multiple chains* and recompose them into a single chain:    `character_generation_chain`      A
    chain responsible for creating multiple characters given a `''genre''`.      `plot_generation_chain`      A
    chain that will create the plot given the `''characters''` and `''genre''` keys.      `scene_generation_chain`      This
    chain will generate any missing scenes that were not initially generated from
    the `plot_generation_chain`.      Let’s start by creating three separate `ChatPromptTemplate`
    variables, one for each chain:    [PRE55]py    Notice that as the prompt templates
    flow from character to plot and scene generation, you add more placeholder variables
    from the previous steps.    The question remains, how can you guarantee that these
    extra strings are available for your downstream `ChatPromptTemplate` variables?    ##
    itemgetter and Dictionary Key Extraction    Within LCEL you can use the `itemgetter`
    function from the `operator` package to extract keys from the previous step, as
    long as a dictionary was present within the previous step:    [PRE56]py    The
    `RunnablePassThrough` function simply passes any inputs directly to the next step.
    Then a new dictionary is created by using the same key within the `invoke` function;
    this key is extracted by using `itemgetter("genre")`.    It’s essential to use
    the `itemgetter` function throughout parts of your LCEL chains so that any subsequent
    `ChatPromptTemplate` placeholder variables will always have valid values.    Additionally,
    you can use `lambda` or `RunnableLambda` functions within an LCEL chain to manipulate
    previous dictionary values. A lambda is an anonynous function within Python:    [PRE57]py    Now
    that you’re aware of how to use `RunnablePassThrough`, `itemgetter`, and `lambda`
    functions, let’s introduce one final piece of syntax: `RunnableParallel`:    [PRE58]py    First,
    you import `RunnableParallel` and create two LCEL chains called `master_chain`
    and `master_chain_two`. These are then invoked with exactly the same arguments;
    the `RunnablePassthrough` then passes the dictionary into the second part of the
    chain.    The second part of `master_chain` and `master_chain_two` will return
    exactly the *same result.*    So rather than directly using a dictionary, you
    can choose to use a `RunnableParallel` function instead. These two chain outputs
    *are interchangeable*, so choose whichever syntax you find more comfortable.    Let’s
    create three LCEL chains using the prompt templates:    [PRE59]py    After creating
    all the chains, you can then attach them to a master LCEL chain.    Input:    [PRE60]py    The
    output is truncated when you see `...` to save space. However, in total there
    were five characters and nine scenes generated.    Output:    [PRE61]py    The
    scenes are split into separate items within a Python list. Then two new prompts
    are created to generate both a character script and a summarization prompt:    [PRE62]py
    `"""``,` `)`  `summarize_prompt` `=` `ChatPromptTemplate``.``from_template``(`     `template``=``"""Given
    a character script, create a summary of the scene.`  `Character script:` `{character_script}``"""``,`
    `)` [PRE63]py   [PRE64] [PRE65] from langchain_core.prompts.chat import ChatPromptTemplate
    from operator import itemgetter from langchain_core.runnables import RunnablePassthrough,
    RunnableLambda  bad_first_input = {     "film_required_age": 18, }  prompt = ChatPromptTemplate.from_template(     "Generate
    a film title, the age is {film_required_age}" )  # This will error: bad_chain
    = bad_first_input | prompt [PRE66] # All of these chains enforce the runnable
    interface: first_good_input = {"film_required_age": itemgetter("film_required_age")}  #
    Creating a dictionary within a RunnableLambda: second_good_input = RunnableLambda(lambda
    x: { "film_required_age": x["film_required_age"] } )  third_good_input = RunnablePassthrough()
    fourth_good_input = {"film_required_age": RunnablePassthrough()} # You can also
    create a chain starting with RunnableParallel(...)  first_good_chain = first_good_input
    | prompt second_good_chain = second_good_input | prompt third_good_chain = third_good_input
    | prompt fourth_good_chain = fourth_good_input | prompt  first_good_chain.invoke({     "film_required_age":
    18 }) # ... [PRE67] from langchain_text_splitters import CharacterTextSplitter
    from langchain.chains.summarize import load_summarize_chain import pandas as pd
    [PRE68] df = pd.DataFrame(generated_scenes) [PRE69] all_character_script_text
    = "\n".join(df.character_script.tolist()) [PRE70] text_splitter = CharacterTextSplitter.from_tiktoken_encoder(     chunk_size=1500,
    chunk_overlap=200 ) docs = text_splitter.create_documents([all_character_script_text])
    [PRE71] chain = load_summarize_chain(llm=model, chain_type="map_reduce") [PRE72]
    summary = chain.invoke(docs) [PRE73] print(summary[''output_text'']) [PRE74] from
    langchain.text_splitter import CharacterTextSplitter from langchain.chains.summarize
    import load_summarize_chain import pandas as pd  df = pd.DataFrame(generated_scenes)  all_character_script_text
    = "\n".join(df.character_script.tolist())  text_splitter = CharacterTextSplitter.from_tiktoken_encoder(     chunk_size=1500,
    chunk_overlap=200 )  docs = text_splitter.create_documents([all_character_script_text])  chain
    = load_summarize_chain(llm=model, chain_type="map_reduce") summary = chain.invoke(docs)
    print(summary[''output_text'']) [PRE75] Aurora and Magnus agree to retrieve a
    hidden artifact, and they enter an ancient library to find a book that will guide
    them to the relic...'' [PRE76] chain = load_summarize_chain(llm=model, chain_type=''refine'')
    [PRE77]` [PRE78][PRE79][PRE80][PRE81][PRE82] [PRE83]`py` [PRE84] [PRE85][PRE86]``py[PRE87]py``
    [PRE88]````'
  prefs: []
  type: TYPE_NORMAL
