<html><head></head><body><section data-pdf-bookmark="Chapter 3. AI Integration and Model Serving" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch03">
<h1><span class="label">Chapter 3. </span>AI Integration and Model Serving</h1>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id629">
<h1>Chapter Goals</h1>
<p>In this chapter, you will learn about:</p>

<ul>
<li>
<p>How different GenAI models work</p>
</li>
<li>
<p>How to integrate and serve generative models into FastAPI</p>
</li>
<li>
<p>How to work with text, image, audio, video, and 3D models</p>
</li>
<li>
<p>How to quickly build a user interface for prototyping</p>
</li>
<li>
<p>Several model-serving strategies in FastAPI</p>
</li>
<li>
<p>How to leverage middleware for service monitoring</p>
</li>
</ul>
</div></aside>

<p>In this chapter, you will learn the mechanisms of various GenAI models and how to serve them in a FastAPI application.
Additionally, using the <a href="https://oreil.ly/9BXmn">Streamlit UI package</a>, you will create a simple browser client for interacting with the model-serving endpoints.
We will explore differing model-serving strategies, how to preload models for efficiency, and how to use FastAPI features for service 
<span class="keep-together">monitoring</span>.</p>

<p>To solidify your learning in this chapter, we will progressively build a FastAPI service using open source GenAI models that generate text, images, audio, and 3D geometries, all from scratch.
In later chapters, you’ll build the functionality to parse documents and web content for your GenAI service so you can talk to them using a language model.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In the previous chapter, you saw how to set up a fresh FastAPI project in Python.
Make sure you have fresh installation ready before you read the rest of this chapter.
Alternatively, you can clone or download the book’s <a href="https://github.com/Ali-Parandeh/building-generative-ai-services">GitHub repository</a>.
Then once cloned, switch to the <code>ch03-start</code> branch, ready for the steps to follow.</p>
</div>

<p>By the end of this chapter, you will have a FastAPI service that serves various open source GenAI models that you can test inside the Streamlit UI.
Additionally, your service will be capable of logging usage data to disk using middleware.</p>






<section data-pdf-bookmark="Serving Generative Models" data-type="sect1"><div class="sect1" id="id39">
<h1>Serving Generative Models</h1>

<p><a data-primary="serving GenAI models" data-type="indexterm" id="ix_ch03-asciidoc0"/>Before you serve pretrained generative models in your application, it is worth learning how these models are trained and generate data.
With this understanding, you can customize the internals of your application to enhance the outputs that you provide to the user.</p>

<p>In this chapter, I will show you how to serve models across a variety of modalities including:</p>

<ul>
<li>
<p><em>Language</em> models based on the transformer neural network architecture</p>
</li>
<li>
<p><em>Audio</em> models in text-to-speech and text-to-audio services based on the aggressive transformer architecture</p>
</li>
<li>
<p><em>Vision</em> models for text-to-image and text-to-video services based on the Stable Diffusion and vision transformer architectures</p>
</li>
<li>
<p><em>3D</em> models for text-to-3D services based on the conditional implicit function encoder and diffusion decoder architecture</p>
</li>
</ul>

<p>This list is not exhaustive and covers a handful of GenAI models.
To explore other models, please visit the <a href="https://oreil.ly/-4wlQ">Hugging Face model repository</a>.<sup><a data-type="noteref" href="ch03.html#id630" id="id630-marker">1</a></sup></p>








<section data-pdf-bookmark="Language Models" data-type="sect2"><div class="sect2" id="id40">
<h2>Language Models</h2>

<p><a data-primary="language models" data-type="indexterm" id="ix_ch03-asciidoc1"/><a data-primary="serving GenAI models" data-secondary="language models" data-type="indexterm" id="ix_ch03-asciidoc2"/>In this section, we talk about language models, including transformers and recurrent neural networks (RNNs).</p>










<section data-pdf-bookmark="Transformers versus recurrent neural networks" data-type="sect3"><div class="sect3" id="id41">
<h3>Transformers versus recurrent neural networks</h3>

<p><a data-primary="language models" data-secondary="transformers versus recurrent neural networks" data-type="indexterm" id="ix_ch03-asciidoc3"/><a data-primary="recurrent neural networks (RNNs), transformers versus" data-type="indexterm" id="ix_ch03-asciidoc4"/><a data-primary="serving GenAI models" data-secondary="language models" data-tertiary="transformers versus recurrent neural networks" data-type="indexterm" id="ix_ch03-asciidoc5"/><a data-primary="transformers" data-secondary="recurrent neural networks versus" data-type="indexterm" id="ix_ch03-asciidoc6"/>The world of AI was shaken with the release of the landmark paper “Attention Is All You Need.”<sup><a data-type="noteref" href="ch03.html#id631" id="id631-marker">2</a></sup>
In this paper, the authors proposed a completely different approach to natural language processing (NLP) and sequence modeling that differed from the existing RNN architectures.</p>

<p><a data-type="xref" href="#transformer_architecture">Figure 3-1</a> shows a simplified version of the proposed transformer architecture from the original paper.</p>

<figure><div class="figure" id="transformer_architecture">
<img alt="bgai 0301" src="assets/bgai_0301.png"/>
<h6><span class="label">Figure 3-1. </span>Transformer architecture</h6>
</div></figure>

<p>Historically, text generation tasks leveraged RNN models to learn patterns in sequential data such as free text.
<a data-primary="tokens" data-secondary="in RNNs" data-type="indexterm" id="id632"/>To process text, these models chunk text into small pieces such as a word or character called a <em>token</em> that can be sequentially processed.</p>

<p><a data-primary="state vector" data-type="indexterm" id="id633"/>RNNs maintain a memory store called a <em>state vector</em>, which carries information from one token to the next throughout the full text sequence, until the end.
This means that by the time you get to the end of the text sequence, the impact of early tokens on the state vector is a lot smaller compared to the most recent tokens.</p>

<p>Ideally, every token should be as important as the other tokens in any text.
However, as RNNs can only predict the next item in a sequence by looking at the items that came before, they struggle with this ideal in capturing long-range dependencies and modeling patterns in large chunks of texts.
As a result, they effectively fail to remember or comprehend essential information or context in large documents.</p>

<p>With the invention of transformers, recurrent or convolutional modeling could now be replaced with a more efficient approach.
<a data-primary="self-attention" data-type="indexterm" id="id634"/>Since transformers don’t maintain a hidden state memory and leverage a new capability termed <em>self-attention</em>, they’re capable of modeling relationships between words, no matter how far apart they appeared in a sentence.
This self-attention component allows the model to “place attention” on contextually relevant words within a sentence.</p>

<p>While RNNs model relationships between neighboring words in a sentence, transformers map pairwise relationships between every word in the text.</p>

<p><a data-type="xref" href="#rnn_vs_transformer">Figure 3-2</a> shows how RNNs process sentences in comparison to transformers.</p>

<figure><div class="figure" id="rnn_vs_transformer">
<img alt="bgai 0302" src="assets/bgai_0302.png"/>
<h6><span class="label">Figure 3-2. </span>RNNs versus transformers in processing sentences</h6>
</div></figure>

<p>What powers the self-attention system are specialized blocks called <em>attention heads</em> that capture pairwise patterns between words as <em>attention maps</em>.</p>

<p><a data-type="xref" href="#head_attention_map">Figure 3-3</a> visualizes the attention map of an attention head.<sup><a data-type="noteref" href="ch03.html#id635" id="id635-marker">3</a></sup>
Connections can be bidirectional with the thickness representing the strength of the relationship between words in the sentence.</p>

<figure><div class="figure" id="head_attention_map">
<img alt="bgai 0303" src="assets/bgai_0303.png"/>
<h6><span class="label">Figure 3-3. </span>View of an attention map inside an attention head</h6>
</div></figure>

<p><a data-primary="attention maps" data-type="indexterm" id="id636"/>A transformer model contains several attention heads distributed across its neural network layers.
Each head computes its own attention map independently to capture relationships between words focusing on certain patterns in the inputs.
Using multiple attention heads, the model can simultaneously analyze the inputs from various angles and contexts to understand complex patterns and dependencies within the data.</p>

<p><a data-type="xref" href="#model_attention_map">Figure 3-4</a> shows the attention maps for each head (i.e., independent set of attention weights) within each layer of the model.</p>

<figure><div class="figure" id="model_attention_map">
<img alt="bgai 0304" src="assets/bgai_0304.png"/>
<h6><span class="label">Figure 3-4. </span>View of the attention maps within the model</h6>
</div></figure>

<p class="less_space pagebreak-before">RNNs also required extensive compute power to train, as the training process couldn’t be parallelized on multiple GPU due to the sequential nature of their training algorithms.
Transformers, on the other hand, process words nonsequentially, so they can run attention mechanisms in parallel on GPUs.</p>

<p>The efficiency of the transformer architecture means that these models are more scalable as long as there is more data, compute power, and memory.
You can build language models with a corpus that spans libraries of books produced by humanity.
All you would need is ample compute power and data to train an LLM.
And, that is exactly what OpenAI did, the company behind the famous ChatGPT application that was powered by several of their proprietary LLMs including GPT-4o.</p>

<p>At the time of this writing, the implementation details behind OpenAI’s LLMs remain a trade secret.
While many researchers have a general understanding of OpenAI’s methods, they may not necessarily have the resources to replicate them.
However, several open source alternatives for research and commercial use have been released since, including Llama (Facebook),
Gemma (Google),
Mistral, and Falcon
to name a few.<sup><a data-type="noteref" href="ch03.html#id637" id="id637-marker">4</a></sup>
At the time of this writing, the model sizes vary between 0.05B and 480B parameters (i.e., model weights and biases) to suit your needs.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id638">
<h1>Hardware Requirements for Open Source LLMs</h1>
<p><a data-primary="large language models (LLMs)" data-secondary="hardware requirements for open source LLMs" data-type="indexterm" id="id639"/><a data-primary="Snowflake Arctic LLM" data-type="indexterm" id="id640"/>The biggest open source LLM at the time of this writing is the multilingual 480B-parameter <a href="https://oreil.ly/DLukR"><em>Snowflake Arctic</em></a>.
<a data-primary="video random access memory (VRAM)" data-type="indexterm" id="id641"/>The recommended hardware to run this massive model is a single AWS/Azure 8xH100 instance, which contains eight H100 data center GPU cards, each providing 80 GB of VRAM.
Other flagship open LLMs such as the multilingual 405B-parameter Llama 3.1 also require similar 
<span class="keep-together">hardware.</span></p>

<p>As of January 2024, one of the best consumer-grade GPU cards you can buy for AI workloads is NVIDIA 4090 RTX, which ships with only 24 GB of VRAM.
A single consumer GPU like 4090 RTX may not be able to run model sizes above 30B due to memory constraints unless the model is quantized (i.e., compressed).</p>

<p>If you want to run a quantized 70B-Llama model, you may need a <a href="https://oreil.ly/dJbKa">64 GB VRAM GPU or multiple smaller cards</a>.
Aside from power supply and cooling challenges with setting up a multi-GPU home server, you may still experience slow prediction rates when running models this size.</p>

<p>You will learn more about the quantization process and using quantized LLMs in <a data-type="xref" href="ch10.html#ch10">Chapter 10</a>.</p>
</div></aside>

<p>Serving LLMs still remains a challenge due to high memory requirements with requirements doubling if you need to train and fine-tune them on your own dataset.
This is because the training process will require caching and reusing model parameters across training batches.
As a result, most organizations may rely on lightweight (up to 3B) models or on APIs of LLM providers such as OpenAI, Anthropic, Cohere, Mistral, etc.</p>

<p>As LLMs grow in popularity, it becomes even more important to understand how they’re trained and how they process data, so let’s discuss underlying mechanisms next.<a data-startref="ix_ch03-asciidoc6" data-type="indexterm" id="id642"/><a data-startref="ix_ch03-asciidoc5" data-type="indexterm" id="id643"/><a data-startref="ix_ch03-asciidoc4" data-type="indexterm" id="id644"/><a data-startref="ix_ch03-asciidoc3" data-type="indexterm" id="id645"/></p>
</div></section>










<section data-pdf-bookmark="Tokenization and embedding" data-type="sect3"><div class="sect3" id="id42">
<h3>Tokenization and embedding</h3>

<p><a data-primary="language models" data-secondary="tokenization and embedding" data-type="indexterm" id="ix_ch03-asciidoc7"/><a data-primary="serving GenAI models" data-secondary="language models" data-tertiary="tokenization and embedding" data-type="indexterm" id="ix_ch03-asciidoc8"/><a data-primary="tokenization" data-type="indexterm" id="ix_ch03-asciidoc9"/>Neural networks can’t process words directly as they’re big statistical models that function on numbers.
To bridge that gap between language and numbers, you need to use <em>tokenization</em>.
With tokenization, you break down text into smaller pieces that a model can process.</p>

<p>Any piece of text must be first sliced into a list of <em>tokens</em> that represent words, syllables, symbols, and punctuations.
These tokens are then mapped to unique numbers so that patterns can be numerically modeled.</p>

<p>By providing a vector of input tokens to a trained transformer, the network can then predict the next best token to generate text, one word at a time.</p>

<p><a data-type="xref" href="#openai_tokenizer">Figure 3-5</a> shows how the OpenAI tokenizer converts text into a sequence of tokens, assigning unique token identifiers to each.</p>

<figure><div class="figure" id="openai_tokenizer">
<img alt="bgai 0305" src="assets/bgai_0305.png"/>
<h6><span class="label">Figure 3-5. </span>OpenAI tokenizer (Source: <a href="https://oreil.ly/S-a9M">OpenAI</a>)</h6>
</div></figure>

<p class="less_space pagebreak-before">So what can you do after you tokenize some text?
These tokens need to be processed further before a language model can process them.</p>

<p><a data-primary="embeddings" data-type="indexterm" id="id646"/>After tokenization, you need to use an <em>embedder</em><sup><a data-type="noteref" href="ch03.html#id647" id="id647-marker">5</a></sup> to convert these tokens into dense vectors of real numbers called <em>embeddings</em>, capturing semantic information (i.e., meaning of each token) in a continuous vector space.
<a data-type="xref" href="#embeddings">Figure 3-6</a> demonstrates these embeddings.</p>

<figure><div class="figure" id="embeddings">
<img alt="bgai 0306" src="assets/bgai_0306.png"/>
<h6><span class="label">Figure 3-6. </span>Assigning an embedding vector of size 
<span class="plain">n</span> to each token during the 
<span class="keep-together">embedding</span> process</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>These embedding vectors use small <em>floating-point numbers</em> (not integers) to capture nuanced relationships between tokens with more flexibility and precision.
They also tend to be <em>normally distributed</em>, so language model training and inference can be more stable and consistent.</p>
</div>

<p>After the embedding process, each token is assigned an embedding vector filled with <em>n</em> numbers.
Each number in the embedding vector focuses on a dimension that represents a specific aspect of the token’s meaning.<a data-startref="ix_ch03-asciidoc9" data-type="indexterm" id="id648"/><a data-startref="ix_ch03-asciidoc8" data-type="indexterm" id="id649"/><a data-startref="ix_ch03-asciidoc7" data-type="indexterm" id="id650"/></p>
</div></section>










<section data-pdf-bookmark="Training transformers" data-type="sect3"><div class="sect3" id="id43">
<h3>Training transformers</h3>

<p><a data-primary="language models" data-secondary="training transformers" data-type="indexterm" id="ix_ch03-asciidoc10"/><a data-primary="serving GenAI models" data-secondary="language models" data-tertiary="training transformers" data-type="indexterm" id="ix_ch03-asciidoc11"/><a data-primary="transformers" data-secondary="training" data-type="indexterm" id="ix_ch03-asciidoc12"/>Once you have a set of embedding vectors, you can train a model on your documents to update the values inside each embedding.
During model training, the training algorithm updates the parameters of the embedding layers so that the embedding vectors describe the meaning of each token as close as possible within the input text.</p>

<p class="less_space pagebreak-before">Understanding how embedding vectors work can be challenging, so let’s try a visualization approach.</p>

<p>Imagine you used a two-dimensional embedding vectors, meaning the vectors  contained only two numbers.
Then, if you plot these vectors, before and after model training, you will observe plots similar to <a data-type="xref" href="#untrained_to_trained_transformer">Figure 3-7</a>.
The embedding vectors of tokens, or words, with similar meanings will be closer to each other.</p>

<figure><div class="figure" id="untrained_to_trained_transformer">
<img alt="bgai 0307" src="assets/bgai_0307.png"/>
<h6><span class="label">Figure 3-7. </span>Training latent space of transformer network using embedding vectors</h6>
</div></figure>

<p><a data-primary="cosine similarity" data-type="indexterm" id="id651"/>To determine the similarity between two words, you can compute the angle between vectors using a calculation known as <em>cosine similarity</em>.
Smaller angles imply higher similarity, representing similar context and meaning.
After training, the cosine similarity calculation of two embedding vectors with similar meanings will validate that those vectors are close to each other.</p>

<p><a data-type="xref" href="#embedding_vectors">Figure 3-8</a> illustrates the full tokenization, embedding, and training process.</p>

<figure><div class="figure" id="embedding_vectors">
<img alt="bgai 0308" src="assets/bgai_0308.png"/>
<h6><span class="label">Figure 3-8. </span>Processing sequential data like a piece of text into a vector of tokens and token embeddings</h6>
</div></figure>

<p>Once you have a trained embedding layer, you can now use it to embed any new input text to the transformer model shown in <a data-type="xref" href="#transformer_architecture">Figure 3-1</a>.<a data-startref="ix_ch03-asciidoc12" data-type="indexterm" id="id652"/><a data-startref="ix_ch03-asciidoc11" data-type="indexterm" id="id653"/><a data-startref="ix_ch03-asciidoc10" data-type="indexterm" id="id654"/></p>
</div></section>










<section data-pdf-bookmark="Positional encoding" data-type="sect3"><div class="sect3" id="id44">
<h3>Positional encoding</h3>

<p><a data-primary="language models" data-secondary="positional encoding" data-type="indexterm" id="id655"/><a data-primary="positional encoding" data-type="indexterm" id="id656"/><a data-primary="serving GenAI models" data-secondary="language models" data-tertiary="positional encoding" data-type="indexterm" id="id657"/>A final step before forwarding the embedding vectors to the attention layers in the transformer network is to implement <em>positional encoding</em>.
The positional encoding process produces the positional embedding vectors that then are summed with the token embedding vectors.</p>

<p>Since transformers process words simultaneously rather than sequentially, positional embeddings are needed to record the word order and context within the sequential data, like sentences.
The resultant embedding vectors capture both meaning and positional information of words in the sentences before they’re passed to the attention mechanisms of the transformer.
This process ensures attention heads have all the information they need to learn patterns effectively.</p>

<p class="less_space pagebreak-before"><a data-type="xref" href="#positional_encoding">Figure 3-9</a> shows the positional encoding process where the positional embeddings are summed with token embeddings.</p>

<figure><div class="figure" id="positional_encoding">
<img alt="bgai 0309" src="assets/bgai_0309.png"/>
<h6><span class="label">Figure 3-9. </span>Positional encoding</h6>
</div></figure>
</div></section>










<section data-pdf-bookmark="Autoregressive prediction" data-type="sect3"><div class="sect3" id="id45">
<h3>Autoregressive prediction</h3>

<p><a data-primary="autoregressive prediction" data-type="indexterm" id="ix_ch03-asciidoc13"/><a data-primary="language models" data-secondary="autoregressive prediction" data-type="indexterm" id="ix_ch03-asciidoc14"/><a data-primary="serving GenAI models" data-secondary="language models" data-tertiary="autoregressive prediction" data-type="indexterm" id="ix_ch03-asciidoc15"/><a data-primary="tokens" data-secondary="autoregressive prediction and" data-type="indexterm" id="ix_ch03-asciidoc16"/>The transformer is an autoregressive (i.e., sequential) model as future predictions are based on the past values, as shown in <a data-type="xref" href="#autoregressive_prediction3">Figure 3-10</a>.</p>

<figure><div class="figure" id="autoregressive_prediction3">
<img alt="bgai 0310" src="assets/bgai_0310.png"/>
<h6><span class="label">Figure 3-10. </span>Autoregressive prediction</h6>
</div></figure>

<p>The model receives input tokens that are then embedded and passed through the network to make the next best token prediction.
This process repeats until a <code>&lt;stop&gt;</code> or end of sentence <code>&lt;eos&gt;</code> token is generated.<sup><a data-type="noteref" href="ch03.html#id658" id="id658-marker">6</a></sup></p>

<p>However, there is a limit to the number of tokens that the model can store in its memory to generate the next token.
<a data-primary="context window" data-type="indexterm" id="id659"/>This token limit is referred to as the model’s <em>context window</em>, which is an important factor to consider during the model selection stage for your GenAI services.</p>

<p>If the context window limit is reached, the model simply discards the least recently used tokens.
This means it can <em>forget</em> the least recently used sentences in documents or messages in a conversation.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>At the time of writing, the context of the least expensive OpenAI <code>gpt-4o-mini</code> model is around ~128,000 tokens, equivalent to more than 300 pages of text.</p>

<p>The largest context window as of March 2025 belongs to <a href="https://oreil.ly/10Mj1">Magic.Dev LTM-2-mini</a> with 100 million tokens. This equals ~10 million lines of code of ~750 novels.</p>

<p>The context window of other models falls in the range of hundreds of thousands of tokens.</p>
</div>

<p>Short windows will lead to loss of information, difficulty maintaining conversations, and reduced coherence with the user query.</p>

<p>On the other hand, long context windows have larger memory requirements and can lead to performance issues or slow services when scaling to thousands of concurrent users who are using your service.
In addition, you will need to consider the costs of relying on models with larger context windows as they tend to be more expensive due to increased compute and memory requirements.
The correct choice will depend on your budget and user needs in your use case.<a data-startref="ix_ch03-asciidoc16" data-type="indexterm" id="id660"/><a data-startref="ix_ch03-asciidoc15" data-type="indexterm" id="id661"/><a data-startref="ix_ch03-asciidoc14" data-type="indexterm" id="id662"/><a data-startref="ix_ch03-asciidoc13" data-type="indexterm" id="id663"/></p>
</div></section>










<section data-pdf-bookmark="Integrating a language model into your application" data-type="sect3"><div class="sect3" id="id46">
<h3>Integrating a language model into your application</h3>

<p><a data-primary="language models" data-secondary="integrating a model into an application" data-type="indexterm" id="ix_ch03-asciidoc17"/><a data-primary="serving GenAI models" data-secondary="language models" data-tertiary="integrating a model into an application" data-type="indexterm" id="ix_ch03-asciidoc18"/>You can download and use a language model within your application with a few lines of code.
<a data-primary="small language model (SLM)" data-type="indexterm" id="ix_ch03-asciidoc19a"/><a data-primary="TinyLlama" data-type="indexterm" id="ix_ch03-asciidoc19"/>In <a data-type="xref" href="#language_model_usage_example">Example 3-1</a>, you will download a TinyLlama model that has 1.1 billion parameters and is pretrained on 3 trillion tokens.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id664">
<h1>Installing TinyLlama Dependencies</h1>
<p>To integrate TinyLlama into your application, you can use the Hugging Face <code>transformers</code> library.<sup><a data-type="noteref" href="ch03.html#id665" id="id665-marker">7</a></sup>
You will also need to install the Pytorch deep learning framework by installing the <code>torch</code> package.
Both packages can be installed via <code>pip</code>.</p>

<p>On Windows, you will need to provide the <code>--index-url</code> flag to <code>pip</code> when installing <code>torch</code> that is compiled for a CUDA-enabled GPU.<sup><a data-type="noteref" href="ch03.html#id666" id="id666-marker">8</a></sup></p>

<pre data-code-language="bash" data-type="programlisting"><code class="c1"># Install `torch` with CUDA 12.4 and `transformers` packages for Windows.</code>

$ pip install transformers torch <code class="se">\</code>
  --index-url https://download.pytorch.org/whl/cu124<code class="w"/></pre>

<p>TinyLlama can’t generate more than a few sentences at a time.
You will also need around 3 GB of disk space and RAM to load this model onto memory for inference.
I recommend running the model on a CUDA-enabled NVIDIA GPU (with the <code>torch</code> wheel compiled for CUDA) as CPU inference can be slow.
Please refer to NVIDIA’s CUDA installation instructions for <a href="https://oreil.ly/LeA1O">Windows</a> or <a href="https://oreil.ly/qjNaO">Linux</a>.</p>

<p>Furthermore, to run <a data-type="xref" href="#language_model_usage_example">Example 3-1</a> on Windows, you may need to install Visual Studio Build Tools 2022 with C++ and .NET development tools to resolve issues with missing DLL libraries and dependencies.</p>
</div></aside>
<div data-type="example" id="language_model_usage_example">
<h5><span class="label">Example 3-1. </span>Download and load a language model from the Hugging Face repository</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># models.py</code>

<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">Pipeline</code><code class="p">,</code> <code class="n">pipeline</code>

<code class="n">prompt</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">How to set up a FastAPI project?</code><code class="s2">"</code>
<code class="n">system_prompt</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">Your name is FastAPI bot and you are a helpful</code>
<code class="s2">chatbot responsible for teaching FastAPI to your users.</code>
<code class="s2">Always respond in markdown.</code>
<code class="s2">"""</code>

<code class="n">device</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">device</code><code class="p">(</code><code class="s2">"</code><code class="s2">cuda</code><code class="s2">"</code> <code class="k">if</code> <code class="n">torch</code><code class="o">.</code><code class="n">cuda</code><code class="o">.</code><code class="n">is_available</code><code class="p">(</code><code class="p">)</code> <code class="k">else</code> <code class="s2">"</code><code class="s2">cpu</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO1-1" id="co_ai_integration_and_model_serving_CO1-1"><img alt="1" src="assets/1.png"/></a>

<code class="k">def</code> <code class="nf">load_text_model</code><code class="p">(</code><code class="p">)</code><code class="p">:</code>
    <code class="n">pipe</code> <code class="o">=</code> <code class="n">pipeline</code><code class="p">(</code>
        <code class="s2">"</code><code class="s2">text-generation</code><code class="s2">"</code><code class="p">,</code>
        <code class="n">model</code><code class="o">=</code><code class="s2">"</code><code class="s2">TinyLlama/TinyLlama-1.1B-Chat-v1.0</code><code class="s2">"</code><code class="p">,</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO1-2" id="co_ai_integration_and_model_serving_CO1-2"><img alt="2" src="assets/2.png"/></a>
        <code class="n">torch_dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">bfloat16</code><code class="p">,</code>
        <code class="n">device</code><code class="o">=</code><code class="n">device</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO1-3" id="co_ai_integration_and_model_serving_CO1-3"><img alt="3" src="assets/3.png"/></a>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">pipe</code>


<code class="k">def</code> <code class="nf">generate_text</code><code class="p">(</code><code class="n">pipe</code><code class="p">:</code> <code class="n">Pipeline</code><code class="p">,</code> <code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">temperature</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.7</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code><code class="p">:</code>
    <code class="n">messages</code> <code class="o">=</code> <code class="p">[</code>
        <code class="p">{</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">system</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="n">system_prompt</code><code class="p">}</code><code class="p">,</code>
        <code class="p">{</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">user</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="n">prompt</code><code class="p">}</code><code class="p">,</code>
    <code class="p">]</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO1-4" id="co_ai_integration_and_model_serving_CO1-4"><img alt="4" src="assets/4.png"/></a>
    <code class="n">prompt</code> <code class="o">=</code> <code class="n">pipe</code><code class="o">.</code><code class="n">tokenizer</code><code class="o">.</code><code class="n">apply_chat_template</code><code class="p">(</code>
        <code class="n">messages</code><code class="p">,</code> <code class="n">tokenize</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">add_generation_prompt</code><code class="o">=</code><code class="kc">True</code>
    <code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO1-5" id="co_ai_integration_and_model_serving_CO1-5"><img alt="5" src="assets/5.png"/></a>
    <code class="n">predictions</code> <code class="o">=</code> <code class="n">pipe</code><code class="p">(</code>
        <code class="n">prompt</code><code class="p">,</code>
        <code class="n">temperature</code><code class="o">=</code><code class="n">temperature</code><code class="p">,</code>
        <code class="n">max_new_tokens</code><code class="o">=</code><code class="mi">256</code><code class="p">,</code>
        <code class="n">do_sample</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
        <code class="n">top_k</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code>
        <code class="n">top_p</code><code class="o">=</code><code class="mf">0.95</code><code class="p">,</code>
    <code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO1-6" id="co_ai_integration_and_model_serving_CO1-6"><img alt="6" src="assets/6.png"/></a>
    <code class="n">output</code> <code class="o">=</code> <code class="n">predictions</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">[</code><code class="s2">"</code><code class="s2">generated_text</code><code class="s2">"</code><code class="p">]</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s2">"</code><code class="s2">&lt;/s&gt;</code><code class="se">\n</code><code class="s2">&lt;|assistant|&gt;</code><code class="se">\n</code><code class="s2">"</code><code class="p">)</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO1-7" id="co_ai_integration_and_model_serving_CO1-7"><img alt="7" src="assets/7.png"/></a>
    <code class="k">return</code> <code class="n">output</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO1-1" id="callout_ai_integration_and_model_serving_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Check if an NVIDIA GPU is available, and if so, set <code>device</code> to the current CUDA-enabled GPU.
Otherwise, continue using the CPU.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO1-2" id="callout_ai_integration_and_model_serving_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Download and load the TinyLlama model into memory with a <code>float16</code> tensor precision data type.<sup><a data-type="noteref" href="ch03.html#id667" id="id667-marker">9</a></sup></p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO1-3" id="callout_ai_integration_and_model_serving_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Move the whole pipeline to GPU on the first load.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO1-4" id="callout_ai_integration_and_model_serving_CO1-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Prepare the message list, which consists of dictionaries that have role and content key-value pairs.
The order of the dictionaries dictates the order of messages from older to newer in a conversation.
The first message is often a system prompt to guide the model’s output in a conversation.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO1-5" id="callout_ai_integration_and_model_serving_CO1-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>Convert the list of chat messages into a list of integer tokens for the model.
The model is then asked to generate output in textual format, not integer tokens <code>tokenize=False</code>.
A generation prompt is also added to the end of chat messages (<code>add_generation_prompt=True</code>)
so that the model is encouraged to generate a response based on the chat history.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO1-6" id="callout_ai_integration_and_model_serving_CO1-6"><img alt="6" src="assets/6.png"/></a></dt>
<dd><p>The prepared prompt is passed to the model with several inference parameters to optimize the text generation performance.
A few of these key inference parameters include:</p>

<ul>
<li>
<p><code>max_new_tokens</code>: Specifies the maximum number of new tokens to generate in the output.</p>
</li>
<li>
<p><code>do_sample</code>: Determines, when producing output, whether to pick a token randomly from a list of suitable tokens (<code>True</code>) or to simply choose the most likely token at each step (<code>False</code>).</p>
</li>
<li>
<p><code>temperature</code>: Controls the randomness of the output generation.
Lower values make the model’s outputs more precise, while higher values allow for more creative responses.</p>
</li>
<li>
<p><code>top_k</code>: Restricts the model’s token predictions to the top K options.
<code>top_k=50</code> means create a list of top 50 most suitable tokens to pick from in the current token prediction step.</p>
</li>
<li>
<p><code>top_p</code>: Implements <em>nucleus sampling</em> when creating a list of most suitable tokens.
<code>top_p=0.95</code> means create a list of the top tokens until you’re satisfied that your list has 95% of the most suitable tokens to pick from, for the current token prediction step.</p>
</li>
</ul></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO1-7" id="callout_ai_integration_and_model_serving_CO1-7"><img alt="7" src="assets/7.png"/></a></dt>
<dd><p>The final output is obtained from the <code>predictions</code> object.
The generated text from TinyLlama includes the full conversation history, with the generated response appended to the end.
The <code>&lt;/s&gt;</code> stop token followed by <code>\n&lt;|assistant|&gt;\n</code> tokens are used to pick the content of the last message in the conversation, which is the model’s response.</p></dd>
</dl></div>

<p><a data-type="xref" href="#language_model_usage_example">Example 3-1</a> is a good starting point; you can still load this model on your CPU and get responses within a reasonable time.
However, TinyLlama may also not perform as well as its larger counterparts.
For production workloads, you will want to use bigger models for better output quality and performance.</p>

<p>You can now use the <code>load_model</code> and <code>predict</code> functions inside a controller function<sup><a data-type="noteref" href="ch03.html#id668" id="id668-marker">10</a></sup> and then add a route handling decorator to serve the model via an endpoint, as shown in <a data-type="xref" href="#text_endpoint">Example 3-2</a>.</p>
<div data-type="example" id="text_endpoint">
<h5><span class="label">Example 3-2. </span>Serving a language model via a FastAPI endpoint</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># main.py</code>

<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code>
<code class="kn">from</code> <code class="nn">models</code> <code class="kn">import</code> <code class="n">load_text_model</code><code class="p">,</code> <code class="n">generate_text</code>

<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">(</code><code class="p">)</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"</code><code class="s2">/generate/text</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO2-1" id="co_ai_integration_and_model_serving_CO2-1"><img alt="1" src="assets/1.png"/></a>
<code class="k">def</code> <code class="nf">serve_language_model_controller</code><code class="p">(</code><code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code><code class="p">:</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO2-2" id="co_ai_integration_and_model_serving_CO2-2"><img alt="2" src="assets/2.png"/></a>
    <code class="n">pipe</code> <code class="o">=</code> <code class="n">load_text_model</code><code class="p">(</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO2-3" id="co_ai_integration_and_model_serving_CO2-3"><img alt="3" src="assets/3.png"/></a>
    <code class="n">output</code> <code class="o">=</code> <code class="n">generate_text</code><code class="p">(</code><code class="n">pipe</code><code class="p">,</code> <code class="n">prompt</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO2-4" id="co_ai_integration_and_model_serving_CO2-4"><img alt="4" src="assets/4.png"/></a>
    <code class="k">return</code> <code class="n">output</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO2-5" id="co_ai_integration_and_model_serving_CO2-5"><img alt="5" src="assets/5.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO2-1" id="callout_ai_integration_and_model_serving_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Create a FastAPI server and add a <code>/generate</code> route handler for serving the model.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO2-2" id="callout_ai_integration_and_model_serving_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>The <code>serve_language_model_controller</code> is responsible for taking the prompt from the request query parameters.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO2-3" id="callout_ai_integration_and_model_serving_CO2-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>The model is loaded into memory.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO2-4" id="callout_ai_integration_and_model_serving_CO2-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>The controller passes the query to the model to perform the prediction.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO2-5" id="callout_ai_integration_and_model_serving_CO2-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>The FastAPI server sends the output as an HTTP response to the client.</p></dd>
</dl></div>

<p>Once the FastAPI service is up and running, you can visit the Swagger documentation page located at <code>http://localhost:8000/docs</code>
to test your new endpoint:</p>

<pre data-type="programlisting">http://localhost:8000/generate/text?prompt="What is FastAPI?"</pre>

<p>If you’re running the code samples on a CPU, it will take around a minute to receive a response from the model, as shown in <a data-type="xref" href="#text_gen_response">Figure 3-11</a>.</p>

<figure><div class="figure" id="text_gen_response">
<img alt="bgai 0311" src="assets/bgai_0311.png"/>
<h6><span class="label">Figure 3-11. </span>Response from TinyLlama</h6>
</div></figure>

<p><a data-primary="hallucinations" data-type="indexterm" id="id669"/>Not a bad response for a small language model (SLM) that runs on a CPU in your own computer, except that TinyLlama has <em>hallucinated</em> that FastAPI uses Flask.
That is an incorrect statement; FastAPI uses Starlette as the underlying web framework, not Flask.</p>

<p><em>Hallucinations</em> refer to outputs that aren’t grounded in the training data or reality.
Even though open source SLMs such as TinyLlama have been trained on impressive number of tokens (3 trillion), a small number of model parameters may have restricted their ability to learn the ground truth in data.<a data-startref="ix_ch03-asciidoc19" data-type="indexterm" id="id670"/><a data-startref="ix_ch03-asciidoc19a" data-type="indexterm" id="id671"/>
Additionally, some unfiltered training data may also have been used, both of which can contribute to more instances of hallucinations.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>When serving language models, always let your users know to fact-check the outputs with external sources as language models may <em>hallucinate</em> and produce incorrect statements.</p>
</div>

<p>You can now use a web browser client in Python to visually test your service with more interactivity compared to using a command-line client.</p>

<p>A great Python package to quickly develop a user interface is <a href="https://oreil.ly/9BXmn">Streamlit</a>, which enables you to create beautiful and customizable UIs for your AI services with little effort.<a data-startref="ix_ch03-asciidoc18" data-type="indexterm" id="id672"/><a data-startref="ix_ch03-asciidoc17" data-type="indexterm" id="id673"/></p>
</div></section>










<section data-pdf-bookmark="Connecting FastAPI with Streamlit UI generator" data-type="sect3"><div class="sect3" id="id47">
<h3>Connecting FastAPI with Streamlit UI generator</h3>

<p><a data-primary="language models" data-secondary="connecting FastAPI with Streamlit UI generator" data-type="indexterm" id="ix_ch03-asciidoc20"/><a data-primary="serving GenAI models" data-secondary="language models" data-tertiary="connecting FastAPI with Streamlit UI generator" data-type="indexterm" id="ix_ch03-asciidoc21"/><a data-primary="Streamlit UI generator" data-type="indexterm" id="ix_ch03-asciidoc22"/>Streamlit allows you to easily create a chat user interface for testing and prototyping with models.
You can install the <code>streamlit</code> package using <code>pip</code>:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install streamlit<code class="w"/></pre>

<p><a data-type="xref" href="#streamlit_chat_ui">Example 3-3</a> shows how to develop a simple UI to connect with your service.</p>
<div data-type="example" id="streamlit_chat_ui">
<h5><span class="label">Example 3-3. </span>Streamlit chat UI consuming the FastAPI /<code>generate</code> endpoint</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># client.py</code>

<code class="kn">import</code> <code class="nn">requests</code>
<code class="kn">import</code> <code class="nn">streamlit</code> <code class="k">as</code> <code class="nn">st</code>

<code class="n">st</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"</code><code class="s2">FastAPI ChatBot</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO3-1" id="co_ai_integration_and_model_serving_CO3-1"><img alt="1" src="assets/1.png"/></a>

<code class="k">if</code> <code class="s2">"</code><code class="s2">messages</code><code class="s2">"</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">st</code><code class="o">.</code><code class="n">session_state</code><code class="p">:</code>
    <code class="n">st</code><code class="o">.</code><code class="n">session_state</code><code class="o">.</code><code class="n">messages</code> <code class="o">=</code> <code class="p">[</code><code class="p">]</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO3-2" id="co_ai_integration_and_model_serving_CO3-2"><img alt="2" src="assets/2.png"/></a>

<code class="k">for</code> <code class="n">message</code> <code class="ow">in</code> <code class="n">st</code><code class="o">.</code><code class="n">session_state</code><code class="o">.</code><code class="n">messages</code><code class="p">:</code>
    <code class="k">with</code> <code class="n">st</code><code class="o">.</code><code class="n">chat_message</code><code class="p">(</code><code class="n">message</code><code class="p">[</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">]</code><code class="p">)</code><code class="p">:</code>
        <code class="n">st</code><code class="o">.</code><code class="n">markdown</code><code class="p">(</code><code class="n">message</code><code class="p">[</code><code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">]</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO3-3" id="co_ai_integration_and_model_serving_CO3-3"><img alt="3" src="assets/3.png"/></a>

<code class="k">if</code> <code class="n">prompt</code> <code class="o">:=</code> <code class="n">st</code><code class="o">.</code><code class="n">chat_input</code><code class="p">(</code><code class="s2">"</code><code class="s2">Write your prompt in this input field</code><code class="s2">"</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO3-4" id="co_ai_integration_and_model_serving_CO3-4"><img alt="4" src="assets/4.png"/></a>
    <code class="n">st</code><code class="o">.</code><code class="n">session_state</code><code class="o">.</code><code class="n">messages</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="p">{</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">user</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="n">prompt</code><code class="p">}</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO3-5" id="co_ai_integration_and_model_serving_CO3-5"><img alt="5" src="assets/5.png"/></a>

    <code class="k">with</code> <code class="n">st</code><code class="o">.</code><code class="n">chat_message</code><code class="p">(</code><code class="s2">"</code><code class="s2">user</code><code class="s2">"</code><code class="p">)</code><code class="p">:</code>
        <code class="n">st</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="n">prompt</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO3-6" id="co_ai_integration_and_model_serving_CO3-6"><img alt="6" src="assets/6.png"/></a>

    <code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code>
        <code class="sa">f</code><code class="s2">"</code><code class="s2">http://localhost:8000/generate/text</code><code class="s2">"</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="p">{</code><code class="s2">"</code><code class="s2">prompt</code><code class="s2">"</code><code class="p">:</code> <code class="n">prompt</code><code class="p">}</code>
    <code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO3-7" id="co_ai_integration_and_model_serving_CO3-7"><img alt="7" src="assets/7.png"/></a>
    <code class="n">response</code><code class="o">.</code><code class="n">raise_for_status</code><code class="p">(</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO3-8" id="co_ai_integration_and_model_serving_CO3-8"><img alt="8" src="assets/8.png"/></a>

    <code class="k">with</code> <code class="n">st</code><code class="o">.</code><code class="n">chat_message</code><code class="p">(</code><code class="s2">"</code><code class="s2">assistant</code><code class="s2">"</code><code class="p">)</code><code class="p">:</code>
        <code class="n">st</code><code class="o">.</code><code class="n">markdown</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">text</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO3-9" id="co_ai_integration_and_model_serving_CO3-9"><img alt="9" src="assets/9.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO3-1" id="callout_ai_integration_and_model_serving_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Add a title to your application that will be rendered to the UI.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO3-2" id="callout_ai_integration_and_model_serving_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Initialize the chat and keep track of the chat history.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO3-3" id="callout_ai_integration_and_model_serving_CO3-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Display the chat messages from the chat history on app rerun.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO3-4" id="callout_ai_integration_and_model_serving_CO3-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Wait until the user has submitted a prompt via the chat input field.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO3-5" id="callout_ai_integration_and_model_serving_CO3-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>Add the user or assistant messages to the chat history.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO3-6" id="callout_ai_integration_and_model_serving_CO3-6"><img alt="6" src="assets/6.png"/></a></dt>
<dd><p>Display the user message in the chat message container.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO3-7" id="callout_ai_integration_and_model_serving_CO3-7"><img alt="7" src="assets/7.png"/></a></dt>
<dd><p>Send a <code>GET</code> request with the prompt as a query parameter to your FastAPI endpoint to generate a response from TinyLlama.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO3-8" id="callout_ai_integration_and_model_serving_CO3-8"><img alt="8" src="assets/8.png"/></a></dt>
<dd><p>Validate the response is OK.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO3-9" id="callout_ai_integration_and_model_serving_CO3-9"><img alt="9" src="assets/9.png"/></a></dt>
<dd><p>Display the assistant message in the chat message container.</p></dd>
</dl></div>

<p>You can now start your Streamlit client application:<sup><a data-type="noteref" href="ch03.html#id674" id="id674-marker">11</a></sup></p>

<pre data-code-language="bash" data-type="programlisting">$ streamlit run client.py<code class="w"/></pre>

<p>You should now be able to interact with TinyLlama inside Streamlit, as shown in <a data-type="xref" href="#streamlit_ui_text_results">Figure 3-12</a>.
All of this was possible with a few short Python scripts.</p>

<figure><div class="figure" id="streamlit_ui_text_results">
<img alt="bgai 0312" src="assets/bgai_0312.png"/>
<h6><span class="label">Figure 3-12. </span>Streamlit client</h6>
</div></figure>

<p><a data-type="xref" href="#tiny_llama_fastapi_architecture">Figure 3-13</a> shows the overall system architecture of the solution we’ve developed so far.</p>

<figure><div class="figure" id="tiny_llama_fastapi_architecture">
<img alt="bgai 0313" src="assets/bgai_0313.png"/>
<h6><span class="label">Figure 3-13. </span>FastAPI service system architecture</h6>
</div></figure>
<div class="less_space pagebreak-before" data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>While the solution in <a data-type="xref" href="#streamlit_chat_ui">Example 3-3</a> is great for prototyping and testing models, it is not suitable for production workloads where several users would need simultaneous access to the model.
This is because with the current setup, the model is loaded and unloaded onto memory every time a request is processed.
Having to load/unload a large model to and from memory is slow and I/O 
<span class="keep-together">blocking.</span></p>
</div>

<p>The TinyLlama service you’ve just built used a <em>decoder</em> transformer, optimized for conversational and chat use cases.
However, the <a href="https://oreil.ly/RqztC">original paper on transformers</a> introduced an architecture that consisted of both an encoder and a decoder.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id675">
<h1>Transformer Variants</h1>
<p><a data-primary="transformers" data-secondary="variants" data-type="indexterm" id="id676"/>There are three types of transformers that you should know about when working with language models, as shown in <a data-type="xref" href="#transformer_architectures">Figure 3-14</a>.</p>

<figure><div class="figure" id="transformer_architectures">
<img alt="bgai 0314" src="assets/bgai_0314.png"/>
<h6><span class="label">Figure 3-14. </span>Transformer architectures</h6>
</div></figure>

<p>Each transformer variant has its own unique capabilities and specializes in certain tasks.</p>
<dl class="less_space pagebreak-before">
<dt>Encoder-decoder transformers</dt>
<dd>

<ul>
<li>
<p>Used for transforming one sequence of information into another</p>
</li>
<li>
<p>Excel at translation, text summarization, question and answering tasks</p>
</li>
</ul>
</dd>
<dt>Encoder-only transformers</dt>
<dd>

<ul>
<li>
<p>Used for understanding and representing the meanings of input sequences</p>
</li>
<li>
<p>Specialize in sentiment analysis, entity extraction, and text classification tasks</p>
</li>
</ul>
</dd>
<dt>Decoder-only transformers</dt>
<dd>

<ul>
<li>
<p>Used for predicting the next token in a sequence</p>
</li>
<li>
<p>Outshine other transformers in text generation, conversational and language modeling tasks</p>
</li>
</ul>
</dd>
</dl>

<p>In practice, you should select the appropriate transformer for your use case based on its specialization and capabilities.</p>
</div></aside>

<p>You should now feel more confident in the inner workings of language models and how to package them in a FastAPI web server.<a data-startref="ix_ch03-asciidoc22" data-type="indexterm" id="id677"/><a data-startref="ix_ch03-asciidoc21" data-type="indexterm" id="id678"/><a data-startref="ix_ch03-asciidoc20" data-type="indexterm" id="id679"/></p>

<p>Language models represent just a fraction of all generative models.
The upcoming sections will expand your knowledge to include the function and serving of models that generate audio, images, and videos.<a data-startref="ix_ch03-asciidoc2" data-type="indexterm" id="id680"/><a data-startref="ix_ch03-asciidoc1" data-type="indexterm" id="id681"/></p>

<p>We can start working with audio models first.</p>
</div></section>
</div></section>








<section data-pdf-bookmark="Audio Models" data-type="sect2"><div class="sect2" id="id48">
<h2>Audio Models</h2>

<p><a data-primary="serving GenAI models" data-secondary="audio models" data-type="indexterm" id="ix_ch03-asciidoc23"/>In GenAI services, audio models are important for creating interactive and realistic sounds.
Unlike text models that you’re now familiar with, which focus on processing and generating text, audio models can handle audio signals.
With them, you can 
<span class="keep-together">synthesize</span> speech, generate music, and even create sound effects for applications like 
<span class="keep-together">virtual</span> assistants, automated dubbing, game development, and immersive audio 
<span class="keep-together">environments</span>.</p>

<p><a data-primary="Bark (Suno AI text-to-audio model)" data-type="indexterm" id="ix_ch03-asciidoc24"/><a data-primary="Suno AI" data-type="indexterm" id="ix_ch03-asciidoc25"/>One of the most capable text-to-speech and text-to-audio models is the Bark model created by Suno AI.
This transformer-based model can generate realistic multilingual speech and audio including music, background noise, and sound effects.</p>

<p>The Bark model consists of four models chained together as a pipeline to synthesize audio waveforms from textual prompts, as shown in <a data-type="xref" href="#bark_pipeline">Figure 3-15</a>.</p>

<figure><div class="figure" id="bark_pipeline">
<img alt="bgai 0315" src="assets/bgai_0315.png"/>
<h6><span class="label">Figure 3-15. </span>Bark synthesis pipeline</h6>
</div></figure>
<dl>
<dt>1. Semantic text model</dt>
<dd>
<p>A causal (sequential) autoregressive transformer model accepts tokenized input text and captures the meaning via semantic tokens.
Autoregressive models predict future values in a sequence by reusing their own previous outputs.</p>
</dd>
<dt>2. Coarse acoustics model</dt>
<dd>
<p>A causal autoregressive transformer receives the semantic model’s outputs and generates the initial audio features, which lack finer details.
Each prediction is based on past and present information in the semantic token sequence.</p>
</dd>
<dt>3. Fine acoustics model</dt>
<dd>
<p>A noncausal auto-encoder transformer refines the audio representation by generating the remaining audio features.
As the coarse acoustics model has generated the entire audio sequence, the fine model doesn’t need to be casual.</p>
</dd>
<dt>4. Encodec audio codec model</dt>
<dd>
<p>The model decodes the output audio array from all previously generated audio codes.</p>
</dd>
</dl>

<p>Bark synthesizes the audio waveform by decoding the refined audio features into the final audio output in the form of spoken words, music, or simple audio effects.</p>

<p><a data-type="xref" href="#small_bark">Example 3-4</a> shows how to use the small Bark model.</p>
<div data-type="example" id="small_bark">
<h5><span class="label">Example 3-4. </span>Download and load the small Bark model from the Hugging Face repository</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># schemas.py</code>

<code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Literal</code>

<code class="n">VoicePresets</code> <code class="o">=</code> <code class="n">Literal</code><code class="p">[</code><code class="s2">"</code><code class="s2">v2/en_speaker_1</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">v2/en_speaker_9</code><code class="s2">"</code><code class="p">]</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO4-1" id="co_ai_integration_and_model_serving_CO4-1"><img alt="1" src="assets/1.png"/></a>

<code class="c1"># models.py</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">AutoProcessor</code><code class="p">,</code> <code class="n">AutoModel</code><code class="p">,</code> <code class="n">BarkProcessor</code><code class="p">,</code> <code class="n">BarkModel</code>
<code class="kn">from</code> <code class="nn">schemas</code> <code class="kn">import</code> <code class="n">VoicePresets</code>

<code class="n">device</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">device</code><code class="p">(</code><code class="s2">"</code><code class="s2">cuda</code><code class="s2">"</code> <code class="k">if</code> <code class="n">torch</code><code class="o">.</code><code class="n">cuda</code><code class="o">.</code><code class="n">is_available</code><code class="p">(</code><code class="p">)</code> <code class="k">else</code> <code class="s2">"</code><code class="s2">cpu</code><code class="s2">"</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">load_audio_model</code><code class="p">(</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">tuple</code><code class="p">[</code><code class="n">BarkProcessor</code><code class="p">,</code> <code class="n">BarkModel</code><code class="p">]</code><code class="p">:</code>
    <code class="n">processor</code> <code class="o">=</code> <code class="n">AutoProcessor</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"</code><code class="s2">suno/bark-small</code><code class="s2">"</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO4-2" id="co_ai_integration_and_model_serving_CO4-2"><img alt="2" src="assets/2.png"/></a>
    <code class="n">model</code> <code class="o">=</code> <code class="n">AutoModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"</code><code class="s2">suno/bark-small</code><code class="s2">"</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO4-3" id="co_ai_integration_and_model_serving_CO4-3"><img alt="3" src="assets/3.png"/></a>
    <code class="k">return</code> <code class="n">processor</code><code class="p">,</code> <code class="n">model</code>


<code class="k">def</code> <code class="nf">generate_audio</code><code class="p">(</code>
    <code class="n">processor</code><code class="p">:</code> <code class="n">BarkProcessor</code><code class="p">,</code>
    <code class="n">model</code><code class="p">:</code> <code class="n">BarkModel</code><code class="p">,</code>
    <code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code>
    <code class="n">preset</code><code class="p">:</code> <code class="n">VoicePresets</code><code class="p">,</code>
<code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">tuple</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">,</code> <code class="nb">int</code><code class="p">]</code><code class="p">:</code>
    <code class="n">inputs</code> <code class="o">=</code> <code class="n">processor</code><code class="p">(</code><code class="n">text</code><code class="o">=</code><code class="p">[</code><code class="n">prompt</code><code class="p">]</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"</code><code class="s2">pt</code><code class="s2">"</code><code class="p">,</code><code class="n">voice_preset</code><code class="o">=</code><code class="n">preset</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO4-4" id="co_ai_integration_and_model_serving_CO4-4"><img alt="4" src="assets/4.png"/></a>
    <code class="n">output</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">generate</code><code class="p">(</code><code class="o">*</code><code class="o">*</code><code class="n">inputs</code><code class="p">,</code> <code class="n">do_sample</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code><code class="o">.</code><code class="n">cpu</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">numpy</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">squeeze</code><code class="p">(</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO4-5" id="co_ai_integration_and_model_serving_CO4-5"><img alt="5" src="assets/5.png"/></a>
    <code class="n">sample_rate</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">generation_config</code><code class="o">.</code><code class="n">sample_rate</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO4-6" id="co_ai_integration_and_model_serving_CO4-6"><img alt="6" src="assets/6.png"/></a>
    <code class="k">return</code> <code class="n">output</code><code class="p">,</code> <code class="n">sample_rate</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO4-1" id="callout_ai_integration_and_model_serving_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Specify supported voice preset options using a <code>Literal</code> type.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO4-2" id="callout_ai_integration_and_model_serving_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Download the small Bark processor, which prepares the input text prompt for the core model.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO4-3" id="callout_ai_integration_and_model_serving_CO4-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Download the Bark model, which will be used to generate the output audio.
Both objects will be needed for audio generation later.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO4-4" id="callout_ai_integration_and_model_serving_CO4-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Preprocess the text prompt with a speaker voice preset embedding and return a Pytorch tensor array of tokenized inputs using <code>return_tensors="pt"</code>.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO4-5" id="callout_ai_integration_and_model_serving_CO4-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>Generate an audio array that contains amplitude values of the synthesized audio signal over time.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO4-6" id="callout_ai_integration_and_model_serving_CO4-6"><img alt="6" src="assets/6.png"/></a></dt>
<dd><p>Get the sampling rate from model generating configurations, which can be used to produce the audio.</p></dd>
</dl></div>

<p>When you generate audio using a model, the output is a sequence of floating-point numbers that represent the <em>amplitude</em> (or strength) of the audio signal at each point in time.</p>

<p>To play back this audio, it needs to be converted to a digital format that can be sent to the speakers.
This involves sampling the audio signal at a fixed rate and quantizing the amplitude values to a fixed number of bits. The <code>soundfile</code> library can help you here by generating the audio file using a <em>sampling rate</em>.
The higher the sampling rate, the more samples that are taken, which enhances the audio quality but also increases the file size.</p>

<p>You can install the <code>soundfile</code> audio library for writing audio files using <code>pip</code>:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install soundfile<code class="w"/></pre>

<p><a data-type="xref" href="#audio_endpoint">Example 3-5</a> shows how you can stream the audio content to the client.</p>
<div data-type="example" id="audio_endpoint">
<h5><span class="label">Example 3-5. </span>FastAPI endpoint for returning generated audio</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># utils.py</code>

<code class="kn">from</code> <code class="nn">io</code> <code class="kn">import</code> <code class="n">BytesIO</code>
<code class="kn">import</code> <code class="nn">soundfile</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="k">def</code> <code class="nf">audio_array_to_buffer</code><code class="p">(</code><code class="n">audio_array</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">,</code> <code class="n">sample_rate</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">BytesIO</code><code class="p">:</code>
    <code class="n">buffer</code> <code class="o">=</code> <code class="n">BytesIO</code><code class="p">(</code><code class="p">)</code>
    <code class="n">soundfile</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">buffer</code><code class="p">,</code> <code class="n">audio_array</code><code class="p">,</code> <code class="n">sample_rate</code><code class="p">,</code> <code class="nb">format</code><code class="o">=</code><code class="s2">"</code><code class="s2">wav</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO5-1" id="co_ai_integration_and_model_serving_CO5-1"><img alt="1" src="assets/1.png"/></a>
    <code class="n">buffer</code><code class="o">.</code><code class="n">seek</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">buffer</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO5-2" id="co_ai_integration_and_model_serving_CO5-2"><img alt="2" src="assets/2.png"/></a>

<code class="c1"># main.py</code>

<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code><code class="p">,</code> <code class="n">status</code>
<code class="kn">from</code> <code class="nn">fastapi</code><code class="nn">.</code><code class="nn">responses</code> <code class="kn">import</code> <code class="n">StreamingResponse</code>

<code class="kn">from</code> <code class="nn">models</code> <code class="kn">import</code> <code class="n">load_audio_model</code><code class="p">,</code> <code class="n">generate_audio</code>
<code class="kn">from</code> <code class="nn">schemas</code> <code class="kn">import</code> <code class="n">VoicePresets</code>
<code class="kn">from</code> <code class="nn">utils</code> <code class="kn">import</code> <code class="n">audio_array_to_buffer</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code>
    <code class="s2">"</code><code class="s2">/generate/audio</code><code class="s2">"</code><code class="p">,</code>
    <code class="n">responses</code><code class="o">=</code><code class="p">{</code><code class="n">status</code><code class="o">.</code><code class="n">HTTP_200_OK</code><code class="p">:</code> <code class="p">{</code><code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"</code><code class="s2">audio/wav</code><code class="s2">"</code><code class="p">:</code> <code class="p">{</code><code class="p">}</code><code class="p">}</code><code class="p">}</code><code class="p">}</code><code class="p">,</code>
    <code class="n">response_class</code><code class="o">=</code><code class="n">StreamingResponse</code><code class="p">,</code>
<code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO5-3" id="co_ai_integration_and_model_serving_CO5-3"><img alt="3" src="assets/3.png"/></a>
<code class="k">def</code> <code class="nf">serve_text_to_audio_model_controller</code><code class="p">(</code>
    <code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code>
    <code class="n">preset</code><code class="p">:</code> <code class="n">VoicePresets</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">v2/en_speaker_1</code><code class="s2">"</code><code class="p">,</code>
<code class="p">)</code><code class="p">:</code>
    <code class="n">processor</code><code class="p">,</code> <code class="n">model</code> <code class="o">=</code> <code class="n">load_audio_model</code><code class="p">(</code><code class="p">)</code>
    <code class="n">output</code><code class="p">,</code> <code class="n">sample_rate</code> <code class="o">=</code> <code class="n">generate_audio</code><code class="p">(</code><code class="n">processor</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">prompt</code><code class="p">,</code> <code class="n">preset</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">StreamingResponse</code><code class="p">(</code>
        <code class="n">audio_array_to_buffer</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">sample_rate</code><code class="p">)</code><code class="p">,</code> <code class="n">media_type</code><code class="o">=</code><code class="s2">"</code><code class="s2">audio/wav</code><code class="s2">"</code>
    <code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO5-4" id="co_ai_integration_and_model_serving_CO5-4"><img alt="4" src="assets/4.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO5-1" id="callout_ai_integration_and_model_serving_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Install the <code>soundfile</code> library to write the audio array to memory buffer using its sampling rate.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO5-2" id="callout_ai_integration_and_model_serving_CO5-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Reset the buffer cursor to the start of the buffer and return the iterable buffer.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO5-3" id="callout_ai_integration_and_model_serving_CO5-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Create a new audio endpoint that returns the <code>audio/wav</code> content type as <code>StreamingResponse</code>.
<code>StreamingResponse</code> is typically used when you want to stream the response data, such as when returning large files or when generating the response data.
It allows you to return a generator function that yields chunks of data to be sent to the client.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO5-4" id="callout_ai_integration_and_model_serving_CO5-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Convert the generated audio array to an iterable buffer that can be passed to streaming response.</p></dd>
</dl></div>

<p><a data-primary="memory buffer" data-type="indexterm" id="id682"/>In <a data-type="xref" href="#audio_endpoint">Example 3-5</a>, you generated an audio array using the small Bark model and streamed the memory buffer of the audio content.
Streaming is more efficient for larger files as the client can consume the content as it is being served.
In previous examples, we didn’t use streaming responses, as generated images or text can be fairly small compared to audio or video content.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Streaming audio content directly from a memory buffer is faster and more efficient than writing the audio array to a file and streaming the content from the hard drive.</p>

<p>If you need the memory available for other tasks, you can write the audio array to a file first and then stream from it using a file reader generator.
You will be trading off latency for memory.</p>
</div>

<p>Now that you have an audio generation endpoint, you can update your Streamlit UI client code to render audio messages.
Update your Streamlit client code as shown in <a data-type="xref" href="#barksmall_streamlit_ui">Example 3-6</a>.</p>
<div class="less_space pagebreak-before" data-type="example" id="barksmall_streamlit_ui">
<h5><span class="label">Example 3-6. </span>Streamlit audio UI consuming the FastAPI <code>/audio</code> generation endpoint</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># client.py</code>

<code class="k">for</code> <code class="n">message</code> <code class="ow">in</code> <code class="n">st</code><code class="o">.</code><code class="n">session_state</code><code class="o">.</code><code class="n">messages</code><code class="p">:</code>
    <code class="k">with</code> <code class="n">st</code><code class="o">.</code><code class="n">chat_message</code><code class="p">(</code><code class="n">message</code><code class="p">[</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">]</code><code class="p">)</code><code class="p">:</code>
        <code class="n">content</code> <code class="o">=</code> <code class="n">message</code><code class="p">[</code><code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">]</code>
        <code class="k">if</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">content</code><code class="p">,</code> <code class="nb">bytes</code><code class="p">)</code><code class="p">:</code>
            <code class="n">st</code><code class="o">.</code><code class="n">audio</code><code class="p">(</code><code class="n">content</code><code class="p">)</code>
        <code class="k">else</code><code class="p">:</code>
            <code class="n">st</code><code class="o">.</code><code class="n">markdown</code><code class="p">(</code><code class="n">content</code><code class="p">)</code>


<code class="k">if</code> <code class="n">prompt</code> <code class="o">:=</code> <code class="n">st</code><code class="o">.</code><code class="n">chat_input</code><code class="p">(</code><code class="s2">"</code><code class="s2">Write your prompt in this input field</code><code class="s2">"</code><code class="p">)</code><code class="p">:</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code>
        <code class="sa">f</code><code class="s2">"</code><code class="s2">http://localhost:8000/generate/audio</code><code class="s2">"</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="p">{</code><code class="s2">"</code><code class="s2">prompt</code><code class="s2">"</code><code class="p">:</code> <code class="n">prompt</code><code class="p">}</code>
    <code class="p">)</code>
    <code class="n">response</code><code class="o">.</code><code class="n">raise_for_status</code><code class="p">(</code><code class="p">)</code>
    <code class="k">with</code> <code class="n">st</code><code class="o">.</code><code class="n">chat_message</code><code class="p">(</code><code class="s2">"</code><code class="s2">assistant</code><code class="s2">"</code><code class="p">)</code><code class="p">:</code>
        <code class="n">st</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="s2">"</code><code class="s2">Here is your generated audio</code><code class="s2">"</code><code class="p">)</code>
        <code class="n">st</code><code class="o">.</code><code class="n">audio</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">content</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO6-1" id="co_ai_integration_and_model_serving_CO6-1"><img alt="1" src="assets/1.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO6-1" id="callout_ai_integration_and_model_serving_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Update the Streamlit client code to render audio content.</p></dd>
</dl></div>

<p>With Streamlit, you can swap components to render any type of content including images, audio, and video.</p>

<p>You should now be able to generate highly realistic speech audio in your updated Streamlit UI, as shown in <a data-type="xref" href="#streamlit_bark_ui">Figure 3-16</a>.</p>

<figure><div class="figure" id="streamlit_bark_ui">
<img alt="bgai 0316" src="assets/bgai_0316.png"/>
<h6><span class="label">Figure 3-16. </span>Rendering audio responses in the Streamlit UI</h6>
</div></figure>

<p>Bear in mind that you’re using the compressed version of the Bark model, but with the light version, you can generate speech and music audio fairly quickly even on a single CPU.
This is in exchange for some audio generation quality.<a data-startref="ix_ch03-asciidoc25" data-type="indexterm" id="id683"/><a data-startref="ix_ch03-asciidoc24" data-type="indexterm" id="id684"/></p>

<p>You should now feel more comfortable serving larger content to your users via streaming responses and working with audio models.</p>

<p>So far, you’ve been building conversational and text-to-speech services.
Now let’s see how to interact with a vision model to build an image generator service.<a data-startref="ix_ch03-asciidoc23" data-type="indexterm" id="id685"/></p>
</div></section>








<section data-pdf-bookmark="Vision Models" data-type="sect2"><div class="sect2" id="id49">
<h2>Vision Models</h2>

<p><a data-primary="serving GenAI models" data-secondary="vision models" data-type="indexterm" id="ix_ch03-asciidoc26"/>Using vision models, you can generate, enhance, and understand visual information from prompts.</p>

<p>Since these models can produce very realistic outputs faster than any human and can understand and manipulate existing visual content, they’re extremely useful for applications like image generators and editors, object detection, image classification and captioning, and augmented reality.</p>

<p><a data-primary="Stable Diffusion (SD)" data-type="indexterm" id="ix_ch03-asciidoc27"/><a data-primary="SD (Stable Diffusion)" data-type="indexterm" id="ix_ch03-asciidoc27a"/>One of the most popular architectures used to train image models is called <em>Stable Diffusion</em> (SD).</p>

<p>SD models are trained to encode input images into a latent space.
This latent space is the mathematical representation of patterns in the training data that the model has learned.
If you try to visualize an encoded image, all you would see is a white noise image, similar to the black and white dots you would see on your TV screen when it loses signal.</p>

<p><a data-type="xref" href="#stable_diffusion">Figure 3-17</a> shows the full process for training and inference and visualizes how images are encoded and decoded via the forward and reverse diffusion processes.
A text encoder using text, images, and semantic maps assists in controlling the output via the reverse diffusion.</p>

<figure><div class="figure" id="stable_diffusion">
<img alt="bgai 0317" src="assets/bgai_0317.png"/>
<h6><span class="label">Figure 3-17. </span>Stable Diffusion training and inference</h6>
</div></figure>

<p>What makes these models magical is their ability to decode noisy images back into original input images.
Effectively, the SD models also learn to remove white noise from an encoded image to reproduce the original image.
The model performs this denoising process over several iterations.</p>

<p>However, you don’t want to re-create images you already have.
You will want the model to create new, never-before-seen images.
But how can an SD model achieve this for you?
The answer lies in the latent space where the encoded noisy images live.
You can change the noise in these images so that when the model denoises them and decodes them back, you get a whole new image that the model has never seen before.</p>

<p>A challenge remains:
how can you control the image generation process so that the model doesn’t produce random images?
The solution is to also encode image descriptions alongside the image.
The patterns in the latent space are then mapped to textual image descriptions of what is seen in each input image.
Now, you use textual prompts to sample the noisy latent space such that the produced output image after the denoising process is what you want.</p>

<p>This is how SD models can generate new images that they’ve never seen before in their training data.
In essence, these models navigate a latent space that contains encoded representations of various patterns and meanings.<sup><a data-type="noteref" href="ch03.html#id686" id="id686-marker">12</a></sup>
The model iteratively refines this noise through a denoising process to produce a novel image not present in its training dataset.</p>

<p>To download an SD model, you will need to have the Hugging Face <code>diffusers</code> library installed:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install diffusers<code class="w"/></pre>

<p><a data-type="xref" href="#sd_model_usage_example">Example 3-7</a> shows how to load an SD model into memory.</p>
<div data-type="example" id="sd_model_usage_example">
<h5><span class="label">Example 3-7. </span>Download and load an SD model from the Hugging Face repository</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># models.py</code>

<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">diffusers</code> <code class="kn">import</code> <code class="n">DiffusionPipeline</code><code class="p">,</code> <code class="n">StableDiffusionInpaintPipelineLegacy</code>
<code class="kn">from</code> <code class="nn">PIL</code> <code class="kn">import</code> <code class="n">Image</code>

<code class="n">device</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">device</code><code class="p">(</code><code class="s2">"</code><code class="s2">cuda</code><code class="s2">"</code> <code class="k">if</code> <code class="n">torch</code><code class="o">.</code><code class="n">cuda</code><code class="o">.</code><code class="n">is_available</code><code class="p">(</code><code class="p">)</code> <code class="k">else</code> <code class="s2">"</code><code class="s2">cpu</code><code class="s2">"</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">load_image_model</code><code class="p">(</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">StableDiffusionInpaintPipelineLegacy</code><code class="p">:</code>
    <code class="n">pipe</code> <code class="o">=</code> <code class="n">DiffusionPipeline</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
        <code class="s2">"</code><code class="s2">segmind/tiny-sd</code><code class="s2">"</code><code class="p">,</code> <code class="n">torch_dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float32</code><code class="p">,</code>
        <code class="n">device</code><code class="o">=</code><code class="n">device</code>
    <code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO7-1" id="co_ai_integration_and_model_serving_CO7-1"><img alt="1" src="assets/1.png"/></a>
    <code class="k">return</code> <code class="n">pipe</code>

<code class="k">def</code> <code class="nf">generate_image</code><code class="p">(</code>
    <code class="n">pipe</code><code class="p">:</code> <code class="n">StableDiffusionInpaintPipelineLegacy</code><code class="p">,</code> <code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code>
<code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">Image</code><code class="o">.</code><code class="n">Image</code><code class="p">:</code>
    <code class="n">output</code> <code class="o">=</code> <code class="n">pipe</code><code class="p">(</code><code class="n">prompt</code><code class="p">,</code> <code class="n">num_inference_steps</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code><code class="o">.</code><code class="n">images</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO7-2" id="co_ai_integration_and_model_serving_CO7-2"><img alt="2" src="assets/2.png"/></a> <a class="co" href="#callout_ai_integration_and_model_serving_CO7-3" id="co_ai_integration_and_model_serving_CO7-3"><img alt="3" src="assets/3.png"/></a>
    <code class="k">return</code> <code class="n">output</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO7-4" id="co_ai_integration_and_model_serving_CO7-4"><img alt="4" src="assets/4.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO7-1" id="callout_ai_integration_and_model_serving_CO7-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Download and load the TinySD model into memory with the less memory efficient <code>float32</code> tensor type.
Using <code>float16</code>, which has limited precision for large and complex models, leads to numerical instability and loss of accuracy.
Additionally, hardware support for <code>float16</code> is limited, so trying to run an SD model on your CPU with the <code>float16</code> tensor type may not be possible.
Source: <a href="https://oreil.ly/rzw8P">Hugging Face</a>.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO7-2" id="callout_ai_integration_and_model_serving_CO7-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Pass the text prompt to the model to generate a list of images and pick the first one.
Some models allow you to generate multiple images in a single inference step.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO7-3" id="callout_ai_integration_and_model_serving_CO7-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>The <code>num_inference_steps=10</code> specifies the number of diffusion steps to perform during inference.
In each diffusion step, a stronger noisy image is produced from previous diffusion steps.
The model generates multiple noisy images by undertaking multiple diffusion steps.
With these images, the model can better understand the patterns of noise that are present in the input data and learn to remove them more effectively.
The more inference steps, the better results you will get, but at the cost of computing power needed and longer processing times.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO7-4" id="callout_ai_integration_and_model_serving_CO7-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>The generated image will be a Python Pillow image type, so you have access to a variety of Pillow’s image methods for post-processing and storage.
For instance, you can call the <code>image.save()</code> method to store the image in your filesystem.</p></dd>
</dl></div>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Vision models are extremely resource hungry.
To load and use a small vision model such as TinySD on CPU, you will need around 
<span class="keep-together">5 GB</span> of disk space and RAM.
However, you can install <code>accelerate</code> using <code>pip install accelerate</code> to optimize resources required so that the model pipeline uses lower CPU memory usage.</p>

<p>When serving video models, you will need to use a GPU.
Later in this chapter, I will show you how to leverage GPUs for video 
<span class="keep-together">models.</span></p>
</div>

<p>You can now package this model into another endpoint as similar to <a data-type="xref" href="#text_endpoint">Example 3-2</a>, with the difference being that the returned response will be an image binary (not text).
Refer to <a data-type="xref" href="#image_endpoint">Example 3-8</a>.</p>
<div data-type="example" id="image_endpoint">
<h5><span class="label">Example 3-8. </span>FastAPI endpoint for returning a generated image</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># utils.py</code>

<code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Literal</code>
<code class="kn">from</code> <code class="nn">PIL</code> <code class="kn">import</code> <code class="n">Image</code>
<code class="kn">from</code> <code class="nn">io</code> <code class="kn">import</code> <code class="n">BytesIO</code>

<code class="k">def</code> <code class="nf">img_to_bytes</code><code class="p">(</code>
    <code class="n">image</code><code class="p">:</code> <code class="n">Image</code><code class="o">.</code><code class="n">Image</code><code class="p">,</code> <code class="n">img_format</code><code class="p">:</code> <code class="n">Literal</code><code class="p">[</code><code class="s2">"</code><code class="s2">PNG</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">JPEG</code><code class="s2">"</code><code class="p">]</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">PNG</code><code class="s2">"</code>
<code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">bytes</code><code class="p">:</code>
    <code class="n">buffer</code> <code class="o">=</code> <code class="n">BytesIO</code><code class="p">(</code><code class="p">)</code>
    <code class="n">image</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="n">buffer</code><code class="p">,</code> <code class="nb">format</code><code class="o">=</code><code class="n">img_format</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">buffer</code><code class="o">.</code><code class="n">getvalue</code><code class="p">(</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO8-1" id="co_ai_integration_and_model_serving_CO8-1"><img alt="1" src="assets/1.png"/></a>

<code class="c1"># main.py</code>

<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code><code class="p">,</code> <code class="n">Response</code><code class="p">,</code> <code class="n">status</code>
<code class="kn">from</code> <code class="nn">models</code> <code class="kn">import</code> <code class="n">load_image_model</code><code class="p">,</code> <code class="n">generate_image</code>
<code class="kn">from</code> <code class="nn">utils</code> <code class="kn">import</code> <code class="n">img_to_bytes</code>

<code class="o">.</code><code class="o">.</code><code class="o">.</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"</code><code class="s2">/generate/image</code><code class="s2">"</code><code class="p">,</code>
         <code class="n">responses</code><code class="o">=</code><code class="p">{</code><code class="n">status</code><code class="o">.</code><code class="n">HTTP_200_OK</code><code class="p">:</code> <code class="p">{</code><code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"</code><code class="s2">image/png</code><code class="s2">"</code><code class="p">:</code> <code class="p">{</code><code class="p">}</code><code class="p">}</code><code class="p">}</code><code class="p">}</code><code class="p">,</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO8-2" id="co_ai_integration_and_model_serving_CO8-2"><img alt="2" src="assets/2.png"/></a>
         <code class="n">response_class</code><code class="o">=</code><code class="n">Response</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO8-3" id="co_ai_integration_and_model_serving_CO8-3"><img alt="3" src="assets/3.png"/></a>
<code class="k">def</code> <code class="nf">serve_text_to_image_model_controller</code><code class="p">(</code><code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code><code class="p">:</code>
    <code class="n">pipe</code> <code class="o">=</code> <code class="n">load_image_model</code><code class="p">(</code><code class="p">)</code>
    <code class="n">output</code> <code class="o">=</code> <code class="n">generate_image</code><code class="p">(</code><code class="n">pipe</code><code class="p">,</code> <code class="n">prompt</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO8-4" id="co_ai_integration_and_model_serving_CO8-4"><img alt="4" src="assets/4.png"/></a>
    <code class="k">return</code> <code class="n">Response</code><code class="p">(</code><code class="n">content</code><code class="o">=</code><code class="n">img_to_bytes</code><code class="p">(</code><code class="n">output</code><code class="p">)</code><code class="p">,</code> <code class="n">media_type</code><code class="o">=</code><code class="s2">"</code><code class="s2">image/png</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO8-5" id="co_ai_integration_and_model_serving_CO8-5"><img alt="5" src="assets/5.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO8-1" id="callout_ai_integration_and_model_serving_CO8-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Create an in-memory buffer, save the image to this buffer in a given format, and then return the raw byte data from the buffer.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO8-2" id="callout_ai_integration_and_model_serving_CO8-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Specify the media content type and status codes for the auto-generated Swagger UI documentation page.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO8-3" id="callout_ai_integration_and_model_serving_CO8-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Specify the response class to prevent FastAPI from adding <code>application/json</code>
as an additional acceptable response media type.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO8-4" id="callout_ai_integration_and_model_serving_CO8-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>The response returned from the model will be Pillow image format.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO8-5" id="callout_ai_integration_and_model_serving_CO8-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>We will need to use the FastAPI <code>Response</code> class to send a special response carrying image bytes with a PNG media type.</p></dd>
</dl></div>

<p><a data-type="xref" href="#tinysd_swagger_docs">Figure 3-18</a> shows the results of testing the new <code>/generate/image</code> endpoint via FastAPI Swagger docs with the text prompt <code>A cosy living room with trees in it</code>.</p>

<figure><div class="figure" id="tinysd_swagger_docs">
<img alt="bgai 0318" src="assets/bgai_0318.png"/>
<h6><span class="label">Figure 3-18. </span>TinySD FastAPI service</h6>
</div></figure>

<p class="less_space pagebreak-before">Now, connect your endpoint to a Streamlit UI for prototyping, as shown in <a data-type="xref" href="#tinysd_streamlit_code">Example 3-9</a>.</p>
<div data-type="example" id="tinysd_streamlit_code">
<h5><span class="label">Example 3-9. </span>Streamlit Vision UI consuming the FastAPI <code><em>/image</em></code> generation endpoint</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># client.py</code>

<code class="o">.</code><code class="o">.</code><code class="o">.</code>

<code class="k">for</code> <code class="n">message</code> <code class="ow">in</code> <code class="n">st</code><code class="o">.</code><code class="n">session_state</code><code class="o">.</code><code class="n">messages</code><code class="p">:</code>
    <code class="k">with</code> <code class="n">st</code><code class="o">.</code><code class="n">chat_message</code><code class="p">(</code><code class="n">message</code><code class="p">[</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">]</code><code class="p">)</code><code class="p">:</code>
        <code class="n">st</code><code class="o">.</code><code class="n">image</code><code class="p">(</code><code class="n">message</code><code class="p">[</code><code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">]</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO9-1" id="co_ai_integration_and_model_serving_CO9-1"><img alt="1" src="assets/1.png"/></a>
<code class="o">.</code><code class="o">.</code><code class="o">.</code>

<code class="k">if</code> <code class="n">prompt</code> <code class="o">:=</code> <code class="n">st</code><code class="o">.</code><code class="n">chat_input</code><code class="p">(</code><code class="s2">"</code><code class="s2">Write your prompt in this input field</code><code class="s2">"</code><code class="p">)</code><code class="p">:</code>
    <code class="o">.</code><code class="o">.</code><code class="o">.</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code>
        <code class="sa">f</code><code class="s2">"</code><code class="s2">http://localhost:8000/generate/image</code><code class="s2">"</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="p">{</code><code class="s2">"</code><code class="s2">prompt</code><code class="s2">"</code><code class="p">:</code> <code class="n">prompt</code><code class="p">}</code>
    <code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO9-2" id="co_ai_integration_and_model_serving_CO9-2"><img alt="2" src="assets/2.png"/></a>
    <code class="n">response</code><code class="o">.</code><code class="n">raise_for_status</code><code class="p">(</code><code class="p">)</code>
    <code class="k">with</code> <code class="n">st</code><code class="o">.</code><code class="n">chat_message</code><code class="p">(</code><code class="s2">"</code><code class="s2">assistant</code><code class="s2">"</code><code class="p">)</code><code class="p">:</code>
        <code class="n">st</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="s2">"</code><code class="s2">Here is your generated image</code><code class="s2">"</code><code class="p">)</code>
        <code class="n">st</code><code class="o">.</code><code class="n">image</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">content</code><code class="p">)</code>

    <code class="o">.</code><code class="o">.</code><code class="o">.</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO9-1" id="callout_ai_integration_and_model_serving_CO9-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Images transferred over the HTTP protocol will be in binary format.
Therefore, we update the display function to render binary image content.
You can use the <code>st.image</code> method to display images to the UI.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO9-2" id="callout_ai_integration_and_model_serving_CO9-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Update the <code>GET</code> request to hit the <code>/generate/image</code> endpoint.
Then, render a textual and image message to the user.</p></dd>
</dl></div>

<p><a data-type="xref" href="#tinysd_streamlitui">Figure 3-19</a> shows the final results of the user experience with the model.</p>

<figure><div class="figure" id="tinysd_streamlitui">
<img alt="bgai 0319" src="assets/bgai_0319.png"/>
<h6><span class="label">Figure 3-19. </span>Rendering image messages in the Streamlit UI</h6>
</div></figure>
<aside class="less_space pagebreak-before" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id687">
<h1>Running XL Models</h1>
<p><a data-primary="serving GenAI models" data-secondary="running XL models" data-type="indexterm" id="id688"/><a data-primary="XL models" data-type="indexterm" id="id689"/>You now see how to implement model-serving endpoints with FastAPI and Streamlit to generate text or images.
We used tiny versions of these models so that you can run the examples on your CPU.
However, your hardware requirements significantly increase if you need to use the XL versions for better quality.
As an example, to run the SDXL model, you will require both 16 GB of CPU RAM and 16 GB of GPU VRAM to generate an image.
This is because you will first need to load the model onto your CPU from disk and then move it to your GPU for inference.
We will cover this process in more detail when discussing model-serving strategies.</p>
</div></aside>

<p>We saw how even with a tiny SD model, you can generate reasonable looking images.
The XL versions can produce even more realistic images but still have their own 
<span class="keep-together">limitations.</span></p>

<p>At the time of writing, the current open source SD models do have certain 
<span class="keep-together">limitations:</span></p>
<dl>
<dt>Coherency</dt>
<dd>
<p>The models can’t produce every detail described in the prompts and complex compositions.</p>
</dd>
<dt>Output size</dt>
<dd>
<p>The output images can only be predefined sizes such as 512 × 512 or 1024 × 1024 pixels.</p>
</dd>
<dt>Composability</dt>
<dd>
<p>You can’t fully control the generated image and define composition in the image.</p>
</dd>
<dt>Photorealism</dt>
<dd>
<p>The generated outputs do show details that give away they’ve been generated by AI.</p>
</dd>
<dt>Legible text</dt>
<dd>
<p>Some models cannot generate legible texts.</p>
</dd>
</dl>

<p>The <code>tinysd</code> model you worked with is an early phase model that has undergone the <em>distillation</em> process (i.e., compression) from the larger V1.5 SD model.<a data-startref="ix_ch03-asciidoc27" data-type="indexterm" id="id690"/><a data-startref="ix_ch03-asciidoc27a" data-type="indexterm" id="id691"/>
As a result, the generated outputs may not meet production standards or be entirely cohesive and could fail to incorporate all the concepts mentioned in the text prompts.
However, the distilled models may perform well if you <a href="https://oreil.ly/Nqtkm"><em>fine-tune</em> them using <em>Low-Rank Adaptation</em> (LoRA)</a> on specific concepts/styles.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id692">
<h1>Low-Rank Adaptation in Fine-Tuning Generative Models</h1>
<p><a data-primary="low-rank adaptation (LoRA)" data-type="indexterm" id="id693"/><a data-primary="serving GenAI models" data-secondary="low-rank adaptation in fine-tuning generative models" data-type="indexterm" id="id694"/>LoRA is a training strategy that introduces a minimal number of trainable parameters to each layer in a model.
The majority of the original model’s parameters remain fixed.</p>

<p>By limiting the number of parameters that need to be trained, LoRA greatly decreases the GPU memory needed for training.
This is quite useful when fine-tuning or training large-scale models, where memory constraints are typically a major challenge to customization.</p>
</div></aside>

<p>You can now build both text- and image-based GenAI services.
However, you may be wondering how to build text-to-video services based on video models.
Let’s learn more about video models, how they work, and how to build an image animator service with them next.<a data-startref="ix_ch03-asciidoc26" data-type="indexterm" id="id695"/></p>
</div></section>








<section data-pdf-bookmark="Video Models" data-type="sect2"><div class="sect2" id="id50">
<h2>Video Models</h2>

<p><a data-primary="serving GenAI models" data-secondary="video models" data-type="indexterm" id="ix_ch03-asciidoc28"/>Video models are some of the most resource-hungry generative models and often require a GPU to produce a short snippet of good quality.
These models have to generate several tens of frames to produce a single second of video, even without any audio content.</p>

<p><a data-primary="Stability AI" data-type="indexterm" id="id696"/>Stability AI has released several open source video models based on the SD architecture on Hugging Face.
We will work with the compressed version of their image-to-video model for a faster image animation service.</p>

<p>To get started, let’s get a small image-to-video model running using <a data-type="xref" href="#video_model_loading">Example 3-10</a>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>To run <a data-type="xref" href="#video_model_loading">Example 3-10</a>, you may need access to a CUDA-capable NVIDIA GPU.</p>

<p>Also, for commercial use of the <code>stable-video-diffusion-img2vid</code> model, please refer to its <a href="https://oreil.ly/DM-0p">model card</a>.</p>
</div>
<div data-type="example" id="video_model_loading">
<h5><span class="label">Example 3-10. </span>Download and load the Stability AI’s <em>img2vid</em> model from the Hugging Face repository</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># models.py</code>

<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">diffusers</code> <code class="kn">import</code> <code class="n">StableVideoDiffusionPipeline</code>
<code class="kn">from</code> <code class="nn">PIL</code> <code class="kn">import</code> <code class="n">Image</code>

<code class="n">device</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">device</code><code class="p">(</code><code class="s2">"</code><code class="s2">cuda</code><code class="s2">"</code> <code class="k">if</code> <code class="n">torch</code><code class="o">.</code><code class="n">cuda</code><code class="o">.</code><code class="n">is_available</code><code class="p">(</code><code class="p">)</code> <code class="k">else</code> <code class="s2">"</code><code class="s2">cpu</code><code class="s2">"</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">load_video_model</code><code class="p">(</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">StableVideoDiffusionPipeline</code><code class="p">:</code>
    <code class="n">pipe</code> <code class="o">=</code> <code class="n">StableVideoDiffusionPipeline</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
        <code class="s2">"</code><code class="s2">stabilityai/stable-video-diffusion-img2vid</code><code class="s2">"</code><code class="p">,</code>
        <code class="n">torch_dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float16</code><code class="p">,</code>
        <code class="n">variant</code><code class="o">=</code><code class="s2">"</code><code class="s2">fp16</code><code class="s2">"</code><code class="p">,</code>
        <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">pipe</code>

<code class="k">def</code> <code class="nf">generate_video</code><code class="p">(</code>
    <code class="n">pipe</code><code class="p">:</code> <code class="n">StableVideoDiffusionPipeline</code><code class="p">,</code> <code class="n">image</code><code class="p">:</code> <code class="n">Image</code><code class="o">.</code><code class="n">Image</code><code class="p">,</code> <code class="n">num_frames</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">25</code>
<code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">list</code><code class="p">[</code><code class="n">Image</code><code class="o">.</code><code class="n">Image</code><code class="p">]</code><code class="p">:</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">image</code><code class="o">.</code><code class="n">resize</code><code class="p">(</code><code class="p">(</code><code class="mi">1024</code><code class="p">,</code> <code class="mi">576</code><code class="p">)</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO10-1" id="co_ai_integration_and_model_serving_CO10-1"><img alt="1" src="assets/1.png"/></a>
    <code class="n">generator</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO10-2" id="co_ai_integration_and_model_serving_CO10-2"><img alt="2" src="assets/2.png"/></a>
    <code class="n">frames</code> <code class="o">=</code> <code class="n">pipe</code><code class="p">(</code>
        <code class="n">image</code><code class="p">,</code> <code class="n">decode_chunk_size</code><code class="o">=</code><code class="mi">8</code><code class="p">,</code> <code class="n">generator</code><code class="o">=</code><code class="n">generator</code><code class="p">,</code> <code class="n">num_frames</code><code class="o">=</code><code class="n">num_frames</code>
    <code class="p">)</code><code class="o">.</code><code class="n">frames</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO10-3" id="co_ai_integration_and_model_serving_CO10-3"><img alt="3" src="assets/3.png"/></a>
    <code class="k">return</code> <code class="n">frames</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO10-1" id="callout_ai_integration_and_model_serving_CO10-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Resize the input image to a standard size expected by model input.
Resizing will also protect against large inputs.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO10-2" id="callout_ai_integration_and_model_serving_CO10-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Create a random tensor generator with the seed set to 42 for reproducible video frame generation.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO10-3" id="callout_ai_integration_and_model_serving_CO10-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Run the frame generation pipeline to produce all video frames at once.

<span class="keep-together">Grab the first</span> batch of generated frames.
This step requires significant video memory.
<code>num_frames</code> specifies the number of frames to generate, while
<code>decode_chunk_size</code> specifies how many frames to generate at once.</p></dd>
</dl></div>

<p>With the model loading functions in place, you can now build the video-serving 
<span class="keep-together">endpoint.</span></p>

<p>However, before you proceed with declaring the route handler, you do need a utility function to process the video model outputs from frames into a streamable video using an I/O buffer.</p>

<p>To export a sequence of frames to videos, you need to encode them into a video container using a video library such as <code>av</code>, which implements Python bindings to the popular
<code>ffmpeg</code> video processing library.</p>

<p>You can install the <code>av</code> library via:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install av<code class="w"/></pre>

<p>Now you can use <a data-type="xref" href="#frames_to_videos">Example 3-11</a> to create streamable video buffers.</p>
<div data-type="example" id="frames_to_videos">
<h5><span class="label">Example 3-11. </span>Exporting video model output from frames to a streamable video buffer using the <code>av</code> library</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># utils.py</code>

<code class="kn">from</code> <code class="nn">io</code> <code class="kn">import</code> <code class="n">BytesIO</code>
<code class="kn">from</code> <code class="nn">PIL</code> <code class="kn">import</code> <code class="n">Image</code>
<code class="kn">import</code> <code class="nn">av</code>

<code class="k">def</code> <code class="nf">export_to_video_buffer</code><code class="p">(</code><code class="n">images</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="n">Image</code><code class="o">.</code><code class="n">Image</code><code class="p">]</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">BytesIO</code><code class="p">:</code>
    <code class="n">buffer</code> <code class="o">=</code> <code class="n">BytesIO</code><code class="p">(</code><code class="p">)</code>
    <code class="n">output</code> <code class="o">=</code> <code class="n">av</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">buffer</code><code class="p">,</code> <code class="s2">"</code><code class="s2">w</code><code class="s2">"</code><code class="p">,</code> <code class="nb">format</code><code class="o">=</code><code class="s2">"</code><code class="s2">mp4</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO11-1" id="co_ai_integration_and_model_serving_CO11-1"><img alt="1" src="assets/1.png"/></a>
    <code class="n">stream</code> <code class="o">=</code> <code class="n">output</code><code class="o">.</code><code class="n">add_stream</code><code class="p">(</code><code class="s2">"</code><code class="s2">h264</code><code class="s2">"</code><code class="p">,</code> <code class="mi">30</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO11-2" id="co_ai_integration_and_model_serving_CO11-2"><img alt="2" src="assets/2.png"/></a>
    <code class="n">stream</code><code class="o">.</code><code class="n">width</code> <code class="o">=</code> <code class="n">images</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">width</code>
    <code class="n">stream</code><code class="o">.</code><code class="n">height</code> <code class="o">=</code> <code class="n">images</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">height</code>
    <code class="n">stream</code><code class="o">.</code><code class="n">pix_fmt</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">yuv444p</code><code class="s2">"</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO11-3" id="co_ai_integration_and_model_serving_CO11-3"><img alt="3" src="assets/3.png"/></a>
    <code class="n">stream</code><code class="o">.</code><code class="n">options</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"</code><code class="s2">crf</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">17</code><code class="s2">"</code><code class="p">}</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO11-4" id="co_ai_integration_and_model_serving_CO11-4"><img alt="4" src="assets/4.png"/></a>
    <code class="k">for</code> <code class="n">image</code> <code class="ow">in</code> <code class="n">images</code><code class="p">:</code>
        <code class="n">frame</code> <code class="o">=</code> <code class="n">av</code><code class="o">.</code><code class="n">VideoFrame</code><code class="o">.</code><code class="n">from_image</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
        <code class="n">packet</code> <code class="o">=</code> <code class="n">stream</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="n">frame</code><code class="p">)</code>   <a class="co" href="#callout_ai_integration_and_model_serving_CO11-5" id="co_ai_integration_and_model_serving_CO11-5"><img alt="5" src="assets/5.png"/></a>
        <code class="n">output</code><code class="o">.</code><code class="n">mux</code><code class="p">(</code><code class="n">packet</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO11-6" id="co_ai_integration_and_model_serving_CO11-6"><img alt="6" src="assets/6.png"/></a>
    <code class="n">packet</code> <code class="o">=</code> <code class="n">stream</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="kc">None</code><code class="p">)</code>
    <code class="n">output</code><code class="o">.</code><code class="n">mux</code><code class="p">(</code><code class="n">packet</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">buffer</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO11-7" id="co_ai_integration_and_model_serving_CO11-7"><img alt="7" src="assets/7.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO11-1" id="callout_ai_integration_and_model_serving_CO11-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Open a buffer for writing an MP4 file and then configure a video stream with AV’s video multiplexer.<sup><a data-type="noteref" href="ch03.html#id697" id="id697-marker">13</a></sup></p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO11-2" id="callout_ai_integration_and_model_serving_CO11-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Set the video encoding to <code>h264</code> at 30 frames per second and make sure the frame dimensions match the frames provided to the function.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO11-3" id="callout_ai_integration_and_model_serving_CO11-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Set the pixel format of the video stream to <code>yuv444p</code> so that each pixel has the full resolution for the <code>y</code> (luminance or brightness) and both <code>u</code> and <code>v</code> (chrominance or color) components.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO11-4" id="callout_ai_integration_and_model_serving_CO11-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Configure the stream’s constant rate factor (CRF) to control the video quality and compression.
Set the CRF to 17 to output a lossless high-quality video with minimal compression.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO11-5" id="callout_ai_integration_and_model_serving_CO11-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>Encode the input frames into encoded packets with the configured stream video multiplexer.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO11-6" id="callout_ai_integration_and_model_serving_CO11-6"><img alt="6" src="assets/6.png"/></a></dt>
<dd><p>Add the encoded frames into the opened video container buffer.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO11-7" id="callout_ai_integration_and_model_serving_CO11-7"><img alt="7" src="assets/7.png"/></a></dt>
<dd><p>Flush any remaining frames in the encoder and combine the resulting packet into the output file before returning the buffer containing the encoded video.</p></dd>
</dl></div>

<p>To use image prompts with the service as file uploads, you must install the <code>python-multipart</code> library:<sup><a data-type="noteref" href="ch03.html#id698" id="id698-marker">14</a></sup></p>

<pre data-code-language="bash" data-type="programlisting">$ pip install python-multipart<code class="w"/></pre>

<p>Once installed, you can set up the new endpoint using <a data-type="xref" href="#video_endpoint">Example 3-12</a>.</p>
<div data-type="example" id="video_endpoint">
<h5><span class="label">Example 3-12. </span>Serving generated videos from the image-to-video model</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># main.py</code>

<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">status</code><code class="p">,</code> <code class="n">FastAPI</code><code class="p">,</code> <code class="n">File</code>
<code class="kn">from</code> <code class="nn">fastapi</code><code class="nn">.</code><code class="nn">responses</code> <code class="kn">import</code> <code class="n">StreamingResponse</code>
<code class="kn">from</code> <code class="nn">io</code> <code class="kn">import</code> <code class="n">BytesIO</code>
<code class="kn">from</code> <code class="nn">PIL</code> <code class="kn">import</code> <code class="n">Image</code>

<code class="kn">from</code> <code class="nn">models</code> <code class="kn">import</code> <code class="n">load_video_model</code><code class="p">,</code> <code class="n">generate_video</code>
<code class="kn">from</code> <code class="nn">utils</code> <code class="kn">import</code> <code class="n">export_to_video_buffer</code>

<code class="o">.</code><code class="o">.</code><code class="o">.</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">post</code><code class="p">(</code>
    <code class="s2">"</code><code class="s2">/generate/video</code><code class="s2">"</code><code class="p">,</code>
    <code class="n">responses</code><code class="o">=</code><code class="p">{</code><code class="n">status</code><code class="o">.</code><code class="n">HTTP_200_OK</code><code class="p">:</code> <code class="p">{</code><code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"</code><code class="s2">video/mp4</code><code class="s2">"</code><code class="p">:</code> <code class="p">{</code><code class="p">}</code><code class="p">}</code><code class="p">}</code><code class="p">}</code><code class="p">,</code>
    <code class="n">response_class</code><code class="o">=</code><code class="n">StreamingResponse</code><code class="p">,</code>
<code class="p">)</code>
<code class="k">def</code> <code class="nf">serve_image_to_video_model_controller</code><code class="p">(</code>
    <code class="n">image</code><code class="p">:</code> <code class="nb">bytes</code> <code class="o">=</code> <code class="n">File</code><code class="p">(</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="p">)</code><code class="p">,</code> <code class="n">num_frames</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">25</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO12-1" id="co_ai_integration_and_model_serving_CO12-1"><img alt="1" src="assets/1.png"/></a>
<code class="p">)</code><code class="p">:</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">BytesIO</code><code class="p">(</code><code class="n">image</code><code class="p">)</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO12-2" id="co_ai_integration_and_model_serving_CO12-2"><img alt="2" src="assets/2.png"/></a>
    <code class="n">model</code> <code class="o">=</code> <code class="n">load_video_model</code><code class="p">(</code><code class="p">)</code>
    <code class="n">frames</code> <code class="o">=</code> <code class="n">generate_video</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">image</code><code class="p">,</code> <code class="n">num_frames</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">StreamingResponse</code><code class="p">(</code>
        <code class="n">export_to_video_buffer</code><code class="p">(</code><code class="n">frames</code><code class="p">)</code><code class="p">,</code> <code class="n">media_type</code><code class="o">=</code><code class="s2">"</code><code class="s2">video/mp4</code><code class="s2">"</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO12-3" id="co_ai_integration_and_model_serving_CO12-3"><img alt="3" src="assets/3.png"/></a>
    <code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO12-1" id="callout_ai_integration_and_model_serving_CO12-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Use the <code>File</code> object to specify <code>image</code> as a form file upload.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO12-2" id="callout_ai_integration_and_model_serving_CO12-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Create a Pillow <code>Image</code> object by passing the image bytes transferred to the service.
The model pipeline expects a Pillow image format as input.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO12-3" id="callout_ai_integration_and_model_serving_CO12-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Export the generated frames as a MP4 video and stream  it to the client using an iterable video buffer.</p></dd>
</dl></div>

<p>With the video endpoint set up, you can now upload images to your FastAPI service to animate them as videos.</p>

<p>There are other video models available on the hub that allow you to generate GIFs and animations.
For additional practice, you can try building a GenAI service with them.
<a data-primary="large vision model (LVM)" data-type="indexterm" id="ix_ch03-asciidoc29a"/><a data-primary="LVM (large vision model)" data-type="indexterm" id="ix_ch03-asciidoc29b"/>While open source video models can produce videos at ample quality, OpenAI’s announcement of a new large vision model (LVM) called Sora
has shaken the video generation industry.</p>










<section data-pdf-bookmark="OpenAI Sora" data-type="sect3"><div class="sect3" id="id51">
<h3>OpenAI Sora</h3>

<p><a data-primary="OpenAI Sora video generation model" data-type="indexterm" id="ix_ch03-asciidoc29"/><a data-primary="serving GenAI models" data-secondary="OpenAI Sora" data-type="indexterm" id="ix_ch03-asciidoc30"/><a data-primary="Sora video generation model" data-type="indexterm" id="ix_ch03-asciidoc31"/>Text-to-video models are limited in their generation capabilities.
Apart from the immense computational power needed to sequentially generate coherent video frames, training these models can be challenging due to:</p>

<ul>
<li>
<p><em>Maintaining temporal and spatial consistency across frames</em> to achieve realistic undistorted video outputs.</p>
</li>
<li>
<p><em>Lack of training data</em> with high-quality caption and metadata needed to train video models.</p>
</li>
<li>
<p><em>Captioning challenges</em> when captioning the content of videos clearly and descriptively is time-consuming and moves beyond drafting short pieces of text.
Captioning must describe the narrative and scenes for each sequence for the model to learn and map the rich patterns contained in the video to text.</p>
</li>
</ul>

<p>Because of these reasons, there has not been a breakthrough with video generation models until the announcement of OpenAI’s Sora model.</p>

<p>Sora is a generalist large vision diffusion transformer model capable of generating videos and images spanning diverse durations, aspect ratios, and resolutions, up to a full minute of high-definition video.
Its architecture is based on the transformers commonly used in LLMs and the diffusion process.
Whereas LLMs use text tokens, Sora uses visual patches.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The Sora model combines elements and principles of the transformer and SD architectures, while in <a data-type="xref" href="#video_model_loading">Example 3-10</a>, you used Stability AI’s SD model to generate videos.</p>
</div>

<p>So what makes Sora different?</p>

<p>Transformers have demonstrated remarkable scalability across language models, computer vision, and image generation, so it made sense for Sora architecture to be based on transformers to handle diverse inputs like text, images, or video frames. Also, since transformers can understand complex patterns and long-range dependencies in sequential data, Sora as a vision transformer can also capture fine-grained temporal and spatial relationships between video frames to generate coherent frames with smooth transitions between them (i.e., exhibiting temporal consistency).</p>

<p>Furthermore, Sora borrows capabilities of the SD models to generate high-quality and visually coherent video frames with precise controls using the iterative noise reduction process.
Using the diffusion process lets Sora generate images with fine detail and desirable properties.</p>

<p>By combining both sequential reasoning of transformers with iterative refinement of SD, Sora can generate high-resolution, coherent, and smooth videos from multimodal inputs like text and images that contain abstract concepts.</p>

<p>Sora’s network architecture is also designed to reduce dimensionality through a 
<span class="keep-together">U-shape</span> network where high-dimensional visual data is compressed and encoded into a latent noisy space.
Sora can then generate patches from the latent space through the denoising diffusion process.</p>

<p>The diffusion process is similar to image-based SD models.
Instead of having a 2D 
<span class="keep-together">U-Net</span> normally used for images, OpenAI has trained a 3D U-Net where the third dimension is a sequence of frames across time (making a video), as shown in <a data-type="xref" href="#images_to_videos">Figure 3-20</a>.</p>

<figure><div class="figure" id="images_to_videos">
<img alt="bgai 0320" src="assets/bgai_0320.png"/>
<h6><span class="label">Figure 3-20. </span>A sequence of images forms a video</h6>
</div></figure>

<p>OpenAI has demonstrated that by compressing videos into patches, as shown in <a data-type="xref" href="#videos_to_patches">Figure 3-21</a>, the model can achieve scalability of learning high-dimensional representations when training on diverse types of videos and images varying in resolution, durations, and aspect ratios.</p>

<figure><div class="figure" id="videos_to_patches">
<img alt="bgai 0321" src="assets/bgai_0321.png"/>
<h6><span class="label">Figure 3-21. </span>Video compression into space-time patches</h6>
</div></figure>

<p>Through the diffusion process, Sora crunches input noisy patches to generate clean videos and images in any aspect ratio, size, and resolution for devices directly in their native screen sizes.</p>

<p>While a text transformer is predicting the next token in a text sequence, Sora’s vision transformer is predicting the next patch to generate an image or a video, as shown in <a data-type="xref" href="#vision_transformer_sequence">Figure 3-22</a>.</p>

<figure><div class="figure" id="vision_transformer_sequence">
<img alt="bgai 0322" src="assets/bgai_0322.png"/>
<h6><span class="label">Figure 3-22. </span>Token prediction by the vision transformer</h6>
</div></figure>

<p>Through training on various datasets, OpenAI overcame the previously mentioned challenges of training vision models such as lack of quality captions, high dimensionality of video data, etc., to name a few.</p>

<p>What is fascinating about Sora and potentially other LVMs is the emerging capabilities they exhibit:</p>
<dl>
<dt>3D consistency</dt>
<dd>
<p>Objects in the generated scenes remain consistent and adjust to perspective even when the camera moves and rotates around the scene.</p>
</dd>
<dt>Object permanence and large range coherence</dt>
<dd>
<p>Objects and people that are occluded or leave a frame at a location will persist when they reappear in the field of view.
In some cases, the model effectively remembers how to keep them consistent in the environment.
This is also referred to as <em>temporal consistency</em> that most video models struggle with.</p>
</dd>
<dt>World interaction</dt>
<dd>
<p>Actions simulated in generated videos realistically affect the environment.
For instance, Sora understands the action of eating a burger should leave a bite mark on it.</p>
</dd>
<dt>Simulating environments</dt>
<dd>
<p>Sora can also simulate worlds—real or fictional environments like in games—while adhering to the rules of interactions in those environments, such as playing a character in a <em>Minecraft</em> level.
In other words, Sora has learned to be a data-driven physics engine.</p>
</dd>
</dl>

<p><a data-type="xref" href="#sora_emerging_capabilities">Figure 3-23</a> illustrates these capabilities.</p>

<figure><div class="figure" id="sora_emerging_capabilities">
<img alt="bgai 0323" src="assets/bgai_0323.png"/>
<h6><span class="label">Figure 3-23. </span>Sora’s emergent capabilities</h6>
</div></figure>

<p>At the time of this writing, Sora  has not yet been released as an API, but open source alternatives have already emerged.
A promising large vision model called “Latte” allows you to fine-tune the LVM on your own visual data.</p>
<div data-type="caution"><h6>Caution</h6>
<p>You can’t yet commercialize some open source models, including Latte, at the time of writing.
Always check the model card and the license to ensure any commercial use is allowed.</p>
</div>

<p>Combining transformers with diffusers to create LVMs is a promising area of research for generating complex outputs like videos.
However, I imagine the same process can be applied for generating other types of high-dimensional data that can be represented as multidimensional arrays<a data-startref="ix_ch03-asciidoc31" data-type="indexterm" id="id699"/><a data-startref="ix_ch03-asciidoc30" data-type="indexterm" id="id700"/><a data-startref="ix_ch03-asciidoc29" data-type="indexterm" id="id701"/><a data-startref="ix_ch03-asciidoc29a" data-type="indexterm" id="id702"/><a data-startref="ix_ch03-asciidoc29b" data-type="indexterm" id="id703"/>.<a data-startref="ix_ch03-asciidoc28" data-type="indexterm" id="id704"/></p>

<p>You should now feel more comfortable building services with text, audio, vision, and video models.
Next, let’s take a look at another set of models capable of generating complex data such as 3D geometries by building a 3D asset generator service.</p>
</div></section>
</div></section>








<section data-pdf-bookmark="3D Models" data-type="sect2"><div class="sect2" id="id52">
<h2>3D Models</h2>

<p><a data-primary="3D models" data-primary-sortas="threeD" data-type="indexterm" id="ix_ch03-asciidoc32"/><a data-primary="serving GenAI models" data-secondary="3D models" data-secondary-sortas="threeD" data-type="indexterm" id="ix_ch03-asciidoc33"/>You now understand how previously mentioned models use transformers and diffusers to generate any form of textual, audio, or visual data.
Producing 3D geometries requires a different approach than image, audio, and text generation because you must account for spatial relationships, depth information, and geometric consistency, which add layers of complexity not present in other data types.</p>

<p><a data-primary="meshes" data-type="indexterm" id="ix_ch03-asciidoc34"/>For 3D geometries, <em>meshes</em> are used to define the shape of an object. Software packages like Autodesk 3ds Max, Maya, and SolidWorks can be used to produce, edit, and render these meshes.</p>

<p>Meshes are effectively a collection of <em>vertices</em>, <em>edges</em>, and <em>faces</em> that reside in a 3D virtual space.
Vertices are points in space that connect to form edges.
Edges form faces (polygons) when they enclose on a flat surface, often in the shape of triangles or quadrilaterals.
<a data-type="xref" href="#vertices_edges_faces">Figure 3-24</a> shows the differences between vertices, edges, and faces.</p>

<figure><div class="figure" id="vertices_edges_faces">
<img alt="bgai 0324" src="assets/bgai_0324.png"/>
<h6><span class="label">Figure 3-24. </span>Vertices, edges, and faces</h6>
</div></figure>

<p>You can define vertices by their coordinates in a 3D space, usually determined by a Cartesian coordinate system (x, y, z).
Essentially, the arrangement and connection of vertices form surfaces of a 3D mesh that define a geometry.</p>

<p><a data-type="xref" href="#mesh">Figure 3-25</a> shows how these features combine to define a mesh of a 3D geometry such as a monkey’s head.</p>

<figure><div class="figure" id="mesh">
<img alt="bgai 0325" src="assets/bgai_0325.png"/>
<h6><span class="label">Figure 3-25. </span>Mesh for 3D geometry of a monkey head using both triangular and 
<span class="keep-together">quadrilateral</span> polygons (shown in Blender, open source 3D modeling software)</h6>
</div></figure>

<p>You can train and use a transformer model to predict the next token in a sequence where the sequence is coordinates of vertices on a 3D mesh surface.
Such a generative model can produce 3D geometries by predicting the next set of vertices and faces within a 3D space that form the desired geometry.
However, the geometry would require thousands of vertices and faces to achieve a smooth surface.</p>

<p>This means for each 3D object, you need to wait for a long time for the generation to complete, and the results may still remain low fidelity.
Because of this, the most capable models (i.e., OpenAI’s Shap-E) in producing 3D geometry train functions (with many parameters) to implicitly define surfaces and volumes in a 3D space.</p>

<p><a data-primary="implicit functions" data-type="indexterm" id="id705"/>Implicit functions are useful for creating smooth surfaces or handling intricate details that are challenging for discrete representations like meshes.
A trained model can consist of an encoder that maps patterns to an implicit function.
<a data-primary="conditional 3D GenAI models" data-primary-sortas="conditional threeD GenAI models" data-type="indexterm" id="id706"/>Instead of explicitly generating sequences of vertices and faces for a mesh,
<em>conditional</em> 3D models can evaluate the trained implicit functions across a continuous 3D space.
As a result, the generation process has a high degree of freedom, control, and flexibility in producing high-fidelity outputs, becoming suitable for applications that require detailed and intricate 3D geometries.</p>

<p><a data-primary="neural radiance fields (NeRF)" data-type="indexterm" id="id707"/>Once the model’s encoder is trained to produce implicit functions, it leverages the <em>neural radiance fields</em> (NeRF) rendering technique, as part of the decoder, to construct 3D scenes.
NeRF maps a pair of inputs—a 3D spatial coordinate and a 3D viewing direction—to an output consisting of an object density and RGB color via the implicit functions.
To synthesize new views in a 3D scene, the NeRF method considers the viewport as a matrix of rays.
Each pixel corresponding to a ray, originates from the camera position, and then extends in the viewing direction.
The color of each ray and associated pixel is computed by evaluating the implicit function along the ray and integrating the results to calculate the RGB color.</p>

<p><a data-primary="signed distance functions (SDFs)" data-type="indexterm" id="id708"/><a data-primary="SDFs (signed distance functions)" data-type="indexterm" id="id709"/>Once the 3D scene is computed, <em>signed distance functions</em> (SDFs) are used to generate meshes, or wireframes of 3D objects by calculating the distance and color of any point to the nearest surface of the 3D object.
Think of SDFs as a way to describe a 3D object by telling you how far away every point in space is from the object’s surface.
This function gives a number for each point: if the point is inside the object, the number is negative; if it’s on the surface, the number is zero; and if it’s outside, the number is positive.
The surface of the object is where all points have the number zero.
SDFs help to turn this information into a 3D mesh.</p>

<p>Despite the use of implicit functions, the quality of outputs is still inferior to human-created 3D assets and may feel cartoonish.
However, with 3D GenAI models, you can generate the initial 3D geometries to iterate over concepts and refine 3D assets quickly.</p>










<section data-pdf-bookmark="OpenAI Shap-E" data-type="sect3"><div class="sect3" id="id53">
<h3>OpenAI Shap-E</h3>

<p><a data-primary="OpenAI Shap-E" data-type="indexterm" id="ix_ch03-asciidoc35"/><a data-primary="serving GenAI models" data-secondary="OpenAI Shap-E" data-type="indexterm" id="ix_ch03-asciidoc36"/><em>Shap-E</em> (developed by OpenAI) is an open source model “conditioned” on input 3D data (descriptions, parameters, partial geometries, colors, etc.) to generate specific 3D shapes.
You can use Shap-E to create an image or text-to-3D services.</p>

<p>As usual, you start by downloading and loading the model from Hugging Face, as shown in <a data-type="xref" href="#loading_shap-e">Example 3-13</a>.</p>
<div data-type="example" id="loading_shap-e">
<h5><span class="label">Example 3-13. </span>Downloading and loading OpenAI’s Shap-E model</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># models.py</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">diffusers</code> <code class="kn">import</code> <code class="n">ShapEPipeline</code>

<code class="n">device</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">device</code><code class="p">(</code><code class="s2">"</code><code class="s2">cuda</code><code class="s2">"</code> <code class="k">if</code> <code class="n">torch</code><code class="o">.</code><code class="n">cuda</code><code class="o">.</code><code class="n">is_available</code><code class="p">(</code><code class="p">)</code> <code class="k">else</code> <code class="s2">"</code><code class="s2">cpu</code><code class="s2">"</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">load_3d_model</code><code class="p">(</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">ShapEPipeline</code><code class="p">:</code>
    <code class="n">pipe</code> <code class="o">=</code> <code class="n">ShapEPipeline</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"</code><code class="s2">openai/shap-e</code><code class="s2">"</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">pipe</code>

<code class="k">def</code> <code class="nf">generate_3d_geometry</code><code class="p">(</code>
    <code class="n">pipe</code><code class="p">:</code> <code class="n">ShapEPipeline</code><code class="p">,</code> <code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">num_inference_steps</code><code class="p">:</code> <code class="nb">int</code>
<code class="p">)</code><code class="p">:</code>
    <code class="n">images</code> <code class="o">=</code> <code class="n">pipe</code><code class="p">(</code>
        <code class="n">prompt</code><code class="p">,</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO13-1" id="co_ai_integration_and_model_serving_CO13-1"><img alt="1" src="assets/1.png"/></a>
        <code class="n">guidance_scale</code><code class="o">=</code><code class="mf">15.0</code><code class="p">,</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO13-2" id="co_ai_integration_and_model_serving_CO13-2"><img alt="2" src="assets/2.png"/></a>
        <code class="n">num_inference_steps</code><code class="o">=</code><code class="n">num_inference_steps</code><code class="p">,</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO13-3" id="co_ai_integration_and_model_serving_CO13-3"><img alt="3" src="assets/3.png"/></a>
        <code class="n">output_type</code><code class="o">=</code><code class="s2">"</code><code class="s2">mesh</code><code class="s2">"</code><code class="p">,</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO13-4" id="co_ai_integration_and_model_serving_CO13-4"><img alt="4" src="assets/4.png"/></a>
    <code class="p">)</code><code class="o">.</code><code class="n">images</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO13-5" id="co_ai_integration_and_model_serving_CO13-5"><img alt="5" src="assets/5.png"/></a>
    <code class="k">return</code> <code class="n">images</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO13-1" id="callout_ai_integration_and_model_serving_CO13-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>This specific Shap-E pipeline accepts textual prompts, but if you want to pass image prompts, you need to load a different pipeline.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO13-2" id="callout_ai_integration_and_model_serving_CO13-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Use the <code>guidance_scale</code> parameter to fine-tune the generation process to better match the prompt.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO13-3" id="callout_ai_integration_and_model_serving_CO13-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Use the <code>num_inference_steps</code> parameter to control the output resolution in exchange for additional computation.
Requesting a higher number of inference steps or increasing the guidance scale can elongate the rendering time in exchange for higher-quality outputs that better follow the user’s request.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO13-4" id="callout_ai_integration_and_model_serving_CO13-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Set the <code>output_type</code> parameter to produce <code>mesh</code> tensors as output.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO13-5" id="callout_ai_integration_and_model_serving_CO13-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>By default, the Shap-E pipeline will produce a sequence of images that can be combined to generate a rotating GIF animation of the object.
You can export this output to either GIFs, videos, or OBJ files that can be loaded in 3D modeling tools such as Blender.</p></dd>
</dl></div>

<p>Now that you have a model loading and 3D mesh generation functions, let’s export the mesh into a buffer using <a data-type="xref" href="#mesh_to_buffer">Example 3-14</a>.</p>
<div data-type="tip"><h6>Tip</h6>
<p><code>open3d</code> is an open source library for processing 3D data such as point clouds, meshes, and color images with depth information (i.e., RGB-D images). You will need to install <code>open3d</code> to run <a data-type="xref" href="#mesh_to_buffer">Example 3-14</a>:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install open3d<code class="w"/></pre>
</div>
<div data-type="example" id="mesh_to_buffer">
<h5><span class="label">Example 3-14. </span>Exporting a 3D tensor mesh to a Wavefront OBJ buffer</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># utils.py</code>

<code class="kn">import</code> <code class="nn">os</code>
<code class="kn">import</code> <code class="nn">tempfile</code>
<code class="kn">from</code> <code class="nn">io</code> <code class="kn">import</code> <code class="n">BytesIO</code>
<code class="kn">from</code> <code class="nn">pathlib</code> <code class="kn">import</code> <code class="n">Path</code>
<code class="kn">import</code> <code class="nn">open3d</code> <code class="k">as</code> <code class="nn">o3d</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">diffusers</code><code class="nn">.</code><code class="nn">pipelines</code><code class="nn">.</code><code class="nn">shap_e</code><code class="nn">.</code><code class="nn">renderer</code> <code class="kn">import</code> <code class="n">MeshDecoderOutput</code>

<code class="k">def</code> <code class="nf">mesh_to_obj_buffer</code><code class="p">(</code><code class="n">mesh</code><code class="p">:</code> <code class="n">MeshDecoderOutput</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">BytesIO</code><code class="p">:</code>
    <code class="n">mesh_o3d</code> <code class="o">=</code> <code class="n">o3d</code><code class="o">.</code><code class="n">geometry</code><code class="o">.</code><code class="n">TriangleMesh</code><code class="p">(</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO14-1" id="co_ai_integration_and_model_serving_CO14-1"><img alt="1" src="assets/1.png"/></a>
    <code class="n">mesh_o3d</code><code class="o">.</code><code class="n">vertices</code> <code class="o">=</code> <code class="n">o3d</code><code class="o">.</code><code class="n">utility</code><code class="o">.</code><code class="n">Vector3dVector</code><code class="p">(</code>
        <code class="n">mesh</code><code class="o">.</code><code class="n">verts</code><code class="o">.</code><code class="n">cpu</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">detach</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">numpy</code><code class="p">(</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO14-2" id="co_ai_integration_and_model_serving_CO14-2"><img alt="2" src="assets/2.png"/></a>
    <code class="p">)</code>
    <code class="n">mesh_o3d</code><code class="o">.</code><code class="n">triangles</code> <code class="o">=</code> <code class="n">o3d</code><code class="o">.</code><code class="n">utility</code><code class="o">.</code><code class="n">Vector3iVector</code><code class="p">(</code>
        <code class="n">mesh</code><code class="o">.</code><code class="n">faces</code><code class="o">.</code><code class="n">cpu</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">detach</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">numpy</code><code class="p">(</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO14-2" id="co_ai_integration_and_model_serving_CO14-3"><img alt="2" src="assets/2.png"/></a>
    <code class="p">)</code>

    <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">mesh</code><code class="o">.</code><code class="n">vertex_channels</code><code class="p">)</code> <code class="o">==</code> <code class="mi">3</code><code class="p">:</code>  <code class="c1"># You have color channels</code>
        <code class="n">vert_color</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">stack</code><code class="p">(</code>
            <code class="p">[</code><code class="n">mesh</code><code class="o">.</code><code class="n">vertex_channels</code><code class="p">[</code><code class="n">channel</code><code class="p">]</code> <code class="k">for</code> <code class="n">channel</code> <code class="ow">in</code> <code class="s2">"</code><code class="s2">RGB</code><code class="s2">"</code><code class="p">]</code><code class="p">,</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code>
        <code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO14-3" id="co_ai_integration_and_model_serving_CO14-4"><img alt="3" src="assets/3.png"/></a>
        <code class="n">mesh_o3d</code><code class="o">.</code><code class="n">vertex_colors</code> <code class="o">=</code> <code class="n">o3d</code><code class="o">.</code><code class="n">utility</code><code class="o">.</code><code class="n">Vector3dVector</code><code class="p">(</code>
            <code class="n">vert_color</code><code class="o">.</code><code class="n">cpu</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">detach</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">numpy</code><code class="p">(</code><code class="p">)</code>
        <code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO14-4" id="co_ai_integration_and_model_serving_CO14-5"><img alt="4" src="assets/4.png"/></a>

    <code class="k">with</code> <code class="n">tempfile</code><code class="o">.</code><code class="n">NamedTemporaryFile</code><code class="p">(</code><code class="n">delete</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">suffix</code><code class="o">=</code><code class="s2">"</code><code class="s2">.obj</code><code class="s2">"</code><code class="p">)</code> <code class="k">as</code> <code class="n">tmp</code><code class="p">:</code>
        <code class="n">o3d</code><code class="o">.</code><code class="n">io</code><code class="o">.</code><code class="n">write_triangle_mesh</code><code class="p">(</code><code class="n">tmp</code><code class="o">.</code><code class="n">name</code><code class="p">,</code> <code class="n">mesh_o3d</code><code class="p">,</code> <code class="n">write_ascii</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">tmp</code><code class="o">.</code><code class="n">name</code><code class="p">,</code> <code class="s2">"</code><code class="s2">rb</code><code class="s2">"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
            <code class="n">buffer</code> <code class="o">=</code> <code class="n">BytesIO</code><code class="p">(</code><code class="n">f</code><code class="o">.</code><code class="n">read</code><code class="p">(</code><code class="p">)</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO14-5" id="co_ai_integration_and_model_serving_CO14-6"><img alt="5" src="assets/5.png"/></a>
        <code class="n">os</code><code class="o">.</code><code class="n">remove</code><code class="p">(</code><code class="n">tmp</code><code class="o">.</code><code class="n">name</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO14-6" id="co_ai_integration_and_model_serving_CO14-7"><img alt="6" src="assets/6.png"/></a>

    <code class="k">return</code> <code class="n">buffer</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO14-1" id="callout_ai_integration_and_model_serving_CO14-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Create an Open3D triangle mesh object.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO14-2" id="callout_ai_integration_and_model_serving_CO14-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Convert the generated mesh from the model into an Open3D triangle mesh object.
To do so, grab vertices and triangles from the generated 3D mesh by moving the mesh vertices and faces tensors to the CPU and converting them to <code>numpy</code> arrays.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO14-4" id="callout_ai_integration_and_model_serving_CO14-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Check if the mesh has three vertex color channels (indicating RGB color data) and stack these channels into a tensor.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO14-5" id="callout_ai_integration_and_model_serving_CO14-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Convert mesh color tensor to a format compatible with Open3D for setting the vertex colors of the mesh.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO14-6" id="callout_ai_integration_and_model_serving_CO14-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>Use a temporary file to create and return a data buffer.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO14-7" id="callout_ai_integration_and_model_serving_CO14-6"><img alt="6" src="assets/6.png"/></a></dt>
<dd><p>Windows doesn’t support <code>NameTemporaryFile</code>’s <code>delete=True</code> option.
Instead, manually remove the created temporary file just before returning the in-memory buffer.</p></dd>
</dl></div>

<p class="less_space pagebreak-before">Finally, you can build the endpoints, as shown in <a data-type="xref" href="#shap-e_endpoint">Example 3-15</a>.</p>
<div data-type="example" id="shap-e_endpoint">
<h5><span class="label">Example 3-15. </span>Creating the 3D model-serving endpoint</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># main.py</code>

<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code><code class="p">,</code> <code class="n">status</code>
<code class="kn">from</code> <code class="nn">fastapi</code><code class="nn">.</code><code class="nn">responses</code> <code class="kn">import</code> <code class="n">StreamingResponse</code>
<code class="kn">from</code> <code class="nn">models</code> <code class="kn">import</code> <code class="n">load_3d_model</code><code class="p">,</code> <code class="n">generate_3d_geometry</code>
<code class="kn">from</code> <code class="nn">utils</code> <code class="kn">import</code> <code class="n">mesh_to_obj_buffer</code>

<code class="o">.</code><code class="o">.</code><code class="o">.</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code>
    <code class="s2">"</code><code class="s2">/generate/3d</code><code class="s2">"</code><code class="p">,</code>
    <code class="n">responses</code><code class="o">=</code><code class="p">{</code><code class="n">status</code><code class="o">.</code><code class="n">HTTP_200_OK</code><code class="p">:</code> <code class="p">{</code><code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"</code><code class="s2">model/obj</code><code class="s2">"</code><code class="p">:</code> <code class="p">{</code><code class="p">}</code><code class="p">}</code><code class="p">}</code><code class="p">}</code><code class="p">,</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO15-1" id="co_ai_integration_and_model_serving_CO15-1"><img alt="1" src="assets/1.png"/></a>
    <code class="n">response_class</code><code class="o">=</code><code class="n">StreamingResponse</code><code class="p">,</code>
<code class="p">)</code>
<code class="k">def</code> <code class="nf">serve_text_to_3d_model_controller</code><code class="p">(</code>
    <code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">num_inference_steps</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">25</code>
<code class="p">)</code><code class="p">:</code>
    <code class="n">model</code> <code class="o">=</code> <code class="n">load_3d_model</code><code class="p">(</code><code class="p">)</code>
    <code class="n">mesh</code> <code class="o">=</code> <code class="n">generate_3d_geometry</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">prompt</code><code class="p">,</code> <code class="n">num_inference_steps</code><code class="p">)</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">StreamingResponse</code><code class="p">(</code>
        <code class="n">mesh_to_obj_buffer</code><code class="p">(</code><code class="n">mesh</code><code class="p">)</code><code class="p">,</code> <code class="n">media_type</code><code class="o">=</code><code class="s2">"</code><code class="s2">model/obj</code><code class="s2">"</code>
    <code class="p">)</code>
    <code class="n">response</code><code class="o">.</code><code class="n">headers</code><code class="p">[</code><code class="s2">"</code><code class="s2">Content-Disposition</code><code class="s2">"</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code>
        <code class="sa">f</code><code class="s2">"</code><code class="s2">attachment; filename=</code><code class="si">{</code><code class="n">prompt</code><code class="si">}</code><code class="s2">.obj</code><code class="s2">"</code>
    <code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO15-2" id="co_ai_integration_and_model_serving_CO15-2"><img alt="2" src="assets/2.png"/></a>
    <code class="k">return</code> <code class="n">response</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO15-1" id="callout_ai_integration_and_model_serving_CO15-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Specify the OpenAPI specification for a successful response to include <code>model/obj</code> as the media content type.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO15-2" id="callout_ai_integration_and_model_serving_CO15-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Indicate to clients that the content of streaming response should be treated as an attachment.</p></dd>
</dl></div>

<p>If you send a request to the <code>/generate/3d</code> endpoint, the download of the 3D object as a Wavefront OBJ file should start as soon as generation is complete.</p>

<p>You can import the OBJ file into any 3D modeling software such as Blender to view the 3D geometry.
Using prompts such as <code>apple</code>, <code>car</code>, <code>phone</code>, and <code>donut</code>
you can generate the 3D geometries shown in <a data-type="xref" href="#shape_e_blender">Figure 3-26</a>.</p>

<figure><div class="figure" id="shape_e_blender">
<img alt="bgai 0326" src="assets/bgai_0326.png"/>
<h6><span class="label">Figure 3-26. </span>3D geometries of a car, apple, phone, and donut imported into Blender</h6>
</div></figure>

<p>If you isolate an object like the apple and enable the wireframe view, you can see all the vertices and edges that make up the apple’s mesh, represented as triangular polygons, as shown<a data-startref="ix_ch03-asciidoc36" data-type="indexterm" id="id710"/> in <a data-type="xref" href="#shape_e_apple_wireframe">Figure 3-27</a>.</p>

<figure><div class="figure" id="shape_e_apple_wireframe">
<img alt="bgai 0327" src="assets/bgai_0327.png"/>
<h6><span class="label">Figure 3-27. </span>Zooming in on the generated 3D mesh to view triangular polygons; inset: viewing the generated apple geometry mesh (including vertices and edges)</h6>
</div></figure>

<p>Shap-E supersedes another older model called <em>Point-E</em> that generates <em>point clouds</em> 
<span class="keep-together">of 3D objects.</span>
This is because Shap-E, compared to Point-E, converges faster and 
<span class="keep-together">reaches</span> comparable or better generation shape quality despite modeling a higher-dimensional, multirepresentation output space.<a data-startref="ix_ch03-asciidoc35" data-type="indexterm" id="id711"/><a data-startref="ix_ch03-asciidoc34" data-type="indexterm" id="id712"/></p>

<p>Point clouds (often used in the construction industry) are a large collection of point coordinates that closely represent a 3D object (such as a building structure) in a real-world space.
Environment scanning devices including LiDAR laser scanners produce point clouds to represent objects within a 3D space at approximate measurements close to the real-world environment.</p>

<p>As 3D models improve, it may be possible to generate objects that closely represent their real counterparts<a data-startref="ix_ch03-asciidoc33" data-type="indexterm" id="id713"/><a data-startref="ix_ch03-asciidoc32" data-type="indexterm" id="id714"/>.<a data-startref="ix_ch03-asciidoc0" data-type="indexterm" id="id715"/></p>
</div></section>
</div></section>
</div></section>






<section data-pdf-bookmark="Strategies for Serving Generative AI Models" data-type="sect1"><div class="sect1" id="id54">
<h1>Strategies for Serving Generative AI Models</h1>

<p><a data-primary="serving GenAI models" data-secondary="strategies" data-type="indexterm" id="ix_ch03-asciidoc37"/>You now should feel more confident building your own endpoints that serve a variety of models from the Hugging Face model repository.
We touched upon a few different models, including those that generate text, image, video, audio, and 3D shapes.</p>

<p>The models you used were small, so they could be loaded and used on a CPU with reasonable outputs.
<a data-primary="video random access memory (VRAM)" data-type="indexterm" id="id716"/>However, in a production scenarios, you may want to use larger models to produce higher-quality results that may run only on GPUs and require a significant amount of video random access memory (VRAM).</p>

<p>In addition to leveraging GPUs, you will need to pick a model-serving strategy from several options:</p>
<dl>
<dt>Be model agnostic</dt>
<dd>
<p>Load models and generate outputs on every request (useful for model swapping).</p>
</dd>
<dt>Be compute efficient</dt>
<dd>
<p>Use the FastAPI lifespan to preload models that can be reused for every request.</p>
</dd>
<dt>Be lean</dt>
<dd>
<p>Serve models externally without frameworks or work with third-party model APIs and interact with them via FastAPI.</p>
</dd>
</dl>

<p>Let’s take a look at each strategy in detail.</p>








<section data-pdf-bookmark="Be Model Agnostic: Swap Models on Every Request" data-type="sect2"><div class="sect2" id="id55">
<h2>Be Model Agnostic: Swap Models on Every Request</h2>

<p><a data-primary="model swapping (model-serving strategy)" data-type="indexterm" id="ix_ch03-asciidoc38"/><a data-primary="serving GenAI models" data-secondary="strategies" data-tertiary="model agnostic: swapping models on every request" data-type="indexterm" id="ix_ch03-asciidoc39"/>In the previous code examples, you defined the model loading and generation functions and then used them in route handler controllers. Using this serving strategy, FastAPI loads a model into RAM (or VRAM if using a GPU) and runs a generation process.
Once FastAPI returns the results, the model is then unloaded from RAM.
The process repeats for the next request.</p>

<p>As the model is unloaded after use, the memory is released to be used by another process or model.
With this approach, you dynamically swap various models in a single request if processing time isn’t a concern.
This means other concurrent requests must wait before the server responds to them.</p>

<p>When serving requests, FastAPI will queue incoming requests and process them in a first in first out (FIFO) order.
This behavior will lead to long waiting times as a model needs to be loaded and unloaded every time.
In most cases, this strategy is not recommended, but if you need to swap between multiple large models and you don’t have sufficient RAM, then you can adopt this strategy for prototyping.
However, in production scenarios, you should never use this strategy for obvious reasons—your users will want to avoid the long wait times.</p>

<p><a data-type="xref" href="#model_loading_on_request">Figure 3-28</a> shows this model service strategy.</p>

<figure><div class="figure" id="model_loading_on_request">
<img alt="bgai 0329" src="assets/bgai_0329.png"/>
<h6><span class="label">Figure 3-28. </span>Loading and using models on every request</h6>
</div></figure>

<p>If you need to use different models in each request and have limited memory, this method can work well for quickly trying things on a less powerful machine with just a few users.
The trade-off is significantly slower processing time due to model 
<span class="keep-together">swapping.</span>
However, in production scenarios, it is better to get larger RAM and use the model preloading strategy with FastAPI application lifespan.<a data-startref="ix_ch03-asciidoc39" data-type="indexterm" id="id717"/><a data-startref="ix_ch03-asciidoc38" data-type="indexterm" id="id718"/></p>
</div></section>








<section data-pdf-bookmark="Be Compute Efficient: Preload Models with the FastAPI Lifespan" data-type="sect2"><div class="sect2" id="id56">
<h2>Be Compute Efficient: Preload Models with the FastAPI Lifespan</h2>

<p><a data-primary="application lifespan" data-type="indexterm" id="ix_ch03-asciidoc40"/><a data-primary="model preloading (model-serving strategy)" data-type="indexterm" id="ix_ch03-asciidoc41"/><a data-primary="serving GenAI models" data-secondary="strategies" data-tertiary="compute efficient: preloading models with FastAPI lifespan" data-type="indexterm" id="ix_ch03-asciidoc42"/>The most compute-efficient strategy for loading models in FastAPI is to use the application lifespan.
With this approach, you load models on application startup and unload them on shutdown.
During shutdown, you can also undertake any cleanup steps required, such as filesystem cleanup or logging.</p>

<p>The main benefit of this strategy compared to the first one mentioned is that you avoid reloading heavy models on each request.
You can load a heavy model once and then make generations on every request coming using a preloaded model.
As a result, you will save several minutes in processing time in exchange for a significant chunk of your RAM (or VRAM if using GPU).
However, your application user experience will improve considerably due to shorter response times.</p>

<p><a data-type="xref" href="#model_loading_lifespan">Figure 3-29</a> shows the model-serving strategy that uses application lifespan.</p>

<figure><div class="figure" id="model_loading_lifespan">
<img alt="bgai 0330" src="assets/bgai_0330.png"/>
<h6><span class="label">Figure 3-29. </span>Using the FastAPI application lifespan to preload models</h6>
</div></figure>

<p>You can implement model preloading using the application lifespan, as shown in <a data-type="xref" href="#model_preloading_lifespan">Example 3-16</a>.</p>
<div data-type="example" id="model_preloading_lifespan">
<h5><span class="label">Example 3-16. </span>Model preloading with application lifespan</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># main.py</code>

<code class="kn">from</code> <code class="nn">contextlib</code> <code class="kn">import</code> <code class="n">asynccontextmanager</code>
<code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">AsyncIterator</code>
<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code><code class="p">,</code> <code class="n">Response</code><code class="p">,</code> <code class="n">status</code>
<code class="kn">from</code> <code class="nn">models</code> <code class="kn">import</code> <code class="n">load_image_model</code><code class="p">,</code> <code class="n">generate_image</code>
<code class="kn">from</code> <code class="nn">utils</code> <code class="kn">import</code> <code class="n">img_to_bytes</code>

<code class="n">models</code> <code class="o">=</code> <code class="p">{</code><code class="p">}</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO16-1" id="co_ai_integration_and_model_serving_CO16-1"><img alt="1" src="assets/1.png"/></a>

<code class="nd">@asynccontextmanager</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO16-2" id="co_ai_integration_and_model_serving_CO16-2"><img alt="2" src="assets/2.png"/></a>
<code class="k">async</code> <code class="k">def</code> <code class="nf">lifespan</code><code class="p">(</code><code class="n">_</code><code class="p">:</code> <code class="n">FastAPI</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">AsyncIterator</code><code class="p">[</code><code class="kc">None</code><code class="p">]</code><code class="p">:</code>
    <code class="n">models</code><code class="p">[</code><code class="s2">"</code><code class="s2">text2image</code><code class="s2">"</code><code class="p">]</code> <code class="o">=</code> <code class="n">load_image_model</code><code class="p">(</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO16-3" id="co_ai_integration_and_model_serving_CO16-3"><img alt="3" src="assets/3.png"/></a>

    <code class="k">yield</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO16-4" id="co_ai_integration_and_model_serving_CO16-4"><img alt="4" src="assets/4.png"/></a>

    <code class="o">.</code><code class="o">.</code><code class="o">.</code> <code class="c1"># Run cleanup code here</code>

    <code class="n">models</code><code class="o">.</code><code class="n">clear</code><code class="p">(</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO16-5" id="co_ai_integration_and_model_serving_CO16-5"><img alt="5" src="assets/5.png"/></a>

<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">(</code><code class="n">lifespan</code><code class="o">=</code><code class="n">lifespan</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO16-6" id="co_ai_integration_and_model_serving_CO16-6"><img alt="6" src="assets/6.png"/></a>

<code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code>
    <code class="s2">"</code><code class="s2">/generate/image</code><code class="s2">"</code><code class="p">,</code>
    <code class="n">responses</code><code class="o">=</code><code class="p">{</code><code class="n">status</code><code class="o">.</code><code class="n">HTTP_200_OK</code><code class="p">:</code> <code class="p">{</code><code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"</code><code class="s2">image/png</code><code class="s2">"</code><code class="p">:</code> <code class="p">{</code><code class="p">}</code><code class="p">}</code><code class="p">}</code><code class="p">}</code><code class="p">,</code>
    <code class="n">response_class</code><code class="o">=</code><code class="n">Response</code><code class="p">,</code>
<code class="p">)</code>
<code class="k">def</code> <code class="nf">serve_text_to_image_model_controller</code><code class="p">(</code><code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code><code class="p">:</code>
    <code class="n">output</code> <code class="o">=</code> <code class="n">generate_image</code><code class="p">(</code><code class="n">models</code><code class="p">[</code><code class="s2">"</code><code class="s2">text2image</code><code class="s2">"</code><code class="p">]</code><code class="p">,</code> <code class="n">prompt</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO16-7" id="co_ai_integration_and_model_serving_CO16-7"><img alt="7" src="assets/7.png"/></a>
    <code class="k">return</code> <code class="n">Response</code><code class="p">(</code><code class="n">content</code><code class="o">=</code><code class="n">img_to_bytes</code><code class="p">(</code><code class="n">output</code><code class="p">)</code><code class="p">,</code> <code class="n">media_type</code><code class="o">=</code><code class="s2">"</code><code class="s2">image/png</code><code class="s2">"</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO16-1" id="callout_ai_integration_and_model_serving_CO16-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Initialize an empty mutable dictionary at the <em>global</em> application scope to hold one or multiple models.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO16-2" id="callout_ai_integration_and_model_serving_CO16-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p><a data-primary="async context manager" data-type="indexterm" id="id719"/>Use the <code>asynccontextmanager</code> decorator to handle startup and shutdown events as part of an async context manager:</p>

<ul>
<li>
<p>The context manager will run code before and after the <code>yield</code> keyword.</p>
</li>
<li>
<p>The <code>yield</code> keyword in the decorated <code>lifespan</code> function separates the startup and shutdown phases.</p>
</li>
<li>
<p>Code prior to the <code>yield</code> keyword runs at application startup before any requests are handled.</p>
</li>
<li>
<p>When you want to terminate the application, FastAPI will run the code after the <code>yield</code> keyword as part of the shutdown phase.</p>
</li>
</ul></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO16-3" id="callout_ai_integration_and_model_serving_CO16-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Preload the model on startup onto the <code>models</code> dictionary.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO16-4" id="callout_ai_integration_and_model_serving_CO16-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Start handling requests as the startup phase is now finished.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO16-5" id="callout_ai_integration_and_model_serving_CO16-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>Clear the model on application shutdown.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO16-6" id="callout_ai_integration_and_model_serving_CO16-6"><img alt="6" src="assets/6.png"/></a></dt>
<dd><p>Create the FastAPI server and pass it the lifespan function to use.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO16-7" id="callout_ai_integration_and_model_serving_CO16-7"><img alt="7" src="assets/7.png"/></a></dt>
<dd><p>Pass the global preloaded model instance to the generation function.</p></dd>
</dl></div>

<p>If you start the application now, you should immediately see model pipelines being loaded onto memory.
Before you applied these changes, the model pipelines used to load only when you made your first request.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p class="fix_tracking">You can preload more than one model into memory using the lifespan model-serving strategy, but this isn’t practical with large GenAI models.
Generative models can be resource hungry, and in most cases you’ll need GPUs to speed up the generation process.
The most powerful consumer GPUs ship with only 24 GB of VRAM. Some models require 18 GB of memory to perform inference, so try to deploy models on separate application instances and GPUs instead.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id720">
<h1>Startup and Shutdown Events</h1>
<p><a data-primary="shutdown event handler functions" data-type="indexterm" id="id721"/><a data-primary="startup event handler functions" data-type="indexterm" id="id722"/>Before the introduction of lifespan async context managers in FastAPI 0.93.0 for handling the application lifespan, separate startup and shutdown event handler functions were commonly used. <a data-type="xref" href="#startup_shutdown_events">Example 3-17</a> shows an example usage.</p>
<div data-type="example" id="startup_shutdown_events">
<h5><span class="label">Example 3-17. </span>Startup and shutdown events</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># main.py</code>
<code class="kn">from</code> <code class="nn">models</code> <code class="kn">import</code> <code class="n">load_image_model</code>

<code class="n">models</code> <code class="o">=</code> <code class="p">{}</code>
<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">()</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">on_event</code><code class="p">(</code><code class="s2">"startup"</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">startup_event</code><code class="p">():</code>
    <code class="n">models</code><code class="p">[</code><code class="s2">"text2image"</code><code class="p">]</code> <code class="o">=</code> <code class="n">load_image_model</code><code class="p">()</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">on_event</code><code class="p">(</code><code class="s2">"shutdown"</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">shutdown_event</code><code class="p">():</code>
    <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s2">"log.txt"</code><code class="p">,</code> <code class="n">mode</code><code class="o">=</code><code class="s2">"a"</code><code class="p">)</code> <code class="k">as</code> <code class="n">logfile</code><code class="p">:</code>
        <code class="n">logfile</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="s2">"Application shutdown"</code><code class="p">)</code></pre></div>

<p>A few resources across the web may use this alternative and legacy approach, so it is worth knowing.<a data-startref="ix_ch03-asciidoc42" data-type="indexterm" id="id723"/><a data-startref="ix_ch03-asciidoc41" data-type="indexterm" id="id724"/><a data-startref="ix_ch03-asciidoc40" data-type="indexterm" id="id725"/></p>
</div></aside>
</div></section>








<section data-pdf-bookmark="Be Lean: Serve Models Externally" data-type="sect2"><div class="sect2" id="id272">
<h2>Be Lean: Serve Models Externally</h2>

<p><a data-primary="serving GenAI models" data-secondary="strategies" data-tertiary="lean: serving models externally" data-type="indexterm" id="ix_ch03-asciidoc43"/>Another strategy to serve GenAI models is to package them as external services via other tools.
You can then use your FastAPI application as the logical layer between your client and the external model server.
In this logical layer, you can handle coordination between models, communication with APIs, management of users, security measures, monitoring activities, content filtering, enhancing prompts, or any other required logic.</p>










<section data-pdf-bookmark="Cloud providers" data-type="sect3"><div class="sect3" id="id57">
<h3>Cloud providers</h3>

<p><a data-primary="cloud providers, for serving GenAI models" data-type="indexterm" id="id726"/>Cloud providers are constantly innovating serverless and dedicated compute solutions that you can use to serve your models externally.
For instance, Azure Machine Learning Studio now provides a PromptFlow tool that you can use to deploy and customize OpenAI or open source language models.
Upon deployment, you will receive a model endpoint run on your Azure compute ready for usage.
However, there is a steep learning curve in using PromptFlow or similar tools as they may require particular dependencies and nontraditional steps to be followed.</p>
</div></section>










<section data-pdf-bookmark="BentoML" data-type="sect3"><div class="sect3" id="id58">
<h3>BentoML</h3>

<p><a data-primary="BentoML" data-type="indexterm" id="ix_ch03-asciidoc44"/>Another great contender for serving models external to FastAPI is BentoML.

<span class="keep-together">BentoML</span> is inspired by FastAPI but implements a different serving strategy, purpose built for AI models.</p>

<p>A huge improvement over FastAPI for handling concurrent model requests is 
<span class="keep-together">BentoML’s</span> ability to run different requests on different worker processes.
It can parallelize CPU-bound requests without you having to directly deal with Python multiprocessing.
On top of this, BentoML can also batch model inferences such that the generation process for multiple users can be done with a single model call.</p>

<p>I covered BentoML in detail in <a data-type="xref" href="ch02.html#ch02">Chapter 2</a>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>To run BentoML, you will need to install a few dependencies first:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install bentoml<code class="w"/></pre>
</div>

<p>You can see how to start a BentoML server in <a data-type="xref" href="#bentoml_usage">Example 3-18</a>.</p>
<div data-type="example" id="bentoml_usage">
<h5><span class="label">Example 3-18. </span>Serving an image model with BentoML</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># bento.py</code>
<code class="kn">import</code> <code class="nn">bentoml</code>
<code class="kn">from</code> <code class="nn">models</code> <code class="kn">import</code> <code class="n">load_image_model</code>

<code class="nd">@bentoml</code><code class="o">.</code><code class="n">service</code><code class="p">(</code>
    <code class="n">resources</code><code class="o">=</code><code class="p">{</code><code class="s2">"</code><code class="s2">cpu</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">4</code><code class="s2">"</code><code class="p">}</code><code class="p">,</code> <code class="n">traffic</code><code class="o">=</code><code class="p">{</code><code class="s2">"</code><code class="s2">timeout</code><code class="s2">"</code><code class="p">:</code> <code class="mi">120</code><code class="p">}</code><code class="p">,</code> <code class="n">http</code><code class="o">=</code><code class="p">{</code><code class="s2">"</code><code class="s2">port</code><code class="s2">"</code><code class="p">:</code> <code class="mi">5000</code><code class="p">}</code>
<code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO17-1" id="co_ai_integration_and_model_serving_CO17-1"><img alt="1" src="assets/1.png"/></a>
<code class="k">class</code> <code class="nc">Generate</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="kc">None</code><code class="p">:</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">pipe</code> <code class="o">=</code> <code class="n">load_image_model</code><code class="p">(</code><code class="p">)</code>

    <code class="nd">@bentoml</code><code class="o">.</code><code class="n">api</code><code class="p">(</code><code class="n">route</code><code class="o">=</code><code class="s2">"</code><code class="s2">/generate/image</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO17-2" id="co_ai_integration_and_model_serving_CO17-2"><img alt="2" src="assets/2.png"/></a>
    <code class="k">def</code> <code class="nf">generate</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code><code class="p">:</code>
        <code class="n">output</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">pipe</code><code class="p">(</code><code class="n">prompt</code><code class="p">,</code> <code class="n">num_inference_steps</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code><code class="o">.</code><code class="n">images</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
        <code class="k">return</code> <code class="n">output</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO17-1" id="callout_ai_integration_and_model_serving_CO17-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Declare a BentoML service with four allocated CPUs.
The service should time out in 120 seconds if the model doesn’t generate in time and should run from port <code>5000</code>.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO17-2" id="callout_ai_integration_and_model_serving_CO17-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Declare an API controller for undertaking the core model generation process.
This controller will hook to BentoML’s API route handler.</p></dd>
</dl></div>

<p>You can then run the BentoML service locally:</p>

<pre data-code-language="bash" data-type="programlisting">$ bentoml serve service:Generate<code class="w"/></pre>

<p>Your FastAPI server can now become a client with the model being served externally.
You can now make HTTP <code>POST</code> requests from within FastAPI to get a response, as shown in <a data-type="xref" href="#fastapi_bentoml_usage">Example 3-19</a>.</p>
<div data-type="example" id="fastapi_bentoml_usage">
<h5><span class="label">Example 3-19. </span>BentoML endpoints via FastAPI</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># main.py</code>

<code class="kn">import</code> <code class="nn">httpx</code>
<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code><code class="p">,</code> <code class="n">Response</code>

<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">(</code><code class="p">)</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code>
    <code class="s2">"</code><code class="s2">/generate/bentoml/image</code><code class="s2">"</code><code class="p">,</code>
    <code class="n">responses</code><code class="o">=</code><code class="p">{</code><code class="n">status</code><code class="o">.</code><code class="n">HTTP_200_OK</code><code class="p">:</code> <code class="p">{</code><code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"</code><code class="s2">image/png</code><code class="s2">"</code><code class="p">:</code> <code class="p">{</code><code class="p">}</code><code class="p">}</code><code class="p">}</code><code class="p">}</code><code class="p">,</code>
    <code class="n">response_class</code><code class="o">=</code><code class="n">Response</code><code class="p">,</code>
<code class="p">)</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">serve_bentoml_text_to_image_controller</code><code class="p">(</code><code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code><code class="p">:</code>
    <code class="k">async</code> <code class="k">with</code> <code class="n">httpx</code><code class="o">.</code><code class="n">AsyncClient</code><code class="p">(</code><code class="p">)</code> <code class="k">as</code> <code class="n">client</code><code class="p">:</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO18-1" id="co_ai_integration_and_model_serving_CO18-1"><img alt="1" src="assets/1.png"/></a>
        <code class="n">response</code> <code class="o">=</code> <code class="k">await</code> <code class="n">client</code><code class="o">.</code><code class="n">post</code><code class="p">(</code>
            <code class="s2">"</code><code class="s2">http://localhost:5000/generate</code><code class="s2">"</code><code class="p">,</code> <code class="n">json</code><code class="o">=</code><code class="p">{</code><code class="s2">"</code><code class="s2">prompt</code><code class="s2">"</code><code class="p">:</code> <code class="n">prompt</code><code class="p">}</code>
        <code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO18-2" id="co_ai_integration_and_model_serving_CO18-2"><img alt="2" src="assets/2.png"/></a>
    <code class="k">return</code> <code class="n">Response</code><code class="p">(</code><code class="n">content</code><code class="o">=</code><code class="n">response</code><code class="o">.</code><code class="n">content</code><code class="p">,</code> <code class="n">media_type</code><code class="o">=</code><code class="s2">"</code><code class="s2">image/png</code><code class="s2">"</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO18-1" id="callout_ai_integration_and_model_serving_CO18-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Create an asynchronous HTTP client using the <code>httpx</code> library.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO18-2" id="callout_ai_integration_and_model_serving_CO18-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Send a <code>POST</code> request to the BentoML image generation model endpoint.<a data-startref="ix_ch03-asciidoc44" data-type="indexterm" id="id727"/></p></dd>
</dl></div>
</div></section>










<section data-pdf-bookmark="Model providers" data-type="sect3"><div class="sect3" id="id59">
<h3>Model providers</h3>

<p><a data-primary="model providers" data-secondary="for serving GenAI models externally" data-type="indexterm" id="ix_ch03-asciidoc45"/><a data-primary="OpenAI, external model serving with" data-type="indexterm" id="ix_ch03-asciidoc46"/><a data-primary="serving GenAI models" data-secondary="model providers for" data-type="indexterm" id="ix_ch03-asciidoc47"/>Aside from BentoML and cloud providers, you can also use external model service providers such as OpenAI.
In this case, your FastAPI application becomes a service wrapper over OpenAI’s API.</p>

<p>Luckily, integrating with model provider APIs such as OpenAI is quite straightforward, as shown in <a data-type="xref" href="#openai_usage">Example 3-20</a>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>To run <a data-type="xref" href="#openai_usage">Example 3-20</a>, you must get an API key and set the <code>OPENAI_API_KEY</code> environment variable to this key, as recommended by OpenAI.</p>
</div>
<div data-type="example" id="openai_usage">
<h5><span class="label">Example 3-20. </span>Integrating with OpenAI service</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># main.py</code>

<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code>
<code class="kn">from</code> <code class="nn">openai</code> <code class="kn">import</code> <code class="n">OpenAI</code>

<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">(</code><code class="p">)</code>
<code class="n">openai_client</code> <code class="o">=</code> <code class="n">OpenAI</code><code class="p">(</code><code class="p">)</code>
<code class="n">system_prompt</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">You are a helpful assistant.</code><code class="s2">"</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"</code><code class="s2">/generate/openai/text</code><code class="s2">"</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">serve_openai_language_model_controller</code><code class="p">(</code><code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code> <code class="o">|</code> <code class="kc">None</code><code class="p">:</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">openai_client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO19-1" id="co_ai_integration_and_model_serving_CO19-1"><img alt="1" src="assets/1.png"/></a>
        <code class="n">model</code><code class="o">=</code><code class="s2">"</code><code class="s2">gpt-4o</code><code class="s2">"</code><code class="p">,</code>
        <code class="n">messages</code><code class="o">=</code><code class="p">[</code>
            <code class="p">{</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">system</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">system_prompt</code><code class="si">}</code><code class="s2">"</code><code class="p">}</code><code class="p">,</code>
            <code class="p">{</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">user</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="n">prompt</code><code class="p">}</code><code class="p">,</code>
        <code class="p">]</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">response</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="o">.</code><code class="n">content</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO19-1" id="callout_ai_integration_and_model_serving_CO19-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Use the <code>gpt-4o</code> model to chat with the model via the OpenAI API.</p></dd>
</dl></div>

<p>And now you should be able to get outputs via external calls to the OpenAI service.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id728">
<h1>LangChain</h1>
<p><a data-primary="LangChain" data-type="indexterm" id="id729"/>You can use the <code>langchain</code> library to switch integration with any LLM providers.
The library also provides excellent tools working with LLMs, which we will cover later in the book.  First, install the library:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install langchain langchain-openai<code class="w"/></pre>

<p>Once <code>langchain</code> is installed, follow <a data-type="xref" href="#langchain_usage">Example 3-21</a> to integrate with external model APIs such as OpenAI.</p>
<div data-type="example" id="langchain_usage">
<h5><span class="label">Example 3-21. </span>Integrating with external provider APIs with LangChain</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># main.py</code>

<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code>
<code class="kn">from</code> <code class="nn">langchain</code><code class="nn">.</code><code class="nn">chains</code><code class="nn">.</code><code class="nn">llm</code> <code class="kn">import</code> <code class="n">LLMChain</code>
<code class="kn">from</code> <code class="nn">langchain_core</code><code class="nn">.</code><code class="nn">prompts</code> <code class="kn">import</code> <code class="n">PromptTemplate</code>
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">OpenAI</code>

<code class="n">llm</code> <code class="o">=</code> <code class="n">OpenAI</code><code class="p">(</code><code class="n">openai_organization</code><code class="o">=</code><code class="s2">"</code><em><code class="s2">YOUR_ORGANIZATION_ID</code></em><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO20-1" id="co_ai_integration_and_model_serving_CO20-1"><img alt="1" src="assets/1.png"/></a>
<code class="n">template</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">Your name is FastAPI bot and you are a helpful</code>
<code class="s2">chatbot responsible for teaching FastAPI to your users.</code>
<code class="s2">Here is the user query: </code><code class="si">{query}</code>
<code class="s2">"""</code>
<code class="n">prompt</code> <code class="o">=</code> <code class="n">PromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="n">template</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO20-2" id="co_ai_integration_and_model_serving_CO20-2"><img alt="2" src="assets/2.png"/></a>
<code class="n">llm_chain</code> <code class="o">=</code> <code class="n">LLMChain</code><code class="p">(</code><code class="n">prompt</code><code class="o">=</code><code class="n">prompt</code><code class="p">,</code> <code class="n">llm</code><code class="o">=</code><code class="n">llm</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO20-3" id="co_ai_integration_and_model_serving_CO20-3"><img alt="3" src="assets/3.png"/></a>

<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">(</code><code class="p">)</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"</code><code class="s2">/generate/text</code><code class="s2">"</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">generate_text_controller</code><code class="p">(</code><code class="n">query</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code><code class="p">:</code>
    <code class="k">return</code> <code class="n">llm_chain</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">query</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO20-4" id="co_ai_integration_and_model_serving_CO20-4"><img alt="4" src="assets/4.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO20-1" id="callout_ai_integration_and_model_serving_CO20-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Create the OpenAI client with the organization ID.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO20-2" id="callout_ai_integration_and_model_serving_CO20-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Construct a prompt template.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO20-3" id="callout_ai_integration_and_model_serving_CO20-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Construct the OpenAI <code>llm_chain</code> object from the prompt template.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO20-4" id="callout_ai_integration_and_model_serving_CO20-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Run the text generation process by passing in the user query.</p></dd>
</dl></div>
</div></aside>

<p class="less_space pagebreak-before">When using external services, be mindful that data will be shared with third-party service providers.
In this case, you may prefer self-hosted solutions if you value data privacy and security.
With self-hosting, the trade-off will be an increased complexity in deploying and managing your own model servers.<a data-startref="ix_ch03-asciidoc47" data-type="indexterm" id="id730"/><a data-startref="ix_ch03-asciidoc46" data-type="indexterm" id="id731"/><a data-startref="ix_ch03-asciidoc45" data-type="indexterm" id="id732"/></p>

<p>If you really want to avoid serving large models yourself, cloud providers can provide managed solutions where your data is never shared with third parties.
An example is Azure OpenAI, which at the time of writing provides snapshots of OpenAI’s best LLMs and image generator<a data-startref="ix_ch03-asciidoc43" data-type="indexterm" id="id733"/>.<a data-startref="ix_ch03-asciidoc37" data-type="indexterm" id="id734"/></p>

<p>You now have a few options for model serving.
One final system to implement before we wrap up this chapter is logging and monitoring of the service.</p>
</div></section>
</div></section>
</div></section>






<section data-pdf-bookmark="The Role of Middleware in Service Monitoring" data-type="sect1"><div class="sect1" id="middleware_monitoring">
<h1>The Role of Middleware in Service Monitoring</h1>

<p><a data-primary="middleware" data-secondary="for service monitoring" data-type="indexterm" id="ix_ch03-asciidoc48"/><a data-primary="monitoring, middleware role in" data-type="indexterm" id="ix_ch03-asciidoc49"/><a data-primary="service monitoring, middleware role in" data-type="indexterm" id="ix_ch03-asciidoc50"/><a data-primary="serving GenAI models" data-secondary="middleware role in service monitoring" data-type="indexterm" id="ix_ch03-asciidoc51"/>You can implement a simple monitoring tool where prompts and responses can be logged alongside their request and response token usage.
To implement the logging system, you can write a few logging functions inside your model-serving controller.
However, if you have multiple models and endpoints, you may benefit from leveraging the FastAPI middleware mechanism.</p>

<p>Middleware is an essential block of code that runs before and after a request is processed by any of your controllers.
You can define custom middleware that you then attach to any API route handlers.
Once the requests reach the route handlers, the middleware acts as an intermediary, processing the requests and responses between the client and server controller.</p>

<p>Excellent uses cases for middleware include logging and monitoring, rate limiting, content filtering, and cross-origin resource sharing (CORS) implementations.</p>

<p><a data-type="xref" href="#middleware_monitoring_example">Example 3-22</a> shows how you can monitor your model-serving handlers.</p>
<div data-type="warning" epub:type="warning"><h1>Usage logging via custom middleware in production</h1>
<p><a data-primary="loggers/logging" data-secondary="usage logging via custom middleware in production" data-type="indexterm" id="id735"/>Don’t use <a data-type="xref" href="#middleware_monitoring_example">Example 3-22</a> in production as the monitoring logs can disappear if you run the application from a Docker container or a host machine that can be deleted or restarted without a mounted persistent volume or logging to a database.</p>

<p>In <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>, you will integrate the monitoring system with a database to persist logs outside the application environment.</p>
</div>
<div class="less_space pagebreak-before" data-type="example" id="middleware_monitoring_example">
<h5><span class="label">Example 3-22. </span>Using middleware mechanisms to capture service usage logs</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># main.py</code>

<code class="kn">import</code> <code class="nn">csv</code>
<code class="kn">import</code> <code class="nn">time</code>
<code class="kn">from</code> <code class="nn">datetime</code> <code class="kn">import</code> <code class="n">datetime</code><code class="p">,</code> <code class="n">timezone</code>
<code class="kn">from</code> <code class="nn">uuid</code> <code class="kn">import</code> <code class="n">uuid4</code>
<code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Awaitable</code><code class="p">,</code> <code class="n">Callable</code>
<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code><code class="p">,</code> <code class="n">Request</code><code class="p">,</code> <code class="n">Response</code>

<code class="c1"># preload model with a lifespan</code>
<code class="o">.</code><code class="o">.</code><code class="o">.</code>

<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">(</code><code class="n">lifespan</code><code class="o">=</code><code class="n">lifespan</code><code class="p">)</code>

<code class="n">csv_header</code> <code class="o">=</code> <code class="p">[</code>
    <code class="s2">"</code><code class="s2">Request ID</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">Datetime</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">Endpoint Triggered</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">Client IP Address</code><code class="s2">"</code><code class="p">,</code>
    <code class="s2">"</code><code class="s2">Response Time</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">Status Code</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">Successful</code><code class="s2">"</code>
<code class="p">]</code>


<code class="nd">@app</code><code class="o">.</code><code class="n">middleware</code><code class="p">(</code><code class="s2">"</code><code class="s2">http</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO21-1" id="co_ai_integration_and_model_serving_CO21-1"><img alt="1" src="assets/1.png"/></a>
<code class="k">async</code> <code class="k">def</code> <code class="nf">monitor_service</code><code class="p">(</code>
    <code class="n">req</code><code class="p">:</code> <code class="n">Request</code><code class="p">,</code> <code class="n">call_next</code><code class="p">:</code> <code class="n">Callable</code><code class="p">[</code><code class="p">[</code><code class="n">Request</code><code class="p">]</code><code class="p">,</code> <code class="n">Awaitable</code><code class="p">[</code><code class="n">Response</code><code class="p">]</code><code class="p">]</code>
<code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">Response</code><code class="p">:</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO21-2" id="co_ai_integration_and_model_serving_CO21-2"><img alt="2" src="assets/2.png"/></a>
    <code class="n">request_id</code> <code class="o">=</code> <code class="n">uuid4</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">hex</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO21-3" id="co_ai_integration_and_model_serving_CO21-3"><img alt="3" src="assets/3.png"/></a>
    <code class="n">request_datetime</code> <code class="o">=</code> <code class="n">datetime</code><code class="o">.</code><code class="n">now</code><code class="p">(</code><code class="n">timezone</code><code class="o">.</code><code class="n">utc</code><code class="p">)</code><code class="o">.</code><code class="n">isoformat</code><code class="p">(</code><code class="p">)</code>
    <code class="n">start_time</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">perf_counter</code><code class="p">(</code><code class="p">)</code>
    <code class="n">response</code><code class="p">:</code> <code class="n">Response</code> <code class="o">=</code> <code class="k">await</code> <code class="n">call_next</code><code class="p">(</code><code class="n">req</code><code class="p">)</code>
    <code class="n">response_time</code> <code class="o">=</code> <code class="nb">round</code><code class="p">(</code><code class="n">time</code><code class="o">.</code><code class="n">perf_counter</code><code class="p">(</code><code class="p">)</code> <code class="o">-</code> <code class="n">start_time</code><code class="p">,</code> <code class="mi">4</code><code class="p">)</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO21-4" id="co_ai_integration_and_model_serving_CO21-4"><img alt="4" src="assets/4.png"/></a>
    <code class="n">response</code><code class="o">.</code><code class="n">headers</code><code class="p">[</code><code class="s2">"</code><code class="s2">X-Response-Time</code><code class="s2">"</code><code class="p">]</code> <code class="o">=</code> <code class="nb">str</code><code class="p">(</code><code class="n">response_time</code><code class="p">)</code>
    <code class="n">response</code><code class="o">.</code><code class="n">headers</code><code class="p">[</code><code class="s2">"</code><code class="s2">X-API-Request-ID</code><code class="s2">"</code><code class="p">]</code> <code class="o">=</code> <code class="n">request_id</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO21-5" id="co_ai_integration_and_model_serving_CO21-5"><img alt="5" src="assets/5.png"/></a>
    <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">usage.csv</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">a</code><code class="s2">"</code><code class="p">,</code> <code class="n">newline</code><code class="o">=</code><code class="s2">"</code><code class="s2">"</code><code class="p">)</code> <code class="k">as</code> <code class="n">file</code><code class="p">:</code>
        <code class="n">writer</code> <code class="o">=</code> <code class="n">csv</code><code class="o">.</code><code class="n">writer</code><code class="p">(</code><code class="n">file</code><code class="p">)</code>
        <code class="k">if</code> <code class="n">file</code><code class="o">.</code><code class="n">tell</code><code class="p">(</code><code class="p">)</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
            <code class="n">writer</code><code class="o">.</code><code class="n">writerow</code><code class="p">(</code><code class="n">csv_header</code><code class="p">)</code>
        <code class="n">writer</code><code class="o">.</code><code class="n">writerow</code><code class="p">(</code> <a class="co" href="#callout_ai_integration_and_model_serving_CO21-6" id="co_ai_integration_and_model_serving_CO21-6"><img alt="6" src="assets/6.png"/></a>
            <code class="p">[</code>
                <code class="n">request_id</code><code class="p">,</code>
                <code class="n">request_datetime</code><code class="p">,</code>
                <code class="n">req</code><code class="o">.</code><code class="n">url</code><code class="p">,</code>
                <code class="n">req</code><code class="o">.</code><code class="n">client</code><code class="o">.</code><code class="n">host</code><code class="p">,</code>
                <code class="n">response_time</code><code class="p">,</code>
                <code class="n">response</code><code class="o">.</code><code class="n">status_code</code><code class="p">,</code>
                <code class="n">response</code><code class="o">.</code><code class="n">status_code</code> <code class="o">&lt;</code> <code class="mi">400</code><code class="p">,</code>
            <code class="p">]</code>
        <code class="p">)</code>
    <code class="k">return</code> <code class="n">response</code>


<code class="c1"># Usage Log Example</code>

<code class="sd">""""
Request ID: 3d15d3d9b7124cc9be7eb690fc4c9bd5
Datetime: 2024-03-07T16:41:58.895091
Endpoint triggered: http://localhost:8000/generate/text
Client IP Address: 127.0.0.1
Processing time: 26.7210 seconds
Status Code: 200
Successful: True
"""</code>

<code class="c1"># model-serving handlers</code>
<code class="o">.</code><code class="o">.</code><code class="o">.</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO21-1" id="callout_ai_integration_and_model_serving_CO21-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Declare a function decorated by the FastAPI HTTP middleware mechanism.
The function must receive the <code>Request</code> object and <code>call_next</code> callback function to be considered valid <code>http</code> middleware.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO21-2" id="callout_ai_integration_and_model_serving_CO21-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Pass the request to the route handler to process the response.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO21-3" id="callout_ai_integration_and_model_serving_CO21-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Generate a request ID for tracking all incoming requests even if an error is raised in <code>call_next</code> during request processing.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO21-4" id="callout_ai_integration_and_model_serving_CO21-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Calculate the response duration to four decimal places.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO21-5" id="callout_ai_integration_and_model_serving_CO21-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>Set custom response headers for the processing time and request ID.</p></dd>
<dt><a class="co" href="#co_ai_integration_and_model_serving_CO21-6" id="callout_ai_integration_and_model_serving_CO21-6"><img alt="6" src="assets/6.png"/></a></dt>
<dd><p>Log the URL of the endpoint triggered, request datetime and ID, client IP address, response processing time, and status code into a CSV file on disk in <code>append</code> mode.</p></dd>
</dl></div>

<p>In this section, you captured information about endpoint usage including processing time, status code, endpoint path, and client IP.</p>

<p>Middleware is a powerful system for executing blocks of code before requests are passed to the route handlers and before responses are sent to the user.
You saw an example of how middleware can be used to log model usage for any model-serving endpoint.<a data-startref="ix_ch03-asciidoc51" data-type="indexterm" id="id736"/><a data-startref="ix_ch03-asciidoc50" data-type="indexterm" id="id737"/><a data-startref="ix_ch03-asciidoc49" data-type="indexterm" id="id738"/><a data-startref="ix_ch03-asciidoc48" data-type="indexterm" id="id739"/></p>
<div data-type="warning" epub:type="warning"><h1>Accessing request and response bodies in middleware</h1>
<p>If you need to track interactions with your models, including prompts and the content they generate, using middleware for logging is more efficient than adding individual loggers to each handler.
However, you should take into account data privacy and performance concerns when logging request and response bodies as the user could submit sensitive or large data to your service, which will require careful handling.</p>
</div>
</div></section>






<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id440">
<h1>Summary</h1>

<p>We covered a lot of concepts in this chapter, so let’s quickly review everything we’ve discussed.</p>

<p>You saw how you can download, integrate, and serve a variety of open source GenAI models from the Hugging Face repository in a simple UI using the Streamlit package, within a few lines of code.
You also reviewed several types of models and how to serve them via FastAPI endpoints.
The models you experimented with were text, image, audio, video, and 3D-based, and you saw how they process data.
You also learned the model architectures and the underlying mechanisms powering these models.</p>

<p>Then, you reviewed several different model-serving strategies including model swapping on request, model preloading, and finally model serving outside the FastAPI application using other frameworks such as BentoML or using third-party APIs.</p>

<p>Next, you noticed that the larger models could take some time to generate responses.
Finally, you implemented a service monitoring mechanism for your models that leverage the FastAPI middleware system for every model-serving endpoint.
You then wrote the logs to disk for future analysis.</p>

<p>You should now feel more confident building your own GenAI services powered by a variety of open source models.</p>

<p>In the next chapter, you will learn more about type safety and its role in eliminating application bugs and reducing uncertainty when working with external APIs and services.
You will also see how to validate requests and response schemas to make your services even more reliable.</p>
</div></section>






<section class="less_space pagebreak-before" data-pdf-bookmark="Additional References" data-type="sect1"><div class="sect1" id="id441">
<h1>Additional References</h1>

<ul>
<li>
<p><a href="https://oreil.ly/HKT8O">“Bark”</a>, in “Transformers” documentation, <em>Hugging Face</em>, accessed on 26 March 2024.</p>
</li>
<li>
<p>Borsos, Z., et al. (2022).
<a href="https://oreil.ly/8YZBr">“AudioLM: A Language Modeling Approach to Audio Generation”</a>.
arXiv preprint arXiv:2209.03143.</p>
</li>
<li>
<p>Brooks, T., et al. (2024).
<a href="https://oreil.ly/52duF">“Video Generation Models as World Simulators”</a>.
OpenAI.</p>
</li>
<li>
<p>Défossez, A., et al. (2022).
<a href="https://oreil.ly/p4_-5">“High-Fidelity Neural Audio Compression”</a>.
arXiv preprint arXiv:2210.13438.</p>
</li>
<li>
<p>Jun, H. &amp; Nichol, A. (2023).
<a href="https://oreil.ly/LzLy0">“Shap-E: Generating Conditional 3D Implicit Functions”</a>.
arXiv preprint arXiv:2305.02463.</p>
</li>
<li>
<p>Kim, B.-K., et al. (2023).
<a href="https://oreil.ly/uErOQ">“BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion”</a>.
arXiv preprint arXiv:2305.15798.</p>
</li>
<li>
<p>Liu, Y., et al. (2024).
<a href="https://oreil.ly/Zr6bJ">“Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models”</a>.
arXiv preprint arXiv:2402.17177.</p>
</li>
<li>
<p>Mildenhall, B., et al. (2020).
<a href="https://oreil.ly/hBiBV">“NeRF:
Representing Scenes as Neural Radiance Fields for View Synthesis”</a>.
arXiv preprint arXiv:2003.08934.</p>
</li>
<li>
<p>Nichol, A., et al. (2022).
<a href="https://oreil.ly/FW-wT">“Point-E: A System for Generating 3D Point Clouds from Complex Prompts”</a>.
arXiv preprint arXiv:2212.08751.</p>
</li>
<li>
<p>Vaswani, A., et al. (2017).
<a href="https://oreil.ly/N4MkH">“Attention Is All You Need”</a>. arXiv preprint arXiv:1706.03762.</p>
</li>
<li>
<p>Wang, C., et al. (2023).
<a href="https://oreil.ly/h1D0e">“Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers”</a>.
arXiv preprint arXiv:2301.02111.</p>
</li>
<li>
<p>Zhang, P., et al. (2024). <a href="https://oreil.ly/Idi1B">“TinyLlama: An Open-Source Small Language Model”</a>. arXiv preprint arXiv:2401.02385.</p>
</li>
</ul>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id630"><sup><a href="ch03.html#id630-marker">1</a></sup> Hugging Face provides access to a wide range of pretrained machine learning models, datasets, and  <span class="keep-together">applications.</span></p><p data-type="footnote" id="id631"><sup><a href="ch03.html#id631-marker">2</a></sup> A. Vaswani et al. (2017), <a href="https://oreil.ly/sO33r">“Attention Is All You Need”</a>, arXiv preprint arXiv:1706.03762.</p><p data-type="footnote" id="id635"><sup><a href="ch03.html#id635-marker">3</a></sup> A great tool for visualizing attention maps is <a href="https://oreil.ly/e2Q7X">BertViz</a>.</p><p data-type="footnote" id="id637"><sup><a href="ch03.html#id637-marker">4</a></sup> You can find the up-to-date list of open source LLMs on the <a href="https://oreil.ly/GZaEr">Open LLM GitHub repository</a>.</p><p data-type="footnote" id="id647"><sup><a href="ch03.html#id647-marker">5</a></sup> An embedding model or an embedding layer such as in a transformer</p><p data-type="footnote" id="id658"><sup><a href="ch03.html#id658-marker">6</a></sup> This sequential token generation process can also limit scalability for long sequences, as each token relies on the previous one.</p><p data-type="footnote" id="id665"><sup><a href="ch03.html#id665-marker">7</a></sup> The <a href="https://huggingface.co">Hugging Face model repository</a> is a resource for AI developers to publish and share their pretrained models.</p><p data-type="footnote" id="id666"><sup><a href="ch03.html#id666-marker">8</a></sup> See the <a href="https://pytorch.org">Pytorch documentation</a> for installation instructions.</p><p data-type="footnote" id="id667"><sup><a href="ch03.html#id667-marker">9</a></sup> <code>float16</code> tensor precision is more memory efficient in memory constraint environments. The computations can be faster but precision is lower compared to <code>float32</code> tensor types. See the <a href="https://oreil.ly/rsmoB">TinyLlama model card</a> for more information.</p><p data-type="footnote" id="id668"><sup><a href="ch03.html#id668-marker">10</a></sup> As we saw in <a data-type="xref" href="ch02.html#ch02">Chapter 2</a>, controllers are functions that handle an API route’s incoming requests and return responses to the client via a logical execution of services or providers.</p><p data-type="footnote" id="id674"><sup><a href="ch03.html#id674-marker">11</a></sup> Streamlit collects usage statistics by default, but you can turn this off using a <a href="https://oreil.ly/m_Jix">configuration file</a>.</p><p data-type="footnote" id="id686"><sup><a href="ch03.html#id686-marker">12</a></sup> The latent space of a trained model when visualized may look like white noise but will contain structured representations that the model has learned to encode and decode.</p><p data-type="footnote" id="id697"><sup><a href="ch03.html#id697-marker">13</a></sup> <em>Multiplexing</em> is the process of combining multiple streams (such as audio, video, and subtitles) into a single file or stream in a synchronized manner.</p><p data-type="footnote" id="id698"><sup><a href="ch03.html#id698-marker">14</a></sup> The <code>python-multipart</code> library is used for parsing <code>multipart/form-data</code>, which is commonly used encoding in file upload form submissions.</p></div></div></section></body></html>