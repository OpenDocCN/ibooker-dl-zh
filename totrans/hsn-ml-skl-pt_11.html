<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 9. Introduction to Artificial Neural Networks"><div class="chapter" id="ann_chapter">
<h1><span class="label">Chapter 9. </span>Introduction to Artificial Neural Networks</h1>


<p>Birds<a data-type="indexterm" data-primary="artificial neural networks (ANNs)" id="xi_artificialneuralnetworksANNs936_1"/> inspired us to fly, burdock plants inspired Velcro, and nature has inspired countless more inventions. It seems only logical, then, to look at the brain‚Äôs architecture for inspiration on how to build an intelligent machine. This is the logic that sparked <em>artificial neural networks</em> (ANNs), machine learning models inspired by the networks of biological neurons found in our brains. However, although planes were inspired by birds, they don‚Äôt have to flap their wings to fly. Similarly, ANNs have gradually become quite different from their biological cousins. Some researchers even argue that we should drop the biological analogy altogether (e.g., by saying ‚Äúunits‚Äù rather than ‚Äúneurons‚Äù), lest we restrict our creativity to biologically plausible systems.‚Å†<sup><a data-type="noteref" id="id2093-marker" href="ch09.html#id2093">1</a></sup></p>

<p>ANNs are at the very core of deep learning. They are versatile, powerful, and scalable, making them ideal to tackle large and highly complex machine learning tasks such as classifying billions of images (e.g., Google Images), powering speech recognition services (e.g., Apple‚Äôs Siri or Google Assistant) and chatbots (e.g., ChatGPT or Claude), recommending the best videos to watch to hundreds of millions of users every day (e.g., YouTube), or learning how proteins fold (DeepMind‚Äôs AlphaFold).</p>

<p>This chapter introduces artificial neural networks, starting with a quick tour of the very first ANN architectures and leading up to multilayer perceptrons (MLPs), which are heavily used today (many other architectures will be explored in the following chapters). In this chapter, we will implement simple MLPs using Scikit-Learn to get our feet wet, and in the next chapter we will switch to PyTorch, as it is a much more flexible and efficient library for neural nets.</p>

<p>Now let‚Äôs go back in time to the origins of artificial neural networks.</p>






<section data-type="sect1" data-pdf-bookmark="From Biological to Artificial Neurons"><div class="sect1" id="id146">
<h1>From Biological to Artificial Neurons</h1>

<p>Surprisingly<a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="evolution of" id="xi_artificialneuralnetworksANNsevolutionof91213_1"/>, ANNs have been around for quite a while: they were first introduced back in 1943 by the neurophysiologist Warren McCulloch and the mathematician Walter Pitts. In their <a href="https://homl.info/43">landmark paper</a>,‚Å†<sup><a data-type="noteref" id="id2094-marker" href="ch09.html#id2094">2</a></sup> ‚ÄúA Logical Calculus of Ideas Immanent in Nervous Activity‚Äù, McCulloch and Pitts presented a simplified computational model of how biological neurons might work together in animal brains to perform complex computations using <em>propositional logic</em>.<a data-type="indexterm" data-primary="propositional logic" id="id2095"/> This was the first artificial neural network architecture. Since then many other architectures have been invented, as you will see.</p>

<p>The early successes of ANNs led to the widespread belief that we would soon be conversing with truly intelligent machines. When it became clear in the 1960s that this promise would go unfulfilled (at least for quite a while), funding flew elsewhere, and ANNs entered a long winter. In the early 1980s, new architectures were invented and better training techniques were developed, sparking a revival of interest in <em>connectionism</em>,<a data-type="indexterm" data-primary="connectionism" id="id2096"/> the study of neural networks. But progress was slow, and by the 1990s other powerful machine learning techniques had been invented, such as support vector machines. These techniques seemed to offer better results and stronger theoretical foundations than ANNs, so once again the study of neural networks was put on hold.</p>

<p>We are now witnessing yet another wave of interest in ANNs. Will this wave die out like the previous ones did? Well, here are a few good reasons to believe that this time is different and that the renewed interest in ANNs will have a much more profound impact on our lives:</p>

<ul>
<li>
<p>There is now a huge quantity of data available to train neural networks, and ANNs frequently outperform other ML techniques on very large and complex problems.</p>
</li>
<li>
<p>The tremendous increase in computing power since the 1990s now makes it possible to train large neural networks in a reasonable amount of time. This is in part due to Moore‚Äôs law (the number of components in integrated circuits has doubled about every 2 years over the last 50 years), but also thanks to the gaming industry, which has stimulated the production of powerful <em>graphical processing units</em> (GPUs) by the millions: GPU cards were initially designed to accelerate graphics, but it turns out that neural networks perform similar computations (such as large matrix multiplications), so they can also be accelerated using GPUs. Moreover, cloud platforms have made this power accessible to everyone.</p>
</li>
<li>
<p>The training algorithms have been improved. To be fair they are only slightly different from the ones used in the 1990s, but these relatively small tweaks have had a huge positive impact.</p>
</li>
<li>
<p>Some theoretical limitations of ANNs have turned out to be benign in practice. For example, many people thought that ANN training algorithms were doomed because they were likely to get stuck in local optima, but it turns out that this is not a big problem in practice, especially for larger neural networks: the local optima often perform almost as well as the global optimum.</p>
</li>
<li>
<p>The invention of the Transformer architecture<a data-type="indexterm" data-primary="Transformer architecture" id="id2097"/> in 2017 (see <a data-type="xref" href="ch15.html#transformer_chapter">Chapter¬†15</a>) has been a game changer: it can process and generate all sorts of data (e.g., text, images, audio) unlike earlier, more specialized, architectures, and it performs great across a wide variety of tasks from robotics to protein folding. Moreover, it scales rather well, which has made it possible to train very large <em>foundation models</em><a data-type="indexterm" data-primary="foundation models" id="id2098"/> that can be reused across many different tasks, possibly with a bit of fine-tuning (that‚Äôs transfer learning), or just by prompting the model in the right way (that‚Äôs <em>in-context learning</em>, or ICL).<a data-type="indexterm" data-primary="ICL (in-context learning)" id="id2099"/><a data-type="indexterm" data-primary="in-context learning (ICL)" id="id2100"/> For instance, you can give it a few examples of the task at hand (that‚Äôs <em>few-shot learning</em>, or FSL),<a data-type="indexterm" data-primary="few-shot learning (FSL)" id="id2101"/><a data-type="indexterm" data-primary="FSL (few-shot learning)" id="id2102"/> or ask it to reason step-by-step (that‚Äôs <em>chain-of-thought</em> prompting, or CoT).<a data-type="indexterm" data-primary="chain-of-thought prompting (CoT)" id="id2103"/><a data-type="indexterm" data-primary="CoT (chain-of-thought prompting)" id="id2104"/> It‚Äôs a new world!</p>
</li>
<li>
<p>ANNs seem to have entered a virtuous circle of funding and progress. Amazing products based on ANNs regularly make the headline news, which pulls more and more attention and funding toward them, resulting in more and more progress and even more amazing products. AI is no longer just powering products in the shadows: since chatbots such as ChatGPT were released, the general public is now directly interacting daily with AI assistants, and the big tech companies are competing fiercely to grab this gigantic market: the pace of innovation is wild.<a data-type="indexterm" data-startref="xi_artificialneuralnetworksANNsevolutionof91213_1" id="id2105"/></p>
</li>
</ul>








<section data-type="sect2" data-pdf-bookmark="Biological Neurons"><div class="sect2" id="id147">
<h2>Biological Neurons</h2>

<p>Before<a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="biological neurons as background to" id="xi_artificialneuralnetworksANNsbiologicalneuronsasbackgroundto9267_1"/><a data-type="indexterm" data-primary="biological neural networks (BNNs)" id="xi_biologicalneuralnetworksBNNs9267_1"/><a data-type="indexterm" data-primary="BNNs (biological neural networks)" id="xi_BNNsbiologicalneuralnetworks9267_1"/> we discuss artificial neurons, let‚Äôs take a quick look at a biological neuron (represented in <a data-type="xref" href="#biological_neuron_wikipedia">Figure¬†9-1</a>). It is an unusual-looking cell mostly found in animal brains. It‚Äôs composed of a <em>cell body</em> containing the nucleus and most of the cell‚Äôs complex components, many branching extensions called <em>dendrites</em>, plus one very long extension called the <em>axon</em>. The axon‚Äôs length may be just a few times longer than the cell body, or up to tens of thousands of times longer. Near its extremity the axon splits off into many branches called <em>telodendria</em>, and at the tip of these branches are minuscule structures called <em>synaptic terminals</em> (or simply <em>synapses</em>), which are connected to the dendrites or cell bodies of other neurons.‚Å†<sup><a data-type="noteref" id="id2106-marker" href="ch09.html#id2106">3</a></sup> Biological neurons produce short electrical impulses called <em>action potentials</em> (APs, or just <em>signals</em>),<a data-type="indexterm" data-primary="action potentials (APs)" id="id2107"/><a data-type="indexterm" data-primary="APs (action potentials)" id="id2108"/><a data-type="indexterm" data-primary="signals" id="id2109"/> which travel along the axons and make the synapses release chemical signals called <em>neurotransmitters</em>. When a neuron receives a sufficient amount of these neurotransmitters within a few milliseconds, it fires its own electrical impulses (actually, it depends on the neurotransmitters, as some of them inhibit the neuron from firing).</p>

<figure class="smallerseventy"><div id="biological_neuron_wikipedia" class="figure">
<img src="assets/hmls_0901.png" alt="Illustration of a biological neuron highlighting key components such as the cell body, dendrites, axon, telodendria, and synaptic terminals, demonstrating the neuron's structure and connection points within neural networks." width="2501" height="1614"/>
<h6><span class="label">Figure 9-1. </span>A biological neuron‚Å†<sup><a data-type="noteref" id="id2110-marker" href="ch09.html#id2110">4</a></sup></h6>
</div></figure>

<p>Thus, individual biological neurons seem to behave in a simple way, but they‚Äôre organized in a vast network of billions, with each neuron typically connected to thousands of other neurons. Highly complex computations can be performed by a network of fairly simple neurons, much like a complex anthill can emerge from the combined efforts of simple ants. The architecture of biological neural networks (BNNs)‚Å†<sup><a data-type="noteref" id="id2111-marker" href="ch09.html#id2111">5</a></sup> is the subject of active research, but some parts of the brain have been mapped. These efforts show that neurons are often organized in consecutive layers, especially in the cerebral cortex (the outer layer of the brain), as shown in <a data-type="xref" href="#biological_neural_network_wikipedia">Figure¬†9-2</a>.<a data-type="indexterm" data-startref="xi_artificialneuralnetworksANNsbiologicalneuronsasbackgroundto9267_1" id="id2112"/><a data-type="indexterm" data-startref="xi_biologicalneuralnetworksBNNs9267_1" id="id2113"/><a data-type="indexterm" data-startref="xi_BNNsbiologicalneuralnetworks9267_1" id="id2114"/></p>

<figure><div id="biological_neural_network_wikipedia" class="figure">
<img src="assets/hmls_0902.png" alt="Illustration of layered neuron networks in the human cerebral cortex, emphasizing the complex organization of biological neural networks." width="2322" height="744"/>
<h6><span class="label">Figure 9-2. </span>Multiple layers in a biological neural network (human cortex)‚Å†<sup><a data-type="noteref" id="id2115-marker" href="ch09.html#id2115">6</a></sup></h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Logical Computations with Neurons"><div class="sect2" id="id148">
<h2>Logical Computations with Neurons</h2>

<p>McCulloch<a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="logical computations with neurons" id="xi_artificialneuralnetworksANNslogicalcomputationswithneurons94010_1"/> and Pitts proposed a very simple model of the biological neuron, which later became known as an <em>artificial neuron</em>:<a data-type="indexterm" data-primary="artificial neuron" id="id2116"/> it has one or more binary (on/off) inputs and one binary output. The artificial neuron activates its output when more than a certain number of its inputs are active. In their paper, McCulloch and Pitts showed that even with such a simplified model it is possible to build a network of artificial neurons that can compute any logical proposition you want. To see how such a network works, let‚Äôs build a few ANNs that perform various logical computations (see <a data-type="xref" href="#nn_propositional_logic_diagram">Figure¬†9-3</a>), assuming that a neuron is activated when at least two of its input connections are active.</p>

<figure class="width-85"><div id="nn_propositional_logic_diagram" class="figure">
<img src="assets/hmls_0903.png" alt="Diagram showing artificial neural networks performing logical computations, including AND, OR, and NOT operations with neurons labeled A, B, and C." width="1207" height="398"/>
<h6><span class="label">Figure 9-3. </span>ANNs performing simple logical computations</h6>
</div></figure>

<p>Let‚Äôs see what these networks do:</p>

<ul>
<li>
<p>The first network on the left is the identity function: if neuron A is activated, then neuron C gets activated as well (since it receives two input signals from neuron A); but if neuron A is off, then neuron C is off as well.</p>
</li>
<li>
<p>The second network performs a logical AND: neuron C is activated only when both neurons A and B are activated (a single input signal is not enough to activate neuron C).</p>
</li>
<li>
<p>The third network performs a logical OR: neuron C gets activated if either neuron A or neuron B is activated (or both).</p>
</li>
<li>
<p>Finally, if we suppose that an input connection can inhibit the neuron‚Äôs activity (which is the case with biological neurons), then the fourth network computes a slightly more complex logical proposition: neuron C is activated only if neuron A is active and neuron B is off. If neuron A is active all the time, then you get a logical NOT: neuron C is active when neuron B is off, and vice versa.</p>
</li>
</ul>

<p>You can imagine how these networks can be combined to compute complex logical expressions (see the exercises at the end of the chapter for an example).<a data-type="indexterm" data-startref="xi_artificialneuralnetworksANNslogicalcomputationswithneurons94010_1" id="id2117"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="The Perceptron"><div class="sect2" id="theperceptron">
<h2>The Perceptron</h2>

<p>The <em>perceptron</em><a data-type="indexterm" data-primary="perceptrons" id="xi_perceptrons95817_1"/><a data-type="indexterm" data-primary="linear models" data-secondary="perceptrons" id="xi_perceptrons95817_2"/> is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt. It is based on a slightly different artificial neuron (see <a data-type="xref" href="#artificial_neuron_diagram">Figure¬†9-4</a>) called a <em>threshold logic unit</em> (TLU), or sometimes a <em>linear threshold unit</em> (LTU).<a data-type="indexterm" data-primary="linear threshold units (LTUs)" id="id2118"/><a data-type="indexterm" data-primary="LTUs (linear threshold units)" id="id2119"/><a data-type="indexterm" data-primary="threshold logic units (TLUs)" id="id2120"/><a data-type="indexterm" data-primary="TLUs (threshold logic units)" id="id2121"/> The inputs and output are numbers (instead of binary on/off values), and each input connection is associated with a weight. The TLU first computes a linear function of its inputs: <em>z</em> = <em>w</em><sub>1</sub> <em>x</em><sub>1</sub> + <em>w</em><sub>2</sub> <em>x</em><sub>2</sub> + ‚ãØ + <em>w</em><sub><em>n</em></sub> <em>x</em><sub><em>n</em></sub> + <em>b</em> = <strong>w</strong><sup>‚ä∫</sup> <strong>x</strong> + <em>b</em>. Then it applies a <em>step function</em> to the result: <em>h</em><sub><strong>w</strong></sub>(<strong>x</strong>) = step(<em>z</em>). So it‚Äôs almost like logistic regression, except it uses a step function instead of the logistic function.‚Å†<sup><a data-type="noteref" id="id2122-marker" href="ch09.html#id2122">7</a></sup> Just like in logistic regression, the model parameters are the input weights <strong>w</strong> and the bias term <em>b</em>.</p>

<figure class="smallerfiftyfive"><div id="artificial_neuron_diagram" class="figure">
<img src="assets/hmls_0904.png" alt="Diagram of a Threshold Logic Unit (TLU) showing how inputs multiplied by weights are summed with a bias, and a step function is applied to determine the output." width="650" height="485"/>
<h6><span class="label">Figure 9-4. </span>TLU: an artificial neuron that computes a weighted sum of its inputs <strong>w</strong><sup>‚ä∫</sup> <strong>x</strong>, plus a bias term <em>b</em>, then applies a step function</h6>
</div></figure>

<p>The most common step function used in perceptrons is the <em>Heaviside step function</em><a data-type="indexterm" data-primary="Heaviside step function" id="id2123"/><a data-type="indexterm" data-primary="step functions, TLU" id="id2124"/> (see <a data-type="xref" href="#step_functions_equation">Equation 9-1</a>). Sometimes the sign function is used instead.</p>
<div data-type="equation" id="step_functions_equation">
<h5><span class="label">Equation 9-1. </span>Common step functions used in perceptrons (assuming threshold = 0)</h5>
<math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mo form="prefix">heaviside</mo>
          <mrow>
            <mo>(</mo>
            <mi>z</mi>
            <mo>)</mo>
          </mrow>
          <mo>=</mo>
          <mfenced separators="" open="{" close="">
            <mtable>
              <mtr>
                <mtd columnalign="left">
                  <mn>0</mn>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mtext>if</mtext>
                    <mspace width="4.pt"/>
                    <mi>z</mi>
                    <mo>&lt;</mo>
                    <mn>0</mn>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd columnalign="left">
                  <mn>1</mn>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mtext>if</mtext>
                    <mspace width="4.pt"/>
                    <mi>z</mi>
                    <mo>‚â•</mo>
                    <mn>0</mn>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
          </mfenced>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="1.em"/>
          <mspace width="1.em"/>
          <mo form="prefix">sgn</mo>
          <mrow>
            <mo>(</mo>
            <mi>z</mi>
            <mo>)</mo>
          </mrow>
          <mo>=</mo>
          <mfenced separators="" open="{" close="">
            <mtable>
              <mtr>
                <mtd columnalign="left">
                  <mrow>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mtext>if</mtext>
                    <mspace width="4.pt"/>
                    <mi>z</mi>
                    <mo>&lt;</mo>
                    <mn>0</mn>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd columnalign="left">
                  <mn>0</mn>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mtext>if</mtext>
                    <mspace width="4.pt"/>
                    <mi>z</mi>
                    <mo>=</mo>
                    <mn>0</mn>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd columnalign="left">
                  <mrow>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mtext>if</mtext>
                    <mspace width="4.pt"/>
                    <mi>z</mi>
                    <mo>&gt;</mo>
                    <mn>0</mn>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>A single TLU can be used for simple linear binary classification. It computes a linear function of its inputs, and if the result exceeds a threshold, it outputs the positive class. Otherwise, it outputs the negative class. This may remind you of logistic regression (<a data-type="xref" href="ch04.html#linear_models_chapter">Chapter¬†4</a>) or linear SVM classification (see the online chapter on SVMs at <a href="https://homl.info" class="bare"><em class="hyperlink">https://homl.info</em></a>). You could, for example, use a single TLU to classify iris flowers based on petal length and width. Training such a TLU would require finding the right values for <em>w</em><sub>1</sub>, <em>w</em><sub>2</sub>, and <em>b</em> (the training algorithm is discussed shortly).</p>

<p>A perceptron is composed of one or more TLUs organized in a single layer, where every TLU is connected to every input. Such a layer is called a <em>fully connected layer</em>, or a <em>dense layer</em>.<a data-type="indexterm" data-primary="dense layers" id="id2125"/><a data-type="indexterm" data-primary="fully connected layers" id="id2126"/> The inputs constitute the <em>input layer</em>.<a data-type="indexterm" data-primary="input layer, neural network" data-seealso="hidden layers" id="id2127"/> And since the layer of TLUs produces the final outputs, it is called the <em>output layer</em>. For example, a perceptron with two inputs and three outputs is represented in <a data-type="xref" href="#perceptron_diagram">Figure¬†9-5</a>.</p>

<figure class="smallersixty"><div id="perceptron_diagram" class="figure">
<img src="assets/hmls_0905.png" alt="Diagram of a perceptron architecture with two input neurons connected to three output neurons in a fully connected layer, illustrating TLUs in the output layer." width="743" height="593"/>
<h6><span class="label">Figure 9-5. </span>Architecture of a perceptron with two inputs and three output neurons</h6>
</div></figure>

<p>This perceptron can classify instances simultaneously into three different binary classes, which makes it a multilabel classifier. It may also be used for multiclass classification.</p>

<p>Thanks to the magic of linear algebra, <a data-type="xref" href="#neural_network_layer_equation">Equation 9-2</a> can be used to efficiently compute the outputs of a layer of artificial neurons for several instances at once.</p>
<div id="neural_network_layer_equation" data-type="equation" class="fifty-percent">
<h5><span class="label">Equation 9-2. </span>Computing the outputs of a fully connected layer</h5>
<math alttext="ModifyingAbove bold upper Y With caret equals phi left-parenthesis bold upper X bold upper W plus bold b right-parenthesis" display="block">
  <mrow>
    <mover accent="true"><mi>ùêò</mi> <mo>^</mo></mover>
    <mo>=</mo>
    <mi>œÜ</mi>
    <mrow>
      <mo>(</mo>
      <mi>ùêóùêñ</mi>
      <mo>+</mo>
      <mi>ùêõ</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><math alttext="ModifyingAbove bold upper Y With caret">
  <mover accent="true"><mi>ùêò</mi> <mo>^</mo></mover>
</math> is the output matrix. It has one row per instance and one column per neuron.</p>
</li>
<li>
<p><strong>X</strong> is the input matrix. It has one row per instance and one column per input feature.</p>
</li>
<li>
<p>The weight matrix <strong>W</strong> contains all the connection weights. It has one row per input feature and one column per neuron.‚Å†<sup><a data-type="noteref" id="id2128-marker" href="ch09.html#id2128">8</a></sup></p>
</li>
<li>
<p>The bias vector <strong>b</strong> contains all the bias terms: one per neuron.</p>
</li>
<li>
<p>The function <em>œï</em> is called the <em>activation function</em>:<a data-type="indexterm" data-primary="activation functions" id="id2129"/> when the artificial neurons are TLUs, it is a step function (we will discuss other activation functions shortly).</p>
</li>
</ul>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In mathematics, the sum of a matrix and a vector is undefined. However, in data science, we allow ‚Äúbroadcasting‚Äù: adding a vector to a matrix means adding it to every row in the matrix. So, 
<span class="keep-together"><strong>XW</strong> + <strong>b</strong></span> first multiplies <strong>X</strong> by <strong>W</strong>‚Äîwhich results in a matrix with one row per instance and one column per output‚Äîthen adds the vector <strong>b</strong> to every row of that matrix, which adds each bias term to the corresponding output, for every instance. Moreover, <em>œï</em> is then applied itemwise to each item in the resulting matrix.</p>
</div>

<p>So, how is a perceptron trained?<a data-type="indexterm" data-primary="training models" data-secondary="perceptrons" id="xi_trainingmodelsperceptrons922033_1"/> The perceptron training algorithm proposed by Rosenblatt was largely inspired by <em>Hebb‚Äôs rule</em>.<a data-type="indexterm" data-primary="Hebb‚Äôs rule" id="id2130"/> In his 1949 book, <em>The Organization of Behavior</em> (Wiley), Donald Hebb suggested that when a biological neuron triggers another neuron often, the connection between these two neurons grows stronger. Siegrid L√∂wel later summarized Hebb‚Äôs idea in the catchy phrase, ‚ÄúCells that fire together, wire together‚Äù; that is, the connection weight between two neurons tends to increase when they fire simultaneously. This rule later became known as Hebb‚Äôs rule (or <em>Hebbian learning</em>)<a data-type="indexterm" data-primary="Hebbian learning" id="id2131"/>. Perceptrons are trained using a variant of this rule that takes into account the error made by the network when it makes a prediction; the <span class="keep-together">perceptron</span> learning rule reinforces connections that help reduce the error. More specifically, the perceptron is fed one training instance at a time, and for each instance it makes its predictions. For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction. The rule is shown in <a data-type="xref" href="#perceptron_update_rule">Equation 9-3</a>.</p>
<div id="perceptron_update_rule" data-type="equation">
<h5><span class="label">Equation 9-3. </span>Perceptron learning rule (weight update)</h5>
<math alttext="w Subscript i comma j Baseline Superscript left-parenthesis next step right-parenthesis Baseline equals w Subscript i comma j Baseline plus eta left-parenthesis y Subscript j Baseline minus ModifyingAbove y With caret Subscript j Baseline right-parenthesis x Subscript i" display="block">
  <mrow>
    <msup><mrow><msub><mi>w</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi></mrow> </msub></mrow> <mrow><mo>(</mo><mtext>next</mtext><mspace width="0.222222em"/><mtext>step</mtext><mo>)</mo></mrow> </msup>
    <mo>=</mo>
    <msub><mi>w</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi></mrow> </msub>
    <mo>+</mo>
    <mi>Œ∑</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>y</mi> <mi>j</mi> </msub>
      <mo>-</mo>
      <msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>j</mi> </msub>
      <mo>)</mo>
    </mrow>
    <msub><mi>x</mi> <mi>i</mi> </msub>
  </mrow>
</math>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><em>w</em><sub><em>i</em>,</sub> <sub><em>j</em></sub> is the connection weight between the <em>i</em><sup>th</sup> input and the <em>j</em><sup>th</sup> <span class="keep-together">neuron</span>.</p>
</li>
<li>
<p><em>x</em><sub><em>i</em></sub> is the <em>i</em><sup>th</sup> input value of the current training instance.</p>
</li>
<li>
<p><math alttext="ModifyingAbove y With caret Subscript j">
  <msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>j</mi> </msub>
</math> is the output of the <em>j</em><sup>th</sup> output neuron for the current training instance.</p>
</li>
<li>
<p><em>y</em><sub><em>j</em></sub> is the target output of the <em>j</em><sup>th</sup> output neuron for the current training instance.</p>
</li>
<li>
<p><em>Œ∑</em> is the learning rate (see <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter¬†4</a>).</p>
</li>
</ul>

<p>The decision boundary of each output neuron is linear, so perceptrons are incapable of learning complex patterns (just like logistic regression classifiers). However, if the training instances are linearly separable, Rosenblatt demonstrated that this algorithm will converge to a solution.‚Å†<sup><a data-type="noteref" id="id2132-marker" href="ch09.html#id2132">9</a></sup> This is called the <em>perceptron convergence theorem</em>.</p>

<p>Scikit-Learn provides a <code translate="no">Perceptron</code> class<a data-type="indexterm" data-primary="sklearn" data-secondary="linear_model.Perceptron" id="id2133"/> that can be used pretty much as you would expect‚Äîfor example, on the iris dataset (introduced in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter¬†4</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_iris</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">Perceptron</code>

<code class="n">iris</code> <code class="o">=</code> <code class="n">load_iris</code><code class="p">(</code><code class="n">as_frame</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code><code class="p">[[</code><code class="s2">"petal length (cm)"</code><code class="p">,</code> <code class="s2">"petal width (cm)"</code><code class="p">]]</code><code class="o">.</code><code class="n">values</code>
<code class="n">y</code> <code class="o">=</code> <code class="p">(</code><code class="n">iris</code><code class="o">.</code><code class="n">target</code> <code class="o">==</code> <code class="mi">0</code><code class="p">)</code>  <code class="c1"># Iris setosa</code>

<code class="n">per_clf</code> <code class="o">=</code> <code class="n">Perceptron</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">per_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>

<code class="n">X_new</code> <code class="o">=</code> <code class="p">[[</code><code class="mi">2</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">],</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">1</code><code class="p">]]</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">per_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>  <code class="c1"># predicts True and False for these 2 flowers</code></pre>

<p>You may have noticed that the perceptron learning algorithm strongly resembles stochastic gradient descent<a data-type="indexterm" data-primary="stochastic gradient descent (SGD)" data-secondary="and perceptron learning algorithm" data-secondary-sortas="perceptron" id="id2134"/> (introduced in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter¬†4</a>). In fact, Scikit-Learn‚Äôs <code translate="no">Perceptron</code> class is equivalent to using an <code translate="no">SGDClassifier</code><a data-type="indexterm" data-primary="SGDClassifier" data-secondary="Perceptron class compared to" id="id2135"/> with the following hyperparameters: <code translate="no">loss="perceptron"</code>, <code translate="no">learning_rate="constant"</code>, <code translate="no">eta0=1</code> (the learning rate), and <code translate="no">penalty=None</code> (no <span class="keep-together">regularization</span>).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Contrary to logistic regression classifiers, perceptrons do not output a class probability. This is one reason to prefer logistic regression over perceptrons. Moreover, perceptrons do not use any regularization by default, and training stops as soon as there are no more prediction errors on the training set, so the model typically does not generalize as well as logistic regression or a linear SVM classifier. However, perceptrons may train a bit faster.<a data-type="indexterm" data-startref="xi_trainingmodelsperceptrons922033_1" id="id2136"/></p>
</div>

<p>In their 1969 monograph, <em>Perceptrons</em>, Marvin Minsky and Seymour Papert highlighted a number of serious weaknesses of perceptrons‚Äîin particular, the fact that they are incapable of solving some trivial problems (e.g., the <em>exclusive OR</em> (XOR) classification problem<a data-type="indexterm" data-primary="XOR (exclusive or) problem" id="id2137"/>;<a data-type="indexterm" data-primary="exclusive or (XOR) problem" id="id2138"/> see the left side of <a data-type="xref" href="#xor_diagram">Figure¬†9-6</a>). This is true of any other linear classification model (such as logistic regression classifiers), but researchers had expected much more from perceptrons, and some were so disappointed that they dropped neural networks altogether in favor of more formal approaches such as logic, problem solving, and search. The lack of practical applications also didn‚Äôt help.</p>

<p>It turns out that some of the limitations of perceptrons can be eliminated by stacking multiple perceptrons. The resulting ANN is called a <em>multilayer perceptron</em> (MLP).<a data-type="indexterm" data-primary="multilayer perceptrons (MLPs)" id="xi_multilayerperceptronsMLPs9266170_1"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="The Multilayer Perceptron and Backpropagation"><div class="sect2" id="id150">
<h2>The Multilayer Perceptron and Backpropagation</h2>

<p>An<a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="backpropagation" id="xi_artificialneuralnetworksANNsbackpropagation92693_1"/><a data-type="indexterm" data-primary="backpropagation" id="xi_backpropagation92693_1"/><a data-type="indexterm" data-primary="multilayer perceptrons (MLPs)" data-secondary="and backpropagation" data-secondary-sortas="backpropagation" id="xi_multilayerperceptronsMLPsandbackpropagation92693_1"/> MLP can solve the XOR problem, as you can verify by computing the output of the MLP represented on the righthand side of <a data-type="xref" href="#xor_diagram">Figure¬†9-6</a>: with inputs (0, 0) or (1, 1), the network outputs 0, and with inputs (0, 1) or (1, 0) it outputs 1. Try verifying that this network indeed solves the XOR problem!<sup><a data-type="noteref" id="id2139-marker" href="ch09.html#id2139">10</a></sup></p>

<p>An MLP is composed of one input layer, one or more layers of artificial neurons (originally TLUs)<a data-type="indexterm" data-primary="threshold logic units (TLUs)" id="id2140"/><a data-type="indexterm" data-primary="TLUs (threshold logic units)" id="id2141"/> called <em>hidden layers</em>, and one final layer of artificial neurons called the <em>output layer</em> (see <a data-type="xref" href="#mlp_diagram">Figure¬†9-7</a>). The layers close to the input layer are usually called the <em>lower layers</em>, and the ones close to the outputs are usually called the <em>upper layers</em>.</p>

<figure class="smallersixty"><div id="xor_diagram" class="figure">
<img src="assets/hmls_0906.png" alt="Diagram illustrating the XOR classification problem and an MLP using threshold logic units to solve it." width="1050" height="785"/>
<h6><span class="label">Figure 9-6. </span>XOR classification problem and an MLP that solves it</h6>
</div></figure>

<figure class="smallersixty"><div id="mlp_diagram" class="figure">
<img src="assets/hmls_0907.png" alt="Diagram of a multilayer perceptron showing a feedforward neural network with two input neurons, a hidden layer of four neurons, and three output neurons." width="797" height="743"/>
<h6><span class="label">Figure 9-7. </span>Architecture of a multilayer perceptron with two inputs, one hidden layer of four neurons, and three output neurons</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The signal flows only in one direction (from the inputs to the outputs), so this architecture is an example of a <em>feedforward neural network</em> (FNN).<a data-type="indexterm" data-primary="feedforward neural network (FNN)" id="id2142"/><a data-type="indexterm" data-primary="FNN (feedforward neural network)" id="id2143"/></p>
</div>

<p>When<a data-type="indexterm" data-startref="xi_perceptrons95817_1" id="id2144"/><a data-type="indexterm" data-startref="xi_perceptrons95817_2" id="id2145"/> an ANN contains a deep stack of hidden layers,‚Å†<sup><a data-type="noteref" id="id2146-marker" href="ch09.html#id2146">11</a></sup> it is called a <em>deep neural network</em> (DNN).<a data-type="indexterm" data-primary="deep neural networks (DNNs)" id="id2147"/> The field of deep learning studies DNNs, and more generally it is interested in models containing deep stacks of computations. Even so, many people talk about deep learning whenever neural networks are involved (even shallow ones).</p>

<p>For many years researchers struggled to find a way to train MLPs, without success. In the early 1960s several researchers discussed the possibility of using gradient descent to train neural networks, but as we saw in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter¬†4</a>, this requires computing the gradients of the model‚Äôs error with regard to the model parameters; it wasn‚Äôt clear at the time how to do this efficiently with such a complex model containing so many parameters, especially with the computers they had back then.</p>

<p>Then, in 1970, a researcher named Seppo Linnainmaa introduced in his master‚Äôs thesis a technique to compute all the gradients automatically and efficiently. This algorithm is now called <em>reverse-mode automatic differentiation</em> (or <em>reverse-mode autodiff</em> for short).<a data-type="indexterm" data-primary="autodiff (automatic differentiation)" id="xi_autodiffautomaticdifferentiation9291267_1"/><a data-type="indexterm" data-primary="reverse-mode autodiff" id="xi_reversemodeautodiff9291267_1"/> In just two passes through the network (one forward, one backward),<a data-type="indexterm" data-primary="backward pass" id="id2148"/><a data-type="indexterm" data-primary="forward pass, in backpropagation" id="id2149"/> it is able to compute the gradients of the neural network‚Äôs error with regard to every single model parameter. In other words, it can find out how each connection weight and each bias should be tweaked in order to reduce the neural network‚Äôs error. These gradients can then be used to perform a gradient descent step. If you repeat this process of computing the gradients automatically and taking a gradient descent step, the neural network‚Äôs error will gradually drop until it eventually reaches a minimum. This combination of reverse-mode autodiff and gradient descent is now called <em>backpropagation</em> (or <em>backprop</em> for short).</p>

<p>Here‚Äôs an analogy: imagine you are learning to shoot a basketball into the hoop. You throw the ball (that‚Äôs the forward pass), and you observe that it went far off to the right side (that‚Äôs the error computation), then you consider how you can change your body position to throw the ball a bit less to the right next time (that‚Äôs the backward pass): you realize that your arm will need to rotate a bit counterclockwise, and probably your whole upper body as well, which in turn means that your feet should turn too (notice how we‚Äôre going down the ‚Äúlayers‚Äù). Once you‚Äôve thought it through, you actually move your body: that‚Äôs the gradient descent step. The smaller the errors, the smaller the adjustments. As you repeat the whole process many times, the error gradually gets smaller, and after a few hours of practice, you manage to get the ball through the hoop every time. Good job!</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>There are various autodiff techniques, with different pros and cons. Reverse-mode autodiff is well suited when the function to differentiate has many variables (e.g., connection weights and biases) and few outputs (e.g., one loss). If you want to learn more about autodiff, check out <a data-type="xref" href="app01.html#autodiff_appendix">Appendix¬†A</a>.</p>
</div>

<p>Backpropagation<a data-type="indexterm" data-startref="xi_autodiffautomaticdifferentiation9291267_1" id="id2150"/><a data-type="indexterm" data-startref="xi_reversemodeautodiff9291267_1" id="id2151"/> can actually be applied to all sorts of computational graphs, not just neural networks: indeed, Linnainmaa‚Äôs master‚Äôs thesis was not about neural nets at all, it was more general. It was several more years before backprop started to be used to train neural networks, but it still wasn‚Äôt mainstream. Then, in 1985, David Rumelhart, Geoffrey Hinton, and Ronald Williams published a <a href="https://homl.info/44">paper</a>‚Å†<sup><a data-type="noteref" id="id2152-marker" href="ch09.html#id2152">12</a></sup> analyzing how backpropagation allows neural networks to learn useful internal representations. Their results were so impressive that backpropagation was quickly popularized in the field. Over 40 years later, it is still by far the most popular training technique for neural networks.</p>

<p>Let‚Äôs run through how backpropagation works again in a bit more detail:</p>

<ul>
<li>
<p>It handles one mini-batch at a time, and goes through the full training set multiple times. If each mini-batch contains 32 instances, and each instance has 100 features, then the mini-batch will be represented as a matrix with 32 rows and 100 columns. Each pass through the training set is called an <em>epoch</em>.<a data-type="indexterm" data-primary="epochs" id="id2153"/></p>
</li>
<li>
<p>For each mini-batch, the algorithm computes the output of all the neurons in the first hidden layer using <a data-type="xref" href="#neural_network_layer_equation">Equation 9-2</a>. If the layer has 50 neurons, then its output is a matrix with one row per sample in the mini-batch (e.g., 32), and 50 columns (i.e., one per neuron). This matrix is then passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the output layer. This is the <em>forward pass</em>:<a data-type="indexterm" data-primary="forward pass, in backpropagation" id="id2154"/> it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.</p>
</li>
<li>
<p>Next, the algorithm measures the network‚Äôs output error<a data-type="indexterm" data-primary="output error, in backpropagation" id="id2155"/> (i.e., it uses a loss function that compares the desired output and the actual output of the network, and returns some measure of the error).</p>
</li>
<li>
<p>Then it computes how much each output layer parameter contributed to the error. This is done analytically by applying the <em>chain rule</em><a data-type="indexterm" data-primary="chain rule" id="id2156"/> (one of the most fundamental rules in calculus), which makes this step fast and precise. The result is one gradient per parameter.</p>
</li>
<li>
<p>The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule, working backward until it reaches the input layer. As explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights and biases in the network by propagating the error gradient backward through the network (hence the name of the algorithm).</p>
</li>
<li>
<p>Finally, the algorithm performs a gradient descent step to tweak all the connection weights and bias terms in the network, using the error gradients it just computed.</p>
</li>
</ul>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>It is important to initialize all the hidden layers‚Äô connection weights randomly, or else training will fail. For example, if you initialize all weights and biases to zero, then all neurons in a given layer will be perfectly identical, and thus backpropagation will affect them in exactly the same way, so they will remain identical. In other words, despite having hundreds of neurons per layer, your model will act as if it had only one neuron per layer: it won‚Äôt be too smart. If instead you randomly initialize the weights, you <em>break the symmetry</em> and allow backpropagation to train a diverse team of neurons.</p>
</div>

<p>In short, backpropagation makes predictions<a data-type="indexterm" data-primary="predictions" data-secondary="backpropagation" id="id2157"/> for a mini-batch (forward pass), measures the error, then goes through each layer in reverse to measure the error contribution from each parameter (reverse pass), and finally tweaks the connection weights and biases to reduce the error (gradient descent step).</p>

<p>In<a data-type="indexterm" data-primary="activation functions" id="xi_activationfunctions93123_1"/> order for backprop to work properly, Rumelhart and his colleagues made a key change to the MLP‚Äôs architecture: they replaced the step function with the logistic function, <em>œÉ</em>(<em>z</em>) = 1 / (1 + exp(‚Äì<em>z</em>)), also called the <em>sigmoid</em> function.<a data-type="indexterm" data-primary="sigmoid activation function" data-secondary="in backpropagation" data-secondary-sortas="backpropagation" id="id2158"/> This was essential because the step function contains only flat segments, so there is no gradient to work with (gradient descent cannot move on a flat surface), while the sigmoid function has a well-defined nonzero derivative everywhere, allowing gradient descent to make some progress at every step. In fact, the backpropagation algorithm works well with many other activation functions, not just the sigmoid function. Here are two other popular choices:</p>
<dl>
<dt>The <em>hyperbolic tangent</em> function: <a data-type="indexterm" data-primary="activation functions" data-secondary="hyperbolic tangent (htan)" id="id2159"/><a data-type="indexterm" data-primary="hyperbolic tangent (htan)" id="id2160"/>tanh(<em>z</em>) = 2<em>œÉ</em>(2<em>z</em>) ‚Äì 1</dt>
<dd>
<p>Just like the sigmoid function, this activation function is <em>S</em>-shaped, continuous, and differentiable, but its output value ranges from ‚Äì1 to 1 (instead of 0 to 1 in the case of the sigmoid function). That range tends to make each layer‚Äôs output more or less centered around 0 at the beginning of training, which often helps speed up convergence.</p>
</dd>
<dt>The rectified linear unit function: <a data-type="indexterm" data-primary="ReLU (rectified linear units)" data-secondary="and backpropagation" data-secondary-sortas="back" id="id2161"/>ReLU(<em>z</em>) = max(0, <em>z</em>)</dt>
<dd>
<p>The ReLU function is continuous but unfortunately not differentiable at <em>z</em> = 0 (the slope changes abruptly, which can make gradient descent bounce around), and its derivative is 0 for <em>z</em> &lt; 0. In practice, however, it works very well and has the advantage of being fast to compute, so it has become the default for most architectures (except the Transformer architecture, as we will see in <a data-type="xref" href="ch15.html#transformer_chapter">Chapter¬†15</a>).‚Å†<sup><a data-type="noteref" id="id2162-marker" href="ch09.html#id2162">13</a></sup> Importantly, the fact that it does not have a maximum output value helps reduce some issues during gradient descent (we will come back to this in <a data-type="xref" href="ch11.html#deep_chapter">Chapter¬†11</a>).</p>
</dd>
</dl>

<p>These popular activation functions and their derivatives are represented in <a data-type="xref" href="#activation_functions_plot">Figure¬†9-8</a>. But wait! Why do we need activation functions in the first place? Well, if you chain several linear transformations, all you get is a linear transformation. For example, if f(<em>x</em>) = 2<em>x</em> + 3 and g(<em>x</em>) = 5<em>x</em> ‚Äì 1, then chaining these two linear functions gives you another linear function: f(g(<em>x</em>)) = 2(5<em>x</em> ‚Äì 1) + 3 = 10<em>x</em> + 1. So if you don‚Äôt have some nonlinearity between layers, then even a deep stack of layers is equivalent to a single layer, and you can‚Äôt solve very complex problems with that. Conversely, a large enough DNN with nonlinear activations can theoretically approximate any continuous function.</p>

<figure><div id="activation_functions_plot" class="figure">
<img src="assets/hmls_0908.png" alt="Diagram illustrating four activation functions‚ÄîHeaviside, ReLU, Sigmoid, and Tanh‚Äîalongside their respective derivatives, highlighting nonlinearity essential for deep neural networks." width="1975" height="765"/>
<h6><span class="label">Figure 9-8. </span>Activation functions (left) and their derivatives (right)</h6>
</div></figure>

<p>OK!<a data-type="indexterm" data-startref="xi_activationfunctions93123_1" id="id2163"/> You know where neural nets came from, what the MLP architecture looks like, and how it computes its outputs. You‚Äôve also learned about the backpropagation algorithm. It‚Äôs time to see MLPs in action!<a data-type="indexterm" data-startref="xi_artificialneuralnetworksANNsbackpropagation92693_1" id="id2164"/><a data-type="indexterm" data-startref="xi_backpropagation92693_1" id="id2165"/><a data-type="indexterm" data-startref="xi_multilayerperceptronsMLPsandbackpropagation92693_1" id="id2166"/></p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Building and Training MLPs with Scikit-Learn"><div class="sect1" id="id402">
<h1>Building and Training MLPs with Scikit-Learn</h1>

<p>MLPs<a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="building and training" id="xi_artificialneuralnetworksANNsbuildingandtraining93265_1"/><a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="building and training MLPs with" id="xi_ScikitLearnbuildingandtrainingMLPswith93265_1"/> can tackle a wide range of tasks, but the most common are regression and classification. Scikit-Learn can help with both of these. Let‚Äôs start with regression.</p>








<section data-type="sect2" data-pdf-bookmark="Regression MLPs"><div class="sect2" id="id151">
<h2>Regression MLPs</h2>

<p>How<a data-type="indexterm" data-primary="multilayer perceptrons (MLPs)" data-secondary="regression MLPs" id="xi_multilayerperceptronsMLPsregressionMLPs93294_1"/><a data-type="indexterm" data-primary="regression models" data-secondary="regression MLPs" id="xi_regressionmodelsregressionMLPs93294_1"/> would you build an MLP for a regression task? Well, if you want to predict a single value (e.g., the price of a house, given many of its features), then you just need a single output neuron: its output is the predicted value. For multivariate regression (i.e., to predict multiple values at once), you need one output neuron per output dimension. For example, to locate the center of an object in an image, you need to predict 2D coordinates, so you need two output neurons. If you also want to place a bounding box around the object, then you need two more numbers: the width and the height of the object. So, you end up with four output neurons.</p>

<p>Scikit-Learn includes an <code translate="no">MLPRegressor</code> class<a data-type="indexterm" data-primary="MLPRegressor" id="xi_MLPRegressor933146_1"/><a data-type="indexterm" data-primary="sklearn" data-secondary="neural_network.MLPRegressor" id="xi_ScikitLearnsklearnneural_networkMLPRegressor933146_1"/>, so let‚Äôs use it to build an MLP with three hidden layers composed of 50 neurons each, and train it on the California housing dataset. For simplicity, we will use Scikit-Learn‚Äôs <code translate="no">fetch_california_housing()</code> function to load the data. This dataset is simpler than the one we used in <a data-type="xref" href="ch02.html#project_chapter">Chapter¬†2</a>, since it contains only numerical features (there is no <code translate="no">ocean_proximity</code> feature), and there are no missing values. The targets are also scaled down: each unit represents $100,000. Let‚Äôs start by importing everything we will need:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">fetch_california_housing</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">root_mean_squared_error</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>
<code class="kn">from</code> <code class="nn">sklearn.neural_network</code> <code class="kn">import</code> <code class="n">MLPRegressor</code>
<code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">make_pipeline</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code></pre>

<p>Next, let‚Äôs fetch the California housing dataset and split it into a training set and a test set:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">housing</code> <code class="o">=</code> <code class="n">fetch_california_housing</code><code class="p">()</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">housing</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">housing</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code></pre>

<p>Now let‚Äôs create an <code translate="no">MLPRegressor</code> model with 3 hidden layers composed of 50 neurons each. The first hidden layer‚Äôs input size (i.e., the number of rows in its weights matrix) and the output layer‚Äôs output size (i.e., the number of columns in its weights matrix) will adjust automatically to the dimensionality of the inputs and targets, respectively, when training starts. The model uses the ReLU activation function in all hidden layers, and no activation function at all on the output layer. We also set <code translate="no">verbose=True</code> to get details on the model‚Äôs progress during training:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">mlp_reg</code> <code class="o">=</code> <code class="n">MLPRegressor</code><code class="p">(</code><code class="n">hidden_layer_sizes</code><code class="o">=</code><code class="p">[</code><code class="mi">50</code><code class="p">,</code> <code class="mi">50</code><code class="p">,</code> <code class="mi">50</code><code class="p">],</code> <code class="n">early_stopping</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                       <code class="n">verbose</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code></pre>

<p>Since neural nets can have a <em>lot</em> of parameters, they have a tendency to overfit the training set. To reduce this risk, one option is to use early stopping (introduced in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter¬†4</a>): when we set <code translate="no">early_stopping=True</code>, the <code translate="no">MLPRegressor</code> class automatically sets aside 10% of the training data and uses it to evaluate the model at each epoch (you can adjust the validation set‚Äôs size by setting <code translate="no">validation_fraction</code>). If the validation score stops improving for 10 epochs, training automatically stops (you can tweak this number of epochs by setting <code translate="no">n_iter_no_change</code>).</p>

<p>Now let‚Äôs create a pipeline to standardize the input features before sending them to the <code translate="no">MLPRegressor</code>. This is very important because gradient descent does not converge very well when the features have very different scales, as we saw in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter¬†4</a>. We can then train the model! The <code translate="no">MLPRegressor</code> class uses a variant of gradient descent called <em>Adam</em> (see <a data-type="xref" href="ch11.html#deep_chapter">Chapter¬†11</a>) to minimize the mean squared error. It also uses a tiny bit of ‚Ñì<sub>2</sub> regularization (you can control its strength via the <code translate="no">alpha</code> hyperparameter, which defaults to 0.0001):</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">pipeline</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">StandardScaler</code><code class="p">(),</code> <code class="n">mlp_reg</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">pipeline</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code><code class="w"/>
<code class="go">Iteration 1, loss = 0.85190332</code>
<code class="go">Validation score: 0.534299</code>
<code class="go">Iteration 2, loss = 0.28288639</code>
<code class="go">Validation score: 0.651094</code>
<code class="go">[...]</code>
<code class="go">Iteration 45, loss = 0.12960481</code>
<code class="go">Validation score: 0.788517</code>
<code class="go">Validation score did not improve more than tol=0.000100 for 10 consecutive</code>
<code class="go">epochs. Stopping.</code></pre>

<p>And there you go, you just trained your very first MLP! It required 45 epochs, and as you can see, the training loss went down at each epoch. This loss corresponds to <a data-type="xref" href="ch04.html#ridge_cost_function">Equation 4-9</a> divided by 2, so you must multiply it by 2 to get the MSE (although not exactly because the loss includes the ‚Ñì<sub>2</sub> regularization term). The validation score generally went up at each epoch. Like every regressor in Scikit-Learn, <code translate="no">MLPRegressor</code> uses the R<sup>2</sup> score by default for evaluation‚Äîthat‚Äôs what the <code translate="no">score()</code> method returns. As we saw in <a data-type="xref" href="ch02.html#project_chapter">Chapter¬†2</a>, the R<sup>2</sup> score measures the ratio of the variance that is explained by the model. In this case, it reaches close to 80% on the validation set, which is fairly good for this task:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">mlp_reg</code><code class="o">.</code><code class="n">best_validation_score_</code><code class="w"/>
<code class="go">0.791536125425778</code></pre>

<p>Let‚Äôs evaluate the RMSE on the test set:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">rmse</code> <code class="o">=</code> <code class="n">root_mean_squared_error</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">rmse</code><code class="w"/>
<code class="go">0.5327699946812925</code></pre>

<p>We get a test RMSE of about 0.53, which is comparable to what you would get with a random forest classifier. Not too bad for a first try! <a data-type="xref" href="#predictions_vs_targets_plot">Figure¬†9-9</a> plots the model‚Äôs predictions versus the targets (on the test set). The dashed red line represents the ideal predictions (i.e., equal to the targets): most of the predictions are close to the targets, but there are still quite a few errors, especially for larger targets.</p>

<figure class="smallersixty"><div id="predictions_vs_targets_plot" class="figure">
<img src="assets/hmls_0909.png" alt="A scatter plot shows the MLP regressor's predictions versus the targets, with most points clustering near the dashed red line indicating ideal predictions." width="1003" height="1004"/>
<h6><span class="label">Figure 9-9. </span>MLP regressor‚Äôs predictions versus the targets</h6>
</div></figure>

<p>Note that this MLP does not use any activation function for the output layer, so it‚Äôs free to output any value it wants. This is generally fine, but if you want to guarantee that the output is always positive, then you should use the ReLU activation function on the output layer, or the <em>softplus</em> activation function<a data-type="indexterm" data-primary="activation functions" data-secondary="softplus" id="id2167"/><a data-type="indexterm" data-primary="softplus activation function" id="id2168"/>, which is a smooth variant of ReLU<a data-type="indexterm" data-primary="ReLU (rectified linear units)" data-secondary="and MLPs" data-secondary-sortas="MLP" id="id2169"/>: softplus(<em>z</em>) = log(1 + exp(<em>z</em>)). Softplus is close to 0 when <em>z</em> is negative, and close to <em>z</em> when <em>z</em> is positive. Finally, if you want to guarantee that the predictions always fall within a given range of values, then you should use the sigmoid function or the hyperbolic tangent, and scale the targets to the appropriate range: 0 to 1 for sigmoid and ‚Äì1 to 1 for tanh. Sadly, the <code translate="no">MLPRegressor</code> class does not support activation functions in the output layer.<a data-type="indexterm" data-startref="xi_MLPRegressor933146_1" id="id2170"/><a data-type="indexterm" data-startref="xi_ScikitLearnsklearnneural_networkMLPRegressor933146_1" id="id2171"/></p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Scikit-Learn does not offer GPU acceleration, and its neural net features are fairly limited. This is why we will switch to PyTorch starting in <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter¬†10</a>. That said, it is quite convenient to be able to build and train a standard MLP in just a few lines of code using Scikit-Learn: it lets you tackle many complex tasks very quickly.</p>
</div>

<p>In general, the mean squared error<a data-type="indexterm" data-primary="mean squared error (MSE)" id="id2172"/><a data-type="indexterm" data-primary="MSE (mean squared error)" id="id2173"/> is the right loss to use for a regression tasks, but if you have a lot of outliers in the training set, you may sometimes prefer to use the mean absolute error instead<a data-type="indexterm" data-primary="MAE (mean absolute error)" id="id2174"/><a data-type="indexterm" data-primary="mean absolute error (MAE)" id="id2175"/>, or preferably the <em>Huber loss</em>,<a data-type="indexterm" data-primary="Huber loss" id="id2176"/> which is a combination of both: it is quadratic when the error is smaller than a threshold <em>Œ¥</em> (typically 1), but linear when the error is larger than <em>Œ¥</em>. The linear part makes it less sensitive to outliers than the mean squared error, and the quadratic part allows it to converge faster and be more precise than the mean absolute error. Unfortunately, <code translate="no">MLPRegressor</code> only supports the MSE loss.</p>

<p><a data-type="xref" href="#regression_mlp_architecture">Table¬†9-1</a> summarizes the typical architecture of a regression MLP.</p>
<table id="regression_mlp_architecture">
<caption><span class="label">Table 9-1. </span>Typical regression MLP architecture</caption>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Typical value</th>
</tr>
</thead>
<tbody>
<tr>
<td><p># hidden layers</p></td>
<td><p>Depends on the problem, but typically 1 to 5</p></td>
</tr>
<tr>
<td><p># neurons per hidden layer</p></td>
<td><p>Depends on the problem, but typically 10 to 100</p></td>
</tr>
<tr>
<td><p># output neurons</p></td>
<td><p>1 per target dimension</p></td>
</tr>
<tr>
<td><p>Hidden activation</p></td>
<td><p>ReLU</p></td>
</tr>
<tr>
<td><p>Output activation</p></td>
<td><p>None, or ReLU/softplus (if positive outputs) or sigmoid/tanh (if bounded outputs)</p></td>
</tr>
<tr>
<td><p>Loss function</p></td>
<td><p>MSE, or Huber if outliers</p></td>
</tr>
</tbody>
</table>

<p>All right, MLPs can tackle regression tasks. What else can they do?<a data-type="indexterm" data-startref="xi_multilayerperceptronsMLPsregressionMLPs93294_1" id="id2177"/><a data-type="indexterm" data-startref="xi_regressionmodelsregressionMLPs93294_1" id="id2178"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Classification MLPs"><div class="sect2" id="id152">
<h2>Classification MLPs</h2>

<p>MLPs<a data-type="indexterm" data-primary="classification" data-secondary="MLPs for" id="xi_classificationMLPsfor94285_1"/><a data-type="indexterm" data-primary="classification" data-secondary="multiclass" id="xi_classificationmulticlass94285_1"/><a data-type="indexterm" data-primary="multilayer perceptrons (MLPs)" data-secondary="classification MLPs" id="xi_multilayerperceptronsMLPsclassificationMLPs94285_1"/> can also be used for classification tasks. For a binary classification problem, you just need a single output neuron using the sigmoid activation function: the output will be a number between 0 and 1, which you can interpret as the estimated probability of the positive class. The estimated probability of the negative class is equal to one minus that number.</p>

<p>MLPs can also easily handle multilabel binary classification tasks (see <a data-type="xref" href="ch03.html#classification_chapter">Chapter¬†3</a>). For example, you could have an email classification system that predicts whether each incoming email is ham or spam, and simultaneously predicts whether it is an urgent or nonurgent email. In this case, you would need two output neurons, both using the sigmoid activation function: the first would output the probability that the email is spam, and the second would output the probability that it is urgent. More generally, you would dedicate one output neuron for each positive class. Note that the output probabilities do not necessarily add up to 1. This lets the model output any combination of labels: you can have nonurgent ham, urgent ham, nonurgent spam, and perhaps even urgent spam (although that would probably be an error).</p>

<p>If each instance can belong only to a single class, out of three or more possible classes (e.g., classes 0 through 9 for digit image classification), then you need to have one output neuron per class, and you should use the softmax activation function <a data-type="indexterm" data-primary="softmax activation function" data-secondary="classification MLPs" id="id2179"/>for the whole output layer (see <a data-type="xref" href="#fnn_for_classification_diagram">Figure¬†9-10</a>). The softmax function (introduced in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter¬†4</a>) will ensure that all the estimated probabilities are between 0 and 1, and that they add up to 1, since the classes are exclusive. As we saw in <a data-type="xref" href="ch03.html#classification_chapter">Chapter¬†3</a>, this is called multiclass classification<a data-type="indexterm" data-primary="multiclass (multinomial) classification" id="id2180"/><a data-type="indexterm" data-primary="multinomial (multiclass) classification" id="id2181"/>.</p>

<p>Regarding the loss function, since we are predicting probability distributions, the cross-entropy loss (or <em>x-entropy</em> or log loss for short, see <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter¬†4</a>) is generally a good choice.</p>

<figure class="smallerseventy"><div id="fnn_for_classification_diagram" class="figure">
<img src="assets/hmls_0910.png" alt="Diagram illustrating a modern Multi-Layer Perceptron (MLP) architecture for classification, featuring input, hidden, and output layers with ReLU and softmax functions." width="831" height="748"/>
<h6><span class="label">Figure 9-10. </span>A modern MLP (including ReLU and softmax) for classification</h6>
</div></figure>

<p><a data-type="xref" href="#classification_mlp_architecture">Table¬†9-2</a> summarizes the typical architecture of a classification MLP.</p>
<table id="classification_mlp_architecture">
<caption><span class="label">Table 9-2. </span>Typical classification MLP architecture</caption>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Binary classification</th>
<th>Multilabel binary classification</th>
<th>Multiclass classification</th>
</tr>
</thead>
<tbody>
<tr>
<td><p># hidden layers</p></td>
<td colspan="3"><p>Typically 1 to 5 layers, depending on the task</p></td>
</tr>
<tr>
<td><p># output neurons</p></td>
<td><p>1</p></td>
<td><p>1 per binary label</p></td>
<td><p>1 per class</p></td>
</tr>
<tr>
<td><p>Output layer activation</p></td>
<td><p>Sigmoid</p></td>
<td><p>Sigmoid</p></td>
<td><p>Softmax</p></td>
</tr>
<tr>
<td><p>Loss function</p></td>
<td><p>X-entropy</p></td>
<td><p>X-entropy</p></td>
<td><p>X-entropy</p></td>
</tr>
</tbody>
</table>

<p>As you might expect, Scikit-Learn offers an <code translate="no">MLPClassifier</code> class<a data-type="indexterm" data-primary="MLPClassifier" id="id2182"/><a data-type="indexterm" data-primary="sklearn" data-secondary="neural_network.MLPClassifier" id="id2183"/> in the <code translate="no">sklearn.neural_network</code> package, which you can use for binary or multiclass classification. It is almost identical to the <code translate="no">MLPRegressor</code> class, except that its output layer uses the softmax activation function, and it minimizes the cross-entropy loss rather than the MSE. Moreover, the <code translate="no">score()</code> method returns the model‚Äôs accuracy rather than the R<sup>2</sup> score. Let‚Äôs try it out.</p>

<p>We could tackle the iris dataset, but that task is too simple for a neural net: a linear model would do just as well and wouldn‚Äôt risk overfitting. So let‚Äôs instead tackle a more complex task: Fashion MNIST.<a data-type="indexterm" data-primary="Fashion MNIST dataset" data-secondary="classification MLPs" id="xi_FashionMNISTdatasetclassificationMLPs9456208_1"/> This is a drop-in replacement of MNIST (introduced in <a data-type="xref" href="ch03.html#classification_chapter">Chapter¬†3</a>). It has the exact same format as MNIST (70,000 grayscale images of 28 √ó 28 pixels each, with 10 classes), but the images represent fashion items rather than handwritten digits, so each class is much more diverse, and the problem turns out to be significantly more challenging than MNIST. For example, a simple linear model reaches about 92% accuracy on MNIST, but only about 83% on Fashion MNIST. Let‚Äôs see if we can do better with an MLP.</p>

<p>First, let‚Äôs load the dataset using the <code translate="no">fetch_openml()</code><a data-type="indexterm" data-primary="fetch_openml()" id="id2184"/> function, very much like we did for MNIST in <a data-type="xref" href="ch03.html#classification_chapter">Chapter¬†3</a>. Note that the targets are represented as strings <code translate="no">'0'</code>, <code translate="no">'1'</code>, ‚Ä¶‚Äã, <code translate="no">'9'</code>, so we convert them to integers:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">fetch_openml</code>

<code class="n">fashion_mnist</code> <code class="o">=</code> <code class="n">fetch_openml</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s2">"Fashion-MNIST"</code><code class="p">,</code> <code class="n">as_frame</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
<code class="n">targets</code> <code class="o">=</code> <code class="n">fashion_mnist</code><code class="o">.</code><code class="n">target</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">int</code><code class="p">)</code></pre>

<p>The data is already shuffled, so we just take the first 60,000 images for training, and the last 10,000 for testing:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code> <code class="o">=</code> <code class="n">fashion_mnist</code><code class="o">.</code><code class="n">data</code><code class="p">[:</code><code class="mi">60_000</code><code class="p">],</code> <code class="n">targets</code><code class="p">[:</code><code class="mi">60_000</code><code class="p">]</code>
<code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">fashion_mnist</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="mi">60_000</code><code class="p">:],</code> <code class="n">targets</code><code class="p">[</code><code class="mi">60_000</code><code class="p">:]</code></pre>

<p>Each image is represented as a 1D integer array containing 784 pixel intensities ranging from 0 to 255. You can use the <code translate="no">plt.imshow()</code> function to plot an image, but first you need to reshape it to <code translate="no">[28, 28]</code>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>

<code class="n">X_sample</code> <code class="o">=</code> <code class="n">X_train</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">)</code>  <code class="c1"># first image in the training set</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">X_sample</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"binary"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p>If you run this code, you should see the ankle boot represented in the top-right corner of <a data-type="xref" href="#fashion_mnist_plot">Figure¬†9-11</a>.</p>

<figure><div id="fashion_mnist_plot" class="figure">
<img src="assets/hmls_0911.png" alt="Grid of the first four samples from each class in the Fashion MNIST dataset, showing various clothing and footwear items labeled by category." width="3457" height="1314"/>
<h6><span class="label">Figure 9-11. </span>First four samples from each class in Fashion MNIST</h6>
</div></figure>

<p>With MNIST, when the label is equal to 5, it means that the image represents the handwritten digit 5. Easy. For Fashion MNIST, however, we need the list of class names to know what we are dealing with. Scikit-Learn does not provide it, so let‚Äôs create it:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">class_names</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"T-shirt/top"</code><code class="p">,</code> <code class="s2">"Trouser"</code><code class="p">,</code> <code class="s2">"Pullover"</code><code class="p">,</code> <code class="s2">"Dress"</code><code class="p">,</code> <code class="s2">"Coat"</code><code class="p">,</code>
               <code class="s2">"Sandal"</code><code class="p">,</code> <code class="s2">"Shirt"</code><code class="p">,</code> <code class="s2">"Sneaker"</code><code class="p">,</code> <code class="s2">"Bag"</code><code class="p">,</code> <code class="s2">"Ankle boot"</code><code class="p">]</code></pre>

<p>We can now confirm that the first image in the training set represents an ankle boot:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">class_names</code><code class="p">[</code><code class="n">y_train</code><code class="p">[</code><code class="mi">0</code><code class="p">]]</code><code class="w"/>
<code class="go">'Ankle boot'</code></pre>

<p>We‚Äôre ready to build the classification MLP:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.neural_network</code> <code class="kn">import</code> <code class="n">MLPClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">MinMaxScaler</code>

<code class="n">mlp_clf</code> <code class="o">=</code> <code class="n">MLPClassifier</code><code class="p">(</code><code class="n">hidden_layer_sizes</code><code class="o">=</code><code class="p">[</code><code class="mi">300</code><code class="p">,</code> <code class="mi">100</code><code class="p">],</code> <code class="n">verbose</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                        <code class="n">early_stopping</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">pipeline</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">MinMaxScaler</code><code class="p">(),</code> <code class="n">mlp_clf</code><code class="p">)</code>
<code class="n">pipeline</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">accuracy</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code></pre>

<p>This code is very similar to the regression code we used earlier, but there are a few differences:</p>

<ul>
<li>
<p>Of course, it‚Äôs a classification task so we use an <code translate="no">MLPClassifier</code> rather than an <code translate="no">MLPRegressor</code>.</p>
</li>
<li>
<p>We use just two hidden layers with 300 and 100 neurons, respectively. You can try a different number of hidden layers, and change the number of neurons as well if you want.</p>
</li>
<li>
<p>We also use a <code translate="no">MinMaxScaler</code><a data-type="indexterm" data-primary="MinMaxScaler" id="id2185"/><a data-type="indexterm" data-primary="sklearn" data-secondary="preprocessing.MinMaxScaler" id="id2186"/><a data-type="indexterm" data-primary="sklearn" data-secondary="preprocessing.StandardScaler" id="id2187"/> instead of a <code translate="no">StandardScaler</code>. We need it to shrink the pixel intensities down to the 0‚Äì1 range rather than 0‚Äì255: having features in this range usually works better with the default hyperparameters used by <code translate="no">MLPClassifier</code>, such as its default learning rate and weight initialization scale. You might wonder why we didn‚Äôt use a <code translate="no">StandardScaler</code><a data-type="indexterm" data-primary="StandardScaler" id="id2188"/>? Well some pixels don‚Äôt vary much across images; for example, the pixels around the edges are almost always white. If we used the <code translate="no">StandardScaler</code>, these pixels would get scaled up to have the same variance as every other pixel: as a result, we would give more importance to these pixels than they probably deserve. Using the <code translate="no">MinMaxScaler</code> often works better than the <code translate="no">StandardScaler</code> for images (but your mileage may vary).</p>
</li>
<li>
<p>Lastly, the <code translate="no">score()</code><a data-type="indexterm" data-primary="score()" id="id2189"/> function returns the model‚Äôs accuracy.</p>
</li>
</ul>

<p>If you run this code, you will find that the model reaches about 89.7% accuracy on the validation set during training (the exact value is given by <code translate="no">mlp_clf.best_validation_score_</code>), but it starts overfitting a bit toward the end, so it ends up at just 89.2% accuracy. When we evaluate the model on the test set, we get 87.1%, which is not bad for this task, although we can do better with other neural net architectures such as convolutional neural networks (<a data-type="xref" href="ch12.html#cnn_chapter">Chapter¬†12</a>).</p>

<p>You probably noticed that training was quite slow. That‚Äôs because the hidden layers have a <em>lot</em> of parameters<a data-type="indexterm" data-primary="hidden layers" data-secondary="number of parameters" id="id2190"/>, so there are many computations to run at each iteration. For example, the first hidden layer has 784 √ó 300 connection weights, plus 300 bias terms, which adds up to 235,500 parameters! All these parameters give the model quite a lot of flexibility to fit the training data, but it also means that there‚Äôs a high risk of overfitting, especially when you do not have a lot of training data. In this case, you may want to use regularization techniques such as early stopping and ‚Ñì<sub>2</sub> regularization.</p>

<p>Once the model is trained, you can use it to classify new images:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code> <code class="o">=</code> <code class="n">X_test</code><code class="p">[:</code><code class="mi">15</code><code class="p">]</code>  <code class="c1"># let's pretend these are 15 new images</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">mlp_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code><code class="w"/>
<code class="go">array([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 8, 3, 4])</code></pre>

<p>All these predictions are correct, except for the one at index 12, which should be a 7 (sneaker) instead of a 8 (bag). You might want to know how confident the model was about these predictions, especially the bad one. For this, you can use <code translate="no">model.predict_proba()</code> instead of <code translate="no">model.predict()</code>, like we did in <a data-type="xref" href="ch03.html#classification_chapter">Chapter¬†3</a>:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_proba</code> <code class="o">=</code> <code class="n">mlp_clf</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_proba</code><code class="p">[</code><code class="mi">12</code><code class="p">]</code><code class="w"/>
<code class="go">array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])</code></pre>

<p>Hmm, that‚Äôs not great: the model is telling us that it‚Äôs 100% confident that the image represents a bag (index 8). So not only is the model wrong, it‚Äôs 100% confident that it‚Äôs right. In fact, across all 10,000 images in the test set, there are only 16 images that the model is less than 99.9% confident about, despite the fact that its accuracy is about 90%. That‚Äôs why you should always treat estimated probabilities with a grain of salt: neural nets have a strong tendency to be overconfident, especially if they are trained for a bit too long.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The targets for classification tasks can be class indices (e.g., 3) or class probabilities, typically one-hot vectors (e.g., [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]). But if your model tends to be overconfident, you can try the <em>label smoothing</em> technique<a data-type="indexterm" data-primary="label smoothing technique" id="id2191"/>:‚Å†<sup><a data-type="noteref" id="id2192-marker" href="ch09.html#id2192">14</a></sup> reduce the target class‚Äôs probability slightly (e.g., from 1 down to 0.9) and distribute the rest evenly across the other classes (e.g., [0.1/9, 0.1/9, 0.1/9, 0.9, 0.1/9, 0.1/9, 0.1/9, 0.1/9, 0.1/9, 0.1/9]).</p>
</div>

<p>Still, getting 90% accuracy on Fashion MNIST is pretty good. You could get even better performance by fine-tuning the hyperparameters, for example using <code translate="no">RandomizedSearchCV</code>, as we did in <a data-type="xref" href="ch02.html#project_chapter">Chapter¬†2</a>. However, the search space is quite large, so it helps to know roughly where to look.<a data-type="indexterm" data-startref="xi_artificialneuralnetworksANNsbuildingandtraining93265_1" id="id2193"/><a data-type="indexterm" data-startref="xi_classificationMLPsfor94285_1" id="id2194"/><a data-type="indexterm" data-startref="xi_classificationmulticlass94285_1" id="id2195"/><a data-type="indexterm" data-startref="xi_FashionMNISTdatasetclassificationMLPs9456208_1" id="id2196"/><a data-type="indexterm" data-startref="xi_multilayerperceptronsMLPsclassificationMLPs94285_1" id="id2197"/><a data-type="indexterm" data-startref="xi_ScikitLearnbuildingandtrainingMLPswith93265_1" id="id2198"/></p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Hyperparameter Tuning Guidelines"><div class="sect1" id="id403">
<h1>Hyperparameter Tuning Guidelines</h1>

<p>The<a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="hyperparameter fine-tuning" id="xi_artificialneuralnetworksANNshyperparameterfinetuning95594_1"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="tuning of" id="id2199"/> flexibility of neural networks is also one of their main drawbacks: there are many hyperparameters to tweak. Not only can you use any imaginable network architecture, but even in a basic MLP you can change the number of layers, the number of neurons and the type of activation function to use in each layer, the weight initialization logic, the type of optimizer to use, its learning rate, the batch size, and more. What are some good values for these hyperparameters?</p>








<section data-type="sect2" data-pdf-bookmark="Number of Hidden Layers"><div class="sect2" id="id153">
<h2>Number of Hidden Layers</h2>

<p>For<a data-type="indexterm" data-primary="hidden layers" id="xi_hiddenlayers95624_1"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="number of hidden layers" id="xi_hyperparametersnumberofhiddenlayers95624_1"/> many problems, you can begin with a single hidden layer and get reasonable results. An MLP with just one hidden layer can theoretically model even the most complex functions, provided it has enough neurons. But deep networks have a much higher <em>parameter efficiency</em><a data-type="indexterm" data-primary="parameter efficiency" id="id2200"/> than shallow ones: they can model complex functions using exponentially fewer neurons than shallow nets, allowing them to reach much better performance with the same amount of training data. This is because their layered structure enables them to reuse and compose features across multiple levels: for example, the first layer in a face classifier may learn to recognize low-level features such as dots, arcs, or straight lines; while the second layer may learn to combine these low-level features into higher-level features such as squares or circles; and the third layer may learn to combine these higher-level features into a mouth, an eye, or a nose; and the top layer would then be able to use these top-level features to classify faces.</p>

<p>Not only does this hierarchical architecture help DNNs converge faster to a good solution, but it also improves their ability to generalize to new datasets. For example, if you have already trained a model to recognize faces in pictures and you now want to train a new neural network to recognize hairstyles, you can kickstart the training by reusing the lower layers of the first network. Instead of randomly initializing the weights and biases of the first few layers of the new neural network, you can initialize them to the values of the weights and biases of the lower layers of the first network. This way the network will not have to learn from scratch all the low-level structures that occur in most pictures; it will only have to learn the higher-level structures (e.g., hairstyles). This is called <em>transfer learning</em><a data-type="indexterm" data-primary="transfer learning" id="id2201"/>.</p>

<p>In summary, for many problems you can start with just one or two hidden layers, and the neural network will work pretty well. For instance, you can easily reach above 97% accuracy on the MNIST dataset<a data-type="indexterm" data-primary="Fashion MNIST dataset" data-secondary="hyperparameter tuning" id="id2202"/> using just one hidden layer with a few hundred neurons, and above 98% accuracy using two hidden layers with the same total number of neurons, in roughly the same amount of training time. For more complex problems, you can ramp up the number of hidden layers until you start overfitting the training set. Very complex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers (or even hundreds, but not fully connected ones, as you will see in <a data-type="xref" href="ch12.html#cnn_chapter">Chapter¬†12</a>), and they need a huge amount of training data. You will rarely have to train such networks from scratch: it is much more common to reuse parts of a pretrained state-of-the-art network that performs a similar task. Training will then be a lot faster and require much less data.<a data-type="indexterm" data-startref="xi_hiddenlayers95624_1" id="id2203"/><a data-type="indexterm" data-startref="xi_hyperparametersnumberofhiddenlayers95624_1" id="id2204"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Number of Neurons per Hidden Layer"><div class="sect2" id="id154">
<h2>Number of Neurons per Hidden Layer</h2>

<p>The<a data-type="indexterm" data-primary="hidden layers" data-secondary="neurons per hidden layer" id="xi_hiddenlayersneuronsperhiddenlayer95694_1"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="neurons per hidden layer" id="xi_hyperparametersneuronsperhiddenlayer95694_1"/><a data-type="indexterm" data-primary="neurons per hidden layer" id="xi_neuronsperhiddenlayer95694_1"/> number of neurons in the input and output layers is determined by the type of input and output your task requires. For example, the MNIST task requires 28 √ó 28 = 784 inputs and 10 output neurons.</p>

<p>As for the hidden layers, it used to be common to size them to form a pyramid, with fewer and fewer neurons at each layer‚Äîthe rationale being that many low-level features can coalesce into far fewer high-level features. A typical neural network for MNIST might have 3 hidden layers, the first with 300 neurons, the second with 200, and the third with 100. However, this practice has been largely abandoned because it seems that using the same number of neurons in all hidden layers performs just as well in most cases, or even better; plus, there is only one hyperparameter to tune, instead of one per layer. That said, depending on the dataset, it can sometimes help to make the first hidden layer a bit larger than the others.</p>

<p>Just like the number of layers, you can try increasing the number of neurons gradually until the network starts overfitting<a data-type="indexterm" data-primary="overfitting of data" data-secondary="number of neurons per hidden layer" id="id2205"/>. Alternatively, you can try building a model with slightly more layers and neurons than you actually need, then use early stopping and other regularization techniques to prevent it from overfitting too much. Vincent Vanhoucke, a Waymo researcher and former Googler, has dubbed this the ‚Äústretch pants‚Äù approach: instead of wasting time looking for pants that perfectly match your size, just use large stretch pants that will shrink down to the right size. With this approach, you avoid bottleneck layers that could ruin your model. Indeed, if a layer has too few neurons, it will lack the computational capacity to model complex relationships, and it may not even have enough representational power to preserve all the useful information from the inputs. For example, if you apply PCA (introduced in <a data-type="xref" href="ch07.html#dimensionality_chapter">Chapter¬†7</a>) to the Fashion MNIST training set, you will find that you need 187 dimensions to preserve 95% of the variance in the data. So if you set the number of neurons in the first hidden layer to some greater number, say 200, you can be confident that this layer will not be a bottleneck. However, you don‚Äôt want to add too many neurons, or else the model will have too many parameters to optimize, and it will take more time and data to train.</p>
<div data-type="tip"><h6>Tip</h6>
<p>In general, you will get more bang for your buck by increasing the number of layers rather than the number of neurons per layer.</p>
</div>

<p>That said, bottleneck layers<a data-type="indexterm" data-primary="bottleneck layers" id="id2206"/> are not always a bad thing. For example, limiting the dimensionality of the first hidden layers forces the neural net to keep only the most important dimensions, which can eliminate some of the noise in the data (but don‚Äôt go too far!). Also, having a bottleneck layer near the output layer can force the neural net to learn good representations of the data in the previous layers (i.e., packing more useful information in less space), which can help the neural net generalize, and can also be useful in and of itself for <em>representation learning</em>. We will get back to that in <a data-type="xref" href="ch18.html#autoencoders_chapter">Chapter¬†18</a>.<a data-type="indexterm" data-startref="xi_hiddenlayersneuronsperhiddenlayer95694_1" id="id2207"/><a data-type="indexterm" data-startref="xi_hyperparametersneuronsperhiddenlayer95694_1" id="id2208"/><a data-type="indexterm" data-startref="xi_neuronsperhiddenlayer95694_1" id="id2209"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Learning Rate"><div class="sect2" id="id155">
<h2>Learning Rate</h2>

<p>The<a data-type="indexterm" data-primary="hyperparameters" data-secondary="learning rate" id="id2210"/><a data-type="indexterm" data-primary="learning rate" id="id2211"/> learning rate is a hugely important hyperparameter. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges, as we saw in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter¬†4</a>). One way to find a good learning rate is to train the model for a few hundred iterations, starting with a very low learning rate (e.g., 10<sup>‚Äì5</sup>) and gradually increasing it up to a very large value (e.g., 10). This is done by multiplying the learning rate by a constant factor at each iteration (e.g., by (10 / 10<sup>-5</sup>)<sup>1¬†/¬†500</sup> to go from 10<sup>‚Äì5</sup> to 10 in 500 iterations). If you plot the loss as a function of the learning rate (using a log scale for the learning rate), you should see it dropping at first. But after a while, the learning rate will be too large, so the loss will shoot back up: the optimal learning rate is often a bit lower than the point at which the loss starts to climb (typically about 10 times lower than the turning point). You can then reinitialize your model and train it normally using this good learning rate.</p>
<div data-type="tip"><h6>Tip</h6>
<p>To change the learning rate during training when using Scikit-Learn, you must set the MLP‚Äôs <code translate="no">warm_start</code> hyperparameter to <code translate="no">True</code>, and fit the model one batch at a time using <code translate="no">partial_fit()</code>, much like we did with the <code translate="no">SGDRegressor</code> in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter¬†4</a>. Simply update the learning rate at each iteration.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Batch Size"><div class="sect2" id="id404">
<h2>Batch Size</h2>

<p>The<a data-type="indexterm" data-primary="hyperparameters" data-secondary="batch size" id="id2212"/> batch size can have a significant impact on your model‚Äôs performance and training time. The main benefit of using large batch sizes is that hardware accelerators like GPUs can process them efficiently (as we will see in <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter¬†10</a>), so the training algorithm will see more instances per second. Therefore, many researchers and practitioners recommend using the largest batch size that can fit in <em>VRAM</em> (video RAM, i.e., the GPU‚Äôs memory). There‚Äôs a catch, though: large batch sizes can sometimes lead to training instabilities, especially with smaller models and at the beginning of training, and the resulting model may not generalize as well as a model trained with a small batch size. Yann LeCun once tweeted ‚ÄúFriends don‚Äôt let friends use mini-batches larger than 32‚Äù, citing a <a href="https://homl.info/smallbatch">2018 paper</a>‚Å†<sup><a data-type="noteref" id="id2213-marker" href="ch09.html#id2213">15</a></sup> by Dominic Masters and Carlo Luschi which concluded that using small batches (from 2 to 32) was preferable because small batches led to better models in less training time.</p>

<p>However, other research points in the opposite direction. For example, in 2017, papers by <a href="https://homl.info/largebatch">Elad Hoffer et al.</a>‚Å†<sup><a data-type="noteref" id="id2214-marker" href="ch09.html#id2214">16</a></sup> and <a href="https://homl.info/largebatch2">Priya Goyal et al.</a>‚Å†<sup><a data-type="noteref" id="id2215-marker" href="ch09.html#id2215">17</a></sup> showed that it is possible to use very large batch sizes (up to 8,192), along with various techniques such as warming up the learning rate (i.e., starting training with a small learning rate, then ramping it up), to obtain very short training times, without any generalization gap.</p>

<p>So one strategy is to use a large batch size, possibly with learning rate warmup, and if training is unstable or the final performance is disappointing, then try using a smaller batch size instead.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Other Hyperparameters"><div class="sect2" id="id156">
<h2>Other Hyperparameters</h2>

<p>Here are two more hyperparameters you can tune if you have the computation budget and the time:</p>
<dl>
<dt>Optimizer</dt>
<dd>
<p><a data-type="indexterm" data-primary="hyperparameters" data-secondary="optimizer" id="id2216"/><a data-type="indexterm" data-primary="optimizers" data-secondary="hyperparameters" id="id2217"/>Choosing a better optimizer than plain old mini-batch gradient descent (and tuning its hyperparameters) can help speed up training and sometimes reach better performance.</p>
</dd>
<dt>Activation function</dt>
<dd>
<p><a data-type="indexterm" data-primary="activation functions" data-secondary="hyperparameter tuning" id="id2218"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="activation function" id="id2219"/>We discussed how to choose the activation function earlier in this chapter: in general, the ReLU activation function is a good default for all hidden layers. In some cases, replacing ReLU with another function can help.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>The optimal learning rate depends on the other hyperparameters‚Äîespecially the batch size‚Äîso if you modify any hyperparameter, make sure to tune the learning rate again.</p>
</div>

<p>For more best practices regarding tuning neural network hyperparameters, check out the excellent <a href="https://homl.info/1cycle">2018 paper</a>‚Å†<sup><a data-type="noteref" id="id2220-marker" href="ch09.html#id2220">18</a></sup> by Leslie Smith. The <a href="https://github.com/google-research/tuning_playbook">Deep Learning Tuning Playbook</a> by Google researchers is also well worth reading. The free e-book <a href="https://homl.info/ngbook"><em>Machine Learning Yearning</em> by Andrew Ng</a> also contains a wealth of practical advice.</p>

<p>Lastly, I highly recommend you go through exercise 1 at the end of this chapter. You will use a nice web interface to play with various neural network architectures and visualize their outputs. This will be very useful to better understand MLPs and grow a good intuition for the effects of each hyperparameter (number of layers and neurons, activation functions, and more).</p>

<p>This concludes our introduction to artificial neural networks and their implementation with Scikit-Learn. In the next chapter, we will switch to PyTorch, the leading open source library for neural networks, and we will use it to train and run MLPs much faster by exploiting the power of graphical processing units (GPUs).<a data-type="indexterm" data-startref="xi_artificialneuralnetworksANNs936_1" id="id2221"/> We will also start building more complex models, with multiple inputs and outputs.<a data-type="indexterm" data-startref="xi_artificialneuralnetworksANNshyperparameterfinetuning95594_1" id="id2222"/><a data-type="indexterm" data-startref="xi_multilayerperceptronsMLPs9266170_1" id="id2223"/></p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="id733">
<h1>Exercises</h1>
<ol>
<li>
<p>This <a href="https://playground.tensorflow.org">neural network playground</a> is a great tool to build your intuitions without writing any code (it was built by the TensorFlow team, but there‚Äôs nothing TensorFlow-specific about it; in fact, it doesn‚Äôt even use TensorFlow). In this exercise, you will train several binary classifiers in just a few clicks, and tweak the model‚Äôs architecture and its hyperparameters to gain some intuition on how neural networks work and what their hyperparameters do. Take some time to explore the <span class="keep-together">following</span>:</p>
<ol>
<li>
<p>The patterns learned by a neural net. Try training the default neural network by clicking the Run button (top left). Notice how it quickly finds a good solution for the classification task. The neurons in the first hidden layer have learned simple patterns, while the neurons in the second hidden layer have learned to combine the simple patterns of the first hidden layer into more complex patterns. In general, the more layers there are, the more complex the patterns can be.</p>
</li>
<li>
<p>Activation functions. Try replacing the tanh activation function with a ReLU activation function, and train the network again. Notice that it finds a solution even faster, but this time the boundaries are linear. This is due to the shape of the ReLU function.</p>
</li>
<li>
<p>The risk of local minima. Modify the network architecture to have just one hidden layer with three neurons. Train it multiple times (to reset the network weights, click the Reset button next to the Play button). Notice that the training time varies a lot, and sometimes it even gets stuck in a local minimum.</p>
</li>
<li>
<p>What happens when neural nets are too small. Remove one neuron to keep just two. Notice that the neural network is now incapable of finding a good solution, even if you try multiple times. The model has too few parameters and systematically underfits the training set.</p>
</li>
<li>
<p>What happens when neural nets are large enough. Set the number of neurons to eight, and train the network several times. Notice that it is now consistently fast and never gets stuck. This highlights an important finding in neural network theory: large neural networks rarely get stuck in local minima, and even when they do, these local optima are often almost as good as the global optimum. However, they can still get stuck on long plateaus for a long time.</p>
</li>
<li>
<p>The risk of vanishing gradients in deep networks. Select the spiral dataset (the bottom-right dataset under ‚ÄúDATA‚Äù), and change the network architecture to have four hidden layers with eight neurons each. Notice that training takes much longer and often gets stuck on plateaus for long periods of time. Also notice that the neurons in the highest layers (on the right) tend to evolve faster than the neurons in the lowest layers (on the left). This problem, called the <em>vanishing gradients</em> problem, can be alleviated with better weight initialization and other techniques, better optimizers (such as AdaGrad or Adam), or batch normalization (discussed in <a data-type="xref" href="ch11.html#deep_chapter">Chapter¬†11</a>).</p>
</li>
<li>
<p>Go further. Take an hour or so to play around with other parameters and get a feel for what they do to build an intuitive understanding about neural <span class="keep-together">networks</span>.</p>
</li>

</ol>
</li>
<li>
<p>Draw an ANN using the original artificial neurons (like the ones in <a data-type="xref" href="#nn_propositional_logic_diagram">Figure¬†9-3</a>) that computes <em>A</em> ‚äï <em>B</em> (where ‚äï represents the XOR operation). Hint: <em>A</em> ‚äï <em>B</em> = <span class="keep-together">(<em>A</em> ‚àß ¬¨ <em>B</em>)</span> ‚à® (¬¨ <em>A</em> ‚àß <em>B</em>).</p>
</li>
<li>
<p>Why is it generally preferable to use a logistic regression classifier rather than a classic perceptron (i.e., a single layer of threshold logic units trained using the perceptron training algorithm)? How can you tweak a perceptron to make it equivalent to a logistic regression classifier?</p>
</li>
<li>
<p>Why was the sigmoid activation function a key ingredient in training the first MLPs?</p>
</li>
<li>
<p>Name three popular activation functions. Can you draw them?</p>
</li>
<li>
<p>Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.</p>
<ol>
<li>
<p>What is the shape of the input matrix <strong>X</strong>?</p>
</li>
<li>
<p>What are the shapes of the hidden layer‚Äôs weight matrix <strong>W</strong><sub><em>h</em></sub> and bias vector <strong>b</strong><sub><em>h</em></sub>?</p>
</li>
<li>
<p>What are the shapes of the output layer‚Äôs weight matrix <strong>W</strong><sub><em>o</em></sub> and bias vector <strong>b</strong><sub><em>o</em></sub>?</p>
</li>
<li>
<p>What is the shape of the network‚Äôs output matrix <strong>Y</strong>?</p>
</li>
<li>
<p>Write the equation that computes the network‚Äôs output matrix <strong>Y</strong> as a function of <strong>X</strong>, <strong>W</strong><sub><em>h</em></sub>, <strong>b</strong><sub><em>h</em></sub>, <strong>W</strong><sub><em>o</em></sub>, and <strong>b</strong><sub><em>o</em></sub>.</p>
</li>

</ol>
</li>
<li>
<p>How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, and which activation function should you use? What about for getting your network to predict housing prices, as in <a data-type="xref" href="ch02.html#project_chapter">Chapter¬†2</a>?</p>
</li>
<li>
<p>What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?</p>
</li>
<li>
<p>Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?</p>
</li>
<li>
<p>Train a deep MLP on the CoverType dataset. You can load it using <code translate="no">sklearn.datasets.fetch_covtype()</code>. See if you can get over 93% accuracy on the test set by fine-tuning the hyperparameters manually and/or using <code translate="no">RandomizedSearchCV</code>.</p>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter‚Äôs notebook, at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id2093"><sup><a href="ch09.html#id2093-marker">1</a></sup> You can get the best of both worlds by being open to biological inspirations without being afraid to create biologically unrealistic models, as long as they work well.</p><p data-type="footnote" id="id2094"><sup><a href="ch09.html#id2094-marker">2</a></sup> Warren S. McCulloch and Walter Pitts, ‚ÄúA Logical Calculus of the Ideas Immanent in Nervous Activity‚Äù, <em>The Bulletin of Mathematical Biology</em> 5, no. 4 (1943): 115‚Äì113.</p><p data-type="footnote" id="id2106"><sup><a href="ch09.html#id2106-marker">3</a></sup> They are not actually attached, just so close that they can very quickly exchange chemical signals.</p><p data-type="footnote" id="id2110"><sup><a href="ch09.html#id2110-marker">4</a></sup> Image by Bruce Blaus (<a href="https://oreil.ly/pMbrK">Creative Commons 3.0</a>). Reproduced from <a href="https://en.wikipedia.org/wiki/Neuron" class="bare"><em class="hyperlink">https://en.wikipedia.org/wiki/Neuron</em></a>.</p><p data-type="footnote" id="id2111"><sup><a href="ch09.html#id2111-marker">5</a></sup> In the context of machine learning, the phrase ‚Äúneural networks‚Äù generally refers to ANNs, not BNNs.</p><p data-type="footnote" id="id2115"><sup><a href="ch09.html#id2115-marker">6</a></sup> Drawing of a cortical lamination by S.¬†Ramon y Cajal (public domain). Reproduced from <a href="https://en.wikipedia.org/wiki/Cerebral_cortex" class="bare"><em class="hyperlink">https://en.wikipedia.org/wiki/Cerebral_cortex</em></a>.</p><p data-type="footnote" id="id2122"><sup><a href="ch09.html#id2122-marker">7</a></sup> Logistic regression and the logistic function were introduced in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter¬†4</a>, along with several other concepts that we will heavily rely on in this chapter, including softmax, cross-entropy, gradient descent, early stopping, and more, so please make sure to read it first.</p><p data-type="footnote" id="id2128"><sup><a href="ch09.html#id2128-marker">8</a></sup> In some libraries, such as PyTorch, the weight matrix is transposed, so there‚Äôs one row per neuron, and one column per input feature.</p><p data-type="footnote" id="id2132"><sup><a href="ch09.html#id2132-marker">9</a></sup> Note that this solution is not unique: when data points are linearly separable, there is an infinity of hyperplanes that can separate them.</p><p data-type="footnote" id="id2139"><sup><a href="ch09.html#id2139-marker">10</a></sup> For example, when the inputs are (0, 1) the lower-left neuron computes 0 √ó 1 + 1 √ó 1 ‚Äì 3 / 2 = ‚Äì1 / 2, which is negative, so it outputs 0. The lower-right neuron computes 0 √ó 1 + 1 √ó 1 ‚Äì 1 / 2 = 1 / 2, which is positive, so it outputs 1. The output neuron receives the outputs of the first two neurons as its inputs, so it computes  <span class="keep-together">0 √ó (‚Äì1) + 1 √ó 1 - 1 / 2 = 1 / 2.</span> This is positive, so it outputs 1.</p><p data-type="footnote" id="id2146"><sup><a href="ch09.html#id2146-marker">11</a></sup> In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see ANNs with dozens of layers, or even hundreds, so the definition of ‚Äúdeep‚Äù is quite fuzzy.</p><p data-type="footnote" id="id2152"><sup><a href="ch09.html#id2152-marker">12</a></sup> David Rumelhart et al., ‚ÄúLearning Internal Representations by Error Propagation‚Äù (Defense Technical Information Center technical report, September 1985).</p><p data-type="footnote" id="id2162"><sup><a href="ch09.html#id2162-marker">13</a></sup> Biological neurons seem to implement a roughly sigmoid (<em>S</em>-shaped) activation function, so researchers stuck to sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is one of the cases where the biological analogy was perhaps misleading.</p><p data-type="footnote" id="id2192"><sup><a href="ch09.html#id2192-marker">14</a></sup> C. Szegedy et al., ‚ÄúRethinking the Inception Architecture for Computer Vision‚Äù, CVPR 2016: 2818‚Äì2826.</p><p data-type="footnote" id="id2213"><sup><a href="ch09.html#id2213-marker">15</a></sup> Dominic Masters and Carlo Luschi, ‚ÄúRevisiting Small Batch Training for Deep Neural Networks‚Äù, arXiv preprint arXiv:1804.07612 (2018).</p><p data-type="footnote" id="id2214"><sup><a href="ch09.html#id2214-marker">16</a></sup> Elad Hoffer et al., ‚ÄúTrain Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks‚Äù, <em>Proceedings of the 31st International Conference on Neural Information Processing Systems</em> (2017): 1729‚Äì1739.</p><p data-type="footnote" id="id2215"><sup><a href="ch09.html#id2215-marker">17</a></sup> Priya Goyal et al., ‚ÄúAccurate, Large Minibatch SGD: Training ImageNet in 1 Hour‚Äù, arXiv preprint arXiv:1706.02677 (2017).</p><p data-type="footnote" id="id2220"><sup><a href="ch09.html#id2220-marker">18</a></sup> Leslie N. Smith, ‚ÄúA Disciplined Approach to Neural Network Hyper-Parameters: Part 1‚ÄîLearning Rate, Batch Size, Momentum, and Weight Decay‚Äù, arXiv preprint arXiv:1803.09820 (2018).</p></div></div></section></div></div></body></html>