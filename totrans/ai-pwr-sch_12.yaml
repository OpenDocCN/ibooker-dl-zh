- en: 10 Learning to rank for generalizable search relevance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 为可推广的搜索相关性学习排名
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: An introduction to machine-learned ranking, also known as learning to rank (LTR)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习排名简介，也称为学习排名（LTR）
- en: How LTR differs from other machine learning methods
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LTR与其他机器学习方法的区别
- en: Training and deploying a ranking classifier
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和部署排名分类器
- en: Feature engineering, judgment lists, and integrating machine-learned ranking
    models into a search engine
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程、判断列表以及将机器学习排名模型集成到搜索引擎中
- en: Validating an LTR model using a train/test split
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练/测试分割验证LTR模型
- en: Performance tradeoffs for LTR-based ranking models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于LTR的排名模型的性能权衡
- en: It’s a random Tuesday. You review your search logs, and the searches range from
    the frustrated runner’s `polar m430 running watch charger` query to the worried
    hypochondriac’s `weird bump` `on nose` `-` `cancer?` to the curious cinephile’s
    `william shatner` `first` `film`. Even though these may be one-off queries, you
    know each user expects nothing less than amazing search results.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 今天是个随意的星期二。你回顾了你的搜索日志，搜索内容从沮丧的跑者的`polar m430 运动手表充电器`查询，到担忧的疑病症患者的`鼻子上的奇怪肿块`
    `-` `癌症？`，再到好奇的电影爱好者的`william shatner` `第一部电影`。即使这些可能只是一次性的查询，你知道每个用户都期望得到不亚于惊人的搜索结果。
- en: 'You feel hopeless. You know many query strings, by themselves, are distressingly
    rare. You have very little click data to know what’s relevant for these searches.
    Every day gets more challenging: trends, use cases, products, user interfaces,
    and even user terminology evolve. How can anyone hope to build search that amazes
    when users seem to constantly surprise us with new ways of searching?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你感到绝望。你知道许多查询字符串本身就很罕见。你几乎没有点击数据来了解这些搜索的相关性。每一天都变得更加具有挑战性：趋势、用例、产品、用户界面，甚至用户术语都在不断演变。当用户似乎不断以新的搜索方式让我们感到惊讶时，任何人如何希望构建令人惊叹的搜索呢？
- en: 'Despair not, there is hope! In this chapter, we’ll introduce generalizable
    relevance models. These models learn the underlying patterns that drive relevance
    ranking. Instead of memorizing that the article entitled “Zits: bumps on nose”
    is the answer for the query `weird` `bump` `on` `nose` `-` `cancer?` we observe
    the underlying pattern—that a strong title match corresponds to high probability
    of relevance. If we can learn these patterns and encode them into a model, we
    can give relevant results *even for search queries we’ve never seen*.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 不要绝望，还有希望！在本章中，我们将介绍可推广的相关性模型。这些模型学习驱动相关性排名的潜在模式。我们不再需要记住标题为“Zits：鼻子上的肿块”的文章是针对查询`weird
    bump on nose - cancer？`的答案，我们观察到潜在的规律——一个强有力的标题匹配对应着高相关性的可能性。如果我们能够学习这些模式并将它们编码到模型中，我们就可以为*我们从未见过的搜索查询*提供相关结果。
- en: 'This chapter explores *learning to rank* (LTR): a technique that uses machine
    learning to create generalizable relevance ranking models. We’ll prepare, train,
    and search with LTR models using the search engine.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了*学习排名* (LTR)：一种使用机器学习来创建可推广的相关性排名模型的技巧。我们将使用搜索引擎准备、训练和搜索LTR模型。
- en: 10.1 What is LTR?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 什么是LTR？
- en: Let’s explore what LTR does. We’ll see how LTR creates generalizable ranking
    models by finding patterns that predict relevance. We’ll then explore more of
    the nuts and bolts of building a model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索LTR做了什么。我们将看到LTR如何通过寻找预测相关性的模式来创建可推广的排名模型。然后我们将探索构建模型的更多细节。
- en: 10.1.1 Moving beyond manual relevance tuning
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 超越手动相关性调整
- en: Recall manual relevance tuning from chapter 3\. We observe factors that correspond
    with relevant results, and we combine those factors mathematically into a *ranking
    function*. The ranking function returns a relevance score that orders results
    as closely as possible to our ideal ranking.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾第3章中的手动相关性调整。我们观察到与相关结果相对应的因素，并将这些因素通过数学方法组合成一个*排名函数*。排名函数返回一个相关性分数，将结果尽可能紧密地排序到我们的理想排名。
- en: For example, consider a movie search engine with documents like those in the
    following listing. This document comes from TheMovieDB (tmdb) corpus ([http://themoviedb.org](http://themoviedb.org)),
    which we’ll use in this chapter. If you wish to follow along with the code for
    this chapter, use this chapter’s first notebook to index the tmdb dataset.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个包含以下列表中文档的电影搜索引擎。这份文档来自TheMovieDB (tmdb)语料库([http://themoviedb.org](http://themoviedb.org))，我们将在本章中使用它。如果你希望跟随本章的代码，请使用本章的第一个笔记本来索引tmdb数据集。
- en: Listing 10.1 A document for the movie *The Social Network*
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.1 电影《社交网络》的文档
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Through endless iterations and tweaks, we might arrive at a generalizable movie
    ranking function that looks something like the next listing.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.2 A generalizable ranking function using manual boosts
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Manually optimizing the feature weights of general ranking functions like this
    to work over many queries can take significant effort, but such optimizations
    are perfect for machine learning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: This is where LTR comes in—it takes our proposed relevance factors and learns
    an optimal ranking function. LTR takes several forms, from a simple set of linear
    weights (like the boosts here) to a complex deep learning model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: To learn the ropes, we’ll build a simple LTR model in this chapter. We’ll find
    the optimal weights for `title`, `overview`, and `release_year` in a scoring function
    like the one in listing 10.2\. With this relatively simple task, we’ll see the
    full lifecycle of developing an LTR solution.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 Implementing LTR in the real world
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we continue to define LTR at a high level, let’s quickly clarify where LTR
    fits into the overall picture of a search system. Then we can look at the kinds
    of data we’ll need to build an LTR model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: We’ll focus on building LTR for production search systems, which can be quite
    different from a research context. We not only need relevant results, but results
    returned suitably quickly, with mainstream, well-understood search techniques.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'Conceptually, invoking LTR usually involves three high-level steps:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Training an LTR model
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploying the model to production
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the model to rank (or rerank) search results
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Most modern search engines support deploying ranking models directly into the
    search engine, allowing the LTR model to be invoked efficiently “where the data
    lives”. Usually, LTR models are significantly slower at ranking than basic keyword-based
    ranking functions like BM25, so LTR models are often only invoked for subsequent-pass
    ranking (or reranking) on a subset of the top search results ranked by an initial,
    faster ranking function. Pushing the LTR model into the engine (if supported)
    prevents the need to return hundreds or thousands of documents and their metadata
    from the search engine to an external model service for reranking, which can be
    slow and inefficient compared to doing the work in-engine and at scale.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, our `ltr` library in this chapter implements pluggable support
    for deploying and invoking each supported search engine or vector database’s native
    LTR model integration capabilities when available. The code in each listing will
    work with any supported engine (see appendix B to change it), but the listing
    output you’ll see in this chapter will reflect Solr’s LTR implementation, since
    Solr is configured by default. If you change the engine, you’ll see the output
    from your chosen engine when you run the Jupyter notebooks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Solr was one of the first major open source search engines to natively support
    LTR model serving, with the capabilities later being ported to a community-developed
    Elasticsearch LTR plugin ([https://github.com/o19s/elasticsearch-learning-to-rank](https://github.com/o19s/elasticsearch-learning-to-rank))
    and then forked to the OpenSearch LTR plugin ([https://github.com/opensearch-project/opensearch-learning-to-rank-base](https://github.com/opensearch-project/opensearch-learning-to-rank-base)).
    As such, the Elasticsearch and OpenSearch LTR plugins implement nearly identical
    concepts as those in Solr. Vespa implements phased ranking (reranking) and the
    ability to invoke models during each phase, and Weaviate also implements various
    reranking capabilities. Other engines that support native LTR will follow similar
    patterns.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 outlines the workflow for developing a practical LTR solution.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F01_Grainger.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 LTR systems transform our training data (judgment lists) into models
    that generalize relevance ranking. This type of system lets us find the underlying
    patterns in our training data.
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You may notice similarities between LTR and traditional machine learning–based
    classification or regression system workflows. But the exceptions are what make
    it interesting. Table 10.1 maps definitions between traditional machine learning
    objectives and LTR.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Table 10.1 Traditional machine learning vs. LTR
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Concept | Traditional machine learning | LTR |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| Training data  | Set of historical or “true” examples the model should try
    to predict, e.g., stock prices on past days, like “Apple” was $125 on June 6th,
    2021\.  | A *judgment list*: A *judgment* simply labels a document as relevant
    or irrelevant for a query. In figure 10.2, *Return of the Jedi* is labeled relevant
    ( `grade` of `1`), for the query `star wars`.  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| Feature  | The data we can use to predict the training data, e.g., Apple
    had 147,000 employees and revenue of $90 billion.  | Data used so that relevant
    results rank higher than irrelevant ones and, ideally, values the search engine
    can compute quickly. Our features are search queries like `title:({keywords})`
    from listing 10.2\.  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| Model  | The algorithm that takes features as input to make a prediction.
    Given that Apple has 157,000 employees on July 6th, 2021, with $95 billion in
    revenue, the model might predict a stock price of $135 for that date.  | Combines
    the ranking features (search queries) together to assign a relevance *score* to
    each potential search result. Results are sorted by score descending, hopefully
    placing more relevant results first.  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: 'This chapter follows the steps in figure 10.1 to train an LTR model:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '*Gather judgments*—We derive judgments from clicks or other sources. We’ll
    cover this step in depth in chapter 11\.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Feature logging*—To train a model, we must combine the judgments with features
    to see the overall pattern. This step requires us to ask the search engine to
    store and compute queries representing the features.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*特征记录*——为了训练一个模型，我们必须将判断与特征结合起来，以查看整体模式。这一步骤需要我们要求搜索引擎存储和计算代表特征的查询。'
- en: '*Transform to a traditional machine learning problem*—You’ll see that most
    LTR really is about translating the ranking task into something that looks more
    like the “traditional machine learning” column in table 10.1\.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*转换为传统的机器学习问题*——你会发现，大多数LTR实际上是将排名任务转换为类似于表10.1中“传统机器学习”列的东西。'
- en: '*Train and evaluate the model*—Here we construct our model and confirm that
    it is, indeed, generalizable, and thus will perform well for queries it hasn’t
    seen.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*训练和评估模型*——在这里，我们构建我们的模型，并确认它确实是可泛化的，因此对于它尚未看到的查询将表现良好。'
- en: '*Store the model*—We upload the model to our search infrastructure, tell the
    search engine which features to use as input, and enable it for users to use in
    their searches.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*存储模型*——我们将模型上传到我们的搜索基础设施，告诉搜索引擎哪些特征作为输入使用，并启用用户在他们的搜索中使用它。'
- en: '*Search using the model*—We finally can execute searches using the model!'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用模型进行搜索*——我们终于可以使用模型进行搜索了！'
- en: The rest of the chapter will walk through each of these steps in detail to build
    our first LTR implementation. Let’s get cracking!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分将详细介绍这些步骤，以构建我们的第一个LTR实现。让我们开始吧！
- en: '10.2 Step 1: A judgment list, starting with the training data'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 步骤 1：判断列表，从训练数据开始
- en: 'You already saw what LTR is at a high level, so let’s get into the nitty-gritty.
    Before implementing LTR, we must first learn about the data used to train an LTR
    model: the judgment list.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经从高层次了解了LTR是什么，那么让我们深入了解。在实现LTR之前，我们首先必须了解用于训练LTR模型的训练数据：判断列表。
- en: A *judgment list* is a list of relevance labels or *grades*, each indicating
    the relevance of a document to a query. Grades can come in a variety of forms.
    For now, we’ll stick to simple *binary judgments*—a `0` indicates an irrelevant
    document and a `1` indicates a relevant one.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*判断列表*是一系列相关性标签或评分，每个标签都指示一个文档与查询的相关性。评分可以有多种形式。目前，我们将坚持简单的*二元判断*——`0`表示无关文档，而`1`表示相关文档。'
- en: 'Using the `Judgment` class provided with this book’s code, we’ll label *The
    Social Network* as relevant for the query `social network` by creating a `Judgment`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本书代码提供的`Judgment`类，我们将通过创建一个`Judgment`将《社交网络》标记为`social network`查询的相关：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It’s more interesting to look over multiple queries. In listing 10.3, we have
    `social network` and `star wars` as two different queries, with movies graded
    as relevant or irrelevant.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 查看多个查询更有趣。在列表10.3中，我们将`social network`和`star wars`作为两个不同的查询，对电影进行相关或不相关的评分。
- en: Listing 10.3 Labeling movie judgments as relevant or irrelevant
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.3 标记电影判断为相关或不相关
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can see that we labeled *Star Trek into Darkness* and *Battlestar Galactica*
    as irrelevant for the query `star wars`, but *Return of the Jedi* as relevant.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们将《星际迷航：暗黑无界》和《星际迷航：银河系漫游指南》标记为与查询`star wars`无关，但将《星球大战：绝地归来》标记为相关。
- en: You’re hopefully asking yourself “where did these grades come from?” Hand labeled
    by movie experts? Based on user clicks? Good questions! Creating a good training
    set, based on user interactions with search results, is crucial for getting LTR
    to work well. To get training data in bulk, we usually derive these labels from
    click traffic using a type of algorithm known as a *click model*. As this step
    is so foundational, we’ll dedicate all of chapter 11 to diving deeper into the
    topic. In this chapter, however, we’ll start with manually labeled judgments so
    we can initially focus on the mechanics of LTR.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道“这些评分是从哪里来的？”是由电影专家手工标记的吗？基于用户点击的吗？好问题！基于用户与搜索结果交互创建一个好的训练集对于LTR良好工作至关重要。为了大量获取训练数据，我们通常使用一种称为*点击模型*的算法从点击流量中提取这些标签。由于这一步骤非常基础，我们将用整个第11章深入探讨这个主题。然而，在这一章中，我们将从手动标记的判断开始，以便我们最初可以专注于LTR的机制。
- en: Each judgment also has a `features` vector, which can be used to train a model.
    The first feature in the `features` vector could be made to correspond to the
    `title` BM25 score, the second to the `overview` BM25 score, and so on. We haven’t
    populated the `features` vectors yet, so if you inspect `sample_judgments[0].features`,
    it’s currently empty (`[]`).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 每个判断还有一个`features`向量，可以用来训练模型。`features`向量的第一个特征可以对应于`title` BM25得分，第二个对应于`overview`
    BM25得分，依此类推。我们还没有填充`features`向量，所以如果您检查`sample_judgments[0].features`，它目前是空的（`[]`）。
- en: Let’s use the search engine to gather some features.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用搜索引擎来收集一些特征。
- en: '10.3 Step 2: Feature logging and engineering'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 步骤2：特征记录和工程
- en: Feature engineering requires identifying patterns between document attributes
    and relevance. For example, we might hypothesize that “relevant results in our
    judgments correspond to strong title matches”. In this case, “title match” would
    be a feature we’d need to define. In this section, you’ll see what features (like
    “title match”) are and how to use a modern search engine to engineer and extract
    these features from a corpus.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程需要识别文档属性和相关性之间的模式。例如，我们可能会假设“我们判断中的相关结果对应于强大的标题匹配”。在这种情况下，“标题匹配”将是我们需要定义的特征。在本节中，您将了解什么是特征（如“标题匹配”），以及如何使用现代搜索引擎从语料库中构建和提取这些特征。
- en: 'For the purposes of LTR, a *feature* is some numerical attribute of the document,
    the query, or the query-document relationship. Features are the mathematical building
    blocks we use to build a ranking function. You’ve already seen a manual ranking
    function with features in listing 10.2: the keyword score in the `title` field
    is one such feature, as are the `release_year` and `overview` keyword scores:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LTR的目的，一个**特征**是文档、查询或查询-文档关系的某些数值属性。特征是我们用来构建排名函数的数学构建块。您已经看到了一个带有特征的手动排名函数，如列表10.2中的关键字得分在`title`字段中就是一个这样的特征，同样还有`release_year`和`overview`关键字得分：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Of course, the features you end up using could be more complex or domain-specific,
    such as the commute distance in a job search, or some knowledge graph relationship
    between query and document. Anything you can compute relatively quickly when a
    user searches might be a reasonable feature.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你最终使用的功能可能更复杂或更具有领域特定性，例如在求职中通勤的距离，或者查询与文档之间的某些知识图谱关系。当用户搜索时，任何可以相对快速计算的内容都可能是一个合理的特征。
- en: '*Feature logging* takes a judgment list and computes features for each labeled
    query-document pair. If we computed the values of each component of listing 10.2
    for the query `social network`, we would arrive at something like table 10.2.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征记录**从判断列表中计算每个标记的查询-文档对的特征。如果我们为查询`social network`计算列表10.2中每个组件的值，我们就会得到类似于表10.2的内容。'
- en: Table 10.2 Features logged for the keywords `social network` for relevant (`grade=1`)
    and irrelevant (`grade=0`) documents
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表10.2 为关键词“social network”记录的相关（`grade=1`）和不相关（`grade=0`）文档的特征
- en: '| Grade | Movie | **`title:({keywords})`** | **`overview: ({keywords})`** |
    **`{!func}release_year`** |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 等级 | 电影 | **`title:({keywords})`** | **`overview: ({keywords})`** | **`{!func}release_year`**
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1  | Social Network  | 8.243603  | 3.8143613  | 2010.0  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 1  | Social Network  | 8.243603  | 3.8143613  | 2010.0  |'
- en: '| 0  | #chicagoGirl  | 0.0  | 6.0172443  | 2013.0  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 0  | #chicagoGirl  | 0.0  | 6.0172443  | 2013.0  |'
- en: '| 0  | Life As We Know It  | 0.0  | 4.353118  | 2010.0  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 0  | Life As We Know It  | 0.0  | 4.353118  | 2010.0  |'
- en: '| 0  | The Cheyenne Social Club  | 3.4286604  | 3.1086721  | 1970.0  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 0  | The Cheyenne Social Club  | 3.4286604  | 3.1086721  | 1970.0  |'
- en: A machine learning algorithm might examine the feature values from table 10.2
    and converge on a good ranking function. From just the data in table 10.2, it
    seems such an algorithm might produce a ranking function with a higher weight
    for the `title` feature and lower weights for the other features.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法可能会检查表10.2中的特征值，并收敛到一个好的排名函数。仅从表10.2中的数据来看，这样的算法可能会产生一个对`title`特征赋予更高权重而对其他特征赋予更低权重的排名函数。
- en: Before we get to the algorithms, however, we need to examine the feature logging
    workflow in a production search system.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们到达算法之前，我们需要检查生产搜索系统中的特征记录工作流程。
- en: 10.3.1 Storing features in a modern search engine
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 在现代搜索引擎中存储特征
- en: Modern search engines that support LTR help us store, manage, and extract features.
    Engines like Solr, Elasticsearch, and OpenSearch track features in a *feature
    store*—a list of named features. It’s crucial that we log features for training
    in a manner consistent with how the search engine will execute the model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 支持LTR的现代搜索引擎帮助我们存储、管理和提取特征。像Solr、Elasticsearch和OpenSearch这样的引擎在*特征存储*中跟踪特征——一个命名特征的列表。我们以与搜索引擎执行模型一致的方式记录用于训练的特征至关重要。
- en: 'As shown in listing 10.4, we generate and upload features to the search engine.
    We use a generic feature store abstraction in the book’s codebase, allowing us
    to generate various search-based features and upload them as a *feature set* to
    the feature store of a supported search engine. Here we create three features:
    a title field relevance score `title_bm25`, an overview field relevance score
    `overview_bm25`, and the value of the `release_year` field. BM25 here corresponds
    to the BM25-based scoring defined in chapter 3, which will be our default method
    for scoring term matches in text fields.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表10.4所示，我们生成并上传特征到搜索引擎。我们在本书的代码库中使用通用的特征存储抽象，允许我们生成各种基于搜索的特征，并将它们作为*特征集*上传到支持搜索引擎的特征存储。在这里，我们创建了三个特征：标题字段的相关度分数`title_bm25`、概述字段的相关度分数`overview_bm25`以及`release_year`字段的值。这里的BM25对应于第3章中定义的基于BM25的评分，这将成为我们在文本字段中评分词匹配的默认方法。
- en: Listing 10.4 Creating three features for LTR
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.4 创建LTR的三个特征
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Engine-specific feature set definition (for `engine=solr`):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 特定于引擎的特征集定义（对于`engine=solr`）：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 The name of the feature'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 特征的名称'
- en: '#2 The feature store where the feature will be saved'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将特征保存到特征存储'
- en: '#3 A parametrized feature taking the keywords (e.g., star wars) and searching
    the title field'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 一个参数化特征，接受关键词（例如，星球大战）并搜索标题字段'
- en: '#4 Another feature that searches against the overview field'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 另一个针对概述字段进行搜索的特征'
- en: '#5 A document-only feature, the release_year of the movie'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 仅文档特征，电影的发布年份'
- en: '#6 params are the same params for a Solr query, allowing you to use the full
    power of Solr’s extensive Query DSL to craft features.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 params是与Solr查询相同的参数，允许您使用Solr广泛的查询DSL的全部功能来构建特征。'
- en: 'The output of listing 10.4 shows the feature set that’s uploaded to the search
    engine—in this case, a Solr feature set. This output will obviously look different
    based on which search engine implementation you configure (as discussed in appendix
    B). The first two features are parameterized: they each take the search keywords
    (`social` `network`, `star wars`) and execute a search on the corresponding field.
    The final one is a field value feature utilizing the release year of a movie,
    which will boost more recent movies higher.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.4的输出显示了上传到搜索引擎的特征集——在本例中，是一个Solr特征集。根据你配置的搜索引擎实现（如附录B所述），此输出将明显不同。前两个特征是参数化的：它们各自接受搜索关键词（例如，“社交网络”、“星球大战”）并在相应的字段上执行搜索。最后一个是一个利用电影发布年份的字段值特征，这将使较新的电影排名更高。
- en: 10.3.2 Logging features from our search engine corpus
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2 从我们的搜索引擎语料库记录特征
- en: With features loaded into the search engine, our next focus will be to log features
    for every row in our judgment list. After we get this last bit of plumbing out
    of the way, we will then train a model that can observe relationships between
    each relevant and irrelevant document for each query.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在将特征加载到搜索引擎后，我们的下一个重点是记录我们判断列表中每一行的特征。在我们完成最后一部分管道之后，我们将训练一个模型，该模型可以观察每个查询中每个相关和不相关文档之间的关系。
- en: For each unique query in our judgment list, we need to extract the features
    for the query’s graded documents. For the query `social network` in the sample
    judgment list from listing 10.3, we have one relevant document (37799) and three
    irrelevant documents (267752, 38408, and 28303).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们判断列表中的每个唯一查询，我们需要提取查询评分文档的特征。对于列表10.3中样本判断列表中的查询“社交网络”，我们有一个相关文档（37799）和三个不相关文档（267752、38408和28303）。
- en: The following listing shows an example of feature logging for the query `social
    network`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了查询“社交网络”的特征记录示例。
- en: Listing 10.5 Logging feature values for `social network` results
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.5 记录“社交网络”结果的特征值
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Relevant and irrelevant documents for the “social network” query'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 “社交网络”查询的相关和不相关文档'
- en: '#2 Queries the search engine for feature values contained in the movies feature
    store'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 查询包含在电影特征存储中的特征值'
- en: 'Engine-specific search request (for `engine=solr`):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 特定于引擎的搜索请求（对于`engine=solr`）：
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Example Solr query syntax to retrieve feature values from each returned
    document'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 示例Solr查询语法，用于从每个返回的文档中检索特征值'
- en: 'Documents with logged features:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 带有日志特征的文档：
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Each feature value logged for this movie for the “social network” query'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 为“社交网络”查询记录的此电影的特征值'
- en: Notice that the search request (for Solr in this case) in listing 10.5 has a
    return field containing square brackets. This syntax tells Solr to return an extra
    field on each document containing the feature data defined in the feature store
    (the `movies` feature store in this case). The `efi` parameter stands for *external
    feature information*, and it’s used here to pass the keyword query (`social network`)
    and any additional query-time information needed to compute each feature. The
    response contains the four requested documents with their corresponding features.
    These parameters will be different for each search engine, but the concepts will
    be similar.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到列表10.5中的搜索请求（在本例中为Solr）包含一个包含方括号的返回字段。这种语法告诉Solr在每份文档上返回一个额外的字段，包含特征存储库中定义的特征数据（在本例中为`movies`特征存储库）。`efi`参数代表*外部特征信息*，它在这里用于传递关键字查询（`social
    network`）以及计算每个特征所需的任何附加查询时间信息。响应包含四个请求的文档及其相应的特征。这些参数对于每个搜索引擎都不同，但概念将是相似的。
- en: 'With some mundane Python data transformation, we can fill in the features for
    the query `social network` in our training set from this response. In listing
    10.6, we apply feature data to judgments for the query `social network`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一些平凡的Python数据转换，我们可以从响应中填充训练集中查询`social network`的特征。在列表10.6中，我们将特征数据应用于查询`social
    network`的判断：
- en: Listing 10.6 Judgments with logged features for query `social network`
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.6 对于查询`social network`的带有日志特征的判断
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Judgment for the movie The Social Network relative to the “social network”
    query, including the logged feature values'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 与“社交网络”查询相关的电影《社交网络》的判断，包括日志特征值'
- en: '#2 An irrelevant document for the “social network” query (note the low first
    feature value, the title_bm25 score of 0.0)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 对于“社交网络”查询的不相关文档（注意第一个特征值的低值，标题BM25得分为0.0）'
- en: In listing 10.6, as we might expect, the first feature value corresponds to
    the first feature in our feature store (`title_bm25`), the second value to the
    second feature in our feature store (`overview_bm25`), and so on. Let’s repeat
    the process of logging features for judgments for the query `star wars`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表10.6中，正如我们可能预期的，第一个特征值对应于我们特征存储库中的第一个特征（`title_bm25`），第二个值对应于我们特征存储库中的第二个特征（`overview_bm25`），依此类推。让我们重复对查询`star
    wars`的判断进行特征日志记录的过程。
- en: Listing 10.7 Logged judgments for the query `star wars`
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.7 对于查询`star wars`的日志判断
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: With the ability to generate logged judgments, let’s expand the judgment list
    to about a hundred movie queries, each with about 40 movies graded as relevant/irrelevant.
    Code for loading and logging features for this larger training set essentially
    repeats the search engine request shown in listing 10.5\. The end result of this
    feature logging looks just like listing 10.7, but created from a much larger judgment
    list.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通过生成日志判断的能力，让我们将判断列表扩展到大约一百个电影查询，每个查询大约有40部电影被标记为相关/不相关。加载和记录这个更大训练集特征的代码基本上重复了列表10.5中显示的搜索引擎请求。特征记录的最终结果看起来就像列表10.7，但来自一个更大的判断列表。
- en: We’ll move on next to consider how to handle the problem of ranking as a machine
    learning problem.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将考虑如何将排名问题作为一个机器学习问题来处理。
- en: '10.4 Step 3: Transforming LTR to a traditional machine learning problem'
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 步骤3：将LTR转换为传统机器学习问题
- en: In this section, we’re going to explore ranking as a machine learning problem.
    This will help us understand how to apply well-known, traditional machine learning
    concepts to our LTR task.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨将排名作为一个机器学习问题。这将帮助我们理解如何将众所周知的传统机器学习概念应用于我们的LTR任务。
- en: The task of LTR is to look over many relevant and irrelevant training examples
    for a query and then build a model to bring more relevant documents to the top
    (and conversely push less relevant documents down). Each training example doesn’t
    have much value by itself; what matters is how it’s ordered alongside its peers
    in a query. Figure 10.2 shows this task, with two queries. The goal is to find
    a scoring function that can use the features to correctly order results.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: LTR的任务是在许多相关和不相关的训练示例中查找查询，然后构建一个模型，将更多相关文档置于顶部（反之，将不太相关的文档推到底部）。每个训练示例本身并没有多少价值；重要的是它在查询中与同侪的排序。图10.2展示了这个任务，有两个查询。目标是找到一个评分函数，可以使用特征来正确排序结果。
- en: '![figure](../Images/CH10_F02_Grainger.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F02_Grainger.png)'
- en: Figure 10.2 LTR is about placing each query’s result set in the ideal order,
    not about predicting individual relevance grades. That means we need to look at
    each query as a case unto itself.
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.2 LTR是关于将每个查询的结果集放置在理想顺序中，而不是关于预测单个相关性等级。这意味着我们需要将每个查询视为一个独立的案例。
- en: 'Contrast LTR with a more traditional pointwise machine learning task: a task
    like predicting a company’s stock price as mentioned in table 10.2 earlier. *Pointwise
    machine learning* means that we can evaluate the model’s accuracy on each example
    in isolation, predicting its absolute value as opposed to its relative value versus
    other examples. We know, just by looking at one company, how well we predicted
    that company’s stock price. Compare figure 10.3 showing a pointwise task to figure
    10.2\. Notice in figure 10.3 that the learned function attempts to predict the
    stock price directly, whereas with LTR, the function’s output is only meaningful
    for ordering items relative to their peers for a query.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 将LTR与一个更传统的点wise机器学习任务进行对比：例如，预测公司在表10.2中提到的股价。*Pointwise机器学习*意味着我们可以独立评估模型在每个示例上的准确性，预测其绝对值而不是与其他示例的相对值。仅通过观察一家公司，我们就能知道我们预测该公司股价的准确性。将图10.3显示的点wise任务与图10.2进行比较。注意在图10.3中，学习到的函数试图直接预测股价，而LTR中，函数的输出仅对查询中相对于其同侪的排序有意义。
- en: '![figure](../Images/CH10_F03_Grainger.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F03_Grainger.png)'
- en: Figure 10.3 Pointwise machine learning tries to optimize predictions of individual
    points (such as a stock price or the temperature). Search relevance is a different
    problem than pointwise prediction. Instead, we need to optimize a ranking of examples
    grouped by a search query.
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.3 Pointwise机器学习试图优化单个点（如股价或温度）的预测。搜索相关性是一个与点wise预测不同的问题。相反，我们需要优化由搜索查询分组的示例的排序。
- en: LTR targets a very different objective (ranking multiple results) than pointwise
    machine learning (predicting specific values of results). Most LTR methods use
    clever alchemy to transmogrify this “ranking of pairs” task into a classification
    task per document that learns to predict which features and feature weights best
    separate “relevant” from “irrelevant” documents. This transformation is the key
    to building a generalizable LTR model that can operate on specific documents as
    opposed to only pairs of documents. We’ll look at one model’s method for transforming
    the ranking task in the next section by exploring a popular LTR model named SVMrank.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: LTR的目标与pointwise机器学习（预测结果的具体值）非常不同（对多个结果进行排序）。大多数LTR方法使用巧妙的炼金术将这个“成对排序”任务转化为每个文档的分类任务，该任务学习预测哪些特征和特征权重最能区分“相关”文档和“不相关”文档。这种转换是构建一个可推广的LTR模型的关键，该模型可以针对特定文档操作，而不仅仅是文档对。我们将在下一节通过探索一个名为SVMrank的流行LTR模型来查看一个模型转换排序任务的方法。
- en: '10.4.1 SVMrank: Transforming ranking to binary classification'
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.1 SVMrank：将排序转换为二元分类
- en: 'At the core of LTR is the model: the actual algorithm that learns the relationship
    between relevance/irrelevance and the features like `title_bm25`, `overview_bm25`,
    etc. In this section, we’ll explore one such model, SVMrank, first understanding
    what “SVM” stands for and then how it can be used to build a great, generalizable
    LTR model.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: LTR的核心是模型：学习相关性/非相关性与`title_bm25`、`overview_bm25`等特征之间关系的实际算法。在本节中，我们将探讨这样一个模型，SVMrank，首先了解“SVM”代表什么，然后了解它如何被用来构建一个优秀且可推广的LTR模型。
- en: SVMrank transforms relevance into a binary classification problem. *Binary classification*
    simply means classifying items as one of two classes (like “relevant” versus “irrelevant”,
    “adult” versus “child”, “dog” versus “cat”) using the available features.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: An *SVM* or *support vector machine* is one method of performing binary classification.
    We won’t go in-depth into SVMs, as you need not be a machine learning expert to
    follow the discussion. Nevertheless, if you want to get a deeper overview of SVMs,
    you can look at a book such as *Grokking Machine Learning* by Luis Serrano (Manning,
    2021).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, an SVM finds the best, most generalizable hyperplane to draw between
    the two classes. A *hyperplane* is a boundary that separates a vector space into
    two parts. A 1D point can be a hyperplane separating a 2D line into two parts,
    just as a line can be a hyperplane separating a 3D space into two parts. A plane
    is usually a 3D boundary separating a 4D space. All of these, as well as boundaries
    even greater than three dimensions are generically referred to as hyperplanes.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: As an example, if we were trying to build a model to predict whether an animal
    is a dog or cat, we might look at a 2D graph of the heights and weights of known
    dogs or cats and draw a line separating the two classes as shown in figure 10.4.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F04_Grainger.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4 SVM example: Is an animal a dog or a cat? This hyperplane (the
    line here) separates these two cases based on two features: height and weight.
    Soon you’ll see how we might do something similar to separate relevant and irrelevant
    search results for a query.'
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A good separating hyperplane drawn between the classes attempts to minimize
    the mistakes it makes in classifying the training data (fewer dogs on the cat
    side and vice versa). We also want a hyperplane that is *generalizable*, meaning
    that it will probably do a good job of classifying animals that weren’t seen during
    training. After all, what good is a model if it can’t make predictions about new
    data? It wouldn’t be very AI-powered!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Another detail to know about SVMs is that they can be sensitive to the range
    of our features. For example, imagine if the `height` feature was millimeters
    instead of centimeters, like in figure 10.5\. It forces the data to stretch out
    on the *x*-axis, and the separating hyperplane looks quite different!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F05_Grainger.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 Separating hyperplane affected by the range of one of the features.
    This causes SVMs to be sensitive to the range of features, and thus we need to
    normalize the features so one feature doesn’t create undue influence on the model.
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: SVMs work best when our data is normalized. *Normalization* just means scaling
    features to a comparable range. We’ll normalize our data by mapping `0` to the
    mean of the feature values. If the average `release_year` is `1990`, movies released
    in 1990 will normalize to `0`. We’ll also map `+1` and `-1` to one standard deviation
    above or below the mean. So if the standard deviation of movie release years is
    22 years, then movies in 2012 turn into a `1.0`; movies in 1968 turn into a `-1.0`.
    We can repeat this for `title_bm25` and `overview_bm25` using those features’
    means and standard deviations in our training data. This helps make the features
    a bit more comparable when finding a separating hyperplane.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: With that brief background out of the way, let’s now explore how SVMrank can
    create a generalizable model to distinguish relevant from irrelevant documents,
    even for queries it has never seen before.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.2 Transforming our LTR training task to binary classification
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With LTR, we must reframe the task from ranking to a traditional machine learning
    task. In this section, we’ll explore how SVMrank transforms ranking into a binary
    classification task suitable for an SVM.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Before we get started, let’s inspect the fully logged training set from the
    end of step 2 for our two favorite queries, `star` `wars` and `social` `network`.
    In this section, we’ll focus on just two features (`title_bm25` and `overview_bm25`)
    to help us explore feature relationships graphically. Figure 10.6 shows these
    two features for every graded document for the `star` `wars` and `social` `network`
    queries, labeling some prominent movies from the training set.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F06_Grainger.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 Logged feature scores for `social network` and `star wars` queries
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: First, normalize the LTR features
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our first step is to normalize each feature. The following listing takes the
    logged output from step 2 and normalizes features into `normed_judgments`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.8 Normalizing logged LTR training data
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Output:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Unnormalized example, with raw title_bm25, overview_bm25, and release_year'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Same judgment, but normalized'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the output from listing 10.8 shows first the logged BM25 scores
    for title and overview (`8.244`, `3.814`) alongside the release year (`2010`).
    These features are then normalized, where `8.244` for `title_bm25` corresponds
    to `4.483` standard deviations above the mean `title_bm25`, and so on for each
    feature.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: We’ve plotted the normalized features in figure 10.7\. This looks very similar
    to figure 10.6, with only the scale on each axis differing.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F07_Grainger.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 Normalized `star wars` and `social network` graded movies. Each
    increment in the graph is a standard deviation above or below the mean.
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Next, we’ll turn ranking into a binary classification learning problem to separate
    the relevant from irrelevant results.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Second, compute the pairwise differences
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With normalized data, we’ve forced features to a consistent range. Now our SVM
    should not be biased by features that happen to have very large ranges. In this
    section, we’re ready to transform the task into a binary classification problem,
    setting the stage for us to train our model.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: SVMrank uses a pairwise transformation to reformulate LTR to a binary classification
    problem. *Pairwise* simply means turning ranking into the task of minimizing out-of-order
    pairs for a query.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: In the rest of this section, we’ll carefully walk through SVMrank’s pairwise
    algorithm, outlined in listing 10.9\. The SVMrank algorithm takes every judgment
    for each query and compares it to every other judgment for that same query. It
    computes the feature differences (`feature_deltas`) between every relevant and
    irrelevant pair for that query. When adding to `feature_deltas`, if the first
    judgment is more relevant than the second, it’s labeled with a `+1` in `predictor_deltas`.
    If the first judgment is less relevant, it is labeled with a `-1`. This pairwise
    transform algorithm yields training data (the `feature_deltas` and `predictor_deltas`)
    needed for binary classification.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.9 Transforming features into pairwise data for SVMrank
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Stores a label of +1 if doc1 is more relevant than doc2.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Stores the feature deltas'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Stores a label of –1 if doc1 is less relevant than doc2.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Stores the feature deltas'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.8 plots the pairwise differences and highlights important points.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F08_Grainger.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 Pairwise differences after SVMrank’s transformation for `social`
    `network` and `star wars` documents, along with a candidate separating hyperplane.
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You’ll notice that the positive pairwise deltas (+) tend to be toward the upper
    right. This means relevant documents have a higher `title_bm25` and `overview_bm25`
    when compared to irrelevant ones.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: That’s a lot to digest! Let’s walk through a few examples carefully, step-by-step,
    to see how this algorithm constructs the data points in figure 10.9\. This algorithm
    compares relevant and irrelevant documents for each query, comparing two documents
    (*Network* and *The Social Network*) within the query `social network` as shown
    in figure 10.9.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F09_Grainger.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 Comparing *Network* to *The Social Network* for the query `social`
    `network`
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'These are the features for *The Social Network*:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 title_bm25 is 4.483 standard deviations above the mean, and overview_bm25
    is 2.100 standard deviations above the mean.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the features for *Network*:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 title_bm25 is 3.101 standard deviations above the mean, and overview_bm25
    is 1.443 standard deviations above the mean.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: We then insert the delta between *The Social Network* and *Network* in the following
    listing.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.10 Calculating and storing the feature delta
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 Adds [1.382, 0.657] to feature_deltas'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: To restate listing 10.10, we might say that here is one example of a movie,
    *The Social Network*, that’s more relevant than the movie *Network* for this query
    `social network`. Interesting! Let’s look at what makes them different. Of course,
    “difference” in math means subtraction, which we’ll do here. Ah yes, after taking
    the difference we see *The Social Network’*s `title_bm25` is `1.382` standard
    deviations higher than *Network’*s; similarly, the `overview_bm25` is `0.657`
    standard deviations higher. Indeed, note the `+` for *The Social Network* minus
    *Network* in figure 10.8 showing the point `[1.382, 0.657]` amongst the deltas.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm would also note that *Network* is less relevant than *The Social
    Network* for the query `social network`, as shown in figure 10.10.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F10_Grainger.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 Comparing *Network* to *The Social Network* for the query `social
    network`
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Just as in listing 10.9, our code captures this difference in relevance between
    these two documents, but this time in the opposite direction (irrelevant-minus-relevant).
    So it’s no surprise that we see the same values, but in the negative.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '#1 Evaluates to [–1.382, –0.657]'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: In figure 10.11, we move on to another relevant-irrelevant comparison of two
    documents for the query `social network`, appending another comparison to the
    new training set.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.11 shows appending both positive deltas (with the more relevant document
    listed first) and negative deltas (with the less relevant document listed first)
    for the highlighted pair of documents compared in figure 10.11.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.11 Adding the positive and negative deltas
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#1 Evaluates to [2.249, 2.544]'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Evaluates to [–2.249, –2.544]'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F11_Grainger.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 Comparing *Social Genocide* to *The Social Network* for the query
    `social network`
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Once we iterate through every pairwise difference between documents matching
    the query `social network` to create a pointwise training set, we can move on
    to also logging differences for other queries. Figure 10.12 shows differences
    for a second query, this time comparing the relevance of documents matching the
    query `star wars`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F12_Grainger.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.12 Comparing *Rogue One: A Star Wars Movie* to *Star!* for the query
    `star wars`. We’ve moved on from `social network` and have begun to look at patterns
    within another query.'
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#1 Rogue One features minus Star! features'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Star! features minus Rogue One features'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: We continue this process of calculating differences between feature values for
    relevant versus irrelevant documents until we have calculated all the pairwise
    differences for our training and test queries.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: You can see back in figure 10.8 that the positive examples show a positive `title_bm25`
    delta, and possibly a slightly positive `overview_bm25` delta. This becomes even
    more clear if we calculate deltas over the full dataset of 100 queries, as shown
    in figure 10.13.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F13_Grainger.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 Full training set with a hyperplane separating relevant from irrelevant
    documents. We see a pattern! Relevant documents have a higher `title_bm25` and
    perhaps a modestly higher `overview_bm25`.
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Interesting! It is now very easy to visually identify that a larger `title_bm25`
    score match is highly correlated with a document being relevant for a query, and
    that having a higher `overview_bm25` score is at least somewhat positively correlated.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth taking a step back now and asking whether this formulation of ranking
    is appropriate for your domain. Different LTR models have their own method of
    mapping pairwise comparisons into classification problems as needed. As another
    example, LambdaMART—a popular LTR algorithm based on boosted trees—uses pairwise
    swapping and measures the change in *discounted cumulative gain* (DCG).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Next up, we’ll train a robust model to capture the patterns in our fully transformed
    ranking dataset.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '10.5 Step 4: Training (and testing!) the model'
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Good machine learning clearly requires a lot of data preparation. Luckily, you’ve
    arrived at the section where we actually train a model! With the `feature_deltas`
    and `predictor_deltas` from the last section, we now have a training set suitable
    for training a ranking classifier. This model will let us predict when documents
    might be relevant, even for queries and documents it hasn’t seen yet.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.1 Turning a separating hyperplane’s vector into a scoring function
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve seen how SVMrank’s separating hyperplane can classify and differentiate
    irrelevant examples from the relevant ones. That’s useful, but you may remember
    that our task is to find *optimal* weights for our features, not just to classify
    documents. Let’s therefore look at how we can *score* search results using this
    hyperplane.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that the separating hyperplane also gives us what we need to learn
    optimal weights. Any hyperplane is defined by the vector orthogonal to the plane.
    So when an SVM machine learning library does its work, it gives us a sense of
    the weights that each feature should have, as shown in figure 10.14.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F14_Grainger.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 Full training set with a candidate-separating hyperplane, showing
    the orthogonal vector defining the hyperplane.
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Think about what this orthogonal vector represents. This vector points in the
    direction of relevance! It says relevant examples are this way, and irrelevant
    ones are in the opposite direction. This vector *definitely* points to `title_bm25`
    having a strong influence on relevance, with some smaller influence coming from
    `overview_bm25`. This vector might be something like:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We used the pairwise transform algorithm in listing 10.9 to compute the deltas
    needed to perform classification between irrelevant and relevant examples. If
    we train an SVM on this data, as in the following listing, the model gives us
    the vector defining the separating hyperplane.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.12 Training a linear SVM with scikit-learn
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '#1 Creates a linear model with sklearn'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Fits to deltas using an SVM'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The vector that defines the separating hyperplane'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Listing 10.12 trains an SVM to separate the `predictor_deltas` (remember they’re
    `+1` and `-1`) using the corresponding `feature_deltas` (the deltas in the normalized
    `title_bm25`, `overview_bm25`, and `release_year` features). The resulting model
    is a vector orthogonal to the separating hyperplane. As expected, it shows a strong
    weight on `title_bm25`, a more modest one on `overview_bm25`, and a weaker weight
    on `release_year`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.2 Taking the model for a test drive
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'How does this model work as a ranking function? Let’s suppose the user enters
    the query `wrath of khan`. How might this model score the document *Star Trek
    II: The Wrath of Khan* relative to this query? The unnormalized feature vector
    indicates a strong title and overview match for this query.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '#1 Raw features for “Star Trek II”'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Normalizing it, each feature value is this many standard deviations above or
    below each feature’s mean:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#1 Normalized features for “Star Trek II”'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'We simply multiply each normalized feature with its corresponding `coef_` value.
    Summing them then gives us a relevance score:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '#1 Relevance score calculation for “Star Trek II”'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'How would this model rank *Star Trek III: The Search for Spock* relative to
    *Star Trek II: The Wrath of Khan* for our query `wrath of khan`? Hopefully not
    nearly as highly! Indeed, it doesn’t:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '#1 Raw features for “Star Trek III”'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Normalized features for “Star Trek III”'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Relevance calculation for “Star Trek III”'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The model seems to be correctly predicting the most relevant answer.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.3 Validating the model
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Testing a couple of queries helps us spot problems, but we’d prefer a more systematic
    way of checking if the model is generalizable.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: One difference between LTR and traditional machine learning is that we usually
    evaluate queries and entire result sets, not individual data points, to prove
    our model is effective. We’ll perform a test/training split at the query level.
    This will let us spot types of queries with problems. We’ll evaluate using a simple
    precision metric, counting the proportion of results in the top *K* (with `k=5`
    in our case) that are relevant. You should choose the relevance metric best suited
    to your own use case.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: First, we will randomly put our queries into a test or training set, as shown
    in the following listing.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.13 Simple test/training split at the query level
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#1 Identifies a random 10% of the judgments to go into the training set'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Places each judgment into training data (10%) or test set (90%)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: With the training data split out, we can perform the pairwise transform trick
    from step 3\. We can then retrain on just the training data.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.14 Train just on training data
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '#1 Fits only to training data'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: So far, we have held back the test data. Just like a good teacher, we don’t
    want to give the student all the answers. We want to see if the model has learned
    anything beyond rote memorization of the training examples.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: In the next listing, we evaluate our model using the test data. This code loops
    over every test query and ranks every test judgment using the model. It then computes
    the precision for the top four judgments.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.15 Can our model generalize beyond the training data?
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '#1 For each test query'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Scores each judgment and ranks this query using the model'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Compute the precisions for this query'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: On multiple runs, you should expect a precision of approximately 0.3–0.4\. Not
    bad for our first iteration, where we just guessed at a few features (`title_bm25`,
    `overview_bm25`, and `release_year`)!
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: In LTR, you can always look back at previous steps to see what might be improved.
    This precision test is the first time we’ve been able to systematically evaluate
    our model, so it’s a natural time to revisit the features to see how the precision
    might be improved in subsequent runs. Go all the way back up to step 2\. See what
    examples are on the wrong side of the separating hyperplane. For example, if you
    look back at figure 10.8, the third Star Wars movie, *Return of the Jedi*, fits
    a pattern of a relevant document that doesn’t have a keyword match in the title.
    In the absence of a title, what other features might be added to help capture
    that a movie belongs in a specific collection like Star Wars? Perhaps there is
    a property within the TMDB dataset that we could experiment with.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: For now, though, let’s take the model we just built and see how we can deploy
    it to production.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '10.6 Steps 5 and 6: Upload a model and search'
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll finally upload our model so that it can be applied to
    rank future search results. We’ll then discuss both applying the model to rank
    all documents, as well as applying it to rerank an already-run and likely more
    efficient initial query. Finally, we’ll discuss some of the performance implications
    of using LTR models in production.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 10.6.1 Deploying and using the LTR model
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Originally, we presented our objective as finding *ideal* boosts for a hardcoded
    ranking function like the one in listing 10.2:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This boosted query indeed multiplies each feature by a weight (the boost) and
    sums the results. But it turns out that we don’t want the search engine to multiply
    the *raw* feature values. Instead, we need the feature values to be normalized.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Many search engines let us store a linear ranking model along with feature normalization
    statistics. We saved the `means` and `std_devs` of each feature, which will be
    used to normalize values for any document being evaluated. These coefficients
    are associated with each feature when uploading the model, as shown in the next
    listing.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.16 Generating and uploading a linear model
  id: totrans-282
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Generated linear model (for `engine=solr`):'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '#1 Feature store to locate the features'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Which feature to execute before evaluating this model'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '#3 How to normalize this feature before applying the weight'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '#4 The weight of each feature in the model'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: The `response` from listing 10.16 is Solr-specific and will change depending
    on which search engine you have configured. Next, we can issue a search using
    the uploaded LTR model, as shown in the following listing.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.17 Ranking all documents with LTR model for `harry potter`
  id: totrans-291
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Engine-specific search request (for `engine=solr`):'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '#1 Executes our model over the maximum number of documents with the specified
    parameters'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'Returned documents:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In listing 10.17, the LTR model ranks all the documents in the corpus using
    the keywords in the `rerank_query` parameter as input to the model. Since no initial
    `query` parameter is specified in the request, no matching filter is applied to
    the collection before the search results (all documents) are ranked by the LTR
    model. Though scoring such a large number of documents with the model will lead
    to nontrivial latency, it allows us to test the model directly, absent of any
    other matching parameters.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Notice in listing 10.17 the use of the term “rerank” in the `rerank_query` parameter.
    As this term implies, LTR usually happens as a second ranking phase on results
    first calculated by a more efficient algorithm (such as BM25 and/or an initial
    Boolean match). This is to reduce the number of documents that must be scored
    by the more expensive LTR model. The following listing demonstrates executing
    a baseline search and then reranking the top `500` results with the LTR model.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.18 Searching for `harry potter` and reranking with the model
  id: totrans-300
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Engine-specific search request (for `engine=solr`):'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '#1 First-pass Solr query—a simple keyword query with BM25 ranking'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Reranks only the top 500 documents'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Returned documents:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This request is much faster, and it still yields the same top results when performing
    the cheaper initial BM25 ranking on the filtered `query` followed by the more
    expensive LTR-based reranking on just the top `500` results.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 10.6.2 A note on LTR performance
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As you can see, many steps are required to build a real-world LTR model. Let’s
    close the chapter with some additional thoughts on practical performance constraints
    in LTR systems:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '*Model complexity*—The more complex the model, the more accurate it *might*
    be. A simpler model can be faster and easier to understand, though perhaps less
    accurate. Here we’ve stuck to a very simple model (a set of linear weights). Imagine
    a complex deep-learning model—how well would that work? Would the complexity be
    worth it? Would it be as generalizable (or could it possibly be more generalizable)?'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rerank depth*—The deeper you rerank, the more you might find additional documents
    that could be hidden gems. On the other hand, the deeper you rerank, the more
    compute cycles your model spends scoring results in your live search engine cluster.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Feature complexity*—If you compute very complex features at query time, they
    might help your model. However, they’ll slow down evaluation and search response
    time.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Number of features*—A model with many features might lead to higher relevance.
    However, it will also take more time to compute every feature on each document,
    so ask yourself which features are crucial. Many academic LTR systems use hundreds.
    Practical LTR systems usually boil these down to dozens. You will almost always
    see diminishing returns for relevance ranking and rising compute and latency costs
    as you continue adding additional features, so prioritizing which features to
    include is important.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-encoders
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A cross-encoder is a specialized kind of machine-learned ranking model. Cross-encoders
    are trained to score the relevance of two pieces of input (usually text), such
    as a query and a document. They use a Transformer architecture to combine both
    pieces of input into a single representation, which is then used in search to
    rank the relevance of the document for the query based upon interpreting both
    the query and document within their shared semantic context. Cross-encoders are
    ranking classifiers, like other LTR models, but they are unique in that they are
    pretrained on a large amount of data and are generally only focused on the textual
    similarity between the query and document instead of other features like popularity,
    recency, or user behavior. While they can be fine-tuned on your dataset, they
    are often used as is, since they are already trained on a large amount of data
    and can generalize well to new textual inputs.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Cross-encoders are very easy to use out of the box, and they’re often the easiest
    way to get started with machine-learned ranking without having to do your own
    training. Cross-encoders tend to be slow, so they’re not typically used to rerank
    large numbers of documents. Our focus in this chapter and in the coming chapter
    is on more flexible models that can use reflected intelligence, including those
    trained on your users’ judgments and implicit judgments from user signals, but
    it’s good to be familiar with cross-encoders, as they are a popular choice for
    many search teams, particularly when just getting started. We’ll cover cross-encoders
    in more detail, with example code, in section 13.7\.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 10.7 Rinse and repeat
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Congrats! You’ve done one full cycle of LTR! Like many data problems, though,
    you’ll likely need to continue iterating on the problem. There’s always something
    new you can do to improve.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'On your second iteration, you might consider the following:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '*New and better features*—Are there types of queries or examples on which the
    model performs poorly, such as `title` searches where there’s no `title` mention?
    (“Star Wars” is not mentioned in the title of *Return of the Jedi*. What features
    could capture these?) Could we incorporate lessons from chapters 1–9 to construct
    more advanced features?'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Training data coverage of all features*—The flip side of more features is
    more training data. As you increase the features you’d like to try, you should
    be wondering whether your training data has enough examples of relevant and irrelevant
    documents across each different combination of your features. Otherwise, your
    model won’t know how to use features to solve the problem.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Different model architectures*—We used a relatively simple model that expects
    features to linearly and independently correlate with relevance, but relevance
    can often be nonlinear and multidimensional. A shopper searching for `ipad` might
    expect the most recent Apple iPad release, except when they add the word “cable”,
    making the query `ipad cable`. For that query, the shopper might just want the
    cheapest cable they can find instead of the most recent. In this case, there may
    be “recency” and “price” features that activate depending on specific keyword
    combinations, necessitating a more complicated model architecture.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next chapter, we will focus on the foundation of good LTR: great judgments!'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learning to rank (LTR) builds generalized ranking functions that can be applied
    across all searches, using robust machine learning techniques.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LTR features generally correspond to search queries. Search engines that support
    LTR often let you store and log features for use when training, and later applying,
    a ranking model.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have tremendous freedom in what features we use to generalize relevance.
    Features could be properties of queries (like the number of terms), properties
    of documents (like popularity), or relationships between queries and documents
    (like BM25 or other relevance scores).
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To do LTR well and apply well-known machine learning techniques, we typically
    reformulate the relevance ranking problem into a traditional, pointwise machine
    learning problem.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVMrank creates simple linear weights on normalized feature values, a good first
    step on your LTR journey.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To be truly useful, we need our model to generalize beyond what it’s learned.
    We can confirm an LTR model’s ability to generalize by setting some judgments
    aside in a test dataset and not using them during training. After training, we
    can then evaluate the model on that previously unseen test dataset to confirm
    the model’s ability to generalize.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once an LTR model is loaded into your search engine, be sure to consider performance
    (as in speed) tradeoffs with relevance. Real-life search systems require both.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
