- en: Chapter 18\. Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第18章 强化学习
- en: '*Reinforcement learning* (RL) is one of the most exciting fields of machine
    learning today, and also one of the oldest. It has been around since the 1950s,
    producing many interesting applications over the years,⁠^([1](ch18.html#idm45720166403424))
    particularly in games (e.g., *TD-Gammon*, a Backgammon-playing program) and in
    machine control, but seldom making the headline news. However, a revolution took
    place [in 2013](https://homl.info/dqn), when researchers from a British startup
    called DeepMind demonstrated a system that could learn to play just about any
    Atari game from scratch,⁠^([2](ch18.html#idm45720166401072)) eventually [outperforming
    humans](https://homl.info/dqn2)⁠^([3](ch18.html#idm45720166399568)) in most of
    them, using only raw pixels as inputs and without any prior knowledge of the rules
    of the games.⁠^([4](ch18.html#idm45720166398304)) This was the first of a series
    of amazing feats, culminating with the victory of their system AlphaGo against
    Lee Sedol, a legendary professional player of the game of Go, in March 2016 and
    against Ke Jie, the world champion, in May 2017\. No program had ever come close
    to beating a master of this game, let alone the world champion. Today the whole
    field of RL is boiling with new ideas, with a wide range of applications.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习*（RL）是当今最激动人心的机器学习领域之一，也是最古老的之一。自上世纪50年代以来一直存在，多年来产生了许多有趣的应用，特别是在游戏（例如
    *TD-Gammon*，一个下棋程序）和机器控制方面，但很少成为头条新闻。然而，一场革命发生在[2013年](https://homl.info/dqn)，当时来自英国初创公司
    DeepMind 的研究人员展示了一个系统，可以从头开始学习玩几乎任何 Atari 游戏，最终在大多数游戏中[超越人类](https://homl.info/dqn2)，只使用原始像素作为输入，而不需要任何关于游戏规则的先验知识。这是一系列惊人壮举的开始，最终在2016年3月，他们的系统
    AlphaGo 在围棋比赛中击败了传奇职业选手李世石，并在2017年5月击败了世界冠军柯洁。没有任何程序曾经接近击败这个游戏的大师，更不用说世界冠军了。如今，整个强化学习领域充满了新的想法，具有广泛的应用范围。'
- en: 'So how did DeepMind (bought by Google for over $500 million in 2014) achieve
    all this? With hindsight it seems rather simple: they applied the power of deep
    learning to the field of reinforcement learning, and it worked beyond their wildest
    dreams. In this chapter I will first explain what reinforcement learning is and
    what it’s good at, then present two of the most important techniques in deep reinforcement
    learning: policy gradients and deep Q-networks, including a discussion of Markov
    decision processes. Let’s get started!'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，DeepMind（2014年被 Google 以超过5亿美元的价格收购）是如何实现所有这些的呢？回顾起来似乎相当简单：他们将深度学习的力量应用于强化学习领域，而且效果超出了他们最疯狂的梦想。在本章中，我将首先解释什么是强化学习以及它擅长什么，然后介绍深度强化学习中最重要的两种技术：策略梯度和深度
    Q 网络，包括对马尔可夫决策过程的讨论。让我们开始吧！
- en: Learning to Optimize Rewards
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习优化奖励
- en: In reinforcement learning, a software *agent* makes *observations* and takes
    *actions* within an *environment*, and in return it receives *rewards* from the
    environment. Its objective is to learn to act in a way that will maximize its
    expected rewards over time. If you don’t mind a bit of anthropomorphism, you can
    think of positive rewards as pleasure, and negative rewards as pain (the term
    “reward” is a bit misleading in this case). In short, the agent acts in the environment
    and learns by trial and error to maximize its pleasure and minimize its pain.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，软件 *代理* 在一个 *环境* 中进行 *观察* 和 *行动*，并从环境中获得 *奖励*。其目标是学会以一种方式行动，以最大化其随时间的预期奖励。如果您不介意有点拟人化，您可以将积极奖励视为快乐，将负面奖励视为痛苦（在这种情况下，“奖励”这个术语有点误导）。简而言之，代理在环境中行动，并通过试错学习来最大化其快乐并最小化其痛苦。
- en: 'This is quite a broad setting, which can apply to a wide variety of tasks.
    Here are a few examples (see [Figure 18-1](#rl_examples_diagram)):'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常广泛的设置，可以应用于各种任务。以下是一些示例（参见 [图18-1](#rl_examples_diagram)）：
- en: The agent can be the program controlling a robot. In this case, the environment
    is the real world, the agent observes the environment through a set of *sensors*
    such as cameras and touch sensors, and its actions consist of sending signals
    to activate motors. It may be programmed to get positive rewards whenever it approaches
    the target destination, and negative rewards whenever it wastes time or goes in
    the wrong direction.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理程序可以是控制机器人的程序。在这种情况下，环境是真实世界，代理通过一组传感器（如摄像头和触摸传感器）观察环境，其行动包括发送信号以激活电机。它可能被编程为在接近目标位置时获得积极奖励，而在浪费时间或走错方向时获得负面奖励。
- en: The agent can be the program controlling *Ms. Pac-Man*. In this case, the environment
    is a simulation of the Atari game, the actions are the nine possible joystick
    positions (upper left, down, center, and so on), the observations are screenshots,
    and the rewards are just the game points.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理可以是控制 *Ms. Pac-Man* 的程序。在这种情况下，环境是 Atari 游戏的模拟，行动是九种可能的摇杆位置（左上、下、中心等），观察是屏幕截图，奖励只是游戏得分。
- en: Similarly, the agent can be the program playing a board game such as Go. It
    only gets a reward if it wins.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，代理可以是玩围棋等棋盘游戏的程序。只有在赢得比赛时才会获得奖励。
- en: The agent does not have to control a physically (or virtually) moving thing.
    For example, it can be a smart thermostat, getting positive rewards whenever it
    is close to the target temperature and saves energy, and negative rewards when
    humans need to tweak the temperature, so the agent must learn to anticipate human
    needs.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理不必控制物理（或虚拟）移动的东西。例如，它可以是一个智能恒温器，每当接近目标温度并节省能源时获得积极奖励，当人类需要调整温度时获得负面奖励，因此代理必须学会预测人类需求。
- en: The agent can observe stock market prices and decide how much to buy or sell
    every second. Rewards are obviously the monetary gains and losses.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理可以观察股市价格并决定每秒买入或卖出多少。奖励显然是货币收益和损失。
- en: Note that there may not be any positive rewards at all; for example, the agent
    may move around in a maze, getting a negative reward at every time step, so it
    had better find the exit as quickly as possible! There are many other examples
    of tasks to which reinforcement learning is well suited, such as self-driving
    cars, recommender systems, placing ads on a web page, or controlling where an
    image classification system should focus its attention.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1801](assets/mls3_1801.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18-1\. Reinforcement learning examples: (a) robotics, (b) *Ms. Pac-Man*,
    (c) Go player, (d) thermostat, (e) automatic trader⁠^([5](ch18.html#idm45720166374256))'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Policy Search
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The algorithm a software agent uses to determine its actions is called its *policy*.
    The policy could be a neural network taking observations as inputs and outputting
    the action to take (see [Figure 18-2](#rl_with_nn_policy_diagram)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1802](assets/mls3_1802.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: Figure 18-2\. Reinforcement learning using a neural network policy
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The policy can be any algorithm you can think of, and it does not have to be
    deterministic. In fact, in some cases it does not even have to observe the environment!
    For example, consider a robotic vacuum cleaner whose reward is the amount of dust
    it picks up in 30 minutes. Its policy could be to move forward with some probability
    *p* every second, or randomly rotate left or right with probability 1 – *p*. The
    rotation angle would be a random angle between –*r* and +*r*. Since this policy
    involves some randomness, it is called a *stochastic policy*. The robot will have
    an erratic trajectory, which guarantees that it will eventually get to any place
    it can reach and pick up all the dust. The question is, how much dust will it
    pick up in 30 minutes?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'How would you train such a robot? There are just two *policy parameters* you
    can tweak: the probability *p* and the angle range *r*. One possible learning
    algorithm could be to try out many different values for these parameters, and
    pick the combination that performs best (see [Figure 18-3](#policy_search_diagram)).
    This is an example of *policy search*, in this case using a brute-force approach.
    When the *policy space* is too large (which is generally the case), finding a
    good set of parameters this way is like searching for a needle in a gigantic haystack.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Another way to explore the policy space is to use *genetic algorithms*. For
    example, you could randomly create a first generation of 100 policies and try
    them out, then “kill” the 80 worst policies⁠^([6](ch18.html#idm45720166354304))
    and make the 20 survivors produce 4 offspring each. An offspring is a copy of
    its parent⁠^([7](ch18.html#idm45720166353536)) plus some random variation. The
    surviving policies plus their offspring together constitute the second generation.
    You can continue to iterate through generations this way until you find a good
    policy.⁠^([8](ch18.html#idm45720166351776))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1803](assets/mls3_1803.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: Figure 18-3\. Four points in the policy space (left) and the agent’s corresponding
    behavior (right)
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Yet another approach is to use optimization techniques, by evaluating the gradients
    of the rewards with regard to the policy parameters, then tweaking these parameters
    by following the gradients toward higher rewards.⁠^([9](ch18.html#idm45720166346048))
    We will discuss this approach, called *policy gradients* (PG), in more detail
    later in this chapter. Going back to the vacuum cleaner robot, you could slightly
    increase *p* and evaluate whether doing so increases the amount of dust picked
    up by the robot in 30 minutes; if it does, then increase *p* some more, or else
    reduce *p*. We will implement a popular PG algorithm using TensorFlow, but before
    we do, we need to create an environment for the agent to live in⁠—so it’s time
    to introduce OpenAI Gym.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to OpenAI Gym
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the challenges of reinforcement learning is that in order to train an
    agent, you first need to have a working environment. If you want to program an
    agent that will learn to play an Atari game, you will need an Atari game simulator.
    If you want to program a walking robot, then the environment is the real world,
    and you can directly train your robot in that environment. However, this has its
    limits: if the robot falls off a cliff, you can’t just click Undo. You can’t speed
    up time either—adding more computing power won’t make the robot move any faster—and
    it’s generally too expensive to train 1,000 robots in parallel. In short, training
    is hard and slow in the real world, so you generally need a *simulated environment*
    at least for bootstrap training. For example, you might use a library like [PyBullet](https://pybullet.org)
    or [MuJoCo](https://mujoco.org) for 3D physics simulation.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的一个挑战是，为了训练一个代理程序，您首先需要一个可用的环境。如果您想编写一个代理程序来学习玩Atari游戏，您将需要一个Atari游戏模拟器。如果您想编写一个行走机器人，那么环境就是现实世界，您可以直接在该环境中训练您的机器人。然而，这也有其局限性：如果机器人掉下悬崖，您不能简单地点击撤销。您也不能加快时间——增加计算能力不会使机器人移动得更快——而且通常来说，同时训练1000个机器人的成本太高。简而言之，在现实世界中训练是困难且缓慢的，因此您通常至少需要一个*模拟环境*来进行引导训练。例如，您可以使用类似[PyBullet](https://pybullet.org)或[MuJoCo](https://mujoco.org)的库进行3D物理模拟。
- en: '[*OpenAI Gym*](https://gym.openai.com)⁠^([10](ch18.html#idm45720166329952))
    is a toolkit that provides a wide variety of simulated environments (Atari games,
    board games, 2D and 3D physical simulations, and so on), that you can use to train
    agents, compare them, or develop new RL algorithms.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[*OpenAI Gym*](https://gym.openai.com)是一个工具包，提供各种模拟环境（Atari游戏，棋盘游戏，2D和3D物理模拟等），您可以用它来训练代理程序，比较它们，或者开发新的RL算法。'
- en: 'OpenAI Gym is preinstalled on Colab, but it’s an older version, so you’ll need
    to replace it with the latest one. You also need to install a few of its dependencies.
    If you are coding on your own machine instead of Colab, and you followed the installation
    instructions at [*https://homl.info/install*](https://homl.info/install), then
    you can skip this step; otherwise, enter these commands:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym在Colab上预先安装，但是它是一个较旧的版本，因此您需要用最新版本替换它。您还需要安装一些它的依赖项。如果您在自己的机器上编程而不是在Colab上，并且按照[*https://homl.info/install*](https://homl.info/install)上的安装说明进行操作，那么您可以跳过这一步；否则，请输入以下命令：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first `%pip` command upgrades Gym to the latest version. The `-q` option
    stands for *quiet*: it makes the output less verbose. The `-U` option stands for
    *upgrade*. The second `%pip` command installs the libraries required to run various
    kinds of environments. This includes classic environments from *control theory*–the
    science of controlling dynamical systems–such as balancing a pole on a cart. It
    also includes environments based on the Box2D library—a 2D physics engine for
    games. Lastly, it includes environments based on the Arcade Learning Environment
    (ALE), which is an emulator for Atari 2600 games. Several Atari game ROMs are
    downloaded automatically, and by running this code you agree with Atari’s ROM
    licenses.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个`%pip`命令将Gym升级到最新版本。`-q`选项代表*quiet*：它使输出更简洁。`-U`选项代表*upgrade*。第二个`%pip`命令安装了运行各种环境所需的库。这包括来自*控制理论*（控制动态系统的科学）的经典环境，例如在小车上平衡杆。它还包括基于Box2D库的环境——一个用于游戏的2D物理引擎。最后，它包括基于Arcade
    Learning Environment（ALE）的环境，这是Atari 2600游戏的模拟器。几个Atari游戏的ROM会被自动下载，通过运行这段代码，您同意Atari的ROM许可证。
- en: 'With that, you’re ready to use OpenAI Gym. Let’s import it and make an environment:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，您就可以使用OpenAI Gym了。让我们导入它并创建一个环境：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we’ve created a CartPole environment. This is a 2D simulation in which
    a cart can be accelerated left or right in order to balance a pole placed on top
    of it (see [Figure 18-4](#cart_pole_diagram)). This is a classic control task.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个CartPole环境。这是一个2D模拟，其中一个小车可以被加速向左或向右，以平衡放在其顶部的杆（参见[图18-4](#cart_pole_diagram)）。这是一个经典的控制任务。
- en: Tip
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The `gym.envs.registry` dictionary contains the names and specifications of
    all the available environments.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`gym.envs.registry`字典包含所有可用环境的名称和规格。'
- en: '![mls3 1804](assets/mls3_1804.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1804](assets/mls3_1804.png)'
- en: Figure 18-4\. The CartPole environment
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-4。CartPole环境
- en: After the environment is created, you must initialize it using the `reset()`
    method, optionally specifying a random seed. This returns the first observation.
    Observations depend on the type of environment. For the CartPole environment,
    each observation is a 1D NumPy array containing four floats representing the cart’s
    horizontal position (`0.0` = center), its velocity (positive means right), the
    angle of the pole (`0.0` = vertical), and its angular velocity (positive means
    clockwise). The `reset()` method also returns a dictionary that may contain extra
    environment-specific information. This can be useful for debugging or for training.
    For example, in many Atari environments, it contains the number of lives left.
    However, in the CartPole environment, this dictionary is empty.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建环境之后，您必须使用`reset()`方法对其进行初始化，可以选择性地指定一个随机种子。这将返回第一个观察结果。观察结果取决于环境的类型。对于CartPole环境，每个观察结果都是一个包含四个浮点数的1D
    NumPy数组，表示小车的水平位置（`0.0` = 中心），其速度（正数表示向右），杆的角度（`0.0` = 垂直），以及其角速度（正数表示顺时针）。`reset()`方法还返回一个可能包含额外环境特定信息的字典。这对于调试或训练可能很有用。例如，在许多Atari环境中，它包含剩余的生命次数。然而，在CartPole环境中，这个字典是空的。
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s call the `render()` method to render this environment as an image. Since
    we set `render_mode="rgb_array"` when creating the environment, the image will
    be returned as a NumPy array:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调用`render()`方法将这个环境渲染为图像。由于在创建环境时设置了`render_mode="rgb_array"`，图像将作为一个NumPy数组返回：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can then use Matplotlib’s `imshow()` function to display this image, as
    usual.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用Matplotlib的`imshow()`函数来显示这个图像，就像往常一样。
- en: 'Now let’s ask the environment what actions are possible:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们询问环境有哪些可能的动作：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Discrete(2)` means that the possible actions are integers 0 and 1, which represent
    accelerating left or right. Other environments may have additional discrete actions,
    or other kinds of actions (e.g., continuous). Since the pole is leaning toward
    the right (`obs[2] > 0`), let’s accelerate the cart toward the right:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `step()` method executes the desired action and returns five values:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '`obs`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: This is the new observation. The cart is now moving toward the right (`obs[1]
    > 0`). The pole is still tilted toward the right (`obs[2] > 0`), but its angular
    velocity is now negative (`obs[3] < 0`), so it will likely be tilted toward the
    left after the next step.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '`reward`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: In this environment, you get a reward of 1.0 at every step, no matter what you
    do, so the goal is to keep the episode running for as long as possible.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '`done`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: This value will be `True` when the episode is over. This will happen when the
    pole tilts too much, or goes off the screen, or after 200 steps (in this last
    case, you have won). After that, the environment must be reset before it can be
    used again.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '`truncated`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: This value will be `True` when an episode is interrupted early, for example
    by an environment wrapper that imposes a maximum number of steps per episode (see
    Gym’s documentation for more details on environment wrappers). Some RL algorithms
    treat truncated episodes differently from episodes finished normally (i.e., when
    `done` is `True`), but in this chapter we will treat them identically.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '`info`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: This environment-specific dictionary may provide extra information, just like
    the one returned by the `reset()` method.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once you have finished using an environment, you should call its `close()` method
    to free resources.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s hardcode a simple policy that accelerates left when the pole is leaning
    toward the left and accelerates right when the pole is leaning toward the right.
    We will run this policy to see the average rewards it gets over 500 episodes:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This code is self-explanatory. Let’s look at the result:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Even with 500 tries, this policy never managed to keep the pole upright for
    more than 63 consecutive steps. Not great. If you look at the simulation in this
    chapter’s notebook, you will see that the cart oscillates left and right more
    and more strongly until the pole tilts too much. Let’s see if a neural network
    can come up with a better policy.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Neural Network Policies
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s create a neural network policy. This neural network will take an observation
    as input, and it will output the action to be executed, just like the policy we
    hardcoded earlier. More precisely, it will estimate a probability for each action,
    and then we will select an action randomly, according to the estimated probabilities
    (see [Figure 18-5](#neural_network_policy_diagram)). In the case of the CartPole
    environment, there are just two possible actions (left or right), so we only need
    one output neuron. It will output the probability *p* of action 0 (left), and
    of course the probability of action 1 (right) will be 1 – *p*. For example, if
    it outputs 0.7, then we will pick action 0 with 70% probability, or action 1 with
    30% probability.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1805](assets/mls3_1805.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Figure 18-5\. Neural network policy
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You may wonder why we are picking a random action based on the probabilities
    given by the neural network, rather than just picking the action with the highest
    score. This approach lets the agent find the right balance between *exploring*
    new actions and *exploiting* the actions that are known to work well. Here’s an
    analogy: suppose you go to a restaurant for the first time, and all the dishes
    look equally appealing, so you randomly pick one. If it turns out to be good,
    you can increase the probability that you’ll order it next time, but you shouldn’t
    increase that probability up to 100%, or else you will never try out the other
    dishes, some of which may be even better than the one you tried. This *exploration*/*exploitation
    dilemma* is central in reinforcement learning.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Also note that in this particular environment, the past actions and observations
    can safely be ignored, since each observation contains the environment’s full
    state. If there were some hidden state, then you might need to consider past actions
    and observations as well. For example, if the environment only revealed the position
    of the cart but not its velocity, you would have to consider not only the current
    observation but also the previous observation in order to estimate the current
    velocity. Another example is when the observations are noisy; in that case, you
    generally want to use the past few observations to estimate the most likely current
    state. The CartPole problem is thus as simple as can be; the observations are
    noise-free, and they contain the environment’s full state.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to build a basic neural network policy using Keras:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We use a `Sequential` model to define the policy network. The number of inputs
    is the size of the observation space—which in the case of CartPole is 4—and we
    have just five hidden units because it’s a fairly simple task. Finally, we want
    to output a single probability—the probability of going left—so we have a single
    output neuron using the sigmoid activation function. If there were more than two
    possible actions, there would be one output neuron per action, and we would use
    the softmax activation function instead.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: OK, we now have a neural network policy that will take observations and output
    action probabilities. But how do we train it?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluating Actions: The Credit Assignment Problem'
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we knew what the best action was at each step, we could train the neural
    network as usual, by minimizing the cross entropy between the estimated probability
    distribution and the target probability distribution. It would just be regular
    supervised learning. However, in reinforcement learning the only guidance the
    agent gets is through rewards, and rewards are typically sparse and delayed. For
    example, if the agent manages to balance the pole for 100 steps, how can it know
    which of the 100 actions it took were good, and which of them were bad? All it
    knows is that the pole fell after the last action, but surely this last action
    is not entirely responsible. This is called the *credit assignment problem*: when
    the agent gets a reward, it is hard for it to know which actions should get credited
    (or blamed) for it. Think of a dog that gets rewarded hours after it behaved well;
    will it understand what it is being rewarded for?'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: To tackle this problem, a common strategy is to evaluate an action based on
    the sum of all the rewards that come after it, usually applying a *discount factor*,
    *γ* (gamma), at each step. This sum of discounted rewards is called the action’s
    *return*. Consider the example in [Figure 18-6](#discounted_rewards_diagram).
    If an agent decides to go right three times in a row and gets +10 reward after
    the first step, 0 after the second step, and finally –50 after the third step,
    then assuming we use a discount factor *γ* = 0.8, the first action will have a
    return of 10 + *γ* × 0 + *γ*² × (–50) = –22\. If the discount factor is close
    to 0, then future rewards won’t count for much compared to immediate rewards.
    Conversely, if the discount factor is close to 1, then rewards far into the future
    will count almost as much as immediate rewards. Typical discount factors vary
    from 0.9 to 0.99\. With a discount factor of 0.95, rewards 13 steps into the future
    count roughly for half as much as immediate rewards (since 0.95^(13) ≈ 0.5), while
    with a discount factor of 0.99, rewards 69 steps into the future count for half
    as much as immediate rewards. In the CartPole environment, actions have fairly
    short-term effects, so choosing a discount factor of 0.95 seems reasonable.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1806](assets/mls3_1806.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18-6\. Computing an action’s return: the sum of discounted future rewards'
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Of course, a good action may be followed by several bad actions that cause the
    pole to fall quickly, resulting in the good action getting a low return. Similarly,
    a good actor may sometimes star in a terrible movie. However, if we play the game
    enough times, on average good actions will get a higher return than bad ones.
    We want to estimate how much better or worse an action is, compared to the other
    possible actions, on average. This is called the *action advantage*. For this,
    we must run many episodes and normalize all the action returns, by subtracting
    the mean and dividing by the standard deviation. After that, we can reasonably
    assume that actions with a negative advantage were bad while actions with a positive
    advantage were good. OK, now that we have a way to evaluate each action, we are
    ready to train our first agent using policy gradients. Let’s see how.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，一个好的行动可能会被几个导致杆迅速倒下的坏行动跟随，导致好的行动获得较低的回报。同样，一个好的演员有时可能会出演一部糟糕的电影。然而，如果我们玩足够多次游戏，平均而言好的行动将获得比坏行动更高的回报。我们想要估计一个行动相对于其他可能行动的平均优势有多大。这被称为*行动优势*。为此，我们必须运行许多情节，并通过减去均值并除以标准差来标准化所有行动回报。之后，我们可以合理地假设具有负优势的行动是坏的，而具有正优势的行动是好的。现在我们有了一种评估每个行动的方法，我们准备使用策略梯度来训练我们的第一个代理。让我们看看如何。
- en: Policy Gradients
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度
- en: 'As discussed earlier, PG algorithms optimize the parameters of a policy by
    following the gradients toward higher rewards. One popular class of PG algorithms,
    called *REINFORCE algorithms*, was [introduced back in 1992](https://homl.info/132)⁠^([11](ch18.html#idm45720165711216))
    by Ronald Williams. Here is one common variant:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面讨论的，PG算法通过沿着梯度朝着更高奖励的方向优化策略的参数。一种流行的PG算法类别称为*REINFORCE算法*，由Ronald Williams于1992年提出。这里是一个常见的变体：
- en: First, let the neural network policy play the game several times, and at each
    step, compute the gradients that would make the chosen action even more likely—but
    don’t apply these gradients yet.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让神经网络策略玩游戏多次，并在每一步计算使选择的行动更有可能的梯度，但暂时不应用这些梯度。
- en: Once you have run several episodes, compute each action’s advantage, using the
    method described in the previous section.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行了几个情节之后，使用前一节中描述的方法计算每个行动的优势。
- en: If an action’s advantage is positive, it means that the action was probably
    good, and you want to apply the gradients computed earlier to make the action
    even more likely to be chosen in the future. However, if the action’s advantage
    is negative, it means the action was probably bad, and you want to apply the opposite
    gradients to make this action slightly *less* likely in the future. The solution
    is to multiply each gradient vector by the corresponding action’s advantage.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个行动的优势是正的，这意味着这个行动可能是好的，你希望应用之前计算的梯度，使这个行动在未来更有可能被选择。然而，如果一个行动的优势是负的，这意味着这个行动可能是坏的，你希望应用相反的梯度，使这个行动在未来略微减少。解决方案是将每个梯度向量乘以相应行动的优势。
- en: Finally, compute the mean of all the resulting gradient vectors, and use it
    to perform a gradient descent step.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，计算所有结果梯度向量的平均值，并用它执行一步梯度下降。
- en: 'Let’s use Keras to implement this algorithm. We will train the neural network
    policy we built earlier so that it learns to balance the pole on the cart. First,
    we need a function that will play one step. We will pretend for now that whatever
    action it takes is the right one so that we can compute the loss and its gradients.
    These gradients will just be saved for a while, and we will modify them later
    depending on how good or bad the action turned out to be:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Keras来实现这个算法。我们将训练之前构建的神经网络策略，使其学会在小车上平衡杆。首先，我们需要一个函数来执行一步。我们暂时假设无论采取什么行动都是正确的，以便我们可以计算损失及其梯度。这些梯度将暂时保存一段时间，我们稍后会根据行动的好坏来修改它们：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s walk though this function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这个函数：
- en: Within the `GradientTape` block (see [Chapter 12](ch12.html#tensorflow_chapter)),
    we start by calling the model, giving it a single observation. We reshape the
    observation so it becomes a batch containing a single instance, as the model expects
    a batch. This outputs the probability of going left.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`GradientTape`块中（参见[第12章](ch12.html#tensorflow_chapter)），我们首先调用模型，给它一个观察值。我们将观察值重塑为包含单个实例的批次，因为模型期望一个批次。这将输出向左移动的概率。
- en: Next, we sample a random float between 0 and 1, and we check whether it is greater
    than `left_proba`. The `action` will be `False` with probability `left_proba`,
    or `True` with probability `1 – left_proba`. Once we cast this Boolean to an integer,
    the action will be 0 (left) or 1 (right) with the appropriate probabilities.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们随机抽取一个介于0和1之间的浮点数，并检查它是否大于`left_proba`。`action`将以`left_proba`的概率为`False`，或以`1
    - left_proba`的概率为`True`。一旦我们将这个布尔值转换为整数，行动将以适当的概率为0（左）或1（右）。
- en: 'We now define the target probability of going left: it is 1 minus the action
    (cast to a float). If the action is 0 (left), then the target probability of going
    left will be 1\. If the action is 1 (right), then the target probability will
    be 0.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们定义向左移动的目标概率：它是1减去行动（转换为浮点数）。如果行动是0（左），那么向左移动的目标概率将是1。如果行动是1（右），那么目标概率将是0。
- en: Then we compute the loss using the given loss function, and we use the tape
    to compute the gradient of the loss with regard to the model’s trainable variables.
    Again, these gradients will be tweaked later, before we apply them, depending
    on how good or bad the action turned out to be.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们使用给定的损失函数计算损失，并使用tape计算损失相对于模型可训练变量的梯度。同样，这些梯度稍后会在应用之前进行调整，取决于行动的好坏。
- en: Finally, we play the selected action, and we return the new observation, the
    reward, whether the episode is ended or not, whether it is truncated or not, and
    of course the gradients that we just computed.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们执行选择的行动，并返回新的观察值、奖励、该情节是否结束、是否截断，当然还有我们刚刚计算的梯度。
- en: 'Now let’s create another function that will rely on the `play_one_step()` function
    to play multiple episodes, returning all the rewards and gradients for each episode
    and each step:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建另一个函数，它将依赖于`play_one_step()`函数来玩多个回合，返回每个回合和每个步骤的所有奖励和梯度：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This code returns a list of reward lists: one reward list per episode, containing
    one reward per step. It also returns a list of gradient lists: one gradient list
    per episode, each containing one tuple of gradients per step and each tuple containing
    one gradient tensor per trainable variable.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码返回了一个奖励列表的列表：每个回合一个奖励列表，每个步骤一个奖励。它还返回了一个梯度列表的列表：每个回合一个梯度列表，每个梯度列表包含每个步骤的一个梯度元组，每个元组包含每个可训练变量的一个梯度张量。
- en: 'The algorithm will use the `play_multiple_episodes()` function to play the
    game several times (e.g., 10 times), then it will go back and look at all the
    rewards, discount them, and normalize them. To do that, we need a couple more
    functions; the first will compute the sum of future discounted rewards at each
    step, and the second will normalize all these discounted rewards (i.e., the returns)
    across many episodes by subtracting the mean and dividing by the standard deviation:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法将使用`play_multiple_episodes()`函数多次玩游戏（例如，10次），然后它将回头查看所有奖励，对其进行折扣，并对其进行归一化。为此，我们需要几个额外的函数；第一个将计算每个步骤的未来折扣奖励总和，第二个将通过减去均值并除以标准差来对所有这些折扣奖励（即回报）在许多回合中进行归一化：
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s check that this works:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下这是否有效：
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The call to `discount_rewards()` returns exactly what we expect (see [Figure 18-6](#discounted_rewards_diagram)).
    You can verify that the function `discount_and_normalize_rewards()` does indeed
    return the normalized action advantages for each action in both episodes. Notice
    that the first episode was much worse than the second, so its normalized advantages
    are all negative; all actions from the first episode would be considered bad,
    and conversely all actions from the second episode would be considered good.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`discount_rewards()`返回了我们预期的结果（见[图18-6](#discounted_rewards_diagram)）。您可以验证函数`discount_and_normalize_rewards()`确实返回了两个回合中每个动作的归一化优势。请注意，第一个回合比第二个回合差得多，因此它的归一化优势都是负数；第一个回合的所有动作都被认为是不好的，反之第二个回合的所有动作都被认为是好的。
- en: 'We are almost ready to run the algorithm! Now let’s define the hyperparameters.
    We will run 150 training iterations, playing 10 episodes per iteration, and each
    episode will last at most 200 steps. We will use a discount factor of 0.95:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎准备好运行算法了！现在让我们定义超参数。我们将运行150次训练迭代，每次迭代玩10个回合，每个回合最多持续200步。我们将使用折扣因子0.95：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We also need an optimizer and the loss function. A regular Nadam optimizer
    with learning rate 0.01 will do just fine, and we will use the binary cross-entropy
    loss function because we are training a binary classifier (there are two possible
    actions—left or right):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个优化器和损失函数。一个常规的Nadam优化器，学习率为0.01，将会很好地完成任务，我们将使用二元交叉熵损失函数，因为我们正在训练一个二元分类器（有两种可能的动作——左或右）：
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We are now ready to build and run the training loop!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备构建和运行训练循环！
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s walk through this code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步走过这段代码：
- en: At each training iteration, this loop calls the `play_multiple_episodes()` function,
    which plays 10 episodes and returns the rewards and gradients for each step in
    each episode.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每次训练迭代中，此循环调用`play_multiple_episodes()`函数，该函数播放10个回合，并返回每个步骤中每个回合的奖励和梯度。
- en: Then we call the `discount_and_normalize_rewards()` function to compute each
    action’s normalized advantage, called the `final_reward` in this code. This provides
    a measure of how good or bad each action actually was, in hindsight.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们调用`discount_and_normalize_rewards()`函数来计算每个动作的归一化优势，这在这段代码中称为`final_reward`。这提供了一个衡量每个动作实际上是好还是坏的指标。
- en: Next, we go through each trainable variable, and for each of them we compute
    the weighted mean of the gradients for that variable over all episodes and all
    steps, weighted by the `final_reward`.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们遍历每个可训练变量，并对每个变量计算所有回合和所有步骤中该变量的梯度的加权平均，权重为`final_reward`。
- en: 'Finally, we apply these mean gradients using the optimizer: the model’s trainable
    variables will be tweaked, and hopefully the policy will be a bit better.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用优化器应用这些均值梯度：模型的可训练变量将被微调，希望策略会有所改善。
- en: And we’re done! This code will train the neural network policy, and it will
    successfully learn to balance the pole on the cart. The mean reward per episode
    will get very close to 200\. By default, that’s the maximum for this environment.
    Success!
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完成了！这段代码将训练神经网络策略，并成功学会在小车上平衡杆。每个回合的平均奖励将非常接近200。默认情况下，这是该环境的最大值。成功！
- en: The simple policy gradients algorithm we just trained solved the CartPole task,
    but it would not scale well to larger and more complex tasks. Indeed, it is highly
    *sample inefficient*, meaning it needs to explore the game for a very long time
    before it can make significant progress. This is due to the fact that it must
    run multiple episodes to estimate the advantage of each action, as we have seen.
    However, it is the foundation of more powerful algorithms, such as *actor-critic*
    algorithms (which we will discuss briefly at the end of this chapter).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚训练的简单策略梯度算法解决了CartPole任务，但是它在扩展到更大更复杂的任务时效果不佳。事实上，它具有很高的*样本效率低*，这意味着它需要很长时间探索游戏才能取得显著进展。这是因为它必须运行多个回合来估计每个动作的优势，正如我们所见。然而，它是更强大算法的基础，比如*演员-评论家*算法（我们将在本章末简要讨论）。
- en: Tip
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Researchers try to find algorithms that work well even when the agent initially
    knows nothing about the environment. However, unless you are writing a paper,
    you should not hesitate to inject prior knowledge into the agent, as it will speed
    up training dramatically. For example, since you know that the pole should be
    as vertical as possible, you could add negative rewards proportional to the pole’s
    angle. This will make the rewards much less sparse and speed up training. Also,
    if you already have a reasonably good policy (e.g., hardcoded), you may want to
    train the neural network to imitate it before using policy gradients to improve
    it.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员试图找到即使代理最初对环境一无所知也能很好运行的算法。然而，除非你在写论文，否则不应该犹豫向代理注入先验知识，因为这将极大加快训练速度。例如，由于你知道杆应该尽可能垂直，你可以添加与杆角度成比例的负奖励。这将使奖励变得不那么稀疏，加快训练速度。此外，如果你已经有一个相当不错的策略（例如硬编码），你可能希望在使用策略梯度来改进之前，训练神经网络来模仿它。
- en: 'We will now look at another popular family of algorithms. Whereas PG algorithms
    directly try to optimize the policy to increase rewards, the algorithms we will
    explore now are less direct: the agent learns to estimate the expected return
    for each state, or for each action in each state, then it uses this knowledge
    to decide how to act. To understand these algorithms, we must first consider *Markov
    decision processes* (MDPs).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看一下另一个流行的算法家族。PG算法直接尝试优化策略以增加奖励，而我们现在要探索的算法则不那么直接：代理学习估计每个状态的预期回报，或者每个状态中每个动作的预期回报，然后利用这些知识来决定如何行动。要理解这些算法，我们首先必须考虑*马尔可夫决策过程*（MDPs）。
- en: Markov Decision Processes
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: In the early 20th century, the mathematician Andrey Markov studied stochastic
    processes with no memory, called *Markov chains*. Such a process has a fixed number
    of states, and it randomly evolves from one state to another at each step. The
    probability for it to evolve from a state *s* to a state *s*′ is fixed, and it
    depends only on the pair (*s*, *s*′), not on past states. This is why we say that
    the system has no memory.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 20世纪初，数学家安德烈·马尔可夫研究了没有记忆的随机过程，称为*马尔可夫链*。这样的过程具有固定数量的状态，并且在每一步中随机从一个状态演变到另一个状态。它从状态*s*演变到状态*s*′的概率是固定的，仅取决于对(*s*,
    *s*′)这一对，而不取决于过去的状态。这就是为什么我们说该系统没有记忆。
- en: '[Figure 18-7](#markov_chain_diagram) shows an example of a Markov chain with
    four states.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18-7](#markov_chain_diagram)显示了一个具有四个状态的马尔可夫链的示例。'
- en: '![mls3 1807](assets/mls3_1807.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1807](assets/mls3_1807.png)'
- en: Figure 18-7\. Example of a Markov chain
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-7. 马尔可夫链示例
- en: 'Suppose that the process starts in state *s*[0], and there is a 70% chance
    that it will remain in that state at the next step. Eventually it is bound to
    leave that state and never come back, because no other state points back to *s*[0].
    If it goes to state *s*[1], it will then most likely go to state *s*[2] (90% probability),
    then immediately back to state *s*[1] (with 100% probability). It may alternate
    a number of times between these two states, but eventually it will fall into state
    *s*[3] and remain there forever, since there’s no way out: this is called a *terminal
    state*. Markov chains can have very different dynamics, and they are heavily used
    in thermodynamics, chemistry, statistics, and much more.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 假设过程从状态*s*[0]开始，并且有70%的概率在下一步保持在该状态。最终，它必定会离开该状态并永远不会回来，因为没有其他状态指向*s*[0]。如果它进入状态*s*[1]，那么它很可能会进入状态*s*[2]（90%的概率），然后立即返回到状态*s*[1]（100%的概率）。它可能在这两个状态之间交替多次，但最终会陷入状态*s*[3]并永远留在那里，因为没有出路：这被称为*终止状态*。马尔可夫链的动态可能非常不同，并且在热力学、化学、统计学等领域被广泛使用。
- en: 'Markov decision processes were first described in the 1950s by [Richard Bellman](https://homl.info/133).⁠^([12](ch18.html#idm45720164871120))
    They resemble Markov chains, but with a twist: at each step, an agent can choose
    one of several possible actions, and the transition probabilities depend on the
    chosen action. Moreover, some state transitions return some reward (positive or
    negative), and the agent’s goal is to find a policy that will maximize reward
    over time.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程是在20世纪50年代由[理查德·贝尔曼](https://homl.info/133)首次描述的。⁠^([12](ch18.html#idm45720164871120))
    它们类似于马尔可夫链，但有一个区别：在每一步中，代理可以选择几种可能的动作之一，转移概率取决于所选择的动作。此外，一些状态转移会产生一些奖励（正面或负面），代理的目标是找到一个能够随时间最大化奖励的策略。
- en: For example, the MDP represented in [Figure 18-8](#mdp_diagram) has three states
    (represented by circles) and up to three possible discrete actions at each step
    (represented by diamonds).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，MDP在[图18-8](#mdp_diagram)中表示有三个状态（由圆圈表示），并且在每一步最多有三种可能的离散动作（由菱形表示）。
- en: '![mls3 1808](assets/mls3_1808.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1808](assets/mls3_1808.png)'
- en: Figure 18-8\. Example of a Markov decision process
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-8. 马尔可夫决策过程示例
- en: 'If it starts in state *s*[0], the agent can choose between actions *a*[0],
    *a*[1], or *a*[2]. If it chooses action *a*[1], it just remains in state *s*[0]
    with certainty, and without any reward. It can thus decide to stay there forever
    if it wants to. But if it chooses action *a*[0], it has a 70% probability of gaining
    a reward of +10 and remaining in state *s*[0]. It can then try again and again
    to gain as much reward as possible, but at one point it is going to end up instead
    in state *s*[1]. In state *s*[1] it has only two possible actions: *a*[0] or *a*[2].
    It can choose to stay put by repeatedly choosing action *a*[0], or it can choose
    to move on to state *s*[2] and get a negative reward of –50 (ouch). In state *s*[2]
    it has no choice but to take action *a*[1], which will most likely lead it back
    to state *s*[0], gaining a reward of +40 on the way. You get the picture. By looking
    at this MDP, can you guess which strategy will gain the most reward over time?
    In state *s*[0] it is clear that action *a*[0] is the best option, and in state
    *s*[2] the agent has no choice but to take action *a*[1], but in state *s*[1]
    it is not obvious whether the agent should stay put (*a*[0]) or go through the
    fire (*a*[2]).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果代理从状态 *s*[0] 开始，可以在行动 *a*[0]、*a*[1] 或 *a*[2] 之间选择。如果选择行动 *a*[1]，它就会肯定留在状态
    *s*[0]，没有任何奖励。因此，如果愿意，它可以决定永远留在那里。但如果选择行动 *a*[0]，它有70%的概率获得+10的奖励并留在状态 *s*[0]。然后它可以一次又一次地尝试获得尽可能多的奖励，但最终会进入状态
    *s*[1]。在状态 *s*[1] 中，它只有两种可能的行动：*a*[0] 或 *a*[2]。它可以通过反复选择行动 *a*[0] 来保持原地，或者选择移动到状态
    *s*[2] 并获得-50的负奖励（疼）。在状态 *s*[2] 中，它别无选择，只能采取行动 *a*[1]，这很可能会将其带回状态 *s*[0]，在途中获得+40的奖励。你明白了。通过观察这个MDP，你能猜出哪种策略会随着时间获得最多的奖励吗？在状态
    *s*[0] 中，很明显行动 *a*[0] 是最佳选择，在状态 *s*[2] 中，代理别无选择，只能采取行动 *a*[1]，但在状态 *s*[1] 中，不明显代理应该保持原地（*a*[0]）还是冒险前进（*a*[2]）。
- en: Bellman found a way to estimate the *optimal state value* of any state *s*,
    noted *V**(*s*), which is the sum of all discounted future rewards the agent can
    expect on average after it reaches the state, assuming it acts optimally. He showed
    that if the agent acts optimally, then the *Bellman optimality equation* applies
    (see [Equation 18-1](#bellman_optimality_equation)). This recursive equation says
    that if the agent acts optimally, then the optimal value of the current state
    is equal to the reward it will get on average after taking one optimal action,
    plus the expected optimal value of all possible next states that this action can
    lead to.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼找到了一种估计任何状态 *s* 的*最优状态值* *V*(*s*) 的方法，这是代理在到达该状态后可以期望的所有折扣未来奖励的总和，假设它采取最优行动。他表明，如果代理采取最优行动，那么*贝尔曼最优性方程*适用（参见[方程18-1](#bellman_optimality_equation)）。这个递归方程表明，如果代理采取最优行动，那么当前状态的最优值等于在采取一个最优行动后平均获得的奖励，再加上这个行动可能导致的所有可能下一个状态的期望最优值。
- en: Equation 18-1\. Bellman optimality equation
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程18-1\. 贝尔曼最优性方程
- en: <math display="block"><mrow><msup><mi>V</mi> <mo>*</mo></msup> <mrow><mo>(</mo>
    <mi>s</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo movablelimits="true" form="prefix">max</mo>
    <mi>a</mi></munder> <munder><mo>∑</mo> <mrow><mi>s</mi><mo>'</mo></mrow></munder>
    <mrow><mi>T</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo>
    <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mrow><mo>[</mo> <mi>R</mi> <mrow><mo>(</mo>
    <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo> <mi>s</mi><mo>'</mo> <mo>)</mo></mrow>
    <mo>+</mo> <mi>γ</mi> <mo>·</mo> <msup><mi>V</mi> <mo>*</mo></msup> <mrow><mo>(</mo>
    <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow> <mtext>for</mtext>
    <mtext>all</mtext> <mi>s</mi></mrow></math>
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msup><mi>V</mi> <mo>*</mo></msup> <mrow><mo>(</mo>
    <mi>s</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo movablelimits="true" form="prefix">max</mo>
    <mi>a</mi></munder> <munder><mo>∑</mo> <mrow><mi>s</mi><mo>'</mo></mrow></munder>
    <mrow><mi>T</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo>
    <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mrow><mo>[</mo> <mi>R</mi> <mrow><mo>(</mo>
    <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo> <mi>s</mi><mo>'</mo> <mo>)</mo></mrow>
    <mo>+</mo> <mi>γ</mi> <mo>·</mo> <msup><mi>V</mi> <mo>*</mo></msup> <mrow><mo>(</mo>
    <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow> <mtext>for</mtext>
    <mtext>all</mtext> <mi>s</mi></mrow></math>
- en: 'In this equation:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*T*(*s*, *a*, *s*′) is the transition probability from state *s* to state *s*′,
    given that the agent chose action *a*. For example, in [Figure 18-8](#mdp_diagram),
    *T*(*s*[2], *a*[1], *s*[0]) = 0.8.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T*(*s*, *a*, *s*′) 是从状态 *s* 转移到状态 *s*′ 的转移概率，假设代理选择行动 *a*。例如，在[图18-8](#mdp_diagram)中，*T*(*s*[2],
    *a*[1], *s*[0]) = 0.8。'
- en: '*R*(*s*, *a*, *s*′) is the reward that the agent gets when it goes from state
    *s* to state *s*′, given that the agent chose action *a*. For example, in [Figure 18-8](#mdp_diagram),
    *R*(*s*[2], *a*[1], *s*[0]) = +40.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R*(*s*, *a*, *s*′) 是代理从状态 *s* 转移到状态 *s*′ 时获得的奖励，假设代理选择行动 *a*。例如，在[图18-8](#mdp_diagram)中，*R*(*s*[2],
    *a*[1], *s*[0]) = +40。'
- en: '*γ* is the discount factor.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*γ* 是折扣因子。'
- en: 'This equation leads directly to an algorithm that can precisely estimate the
    optimal state value of every possible state: first initialize all the state value
    estimates to zero, and then iteratively update them using the *value iteration*
    algorithm (see [Equation 18-2](#value_iteration_equation)). A remarkable result
    is that, given enough time, these estimates are guaranteed to converge to the
    optimal state values, corresponding to the optimal policy.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程直接导致了一个算法，可以精确估计每个可能状态的最优状态值：首先将所有状态值估计初始化为零，然后使用*值迭代*算法进行迭代更新（参见[方程18-2](#value_iteration_equation)）。一个显著的结果是，给定足够的时间，这些估计将收敛到最优状态值，对应于最优策略。
- en: Equation 18-2\. Value iteration algorithm
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程18-2\. 值迭代算法
- en: <math display="block"><mrow><msub><mi>V</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>←</mo> <munder><mo movablelimits="true"
    form="prefix">max</mo> <mi>a</mi></munder> <munder><mo>∑</mo> <mrow><mi>s</mi><mo>'</mo></mrow></munder>
    <mrow><mi>T</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo>
    <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mrow><mo>[</mo> <mi>R</mi> <mrow><mo>(</mo>
    <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo> <mi>s</mi><mo>'</mo> <mo>)</mo></mrow>
    <mo>+</mo> <mi>γ</mi> <mo>·</mo> <msub><mi>V</mi> <mi>k</mi></msub> <mrow><mo>(</mo>
    <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow> <mtext>for</mtext>
    <mtext>all</mtext> <mi>s</mi></mrow></math>
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: In this equation, *V*[*k*](*s*) is the estimated value of state *s* at the *k*^(th)
    iteration of the algorithm.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This algorithm is an example of *dynamic programming*, which breaks down a complex
    problem into tractable subproblems that can be tackled iteratively.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Knowing the optimal state values can be useful, in particular to evaluate a
    policy, but it does not give us the optimal policy for the agent. Luckily, Bellman
    found a very similar algorithm to estimate the optimal *state-action values*,
    generally called *Q-values* (quality values). The optimal Q-value of the state-action
    pair (*s*, *a*), noted *Q**(*s*, *a*), is the sum of discounted future rewards
    the agent can expect on average after it reaches the state *s* and chooses action
    *a*, but before it sees the outcome of this action, assuming it acts optimally
    after that action.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how it works. Once again, you start by initializing all the Q-value
    estimates to zero, then you update them using the *Q-value iteration* algorithm
    (see [Equation 18-3](#q_value_iteration_equation)).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Equation 18-3\. Q-value iteration algorithm
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>Q</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>←</mo>
    <munder><mo>∑</mo> <mrow><mi>s</mi><mo>'</mo></mrow></munder> <mrow><mi>T</mi>
    <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>,</mo> <mi>s</mi><mo>'</mo>
    <mo>)</mo></mrow> <mrow><mo>[</mo> <mi>R</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo>
    <mi>a</mi> <mo>,</mo> <mi>s</mi><mo>'</mo> <mo>)</mo></mrow> <mo>+</mo> <mi>γ</mi>
    <mo>·</mo> <munder><mo movablelimits="true" form="prefix">max</mo> <mrow><mi>a</mi><mo>'</mo></mrow></munder>
    <mrow><msub><mi>Q</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi>s</mi><mo>'</mo>
    <mo>,</mo> <mi>a</mi><mo>'</mo> <mo>)</mo></mrow></mrow> <mo>]</mo></mrow></mrow>
    <mtext>for</mtext> <mtext>all</mtext> <mrow><mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have the optimal Q-values, defining the optimal policy, noted *π**(*s*),
    is trivial; when the agent is in state *s*, it should choose the action with the
    highest Q-value for that state: <math><msup><mi>π</mi><mo>*</mo></msup><mo>(</mo><mi>s</mi><mo>)</mo><mo>=</mo><munder><mo>argmax</mo><mi>a</mi></munder><msup><mi>Q</mi><mo>*</mo></msup><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></math>.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply this algorithm to the MDP represented in [Figure 18-8](#mdp_diagram).
    First, we need to define the MDP:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For example, to know the transition probability of going from *s*[2] to *s*[0]
    after playing action *a*[1], we will look up `transition_probabilities[2][1][0]`
    (which is 0.8). Similarly, to get the corresponding reward, we will look up `rewards[2][1][0]`
    (which is +40). And to get the list of possible actions in *s*[2], we will look
    up `possible_actions[2]` (in this case, only action *a*[1] is possible). Next,
    we must initialize all the Q-values to zero (except for the impossible actions,
    for which we set the Q-values to –∞):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now let’s run the Q-value iteration algorithm. It applies [Equation 18-3](#q_value_iteration_equation)
    repeatedly, to all Q-values, for every state and every possible action:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'That’s it! The resulting Q-values look like this:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For example, when the agent is in state *s*[0] and it chooses action *a*[1],
    the expected sum of discounted future rewards is approximately 17.0.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'For each state, we can find the action that has the highest Q-value:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This gives us the optimal policy for this MDP when using a discount factor
    of 0.90: in state *s*[0] choose action *a*[0], in state *s*[1] choose action *a*[0]
    (i.e., stay put), and in state *s*[2] choose action *a*[1] (the only possible
    action). Interestingly, if we increase the discount factor to 0.95, the optimal
    policy changes: in state *s*[1] the best action becomes *a*[2] (go through the
    fire!). This makes sense because the more you value future rewards, the more you
    are willing to put up with some pain now for the promise of future bliss.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Temporal Difference Learning
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning problems with discrete actions can often be modeled as
    Markov decision processes, but the agent initially has no idea what the transition
    probabilities are (it does not know *T*(*s*, *a*, *s*′)), and it does not know
    what the rewards are going to be either (it does not know *R*(*s*, *a*, *s*′)).
    It must experience each state and each transition at least once to know the rewards,
    and it must experience them multiple times if it is to have a reasonable estimate
    of the transition probabilities.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The *temporal difference (TD) learning* algorithm is very similar to the Q-value
    iteration algorithm, but tweaked to take into account the fact that the agent
    has only partial knowledge of the MDP. In general we assume that the agent initially
    knows only the possible states and actions, and nothing more. The agent uses an
    *exploration policy*—for example, a purely random policy—to explore the MDP, and
    as it progresses, the TD learning algorithm updates the estimates of the state
    values based on the transitions and rewards that are actually observed (see [Equation
    18-4](#td_learning_equation)).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Equation 18-4\. TD learning algorithm
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><msub><mi>V</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>(</mo><mi>s</mi><mo>)</mo><mo>←</mo><mo>(</mo><mn>1</mn><mo>-</mo><mi>α</mi><mo>)</mo>
    <msub><mi>V</mi><mi>k</mi></msub> <mo>(</mo><mi>s</mi><mo>)</mo><mo>+</mo><mi>α</mi>
    <mfenced><mrow><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo> <msub><mi>V</mi><mi>k</mi></msub>
    <mo>(</mo><mi>s</mi><mo>'</mo><mo>)</mo></mrow></mfenced> <mtext>or, equivalently: </mtext>
    <msub><mi>V</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>(</mo><mi>s</mi><mo>)</mo><mo>←</mo><msub><mi>V</mi><mi>k</mi></msub>
    <mo>(</mo><mi>s</mi><mo>)</mo><mo>+</mo><mi>α</mi><mo>·</mo> <msub><mi>δ</mi><mi>k</mi></msub>
    <mo>(</mo><mi>s</mi><mo>,</mo><mi>r</mi><mo>,</mo> <mi>s</mi><mo>'</mo><mo>)</mo>
    <mtext>with </mtext><msub><mi>δ</mi><mi>k</mi></msub> <mo>(</mo><mi>s</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>s</mi><mo>′</mo>
    <mo>)</mo><mo>=</mo><mi>r</mi><mo>+</mo><mi>γ</mi> <mo>·</mo> <msub><mi>V</mi><mi>k</mi></msub>
    <mo>(</mo><mi>s</mi><mo>'</mo><mo>)</mo><mo>-</mo> <msub><mi>V</mi><mi>k</mi></msub><mo>(</mo><mi>s</mi><mo>)</mo></math>
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '*α* is the learning rate (e.g., 0.01).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*r* + *γ* · *V*[*k*](*s*′) is called the *TD target*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*δ*[k](*s*, *r*, *s*′) is called the *TD error*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A more concise way of writing the first form of this equation is to use the
    notation <math><mi>a</mi><munder><mo>←</mo><mi>α</mi></munder><mi>b</mi></math>,
    which means *a*[*k*+1] ← (1 – *α*) · *a*[*k*] + *α* ·*b*[*k*]. So, the first line
    of [Equation 18-4](#td_learning_equation) can be rewritten like this: <math><mi>V</mi><mo>(</mo><mi>s</mi><mo>)</mo><munder><mo>←</mo><mi>α</mi></munder><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><mi>V</mi><mo>(</mo><mi>s</mi><mo>''</mo><mo>)</mo></math>.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TD learning has many similarities with stochastic gradient descent, including
    the fact that it handles one sample at a time. Moreover, just like SGD, it can
    only truly converge if you gradually reduce the learning rate; otherwise, it will
    keep bouncing around the optimum Q-values.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: For each state *s*, this algorithm keeps track of a running average of the immediate
    rewards the agent gets upon leaving that state, plus the rewards it expects to
    get later, assuming it acts optimally.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Q-Learning
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similarly, the Q-learning algorithm is an adaptation of the Q-value iteration
    algorithm to the situation where the transition probabilities and the rewards
    are initially unknown (see [Equation 18-5](#q_learning_equation)). Q-learning
    works by watching an agent play (e.g., randomly) and gradually improving its estimates
    of the Q-values. Once it has accurate Q-value estimates (or close enough), then
    the optimal policy is just choosing the action that has the highest Q-value (i.e.,
    the greedy policy).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Equation 18-5\. Q-learning algorithm
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mi>Q</mi><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo>
    <munder><mo>←</mo><mi>α</mi></munder> <mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo>
    <munder><mi>max</mi><mrow><mi>a</mi><mo>'</mo></mrow></munder> <mi>Q</mi><mo>(</mo><mi>s</mi><mo>'</mo><mo>,</mo>
    <mi>a</mi><mo>'</mo><mo>)</mo></math>
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: For each state-action pair (*s*, *a*), this algorithm keeps track of a running
    average of the rewards *r* the agent gets upon leaving the state *s* with action
    *a*, plus the sum of discounted future rewards it expects to get. To estimate
    this sum, we take the maximum of the Q-value estimates for the next state *s*′,
    since we assume that the target policy will act optimally from then on.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement the Q-learning algorithm. First, we will need to make an agent
    explore the environment. For this, we need a step function so that the agent can
    execute one action and get the resulting state and reward:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now let’s implement the agent’s exploration policy. Since the state space is
    pretty small, a simple random policy will be sufficient. If we run the algorithm
    for long enough, the agent will visit every state many times, and it will also
    try every possible action many times:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, after we initialize the Q-values just like earlier, we are ready to run
    the Q-learning algorithm with learning rate decay (using power scheduling, introduced
    in [Chapter 11](ch11.html#deep_chapter)):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This algorithm will converge to the optimal Q-values, but it will take many
    iterations, and possibly quite a lot of hyperparameter tuning. As you can see
    in [Figure 18-9](#q_value_plot), the Q-value iteration algorithm (left) converges
    very quickly, in fewer than 20 iterations, while the Q-learning algorithm (right)
    takes about 8,000 iterations to converge. Obviously, not knowing the transition
    probabilities or the rewards makes finding the optimal policy significantly harder!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1809](assets/mls3_1809.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: Figure 18-9\. Learning curve of the Q-value iteration algorithm versus the Q-learning
    algorithm
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The Q-learning algorithm is called an *off-policy* algorithm because the policy
    being trained is not necessarily the one used during training. For example, in
    the code we just ran, the policy being executed (the exploration policy) was completely
    random, while the policy being trained was never used. After training, the optimal
    policy corresponds to systematically choosing the action with the highest Q-value.
    Conversely, the policy gradients algorithm is an *on-policy* algorithm: it explores
    the world using the policy being trained. It is somewhat surprising that Q-learning
    is capable of learning the optimal policy by just watching an agent act randomly.
    Imagine learning to play golf when your teacher is a blindfolded monkey. Can we
    do better?'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Exploration Policies
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Of course, Q-learning can work only if the exploration policy explores the
    MDP thoroughly enough. Although a purely random policy is guaranteed to eventually
    visit every state and every transition many times, it may take an extremely long
    time to do so. Therefore, a better option is to use the *ε-greedy policy* (ε is
    epsilon): at each step it acts randomly with probability *ε*, or greedily with
    probability 1–*ε* (i.e., choosing the action with the highest Q-value). The advantage
    of the *ε*-greedy policy (compared to a completely random policy) is that it will
    spend more and more time exploring the interesting parts of the environment, as
    the Q-value estimates get better and better, while still spending some time visiting
    unknown regions of the MDP. It is quite common to start with a high value for
    *ε* (e.g., 1.0) and then gradually reduce it (e.g., down to 0.05).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, rather than relying only on chance for exploration, another approach
    is to encourage the exploration policy to try actions that it has not tried much
    before. This can be implemented as a bonus added to the Q-value estimates, as
    shown in [Equation 18-6](#exploration_function_equation).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Equation 18-6\. Q-learning using an exploration function
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mi>Q</mi><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo>
    <munder><mo>←</mo><mi>α</mi></munder> <mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><munder><mi>max</mi>
    <mrow><mi>a</mi><mo>'</mo></mrow></munder> <mi>f</mi> <mfenced><mrow><mi>Q</mi><mo>(</mo><mi>s</mi><mo>'</mo><mo>,</mo><mi>a</mi><mo>'</mo>
    <mo>)</mo><mo>,</mo><mi>N</mi><mo>(</mo><mi>s</mi><mo>'</mo><mo>,</mo> <mi>a</mi><mo>'</mo><mo>)</mo></mrow></mfenced></math>
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '*N*(*s*′, *a*′) counts the number of times the action *a*′ was chosen in state
    *s*′.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f*(*Q*, *N*) is an *exploration function*, such as *f*(*Q*, *N*) = *Q* + *κ*/(1
    + *N*), where *κ* is a curiosity hyperparameter that measures how much the agent
    is attracted to the unknown.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approximate Q-Learning and Deep Q-Learning
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main problem with Q-learning is that it does not scale well to large (or
    even medium) MDPs with many states and actions. For example, suppose you wanted
    to use Q-learning to train an agent to play *Ms. Pac-Man* (see [Figure 18-1](#rl_examples_diagram)).
    There are about 150 pellets that Ms. Pac-Man can eat, each of which can be present
    or absent (i.e., already eaten). So, the number of possible states is greater
    than 2^(150) ≈ 10^(45). And if you add all the possible combinations of positions
    for all the ghosts and Ms. Pac-Man, the number of possible states becomes larger
    than the number of atoms in our planet, so there’s absolutely no way you can keep
    track of an estimate for every single Q-value.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: The solution is to find a function *Q*[**θ**](*s*, *a*) that approximates the
    Q-value of any state-action pair (*s*, *a*) using a manageable number of parameters
    (given by the parameter vector **θ**). This is called *approximate Q-learning*.
    For years it was recommended to use linear combinations of handcrafted features
    extracted from the state (e.g., the distances of the closest ghosts, their directions,
    and so on) to estimate Q-values, but in 2013, [DeepMind](https://homl.info/dqn)
    showed that using deep neural networks can work much better, especially for complex
    problems, and it does not require any feature engineering. A DNN used to estimate
    Q-values is called a *deep Q-network* (DQN), and using a DQN for approximate Q-learning
    is called *deep Q-learning*.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Now, how can we train a DQN? Well, consider the approximate Q-value computed
    by the DQN for a given state-action pair (*s*, *a*). Thanks to Bellman, we know
    we want this approximate Q-value to be as close as possible to the reward *r*
    that we actually observe after playing action *a* in state *s*, plus the discounted
    value of playing optimally from then on. To estimate this sum of future discounted
    rewards, we can just execute the DQN on the next state *s*′, for all possible
    actions *a*′. We get an approximate future Q-value for each possible action. We
    then pick the highest (since we assume we will be playing optimally) and discount
    it, and this gives us an estimate of the sum of future discounted rewards. By
    summing the reward *r* and the future discounted value estimate, we get a target
    Q-value *y*(*s*, *a*) for the state-action pair (*s*, *a*), as shown in [Equation
    18-7](#target_q_value_equation).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Equation 18-7\. Target Q-value
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math><mi>y</mi><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo><mo>=</mo><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><munder><mi>max</mi><mrow><mi>a</mi><mo>'</mo></mrow></munder><msub><mi>Q</mi><mi
    mathvariant="bold">θ</mi></msub><mo>(</mo><mi>s</mi><mo>'</mo><mo>,</mo><mi>a</mi><mo>'</mo><mo>)</mo></math>
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: With this target Q-value, we can run a training step using any gradient descent
    algorithm. Specifically, we generally try to minimize the squared error between
    the estimated Q-value *Q*[**θ**](*s*, *a*) and the target Q-value *y*(*s*, *a*),
    or the Huber loss to reduce the algorithm’s sensitivity to large errors. And that’s
    the deep Q-learning algorithm! Let’s see how to implement it to solve the CartPole
    environment.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Deep Q-Learning
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing we need is a deep Q-network. In theory, we need a neural net
    that takes a state-action pair as input, and outputs an approximate Q-value. However,
    in practice it’s much more efficient to use a neural net that takes only a state
    as input, and outputs one approximate Q-value for each possible action. To solve
    the CartPole environment, we do not need a very complicated neural net; a couple
    of hidden layers will do:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To select an action using this DQN, we pick the action with the largest predicted
    Q-value. To ensure that the agent explores the environment, we will use an *ε*-greedy
    policy (i.e., we will choose a random action with probability *ε*):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Instead of training the DQN based only on the latest experiences, we will store
    all experiences in a *replay buffer* (or *replay memory*), and we will sample
    a random training batch from it at each training iteration. This helps reduce
    the correlations between the experiences in a training batch, which tremendously
    helps training. For this, we will just use a double-ended queue (`deque`):'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Tip
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A *deque* is a queue elements can be efficiently added to or removed from on
    both ends. Inserting and deleting items from the ends of the queue is very fast,
    but random access can be slow when the queue gets long. If you need a very large
    replay buffer, you should use a circular buffer instead (see the notebook for
    an implementation), or check out [DeepMind’s Reverb library](https://homl.info/reverb).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Each experience will be composed of six elements: a state *s*, the action *a*
    that the agent took, the resulting reward *r*, the next state *s′* it reached,
    a Boolean indicating whether the episode ended at that point (`done`), and finally
    another Boolean indicating whether the episode was truncated at that point. We
    will need a small function to sample a random batch of experiences from the replay
    buffer. It will return six NumPy arrays corresponding to the six experience elements:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s also create a function that will play a single step using the *ε*-greedy
    policy, then store the resulting experience in the replay buffer:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, let’s create one last function that will sample a batch of experiences
    from the replay buffer and train the DQN by performing a single gradient descent
    step on this batch:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here’s what’s happening in this code:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: First we define some hyperparameters, and we create the optimizer and the loss
    function.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we create the `training_step()` function. It starts by sampling a batch
    of experiences, then it uses the DQN to predict the Q-value for each possible
    action in each experience’s next state. Since we assume that the agent will be
    playing optimally, we only keep the maximum Q-value for each next state. Next,
    we use [Equation 18-7](#target_q_value_equation) to compute the target Q-value
    for each experience’s state-action pair.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to use the DQN to compute the Q-value for each experienced state-action
    pair, but the DQN will also output the Q-values for the other possible actions,
    not just for the action that was actually chosen by the agent. So, we need to
    mask out all the Q-values we do not need. The `tf.one_hot()` function makes it
    possible to convert an array of action indices into such a mask. For example,
    if the first three experiences contain actions 1, 1, 0, respectively, then the
    mask will start with `[[0, 1], [0, 1], [1, 0],...]`. We can then multiply the
    DQN’s output with this mask, and this will zero out all the Q-values we do not
    want. We then sum over axis 1 to get rid of all the zeros, keeping only the Q-values
    of the experienced state-action pairs. This gives us the `Q_values` tensor, containing
    one predicted Q-value for each experience in the batch.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we compute the loss: it is the mean squared error between the target
    and predicted Q-values for the experienced state-action pairs.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we perform a gradient descent step to minimize the loss with regard
    to the model’s trainable variables.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This was the hardest part. Now training the model is straightforward:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We run 600 episodes, each for a maximum of 200 steps. At each step, we first
    compute the `epsilon` value for the *ε*-greedy policy: it will go from 1 down
    to 0.01, linearly, in a bit under 500 episodes. Then we call the `play_one_step()`
    function, which will use the *ε*-greedy policy to pick an action, then execute
    it and record the experience in the replay buffer. If the episode is done or truncated,
    we exit the loop. Finally, if we are past episode 50, we call the `training_step()`
    function to train the model on one batch sampled from the replay buffer. The reason
    we play many episodes without training is to give the replay buffer some time
    to fill up (if we don’t wait enough, then there will not be enough diversity in
    the replay buffer). And that’s it: we just implemented the Deep Q-learning algorithm!'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 18-10](#dqn_rewards_plot) shows the total rewards the agent got during
    each episode.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1810](assets/mls3_1810.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: Figure 18-10\. Learning curve of the deep Q-learning algorithm
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see, the algorithm took a while to start learning anything, in part
    because *ε* was very high at the beginning. Then its progress was erratic: it
    first reached the max reward around episode 220, but it immediately dropped, then
    bounced up and down a few times, and soon after it looked like it had finally
    stabilized near the max reward, at around episode 320, its score again dropped
    down dramatically. This is called *catastrophic forgetting*, and it is one of
    the big problems facing virtually all RL algorithms: as the agent explores the
    environment, it updates its policy, but what it learns in one part of the environment
    may break what it learned earlier in other parts of the environment. The experiences
    are quite correlated, and the learning environment keeps changing—this is not
    ideal for gradient descent! If you increase the size of the replay buffer, the
    algorithm will be less subject to this problem. Tuning the learning rate may also
    help. But the truth is, reinforcement learning is hard: training is often unstable,
    and you may need to try many hyperparameter values and random seeds before you
    find a combination that works well. For example, if you try changing the activation
    function from `"elu"` to `"relu"`, the performance will be much lower.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Reinforcement learning is notoriously difficult, largely because of the training
    instabilities and the huge sensitivity to the choice of hyperparameter values
    and random seeds.⁠^([13](ch18.html#idm45720163009904)) As the researcher Andrej
    Karpathy put it, “[Supervised learning] wants to work. […​] RL must be forced
    to work”. You will need time, patience, perseverance, and perhaps a bit of luck
    too. This is a major reason RL is not as widely adopted as regular deep learning
    (e.g., convolutional nets). But there are a few real-world applications, beyond
    AlphaGo and Atari games: for example, Google uses RL to optimize its datacenter
    costs, and it is used in some robotics applications, for hyperparameter tuning,
    and in recommender systems.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: You might wonder why we didn’t plot the loss. It turns out that loss is a poor
    indicator of the model’s performance. The loss might go down, yet the agent might
    perform worse (e.g., this can happen when the agent gets stuck in one small region
    of the environment, and the DQN starts overfitting this region). Conversely, the
    loss could go up, yet the agent might perform better (e.g., if the DQN was underestimating
    the Q-values and it starts correctly increasing its predictions, the agent will
    likely perform better, getting more rewards, but the loss might increase because
    the DQN also sets the targets, which will be larger too). So, it’s preferable
    to plot the rewards.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: The basic deep Q-learning algorithm we’ve been using so far would be too unstable
    to learn to play Atari games. So how did DeepMind do it? Well, they tweaked the
    algorithm!
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-Learning Variants
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s look at a few variants of the deep Q-learning algorithm that can stabilize
    and speed up training.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Fixed Q-value Targets
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the basic deep Q-learning algorithm, the model is used both to make predictions
    and to set its own targets. This can lead to a situation analogous to a dog chasing
    its own tail. This feedback loop can make the network unstable: it can diverge,
    oscillate, freeze, and so on. To solve this problem, in their 2013 paper the DeepMind
    researchers used two DQNs instead of one: the first is the *online model*, which
    learns at each step and is used to move the agent around, and the other is the
    *target model* used only to define the targets. The target model is just a clone
    of the online model:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, in the `training_step()` function, we just need to change one line to
    use the target model instead of the online model when computing the Q-values of
    the next states:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, in the training loop, we must copy the weights of the online model
    to the target model, at regular intervals (e.g., every 50 episodes):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Since the target model is updated much less often than the online model, the
    Q-value targets are more stable, the feedback loop we discussed earlier is dampened,
    and its effects are less severe. This approach was one of the DeepMind researchers’
    main contributions in their 2013 paper, allowing agents to learn to play Atari
    games from raw pixels. To stabilize training, they used a tiny learning rate of
    0.00025, they updated the target model only every 10,000 steps (instead of 50),
    and they used a very large replay buffer of 1 million experiences. They decreased
    `epsilon` very slowly, from 1 to 0.1 in 1 million steps, and they let the algorithm
    run for 50 million steps. Moreover, their DQN was a deep convolutional net.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s take a look at another DQN variant that managed to beat the state
    of the art once more.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Double DQN
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a [2015 paper](https://homl.info/doubledqn),⁠^([14](ch18.html#idm45720162889472))
    DeepMind researchers tweaked their DQN algorithm, increasing its performance and
    somewhat stabilizing training. They called this variant *double DQN*. The update
    was based on the observation that the target network is prone to overestimating
    Q-values. Indeed, suppose all actions are equally good: the Q-values estimated
    by the target model should be identical, but since they are approximations, some
    may be slightly greater than others, by pure chance. The target model will always
    select the largest Q-value, which will be slightly greater than the mean Q-value,
    most likely overestimating the true Q-value (a bit like counting the height of
    the tallest random wave when measuring the depth of a pool). To fix this, the
    researchers proposed using the online model instead of the target model when selecting
    the best actions for the next states, and using the target model only to estimate
    the Q-values for these best actions. Here is the updated `training_step()` function:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Just a few months later, another improvement to the DQN algorithm was propose;
    we’ll look at that next.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Prioritized Experience Replay
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of sampling experiences *uniformly* from the replay buffer, why not
    sample important experiences more frequently? This idea is called *importance
    sampling* (IS) or *prioritized experience replay* (PER), and it was introduced
    in a [2015 paper](https://homl.info/prioreplay)⁠^([15](ch18.html#idm45720162777584))
    by DeepMind researchers (once again!).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, experiences are considered “important” if they are likely
    to lead to fast learning progress. But how can we estimate this? One reasonable
    approach is to measure the magnitude of the TD error *δ* = *r* + *γ*·*V*(*s*′)
    – *V*(*s*). A large TD error indicates that a transition (*s*, *a*, *s*′) is very
    surprising, and thus probably worth learning from.⁠^([16](ch18.html#idm45720162769408))
    When an experience is recorded in the replay buffer, its priority is set to a
    very large value, to ensure that it gets sampled at least once. However, once
    it is sampled (and every time it is sampled), the TD error *δ* is computed, and
    this experience’s priority is set to *p* = |*δ*| (plus a small constant to ensure
    that every experience has a nonzero probability of being sampled). The probability
    *P* of sampling an experience with priority *p* is proportional to *p*^(*ζ*),
    where *ζ* is a hyperparameter that controls how greedy we want importance sampling
    to be: when *ζ* = 0, we just get uniform sampling, and when *ζ* = 1, we get full-blown
    importance sampling. In the paper, the authors used *ζ* = 0.6, but the optimal
    value will depend on the task.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s one catch, though: since the samples will be biased toward important
    experiences, we must compensate for this bias during training by downweighting
    the experiences according to their importance, or else the model will just overfit
    the important experiences. To be clear, we want important experiences to be sampled
    more often, but this also means we must give them a lower weight during training.
    To do this, we define each experience’s training weight as *w* = (*n* *P*)^(–*β*),
    where *n* is the number of experiences in the replay buffer, and *β* is a hyperparameter
    that controls how much we want to compensate for the importance sampling bias
    (0 means not at all, while 1 means entirely). In the paper, the authors used *β*
    = 0.4 at the beginning of training and linearly increased it to *β* = 1 by the
    end of training. Again, the optimal value will depend on the task, but if you
    increase one, you will usually want to increase the other as well.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at one last important variant of the DQN algorithm.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Dueling DQN
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *dueling DQN* algorithm (DDQN, not to be confused with double DQN, although
    both techniques can easily be combined) was introduced in yet another [2015 paper](https://homl.info/ddqn)⁠^([17](ch18.html#idm45720162754592))
    by DeepMind researchers. To understand how it works, we must first note that the
    Q-value of a state-action pair (*s*, *a*) can be expressed as *Q*(*s*, *a*) =
    *V*(*s*) + *A*(*s*, *a*), where *V*(*s*) is the value of state *s* and *A*(*s*,
    *a*) is the *advantage* of taking the action *a* in state *s*, compared to all
    other possible actions in that state. Moreover, the value of a state is equal
    to the Q-value of the best action *a*^* for that state (since we assume the optimal
    policy will pick the best action), so *V*(*s*) = *Q*(*s*, *a*^*), which implies
    that *A*(*s*, *a*^*) = 0\. In a dueling DQN, the model estimates both the value
    of the state and the advantage of each possible action. Since the best action
    should have an advantage of 0, the model subtracts the maximum predicted advantage
    from all predicted advantages. Here is a simple DDQN model, implemented using
    the functional API:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The rest of the algorithm is just the same as earlier. In fact, you can build
    a double dueling DQN and combine it with prioritized experience replay! More generally,
    many RL techniques can be combined, as DeepMind demonstrated in a [2017 paper](https://homl.info/rainbow):⁠^([18](ch18.html#idm45720162553536))
    the paper’s authors combined six different techniques into an agent called *Rainbow*,
    which largely outperformed the state of the art.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, deep reinforcement learning is a fast-growing field and there’s
    much more to discover!
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Overview of Some Popular RL Algorithms
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we close this chapter, let’s take a brief look at a few other popular
    algorithms:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[*AlphaGo*](https://homl.info/alphago)⁠^([19](ch18.html#idm45720162511200))'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: AlphaGo uses a variant of *Monte Carlo tree search* (MCTS) based on deep neural
    networks to beat human champions at the game of Go. MCTS was invented in 1949
    by Nicholas Metropolis and Stanislaw Ulam. It selects the best move after running
    many simulations, repeatedly exploring the search tree starting from the current
    position, and spending more time on the most promising branches. When it reaches
    a node that it hasn’t visited before, it plays randomly until the game ends, and
    updates its estimates for each visited node (excluding the random moves), increasing
    or decreasing each estimate depending on the final outcome. AlphaGo is based on
    the same principle, but it uses a policy network to select moves, rather than
    playing randomly. This policy net is trained using policy gradients. The original
    algorithm involved three more neural networks, and was more complicated, but it
    was simplified in the [AlphaGo Zero paper](https://homl.info/alphagozero),⁠^([20](ch18.html#idm45720162506192))
    which uses a single neural network to both select moves and evaluate game states.
    The [AlphaZero paper](https://homl.info/alphazero)⁠^([21](ch18.html#idm45720162504560))
    generalized this algorithm, making it capable of tackling not only the game of
    Go, but also chess and shogi (Japanese chess). Lastly, the [MuZero paper](https://homl.info/muzero)⁠^([22](ch18.html#idm45720162502384))
    continued to improve upon this algorithm, outperforming the previous iterations
    even though the agent starts out without even knowing the rules of the game!
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Actor-critic algorithms
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'Actor-critics are a family of RL algorithms that combine policy gradients with
    deep Q-networks. An actor-critic agent contains two neural networks: a policy
    net and a DQN. The DQN is trained normally, by learning from the agent’s experiences.
    The policy net learns differently (and much faster) than in regular PG: instead
    of estimating the value of each action by going through multiple episodes, then
    summing the future discounted rewards for each action, and finally normalizing
    them, the agent (actor) relies on the action values estimated by the DQN (critic).
    It’s a bit like an athlete (the agent) learning with the help of a coach (the
    DQN).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[*Asynchronous advantage actor-critic (A3C)*](https://homl.info/a3c)⁠^([23](ch18.html#idm45720162497280))'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: This is an important actor-critic variant introduced by DeepMind researchers
    in 2016 where multiple agents learn in parallel, exploring different copies of
    the environment. At regular intervals, but asynchronously (hence the name), each
    agent pushes some weight updates to a master network, then it pulls the latest
    weights from that network. Each agent thus contributes to improving the master
    network and benefits from what the other agents have learned. Moreover, instead
    of estimating the Q-values, the DQN estimates the advantage of each action (hence
    the second A in the name), which stabilizes training.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[*Advantage actor-critic (A2C)*](https://homl.info/a2c)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: A2C is a variant of the A3C algorithm that removes the asynchronicity. All model
    updates are synchronous, so gradient updates are performed over larger batches,
    which allows the model to better utilize the power of the GPU.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[*Soft actor-critic (SAC)*](https://homl.info/sac)⁠^([24](ch18.html#idm45720162490480))'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: SAC is an actor-critic variant proposed in 2018 by Tuomas Haarnoja and other
    UC Berkeley researchers. It learns not only rewards, but also to maximize the
    entropy of its actions. In other words, it tries to be as unpredictable as possible
    while still getting as many rewards as possible. This encourages the agent to
    explore the environment, which speeds up training, and makes it less likely to
    repeatedly execute the same action when the DQN produces imperfect estimates.
    This algorithm has demonstrated an amazing sample efficiency (contrary to all
    the previous algorithms, which learn very slowly).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[*Proximal policy optimization (PPO)*](https://homl.info/ppo)⁠^([25](ch18.html#idm45720162487536))'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm by John Schulman and other OpenAI researchers is based on A2C,
    but it clips the loss function to avoid excessively large weight updates (which
    often lead to training instabilities). PPO is a simplification of the previous
    [*trust region policy optimization*](https://homl.info/trpo)⁠^([26](ch18.html#idm45720162483456))
    (TRPO) algorithm, also by OpenAI. OpenAI made the news in April 2019 with its
    AI called OpenAI Five, based on the PPO algorithm, which defeated the world champions
    at the multiplayer game *Dota 2*.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[*Curiosity-based exploration*](https://homl.info/curiosity)⁠^([27](ch18.html#idm45720162479024))'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'A recurring problem in RL is the sparsity of the rewards, which makes learning
    very slow and inefficient. Deepak Pathak and other UC Berkeley researchers have
    proposed an exciting way to tackle this issue: why not ignore the rewards, and
    just make the agent extremely curious to explore the environment? The rewards
    thus become intrinsic to the agent, rather than coming from the environment. Similarly,
    stimulating curiosity in a child is more likely to give good results than purely
    rewarding the child for getting good grades. How does this work? The agent continuously
    tries to predict the outcome of its actions, and it seeks situations where the
    outcome does not match its predictions. In other words, it wants to be surprised.
    If the outcome is predictable (boring), it goes elsewhere. However, if the outcome
    is unpredictable but the agent notices that it has no control over it, it also
    gets bored after a while. With only curiosity, the authors succeeded in training
    an agent at many video games: even though the agent gets no penalty for losing,
    the game starts over, which is boring so it learns to avoid it.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Open-ended learning (OEL)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective of OEL is to train agents capable of endlessly learning new and
    interesting tasks, typically generated procedurally. We’re not there yet, but
    there has been some amazing progress over the last few years. For example, a [2019
    paper](https://homl.info/poet)⁠^([28](ch18.html#idm45720162472416)) by a team
    of researchers from Uber AI introduced the *POET algorithm*, which generates multiple
    simulated 2D environments with bumps and holes and trains one agent per environment:
    the agent’s goal is to walk as fast as possible while avoiding the obstacles.
    The algorithm starts out with simple environments, but they gradually get harder
    over time: this is called *curriculum learning*. Moreover, although each agent
    is only trained within one environment, it must regularly compete against other
    agents, across all environments. In each environment, the winner is copied over
    and it replaces the agent that was there before. This way, knowledge is regularly
    transferred across environments, and the most adaptable agents are selected. In
    the end, the agents are much better walkers than agents trained on a single task,
    and they can tackle much harder environments. Of course, this principle can be
    applied to other environments and tasks as well. If you’re interested in OEL,
    make sure to check out the [Enhanced POET paper](https://homl.info/epoet),⁠^([29](ch18.html#idm45720162468448))
    as well as DeepMind’s [2021 paper](https://homl.info/oel2021)⁠^([30](ch18.html#idm45720162466960))
    on this topic.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-277
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’d like to learn more about reinforcement learning, check out the book
    [*Reinforcement Learning*](https://homl.info/rlbook) by Phil Winder (O’Reilly).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'We covered many topics in this chapter: policy gradients, Markov chains, Markov
    decision processes, Q-learning, approximate Q-learning, and deep Q-learning and
    its main variants (fixed Q-value targets, double DQN, dueling DQN, and prioritized
    experience replay), and finally we took a quick look at a few other popular algorithms.
    Reinforcement learning is a huge and exciting field, with new ideas and algorithms
    popping out every day, so I hope this chapter sparked your curiosity: there is
    a whole world to explore!'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How would you define reinforcement learning? How is it different from regular
    supervised or unsupervised learning?
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you think of three possible applications of RL that were not mentioned in
    this chapter? For each of them, what is the environment? What is the agent? What
    are some possible actions? What are the rewards?
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the discount factor? Can the optimal policy change if you modify the
    discount factor?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you measure the performance of a reinforcement learning agent?
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the credit assignment problem? When does it occur? How can you alleviate
    it?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the point of using a replay buffer?
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is an off-policy RL algorithm?
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use policy gradients to solve OpenAI Gym’s LunarLander-v2 environment.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a double dueling DQN to train an agent that can achieve a superhuman level
    at the famous Atari *Breakout* game (`"ALE/Breakout-v5"`). The observations are
    images. To simplify the task, you should convert them to grayscale (i.e., average
    over the channels axis) then crop them and downsample them, so they’re just large
    enough to play, but not more. An individual image does not tell you which way
    the ball and the paddles are going, so you should merge two or three consecutive
    images to form each state. Lastly, the DQN should be composed mostly of convolutional
    layers.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you have about $100 to spare, you can purchase a Raspberry Pi 3 plus some
    cheap robotics components, install TensorFlow on the Pi, and go wild! For an example,
    check out this [fun post](https://homl.info/2) by Lukas Biewald, or take a look
    at GoPiGo or BrickPi. Start with simple goals, like making the robot turn around
    to find the brightest angle (if it has a light sensor) or the closest object (if
    it has a sonar sensor), and move in that direction. Then you can start using deep
    learning: for example, if the robot has a camera, you can try to implement an
    object detection algorithm so it detects people and moves toward them. You can
    also try to use RL to make the agent learn on its own how to use the motors to
    achieve that goal. Have fun!'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch18.html#idm45720166403424-marker)) For more details, be sure to check
    out Richard Sutton and Andrew Barto’s book on RL, *Reinforcement Learning: An
    Introduction* (MIT Press).'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch18.html#idm45720166401072-marker)) Volodymyr Mnih et al., “Playing Atari
    with Deep Reinforcement Learning”, arXiv preprint arXiv:1312.5602 (2013).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch18.html#idm45720166399568-marker)) Volodymyr Mnih et al., “Human-Level
    Control Through Deep Reinforcement Learning”, *Nature* 518 (2015): 529–533.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch18.html#idm45720166398304-marker)) Check out the videos of DeepMind’s
    system learning to play *Space Invaders*, *Breakout*, and other video games at
    [*https://homl.info/dqn3*](https://homl.info/dqn3).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch18.html#idm45720166374256-marker)) Images (a), (d), and (e) are in the
    public domain. Image (b) is a screenshot from the *Ms. Pac-Man* game, copyright
    Atari (fair use in this chapter). Image (c) is reproduced from Wikipedia; it was
    created by user Stevertigo and released under [Creative Commons BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch18.html#idm45720166354304-marker)) It is often better to give the poor
    performers a slight chance of survival, to preserve some diversity in the “gene
    pool”.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch18.html#idm45720166353536-marker)) If there is a single parent, this
    is called *asexual reproduction*. With two (or more) parents, it is called *sexual
    reproduction*. An offspring’s genome (in this case a set of policy parameters)
    is randomly composed of parts of its parents’ genomes.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch18.html#idm45720166351776-marker)) One interesting example of a genetic
    algorithm used for reinforcement learning is the [*NeuroEvolution of Augmenting
    Topologies*](https://homl.info/neat) (NEAT) algorithm.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch18.html#idm45720166346048-marker)) This is called *gradient ascent*.
    It’s just like gradient descent, but in the opposite direction: maximizing instead
    of minimizing.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch18.html#idm45720166329952-marker)) OpenAI is an artificial intelligence
    research company, funded in part by Elon Musk. Its stated goal is to promote and
    develop friendly AIs that will benefit humanity (rather than exterminate it).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '^([11](ch18.html#idm45720165711216-marker)) Ronald J. Williams, “Simple Statistical
    Gradient-Following Algorithms for Connectionist Reinforcement Leaning”, *Machine
    Learning* 8 (1992) : 229–256.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '^([12](ch18.html#idm45720164871120-marker)) Richard Bellman, “A Markovian Decision
    Process”, *Journal of Mathematics and Mechanics* 6, no. 5 (1957): 679–684.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch18.html#idm45720163009904-marker)) A great [2018 post](https://homl.info/rlhard)
    by Alex Irpan nicely lays out RL’s biggest difficulties and limitations.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '^([14](ch18.html#idm45720162889472-marker)) Hado van Hasselt et al., “Deep
    Reinforcement Learning with Double Q-Learning”, *Proceedings of the 30th AAAI
    Conference on Artificial Intelligence* (2015): 2094–2100.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch18.html#idm45720162777584-marker)) Tom Schaul et al., “Prioritized
    Experience Replay”, arXiv preprint arXiv:1511.05952 (2015).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch18.html#idm45720162769408-marker)) It could also just be that the rewards
    are noisy, in which case there are better methods for estimating an experience’s
    importance (see the paper for some examples).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: ^([17](ch18.html#idm45720162754592-marker)) Ziyu Wang et al., “Dueling Network
    Architectures for Deep Reinforcement Learning”, arXiv preprint arXiv:1511.06581
    (2015).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '^([18](ch18.html#idm45720162553536-marker)) Matteo Hessel et al., “Rainbow:
    Combining Improvements in Deep Reinforcement Learning”, arXiv preprint arXiv:1710.02298
    (2017): 3215–3222.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '^([19](ch18.html#idm45720162511200-marker)) David Silver et al., “Mastering
    the Game of Go with Deep Neural Networks and Tree Search”, *Nature* 529 (2016):
    484–489.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '^([20](ch18.html#idm45720162506192-marker)) David Silver et al., “Mastering
    the Game of Go Without Human Knowledge”, *Nature* 550 (2017): 354–359.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: ^([21](ch18.html#idm45720162504560-marker)) David Silver et al., “Mastering
    Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm”,
    arXiv preprint arXiv:1712.01815.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: ^([22](ch18.html#idm45720162502384-marker)) Julian Schrittwieser et al., “Mastering
    Atari, Go, Chess and Shogi by Planning with a Learned Model”, arXiv preprint arXiv:1911.08265
    (2019).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '^([23](ch18.html#idm45720162497280-marker)) Volodymyr Mnih et al., “Asynchronous
    Methods for Deep Reinforcement Learning”, *Proceedings of the 33rd International
    Conference on Machine Learning* (2016): 1928–1937.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '^([24](ch18.html#idm45720162490480-marker)) Tuomas Haarnoja et al., “Soft Actor-Critic:
    Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor”,
    *Proceedings of the 35th International Conference on Machine Learning* (2018):
    1856–1865.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: ^([25](ch18.html#idm45720162487536-marker)) John Schulman et al., “Proximal
    Policy Optimization Algorithms”, arXiv preprint arXiv:1707.06347 (2017).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '^([26](ch18.html#idm45720162483456-marker)) John Schulman et al., “Trust Region
    Policy Optimization”, *Proceedings of the 32nd International Conference on Machine
    Learning* (2015): 1889–1897.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '^([27](ch18.html#idm45720162479024-marker)) Deepak Pathak et al., “Curiosity-Driven
    Exploration by Self-Supervised Prediction”, *Proceedings of the 34th International
    Conference on Machine Learning* (2017): 2778–2787.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '^([28](ch18.html#idm45720162472416-marker)) Rui Wang et al., “Paired Open-Ended
    Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning
    Environments and Their Solutions”, arXiv preprint arXiv:1901.01753 (2019).'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '^([29](ch18.html#idm45720162468448-marker)) Rui Wang et al., “Enhanced POET:
    Open-Ended Reinforcement Learning Through Unbounded Invention of Learning Challenges
    and Their Solutions”, arXiv preprint arXiv:2003.08536 (2020).'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: ^([30](ch18.html#idm45720162466960-marker)) Open-Ended Learning Team et al.,
    “Open-Ended Learning Leads to Generally Capable Agents”, arXiv preprint arXiv:2107.12808
    (2021).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
